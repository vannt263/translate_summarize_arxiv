{
  "article_text": [
    "ensemble learning methods , with a typical example being boosting @xcite , have been successfully applied to many machine learning and computer vision applications .",
    "its excellent performance and fast evaluation have made ensemble learning one of the most widely - used learning methods , together with kernel machines like svms . in the literature ,",
    "the general connection between boosting and svm has been shown by schapire et al .",
    "@xcite , and rtsch et al .",
    "in particular , rtsch et al .",
    "@xcite developed a mechanism to convert svm algorithms to boosting - like algorithms by translating the quadratic programs ( qp ) of svms into linear programs ( lp ) of boosting ( similar to lpboost @xcite ) .",
    "a one - class boosting method was then designed by converting one - class svm into an lp . following this vein ,",
    "a direct approach to multi - class boosting was developed in @xcite by using the loss function in crammer and singer s multi - class svm @xcite .",
    "the recipe to transfer algorithms is essentially @xcite : `` the sv - kernel is replaced by an appropriately constructed hypothesis space for leveraging where the optimization of _ an analogous mathematical program is done using @xmath0 instead of @xmath1-norm .",
    "_ '' this transfer is _ indirect _ in the sense that one has to design a different @xmath0 norm regularized mathematical program .",
    "we suspect that this is due to the widely - adopted belief that boosting methods need the sparsity - inducing @xmath0-norm regularization so that the final ensemble model only relies on a subset of weak learners @xcite .",
    "regularization so that the kernel trick can be applied , although @xmath2 svm @xcite takes a different approach . ] in this work , we show that it is possible to design ensemble learning methods by _ directly _ solving standard svm optimization problems . unlike @xcite , no mathematical transform is needed .",
    "the only optimization technique that our framework relies on is column generation . with the proposed framework ,",
    "the advantages that we can think of are : 1 ) many kernel methods can directly have an equivalent ensemble model ; 2 ) as conventional boosting methods , our ensemble models are iteratively learned . at each iteration , compared with the @xmath2 optimization involved in the indirect approach @xcite , our optimization problems are much simpler .",
    "for the first time , we enable the use of fast linear svm optimization software for ensemble learning .",
    "3 ) as the fully - corrective boosting methods in @xcite , our ensemble learning procedure is also fully corrective . therefore the convergence speed is often much faster than stage - wise boosting .",
    "4 ) kernel svms usually offer promising classification accuracies at the price of high usage of memory and evaluation time , especially when the size of training data is large .",
    "recall that the number of support vectors is linearly proportional to the number of training data @xcite .",
    "ensemble models , on the other hand , are often much faster to evaluate .",
    "ensemble learning is also more flexible in that the user can determine the number of weak learners used .",
    "typically an ensemble model uses less than a few thousand weak learners .",
    "ensemble learning can also select features by using decision stumps or trees as weak learners , while nonlinear kernels are defined on the entire feature space .",
    "the developed framework tries to enjoy the best of both worlds of kernel machines and ensemble learning .",
    "additional contributions of this work include : 1 ) to exemplify the usefulness of this proposed framework , we introduce a new multi - class boosting method based on the recent multi - class svm @xcite .",
    "the new multi - class boosting is effective in performance and can be efficiently learned since a closed - form solution exists at each iteration .",
    "2 ) we introduce fourier features as weak learners for learning the strong classifier .",
    "fourier features approximate the radial basis function ( rbf ) gaussian kernel .",
    "our experiments demonstrate that fourier weak learners usually outperforms decision stumps and linear perceptrons .",
    "3 ) we also show that multiple kernel learning is made much easier with the proposed framework .",
    "the general connection between svm and boosting has been discussed by a few researchers @xcite at a high level . to our knowledge",
    ", the work here is the first one that attempts to build ensemble models by solving svm s optimization problem .",
    "we review some closest work next .",
    "boosting has been extensively studied in the past decade @xcite .",
    "our methods are close to @xcite in that we also use column generation ( cg ) to select weak learners and fully - correctively update weak learners coefficients . because we are solving the svm problem , instead of the @xmath2 regularized boosting problem , conventional cg can not be directly applied .",
    "we use cg in a novel way  instead of looking at dual constraints , we rely on the kkt conditions .",
    "if one uses an infinitely many weak learners in boosting @xcite ( or hidden units in neural networks @xcite ) , the model is equivalent to svm with a certain kernel . in particular",
    ", it shows that when the feature mapping function @xmath3 contains infinitely many randomly distributed decision stumps , the kernel function @xmath4 is the stump kernel of the form @xmath5 .",
    "here @xmath6 is a constant , which has no impact on the svm training .",
    "moreover , when @xmath7 , , a perceptron , the corresponding kernel is called the perceptron kernel @xmath8 @xmath9 .    loosely speaking ,",
    "boosting can be seen as explicitly computing the kernel mapping functions because , as pointed out in @xcite , a kernel constructed by the inner product of weak learners outputs satisfies the mercer s condition .",
    "random fourier features ( rff ) @xcite have been applied to large - scale kernel methods .",
    "rff is designed by using the fact that a shift - invariant kernel is the fourier transform of a non - negative measure .",
    "yang et al .",
    "show that rff does not perform well due to its _ data - independent _ sampling strategy when there is a large gap in the eigen - spectrum of the kernel matrix @xcite . in @xcite , it shows that for homogeneous additive kernels , the kernel mapping function can be exactly computed .",
    "when rff is used as weak learners in the proposed framework here , the greedy cg based rff selection can be viewed as _ data - dependent _ feature selection . indeed ,",
    "our experiments demonstrate that our method performs much better than random sampling .",
    "we first review some fundamental concepts in svms and boosting .",
    "we then show the connection between these two methods and show how to design column generation based ensemble learning methods that directly solve the optimization problems in kernel methods .",
    "let us consider binary classification for the time being .",
    "assume that the input data points are @xmath10 @xmath11 @xmath12 with @xmath13 for svms , it is well known that the original data @xmath14 are implicitly mapped to a feature space through a mapping function @xmath15 .",
    "the function @xmath16 is implicitly defined by a kernel function @xmath17 , which computes the inner product in @xmath18 .",
    "svm finds a hyperplane that best separates the data by solving : @xmath19 subject to the margin constraints @xmath20 , @xmath21 . here",
    "@xmath22 is a vector of all @xmath23 s .",
    "the lagrange dual can be easily derived : @xmath24 here @xmath25 is the trade - off parameter ; @xmath26 is the kernel matrix with @xmath27 ; and @xmath28 denotes element - wise matrix multiplication , , hadamard product .",
    "@xmath29 is the label matrix with @xmath30 @xmath31 @xmath32^\\t $ ] .",
    "note that in the case of linear svms , , @xmath33 , there are fast and scalable algorithms for training linear svms , , @xcite .",
    "ensemble learning methods , with boosting being the typical example , usually learns a strong classifier / model by linearly combining a finite set of weak learners .",
    "formally the learned model is @xmath34 with @xmath35^\\t .",
    "\\end{aligned}\\ ] ] therefore , in the case of boosting , the feature mapping function is explicitly learned : @xmath36 @xmath37 @xmath38 @xmath39 @xmath40^\\t ,          $ ] where @xmath41 is the weak learner .",
    "it is easy to see that a kernel induced by the weak learner set @xmath42 is a valid one and its corresponding kernel matrix must be positive semidefinite .",
    "next let us take lpboost as an example to see how cg is used to explicitly learn weak learners , which is the core of most boosting methods @xcite .",
    "the primal program of lpboost can be written as @xmath43 subject to the margin constraint @xmath44 , @xmath45 . with @xmath46",
    "defined in .",
    "the dual of is @xmath47 note that the last constraint in the dual is a set of @xmath48 constraints .",
    "often , the number of possible weak learners can be infinitely large . in this case",
    "it is intractable to solve either the primal or dual . in this case , cg can be used to solve the problem .",
    "these original problems are referred to as the master problems .",
    "the cg method solves these problems by incrementally selecting a subset of columns ( variables in the primal and constraints in the dual ) and optimizing the restricted problem on the subset of variables .",
    "so the basic idea of cg is to add one constraint at a time to the dual problem until an optimal solution is identified . in terms of the primal problem",
    ", cg solves the problem on a subset of variables , which corresponds to a subset of constraints in the dual .",
    "if a constraint absent from the dual problem is violated by the solution to the restricted problem , this constraint needs to be included in the dual problem to further restrict its feasible region . to speed convergence",
    "we would like to find the one with maximum deviation ( most violated dual constraint ) , that is , the base learning algorithm must deliver a function @xmath49 such that @xmath50 if there is no weak learner @xmath51 for which the dual constraint @xmath52 is violated , then the current combined hypothesis is the optimal solution over all linear combinations of weak learners .",
    "that is the main idea of lpboost @xcite and its extension @xcite .",
    "it has been believed that here two components have played an essential role in this procedure of deriving this meaningful dual such that cg can be applied .",
    "1 ) this derivation relies on the @xmath2 norm regularization in the primal objective of @xmath53 .",
    "2 ) the constraint of nonnegative @xmath54 lead to the dual inequality constraint . without this nonnegative constraint ,",
    "the last dual constraint becomes an equality : @xmath55 . in terms of optimization",
    ", the constraint @xmath56 causes difficulties .",
    "we will show the remedies for these difficulties in the next section .",
    "we show how to derive ensemble learning directly from kernel methods like svm .",
    "our goal is to explicitly solve . in other words , similar to boosting ,",
    "we iteratively solve by _ explicitly learning the kernel mapping function @xmath3_. at the first glance , it is unclear how to use the idea of cg to derive a boosting - like procedure similar to lpboost , as discussed above . in order to add a weak learner @xmath57 into @xmath46 by finding the most violated dual constraint  as a starting point ",
    "we must have a dual constraint containing @xmath46 . from the dual problem of svm",
    ", the main difficulty here is that the dual constraints are two types of simple linear constraints on the dual variable @xmath58 .",
    "the dual constraints do not have @xmath59 at all . a condition for applying cg",
    "is that the duality gap between the primal and dual problems is zero ( strong duality ) . generally , the primal problem must be convex and both the primal and dual are feasible , so the slater condition holds .",
    "in such a case , the kkt conditions are necessary conditions for a solution to be optimal .",
    "one such condition in deriving the dual from is @xmath60 this kkt condition is the root of the _ representer theorem _ in kernel methods , which states that a minimizer of a regularized empirical risk function defined over a reproducing kernel hilbert space can be represented as a finite linear combination of kernel products evaluated on the input points .",
    "we can verify the optimality by checking the dual feasibility and kkt conditions . at optimality , must hold for all @xmath61 , , @xmath62 must hold for all @xmath61 . for the columns / weak learners in the current working set , , @xmath63 , the corresponding condition in is satisfied by the current solution . for the weak learners that are not selected yet , they do not appear in the current restricted optimization problem and the corresponding @xmath64 .",
    "it is easy to see that if @xmath65 for any @xmath66 that is not in the current working set , then current solution is already the globally optimal one .",
    "so , our base learning strategy to check the optimality as well as to select the best weak learner @xmath67 is : @xmath68 different from , here we select the weak learner with the score @xmath69 farthest from 0 , which can be negative .",
    "now we show that using to choose a weak learner is not heuristic in terms of solving the svm problem of .    at iteration @xmath70 ,",
    "the weak learner selected using decreases the duality gap the most for the current solution obtained at iteration @xmath48 , in terms of solving the svm primal problem or dual .    to prove the above result ,",
    "let us check the dual objective in .",
    "we denote the current working set ( corresponding to current selected weak learners ) by @xmath71 and the rest by @xmath72 .",
    "the dual objective in is @xmath73 here the @xmath74 entry of @xmath75 is @xmath76 and likewise , @xmath77 clearly the sum of first terms in equals to the objective value of the primal problem with the current solution : @xmath78 .",
    "therefore the duality gap is the last term of : @xmath79 ^ 2 .",
    "$ ] clearly minimization of this duality gap leads to the base learning rule .",
    "next , we show that it can be equivalent between and .",
    "let us assume that the weak learn set @xmath80 is negation complete ; , if @xmath81 , then @xmath82(\\cdot ) \\in \\calh $ ] ; and vice versa .",
    "here @xmath82(\\cdot ) $ ] means the function @xmath82(\\cdot ) = - ( \\wl(\\cdot))$ ] . then to solve",
    ", we only need to solve .",
    "[ res : wl ]    this result is straightforward . because @xmath80 is negation complete , if a maximizer of , @xmath83 , leads to @xmath84 @xmath85 , then @xmath86(\\cdot ) \\in \\calh $ ] is also a maximizer of such that @xmath84 @xmath87 .",
    "therefore we can always solve to obtain the maximizers of .    at this point",
    ", we are ready to design cg based ensemble learning for solving the svm problem , analogue to boosting , , lpboost .",
    "the proposed ensemble learning method , termed , is summarized in algorithm [ alg:1 ] .",
    "note that at line 6 , we can use very efficient linear svm solvers to solve either the primal or dual . in our experiments ,",
    "we have used @xcite .",
    "[ alg:1 ]    having shown how to solve the standard svm problem using cg , we provide another example application of the proposed framework by developing a new multi - class ensemble method using the idea of simplex coding @xcite .",
    "as most real - world problems are inherently multi - class , multi - class learning is becoming increasingly important .",
    "coding matrix based boosting methods are one of the popular boosting approaches to multi - class classification .",
    "methods in this category include adaboost.mo @xcite , adaboost.oc and adaboost.ecc @xcite .",
    "shen and hao proposed a direct approach to multi - class boosting in @xcite . here",
    "we proffer a new multi - class ensemble learning method based on the simplex least - squares svm ( sls - svm ) introduced in @xcite .",
    "sls - svm can be seen as a generalization of the binary ls - svm ( least - squares svm ) . for binary classification ,",
    "ls - svm fits the decision function output to the label : @xmath88 with @xmath89 . in the case of multi - class classification ,",
    "label @xmath90 .",
    "here we have @xmath91 classes .",
    "simplex coding maps each class label to @xmath92 most separated vectors @xmath93 on the unit hypersphere in @xmath94 .",
    "so we need to learn @xmath95 classifiers .",
    "let us assume that the simplex label coding function is @xmath96 @xmath97 @xmath98 ( see appendix ) .",
    "the label matrix @xmath99 collects training data s coded labels such that each row @xmath100 .",
    "sls - svm trains the @xmath95 classifiers simultaneously by minimizing the following regularized problem : @xmath101 here the model parameters to optimize are @xmath102 , @xmath103 , for @xmath104 .",
    "problem can be solved by deriving its dual ( see appendix ) and the solutions are : @xmath105 here @xmath106 denotes the learned weak classifiers response on the whole training data such that each column @xmath107 ; @xmath108 ; @xmath109 is the @xmath110 identity matrix ; @xmath111 is the dual lagrange multiplier associated with the equality constraints of @xmath112 .",
    "note that , the inverse of @xmath113 can be computed efficiently incrementally ( see the supplementary document ) .",
    "here one of the kkt conditions that cg relies on is ( last equation of ): @xmath114 similar to the binary case , the subproblem for generating weak learners is @xmath115 for the same reason as in result [ res : wl ] , we have removed the absolute operation without changing the essential problem .",
    "a subtle difference from the binary classification is that we pick the best weak learner across all the @xmath95 classifiers .",
    "[ alg : lssvm ]    we summarize our multi - class ensemble method in algorithm [ alg : lssvm ] .",
    "the output is the @xmath95 classifiers : @xmath116 ,          \\end{split}\\ ] ] the classification rule assigns the label @xmath117 to the test datum @xmath14 .",
    "we run experiments on binary and multi - class classification problems , and compare our methods against classical boosting methods .",
    "we conduct experiments on synthetic as well as real datasets , namely , 2d toy , 13 uci benchmark datasets ] , and then on several vision tasks such as digits recognition , pedestrian detection .    * 2d toy data * the data are generated by sampling points from a 2d gaussian distribution .",
    "all points within a certain radius belong to one class and all outside belong to the other class .",
    "the decision boundary obtained by with decision stumps is plotted in fig .",
    "[ figtoy2000 ] .",
    "as can be seen , our method converges faster than adaboost because is fully corrective .",
    "* uci benchmark * for the uci experiment , we use three different weak learners , namely , decision stumps , perceptrons and fourier weak learners , with each compared with the corresponding kernel svms and other boosting methods .",
    "we use decision stumps ( stump kernel for svm ) in this experiment .",
    "we compared our method with adaboost , lpboost , all using decision stumps as the weak learner .",
    "all the parameters are chosen using @xmath118-fold cross validation .",
    "the maximum iteration for adaboost , lpboost @xcite and our are searched from @xmath119 .",
    "results of svms with the stump kernel are also reported @xcite .",
    "results are the average of 5 random splits on each dataset . from table",
    "[ tabaccucistump ] , we can see that overall , all the methods achieve comparable accuracy , with being marginally the best , and svm the second best .    in the second experiment",
    ", we compare our method ( using 500 weak learners ) against several other methods such as svm using ( 1 ) perceptrons @xmath120 as weak learners and the perceptron kernel for svm , and ( 2 ) fourier cosine functions @xcite @xmath121 as weak learners and gaussian rbf kernel for svm .",
    "we did not optimize the weak learner s parameters @xmath122 .",
    "instead , we sample 2000 pairs of @xmath122 according to their distributions as described in @xcite and @xcite , and then pick the one that maximizes the weak learner selection criterion in equ .  .",
    "in the case of the perceptron kernel , same as the decision stump kernel , it is parameter free . in the case of gaussian rbf kernel ( corresponding fourier weak learners ) , there is gaussian bandwidth parameter @xmath123 . here",
    "@xmath124 in fourier is sampled from a gaussian distribution with the same bandwidth .",
    "we cross validate this bandwidth parameter @xmath123 with the svm and use the same @xmath123 for sampling fourier weak learners for use in .",
    "ideally one can cross validate @xmath123 with , which needs extra computation overhead .",
    "this might be the reason why rbf svm is slightly better with fourier weak learners as shown in table [ tabaccucirbf ] , because uses the optimal @xmath123 of svm .",
    "while in the case of perceptrons , performs on par with svm .",
    "note that as expected , in general , and svm again outperform adaboost and lpboost .",
    "we have also compared our with rff @xcite .",
    "although uses 500 features ( weak learners ) and rff uses 2000 features , still slightly outperforms rff .",
    "* computation efficiency of * we evaluate the computation efficiency of the proposed .",
    "as mentioned , at each cg iteration of , we need to solve a linear svm and therefore we can take advantage of off - the - shelf fast linear svm solvers . here",
    "we have used .",
    "we compare against lpboost because both are fully - corrective boosting . at each iteration",
    ", lpboost needs to solve a linear program .",
    "we use the state - of - the - art commercial solver mosek @xcite to solve the dual problem .",
    "the dual problem has less number of variables than the primal .",
    "thus it more efficient for mosek to solve the dual problem .",
    "we run experiments on a standard desktop using the mnist data to differentiate odd from even digits .",
    "first we vary the number of iterations ( selected weak learners ) while fixing the number of training data to be @xmath125 . for the second one ,",
    "we vary the number of training data and fix the iteration number to be 100 .",
    "[ fig : speed1 ] reports the comparison results .",
    "note that the cpu time includes the training time of decision stumps .",
    "overall , is orders of magnitude faster than lpboost .          to demonstrate the potential effectiveness of the proposed ensemble learning method in multi - class classification task , we test the proposed algorithm both on uci and image benchmark datasets . for fair comparison , we focus on the multi - class algorithms using binary weak learners , including adaboost.ecc @xcite , adaboost.mh @xcite and multiboost @xcite using the exponential loss .",
    "the proposed method is more related to multiboost in the sense that both use the fully - corrective boosting framework , yet it employs an ls regression - type formulation of multi - class classifier and a closed - form solution exists for the sub optimization problem during each iteration . for all boosting algorithms ,",
    "decision stumps are chose as the weak learners due to its simplicity and the controlled complexity .",
    "similar to the binary classification experiments , the maximum number of iteration is set to 500 .",
    "the regularization parameters in our and multiboost @xcite are both determined by 5-fold cross validation .",
    "we first test the proposed on 7 uci datasets , and then on several vision tasks .",
    "the results are summarized in table [ table : lssvm ] .    *",
    "uci datasets * for each of the 7 dataset , all samples are randomly divided into 75% for training and 25% for test , regardless of the existence of a pre - specified split . each algorithm",
    "is evaluated 10 times and the average results are reported .    *",
    "handwritten digit recognition * three handwritten digit datasets are evaluated here , namely , mnist , usps and pendigits . for mnist , we randomly sample 1000 examples from each class for training and use the original test set of 10000 examples for test . for usps , we randomly select @xmath126 for training and the rest for test . *",
    "image classification *",
    "we then apply the proposed for image classification on several datasets : pascal07 @xcite , labelme @xcite and cifar10 . for pascal07 , we use 5 types of features provided in @xcite . for labelme",
    ", we use the labelme-12 - 50k subset and generate the gist @xcite features .",
    "images which have more than one class labels are excluded for these two datasets .",
    "we use 70% of the data for training , and the rest for test . as for cifar10",
    ", we also use the gist @xcite features and use the provided test and training sets .",
    "* scene recognition * the scene15 dataset consists of 4,485 images of 9 outdoor scenes and 6 indoor scenes .",
    "we randomly select 100 images per class for training and the rest for test .",
    "each image is divided into 31 sub - windows , each of which is represented as a histogram of 200 visual code words , leading to a 6200d representation . for the sun dataset , we construct a subset of the original dataset containing 25 categories , where the top 200 images are selected from each category . for the subset",
    ", we randomly select 80% data for training and the rest for test .",
    "hog features described in @xcite are used as the image feature .    from table [ table : lssvm ]",
    ", we can see that the proposed achieves overall best performance , especially on the vision datasets .",
    "figure [ fig : mc - iter ] shows the test error and training time comparison with respect to different iteration numbers on four image datasets . our proposed performs slightly better than all the other methods in terms of classification accuracy while being more efficient than adaboost.mh and multiboost .             +",
    "kernel methods are popular in domains even outside of the computer science community largely because they are easy to use and there are highly optimized software available . on the other hand ,",
    "ensemble learning is being developed in a separate direction and has also found its applications in various domains . in this work ,",
    "we show that _ one can directly design ensemble learning methods from kernel methods like svms . _ in other words , one may directly solve the optimization problems of kernel methods by using column generation technique .",
    "the learned ensemble model is equivalent to learning the explicit feature mapping functions of kernel methods .",
    "this new insight about the precise correspondence enables us to design new algorithms .",
    "in particular we have shown two examples of new ensemble learning methods which have roots in svms .",
    "extensive experiments show the advantages of these new ensemble methods over conventional boosting methods in term of both classification accuracy and computation efficiency .",
    "the lagrangian of problem ( [ eq : slsvm1 ] ) is @xmath127 where @xmath128 is the collection of lagrange variables corresponded to @xmath112 .",
    "the optimization problem can be solved by setting its first order partial derivative with respect to the parameters @xmath129 and @xmath130 to zeros : @xmath131          the inverse for @xmath113 can be computed efficiently as follows .",
    "suppose @xmath138 and @xmath139 are matrices in the @xmath140th and @xmath141th iteration , respectively .",
    "we have @xmath142^{\\t}$ ] , where @xmath143^{\\t}$ ] .",
    "it is easy to see that @xmath144 and @xmath145 are symmetric matrices .",
    "so , @xmath146 let @xmath147 , the update process finally is @xmath148               we performed experiments on the uci spam dataset to demonstrate the feature selection of the proposed method when using decision stump as weak learner .",
    "the task is to differentiate spam emails according to word frequencies .",
    "we use adaboost as a baseline .",
    "the maximum iterations are both set to 60 due to fast convergence and no overfitting observed hereafter .",
    "we use 5-fold cross validation to choose the best hyper parameter @xmath149 in .",
    "the results , shown in table [ tabspam ] , are reported over 20 different runs with training / testing ratio being @xmath150 .",
    "[ figspam ] plots the average frequencies over the @xmath151 rounds .",
    "as can be observed , both algorithms select important features such as  free \" ( feature # 16 ) ,  hp \" ( 25 ) and  ! \" ( 52 ) with high frequencies . as for the other features , the two methods showed different inclinations .",
    "tends to select features like  remove \" ( 7 ) ,  you \" ( 19 ) ,  $ \" ( 53 ) which are intuitively meaningful for the classification . on the contrary ,",
    "the favorite ones of adaboost are  george \" ( 27 ) ,  meeting \" ( 42 ) and  edu \" ( 46 ) which are more irrelevant for spam email detection .",
    "this explains why our method slightly outperformed adaboost in test accuracy ."
  ],
  "abstract_text": [
    "<S> ensemble methods such as boosting combine multiple learners to obtain better prediction than could be obtained from any individual learner . here </S>",
    "<S> we propose a principled framework for _ directly _ constructing ensemble learning methods from kernel methods . unlike previous studies showing the equivalence between boosting and support vector machines ( svms ) , which needs a translation procedure , </S>",
    "<S> we show that it is possible to design boosting - like procedure to solve the svm optimization problems . </S>",
    "<S> in other words , it is possible to design ensemble methods directly from svm without any middle procedure . </S>",
    "<S> this finding not only enables us to design new ensemble learning methods directly from kernel methods , but also makes it possible to take advantage of those highly - optimized fast linear svm solvers for ensemble learning . </S>",
    "<S> we exemplify this framework for designing binary ensemble learning as well as a new multi - class ensemble learning methods . </S>",
    "<S> experimental results demonstrate the flexibility and usefulness of the proposed framework .    </S>",
    "<S> kernel , support vector machines , ensemble learning , column generation , multi - class classification . </S>"
  ]
}