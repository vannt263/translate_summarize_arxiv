{
  "article_text": [
    "in many applications , only a finite set of interesting samples sharing common traits are given , and the goal is to search other samples sharing the same traits from datasets .",
    "for example , in rare astrophysical objects searching , the finite set can be a series of spectra known as a specific unusual class comparing with main sequence stars(such as carbon stars , dz white dwarfs , l dwarfs , etc . ) , and the goal is to search for as many as spectra with the same class in massive astronomical data sets . in this application ,",
    "the number of positive ( interesting but rare ) samples are limited , while the unlabelled samples dominate the dataset .",
    "conceptually , learning from the positive and unlabeled samples is usually called pu ( positive - unlabeled ) learning which arises frequently in retrieval applications .",
    "formally , we assume that @xmath0 denotes a set of data belonging to the instance space @xmath1 , @xmath2 is a set of ( usually small ) data with positive labels , and @xmath3 is the set of ( usually large ) unlabeled data .",
    "then we need to learn from @xmath4 and @xmath5 in order to identify the positive samples from @xmath5 as accurate as possible .",
    "the purpose of pu learning is to learn a scoring function @xmath6 from @xmath4 and @xmath5 , which is able to predict a score to each unlabeled data in @xmath5 . for @xmath7 @xmath8 ,",
    "the higher the prediction score @xmath9 indicates the larger probability to be a positive sample .",
    "there are many different algorithms developed to solve the pu learning problem over the last decade , and we summarise them into two groups : classification - based pu learning and rank - based pu learning .",
    "the classification - based pu learning could be traced back to building classifier with only positive sample , such as one - class svm ( ocsvm ) @xcite and support vector data description ( svdd ) @xcite .",
    "both ocsvm and svdd need sufficient positive samples to garentee the boundary of the positive class can be induced precisely . aside from the positive samples",
    ", unlabeled samples can also provide useful information , and have been used along with the positive samples by biased svm @xcite .",
    "mordelet et al .",
    "@xcite generalised biased svm and proposed a method employing bootstrap aggregating ( bagging ) techniques @xcite , called bagging svm .",
    "the author showed that bagging svm method can match and even outperform the performance of biased svm . furthermore , bagging svm can greatly alleviate the computation burden of biased svm , in particular when unlabeled samples domainate the dataset . to the best of our knowledge , bagging svm represents the state - of - the - art algorithm for pu learning .    for rank - based pu learning ,",
    "the core idea is to build a ranking model which ranks a set of unlabeled samples based on their relevance scores for the given positive samples .",
    "particularly , the graph - based ranking method is widely used in pu learning , such as label propagation ( lp ) @xcite and manifold ranking ( mr ) @xcite .",
    "these methods have been applied to search for carbon stars from massive spectral data , successfully retrieving 260 and 183 carbon stars from sdss dr8 and large sky area multi - object fiber spectroscopic telescope ( lamost ) , respectively @xcite .",
    "other recent works have considered the pu learning as the bipartite ranking problem @xcite .",
    "specifically , the negative samples are chosen from @xmath5 according to some rules , such as similarity measure @xcite and random sampling @xcite .",
    "once a sample in @xmath5 is chosen as a negative one , it will be assigned a negative label in the training stage . after generating procedure for negative samples ,",
    "the positive and negative samples in the rest of @xmath5 are called relevant and irrelevant samples , respectively . then",
    ", they develop models ranking positive samples ahead of the chosen negative samples , based on the assumption that such models would also place the other relevant samples ahead of irrelevant samples .    in this paper , we treat the rare spectra retrieval as the bipartite ranking task , and present a new pu learning method to solve this problem .",
    "more precisely , inspired by the idea of bagging techniques which have shown to be useful for improving the stability and accuracy of machine learning algorithms @xcite , we developed a new method , namely , baggingtoppush that combines bagging techniques with toppush model @xcite .",
    "it is worth noting that baggingtoppush method focus on optimising ranking accuracy at the top of the ranked list .",
    "this fulfils the scientific need for an accurately top ranked list of candidates under a given rare category .",
    "furthermore , baggingtoppush is a highly efficient approach that has computational complexity linear in the number of training samples . in rare spectra retrieval",
    ", we usually only have a small set of positive samples and a large set of unlabeled samples .",
    "in other words , it does not contain any explicit set of negative samples ( irrelevant spectra ) , and it is time - consuming to manually select negative ones frequently .",
    "besides that , a small portion of the negative samples selected manually may not be able to represent the overall nature of the negative data . here , we follow the method used by mordelet et al .",
    "@xcite and randomly select the unlabeled spectra to generate ` negative ' samples . then the baggingtoppush method consists in aggregating bipartite ranking functions trained to place @xmath4 before a small random subsample of @xmath5 . in order to check the effectiveness and efficiency of the method for rare spectra retrieval ,",
    "several popular pu learning methods are introduced and compared with the proposed one in this paper . for easier application , the influence of baggingtoppush s parameters on the ranking performance",
    "were also analysed to obtain safe parameter choices .",
    "the paper is organised as follows . in section 2 ,",
    "the toppush method based on bipartite ranking is briefly described firstly , and then the development of a bagging strategy for rare spectra retrieval is presented . in section 3 , the experimental data , comparative methods , parameters setting , and evaluation metrics are given .",
    "in section 4 , the detailed experimental evaluation and comparison of several pu learning methods are shown .",
    "finally , the conclusion is drawn in section 5 .",
    "in recent years , bipartite ranking has attracted much attention because of its successful applications in several areas such as information retrieval and recommendation systems @xcite .",
    "the goal of bipartite ranking is to learn a ranking model such that those samples belonging to one category are ranked higher than those samples belonging to the other category . in some data mining applications such as web page searching and rare spectra retrieval , to learn a ranking function with well performance at the top of the ranked list are more interested since only the top ranked candidates are possible to be examined by experts@xcite .",
    "toppush proposed by li et al .",
    "@xcite is such a bipartite ranking model that can efficiently optimise the ranking accuracy at the top .",
    "in contrast to most other methods for bipartite ranking whose computational costs grow quadratically in the number of training samples , the time complexity of toppush algorithm is only linear in the number of training samples . in the following of this section , we will first describe the toppush algorithm , and then develop the bagging strategy used to retrieve rare spectra .",
    "let @xmath10 be a set of training data , including @xmath11 positive samples and @xmath12 negative samples randomly sampled from @xmath4 and @xmath5 , respectively , i.e. , @xmath13 and @xmath14 .",
    "the goal of toppush is to learn a ranking function @xmath6 that is likely to maximise the number of positive samples that are ranked before the first negative sample .",
    "this objective can be translated into the minimisation of the following loss function @xmath15 where @xmath16 is the indicator function with @xmath17 and 0 otherwise . by minimising the loss function in equation ( [ toppush loss ] )",
    ", it essentially pushes negative samples away from the top of the ranked list , leading to more positive ones placed at the top . since the indicator function @xmath16 is not a smooth function , li et al .",
    "replaced the indicator function in equation ( [ toppush loss ] ) with its convex surrogate loss function @xmath18 that is non - decreasing and differentiable , leading to the following loss function @xmath19 in practice , the convex surrogate loss functions include truncated quadratic loss @xmath20 , exponential loss @xmath21 and logistic loss @xmath22 , etc . here , we restrict ourselves to the truncated quadratic loss function .    for linear ranking function ( @xmath23 ) , the learning problem is given by the following optimisation formulation @xmath24 where @xmath25 is the weight vector to be learned , and @xmath26 is a regularisation parameter that controls the model complexity .",
    "more discussions about the optimisation , computational complexity and performance guarantee for toppush algorithm can be found in @xcite .      in rare spectra retrieval ,",
    "given some rare spectra our final objective is to rank relevant samples ahead of irrelevant samples . to this end",
    ", one key assumption is that learning to place @xmath4 ( rare spectra ) before a small random subsample of @xmath5 ( unlabeled spectra ) is a good proxy to our objective .",
    "however , the unlabeled spectra set @xmath5 is contaminated by hidden positive spectra , and the percentage of positive spectra in @xmath5 is usually unknown in real - world applications . for a small random subsample of @xmath5 , the contamination ( percentage of rare spectra ) can be small or large , which will induce a large instability in the ranking function .",
    "fortunately , this situation can be advantageously exploited by bagging techniques which are designed to improve the stability and accuracy of unstable machine learning algorithms @xcite .",
    "we assume that @xmath27 is the number of samples randomly selected from @xmath5 by one bootstrap and @xmath28 is the number of bootstraps .",
    "the baggingtoppush first creates a series of bipartite ranking function trained to rank @xmath4 ahead of the random subsamples of @xmath5 . the output of each of these bipartite ranking functions @xmath29 can assign a ranking score to any sample .",
    "then the final aggregated ranking function @xmath30 can be simply defined as the average score of the individual bipartite ranking functions , and we can sort spectral samples according to @xmath30 in a descending order and return the top ranked ones as results . in summary ,",
    "the baggingtoppush method for rare spectra retrieval is presented in algorithm 1 .",
    "note that the input variable @xmath31 plays the same role as that in equation ( [ optimizasion problem ] ) , i.e. , it controls the complexity of each toppush model .",
    "the smaller the value of @xmath31 , the more complicated model we have , i.e. , the more time will be consumed at the training stage .",
    "[ ht ]    * input : * @xmath4 , @xmath5 , @xmath27 , @xmath28 , @xmath31 . + * output : * ranking function @xmath6 .    1 .",
    "* for * @xmath32 to @xmath28 * do * * draw a subsample @xmath33 of size @xmath27 from the set of unlabeled spectra @xmath5 .",
    "* train a toppush model @xmath29 to rank @xmath4 ahead of @xmath33 .",
    "* return * @xmath34",
    "in this section , we conducted a set of rare spectra retrieval experiments and presented detailed experimental comparison to evaluate the effectiveness and efficiency of various methods . specifically , we investigated the influence of different model parameter choices for our baggingtoppush method to make a reference for simply application .",
    "we use the spectral data set taken from the sloan digital sky survey ( sdss ) , data release 10 ( dr10 ) @xcite , and choose the carbon stars as the rare astrophysical objects to demonstrate the performance of our method .",
    "note that other types of rare spectra can also be retrieved by our method , and here we focused on carbon stars .",
    "specifically , we carefully select 450 samples from all carbon stars classified by the spectroscopic pipeline of sdss , and randomly select 100,000 samples from other types of stellar spectra . since",
    "carbon stars are quite rare , we roughly consider that there are no carbon stars mixed in these 100,000 stellar spectra .",
    "all spectral data have the wavelength coverage from 3917.4   to 8974.3   and thus the dimensionality is 3601 .",
    "generally , the spectral data should be preprocessed to facilitate retrieval .",
    "the preprocessing includes de - noising , normalisation , feature selection , etc . in the experiments ,",
    "we first employ a median filter with width 10   to eliminate the disturbance of narrow skylines and noise .",
    "one filtered example is presented in fig .",
    "[ denoising ] .",
    "obviously , the skylines and noise have been removed effectively after filtering .",
    "we then normalise the spectral flux by mapping the minimum and maximum flux value of each spectral data to @xmath35 $ ] .",
    "this is very useful for data mining applications where the input data are generally distributed on widely different scales . in rare astrophysical objects searching ,",
    "we are often confronted with very high dimensional spectral data which may contains a lot of non - informative or noisy features , so it is necessary to extract the main information hidden in the spectral data .",
    "we finally apply the principal component analysis ( pca ) which has been widely used in spectra classification problems to obtain the low - dimensional data representation , and 50 principal components have been retained .      in this subsection",
    ", we briefly introduce some other pu learning methods which can be used to retrieve rare spectra .",
    "we select one - class svm ( ocsvm ) and bagging svm ( baggingsvm ) to serve as the classification - based methods , in which baggingsvm represents the state - of - the - art one .",
    "furthermore , label propagation ( lp ) , efficient manifold ranking ( emr ) and local regression and global alignment ( lrga ) can serve as the rank - based methods .    the formulation of ocsvm proposed by sch@xmath36lkopf et al .",
    "can be summarized as mapping the data ( only positive class ) into a feature space @xmath37 using an appropriate kernel function , and then trying to find a hyperplane to separate the mapped vectors from the origin with maximum margin @xcite .",
    "then for a new , previously unseen sample , its label can be determined by this hyperplane .    before we introduce the bagging svm ,",
    "let us briefly cover the biased svm @xcite .",
    "biased svm treats all the unlabeled samples as negative samples .",
    "then the svm classifier is built by giving appropriate weights to the positive and unlabeled samples , respectively .",
    "based on the idea of bagging and biased svm , mordelet et al .",
    "@xcite proposed bagging svm method , which consists in aggregating biased svm classifiers trained to discriminate @xmath4 from a small random subsample of @xmath5 .",
    "the motivation behind bagging svm is to exploit an intrinsic feature of pu learning to benefit from classifier aggregation through a random subsample strategy , and we borrowed this idea in our baggingtoppush method .",
    "lp proposed by zhou et al .",
    "@xcite is one of the state - of - the - art graph - based learning algorithms .",
    "@xcite have successfully applied the lp algorithm to search carbon stars and dz white dwarfs from data release 8 ( dr8 ) of the sloan digital sky survey ( sdss ) .",
    "specifically , they have found 260 new carbon stars and 29 new dz white dwarfs from sdss dr8 .",
    "the key assumption in lp is that neighboring data points in high dimensional space should share the similar semantic labels . given a set of data , the generated graph is often represented as an adjacency matrix in which the elements save the edge weights between any two points .",
    "generally , the k - nn graph scheme is the most popular approach for graph construction . in the k - nn graph ,",
    "the weights are usually defined by the gaussian kernel .",
    "emr proposed by xu et al .",
    "@xcite is a new framework for large scale retrieval problems .",
    "@xcite have applied the emr to search carbon stars from the large sky area multi - object fiber spectroscopic telescope ( lamost ) pilot survey . using this algorithm",
    ", they totally found 183 carbon stars , and 158 of them are new findings .",
    "the goal of emr is to address the shortcomings of mr @xcite from scalable graph construction and efficient computation .",
    "specifically , emr precomputes an anchor graph on the data set instead of the traditional k - nn graph to enhance the computation speed of mr .",
    "the normalized laplacian matrix in lp is usually calculated based on gaussian function .",
    "however , it has been reported that gaussian function is sensitive to the width parameter @xcite , and there is usually no truth to tune the width parameter of gaussian kernel in real - world retrieval applications . to overcome this problem , lrga @xcite learns the laplacian matrix by local regression and global alignment and shows to be insensitive to parameters .      to simulate a pu learning problem",
    ", we randomly select a given number of carbon star spectra to create a positive set @xmath4 , while @xmath5 contains the non - selected carbon star spectra and all of the other spectra . to investigate the influence of the number of known positive samples",
    ", we varied the size of @xmath4 ( np ) in @xmath38 .",
    "for each value of np , we trained all 6 methods described above ( baggingtoppush , ocsvm , baggingsvm , lp , emr , lrga ) and ranked the spectra in @xmath5 by decreasing score .",
    "the parameters of all 6 algorithms were carefully tuned on our data . for baggingtoppush , we varied the regularisation parameter @xmath31 that controls the model complexity in @xmath39 , the size of bootstrap samples @xmath27 in @xmath40 and the number of bootstraps @xmath28 in @xmath41 . for ocsvm , the parameter @xmath42 denotes an upper bound on the fraction of outliers ( training examples regarded out - of - class ) and a lower bound on the fraction of training examples used as support vectors , and we varied @xmath42 in @xmath43 . for baggingsvm , the penalty parameter @xmath44 which determines the influence of the misclassification on the objective function was chosen from @xmath45 ,",
    "the size of bootstrap samples equals to np , and the number of bootstraps equals to 30 . for lp and lrga",
    ", we constructed the k - nn graph where k@xmath465 . furthermore , in lp the width parameter of gaussian kernel was set to @xmath47 , and the weight parameter @xmath48 which balances the smoothness constraint and the fitting constraint in objective function was set to 0.99 . for emr , we set the number of anchors to @xmath49 , and the weight parameter @xmath48 to 0.99 as well , which consistent with the experiments performed in @xcite .",
    "all methods are implemented in matlab environment on a workstation with 12-core intel(r ) xeon(r ) ( 3.47ghz ) with 96 gb ram .",
    "the implementations of ocsvm and baggingsvm are based on the libsvm package @xcite .",
    "we perform 50 independent trials for each algorithm , and the averaged results are reported .      each spectral sample in the testing set",
    "is ranked based on its prediction score . due to time constraint ,",
    "only a small set of the top ranked spectra will be validated by astronomer .",
    "it indicates that a best ranking means all the relevant spectra should be ranked in the top positions .",
    "hence , we evaluate the models performance using the following several information retrieval metrics , which mainly focus on behaviour at the top of a ranked list .",
    "* the precision at @xmath50 ( @xmath51 ) : it measures what fraction of the top @xmath50 ranked spectra belong to the given rare category .",
    "* the recall at @xmath50 ( @xmath52 ) : it measures what fraction of the known rare spectra are retrieved within the top @xmath50 ranked spectra . * the average precision at @xmath50 ( @xmath53 ) : by computing the precision and recall at every position in the ranked list of spectra",
    ", one can plot a precision - recall ( pr ) curve , and ap denotes the area under the pr curve .",
    "@xmath53 is defined as : @xmath54 where @xmath55 denotes the set of relevant spectra in unlabelled data set @xmath5 and @xmath56 is the size of @xmath55 . * the area under the receiver operating characteristic curve ( auc )",
    ": it measures the global ranking performance of the model , wherever the incorrect pair - wise ordering occurs in the ranking list .",
    "to compare the ranking effectiveness at the top of a ranked list , the average performance ( mean@xmath57std ) of different methods are shown in fig .",
    "[ fig2 ] , [ fig3 ] and [ fig4 ] . from the results , we can find that lrga obtains better precision and recall when np=1(as seen in the left panels of fig 2,3,4 ) , and baggingtoppush obtains better results when np@xmath581(as seen in the middle and right panels of fig 2,3,4 ) .",
    "this is consistent with the design of baggingtoppush that aims to maximise the accuracy at the top of the ranked list .",
    "compared to other methods , the performance of ocsvm is worst , and this is related to the fact that ocsvm usually needs a sufficiently large number of positive samples .",
    "although lp and emr have been successfully used in @xcite to search carbon stars from sdss and lamost respectively , the experimental results show that lp or emr is not the best method to retrieval carbon stars .",
    "especially , the performance of emr is poor compared to baggingtoppush or lrga .    in terms of evaluation metric auc , which measures the global ranking performance of the model",
    ", we list the average results in table [ auc ] .",
    "we can see that baggingtoppush achieves best auc values for most np values ( only worse than lrga when np=1 ) .",
    "this indicates that baggingtoppush not only has better accuracy at the top of ranked list , but also has excellent global ranking performance . in rare astrophysical objects searching ,",
    "if only a few of positive samples ( @xmath59 3 ) are given , one can utilise the lrga method to retrieve the relevant samples , otherwise , we empirically found that baggingtoppush is the best method used to obtain an accurate ranked list .    to intuitively compare retrieval recall performance of different methods on our spectral data set",
    ", we have visualised the retrieval results of 450 carbon stars in 3d space constructed by pca in fig .",
    "[ fig5 ] . under different numbers of positive training samples np",
    ", it shows the data distributions of the carbon stars selected to be positive samples and the retrieved results of top 500 at the ranked list .",
    "the more blue circles , the better retrieval performance .",
    "as expected , the retrieval performances of different methods increase with np , and baggingtoppush always obtain better results than lp and emr .    in fig .",
    "[ fig6 ] , we plot the top 20 spectra retrieved by each method with np=1 to compare the correctness of different methods . from the results , we can see that the spectra retrieved by ocsvm are mostly incorrect .",
    "although the top 20 spectra retrieved by other 5 methods are all correct , we can further find that the results retrieved by baggingtoppush and lrga are visually more close to the given positive example than lp , emr and baggingsvm .      to evaluate ranking efficiency",
    ", we run all 6 pu learning methods on our data set , and record their computation cpu times in fig .",
    "[ fig7 ] . from the results",
    ", we can see that the baggingtoppush is prominently faster than emr , baggingsvm , lp , and lrga , while the gaps between baggingtoppush and ocsvm is very small .",
    "this result owns to the fact that the computational complexity of baggingtoppush and ocsvm is linear in the number of training samples .",
    "so , with reasonable values of @xmath27 , @xmath28 and @xmath31 , the baggingtoppush method not only has better ranking performance but also spends remarkably less computational time .",
    "the selection of the model parameters usually plays an important role to many machine learning methods because different settings of the model parameters may have impact on the performance of an algorithm directly . in the following ,",
    "we evaluate the performance of our baggingtoppush method with different values of the parameters .",
    "as shown in algorithm 1 , there are three parameters in baggingtoppush method : @xmath27 , @xmath28 , and @xmath31 .",
    "parameter @xmath27 is the number of samples randomly selected from @xmath5 by one bootstrap and parameter @xmath28 is the number of bootstraps .",
    "fig.[fig8 ] shows the performance variations of baggingtoppush with respect to @xmath28 , @xmath27 , and different values of np . from fig.[fig8 ] , the performance of baggingtoppush is not sensitive to the selection of @xmath27 and @xmath28 when @xmath60 & @xmath61 .",
    "intuitively , the larger @xmath27 and @xmath28 , the more time cost in ranking , thus we just select @xmath62 and @xmath63 in our experiments . in spectral retrieval applications , the selection of parameter @xmath27 is related to the number of positive samples and the true proportion of positive samples hidden in @xmath5 , so it is a parameter that needs to be tuned based on specific cases .",
    "[ fig9 ] shows the performance variations of baggingtoppush as a function of regularisation parameter @xmath31 for different values of np .",
    "we can find that the ranking performance is closely related to @xmath31 only when the number of positive training samples @xmath64 .",
    "when @xmath65 , the ranking performance is not sensitive to the selection of @xmath31 .",
    "since the computational cost of toppush reduces when a larger value of @xmath31 is used @xcite , we set @xmath66 in our experiments .",
    "in rare spectra retrieval application , how to extract the key features from an initial set of spectral data to facilitate the subsequent learning is a challenging problem .",
    "since the features in carbon star spectra are very broad , we directly apply pca to get the most informative features .",
    "however , if the spectral features of some rare type objects are sharp or indistinct , we need to extract the informative features carefully by defining some spectral line indices .    in this paper , we focus on the pu learning problems in rare astrophysical objects searching , and present the baggingtoppush approach to retrieve the rare spectra in massive astronomical datasets . based on the bipartite ranking model and bagging techniques , the new method aggregates bipartite ranking functions which are trained to place positive samples ahead of a small random subsample of all the unlabelled samples .",
    "the proposed method has the merit of high accuracy at the top of the ranked list , which is useful in searching for rare astronomical objects . compared with previous algorithms",
    "used to search for rare spectra , baggingtoppush not only has better retrieval performance but also spends remarkably less computational time .",
    "we also investigated the influence of model parameters on ranking performance , and provided optimal parameter choices making our method simple to apply .",
    "amini , m. r. , truong , t. v. , & goutte , c. , 2008 , sigir , 99106 ahn , c. p. , alexandroff , r. , allende p. c. , anders , f. , anderson , s. f. , anderton , t. , andrews , b. h. , aubourg ,  . ,",
    "bailey , st . ,",
    "bastien , f. a. , & others , 2014 , apjs , 211 , 17 boyd , s. , cortes , c. , mohri , m. , & radovanovic , a. , 2012 , nips , 953961 breiman , l. , 1996 , machine learning , 24 , 123 clmenon , s. , & vayatis , n. , 2007 , jmlr , 8 , 2671 chang , c. c. , & lin , c. j. , 2011 , acm tist , 2 , 127 kotlowski , w. , dembczynski , k. j. , & huellermeier , e. , 2011 , icml , 11131120 liu , b. , dai , y. , li , x. , lee , w. s. & yu , p. s. , 2003 , icdm , 179186 liu , t.y . , 2009 , foundations and trends in information retrieval , 3 , 225 li n. ,",
    "jin r. , & zhou z. h. , 2014 , nips , 15021510 lee , c. h. , oluwasanmi k. , & joydeep g. , 2013 , embc , 3459 - 3462 manevitz , l. m. , & yousef , m. , 2002 , jmlr , 2 , 139 mordelet , f. , & vert , j. p. , 2014",
    ", pattern recognition letters , 37 , 201 mordelet , f. , & vert , j. p. , 2011",
    ", bmc bioinformatics , 12 , 389 narasimhan , h. , & agarwal , s. , 2013 , nips , 29132921 rendle , s. , balby m. l. , nanopoulos , a. , & schmidt t. l. , 2009 , sigkdd , 727736 schlkopf , b. , williamson , r. c. , smola , a. j. , shawe - taylor , j. , & platt , j. c. , 1999 , nips , 12 , 582588 si , j. m. , luo , a. l. , li , y. b. , zhang , j. n. , wei , p. , wu , y. h. , wu , f. c. , & zhao , y. h. , 2014 , science china physics , mechanics and astronomy , 57 , 176186 si , j. m. , li , y. b. , luo , a. l. , tu l. p. , shi z. x. , zhang , j. n. , wei p. , zhao , g. , wu , y. h. , wu , f. c. , zhao , y. h. , & others 2015 , raa , 15 , 1671 tax , d. m. , & duin , r. p. , 2004",
    ", machine learning , 54 , 45 wang , f. , & zhang , c. , 2008 , tkde , 20 , 55 xu , b. , bu , j. , chen , c. cai , d. , he , x. , liu , w. , & luo , j. , 2011 , sigir , 525534 yang , y. , xu , d. , nie , f. , luo , j. , & zhuang , y. , 2009 , acm multimedia , 175184 zhang , y. , & zhou , z.h .",
    ", 2009 , ijcai , 13571362 zhou , d. , bousquet , o. , lal , t. n. , weston , j. , & schlkopf , b. , 2004 , nips , 16 , 321 zhou , d. , weston , j. , gretton , a. , bousquet , o. , & schlkopf , b. , 2004 , nips , 16 , 169"
  ],
  "abstract_text": [
    "<S> one of important aims of astronomical data mining is to systematically search for specific rare objects in a massive spectral dataset , given a small fraction of identified samples with the same type . </S>",
    "<S> most existing methods are mainly based on binary classification , which usually suffer from uncompleteness when the known samples are too few . while , rank - based methods would provide good solutions for such case . after investigating several algorithms , a method combining bipartite ranking model with bootstrap aggregating techniques </S>",
    "<S> was developed in this paper . </S>",
    "<S> the method was applied in searching for carbon stars in the spectral data of sloan digital sky survey ( sdss ) dr10 , and compared with several other popular methods used in data mining . </S>",
    "<S> experimental results validate that the proposed method is not only the most effective but also less time consuming among these competitors automatically searching for rare spectra in a large but unlabelled dataset . </S>"
  ]
}