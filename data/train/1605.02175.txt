{
  "article_text": [
    "the primary concern of this paper is the erasure channel , which is a common digital communication channel model that plays a fundamental role in coding and information theory . throughout the paper",
    ", we assume that time is discrete and indexed by the integers . at time @xmath1",
    ", the erasure channel of interest can be described by the following equation : @xmath2 where the channel input @xmath3 , supported on an irreducible finite - type constraint @xmath4 , is a stationary process taking values from the input alphabet @xmath5 , and the erasure process @xmath6 , independent of @xmath3 , is a binary stationary and ergodic process with _ erasure rate _ @xmath7 , and @xmath8 is the channel output process over the output alphabet @xmath9 .",
    "the word `` erasure '' as in the name of our channel naturally arises if a `` @xmath10 '' is interpreted as an erasure at the receiving end of the channel ; so , at time @xmath1 , the channel output @xmath11 is nothing but the channel input @xmath12 if @xmath13 , but an erasure if @xmath14",
    ".    let @xmath15 denote the set of all the finite length words over @xmath16 .",
    "let @xmath17 be a finite subset of @xmath15 , and let @xmath4 be the _ finite - type constraint _ with respect to @xmath17 , which is a subset of @xmath15 consisting of all the finite length words over @xmath16 , each of which does not contain any element in @xmath17 as a contiguous subsequence ( or , roughly , elements in @xmath17 are `` forbidden '' in @xmath4 ) .",
    "the most well known example is the @xmath18-run - length - limited ( rll ) constraint over the alphabet @xmath19 , which forbids any sequence with fewer than @xmath20 or more than @xmath21 consecutive @xmath22 s in between two successive @xmath23 s ; in particular , a prominent example is the @xmath24-rll constraint , a widely used constraint in magnetic recording and data storage ; see  @xcite . for the @xmath25-rll constraint with @xmath26 ,",
    "a forbidden set @xmath17 is @xmath27 when @xmath28 , one can choose @xmath17 to be @xmath29 in particular when @xmath30 , @xmath17 can be chosen to be @xmath31 .",
    "the _ length _ of @xmath17 is defined to be that of the longest words in @xmath17 .",
    "generally speaking , there may be many such @xmath17 s with different lengths that give rise to the same constraint @xmath4 ; the length of the shortest such @xmath17 s minus @xmath22 gives the _ topological order _ of @xmath4 .",
    "for example , the topological order of the @xmath0-rll constraint , whose shortest @xmath17 proves to be @xmath31 , is @xmath22 .",
    "a finite - type constraint @xmath4 is said to be _ irreducible _",
    "if for any @xmath32 , there is a @xmath33 such that @xmath34 .    as mentioned before , the input process @xmath35 of our channel ( [ mec ] )",
    "is assumed to be _ supported _ on an irreducible finite - type constraint @xmath4 , namely , @xmath36 , where @xmath37 the capacity of the channel ( [ mec ] ) , denoted by @xmath38 , can be computed by @xmath39 where the supremum is taken over all stationary processes @xmath35 supported on @xmath4 . here , we note that input - constraints  @xcite are widely used in various real - life applications such as magnetic and optical recording  @xcite and communications over band - limited channels with inter - symbol interference  @xcite .",
    "particularly , we will pay special attention in this paper to a binary erasure channel with erasure rate @xmath40 ( bec(@xmath40 ) ) with the input supported on the @xmath0-rll constraint , denoted by @xmath41 throughout the paper .",
    "when there is no constraint imposed on the input process @xmath35 , that is , @xmath42 , it is well known that @xmath43 ; see theorem  [ fbnot ] . when @xmath44 , that is , when the channel is perfect with no erasures , @xmath45 proves to be the _ noiseless capacity _ of the constraint @xmath4 , which can be achieved by a unique @xmath46-th order markov chain @xmath47 with @xmath48  @xcite . on the other hand , other than these two above - mentioned `` degenerated '' cases , `` explicit '' analytic formulas of capacity for `` non - degenerated '' cases have remained evasive , and the problem of analytically characterizing the noisy constrained capacity is widely believed to be intractable .    the problem of numerically computing the capacity @xmath45 seems to be as challenging : the computation of the capacity of a general channel with memory or input constraints is notoriously difficult and has been open for decades ; and the fact that our erasure channel is only a special class of such ones does not appear to make the problem easier . here , we note that for a discrete memoryless channel , shannon gave a closed - form formula of the capacity in his celebrated paper  @xcite , and blahut  @xcite and arimoto  @xcite , independently proposed an algorithm which can efficiently compute the capacity and the capacity - achieving distribution simultaneously .",
    "however , unlike the discrete memoryless channels , the capacity of a channel with memory or input constraints in general admits no single - letter characterization and very little is known about the efficient computation of the channel capacity . to date , most known results in this regard have been in the forms of numerically computed bounds : for instance , numerically computed lower bounds by arnold and loeliger  @xcite , a.  kavcic  @xcite , pfister , soriaga and siegel  @xcite , vontobel and arnold  @xcite .",
    "one of the most effective strategies to compute the capacity of channels with memory or input constraints is the so - called _ markov approximation _ scheme .",
    "the idea is that instead of maximizing the mutual information rate over all stationary processes , one can maximize the mutual information rate over all @xmath46-th order markov processes to obtain the @xmath46-th order markov capacity . under suitable assumptions ( see , e.g. ,  @xcite ) , when @xmath46 tends to infinity , the corresponding sequence of markov capacities will tend to the channel capacity . for our erasure channel , the _",
    "@xmath46-th order markov capacity _ is defined as @xmath49 where the supremum is taken over all @xmath46-th order markov chains supported on @xmath4 .",
    "the main contributions of this work are the characterization of the asymptotics of the above - mentioned input - constrained erasure channel capacity . of great relevance to this work",
    "are results by han and marcus  @xcite , jacquet and szpankowski  @xcite , which have characterized asymptotics of the capacity of the a binary symmetric channel with crossover probability @xmath40 ( bsc(@xmath40 ) ) with the input supported on the @xmath0-rll constraint .",
    "the approach in the above - mentioned work is to obtain the asymptotics of the mutual information rate first , and then apply some bounding argument to obtain that of the capacity .",
    "the approach in this work roughly follows the same strategy , however , as elaborated below , our approach differs from theirs to a great extent in terms of technical implementations .    throughout the paper",
    ", we use the logarithm with base @xmath50 in the proofs and we use the logarithms with base @xmath23 in the numerical computations of the channel capacity . below is a brief account of our results and methodology employed in this work .",
    "the starting point of our approach is lemma  [ nformula ] in section  [ mutual - information - rate - section ] , a key lemma that expresses the conditional entropy @xmath51 in a form that is particularly effective for analyzing asymptotics of @xmath45 when @xmath40 is close to @xmath10 .",
    "as elaborated in theorem  [ wolfsconjecture ] , lemma  [ nformula ] naturally gives a lower and upper bound on @xmath45 , where the lower bound gives a counterpart result of wolf s conjecture for a bec(@xmath40 ) .",
    "moreover , when applied to the case when @xmath35 is a markov chain , lemma  [ nformula ] yields some explicit series expansion type formulas in theorem  [ entropyformula ] and corollary  [ memorylessec ] , which aptly pave the way for characterizing the asymptotics of the input - constrained erasure channel capacity . here",
    "we remark that the method in  @xcite have been further developed for more general families of memory channels in  @xcite via examining the contractiveness of an associated random dynamical system  @xcite .",
    "however , the methodology to derive asymptotics of the mutual information rate in this work capitalizes on certain characteristics that are in a sense unique to erasure channels .    in section  [ general - asymptotics ]",
    ", we consider a memoryless erasure channel with the input supported on an irreducible finite - type constraint , and in theorem  [ asyerasurec ] , we derive partial asymptotics of its capacity @xmath45 in the vicinity of @xmath44 where @xmath45 is written as the sum of a constant term , a linear term in @xmath40 and an @xmath52-term .",
    "the lower bound part in the proof of this theorem follows from an easy application of theorem  [ entropyformula ] , and the upper bound part hings on an adapted argument in  @xcite .    in section  [ binary - asymptotics ]",
    ", we consider a bec(@xmath40 ) with the input being a first - order markov process supported on the @xmath0-rll constraint @xmath41 .",
    "within this special setup , we show in theorem  [ concavitybec ] that the @xmath53 is strictly concave with respect to some parameterization of @xmath35 . and in section  [ sub-2 ] , we numerically evaluate @xmath54 and the corresponding capacity - achieving distribution using the randomized algorithm proposed in  @xcite which proves to be convergent given the concavity of @xmath53",
    ". moreover , the concavity of @xmath53 guarantees the uniqueness of the capacity achieving distribution , based on which we derive full asymptotics of the above input - constrained bec(@xmath40 ) around @xmath44 in theorem  [ foc ] , where @xmath55 is expressed as an infinite sum of all @xmath56-terms .    in section  [ feedback - section ]",
    ", we turn to the scenarios when there might be feedback in our erasure channel .",
    "we first prove in theorem  [ fbnot ] that when there is no input constraint , the feedback does not increase the capacity of the erasure channel even with the presence of the channel memory .",
    "when the input constraint is not trivial , however , we show in theorem  [ yonglong - feedback ] that feedback does increase the capacity using the example of a bec(@xmath40 ) with the ( @xmath57)-rll input constraint , and so feedback may increase the capacity of input - constrained erasure channels even if there is no channel memory .",
    "the results obtained in this section suggest the intricacy of the interplay between feedback , memory and input constraints .",
    "in this section , we focus on the mutual information of the erasure channel ( [ mec ] ) introduced in section  [ introduction - section ] .",
    "the starting point of our approach is the following key lemma , which is particularly effective for analysis of input - constrained erasure channels .",
    "[ nformula ] for any @xmath58 , we have @xmath59}h(x_{0}|x_{d})p(e_0=1,e_{d}=1,e_{d^c}=0),\\ ] ] where @xmath60 \\triangleq \\{-n,\\cdots,-1\\}$ ] .",
    "note that @xmath61 where @xmath62 from the independence of @xmath3 and @xmath6 , it follows that @xmath63 here and throughout the paper , let @xmath64 be the set of all finite length words over @xmath65 and we define , for any @xmath66 , @xmath67 and @xmath68 for @xmath69 , @xmath70 where @xmath71 follows from the fact that @xmath72 .",
    "similarly , for @xmath73 , @xmath74 where @xmath71 follows from the fact that @xmath75 .",
    "therefore , @xmath76}\\sum_{y_{-n}^0:\\mathcal{i}(y_{-n}^{-1})=d , y_{0}=0}p(y_{-n}^{0})\\log p(e_0=0|e_{d})=1,e_{d^c}=0)\\notag\\\\ & \\stackrel{(a)}{=}-\\sum_{d \\subseteq [ -n,-1]}p(e_0=0,e_{d}=1,e_{d^c}=0)\\log p(e_0=0|e_{d}=1,e_{d^c}=0)\\label{0erasure},\\end{aligned}\\ ] ] where @xmath71 follows from the fact that for any given @xmath77 $ ] , @xmath78 also , we have @xmath79}p(e_0=1,e_{d}=1,e_{d^c}=0)\\log p(e_0=1|e_{d}=1,e_{d^c}=0)\\label{1erasure},\\end{aligned}\\ ] ] where @xmath71 follows from a similar argument as in the proof of  ( [ 0erasure ] ) and @xmath80 from  ( [ pformula ] ) , it then follows that @xmath81}\\sum_{y_{-n}^{0 } : \\mathcal{i}(y_{-n}^{0})=d\\cup\\{0\\}}p_{x}(y_{d},y_0)p(e_0=1,e_{d}=1,e_{d^c}=0)\\log p_{x}(y_{0}|y_{d})\\notag\\\\ & = \\sum_{d \\subseteq [ -n , -1]}h(x_0|x_{d})p(e_0=1,e_{d}=1,e_{d^c}=0).\\label{eerasure}\\end{aligned}\\ ] ] the desired formula for @xmath51 then follows from ( [ 0erasure ] ) , ( [ 1erasure ] ) and ( [ eerasure ] ) .",
    "one of the immediate applications of lemma  [ nformula ] is the following lower and upper bounds on @xmath45 .",
    "[ wolfsconjecture ] @xmath82    for the upper bound , it follows from lemma  [ nformula ] that @xmath83}h(x_{0}|x_{d})p(e_0=1,e_{d}=1,e_{d^c}=0)\\notag\\\\ & \\stackrel{(a)}{\\leq } \\lim_{n\\to \\infty}\\sum_{d \\subseteq [ -n,-1]}h(x_{0 } ) p(e_0=1,e_{d}=1,e_{d^c}=0)\\notag\\\\ & \\leq \\lim_{n\\to \\infty}\\sum_{d \\subseteq [ -n,-1 ] } p(e_0=1,e_{d}=1,e_{d^c}=0 ) \\log k \\notag\\\\ & = p(e_0=1 ) \\log k \\notag\\\\ & = ( 1-{\\varepsilon } ) \\log k , \\notag\\end{aligned}\\ ] ] where we have used the fact that conditioning reduces entropy for @xmath71 .",
    "assume @xmath4 is of topological order @xmath46 , and let @xmath47 be the @xmath46-order markov chain that achieves the noiseless capacity @xmath84 of the constraint @xmath85 .",
    "again , it follows from lemma  [ nformula ] that @xmath86}h(\\hat{x}_{0}|\\hat{x}_{d})p(e_0=1,e_{d}=1,e_{d^c}=0)\\notag\\\\ & \\ge \\lim_{n\\to \\infty}\\sum_{d \\subseteq [ -n , -1]}h(\\hat{x}_{0}|\\hat{x}_{-m}^{-1},\\hat{x}_{d})p(e_0=1,e_{d}=1,e_{d^c}=0)\\notag\\\\ & \\stackrel{(a)}{= } \\lim_{n\\to \\infty}\\sum_{d \\subseteq [ -n , -1]}h(\\hat{x}_{0}|\\hat{x}_{-m}^{-1})p(e_0=1,e_{d}=1,e_{d^c}=0)\\notag\\\\ & = p(e_0=1 ) h(\\hat{x}_{0}|\\hat{x}_{-m}^{-1 } ) \\notag\\\\ & = ( 1-{\\varepsilon } ) c(\\mathcal{s } , 0 ) , \\notag\\end{aligned}\\ ] ] where we have used the fact that @xmath87 is an @xmath46-th order markov chain for @xmath71 .",
    "the upper bound part of theorem  [ wolfsconjecture ] also follows from the well - known fact that ( see theorem  [ fbnot ] ) @xmath88 and for any @xmath4 , @xmath89 which is obviously true .",
    "let @xmath90 denote the capacity of a bsc(@xmath40 ) with the @xmath18-rll constraint . in  @xcite",
    "wolf posed the following conjecture on @xmath91 : @xmath92 where @xmath93 .",
    "a weaker form of this bound has been established in  @xcite by counting the possible subcodes satisfying the @xmath18-rll constraint in some linear coding scheme , but the conjecture for the general case still remains open .",
    "it is well known that @xmath94 is the capacity of a bsc(@xmath40 ) without any input constraint , and @xmath95 is the capacity of a bec(@xmath40 ) without any input constraint .",
    "so , for an input - constrained bec(@xmath40 ) , the lower bound part of theorem  [ wolfsconjecture ] gives a counterpart result of wolf s conjecture .    when applied to the channel with a markovian input , lemma  [ nformula ]",
    "gives a relatively explicit series expansion type formula for the mutual information rate of ( [ mec ] ) .",
    "[ entropyformula ] assume @xmath3 is an @xmath46-th order input markov chain .",
    "then , @xmath96 where @xmath97 and @xmath98 : \\mbox{for all $ j=1,\\cdots , u$ } , \\{i_{j},i_{j}+1,\\cdots , i_{j}+m\\ } \\not\\subseteq \\ { i_1,\\cdots , i_{u}\\}\\}$ ] and @xmath99 , here @xmath100 denotes the remainder of @xmath101 divided by @xmath46 .",
    "note that @xmath102 where @xmath71 follows from the independence of @xmath3 and @xmath103 . from lemma  [ nformula ]",
    ", it then follows that @xmath104}h(x_0|x_{d})p(e_0=1,e_{d}=1,e_{d^c}=0).\\end{aligned}\\ ] ] now , letting @xmath105:|d|=u\\ } \\mbox { and } b_1(n , u)=b(n , u)-b_{2}(n , u),\\ ] ] we deduce that , for @xmath106 @xmath107}p(e_{a(k , i_1^t)}=1,e_d=1,e_{\\bar{a}(k , i_1^t)}=0,e_{[-n ,- k - m-1]-d}=0)&=p(e_{a(k , i_1^t)}=1,e_{\\bar{a}(k , i_1^t)}=0).\\end{aligned}\\ ] ] and @xmath108}\\left\\{h(x_0|x_{a(k , i_1^t)},x_d)\\right.\\\\ & \\hspace{4.5mm}\\times \\left.p(e_{a(k , i_1^t)}=1,e_d=1,e_{\\bar{a}(k , i_1^t)}=0,e_{[-n ,- k - m-1]-d}=0)\\right\\}\\\\ & \\stackrel{(a)}{=}\\sum_{k=0}^{n - m+1}\\sum_{t=0}^{b(k-1,m)}\\sum_{\\{i_1^t\\}\\in b(k-1,t)}\\sum_{d \\subseteq [ -n ,- k - m-1]}\\left\\{h(x_0|x_{a(k , i_1^t)})\\right.\\\\ & \\hspace{4.5mm}\\times\\left.p(e_{a(k , i_1^t)}=1,e_d=1,e_{\\bar{a}(k , i_1^t)}=0,e_{[-n ,- k - m-1]-d}=0)\\right\\}\\\\ & = \\sum_{k=0}^{n - m+1}\\sum_{t=0}^{b(k-1,m)}\\sum_{\\{i_1^t\\}\\in b(k-1,t)}h(x_0|x_{a(k , i_1^t)})p(e_{a(k , i_1^t)}=1,e_{\\bar{a}(k , i_1^t)}=0),\\end{aligned}\\ ] ] where @xmath71 follows from the fact that @xmath3 is an @xmath46-th order markov chain .",
    "then it follows that @xmath109}h(x_0|x_{d})p(e_0=1,e_{d}=1,e_{d^c}=0)\\notag\\\\ & = \\sum_{k=0}^{n}\\ \\sum_{\\{i_1,\\dots , i_k\\}\\in b(n , k)}h(x_0|x_{i_1^k})p(e_0=1,e_{i_1^k}=1,e_{\\bar{i}_1^k}=0)\\notag\\\\ & = \\left(\\sum_{k = m}^{n}\\ \\sum_{\\{i_1,\\dots , i_k\\}\\in b_{1}(n , k)}+\\sum_{k=0}^{b(n , m)}\\sum_{\\{i_1,\\dots , i_k\\}\\in b_{2}(n , k)}\\right)h(x_0|x_{i_1^k})p(e_0=1,e_{i_1^k}=1,e_{\\bar{i}_1^k}=0)\\notag\\\\ & = \\sum_{k=0}^{n - m+1}\\sum_{t=0}^{b(k-1,m)}\\sum_{\\{i_1^t\\}\\in b(k-1,t)}h(x_0|x_{a(k , i_1^t)})p(e_{a(k , i_1^t)}=1,e_{\\bar{a}(k , i_1^t)}=0)+t(n),\\label{part}\\end{aligned}\\ ] ] where @xmath110 it follows from @xmath111 that @xmath112 where @xmath113 is the event that `` there is no @xmath46 consecutive @xmath22 s in @xmath114 '' .",
    "now , let @xmath115 for @xmath116 .",
    "then it follows from the assumption that @xmath117 is also a stationary and ergodic process with @xmath118 . using poincare s recurrence theorem  @xcite",
    ", we have that @xmath119 , which implies that @xmath120 , where @xmath121 denotes the event that `` there is no @xmath46 consecutive @xmath22 s in @xmath122 '' .",
    "this , together with the fact that @xmath123 , implies that @xmath124 , and therefore the proof of the theorem is complete .",
    "the following corollary can be readily deduced from theorem  [ entropyformula ] .",
    "[ memorylessec ] assume that @xmath6 is i.i.d .",
    "and @xmath125 is an @xmath46-th order markov chain .",
    "then @xmath126 where @xmath127 in particular , if @xmath125 is a first - order markov chain , @xmath128    [ verduerasure ] a series expansion type formula for @xmath129 different from ( [ first ] ) is given in theorem @xmath130 of  @xcite for a discrete memoryless erasure channel with a first - order input markov chain .",
    "it can be verified that these two formulas are `` equivalent '' in the sense that either one can be deduced from the other one via simple derivations .",
    "the form that our formula takes however makes it particularly effective for the capacity analysis of an input - constrained erasure channel .",
    "in this section , we will focus on the case when @xmath6 is i.i.d .",
    "and @xmath4 is an irreducible finite - type constraint of topological order @xmath46 . with lemma  [ nformula ] and corollary  [ memorylessec ] established ,",
    "we are ready to characterize the asymptotics of the capacity of this type of erasure channels .    as mentioned in section  [ introduction - section ] , when @xmath44 , it is well known  @xcite that there exists an @xmath46th - order markov chain @xmath47 with @xmath48 such that @xmath131 where the maximization is over all stationary processes supported on @xmath4 .",
    "the following theorem characterizes the asymptotics of @xmath45 near @xmath44 .",
    "[ asyerasurec ] assume that @xmath6 is i.i.d .",
    "then , @xmath132 moreover , for any @xmath133 , @xmath134 is of the same asymptotic form as in ( [ asyerasurec ] ) , namely , @xmath135    to establish  ( [ highsnrasy ] ) , we prove that @xmath45 is lower and upper bounded by the same asymptotic form as in ( [ highsnrasy ] ) .    for the lower bound part",
    ", we consider the channel ( [ mec ] ) with @xmath47 as its input .",
    "note that @xmath136 and furthermore , for @xmath137 @xmath138 where we have defined @xmath139 it then follows that @xmath140 where @xmath71 follows from @xmath141 and the constant in @xmath52 depends only on @xmath46 and @xmath142 . then , from corollary  [ memorylessec ] , it follows that @xmath143 where @xmath144 follows from ( [ tailestimate ] ) and @xmath145 this , together with ( [ parry ] ) , establishes that @xmath45 is lower bounded by the asymptotic form in ( [ highsnrasy ] ) .",
    "for the upper bound part , we will adapt the argument in  @xcite .",
    "let @xmath146 and @xmath147 where @xmath148 . in this proof , we define @xmath149 it then follows from lemma  [ nformula ] that @xmath150 where @xmath151,|d|=k } h(x_0|x_{d})(1-{\\varepsilon})^{k+1}{\\varepsilon}^{n - k}.\\ ] ] let @xmath152 maximize @xmath153 . as @xmath153 is continuous in @xmath154 and is maximized at @xmath155 when @xmath44 , there exists some @xmath156 ( depends on @xmath1 ) and @xmath157 such that for all @xmath158 , @xmath159 .",
    "then for @xmath160 , there exists some constant @xmath161 ( depends on @xmath1 ) such that @xmath162 from now on , we write @xmath163 letting @xmath164 be the hessian of @xmath165 , we now expand @xmath165 and @xmath166 around @xmath167 to obtain @xmath168 and @xmath169 where @xmath170 contains all @xmath171 as its coordinates . since @xmath172 is negative definite ( see lemma 3.1  @xcite ) , we deduce that , for @xmath173 sufficiently small , @xmath174 without loss of generality , we henceforth assume @xmath172 is a diagonal matrix with all diagonal entries denoted @xmath175 ( since otherwise we can diagonalize @xmath172 ) . now , let @xmath176 and let @xmath177 denote the complement of @xmath178 in @xmath179 : @xmath180 then , we have @xmath181 where @xmath71 follows from the easily verifiable fact that for any @xmath182 , @xmath183 since @xmath184 is an @xmath46-th order markov chain , we deduce that @xmath185 we now ready to deduce that , for some positive constant @xmath186 , @xmath187 which further implies that @xmath188 the proof of ( [ highsnrasy ] ) is then complete .    with @xmath45 replaced with @xmath189 , the proof of ( [ highsnrasy ] ) also establishes ( [ highsnrasy-1 ] ) .    in a fairly general setting ( where input constraints are not considered ) , a similar asymptotic formula with a constant term , a term linear in @xmath40 and",
    "a residual @xmath190-term has been derived in theorem @xmath191 of  @xcite .    as an immediate corollary of theorem  [ asyerasurec ]",
    ", the following result gives asymptotics of the capacity of a bec(@xmath40 ) with the input supported on the @xmath0-rll constraint @xmath41 .",
    "[ asmpt ] assume @xmath192 and @xmath6 is i.i.d .",
    "then , we have @xmath193 and for any @xmath58 , @xmath194 is of the same asymptotic form , namely , @xmath195    let @xmath196 .",
    "it is well known  @xcite that the noiseless capacity @xmath197 and the first - order markov chain @xmath87 with the following transition probability matrix @xmath198 achieves the noiseless capacity , that is , @xmath199 .",
    "furthermore , via straightforward computations , we deduce that @xmath200 which , together with the fact that @xmath201 implies @xmath193 as desired .    and the asymptotic form of @xmath194 follows from similar computations .",
    "the asymptotic form in ( [ first - order - partial ] ) only gives partial asymptotics of @xmath194 for any @xmath58 .",
    "for the case @xmath202 , we will derive later on the full asymptotics of @xmath54 ; see ( [ first - order - full ] ) in section  [ binary - asymptotics ] .",
    "in this section , we will focus on a bec(@xmath40 ) with the input being a first - order markov process supported on the @xmath0-rll constraint @xmath41 . to be more precise , we assume that @xmath192 , @xmath6 is i.i.d . and",
    "@xmath125 is a first - order markov chain , taking values in @xmath203 and having the following transition probability matrix : @xmath204 in section  [ sub-1 ] , we will show that @xmath53 is concave with respect to @xmath205 , and in section  [ sub-2 ] , we apply the algorithm in  @xcite to numerically evaluate @xmath54 , whose convergence is guaranteed by the above - mentioned concavity result .",
    "finally , in section  [ sub-3 ] , we characterize the full asymptotics of @xmath54 around @xmath44 .      the concavity of the mutual information rate of special families of finite - state machine channels ( fsmcs ) has been considered in  @xcite and  @xcite .",
    "the results therein actually imply that the concavity of @xmath53 with respect to some parameterization of the markov chain @xmath35 when @xmath40 is small enough . in this section , however , we will show that @xmath53 is concave with respect to @xmath205 , irrespective of the values of @xmath40 .",
    "below is the main theorem of this section .",
    "[ concavitybec ] for all @xmath206 , @xmath207 is strictly concave with respect to @xmath205 , @xmath208 .    from corollary  [ memorylessec ] , it follows that to prove the theorem , it suffices to show that for any @xmath58 , @xmath209 is strictly concave with respect to @xmath205 , @xmath208 . to prove this",
    ", we will deal with the following several cases :    * * straightforward computations give @xmath210 and @xmath211 one checks that the function within the brace is negative and it takes the maximum at @xmath212 . therefore @xmath213 is strictly concave in @xmath205 .    * * by definition , @xmath214 the following facts can be verified easily :    * @xmath215 ; * the @xmath1-step transition probability matrix of the markov chain @xmath3 is @xmath216 where @xmath217 * @xmath218 is strictly concave with respect to @xmath219 for @xmath220 .    with the above notation",
    ", we have @xmath221 and @xmath222 where @xmath223 and @xmath224 .",
    "it follows from ( i ) and ( iii ) that the term ( [ term1 ] ) is strictly negative .",
    "so , to prove the theorem , it suffices to show that @xmath225 .    by the mean value theorem , @xmath226 where @xmath227 lies between @xmath228 and @xmath229 . as a function of @xmath227",
    ", @xmath230 takes the maximum at @xmath228 .",
    "it then follows that @xmath231\\}}{(1+\\theta)^{4}}\\\\ & { } & + \\frac{c_{n+1}\\{4-(-\\theta)^{n-1}[(n^2 - 3n)\\theta^2 + 2(n^2-n-2)+n^2+n]\\}}{(1+\\theta)^{4}}\\\\ & = & \\frac{c_{n}[2\\theta-2+q(n,\\theta)(-\\theta)^{n-1}]}{(1+\\theta)^{4}}+\\frac{c_{n+1}[4-q(n,\\theta)(-\\theta)^{n-1}]}{(1+\\theta)^{4}},\\end{aligned}\\ ] ] where @xmath232 and @xmath233 we then consider the following several cases :    * * we first consider the case that @xmath234 . for this case , obviously we have @xmath235 , @xmath236 and @xmath237 , which further implies that @xmath238}{(1+\\theta)^{4}}+\\frac{c_{n+1}[4+q(n,\\theta)\\theta^{n-1}]}{(1+\\theta)^{4}}<0.\\end{aligned}\\ ] ] now , for the case that @xmath239 , again obviously we have @xmath240 and furthermore , @xmath241 where we have used the fact that @xmath242 for a positive even number @xmath1 .",
    "now , we are ready to deduce that @xmath243}{(1+\\theta)^{4}}+\\frac{c_{n+1}[4+q(n,\\theta)\\theta^{n-1}]}{(1+\\theta)^{4}}\\\\ & = \\frac{(c_{n+1}-c_{n})(4+q(n,\\theta)\\theta^{n-1})}{(1+\\theta)^{4}}+\\frac{c_{n}(2\\theta+2)}{(1+\\theta)^{4}}\\\\ & < 0.\\end{aligned}\\ ] ] * * in this case , we have @xmath244}{(1+\\theta)^{4}}+\\frac{c_{n+1}[4-q(n,\\theta)\\theta^{n-1}]}{(1+\\theta)^{4}}\\\\ & = \\frac{(c_{n+1}-c_{n})[4-q(n,\\theta)\\theta^{n-1}]}{(1+\\theta)^{4}}+\\frac{c_{n}(2\\theta+2)}{(1+\\theta)^{4}}\\\\ & \\le\\frac{1}{(1+\\theta)^{4}}\\left\\{\\frac{[4-q(n,\\theta)\\theta^{n-1}]\\theta^{n}}{(1-y_{2})y_{2}}+(2\\theta+2)(1/g_{n}-2)\\right\\},\\end{aligned}\\ ] ] where the last inequality follows from the mean value theorem , the inequality @xmath245 for @xmath246 and the fact that @xmath247 lies between @xmath228 and @xmath229 .",
    "let @xmath248 and    @xmath249    , where @xmath250 $ ] . then @xmath251\\theta^{n}}{(1-g_{2})g_{2}}+(2\\theta+2)(1/g_{n}-2)\\right\\}\\\\ & \\le&\\frac{(2\\theta-2 - 4\\theta^n)b_{n}c_{n}+(1+\\theta)(1+\\theta^n)\\theta^n(4-q(n,\\theta)\\theta^{n-1})}{b_{n}c_{n}(1+\\theta)^{3}(1+\\theta^n)}. \\vspace{-3mm}\\end{aligned}\\ ] ]    note that the above numerator , as a function of @xmath252 , takes the maximum at @xmath253",
    ". denote this maximum by @xmath254 , where @xmath255    to complete the proof , it suffices to prove @xmath256 .",
    "substituting @xmath257 into @xmath258 , we have @xmath259\\\\ & \\ge&h(n,\\theta),\\end{aligned}\\ ] ] where @xmath260    the following facts can be easily verified :    * @xmath261 takes the minimum at some @xmath262 , where @xmath262 satisfies the following equation @xmath263 * @xmath264 for @xmath265 $ ] and @xmath266",
    ".    it then follows from ( a ) and ( b ) that @xmath267 for @xmath268 . solving  ( [ theta0 ] ) in @xmath269",
    ", we have @xmath270    for @xmath266 , substituting @xmath271 into @xmath261 , we have @xmath272\\\\ & \\ge\\theta_{0}^{n-1 } v(n),\\end{aligned}\\ ] ] where @xmath273 one checks that @xmath274 for @xmath275 . now , with the fact that @xmath276 for @xmath277 ( this can be verified via tedious yet straightforward computations since @xmath258 is an elementary function ) , we conclude that for @xmath278 for all @xmath279 and @xmath280 $ ] .",
    "when @xmath44 , that is , when the channel is `` perfect '' with no erasures , both @xmath45 and @xmath189 boil down to the noiseless capacity of the constraint @xmath4 , which can be explicitly computed  @xcite ; however , little progress has been made for the case when @xmath281 due to the lack of simple and explicit characterization for @xmath38 and @xmath282 . in terms of numerically computing @xmath45 and @xmath189 , relevant work can be found in the subject of fsmcs , as input - constrained memoryless erasure channels can be regarded as special cases of fsmcs .",
    "unfortunately , the capacity of an fsmc is still largely unknown and the fact that our channel is only a special fsmc does not seem to make the problem easier .",
    "recently , vontobel _ et al . _",
    "@xcite proposed a generalized blahut - arimoto algoritm ( gbaa ) to compute the capacity of an fsmc ; and in  @xcite , han also proposed a randomized algorithm to compute the capacity of an fsmc . for both algorithms ,",
    "the concavity of the mutual information rate is a desired property for the convergence ( the convergence of the gbaa requires , in addition , the concavity of certain conditional entropy rate ) . on the other hand , as elaborated in  @xcite , such a desired property , albeit established for a few special cases  @xcite , is not true in general .    the concavity established in the previous section allows us to numerically compute @xmath54 using the algorithm in  @xcite .",
    "the randomized algorithm proposed in  @xcite iteratively compute @xmath283 in the following way : @xmath284,\\\\ \\theta_{n}+a_{n}g_{n^b}(\\theta_{n } ) , & \\mbox{otherwise } , \\end{cases}\\ ] ] where @xmath285 is a simulator for @xmath286 ( for details , see  @xcite ) .",
    "the author shows that @xmath283 converges to the first - order capacity - achieving distribution if @xmath207 is concave with respect to @xmath205 , which has been proven in theorem  [ concavitybec ] .",
    "therefore , with proven convergence , this algorithm can be used to compute the first - order capacity - achieving distribution @xmath287 and the first - order capacity @xmath54 ( in bits ) , which are shown in fig .",
    "[ capacity - achievingdistribution ] and fig .",
    "[ capacity1 ] , respectively .              as in section  [ sub-1 ] , the noiseless capacity of @xmath24-rll constraint @xmath41",
    "is achieved by the first - order markov chain with the transition probability matrix  ( [ achievingmatrix ] ) .",
    "so , we have @xmath288 where @xmath196 . in this section ,",
    "we give a full asymptotic formula for @xmath289 around @xmath44 , which further leads to a full asymptotic formula for @xmath54 around @xmath44 .",
    "the following theorem gives the taylor series of @xmath290 in @xmath40 around @xmath44 , which leads to an explicit formula for the @xmath1-th derivative of @xmath54 at @xmath44 , whose coefficients can be explicitly computed .    [ foc ]",
    "a ) @xmath291 is analytic in @xmath40 for @xmath206 and @xmath292 where @xmath293 is taken over all nonnegative intergers @xmath294 satisfying the constraint @xmath295 and @xmath296    \\b ) @xmath54 is analytic in @xmath40 for @xmath206 with the following taylor series expansion around @xmath44 : @xmath297 where @xmath298 .",
    "\\a ) for @xmath299 , @xmath300 with theorem  [ concavitybec ] establishing the concavity of @xmath53 , @xmath290 should be the unique zero point of the derivative of the mutual information rate .",
    "so , @xmath301 and satisfies @xmath302 according to the analytic implicit function theorem  @xcite , @xmath291 is analytic in @xmath40 for @xmath303 . in the following the @xmath304-th order derivative of @xmath291 at @xmath44",
    "is computed .",
    "it follows from the leibniz formula and the faa di bruno formula  @xcite that @xmath305 which immediately implies a ) .",
    "\\b ) note that @xmath306 it then follows from the leibniz formula that @xmath307 therefore , @xmath308 which immediately implies b ) .    despite their convoluted looks , ( [ theta - max ] ) and ( [ first - order - full ] ) are explicit and computable .",
    "below , we list the coefficients of @xmath54 ( in bits ) and @xmath291 up to the third order , which are numerically computed according to ( [ theta - max ] ) and ( [ first - order - full ] ) and rounded off to the ten thousandths decimal digit :    [ cols=\"^,^,^,^,^\",options=\"header \" , ]",
    "in this section , we consider the input - constrained erasure channel ( [ mec ] ) as in section  [ introduction - section ] however with possible feedback , and we are interested in comparing its feedback capacity @xmath309 and its non - feedback capacity @xmath45 .",
    "the following theorem states that for the erasure channel without any input - constraint , feedback does not increase the capacity and both of them can be computed explicitly .",
    "this result is in fact implied by theorem @xmath130 in  @xcite , where a random coding argument has been employed in the proof ; we nonetheless give an alternative proof in appendix  [ fb - nfb ] for completeness .",
    "[ fbnot ] for the erasure channel  ( [ mec ] ) without any input constraints , feedback does not increase the capacity , and we have @xmath310    on the other hand , we will show in the following that feedback may increase the capacity when the input constraint in the erasure channel is non - trivial . as elaborated below , this",
    "is achieved by comparing the asymptotics of the feedback capacity and the non - feedback capacity for a special input - constrained erasure channel .    in  @xcite ,",
    "sabag _ et al .",
    "_ computed an explicit formula of feedback capacity for bec with @xmath24-rll input constraint @xmath41 .",
    "[ fbc ]  @xcite the feedback capacity of the @xmath24-rll input - constrained erasure channel is @xmath311 where the unique maximizer @xmath312 satisfies @xmath313    clearly , the explicit formula in theorem  [ fbc ] readily gives the asymptotics of the feedback capacity .",
    "to see this , note that @xmath314 and @xmath315 .",
    "straightforward computations yield @xmath316 hence , @xmath317 it then follows from straightforward computations that for the case when @xmath40 is close to @xmath10 , @xmath318 .",
    "so , we have proven the following theorem :    [ yonglong - feedback ] for a bec(@xmath40 ) with the @xmath0-rll input constraint , feedback increases the channel capacity when @xmath40 is small enough .",
    "an independent work in  @xcite also found that feedback does increase the capacity of a bec(@xmath40 ) with the same input constraint @xmath41 , by comparing a tighter bound of non - feedback capacity @xmath319 , obtained via a dual capacity approach , with the feedback capacity @xmath320 .",
    "recently , sabag _ et al . _",
    "@xcite also computed an explicit asymptotic formula for the feedback capacity of a bsc(@xmath40 ) with the input supported on the @xmath24-rll constraint . by comparing the asymptotics of the feedback capacity with the that of non - feedback capacity",
    "@xcite , they showed that feedback does increase the channel capacity in the high snr regime .",
    "it is well known that for any memoryless channel without any input constraint , feedback does not increase the channel capacity .",
    "theorem  [ fbnot ] states that when there is no input constraint , the feedback does not increase the capacity of the erasure channel even with the presence of the channel memory .",
    "theorem  [ yonglong - feedback ] says that feedback may increase the capacity of input - constrained erasure channels even if there is no channel memory .",
    "these two theorems , together with the results in  @xcite , suggest the intricacy of the interplay between feedback , memory and input constraints .",
    "we first prove that @xmath321    a similar argument using the independence of @xmath322 and @xmath323 as in the proof of  ( [ pformula ] ) yields that @xmath324 it then follows that @xmath325}\\sum_{y_1^n:\\mathcal{i}(y_1^n)=d}p(e_{d}=1 , e_{d^c}=0)p(x_{d}=y_{d})\\log p(e_{d}=1 , e_{d^c}=0)\\nonumber\\\\ & { } \\hspace{4.5mm}-\\sum_{d \\subseteq [ 1 , n]}\\sum_{y_1^n:\\mathcal{i}(y_1^n)=d}p(e_{d}=1 , e_{d^c}=0)p(x_{d}=y_{d } ) \\log p(x_{d}=y_{d})\\nonumber\\\\ & = -\\sum_{d \\subseteq [ 1 , n]}p(e_{d}=1 , e_{d^c}=0)\\log p(e_{d}=1 , e_{d^c}=0)+\\sum_{d\\subseteq [ 1 , n]}p(e_{d}=1 , e_{d^c}=0)h(x_d)\\nonumber\\\\ & = \\sum_{d \\subseteq [ 1 , n]}p(e_{d}=1 , e_{d^c}=0)h(x_d)+h(e_1^n)\\nonumber\\\\ & \\le h(e_1^n)+\\sum_{d \\subseteq [ 1 , n]}p(e_{d}=1 ,",
    "e_{d^c}=0)|d| \\log k\\nonumber\\\\ & = h(e_1^n)+\\ee[e_1+\\cdots+e_n ] \\log k \\label{paomo},\\end{aligned}\\ ] ] where the only inequality becomes equality if @xmath3 is i.i.d . with the uniform distribution .",
    "it then further follows that @xmath326 \\log k\\\\ & \\stackrel{(b)}{= } p(e_1=1 ) \\log k\\\\ & = ( 1-{\\varepsilon } ) \\log k , \\end{aligned}\\ ] ] where @xmath71 follows from ( [ paomo ] ) and @xmath144 follows from the ergodicity of @xmath6 . the desired ( [ c - nfb ] ) then follows from the fact that the only inequality @xmath71 becomes equality if @xmath3 is i.i.d . with the uniform distribution",
    ".      let @xmath328 , independent of @xmath323 , be the message to be sent and @xmath329 denote the encoding function .",
    "as shown in  @xcite , @xmath330 using the chain rule for entropy , we have @xmath331 where ( a ) follows from the fact that @xmath332 is a function of @xmath328 and @xmath333 and @xmath334 if and only if @xmath335 , ( b ) follows from the independence of @xmath328 and @xmath323 .    note that for @xmath336 , @xmath337 and for @xmath338 , @xmath339 where @xmath71 follows from the independence of @xmath328 and @xmath323 .",
    "it then follows that @xmath340 where @xmath71 follows from  ( [ not0p ] ) and @xmath144 follows from  ( [ 0p ] ) . since for any @xmath341 $ ] , @xmath342 which implies that @xmath343 is an @xmath344-dimensional probability mass function",
    "therefore , through a similar argument as before , we have @xmath345}\\sum_{y_1^n:\\mathcal{i}(y_1^n)=d}p(e_{d}=1 , e_{d^c}=0)q(y_1^n ) \\log q(y_1^n)\\nonumber\\\\ & \\le h(e_1^n)+\\sum_{d \\subseteq [ 1 , n]}p(e_{d}=1 , e_{d^c}=0)|d| \\log k\\nonumber\\\\ & = h(e_1^n)+\\ee[e_1+\\cdots+e_n ] \\log k , \\label{hz}\\end{aligned}\\ ] ] where the inequality follows from the fact that @xmath346 is an @xmath347-dimensional probability mass function .      * acknowledgement .",
    "* we would like to thank navin kashyap , haim permuter , oron sabag and wenyi zhang for insightful discussions and suggestions and for pointing out relevant references that result in great improvements in many aspects of this work .",
    "d.  m.  arnold and h.  a.  loeliger , `` on the information rate of binary - input channels with memory , '' in _ proceedings of ieee international conference on communications _",
    ", vol .  9 , pp . 26922695 , jun .",
    "d.  m.  arnold , h.  a.  loeliger , p.  o.  vontobel , a.  kavcic and w.  zeng ,  simulation - based computation of information rates for channels with memory , \" _ ieee .",
    "inf . theory _ , vol.52 , no.8 , pp .  34983508 ,",
    "2006 .",
    "b.  marcus , r.  roth , and p.  siegel , `` constrained systems and coding for recording channels , '' _ handbook of coding theory _ ,",
    "i , ii.1em plus 0.5em minus 0.4emamsterdam : north - holland , pp . 16351764 , 1998 .",
    "h.  d.  pfister , `` the capacity of finite - state channels in the high - noise regime , '' _ entropy of hidden markov processes and connections to dynamical systems _ , london math .",
    "lecture note series .",
    "cambridge : cambridge univ . press , 2011 , vol .",
    "179222 .",
    "h.  pfister , j.  b.  soriaga , and p.  siegel , `` on the achievable information rates of finite state isi channels , '' in _ proceedings of ieee global telecommunications conference _",
    ", vol .  5 , pp . 29922996 , nov .",
    "2001 .",
    "yang , shaohua and kavcic , a. and tatikonda , s.,``feedback capacity of stationary sources over gaussian intersymbol interference channels , '' _ global telecommunications conference , 2006 .",
    "globecom 06 .",
    ", no . , pp.16 , nov . 27 2006-dec . 1 2006 .        p.  o.  vontobel , a.  kavi , d.  m.  arnold , and h.  a.  loeliger , `` a generalization of the blahut - arimoto algorithm to finite - state channels , '' _ ieee trans .",
    "inf . theory _",
    "54 , no .",
    "18871918 , may 2008 ."
  ],
  "abstract_text": [
    "<S> in this paper , we examine an input - constrained erasure channel and we characterize the asymptotics of its capacity when the erasure rate is low . more specifically , for a general memoryless erasure channel with its input supported on an irreducible finite - type constraint , we derive partial asymptotics of its capacity , using some series expansion type formulas of its mutual information rate ; and for a binary erasure channel with its first - order markovian input supported on the @xmath0-rll constraint , based on the concavity of its mutual information rate with respect to some parameterization of the input , we numerically evaluate its first - order markov capacity and further derive its full asymptotics . </S>",
    "<S> the asymptotics obtained in this paper , when compared with the recently derived feedback capacity for a binary erasure channel with the same input constraint , enable us to draw the conclusion that feedback may increase the capacity of an input - constrained channel , even if the channel is memoryless .    </S>",
    "<S> _ index terms : _ erasure channel , input constraint , capacity , feedback . </S>"
  ]
}