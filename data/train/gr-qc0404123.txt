{
  "article_text": [
    "the ongoing search for the direct detection of gravitational waves using earth based experiments involves the analysis of observations made at a variety of different detectors .",
    "these observations are time series samples of the detector state that are then processed by various means to identify gravitational wave candidates . broadly speaking the searches for gravitational waves",
    "may be broken up into two categories , those searches that are based upon a model , such as the search for gravitational waves from inspiraling binaries , and those searches that endeavor to find gravitational waves without using a model .",
    "the latter category of searches are often referred to as `` burst '' searches .",
    "these searches typically seek to identify portions of data that are , for a short period , anomalously `` loud '' in comparison to the surrounding data .    identifying a gravitational wave burst in the absence of a source model",
    "is an involved and potentially computationally expensive process .",
    "this is especially true when the ratio of signal power to noise power is low .",
    "a convenient and natural approach to mitigating the computational expense of identifying such bursts divides the problem of detection into two parts . in the first part",
    ", an inexpensive procedure is used to identify candidate sections of data that trigger the second part of the analysis .",
    "the second part of the analysis focuses on the subintervals of data identified by the first .",
    "it is a more computationally complex and expensive analysis that either discards the candidate or identifies it as a gravitational wave burst . in this way the first part of the analysis carried out by a so - called `` event - trigger generator '' , performs triage on the data that must be analyzed by the more complex second stage of the analysis .",
    "blocknormal is an event trigger generator that analyzes data in the time domain and searches for moments in time where the statistical character of the time series data changes . in particular , blocknormal characterizes the time series between change - points by the mean(@xmath0 ) and variance(@xmath1 ) of the samples .",
    "change - points are thus demarcation points , separating `` blocks '' of data that are consistent with a distribution having a given mean and variance , which differs from the mean and/or variance that best characterizes the data in an adjacent block .",
    "the onset of a signal in the data will , because it is uncorrelated with the detector noise , increase the variance of the time series for as long as the signal is present with significant power . in this way blocks with variance greater than a `` background '' variance mark candidate gravitational wave bursts .",
    "since candidate gravitational wave bursts are identified with changes in the detector noise character it is best if the detector noise is itself stationary and white .",
    "the blocknormal analysis thus starts by identifying long segments of data , epochs , which are relatively stationary .",
    "this process involves comparing adjacent stretches of data of fixed a duration long relative to the expected duration of a gravitational wave burst and asking whether adjacent stretches have consistent means and variances . in order to avoid any bias that might come from analyzing outliers in the tails of the distribution ( where one might expect any true signal to be located ) , the mean and variance",
    "are computed on only those samples that are within the 2.5th and 97.5th percentiles of the sample values .",
    "if two consecutive stretches are inconsistent at the @xmath2 confidence level , the begining of the first stretch is used to define a new stationary segment .",
    "segments thus defined are split into a set of frequency bands whose lower band edge is heterodyned to zero frequency .",
    "this base - banding allows for a crude determination of the frequency of any identified triggers .",
    "line and other spectral features are removed , either by kalman filtering or by regression against diagnostic channels , and the final data whitened with a linear filter .",
    "once the data has undergone the base - banding and whitening process , the search for change - points begins in earnest .",
    "the method employed is similar to that described in  @xcite and relies on a bayesian analysis of the relative probability of two different hypotheses :    * @xmath3 , that the time series segment @xmath4 is drawn from a distribution characterized by a single mean and variance ; and * @xmath5 , that the time series segment @xmath4 consists of two continuous and adjacent subsegments each drawn from a distribution characterized by a different mean and/or variance .    given the time series segment @xmath4 , consisting of @xmath6 samples , we write the probability of hypothesis @xmath3 as @xmath7 and the probability of hypothesis @xmath5 as @xmath8 .",
    "the odds of @xmath5 compared to @xmath3 is thus : @xmath9 applying bayes theorem and simplifying this becomes : @xmath10 where , @xmath11 is independent of the data @xmath4 itself , although it does depend on the number of samples @xmath6 .",
    "if @xmath5 is true , then @xmath3 will be a good hypothesis for two subsets of the data , one from sample 1 to @xmath12 , denoted @xmath13 and another from @xmath14 to @xmath6 , @xmath15",
    ". then we can write : @xmath16    to calculate @xmath17 we thus need only be able to calculate @xmath18 for arbitrary time series @xmath4 the probability that a given data set is drawn from a normal distribution with unknown mean and variance is equal to : @xmath19-\\mu)^{2}}{2\\sigma^{2}}}\\end{aligned}\\ ] ]    where @xmath20 is the a priori probability that the mean takes on a value @xmath0 and the variance a value @xmath21 . with the usual uninformative priors for @xmath0 and @xmath22 ( @xmath23 and @xmath24 )",
    "the integral for @xmath18 can be evaluated in closed form : @xmath25^{-\\frac{(n-1)}{2}}i_{n-2 } \\\\",
    "i_{n } \\equiv ( n-1 ) ! ! \\left\\{\\begin{array}{ll}1",
    "& \\mbox{$n$ odd } \\\\ \\sqrt{\\frac{\\pi}{2 } } & \\mbox{$n$ even } \\end{array } \\right.\\end{aligned}\\ ] ] where @xmath26 an @xmath27 .",
    "the value of @xmath17 is therefore the odds that there is a change - point in @xmath28 to there not being any change - points , and the value @xmath29 is related to the odds that there is a change - point at sample @xmath14 to there not being one anywhere in @xmath4 .",
    "the calculated value of @xmath17 is compared to a threshold , @xmath30 , and if greater than this threshold , a change - point is considered to be at the sample with the largest value of @xmath29 .",
    "figure  [ signal ] shows some simulated data along with @xmath29 for that data .",
    "the two peaks in the value of @xmath29 corresponds to where the mean of the simulated noise changes .",
    "[ signal ]   as a function of the hypothetical change point time.,title=\"fig : \" ]    this process either leaves @xmath4 free of change - points , or it divides the data into two subsets . in the latter case , the blocknormal pipeline repeats the change - point analysis on these subsets and all subsequent subsets , until either no more change - points are found , or the subset is less than 4 data points long . by this iteration method",
    "blocknormal breaks the data at these change - points into a set of blocks , each of which is free of any change point .",
    "a final refinement step is taken where successive pairs of blocks are analyzed to check that the change - point would still be considered significant over the subset of the data that is contained within the two blocks .",
    "blocknormal identifies blocks  time series segments between successive change - points  are well characterized by a mean , a variance , a frequency band , a start time , and a duration . to identify unusual blocks , the means and variances",
    "are compared to the mean and variance , @xmath31 and @xmath32 of all the data in their band from the epoch in which they were found .",
    "blocknormal defines `` events '' to be blocks in which the following condition holds true for a value , @xmath33 , that is used to characterize the block : @xmath34 here , @xmath35 , is called the event threshold and is a free parameter in the algorithm which adjusts the sensitivity in defining what `` unusual '' means .",
    "there are a limited number of reasonable other possible threshold requirements based on the three characteristics of a block , @xmath0,@xmath22,@xmath36 , however , at this time these have not been explored .",
    "once events have been identified , immediately adjacent events in the same frequency band are clustered together , with a peak - time for the cluster defined by the central time of the block with the largest value of @xmath37 .",
    "a cluster `` energy '' is a sum over the @xmath38 blocks that comprise it : @xmath39 \\end{aligned}\\ ] ] where @xmath40 is the duration in samples .",
    "a key factor in building confidence in any identification of gravitational waves is the presence of a signal in different detectors . using this `` coincidence '' as the basis for further reducing the number of periods of interest , blocknormal requires that there be coincidence in time between events in the same band but different detectors before a `` trigger ''",
    "is formed . triggers from different bands",
    "are merged if they overlap in time into a single trigger .",
    "figure  [ coinc ] illustrates how this coincidence and merging step works using the three ligo interferometers .",
    "the blocknormal event trigger generator is a time domain analysis in base - banded data that identifies blocks in time which are well characterized by a mean and variance .",
    "blocks with means and variances that are unusually large compared to the mean and variance of the much longer data segment containing them are marked for consideration as being unusual events .",
    "several coincident events in different detectors together form a trigger which can be used to define periods of interest that a more computationally intense analysis can use to reduce the total computing time needed to search for unmodeled gravitational wave events .",
    "this work was supported by the center for gravitational wave physics , the international virtual data grid laboratory , and the national science foundation under award phy 00 - 99559 .",
    "the international virtual data grid laboratory is supported by the national science foundation under cooperative agreement phy-0122557 ; the center for gravitational wave physics is supported by the national science foundation under cooperative agreement phy 01- 14375 .",
    "1 smith a f m 1975 _ biometrika _ * 62*,2 p. 407416"
  ],
  "abstract_text": [
    "<S> in the search for unmodeled gravitational wave bursts , there are a variety of methods that have been proposed to generate candidate events from time series data . </S>",
    "<S> block normal is a method of identifying candidate events by searching for places in the data stream where the characteristic statistics of the data change . </S>",
    "<S> these change - points divide the data into blocks in which the characteristics of the block are stationary . </S>",
    "<S> blocks in which these characteristics are inconsistent with the long term characteristic statistics are marked as event - triggers which can then be investigated by a more computationally demanding multi - detector analysis . </S>"
  ]
}