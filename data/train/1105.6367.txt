{
  "article_text": [
    "in this paper we consider the solution of ill - conditioned linear systems@xmath0 in which we assume @xmath1 to be full rank with singular values that gradually decay to @xmath2 . as reference problems we consider the linear systems arising from the discretization of fredholm integral equation of the first kind ( commonly referred to as discrete ill - posed problems @xcite ) , where @xmath3 represents the discretization of a compact operator .",
    "most of the arguments here presented can also be applied to certain saddle point problems ( see e.g. @xcite ) or even vandermonde type systems arising from interpolation theory ( see e.g. @xcite ) . for important applications , involving for instance vandermonde type systems ,",
    "@xmath4 is assumed to be error - free . on the other hand , working with discrete ill - posed problems one typically assumes the right - hand side @xmath4 affected by noise . in this paper",
    "we consider both cases , taking into account the two possible situations .    in this framework",
    ", it is well known that many krylov type methods such as the cg and the gmres possess certain regularizing properties that allow to consider them as effective alternative to the popular tikhonov regularization method , based on the minimization of the functional@xmath5(@xmath6 denoting the euclidean vector norm ) where @xmath7 is a given parameter and @xmath8 is a regularization matrix ( see e.g. @xcite and @xcite for a background ) .",
    "indeed , since most of krylov methods working with @xmath3 or @xmath9 initially pick up the largest singular values of @xmath3 , they can be interpreted as regularization methods in which the regularization parameter is the iteration number @xmath10 .",
    "we may refer to the recent paper @xcite and the reference therein for an analysis of the spectral approximation properties of the arnoldi - based methods and again @xcite  6 for the cg - like methods .",
    "anyway , in the framework of discrete ill - posed problems , krylov subspace methods also present some important drawbacks .",
    "first of all we may have semi - convergence , that is , the method initially converges but rather rapidly diverges .",
    "this phenomenon typically appears when the krylov method is implemented with the re - orthogonalization of the krylov vectors ( as for instance in the case of the matlab version of the gmres , where the orthogonality of the krylov basis is guaranteed at the machine precision by the use of the householder transformations ) . in this situation , after approximating the larger singular values ( oversmoothing ) the method is also able to provide a good approximation to the smallest ones ( undersmoothing ) .",
    "this allows to reach the maximum accuracy , attained for a certain @xmath11 , but at the same time a reliable stopping criterium needs to be used to avoid divergence . on the other side ,",
    "if a krylov method is implemented without re - orthogonalization it is typically not able to produce good approximation of the smallest singular values .",
    "after say @xmath12 iterations ( normally with @xmath13 , hence in a situation of oversmoothing ) multiple or spurious approximations of the smallest singular values typically appears because of the loss of orthogonality , and the iteration stagnates around @xmath14 .",
    "in this situation a valid stopping rule is no more so crucial but unfortunately the attainable accuracy is generally much poorer than the one obtained by the same method with re - orthogonalization .",
    "we refer to @xcite  6.7 for an exhaustive explanation about the influence of re - orthogonalization in some classical krylov methods .    in order to overcome these problems , in this paper",
    "we present a new method that can be referred to as a preconditioned iterative solver in which the preconditioner is @xmath15 or @xmath16 . in detail , in the noise - free case , the method is based on the solution of the regularized system@xmath17and then on the computation of the solution @xmath18 as@xmath19where @xmath20 , using the standard arnoldi method for matrix functions based on the construction of the krylov subspaces with respect to @xmath21 and @xmath22 , that is , @xmath23 . the method can be viewed as a preconditioned iterative method , since @xmath24 .",
    "we have to remember that a regularization of the type @xmath25 has been considered by franklin in @xcite when @xmath3 is spd .",
    "it is worth noting that , with respect to standard preconditioned krylov methods , in our method only one system with the preconditioner has to be solved so reducing the computational cost .",
    "moreover it is important to point out that for problems in which the singular values of @xmath3 rapidly decay to 0 , as those considered in this paper , each krylov method based on @xmath3 shows a superlinear convergence ( see @xcite chapter 5 ) .",
    "for our method , this fast convergence is preserved since we still work with @xmath3 for the computation of ( [ mf ] ) ( see section [ sec3 ] for details ) .",
    "as we shall see , this idea , i.e. , first regularize then reconstruct , will allow to solve efficiently the problem of divergence without loosing accuracy with respect to the most effective solvers .",
    "the method can be extended to problems in which the right hand side @xmath4 is affected by noise just considering as preconditioner the matrix @xmath16 ( cf .",
    "( [ mn ] ) ) . as",
    "before the idea is to solve the system@xmath26and then to approximate the solution @xmath18 by means of a matrix function evaluation@xmath27where @xmath28 is as before and @xmath29 .",
    "we need to point out that we could unify the theory taking @xmath30 for the noise - free case , and hence work always with the krylov subspaces with respect to the matrix @xmath31 .",
    "however , since @xmath3 is ill - conditioned , for evident reasons , we prefer to consider two separate situations .",
    "thus , we shall denote by asp ( arnoldi with shift preconditioner ) and atp ( arnoldi with tikhonov preconditioner ) the two approaches , respectively , for noise - free and noisy problems respectively .",
    "besides the stability and the good accuracy , there is a third important property that holds in both cases : the reconstruction phase , that is , the matrix function computation , allows to select initially the parameter @xmath32 even much larger ( heavy oversmoothing ) than the one considered optimal by the standard parameter - choice analysis ( l - curve , discrepancy principle , ... , see @xcite for a background ) , without important changes in terms of accuracy . in this sense",
    "the method can be considered somehow independent of the parameter @xmath33 ( see the filter factor analysis presented in section [ sec4 ] ) .",
    "we remark that the idea of using matrix function evaluations to improve the accuracy of the regularization of ill - conditioned linear systems has already been considered in @xcite .",
    "anyway it is important to point out that the approach here presented is completely different since , as said before , only one regularized system needs to be solved .",
    "indeed , in @xcite the authors considers approximations belonging to the krylov subspaces generated by @xmath34 or @xmath35 ( rational krylov approach ) that requires the solution of a regularized linear system at each krylov step .",
    "here we consider polynomial type approximations .",
    "the paper is structured as follows . in section [ sec2 ]",
    "we provide a background about the basic features of the arnoldi method for matrix functions and we present the methods ( asp and atp ) studied in the paper . in section [ sec3 ]",
    "we analyze the error of the asp method , providing also some consideration about the error of both methods in inexact arithmetic . in section [ sec4 ]",
    "we analyze the filter factors of the methods . in section [ sec5 ]",
    "we present some numerical experiments , and a test of image restoration is shown in section [ sec6 ] .",
    "some final comments are given in section [ sec7 ] .",
    "as already partially explained in the introduction , the asp method approximates the solution of the ill - conditioned system @xmath36 in two steps , first solving in some way the regularized system @xmath37and then recovering the solution @xmath18 from the system @xmath38that is equivalent to compute @xmath39where@xmath40independently of the way we intend to approximate @xmath18 from ( fa ) , this represents a novel approach because contrary to standard preconditioned iterative methods here only one linear system with the preconditioner needs to be solved .",
    "of course this is possible because of the special preconditioner we are using but , in principle , the idea can be extended to any polynomial preconditioner .    for the computation of",
    "@xmath41 we use the standard arnoldi method ( or lanczos in the symmetric case ) projecting the matrix@xmath42  onto the krylov subspaces generated by @xmath3 and @xmath22 , that is @xmath43 . for the construction of the subspaces @xmath44",
    ", the arnoldi algorithm generates an orthonormal sequence@xmath45 , with @xmath46 , such that @xmath47 ( here and below the norm used is always the euclidean norm ) . for every @xmath10 , in matrix formulation",
    ", we have @xmath48where @xmath49 $ ] , @xmath50 is an upper hessenberg matrix with entries @xmath51 and @xmath52 is the @xmath53-th vector of the canonical basis of  @xmath54 .",
    "the @xmath10-th arnoldi approximation to @xmath55 is defined as @xmath56(see @xcite and the references therein for a background ) .",
    "for the computation @xmath57 , since the method is expected to produce a good approximation of the solution in a relatively small number of iterations ( see section [ sec3 ] ) , that is for @xmath58 , one typically considers a certain rational approximation to @xmath28 , or , as in our case , the schur - parlett algorithm , @xcite chapter 9 .",
    "we denote by asp method the iteration ( [ asp ] ) independently of the method chosen for solving ( [ sysf1 ] ) . starting from @xmath59 , at each step of the arnoldi algorithm",
    ", we only have to compute the vectors @xmath60 , @xmath61 . below the algorithm used to implement the method .",
    "+ * asp algorithm * +   +    nn = xx = xx = xx = xx = @xmath62 , @xmath63 + * define * @xmath64 + * solve * @xmath65 + @xmath66 + * for * @xmath67 * do * + @xmath68 + @xmath69 + @xmath70 + @xmath71 + @xmath72 + @xmath57 by schur - parlett algorithm + @xmath73 + * end for *    in the above algorithm , the arnoldi method is implemented with the modified gram - schmidt process .",
    "therefore , as is well known , the theoretical orthogonality of the basis is lost quite rapidly and consequently the method is not able to pick up the singular values clustered near 0 .",
    "for this reason at a certain point during the iteration ( [ asp ] ) the method is no longer able to improve the quality of the approximation and it stagnates , typically quite close to the best attainable approximation , and almost independently of the choice @xmath33 ( see section [ sec5 ] ) .    regarding the attainable accuracy ( assuming that the seed @xmath74 is not affected by error ) , by the definition of @xmath28 it depends on the the conditioning of @xmath75 . denoting by @xmath76 the condition number with respect to the euclidean norm ,",
    "theoretically the best situation is attained defining @xmath33 such that@xmath77that is , the condition number of the preconditioner is equal to the condition number of the preconditioned system .",
    "it is quite easy to prove ( see e.g. @xcite ) that in the spd case taking @xmath78 , where @xmath79 and @xmath80 are respectively the smallest and the largest eigenvalue of @xmath3 , we obtain @xmath81 .",
    "the preconditioning effect of @xmath82 of course depends on the choice of @xmath33 . by ( [ eq ] ) it is necessary to find a compromise between the preconditioning and the accuracy in the solution of the systems with @xmath83 . in this sense formula ( [ eq ] ) , that theoretically represents the optimal situation also implicitly states a lower bound for the attainable accuracy .",
    "indeed , many numerical experiments arising from the discretization of fredholm integral equation of the first kind , in which we have examined the behavior of some classical krylov methods such as the gmres and the cg preconditioned with @xmath82 , have revealed that we can substantially improve the rate of convergence ( taking @xmath84 , see again @xcite for a discussion ) but we are not able to improve the accuracy over a certain limit .    the asp method can be extended to problems in which the exact right hand side @xmath85 is affected by noise . anyway , since in presence of noise a good approximation of the exact solution may be meaningless , we extend the idea using the classical tikhonov regularization . moreover , many experiments have shown that the asp method generally produces poor results for problem with noise .",
    "we assume in particular to know only a perturbed right - hand side @xmath86 , where @xmath87 is the perturbation .",
    "given @xmath88 and @xmath89 such that @xmath90 is non singular , for approximating the solution of @xmath91 we solve the regularized system@xmath92and then we approximate @xmath18 by computing@xmath93where @xmath28 is defined by ( [ fz ] ) and @xmath94 . as before , for the computation of @xmath95 we use the standard arnoldi method projecting the matrix@xmath96onto the krylov subspaces generated by @xmath31 and @xmath22 . now , at each step we have have to compute the vectors @xmath97 , @xmath61 , with @xmath46 , that is , to solve the systems @xmath98this means that we actually do not need @xmath31 explicitly .",
    "the algorithm is almost identical to the one given for the asp method , apart from the two steps inserted in a box .",
    "+ * atp algorithm * +   +    nn = xx = xx = xx = xx = @xmath99 , @xmath63 + * define * @xmath64 +   + @xmath66 + * for * @xmath67 * do * +   + @xmath69 + @xmath70 + @xmath71 + @xmath72 + @xmath57 by schur - parlett algorithm + @xmath73 + * end for *    this kind of approach is somehow related with the so - called iterated tikhonov regularization ( see for instance @xcite or @xcite ) , with the important difference that now only one regularized system has to be solved .",
    "[ r1]it is worth noting that the matrix @xmath31 is @xmath90-symmetric , that is , for each @xmath100@xmath101because of this property the atp method can be symmetrized using the lanczos process based on this new metric .",
    "while this approach is promising because computationally less expensive , some preliminary experiments have revealed that it is also quite unstable and , in general , less accurate than the atp method .",
    "for this reason the analysis presented in the next sections does not regard this symmetric variant , that we plan to consider in a future work .",
    "in exact arithmetic the error of the asp method is given by @xmath102 where @xmath103 is defined by ( [ asp ] ) .",
    "if we denote by @xmath104 the vector space of polynomials of degree at most @xmath105 , it can be seen that @xmath106where @xmath22 is the solution of ( [ sysf1 ] ) and @xmath107 interpolates , in the hermite sense , the function @xmath28 at the eigenvalues of @xmath50 , the so - called ritz values . exploiting the interpolatory nature of the standard arnoldi method , we notice , as pointed out also in @xcite , that the error can be expressed in the form@xmath108where@xmath109(see also @xcite ) , and @xmath110from ( [ erapr ] ) , a bound for @xmath111 can be derived working with the field of values of @xmath3 , defined as@xmath112indeed , we can state the following result ( see also the recent papers br and @xcite for a background about the error analysis of the standard arnoldi method for matrix functions ) .    [ pg]assume that @xmath113 .",
    "then@xmath114where @xmath115 is the leftmost point of @xmath116 and @xmath117 . in the symmetric case we can take @xmath118 .    from @xcite",
    ", we know that@xmath119with @xmath117 , and hence by ( [ erapr ] ) @xmath120now @xmath121 is a divided difference that can be bounded using the hermite - genocchi formula ( see e.g. @xcite ) , so that @xmath122where @xmath123 denotes the convex hull of the point set given by @xmath124 and @xmath125 . since @xmath126 , by some well known properties of the arnoldi algorithm , and using the relation@xmath127that arises from ( [ cla ] ) ( see @xcite ) , the result follows .    since @xmath128 , the leftmost point of @xmath116 ,",
    "can be really small for the problems we are dealing with , formula ( [ errb ] ) can surely be considered too pessimistic with respect to what happens in practice .",
    "however , the upper bound given by ( [ errb ] ) allows to derive some important information about the behavior of the error .",
    "first of all , it states that the rate of convergence is little influenced by the choice of @xmath33 , and this is confirmed by the analysis given in section [ sec4 ] and by the numerical experiments .",
    "secondly , it states that , independently of its magnitude , the error decay is related with the rate of the decay of @xmath129 .",
    "we need the following result ( cf .",
    "@xcite theorem 5.8.10 ) .",
    "[ nev]let @xmath130 and @xmath131 , @xmath61 , be respectively the singular values and the eigenvalues of an operator @xmath3 .",
    "assume that @xmath132 and@xmath133let @xmath134 .",
    "then@xmath135where@xmath136    of course , the hypothesis ( [ dec ] ) is fulfilled by many problems arising from the discretization of integral equations , in many cases with @xmath137 quite small .",
    "now , using the relation ( @xcite p. 269 ) , @xmath138that holds for each monic polynomial @xmath139 of exact degree @xmath10 , we can say that theorem [ nev ] reveals that for discrete ill - posed problems the rate of decay of @xmath140 is superlinear and depends on the @xmath137-summability of the singular values of @xmath3 , i.e. , on the degree of ill - posedness of the problem ( cf .",
    "@xcite def . 2.42 ) .    in computer arithmetics ,",
    "we need to assume that @xmath22 , solution of ( [ sysf1 ] ) is approximated by @xmath141 with an accuracy depending on the choice of @xmath33 and the method used . in this way",
    ", the arnoldi algorithm actually constructs the krylov subspaces @xmath142 .",
    "hence the error can be written as@xmath143the above formula expresses the error in two terms , one depending on the accuracy of the arnoldi method for matrix functions and one on the accuracy in the computation of @xmath22 . roughly speaking we can state that for small values of @xmath33 , @xmath144 ( cf .",
    "( [ fa ] ) ) and we have that @xmath145 .",
    "this means that the method is not able to improve the accuracy provided by the solution of the initial system . for large @xmath33",
    "we have that @xmath146 because the system ( [ sysf1 ] ) is well conditioned , but even assuming that @xmath147 that in principle may happen even if @xmath148 is large , we have another lower bound due the ill conditioning of @xmath149 since now @xmath150 has a poor effect as preconditioner .    regarding the optimal choice of @xmath33 we can make the following consideration . unless the re - orthogonalization or the householder implementation is adopted ,",
    "the arnoldi method typically stagnates around the best approximation @xmath14 because of the loss of orthogonality of the krylov basis .",
    "therefore let @xmath151 be such that@xmath152then by ( [ 2p ] ) the optimal value of @xmath33 depends on the method used to compute @xmath153 and is given by@xmath154of course the above formula is interesting only by a theoretical point of view . in practice ,",
    "as mentioned in the introduction , one could try to compare the conditioning of @xmath82 and @xmath155 , by approximating the solution of@xmath156with respect to @xmath33 .",
    "however , since the computation of @xmath74 comes first , it is suitable to take @xmath33 a bit larger than the solution of ( [ cc ] ) .",
    "note that generally such solution can be approximated by @xmath157 .    for the atp method",
    "the analysis is almost identical since the error is given by@xmath158where @xmath159 , @xmath160 , and @xmath161 .",
    "hence , as before we have@xmath162where @xmath163 is again defined by ( [ pol ] ) .",
    "this expression is important since it states that theoretically we may take @xmath33 very large , thus oversmoothing , in order to reduce the effect of noise and then leaving to the arnoldi algorithm the task of recovering the solution .",
    "unfortunately , the main problem is that , as before , @xmath164 may be ill - conditioned for @xmath33 large .",
    "henceforth , even in this case we should find a compromise for the selection of a suitable value of @xmath33 , but contrary to the asp method for noise - free problems it is difficult to design a theoretical strategy .",
    "indeed everything depends on the problem and on the operator @xmath8 . in most cases the noise on the right - hand side produces an increment of the high - frequency components of @xmath85 , that are emphasized on the solution by the nature of the problem . for this reason",
    "@xmath8 is generally taken as a high - pass filter , as for instance a derivative operator , and the solution of ( [ mn ] ) can be interpreted as a numerical approximation via penalization of the constrained minimization problem@xmath165while in standard constrained minimization one approximates the solution taking @xmath33 very large ( theoretically @xmath166 ) , in our case @xmath8 is hardly able to detect efficiently the effect of noise on the numerical solution so that one is forced to adopt some heuristic criterium such as the l - curve analysis . in general terms",
    "we can say that if the solution is smooth and involves only low frequencies then a high - pass filter should lead to a good approximation taking @xmath33 `` large '' . on the other side",
    "if the solution involves itself high - frequencies as in the case of discontinuities , then it is better to undersmooth the problem so reducing the effect of the filter .",
    "we have made these considerations just to point out that a general theoretical indication on the choice of @xmath33 is not possible dealing with problems affected by error .",
    "what we can do is to derive methods able to reduce the dependence on this choice , and the atp method seems to have some chances in this direction .",
    "in order to understand the action of the second phase of the methods , i.e. , the matrix function evaluation applied to the regularized solution ( cf .",
    "( [ fa ] ) and ( [ fq ] ) ) , below we investigate the corresponding filter factors .",
    "assuming for simplicity that @xmath3 is diagonalizable , that is , @xmath167 where @xmath168 , for the asp method we have@xmath169where @xmath170 is the eigenvector associated with @xmath171 , and @xmath172 denotes the @xmath173-th component of a vector .",
    "after the first phase , the filter factors are thus @xmath174 .",
    "since from ( [ pol ] ) , we have @xmath175 where @xmath163 interpolates the function @xmath28 at the eigenvalues of @xmath50 , we immediately obtain@xmath176therefore , at the @xmath10-th step of the asp method the filter factors are given by@xmath177let us compare , with an example , the behavior of the filter factors .",
    "similarly to what was made in @xcite , we consider the problem gravity taken from the hansen s ` regularization tools ` @xcite , with dimension @xmath178 . in figure",
    "[ figure0 ] , the filter factors @xmath179 and @xmath180 , for @xmath181 are plotted . as regularization parameter",
    "we have chosen @xmath182 . since the problem is spd , for more clarity in the pictures , the eigenvalues @xmath171 have been sorted in decreasing order .",
    "( asterisk ) and @xmath183 ( circle ) with @xmath184 , for gravity(12).,width=453 ]    while the problem is rather simple the pictures clearly represent the action of the arnoldi ( lanczos in this case ) steps .",
    "since the arnoldi ( lanczos ) algorithm initially picks up the largest eigenvalues , it automatically corrects the filters corresponding to the low - middle frequencies ( @xmath185 ) , keep damping the highest ones .",
    "the second phase thus performs a correction , but the properties of the arnoldi algorithm guarantees that the method can still be interpreted as a regularizing approach .    for a better explanation of figure [ figure0 ] , let us assume for simplicity that the ritz values @xmath186 , @xmath187 , are distinct ( as in the example ) , so that we can write @xmath188where @xmath189 , @xmath187 are the lagrange polynomials .",
    "hence we obtain@xmath190since the arnoldi algorithm ensures that @xmath191 for @xmath192 we have @xmath193 for @xmath194 . for @xmath195 and when @xmath196 we have that@xmath197 so that the filters are close to the ones of the uncorrected scheme . of course , numerically , the problems start to appear when the arnoldi algorithm fails to provide good approximations of the eigenvalues of @xmath3 , but it is important to observe that , at least in exact arithmetics , the choice of @xmath32 only influences the high frequencies .",
    "for this reason , at least for the asp method , this choice is more related to the conditioning of the subproblems ( cf .",
    "section [ sec3 ] ) .",
    "the filter factor analysis just presented remains valid also for the atp method .",
    "taking @xmath30 in ( [ tt ] ) and using the svd decomposition we easily find that the filter factors are now given by@xmath198and hence our considerations for the asp method remains true also for this case .",
    "of course for @xmath199 we just need to consider the gsvd . for problems with noise ,",
    "the choice of @xmath33 is of great importance .",
    "anyway we have just seen that the correction phase allows to reproduce the low frequencies independently of this choice . in this sense",
    ", in practice we can take @xmath33 even very large in order to reduce as much as possible the influence of noise .",
    "this section is devoted to the numerical experiments obtained on a single processor computer intel core duo t5800 with matlab 7.9 .",
    "our goal is to prove numerically what we consider the valuable properties of the asp and the atp methods , that is , accuracy and speed comparable with the most effective iterative solvers , stability , and reduced dependence on the parameter @xmath33 . for the experiments we consider problems taken from the ` regularization tools matlab ` package by hansen @xcite .",
    "our comparison method is the matlab version of the gmres , that is implemented with the householder algorithm that guarantees the orthogonality of the krylov basis at the machine precision . for the problems here considered",
    "the gmres method has shown to be the most accurate , if compared to other well known methods that we can found in the literature .",
    "since it is also quite unstable , it is generally implemented together with the discrepancy principle as stopping criterium ( where it is possible of course ) , but not always with good results .",
    "we point out that the modified gram - schmidt version of the gmres has also been considered in the experiments ( even if not reported ) ; this version is stable , but unfortunately the attainable accuracy loses one or even two order of magnitude with respect to the version implemented by matlab .",
    "other methods such as the cgls and lsqr are widely inferior for the problems here considered .",
    ", for noise - free baart(240).,width=453 ]    in all experiments the arnoldi algorithm for the asp and the atp methods , as said in section [ sec2 ] , is implemented with the modified gram - schmidt orthogonalization , and the initial linear system is solved with the lu or the cholesky factorization .",
    ".results for baart(240 ) in the noise - free case . [ cols=\"<,^,^,^\",options=\"header \" , ]",
    "in this paper we have presented a new approach for the solution of discrete ill - posed problems .",
    "the basic idea is to solve the problem in two steps : first regularize and then reconstruct .",
    "we have described two methods based on this idea , the asp method that is actually a particular preconditioned iterative solver , and the atp method that is a method that tries to improve the approximation arising from the tikhonov regularization . in both cases",
    "the reconstruction is performed evaluating a matrix function by means of the standard arnoldi method .",
    "this idea can also be interpreted as a modification of the iterated tikhonov regularization ( see for instance @xcite and @xcite ) .",
    "being iterative , both methods should be interpreted as methods depending on two parameters , that is , @xmath33 and the number of iterations @xmath10 .",
    "actually our implementation of the arnoldi method ( gram - schmidt ) is very stable so that for a fixed @xmath33 , the undersmoothing effect , theoretically determined by taking @xmath10 large , in general does not deteriorate the approximation .",
    "therefore the only important parameter is @xmath32 .",
    "anyway , the most important property of both methods is that they do not need an accurate estimate of this parameter to work properly ( cf .",
    "section [ sec4 ] , figures [ figure4 ] and [ figure5 ] , and table 4 ) .",
    "of course this property is particularly attractive for problems in which a parameter - choice analysis is too expensive or even unfeasible as for instance for large scale problems such as the image restoration .    as possible future developments , we observe that the asp method could be quite easily extended to work in connection with polynomial preconditioners ( see e.g. @xcite for a background ) .",
    "this can be done replacing @xmath200 with a suitable @xmath201 and changing accordingly the matrix function to evaluate .",
    "also the symmetric version of the atp method ( see remark [ r1 ] ) seems quite interesting and requires further investigation .",
    "finally , we want to point out that the present paper was just intended to present the basic ideas and properties of the methods ; in this sense , a reliable implementation with stopping criterium , choice of @xmath33 , etc . , has still to be done .",
    "b. hofman , _ regularization of nonlinear problems and the degree of ill - posedness _ , in g. anger , r. gorenflo , h. jochmann , h. moritz and w. webers ( eds . ) , inverse problems : principle and applications in geophysics , technology and medicine , akademie verlag , berlin , 1993 ."
  ],
  "abstract_text": [
    "<S> for the solution of discrete ill - posed problems , in this paper a novel preconditioned iterative method based on the arnoldi algorithm for matrix functions is presented . </S>",
    "<S> the method is also extended to work in connection with tikhonov regularization . </S>",
    "<S> numerical experiments arising from the solution of integral equations and image restoration are presented .    </S>",
    "<S> * keywords : * tikhonov regularization , matrix functions , arnoldi method . </S>"
  ]
}