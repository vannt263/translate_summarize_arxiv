{
  "article_text": [
    "consider the following primal optimization problem , where @xmath0 and @xmath1 are both convex , and @xmath1 can be non - smooth .",
    "@xmath2 @xmath3       this structure , that we will assume , encapsulates problems such as the least absolute selection and shrinkage operator ( lasso ) , where @xmath0 is smooth and @xmath1 is non - smooth , and support vector machines ( svms ) , where both @xmath0 and @xmath1 are smooth @xcite .",
    "the following six convergence conditions are necessary and sufficient .",
    "lipschitz continuity bounds how fast fast a continous function can change : @xmath4 points on the graph of a lipschitz continuous function , the absolute value of the slope of the line connecting these two points is bounded by some definte - real number , called it s lipschitz constant .",
    "the implication of this is that when descending a lipschitz continuous function , we no longer have to consider violent fluctuations in the gradient ; this will turn out to be a valuable inference when selecting stepsizes later on .",
    "@xmath6   @xmath4 @xmath7 @xmath8 @xmath9 @xcite      i.e the eigenvalues of the hessian are bounded above by l @xcite .",
    "@xmath10   @xmath11      for some pre - image @xmath12 , image values for nearby @xmath12 are near the image of @xmath12 , or less than the image of @xmath12 .",
    "this is a weaker notion of continuity for extended - real valued functions @xcite .    ,",
    "but not at @xmath13      that is , a minimum exists ; this should nt be a surprise for convex regularizers , as the sum of two convex functions is too , convex @xcite .",
    "@xmath15       @xmath17 @xcite      the consequence of this condition is that we can , indeed , compute our gradient @xcite ; this sometimes is nt the case for some more exotic problems @xcite .",
    "@xmath20      like the conditions on the gradient , this condition , when satisfied , means that we can compute our proximal , subject to some error @xmath22 @xcite .",
    "often times it is purely impossible , or computationally unatractive , to compute an exact gradient or proximal ; in these cases , it s neccessary to analyse the behavior of the sequence of errors in the limit ; this is the case for the gradient , when using the mini - batch method for a trivially seperable @xmath0 @xcite .    provided are the conditions on the sequences of gradient errors , @xmath25 , and sequences of proximity errors @xmath22 for a selection of the aforementioned methods .      for the basic prox - grad where @xmath0 is convex , the sequence of the normed errors of the gradient , @xmath26 , and the sequence of errors of the proximal , @xmath27 , are summable and decrease as @xmath28 for any @xmath29 @xcite .",
    "@xmath26 and @xmath27 must decrease to zero linearly @xcite .      for accelerated prox - grad where @xmath0 is convex , the sequence of the normed errors of the gradient , @xmath26 , and the sequence of errors of the proximal , @xmath27 , are summable decrease as @xmath30 for any @xmath29 @xcite .",
    "@xmath31 and @xmath32 must decrease linearly to zero @xcite .",
    "c c c c + & convex & strongly convex + [ 0.5ex ] sub - gradient & @xmath33 & @xmath34 & + prox - gradient & @xmath34 & @xmath35 & + accelerated prox - grad & @xmath36 & @xmath37 + admm & @xmath34 & @xmath38 & + [ 1ex ]",
    "in summary , there is a substantial amount of mathematical theory backing the convergence conditions and rates of the proximal methods utilized to solve regularized machine learning objectives .",
    "luckily , a substantial subset of modern machine learning algorithms , such as support vector machines ( svms ) , and the least absolute selection and shrinkage operator ( lasso ) @xcite , can be casted into this form , making the outlined theory in this short paper sufficient for practical use by machine learning practioners in the field",
    ".    further research will include insight into the convergence rate of the strongly convex case of admm , and perhaps even an investigation into the theory behind the vexing emperical results that emerge when objective seperability is exploited , and the computation is distributed over n - machines .",
    "n. parikh and s. boyd .",
    "proximal algorithms . in foundations and trends in optimization ,",
    "pages 196 , 2013 .",
    "mark w. schmidt , nicolas le roux , and francis bach .",
    "convergence rates of inexact proximal - gradient methods for convex optimization . in nips ,",
    "pages 14581466 , 2011 .",
    "andrew cotter , ohad shamir , nathan srebro , and karthik sridharan .",
    "better mini - batch algorithms via accelerated gradient methods . in nips ,",
    "pages 942950 , 2011 .",
    "bingshen he and xiaoming yuan . on the o(1/t )",
    "convergence rate of alternating direction method . in optimization",
    "online , pages 17 , 2011 ."
  ],
  "abstract_text": [
    "<S> analysis of the convergence rates of modern convex optimization algorithms can be achived through binary means : analysis of emperical convergence , or analysis of theoretical convergence . </S>",
    "<S> these two pathways of capturing information diverge in efficacy when moving to the world of distributed computing , due to the introduction of non - intuitive , non - linear slowdowns associated with broadcasting , and in some cases , gathering operations . despite these nuances in the rates of convergence </S>",
    "<S> , we can still show the existence of convergence , and lower bounds for the rates . </S>",
    "<S> this paper will serve as a helpful cheat - sheet for machine learning practitioners encountering this problem class in the field . </S>"
  ]
}