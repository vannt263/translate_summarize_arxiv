{
  "article_text": [
    "self - gravity is one of the most essential physical processes in the universe , and plays important roles in almost all categories of astronomical objects such as globular clusters , galaxies , galaxy clusters , etc . in order to follow the evolution of such systems ,",
    "gravitational @xmath0-body solvers have been widely used in numerical astrophysics .",
    "due to prohibitively expensive computational cost in directly solving @xmath0-body problems , many efforts have been made to reduce it in various ways . for example , several sophisticated algorithms to compute gravitational forces among many particles with reduced computational cost have been developed , such as tree method @xcite , pppm method @xcite , treepm method @xcite , etc .",
    "another approach is to improve the computational performance with the aid of additional hardware , such as grape ( gravity pipe ) systems , special - purpose accelerators for gravitational @xmath0-body simulations @xcite , and general - purpose computing on graphics processing units ( gpgpus ) .",
    "grape systems have been used for further improvement of existing @xmath0-body solvers such as tree method @xcite , pppm method @xcite , treepm method @xcite , p@xmath15m@xmath15 tree method @xcite , and pppt method @xcite .",
    "they have also adapted to simulation codes for dense stellar systems based on fourth - order hermite scheme , such as nbody4 @xcite , nbody1 @xcite , kira @xcite , and gorilla @xcite .",
    "recently , @xcite , @xcite , @xcite , and @xcite explored the capability of commodity graphics processing units ( gpus ) as hardware accelerators for @xmath0-body simulations and achieved similar to or even higher performance than the grape-6a and grape - dr board .",
    "a different approach to improve the performance of @xmath0-body calculations is to utilize streaming simd extensions ( hereafter sse ) , a simd ( single instruction , multiple data ) instruction set implemented on x86 and x86_64 processors .",
    "@xcite exploited the sse and sse2 instruction sets , and achieved speeding up of the hermite scheme @xcite in mixed precision for collisional self - gravitating systems .",
    "although unpublished in literature , nitadori , yoshikawa , & makino have also developed a numerical library for @xmath0-body calculations in single - precision for collisionless self - gravitating systems in which two - body relaxation is not physically important and therefore single - precision floating - point arithmetic suffices for the required numerical accuracy .",
    "furthermore , along this approach , they have also improved the performance in computing arbitrarily - shaped forces with a cutoff distance , defined by a user - specified function of inter - particle separation . such capability to compute force shapes other than newton",
    "s inverse - square gravity is necessary in pppm , treepm , and ewald methods .",
    "it should be noted that grape-5 and the later families of grape systems have similar capability to compute the newton s force multiplied by a user - specified cutoff function @xcite , and can be used to accelerate pppm and treepm methods for cosmological @xmath0-body simulations @xcite .",
    "based on these achievements , a publicly available software package to improve the performance of both collisional and collisionless @xmath0-body simulations has been developed , which was named `` phantom - grape '' after the conventional grape system .",
    "a set of application programming interfaces of phantom - grape for collisionless simulations is compatible to that of grape-5 .",
    "phantom - grape is widely used in various numerical simulations for galaxy formation @xcite and the cosmological large - scale structures @xcite .",
    "recently , a new processor family with `` sandy bridge '' micro - architecture by intel corporation and that with `` bulldozer '' micro - architecture by amd corporation have been released . both of the processors support a new set of instructions known as advanced vector extensions ( avx ) , an enhanced version of the sse instructions . in the avx instruction",
    "set , the width of the simd registers is extended from 128-bit to 256-bit .",
    "we can perform simd operations on two times larger data than before .",
    "therefore , the performance of a calculation with the avx instructions should be two times higher than that with the sse instructions if the execution unit is also extended to 256-bit .",
    "@xcite ( hereafter , paper i ) developed a software library for _ collisional _",
    "@xmath0-body simulations using the avx instruction set in the mixed precision , and achieved a fairly high performance . in this paper , we present a similar library implemented with the avx instruction set but for _ collisionless _ @xmath0-body simulations in single - precision .",
    "the structure of this paper is as follows . in section [ sec : avx ]",
    ", we overview the avx instruction set . in section [ sec : implementation ] , we describe the implementation of phantom - grape . in section [ sec :",
    "accuracy ] and [ sec : performance ] , we show the accuracy and performance , respectively . in section [ sec :",
    "summary ] , we summarize this paper .",
    "in this section , we present a brief review of the advanced vector extensions ( avx ) instruction set .",
    "details of the difference between sse and avx is described in section 3.1 of paper i. avx is a simd instruction set as well as sse , and supports many operations , such as addition , subtraction , multiplication , division , square - root , approximate inverse - square - root , several bitwise operations , etc . in such operations , dedicated registers with 256-bit length called `` ymm registers ''",
    "are used to store the eight single - precision floating - point numbers or four double - precision floating - point numbers .",
    "note that the lower 128-bit of the ymm registers have alias name `` xmm registers '' , and can be used as the dedicated registers for the sse instructions for a backward compatibility .",
    "an important feature of avx and sse instruction sets is the fact that they have a special instruction for a very fast approximation of inverse - square - root with an accuracy of about 12-bit .",
    "actually , this instruction is quite essential to improve the performance of the gravitational force calculations , since the most expensive part in the force calculation is an execution of inverse - square - root of squared distances of the particle pairs .",
    "as already discussed in @xcite , the approximate values can be adopted as initial values of the newton - raphson iteration to improve the accuracy , and we can obtain 24-bit accuracy after one newton - raphson iteration . for collisionless self - gravitating systems , however , the accuracy of @xmath16 12 bits is sufficient because the accuracy of inverse - square - root does not affect the resultant force accuracy if one adopts an approximate @xmath0-body solver such as tree , pppm and treepm methods .",
    "therefore , we use the raw approximate instruction throughout this study .    since the present - day compilers can not always detect concurrency of the loops effectively , and can not fully resolve the mutual dependency among data in the code , it is quite rare that compilers generate codes with simd instructions in effective manners from codes expressed in high - level languages . for an efficient use of the avx instructions",
    ", we need to program with assembly - languages explicitly or compiler - dependent intrinsic functions and data type extensions . in assembly - languages",
    ", we can manually control the assignment of ymm registers to computational data , and minimize the access to the main memory by optimizing the assignment of each register . in this work",
    ", we adopt an implementation of the avx instructions using inline - assembly language with c expression operands , embedded in c - language , which is a part of language extensions of gcc ( gnu compiler collection ) .",
    "here , we describe the detailed implementation to accelerate @xmath0-body calculation using the avx instructions . for a given set of positions @xmath17 of @xmath0 particles ,",
    "we try to accelerate the calculations of a gravitational force given as follows : @xmath18 where @xmath19 is the gravitational constant , @xmath20 the mass of the @xmath21-th particle , and @xmath22 the gravitational softening length .",
    "in addition to that , we also try to accelerate the computations of central forces among particles with an arbitrary force shape @xmath1 given by @xmath23 where @xmath1 specifies the shape of the force as a function of inter - particle separation @xmath24 with a cutoff distance @xmath2 ( i.e. @xmath3 at @xmath4 ) . in the above expressions , particles with subscript `` @xmath21 '' exert forces on those with subscript `` @xmath25 '' . in the rest of this paper , the former",
    "are referred to as `` @xmath21-particles '' , and the latter as `` @xmath25-particles '' just for convenience .    since individual forces exerted by @xmath21-particles on @xmath25-particles can be computed independently , we can calculate forces exerted by multiple @xmath21-particles on multiple @xmath25-particles in parallel . as described in the previous section , the avx instructions can execute operations of eight single - precision floating - point numbers on ymm registers in parallel . by utilizing this feature of the avx instructions , the forces on four @xmath25-particles from two @xmath21-particles",
    "can be computed simultaneously in a simd manner .",
    "in computing the forces on four @xmath25-particles from two @xmath21-particles , we assign the data of @xmath25- and @xmath21-particles to ymm registers in the way shown in figure  [ fig : assignment ] .",
    "suppose that @xmath26 and @xmath27 in figure [ fig : assignment ] are @xmath28-components of @xmath25- and @xmath21-particles , respectively .",
    "subtracting data in the ymm register ( 1 ) of figure  [ fig : assignment ] from data in the ymm register ( 2 ) of figure  [ fig : assignment ] , we simultaneously obtain @xmath28-components of eight relative positions @xmath29 in the ymm register ( 3 ) of figure  [ fig : assignment ] .    in order to effectively realize such simd computations with the avx instructions , we define the structures for @xmath25-particles , @xmath21-particles and the resulting forces and potentials shown in list  [ list : structures ] . before computing the forces on @xmath25-particles , the positions and softening lengths of @xmath25-particles",
    "are stored in the structure ` ipdata ` , and the positions and masses of @xmath21-particles are in the structure ` jpdata ` .",
    "the resulting forces are stored in the structure ` fodata ` .",
    "note that the structures ` ipdata ` and ` fodata ` contain the data of four @xmath25-particles , while the structure ` jpdata ` has the data for a single @xmath21-particle .",
    "note that the positions , softening lengths , and forces of @xmath25-particles in the structures ` ipdata ` and ` fodata ` are declared as arrays of four single - precision floating - point numbers .",
    "thus , the data on each array can be suitably loaded onto , or stored from the lower 128-bit of one ymm register .",
    "the assignment of the @xmath25-particles data shown in ( 1 ) of figure  [ fig : assignment ] can be realized by loading the data of four @xmath25-particles onto the lower 128-bit of one ymm register , and copying the data to its upper 128-bit .    as for @xmath21-particles , since the structure ` jpdata ` consists of four single - precision floating - point numbers , we can load the positions and the masses of two @xmath21-particles in one ymm - register at one time if they are aligned on the 32-byte boundaries . by broadcasting the @xmath30-th element ( @xmath31 and 3 ) in each of the lower and upper 128-bit to all the other elements , we can realize the assignment of the @xmath21-particle data as depicted in ( 2 ) of figure  [ fig : assignment ] .    after executing the gravitational force loop over @xmath21-particles , the partial forces on four @xmath25-particles exerted by different sets of @xmath21-particles",
    "are stored in the upper and lower 128-bit of a ymm register .",
    "operating sum reduction on the upper and lower 128-bit of the ymm register , and storing the results into its lower 128-bit , we can smoothly store the results into the structure ` fodata ` .    .... //",
    "structure for i - particles typedef struct ipdata {    float x[4 ] ;    float y[4 ] ;    float z[4 ] ;    float eps2[4 ] ; } ipdata , * pipdata ;    // structure for j - particles typedef struct jpdata {    float x , y , z , m ; } jpdata , * pjpdata ;    // structure for the resulting forces // and potentials of i - particles typedef struct fodata {    float ax[4 ] ;    float ay[4 ] ;    float az[4 ] ;    float phi[4 ] ; } fodata , * pfodata ; ....      for the readability of the source codes shown below , let us introduce some preprocessor macros which are expanded into inline assembly codes . the definitions of the macros used in this paper are given in list  [ list : macros ] .",
    "for macros with two and three operands , the results are stored in the second and third one , respectively , and the other operands are source operands . in these macros , operands named ` src ` , ` src1 ` , ` src2 ` , and ` dst ` designate the data in xmm or ymm registers , and those named ` mem ` , ` mem64 ` , ` mem128 ` , and ` mem256 ` are data in the main memory or the cache memory , where numbers after ` mem ` indicate their size and alignment in bits .",
    "brief descriptions of these macros are summarized in table  [ tab : macros ] .",
    "more detailed explanation of the avx instructions can be found in intel s website .",
    ".... # define vzeroall asm(\"vzeroall \" ) ; # define vloadps(mem256 , dst ) \\",
    "asm(\"vmovaps % 0 , % \" dst::\"m\"(mem256 ) ) ; # define vstorps(reg , mem256 ) \\    asm(\"vmovaps %",
    "\" reg \" , % 0 \" : : \" m\"(mem256 ) ) ; # define vloadps(mem128 , dst ) \\",
    "asm(\"vmovaps % 0 , % \" dst::\"m\"(mem128 ) ) ; # define vstorps(reg , mem128 ) \\    asm(\"vmovaps % \" reg \" , % 0 \" : : \" m\"(mem128 ) ) ; # define vloadlps(mem64 , dst ) \\",
    "asm(\"vmovlps % 0 , % \" dst \" , % \" dst::\"m\"(mem64 ) ) ; # define vloadhps(mem64 , dst ) \\",
    "asm(\"vmovhps % 0 , % \" dst \" , % \" dst::\"m\"(mem64 ) ) ; # define vbcastl128(src , dst ) \\",
    "asm(\"vperm2f128 % 0 , % \" src \" , % \" src \\    \" , % \" dst \" \" : : \" g\"(0x00 ) ) ; # define vcopyu128tol128(src , dst ) \\    asm(\"vextractf128 % 0 , % \" src \" , % \" dst \\    \" \" : : \" g\"(0x01 ) ) ; # define vgatherl128(src1,src2,dst ) \\    asm(\"vperm2f128 % 0 , % \" src2 \" ,",
    "% \" src1 \\    \" , % \" dst \" \" : : \" g\"(0x02 ) ) ; # define vcopyall(src , dst ) \\",
    "asm(\"vmovaps % 0 , % \" src \" , % \" dst ) ; # define vbcast0(src , dst ) \\",
    "asm(\"vshufps % 0 , % \" src \" , % \" src \\    \" , % \" dst \" \" : : \" g\"(0x00 ) ) ; # define vbcast1(src , dst ) \\",
    "asm(\"vshufps % 0 , % \" src \" , % \" src \\    \" , % \" dst \" \" : : \" g\"(0x55 ) ) ; # define vbcast2(src , dst ) \\",
    "asm(\"vshufps % 0 , % \" src \" , % \" src \\    \" , % \" dst \" \" : : \" g\"(0xaa ) ) ; # define vbcast3(src , dst ) \\    asm(\"vshufps % 0 , % \" src \" , % \" src \\    \" , % \" dst \" \" : : \" g\"(0xff ) ) ; # define vmix0(src1,src2,dst ) \\",
    "asm(\"vshufps % 0 , % \" src2 \" , % \" src1 \\    \" , % \" dst \" \" : : \" g\"(0x88 ) ) ; # define vmix1(src1,src2,dst ) \\",
    "asm(\"vshufps % 0 , % \" src2 \" , % \" src1 \\    \" , % \" dst \" \" : : \" g\"(0xdd ) ) ; # define vaddps(src1 , src2 , dst ) \\",
    "asm(\"vaddps \" src1 \" , \" src2 \" , \" dst ) ; # define vsubps(src1 , src2 , dst ) \\",
    "asm(\"vsubps \" src1 \" , \" src2 \" , \" dst ) ; # define vsubps_m(mem256 , src , dst ) \\",
    "asm(\"vsubps % 0 , % \" src \" , % \" dst \\    \" \" : : \" m\"(mem256 ) ) ; # define vmulps(src1 , src2 , dst ) \\",
    "asm(\"vmulps \" src1 \" , \" src2 \" , \" dst ) ; # define vrsqrtps(src , dst ) \\",
    "asm(\"vrsqrtps \" src \" , \" dst ) ; # define vminps(src1 , src2 , dst ) \\    asm(\"vminps \"   src1 \" , \" src2 \" , \" dst ) ; # define vpsrld(imm , src1 , src2 ) \\",
    "asm(\"vpsrld % 0 , % \" src1 \" , % \" src2::\"i\"(imm ) ) ; # define vpslld(imm , src1 , src2 ) \\",
    "asm(\"vpslld % 0 , % \" src1 \" , % \" src2::\"i\"(imm ) ) ; # define prefetch(mem ) \\    asm(\"prefetcht0 % 0\"::\"m\"(mem ) ) ; ....    [ cols=\"<,<\",options=\"header \" , ]     & 1000 & 00000000000000000 & 1023 +    an affine - transformed squared distance at a sampling point with an index specified by a lower @xmath32 exponent bits @xmath33 and an upper @xmath34 fraction bits @xmath35 is expressed as @xmath36 the ratio between inter - particle distances whose affine - transformed squared distances are @xmath37 and @xmath38 is given by @xmath39 where @xmath40 is assumed for the last approximation . the interval between inter - particle distances",
    "whose affine - transformed distances are @xmath41 and @xmath42 is calculated as @xmath43 where we also assume @xmath40 and @xmath44 for the last approximation .",
    "therefore , the sampling points with the same fraction bits are distributed uniformly in logarithmic scale , and those with the same exponent bits are aligned uniformly in linear scale unless the fraction bit is small .    as an example",
    ", we illustrate how the sampling points of the look - up table depend on the pre - defined integers @xmath32 and @xmath34 in figure  [ fig : comp_ef ] .",
    "we first see the cases in which either of @xmath32 and @xmath34 is zero , in order to see the roles of the integers @xmath32 and @xmath34 .",
    "as seen in figure  [ fig : comp_ef ] , the intervals of sampling points are roughly uniform in linear scale for the case @xmath45 ( the bottom line in the top panel ) , and uniform in logarithmic scale for the case @xmath46 ( the middle line in the bottom panel ) , unless @xmath47 is small .",
    "as expected above , the integers @xmath32 and @xmath34 control the number of sampling points in logarithmic and linear scales , respectively .    by comparing the sampling points with @xmath48 and those with @xmath49 ( see the top panel of figure  [ fig : comp_ef ] ) , it can be seen that all intervals of the sampling points with @xmath48 ( indicated by the vertical dashed lines and double - headed arrows ) are divided nearly equally into @xmath50 regions by the sampling points with @xmath51 .",
    "thus , our binning scheme is a hybrid of the linear and logarithmic binning schemes .     and @xmath34 .",
    "the top and bottom panels take horizontal axes in linear and logarithmic scales , respectively . ]",
    "figure  [ fig : decide_ef ] shows the comparison of the several binning in which the number of sampling points is fixed to @xmath52 .",
    "one can see that the binning with @xmath51 has sufficient sampling points in the range of @xmath53 , whereas the binning with the other sets of @xmath54 only samples the region of @xmath55 .",
    "the number of the extracted exponent bit @xmath32 should be large enough so that the scale of the softening length should be sufficiently resolved .",
    "for example , if @xmath56 , @xmath32 should be set to at least equal to or larger than @xmath57 .     and @xmath34 .",
    "]    in list  [ list : bit_binning ] , we present routines for constructing the look - up table . in our implementation , the look - up table contains two values : one is the force at a sampling point @xmath58 , @xmath59 and the other is its difference from the next sampling point @xmath60 divided by the interval of the affine - transformed squared distance @xmath61 where subscript @xmath62 indicates indices of the look - up table , and is expressed as @xmath63 .",
    "using these two values , we can compute the linear interpolation of @xmath64 at a radius @xmath24 with @xmath65 by @xmath66 .",
    "the @xmath67 and @xmath68 , are stored in a two - dimensional array declared as ` force_table[tbl_size][2 ] ` , where ` tbl_size ` is the number of the sampling points ( @xmath69 ) and the values of the @xmath67 and @xmath68 are stored in the ` force_table[k][0 ] ` and ` force_table[k][1 ] ` , respectively . since the values of @xmath67 and @xmath68 are stored in the adjacent memory address , we can avoid the cache misses in computing the linearly interpolated values of @xmath64 .",
    ".... # define exp_bit ( 4 ) # define frc_bit ( 6 ) # define tbl_size ( 1 < < ( exp_bit+frc_bit ) ) //",
    "1024    extern float force_table[tbl_size][2 ] ; // 8 kb    union pack32 {    float f ;    unsinged int u ; } ;    void generate_force_table(float rcut ) {    unsigned int tick ;    float fmax , r2scale , r2max ;    union pack32 m32 ;       float force_func(float ) ;       tick = ( 1 < < ( 23-frc_bit ) ) ;    fmax = ( 1 < < ( 1<<exp_bit))*(2.0 - 1.0/(1<<frc_bit ) ) ;    r2max = rcut*rcut ;    r2scale = ( fmax-2.0f)/r2max ;       for(i=0,m32.f=2.0f;i < tbl_size;i++,m32.u+=tick ) {      float f , r2 , r ;           f = m32.f ;      r2 = ( f-2.0)/r2scale ;      float r = sqrtf(r2 ) ;      force_table[i][0 ] = force_func(r ) ;    }       for(i=0,m32.f=2.0f;i <",
    "tbl_size-1;i++ ) {      float x0 = m32.f ;      m32.u + = tick ;      float x1 = m32.f ;      float y0 = force_table[i][0 ] ;      float y1 = ( i==tbl_size-1 ) ?",
    "0.0                                 : force_table[i+1][0 ] ;      force_table[i][1 ] = ( y1-y0)/(x1-x0 ) ;    }    force_table[i][1 ] = 0.0f ; } ....    in figure  [ fig : binning ] , we compare the conventional binning with equal intervals in squared distances to our binning with @xmath70 and @xmath71 ( i.e. 64 sampling points ) , for the @xmath72-force shape @xcite used in the pppm scheme .",
    "although we adopt @xmath73 in the rest of this paper , we set @xmath71 here just for good visibility of the difference of the two binning schemes .",
    "it should be noted that the number of sampling points is the same ( 64 ) in both schemes .",
    "compared with the conventional binning scheme in the top panel , our binning scheme can faithfully reproduce the given functional form even at distances smaller than the gravitational softening length .     in the conventional scheme with 64 constant intervals in @xmath74 ( top panel ) and in our scheme with @xmath70 and @xmath71 ( bottom panel ) between",
    "@xmath75 $ ] .",
    "although we adopt @xmath73 elsewhere in this paper , we set @xmath71 here for viewability .",
    "@xmath76 is assumed as a functional form of @xmath1 , in which @xmath77 is the @xmath72-profile @xcite ( see equation ( [ eq : s2 ] ) ) .",
    "solid lines indicate the shape of @xmath64 .",
    "vertical dashed lines in both panels are the locations of the gravitational softening length @xmath22 . ]      in calculating the arbitrary central forces , the data of @xmath25- and @xmath21-particles are stored in the structures ` ipdata ` and ` jpdata ` , respectively , in the same manner as described in the case for calculating the newton s force , except that the coordinates of @xmath25- and @xmath21-particles are scaled as @xmath78 so that we can quickly compute the affine - transformed squared inter - particle distances between @xmath25- and @xmath21-particles . as in the case of the newton s force",
    ", we compute the forces of four @xmath25-particles exerted by two @xmath21-particles using the avx instructions . using the scaled positions of the particles , the calculation of the forces is performed in the force loop as follows ;    1 .",
    "calculate an affine - transformed distance between @xmath25- and @xmath21-particles , @xmath79 , as @xmath80 where the function `` @xmath81 '' returns the minimum value among arguments .",
    "derive an index @xmath62 of the look - up table from the affine - transformed squared distance , @xmath79 , computed in the previous step by applying a bitwise - logical right shift by @xmath82 bits and reinterpreting the result as an integer .",
    "3 .   refer to the look - up table to obtain @xmath67 and @xmath68 .",
    "note that the address of the pointer to ` fcut ` is decremented by 1<<(30-(23-f ) ) in advance ( see line 24 in list  [ list : arbitraryforce ] ) to correct the effect of the most significant exponential bit of @xmath79 .",
    "4 .   derive an affine - transformed distance @xmath83 that corresponds to the @xmath62-th sampling point @xmath58 by applying a bitwise - logical left shift by @xmath82 bits to @xmath62 and reinterpreting the result as a single - precision floating - point number .",
    "compute the value of @xmath84 by the linear interpolation of @xmath67 and @xmath85 . using the values of @xmath67 and @xmath68 ,",
    "the interpolation can be performed as @xmath86 6 .",
    "accumulate scaled `` forces '' on @xmath25-particles as @xmath87    after the force loop , the scaled `` forces '' are rescaled back as @xmath88    the actual code of the force loop for the calculation of the central force with an arbitrary force shape is shown in list  [ list : arbitraryforce ] .",
    "note that bitwise - logical shift instructions such as ` vpsrld ` and ` vpslld ` can be operated only to xmm registers or the lower 128-bit of ymm registers . in order to operate bitwise - logical shift instructions to data in the upper 128-bit of a ymm register",
    ", we have to copy the data to the lower 128-bit of another ymm register .",
    "bitwise - logical shift operations to the upper 128-bit of ymm registers are supposed to be implemented in the future avx2 instruction set .",
    "also note that we can not refer to the look - up table in a simd manner and have to issue the ` vloadlps ` and ` vloadhps ` instructions one by one ( see lines 8992 and 9497 in list  [ list : arbitraryforce ] ) .",
    "except for those operations , all the other calculations are performed in a simd manner using the avx instructions .",
    ".... # define frc_bit ( 6 ) # define align32 _ _ attribute _ _ ( ( aligned(32 ) ) ) # define align64 _ _ attribute _ _ ( ( aligned(64 ) ) )    typedef float   v4sf _ _ attribute _ _ ( ( vector_size(16 ) ) ) ; typedef struct ipdata_reg {    float x[8 ] ;    float y[8 ] ; } ipdata_reg , * pipdata_reg ;    void gravitykernel(pipdata ipdata ,                       pjpdata jp ,                      pfodata fodata ,                      int nj ,                      float fcut[][2 ] ,                      v4sf r2cut , v4sf accscale ) {    int j ;    unsigned long int align64 idx[8 ]    = { 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 } ;    ipdata_reg align32 ipdata_reg ;    static v4sf two = { 2.0f , 2.0f , 2.0f , 2.0f } ;      fcut -= ( 1<<(30-(23-frc_bit ) ) ) ;      vzeroall ;      vloadps(ipdata->x[0 ] , x2_x ) ;    vloadps(ipdata->y[0 ] , y2_x ) ;    vloadps(ipdata->z[0 ] , z2_x ) ;    vloadps(r2cut , r2cut_x ) ;    vloadps(two , two_x ) ;    vbcastl128(x2 , x2 ) ;    vstorps(x2 , ipdata_reg.x[0 ] ) ;    vbcastl128(y2 , y2 ) ;    vstorps(y2 , ipdata_reg.y[0 ] ) ;    vbcastl128(z2 , zi ) ;    vbcastl128(r2cut , r2cut ) ;    vbcastl128(two , two ) ;      vloadps(*jp , mj ) ;    jp + = 2 ;      vbcast0(mj , x2 ) ;    vbcast1(mj , y2 ) ;    vbcast2(mj , z2 ) ;      vsubps_m(*ipdata_reg.x , x2 , dx ) ;    vmulps(dx , dx , x2 ) ;    vaddps(two , x2 , x2 ) ;      vsubps_m(*ipdata_reg.y , y2 , dy ) ;    vmulps(dy , dy , y2 ) ;    vaddps(x2 , y2 , y2 ) ;         vsubps(zi , z2 , dz ) ;    vmulps(dz , dz , z2 ) ;    vaddps(y2 , z2 , y2 ) ;         vbcast3(mj , mj ) ;    vmulps(mj , dx , dx ) ;    vmulps(mj , dy , dy ) ;    vmulps(mj , dz , dz ) ;      vminps(r2cut , y2 , z2 ) ;      for(j = 0 ; j < nj ; j + = 2 ) {      vloadps(*jp , mj ) ;      jp + = 2 ;        vcopyu128tol128(z2 , y2_x ) ;      vpsrld(23-frc_bit , y2_x , y2_x ) ;      vpsrld(23-frc_bit , z2_x , x2_x ) ;               vstorps(x2_x , idx[0 ] ) ;      vstorps(y2_x , idx[4 ] ) ;        vpslld(23-frc_bit , y2_x , y2_x ) ;      vpslld(23-frc_bit , x2_x , x2_x ) ;        vgatherl128(y2 , x2 , y2 ) ;      vsubps(y2 , z2 , z2 ) ;        vbcast0(mj , x2 ) ;      vbcast1(mj , y2 ) ;      vsubps_m(*ipdata_reg.x , x2 , x2 ) ;      vsubps_m(*ipdata_reg.y , y2 , x2 ) ;        vloadlps(*fcut[idx[4 ] ] , buf0_x ) ;      vloadhps(*fcut[idx[5 ] ] , buf0_x ) ;      vloadlps(*fcut[idx[0 ] ] , buf1_x ) ;      vloadhps(*fcut[idx[1 ] ] , buf1_x ) ;      vgatherl128(buf0 , buf1 , buf1 ) ;      vloadlps(*fcut[idx[6 ] ] , buf2_x ) ;      vloadhps(*fcut[idx[7 ] ] , buf2_x ) ;      vloadlps(*fcut[idx[2 ] ] , buf0_x ) ;      vloadhps(*fcut[idx[3 ] ] , buf0_x ) ;      vgatherl128(buf2 , buf0 , buf2 ) ;      vmix1(buf1 , buf2 , buf0 ) ;      vmix0(buf1 , buf2 , buf2 ) ;        vmulps(z2 , buf0 , buf0 ) ;        vbcast2(mj , z2 ) ;      vbcast3(mj , mj ) ;      vsubps(zi , z2 , z2 ) ;        vaddps(buf0 , buf2 , buf2 ) ;      vmulps(buf2 , dx , dx ) ;      vmulps(buf2 , dy , dy ) ;      vmulps(buf2 , dz , dz ) ;        vaddps(dx , ax , ax ) ;      vaddps(dy , ay , ay ) ;      vaddps(dz , az , az ) ;        vcopyall(x2 , dx ) ;      vcopyall(y2 , dy ) ;      vcopyall(z2 , dz ) ;        vmulps(x2 , x2 , x2 ) ;      vmulps(y2 , y2 , y2 ) ;      vmulps(z2 , z2 , z2 ) ;        vaddps(two , x2 , x2 ) ;      vaddps(z2 , y2 , y2 ) ;      vaddps(x2 , y2 , y2 ) ;           vmulps(mj , dx , dx ) ;      vmulps(mj , dy , dy ) ;      vmulps(mj , dz , dz ) ;      vminps(r2cut , y2 , z2 ) ;    }    vcopyu128tol128(ax , x2_x ) ;    vaddps(ax , x2 , ax ) ;    vcopyu128tol128(ay , y2_x ) ;    vaddps(ay , y2 , ay ) ;    vcopyu128tol128(az , z2_x ) ;    vaddps(az , z2 , az ) ;      vmulps_m(accscale , ax_x , ax_x ) ;    vmulps_m(accscale , ay_x , ay_x ) ;    vmulps_m(accscale , az_x , az_x ) ;      vstorps(ax_x , * fodata->ax ) ;    vstorps(ay_x , * fodata->ay ) ;    vstorps(az_x , * fodata->az ) ; } ....    although the avx instruction set takes the non - destructive 3-operand form , the copy instruction between registers appeared in the code above , which was intended to avoid the inter - register dependencies .      on multi - core processors",
    ", we can parallelize the calculations of the forces of @xmath25-particles for both of the newton s force and arbitrary central forces using the openmp programming interface by assigning a different set of four @xmath25-particles onto each processor core .",
    "list  [ list : parallel ] shows a code fragment for the parallelization of the computations of the newton s force .",
    "the calculation of an arbitrary force can be parallelized similarly to that of newton s force .",
    ".... # define isimd 4    extern ipdata ipos[ni_memmax / isimd ] ; extern jpdata jpos[nj_memmax ] ; extern fodata iacc[ni_memmax / isimd ] ;    int nig = ni / isimd + ( ni % isimd ? 1 : 0 )     # pragma omp parallel for for(i = 0 ; i < nig ; i++ )    gravitykernel(&ipos[i ] , & iacc[i ] , jpos , nj ) ; ....      with the implementations of the force calculation accelerated with the avx instructions described above , we develop a set of application programming interfaces ( apis ) for @xmath0-body simulations , which is compatible to grape-5 library , except that our library do not support functions to search for neighbours of a given particle .",
    "the apis are shown in list  [ list : api ] . `",
    "g5_set_xmj ` sends the data of @xmath21-particles to the array of the structure ` jpdata ` . `",
    "g5_calculate_force_on_x ` sends the data of @xmath25-particles to the array of the structure ` ipdata ` , and computes the forces and potentials of @xmath25-particles and returns them into the arrays ` ai ` and ` pi ` , respectively .    in the function ` g5_open ` , we derive statistical bias of the fast approximation of inverse - square - root , ` vrsqrtps ` instruction .",
    "as @xcite reported , the results of this instruction contains a bias which is implementation - dependent .",
    "we statistically correct this bias in the same way as @xcite .",
    "softening length and the number of @xmath21-particles are set by the functions ` g5_set_eps_to_all ` and ` g5_set_n ` , respectively . `",
    "g5_close ` does nothing and is just for compatibility with the grape-5 library .",
    "list  [ list : sample ] shows a code fragment to perform an @xmath0-body simulation , using this apis .",
    ".... void g5_open(void ) ; void g5_close(void ) ; void g5_set_eps_to_all(double eps ) ; void g5_set_n(int nj ) ; void g5_set_xmj(int adr ,                  int nj ,                  double ( * xj)[3 ] ,                  double * mj ) ; void g5_calculate_force_on_x(double ( * xi)[3 ] ,                               double ( * ai)[3 ] ,                               double * pi ,                               int ni ) ; ....    .... int n ;              // the number of particles double m[nmax ] ;     // mass double x[nmax][3 ] ; // position double v[nmax][3 ] ; //",
    "velocity double a[nmax][3 ] ; //",
    "force double p[nmax ] ;     //",
    "potential double t ;           //",
    "time double tend ;        // time at the finish time double dt ;          // timestep void time_integrator(int ,                       double ( * ) [ 3 ] ,                       double ( * ) [ 3 ] ,                       double ( * ) [ 3 ]                       double ) ;                     // function for time integration    g5_open ( ) ; g5_set_eps_to_all(eps ) ; g5_set_n(n ) ; while(t < tend ) {    g5_set_xmj(0,n , x , m ) ;    g5_calculate_force_on_x(x , a , p , n ) ;    time_integrator(n , x , v , a , dt ) ;    t + = dt ; } g5_close ( ) ; ....    for the version of arbitrary force shape , we provide a new api call to set the force - table through a function pointer , which is not compatible to the grape-5 api .",
    "we investigate accuracy of forces and potentials obtained by our implementation for newton s force . for this purpose ,",
    "we compute the forces and potentials of particles in the plummer models using our implementations and compare them with those computed fully in double - precision floating - point numbers without any explicit use of the avx instructions . for the calculations of the forces and the potentials , we adopt the direct particle - particle method and",
    "the softening length of @xmath89 , where @xmath90 is a virial radius of the plummer model and @xmath0 is the number of particles .",
    "figure  [ fig : newton_error ] shows the cumulative distribution of relative errors in the forces and the potentials of particles , @xmath91 and @xmath92 where @xmath93 and @xmath94 are the force and the potential calculated using our implementation , and @xmath95 and @xmath96 are those computed fully in double - precision .",
    "it can be seen that most of the particles have errors less than @xmath97 .",
    "these errors primarily come from the approximate inverse - square - root instruction ` vrsqrtps ` , whose accuracy is about 12-bit , and consistent with the typical errors of @xmath98 .",
    "while the errors of the forces are distributed down to less than @xmath99 , the errors of the potentials are mostly larger than @xmath100 .",
    "it can be ascribed to the way of excluding the contribution of self - interaction to the potentials . in computing a potential of the @xmath25-th particle ,",
    "we accumulate the contribution from particle pairs between the @xmath25-th particle and all the particles including itself , and then subtract the contribution of the potential between the @xmath25-th particle and itself , @xmath101 to finally obtain the correct potential of the @xmath25-th particle .",
    "note that the potential between the @xmath25-th particle and itself is largest among the potentials between the @xmath25-th particle and all the particles , since the separation between @xmath25-particle and itself is zero .",
    "thus , the subtraction of the `` potential '' due to the self - interaction causes the cancellation of the significant digits , and consequently degrades the accuracy of the potentials .",
    "a remedy for such degradation of the accuracy is to avoid the self - interaction in the force loop .",
    "in fact , we do so in calculating the potentials in double - precision ( @xmath96 ) in figure  [ fig : newton_error ] .",
    "however , such treatment requires conditional bifurcation inside the force loop , and significantly reduces the computational performance .",
    "the potentials of particles are usually necessary only for checking the total energy conservation , and the accuracy obtained in our implementation is sufficient for that purpose . for these reasons , we choose the original way to compute the potentials of particles in our implementation .          in order to see accuracies of central forces with an arbitrary shape obtained in our implementation , we choose a force shape which is frequently adopted in cosmological @xmath0-body simulations using pppm or treepm methods .",
    "such methods are comprised of the particle  mesh ( pm ) and the particle  particle ( pp ) parts which compute long- and short - range components of inter - particle forces , respectively .",
    "our implementation of the calculation of arbitrarily - shaped central forces can accelerate the calculation of the pp part , in which the force shape is different from the newton s force and is expressed as @xmath102 where @xmath103 is the so - called @xmath72-profile with a softening length of @xmath26 @xcite given by @xmath104    we calculate forces exerted between @xmath57k particle pairs with various separations uniformly distributed in @xmath105 in the range of @xmath106 using our implementation described in section  [ sec : methodarbitrary ] , where @xmath107k is equal to @xmath108 .",
    "we set @xmath22 and @xmath2 to @xmath109 and @xmath110 , and masses to unity . in creating the look - up table of the force shape , we set @xmath70 and @xmath73",
    ".    figure  [ fig : s2_error ] shows a functional form of @xmath111 ( solid curve ) and @xmath1 ( dashed curve ) in the top panel and relative errors of forces including both pp and pm parts , i.e. @xmath111 , in the bottom panel as a function of @xmath112 . in figure",
    "[ fig : s2_error ] , we can see that the relative errors are less than @xmath113 , which are sufficiently accurate for cosmological @xmath0-body simulations .     and @xmath1 ( upper panel ) and the relative errors of forces of particle pairs with a separation @xmath24 ( bottom panel ) as a function of @xmath112 , where the forces include both pp and pm parts . here , @xmath114 and @xmath115 are , respectively , an absolute force calculated with our implementation and that obtained by performing all the calculations in double - precision without referring to the look - up table .",
    "the separations of particle pairs are distributed uniformly in @xmath105 in the range of @xmath116 . ]",
    "in this section , we present the performance of our implementation of the collisionless @xmath0-body simulation using the avx instructions ( hereafter avx - accelerated implementation ) . for the measurement of the performance , we use an intel core i72600 processor with 8 mb cache memory and a frequency of @xmath117 ghz , which contains four processor cores . in measuring the performance ,",
    "intel turbo boost technology is disabled , and intel hyper - threading technology ( htt ) is enabled . a compiler which we adopt is gcc 4.4.5 , with options -o3 -ffast - math -funroll - loops . to see the advantage of the avx instructions relative to the sse instructions",
    ", we also develop the implementations with the sse instructions rather than the avx instructions both for newton s force and arbitrarily - shaped force ( sse - accelerated implementation ) .",
    "first , we show the performance of our implementation for newton s force .",
    "the performance is measured by executing the direct particle - particle calculation of the plummer model with the number of particles from 0.5k to 32k .",
    "the left panel of figure  [ fig : newton_pfm ] depicts the performances of the avx- and sse - accelerated implementations . for comparison",
    ", we also show the performance of an implementation without any explicit use of simd instructions ( labeled as `` w / o simd '' in the left panel of figure  [ fig : newton_pfm ] ) .",
    "the numbers of interactions per second are @xmath5 in the case of the avx - accelerated implementation with a single thread , which corresponds to @xmath6  gflops , where the number of floating - point operations for the computation of force and potential for one pair of particles is counted to be @xmath7 .",
    "the performances of the sse- and avx - accelerated implementations with a single thread are higher than those without simd instructions by @xmath118 and @xmath8 times , respectively , and higher than those expected from the degree of concurrency of the sse and avx instructions for single - precision floating - point number ( 4 and 8 , respectively ) .",
    "this is because a very fast instruction of approximate inverse - square - root is not used in the `` w / o simd '' implementation . on the other hand ,",
    "the performance with the avx - accelerated implementation is higher than that of the sse - accelerated implementation roughly by a factor of two as expected .",
    "furthermore , in the left panel of figure  [ fig : newton_pfm ] , we show the performance of a gpu - accelerated @xmath0-body code based on the direct particle - particle method implemented using the cuda language , where the gpu board is nvidia geforce gtx 580 connected through the pci - express gen2 x16 link .",
    "the gpu - accelerated @xmath0-body code computes the forces and potentials of the particles using gpus , and integrate the equations of the motion of the particles on a cpu .",
    "thus , the communication of the particle data between the main memory of the host machine and the device memory on the gpu boards is required , and can hamper the total efficiency of the code .",
    "of course , if all the calculations are performed on gpus , we might not suffer from such overhead .",
    "however , the performance of such implementation can not be fairly compared with those of the avx- and sse - accelerated implementations , because the communication of the particle data is inevitable when we perform @xmath0-body simulations with multiple gpus or with multiple nodes equipped with gpus , regardless of the @xmath0-body solvers such as tree and treepm methods .",
    "the performances of the avx- and sse - accelerated implementations are almost independent of the total number of particles , @xmath0 . on the other hand ,",
    "the performance of the gpu - accelerated implementation strongly depends on the number of particles @xmath0 , due to the non - negligible overhead caused by the particle data communication . for @xmath119k ,",
    "the performance of the gpu - accelerated implementation is only 5% of that for @xmath120k .",
    "thus , for small @xmath0 ( @xmath121k and @xmath107k ) , the performance of the avx - accelerated implementation with four threads is higher than that with gpu - accelerated implementation , although , for large @xmath0 ( @xmath122@xmath123 ) , the performance of the gpu - accelerated implementation is higher than that of the avx - accelerated implementation .",
    "these features can be explained by the communication overhead in the gpu - accelerated implementation .",
    "so far , we see the performance of our code in the case that both the numbers of @xmath25- and @xmath21-particles ( @xmath124 and @xmath125 , respectively ) are the same and equal to @xmath0 .",
    "however , in actual computations of forces in collisionless @xmath0-body simulations based on various @xmath0-body solvers such as pppm , tree , and treepm methods , the numbers of @xmath25- and @xmath21-particles @xmath124 and @xmath125 are much smaller than the total number of particles @xmath0 . in the tree method modified for the effective force with external hardwares or softwares as described in @xcite , for example , @xmath124 is the number of particles , for which a tree traverse is performed simultaneously and the resultant interaction list ( size @xmath125 ) is shared , and typically around @xmath118@xmath126 . furthermore , if one adopts the individual timestep algorithm , the number of @xmath25-particles @xmath124 gets even smaller .",
    "the number of @xmath21-particles @xmath125 is also decreased in tree and treepm methods .",
    "therefore , we show the performance for typical @xmath124 and @xmath125 in the realistic situations of typical collisionless @xmath0-body simulations .",
    "the right panel of figure  [ fig : newton_pfm ] shows the performance of the avx - accelerated implementation using four threads with four processor cores ( black lines ) and that of the gpu - accelerated one ( red lines ) for various set of @xmath124 and @xmath125 .",
    "it can be seen that the obtained performance gets lower for the smaller @xmath124 and @xmath125 , regardless of the implementations . for the avx- and sse - accelerated implementations",
    ", this feature is due to the overhead of storing the particle data into the structures ` ipdata ` and ` jpdata ` shown in list  [ list : structures ] .",
    "the amount of the overhead of storing @xmath25- and @xmath21-particles are proportional to @xmath124 and @xmath125 , respectively , and the computational cost is proportional to @xmath127 . keeping this in mind the low performance with @xmath128 compared with those with @xmath129 can be ascribed to the overhead of storing @xmath21-particles to the structure ` jpdata ` . for the gpu - accelerated implementation",
    ", the overhead originates from the transfer of the particle data to the memory on gpus .",
    "it can be seen that the performance of the avx - accelerated implementation has rather mild dependence on @xmath124 and @xmath125 , while that of the gpu - accelerated one relatively strongly depends on @xmath125",
    ". such difference reflects the fact that the bandwidths and latency of the communication between gpus and cpus are rather poor compared with those of memory access between cpus and main memory .",
    "thus , the performance of the gpu - accelerated implementation is apparently superior to the avx - accelerated one only when both of @xmath124 and @xmath125 are sufficiently large ( say , @xmath130k and @xmath131k ) .        at the end of this section",
    ", we apply our avx - accelerated implementation to barnes - hut tree method @xcite , and measure its performance .",
    "our tree code is based on the pp part of treepm code implemented by @xcite and @xcite , in which they accelerated the calculations of the gravitational forces of the @xmath72-profile using grape-5 and grape-6a systems under the periodic boundary condition .",
    "we modify the tree code such that it can compute the newton s force under the vacuum boundary condition .",
    "since both of grape-6a systems and phantom - grape library support the same apis , we can easily utilize the capability of phantom - grape by simply exchanging the software library .    using the tree code described above",
    ", we calculate gravitational forces and potentials of all the particles in a plummer model and a king model with the dimensionless central potential depth @xmath132 .",
    "we measure the performance on an intel core i72600 processor .",
    "for the comparison with other codes , we also measure the performance of the same code but without any explicit use of simd instructions , and the publicly available code bonsai @xcite , which is a gpu - accelerated @xmath0-body code based on the tree method .",
    "the performance of the bonsai code is measured on a system with nvidia geforce gtx 580 .",
    "since the bonsai code utilizes the quadrupole moments of the particle distribution in each tree node as well as the monopole moments in the force calculations , for a fair comparison of the performance with the bonsai code , we give our tree code a capability to use the quadrupole moments in each tree node , although the original code uses only the monopole moments .",
    "we represent these multipole moments as pseudo - particles , using pseudo - particle multipole method @xcite .",
    "figure  [ fig : treenw ] shows the wall clock time to compute gravitational forces and potentials for each tree code .",
    "we show the both results with the code which uses the quadrupole moments ( lower panels ) and the one which uses only the monopole moments ( upper panels ) .",
    "note that the wall clock time includes the time for tree construction , tree traverse and calculations of forces and potentials but we exclude the time to integrate orbital motion of particles .",
    "as expected , the wall clock time with the avx - accelerated implementation is roughly 10 times shorter than those without any explicit use of simd instructions , owing to parallelism to calculate forces and potentials .",
    "the wall clock time with the avx - accelerated implementation is about only three times longer than those with bonsai , despite that theoretical peak performance of intel core i72600 ( @xmath133  gflops ) is lower than that of nvidia geforce gtx 580 ( @xmath134  gflops ) by a factor of 7.3 in single - precision .",
    "we expect that the performance of our avx - accelerated implementation could be close to that of the bonsai in the following situations .",
    "when we adopt individual timestep algorithm , the number of @xmath25-particles is effectively decreased , and a part of gpu cores becomes inactive .",
    "thus , the performance of gpu - accelerated implementation would be degraded more rapidly than that of our avx - accelerated implementation .",
    "furthermore , when we use gpu - accelerated implementation on massively parallel environments , the communication between cpus and gpus is inevitable , which also degrades the performance of gpu - accelerated implementation .",
    "the left panel of figure  [ fig : s2_pfm ] shows the performance of our implementation to calculate forces with an arbitrary force shape accelerated with the avx and sse instructions . for the comparison",
    ", we also plot the performance of an implementation without any explicit use of the simd instructions .",
    "the numbers of exponent and fraction bits used to referring the look - up table are set to @xmath70 and @xmath135 , respectively .",
    "the performance of the avx - accelerated implementation with a single thread is @xmath9 and @xmath14 times higher than that of the sse - accelerated one and the one without any simd instructions , respectively .",
    "these forces with the use of the avx instructions are lower than those expected from the degree of concurrency of their simd operations , @xmath136 , mainly because the reference of a look - up table is not carried out in a simd manner .",
    "the performance with multi - thread parallelization is almost proportional to the number of threads up to four threads . if the htt is activated , the performance with eight threads is higher than that with four threads by a few percent .",
    "the right panel of figure  [ fig : s2_pfm ] shows the performance of the avx - accelerated implementation with eight threads for a various set of @xmath124 and @xmath125 . for @xmath129 ,",
    "the performance is almost independent of @xmath124 and @xmath125 , and for @xmath137 it is about half the performance with @xmath129 .",
    "this is again due to the overhead of copying @xmath21-particle data to the structure ` jpdata ` , as is the case in the calculation of newton s force . such weak dependence of the performance on @xmath124 and @xmath125 are also preferable for the calculations of the forces in the pppm and treepm methods especially with the individual timestep scheme .",
    "using the avx instructions , the new simd instructions of x86 processors , we develop a numerical library to accelerate the calculations of newton s forces and arbitrarily shaped forces for @xmath0-body simulations .",
    "we implement the library by means of inline - assembly embedded in c - language with gcc extensions , which enables us to manually control the assignment of the ymm registers to computational data , and extract the full capability of a cpu core . in computing arbitrarily shaped forces",
    ", we refer to a look - up table , which is constructed with a novel scheme so that the binning is optimized to ensure good numerical accuracy of the computed forces while its size is kept small enough to avoid cache misses .",
    "the performance of the version for newton s forces reaches @xmath138 interactions per second with a single thread , which is about @xmath9 times and @xmath8 times higher than those of the implementation with the sse instructions and without any explicit use of simd instructions , respectively .",
    "the use of the fast inverse - square - root instruction is a key ingredient of the improvement of the performance in the implementation with the sse and avx instructions .",
    "the performance of the version for arbitrarily shaped forces is @xmath9 and @xmath14 times higher than those implemented with the sse instructions and without any explicit use of the simd instructions .",
    "furthermore , our implementation supports the thread parallelization on a multi - core processor with the openmp programming interface , and has a good scalability regardless of the number of particles .    while the performance of our implementation using the avx instructions",
    "is moderate compared with that of the gpu - accelerated implementation , the most remarkable advantage of our implementation is the fact that the performance has much weaker dependence on the numbers of @xmath25- and @xmath21-particles than that of the gpu - accelerated implementation .",
    "this feature is also the case for the calculation of the arbitrarily shaped force , and can be explained by the relatively large overhead of the data transfer between gpus and main memory of their host computers . in actual calculations of forces with popular @xmath0-body solvers such as the tree - method and the treepm - method combined with the individual timestep scheme , the numbers of @xmath25- and @xmath21-particles can not be always large enough to extract the full capability of gpus . in that sense ,",
    "our implementation is more suitable in accelerating the calculations of forces using the tree- and treepm - methods .",
    "another advantage of our implementation is its portability . with this library",
    ", we can carry out collisionless @xmath0-body simulations with a good performance even on supercomputer systems without any gpu - based accelerators .",
    "note that massively parallel systems with gpu - based accelerators , at least currently , are not ubiquitous . even on processors other than the x86 architecture ,",
    "most of them supports similar simd instruction sets ( e.g. vector multimedia extension on ibm power series , and hpc - ace on sparc64 viiifx , etc . )",
    "our library can be ported to these processors with some acceptable efforts .",
    "finally let us mention the possible future improvement of our implementation .",
    "fused multiply - add ( fma ) instructions which have already been implemented in the `` bulldozer '' cpu family by amd corporation , and is scheduled to be introduced in the `` haswell '' processor by intel corporation in 2013 .",
    "the use of the fma instructions will improve the performance and accuracy of the calculations of forces to some extent .",
    "the numerical library `` phantom - grape '' developed in this work is publicly available at http://code.google.com / p / phantom - grape/.",
    "we thank dr .",
    "takayuki saitoh for valuable comments on this work .",
    "a. tanikawa thanks yohei miki and go ogiya for fruitful discussion on gpu .",
    "numerical simulations have been performed with computational facilities at the center for computational sciences in university of tsukuba .",
    "this work was supported by scientific research for challenging exploratory research ( 21654026 ) , grant - in - aid for young scientists ( start - up : 21840015 ) , the first project based on the grants - in - aid for specially promoted research by mext ( 16002003 ) , and grant - in - aid for scientific research ( s ) by jsps ( 20224002 ) .",
    "k. nitadori and t. okamoto acknowledge financial support by mext hpci strategic program .",
    "barnes , j . , & hut , p .",
    "1986 , nature , 324 , 446 bdorf , j . , gaburov , e . ,",
    "& portegies zwart , s .",
    "2012 , journal of computational physics , 231 , 2825 brieu , p.p . , summers , f.j . , ostriker , j.p .",
    "1995 , apj , 453 , 566 fukushige , t. , makino , j. , kawai , a. 2005 , pasj , 57 , 1009 gaburov , e. , harfst , s. , & portegies zwart , s. 2009 , newa , 14 , 630 hamada , t. , iitaka , t. 2007 , submitted , arxiv : astro - ph/0703100 harfst , s. , gualandris , a. , merritt , d. , spurzem , r. , portegies zwart , s. , & berczik , p. 2007",
    ", new astronomy , 12 , 357 hockney , r.w . ,",
    "eastwood , j.w .",
    "1981 , computer simulation using particles ( new york : mcgraw - hill ) .",
    "ishiyama , t. , fukushige , t. , & makino , j. 2008 , pasj , 60 , 13 ishiyama , t. , fukushige , t. , & makino , j. 2009 , apj , 696 , 2115 ishiyama , t. , fukushige , t. , & makino , j. 2009 , pasj , 61 , 1319 ishiyama , t. , makino , j. , & ebisuzaki , t. 2010 , apj , 723 , 195 ishiyama , t. , makino , j. , portegies zwart , s. , groen , d. , nitadori , k. , rieder , s. , de laat , c. , mcmillan , s. , hiraki , k. , & harfst , s. 2011 , submitted , arxiv1101.2020 johnson , v. , & aarseth , s. 2006 , aspc , 351 , 165 kawai , a. , fukushige , t. , makino , j. , & taiji , m. 2000 , pasj , 52 , 659 kawai , a. , & makino 2001 , apjl , 550 , 143 kawai , a. , makino , j. , ebisuzaki , t. 2004 , apjs , 151 , 13 .",
    "makino , j. 1991 , pasj , 43 , 621 .",
    "makino , j. , & aarseth , s. 1992 , pasj , 44 , 141 makino , j. , fukushige , t. , koga , m. , namura , k. 2003 , pasj , 55 , 1163 nitadori , k. , makino , j. , & hut , p. 2006 , newa , 12 , 169 oshino , s. , funato , y. , & makino , j. 2011 , pasj , 63 , 881 portegies zwart , s. , belleman , r. & geldof , p. 2007",
    ", new astronomy , 12 , 641 portegies zwart , s. , mcmillan , s. , groen , d. , gualandris , a. , sipior , m. , & vermin , w. 2008 , new astronomy , 13 , 285 saitoh , t. r. , daisaka , h. , kokubo , e. , makino , j. , okamoto , t. , tomisaka , k. , wada , k. & yoshida , n. 2008 , pasj , 60 , 667 saitoh , t. r. , daisaka , h. , kokubo , e. , makino , j. , okamoto , t. , tomisaka , k. , wada , k. , & yoshida , n. 2009 , pasj , 61 , 481 sugimoto , d. , chikada , y. , makino , j. , ito , t. , ebisuzaki , t. , umemura , m. 1990 .",
    "nature 345 , 33 .",
    "tanikawa , a. , & fukushige , t. 2009 , pasj , 61 , 721 tanikawa , a. , yoshikawa , k. , okamoto , t. , & nitadori , k. 2012 , new astronomy , 17 , 82 ( paper i ) xu , g. , 1995 .",
    "apjs , 98 355 yoshikawa , k. , fukushige , t. 2005 .",
    "pasj 57 , 849 ."
  ],
  "abstract_text": [
    "<S> we have developed a numerical software library for collisionless @xmath0-body simulations named `` phantom - grape '' which highly accelerates force calculations among particles by use of a new simd instruction set extension to the x86 architecture , advanced vector extensions ( avx ) , an enhanced version of the streaming simd extensions ( sse ) . in our library , not only the newton s forces , but also central forces with an arbitrary shape @xmath1 , which has a finite cutoff radius @xmath2 ( i.e. @xmath3 at @xmath4 ) , can be quickly computed . in computing such central forces with an arbitrary force shape @xmath1 , we refer to a pre - calculated look - up table . </S>",
    "<S> we also present a new scheme to create the look - up table whose binning is optimal to keep good accuracy in computing forces and whose size is small enough to avoid cache misses . using an intel core i72600 processor , we measure the performance of our library for both of the newton s forces and the arbitrarily shaped central forces . in the case of newton s forces , we achieve @xmath5 interactions per second with one processor core ( or @xmath6  gflops if we count @xmath7 operations per interaction ) , which is @xmath8 times higher than the performance of an implementation without any explicit use of simd instructions , and @xmath9 times than that with the sse instructions . with four processor cores , </S>",
    "<S> we obtain the performance of @xmath10 interactions per second ( or @xmath11  gflops ) . in the case of the arbitrarily shaped central forces </S>",
    "<S> , we can calculate @xmath12 and @xmath13 interactions per second with one and four processor cores , respectively . </S>",
    "<S> the performance with one processor core is @xmath14 times and @xmath9 times higher than those of the implementations without any use of simd instructions and with the sse instructions . </S>",
    "<S> these performances depend only weakly on the number of particles , irrespective of the force shape . </S>",
    "<S> it is good contrast with the fact that the performance of force calculations accelerated by graphics processing units ( gpus ) depends strongly on the number of particles . </S>",
    "<S> substantially weak dependence of the performance on the number of particles is suitable to collisionless @xmath0-body simulations , since these simulations are usually performed with sophisticated @xmath0-body solvers such as tree- and treepm - methods combined with an individual timestep scheme . </S>",
    "<S> we conclude that collisionless @xmath0-body simulations accelerated with our library have significant advantage over those accelerated by gpus , especially on massively parallel environments .    ,    ,    &    stellar dynamics , method : @xmath0-body simulations </S>"
  ]
}