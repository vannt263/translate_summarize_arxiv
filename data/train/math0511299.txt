{
  "article_text": [
    "we give here notations and introduce the inductive and transductive settings .",
    "let @xmath0 be a measure space and let @xmath1 denote the borel @xmath2-algebra on @xmath3 .      in the inductive setting , we assume that @xmath4 is a distribution on pairs @xmath5 taking values in @xmath6 , that @xmath4 is such that : @xmath7 and that we observe @xmath8 independent pairs @xmath9 for @xmath10 .",
    "our objective is then to estimate the regression function on the basis of the observations .",
    "we denote : @xmath11      in the transductive case , we will assume that , for a given integer @xmath12 , @xmath13 is some exchangeable probability measure on the space @xmath14",
    ". we will write @xmath15 a random vector distributed according to @xmath13 .",
    "[ exchp ] for any integer @xmath16 , let @xmath17 denote the set of all permutations of @xmath18 .",
    "we say that @xmath13 is exchangeable if for any @xmath19 we have : @xmath20 has the same distribution under @xmath13 that @xmath21 .",
    "we assume that we observe @xmath22 and @xmath23 ; and the observation @xmath21 is usually called the training sample , while the other part of the vector , @xmath24 is called the test sample . in this case , we only focus on the estimation of the values @xmath25 .",
    "this is why vapnik @xcite called this kind of inference `` transductive inference '' when he introduced it .",
    "note that in this setting , the pairs @xmath26 are not necessarily independent , but are identically distributed .",
    "we will let @xmath4 denote their marginal distribution , and we can here again define the regression function @xmath27 .",
    "actually , most statistical problems being usually formulated in the inductive setting , the reader may wonder about the pertinence of the study of the transductive setting .",
    "let us think of the following examples : in quality control , or in a sample survey , we try to infer informations about a whole population from observations on a small sample . in this cases",
    ", transductive inference seems actually more adapted than inductive inference , with @xmath8 the size of the sample and @xmath28 the size of the population .",
    "one can see that the use of inductive results in this context is only motivated by the large values of @xmath29 ( the inductive case is the limit case of the transductive case where @xmath30 ) . in the problems connected with regression estimation or classification , we can imagine a case where a lot of images are collected for example on the internet .",
    "the time to label every picture according to the fact that it represents , or not , a given object being too long , one can think of labeling only @xmath31 over @xmath32 images , and to use then a transductive algorithm to label automatically the other data .",
    "we hope that these examples can convince the reader that the use of the transductive setting is not unrealistic .",
    "however , the reader that is not convinced should remember that the transductive inference was first introduced by vapnik mainly as a tool to study the inductive case : there are techniques to get rid of the second part of the sample by taking an expectation with respect to it and obtain results valid in the inductive setting ( see for example a result by panchenko used in this paper , @xcite ) .      in both settings",
    ", we are going to use the same model to estimate the regression function : @xmath33 .",
    "the only thing we assume about @xmath33 is that it is a vector space of functions .",
    "note in particular that we do not assume that @xmath27 belongs to @xmath33 .      in both settings , we give a pac inequality on the risk of estimators in one - dimensional models of the form : @xmath34 for a given @xmath35 .",
    "this result motivates an algorithm that performs iterative feature selection in order to perform regression estimation .",
    "we will then remark that the selection procedure gives the guarantee that every selected feature actually improves the current estimator .    in the inductive setting",
    "( section  [ inductive ] ) , it means that we estimate @xmath36 by a function @xmath37 , but the selection procedure can only be performed if the statistician knows the marginal distribution @xmath38 of @xmath39 under @xmath4 .    in the transductive case ( section  [ sectiontrans ] )",
    ", the estimation of @xmath40 can be performed by the procedure without any prior knowledge about the marginal distribution of @xmath39 under @xmath4 .",
    "we first focus on the case @xmath41 , and then on the general case @xmath42 .    finally , in section  [ rate ] , we use the main result of the paper ( the fact that every selected feature improves the performance of the estimator ) as an oracle inequality , to compute the rate of convergence of the estimator in sobolev and besov spaces .    the last section ( section  [ proofs ] ) is dedicated to the proofs .",
    "the literature on iterative methods for regression estimation is very important , let us mention one of the first algorithm , adaline , by widrow and hoff @xcite , or more recent versions like boosting , see @xcite and the references within .",
    "the technique developed here has some similarity with the so - called greedy algorithms , see @xcite ( and the references within ) for a survey and some recent results .",
    "however , note that in this techniques , the iterative update of the estimator is motivated by algorithmic issues , and is not motivated statistically .",
    "in particular , adaline has no guarantee against overfitting if the number of variables @xmath43 is large ( say @xmath44 ) . for greedy algorithms ,",
    "on has to specify a particular penalization if one wants to get a guarantee against overfitting .",
    "the same remark can be done about boosting algorithm . here , the algorithm is motivated by a statistical result , and as a consequence has theoretical guarantees against overlearning .",
    "it stays however computationally feasible , some pseudo - code is given in the paper .",
    "closer to our technique are the methods of aggregation of statistical estimators , see @xcite and @xcite and more recently the mirror descent algorithm studied in @xcite or @xcite .",
    "in this papers , oracle inequalities are given ensuring that the estimator performs as well as the best ( linear or convex ) aggregation of functions in a given family , up to an optimal term .",
    "note that these inequalities are given in expectation , here almost all results are given in a deviation bound ( or pac bound , a bound that is true with high probability , from which we derive a bound in expectation in section  [ rate ] ) .",
    "similar bounds where given for the pac - bayesian model aggregation developed by catoni @xcite , yang @xcite and audibert  @xcite . in some way , the algorithm proposed in this paper can be seen as a practical way to implement these results .",
    "note that nearly all the methods in the papers mentioned previously where designed especially for the inductive setting .",
    "very few algorithms were created specifically for the transductive regression problem .",
    "the algorithm described in this paper seems more adapted to the transductive setting ( remember that the procedure can be performed in the inductive setting only if the statistician knows the marginal distribution of @xmath39 under @xmath4 , while there is no such assumption in the transductive context ) .",
    "let us however start with a presentation of our method in the inductive context .",
    "we put : @xmath45 , } \\\\",
    "\\lefteqn{r(\\theta )   = \\frac{1}{n}\\sum_{i=1}^{n } \\bigl(y_{i}-\\theta(x_{i } ) \\bigr)^{2 } , } \\end{aligned}\\ ] ] and in this setting , our objective is @xmath46 given by : @xmath47      we suppose that we have an integer @xmath48 and that we are given a finite family of functions : @xmath49    let us put , for any @xmath50 : @xmath51}{p [ \\theta_{k}(x)^{2 } ] } , } \\\\",
    "\\lefteqn{\\hat{\\alpha}_{k }   = \\mathop{\\arg\\min}_{\\alpha\\in{\\mathbb{r } } } r(\\alpha\\theta_{k } ) = \\frac{{(1/n)}\\sum_{i=1}^{n } \\theta_{k}(x_{i})y_{i}}{(1/n)\\sum_{i=1}^{n } \\theta_{k}(x_{i})^{2 } } , } \\\\",
    "\\lefteqn{\\mathcal{c}_{k }   = \\frac{(1/n)\\sum_{i=1}^{n } \\theta_{k}(x_{i})^{2 } } { p [ \\theta_{k}(x)^{2 } ] } .}\\end{aligned}\\ ] ]    [ lastth ] moreover , let us assume that @xmath4 is such that @xmath52 is bounded by a constant @xmath53 , and such that : @xmath54^{2 } \\bigr\\ } \\leq\\sigma^{2 } < + \\infty.\\ ] ] we have , for any @xmath55 , with @xmath56-probability at least @xmath57 , for any @xmath58 : @xmath59}{n } \\biggl[\\frac { ( 1/n)\\sum_{i=1}^{n}\\theta_{k}(x_{i})^{2}y_{i}^{2 } } { p[\\theta_{k}(x)^{2 } ] } + b^{2 } + \\sigma^{2 } \\biggr].\\ ] ]    the proof of this theorem is given in section  [ subprooflastth ] .",
    "let us put , for any @xmath60 : @xmath61}.\\ ] ] let also @xmath62 denote the norm associated with this distance , @xmath63 , and @xmath64 the associated scalar product : @xmath65 .\\ ] ]    because @xmath66 we have : @xmath67    so the theorem can be written : @xmath68 where @xmath69 is the right - hand side of inequality  ( [ lasteq ] ) .    now , note that @xmath70 is the orthogonal projection of : @xmath71 onto the space @xmath72 , with respect to the inner product @xmath64 : @xmath73    we define , for any @xmath29 and @xmath74 : @xmath75    then the theorem is equivalent to the following corollary .",
    "we have : @xmath76 \\geq1-\\varepsilon.\\ ] ]    in other words : @xmath77 is a confidence region at level @xmath74 for @xmath78 .",
    "we write @xmath79 the orthogonal projection into @xmath80 with respect to the distance @xmath81 .",
    "note that this orthogonal projection is not a projection on a linear subspace of @xmath33 , and so it is not a linear mapping .",
    "the previous corollaries of theorem [ lastth ] motivate the following iterative algorithm :    * choose @xmath82 , for example , @xmath83 ; * at step @xmath84 , we have : @xmath85 .",
    "choose @xmath86 ( this choice can of course be data dependent ) , and take : @xmath87 * we can use the following stopping rule : @xmath88 , where @xmath89 .",
    "[ hatf ] let @xmath90 denote the stopping step , and : @xmath91 the corresponding function .",
    "[ propalgo ] we have : @xmath92 \\geq1-\\varepsilon.\\ ] ]    this is just a consequence of the preceding corollary .",
    "let us assume that : @xmath93 let us choose @xmath94 .",
    "we have , for a @xmath58 : @xmath95 where @xmath96 is the projection into a convex set that contains @xmath46 .",
    "this implies that : @xmath97 or : @xmath98 which can be written : @xmath99-r(\\overline{\\theta } ) \\geq r \\bigl[\\theta^{(n ) } \\bigr]-r(\\overline{\\theta } ) + d_{p}^{2 } \\bigl(\\theta^{(n-1)},\\theta^{(n ) } \\bigr).\\ ] ]    actually , the main point in the motivation of the algorithm is that , with probability at least @xmath57 , whatever the current value @xmath100 , whatever the feature @xmath58 ( even chosen on the basis of the data ) , @xmath101 is a better estimator than @xmath102 .",
    "so we can choose @xmath103 as we want in the algorithm .",
    "for example , theorem [ propalgo ] motivates the choice : @xmath104 this version of the algorithm is detailed in fig .  1 .",
    "if looking for the exact maximum of @xmath105 with respect to @xmath29 is too computationally intensive we can use any heuristic to choose @xmath103 , or even skip this maximization and take : @xmath106    p11.0 cm we have @xmath55 , @xmath107 , @xmath8 observations @xmath108 , @xmath43 features @xmath109 and @xmath110 .",
    "compute at first every @xmath111 and @xmath69 for @xmath58 .",
    "set @xmath112 .",
    "repeat :    * set @xmath113 ; * set _ best____improvement__@xmath114 ; * for @xmath58 , compute : @xmath115 , } \\\\",
    "\\lefteqn{\\gamma_{k } \\leftarrow\\hat{\\alpha}_{k } - \\frac{1}{v_{k}}\\sum_{j=1}^{m}c_{j}p \\bigl[\\theta_{j}(x)\\theta_{k}(x ) \\bigr ] , } \\\\",
    "\\lefteqn{\\delta_{k } \\leftarrow v_{k } \\bigl ( |\\gamma_{k } | - \\beta(\\varepsilon , k ) \\bigr)_{+}^{2},}\\end{aligned}\\ ] ] and if @xmath116 , set : @xmath117 * if _ best____improvement__@xmath118 set : @xmath119    until _ best____improvement__@xmath120 ( where @xmath121 if @xmath122 and @xmath31 otherwise ) .",
    "note that at each step @xmath123 , @xmath102 is given by : @xmath124 so after the last step we can return the estimator : @xmath125 +    let us assume that @xmath126 $ ] and let us put @xmath127 .",
    "let @xmath128 be an orthonormal basis of  @xmath33 .",
    "the choice of @xmath43 should not be a problem , the algorithm itself avoiding itself overlearning we can take a large value of @xmath43 like @xmath44 . in this setting , the algorithm is a procedure for ( soft ) thresholding of coefficients . in the particular case of a wavelets basis , see @xcite or @xcite for a presentation of wavelets coefficient thresholding . here",
    ", the threshold is not necessarily the same for every coefficient .",
    "we can remark that the sequential projection on every @xmath29 is sufficient here : @xmath129 after that @xmath130 for every @xmath131 ( because all the directions of the different projections are orthogonals ) .",
    "actually , it is possible to prove that the estimator is able to adapt itself to the regularity of the function to achieve a good mean rate of convergence . more precisely ,",
    "if we assume that the true regression function has an ( unknown ) regularity @xmath132 , then it is possible to choose @xmath43 and @xmath74 in such a way that the rate of convergence is : @xmath133 we prove this point in section  [ rate ] .",
    "note that in its general form , the algorithm does not require any assumption about the dictionary of functions @xmath134 .",
    "this family can be non - orthogonal , it can even be redundant ( the dimension of the vector space generated by @xmath135 can be smaller than @xmath43 ) .",
    "it is possible to generalize theorem [ lastth ] to models of dimension larger than @xmath31 .",
    "the algorithm itself can take advantage of these generalizations .",
    "this point is developed in @xcite , where some experiences about the performances of our algorithm can also be found .",
    "note that an improvement of the inequality in theorem [ lastth ] ( inequality  ( [ lasteq ] ) ) would allow to apply the same method , but would lead to smaller confidence regions and so to better performances .",
    "the end of this section is dedicated to improvements ( and generalizations ) of this bound .    until the end of section  [ inductive ] , we assume that @xmath33 and @xmath4 are such that : @xmath136 < + \\infty.\\ ] ]    for any random variable @xmath137 we put : @xmath138 , } \\\\",
    "\\lefteqn{m^{3}(t )   = p \\bigl [ ( t - pt ) ^{3 } \\bigr],}\\end{aligned}\\ ] ] and we define , for any @xmath139 , @xmath140 by : @xmath141 } .\\ ] ] for any random variables @xmath142 and any @xmath139 we put : @xmath143 , } \\\\",
    "\\lefteqn{m^{3 } _ { \\gamma t}\\bigl(t'\\bigr )   = p _ { \\gamma t } \\bigl [ \\bigl(t'-p _ { \\gamma t}t ' \\bigr)^{3 } \\bigr].}\\end{aligned}\\ ] ]    section  [ sub1 ] gives an improvement of theorem [ lastth ] while section  [ subsvm ] extends it to the case of a data - dependant family @xmath135 .",
    "[ th1 ] let us put : @xmath144 then we have , for any @xmath55 , with @xmath56-probability at least @xmath57 , for any @xmath58 : @xmath145 } + \\frac{\\log^{3 } ( 2m/\\varepsilon)}{n^{{3}/{2 } } } c_{n}(p , m,\\varepsilon,\\theta_{k } ) , \\ ] ] where we have : @xmath146 } \\\\ & & { } + i_{\\theta_{k } } \\biggl(\\sqrt{\\frac{2\\log ( { 2m}/{\\varepsilon})}{n v ( w_{\\theta_{k } } ) } } \\biggr)^{4 } \\frac{\\log^{2 } ( { 2m}/{\\varepsilon } ) } { \\sqrt{n}v ( w_{\\theta_{k } } ) ^{6}p [ \\theta_{k}(x)^{2 } ] } , \\end{aligned}\\ ] ] with : @xmath147    for the proof , see section  [ subproof1 ] .",
    "actually , the method we proposed requires to be able to compute explicitly the upper bound in this theorem .",
    "remark that , with @xmath74 and @xmath43 fixed : @xmath148^{2 } } { 9 v ( w_{\\theta_{k } } ) ^{{5}/{2}}p [ \\theta_{k}(x)^{2 } ] } , \\ ] ] and so we can choose to consider only the first - order term .",
    "another possible choice is to make stronger assumptions on @xmath4 and @xmath135 that allow to upper bound explicitly @xmath149 .",
    "for example , if we assume that @xmath150 is bounded by @xmath151 and that @xmath152 is bounded by @xmath153 then @xmath154 is bounded by @xmath155 and we have ( basically ) : @xmath156 } + \\frac{4096 c_{k}^{4 } \\log^{3 } ( { 2m}/{\\varepsilon})}{81 \\sqrt{n}v ( w_{\\theta_{k } } ) ^{6 } p [ \\theta_{k}(x)^{2 } ] } .\\ ] ]    the main problem is actually that the first - order term contains the quantity @xmath157 that is not observable , and we would like to be able to replace this quantity by its natural estimator : @xmath158^{2 } .\\ ] ]    the following theorem justifies this method",
    ".    [ th1bis ] if we assume that there is a constant @xmath159 such that : @xmath160<\\infty,\\ ] ] we have , for any @xmath55 , with @xmath56-probability at least @xmath57 , for any @xmath58 : @xmath161 } + \\frac{\\log ( { 4m}/{\\varepsilon})}{n^{{3}/{2 } } } c_{n}'(p , m,\\varepsilon,\\theta_{k } ) , \\ ] ] where we have : @xmath162^{2},\\ ] ] and @xmath163 } \\biggl[\\sqrt{2v \\bigl(w_{\\theta_{k}}^{2 } \\bigr ) } + \\frac{\\log ( 2m/\\varepsilon)}{\\sqrt{n } v ( w_{\\theta_{k}}^{2 } ) } j_{\\theta_{k } } \\biggl(\\sqrt{\\frac{2\\log(2m/\\varepsilon)}{n v ( w_{\\theta_{k}}^{2 } ) } } \\biggr ) \\biggr ] \\\\ & & { } + \\frac{2\\log^{{1}/{2}}({4m}/{\\varepsilon})}{p [ \\theta_{k}(x)^{2 } ] } \\biggl [ \\sqrt{2v ( w_{\\theta_{k } } ) } + \\frac{\\log^{2}({2m}/{\\varepsilon})}{\\sqrt{n } v ( w_{\\theta_{k } } ) ^{3 } } i_{\\theta_{k } } \\biggl(\\sqrt{\\frac{2\\log({4m}/{\\varepsilon})}{n v ( w_{\\theta_{k } } ) } } \\biggr ) \\biggr ] \\\\ & & { } \\times \\biggl [ \\bigg|\\frac{2}{n}\\sum_{i=1}^{n}y_{i } \\theta_{k}(x_{i } ) \\bigg| \\sqrt{\\frac{2v ( w_{\\theta_{k } } ) \\log ( 4m/\\varepsilon)}{n } } + \\frac{\\log^{{5}/{2 } } ( { 2m}/{\\varepsilon})}{n v ( w_{\\theta_{k } } ) ^{3 } } i_{\\theta_{k } } \\biggl(\\sqrt{\\frac{2\\log ( { 4m}/{\\varepsilon})}{n v ( w_{\\theta_{k } } ) } } \\biggr ) \\biggr]\\end{aligned}\\ ] ] and @xmath164    the proof is given in section  [ subproof1 ] .      thanks to a method due to seeger @xcite , it is possible to extend this method to the case where the set @xmath135 is data dependent in the following way : @xmath165 where for any @xmath166 , the cardinality of the set @xmath167 depends only on @xmath8 , not on @xmath168 .",
    "we will write @xmath169 this cardinality .",
    "so we have : @xmath170 we put : @xmath171    in this case , we need some adaptations of our previous notations .",
    "we put , for @xmath10 : @xmath172 for any @xmath173 , we write : @xmath174}{p [ \\theta_{i , k}(x)^{2 } ] } , } \\\\",
    "\\lefteqn{\\mathcal{c}_{i , k }   = \\frac{{1}/{(n-1)}\\sum _ { j\\neq i } \\theta_{i , k}(x_{j})^{2 } } { p [ \\theta_{i , k}(x)^{2 } ] } .}\\end{aligned}\\ ] ]    [ thsvm ] we have , for any @xmath55 , with @xmath56-probability at least @xmath57 , for any @xmath175 and @xmath10 : @xmath176 } \\\\ & & { } + \\frac{\\log^{3 } ( { 2nm'(n)}/{\\varepsilon})}{(n-1)^{{3}/{2 } } } c_{n-1}\\bigl(p , nm'(n),\\varepsilon,\\theta_{i , k}\\bigr).\\end{aligned}\\ ] ]    the proof is given in section  [ subproof1 ] .",
    "we can use this theorem to build an estimator using the algorithm described in the previous subsection , with obvious changes in the notations .",
    "let us consider the case where @xmath177 is a hilbert space with scalar product @xmath178 , and : @xmath179 where @xmath180 is an application @xmath181 .",
    "let us put @xmath182= \\",
    "{ \\<\\varpsi(x),\\varpsi(\\cdot ) \\ > \\}$ ] . in this case",
    "we have @xmath183 and the estimator is of the from : @xmath184 let us define , @xmath185 the function @xmath186 is called the kernel , and : @xmath187 that is called the set of support vectors . then the estimate has the form of a support vector machine ( svm ) : @xmath188 svm where first introduced by boser , guyon and vapnik @xcite in the context of classification , and then generalized by vapnik @xcite to the context of regression estimation . for a general introduction to svm ,",
    "see also @xcite and @xcite .",
    "[ msvm ] a widely used kernel is the gaussian kernel : @xmath189 where @xmath190 is some distance over the space @xmath191 and @xmath192 .",
    "but in practice , the choice of the parameter @xmath193 is difficult . a way to solve",
    "this problem is to introduce multiscale svm .",
    "we simply take @xmath33 as the set of all bounded functions @xmath194 . now , let us put : @xmath195= \\bigl\\{k_{2}(x,\\cdot),k_{2^{2}}(x,\\cdot),\\ldots , k_{2^{m'(n)}}(x,\\cdot ) \\bigr\\}.\\ ] ] in this case , we obtain an estimator of the form : @xmath196 that could be called multiscale svm .",
    "remark that we can use this technique to define svm using simultaneously different kernels ( not necessarily the same kernel at different scales ) .",
    "let us recall that we assume that @xmath42 , that @xmath13 is some exchangeable probability measure ( let us recall that exchangeability is defined in definition [ exchp ] ) on the space @xmath197 .",
    "let @xmath15 denote a random vector distributed according to @xmath13 .",
    "let us remark that under this condition , the marginal distribution of every @xmath198 is the same , we will call @xmath4 this distribution . in the particular case where the observations are i.i.d . , we will have @xmath199 , but what follows still holds for general exchangeable distributions @xmath13 .",
    "we assume that we observe @xmath22 and @xmath23 . in this case , we only focus on the estimation of the values @xmath25 .",
    "we put , for any @xmath35 : @xmath200 our objective is : @xmath201 if the minimum of @xmath202 is not unique then we take for @xmath203 any element of @xmath33 reaching the minimum value of @xmath202 .",
    "let @xmath135 be a finite family of vectors belonging to @xmath33 , so that @xmath204 .",
    "actually , @xmath135 is allowed to be data - dependent : @xmath205 but we assume that the function @xmath206 is exchangeable with respect to its @xmath28 arguments , and is such that @xmath207 depends only on @xmath8 , not on @xmath208 .",
    "the problem of the indexation of the elements of @xmath135 is not straightforward and we must be very careful about it .",
    "let @xmath209 be a complete order on @xmath33 , and write : @xmath210 where @xmath211 remark that , in this case , every @xmath212 is an exchangeable function of @xmath208 .",
    "now , let us write , for any @xmath213 : @xmath214      in a first time we focus on the case where @xmath41 as a method due to catoni @xcite brings a substantial simplification of the bound in this case .",
    "[ thtrans ] we have , for any @xmath215 , with @xmath216-probability at least @xmath57 , for any @xmath217 : @xmath218-r_{2}\\bigl(\\alpha_{2}^{h}\\cdot \\theta_{h}\\bigr ) \\leq4 \\biggl [ \\frac{(1/n)\\sum_{i=1}^{2n}\\theta_{h}(x_{i})^{2}y_{i}^{2 } } { ( 1/n)\\sum_{i = n+1}^{2n}\\theta_{h}(x_{i})^{2 } } \\biggr ] \\frac{\\log ( { 2m}/{\\varepsilon})}{n } .\\ ] ]    here again , it is possible to make some hypothesis in order to make the right - hand side of the theorem observable .",
    "in particular , if we assume that : @xmath219 then we can get a looser observable upper bound : @xmath220-r_{2}\\bigl(\\alpha_{2}^{h}\\cdot\\theta_{h}\\bigr ) \\leq4 \\biggl [ b^{2}+ \\frac{({1}/{n})\\sum_{i=1}^{n}\\theta_{h}(x_{i})^{2}y_{i}^{2 } } { ( { 1}/{n})\\sum_{i = n+1}^{2n}\\theta_{h}(x_{i})^{2 } } \\biggr ] \\frac{\\log ( { 2m}/{\\varepsilon})}{n } \\biggr\\ } \\geq1-\\varepsilon.\\end{aligned}\\ ] ] if we do not want to make this assumption , we can use the following variant , that gives a first - order approximation for the bound .",
    "[ cortrans ] for any @xmath215 , with @xmath216-probability at least @xmath57 , for any @xmath217 : @xmath221-r_{2}\\bigl(\\alpha_{2}^{h}\\cdot\\theta_{h}\\bigr ) } \\\\",
    "\\lefteqn{\\quad \\leq\\frac{8 \\log ( { 4m}/{\\varepsilon})}{n } \\biggl [ \\frac{({1}/{n})\\sum_{i=1}^{n}\\theta_{h}(x_{i})^{2}y_{i}^{2 } } { ( { 1}/{n})\\sum_{i = n+1}^{2n}\\theta_{h}(x_{i})^{2 } } + \\sqrt{\\frac{({1}/{n})\\sum_{i=1}^{2n}\\theta_{h}(x_{i})^{4}y_{i}^{4}\\log ( { 2m}/{\\varepsilon})}{2n}}\\biggr].}\\end{aligned}\\ ] ]    let us assume that @xmath150 is such that we know two constants @xmath222 and @xmath223 such that : @xmath224 then we have , with probability at least @xmath57 : @xmath225 combining both inequalities leads by a union bound argument leads to : @xmath221-r_{2}\\bigl(\\alpha_{2}^{h}\\cdot\\theta_{h}\\bigr ) } \\\\",
    "\\lefteqn{\\quad\\leq\\frac{8 \\log ( { 8m}/{\\varepsilon})}{n } \\biggl [ \\frac{({1}/{n})\\sum_{i=1}^{n}\\theta_{h}(x_{i})^{2}y_{i}^{2 } } { ( { 1}/{n})\\sum_{i = n+1}^{2n}\\theta_{h}(x_{i})^{2 } } + \\sqrt{\\frac{({1}/{n})\\sum_{i=1}^{2n}\\theta_{h}(x_{i})^{4}\\log ( { 4m}/{\\varepsilon})\\log^{4 } ( { 4nb_{y}}/{\\varepsilon})}{2nb_{y}^{4 } } } \\biggr].}\\end{aligned}\\ ] ]    the proofs of both theorems are given in the proofs section , more precisely in section  [ subproof2 ] .",
    "let us compare the first - order term of this theorem to the analogous term in the inductive case ( theorems [ th1 ]  and  [ th1bis ] ) .",
    "the factor of the variance term is @xmath226 instead of @xmath227 in the inductive case .",
    "a factor @xmath227 is to be lost because we have here the variance of a sample of size @xmath228 instead of @xmath8 in the inductive case . but another factor @xmath227 is lost here .",
    "moreover , in the inductive case , we obtained the real variance of @xmath229 instead of the moment of order @xmath227 here .    in the next subsection , we give several improvements of these bounds , that allows to recover a real variance , and to recover the factor @xmath227 .",
    "we also give a version that allows to deal with a test sample of different size , this being a generalization of theorem [ thtrans ] more than of its improved variants .",
    "we then give the analog of the algorithm proposed in the inductive case in this transductive setting .",
    "the proof of all the theorems of this subsection is given in the next section .",
    "we introduce some new notations .",
    "we write : @xmath230 and , in the case of a model @xmath58 : @xmath231    the we have the following theorem .",
    "[ thimp1 ] we have , for any @xmath215 , with @xmath216-probability at least @xmath57 , for any @xmath217 : @xmath232^{2 } } { ( { 1}/{n})\\sum_{i = n+1}^{2n}\\theta_{h}(x_{i})^{2 } } \\biggr]\\frac{\\log({2m}/{\\varepsilon})}{n}.\\ ] ]    for the proof see section  [ subproof3 ] .",
    "it is moreover possible to modify the upper bound to make it observable .",
    "we obtain that with @xmath216-probability at least @xmath57 , for any @xmath58 : @xmath233-r_{2}\\bigl(\\alpha_{2}^{h}\\theta_{h}\\bigr ) \\leq\\frac{16 \\log ( { 4m}/{\\varepsilon})}{n } \\biggl[\\frac{1}{n}\\sum_{i=1}^{n } \\bigl(\\theta_{h}(x_{i})y_{i}-\\alpha_{1}^{h}\\theta _ { h}(x_{i})^{2 } \\bigr)^{2 } \\biggr ] + \\euscript{o } \\biggl ( \\biggl[\\frac{\\log ( { m}/{\\varepsilon})}{n } \\biggr]^{{3}/{2 } } \\biggr).\\end{aligned}\\ ] ]    so we can see that this theorem is an improvement on theorem [ thtrans ] when some features @xmath234 are well correlated with @xmath150 .",
    "but we loose another factor @xmath227 by making the first - order term of the bound observable .",
    "[ thimp2 ] we have , for any @xmath215 , with @xmath216-probability at least @xmath57 , for any @xmath217 : @xmath235\\frac{2\\log ( { 2m}/{\\varepsilon})}{n } \\frac{v_{1}(\\theta_{h})+v_{2}(\\theta_{h})}{({1}/{n})\\sum_{i = n+1}^{2n } \\theta_{h}(x_{i})^{2 } } , \\ ] ] where : @xmath236^{2 } , } \\\\ \\lefteqn{v_{2}(\\theta_{h } )   = \\frac{1}{n}\\sum_{i = n+1}^{2n } \\biggl[y_{i}\\theta_{h}(x_{i})-\\frac{1}{n}\\sum _ { j = n+1}^{2n}y_{j}\\theta_{h}(x_{j } ) \\biggr]^{2}.}\\end{aligned}\\ ] ] it is moreover possible to give an observable upper bound : we obtain that with @xmath216-probability at least @xmath57 , for any @xmath58 : @xmath233-r_{2}\\bigl(\\alpha_{2}^{h}\\theta_{h}\\bigr ) & \\leq & \\biggl[\\frac{1}{1-{2\\log ( { 4m}/{\\varepsilon})}/{n } } \\biggr ] \\frac{4\\log ( { 4m}/{\\varepsilon})}{n}\\frac{v_{1}(\\theta_{h})}{({1}/{n})\\sum_{i = n+1}^{2n } \\theta_{h}(x_{i})^{2 } } \\\\ & & { } + \\biggl[\\frac{1}{1- { 2\\log ( { 4m}/{\\varepsilon})}/{n } } \\biggr ] 2 \\bigl(2+\\sqrt{2 } \\bigr ) \\biggl(\\frac{\\log ( { 6m}/{\\varepsilon})}{n } \\biggr)^{{3}/{2 } } \\frac{\\sqrt{({1}/{n})\\sum_{i=1}^{2n}\\theta _ { h}(x_{i})^{4}y_{i}^{4}}}{({1}/{n})\\sum_{i = n+1}^{2n } \\theta_{h}(x_{i})^{2 } } .\\end{aligned}\\ ] ]    here again , we can make the bound fully observable under an exponential moment or boundedness assumption about @xmath150 . for a complete proof",
    "see section  [ subproof4 ] .",
    "we need some new notations in this case .",
    "let us put : @xmath237 and , for any @xmath35 : @xmath238^{2 } \\bigr\\ } .\\ ] ]    then we have the following theorem .",
    "[ thimp3 ] let us assume that we have constants @xmath239 and @xmath240 such that , for any @xmath217 : @xmath241 for any @xmath55 , with @xmath13 probability at least @xmath57 we have , for any @xmath217 : @xmath242.}\\end{aligned}\\ ] ]    here again , it is possible to replace the variance term by its natural estimator : @xmath243^{2 } .\\ ] ] for a complete proof of the theorem see the section dedicated to the proofs ( more precisely section  [ subproof5 ] ) .",
    "we give here the interpretation of the preceding theorems in terms of confidence ; this motivates an algorithm similar to the one described in the inductive case .",
    "we take , for any @xmath244 : @xmath245^{2}}.\\ ] ] let also @xmath246 and : @xmath247 we define , for any @xmath217 and @xmath74 : @xmath248 where @xmath249 is the upper bound in theorem [ thtrans ] ( or in any other theorem given in the transductive section ) .    for the same reasons as in the inductive case , these theorems imply the following result .",
    "we have : @xmath250 \\geq1-\\varepsilon.\\ ] ]    we call @xmath251 the orthogonal projection into @xmath252 with respect to the distance @xmath253 .",
    "we propose the following algorithm :    * choose @xmath82 ( for example @xmath254 ) ; * at step @xmath84 , we have : @xmath85 .",
    "choose @xmath255 , for example : @xmath256 and take : @xmath257 * we can use the following stopping rule : @xmath258 where @xmath89 .",
    "we write @xmath90 the stopping step , and : @xmath259 the corresponding function .    here",
    "again we give a detailed version of the algorithm , see fig .",
    "remark that as in the inductive case , we are allowed to use whatever heuristic to choose @xmath103 if we want to avoid the maximization .",
    "p11.0 cm we have @xmath55 , @xmath107 , @xmath8 observations @xmath108 and also @xmath260 , @xmath43 features @xmath109 and @xmath110 .",
    "first , compute every @xmath261 and @xmath249 for @xmath217 . set @xmath112 .",
    "repeat :    * set @xmath113 ; * set _ best____improvement__@xmath114 ; * for @xmath217 , compute : @xmath262 and if @xmath263__best____improvement _ , set : @xmath264 * if _ best____improvement__@xmath118 set : @xmath265    until @xmath266 .",
    "return the estimation : @xmath267 = \\bigl[\\hat{\\theta } ( x_{n+1}),\\ldots,\\hat{\\theta}(x_{(k+1)n } ) \\bigr],\\ ] ] where : @xmath268 +    we have : @xmath269 \\geq 1-\\varepsilon.\\ ] ]    the proof of this theorem is exactly the same as the proof of theorem [ propalgo ] .",
    "let us consider the case where @xmath135 does not depend on the observations .",
    "we can , for example , choose a basis of @xmath33 , or a basis of a subspace of @xmath33 .",
    "we obtain an estimator of the form : @xmath270 in the case when @xmath271 is a wavelet basis , then we obtain here again a procedure for thresholding wavelets coefficients .",
    "let us choose @xmath33 as the set of all functions @xmath194 , a family of kernels @xmath272 for a @xmath273 and : @xmath274 in this case we have @xmath275 .",
    "we obtain an estimator of the form : @xmath276 let us put : @xmath277 we have : @xmath278 that is a support vector machine with different kernel estimate ; like in example [ msvm ] , the kernels @xmath279 can be the same kernel taken at different scales .",
    "take the same @xmath33 and consider the kernel : @xmath280 let us consider a principal component analysis ( pca ) of the family : @xmath281 by performing a diagonalization of the matrix : @xmath282 this method is known as kernel pca , see for example  @xcite .",
    "we obtain eigenvalues : @xmath283 and associated eigenvectors @xmath284 , associated to elements of @xmath33 : @xmath285 that are exchangeable functions of the observations . using the family : @xmath286 we obtain an algorithm that selects which eigenvectors are going to be used in the regression estimation .",
    "this is very close to the kernel projection machine ( kpm ) described by blanchard , massart , vert and zwald @xcite in the context of classification .",
    "we conclude this paper by coming back to the inductive case .",
    "we use theorem [ propalgo ] as an oracle inequality to show that the obtained estimator is adaptative , which means that if we assume that the true regression function @xmath27 has an unknown regularity @xmath132 , then the estimator is able to reach the optimal speed of convergence @xmath287 up to a @xmath288 factor .      here",
    "we assume that @xmath191 is a compact interval of @xmath289 , that @xmath127 and that @xmath4 is such that @xmath290 with @xmath291 independent of @xmath39 , @xmath292 and @xmath293 .",
    "we assume that @xmath128 is an orthonormal basis of @xmath33 .",
    "we still have to choose @xmath48 and we will take @xmath134 .",
    "remark that the orthogonality means here that @xmath294=1 $ ] for any @xmath42 , and that : @xmath295=0\\ ] ] for any @xmath296 .",
    "now , let us put : @xmath297 ( that depends effectively on @xmath43 by @xmath298 ) , and let us assume that @xmath27 satisfies the two following conditions : it is regular , namely there is an unknown @xmath299 and a @xmath300 such that : @xmath301 and that we have a constant @xmath302 such that : @xmath303 with @xmath53 known to the statistician .",
    "it follows that : @xmath304 if follows that every set , for @xmath58 : @xmath305 is a convex set that contains @xmath27 and such that the orthogonal projection : @xmath306 ( where @xmath307 denotes the orthogonal projection on @xmath308 ) can only improve an estimator : @xmath309 actually , note that this projection just consists in thresholding very large coefficients to a limited value .",
    "this modification is necessary in what follows , but this is just a technical remark : most of the time , our estimator wo nt be modified by @xmath310 for any @xmath43 .",
    "remember also that in this context , the estimator given in definition [ hatf ] is just : @xmath311    [ speed ] let us assume that @xmath127 , @xmath312 $ ] and @xmath128 is an orthonormal basis of @xmath33 .",
    "let us assume that we are in the idealized regression model : @xmath313 where @xmath292 , @xmath314 and @xmath291 and @xmath39 are independent , and @xmath2 is known .",
    "let us assume that @xmath315 is such that there is an unknown @xmath299 and an unknown @xmath300 such that : @xmath316 and that we have a constant @xmath302 such that : @xmath303 with @xmath53 known to the statistician .",
    "then our estimator @xmath317 ( given in definition with @xmath318 here , build using the bound @xmath69 given in theorem ) , with @xmath319 and @xmath44 , is such that , for any @xmath320 , @xmath321",
    "\\leq c'(c , b,\\sigma ) \\biggl(\\frac{\\log n}{n } \\biggr)^{{2\\beta}/{(2\\beta+1 ) } } .\\ ] ]    here again , the proof is given at the end of the paper ( section  [ subproofrate ] ) .",
    "let us just remark that , in the case where @xmath322 $ ] , @xmath4 is the lebesgue measure , and @xmath128 is the trigonometric basis , the condition : @xmath323 is satisfied for @xmath324 as soon as @xmath325 where @xmath326 is the sobolev class : @xmath327 the minimax rate of convergence in @xmath326 is @xmath328 , so we can see that our estimator reaches the best rate of convergence up to a @xmath288 factor with an unknown @xmath132 .",
    "we here extend the previous result to the case of a besov space @xmath329 in the case of a wavelet basis ( see @xcite or  @xcite ) .    [ rate2 ] let us assume that @xmath330 $ ] , that @xmath38 is uniform on @xmath191 and that @xmath331 is a wavelet basis , together with a function @xmath332 , satisfying the conditions given in @xcite , with @xmath332 and @xmath333 supported by @xmath334 $ ] .",
    "let us assume that @xmath335 with @xmath336 , @xmath337 , with : @xmath338\\rightarrow{\\mathbb{r } } , g(\\cdot)=\\alpha\\phi(\\cdot ) + \\sum_{j=0}^{\\infty}\\sum_{k=1}^{2^{j}}\\beta _ { j , k}\\psi_{j , k}(\\cdot ) , \\\\ & & \\sum_{j=0}^{\\infty}2^{jq ( s-{1}/{2}-{1}/{p } ) } \\biggl[\\sum _ { k=1}^{2^{j } } |\\beta_{j , k } |^{p } \\biggr]^{{q}/{p } } = \\|g\\|_{s , p , q}^{q } < + \\infty \\biggr\\}\\end{aligned}\\ ] ] ( with obvious changes for @xmath339 or @xmath340 ) with unknown constants @xmath341 , @xmath342 and @xmath343 and that for any @xmath344 , @xmath345 for a known constant @xmath53 .",
    "let us choose : @xmath346 ( so @xmath347 ) and @xmath319 in the definition of @xmath317 .",
    "then we have : @xmath321 = \\euscript{o } \\biggl ( \\biggl(\\frac{\\log n}{n } \\biggr)^{{2s}/{(2s+1 ) } } ( \\log n ) ^ { ( 1-{2}/{((1 + 2s)q ) } ) _ { + } } \\biggr).\\ ] ]    let us remark that we obtain nearly the same rate of convergence than in @xcite , namely the minimax rate of convergence up to a @xmath288 factor .",
    "for the proof , see section  [ subproofrate ] .",
    "the order of the proofs is exactly the order of apparition of the results in the paper , except for the first theorem ( theorem [ lastth ] ) : its proof using lemmas proved in the transductive setting , it is given after the proof of the transductive theorems .",
    "first , we prove a lemma that is the basis of proofs of theorems [ th1][thsvm ] .",
    "[ inequality ] we have , for any @xmath35 , @xmath348 and @xmath349 : @xmath350 and @xmath351    for the first equality , we write : @xmath352 for the reverse equality , the proof is exactly the same , replacing @xmath193 by @xmath353 .",
    "we can now give the proof of both theorems .",
    "proof of theorem [ th1 ] let us choose @xmath58 , for any @xmath354 and @xmath355 we have : @xmath356 -\\eta_{k } \\biggr\\ } }",
    "\\\\ \\lefteqn{\\quad =   \\biggl\\ { p \\exp \\biggl[\\frac{\\lambda_{k}}{n } w_{\\theta_{k } } - \\frac{\\eta _ { k}}{n } \\biggr ] \\biggr\\}^{n } } \\\\ \\lefteqn{\\quad =   \\exp\\biggl [ \\frac{\\lambda_{k}^{2}}{2n}v ( w_{\\theta_{k } } ) + \\frac{\\lambda_{k}^{3}}{2n^{2}}\\int_{0}^{1}(1-\\beta)^{2}m^{3 } _ { ( { \\beta\\lambda_{k}}/{n } ) w_{\\theta_{k } } } ( w_{\\theta_{k } } ) \\,\\mathrm{d}\\beta - \\eta_{k } \\biggr]}\\end{aligned}\\ ] ] by the first equality of lemma [ inequality ] . by the same way , using the reverse inequality we obtain : @xmath357 -\\eta_{k } \\biggr\\ } } \\\\",
    "\\lefteqn{\\quad = \\exp\\biggl [ \\frac{\\lambda_{k}^{2}}{2n}v ( w_{\\theta_{k } } ) -\\frac{\\lambda_{k}^{3}}{2n^{2}}\\int_{0}^{1}(1-\\beta)^{2}m^{3 } _ { ( \\beta\\lambda_{k}/n ) w_{\\theta_{k } } } ( w_{\\theta_{k } } ) \\,\\mathrm{d}\\beta - \\eta_{k } \\biggr].}\\end{aligned}\\ ] ] so we obtain , for any @xmath58 , for any @xmath354 and @xmath355 : @xmath358 \\cosh\\biggl [ \\frac{\\lambda_{k}^{3}}{2n^{2}}\\int_{0}^{1}(1-\\beta)^{2}m^{3 } _ { ( \\beta\\lambda_{k}/n ) w_{\\theta_{k } } } ( w_{\\theta_{k } } ) \\,\\mathrm{d}\\beta \\biggr ] } \\\\",
    "\\lefteqn{\\quad \\leq 2 \\exp\\biggl [ \\frac{\\lambda_{k}^{2}}{2n}v ( w_{\\theta_{k } } ) -\\eta_{k } + \\frac{\\lambda_{k}^{6}}{8n^{4 } } \\biggl(\\int_{0}^{1}(1-\\beta)^{2}m^{3 } _ { ( \\beta\\lambda_{k}/n ) w_{\\theta_{k } } } ( w_{\\theta_{k } } ) \\,\\mathrm{d}\\beta\\biggr)^{2 } \\biggr ] , } \\end{aligned}\\ ] ] since , for any @xmath359 , we have : @xmath360 now , let us choose @xmath55 and put : @xmath361 we obtain : @xmath362 and so : @xmath363 \\geq1 - \\varepsilon.}\\end{aligned}\\ ] ] now , we put : @xmath364 we obtain , with @xmath56-probability at least @xmath57 , for any @xmath58 : @xmath365 for short , we take the notation of the theorem : @xmath366 now , dividing both sides by : @xmath367\\ ] ] we obtain : @xmath368 } \\biggl [ \\sqrt{\\frac{2v ( w_{\\theta_{k } } ) \\log ( { 2m}/{\\varepsilon})}{n } } + \\frac{i_{\\theta_{k}}^{2 } ( { \\lambda_{k}}/{n } ) \\log^{{5}/{2 } } ( { 2m}/{\\varepsilon})}{n v ( w_{\\theta_{k } } ) ^{3 } } \\biggr].\\ ] ] in order to conclude , just remark that : @xmath369 .\\ ] ]    proof of theorem [ th1bis ] remark that , for any @xmath35 : @xmath370 we will deal with each term separately . for the first term , let us remark that we obtain the following result that is obtained exactly as lemma [ inequality ] .",
    "for any @xmath35 : @xmath371 -\\eta\\bigr\\ } = \\exp\\biggl\\{\\frac{\\gamma^{2}}{2}v \\bigl(w_{\\theta}^{2 } \\bigr ) + \\frac{\\gamma^{3}}{2}\\int_{0}^{1}(1-\\beta)^{2}m^{3}_{\\gamma\\beta w_{\\theta}^{2 } } \\bigl(w_{\\theta}^{2 } \\bigr)\\,\\mathrm{d}\\beta-\\eta\\biggr\\}.\\end{aligned}\\ ] ] let us apply this result to every @xmath372 for @xmath58 : @xmath373 -\\eta_{k } \\biggr\\ } = \\exp\\biggl\\{\\frac{\\lambda_{k}^{2}}{2n}v \\bigl(w_{\\theta_{k}}^{2 } \\bigr ) + \\frac{\\lambda_{k}^{3}}{2n } j_{k } \\biggl(\\frac{\\lambda_{k}}{n } \\biggr ) -\\eta_{k } \\biggr\\},\\end{aligned}\\ ] ] where :",
    "@xmath164 taking @xmath374 and @xmath375 we obtain that the following inequality is satisfied with @xmath56-probability at least @xmath376 , for any @xmath29 : @xmath377 for short .",
    "now , we try to upper bound the second term , @xmath378 .",
    "remark that , for any @xmath379 : @xmath380 remember that in the proof of theorem [ th1 ] we got the upper bound , with probability at least @xmath376 , for any @xmath29 : @xmath381 that gives : @xmath382 for short .",
    "let us combine inequalities ( [ inequ1 ] )  and  ( [ inequ2 ] ) .",
    "we obtain that , with probability at least @xmath57 , for every @xmath29 we have : @xmath383    proof of theorem [ thsvm ] this proof is a variant of the proof of theorem [ th1 ] , the method it uses is due to seeger @xcite",
    ". let us define , for any @xmath10 : @xmath384 let us choose @xmath385 , for any @xmath386 and @xmath387 we have : @xmath388 -\\eta_{i , k } \\biggr\\ } } \\\\",
    "\\lefteqn{\\quad \\leq\\exp\\biggl [ \\frac{\\lambda_{i , k}}{2(n-1 ) } v ( w_{\\theta_{i , k } } ) + \\frac{\\lambda_{i , k}^{3}}{2(n-1)^{2 } } \\int_{0}^{1 } ( 1-\\beta ) ^{2}m^{3}_{(\\beta\\lambda_{i , k}/{n-1 } ) w_{\\theta_{i , k } } } ( w_{\\theta_{i , k } } ) \\,\\mathrm{d}\\beta -\\eta_{i , k } \\biggr]}\\end{aligned}\\ ] ] by the first equality of lemma [ inequality ] . in the same way",
    ", we obtain the reverse inequality and , combining both results , for any @xmath385 , for any @xmath389 and @xmath390 : @xmath391 \\cosh\\biggl [ \\frac{\\lambda_{i , k}^{3}}{2(n-1)^{2}}i_{i , k } \\biggr ] } \\\\ \\lefteqn{\\quad \\leq2 \\exp\\biggl [ \\frac{\\lambda_{i , k}^{2}}{2(n-1)}v ( w_{\\theta_{i , k } } ) -\\eta_{i , k } + \\frac{\\lambda_{i , k}^{6}}{8(n-1)^{4}}i_{i , k}^{2 } \\biggr],}\\end{aligned}\\ ] ] where : @xmath392 for short .",
    "now , let us choose @xmath55 and put : @xmath393 we obtain : @xmath394 now , we put : @xmath395 and achieve the proof exactly as for theorem [ th1 ] .      here again , the first thing to do is to prove a general deviation inequality .",
    "this one is a variant of the one given by catoni @xcite .",
    "we go back to the notations of theorem [ thtrans ] and [ cortrans ] , with test sample of size @xmath8 .",
    "let @xmath396 denote the set of all functions : @xmath397 for the sake of simplicity , such that @xmath398 is exchangeable with respect to its @xmath228 first arguments .",
    "[ learninglemma ] for any exchangeable probability distribution @xmath399 on @xmath400 , for any measurable function @xmath401 that is exchangeable with respect to its @xmath402 arguments , for any measurable function @xmath403 that is exchangeable with respect to its @xmath402 arguments , for any @xmath35 and any @xmath404 : @xmath405-g",
    "\\bigl[\\theta(x_{i}),y_{i } \\bigr ] \\bigr\\ } - \\frac{\\lambda^{2}}{c_{g}n^{2 } } \\sum_{i=1}^{2n } g \\bigl[\\theta ( x_{i}),y_{i } \\bigr]^{2 } - \\eta \\biggr ) \\leq\\mathcal{p } \\exp ( - \\eta ) } \\end{aligned}\\ ] ] and the reverse inequality : @xmath406-g \\bigl[\\theta(x_{i+n}),y_{i+n } \\bigr ] \\bigr\\ } - \\frac{\\lambda^{2}}{c_{g}n^{2 } } \\sum_{i=1}^{2n } g \\bigl[\\theta ( x_{i}),y_{i } \\bigr]^{2 } - \\eta \\biggr ) \\leq\\mathcal{p } \\exp ( - \\eta ) , \\end{aligned}\\ ] ] where we write : @xmath407 for short , and : @xmath408    in order to prove the first inequality , we write : @xmath405-g \\bigl[\\theta(x_{i}),y_{i } \\bigr ] \\bigr\\ } - \\frac{\\lambda^{2}}{n^{2 } } \\sum_{i=1}^{2n } g \\bigl[\\theta(x_{i}),y_{i } \\bigr]^{2 } - \\eta \\biggr ) } \\\\",
    "\\lefteqn{\\quad = \\mathcal{p } \\exp\\biggl(\\sum_{i=1}^{n } \\log\\cosh \\biggl\\ { \\frac{\\lambda}{n } g \\bigl[\\theta(x_{i+n}),y_{i+n } \\bigr]-\\frac{\\lambda}{n}g \\bigl[\\theta(x_{i}),y_{i } \\bigr ] \\biggr\\ } - \\frac{\\lambda^{2}}{n^{2 } } \\sum_{i=1}^{2n } g \\bigl[\\theta(x_{i}),y_{i } \\bigr]^{2 } - \\eta \\biggr).}\\end{aligned}\\ ] ] this last step is true because @xmath399 is exchangeable .",
    "we conclude by using the inequality : @xmath409 we obtain : @xmath410-\\frac{\\lambda}{n}g \\bigl[\\theta(x_{i}),y_{i } \\bigr ] \\biggr\\ } & \\leq & \\frac{\\lambda^{2}}{2n^{2 } } \\bigl\\ { g \\bigl[\\theta(x_{i+n}),y_{i+n } \\bigr]-g \\bigl[\\theta(x_{i}),y_{i } \\bigr ] \\bigr\\}^{2 } \\\\ & \\leq & \\frac{\\lambda^{2}}{c_{g } n^{2 } } g \\bigl[\\theta(x_{i } ) , y_{i } \\bigr]^{2 } .\\end{aligned}\\ ] ] the proof for the reverse inequality is exactly the same .    we can now give the proof of the theorems .",
    "proof of theorem [ thtrans ] from now on we assume that the hypothesis of theorem [ thtrans ] are satisfied .",
    "let us choose @xmath411 and apply lemma [ learninglemma ] with @xmath412 , and @xmath398 such that @xmath413 .",
    "we obtain : for any exchangeable distribution  @xmath399 , for any measurable function @xmath414 that is exchangeable with respect to its @xmath402 arguments , for any @xmath35 : @xmath415 - \\frac{\\lambda^{2}}{n^{2 } } \\sum_{i=1}^{2n } \\theta(x_{i})^{2}y_{i}^{2 } + \\log\\varepsilon ' \\biggr ) \\leq\\varepsilon'\\ ] ] and the reverse inequality : @xmath416 - \\frac{\\lambda^{2}}{n^{2 } } \\sum_{i=1}^{2n } \\theta(x_{i})^{2}y_{i}^{2 } + \\log\\varepsilon ' \\biggr ) \\leq\\varepsilon'.\\ ] ]    let us denote : @xmath417 - \\frac{\\lambda^{2}}{n^{2 } } \\sum_{i=1}^{2n } \\theta(x_{i})^{2}y_{i}^{2 } \\bigg| + \\log\\varepsilon'.\\ ] ]    the previous inequalities imply that : for any exchangeable @xmath399 , for any measurable function @xmath403 that is exchangeable with respect to its @xmath402 arguments , for any @xmath418 : @xmath419    now , let us introduce a new conditional probability measure : @xmath420    remark that @xmath216 being exchangeable , we have , for any bounded function @xmath421 , @xmath422 the measure @xmath423 is exchangeable , so we can apply eq .",
    "( [ ineq1 ] ) . for any values of @xmath424 we have : @xmath425    in particular , we can choose @xmath426 as an exchangeable function of @xmath427 , because we will have : @xmath428    here , we choose as functions @xmath379 the members of @xmath135 : @xmath429 ( remember that we choose this indexation in such a way that for any @xmath29 , @xmath430 is an exchangeable function of @xmath427 ) .",
    "we have , for any @xmath431 that are @xmath43 exchangeable functions of @xmath427 : @xmath432 } \\\\ \\lefteqn{\\quad = p_{2n } \\biggl[\\bigcup_{k=1}^{m } \\bigl\\ { f\\bigl((z_{1},\\ldots , z_{2n}),\\theta_{k},\\varepsilon',\\lambda_{k}\\bigr ) > 0 \\bigr\\ } \\biggr ] } \\\\ \\lefteqn{\\quad \\leq p_{2n } \\biggl[\\sum_{k=1}^{m } 1 \\bigl ( f\\bigl((z_{1},\\ldots , z_{2n}),\\theta_{k},\\varepsilon',\\lambda_{k}\\bigr ) > 0 \\bigr ) \\biggr ] } \\\\",
    "\\lefteqn{\\quad = p_{2n}\\overline{p } \\biggl[\\sum_{k=1}^{m } 1 \\bigl ( f\\bigl((z_{1},\\ldots , z_{2n}),\\theta_{k},\\varepsilon',\\lambda_{k}\\bigr ) > 0 \\bigr ) \\biggr ] } \\\\",
    "\\lefteqn{\\quad = p_{2n } \\sum_{k=1}^{m } \\overline{p } \\bigl [ 1 \\bigl ( f\\bigl((z_{1},\\ldots , z_{2n}),\\theta_{k},\\varepsilon',\\lambda_{k}\\bigr ) > 0 \\bigr ) \\bigr ] } \\\\",
    "\\lefteqn{\\quad \\leq p_{2n } \\sum_{k=1}^{m } \\overline{p } \\exp f\\bigl((z_{1},\\ldots , z_{2n}),\\theta_{k},\\varepsilon',\\lambda_{k}\\bigr).}\\end{aligned}\\ ] ]    now let us apply inequality ( [ ineq1 ] ) , we obtain : @xmath433 \\leq p_{2n } \\sum_{k=1}^{m } 2 \\varepsilon ' = 2 \\varepsilon ' m = \\varepsilon\\ ] ] if we choose : @xmath434    from now , we assume that the event : @xmath435 is satisfied .",
    "it can be written , for any @xmath58 : @xmath436 \\bigg| \\leq\\frac{\\lambda_{k}}{n^{2 } } \\sum_{i=1}^{2n}\\theta_{k}(x_{i})^{2}y_{i}^{2 } + \\frac{\\log ( { 2m}/{\\varepsilon})}{\\lambda_{k } } .\\ ] ]    let us divide both inequalities by : @xmath437 we obtain , for any @xmath438 : @xmath439    it is now time to choose the functions @xmath440 .",
    "we try to optimize the right - hand side with respect to @xmath440 , and obtain a minimal value for : @xmath441 this choice is admissible because it is exchangeable with respect to @xmath427 .",
    "so we have , for any @xmath58 : @xmath442 \\log ( { 2m}/{\\varepsilon } ) } } { ( { 1}/{n})\\sum_{i = n+1}^{2n}\\theta_{k}(x_{i})^{2 } } .\\ ] ]    finally , remark that : @xmath443 - r_{2}(\\alpha_{2}^{k}\\theta_{k } ) } { ( { 1}/{n})\\sum_{i = n+1}^{2n}\\theta_{k}(x_{i})^{2 } } } , \\ ] ] which leads to the conclusion that for any @xmath438 : @xmath444 - r_{2}\\bigl(\\alpha_{2}^{k}\\theta_{k}\\bigr ) \\leq2^{2 } \\frac { ( { 1}/{n^{2}})\\sum_{i=1}^{2n } [ \\theta_{k}(x_{i})^{2}y_{i}^{2 } ] \\log ( { 2m}/{\\varepsilon } ) } { ( { 1}/{n})\\sum_{i = n+1}^{2n}\\theta_{k}(x_{i})^{2 } } .\\ ] ] this ends the proof .",
    "proof of theorem [ cortrans ] we write : @xmath445 and try to upper bound the second term .",
    "we apply lemma [ learninglemma ] , but this time with @xmath398 such that @xmath446 that is nonnegative , and obtain , for any @xmath74 , for any ( exchangeables ) @xmath379 and @xmath447 : @xmath448 we choose : @xmath449 we apply this result to every @xmath450 , and combine it with theorem [ thtrans ] by a union bound argument to obtain the result .",
    "first of all , we give the following obvious variant of lemma  [ learninglemma ] :    [ ll2 ] for any exchangeable probability distribution @xmath399 on @xmath400 , for any measurable function @xmath401 that is exchangeable with respect to its @xmath402 arguments , for any measurable function @xmath403 that is exchangeable with respect to its @xmath402 arguments , for any @xmath35 : @xmath451 - \\bigl[\\theta(x_{i})y_{i}-\\alpha(\\theta)\\theta(x_{i})^{2 } \\bigr ] \\bigr\\ } } \\\\",
    "\\lefteqn{\\quad { } - \\frac{\\lambda^{2}}{n^{2 } } \\sum_{i=1}^{2n } \\bigl[\\theta(x_{i})y_{i}-\\alpha ( \\theta)\\theta(x_{i})^{2 } \\bigr]^{2}- \\eta \\biggr ) \\leq\\mathcal{p } \\exp ( - \\eta ) } \\end{aligned}\\ ] ] and the reverse inequality , where : @xmath452    this is actually just an application of lemma  [ learninglemma ] , we just need to remark that @xmath453 is an exchangeable function of @xmath427 , and so we can take in lemma  [ learninglemma ] : @xmath454 that means that : @xmath455 = \\theta(x_{i})y_{i}-\\alpha(\\theta)\\theta ( x_{i})^{2 } .\\ ] ]    proof of theorem  [ thimp1 ] proceeding exactly in the same way as in the proof of theorem [ thtrans ] , we obtain the following inequality with probability at least @xmath57 : @xmath456^{2 } } { ( { 1}/{n})\\sum_{i = n+1}^{2n}\\theta_{k}(x_{i})^{2 } } \\biggr]\\frac{\\log ( 2m/\\varepsilon)}{n}.\\ ] ] this proves the theorem .    before giving the proof of the next theorem , let us see how we can make the first - order term observable in this theorem . for example ,",
    "we can write : @xmath457^{2 } & = & \\bigl[\\theta_{k}(x_{i})y_{i}-\\alpha_{1}^{k}\\theta_{k}(x_{i})^{2 } \\bigr]^{2 } + \\bigl[\\alpha_{1}^{k}-\\alpha_{1,2}^{k } \\bigr]^{2}\\theta_{k}(x_{i})^{4 } \\\\ & & { } + 2 \\bigl[\\theta_{k}(x_{i})y_{i}-\\alpha_{1}^{k}\\theta_{k}(x_{i})^{2 } \\bigr ] \\bigl[\\alpha_{1}^{k}-\\alpha_{1,2}^{k } \\bigr]\\theta_{k}(x_{i})^{2}.\\end{aligned}\\ ] ]    remark that it is obvious that : @xmath458 and so : @xmath457^{2 } & \\leq & \\bigl[\\theta_{k}(x_{i})y_{i}-\\alpha_{1}^{k}\\theta_{k}(x_{i})^{2 } \\bigr]^{2 } + \\bigl[\\alpha_{1}^{k}-\\alpha_{2}^{k } \\bigr]^{2}\\theta_{k}(x_{i})^{4 } \\\\ & & { } + 2 \\big|\\theta_{k}(x_{i})y_{i}-\\alpha_{1}^{k}\\theta_{k}(x_{i})^{2 } \\big| \\big|\\alpha_{1}^{k}-\\alpha_{2}^{k } \\big|\\theta_{k}(x_{i})^{2}.\\end{aligned}\\ ] ]    now , just write : @xmath459 and so we get : @xmath457^{2 } & \\leq & \\bigl[\\theta_{k}(x_{i})y_{i}-\\alpha_{1}^{k}\\theta_{k}(x_{i})^{2 } \\bigr]^{2 } + \\bigl[\\mathcal{c}^{k}\\alpha_{1}^{k}-\\alpha_{2}^{k } \\bigr]^{2}\\theta_{k}(x_{i})^{4 } \\\\ & & { } + 2 \\big|\\mathcal{c}^{k}\\alpha_{1}^{k}-\\alpha_{2}^{k } \\big| \\big|\\bigl(1-\\mathcal { c}^{k}\\bigr)\\alpha_{1}^{k } \\big|\\theta_{k}(x_{i})^{4 } + \\bigl(1-\\mathcal{c}^{k } \\bigr)^{2 } \\bigl(\\alpha_{1}^{k } \\bigr)^{2 } \\theta_{k}(x_{i})^{4 } \\\\ & & { } + 2 \\big|\\theta_{k}(x_{i})y_{i}-\\alpha_{1}^{k}\\theta_{k}(x_{i})^{2 } \\big| \\big|\\mathcal{c}^{k}\\alpha_{1}^{k}-\\alpha_{2}^{k } \\big|\\theta_{k}(x_{i})^{2 } \\\\ & & { } + 2 \\big|\\theta_{k}(x_{i})y_{i}-\\alpha_{1}^{k}\\theta_{k}(x_{i})^{2 } \\big| \\big|\\bigl(\\mathcal{c}^{k}-1\\bigr)\\alpha_{1}^{k } \\big|\\theta_{k}(x_{i})^{2}.\\end{aligned}\\ ] ] so finally , eq .  ( [ var1 ] ) left us with a second degree inequality with respect to @xmath460 or @xmath461 that we can solve to obtain the following result : with probability at least @xmath57 , as soon as we have : @xmath462^{2 } > \\biggl[\\frac{1}{n}\\sum_{i=1}^{2n}\\theta_{k}(x_{i})^{4 } \\biggr ] \\frac{4\\log ( { 2m}/{\\varepsilon})}{n } , \\ ] ] which is always true for large enough @xmath8 , the quantity @xmath460 belongs to the interval : @xmath463^{2 } -(4/n)\\sum_{i=1}^{2n}\\theta_{k}(x_{i})^{4 } ) } } { [ ( 1/n)\\sum_{i = n+1}^{2n}\\theta_{k}(x_{i})^{2 } ] ^{2 } - ( 4\\log ( 2m/\\varepsilon)/n ) [ ( 1/n)\\sum_{i=1}^{2n}\\theta_{k}(x_{i})^{4 } ] } \\biggr]\\ ] ] with the following notations : @xmath464^{2 } , }",
    "\\\\ \\lefteqn{b   = \\frac{1}{n}\\sum_{i=1}^{2n } 2 \\theta_{k}(x_{i})^{2 } \\bigl [ \\big|\\alpha_{k}^{1}\\bigl(1-\\mathcal{c}^{k}\\bigr ) \\big|\\theta_{k}(x_{i})^{2 } + \\big|\\theta_{k}(x_{i})y_{i}-\\alpha_{1}^{k}\\theta_{k}(x_{i})^{2 } \\big| \\bigr].}\\end{aligned}\\ ] ] remark that only one of the bounds of the interval is positive .",
    "so we obtain the following result : with @xmath216-probability at least @xmath57 , as soon as : @xmath462^{2 } > \\biggl[\\frac{1}{n}\\sum_{i=1}^{2n}\\theta_{k}(x_{i})^{4 } \\biggr ] \\frac{4\\log ( 2m/\\varepsilon)}{n}\\ ] ] we have : @xmath465-r_{2}\\bigl(\\alpha_{2}^{k}\\theta_{k}\\bigr ) } \\\\ \\lefteqn{\\quad \\leq \\frac{4\\log^{2 } ( { 2m}/{\\varepsilon})}{n^{2 } } \\biggl[\\frac{1}{n}\\sum _ { i=1}^{2n}\\theta_{k}(x_{i})^{2 } \\biggr ] } \\\\",
    "\\lefteqn{\\qquad { } \\times \\biggl [ \\frac { b + \\sqrt{b^{2 } + a ( ( n/\\log(2m/\\varepsilon ) ) [ ( { 1}/{n})\\sum_{i = n+1}^{2n}\\theta_{k}(x_{i})^{2 } ] ^{2 } -({4}/{n})\\sum_{i=1}^{2n}\\theta_{k}(x_{i})^{4 } ) } } { [ ( { 1}/{n})\\sum_{i = n+1}^{2n}\\theta_{k}(x_{i})^{2 } ] ^{2 } - ( ( 4\\log ( 2m/\\varepsilon))/n ) [ ( 1/n)\\sum_{i=1}^{2n}\\theta_{k}(x_{i})^{4 } ] } \\biggr]^{2}.}\\end{aligned}\\ ] ] we can notice that this bound may be written : @xmath466-r_{2}\\bigl(\\alpha_{2}^{k}\\theta_{k}\\bigr ) & \\leq & \\frac{8 a \\log ( { 2m}/{\\varepsilon})}{n } + \\euscript{o } \\biggl ( \\biggl[\\frac{\\log ( { m}/{\\varepsilon})}{n } \\biggr]^{{3}/{2 } } \\biggr ) \\\\ & = &   \\frac{8\\log ( 2m/\\varepsilon)}{n } \\biggl[\\frac{1}{n}\\sum_{i=1}^{2n } \\bigl(\\theta_{k}(x_{i})y_{i}-\\alpha _ { 1}^{k}\\theta_{k}(x_{i})^{2 } \\bigr)^{2 } \\biggr ] + \\euscript{o } \\biggl ( \\biggl[\\frac{\\log ( { m}/{\\varepsilon})}{n } \\biggr]^{{3}/{2 } } \\biggr).\\end{aligned}\\ ] ] the next step would be now to replace the bound by an observable quantity , by getting a bound like : @xmath467 with high probability .",
    "this can be done very simply , using lemma [ learninglemma ] with this time : @xmath468 we obtain the bound : @xmath469-r_{2}\\bigl(\\alpha_{2}^{k}\\theta_{k}\\bigr ) \\leq\\frac{16 \\log ( 4m/\\varepsilon)}{n } \\biggl[\\frac{1}{n}\\sum_{i=1}^{n } \\bigl(\\theta_{k}(x_{i})y_{i}-\\alpha_{1}^{k}\\theta _ { k}(x_{i})^{2 } \\bigr)^{2 } \\biggr ] + \\euscript{o } \\biggl ( \\biggl[\\frac{\\log ( m/\\varepsilon)}{n } \\biggr]^{{3}/{2 } } \\biggr).\\ ] ]      the proof is exactly similar , we just use a new variant of lemma [ learninglemma ] , that is based on an idea introduced by catoni @xcite in the context of classification .",
    "let us write : @xmath470 for short .",
    "we also introduce a conditional probability measure : @xmath471    remark that , because @xmath399 is exchangeable , we have , for any function @xmath472 : @xmath473 .\\ ] ]    [ lemma2 ] for any exchangeable probability distribution @xmath399 on @xmath400 , for any measurable function @xmath401 that is exchangeable with respect to its @xmath402 arguments , for any measurable function @xmath403 which is such that , for any @xmath474 : @xmath475 for any @xmath35 : @xmath476 - \\mathcal{p}^{(2 ) } \\biggl[\\frac{\\lambda^{2}}{2n^{2 } } \\frac{1}{n}\\sum _ { i=1}^{n } \\bigl[t_{\\theta}(z_{i})-t_{\\theta}(z_{i+n } ) \\bigr]^{2 } \\biggr ] - \\eta \\biggr\\ } \\leq\\mathcal{p } \\exp ( - \\eta ) \\ ] ] and the reverse inequality .",
    "let @xmath477 denote the left - hand side of lemma [ lemma2 ] . for short ,",
    "let us put : @xmath478^{2 } = \\frac{1}{n}\\sum_{i=1}^{n } \\bigl[t_{\\theta}(z_{i})-t_{\\theta}(z_{i+n } ) \\bigr]^{2}.\\ ] ] then we have : @xmath479 -\\frac{\\lambda^{2}}{2n}s(\\theta ) -\\eta\\biggr ) \\\\",
    "& \\leq &   p_{2n } p^{(2)}\\exp \\biggl(\\frac{\\lambda}{n}\\sum_{i=1}^{n } \\bigl[t_{\\theta}(z_{i})-t_{\\theta}(z_{i+n } ) \\bigr ] -\\frac{\\lambda^{2}}{2n}s(\\theta ) -\\eta\\biggr),\\end{aligned}\\ ] ] by jensen s conditional inequality .",
    "now , we can conclude as in lemma [ learninglemma ] : @xmath480 \\biggr\\ } -\\frac{\\lambda^{2}}{2n}s(\\theta)-\\eta\\biggr ) \\\\ & \\leq & p_{2n}\\exp\\biggl(\\frac{\\lambda^{2}}{2n^{2}}\\sum_{i=1}^{n } \\bigl[t_{\\theta}(z_{i})-t_{\\theta}(z_{i+n } ) \\bigr]^{2 } -\\frac{\\lambda^{2}}{2n}s(\\theta)-\\eta\\biggr ) \\\\ & = &   p_{2n}\\exp ( -\\eta).\\end{aligned}\\ ] ]    proof of theorem [ thimp2 ] we apply both inequalities of lemma [ lemma2 ] to every @xmath481 , and we take : @xmath482 we obtain , for any @xmath58 : @xmath476 - \\log\\frac{2m}{\\varepsilon } - \\eta \\biggr\\ } \\leq\\varepsilon.\\ ] ] or , with probability at least @xmath57 , for any @xmath29 : @xmath483 \\leq\\sqrt{\\frac{2\\log ( 2m/\\varepsilon)}{n } } \\bigl[\\mathcal{p}^{(2 ) } \\bigl(s(\\theta)^{-{1}/{2 } } \\bigr ) \\bigr]^{-1},\\ ] ] so : @xmath484^{2 } \\leq\\frac{2\\log ( { 2m}/{\\varepsilon})}{n } \\mathcal{p}^{(2)}s(\\theta).\\ ] ] we end the first part of the proof by noting that : @xmath485^{2}.\\ ] ] now , let us see how we can obtain the second part of the theorem .",
    "note that : @xmath486 we upper bound the first term by using lemma [ learninglemma ] with @xmath487 , so with probability at least @xmath57 , for any @xmath29 : @xmath488 for the second - order term , we use both inequalities of lemma [ learninglemma ] with @xmath489 , so with probability at least @xmath57 , for any @xmath29 : @xmath490 putting all pieces together ( and replacing @xmath74 by @xmath491 ) ends the proof .",
    "proof of theorem [ thimp3 ] we introduce the following conditional probability measures , for any @xmath10 : @xmath492and @xmath493 and , finally , remember that : @xmath494    note that , by exchangeability , for any nonnegative function @xmath495 we have , for any @xmath10 : @xmath496    [ klemma ] let @xmath497 be a function @xmath498 .",
    "for any exchangeable functions @xmath447 , @xmath499 and @xmath500 we have : @xmath501 -\\frac{1}{n}\\sum_{i=1}^{n } \\chi \\bigl[\\theta(x_{i})y_{i } \\bigr ] \\biggr]-\\eta\\biggr\\ } } \\\\",
    "\\lefteqn{\\quad \\leq\\exp(-\\eta ) \\exp\\biggl\\ { \\frac{\\lambda^{2}(1+k)^{2}}{2nk^{2 } } \\mathbf{p } \\bigl\\ { \\bigl[\\chi\\bigl(\\theta(x)y \\bigr ) - \\mathbf{p } \\chi\\bigl(\\theta(x)y \\bigr ) \\bigr]^{2 } \\bigr\\ } }",
    "\\\\ \\lefteqn{\\qquad { } + \\frac{\\lambda^{3}(1+k)^{3}}{6n^{2 } k^{3 } } \\bigl[\\sup_{i\\in\\{1,\\ldots,(k+1)n\\}}\\chi \\bigl(\\theta(x_{i})y_{i } \\bigr)-\\inf_{i\\in\\ { 1,\\ldots,(k+1)n\\ } } \\chi\\bigl(\\theta(x_{i})y_{i } \\bigr ) \\bigr]^{3 } \\biggr\\ } , } \\end{aligned}\\ ] ] where we put @xmath502 , @xmath503 and @xmath504 for short .",
    "we have the reverse inequality as well .    before giving the proof ,",
    "let us introduce the following useful notations .",
    "we put , for any @xmath35 , for any function @xmath497 : @xmath505 and @xmath506 that means that : @xmath507 we also put : @xmath508    proof of the lemma [ klemma ] remark that , for any exchangeable functions @xmath447 , @xmath499 and @xmath509 we have : @xmath510 -\\frac{1}{n}\\sum_{i=1}^{n } g \\bigl[\\theta(x_{i})y_{i } \\bigr ] \\biggr]-\\eta\\biggr\\ } } \\\\",
    "\\lefteqn{\\quad =   \\exp(-\\eta ) \\prod_{i=1}^{n } { \\mathrm{p}}_{i } \\exp \\biggl\\ { \\frac{\\lambda}{kn}\\sum_{j=1}^{k}\\chi_{i+jn}^{\\theta } - \\frac{\\lambda}{n}\\chi_{i}^{\\theta } \\biggr\\ } } \\\\",
    "\\lefteqn{\\quad =   \\exp(-\\eta ) \\prod_{i=1}^{n } \\exp \\biggl\\ { \\frac{\\lambda}{kn}\\sum_{j=0}^{k}\\chi_{i+jn}^{\\theta } \\biggr\\ } \\prod_{i=1}^{n } { \\mathrm{p}}_{i } \\exp \\biggl\\ { - \\frac{\\lambda(1+k)}{kn}\\chi _ { i}^{\\theta } \\biggr\\},}\\end{aligned}\\ ] ] where we put @xmath511 , @xmath512 and @xmath513 for short .",
    "now , we have : @xmath514 and , for any @xmath10 : @xmath515 } \\\\ \\lefteqn{\\qquad { }   - \\int_{0}^{{\\lambda(1+k)}/{(nk ) } } \\frac{1}{2 } \\biggl(\\frac{\\lambda(1+k)}{nk}-\\beta\\biggr)^{2 } \\frac{1}{{\\mathrm{p}}_{i } \\exp[-\\beta\\chi_{i}^{\\theta } ] } { \\mathrm{p}}_{i } \\biggl [ \\biggl(\\chi_{i}^{\\theta}-\\frac{{\\mathrm{p}}_{i } \\ { \\chi _ { i}^{\\theta } \\exp[-\\beta\\chi_{i}^{\\theta } ] \\ } } { { \\mathrm{p}}_{i } \\exp[-\\beta\\chi_{i}^{\\theta } ] } \\biggr)^{3}\\exp\\bigl(-\\beta\\chi _ { i}^{\\theta } \\bigr ) \\biggr]\\,\\mathrm{d}\\beta.}\\end{aligned}\\ ] ] note that , for any @xmath516 : @xmath517 } { \\mathrm{p}}_{i } \\biggl [ \\biggl(\\chi_{i}^{\\theta}-\\frac{{\\mathrm{p}}_{i } \\ { \\chi _ { i}^{\\theta } \\exp[-\\beta\\chi_{i}^{\\theta } ] \\ } } { { \\mathrm{p}}_{i } \\exp[-\\beta\\chi_{i}^{\\theta } ] } \\biggr)^{3}\\exp\\bigl(-\\beta\\chi _ { i}^{\\theta } \\bigr ) \\biggr ] \\leq \\bigl[\\sup_{j\\in\\{1,\\ldots , k\\}}\\chi_{i+(j-1)n}^{\\theta}-\\inf_{j\\in\\{1,\\ldots , k\\}}\\chi_{i+(j-1)n}^{\\theta } \\bigr]^{3},\\ ] ] and",
    "so : @xmath518 \\\\ & & { } + \\frac{\\lambda^{3}(1+k)^{3}}{6n^{2 } k^{3 } } \\bigl[\\sup_{i\\in\\{1,\\ldots,(k+1)n\\}}\\chi_{i}^{\\theta}-\\inf_{i\\in\\{1,\\ldots , ( k+1)n\\}}\\chi_{i}^{\\theta } \\bigr]^{3}.\\end{aligned}\\ ] ] note that : @xmath519 and so : @xmath520 remark also that : @xmath521 \\leq \\frac{1}{(k+1)n } \\sum_{i=1}^{(k+1)n } \\biggl[\\chi_{i}^{\\theta}- \\biggl(\\frac{1}{(k+1)n}\\sum_{j=1}^{(k+1)n}\\chi _ { j}^{\\theta } \\biggr ) \\biggr]^{2 } = \\mathbf{p } \\bigl [ \\bigl(\\chi^{\\theta}- \\mathbf{p } \\chi^{\\theta } \\bigr)^{2 } \\bigr ] , \\ ] ] we obtain : @xmath522-\\eta\\biggr\\ } } \\\\",
    "\\lefteqn{\\quad = \\exp(-\\eta ) \\exp\\biggl\\ { \\frac{\\lambda^{2}(1+k)^{2}}{2nk^{2 } } \\mathbf{p } \\bigl [ \\bigl(\\chi^{\\theta } - \\mathbf{p } \\chi^{\\theta } \\bigr)^{2 } \\bigr ] + \\frac{\\lambda^{3}(1+k)^{3}}{6n^{2 } k^{3 } } \\bigl[\\sup_{i\\in\\{1,\\ldots,(k+1)n\\}}\\chi_{i}^{\\theta}-\\inf_{i\\in\\{1,\\ldots , ( k+1)n\\}}\\chi_{i}^{\\theta } \\bigr]^{3 } \\biggr\\ } .}\\end{aligned}\\ ] ] the proof of the reverse inequality is exactly the same .",
    "let us choose here again @xmath497 such that @xmath523 , namely : @xmath524 . by the use of a union bound argument on elements of @xmath135",
    "we obtain , for any @xmath55 , for any exchangeable function @xmath525 , with probability at least @xmath57 , for any @xmath217 : @xmath526 + \\frac{\\lambda^{2 } ( 1 + { 1}/{k } ) ^{3}}{6n^{2 } } \\mathcal{s}_{id}(\\theta_{h})^{3 } + \\frac{\\log ( { m}/{\\varepsilon})}{\\lambda } .}\\end{aligned}\\ ] ] let us choose , for any @xmath217 : @xmath527 } } , \\ ] ] the bound becomes : @xmath528 \\log ( { m}/{\\varepsilon } ) } { 2n } } + \\frac{\\mathcal{s}_{id}(\\theta_{h})^{3}\\log ( { m}/{\\varepsilon } ) } { 3n \\mathbf{p } [ ( \\chi^{\\theta_{h } } - \\mathbf{p } \\chi^{\\theta_{h } } ) ^{2 } ] } \\biggr ] .}\\end{aligned}\\ ] ]    we use the reverse inequality exactly in the same way , we then combine both inequality by a union bound argument and obtain the following result . for any @xmath55 , with @xmath13 probability",
    "at least @xmath57 we have , for any @xmath217 : @xmath529 , \\end{aligned}\\ ] ] remember that : @xmath530^{2 } \\bigr\\}.\\ ] ]    we now give a new lemma .",
    "let us assume that @xmath4 is such that , for any @xmath217 : @xmath531 this is for example the case if @xmath532 is subgaussian , with any @xmath533 and @xmath534 \\biggr\\}.\\ ] ] then we have , for any @xmath535 : @xmath536    we have : @xmath537 now , let use choose : @xmath538 and we obtain the lemma .    as a consequence , using a union bound argument , we have , for any @xmath535 , with probability at least @xmath57 , for any @xmath217 : @xmath539    by plugging the lemma into eq .",
    "( [ ancientheorem ] ) we obtain the theorem .",
    "actually , the proof is quite direct now : instead of using the techniques given in the section devoted to the inductive case , we use a result valid in the transductive case and integrate it with respect to the test sample .",
    "this idea is quite classical in learning theory , and was actually one of the reason for the introduction of the transductive setting ( see @xcite for example ) .",
    "there are several ways to perform this integration ( see for example @xcite ) , here we choose to apply a result obtained by panchenko @xcite that gives a particularly simple result here .",
    "let us assume that we have i.i.d .",
    "variables @xmath540 ( with distribution @xmath4 and values in @xmath289 ) and an independent copy @xmath541 of @xmath542 .",
    "let @xmath543 for @xmath544 be three measurables functions taking values in @xmath289 , and @xmath545 .",
    "let us assume that we know two constants @xmath546 and @xmath547 such that , for any @xmath548 : @xmath549 \\leq a\\exp(-au ) .\\ ] ] then , for any @xmath548 : @xmath550 \\geq p^{\\otimes2n } \\bigl[\\xi_{2}\\bigl(t , t'\\bigr)|t \\bigr]+",
    "\\sqrt{p^{\\otimes2n } \\bigl[\\xi_{3}\\bigl(t , t'\\bigr)|t \\bigr]u } \\bigr\\ } \\leq a\\exp(1-au ) .\\ ] ]    proof of theorem [ lastth ] a simple application of the first inequality of lemma [ learninglemma ] ( given as a tool for the proof of the transductive results ) with @xmath55 , any @xmath58 , @xmath551 , @xmath552 and : @xmath553 leads us to the following bound , for any @xmath29 : @xmath554 } { \\sqrt{({1}/{n})\\sum_{i=1}^{2n}\\theta_{k}(x_{i})^{2}y_{i}^{2}}}-2\\eta\\biggr ] \\leq\\exp(-\\eta),\\ ] ] or : @xmath555 \\geq \\sqrt{\\frac{4\\eta}{n^{2}}\\sum_{i=1}^{2n}\\theta_{k}(x_{i})^{2}y_{i}^{2 } } \\biggr ] \\leq\\exp(-\\eta ) = \\frac{\\varepsilon}{2k\\exp(1)}.\\ ] ] we now apply panchenko s lemma with : @xmath556 and @xmath557 .",
    "we obtain : @xmath558 \\bigr ] \\geq \\sqrt{\\frac{4\\eta}{n^{2}}\\sum_{i=1}^{n } \\bigl[\\theta _ { k}(x_{i})^{2}y_{i}^{2}+p \\bigl[\\theta_{k}(x)^{2}y^{2 } \\bigr ] \\bigr ] } \\biggr ] \\leq\\exp(1-\\eta ) = \\frac{\\varepsilon}{2k}.\\end{aligned}\\ ] ] remark finally that : @xmath559 \\leq p \\bigl[\\theta_{k}(x)^{2 } \\bigr]\\bigl(b^{2}+\\sigma^{2}\\bigr).\\ ] ] we proceed exactly in the same way with the reverse inequalities for any @xmath29 and combine the obtained @xmath560 inequalities to obtain the result : @xmath561 \\big| } \\\\ \\lefteqn{\\quad \\geq \\sqrt{\\frac{4 + 4\\log ( { 2m}/{\\varepsilon})}{n^{2}}\\sum_{i=1}^{n } \\bigl\\{\\theta _ { k}(x_{i})^{2}y_{i}^{2}+p \\bigl[\\theta_{k}(x)^{2 } \\bigr]\\bigl(b^{2}+\\sigma^{2}\\bigr ) \\bigr\\ } } \\biggr\\ } } \\\\",
    "\\lefteqn{\\qquad = p^{\\otimes2n } \\biggl\\ { \\exists k\\in\\{1,\\ldots , m\\ } , \\frac{1}{n}\\sum_{i=1}^{n } \\big|\\theta_{k}(x_{i})y_{i}-p \\bigl[\\theta_{k}(x)y \\bigr ] \\big| } \\\\",
    "\\lefteqn{\\quad \\geq \\sqrt{\\frac{4 + 4\\log ( { 2m}/{\\varepsilon})}{n^{2}}\\sum_{i=1}^{n } \\bigl\\{\\theta _ { k}(x_{i})^{2}y_{i}^{2}+p \\bigl[\\theta_{k}(x)^{2 } \\bigr]\\bigl(b^{2}+\\sigma^{2}\\bigr ) \\bigr\\ } } \\biggr\\ } \\leq\\varepsilon}\\end{aligned}\\ ] ] that ends the proof .",
    "proof of theorem [ speed ] let us begin the proof with a general @xmath43 and @xmath74 , the reason of the choice @xmath44 and @xmath319 will become clear .",
    "let us also call @xmath562 the event satisfied with probability at least @xmath57 in theorem [ lastth ] .",
    "we have : @xmath563 = p^{\\otimes n } \\bigl[1_{\\mathcal{e}(\\varepsilon ) } \\big\\|\\varpi_{p}^{\\mathcal{f},m}\\hat { \\theta}-f \\big\\|_{p}^{2 } \\bigr ] + p^{\\otimes n } \\bigl [ ( 1 - 1_{\\mathcal{e}(\\varepsilon ) } ) \\big\\|\\varpi_{p}^{\\mathcal { f},m}\\hat{\\theta}-f \\big\\|_{p}^{2 } \\bigr].\\ ] ] first of all , it is obvious that : @xmath564 & \\leq & 2 p^{\\otimes n } \\bigl [ ( 1 - 1_{\\mathcal{e}(\\varepsilon ) } ) \\bigl ( \\big\\|\\varpi_{p}^{\\mathcal{f},m}\\hat{\\theta } \\big\\|_{p}^{2}+ \\|f \\|_{p}^{2 } \\bigr ) \\bigr ] \\\\ & \\leq & 2 \\varepsilon \\bigl(b^{2}m + b^{2 } \\bigr ) = 2\\varepsilon(m+1)b^{2}.\\end{aligned}\\ ] ] for the other term , just remark that , for any @xmath565 : @xmath566}{n } \\biggl [ \\frac{1}{n}\\sum_{i=1}^{n}\\theta_{k}(x_{i})^{2}y_{i}^{2 } + b^{2 } + \\sigma^{2 } \\biggr ] + \\|\\overline{\\theta}_{m'}-f \\|_{p}^{2}.\\end{aligned}\\ ] ] this is where theorem [ propalgo ] has been used as an oracle inequality : the estimator that we have , with @xmath567 , is better than the one with the `` good choice '' @xmath568 .",
    "we also have : @xmath569 & \\leq & p^{\\otimes n } \\biggl [ \\sum_{k=1}^{m ' } \\frac{4 [ 1+\\log ( { 2m}/{\\varepsilon})]}{n } \\biggl [ \\frac{1}{n}\\sum_{i=1}^{n}\\theta_{k}(x_{i})^{2}y_{i}^{2 } + b^{2 } + \\sigma^{2 } \\biggr ] \\biggr ] + \\bigl(m'\\bigr)^{-2\\beta}c \\\\ & \\leq & m'\\frac{8 [ 1+\\log ( 2m/\\varepsilon ) ] } { n } \\bigl[b^{2 } + \\sigma^{2 } \\bigr].\\end{aligned}\\ ] ] so finally , we obtain , for any @xmath565 : @xmath563 \\leq m ' \\frac{8 [ 1+\\log ( 2m/\\varepsilon ) ] } { n } \\bigl[b^{2 } + \\sigma^{2 } \\bigr ] + \\bigl(m'\\bigr)^{-2\\beta}c + 2\\varepsilon(m+1)b^{2}.\\ ] ] the choice of : @xmath570 leads to a first term of order @xmath571 and a second term of order @xmath572 .",
    "the choice of @xmath44 and @xmath319 gives a first and a second term of the desired order @xmath573 while keeping the third term at order @xmath574 .",
    "this proves the theorem .",
    "proof of theorem [ rate2 ] here again let us write @xmath562 the event satisfied with probability at least @xmath57 in theorem  [ lastth ] .",
    "we have : @xmath575 = p^{\\otimes n } \\bigl[1_{\\mathcal{e}(\\varepsilon ) } \\big\\|\\varpi_{p}^{\\mathcal{f},m}\\hat{\\theta}-f \\big\\|_{p}^{2 } \\bigr ] + p^{\\otimes n } \\bigl [ ( 1 - 1_{\\mathcal{e}(\\varepsilon ) } ) \\big\\|\\varpi_{p}^{\\mathcal{f},m}\\hat{\\theta}-f \\big\\|_{p}^{2 } \\bigr].\\ ] ] for the first term we still have : @xmath576 for the second term , let us write the expansion of @xmath27 into our wavelet basis : @xmath577 and @xmath578 the estimator @xmath317 .",
    "let us put @xmath579 .",
    "@xmath580 for any @xmath581 , as soon as @xmath562 is satisfied ( here again we used theorem [ propalgo ] as an oracle inequality ) .",
    "now , we follow the technique used in @xcite and @xcite ( see also the end of the third chapter in @xcite ) . as soon as @xmath562",
    "is satisfied we have : @xmath582 by hlder s inequality we have , as soon as @xmath583 : @xmath584^{{2}/{(1 + 2s ) } } \\leq \\|f\\|_{s , p , q}^{{2}/{(1 + 2s ) } } j^ { ( 1-{2}/{((1 + 2s)q ) } ) _ { + } } , \\ ] ] let us put @xmath585 .",
    "finally , note that we have , for @xmath586 : @xmath587 as @xmath588 we have : @xmath589 for some @xmath590 and so : @xmath591 for some @xmath592 . in the case",
    "where @xmath593 we use ( see @xcite , for @xmath594 ) : @xmath595 to obtain : @xmath596 so we have : @xmath597 let us remember that : @xmath598 and that @xmath319 , and take : @xmath599 to obtain the desired rate of convergence .",
    "i would like to thank my phd advisor , professor olivier catoni , for his constant help , and the anonymous referee for very useful comments and remarks .",
    "b.  e. boser , i.  m. guyon and v.  n. vapnik .",
    "a training algorithm for optimal margin classifiers . in _ proceedings of the 5th annual acm workshop on computational learning theory _ , pp .",
    "144152 , acm , 1992 .",
    "a.  juditsky , p.  rigollet and a.  tsybakov .",
    "mirror averaging , aggregation and model selection . in _",
    "meeting on statistical and probabilistic methods of model selection _ , pp .",
    "oberwolfach reports , 2005 ."
  ],
  "abstract_text": [
    "<S> this paper presents a new algorithm to perform regression estimation , in both the inductive and transductive setting . </S>",
    "<S> the estimator is defined as a linear combination of functions in a given dictionary . </S>",
    "<S> coefficients of the combinations are computed sequentially using projection on some simple sets . </S>",
    "<S> these sets are defined as confidence regions provided by a deviation ( pac ) inequality on an estimator in one - dimensional models . </S>",
    "<S> we prove that every projection the algorithm actually improves the performance of the estimator . </S>",
    "<S> we give all the estimators and results at first in the inductive case , where the algorithm requires the knowledge of the distribution of the design , and then in the transductive case , which seems a more natural application for this algorithm as we do not need particular information on the distribution of the design in this case . </S>",
    "<S> we finally show a connection with oracle inequalities , making us able to prove that the estimator reaches minimax rates of convergence in sobolev and besov spaces . </S>"
  ]
}