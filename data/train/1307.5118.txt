{
  "article_text": [
    "_ reinforcement learning",
    "_ ( rl ) is a framework to let an agent learn an optimal control policy in an unknown environment so that expected future rewards are maximized @xcite .",
    "the rl methods developed so far can be categorized into two types : _ policy iteration _ where policies are learned based on value function approximation @xcite and _ policy search _ where policies are learned directly to maximize expected future rewards @xcite .",
    "a value function represents expected future rewards as a function of a state or a state and an action . in the policy iteration framework , approximation of the value function for the current policy and improvement of the policy based on the learned value function are iteratively performed until an optimal policy is found .",
    "thus , accurately approximating the value function is a challenge in the value function based approach .",
    "so far , various machine learning techniques have been employed for better value function approximation , such as least - squares approximation @xcite , manifold learning @xcite , efficient sample reuse @xcite , active learning @xcite , and robust learning @xcite .    however , because policy functions are learned indirectly via value functions in policy iteration , improving the quality of value function approximation does not necessarily yield a better policy function .",
    "furthermore , because a small change in value functions can cause a big change in policy functions , it is not safe to use the value function based approach for controlling expensive dynamic systems such as a humanoid robot .",
    "another weakness of the value function approach is that it is difficult to handle continuous actions because a maximizer of the value function with respect to an action needs to be found for policy improvement .    on the other hand , in the policy search approach ,",
    "policy functions are determined so that expected future rewards are directly maximized .",
    "a popular policy search method is to update policy functions via gradient ascent .",
    "however , a classic policy gradient method called reinforce @xcite tends to produce gradient estimates with large variance , which results in unreliable policy improvement @xcite .",
    "more theoretically , it was shown that the variance of policy gradients can be proportional to the length of an agent s trajectory , due to the stochasticity of policies @xcite .",
    "this can be a critical limitation in rl problems with long trajectories .    to cope with this problem , a novel policy gradient method called _ policy gradients with parameter - based exploration _",
    "( pgpe ) was proposed @xcite . in pgpe ,",
    "deterministic policies are used to suppress irrelevant randomness and useful stochasticity is introduced by drawing policy parameters from a prior distribution . then , instead of policy parameters , hyper - parameters included in the prior distribution are learned from data . thanks to this prior - based formulation ,",
    "the variance of gradient estimates in pgpe is independent of the length of an agent s trajectory @xcite .",
    "however , pgpe still suffers from an instability problem in small sample cases . to further improve the practical performance of pgpe , an efficient sample reuse method called _ importance - weighted pgpe _ ( iw - pgpe ) was proposed recently and demonstrated to achieve the state - of - the - art performance @xcite .",
    "the rl methods reviewed above are categorized into the _ model - free _ approach , where policies are learned without explicitly modeling the unknown environment ( i.e. , the transition probability of the agent in the environment ) . on the other hand ,",
    "an alternative approach called the _ model - based _ approach explicitly models the environment in advance and uses the learned environment model for policy learning @xcite . in the model - based approach",
    ", no additional sampling cost is necessary to generate artificial samples from the learned environment model .",
    "the model - based approach is particularly advantageous in the policy search scenario .",
    "for example , given a fixed budget for data collection , iw - pgpe requires us to determine the _ sampling schedule _ in advance .",
    "more specifically , we need to decide , e.g. , whether we gather many samples in the beginning or only a small batch of samples are collected for a longer period . however , optimizing the sampling schedule in advance is not possible without strong prior knowledge .",
    "thus , we need to just blindly design the sampling schedule in practice , which can cause significant performance degradation . on the other hand",
    ", the model - based approach does not suffer from this problem because we can draw as many trajectory samples as we want from the learned transition model without additional sampling costs .",
    "another advantage of the model - based approach lies in _ baseline subtraction_. in the gradient - based policy search methods such as reinforce and pgpe , subtraction of a baseline from a gradient estimate is a vital technique to reduce the estimation variance of policy gradients @xcite .",
    "if the baseline is estimated from samples that are statistically independent of samples used for the estimation of policy gradients , variance reduction can be carried out without increasing the estimation bias . however ,",
    "such independent samples are not available in practice ( if available , they should be used for policy gradient estimation ) , and thus variance reduction by baseline subtraction is practically performed at the expense of bias increase . on the other hand , in the model - based scenario , we can draw as many trajectory samples as we want from the learned transition model without additional sampling costs . therefore , two statistically independent sets of samples can be generated and they can be separately used for policy gradient estimation and baseline estimation .",
    "if the unknown environment is accurately approximated , the model - based approach can fully enjoy all the above advantages .",
    "however , accurately estimating the transition model from a limited amount of trajectory data in multi - dimensional continuous state and action spaces is highly challenging .",
    "although the model - based method that does not require an accurate transition model was developed @xcite , it is only applicable to deterministic environments , which significantly limits its range of applications in practice . on the other hand , a recently proposed model - based policy search method called",
    "pilco @xcite learns a probabilistic transition model by the gaussian process ( gp ) @xcite , and explicitly incorporates long - term model uncertainty",
    ". however , pilco requires states and actions to follow gaussian distributions and the reward function to be a particular exponential form to ensure that the policy evaluation is performed in a closed form and policy gradients are computed analytically for policy improvement .",
    "these strong requirements make pilco practically restrictive . to overcome such limitations of existing approaches ,",
    "we propose a highly practical policy - search algorithm by extending the model - free pgpe method to the model - based scenario . in the proposed model - based pgpe ( m - pgpe ) method",
    ", the transition model is learned by the state - of - the - art non - parametric conditional density estimator called _ least - squares conditional density estimation _ ( lscde )",
    "@xcite , which has various superior properties : it can directly handle multi - dimensional inputs and outputs , it was proved to achieve the optimal convergence rate @xcite , it has high numerical stability @xcite , it is robust against outliers @xcite , its solution can be analytically and efficiently computed just by solving a system of linear equations @xcite , and generating samples from the learned conditional density is straightforward . through experiments ,",
    "we demonstrate that the proposed m - pgpe method is a promising approach .",
    "the rest of this paper is structured as follows . in section  [ sec : mf ] , we formulate the rl problem and review model - free rl methods including pgpe .",
    "we then propose the model - based pgpe method in section  [ sec : mb ] , and experimentally demonstrate its usefulness in section  [ sec : experiments ] .",
    "finally , we conclude in section  [ sec : conclusion ] .",
    "in this section , we first formulate our rl problem and review existing model - free policy search methods .",
    "let us consider a markov decision problem consisting of the following elements :    * @xmath0 : a set of continuous states .",
    "* @xmath1 : a set of continuous actions . * @xmath2 : the ( unknown ) probability density of initial states .",
    "* @xmath3 : the ( unknown ) conditional probability density of visiting state @xmath4 from state @xmath5 by action @xmath6 .",
    "* @xmath7 : the immediate reward function for the transition from @xmath5 to @xmath4 by @xmath6 .",
    "let @xmath8 be a policy of an agent parameterized by @xmath9 , which is the conditional probability density of taking action @xmath6 at state @xmath5 .",
    "let @xmath10\\end{aligned}\\ ] ] be a history , which is a sequence of states and actions with finite length @xmath11 generated as follows : first , the initial state @xmath12 is determined following the initial - state probability density @xmath2 .",
    "then action @xmath13 is chosen following policy @xmath8 , and next state @xmath14 is determined following the transition probability density @xmath3 . this process is repeated @xmath11 times .",
    "let @xmath15 be the return for history @xmath16 , which is the discounted sum of future rewards the agent can obtain : @xmath17 where @xmath18 $ ] is a discount factor .",
    "the expected return is given by @xmath19 where @xmath20 is the probability density of observing history @xmath16 : @xmath21    the goal of rl is to find optimal policy parameter @xmath22 that maximizes the expected return @xmath23 : @xmath24      reinforce @xcite is a classic method for learning the policy parameter @xmath9 via gradient ascent : @xmath25 where @xmath26 denotes the learning rate and @xmath27 denotes the gradient of @xmath23 with respect to @xmath9 .    the gradient @xmath27 can be expressed as @xmath28 where we used @xmath29    in the above expression , the probability density of histories , @xmath20 , is unknown .",
    "suppose that we are given @xmath30 roll - out samples @xmath31 for the current policy , where @xmath32.\\end{aligned}\\ ] ] then the expectation over @xmath20 can be approximated by the empirical average over the samples @xmath31 , i.e. , an empirical approximation of the gradient @xmath27 is given by @xmath33 it is known @xcite that the variance of the above gradient estimator can be reduced by subtracting the baseline @xmath34 : @xmath35 where @xmath36    let us consider the following gaussian policy model with policy parameter @xmath37 : @xmath38 where @xmath39 denotes the transpose , @xmath40 is the gaussian mean , @xmath41 is the gaussian standard deviation , and @xmath42 is the basis function vector . then the policy gradients are explicitly expressed as @xmath43    reinforce is a simple policy - search algorithm that directly updates policies to increase the expected return . however , gradient estimates tend to have large variance even if it is combined with variance reduction by baseline subtraction . for this reason ,",
    "policy update by reinforce tends to be unreliable @xcite . in particular",
    ", the variance of gradient estimates in reinforce can be proportional to the length of the history , @xmath11 , due to the stochasticity of policies @xcite .",
    "this can be a critical limitation when the history is long .      to overcome the above limitation of reinforce , a novel policy - search method called _ policy gradients with parameter - based exploration _",
    "( pgpe ) was proposed recently @xcite . in pgpe , a deterministic policy ( such as the linear policy )",
    "is adopted , and the stochasticity for exploration is introduced by drawing the policy parameter @xmath9 from a prior distribution @xmath44 with hyper - parameter @xmath45 . thanks to this per - trajectory formulation , the variance of gradient estimates can be drastically reduced .    in the pgpe formulation ,",
    "the expected return is represented as a function of the hyper - parameter @xmath45 : @xmath46 differentiating this with respect to @xmath45 , we have @xmath47    because of the per - trajectory formulation , roll - out samples in the pgpe framework are accompanied with policy parameters , i.e. , @xmath48 . based on these paired samples , an empirical estimator of the above gradient ( with baseline subtraction )",
    "is given as follows @xcite : @xmath49 where @xmath50    let us employ the linear deterministic policy , i.e. , action @xmath6 is chosen as @xmath51 for some basis function @xmath52 .",
    "the parameter vector @xmath9 is drawn from the gaussian prior distribution with hyper - parameter @xmath53 . here",
    "@xmath54 denotes the gaussian mean vector and @xmath55 denotes the vector consisting of the gaussian standard deviation in each element : @xmath56 where @xmath57 , @xmath58 , @xmath59 , and @xmath60 are the @xmath61-th elements of @xmath9 , @xmath45 , @xmath54 , and @xmath55 , respectively .",
    "then the derivatives of @xmath62 with respect to @xmath59 and @xmath60 are given as follows : @xmath63      a popular idea to further improve the performance of rl methods is to reuse previously collected samples @xcite .",
    "such a sample - reuse strategy is particularly useful when data sampling costs is high ( e.g. , robot control ) .",
    "_ importance - weighted _ pgpe ( iw - pgpe ) @xcite combines the sample - reuse idea with pgpe .",
    "technically , iw - pgpe can be regarded as an off - policy extension of pgpe , where data collecting policies are different from the current policy . in the pgpe formulation",
    ", such a off - policy scenario can be regarded as the situation where data collecting policies and the current policy are drawn from different prior distributions ( more specifically , different hyper - parameters ) .",
    "let @xmath45 be the hyper - parameter for the current policy and @xmath64 be the hyper - parameter for a data collecting policy .",
    "let us denote data samples collected with hyper - parameter @xmath64 as @xmath65 .",
    "when the data collecting policy is different from the current policy , _ importance sampling _ is a useful technique to correct the estimation bias caused by differing distributions @xcite .",
    "more specifically , the gradient is estimated as @xmath66 where @xmath67 is the _ importance weight _ defined as @xmath68 and @xmath34 is the baseline given by @xmath69 through experiments , the iw - pgpe method was demonstrated to be the best performing algorithm in model - free rl approaches @xcite .",
    "the purpose of this paper is to develop a model - based counterpart of pgpe .",
    "model - based rl first estimates the transition model and then learns a policy based on the estimated transition model .",
    "because one can draw as many trajectory samples as one wants from the learned transition model without additional sampling costs , the model - based approach can work well if the transition model is accurately estimated @xcite . in this section ,",
    "we extend pgpe to a model - based scenario .",
    "we first review an existing model estimation method based on the gaussian process ( gp ) @xcite and point out its limitations .",
    "then we propose to use the state - of - the - art conditional density estimator called _ least - squares conditional density estimation _",
    "( lscde ) @xcite in the model - based pgpe method .",
    "pgpe can be extended to a model - based scenario as follows .    1",
    ".   collect transition samples @xmath70 .",
    "2 .   obtain transition model @xmath71 by a model estimation method from @xmath70 .",
    "3 .   initialize hyper - parameter @xmath45 .",
    "4 .   draw policy parameter @xmath9 from prior distribution @xmath44 .",
    "generate many samples @xmath72 from @xmath71 and current policy @xmath8 .",
    "estimate baseline @xmath34 and gradient @xmath73 from disjoint subsets of @xmath72",
    "update hyper - parameter as @xmath74 , where @xmath26 denotes the learning rate",
    "repeat steps 47 until @xmath45 converges .",
    "below , we consider the problem of approximating the transition probability @xmath3 from samples @xmath70 , and review transition model estimation methods .      here",
    "we review a transition model estimation method based on gp .    in the gp framework ,",
    "the problem of transition probability estimation is formulated as the regression problem of predicting output @xmath4 given input @xmath5 and @xmath6 under gaussian noise : @xmath75 where @xmath76 is an unknown regression function and @xmath77 is independent gaussian noise .",
    "then , the gp estimate of the transition probability density @xmath3 for an arbitrary test input @xmath5 and @xmath6 is given by the gaussian distribution with mean and variance given by @xmath78 respectively .",
    "here , @xmath79 denotes the @xmath80-dimensional identity matrix .",
    "@xmath81 is the @xmath80-dimensional vector and @xmath82 is the @xmath83 gram matrix defined by @xmath84 @xmath85 denotes the covariance function , which is , e.g. , defined by @xmath86-[{\\boldsymbol{s}}'^\\top,{a}'])\\boldsymbol{\\theta}([{\\boldsymbol{s}}^\\top,{a}]-[{\\boldsymbol{s}}'^\\top,{a}'])^\\top\\right).\\ ] ] here , @xmath87 and @xmath88 are hyperparameters , and together with the noise variance @xmath89 , the hyperparameters are determined by evidence maximization @xcite .",
    "as shown above , the gp - based model estimation method requires the strong assumption that the transition probability density @xmath3 is gaussian .",
    "that is , gp is non - parametric as a regression method of estimating the conditional mean , it is parametric ( gaussian ) as a conditional density estimator . such a conditional gaussian assumption is highly restrictive in rl problems .      to overcome the restriction of the gp - based model estimation method",
    ", we propose to use lscde .",
    "let us model the transition probability @xmath3 by the following linear - in - parameter model : @xmath90 where @xmath91 is the @xmath80-dimensional basis function vector and @xmath92 is the @xmath80-dimensional parameter vector .",
    "if @xmath80 is too large , we may reduce the number of basis functions by only using a subset of samples as gaussian centers .",
    "we may use different gaussian widths for @xmath5 and @xmath6 if necessary .",
    "the parameter @xmath92 in the model is learned so that the following squared error is minimized : @xmath93 this can be expressed as @xmath94 where we used @xmath95 in the second term and @xmath96 because @xmath97 is constant , we only consider the first two terms from here on : @xmath98 where @xmath99 note that , for the gaussian model , the @xmath100-th element of @xmath101 can be computed analytically as @xmath102    because @xmath103 and @xmath104 included in @xmath105 contain the expectations over unknown densities @xmath106 and @xmath107 , they are approximated by sample averages .",
    "then we have @xmath108 where @xmath109    by adding an @xmath110-regularizer to @xmath111 to avoid overfitting , the lscde optimization criterion is given as @xmath112,\\end{aligned}\\ ] ] where @xmath113 @xmath114 is the regularization parameter . taking the derivative of the above objective function and equating it to zero",
    ", we can see that the solution @xmath115 can be obtained just by solving the following system of linear equations : @xmath116 where @xmath79 denotes the @xmath80-dimensional identity matrix .",
    "thus , the solution @xmath115 is given analytically as @xmath117 because conditional probability densities are non - negative by definition , we modify the solution @xmath115 as @xmath118 where @xmath119 denotes the @xmath80-dimensional zero vector and ` @xmath120 ' for vectors are applied in the element - wise manner .",
    "finally , we renormalize the solution in the test phase . more specifically , given a test input point @xmath121 , the final lscde solution is given as @xmath122 where , for the gaussian model , the denominator in eq .",
    "can be analytically computed as @xmath123 lscde was proved to achieve the optimal non - parametric convergence rate to the true conditional density in the mini - max sense @xcite , meaning that no method can outperform this simple lscde method asymptotically .",
    "model selection of the gaussian width @xmath124 and the regularization parameter @xmath113 is possible by cross - validation .",
    "a matlab@xmath125 implementation of lscde is available from    ` http://sugiyama-www.cs.titech.ac.jp/~sugi/software/lscde/ ' .",
    "in this section , we demonstrate the usefulness of the proposed method through experiments .      for illustration purposes ,",
    "let us first consider a simple continuous chain - walk task ( figure  [ fig : cwalk - view ] ) .",
    "let @xmath126,\\\\ { { \\mathcal a}}&=[-5 , 5],\\\\ { { r}(s,{a},s')}&= \\begin{cases } 1&(4<s'<6),\\\\ 0&(\\mathrm{otherwise}).\\\\ \\end{cases}\\end{aligned}\\ ] ] that is , the agent receives positive reward @xmath127 at the center of the state space .",
    "we set the episode length at @xmath128 , the discount factor at @xmath129 , and the learning rate at @xmath130 .",
    "we use the following linear - in - parameter policy model : @xmath131 where @xmath132 .        as transition dynamics , we consider two setups :    gaussian : : :    the true transition dynamics is given by @xmath133 where    @xmath134 is the gaussian noise with mean    @xmath135 and standard deviation @xmath136 .",
    "bimodal : : :    the true transition dynamics is given by @xmath137 where    @xmath134 is the gaussian noise with mean    @xmath135 and standard deviation @xmath136 , and the sign    of @xmath138 is randomly chosen with probability    @xmath139 .    we compare the following three policy search methods :    m - pgpe(lscde ) : : :    the model - based pgpe method with transition model estimated by lscde .",
    "m - pgpe(gp ) : : :    the model - based pgpe method with transition model estimated by gp .",
    "iw - pgpe : : :    the model - free pgpe method with sample reuse by importance    weighting    @xcite .",
    "below , we consider the situation where the budget for data collection is limited to @xmath140 episodic samples .",
    "when the transition model is learned in the m - pgpe methods , all @xmath140 samples are gathered randomly in the beginning at once .",
    "more specifically , the initial state @xmath141 and the action @xmath13 are chosen from the uniform distributions over @xmath0 and @xmath1 , respectively .",
    "then the next state @xmath142 and the immediate reward @xmath143 are obtained . then the action @xmath144 is chosen from the uniform distribution over @xmath1 , and the next state @xmath145 and the immediate reward @xmath146 are obtained .",
    "this process is repeated until we obtain @xmath147 .",
    "this gives a trajectory sample , and we repeat this data generation process @xmath30 times to obtain @xmath30 trajectory samples",
    ".    figure  [ fig : trans_pure ] and figure  [ fig : trans_bi ] illustrate the true transition dynamics and its estimates obtained by lscde and gp in the gaussian and bimodal cases . figure  [ fig : trans_pure ] shows that both lscde and gp can learn the entire profile of the true transition dynamics well in the gaussian case .",
    "on the other hand , figure  [ fig : trans_bi ] shows that lscde can still successfully capture the entire profile of the true transition model well even in the bimodal case , but gp fails to capture the bimodal structure .    based on the estimated transition models , we learn policies by the m - pgpe method .",
    "we generate @xmath148 artificial samples for policy gradient estimation and another @xmath148 artificial samples for baseline estimation from the learned transition model .",
    "then policy is updated based on these artificial samples .",
    "we repeat this policy update step @xmath149 times . for evaluating the return of a learned policy , we use @xmath150 additional test episodic samples which are not used for policy learning .",
    "figure  [ fig : cwalk - results - pure ] and figure  [ fig : cwalk - results - bi ] depict the average performance of learned policies over @xmath150 runs .",
    "as expected , the gp - based method performs very well in the gaussian case , but lscde still exhibits reasonably good performance . in the bimodal case ,",
    "gp performs poorly and lscde gives much better policies than gp .",
    "this illustrates the high flexibility of lscde .",
    "next , we compare the performance of m - pgpe with the model - free iw - pgpe method .    for the iw - pgpe method , we need to determine the schedule of collecting @xmath149 samples under the fixed budget scenario .",
    "first , we illustrate how the choice of sampling schedules affects the performance of iw - pgpe .",
    "figure  [ fig : cwalk - schedule - pure ] and figure  [ fig : cwalk - schedule - bi ] show expected returns averaged over @xmath150 runs under the sampling schedule that a batch of @xmath151 samples are gathered @xmath152 times for different @xmath151 values . in our implementation of iw - pgpe , policy update",
    "is performed @xmath150 times after observing each batch of @xmath151 samples , because we empirically observed that this performs better than performing policy update only once .",
    "figure  [ fig : cwalk - schedule - pure ] shows that the performance of iw - pgpe depends heavily on the sampling schedule , and gathering @xmath153 samples at once is shown to be the best choice in the gaussian case .",
    "figure  [ fig : cwalk - schedule - bi ] shows that gathering @xmath153 samples at once is also the best choice in the bimodal case .",
    "although the best sampling schedule is not accessible in practice , we use this optimal sampling schedule for iw - pgpe .",
    "figure  [ fig : cwalk - results - pure ] and figure  [ fig : cwalk - results - bi ] also include returns of iw - pgpe averaged over @xmath150 runs as functions of the sampling steps .",
    "these graphs show that iw - pgpe can improve the policies only in the beginning , because all samples are gathered at once in the beginning .",
    "the performance of iw - pgpe may be further improved if it is possible to gather more samples , but this is prohibited under the fixed budget scenario . on the other hand , return values of m - pgpe constantly increase throughout iterations , because artificial samples can be kept generated without additional sampling costs .",
    "this illustrates a potential advantage of model - based rl methods .",
    "+     runs for gaussian transition dynamics with different sampling schedules ( e.g. , @xmath154 means gathering @xmath155 samples @xmath156 times).,scaledwidth=100.0% ]        runs for gaussian transition dynamics with different sampling schedules ( e.g. , @xmath154 means gathering @xmath155 samples @xmath156 times).,scaledwidth=100.0% ]     +     runs for bimodal transition with different sampling schedules ( e.g. , @xmath154 means gathering @xmath155 samples @xmath156 times ) . ]",
    "runs for bimodal transition with different sampling schedules ( e.g. , @xmath154 means gathering @xmath155 samples @xmath156 times ) . ]      finally , we evaluate the performance of m - pgpe on a practical control problem of a simulated upper - body model of the humanoid robot _ cb - i _",
    "@xcite ( see figure  [ fig : cbi ] ) .",
    "we use its simulator for experiments ( see figure  [ fig : simulator ] ) .",
    "the goal of the control problem is to lead the end - effector of the right arm ( right hand ) to the target object .",
    "the simulator is based on the upper - body of the cb - i humanoid robot , which has 9 joints for shoulder pitch , shoulder roll , elbow pitch of the right arm , shoulder pitch , shoulder roll , elbow pitch of the left arm , waist yaw , torso roll , and torso pitch .    at each time step , the controller receives a state vector from the system and sends out an action vector .",
    "the state vector is 18-dimensional and real - valued , which corresponds to the current angle in degree and the current angular velocity for each joint .",
    "the action vector is 9-dimensional and real - valued , which corresponds to the target angle of each joint in degree .",
    "we simulate a noisy control system by perturbing action vectors with independent bimodal gaussian noise .",
    "more specifically , for each action element , we add gaussian noise with mean @xmath135 and standard deviation @xmath157 with probability @xmath158 , and gaussian noise with mean @xmath159 and standard deviation @xmath157 with probability @xmath160 .",
    "the initial posture of the robot is fixed to standing up straight with arms down .",
    "the the target object is located in front - above of the right hand which is reachable by using the controllable joints .",
    "the reward function at each time step is defined as @xmath161 where @xmath162 is the distance between the right hand and target object at time step @xmath163 , and @xmath164 is the sum of control costs for each joint . the coefficient @xmath165 is multiplied to keep the values of the two terms in the same order of magnitude . the deterministic policy model used in pgpe",
    "is defined as @xmath166 with the basis function @xmath167 .",
    "we set the episode length at @xmath168 , the discount factor at @xmath169 , and the learning rate at @xmath170 .",
    "first , we only use 2 joints among the 9 joints , i.e. , we allow only the right shoulder pitch and right elbow pitch to be controlled , while the other joints remain still at each time step ( no control signal is sent to these joints ) . therefore , the dimensionality of state vector @xmath171 and action vector @xmath172 is @xmath156 and @xmath173 , respectively . under this simplified setup ,",
    "we compare the performance of m - pgpe(lscde ) , m - pgpe(gp ) , and iw - pgpe .",
    "we suppose that the budget for data collection is limited to @xmath174 episodic samples . for the m - pgpe methods ,",
    "all samples are collected at first using the uniformly random initial states and policy .",
    "more specifically , the initial state is chosen from the uniform distributions over @xmath175 . at each time step , the @xmath61-th element of action vector @xmath176 is chosen from the uniform distribution on @xmath177 $ ] . in total , we have 5000 transition samples for model estimation .",
    "then , we generate 1000 artificial samples for policy gradient estimation and another 1000 artificial samples for baseline estimation from the learned transition model , and update the control policy based on these artificial samples . for the iw - pgpe method",
    ", we performed preliminary experiments to determine the optimal sampling schedule ( figure  [ fig : simu_schedule2j ] ) , showing that collecting @xmath155 samples @xmath178 times yields the highest average return .",
    "we use this sampling schedule for performance comparison with the m - pgpe methods .",
    "returns obtained by each method averaged over 10 runs are plotted in figure  [ fig : simu_result2j ] , showing that m - pgpe(lscde ) tends to outperform both m - pgpe(gp ) and iw - pgpe .",
    "figure  [ fig : simu_traject2j ] illustrates an example of the reaching motion with 2-joints obtained by m - pgpe(lscde ) at the @xmath179th iteration policy .",
    "this shows that the learned policy successfully leads the right hand to the target object within only @xmath180 steps in this noisy control system .    ) .",
    "note that policy update is performed 100 times after observing each batch of samples , which we confirmed to perform well .",
    "the iw - pgpe curve is elongated to have the same horizontal scale as others . ,",
    "scaledwidth=60.0% ]    ) .",
    "note that policy update is performed 100 times after observing each batch of samples , which we confirmed to perform well .",
    "the iw - pgpe curve is elongated to have the same horizontal scale as others .",
    ", scaledwidth=60.0% ]      finally , we evaluate the performance of the proposed method on the reaching task with 9 joints , i.e. , all joints are allowed to move . in this experiment",
    ", we compare learning performance between m - pgpe(lscde ) and iw - pgpe .",
    "we do not include m - pgpe(gp ) since it is outperformed by m - pgpe(lscde ) on the previous 2-joints experiments , and furthermore the gp - based method requires an enormous amount of computation time .",
    "the experimental setup is essentially the same as the 2-joints experiments , but we have a budget for gathering @xmath181 samples for this complex and high - dimensional task .",
    "the position of the target object is moved to far left , which is not reachable by using just 2-joints .",
    "thus , the robot is required to move other joints to reach the object with right hand .",
    "we randomly choose 5000 samples for gaussian centers for m - pgpe(lscde ) .",
    "the sampling schedule for iw - pgpe was set to 1000 samples at once , which is the best sampling schedule according to figure  [ fig : simu_schedule9j ] .",
    "the returns obtained by m - pgpe(lscde ) and iw - pgpe averaged over 30 runs are plotted in figure  [ fig : simu_result9j ] , showing that m - pgpe(lscde ) tends to outperform the state - of - the - art iw - pgpe method in this challenging robot control task .",
    "figure  [ fig : simu - traject9j ] shows a typical example of the reaching motion with 9 joints obtained by m - pgpe(lscde ) at the 1000th iteration .",
    "the images show that the policy learned by m - pgpe(lscde ) leads the right hand to the distant object successfully within 14 steps",
    ".     means gathering @xmath182 samples 10 times ) .",
    ", scaledwidth=60.0% ]    ) .",
    "note that policy update is performed 100 times after observing each batch of samples .",
    "the iw - pgpe curve is elongated to have the same horizontal scale as m - pgpe(lscde ) . ,",
    "scaledwidth=60.0% ]    overall , the proposed m - pgpe(lscde ) method is shown to be promising in the noisy and high - dimensional humanoid robot arm reaching task .",
    "we extended the model - free pgpe method to a model - based scenario , and proposed to combine it with a model estimator called lscde . under the fixed sampling budget , appropriately designing a sampling schedule is critical for the model - free iw - pgpe method , while this is not a problem for the proposed model - based pgpe method . through experiments , we confirmed that gp - based model estimation is not as flexible as the lscde - based method when the transition model is not gaussian , and the proposed model - based pgpe based on lscde was overall demonstrated to be promising .",
    "vt was supported by the jasso scholarship , tz was supported by the mext scholarship , jm was supported by mext kakenhi 23120004 , and ms was supported by the first project .",
    "s.  kakade .",
    "a natural policy gradient . in t.",
    "g. dietterich , s.  becker , and z.  ghahramani , editors , _ advances in neural information processing systems 14 _ , pages 15311538 , cambridge , ma , 2002 . mit press .",
    "r.  s. sutton , d.  mcallester , s.  singh , and y.  mansour .",
    "policy gradient methods for reinforcement learning with function approximation . in s.a .",
    "solla , t.k .",
    "leen , and k .-",
    "mller , editors , _ advances in neural information processing systems 12 _ , pages 10571063 . mit press , 2000 ."
  ],
  "abstract_text": [
    "<S> the goal of reinforcement learning ( rl ) is to let an agent learn an optimal control policy in an unknown environment so that future expected rewards are maximized . </S>",
    "<S> the model - free rl approach directly learns the policy based on data samples . </S>",
    "<S> although using many samples tends to improve the accuracy of policy learning , collecting a large number of samples is often expensive in practice . on the other hand , the model - based rl </S>",
    "<S> approach first estimates the transition model of the environment and then learns the policy based on the estimated transition model . </S>",
    "<S> thus , if the transition model is accurately learned from a small amount of data , the model - based approach can perform better than the model - free approach . in this paper </S>",
    "<S> , we propose a novel model - based rl method by combining a recently proposed model - free policy search method called _ policy gradients with parameter - based exploration _ and the state - of - the - art transition model estimator called _ least - squares conditional density estimation_. through experiments , we demonstrate the practical usefulness of the proposed method . </S>"
  ]
}