{
  "article_text": [
    "the problem of estimating an unknown signal from noisy linear observations is fundamental in signal processing .",
    "the estimation task is often formulated as the linear inverse problem @xmath0 where the goal is to compute the unknown signal @xmath1 from the noisy measurements @xmath2 . here , the matrix @xmath3 models the response of the acquisition device and the vector @xmath4 represents the measurement noise , which is often assumed to be i.i.d .  gaussian .",
    "when the problem   is ill - posed , the standard approach is to rely on the regularized least - squares estimator @xmath5 where the functional @xmath6 is a regularizer that promotes solutions with desirable properties such as transform - domain sparsity or positivity .",
    "one of the most widely used regularizers in imaging is the total variation ( tv ) , whose anisotropic variant can be defined as @xmath7\\|_{\\ell_1 } = \\lambda \\sum_{n=1}^n \\sum_{d = 1}^d |[\\dbf_d \\xbf]_n|,\\ ] ] where @xmath8 is the discrete gradient operator , @xmath9 is a parameter controlling amount of regularization , and @xmath10 is the number of dimensions in the signal .",
    "the matrix @xmath11 denotes the finite difference operation along the dimension @xmath12 with appropriate boundary conditions ( periodization , etc . ) .",
    "the tv prior has been originally introduced by rudin _",
    "et al . _",
    "@xcite as a regularization approach capable of removing noise , while preserving image edges .",
    "it is often interpreted as a sparsity - promoting @xmath13-penalty on the image gradient .",
    "tv regularization has proven to be successful in a wide range of applications in the context of sparse recovery of images from incomplete or corrupted measurements  @xcite .",
    "the minimization   with the tv regularization is a nontrivial optimization task .",
    "the challenging aspects are the non - smooth nature of the regularization term   and the massive quantity of data that typically needs to be processed .",
    "proximal gradient methods  @xcite such as iterative shrinkage / thresholding algorithm ( ista )  @xcite or alternating direction method of multipliers ( admm )  @xcite are standard approaches to circumvent the non - smoothness of the tv regularizer .    for the optimization problem",
    ", ista can be written as    [ eq : ista ] @xmath14    where @xmath15 is a step - size that can be determined a priori to ensure convergence  @xcite .",
    "iteration   combines the gradient - descent step   with a proximal operation   defined as @xmath16 the proximal operator corresponds to the regularized solution of the denoising problem where @xmath17 is an identity . because of its simplicity , ista and its accelerated variants are among the methods of choice for solving practical linear inverse problems  @xcite .",
    "nonetheless , ista  based optimization of tv is complicated by the fact that the corresponding proximal operator does not admit a closed form solution .",
    "practical implementations rely on computational solutions that require an additional nested optimization algorithm for evaluating the tv proximal  @xcite .",
    "this typically leads to a prohibitively slow reconstruction when dealing with very large scale imaging problems such as the ones in 3d microscopy  @xcite .    in this paper",
    ", we propose a novel approach for solving tv  based imaging problems that requires no nested iterations .",
    "this is achieved by substituting the proximal of tv with an alternative that amounts to evaluating several simpler proximal operators .",
    "one of our major contributions is the proof that the approach can achieve the true tv solution with arbitrarily high precision .",
    "we believe that the results presented in this paper are useful to practitioners working with very large scale problems that are common in 3d imaging , where the bottleneck is often in the evaluation of the tv proximal .",
    "in this section , we present our main results .",
    "we start by introducing the proposed approach and then follow up by analyzing its convergence .",
    "we turn our attention to a more general optimization problem @xmath18 where the cost functional is of the following form @xmath19 the precise connection between   and tv - regularized cost functional will be discussed shortly .",
    "we assume that the data - fidelity term @xmath20 is convex and differentiable with a lipschitz continuous gradient .",
    "this means that there exists a constant @xmath21 such that , for all @xmath22 , @xmath23 .",
    "we also assume that each @xmath24 is a continuous , convex function that is possibly nondifferentiable and that the optimal value @xmath25 is finite and attained at @xmath26 .",
    "we consider parallel proximal algorithms that have the following form    [ eq : parallelproximalalgorithm ] @xmath27    where @xmath28 is the proximal operator associated with @xmath29 .",
    "we are specifically interested in the case where the proximals @xmath28 have a closed form , in which case they are preferable to the computation of the full proximal @xmath30 .",
    "we now establish a connection between   and tv - regularized cost . define a linear transform @xmath31 that consists of two sub - operators : the averaging operator @xmath32 and the discrete gradient @xmath33 as in  , both normlized by @xmath34 .",
    "the averaging operator consists of @xmath10 matrices @xmath35 that denote the pairwise averaging along the dimension @xmath12 .",
    "accordingly , the operator @xmath36 is a union of scaled and shifted discrete haar wavelet and scaling functions along each dimension  @xcite .",
    "since we consider all possible shifts along each dimension the transform is redundant and can be interpreted as the union of @xmath37 , scaled , orthogonal tranforms @xmath38 the transform @xmath36 and its pseudo - inverse @xmath39\\ ] ] satisfy the following two properties of parseval frames  @xcite @xmath40 and @xmath41 one can thus express the tv regularizer as the following sum @xmath42_n|,\\ ] ] where @xmath43 $ ] is the set of all detail coefficients of the transform @xmath44 . then , the proposed parallel proximal algorithm for tv can be expressed as follows    [ eq : partv ] @xmath45    where @xmath46 is the component - wise shrinkage function @xmath47 which is applied only on scaled differences @xmath48 .",
    "the algorithm in   is closely related to a technique called cycle spinning  @xcite that is commonly used for improving the performance of wavelet - domain denoising . in particular , when @xmath49 and @xmath50 , for all @xmath51 , the algorithm yields the solution @xmath52 which can be interpreted as the traditional cycle spinning algorithm restricted to the haar wavelet - transform . in the context of image denoising , the connections between tv and cycle - spinning were originally established in  @xcite .",
    "the convergence results in this section assume that the gradient of @xmath20 and subgradients of @xmath24 are bounded , i.e. , there exists @xmath53 such that for all @xmath54 and @xmath55 , @xmath56 and @xmath57 .",
    "the following proposition is crucial for establishing the convergence of the parallel proximal algorithm .",
    "[ prop : prop1 ] consider the cost function   and the algorithm  .",
    "then , for all @xmath51 , and for any @xmath1 , we have @xmath58    _ proof : _ see appendix .",
    "proposition  [ prop : prop1 ] allows us to develop various types of convergence results .",
    "for example , if @xmath26 is the optimal point and if we pick a sufficiently small step @xmath59 , then the @xmath60 will be closer to @xmath26 than @xmath61 .",
    "this argument can be formalized into the following proposition .",
    "[ prop : prop2 ] assume a fixed step size @xmath62 .",
    "then , we have that @xmath63    _ proof : _ see appendix .",
    "proposition  [ prop : prop2 ] states that for a constant step - size , convergence can be established to the neighborhood of the optimimum , which can be made arbitrarily close to @xmath64 by letting @xmath65 .",
    "is plotted against the iteration number for 3 distinct step - sizes @xmath66 .",
    "the plot illustrates the convergence of the fast parallel proximal algorithm to the minimizer of the tv cost functional.,width=321 ]    ; ( b ) @xmath67 ; ( c ) the true tv solution . even for @xmath68",
    "the solution of the accelerated parallel proximal algorithm is visually and quantitatively close to the true tv result.,width=321 ]    in this section , we empirically illustrate that our results hold more generally than suggested by proposition  [ prop : prop2 ] .",
    "specifically , we consider the accelerated parallel proximal algorithm based on fista  @xcite    [ eq : fastparallelproximalalgorithm ] @xmath69    with @xmath70 , @xmath71 , and @xmath72 .",
    "method   preserves the simplicity of the ista approach   but provides a significantly better rate of convergence , which enhances potential applicability of the method .",
    "we consider an estimation problem where the shepp - logan phantom of size @xmath73 is reconstructed from @xmath74 linear measurements with awgn corresponding to @xmath75 db snr .",
    "the measurement matrix is i.i.d .",
    "gaussian @xmath76_{mn } \\sim \\mathcal{n}(0 , 1/m)$ ] .",
    "figure  [ fig : fig1 ] illustrates the per - iteration gap @xmath77 , where @xmath60 is computed with the fast parallel proximal method   and @xmath78 is the tv - regularized least - squares cost . the regularization parameter @xmath79 was manually selected for the optimal snr performance of tv .",
    "we compare 3 different step - sizes @xmath68 , @xmath80 , and @xmath67 , where @xmath81 is the lipschitz constant .",
    "proposition  [ prop : prop2 ] suggests that the gap @xmath82 is proportional to the step - size and shrinks to 0 as the step - size is reduced .",
    "such behavior is clearly observed in figure  [ fig : fig1 ] , which suggests that our results potentially hold for the accelerated parallel proximal algorithm .",
    "figure  [ fig : fig2 ] compares the quality of the estimated images , for @xmath68 and @xmath67 , against the tv solution .",
    "we note that , even for @xmath68 , the solution of our algorithm is very close to the true tv result , both qualitatively and quantitatively .",
    "this implies that , while requiring no nested iterations , our parallel proximal approach can potentially approximate the solution of tv with arbitrarily accurate precision at @xmath83 convergence rate of fista .",
    "the results in this paper are most closely related to the work on tv  based imaging by beck and teboulle  @xcite .",
    "while their approach requires additional nested optimization to compute the tv proximal , we avoid this by relying on multiple simplified proximals computed in parallel .",
    "our proofs rely on several results from convex optimization that were used by bertsekas  @xcite for analyzing a different family of algorithms called incremental proximal methods .",
    "finally , two earlier papers with the author describe the relationship between cycle spinning and tv  @xcite , but concentrate on a fundamentally different families of optimization algorithms .",
    "the parallel proximal method and its accelerated version , which were presented in this paper , are beneficial in the context of tv regularized image reconstruction , especially when the computation of the tv proximal is costly .",
    "we presented a combination of theoretical and empirical evidence demonstrating that these methods can compute the tv solution at the competitive global convergence rates without resorting to expensive sub - iterations .",
    "future work will aim at extending the theoretical analysis presented here and by applying the methods to a larger class of imaging problems .",
    "we now prove the propositions in section  [ sec : theoreticalconvergence ] .",
    "the formalism used here is closely related to the analysis of incremental proximal methods that were studied by bertsekas  @xcite .",
    "related techniques were also used to analyze the convergence of recursive cycle spinning algorithm in  @xcite .",
    "we define an intermediate quantity @xmath84 .",
    "the optimality conditions for   imply that there must exist @xmath85 subgradient vectors @xmath86 such that @xmath87 this implies that @xmath88 where @xmath89 then we can write @xmath90 by using the triangle inequality and noting that all the subgradients are bounded , we can bound the last term as @xmath91 to bound the second term we proceed in two steps .",
    "we first write that @xmath92 where we used the convexity of @xmath20 , the cauchy - schwarz inequality , and the bound on the gradients . in a similar way , we can write that @xmath93 where we used the convexity of @xmath24s , the relationships   and  , as well as bounds obtained via the cauchy - schwarz inequality . by plugging  ,  , and   into   and by reorganizing the terms , we obtain the claim .      by following an approach similar to bertsekas  @xcite",
    ", we prove the result by contradiction .",
    "assume that   does not hold .",
    "then , there must exist @xmath94 such that @xmath95 let @xmath96 be such that @xmath97 and let @xmath98 be large enough so that for all @xmath99 , we have @xmath100 by combining   and  , we obtain that for all @xmath99 @xmath101 then from proposition  [ prop : prop1 ] , for all @xmath99 , @xmath102 by iterating the inequality over @xmath55 , we have for all @xmath99 , @xmath103 which can not hold for arbitrarily large @xmath55 .",
    "this completes the proof .",
    "m.  m. bronstein , a.  m. bronstein , m.  zibulevsky , and h.  azhari , `` reconstruction in diffraction ultrasound tomography using nonuniform fft , '' _ ieee trans . med .",
    "_ , vol .  21 , no .  11 , pp . 13951401 , november 2002",
    ".    m.  v. afonso , j.  m. bioucas - dias , and m.  a.  t. figueiredo , `` fast image recovery using variable splitting and constrained optimization , '' _ ieee trans .",
    "image process .",
    "_ , vol .  19 , no .  9 , pp . 23452356 , september 2010 .",
    "e.  j. cands , j.  romberg , and t.  tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ ieee trans .",
    "inf . theory _",
    "52 , no .  2 ,",
    "pp . 489509 , february 2006 .",
    "j.  p. oliveira , j.  m. bioucas - dias , and m.  a.  t. figueiredo , `` adaptive total variation image deblurring : a majorization - minimization approach , '' _ signal process .",
    "_ , vol .  89 , no .  9 , pp .",
    "16831693 , september 2009 .",
    "i.  daubechies , m.  defrise , and c.  d. mol , `` an iterative thresholding algorithm for linear inverse problems with a sparsity constraint , '' _ communications on pure and applied mathematics _ , vol .",
    "57 , no .  11 , pp . 14131457 , november 2004 .    j.  m. bioucas - dias and m.  a.  t. figueiredo , `` a new twist : two - step iterative shrinkage / thresholding algorithms for image restoration , '' _ ieee trans",
    ". image process .",
    "_ , vol .  16 , no .  12 , pp .",
    "29923004 , december 2007 .",
    "s.  boyd , n.  parikh , e.  chu , b.  peleato , and j.  eckstein , `` distributed optimization and statistical learning via the alternating direction method of multipliers , '' _ foundations and trends in machine learning _ ,",
    "vol .  3 , no .  1 , pp . 1122 , 2011 .",
    "a.  beck and m.  teboulle , `` fast gradient - based algorithm for constrained total variation image denoising and deblurring problems , '' _ ieee trans .",
    "image process .",
    "_ , vol .  18 , no .  11 , pp . 24192434 ,",
    "november 2009 .",
    "u.  s. kamilov , e.  bostan , and m.  unser , `` wavelet shrinkage with consistent cycle spinning generalizes total variation denoising , '' _ ieee signal process .",
    "_ , vol .  19 , no .  4 , pp . 187190 , april 2012 .      u.  s. kamilov , e.  bostan , and m.  unser , `` variational justification of cycle spinning for wavelet - based solutions of inverse problems , '' _ ieee signal process .",
    "_ , vol .  21 , no .  11 , pp . 13261330 , november 2014"
  ],
  "abstract_text": [
    "<S> total variation ( tv ) is a widely used regularizer for stabilizing the solution of ill - posed inverse problems . in this paper </S>",
    "<S> , we propose a novel proximal - gradient algorithm for minimizing tv regularized least - squares cost functional . </S>",
    "<S> our method replaces the standard proximal step of tv by a simpler alternative that computes several independent proximals . </S>",
    "<S> we prove that the proposed parallel proximal method converges to the tv solution , while requiring no sub - iterations . </S>",
    "<S> the results in this paper could enhance the applicability of tv for solving very large scale imaging inverse problems . </S>"
  ]
}