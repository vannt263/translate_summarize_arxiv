{
  "article_text": [
    "exploiting the structure of data is key to develop fast algorithms and , in the context of linear algebra , this principle is at the heart of algorithms for _ displacement structured matrices_. these algorithms can speed up for instance the inversion of a given matrix whenever this matrix has a structure close to that of a toeplitz , hankel , vandermonde , or cauchy matrix .",
    "the idea is to represent structured matrices succinctly by means of their _ generators _ with respect to suitable _ displacement operators _ , and to operate on this succinct data structure .",
    "* displacement operators .",
    "* let @xmath13 be a field . to measure the extent to which a matrix @xmath14 possesses some structure , it is customary to use its _ displacement rank _ , that is , the rank of its image through a _ displacement operator _  @xcite .",
    "there exist two broad classes of displacement operators : _",
    "sylvester operators _ , of the form @xmath15 and _ stein operators _ , of the form @xmath16 in both cases , @xmath17 and @xmath18 are fixed matrices in @xmath19 and @xmath20 , respectively . for any such operator ,",
    "say @xmath21 , the rank of @xmath22 is called the @xmath21-_displacement rank _ of @xmath1 , or simply its displacement rank if @xmath21 is clear from the context . loosely speaking",
    ", the matrix @xmath1 is called _ structured _ ( with respect to the operator @xmath21 ) if its @xmath21-displacement rank is small compared to its sizes @xmath23 and @xmath24 .",
    "we say that a matrix pair @xmath25 in @xmath26 is an @xmath21-_generator of length _",
    "@xmath3 of @xmath27 if it satisfies @xmath28 , with @xmath29 the transpose of @xmath30 .",
    "again , if @xmath21 is clear from the context , we shall simply say _ generator of length _ @xmath3 .",
    "provided @xmath21 is invertible and when @xmath3 is small , such a generator can play the role of a succinct data structure to represent the matrix @xmath1 .",
    "the smallest possible value for @xmath3 is the rank of @xmath22 .",
    "if @xmath1 is invertible and structured with respect to @xmath21 , then its inverse @xmath31 is structured with respect to the operator  @xmath32 obtained by swapping @xmath17 and @xmath18 : @xmath33 more precisely , the above claim says that the ranks of @xmath22 and @xmath34 are the same .",
    "( see  ( * ? ? ?",
    "* theorem  1.5.3 ) and , for the case not handled there , see  @xcite ; for completeness , we give a proof in appendix  [ app : proof - inverse - is - structured ] . )    * some classical operators .",
    "* consider first toeplitz - like matrices .",
    "for @xmath35 in @xmath13 , it is customary to define the @xmath36 cyclic down - shift matrix @xmath37 then , using sylvester operators , a matrix @xmath38 will be called toeplitz - like if @xmath39 has a low rank compared to @xmath23 and @xmath24 .",
    "( this rank is independent of the choice of @xmath35 and @xmath40 , up to an additive constant of absolute value at most two . ) using stein operators , one would consider instead the rank of @xmath41 .",
    "besides @xmath42 and its transpose it is also useful to consider the diagonal matrix  @xmath43 , whose diagonal is given by @xmath44 .",
    "specifically , popular choices take @xmath45 with @xmath46 , @xmath47 and @xmath48 , for either sylvester or stein operators .",
    "this family covers in particular toeplitz and hankel - like structures , where both @xmath17 and @xmath18 are ( transposed ) cyclic down - shift matrices ; vandermonde - like structures , where one of the matrices @xmath17 and @xmath18 is a ( transposed ) cyclic down - shift matrix and the other one is diagonal ; and cauchy - like structures , where both @xmath17 and @xmath18 are diagonal .",
    "we say that a displacement operator is of _ toeplitz / hankel type _ if it falls into the first category , and of _ vandermonde / cauchy type _ if it falls into the second or third one .",
    "* block - diagonal companion matrices . *",
    "the first contribution of this paper is to generalize the classical displacement operators seen before .",
    "let @xmath49 $ ] be the univariate polynomial ring over @xmath13 , and for @xmath50 , let @xmath49_\\delta$ ] be the @xmath13-vector space of polynomials of degree less than @xmath51 .",
    "furthermore , for @xmath52 $ ] monic of degree @xmath53 , denote by @xmath54 the @xmath55 companion matrix associated with @xmath56 : @xmath57 two special cases are the @xmath36 matrix @xmath42 defined in  ( [ eq : z_m_phi ] ) and the @xmath58 matrix given by @xmath59 , which are associated with the polynomials @xmath60 and @xmath61 , respectively .",
    "given @xmath62 , let now @xmath63 denote a family of monic nonconstant polynomials in @xmath49 $ ] , and let @xmath64 with @xmath65 for all @xmath66 .",
    "we will call _ block - diagonal companion matrix _ associated with @xmath67 the @xmath68 block - diagonal matrix @xmath69 whose @xmath66th diagonal block is the @xmath70 companion matrix @xmath71 : @xmath72 we write @xmath73 to denote the product @xmath74 , which is the characteristic polynomial of  @xmath69 . finally , we associate with @xmath67 the following assumption on its elements : @xmath75    * associated displacement operators . * with @xmath67 as above",
    ", we now consider another family of monic nonconstant polynomials , namely @xmath76 , with respective degrees @xmath77 ; we let @xmath78 and @xmath79 . in",
    "what follows , we assume that both assumptions @xmath80 and @xmath81 hold .",
    "let then @xmath82 and @xmath83 be the block - diagonal companion matrices associated with @xmath67 and @xmath84 . in this paper , we consider the following eight operators : @xmath85 @xmath86 we shall call them the _ operators associated with @xmath87 of sylvester or stein type _",
    ", respectively , and say that they have _ format _",
    "two of these operators will be highlighted , @xmath89 and @xmath90 , and we will call them the _ basic _ operators associated with @xmath87 . indeed , we will be able to derive convenient formulas to invert them ; this will in turn allow us to deal with the other six operators by suitable reductions .",
    "tables  [ tab3 ] and  [ tab3b ] show that the classical structures defined by  ( [ eq : classical - disp - operators ] ) follow as special cases of these operators , for suitable choices of @xmath87 making @xmath69 or @xmath91 a diagonal or a ( transposed ) cyclic down - shift matrix .",
    "c||c|c & @xmath92 and @xmath93 & @xmath94 and @xmath95 + @xmath96 and @xmath97 & hankel - like &    .some particular cases for the sylvester operator @xmath89 . [ cols=\"^ \" , ]     & @xmath98 & @xmath23 + @xmath96 and @xmath99 & @xmath98 & @xmath23 + @xmath100 for all @xmath66 & @xmath101 & @xmath102 + @xmath103 & @xmath104 & @xmath101    finally , when considering two families of polynomials @xmath67 and @xmath84 , we will write @xmath105    * background work . * following landmark publications such as  @xcite , the literature on structured linear systems has vastly developed , and we refer especially to  @xcite for comprehensive overviews .",
    "it was recognized early that a key ingredient for the development of fast algorithms was the ability to perform matrix - vector products efficiently . for toeplitz - like and hankel - like systems ,",
    "this was done in  @xcite ; for vandermonde and cauchy structures , this is in  @xcite . in our notation , these references establish bounds of the form @xmath106 for operators of format @xmath107 of toeplitz / hankel type or vandermonde / cauchy type , respectively .",
    "if a matrix @xmath4 has @xmath108 columns , then the product @xmath109 can be computed by performing  @xmath108 matrix - vector products with @xmath1 . in other words , we can take @xmath110 which yields the bounds @xmath111 and @xmath112 for the cases seen above .    such matrix - matrix products , with @xmath113 , are the main ingredients in the morf / bitmead - anderson ( mba ) algorithm to invert structured matrices  @xcite of toeplitz / hankel type , which combines the displacement rank approach of  @xcite with strassen s divide - and - conquer approach for dense matrix inversion  @xcite .",
    "this algorithm initially required several genericity conditions to hold but , based on the randomized regularization technique of kaltofen and saunders  @xcite , it was then further extended in  @xcite to handle arbitrary matrices ( see also  @xcite and  @xcite ) . for operators of format @xmath107 of toeplitz , hankel , vandermonde or cauchy type , the cost of inversion @xmath114 can then ( roughly speaking ) be taken in @xmath115 see  @xcite for the case of vandermonde - like and cauchy - like matrices , and  @xcite for a unified treatment of all four structures . using the above upper bound on the cost of multiplication @xmath116",
    ", we see that this is @xmath117 times the cost of a matrix - vector product ( strictly speaking , some precautions are necessary to make such a statement ; for instance , the algorithm becomes probabilistic ) .",
    "this results in running times of the form @xmath118 or @xmath119 for inverting respectively toeplitz / hankel - like matrices and vandermonde / cauchy - like matrices of size @xmath23 and displacement rank @xmath3 . in the vandermonde / cauchy case ,",
    "another approach due to pan  @xcite proceeds by reduction to the toeplitz / hankel case ; the running time is then reduced to @xmath120 .",
    "going beyond the four basic structures , olshevsky and shokrollahi introduced a common generalization thereof , where the displacement matrices take the form of block - diagonal jordan matrices  @xcite .",
    "we will discuss their result in more detail after stating our main theorems . for the moment",
    ", we mention that their paper gives an algorithm for matrix - vector product with running times ranging from @xmath121 depending on the block configuration of the supporting jordan matrices ; for the four basic structures seen above , we recover the running times seen before . in  @xcite",
    ", this result is used to sketch an extension of the mba algorithm to such matrices .",
    "as before , the running time for inversion increases by a factor @xmath122 compared to the time for matrix - vector multiplication , ranging from @xmath123 depending on the operator .    when @xmath124 , all above results for matrix - vector product or matrix inversion are within a polylogarithmic factor of being linear - time",
    ". however , when @xmath3 is not constant , this is not the case anymore ; in the worst case where @xmath125 , the running times for inversion grow like @xmath126 ( neglecting logarithmic factors again ) , whereas dense linear algebra algorithms take time @xmath127 . in  @xcite , bostan , jeannerod and",
    "schost gave algorithms for inversion of structured matrices of the four classical types with running time @xmath128 .",
    "this is satisfactory for @xmath129 , since we recover the cost of dense linear algebra , up to logarithmic factors .",
    "however , for @xmath130 , this algorithm is slightly slower ( by a factor @xmath131 ) than the mba algorithm .    * main results . * the results in this paper cover in a uniform manner the general class of operators based on companion matrices introduced above .",
    "inspired by  @xcite , we obtain algorithms for structured matrix multiplication , inversion , and linear system solving whose running times grow with @xmath3 as @xmath132 instead of @xmath133 ; however , we manage to avoid the loss of the @xmath131 factor observed in  @xcite , thus improving on the results of that paper .",
    "all our algorithms assume exact arithmetic ( and we do not claim any kind of accuracy guarantee when using finite precision arithmetic ) .",
    "note also that they are deterministic for multiplication and , similarly to  @xcite , randomized ( las vegas ) for inversion and linear system solving .",
    "let @xmath67 and @xmath84 be as before .",
    "we give in section  [ sec : invert ] inversion formulas for the basic operators @xmath134 or @xmath135 that allow us to recover a matrix @xmath1 from one of its generators , generalizing simultaneously previous results for toeplitz , hankel , vandermonde , and cauchy displacement operators .",
    "from these formulas , one readily deduces that a matrix - vector product @xmath136 can be computed in time @xmath137 which is @xmath138 for @xmath139 .    for matrix - matrix products , the direct approach which is based on the estimate @xmath140 thus leads to @xmath141 , which is sub - optimal , as we pointed out before .",
    "the following theorem improves on this straightforward approach , for all operators associated with @xmath87 .",
    "[ theo : mainsylv ] let @xmath63 and @xmath76 be two families of polynomials in @xmath49 $ ] .",
    "assume that in each of these families the polynomials are monic , nonconstant and pairwise coprime , and denote @xmath142 and @xmath143 , then , for any invertible operator @xmath21 associated with @xmath87 , we can take @xmath144 with @xmath139 , @xmath145 , and @xmath146 .",
    "furthermore , @xmath147    using the estimates for @xmath148 given in  ( [ eq : m_prime_mat ] ) , the estimate for @xmath149 becomes @xmath150   o\\left ( { \\alpha'}^{\\omega-2}\\beta ' p   + \\beta ' \\sm(p)\\lg(p ) \\right )   & \\text{if char}(\\f)=0 \\text{~or~ } |\\f|{\\geqslant}2p .",
    "\\end{cases}\\ ] ] thus , these results provide a continuum between two extreme cases .",
    "when @xmath151 , the cost is @xmath152 ; when @xmath3 and @xmath108 are large , the first term is dominant , with total costs respectively @xmath153 and @xmath154 . disregarding logarithmic factors",
    ", this is always @xmath155 : this matches the cost ( up to logarithmic factors ) of dense , unstructured linear algebra algorithms for multiplying matrices of dimensions @xmath156 and @xmath157 with @xmath158 .",
    "in fact , when @xmath159 we recover exactly the cost bound @xmath160 of the multiplication of two dense unstructured matrices of dimensions @xmath161 and @xmath162 , since then @xmath163 and @xmath164 .    for inversion and system solving ,",
    "the bounds given above on @xmath165 yield @xmath166    o\\big ( \\alpha^{\\omega-1}p \\,\\lg(p)+ \\alpha\\lg(\\alpha)\\sm(p)\\lg(p ) \\big ) & \\text{if char}(\\f)=0 \\text{~or~ } |\\f|{\\geqslant}2p ; \\end{cases}\\ ] ] and , when @xmath167 ( so that the input matrix is @xmath168 and not structured with respect to the operator  @xmath169 ) , the obtained cost is @xmath170 , as can be expected .",
    "the following theorem highlights situations where we can take both @xmath171 and @xmath172 in @xmath173 . in this case , we get slightly better bounds than in the general case for the problems @xmath174 ( for inversion and solve , the terms above are always negligible , for any choice of @xmath67 and @xmath84 ) . in view of tables  [ tab3 ] and  [ tab3b ] , we see that these special cases correspond to operators of toeplitz / hankel type , or vandermonde / cauchy type , when their supports are points in geometric progression .",
    "[ theo : mainsylv2 ] all notation and assumptions being as in theorem  [ theo : mainsylv ] , suppose in addition that one of the following holds :    * either @xmath96 and @xmath99 for some @xmath175 , or @xmath176 and there exist @xmath177 such that @xmath178 for all @xmath66 ; * either @xmath92 and @xmath179 for some @xmath180 , or @xmath94 and there exist @xmath181 such that @xmath182 for all @xmath183",
    ".    then , for any invertible operator @xmath21 associated with @xmath87 , we can take @xmath184 with @xmath139 , @xmath145 , and @xmath146 .",
    "using once again the bounds on @xmath148 given in  ( [ eq : m_prime_mat ] ) , this is thus @xmath185    o({\\alpha'}^{\\omega-2}\\beta ' p + \\beta'\\lg(\\alpha ' ) \\sm(p ) ) & \\text{if char}(\\f)=0 \\text{~or~ } |\\f|{\\geqslant}2p .",
    "\\end{cases}\\ ] ]    * comparison with previous work .",
    "* let us briefly compare our results with previous ones , in increasing order of generality .    for classical operators of hankel , toeplitz , vandermonde or cauchy type ,",
    "our results match classical ones in the cases @xmath151 ( for multiplication ) or @xmath124 ( for inversion ) .",
    "when we drop such assumptions , our results improve on all previous ones .",
    "for instance , for the inversion of toeplitz - like or hankel - like matrices , the best previous results were either @xmath186 in  @xcite , or @xmath187 in  @xcite .",
    "we improve them simultaneously , by dropping the cost to @xmath188 .",
    "we mentioned earlier the work of olshevsky and shokrollahi  @xcite , who consider sylvester operators where the displacement matrices are block - diagonal , with jordan blocks .",
    "a jordan block of size @xmath23 associated with @xmath189 is similar to the companion matrix of @xmath190 ; the similarity matrix is the `` pascal '' matrix of the mapping @xmath191 , for @xmath192_m$ ] . because the similarity matrix and its inverse can be applied in time @xmath104 to a vector  @xcite , the problem considered by olshevsky and shokrollahi is essentially equivalent to a particular case of ours .",
    "as it turns out , for these particular cases , the results in  @xcite for matrix / vector product and inversion are equivalent to ours , when the displacement rank @xmath3 is @xmath193 . for larger",
    "@xmath3 , our results improve on those of  @xcite , whose costs are quadratic in @xmath3 .",
    "finally , to the best of our knowledge , no previous work addressed the general case of block - companion matrices that we deal with in this paper .    * an application .",
    "* we are able to address problems related to _",
    "simultaneous approximation _ using our operators .",
    "suppose that we are given @xmath63 , with sum of degrees @xmath23 , together with `` residuals '' @xmath194 with all @xmath195 in @xmath49 $ ] such that @xmath196 for all @xmath197 .",
    "we are looking for polynomials @xmath198 in @xmath49 $ ] such that @xmath199 holds for @xmath200 , and with prescribed degree bounds ( the polynomial @xmath201 should be in @xmath49_{n_j}$ ] , with @xmath202 ) .",
    "the matrix of the corresponding linear system , where the coefficients of @xmath198 are unknown , has size @xmath102 and displacement rank @xmath203 for the operator @xmath90 , where @xmath84 is the `` vector '' consisting of the unique polynomial @xmath204 , where @xmath35 is chosen such that the assumptions of theorem  [ theo : mainsylv ] hold .",
    "hence , we can solve such a system using @xmath205 base field operations .",
    "this is to be compared with an algorithm given in  @xcite , that has cost @xmath206 , but which does not require the assumption that @xmath207 be pairwise coprime .",
    "this problem generalizes hermite - pad ( with @xmath96 and @xmath208 ) and so - called m - pad approximation , with @xmath209 , for pairwise distinct @xmath210 in @xmath13 .",
    "a typical instance is the reconstruction of the minimal polynomial of an algebraic function .",
    "suppose that @xmath211 is a root of a polynomial @xmath212[y]$ ] , so @xmath211 lies in the algebraic closure of @xmath213 .",
    "given the power series expansion of @xmath211 at high enough precision around a point  @xmath214 , it is possible to reconstruct @xmath215 by means of hermite - pad approximation .",
    "using our algorithm , we obtain the same output for the same cost , starting this time from approximations at points @xmath216 , each of them requiring smaller precision .",
    "* organization of the paper .",
    "* section  [ sec : prelim ] introduces notation used all along this paper , and discusses a few classical questions about polynomial arithmetic , such as chinese remaindering .",
    "section  [ sec : invert ] gives _ inversion formulas _ for the operators we discuss , which generalize well - known results for classical operators . in section  [ sec : equiv ] , we use these formulas in order to reduce most of our questions to similar questions for _ basic operators _ , or even toeplitz / hankel operators .",
    "section  [ sec : multiplication_alogrithms ] gives the main algorithmic result in this paper , an improved algorithm for the multiplication of structured matrices .",
    "finally , in section  [ sec : inverse ] , we show how this new multiplication algorithm can be used within the mba approach to accelerate structured inversion and structured system solving",
    ".    * acknowledgements . *",
    "we thank an anonymous referee for many detailed comments .",
    "in this section , we review some classical operations on polynomials and matrices : we introduce a short list of useful matrices , such as multiplication matrices , reversal matrices and krylov matrices , and we discuss questions related to modular computations , or multiple reduction and chinese remaindering .    throughout the article , we will use some well - known complexity results on polynomial arithmetic ; as a general rule , they can be found in the books  @xcite ,  @xcite , or  @xcite .",
    "matrices ( resp .",
    "vectors ) are written in upper - case ( resp .",
    "lower - case ) sans - serif font .",
    "if @xmath1 ( resp .",
    "@xmath4 , @xmath217 ,  ) is a matrix , @xmath218 ( resp .",
    "@xmath219 , @xmath220 ,  ) is its @xmath66th column . if @xmath221 ( resp .",
    "@xmath222 , @xmath223 ,  ) is a vector , its @xmath66th entry is written @xmath210 ( resp .",
    "@xmath224 ,  ) .",
    "special matrices ( cyclic , companion , diagonal ,   ) will be written with blackboard bold letters ( @xmath225 , @xmath226 , @xmath227 ,  ) .",
    "the entries of an @xmath228 matrix @xmath1 are indexed from @xmath229 to @xmath230 ( row indices ) and from @xmath229 to @xmath231 ( column indices ) .",
    "we will also use the following notation .    * for @xmath232^t\\in    \\f^m$ ] , we write @xmath233 to denote the polynomial @xmath234_m$ ] . * for any polynomial @xmath235 $ ] of degree at most @xmath236 , @xmath237 denotes its reverse polynomial @xmath238 $ ] .    for @xmath239 , @xmath240 is the @xmath241 reversal matrix , with @xmath242s on the antidiagonal only .",
    "more generally , for @xmath243 , @xmath244 is the @xmath241 matrix with @xmath242s on the upper antidiagonal entries of indices @xmath245 , so @xmath246 ; by convention , for @xmath247 , @xmath248 is the zero matrix of size @xmath23 .",
    "finally , for @xmath249 , @xmath250 , and @xmath251 , we write @xmath252 to denote the krylov matrix in @xmath253 whose columns are @xmath254 .      in this subsection",
    ", we introduce additional notation and give basic results related to multiple reduction and chinese remaindering .",
    "consider pairwise - coprime monic polynomials @xmath63 in @xmath49 $ ] with @xmath255 , and let @xmath64 and @xmath256",
    ".    recall that we associated with the family @xmath67 the cost functions @xmath257 and @xmath258 .",
    "they will help us measure the cost of the following operations . to @xmath67 , we associate the _ multiple reduction _ mapping , and its inverse , _ chinese remaindering _ , defined by @xmath259_m & \\to & \\f[x]_{m_1 } \\times \\cdots \\times \\f[x]_{m_d}\\\\ & a & \\mapsto & ( a \\bmod p_1,\\dots , a \\bmod p_d )   \\end{array}\\ ] ] and @xmath260_{m_1 } \\times \\cdots \\times \\f[x]_{m_d } & \\to & \\f[x]_m \\\\    & ( a_1,\\dots , a_d ) & \\mapsto & a,\\text{~such that $ a \\bmod p_i = a_i$ for all $ i$. }   \\end{array}\\ ] ] a related question is _ linear recombination _ , defined as the following isomorphism : @xmath261_{m_1 } \\times \\cdots \\times \\f[x]_{m_d } & \\to & \\f[x]_m \\\\    & ( a_1,\\dots , a_d ) & \\mapsto & a_1 p_2 \\cdots p_d + \\cdots + p_1 \\cdots p_{d-1 } a_d .",
    "\\end{array}\\ ] ] fast algorithms for these three operations lead to the costs given in the next lemma .",
    "although such costs can be found in or deduced from  @xcite , they do not seem to have been presented in this way , with a common precomputation involving both functions @xmath257 and @xmath258 , and then an extra cost involving only @xmath257 .",
    "[ lemma : wx ] let @xmath262 as above be given and assume that @xmath80 holds . then",
    ", after a precomputation of time @xmath263 that yields @xmath73 as a by - product , one can apply the maps @xmath264 , its inverse @xmath265 , as well as @xmath266 and its inverse , to any vector in time @xmath267 .    for @xmath268 ,",
    "let us define @xmath269 and @xmath270 .",
    "( in particular , if @xmath96 then @xmath271 , while if @xmath272 is the linear polynomial @xmath273 then @xmath274 is the value of the derivative of @xmath73 at @xmath210 . ) using these polynomials , we first note how the mappings @xmath265 and @xmath266 relate to each other : chinese remaindering of @xmath275 is done by computing @xmath276 for all @xmath66 , and then applying @xmath266 to these products ; conversely , to perform linear recombination it suffices to compute the products @xmath277 and then to apply @xmath265 . note also that the inverse of @xmath266 can be obtained by first using @xmath264 and then multiplying by the @xmath278 s modulo @xmath272 .",
    "suppose we are in the case where @xmath279 with the @xmath210 s in _ geometric _ progression .",
    "then , we can compute @xmath280 and apply @xmath264 and @xmath265 in time @xmath267 using the algorithms of  @xcite . using the remark above on the relation between @xmath265 and @xmath266 , the claim carries over to @xmath266 and its inverse , so",
    "the lemma is proved in this case .    in the general case",
    ", we can precompute @xmath73 , as well as apply @xmath264 and @xmath266 ( without any further precomputation ) in time @xmath267 using respectively theorems  2.19 ,  3.19 and step 2 ( or 5 ) of theorem 3.21 in  @xcite .    let @xmath281 .",
    "since @xmath282 is obtained by applying @xmath266 to the polynomials @xmath283 , it can thus be precomputed in time @xmath267 .",
    "the modular images @xmath284 , which coincide with the polynomials @xmath285 seen above , can be precomputed in the same time by applying @xmath264 .",
    "finally , we precompute the inverses @xmath286 in time @xmath287 by fast extended gcd ( when @xmath96 , this is straightforward , since we have seen that @xmath288 ; otherwise , each @xmath278 is obtained in time @xmath289 using the fast extended euclidean algorithm  ( * ? ? ?",
    "* cor .  3.14 ) ) .    using the first paragraph",
    ", we can then perform chinese remaindering for the cost @xmath267 of @xmath266 , plus modular multiplications by the @xmath278 s .",
    "the cost of the latter is @xmath104 , by the super - linearity of @xmath8 , which is @xmath267 .",
    "the same holds for the inverse of @xmath266 , which is reduced to @xmath264 and modular multiplications by the @xmath278 s .    in matrix terms",
    ", we will need the following notation in the next sections ( here and hereafter , we use canonical monomial bases for vector spaces such as @xmath49_m$ ] ) :    * @xmath290 is the matrix of the mapping @xmath266 ; * @xmath291 is the matrix of the mapping @xmath264 .",
    "[ lemma : w ] let @xmath262 as above be given and assume that @xmath80 holds . then",
    ", after a precomputation of time @xmath263 , one can compute @xmath292 , as well as @xmath293 , @xmath294 , @xmath295 and @xmath296 for any @xmath297 in @xmath298 in time @xmath267 .    for @xmath292 , @xmath293 and @xmath295 ,",
    "this is nothing else than the previous lemma . to compute with the transpose of @xmath299 , we could rely on the transposition principle  @xcite , but it is actually possible to give direct algorithms .",
    "algorithm  tsimulmod in  @xcite shows how to reduce the computation of @xmath294 to an application of @xmath266 and a few power series multiplications / inversions that involve only @xmath73 and the @xmath272 s , and whose costs add up to @xmath104 .",
    "thus , the claimed @xmath267 follows from the previous lemma .",
    "it is straightforward to invert that algorithm step - by - step , obtaining an algorithm for computing @xmath296 in time @xmath267 as well .      given a monic polynomial @xmath300 $ ] of degree @xmath23 , we will frequently work with the residue class ring @xmath301/p$ ] ; doing so , we will identify its elements with polynomials in @xmath49_m$ ] . in this subsection",
    ", we review several questions related to computations in @xmath302 , such as modular multiplication and its transpose .    as a preamble",
    ", we define two useful matrices related to @xmath73 .",
    "first , for @xmath251 , @xmath303 denotes the matrix of reduction modulo @xmath73 , which maps a polynomial @xmath304_\\ell$ ] to @xmath305 .",
    "computationally , applying @xmath306 to a vector amounts to doing a euclidean division .",
    "next , writing @xmath307 ( so that @xmath308 ) , we will denote by @xmath309 the @xmath241 hankel matrix @xmath310 \\vdots & \\iddots & 1 & \\\\[-1 mm ] p_{m-1 } & \\iddots & & \\\\ 1 & & &   \\end{bmatrix}.\\ ] ] operations with @xmath309 are fast , due to the triangular hankel nature of this matrix . the only non - trivial part of the following lemma concerns the inverse of @xmath309 .",
    "it is proved in  @xcite for triangular toeplitz matrices ; the extension to the hankel case is straightforward , since @xmath311 with @xmath312 triangular toeplitz .",
    "[ lemma : y ] given @xmath73 , for @xmath297 in @xmath298 , one can compute @xmath313 and @xmath314 in @xmath104 .",
    "let us now discuss multiplication in @xmath302 , often called _",
    "modular multiplication_. computationally , it is done by a standard polynomial multiplication , followed by a euclidean division ; altogether , this takes time @xmath104 .    in matrix terms , recall that @xmath315 denotes the @xmath241 companion matrix associated with @xmath73 ; equivalently , this is the matrix of the multiplication - by-@xmath316 endomorphism in @xmath302 . more generally , for @xmath56 in @xmath49 $ ] , we denote by @xmath317 the @xmath241 matrix of multiplication by @xmath56 in @xmath302 ; that is , the matrix whose @xmath318 entry is the coefficient of @xmath319 in @xmath320 , for @xmath321 .",
    "thus , by what was said above , applying @xmath317 to a vector can be done in time @xmath104 .",
    "note also that @xmath322 and that @xmath323 .",
    "yet more generally , for @xmath251 , @xmath324 is the matrix whose entries are the coefficients of the same polynomials @xmath320 as above , but now for @xmath325 .",
    "the following easy lemma simply says that for @xmath56 in @xmath49_m$ ] and @xmath326 in @xmath49_\\ell$ ] , @xmath327 .",
    "[ lemma : cc ] let @xmath56 be in @xmath49_m$ ] and @xmath251 .",
    "then @xmath328 .",
    "we will also use a dual operation , involving the space @xmath329 of @xmath13-linear forms @xmath330 .",
    "such a linear form will be given by the values it takes on the monomial basis @xmath331 of @xmath302 .",
    "then , _ transposed multiplication _ is the operation mapping @xmath332 to the linear form @xmath333 , defined by @xmath334 , where the multiplication takes place in @xmath302 .",
    "the name reflects the fact that for fixed @xmath56 , transposed multiplication by @xmath56 is indeed the dual map of the multiplication - by-@xmath56 endomorphism of @xmath302 . in matrix terms , transposed product by @xmath56 amounts to multiplication by @xmath335 .",
    "transposed products can be computed in the same time @xmath104 as `` standard '' modular multiplications , by a dual form of modular multiplication  @xcite .",
    "however , such an algorithm relies on middle product techniques  @xcite , which are not straightforward to describe .",
    "the following lemma shows an alternative approach , with same cost up to a constant factor : to perform a transposed product by @xmath56 , it suffices to do a modular multiplication by @xmath56 , with a pre- and post - multiplication by @xmath309 and its inverse ( which can both be done in @xmath104 , see lemma  [ lemma : y ] ) .",
    "for @xmath336 , this result is sometimes referred to as saying that @xmath309 is a _",
    "symmetrizer _ for the polynomial @xmath73 ; see  @xcite .",
    "[ lemma : transp ] for all @xmath192_{m}$ ] , we have @xmath337 .",
    "equivalently , @xmath338 .    by linearity",
    ", it is enough to consider the case of @xmath339 , for @xmath340 .",
    "the case @xmath247 is clear , since @xmath341 is the identity matrix . for @xmath342 ,",
    "this result is well - known , see for instance  ( * ? ? ?",
    "* ex .  3 , ch .",
    "13 ) ; it can be proved by simple inspection .",
    "once the claim is established for @xmath316 , an easy induction shows that it holds for @xmath343 , for an arbitrary @xmath251 , using the fact that @xmath344 .",
    "we now discuss krylov matrices derived from ( transposed ) companion matrices .",
    "[ lemma:2 ] let @xmath250 , @xmath251 , and @xmath345_m$ ] .",
    "then    * @xmath346 and * @xmath347 with @xmath348 .",
    "the first assertion is clear .",
    "indeed , the columns of the krylov matrix @xmath349 are the coefficient vectors of the polynomials @xmath350 , for @xmath351 , so that @xmath352 ; the claim now follows from lemma  [ lemma : cc ] . for the second assertion",
    ", the fact that @xmath353 , which follows from lemma  [ lemma : transp ] , implies that @xmath354 . using the first point concludes the proof .",
    "we now consider a family of monic polynomials @xmath63 , and briefly discuss the extension of the previous claims to this context .",
    "as before , we write @xmath355 and @xmath356 .    in terms of matrices ,",
    "we have already mentioned the definition of the block - diagonal companion matrix @xmath69 associated with @xmath67 , whose blocks are @xmath357 .",
    "similarly , @xmath358 will denote the block - diagonal matrix with hankel blocks @xmath359 .",
    "the next lemma , which is about @xmath358 and its inverse , is a direct consequence of lemma  [ lemma : y ] ; the complexity estimate follows from the super - linearity of the function  @xmath8 .",
    "[ lemma : y2 ] given @xmath67 , for @xmath297 in @xmath298 , one can compute @xmath360 and @xmath361 in @xmath104 .",
    "another straightforward result is the following extension of lemma  [ lemma : transp ] .",
    "[ lemma : transp2 ] the relation @xmath362 holds .",
    "next , we discuss computations with krylov matrices derived from @xmath69 and its transpose .",
    "we will only need some special cases , for which very simple formulas are available ( and for which invertibility will be easy to prove ) . recall that @xmath363 denotes the matrix of the map @xmath264 of multiple reduction modulo @xmath207 ; thus , it is obtained by stacking the matrices @xmath364 .",
    "[ lemma : krylov ] writing @xmath365 to denote the @xmath66th unit vector in @xmath298 , the following holds :    * @xmath366 with @xmath367^t \\in \\f^m$ ] , and * @xmath368 with @xmath369^t \\in \\f^m$ ] .    the block structure of @xmath69 and the definition of @xmath370 imply that @xmath371 is obtained by stacking the matrices @xmath372 for @xmath373 .",
    "since @xmath374_{m_i}$ ] , lemma  [ lemma:2 ] gives @xmath375 for all  @xmath66 ; this proves the first case . in the second case",
    ", @xmath376 is decomposed into blocks @xmath377 , which , by lemma  [ lemma:2 ] and since @xmath378_{m_i}$ ] , are equal to @xmath379 .",
    "finally , the following lemma discusses computations involving two families of monic polynomials @xmath67 and @xmath84 .",
    "[ lemma : iq ] let @xmath63 and @xmath76 , such that @xmath80 and @xmath81 hold , and let @xmath380 and @xmath381 .",
    "then , the following holds :    * if @xmath382 , we can compute all @xmath383 , for @xmath373 , in time @xmath384 .",
    "* let @xmath385 and @xmath386 .",
    "if @xmath387 , we can compute all @xmath388 , for @xmath373 , in time @xmath384 .",
    "let @xmath139 .",
    "we first discuss a particular case , where @xmath389 and @xmath390 and @xmath391 . in this case",
    ", we can compute @xmath392 in linear time @xmath393  ( * ? ? ?",
    "* lemma  3 ) , provided it is well - defined ; the same holds for @xmath394 .",
    "thus , our claim holds , since in this case , @xmath395 .",
    "if @xmath73 is not as above , we compute @xmath396 in time @xmath397 using lemma  [ lemma : wx ] , then @xmath398 in time @xmath173 , which is @xmath399 .",
    "using lemma  [ lemma : wx ] again , we deduce all @xmath400 , for @xmath401 , in time @xmath267 ; finally , we invert all remainders using fast extended gcd with the @xmath272 s in time @xmath402 ; under our assumption on @xmath73 , this is @xmath287 .",
    "the total cost is @xmath384 , so our claim for the inverses of @xmath396 modulo the @xmath272 s holds .",
    "we proceed similarly for @xmath403 .",
    "the last case to consider is when @xmath96 and @xmath390 , but with @xmath396 not of the form @xmath404 .",
    "we proceed in the converse direction : we first reduce and invert @xmath73 modulo all  @xmath405 , for all @xmath406 ; this takes time @xmath384 . by chinese remaindering",
    ", we obtain @xmath407 , without increasing the cost .",
    "knowing @xmath408 , we deduce @xmath409 , since @xmath408 and @xmath215 satisfy @xmath410 ; this costs an extra @xmath173 , which is @xmath399 , so our claim is proved .",
    "we proceed similarly for @xmath403 .",
    "let us consider @xmath63 and @xmath76 tuples of monic polynomials in @xmath49 $ ] , and let @xmath411 , @xmath77 , @xmath23 , @xmath24 , @xmath73 and @xmath396 be as before .",
    "we assume the coprimality conditions @xmath80 and @xmath81 . in this section ,",
    "we establish inversion formulas for the two basic operators of sylvester and stein types associated with @xmath87 . in other words , given two matrices @xmath25 in @xmath412 , we show how to reconstruct the matrices @xmath1 and @xmath413 in @xmath414 such that @xmath415 provided the corresponding operators are invertible",
    ". we will use the following objects .",
    "* we denote by @xmath416 and @xmath417 the columns of @xmath418 and @xmath30 .",
    "corresponding to the partition of @xmath69 into blocks , we partition each @xmath419 , @xmath420 , into smaller column vectors @xmath421 ; similarly , we partition each @xmath422 into @xmath423 according to the block structure of @xmath424 .",
    "the matrices @xmath418 and @xmath30 themselves are partitioned into matrices @xmath425 and @xmath426 ; @xmath427 has size @xmath428 and columns @xmath429 , whereas @xmath430 has size @xmath431 and columns @xmath432 . * for @xmath200 and @xmath433 , we let @xmath434 , so that @xmath435 is in @xmath49_{m_i}$ ] . similarly , for @xmath436 , we define @xmath437_{n_j}$ ] . +",
    "we further let @xmath438 be the unique polynomial in @xmath49_m$ ] such that @xmath439 holds for @xmath200 ; in other words , we have @xmath440 the polynomial @xmath441 is defined similarly , replacing @xmath435 and @xmath272 by @xmath442 and @xmath405 .    in the following theorem ,",
    "recall that the necessary and sufficient condition for the sylvester operator to be invertible is that @xmath382 ; for the stein operator , the condition is that @xmath387 , with @xmath386 .",
    "[ theo : rec ] the following holds :    * suppose that @xmath382 .",
    "then , the unique matrix @xmath38 such that @xmath443 is given by @xmath444 where @xmath445 is the block - diagonal matrix with blocks @xmath446 for @xmath200 , where @xmath446 denotes @xmath447 .",
    "* suppose that @xmath387 .",
    "then , the unique matrix @xmath448 such that @xmath449 is given by @xmath450 where @xmath451 is the block - diagonal matrix with blocks @xmath452 for @xmath200 , where @xmath452 denotes @xmath453 .    the following corollary on the cost of matrix - vector multiplication follows directly ; the more difficult case of matrix - matrix multiplication will be handled in section  [ sec : multiplication_alogrithms ] .",
    "[ coro : mul1 ] we can take @xmath454 and @xmath455    the proof is mostly the same in both cases ; it amounts to estimating first the cost of computing the polynomials that define the matrices involved in theorem  [ theo : rec ] , and then the cost of multiplying each of these matrices by a single vector .",
    "given the families of polynomials @xmath67 and @xmath84 , we can compute the products @xmath73 and @xmath396 in time @xmath456 , by lemma  [ lemma : wx ] .",
    "moreover , since both assumptions @xmath80 and @xmath81 hold and depending on whether @xmath382 or @xmath387 , we deduce from lemma  [ lemma : iq ] that we can compute the inverses of @xmath396 , or of @xmath403 , modulo all @xmath272 s in time @xmath457 .",
    "finally , given further the generator @xmath25 in @xmath458 , it follows from lemma  [ lemma : wx ] that the polynomials @xmath459 and @xmath460 can be obtained in time @xmath461 and @xmath462 , respectively .",
    "thus , the overall cost of deducing the needed polynomials from @xmath67 , @xmath84 , @xmath418 , @xmath30 is in @xmath463 .",
    "we now turn to the cost of the matrix - vector products performed when applying theorem  [ theo : rec ] to the evaluation of @xmath464 or @xmath465 for some @xmath297 in @xmath466 .",
    "lemma  [ lemma : y2 ] shows that the cost of multiplying @xmath467 by @xmath297 is @xmath468 , which is @xmath173 . by lemma  [ lemma : w ] ,",
    "the cost for @xmath469 is @xmath470 .",
    "the inner sum amounts to @xmath203 multiplications modulo @xmath73 or @xmath396 , for a total of @xmath471 .",
    "lemma  [ lemma : w ] also shows that the cost for @xmath299 is @xmath472 .",
    "finally , multiplying by @xmath473 or @xmath474 amounts to performing modular multiplications modulo all @xmath272 s , and this can be done in time @xmath475 .",
    "hence the cost of all these matrix - vector products is in @xmath476 ; this fits the announced bound , since @xmath477 .",
    "the rest of this section is devoted to proving theorem  [ theo : rec ] above .",
    "assuming that @xmath382 , we show how to reconstruct the unique matrix @xmath478 such that @xmath443 .",
    "we first show how to reconstruct all blocks in an ad - hoc block decomposition of @xmath1 , then use chinese remaindering .",
    "[ [ step-1-reconstruction - of - the - blocks - of - ma ] ] step 1 : reconstruction of the blocks of @xmath1 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    partitioning the matrix @xmath1 into blocks @xmath479 conformally with the block - diagonal structure of the matrices @xmath69 and @xmath91 , we have @xmath480 for all @xmath197 . in this paragraph , we prove a reconstruction formula for each block @xmath479 .",
    "[ lemma : aij ] for all @xmath481 and @xmath482 , we have @xmath483    fix @xmath66 and @xmath183 .",
    "note that @xmath71 and @xmath484 can not be simultaneously singular , since @xmath272 and @xmath405 are coprime . then , for all @xmath251 , a slight variation of  ( * ? ? ?",
    "* theorem  4.8 ) ( with both cases `` @xmath71 nonsingular '' and `` @xmath484 nonsingular '' covered by a single identity ) gives @xmath485 where @xmath486 denotes the krylov matrix of column length @xmath487 associated with the matrix @xmath71 and the vector @xmath488 .",
    "writing @xmath489 , we obtain , for @xmath490 , @xmath491 since we can inflate all matrices @xmath492 to size @xmath493 , replacing them by @xmath494 .",
    "note that the above equality then also holds for @xmath247 , since @xmath495 by convention .",
    "summing over all @xmath496 and using the fact that @xmath497 , we deduce @xmath498 using the first part of lemma  [ lemma:2 ] together with the fact that for @xmath499 the matrix @xmath500 is invertible with inverse @xmath501 , we obtain @xmath502 hence , applying lemma  [ lemma : transp ] to @xmath442 and since multiplication matrices mod @xmath272 commute , @xmath503    [ [ step-2-a - first - reconstruction - formula - for - ma ] ] step 2 : a first reconstruction formula for @xmath1 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath504 be the @xmath228 block matrix @xmath505 similarly , for @xmath420 , we define @xmath506 and @xmath507 as the block diagonal matrices , with respectively the multiplication matrices @xmath508 and @xmath509 on the diagonal .    putting all blocks @xmath479 together ,",
    "we deduce the following reconstruction formula for @xmath1 ; the proof is a straightforward application of the previous lemma .",
    "[ lemma : a ] we have @xmath510    [ [ step-3-using - a - factorization - of - b_vpvq ] ] step 3 : using a factorization of @xmath504 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    next , we use the fact that @xmath504 has a block structure similar to a cauchy matrix to factor it explicitly . for this , we introduce a matrix related to  @xmath299 : we let @xmath511 be the @xmath228 matrix of multiple reduction modulo @xmath207 , taking as input a polynomial of degree less than @xmath24 , instead of @xmath23 for @xmath299 .",
    "[ lemma : b ] the equality @xmath512 holds , where @xmath445 is the block - diagonal matrix with blocks @xmath446 , for @xmath200 .    observe that , given the coefficient vectors of polynomials @xmath513 , with @xmath514_{n_j}$ ] for all @xmath183 , the matrix @xmath504 returns the coefficients of @xmath515     & = \\frac { f_1 q_2\\cdots q_e + \\cdots + q_1 \\cdots q_{e-1}f_e}{q_1 \\cdots q_e } \\bmod p_i ,     \\end{aligned}\\ ] ] for @xmath200 .",
    "computing the polynomials @xmath516 can be done as follows :    * compute @xmath517 by calling @xmath266 ; * compute @xmath518 , for @xmath200 ; * deduce @xmath519 , for @xmath200 .",
    "this proves the requested factorization of @xmath504 .    as a result , we deduce the following equalities for @xmath1 : @xmath520 the last equality is due to the fact that the block - diagonal matrices @xmath506 and @xmath473 commute ( since all pairwise corresponding blocks are multiplication matrices modulo the same polynomials @xmath272 ) .    [",
    "[ step-4-using - chinese - remaindering ] ] step 4 : using chinese remaindering + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the next step will allow us to take @xmath511 and @xmath521 out of the inner sum .",
    "for any polynomial @xmath522 , any @xmath401 and any @xmath420 , it is equivalent to _ ( i ) _ reduce @xmath522 modulo @xmath272 and multiply it by @xmath435 modulo @xmath272 and _ ( ii ) _ multiply @xmath522 by the polynomial @xmath438 ( defined above ) modulo @xmath73 , and reduce it modulo  @xmath272 . in other words",
    ", we have the commutation relation @xmath523 . similarly , @xmath524 and this concludes the proof of the first part of theorem  [ theo : rec ] .",
    "the proof for stein operator case follows the same steps as for the sylvester case .",
    "let @xmath413 be such that @xmath525 .",
    "just like we decomposed @xmath1 into blocks @xmath479 , we decompose @xmath413 into blocks @xmath526 , such that @xmath527 . extending the notation used above",
    ", we write @xmath528 , so that we have @xmath529 .",
    "the following lemma is then the analogue of lemma  [ lemma : aij ] for stein operators and , as detailed in appendix  [ app : proof - lem : inverse - for - stein ] , can be proved in the same way using  ( * ? ? ?",
    "* theorem  4.7 ) .    [ lem : inverse - for - stein ] for all @xmath481 and @xmath482 , we have @xmath530    mimicking the construction in the previous section , we introduce the @xmath531 block matrix @xmath532 given by @xmath533 we will use again the matrices @xmath506 and @xmath507 introduced before , as well as the block - diagonal matrix @xmath534 having blocks @xmath535 on the diagonal .",
    "this leads us to the following analogue of lemma  [ lemma : a ] , whose proof is straightforward .",
    "[ lemma : ap ] we have @xmath536    the next step is to use the following factorization of @xmath532 , or more precisely of @xmath537 .",
    "the proof is the same as that of lemma  [ lemma : b ] , up to taking into account the reversals induced by the matrices @xmath535 .",
    "[ lemma : bp ] the equality @xmath538 holds , where @xmath539 is the block - diagonal matrix with blocks @xmath452 , for @xmath200 .",
    "we conclude the proof of our theorem as before , using the relations @xmath540 \\mc_k\\ , \\w_{\\vp , n } & = \\w_\\vp \\ , \\cc_{\\gamma_k , p , n},\\\\[1 mm ] \\xx_\\vq\\ , \\md_k&=\\cc_{\\eta_k , q } \\,\\xx_\\vq.\\end{aligned}\\ ] ]",
    "let @xmath541 be as in theorems  [ theo : mainsylv ] and  [ theo : mainsylv2 ] , and let as before @xmath139 .",
    "we are now going to extend the complexity estimates for matrix - vector multiplication given in corollary  [ coro : mul1 ] to more operators ( not only the basic ones ) , by providing reductions to the hankel operator @xmath542 .",
    "[ theo : equiv ] suppose that @xmath80 and @xmath81 hold .",
    "then for any displacement operator @xmath21 associated with @xmath87 , we can take @xmath543   \\sms(\\mcl , \\alpha ) & { \\leqslant}\\sms(\\nabla_{\\zz_{m,0},\\zz_{n,1}^t } , \\alpha+2 )      + o\\big(\\sd(\\vp,\\vq ) + \\alpha\\sc(\\vp,\\vq ) \\big ) , \\\\[1 mm ]   \\smi(\\mcl , \\alpha ) & { \\leqslant}\\smi(\\nabla_{\\zz_{m,0},\\zz_{m,1}^t } , \\alpha+2 )   + o\\big(\\sd(\\vp,\\vq ) + \\alpha\\sc(\\vp,\\vq ) + \\alpha^{\\omega-1 } m\\big ) .\\end{aligned}\\ ] ]    the rest of this section is devoted to the proof of this theorem . in what follows ,",
    "we write several formulas involving some matrices @xmath418 and  @xmath30 . in these formulas ,",
    "@xmath418 and @xmath30 will always be taken in @xmath544 and @xmath545 , respectively , for some @xmath546 ( and with @xmath547 when dealing with matrix inversion ) .",
    "table  [ tab4 ] shows that if a matrix @xmath1 is structured for one of the operators associated with @xmath87 , then simple pre- and post - multiplications transform @xmath1 into a matrix which is structured for one of the two _ basic _ operators associated with @xmath87 . all equivalences in this table",
    "follow directly from the identities @xmath362 and @xmath548 ( lemma  [ lemma : transp2 ] ) and from @xmath549 and @xmath467 being invertible and symmetric .",
    "combining these equivalences with lemma  [ lemma : y2 ] will lead to the cost bounds in lemma  [ lemma : redbasic ] below , expressed in terms of basic operators .",
    "@xmath550 \\delta_{\\cc_\\vp,\\cc_\\vq}(\\ma ) = \\mg\\ , \\mh^t & \\iff   \\delta_{\\cc_\\vp,\\cc_\\vq^t}(\\ma \\,\\y_\\vq ) = \\mg\\ , ( \\y_\\vq\\mh)^t   \\\\",
    "\\delta_{\\cc_\\vp^t,\\cc_\\vq^t}(\\ma ) = \\mg\\ , \\mh^t & \\iff   \\delta_{\\cc_\\vp,\\cc_\\vq^t}(\\y_\\vp\\ , \\ma ) = ( \\y_\\vp\\mg)\\ , \\mh^t   \\\\ \\delta_{\\cc_\\vp^t,\\cc_\\vq}(\\ma ) = \\mg\\ , \\mh^t & \\iff   \\delta_{\\cc_\\vp,\\cc_\\vq^t}(\\y_\\vp\\ , \\ma\\ , \\y_\\vq ) = ( \\y_\\vp\\mg ) ( \\y_\\vq\\mh)^t.\\end{aligned}\\ ] ]    [ lemma : redbasic ] let @xmath21 be a displacement operator associated with @xmath87 . then :    * if @xmath21 is a sylvester operator , we can take @xmath551    \\sms(\\mcl,\\alpha ) & { \\leqslant}\\sms(\\nabla_{\\cc_\\vp,\\cc_\\vq^t},\\alpha)+o(\\alpha \\sm(p)),\\\\[1 mm ]    \\smi(\\mcl,\\alpha ) & { \\leqslant}\\smi(\\nabla_{\\cc_\\vp,\\cc_\\vq^t},\\alpha)+o(\\alpha \\sm(m ) ) ;      \\end{aligned}\\ ] ] * if @xmath21 is a stein operator , we can take @xmath552    \\sms(\\mcl,\\alpha ) & { \\leqslant}\\sms(\\delta_{\\cc_\\vp,\\cc_\\vq^t},\\alpha)+o(\\alpha \\sm(p)),\\\\[1 mm ]    \\smi(\\mcl,\\alpha ) & { \\leqslant}\\smi(\\delta_{\\cc_\\vp,\\cc_\\vq^t},\\alpha)+o(\\alpha \\sm(m ) ) .",
    "\\end{aligned}\\ ] ]    we give the proof for the sylvester operator @xmath553 , which appears in the third row of table  [ tab4 ] ; the other five cases in that table can be handled similarly .",
    "suppose first that , given an @xmath21-generator @xmath25 for @xmath554 as well as a matrix @xmath555 , we want to compute @xmath109 .",
    "we begin by computing a @xmath556-generator @xmath557 for @xmath558 ( cf .  the third row of table  [ tab4 ] ) .",
    "lemma  [ lemma : y2 ] implies that this takes time @xmath559 .",
    "then , we evaluate @xmath560 from right to left . the products by @xmath561 and @xmath562 take time @xmath563 , and the product by @xmath413 takes time @xmath564 ; thus , the first claim is proved .",
    "suppose now that we are given a vector @xmath565 in addition to the matrix @xmath1 . in order to solve the system @xmath566 , we solve @xmath567 , with @xmath413 as defined above and @xmath568 .",
    "the latter system admits a solution if and only if the former one does ; if @xmath569 is a solution of @xmath567 , then @xmath570 is a solution of @xmath571 .",
    "hence , as before , we set up an @xmath89-generator @xmath557 for @xmath413 , using @xmath559 operations in  @xmath13 , and compute @xmath572 in time @xmath104",
    ". then , in time @xmath573 we either assert that the system @xmath567 has no solution , or find such a solution @xmath569 .",
    "finally , we recover @xmath221 in time @xmath468 .",
    "this proves the second claim .",
    "finally , assume that @xmath574 and consider the question of inverting @xmath1 .",
    "this matrix is invertible if and only if the matrix @xmath413 defined above is invertible .",
    "again , we set up a @xmath89-generator @xmath557 for @xmath413 , using @xmath575 operations in @xmath13 .",
    "then , in time @xmath576 we either assert that @xmath413 is not invertible or deduce a @xmath577-generator for @xmath578 , say @xmath579 .",
    "finally , if @xmath413 is invertible then , using @xmath580 and @xmath581 , we obtain a @xmath582-generator @xmath583 for @xmath31 in time @xmath584 .",
    "this proves the last claim .",
    "our second reduction is less straightforward : we use pan s idea of multiplicative transformation of operators  @xcite to reduce _ basic _ operators to an operator of hankel type .",
    "there is some flexibility in the choice of the target operator , here the sylvester operator @xmath542 ; the only ( natural ) requirement is that this target operator remains invertible .",
    "the following proposition summarizes the transformation process .",
    "although the formulas are long , they describe simple processes : for an operation such as multiplication , inversion or system solving , this amounts to e.g.  the analogue operation for an operator of hankel type , several products with simple matrices derived from @xmath67 and  @xmath84 , and @xmath193 matrix - vector products with the input matrix or its transpose .",
    "[ prop : redhank ] let @xmath585 , and suppose that @xmath80 and @xmath81 hold .",
    "then @xmath586 \\sms(\\mcl , \\alpha ) & { \\leqslant}\\sms(\\nabla_{\\zz_{m,0},\\zz_{n,1}^t } , \\alpha+2 )      + o\\big(\\sd(\\vp,\\vq ) + \\alpha\\sc(\\vp,\\vq ) \\big ) , \\label{prop : ii}\\\\[1 mm ] \\smi(\\mcl , \\alpha ) & { \\leqslant}\\smi(\\nabla_{\\zz_{m,0},\\zz_{m,1}^t } , \\alpha+2 )      + o\\big(\\sd(\\vp,\\vq ) + \\alpha\\sc(\\vp,\\vq ) + \\alpha^{\\omega-1 } m\\big ) \\label{prop : iii}.    \\end{aligned}\\ ] ]    the proof of proposition  [ prop : redhank ] will occupy the rest of this subsection . before that , though , we point out that theorem  [ theo : equiv ] follows directly from this proposition and lemma  [ lemma : redbasic ] . in particular , since @xmath587 for @xmath588 , overheads such as @xmath575 or @xmath559 that appear in that lemma are absorbed into terms such as @xmath589 that appear in the proposition .",
    "[ [ preliminaries ] ] preliminaries + + + + + + + + + + + + +    to establish each of the claims in the proposition above , it will be useful to have simple matrices @xmath590 and",
    "@xmath591 for pre-/post - multiplying @xmath1 and whose displacement rank with respect to @xmath592 and @xmath593 , respectively , is small .",
    "lemma  [ lemma : lr ] below shows that a possible choice , leading to displacement ranks at most @xmath242 , is @xmath594 in order to define generators for such matrices , we first rewrite the products @xmath595 and @xmath596 as @xmath597 and @xmath598 , and let @xmath599^t$ ] and @xmath600^t$ ] be the coefficient vectors of the polynomials @xmath601 and @xmath602 .",
    "then , we let @xmath603\\in \\f^m , \\quad \\u = \\y^{-1}_\\vp\\,\\w_\\vp\\,\\m \\in \\f^m , \\quad \\s = \\left [ \\begin{matrix } 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{matrix } \\right ] \\in \\f^n , \\quad \\r = -\\y_\\vq^{-1}\\,\\w_\\vq\\ , ( \\n + \\s ) \\in \\f^n.\\ ] ]    [ lemma : lr ] the relations @xmath604 and @xmath605 hold .    according to the second part of lemma  [ lemma : krylov ] , @xmath606 is equal to the krylov matrix @xmath376 , so that the columns of @xmath607 are @xmath608 @xmath609 .",
    "hence only the first column of @xmath610 is nonzero , and it is equal to @xmath611 .",
    "after transposition , this leads to @xmath612 .",
    "we can now check that @xmath613 as follows .",
    "first , from lemma  [ lemma : transp2 ] and since @xmath549 is invertible , we see that @xmath614 .",
    "then , the shape of @xmath615 implies that @xmath616 is the vector @xmath370 defined in the first part of lemma  [ lemma : krylov ] , so that @xmath617 .",
    "now , the special shape of @xmath370 implies that @xmath618 is the vector whose @xmath66th subvector of length @xmath619 contains the coefficients of @xmath620 . in other words , @xmath621",
    "this shows that @xmath622 , which concludes the proof of the first relation .    for the second relation , the proof is the same , taking into account that we are now considering the operator @xmath593 and that @xmath623 .",
    "[ [ proof - of - for - mclnabla_cc_vpcc_vqt ] ] proof of   for @xmath134 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath1 be in @xmath414 and let @xmath25 be a @xmath89-generator for @xmath1 . with @xmath624 as in the previous paragraph , define further @xmath625 \\quad\\text{and}\\quad    { \\mh ' } = \\big [ \\ , \\mr^t \\,\\ma^t\\,\\u\\ |\\ \\mr^t\\,\\mh\\ |\\",
    "\\s\\ , \\big].\\ ] ]    [ lemma : multpan ] the matrix @xmath626 satisfies @xmath627 .    applying the general formula @xmath628 ( which follows directly from  ( * ? ?",
    "? * theorem  1.5.4 ) ) and then using lemma  [ lemma : lr ] , we obtain @xmath629 which is the announced equality .    to compute a product of the form @xmath0 with @xmath630 , we first compute the matrices @xmath631 and @xmath632 described above .",
    "to obtain @xmath631 , it suffices to set up the vectors @xmath633 and @xmath634 and to compute @xmath635 matrix - vector products by  @xmath590 . given @xmath25 and @xmath633 , corollary",
    "[ coro : mul1 ] implies that @xmath634 is obtained in time @xmath636 on the other hand , it follows from lemmas  [ lemma : w ] and  [ lemma : y2 ] that the vector @xmath633 is obtained in time @xmath637 and , after some precomputation of time @xmath638 , that the @xmath635 products by @xmath590 are obtained in time @xmath639 .",
    "thus , overall , @xmath631 is obtained in time @xmath640 .",
    "we proceed similarly for @xmath632 .",
    "the only differences are that we have to do matrix - vector products involving @xmath641 and @xmath642 instead of @xmath590 and @xmath1 .",
    "for the former , lemmas  [ lemma : w ] and  [ lemma : y2 ] show that after a precomputation of cost @xmath637 , the cost of one such product is @xmath643 . for the latter , recall that , as pointed out in the introduction , from the given @xmath89-generator of @xmath1 , we can deduce in negligible time a @xmath582-generator of @xmath642 of the same length @xmath3 .",
    "this allows us to do matrix - vector products with @xmath642 with the same asymptotic cost @xmath644 as for @xmath1 .",
    "thus , the overall cost for computing @xmath632 is asymptotically the same as for @xmath631 .",
    "let us now bound the cost of deducing the product @xmath0 from @xmath645 .",
    "note first that under the coprimality assumptions @xmath80 and @xmath81 the matrices @xmath299 and @xmath646 are invertible , and so are @xmath590 and @xmath591 .",
    "consequently , @xmath647 and it suffices to bound the cost of each of the three products @xmath648 , @xmath649 , and @xmath650 . by reusing the same precomputation as for @xmath632 and applying again lemmas  [ lemma : w ] and  [ lemma : y2 ] ,",
    "we obtain @xmath651 for an additional cost of @xmath652 , via @xmath108 matrix - vector products by  @xmath653 .",
    "then , lemma  [ lemma : multpan ] says that @xmath413 is a hankel - like matrix for which a @xmath542-generator of length @xmath654 is @xmath655 .",
    "hence the cost for deducing @xmath656 from @xmath657 is at most @xmath658 .",
    "finally , @xmath650 is obtained for an additional cost of @xmath659 , by reusing the same precomputation as for @xmath631 and by performing @xmath108 matrix - vector products by @xmath660 .    to summarize",
    ", we have shown that @xmath655 can be obtained in time @xmath661 and that @xmath0 can be deduced from @xmath645 in time @xmath662 .",
    "adding these two costs thus proves the bound   in proposition  [ prop : redhank ] for @xmath134 .",
    "[ [ proof - of - for - mclnabla_cc_vpcc_vqt-1 ] ] proof of   for @xmath134 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for @xmath14 given by some @xmath89-generator @xmath25 of length @xmath3 and given @xmath663 in @xmath298 , we now want to find a ( nontrivial ) solution of @xmath664 , or determine that no solution exists .    define @xmath665 . because @xmath590 and @xmath591 are invertible matrices ,",
    "solving the system @xmath666 is equivalent to solving @xmath667 , with then @xmath668 . as in the previous paragraph",
    ", we can compute a generator @xmath655 of @xmath413 for the operator @xmath542 in time @xmath669 .",
    "the cost of computing @xmath572 from @xmath663 ( and @xmath221 from @xmath569 ) fits into the same bound , and solving the new system @xmath670 takes time @xmath671 .",
    "summing these costs , we prove the second item in the proposition .",
    "[ [ proof - of - for - mclnabla_cc_vpcc_vqt-2 ] ] proof of   for @xmath134 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    assume now that @xmath574 and that @xmath249 is given by a @xmath89-generator @xmath25 of length @xmath3",
    ". as before , @xmath590 and @xmath591 are invertible by assumption , so that @xmath1 is invertible if and only if @xmath672 is invertible . by lemma  [ lemma : multpan ]",
    "this matrix @xmath413 has displacement rank at most @xmath654 for @xmath673 .",
    "thus , as explained in the introduction , if @xmath413 is invertible then its inverse has displacement rank at most @xmath654 for @xmath674 .",
    "the next lemma shows that if @xmath675 is a @xmath674-generator for the inverse of @xmath413 , then a @xmath577-generator for the inverse of @xmath1 is given by the matrices @xmath676 \\quad\\text{and}\\quad   \\mh_{\\rm inv } = \\big [ \\ \\ml^t \\,{\\ma'}^{-t}\\,\\s\\ |\\ \\ml^t\\,\\mh_{\\rm inv}'\\",
    "|\\ \\u\\ \\big].\\ ] ]    [ lemma : generation_of_the_inverse_of_a ] the matrix @xmath31 satisfies @xmath677 .    as for lemma  [ lemma : multpan ] the proof follows from  ( * ? ?",
    "* theorem  1.5.4 ) and lemma  [ lemma : lr ] : @xmath678 to conclude , we remark that @xmath679 .",
    "the bound in   can now be established as follows .",
    "we begin by computing @xmath655 , which by lemma  [ lemma : multpan ] is a @xmath673-generator of length @xmath654 of  @xmath413 ; as shown in the previous paragraph , this is done in time @xmath661 .",
    "then , in time @xmath680 we either conclude that @xmath413 is singular , or produce a @xmath674-generator @xmath681 of length @xmath654 for the inverse of @xmath413 . finally , if @xmath413 is invertible we proceed in two steps : first , using the same amount of time as for @xmath631 and @xmath632 , we set up the matrices @xmath682 and @xmath683 introduced before lemma  [ lemma : generation_of_the_inverse_of_a ] .",
    "then we reduce each of these two matrices to arrive at a generator of length @xmath3 ; this generator compression step can be done in time @xmath684 , in view of  ( * ? ? ?",
    "* remark  4.6.7 ) .",
    "overall , these costs add up to the result reported in  .    [ [ proof - of - for - mcldelta_cc_vpcc_vqt ] ] proof of     for @xmath135 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we only sketch the proof in these cases .",
    "this time , we rely on the ( easily verified ) general formula @xmath685 we use it to write @xmath686 this allows us to reduce questions ( multiplication , linear system solving , inversion ) related to @xmath1 to the same questions for @xmath687 , where @xmath413 is given through a @xmath688-generator of length @xmath654 . then , the relations @xmath689 and @xmath690 imply @xmath691 this allows us to reduce our problems to computations with the matrix @xmath692 given by means of a @xmath542-generator of length @xmath654 .    for inversion ( where @xmath574 )",
    ", inverting @xmath692 leads to a @xmath674-generator of @xmath693 .",
    "then , using the relations on @xmath694 given above , we obtain @xmath695 and thus a @xmath696-generator of @xmath697 .",
    "the general formula above further leads to @xmath698 since @xmath699 , this gives a @xmath700-generator of @xmath31 whose length @xmath654 is finally reduced to @xmath3 .    in all cases",
    "lemma  [ lemma : lr ] can be reused , and the requested complexity bounds are then derived in the same way as for the three cases above , up to a minor difference : we also need to take into account the cost of multiplication by @xmath91 or its transpose with a vector .",
    "however , due to the sparse nature of this matrix , this cost is easily seen to be @xmath701 , so it does not impact the asymptotic estimate .",
    "in this section , we prove bounds on the cost of the product @xmath0 , where @xmath1 is a structured matrix in @xmath414 and @xmath4 is an arbitrary matrix in @xmath702 ; this will prove the claims in theorems  [ theo : mainsylv ] and  [ theo : mainsylv2 ] regarding multiplication .",
    "[ theo : mul ] for any invertible operator @xmath21 associated with @xmath87 , we can take @xmath703 with @xmath139 , @xmath145 , and @xmath146 .",
    "using lemma  [ lemma : redbasic ] , it will be sufficient to consider multiplication for the basic operators associated with @xmath87 .",
    "thus , throughout this section we let @xmath704 be one of the two operators @xmath705 and @xmath90 and , given @xmath706 and @xmath707 , we show how to compute the product @xmath708 with @xmath1 the unique matrix in @xmath414 such that @xmath709 .",
    "in subsection  [ ssec : mulss ] we deal with the cases covered by theorem  [ theo : rec ] ; a key step of the proof ( corresponding to the case @xmath710 ) is deferred to subsection  [ section : r ] due to its length .",
    "we then extend our results to all remaining cases in subsection  [ ssec : rgen ] .    in view of theorem  [ theo :",
    "equiv ] , it would actually be sufficient to assume that @xmath21 is the hankel operator @xmath542 .",
    "this would allow us to bypass most of the derivation in subsection  [ ssec : rgen ] ; however , such a restriction does not seem to lead to a better cost estimate than our direct approach , which we include for completeness .",
    "let us first assume that @xmath134 .",
    "given @xmath67 , @xmath84 and the generator @xmath25 , and a matrix @xmath711 , theorem  [ theo : rec ] shows that @xmath109 can be computed as follows :    1 .",
    "[ s0 ] compute the polynomials @xmath73 , @xmath396 , @xmath438 , @xmath441 , @xmath712 for @xmath713 and @xmath481 ; 2 .",
    "[ s1 ] compute the @xmath714 matrix @xmath715 ; 3 .",
    "[ s2 ] compute the @xmath716 matrix @xmath217 given by @xmath717 4 .",
    "[ s3 ] return the @xmath716 matrix @xmath718 .",
    "proceeding as in the proof of corollary  [ coro : mul1 ] , we see that steps  [ s0 ] , [ s1 ] , [ s3 ] can be completed in time @xmath719 .",
    "we are thus left with analyzing step  [ s2 ] .",
    "let @xmath720 be the columns of @xmath651 , and for @xmath721 let @xmath722_n$ ] be the polynomial whose coefficients are the entries of @xmath219 . in polynomial terms , given @xmath723 in @xmath49_n$",
    "] , we want to compute @xmath724 in @xmath49_m$ ] such that @xmath725 for @xmath726 ; then , the @xmath23 coefficients of @xmath727 will make up the @xmath66th column of the matrix @xmath217 .    to compute @xmath727 , we can compute the sum first , and then reduce it modulo @xmath73 .",
    "the core problem is thus to compute @xmath728 in @xmath49_{m+n-1}$ ] for all @xmath726 ; the cost is given in theorem  [ theo : rgen ] of subsection  [ ssec : rgen ] below : writing @xmath145 and @xmath146 , one can compute @xmath729 in time @xmath730 finally , since @xmath731 has degree less than @xmath732 and since @xmath73 has degree @xmath23 , computing each @xmath733 takes time @xmath734 , so we can deduce @xmath735 from @xmath736 for a negligible total cost of @xmath559 .    in the case",
    "@xmath135 , the approach is almost the same .",
    "a minor difference is that we replace @xmath473 by @xmath474 : this has no influence on the cost .",
    "the other difference is the matrix  @xmath737 appearing in the inversion formula for @xmath90 ( second part of theorem  [ theo : rec ] ) ; in polynomial terms , this now leads us to compute the sums @xmath738 for @xmath739 , and then to reduce each of them modulo @xmath73 .",
    "this problem is actually an instance of the question treated above , for we have @xmath740 since knowing @xmath741 gives us @xmath742 for free , we can appeal here as well to theorem  [ theo : rgen ] to compute @xmath742 , using @xmath743 instead of @xmath438 .",
    "the cost estimate is the same as before , namely ,  ( [ eq : cos - of - thm31 ] ) , thus leading to theorem  [ theo : mul ] regarding a product of the form @xmath109 for the operator @xmath135 .",
    "the previous subsection has shown that for both sylvester s and stein s displacement operators , computing the product @xmath0 essentially reduces to the following basic problem : given as input    * a monic polynomial @xmath396 of degree @xmath24 , * polynomials @xmath745 in @xmath49_m$ ] and @xmath746 in @xmath49_n$ ] ,    compute @xmath747 the naive algorithm computes all sums independently ; writing @xmath139 , its cost is @xmath748 . in this subsection",
    "we establish the following improved complexity estimate in the special case where @xmath744 .",
    "( this result will be further extended to the case of a general @xmath396 of degree @xmath24 in subsection  [ ssec : rgen ] . )",
    "[ theo : r ] assume @xmath744 and @xmath749 , and let @xmath588 , @xmath145 , and @xmath146 .",
    "then one can compute the polynomials @xmath729 defined above in time @xmath750    to prove theorem  [ theo : r ] , we first rephrase our problem in polynomial matrix terms .",
    "let @xmath751_m^{\\alpha\\times 1},\\quad \\mv \\in \\f[x]_n^{\\alpha\\times    1},\\quad \\mw \\in \\f[x]_n^{\\beta\\times 1}\\ ] ] be the polynomial vectors with respective entries @xmath752 , @xmath753 and @xmath754 .",
    "then , our problem amounts to computing the polynomial row vector @xmath755_{m+n-1}^{1\\times\\beta}$ ] such that @xmath756 we first solve that problem in the special case where @xmath757 ( this is where most of the difficulty takes place ) ; then , we reduce the other cases of arbitrary @xmath758 to this case .    [ [ case - alphabeta ] ] case @xmath757 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the following lemma is the basis of our algorithm in this case ; it is taken from the proof of  ( * ? ? ?",
    "* lemma  12 ) .",
    "[ lem : recursive - formula - for - r ] let @xmath759 be in @xmath760 with @xmath761 even .",
    "let @xmath762 be in @xmath49_\\nu^{\\alpha\\times\\gamma}$ ] and define @xmath763 @xmath764 then the matrices @xmath765 $ ] and @xmath766 $ ] are in @xmath49_{\\nu/2}^{\\alpha \\times 2\\gamma}$ ] , and we have @xmath767[\\mw_1 \\,\\ , \\mw_0]^t \\bmod x^{\\nu/2}\\big).\\ ] ]    we will want to apply this lemma recursively , starting from @xmath768 and @xmath769 .",
    "it will be convenient to have both @xmath24 and @xmath3 be powers of two .    *",
    "if @xmath24 is not a power of two , we define @xmath770 and @xmath771 .",
    "one may then check that @xmath772 for any @xmath773 and nonzero @xmath774 in @xmath49 $ ] ; applying this identity componentwise to the definition of @xmath591 gives @xmath775 where @xmath776 and @xmath777 have degree less than @xmath778 . * if @xmath3 is not a power of two , we define @xmath779 and @xmath780 .",
    "then , we can introduce @xmath781 dummy polynomials @xmath752 , @xmath753 and @xmath754 , all equal to zero , without affecting the value of @xmath782",
    ".    the resulting algorithm is given in figure  [ fig1 ] , with a top - level procedure mul that handles the cases when @xmath24 or @xmath3 are not powers of two by defining @xmath778 and @xmath783 as above , and a main recursive procedure mul_rec .",
    "we initially set @xmath784 and @xmath769 ; through each recursive call , the width @xmath785 of the matrices @xmath776 and @xmath786 is doubled , while their degree @xmath761 is divided by two , so that the invariant @xmath787 is maintained .",
    "[ prop : recursive - routine ] let @xmath788 .",
    "algorithm mul of figure  [ fig1 ] works correctly in time @xmath789    correctness is a direct consequence of lemma  [ lem : recursive - formula - for - r ] and the discussion that followed it .",
    "for the cost analysis , we let @xmath790 denote the cost of mul_rec called upon parameters @xmath783 and @xmath785 such that @xmath791 .",
    "note that the total cost of mul will then be at most @xmath792 , with @xmath793 .",
    "in particular , below , we always have @xmath794 .",
    "another useful remark is that we always have @xmath795 , since @xmath796 and @xmath797 .",
    "if @xmath798 , we have @xmath799 , so we are in the base case of the algorithm , where @xmath776 and @xmath786 are in @xmath49^{\\overline \\alpha\\times \\overline \\alpha}_\\nu$ ] .",
    "we first compute @xmath800 in time @xmath801 .",
    "then , given @xmath802 in @xmath49_m^{\\overline \\alpha\\times      1}$ ] and @xmath803 in @xmath49_\\nu^{\\overline\\alpha\\times\\overline\\alpha}$ ] , we obtain @xmath804 in two steps as follows : rewriting @xmath805 in the form @xmath806\\mu'$ ] with @xmath807 and @xmath808_c^{\\overline\\alpha\\times\\overline\\alpha}$ ] , we compute first @xmath809 in time @xmath810 , and then deduce @xmath591 from @xmath811 using at most @xmath812 additions in @xmath13 . in summary ,",
    "@xmath813 using @xmath814 and @xmath815",
    ". therefore , there is a constant @xmath816 with @xmath817    let us now bound @xmath790 when @xmath818 .",
    "given @xmath819_\\nu^{\\overline \\alpha\\times\\gamma}$ ] , we can compute @xmath820 , @xmath821 , @xmath822 , @xmath823 for free .",
    "then @xmath803 is computed recursively in time @xmath824 , and it remains to bound the cost of producing the result as @xmath825 , with @xmath826 .",
    "for now let us write @xmath827 for the cost of computing @xmath811 .",
    "given @xmath803 in @xmath49_{m+\\nu'-1}^{1\\times\\overline\\alpha}$ ] with @xmath828 and @xmath811 in @xmath49_{m+\\nu-1}^{1\\times\\overline\\alpha}$ ] , we can add them together using at most @xmath829 additions in @xmath13 .",
    "since @xmath830 , we deduce that for @xmath831 , @xmath832 in order to bound @xmath827 let us rewrite as before @xmath805 as @xmath833\\mu'$ ] with @xmath834 and @xmath808_c^{\\gamma \\times \\overline\\alpha}$ ] ; we compute @xmath811 as @xmath835    ( \\mu ' \\mv_0 \\mw_0^t)$ ] .",
    "the product @xmath836 involves three polynomial matrices of respective dimensions @xmath837 , @xmath838 , @xmath837 , with entries of degree less than @xmath839 .",
    "furthermore , since @xmath840 and , as seen above , @xmath841 , we have @xmath842 .",
    "since @xmath843 , we can proceed as in the second case considered in the proof of  ( * ? ? ? * lemma  7 ) to show that @xmath836 can be evaluated as @xmath844 in time @xmath845 since @xmath836 has dimensions @xmath846 and degree less than @xmath847 , reconstructing @xmath811 from that product requires at most @xmath848 additions in @xmath13 .",
    "thus , using @xmath791 , @xmath849 combining this bound with  ( [ eq : ck ] ) and the fact that @xmath850 , we deduce that there exists a constant @xmath851 such that for all @xmath831 , @xmath852    taking all @xmath853 into account , we deduce from  ( [ eq : c0-detailed ] ) and  ( [ eq : ck - detailed ] ) that the total time of mul is bounded as @xmath854 the conclusion follows from the facts that @xmath855 and that @xmath856 .",
    "[ [ case - alpha - beta ] ] case @xmath857 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this situation , we split the vector @xmath786 into vectors @xmath858 , with each @xmath859 in @xmath49_n^{\\alpha \\times 1}$ ] and @xmath860 ( so @xmath861 may be padded with zeros ) .",
    "then , algorithm mul is applied @xmath862 times ( namely , to the @xmath863 ) and by lemma  [ prop : recursive - routine ] we obtain @xmath736 in time @xmath864    [ [ case - beta - alpha ] ] case @xmath865 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this case , we split the vectors @xmath802 and @xmath776 into @xmath866 and @xmath867 , with each @xmath868 and @xmath869 in @xmath49_n^{\\beta    \\times 1}$ ] and @xmath870 .",
    "as before , algorithm mul is applied @xmath862 times , but now to the @xmath871 , for a cost of @xmath872 the @xmath862 results thus produced are then added , for a negligible cost of @xmath873 .    in the next subsection ,",
    "we denote by mul(@xmath874 ) the algorithm that handles all possible cases for @xmath749 , following the discussion in the above paragraphs .",
    "we now address the case of an arbitrary @xmath396 .",
    "given such a @xmath396 of degree @xmath24 in @xmath49 $ ] , as well as @xmath745 in @xmath49_m$ ] , @xmath875 in @xmath49_n$ ] and @xmath876 in @xmath49_n$ ] , we recall that our goal is to compute @xmath877    [ theo : rgen ] assume @xmath749 , and let @xmath588 , @xmath145 , and @xmath146",
    ". then one can compute the polynomials @xmath729 in time @xmath750    to prove theorem  [ theo : rgen ] , the idea is to use the well - known fact that euclidean division reduces to power series inversion and multiplication ; in this way , the proof of theorem  [ theo : rgen ] will reduce to that of theorem  [ theo : r ] .",
    "the first step is to replace remainders by quotients . by applying the euclidean division equality @xmath878 and defining @xmath879 we easily deduce the following lemma .",
    "@xmath880 for @xmath726 .",
    "the main issue in our algorithm will be the computation of @xmath881 ; we will actually compute their reverse polynomials . observe that @xmath882 has degree at most @xmath230",
    ", @xmath883 has degree at most @xmath884 , and @xmath885 has degree at most @xmath886 .",
    "we thus introduce the polynomials @xmath887 ; knowing those , we can recover the polynomials @xmath885 for free . for @xmath420 and @xmath726 ,",
    "let us also define @xmath888 all these polynomials are related by the following formula .",
    "@xmath889 for @xmath726 .",
    "using that @xmath890 for any polynomials @xmath891 $ ] of respective degrees @xmath892 and @xmath893 , we deduce from the definitions of @xmath885 and @xmath894 that @xmath895 following  @xcite , we conclude with the equalities @xmath896    figure  [ fig2 ] details the algorithm we just sketched . to prove theorem  [ theo : rgen ]",
    ", it suffices to observe that the cost boils down to one call to @xmath174 plus @xmath897 for all other operations , and then to apply theorem  [ theo : r ] .",
    "we conclude in this section by establishing the cost bounds announced in theorem  [ theo : mainsylv ] for structured inversion and structured system solving .",
    "we rely on theorem  [ theo : equiv ] from section  [ sec : equiv ] to reduce our task to the case of the hankel operator @xmath542 , then use the fast multiplication algorithm of section  [ sec : multiplication_alogrithms ] to speed up the morf / bitmead - anderson ( mba ) approach .",
    "[ theo : inv ] for any invertible operator @xmath21 associated with @xmath87 , we can take @xmath898 and @xmath899    recall that theorem  [ theo : equiv ] in section  [ sec : equiv ] proves that we can take @xmath900 and @xmath901 so we are left with estimating the cost of inversion and system solving for @xmath542 . for this , we shall use the mba approach with preconditioning introduced in  @xcite together with the `` compression - free '' formulas from  @xcite for generating the matrix inverse .",
    "such formulas being expressed with products of the form ( structured matrix ) @xmath902 ( unstructured matrix ) , this provides a way to exploit our results  for fast structured matrix multiplication from section  [ sec : multiplication_alogrithms ] .",
    "specifically , starting from a @xmath542-generator @xmath25 of length @xmath3 of @xmath14 , we proceed in three steps as  follows .",
    "_ preconditioning . _ from @xmath25 we begin by deducing a generator of the preconditioned matrix @xmath903 where each @xmath904 is a unit upper triangular toeplitz matrix defined by some random vector  @xmath905 ; as shown in  @xcite , such a preconditioning ensures that , with high probability , @xmath906 has _ generic rank profile _",
    ", that is , denoting by @xmath907 the leading principal submatrix of @xmath906 of dimensions @xmath908 , @xmath909    in our case , preconditioning will be done in such a way that the generator of @xmath906 corresponds not to @xmath542 but to another hankel operator , namely @xmath910 ( algorithm precond , figure  [ fig : precond ] ) .",
    "the latter operator being singular but still _ partly regular _",
    "@xcite , such a generator will in fact be a triple @xmath911 , with @xmath912 of length @xmath203 and @xmath913 carrying the last row of @xmath906 .",
    "also , we will guarantee that the matrices @xmath914 and @xmath915 have the following special shape : their first @xmath3 columns are precisely @xmath916 and @xmath917",
    ". these two requirements will be key to exploit the `` compression - free '' inversion scheme of  @xcite and then to move back from , say , @xmath918 to @xmath31 .",
    "_ computation of the leading principal inverse .",
    "_ assuming that @xmath906 has generic rank profile and given @xmath911 , we now consider computing a compact representation of the inverse of its largest nonsingular leading principal submatrix , that is , of @xmath919 with @xmath920 since @xmath921 is structured with respect to @xmath922 , its inverse is structured with respect to @xmath923 and , as recalled in  ( [ eq : mclp ] ) , @xmath924 therefore , our second step ( detailed in section  [ ssec : leading_principal_inverse ] ) will be to compute a @xmath923-generator of @xmath919 of length @xmath925 .",
    "in fact , following  @xcite , we give an algorithm ( , figure  ( [ fig : lpinv ] ) ) that does _ not _ assume @xmath906 has generic rank profile but discovers whether this is the case or not .",
    "it calls first an auxiliary routine , called , which returns the size @xmath487 of the largest leading principal submatrix @xmath926 having generic rank profile together with a generator of the inverse @xmath927 . then , from such a generator , one can check efficiently whether @xmath487 equals @xmath928 , which is a condition equivalent to @xmath906 having generic rank profile .",
    "thus , overall , algorithm  generates @xmath919 if and only if @xmath906 has generic rank profile ( and reports failure otherwise , since this provides a way of certifying whether preconditioning @xmath1 was successful or not ) .",
    "algorithm  calls a core recursive routine , which can be seen as a combination of kaltofen s algorithm leading principal inverse  @xcite and  ( * ? ? ?",
    "* algorithm  geninvhl ) , which thus relies on products of the form ( structured matrix ) @xmath902 ( unstructured matrix ) .",
    "also , the generating matrices @xmath929 and @xmath930 produced by algorithm are specified to have the following shape : @xmath931 where @xmath932 is made from the first @xmath487 rows of @xmath914 , and similarly for @xmath933 .",
    "when @xmath934 , the same specification is inherited by the generator @xmath935 produced by algorithm , whose matrices satisfy @xmath936 where @xmath937 and @xmath938 are the first @xmath892 rows of @xmath914 and  @xmath915 , and",
    "@xmath939 is the first row of @xmath919 .    _",
    "generating inverses and solving linear systems .",
    "_ given the rank @xmath892 of @xmath906 and the @xmath923-generator @xmath935 of @xmath940 as above , it is immediate to decide whether @xmath906 and @xmath1 are invertible and , if so , to deduce the @xmath941-generator of @xmath31 given by @xmath942 this corresponds to algorithm inv in figure  [ fig : algo_inv ] .",
    "now , given an additional vector @xmath943 , we reduce the study of @xmath944 to that of the equivalent linear system @xmath945 , where @xmath946 and for which an algorithm can be derived directly from the one in  @xcite , with the additional guarantee that if @xmath947 and the column rank of @xmath1 is not full , then a nonzero solution @xmath221 is obtained .",
    "this corresponds to algorithm solve in figure  [ fig : algo_solve ] .",
    "( clearly , both inv and solve are las vegas algorithms``always correct , probably fast '' , thanks to the specification of algorithm . )",
    "the next three sections provide detailed descriptions of the algorithms mentioned above , namely precond , , , , inv , solve , together with their correctness and complexity proofs , thereby establishing theorem  [ theo : inv ] .",
    "we precondition our structured matrix @xmath1 as shown in figure  [ fig : precond ] below . here and hereafter",
    ", @xmath948 denotes the @xmath66th unit vector in @xmath949 and , for any given @xmath950 matrix @xmath17 such that @xmath951 , we write @xmath952 for the @xmath953 matrix obtained by keeping only the first @xmath3 columns of @xmath954 .    [ lem : preconditioning ] algorithm precond works correctly in time @xmath955 .",
    "furthermore , if the vectors @xmath956 and @xmath957 have their first entry equal to @xmath242 and their remaining @xmath958 entries chosen uniformly at random from a finite subset @xmath959 , then the matrix @xmath960 has generic rank profile with probability at least @xmath961 where @xmath962 and @xmath963 is the cardinality of @xmath215 .",
    "we start by checking @xmath964 , from which correctness follows .",
    "writing @xmath965 and @xmath966 and applying  ( [ eq : general - formula - abc ] ) gives @xmath967 and it remains to check that @xmath968 and @xmath969 .",
    "since @xmath970 is upper triangular toeplitz , the matrix @xmath971 is zero everywhere except on its last column , which is equal to @xmath972 , and on its first row , which is equal to @xmath973 .",
    "hence @xmath974 . for @xmath975",
    ", we proceed similarly , by noting that @xmath976 is zero everywhere but on its first column and last row , equal to @xmath977 and @xmath978 , respectively .",
    "let us now bound the cost of deducing @xmath914 , @xmath915 , @xmath913 from @xmath418 , @xmath30 , @xmath979 , @xmath980 .",
    "first , we set up the @xmath981 matrices @xmath982 and @xmath983 and the @xmath984 matrices @xmath985 and @xmath986 in time @xmath393 .",
    "then , since @xmath1 satisfies @xmath987 , multiplying @xmath1 or @xmath642 be a single vector can be done in time  @xmath955 , using for example the reconstruction formula in  ( * ? ? ?",
    "* ( 21b ) ) .",
    "hence we obtain the products @xmath988 , @xmath989 , and @xmath990 in time @xmath955 . finally , since @xmath418 and @xmath30 have @xmath3 columns each , it remains to multiply @xmath203 vectors by the triangular toeplitz matrices @xmath970 and @xmath991 , and this can be done in time @xmath955 .",
    "overall , the cost of the algorithm is thus bounded by @xmath955 .",
    "the probability analysis is due to kaltofen and saunders  ( * ? ? ?",
    "* theorem  2 ) .      in order to generate the inverse of the largest nonsingular leading principal submatrix of  @xmath1 , we start with the generation of the largest leading principal submatrix having generic rank profile .",
    "this is done by algorithms  and  displayed in figures  [ fig : algo_largestrec ] and  [ fig : algo_largest ] . in algorithm",
    "the matrices @xmath992 , @xmath427 , @xmath430 for @xmath993 have dimensions @xmath994 , @xmath995 , @xmath996 , respectively , and correspond to the block partitions @xmath997 similarly , @xmath998 denotes the last row of @xmath992 .    [",
    "lem : largest_rec ] algorithm  is correct and if @xmath574 is an integer power of two , then its cost is @xmath999    correctness follows directly from combining the analysis in  @xcite with the formulas for @xmath1000 , @xmath1001 , @xmath1002 , @xmath1003 , @xmath1004 , @xmath615 given in  @xcite .    assuming @xmath574 is a power of two , let @xmath1005 denote the cost of algorithm .",
    "we begin by showing that we can take @xmath1006 given @xmath418 and @xmath30 we compute the product @xmath1007 in time @xmath1008 .",
    "then , starting from the last row of @xmath1 and using the fact that for @xmath1009 the @xmath318 entry of @xmath1007 equals @xmath1010 , we deduce all the entries of @xmath1 in time @xmath1011 .",
    "let us now bound the cost when @xmath1012 .",
    "proceeding as in the proof of  ( * ? ? ?",
    "* lemma  5 ) , it is easily seen that one can compute some generators of length at most @xmath654 for the matrices @xmath1013 , @xmath1014 , @xmath1015 , @xmath1016 as well as the vectors @xmath1017 , @xmath1018 , @xmath1019 , @xmath1002 for a total time in @xmath1020 .",
    "the number of additions is in @xmath1021 and we also have to perform the four matrix products @xmath1022 , @xmath1023 , @xmath1024 , and @xmath1025 .",
    "for example , @xmath1026 has displacement rank at most @xmath654 with respect to the operator @xmath1027 , so that using a decomposition of the form @xmath1028 with @xmath1029 and @xmath1030 of displacement ranks at most @xmath3 and @xmath1031 , respectively , we can evaluate the product @xmath1022 in time @xmath1032 ; by theorem  [ theo : mainsylv2 ] , this is in @xmath1033 . adding up all these costs",
    "thus leads to @xmath1034 hence , for some constant @xmath816 , @xmath1035 with @xmath1036 such that @xmath1037 , that is , @xmath1038 . defining @xmath1039",
    ", we have @xmath1040 $ ] , which implies @xmath1041 and thus , since @xmath23 is an integer power of two , @xmath1042 besides , @xmath1043 , which by assumption on @xmath148 implies @xmath1044 third , @xmath1045 .",
    "consequently , for some constant @xmath851 , @xmath1046 now , @xmath1047 implies @xmath1048 , where @xmath1049 denotes the smallest integer power of two greater than or equal to @xmath1050 .",
    "hence @xmath1051 and @xmath1052 by assumption on @xmath148 , so that the sum in the above bound on @xmath1005 is in @xmath1053 , as announced .",
    "[ lem : largest ] let @xmath588 .",
    "algorithm  works correctly in time @xmath1054    let us show first how to generate the augmented matrix @xmath1055 .",
    "if @xmath1056 , then @xmath1057 and it suffices to take @xmath1058 , @xmath1059 , and @xmath1060 . if @xmath1061 then @xmath1062 leads to the following generator of length @xmath1063 : @xmath1064;\\ ] ] the range constraint on @xmath1065 is satisfied , since @xmath1066 , and , on the other hand , the upper left corners of @xmath1067 and @xmath1068 are @xmath418 and @xmath30 , respectively .",
    "proceeding similarly when @xmath1069 , we take @xmath1070 , \\qquad \\overline{\\mh } = \\begin{bmatrix } \\mh & \\\\ & \\e_{\\overline{p}-n,1 } \\end{bmatrix}\\!,\\ ] ] where @xmath1071 is the last column of @xmath1 and can be obtained in time @xmath1072 .",
    "it remains to handle the case @xmath1073 , where @xmath1 is bordered by both some zero rows and some zero columns . in this case , we deduce from @xmath1062 that a @xmath1074-generator of length @xmath654 of @xmath1055 is given by @xmath1075 again , it is clear that the upper left corners of @xmath1067 and @xmath1068 are @xmath418 and @xmath30 , respectively .",
    "furthermore , if @xmath1076 , we can take @xmath1077 ; otherwise , since @xmath1078 , we are in the situation where @xmath1079 and @xmath1080 . in this case",
    "we shall proceed as follows to reduce the generator length from @xmath654 to @xmath635 while ensuring that @xmath418 and @xmath30 are in the upper left corners of the new generator matrices : the matrices @xmath1067 and @xmath1068 defined above have dimensions @xmath1081 and are given  by @xmath1082 since @xmath418 is @xmath1083 , we can deduce from @xmath418 and @xmath1071 a vector @xmath1084 such that @xmath1085 in time @xmath1008 ; then , using the fact that @xmath1086 we conclude that a suitable @xmath1074-generator of @xmath1055 is obtained by taking the first @xmath1087 columns of @xmath1088 and @xmath1089 . finally , obtaining the vector @xmath1090 is free , since the last row of @xmath1055 is either the last row of @xmath1 ( which is part of the input ) or the zero row .",
    "to summarize , we can always find a suitable generator @xmath1091 of the augmented matrix @xmath1055 in time @xmath1092 , that is , since @xmath1093 and @xmath1094 , @xmath1095    since @xmath1096 and @xmath1097 is a power of two , lemma  [ lem : largest_rec ] implies that the output @xmath1098 of  is obtained in time @xmath1099 and satisfies the following : @xmath487 is the size of the largest leading principal submatrix of  @xmath1 having generic rank profile ; furthermore , denoting this matrix by @xmath1100 , we have @xmath1101 equal to the first row of @xmath1102 and @xmath1103 and @xmath1104 with @xmath1105 and @xmath1106 the @xmath1107 submatrices consisting of the first @xmath487 rows of @xmath1067 and @xmath1068 . since @xmath1108 and @xmath1109 are in the upper left corners of @xmath1067 and @xmath1068 and since @xmath1110 , we deduce that @xmath1111 \\qquad \\mbox{and } \\qquad \\overline{\\mh}_\\ell = [ \\mh_\\ell \\,\\,*].\\ ] ] consequently , by keeping only the first @xmath3 columns of each of @xmath1112 and @xmath1113 , we obtain the matrix pair @xmath1114 such that @xmath1115 this concludes the proof of correctness .",
    "for the cost , note that the smallest integer power of two greater than or equal to @xmath1116 is less than @xmath1117 and , on the other hand , and that @xmath1118 .",
    "hence @xmath1119 , and the latter expression dominates over the term in @xmath1120 .",
    "[ lem : lp_inv ] let @xmath788 .",
    "algorithm  works correctly in time @xmath1121    by lemma  [ lem : largest ] , the call to  yields @xmath1122 such that @xmath487 is the order of the largest leading principal submatrix of @xmath1 having generic rank profile , @xmath1123 , @xmath1124 , and @xmath1125 .",
    "thanks to the special shape of the matrices @xmath1126 and @xmath1127 , we can check as in  @xcite that the expressions for @xmath1000 , @xmath1001 , @xmath1002 lead to a @xmath1128-generator of length @xmath635 of the schur complement @xmath1129 : @xmath1130 [ \\mh_\\ms |\\u_\\ms]^t.\\ ] ] now , since both @xmath1128 and @xmath1100 are invertible , @xmath1131 correctness then follows , since @xmath1132 if and only if @xmath1 has generic rank profile .    to bound the cost , recall first from lemma  [ lem : largest ] than @xmath487 , @xmath1126 , @xmath1127 , @xmath1133 are computed in time @xmath1134 .",
    "then we can obtain @xmath1000 in time @xmath1135 by evaluating the product @xmath1136 as follows .",
    "defining @xmath1137 \\in \\f^{m\\times n}$ ] and @xmath1138 \\in \\f^{n \\times \\alpha}$ ] , we can deduce from @xmath1139 a @xmath1140-generator @xmath1141 of length @xmath1142 of @xmath1143 in time @xmath1144 .",
    "writing @xmath1145 with @xmath1146 of displacement rank @xmath1147 and @xmath1148 of displacement rank  @xmath1031 , we can evaluate @xmath1149 in time @xmath1150 , which by theorem  [ theo : mainsylv2 ] is in @xmath1151 .",
    "it remains to extract @xmath1152 from @xmath1153 $ ] and to add it to @xmath985 , for an overhead of @xmath873 .",
    "since @xmath1154 , we have thus obtained @xmath1000 in time @xmath1135 . the same cost bound can be derived for the matrix @xmath1001 and , on the other hand , the cost bound @xmath1155 applies to the computation of @xmath1002 and follows from computing a generator of length @xmath203 of @xmath1156 and then multiplying by and subtracting from a vector . finally , given @xmath1000 @xmath1001 , @xmath1002 ,",
    "the rank @xmath1157 can be deduced in time @xmath1158 .",
    "the conclusion for the cost then follows from the fact that @xmath1159 and @xmath1160 are both in @xmath1134 .",
    "we conclude by showing how to apply algorithm  from the previous section to our initial problems @xmath1161 and @xmath1162 .",
    "this corresponds to algorithms inv and solve given in figures  [ fig : algo_inv ] and  [ fig : algo_solve ] .",
    "[ thm : algo_inverse ] algorithm inv works correctly in time @xmath1163 it makes @xmath1164 random choices in @xmath13 and fails with probability less than  @xmath1165 if @xmath1166 .    by applying lemma  [ lem : preconditioning ] to the case @xmath574",
    ", we see that @xmath1167 is the last row of @xmath1168 and that @xmath914 and @xmath915 are @xmath1169 matrices such that @xmath1170 if @xmath1171 then @xmath1172 `` failure '' , and this is what we return .",
    "assume now that @xmath1173 .",
    "in this case , preconditioning has ensured that @xmath906 has generic rank profile .",
    "the number @xmath892 produced by  thus satisfies @xmath1174 , and @xmath1 is singular if and only if @xmath1175 . when @xmath1176 , we have @xmath1177 , so the matrices @xmath1178 and @xmath1179 produced by  satisfy @xmath1180 and @xmath1181 . using the special shape of the first @xmath3 columns of @xmath914 and @xmath915 , we conclude that the returned matrices @xmath1003 and @xmath1004 are @xmath1182 and @xmath1183 , as wanted .",
    "( note that although it is produced by , the first row of the inverse of @xmath921 is not needed here and denoted by @xmath1184 ; we will need it , however , when solving linear systems at the end of this section . )    applying lemmas  [ lem : preconditioning ] and  [ lem : lp_inv ] with @xmath1185 shows that the cost of calling precond and  is @xmath1186 ; on the other hand , the toeplitz structure of @xmath970 and @xmath991 implies that @xmath1003 and @xmath1004 can be deduced in time @xmath1187 from @xmath979 , @xmath980 , @xmath1178 , and @xmath1179 .",
    "writing @xmath1065 and @xmath1188 for the smallest integer powers of two greater than or equal to @xmath3 and @xmath1050 , we have @xmath1189 , from which the claimed cost bound follows .",
    "finally , by lemma  [ lem : preconditioning ] , the preconditioned matrix @xmath906 has generic rank profile with probability @xmath1190 . since @xmath1191 , we have @xmath1192 , so that @xmath1166 implies @xmath1193 .    for solving @xmath1194 , we work on the equivalent ( preconditioned ) system @xmath1195 such that @xmath960 and @xmath1196 , for which any solution @xmath1197 yields a solution @xmath1198 .",
    "algorithm  solve in figure  [ fig : algo_solve ] uses the following notation : writing as before @xmath892 for the rank of @xmath1 , we partition @xmath906 and @xmath1199 into blocks as @xmath1200 with @xmath1201 , @xmath1202 , @xmath1203 and @xmath1204 .    [",
    "thm : algo_solve ] algorithm solve works correctly in time @xmath1205 with @xmath588 . it makes @xmath958 random choices in @xmath13 and fails with probability less than  @xmath1165 if @xmath1206 with @xmath1207 .",
    "recalling that @xmath571 is equivalent to @xmath1208 with @xmath1168 , @xmath1209 and @xmath946 , we see that the algorithm begins by generating the matrix @xmath906 . if @xmath1210 , then @xmath906 has generic rank profile and thus @xmath1211 .",
    "this implies @xmath1212 and , partitioning @xmath1199 conformally with @xmath906 , we deduce that the linear system @xmath1195 is equivalent to @xmath1213 0 & 0 \\end{bmatrix } \\widetilde\\x   = \\begin{bmatrix } \\i_r & 0 \\\\[1 mm ] -\\widetilde\\ma_{21 } \\widetilde\\ma_r^{-1 }   & \\i_{m - r } \\end{bmatrix } \\!\\",
    "! \\begin{bmatrix }",
    "\\widetilde \\b_1 \\\\[1 mm ] \\widetilde \\b_2 \\end{bmatrix}\\!.\\ ] ] if @xmath1214 is such that @xmath1215 for @xmath1216 , then no solution exists .",
    "else , it is easily checked that any vector of the form @xmath1217 -\\v\\end{bmatrix } \\quad \\mbox{with } \\quad \\v \\in \\f^{n - r}\\ ] ] is a solution to @xmath1218 ; furthermore , taking @xmath1219 when @xmath1220 ensures that this solution is nontrivial .",
    "pre - multiplying this solution by @xmath1221 then yields a nontrivial solution to the original system , so correctness follows .    by lemmas  [ lem : preconditioning ] and  [ lem : lp_inv ] , calling precond and  uses @xmath1222 operations in @xmath13 .",
    "then , since @xmath970 and @xmath991 are toeplitz matrices , we can deduce @xmath1196 and @xmath1223 from @xmath979 , @xmath980 , @xmath663 , @xmath1197 in time @xmath1224 .",
    "finally , given the generators @xmath1225 and @xmath1226 of @xmath906 and @xmath919 , we can generate the submatrices @xmath1227 and @xmath1228 and perform the matrix - vector products @xmath1229 , @xmath1230 , and @xmath1231 in time @xmath1144 . recalling that @xmath1232 , we conclude that the total cost is thus in @xmath1134 .",
    "the probability analysis is the same as for inversion .",
    "_ if one wants to sample uniformly the solution manifold of @xmath1233 , then it suffices to replace the unit vector @xmath1234 in algorithm solve by a vector @xmath1235 whose entries are selected uniformly at random from the subset @xmath215 .",
    "this way of constructing @xmath221 corresponds to the technique introduced by kaltofen and saunders in  ( * ? ? ?",
    "* theorem  4 ) , but requires only @xmath1236 additional random choices instead of @xmath24 .",
    "10    , _ evaluating polynomials at fixed sets of points _ , siam j. comput . , 4 ( 1975 ) ,",
    "533539 .    , _ polynomial and matrix computations , volume 1 : fundamental algorithms _ ,",
    "birkhuser , 1994 .    , _ asymptotically fast solution of toeplitz and related systems of linear equations _ , linear algebra appl",
    ". , 34 ( 1980 ) , pp .",
    "103116 .    , _ solving structured linear systems with large displacement rank _ , theoret .",
    ", 407 ( 2008 ) , pp .",
    "155181 .    , _ complexity issues in bivariate polynomial factorization _ , proceedings of issac04 , acm , 2004 , pp .",
    "4249 .    , _ tellegen s principle into practice _ , proceedings of issac03 , acm , 2003 , pp .",
    "3744 .    , _ polynomial evaluation and interpolation on special sets of points _ ,",
    "j. complexity , 21 ( 2005 ) , pp .",
    "420446 .    , _ algebraic complexity theory _ ,",
    "springer , 1997 .    , _ on fast multiplication of polynomials over arbitrary algebras",
    "_ , acta inform . , 28 ( 1991 ) , pp .  693701 .    , _ faster algorithms for multivariate interpolation with multiplicities and simultaneous polynomial approximations _ , ieee trans .",
    "theory , 61 ( 2015 ) , pp .  23702387 .",
    ", _ new inversion formulas for matrices classified in terms of their distance from toeplitz matrices _ , linear algebra appl . , 27 ( 1979 ) , pp .",
    "3160 .    , _ modern computer algebra _ , cambridge university press , third  ed . ,",
    "2013 .    , _ complexity of multiplication with vectors for structured matrices _ , linear algebra appl",
    ". , 202 ( 1994 ) , pp .",
    "163192 .    , _ fast algorithms with preprocessing for matrix - vector multiplication problems _ , j. complexity , 10 ( 1994 ) , pp .",
    "411427 .    , _",
    "the middle product algorithm .",
    "algebra engrg .",
    "comput . , 14 ( 2004 ) , pp .",
    "415438 .    , _ computing specified generators of structured matrix inverses _ , proceedings of issac10 , acm , 2010 , pp .",
    "281288 .",
    ", _ displacement ranks of a matrix _ , bull .",
    "( n.s . ) , 1 ( 1979 ) , pp .  769773 .    , _ displacement ranks of matrices and linear equations _ , j. math .",
    "appl . , 68 ( 1979 ) , pp .",
    "395407 .    , _ asymptotically fast solution of toeplitz - like singular linear systems _ , proceedings of issac94 , acm , 1994 , pp .",
    "297304 .",
    ", _ analysis of coppersmith s block wiedemann algorithm for the parallel solution of sparse linear systems _ , math . comp .",
    ", 64 ( 1995 ) , pp .",
    "777806 .    , _ on wiedemann s method of solving sparse linear systems _ , in aaecc-9 , vol .  539 of lecture notes in computer science , springer , 1991 , pp .",
    "2938 .    , _ the theory of matrices , second edition _ , computer science and applied mathematics , academic press , 1985 .",
    ", _ powers of tensors and fast matrix multiplication _ , in proceedings of issac14 , acm , 2014 , pp .  296303 .    , _ fast algorithms for multivariable systems _ , phd thesis , dept . of electrical engineering , stanford university , stanford , 1974 .    , _ doubling algorithms for toeplitz and related equations _ , in proceedings of the ieee international conference on acoustics , speech and signal processing , 1980 , pp",
    ".  954959 .    , _ a unified superfast algorithm for boundary rational tangential interpolation problems and for inversion and factorization of dense structured matrices _ , in proc .",
    "39th ieee focs , 1998 , pp .",
    "192201 .    ,",
    "_ a unified superfast algorithm for confluent tangential interpolation problem and for structured matrices _ , in advanced signal processing algorithms , architectures , and implementations , aspaaiix , spie , 1999 , pp .",
    "312323 .    , _ matrix - vector product for confluent cauchy - like matrices with application to confluent rational interpolation _ , in stoc00 , acm , 2000 , pp .  573581 .    ,",
    "_ trilinear aggregating with implicit canceling for a new acceleration of matrix multiplication _ , comp .",
    "& maths . with appls . ,",
    "8 ( 1982 ) , pp .  2334 .    , _ on computations with dense structured matrices _ ,",
    "math . comp .",
    ", 55 ( 1990 ) , pp .",
    "179190 .    , _ a unified superfast divide - and - conquer algorithm for structured matrices_. msri preprint 1999 - 033 , mathematical sciences research institute , berkeley , ca , april 1999 .    ,",
    "_ nearly optimal computations with structured matrices _ , in soda00 , acm , 2000 , pp",
    ".  953962 .    ,",
    "_ structured matrices and polynomials _ , birkhuser boston inc . , 2001 .",
    ", _ transformations of matrix structures work again _ , linear algebra appl . , 465 ( 2015 ) , pp .",
    "107138 .    ,",
    "_ inversion of displacement operators _ , siam j. matrix anal .",
    "appl . , 24 ( 2003 ) , pp .",
    "660677 .    ,",
    "_ superfast algorithms for cauchy - like matrix computations and extensions .",
    "_ , linear algebra appl . , 310 ( 2000 ) ,",
    "83108 .    , _ schnelle multiplikation von polynomen ber krpern der charakteristik 2 _ , acta inform .",
    ", 7 ( 1977 ) , pp .",
    "395398 .    , _ schnelle multiplikation groer zahlen _ , computing , 7 ( 1971 ) , pp .",
    "281292 .    , _ fast algorithms for elementary operations with complex power series _ ,",
    "diskret . mat .",
    ", 22 ( 2010 ) , pp .",
    ", _ gaussian elimination is not optimal _ , numer .",
    ", 13 ( 1969 ) , pp .",
    "consider first the sylvester case , where @xmath1237 and @xmath1238 .",
    "if @xmath1239 , then pre- and post - multiplying both sides of this equality by @xmath31 gives @xmath1240 , so that @xmath34 equals @xmath1241 and thus has the same rank as @xmath22 .",
    "consider now the stein case , where @xmath1242 and @xmath1243 .",
    "defining the matrix @xmath1244 we have @xmath1245 and @xmath1246 since the four matrices applied to @xmath4 are invertible , we deduce that @xmath1247 using @xmath1248 , we see again that @xmath22 and @xmath34 have the same rank .",
    "fix @xmath66 and @xmath183 . for all @xmath251 ,  (",
    "* theorem  4.7 ) gives @xmath1249 with @xmath1250 and @xmath1251 as in the proof of lemma  [ lemma : aij ] .",
    "writing @xmath489 , and multiplying the former equality on the left by @xmath1252 , we get , for @xmath1253 , @xmath1254 since the last @xmath487 columns of @xmath1255 are precisely @xmath1256 ; note that the equality also holds for @xmath247 .",
    "summing over all @xmath496 , and using the fact that @xmath1257 , we deduce @xmath1258 the rest of the proof now follows exactly that of lemma  [ lemma : aij ] ."
  ],
  "abstract_text": [
    "<S> for matrices with displacement structure , basic operations like multiplication , inversion , and linear system solving can all be expressed in terms of the following task : evaluate the product @xmath0 , where @xmath1 is a structured @xmath2 matrix of displacement rank @xmath3 , and @xmath4 is an arbitrary @xmath5 matrix . given  @xmath4 and a so - called _ generator _ of @xmath1 , this product is classically computed with a cost ranging from @xmath6 to @xmath7 arithmetic operations , depending on the type of structure of @xmath1 ; here , @xmath8 is a cost function for polynomial multiplication . in this paper , we first generalize classical displacement operators , based on block diagonal matrices with companion diagonal blocks , and then design fast algorithms to perform the task above for this extended class of structured matrices . the cost of these algorithms ranges from @xmath9 to @xmath10 , with @xmath11 such that two @xmath2 matrices over a field can be multiplied using @xmath12 field operations . by combining this result with classical randomized regularization techniques , </S>",
    "<S> we obtain faster las vegas algorithms for structured inversion and linear system solving .    structured linear algebra , matrix multiplication , computational complexity    65f05 , 68q25 </S>"
  ]
}