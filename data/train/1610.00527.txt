{
  "article_text": [
    "video modelling has remained a challenging problem due to the complexity and ambiguity inherent in video data .",
    "current approaches range from mean squared error models based on deep neural networks  @xcite , to models that predict quantized image patches  @xcite , incorporate motion priors  @xcite or use adversarial losses  @xcite . despite the wealth of approaches , future frame predictions that are free of systematic artifacts ( e.g. blurring )",
    "have been out of reach even on relatively simple benchmarks like moving mnist  @xcite .",
    "we propose the video pixel network ( vpn ) , a generative video model based on deep neural networks , that reflects the factorization of the joint distribution of the pixel values in a video .",
    "the model encodes the four - dimensional structure of video tensors and captures dependencies in the time dimension of the data , in the two space dimensions of each frame and in the color channels of a pixel .",
    "this makes it possible to model the stochastic transitions locally from one pixel to the next and more globally from one frame to the next without introducing independence assumptions in the conditional factors .",
    "the factorization further ensures that the model stays fully tractable ; the likelihood that the model assigns to a video can be computed exactly .",
    "the model operates on pixels without preprocessing and predicts discrete multinomial distributions over raw pixel intensities , allowing the model to estimate distributions of any shape .",
    "the architecture of the vpn consists of two parts : resolution preserving cnn encoders and pixelcnn decoders @xcite .",
    "the cnn encoders preserve at all layers the spatial resolution of the input frames in order to maximize representational capacity .",
    "the outputs of the encoders are combined over time with a convolutional lstm that also preserves the resolution @xcite .",
    "the pixelcnn decoders use masked convolutions to efficiently capture space and color dependencies and use a softmax layer to model the multinomial distributions over raw pixel values .",
    "the network uses dilated convolutions in the encoders to achieve larger receptive fields and better capture global motion .",
    "the network also utilizes newly defined multiplicative units and corresponding residual blocks .",
    "we evaluate vpns on two benchmarks .",
    "the first is the moving mnist dataset @xcite where , given 10 frames of two moving digits , the task is to predict the following 10 frames . in  sect .",
    "[ sec : moving_mnist ] we show that the vpn achieves 87.6 nats / frame , a score that is near the lower bound on the loss ( calculated to be 86.3 nats / frame ) ; this constitutes a significant improvement over the previous best result of 179.8 nats / frame  @xcite .",
    "the second benchmark is the robotic pushing dataset @xcite where , given two natural video frames showing a robotic arm pushing objects ,    the task is to predict the following 18 frames .",
    "we show that    the vpn not only generalizes to new action sequences with objects seen during training , but also to new action sequences involving _ novel _ objects not seen during training .",
    "random samples from the vpn preserve remarkable detail throughout the generated sequence .",
    "we also define a baseline model that lacks the space and color dependencies .",
    "this lets us see that the latter dependencies are crucial for avoiding systematic artifacts in generated videos .",
    "[ fig : dependencies ]",
    "in this section we define the probabilistic model implemented by video pixel networks .",
    "let a video @xmath0 be a four - dimensional tensor of pixel values @xmath1 , where the first ( temporal ) dimension @xmath2 corresponds to one of the frames in the video , the next two ( spatial ) dimensions @xmath3 index the pixel at row @xmath4 and column @xmath5 in frame @xmath6 , and the last dimension @xmath7 denotes one of the three rgb channels of the pixel .",
    "we let each @xmath1 be a random variable that takes values from the rgb color intensities of the pixel .    by applying the chain rule to factorize the video likelihood @xmath8 as a product of conditional probabilities , we can model it in a tractable manner and without introducing independence assumptions :    @xmath9    here @xmath10 comprises the rgb values of all pixels to the left and above the pixel at position @xmath11 in the current frame @xmath6 , as well as the rgb values of pixels from all the previous frames .    note that the factorization itself does not impose a unique ordering on the set of variables .",
    "we choose an ordering according to two criteria .",
    "the first criterion is determined by the properties and uses of the data ; frames in the video are predicted according to their temporal order .",
    "the second criterion favors orderings that can be computed efficiently ; pixels are predicted starting from one corner of the frame ( the top left corner ) and ending in the opposite corner of the frame ( the bottom right one ) as this allows for the computation to be implemented efficiently @xcite .",
    "the order for the prediction of the colors is chosen by convention as r , g and b.    the vpn models directly the four dimensions of video tensors .",
    "we use @xmath12 to denote the @xmath6-th frame @xmath13 in the video @xmath0 .",
    "figure  [ fig : dependencies ] illustrates the fourfold dependency structure for the green color channel value of the pixel @xmath14 in frame @xmath12 , which depends on : ( i ) all pixels in all the previous frames @xmath15 ; ( ii ) all three colors of the already generated pixels in @xmath12 ; ( iii ) the already generated red color value of the pixel @xmath14 .",
    "we follow the pixelrnn approach @xcite in modelling each conditional factor as a discrete multinomial distribution over 256 raw color values .",
    "this allows for the predicted distributions to be arbitrarily multimodal .",
    "we compare the vpn model with a baseline model that encodes the temporal dependencies in videos from previous frames to the next , but ignores the spatial dependencies between the pixels within a frame and the dependencies between the color channels . in this case the joint distribution is factorized by introducing independence assumptions : @xmath16 figure [ fig : dependencies ] illustrates the conditioning structure in the baseline model .",
    "the green channel value of pixel @xmath17 only depends on the values of pixels in previous frames .",
    "various models have been proposed that are similar to our baseline model in that they capture the temporal dependencies only @xcite      to illustrate the properties of the two factorizations , suppose that a model needs to predict the value of a pixel @xmath14 and the value of the adjacent pixel @xmath18 in a frame @xmath19 , where the transition to the frame @xmath19 from the previous frames @xmath20 is non - deterministic . for a simple example , suppose the previous frames @xmath20 depict a robotic arm and in the current frame @xmath19 the robotic arm is about to move either left or right .",
    "the baseline model estimates @xmath21 and @xmath22 as distributions with two modes , one for the robot moving left and one for the robot moving right .",
    "sampling independently from @xmath21 and @xmath22 can lead to two inconsistent pixel values coming from distinct modes , one pixel value depicting the robot moving left and the other depicting the robot moving right .",
    "the accumulation of these inconsistencies for a few frames leads to known artifacts such as blurring of video continuations . by contrast , in this example , the vpn estimates @xmath21 as the same bimodal distribution , but then estimates @xmath23 _ conditioned on the selected value of _ @xmath14 .",
    "the conditioned distribution is unimodal and , if the value of @xmath14 is sampled to depict the robot moving left , then the value of @xmath18 is sampled accordingly to also depict the robot moving left .",
    "generating a video tensor requires sampling @xmath24 variables , which for a second of video with resolution @xmath25 is in the order of @xmath26 samples .",
    "this figure is in the order of @xmath27 for generating a single image or for a second of audio signal @xcite , and it is in the order of @xmath28 for language tasks such as machine translation @xcite .",
    "in this section we construct a network architecture capable of computing efficiently the factorized distribution in  sect .  [",
    "sec : factor ] .",
    "the architecture consists of two parts .",
    "the first part models the temporal dimension of the data and consists of resolution preserving cnn encoders whose outputs are given to a convolutional lstm .",
    "the second part models the spatial and color dimensions of the video and consists of pixelcnn architectures @xcite that are conditioned on the outputs of the cnn encoders .",
    "given a set of video frames @xmath29 , the vpn first encodes each of the first @xmath30 frames @xmath31 with a cnn encoder .",
    "these frames form the histories that condition the generated frames .",
    "each of the cnn encoders is composed of @xmath32 ( @xmath33 in the experiments ) residual blocks ( sect .",
    "[ sec : impl_details ] ) and the spatial resolution of the input frames is preserved throughout the layers in all the blocks . preserving",
    "the resolution is crucial as it allows the model to condition each pixel that needs to be generated without loss of representational capacity .",
    "the outputs of the  @xmath30 cnn encoders , which are computed in parallel during training , are given as input to a convolutional lstm , which also preserves the resolution .",
    "this part of the vpn computes the temporal dependencies of the video tensor and is represented in fig .",
    "[ fig : dependencies ] by the shaded blocks .",
    "the second part of the vpn architecture computes dependencies along the space and color dimensions .",
    "the @xmath30 outputs of the first part of the architecture provide representations for the contexts that condition the generation of a portion of the @xmath34 frames @xmath35 ; if one generates all the @xmath34 frames , then the first frame @xmath36 receives no context representation .",
    "these context representations are used to condition decoder neural networks that are pixelcnns .",
    "pixelcnns are composed of @xmath37 resolution preserving residual blocks ( @xmath38 in the experiments ) , each in turn formed of _ masked _ convolutional layers .",
    "since we treat the pixel values as discrete random variables , the final layer of the pixelcnn decoders is a softmax layer over 256 intensity values for each color channel in each pixel .",
    "figure  [ fig : dependencies ] depicts the two parts of the architecture of the vpn .",
    "the decoder that generates pixel @xmath14 of frame @xmath12 sees the context representation for all the frames up to @xmath39 coming from the preceding cnn encoders .",
    "the decoder also sees the pixel values above and left of the pixel @xmath14 in the current frame @xmath12 that is itself given as input to the decoder .",
    "we implement the baseline model by using the same cnn encoders to build the context representations .",
    "in contrast with pixelcnns , the decoders in the baseline model are cnns that do not use masking on the weights ; the frame to be predicted thus can not be given as input .",
    "as shown in fig .",
    "[ fig : dependencies ] , the resulting neural network captures the temporal dependencies , but ignores spatial and color channel dependencies within the generated frames .",
    "just like for vpns , we make the neural architecture of the baseline model resolution preserving in all the layers .",
    "in this section we describe two basic operations that are used as the building blocks of the vpn .",
    "the first is the _ multiplicative unit _ ( mu ,  sect .",
    "[ sec : mu ] ) that contains multiplicative interactions inspired by lstm  @xcite gates .",
    "the second building block is the _ residual multiplicative block _",
    "( rmb ,  sect .",
    "[ sec : rmb ] ) that is composed of multiple layers of mus .      a multiplicative unit ( fig .",
    "[ fig : mu ] ) is constructed by incorporating lstm - like gates into a convolutional layer .",
    "given an input @xmath40 of size @xmath41 , where @xmath42 corresponds to the number of channels , we first pass it through four convolutional layers to create an update @xmath43 and three gates @xmath44 . the input , update , and gates",
    "are then combined in the following manner :    @xmath45    where @xmath46 is the sigmoid non - linearity and @xmath47 is component - wise multiplication .",
    "biases are omitted for clarity . in our experiments",
    "the convolutional weights @xmath48 use a kernel of size @xmath49 .",
    "unlike lstm networks , there is no distinction between _ memory _ and _ hidden _ states .",
    "also , unlike highway networks @xcite and grid lstm @xcite , there is no setting of the gates such that @xmath50 simply returns the input @xmath40 ; the input is always processed with a non - linearity .          to allow for easy gradient propagation through many layers of the network , we stack two mu layers in a residual multiplicative block ( fig .",
    "[ fig : rmb ] ) where the input has a residual ( additive skip ) connection to the output @xcite . for computational efficiency , the number of channels is halved in mu layers inside the block . namely , given an input layer @xmath40 of size @xmath51 with @xmath52 channels , we first apply a @xmath53 convolutional layer that reduces the number of channels to @xmath42 ; no activation function is used for this layer , and it is followed by two successive mu layers each with a convolutional kernel of size @xmath49 .",
    "we then project the feature map back to @xmath52 channels using another @xmath54 convolutional layer .",
    "finally , the input @xmath40 is added to the overall output forming a residual connection . such a layer structure is similar to the bottleneck residual unit of  @xcite .",
    "formally , the residual multiplicative block ( rmb ) is computed as follows : @xmath55 we also experimented with a standard residual block of  @xcite which uses relu non - linearities ",
    "see  sect .",
    "[ sec : moving_mnist ] and  [ sec : robotic_pushing ] for details .      having a large receptive field helps the model to capture the motion of larger objects .",
    "one way to increase the receptive field without much effect on the computational complexity is to use dilated convolutions  @xcite , which make the receptive field grow exponentially , as opposed to linearly , in the number of layers . in the variant of vpn that uses dilation , the dilation rates are the same within each rmb , but they double from one rmb to the next up to a chosen maximum size , and then repeat  @xcite .",
    "in particular , in the cnn encoders we use two repetitions of the dilation scheme @xmath56 , for a total of 8 rmbs .",
    "we do not use dilation in the decoders .",
    ".cross - entropy results in nats / frame on the moving mnist dataset . [ cols=\"<,^,^,^\",options=\"header \" , ]      table  [ table : robot_act ] reports the results of the baseline model and variants of the vpn on the robotic pushing validation and test sets . the best variant of the vpn has a @xmath57 reduction in negative log - likelihood over the baseline model .",
    "this highlights the importance of space and color dependencies in non - deterministic environments . the results on the validation and test datasets with seen objects and on the test dataset with novel objects are similar .",
    "this shows that the models have learned to generalize well not just to new action sequences , but also to new objects .",
    "furthermore , we see that using multiplicative interactions in the vpn gives a significant improvement over using relus",
    ".    figures [ fig : robot_valid][fig : robot_comp ] visualize the samples generated by our models .",
    "figure [ fig : robot_valid ] contains random samples of the vpn on the validation set with seen objects ( together with the corresponding ground truth ) .",
    "the model is able to distinguish between the robotic arm and the background , correctly handling occlusions and only pushing the objects when they come in contact with the robotic arm .",
    "the vpn generates the arm when it enters into the frame from one of the sides .",
    "the position of the arm in the samples is close to that in the ground truth , suggesting the vpn has learned to follow the actions .",
    "the generated videos remain detailed throughout the 18 frames and few artifacts are present .",
    "the samples remain good showing the ability of the vpn to generalize to new sequences of actions .",
    "figure [ fig : robot_novel ] evaluates an additional level of generalization , by showing samples from the test set with _ novel _ objects not seen during training .",
    "the vpn seems to identify the novel objects correctly and generates plausible movements for them .",
    "the samples do not appear visibly worse than in the datasets with seen objects .",
    "figure [ fig : robot_cont ] demonstrates the probabilistic nature of the vpn , by showing multiple different video continuations that start from the same context frames and are conditioned on the same sequence of 18 future actions .",
    "the continuations are plausible and varied , further suggesting the vpn s ability to generalize .",
    "figure [ fig : robot_base ] shows samples from the baseline model .",
    "in contrast with the vpn samples , we see a form of high frequency noise appearing in the non - deterministic movements of the robotic arm . this can be attributed to the lack of space and color dependencies , as discussed in sec .",
    "[ sec : discussion ] . figure [ fig : robot_comp ] shows a comparison of continuations of the baseline model and the vpn from the same context sequence . besides artifacts , the baseline model also seems less responsive to the actions .",
    "we have introduced the video pixel network , a deep generative model of video data that models the factorization of the joint likelihood of video .",
    "we have shown that , despite its lack of specific motion priors or surrogate losses , the vpn approaches the lower bound on the loss on the moving mnist benchmark that corresponds to a large improvement over the previous state of the art .",
    "on the robotic pushing dataset , the vpn achieves significantly better likelihoods than the baseline model that lacks the fourfold dependency structure ; the vpn generates videos that are free of artifacts and are highly detailed for many frames into the future .",
    "the fourfold dependency structure provides a robust and generic method for generating videos without systematic artifacts ."
  ],
  "abstract_text": [
    "<S> we propose a probabilistic video model , the video pixel network ( vpn ) , that estimates the discrete joint distribution of the raw pixel values in a video . the model and the neural architecture reflect the time , space and color structure of video tensors and encode it as a four - dimensional dependency chain . </S>",
    "<S> the vpn approaches the best possible performance on the moving mnist benchmark , a leap over the previous state of the art , and the generated videos show only minor deviations from the ground truth . </S>",
    "<S> the vpn also produces detailed samples on the action - conditional robotic pushing benchmark and generalizes to the motion of novel objects . </S>"
  ]
}