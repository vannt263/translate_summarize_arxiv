{
  "article_text": [
    "lossy data compression is a core technology of contemporary broadband communication .",
    "the best achievable performance of lossy compression is theoretically provided by rate - distortion theorems , which were first proved by shannon for memoryless sources @xcite .",
    "unfortunately , shannon s proofs are not constructive and suggest few clues for how to design practical codes .",
    "consequently , no practical schemes saturating the potentially optimal performance of lossy compression represented by the rate - distortion function ( rdf ) have been found yet , even for simple information sources .",
    "therefore , the quest for better lossy compression codes remains a major problem in the field of information theory ( it ) .",
    "recent research on error correcting codes has revealed a similarity between it and statistical mechanics ( sm ) of disordered systems  @xcite . because it has been shown that methods from sm can be useful to analyse subjects of it , it is natural to expect that a similar approach might also bring about novel developments in lossy compression .",
    "this research is promoted by such a motivation .",
    "specifically , we propose a simple compression code for uniformly biased binary data devised on input - output relations of a perceptron .",
    "theoretical evaluation based on the replica method ( rm ) indicates that this code potentially saturates the rdf in most cases although exactly performing the compression is computationally difficult . to resolve this difficulty",
    ", we develop a computationally tractable algorithm based on belief propagation ( bp )  @xcite , which offers performance that approaches the rdf in a practical time scale when optimally tuned .",
    "we describe a general scenario for lossy data compression of memoryless sources .",
    "original data is denoted as @xmath0 , which is assumed to comprise a sequence of @xmath1 discrete or continuous random variables that are generated independently from an identical stationary distribution @xmath2 .",
    "the purpose of lossy compression is to compress @xmath3 into a binary expression @xmath4 , allowing a certain amount of _ distortion _ between the original data @xmath3 and its representative vector @xmath5 when @xmath6 is retrieved from @xmath7 .    in this study",
    ", distortion is measured using a distortion function that is assumed to be defined in a component - wise manner as @xmath8 , where @xmath9 .",
    "a code @xmath10 is specified by a map @xmath11 , which is used in the restoration phase .",
    "this map also reasonably determines the compression phase as @xmath12 where @xmath13 represents the argument @xmath7 that minimises @xmath14 .",
    "when @xmath15 is generated from a certain code ensemble , typical codes satisfy the fidelity criterion @xmath16 for a given permissible distortion @xmath17 and typical original data @xmath3 with probability @xmath18 in the limit @xmath19 maintaining the coding rate @xmath20 constant , if and only if @xmath21 is larger than a certain critical rate @xmath22 that is termed the _ rate - distortion function_.    however , for finite @xmath1 and @xmath23 , any code has a finite probability @xmath24 of breaking the fidelity ( [ eq : fidelity ] ) , even for @xmath25 .",
    "similarly , for @xmath26 , eq . ( [ eq : fidelity ] ) is satisfied with a certain probability @xmath27 . for reasonable code ensembles ,",
    "the averages of these probabilities are expected to decay exponentially with respect to @xmath1 when the data length @xmath1 is sufficiently large .",
    "therefore , the two _ error exponents _",
    "@xmath28 for @xmath29 and @xmath30 for @xmath26 , where @xmath31 represents the average over the code ensemble , can be used to characterise the potential ability of the ensemble of finite data lengths .",
    "it is conjectured that the components of @xmath7 are preferably unbiased and uncorrelated in order to minimise loss of information in the original data from a binary information source , which implies that the entropy per bit in @xmath7 must be maximised . on the other hand , in order to reduce the distortion , the representative vector @xmath32 should be placed close to the typical sequences of the original data that are biased .",
    "unfortunately , it is difficult to construct a code that satisfies these two requirements using only linear transformations over the boolean field because a linear transformation generally reduces statistical bias in sequences , which implies that one can not produce a biased representative vector from the unbiased compressed sequence .",
    "one method to design a code that has the above properties is to introduce a nonlinear transformation .",
    "a perceptron provides a simple scheme for carrying out this task . to specify lossy data compression codes for binary original data @xmath33 generated from a memoryless source ,",
    "we define a map by utilising perceptrons from the compressed expression @xmath34 to the representative sequence @xmath35 as @xmath36 where @xmath37 is a function for which the output is limited to @xmath38 and @xmath39 are randomly predetermined @xmath23-dimensional vectors that are generated from an @xmath23-dimensional normal distribution @xmath40 $ ] .",
    "these vectors are known to the encoder and decoder .",
    "we adopt an output function @xmath41 for @xmath42 , and @xmath43 otherwise , which eventually offers optimal performance .",
    "we measure the distortion by the hamming distance @xmath44 .",
    "$ ] then , the compression phase for the given data @xmath3 can be defined as finding a vector @xmath7 that minimises the resulting distortion @xmath45 , and the retrieval process can be performed easily using eq .",
    "( [ perceptron ] ) from a given sequence @xmath7 .",
    "the performance evaluation has been investigated theoretically from the perspective of sm , which is not specialised for this perceptron - based code .",
    "rather , it is a general one as mentioned briefly below .",
    "let us regard the distortion function @xmath46 as the hamiltonian for the dynamical variable @xmath7 , which also depends on predetermined variables @xmath3 and @xmath10 . the resulting distortion ( per bit ) for a given @xmath3 and @xmath10",
    "is represented as @xmath47 .",
    "we start with a statistical mechanical inequality @xmath48 which holds for any sets of @xmath49 and @xmath10 .",
    "the physical implication of this is that the ground state energy @xmath50 ( per component ) is lower bounded by the free energy @xmath51 ( per component ) for an arbitrary temperature @xmath52 . in particular",
    ", the free energy @xmath51 agrees with @xmath50 in the zero temperature limit @xmath53 , which is the key for the analysis .",
    "the distribution of the free energy @xmath54 is expected to peak at its typical value of @xmath55 where @xmath56 denotes the average over @xmath3 and @xmath10 , and decays exponentially away from @xmath57 as @xmath58 $ ] for large @xmath1 . here",
    ", we assume that @xmath59 is a convex downward function that is minimised to @xmath60 at @xmath61 .",
    "this formulation implies that , for @xmath62 , the logarithm of the moment of the partition function @xmath63 , @xmath64 , can be evaluated by the saddle point method as @xmath65 based on the legendre transformation ( [ eq : g_legendre ] ) , @xmath66 is assessed by the inverse transformation @xmath67 from @xmath68 , which can be evaluated using the rm analytically extending expressions obtained for @xmath69 to @xmath70 .",
    "the above argument indicates that the typical value of the distortion averaged over the generation with respect to @xmath3 and @xmath10 can be evaluated as @xmath71 and that the average error exponent @xmath72 , which is an abbreviation denoting @xmath73 and @xmath74 , can be assessed as @xmath75 where @xmath76 is a function of @xmath77 that is determined by the extremum condition of eq .",
    "( [ eq : legendre ] ) as @xmath78 .",
    "equations([typical_case ] ) and ( [ exact_exponent ] ) constitute the basis of our approach .    when the above general framework was applied to the random code ensemble , which is not a practical coding scheme , but can exhibit optimal performance , the theoretical limitations  the rdf and optimal error exponents derived in it",
    "@xcite  were reproduced correctly @xcite .",
    "these results support the validity of our theoretical framework .",
    "in addition to consistency with the existing results , we demonstrated the wide applicability of our framework for the perceptron - based code in @xcite , which indicated that the perceptron - based code can also saturate the theoretical limitations in most cases .",
    "calculation using the rm implies that the perceptron - based code potentially provides optimal performance for binary memoryless sources .",
    "however , this is insufficient when it is necessary to obtain a compressed sequence @xmath7 for a given finite length of original data @xmath3 .    for the perceptron - based code ,",
    "the compression phase to follow the prescription ( [ eq : selecting_s ] ) is computationally difficult because it requires a comparison over @xmath79 patterns to extract the ground state for the relevant boltzmann distribution @xmath80 / z(\\beta ; \\vec{y } , \\ { \\vec{x}^\\mu \\}).$ ] the boltzmann factor",
    "is rewritten here for the sake of subsequent expressions as @xmath81 = \\prod_{\\mu = 1}^m \\xi_{k , y^\\mu } \\left(\\frac{1}{\\sqrt{n } } \\sum_{i=1}^n x_i^\\mu s_i \\right ) , \\label{factorize}\\end{aligned}\\ ] ] where we define @xmath82 $ ] .",
    "we require computationally tractable algorithms that generate a probable state @xmath7 from the boltzmann distribution .",
    "the bp is known as a promising approach for such tasks .",
    "it is an iterative algorithm that efficiently calculates the marginal posterior probabilities @xmath83 based on the property that the potential function is factored as shown in eq .",
    "( [ factorize ] ) . in general",
    ", the fixed point of this algorithm generally provides the solution of the bethe approximation @xcite known in sm .    to introduce this algorithm to the current system ,",
    "let us graphically describe this factorization , denoting the predetermined variables ( @xmath84 and @xmath85 ) and compressed sequence ( @xmath86 ) by two kinds of nodes , then connecting them by an edge when they are included in a common factor , which can be expressed as a complete bipartite graph shown in fig .",
    "[ fig : bipartite ] .    on that graph , bp can be represented as an algorithm that passes messages between the two kinds of nodes through edges as @xmath87 where @xmath88 is an index for counting the number of updates .",
    "the marginalised posterior at the @xmath89th update is given as @xmath90 . because @xmath91 is a binary variable",
    ", one can parameterize the above functions as distributions @xmath92 and @xmath93 .",
    "is related with every bit of the compressed sequence .",
    ", width=302 ]    since the computational cost for the summation in eq .",
    "( [ message2 ] ) grows exponentially in @xmath23 , it is extremely difficult to perform this algorithm exactly .",
    "however , because @xmath85 are generated independently from @xmath94 $ ] , this summation can be well approximated by a one - dimensional integral of a gaussian distribution @xcite , the centre and the variance of which are @xmath95 , @xmath96 where @xmath97 , respectively .",
    "this approximation makes it possible to carry out the belief updates ( [ message2 ] ) and ( [ message1 ] ) in a practical time scale , providing a set of self - consistent equations @xmath98 where we define @xmath99 , and @xmath100 for any function @xmath101 . employing these variables , the approximated posterior average of @xmath86 at the @xmath89th update",
    "can be computed as @xmath102 .",
    "the number of variables can be further reduced to @xmath103 when @xmath104 is large by employing eq .",
    "( [ mmu ] ) @xcite .",
    "one can approximately transform eq .",
    "( [ mmu ] ) into @xmath105 utilising this equation and taking into consideration that the influence of each element in the sum is sufficiently small compared to the remains , the following approximations hold .",
    "@xmath106 because the last term in eq .",
    "( [ new_delta ] ) is infinitesimal , the taylor expansion is applicable to eq .",
    "( [ mhat ] ) , providing @xmath107 where we define @xmath108 .",
    "it is important to notice that the second term of the right - hand side in eq .",
    "( [ new_mmul ] ) , which can be negligible compared to the first term , becomes an influential element when the posterior average @xmath109 is calculated .",
    "using the new notations @xmath110 and @xmath111 , the compression algorithm is finally expressed as @xmath112 , \\label{g_mu } \\\\ & & \\left ( u_\\mu^t   \\equiv   \\delta^t_{\\mu } - ( 1-q^t ) a^t_\\mu + \\sqrt{1-q^t } z \\right ) , \\notag \\\\ m^t_l & = & \\tanh \\left [ \\sum_{\\mu=1}^m   \\frac{x_l^\\mu}{\\sqrt{n } }   a^t_\\mu   - \\frac { g^{t-1 } } { n } m_l^{t-1 }   \\right ] .",
    "\\label{final_ml}\\end{aligned}\\ ] ] for the perceptron - based code , we can calculate @xmath113 , \\label{tau1}\\\\ \\int dz~\\xi^{\\prime \\prime}_{k , y^\\mu } ( u_\\mu^t )   & = &   \\frac{(1- e^{-\\beta } ) y^\\mu } { \\sqrt{2\\pi}(1-q^t ) } \\left [ w^t_{\\mu - } \\exp \\left\\ { - \\frac { \\left ( w^t_{\\mu - } \\right)^2 } { 2 } \\right\\ } -   w^t_{\\mu + } \\exp \\left\\ { - \\frac { \\left ( w^t_{\\mu + } \\right)^2 } { 2 } \\right\\ } \\right ] , \\label{tau2}\\end{aligned}\\ ] ] where @xmath114    the exact solution of @xmath115 trivially vanishes because of the mirror symmetry @xmath116 .",
    "this fact implies that one can not determine the more probable sign of @xmath91 even if the update iteration is successful .",
    "a similar phenomenon was also reported in codes of another type  @xcite . to resolve this problem",
    ", we heuristically introduce an inertia term that has been employed for lossy compression of an unbiased source @xcite , in eq .",
    "( [ final_ml ] ) as @xmath117,$ ] where @xmath118 is a constant ( @xmath119 ) .",
    "experimental results are shown in fig .",
    "[ fig : bp ] together with the rdfs . given original data generated from a binary stationary distribution @xmath120 and vector @xmath121 , we compressed the original data into a shorter sequence using the bp .",
    "the final value of @xmath91 was determined as @xmath122 $ ] .",
    "the values of @xmath123 and @xmath77 were set to theoretically optimal values evaluated from the rm - based analysis ; the value of @xmath118 was determined by trial and error . in the figure , the bit error rates averaged over 100 runs are plotted as a function of the compression rate @xmath21 for bias @xmath124 and @xmath125 . while the compressed sequence was fixed to @xmath126 bits ( @xmath127 when @xmath128 ) , the length of the original data was adjusted in accordance with the compression rate .",
    "we stopped the iteration at the 35th update and determined the compressed sequence from the result at that time , even if the algorithm did not converge .",
    "as the compression rate becomes smaller , the performance approaches the rdf in the case of @xmath129 and @xmath125 .",
    "in particular , the performance for @xmath130 is superior to results reported in the it literature as a binary memoryless source @xcite .",
    "however , the results for @xmath131 yield poor performance compared to those for @xmath132 , even though the situation from the perspective of information is the same as @xmath132 , which might be the result of asymmetric influences of input - output relations between those two cases .",
    "improvement of this behaviour is a subject of future work .     and @xmath125 .",
    "in the region of the low compression rate , the performance approaches the rdf in the case of @xmath129 and @xmath125 .",
    ", width=340 ]",
    "we have investigated the performance of lossy data compression for uniformly biased binary data .",
    "analyses based on the rm indicate the great potential of the perceptron - based code , which is also partially confirmed by a practically tractable algorithm based on bp . a close relationship between the macroscopic dynamics of bp and the replica analysis ,",
    "was recently reported @xcite .",
    "investigation in such a direction in the current case is under way ."
  ],
  "abstract_text": [
    "<S> the encoder and decoder for lossy data compression of binary memoryless sources are developed on the basis of a specific - type nonmonotonic perceptron . </S>",
    "<S> statistical mechanical analysis indicates that the potential ability of the perceptron - based code saturates the theoretically achievable limit in most cases although exactly performing the compression is computationally difficult . to resolve this difficulty </S>",
    "<S> , we provide a computationally tractable approximation algorithm using belief propagation ( bp ) , which is a current standard algorithm of probabilistic inference . introducing several approximations and heuristics </S>",
    "<S> , the bp - based algorithm exhibits performance that is close to the achievable limit in a practical time scale in optimal cases . </S>"
  ]
}