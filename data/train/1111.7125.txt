{
  "article_text": [
    "as the amount and variety of biomedical data increase , so does the hope of finding biomarkers , that is , substances that can be used as indicators of specific medical conditions .",
    "it can also be possible to detect new , subtle disease subtypes and monitor disease progression . in these latter cases",
    "an exploratory approach may be beneficial in order to detect previously unknown patterns .",
    "exploratory analysis methods providing a visually representable result are particularly appealing since they allow the unparalleled power of the human brain to be used to find potentially interesting structures and patterns in the data .",
    "the inability to interpret objects in more than three dimensions has motivated the development of methods that create a low - dimensional representation summarizing the main features of the observed data .",
    "probably the most well - known such method is principal components analysis ( pca ) [ @xcite ; @xcite ( @xcite ) ] which provides the best approximation ( measured by the frobenius norm ) of a given rank to a data matrix , and which is used extensively [ see , e.g. , @xcite ; @xcite for applications to gene expression data ] .",
    "one particularly appealing aspect of pca is that its formulation in terms of the singular value decomposition ( svd ) provides also a low - dimensional representation of the variables , which is directly synchronized with the sample representation .",
    "this allows for a visually guided interpretation of the impact of each variable on the patterns seen among the samples . the joint visualization obtained by depicting both the sample and variable representations in the same plot",
    "is commonly referred to as a biplot [ @xcite ; @xcite ] .",
    "biplots have been used for visualization and interpretation of many different types of data [ e.g. , @xcite ; @xcite ; @xcite ; @xcite ; @xcite ] .",
    "the usefulness of pca is dependent upon the assumption that the euclidean distance between the variable profiles of a pair of samples provides a good measure of the dissimilarity between the samples .",
    "it is easy to imagine situations where this is not true , for example , if two samples should be considered similar if they show similar , unusually high or low values on only a small subset of the variables irrespective of the values of the rest of the variables , or if the samples are distributed along a nonlinear manifold .",
    "furthermore , to be extracted by the first few principal components , which are usually used for visualization and interpretation , a pattern must encode a substantial part of the variance in the data set .",
    "this means that small groups of samples may be difficult to extract visually , even if they share a  characteristic variable profile .    to address the shortcomings of pca and allow accurate visualization of more complex sample configurations , a variety of generalizations and alternatives to pca",
    "have been proposed , such as projection pursuit [ @xcite ; @xcite ] , kernel pca [ @xcite ] and other manifold learning methods such as isomap [ @xcite ] , locally linear embedding [ @xcite ] and laplacian eigenmaps [ @xcite ] .",
    "most of these methods do not automatically provide a related variable representation , which makes it more difficult to formulate hypotheses concerning the relationship between the variables and the patterns seen among the samples .",
    "in particular , this is true for methods based on multidimensional scaling ( mds ) , which create a low - dimensional sample representation based only on a given matrix of dissimilarities between the samples .    in this paper",
    "we present cumbia ( computational unsupervised method for bivisualization analysis ) , an exploratory mds - based method for creating a common low - dimensional representation of both the samples and the variables of a data set .",
    "we use the term `` bivisualization '' to denote both the process of creating low - dimensional sample and variable visualizations and the resulting joint representations . when using cumbia , we define a  measure of the dissimilarity between a sample and a variable , and use this to calculate sample  sample and variable  variable dissimilarities . all dissimilarities are put into a common dissimilarity matrix .",
    "finally , we apply classical mds to obtain a joint low - dimensional sample and variable representation . in this way",
    ", we obtain a biplot - like result where the relations between samples and variables can be readily explored .",
    "we apply cumbia to a synthetic data set as well as real - world data sets , and show that it provides useful bivisualizations which are often more informative than the biplots obtained by conventional methods for data sets containing small sample clusters sharing exceptional values for relatively few variables . in many cases",
    ", pca will fail to find these groups because they do not encode enough of the variance in the data .",
    "we therefore believe that the proposed method may be a valuable complement to existing methods for hypothesis generation and visual exploratory analysis of multivariate data sets .",
    "the approach described in this paper provides a joint visualization of both samples and variables , which is particularly useful for data sets containing small groups of samples sharing extreme values of few variables . to our knowledge , this problem has not been specifically addressed by previously proposed methods . in this section",
    "we compare our approach to some existing methods for finding and visualizing `` interesting '' variable combinations and corresponding sample groups .    constructing",
    "a biplot when the sample representation is obtained by pca is straightforward , as will be shown in section [ biplots ] .",
    "the _ nonlinear biplot _ was introduced by @xcite to generalize this result to more general sample representations . for",
    "a sample representation obtained by a  given ordination method , such as pca or mds ( based on a specific dissimilarity measure ) , gower and harding construct the variable representation by letting one variable at a time vary in a `` pseudo - sample , '' while keeping the values of the other variables fixed at their mean values across the original samples .",
    "then , the ( usually nonlinear ) trajectory of the pseudo - sample in the original sample representation is taken as a representation of the variable .",
    "these trajectories can often be interpreted in much the same way as ordinary coordinate axes .",
    "the approach described in our paper is different from that in @xcite , since both samples and variables are treated on an equal footing in the mds and , hence , all dissimilarities are used to obtain the low - dimensional representations . moreover",
    ", the nonlinear biplots may be hard to interpret when the number of variables is large .",
    "cumbia provides a joint low - dimensional representation of samples and variables which highlights other patterns than conventional multivariate visualization methods and where small groups of related objects are often readily visible .",
    "biclustering methods [ e.g. , @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ] have been proposed in different applications with the explicit aim of extracting subsets of samples ( documents ) and genes ( words ) , so - called biclusters , such that the variables in a subset are strongly related across the corresponding sample subset .",
    "some of the biclustering methods adopt a weighted bipartite graph approach [ @xcite ; @xcite ] .",
    "such an approach lies as the foundation also for cumbia .",
    "there are , however , important differences between biclustering methods and cumbia .",
    "the genes in a bicluster are extracted to exhibit similar profiles across the samples in the bicluster , while the variable clusters found by cumbia are highly expressed in the closely related samples compared to the rest .",
    "furthermore , biclustering algorithms aim to provide an exhaustive collection of significant biclusters , while visualization methods like the one we propose provide a visual representation of the most important features of the entire data set .",
    "this representation immediately allows the researcher to find clusters , detect outliers and obtain insights into the structure of the data which can be used to generate hypotheses . a further potential advantage of visualization methods compared to clustering is the ability to put objects `` in between '' two clusters , and to visualize the relationship between different clusters . in summary",
    ", although they are somewhat similar , biclustering and cumbia have different objectives and therefore are not likely to give the same results .",
    "projection pursuit methods [ @xcite ; @xcite ] are designed to search for particularly `` interesting '' directions in a multivariate data set , where `` interestingness '' can be defined , for example , as multimodality or deviation from gaussianity .",
    "pca is one example of a projection pursuit method , where the interesting directions are those with maximal variance . in this special case ,",
    "the optimal directions can be obtained by solving an eigenvalue problem but , in general , projection pursuit methods are iterative and the result may depend on the initialization .",
    "if the projections onto the extracted directions and the contributions of the variables to these are visualized simultaneously , the result can be interpreted to some extent like a biplot .",
    "in the following , we let @xmath0 denote a data matrix , containing the measured values of @xmath1 random variables in @xmath2 samples .",
    "we denote the element in the @xmath3th row and @xmath4th column of a matrix  @xmath5 by @xmath6 .",
    "furthermore , the frobenius norm of an @xmath7 matrix @xmath5 is defined  by @xmath8      in this section we will recapitulate how the singular value decomposition allows us to represent both the samples and the variables of a data set in lower - dimensional spaces . on a pair of such low - dimensional spaces",
    "we can define a  bilinear real - valued function , which when applied to a sample and a variable immediately approximates the value for the variable in that sample .",
    "this bilinear function will then be used to create a dissimilarity measure relating samples and variables .",
    "the singular value decomposition ( svd ) of a matrix @xmath9 with rank  @xmath10 is given by @xmath11 where @xmath12\\in\\mathbb{r}^{n\\times r}$ ] , @xmath13\\in\\mathbb{r}^{p\\times r}$ ] and @xmath14 .",
    "the columns of @xmath15 and @xmath16 are pairwise orthogonal and of unit length ( so @xmath17 ) , and @xmath18 is a diagonal matrix containing the positive singular values of @xmath19 in decreasing order along the diagonal",
    ". we will denote @xmath20 $ ] , @xmath21 $ ] , @xmath22 for @xmath23 .",
    "the svd can be used to create a rank-@xmath24 approximation of @xmath19 by @xmath25 we note that @xmath26 .",
    "the eckart  young theorem [ @xcite ] states that this approximation is optimal in the sense that @xmath27 the error in the approximation is given by @xmath28 [ @xcite ] .",
    "given a rank-@xmath24 approximation @xmath29 of a data matrix @xmath19 , we want to visualize its rows and columns in @xmath24-dimensional spaces ( typically @xmath30 or @xmath31 ) . for a fixed @xmath32 $ ]",
    ", we define @xmath24-dimensional spaces  @xmath33 and @xmath34 as the span of the orthogonal columns of @xmath35 and @xmath36 , respectively .",
    "next , we rewrite @xmath29 as @xmath37 this shows that the rows of @xmath36 can be seen as the coordinates for the approximated samples ( the rows of @xmath29 ) in the space @xmath33 .",
    "similarly , the rows of @xmath35 can be seen as the coordinates for the approximated variables in the space @xmath34 .",
    "hence , we take the @xmath2 rows of @xmath38 as the @xmath24-dimensional representations of the samples , and the @xmath1 rows of @xmath35 as the @xmath24-dimensional representations of the variables . choosing @xmath39 corresponds to conventional pca where the low - dimensional sample representation is given by the rows of @xmath40 and the _ principal components _",
    "( pcs ) are the columns of @xmath41 [ @xcite ; @xcite ] . with this choice of @xmath42 ,",
    "the pca representation provides an approximation of the euclidean distances between the samples of the data set [ @xcite ] . choosing instead @xmath43 would approximate the euclidean distances between the variables .",
    "we next define bilinear functions @xmath44 , by @xmath45 where @xmath46 and @xmath47 are the coordinate sequences of @xmath48 and @xmath49 in @xmath33 and @xmath34 , respectively .",
    "we note that the value for variable @xmath50 in sample @xmath51 can be computed as @xmath52 and approximated by @xmath53 for @xmath23 .    in classical biplots ,",
    "the samples are represented by the rows of @xmath36 and the variables are represented by the rows of @xmath35 in the same low - dimensional plot [ @xcite ] .",
    "then it follows from ( [ biplot ] ) and ( [ approximatevalue ] ) that the value of the variable @xmath50 in the sample @xmath51 can be approximated by taking the usual scalar product between the coordinate sequences for @xmath51 and @xmath50 [ @xcite ] .",
    "this makes it possible to use the low - dimensional biplots to visually draw conclusions about the relationships between groups of samples and variables .      using the value of @xmath54 as a measure of the similarity between sample @xmath51 and variable @xmath55 ,",
    "we define the squared dissimilarity between @xmath51 and @xmath50 as @xmath56 where @xmath57 is the largest singular value of @xmath19 ( this is a natural choice , making all dissimilarities nonnegative ) .",
    "we note that this is just one way of transforming a measure of similarity to a dissimilarity , and that there could be other possible transformations . to define the dissimilarities between two objects of the same type ( i.e. , two samples or two variables ) , we create a weighted bipartite graph . in this graph",
    ", each sample is connected to all variables , and each variable to all samples .",
    "the weight of an edge is taken as the dissimilarity between the corresponding nodes , calculated by ( [ distance ] ) .",
    "the dissimilarity  @xmath58 between two samples [ or @xmath59 between two variables ] is then defined as the shortest distance between the corresponding nodes in the weighted graph .",
    "together with ( [ distance ] ) , this yields a joint @xmath60 dissimilarity matrix containing the dissimilarities between all pairs of objects . in this work ,",
    "we restrict our attention to paths consisting of only two edges ( i.e. , going from one sample to another via only one variable , and vice versa ) , which will allow us to compute the sample  sample and variable  variable dissimilarities without actually creating the graph . by allowing more complex paths ,",
    "two samples could be considered similar if they are both similar to a  third sample , even if these similarities are due to completely different sets of variables . however",
    ", this may not be desirable in an application where the goal is to find biomarkers , since these should ideally be expressed very strongly in all samples in the corresponding group .. ]    from ( [ exactvalue ] ) , we note that if we choose @xmath61 , the dissimilarity between a  sample and a variable depends only on @xmath62 and the expression value of the variable in that sample .",
    "if we choose @xmath63 , ( [ approximatevalue ] ) implies that the dissimilarity @xmath64 is calculated from the approximated value of @xmath65 obtained by svd .",
    "using @xmath63 may be an advantage from a noise reduction point of view , since we in this case discard the smallest singular values and represent the data matrix only by its dominant features .",
    "it is important to note that by using a very small value of @xmath24 , we may discard a large part of the true signal as well .      to obtain a low - dimensional representation of the samples and variables from the dissimilarity matrix @xmath66",
    ", we apply classical mds [ @xcite ] .",
    "classical mds finds a low - dimensional projection with interpoint euclidean distances collected in the matrix @xmath67 , such that @xmath68 is minimized [ @xcite ; @xcite ] . here",
    ", @xmath69 where @xmath70 , @xmath71 with @xmath72 denoting the column vector with all entries equal to one , and @xmath73 is the number of objects .",
    "the optimal representation is obtained by the top eigenvectors of @xmath74 , scaled by the square root of the corresponding eigenvalues .",
    "if @xmath66 is a euclidean distance matrix , @xmath74 is a corresponding inner product matrix and classical mds returns the projections onto the principal components [ @xcite ] .",
    "if @xmath66 does not correspond to distances in a euclidean space , then @xmath74 is not positive semidefinite and , hence , some eigenvalues of @xmath74 are negative [ @xcite ] . in this case",
    "it is common either to add a suitable constant to all off - diagonal entries of @xmath66 , thereby making it correspond to a  distance matrix in a euclidean space [ @xcite ] , or to simply ignore the negative eigenvalues and compute the representation from the eigenvectors corresponding to the largest positive eigenvalues . in this paper",
    "we apply the latter approach .",
    "algorithm [ algorithm1 ] summarizes the main steps of cumbia and a small schematic example is provided in the supplementary material .",
    "input : data matrix @xmath0 , number of paths to average over ( @xmath75 ) .    1 .   [ compdiss ]",
    "compute the dissimilarities for all sample  variable pairs using ( [ distance ] ) .",
    "2 .   create a weighted bipartite graph , where the weight of an edge between a sample and a variable is equal to the dissimilarity computed in step [ compdiss ] .",
    "3 .   compute the dissimilarities for all sample  sample and variable ",
    "variable pairs as distances in the graph .",
    "average over the @xmath75 shortest paths .",
    "collect all dissimilarities in a common dissimilarity matrix and perform classical mds .",
    "visualize the result in a few dimensions .",
    "from the construction of the dissimilarity ( [ distance ] ) between samples and variables and the computation of sample  sample dissimilarities as graph distances it follows that two samples are considered similar if they share a high value for a single variable .",
    "this means that the proposed dissimilarity measure emphasizes mainly the large values in the data matrix @xmath19 .",
    "hence , as for pca and many other multivariate techniques , the scale of the variables will influence the results .",
    "the data can be normalized to the same scale before these methods are applied , for example , by subtracting the mean value and dividing by the standard deviation of each variable to obtain a matrix of @xmath76-scores .    with the proposed dissimilarity measure ,",
    "two identical samples will almost certainly have a positive dissimilarity with each other , which is somewhat counterintuitive . in this paper",
    "we put the dissimilarity between identical samples or variables to zero but other solutions are possible , such as multiplying the dissimilarity values with function values which are zero for identical objects and rise steeply toward one as the objects become more dissimilar .",
    "the function can be , for example , a sigmoidal function of the euclidean distance between the objects .",
    "in many practical applications , identical or near - identical objects are very uncommon and , therefore , this is not likely to have a major impact on the results from real data sets .",
    "it is important to note that from the construction of the bivisualization , it follows that it should be interpreted in terms of the relative distances between objects and not , as in conventional principal components biplots , in terms of the inner products between samples and variables .      creating a graph with edges connecting every sample ",
    "variable pair and computing the distances in the graph can be a time - consuming task if the number of variables or samples is large .",
    "however , by the construction of the dissimilarity measure ( [ distance ] ) , the dissimilarity matrix can be computed directly from the matrix @xmath29 and the largest singular value of @xmath19 by @xmath77   \\eqntext{1\\leq i , j\\leq p,\\ \\mathbf{w}_i\\neq\\mathbf{w}_j , } \\\\   d_s(\\mathbf{s}_i,\\mathbf{w}_j ) & = & \\sqrt{\\lambda_1 - ( x_s)_{ij } } ,    \\qquad 1\\leq i\\leq n,\\",
    "1\\leq j\\leq p , \\nonumber\\end{aligned}\\ ] ] where we let @xmath78 denote the element in the @xmath79th row and @xmath3th column of @xmath29 , and similarly for @xmath80 .",
    "the self - dissimilarities are always put to zero .",
    "however , also the classical mds has a high computational complexity , which implies that the number of samples and variables should not be too large .",
    "hence , in large data sets such as genome - wide expression data sets a variable selection should be performed before applying cumbia .",
    "the variable selection can be guided by expert knowledge in the field .",
    "alternatively , the algorithm can initially be applied , for example , to the probes from each chromosome individually or to random subsets of the variables .      since the visualization algorithm",
    "as described above depends only on the shortest path between two objects in the graph , it is sensitive to outliers , for example , large measurement errors for single variables . the stability can be increased by averaging over the @xmath75 shortest paths between any pair of samples ( or variables ) , but it should be noted that choosing a large @xmath75 decreases the ability to detect very small sample and variable groups .",
    "such a stabilization also permits a computationally efficient implementation , by replacing the @xmath81 value in ( [ compdistance ] ) by the average of the @xmath75 smallest values .",
    "it is possible to choose different values of @xmath75 for sample pairs and variable pairs .",
    "as described above , cumbia emphasizes the variables which are overexpressed in a  group of samples , and these variables and samples are placed close to each other in the low - dimensional joint visualization .",
    "the dissimilarities between jointly underexpressed variables are also calculated based on their highest expression values .",
    "since these may be very low , a group of variables which are jointly underexpressed may obtain large dissimilarities with each other .",
    "this means that these variables may not form a tight cluster located far from the corresponding samples , as in pca .",
    "the method can be adjusted to emphasize also this type of relationship , by changing the calculation of the sample  sample and variable  variable dissimilarities ( see the supplementary material for details ) .",
    "in order to illustrate and visually evaluate the characteristics of cumbia , we apply it to synthetic data as well as real - world data sets and compare the results to other methods .",
    "the first two examples illustrate the benefits of using cumbia for visualization of data sets where the nonrandom variation is attributable to a small group of variables being overexpressed in few samples , and the third example shows that cumbia performs well also in an example where the informative features encode a  large part of the variance in the data set , which is the situation where pca is most useful . taken together",
    ", these examples suggest that cumbia can provide useful visualizations in many different situations and since the feature extraction is not guided by variance content , we can obtain other insights into the data structure than with , for example , pca . in all examples , we compute the dissimilarity between pairs of samples ( or pairs of variables ) by averaging over the @xmath82 shortest paths in the graph .",
    "we use the original formulation of the algorithm , which means that we will focus on finding overexpressed variables .",
    "furthermore , we use @xmath83 to calculate the cumbia dissimilarity matrix ( [ compdistance ] ) , that is , we apply the method to the values in the original data matrix .",
    "we compare the visualizations obtained by cumbia to the biplots obtained from pca as well as results from a projection pursuit algorithm and the samba biclustering method [ @xcite ] .",
    "we applied the projection pursuit method implemented in the fastica package ( version 1.1 - 11 ) [ @xcite ] for r. this method searches for directions where the data show the largest deviation from gaussianity .",
    "first , the data are whitened by projecting onto the leading @xmath84 principal components , and then the projection pursuit directions are sequentially extracted from the whitened data .",
    "since these directions are not naturally ordered , we show all @xmath84 projection pursuit components and the corresponding sample representations in the supplementary figures .",
    "samba was applied through the expander software ( version 5.09 ) [ @xcite ] . as noted in section  [ relatedwork ] ,",
    "the aim of the biclustering methods is slightly different than that of cumbia , and the comparison mainly serves as an illustration of the different knowledge that can be visually extracted using cumbia compared to these methods .",
    "more examples showing the effect of choosing different parameter values in cumbia are available in the supplementary material [ @xcite ] .",
    "we simulate a data matrix @xmath19 consisting of 60 samples and 1,500 variables by letting @xmath85 hence , there is a small group of 25 variables characterizing a group of six samples .",
    "each variable is mean - centered and scaled to unit variance across all samples .",
    "figure  [ simex1 ] shows the low - dimensional representations of samples and variables obtained by cumbia and pca .",
    "we note that the small size of the related sample and variable group makes it impossible to extract clearly with pca in the first three components . even if more components are included , the two groups do not separate ( data not shown ) .",
    "we use @xmath86 principal components to whiten the data before applying the projection pursuit algorithm .",
    "the small group of six samples is not visible in any of the projection pursuit components either ( see the supplementary figures ) .",
    "in contrast , the first cumbia component discriminates the small sample group and the related variables from the rest .",
    "scree plots for cumbia and pca are available in the supplementary material . applying samba to the synthetic data set does not return any biclusters .",
    "next , we consider a real microarray data set , from a study of gene expression profiles from 61 normal human cell cultures .",
    "the cell cultures are taken from five cell types in 23 different tissues or organs , in total 31 different tissue / cell type combinations .",
    "the data set was downloaded from the national center for biotechnology informations ( ncbi ) gene expression omnibus ( geo , http://www.ncbi.nlm.nih.gov/geo/[http://www.ncbi . ]",
    "http://www.ncbi.nlm.nih.gov/geo/[nlm.nih.gov/geo/ ] , data set gds1402 ) .",
    "the original data set consists of 19,664 variables .",
    "we remove the variables containing missing values ( 2,741 variables ) or negative expression values ( another 517 variables ) , and the remaining values are @xmath87-transformed .",
    "to illustrate the ability of cumbia to detect small sample and variable clusters , we create a new data set from a subset of the variables in the microarray data set .",
    "we select two of the nontrivial sample subgroups , cardiac stromal cells ( @xmath88 ) and umbilical artery endothelial cells ( @xmath89 ) .",
    "for each of these sample subgroups and for each variable , we perform a @xmath90-test contrasting the selected subgroup against all other samples . for each of the two subgroups",
    ", we include the 50 variables having the highest positive value of the @xmath90-statistic .",
    "we further extend the new data set with the 1,500 variables showing the least discriminative power ( the lowest value of the @xmath91-statistic ) in an @xmath91-test contrasting all 31 subgroups .",
    "finally , all variables are mean centered and scaled to unit variance across the samples .",
    "the final data set now consists of @xmath92 variables and @xmath93 samples .",
    "this data set contains two relatively small sample groups , each of which is characterized by high values for a small subset of the variables .",
    "furthermore , the vast majority ( @xmath94 ) of the variables are not related to any of the predefined subgroups .",
    "figure  [ realex1 ] shows the low - dimensional representations of the samples and variables obtained by cumbia ( panel a ) and pca ( panel  b ) .",
    "the first two cumbia components successfully pick up the two small sample subgroups as well as the variables which are responsible for their close relation .",
    "these patterns do not encode enough variance to be seen in any of the three first principal components ( panel  b ) . in the projection onto the fourth and fifth principal components , the three cardiac stromal cell samples are visible as well as four of the six umbilical artery endothelial cells ( data not shown ) .",
    "clearly , by considering not only the variance of the extracted components as a measure of informativeness , cumbia highlights other features than pca .",
    "scree plots are available as the supplementary material .",
    "we used @xmath86 principal components for the whitening preceding the projection pursuit algorithm , which is able to detect the group of cardiac stromal cells , but the umbilical artery cells are considerably harder to extract ( see the supplementary figures ) .",
    "the projection pursuit algorithm further finds one single umbilical artery cell occupying one component together with a group of underexpressed variables . by modifying the cumbia algorithm to search for both over- and underexpressed variables",
    ", we also find this pattern ( see the supplementary material , figure s2 ) .",
    "for this data set , samba returns 26 biclusters with significant overlaps .",
    "eleven of these contain two of the cardiac stromal cells ( but none of them contain all three ) .",
    "eight biclusters contain at least two umbilical artery endothelial cell samples ( one contains all six ) .",
    "again , we note that the purpose of biclustering is not quite the same as the purpose of visualization which can also be seen in this example .    ) , and the 50 variables with highest discriminative power for this sample group , respectively .",
    "green markers similarly represent the umbilical artery endothelial cells ( @xmath89 ) and the corresponding variables .",
    "black markers represent samples from all other subgroups , and the 1,500 variables from the original data set which are least discriminating in an @xmath91-test contrasting all 31 tissue / cell type combinations in the data set . ]      in the previous examples we have shown that for data sets where the main nonrandom variation is attributable to small groups of samples sharing extreme values for small groups of variables , cumbia can produce sample and variable visualizations that are more informative than those resulting from pca and the applied projection pursuit algorithm .",
    "now , we consider a data set containing measurements of 1,145 micrornas in 20 human leukemia cell lines ( unpublished data ) .",
    "the cell lines correspond to three different leukemia types ; cml ( chronic myeloid leukemia ) , aml ( acute myeloid leukemia ) and b - all ( b - cell acute lymphoblastic leukemia ) .",
    "figure  [ mirnafigure ] shows the visualizations obtained by cumbia and pca . in this case",
    ", the feature distinguishing three of the cml samples ( red markers ) from the rest of the samples contains enough variance to be picked up by pca .",
    "the discrimination of these samples is apparent also with cumbia , where furthermore the third component effectively discriminates the aml group ( blue ) from the b - all group ( green ) .",
    "this effect is more readily visible than in the pca visualization .",
    "the cml group is biologically heterogeneous which can also be seen in the visualizations . to facilitate the interpretation of the visualizations ,",
    "we have colored all variables which are significantly higher expressed in one sample group than in the others .",
    "the heterogeneity of the cml group is reflected also here , in that some of the variables which are closely related to the three deviating cml samples are not significantly differentially expressed in the whole cml group . on the other hand , it is clear that the variables which have the most negative values on the third cumbia component are all highly expressed in the closely located aml samples ( blue ) .",
    "scree plots for cumbia and pca are available in the supplementary material .",
    "we used @xmath95 principal components in the whitening for projection pursuit , and the resulting components are shown in the supplementary figures . in this case , the sample representations from projection pursuit results are not very different from those of cumbia , but the coupling between the salient sample groups and the corresponding discriminating variables is stronger with cumbia . in the absence of external annotations ,",
    "this possibly enables formulating sharper and more correct hypotheses .",
    "applying samba to this data set returns 16 biclusters .",
    "generally , from these biclusters it is difficult to extract information distinguishing the three leukemia subtypes .",
    "-test , one - tailed @xmath96 , note that this information was not used to obtain the visualizations , but is merely displayed to facilitate the interpretation ) . ]    taken together , the examples indicate that cumbia is a useful complement to existing visualization methods in different contexts .",
    "it can find features commonly detected by existing methods such as pca and projection pursuit , but also features that are difficult to find with these methods .",
    "we have described cumbia : an unsupervised algorithm for exploratory analysis and simultaneous visualization of the samples and variables of a multivariate data set .",
    "the basis of the algorithm is classical multidimensional scaling ( mds ) , which is applied to a joint dissimilarity matrix and produces a common low - dimensional representation of samples and variables .",
    "the dissimilarity between a sample and a variable is based on the expression level of the variable in the ; a  higher expression level gives a lower dissimilarity .",
    "the dissimilarity between two samples ( or two variables ) is then defined by graph distances , influenced mainly by the variables ( samples ) with a high total expression level in the two samples ( variables ) . by applying the method to a synthetic as well as real - world data sets ,",
    "we have shown its ability to extract relevant sample and variable groups . compared to pca , which is commonly used for visualization of high - dimensional data ,",
    "the proposed method is advantageous for extracting small related variable and sample subgroups . according to the proposed dissimilarity measure",
    ", two samples will be considered close if they share a high value of one or a small group of variables .",
    "this is in contrast to pca , where the entire variable profiles are used to calculate the distance between a pair of samples .",
    "we believe that the proposed method may be a valuable complement to existing methods for exploratory analysis of multivariate data , to extract closely related sample clusters and immediately find the variables which are responsible for the discrimination .",
    "this group of variables can then be analyzed further and may constitute potential biomarkers for the corresponding sample group . as described in this paper , the proposed algorithm",
    "is mainly directed toward finding groups of samples sharing a high expression value of a , possibly small , group of variables , but can be adjusted to emphasize also jointly underexpressed variables .    by choosing different values of @xmath75 ( the number of paths to average over in the calculation of the cumbia dissimilarities ) , it is possible to detect different structures .",
    "a small value of @xmath75 makes it possible to find very small sample and variable groups but makes the method sensitive to noisy data . with increasing @xmath75",
    "the method becomes more robust , but it is also more difficult to detect the smallest groups . in an exploratory study , cumbia could be applied with different values of @xmath75 to find as many potentially relevant patterns as possible .",
    "putting the negative eigenvalues to zero in the classical mds as we have done in this paper potentially discards interesting information , as discussed by @xcite .",
    "interestingly , in the examples that we have given most eigenvalues are positive , but there is one large negative eigenvalue which corresponds to an eigenvector separating the sample objects from the variable objects . however , since we are mainly interested in the interaction between samples and variables , we focus on the largest positive eigenvalues of the inner product matrix and the corresponding eigenvectors .    the induced dissimilarities from cumbia may be potentially useful for clustering of samples and/or variables , for example , by hierarchical clustering [ @xcite ; @xcite ] .",
    "one would then expect small sample groups , characterized by few variables , to be clustered more closely than with hierarchical clustering based on , for example , euclidean distance .",
    "the dissimilarities can potentially also be used for simultaneous feature and sample selection from the data set by backward feature elimination , in a manner similar , for example , to the `` gene shaving '' [ @xcite ] and `` recursive feature elimination '' [ @xcite ] procedures .",
    "this could be done in the following way .",
    "first , the joint cumbia dissimilarity matrix for the entire data set is calculated .",
    "then , for each object ( sample or variable ) , the mean value of the @xmath97 smallest dissimilarities between the object and all objects of the same type ( i.e. , samples or variables ) are calculated for a suitable choice of @xmath97 . a given fraction of the objects , consisting of those with the largest value of the mean dissimilarity score ,",
    "can then be removed .",
    "this gives a new data matrix , with fewer samples and variables , to which the process may be applied .",
    "this algorithm provides a sequence of nested sample  variable biclusters .",
    "the optimal cluster size should be determined based on a suitably chosen optimality criterion .",
    "furthermore , when a bicluster has been found , the included variables and samples may be removed from the data set and another , disjoint bicluster may be found from the resulting matrix ."
  ],
  "abstract_text": [
    "<S> in order to find previously unknown subgroups in biomedical data and generate testable hypotheses , visually guided exploratory analysis can be of tremendous importance . in this paper </S>",
    "<S> we propose a new dissimilarity measure that can be used within the multidimensional scaling framework to obtain a joint low - dimensional representation of both the samples and variables of a multivariate data set , thereby providing an alternative to conventional biplots . in comparison with biplots , </S>",
    "<S> the representations obtained by our approach are particularly useful for exploratory analysis of data sets where there are small groups of variables sharing unusually high or low values for a small group of samples .    . </S>"
  ]
}