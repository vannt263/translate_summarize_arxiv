{
  "article_text": [
    "recent advances in multi - electrode recording techniques allow simultaneous measurements of neural activity from a large population of interacting neurons  @xcite .",
    "a population of neurons encodes various information by its collective spiking activity patterns , namely , neural codewords  @xcite .",
    "these codewords are passed and interpreted by a downstream circuit for further information processing .",
    "characterizing the organization of the codewords is therefore critical for our understanding of neural coding .    to characterize the distribution of codewords ,",
    "a maximum entropy model  @xcite with pairwise interaction terms has been fitted to neuroscience data  @xcite .",
    "this model that fits the first two moments of activity statistics was reported to characterize real data well in small groups of neurons .",
    "importantly , these studies also suggest that codewords are restricted due to neural interactions within a small subset of the state space , namely , the space composed of all possible combinations of each neuron s binary activity .",
    "however , the geometrical organization of codewords is not well understood .",
    "interestingly , the codewords of the well - known hopfield network  @xcite are also restricted within a small subset of state space due to strong constraints imposed by interactions between neurons .",
    "the state space of the hopfield network is organized into multiple basins of attraction  @xcite , with which a simple glauber dynamics  @xcite can recall one of memorized patterns hinted by a distorted initial pattern .",
    "this is the so - called associative memory  @xcite .",
    "although both the neuroscience model described above and the hopfield network belong to the pairwise maximum entropy model , it remains largely unknown if their codeword - spaces , composed of all codewords , share common features .",
    "recent investigation of retinal activity data revealed multiple local energy minima ( lem ) in a fitted maximum entropy model  @xcite .",
    "however , it does not provide how neural codewords are geometrically organized because demonstration of the codeword - space structure entails consideration of all possible states .",
    "the high dimensionality of the state space prevents an exhaustive search except in small networks , and standard dimensionality - reduction techniques can easily abolish underlying structure by neglecting many relevant dimensions .",
    "hence , an efficient new technique is in need to visualize the neural codeword - space .",
    "one insight is that distance between codewords is an important factor that constrains neural dynamics ",
    "previous experiments have shown that state transitions are mostly restricted to neighboring codewords and nearby codewords are known to encode similar information  @xcite . based on this observation , we propose the distance - constrained statistical mechanics analysis  @xcite to concisely characterize the codeword - space structure based on the distance from a reference codeword .",
    "in particular , we present an advanced mean - field framework that computes the entropy of codewords as a function of hamming distance from any reference state . by applying this technique to both the hopfield network and retinal data , we explore their codeword - space structures , i.e. , whether codewords are divided into multiple clusters .",
    "\\(a ) ) with low spiking rate constraint of stored patterns . (",
    "a ) a first order phase transition in hamming distance @xmath0 when the coupling field @xmath1 is tuned .",
    "the reference pattern is the one with four spikes ( see ( b ) ) .",
    "the inset shows that the first order transition vanishes in the high temperature regime ( @xmath2 ) .",
    "( b ) entropy per neuron as a function of hamming distance from a reference stored pattern with different spike - counts .",
    "the pattern with zero spike - count is named all - silent ( as ) state .",
    "the curves correspond to the low-@xmath0 branch of the hysteresis loop ( see ( a ) ) .",
    "the inset shows a trivial entropy landscape identical for all references in high temperature regime .",
    ", title=\"fig : \" ] .1 cm ( b ) ) with low spiking rate constraint of stored patterns .",
    "( a ) a first order phase transition in hamming distance @xmath0 when the coupling field @xmath1 is tuned .",
    "the reference pattern is the one with four spikes ( see ( b ) ) .",
    "the inset shows that the first order transition vanishes in the high temperature regime ( @xmath2 ) .",
    "( b ) entropy per neuron as a function of hamming distance from a reference stored pattern with different spike - counts .",
    "the pattern with zero spike - count is named all - silent ( as ) state .",
    "the curves correspond to the low-@xmath0 branch of the hysteresis loop ( see ( a ) ) .",
    "the inset shows a trivial entropy landscape identical for all references in high temperature regime .",
    ", title=\"fig : \" ] .1 cm    ) has an energy @xmath3 .",
    "( a ) when neural interactions are strong , neural codewords can be organized into multiple clusters in the state space .",
    "( b ) when the neural population is sufficiently noisy , a trivial structure ( a single cluster of neural codewords ) is observed . ]",
    "we first introduce a statistical mechanics framework to characterize codeword organization in the state space .",
    "let @xmath4 be binary activity of neuron @xmath5 and @xmath6 be a state vector , representing population activity of @xmath7 neurons . here",
    "@xmath8 indicates that neuron @xmath9 is active and @xmath10 indicates that neuron @xmath9 is silent .",
    "the symbol @xmath11 represents the transpose operation .    according to the maximum entropy principle  @xcite",
    ", the activity state follows the boltzmann distribution @xmath12 where @xmath13 is the inverse temperature or neural reliability ( @xmath14 unless otherwise indicated ) and the energy @xmath15 .",
    "@xmath16 denotes a spiking bias vector and @xmath17 a functional coupling matrix .",
    "geometrical organization of codewords is studied by introducing a modified probability distribution @xmath18 , where coupling field @xmath1 is introduced to control the overlap @xmath19 between state @xmath20 and reference one @xmath21 .",
    "this perturbed probability measure gives the free energy per neuron defined by @xmath22 where @xmath23 is the energy- and overlap - dependent free energy that characterizes the probability of states having energy @xmath24 and overlap @xmath25 , and @xmath26 denotes entropy ( log - number of states ) per neuron with energy @xmath24 and overlap @xmath25 . if the system - size @xmath7 is large , the integral in eq .",
    "( [ eq : f ] ) is typically dominated by a combination @xmath27 that minimizes @xmath28 , i.e. , @xmath29 .",
    "we compute @xmath30 and @xmath31 that minimize @xmath28 by applying the bethe approximation  @xcite ( see methods ) . by recursively solving the mean field equation",
    ", we estimate a local ( or global ) minimum of the free energy and @xmath27 corresponding to this minimum .",
    "notably , these values of @xmath30 and @xmath31 characterize the energy and overlap of typically observed states ( namely codewords ) , respectively .",
    "meanwhile , the entropy of codewords @xmath32 can be also computed according to eq .",
    "( [ eq : feq ] ) .",
    "we define hamming distance @xmath33 that counts how many neurons have distinct activity in state @xmath20 and reference state @xmath34 .",
    "the typical value of the overlap @xmath31 can be transformed to the typical value of hamming distance per neuron @xmath35 . in the following sections , we omit the @xmath30 dependency of the entropy and",
    "report it as a function of @xmath0 , i.e. , @xmath36 .      using the mean field method",
    ", we first investigate the structure of codeword - space in the hopfield network  @xcite . in this model ,",
    "the coupling between neuron @xmath9 and @xmath37 is constructed as @xmath38 for a network of @xmath39 neurons , where @xmath40 random binary patterns ( indexed by @xmath41 ) are stored . in each pattern , stored activity @xmath42 of neuron @xmath9 takes @xmath43 with probability @xmath44 and @xmath45 with probability @xmath46 .",
    "@xmath47 is chosen to fit the activity level of retinal neurons we study in the next section .",
    "note that , in the hopfield model , the neurons have zero spiking bias parameters ( @xmath48 ) .",
    "according to the previous section , we compute the typical distance @xmath0 as we increase @xmath1 from @xmath49 to @xmath50 , and then decrease it from @xmath50 to @xmath49 ( fig .",
    "[ hopf ] ( a ) ) . more precisely , after the convergence of the mean field equations at some @xmath1 , we change @xmath1 by a small amount and restart iteration from the previous fixed point ( see methods ) .",
    "the reference state @xmath34 is set to one of the stored patterns .",
    "remarkably , we find a first - order phase transition of @xmath0 , characterized by the hysteresis loop ( fig .",
    "[ hopf ] ( a ) ) .",
    "as we decrease @xmath1 from high to low values , the typical distance suddenly jumps at around @xmath51 from @xmath52 to @xmath53 , implying a non - trivial structure of the codeword - space .    in order to more directly visualize the non - trivial structure of the codeword - space",
    ", we plot the entropy of codewords computed at various distance @xmath0 away from each stored pattern .",
    "only the entropy values corresponding to the low-@xmath0 branch of the hysteresis loop are shown in the figure .",
    "as shown in fig .",
    "[ hopf ] ( b ) , each stored pattern has a dense core of codewords around itself , which discontinuously falls off at some distance .",
    "this indicates that codewords are organized into multiple clusters , separated by non - codeword states ( i.e. , gaps ) . among three stored patterns , the all - silent ( as ) state has the largest core due to the low spiking rate constraint of stored patterns ( small @xmath47 )",
    "this clustering results from the attractor structure  @xcite in the retrieval phase of the model . within the hysteresis loop , there are two local minima of the free energy ( eq .",
    "( [ eq : feq ] ) ) competing with each other .",
    "low-@xmath0 minimum corresponds to the nearby codewords of stored patterns ( @xmath54 ) , while high-@xmath0 minimum corresponds to nearby codewords of corresponding reversed patterns ( @xmath55 ) .",
    "thus each stored pattern has distinct entropy landscape surrounding it .",
    "the codeword - space clustering is necessary for successful memory retrieval in the hopfield network .",
    "in fact , in a high temperature regime ( non - recallable phase ) , the first order transition and the non - trivial entropy landscape are absent , as observed in the insets of fig .",
    "all reference patterns display the same entropy landscape without entropy gaps , and thus the patterns can not be distinguished from each other . in this non - recallable phase of the hopfield network , there do exist multiple lem ( see fig .",
    "[ lands ] ( b ) ) under greedy descent dynamics ( gdd , see methods ) on the energy surface , while the codeword - space structure is trivial without entropy gaps .",
    "( a ) a first order phase transition in hamming distance when the coupling field is tuned .",
    "the reference is a codeword of ten spike - counts .",
    "the neural codeword - space structure is shaped by the correlations in the neural spiking activity .",
    "the first - order transition disappears for independent model ( ind ) .",
    "@xmath56 defines the distance at which the low-@xmath0 branch in the hysteresis loop terminates .",
    "( b ) distance - dependent entropy landscape from reference neural codewords of different spike - counts . ( c )",
    "maximum distance @xmath56 versus spike - counts of the reference states ( distance from as state ) .",
    "five references for each spike - count are randomly chosen .",
    "the line is a linear fit .",
    "( d ) distance entropy from neural codewords ( state @xmath57 ) and their corresponding lem ( state @xmath58 ) .",
    "the corresponding lem are identified by gdd .",
    "the random codeword limit is the upper bound .",
    ", title=\"fig : \" ] .1 cm ( b ) ) .",
    "( a ) a first order phase transition in hamming distance when the coupling field is tuned .",
    "the reference is a codeword of ten spike - counts .",
    "the neural codeword - space structure is shaped by the correlations in the neural spiking activity .",
    "the first - order transition disappears for independent model ( ind ) .",
    "@xmath56 defines the distance at which the low-@xmath0 branch in the hysteresis loop terminates .",
    "( b ) distance - dependent entropy landscape from reference neural codewords of different spike - counts . ( c )",
    "maximum distance @xmath56 versus spike - counts of the reference states ( distance from as state ) .",
    "five references for each spike - count are randomly chosen .",
    "the line is a linear fit .",
    "( d ) distance entropy from neural codewords ( state @xmath57 ) and their corresponding lem ( state @xmath58 ) .",
    "the corresponding lem are identified by gdd .",
    "the random codeword limit is the upper bound .",
    ", title=\"fig : \" ] .1 cm ( c ) ) .",
    "( a ) a first order phase transition in hamming distance when the coupling field is tuned .",
    "the reference is a codeword of ten spike - counts .",
    "the neural codeword - space structure is shaped by the correlations in the neural spiking activity .",
    "the first - order transition disappears for independent model ( ind ) .",
    "@xmath56 defines the distance at which the low-@xmath0 branch in the hysteresis loop terminates .",
    "( b ) distance - dependent entropy landscape from reference neural codewords of different spike - counts . ( c )",
    "maximum distance @xmath56 versus spike - counts of the reference states ( distance from as state ) .",
    "five references for each spike - count are randomly chosen .",
    "the line is a linear fit .",
    "( d ) distance entropy from neural codewords ( state @xmath57 ) and their corresponding lem ( state @xmath58 ) .",
    "the corresponding lem are identified by gdd .",
    "the random codeword limit is the upper bound .",
    ", title=\"fig : \" ] .1 cm ( d ) ) .",
    "( a ) a first order phase transition in hamming distance when the coupling field is tuned .",
    "the reference is a codeword of ten spike - counts .",
    "the neural codeword - space structure is shaped by the correlations in the neural spiking activity .",
    "the first - order transition disappears for independent model ( ind ) .",
    "@xmath56 defines the distance at which the low-@xmath0 branch in the hysteresis loop terminates .",
    "( b ) distance - dependent entropy landscape from reference neural codewords of different spike - counts . ( c )",
    "maximum distance @xmath56 versus spike - counts of the reference states ( distance from as state ) .",
    "five references for each spike - count are randomly chosen .",
    "the line is a linear fit .",
    "( d ) distance entropy from neural codewords ( state @xmath57 ) and their corresponding lem ( state @xmath58 ) .",
    "the corresponding lem are identified by gdd .",
    "the random codeword limit is the upper bound .",
    ", title=\"fig : \" ] .1 cm      the next important question is how codewords of a real neural population are organized . to elucidate this question , we analyze spiking activity data of populations of retinal ganglion cells under a repeated naturalistic movie stimulus  @xcite .",
    "although multiple lem were previously found using this data set  @xcite , it is still unknown if the observed network has clustering of codewords or not ( fig .",
    "[ lands ] ( a ) ) .",
    "we therefore characterize the geometrical organization of retinal codewords by applying the same method as used in the hopfield network .",
    "the neural spike trains in a population of @xmath7 neurons are binned with a @xmath59 ms temporal resolution to have @xmath7-dimensional spiking states @xmath20 . spiking bias @xmath16 and functional coupling @xmath17",
    "are fitted to the spike train data to reproduce the mean activity and pairwise correlation of the data ( see methods ) .",
    "we choose randomly a network sample of the size @xmath39 from the neural data ( the behavior reported below does not change qualitatively when another sample is chosen , see supplementary fig .",
    "[ retina02 ] ) .",
    "despite no clear similarity in the connectivity structure to the hopfiled network , the retinal network displays the first - order phase transition with a hysteresis loop , qualitatively resembling the hopfield model ( fig .  [ retina ] ( a ) ) .",
    "this establishes that codewords of the retinal network are also clustered .",
    "furthermore , by constructing an independent maximum entropy model , where only the mean activity is fitted to the data with @xmath60 , we show that the first - order phase transition disappears , indicating that it is the non - trivial neural correlations that shape the clustering of codewords .",
    "[ retina ] ( b ) shows the entropy as a function of @xmath0 when neural codewords of different spike - counts are selected as references .",
    "again , only the entropy values corresponding to the low-@xmath0 branch of the hysteresis loop are shown in the figure .",
    "the high-@xmath0 branch is not biologically plausible , since the neural code is sparse .",
    "the entropy landscape is strongly dependent of the reference . in general , the higher spike - counts a neural codeword has , the larger distance its entropy curve extends over , enhancing the ability of the high spike - count codeword to come back to the sparse coding regime around the as state . to quantify this property , fig .",
    "[ retina ] ( c ) plots the maximum distance @xmath56 at which the low-@xmath0 branch in the hysteresis loop terminates as a function of spike - counts of the reference codeword .",
    "we find that @xmath56 increases linearly with the spike - counts ( distance from the as state ) and the estimated slope is @xmath61 .",
    "the slope close to one is also observed in another typical example ( see supplementary fig .  [ retina02 ] ) .",
    "note that the distance to the as state is typically smaller than @xmath56 .",
    "this implies that , even if the neural codeword is far away from the as state , it still has easy access to the sparse coding regime around the as state within reasonable time , which highlights the potential role of the as state  @xcite .",
    "the as state plays a special role here because the entropy curve from the as reference state grows much more rapidly as a function of distance than from the other codewords ( fig .",
    "[ retina ] ( d ) ) .",
    "indeed , its growth is close to the upper bound given by the random codeword limit ( @xmath62 ) , in which every state is equally likely .",
    "this indicates that the as state has the densest core of codewords around it , which would facilitate frequent visits from other neural codewords ( see supplementary fig .",
    "[ evod ] ) . as previously observed  @xcite ,",
    "a large portion of neural patterns ( about @xmath63 of @xmath64 patterns ) are observed to evolve to the as state by following gdd ( see methods ) .",
    "[ retina ] ( d ) reports the distance - dependent entropy landscape for some reference lem codewords ( e.g. , state @xmath58 ) obtained by running the gdd method starting from corresponding reference non - lem codewords ( resp .",
    "state @xmath57 ) ( see the corresponding multidimensional scaling ( mds ) map of lem in supplementary fig .",
    "[ mdsln ] ( a ) ) .",
    "the result shows that each reference has a different landscape , and at small @xmath0 , the entropy around a non - lem codeword is typically smaller than that for the corresponding lem codeword .",
    "moreover , for some states ( e.g. , @xmath50 and @xmath65 ) , there exist two continuous parts separated by a gap in the distance entropy curve .",
    "we shall elaborate this phenomenon in the following section by studying a larger population , where the effect becomes much more evident .",
    "this shows another clear evidence for the clustering of neural codewords .",
    "( a ) two hysteresis loops are observed .",
    "the reference is state @xmath66 ( see ( c ) ) .",
    "( b ) maximum distance @xmath56 at which the low-@xmath0 branch in the hysteresis loop terminates versus spike - counts of the reference ( distance from as state ) .",
    "five references for each spike - count are randomly chosen .",
    "( c ) entropy curve for different codewords and their assigned lem .",
    "complex structure is observed for state @xmath67 , @xmath66 and @xmath68 .",
    ", title=\"fig : \" ] .1 cm ( b ) ) . ( a ) two hysteresis loops are observed .",
    "the reference is state @xmath66 ( see ( c ) ) .",
    "( b ) maximum distance @xmath56 at which the low-@xmath0 branch in the hysteresis loop terminates versus spike - counts of the reference ( distance from as state ) .",
    "five references for each spike - count are randomly chosen .",
    "( c ) entropy curve for different codewords and their assigned lem .",
    "complex structure is observed for state @xmath67 , @xmath66 and @xmath68 .",
    ", title=\"fig : \" ] .1 cm ( c ) ) .",
    "( a ) two hysteresis loops are observed .",
    "the reference is state @xmath66 ( see ( c ) ) .",
    "( b ) maximum distance @xmath56 at which the low-@xmath0 branch in the hysteresis loop terminates versus spike - counts of the reference ( distance from as state ) .",
    "five references for each spike - count are randomly chosen .",
    "( c ) entropy curve for different codewords and their assigned lem .",
    "complex structure is observed for state @xmath67 , @xmath66 and @xmath68 .",
    ", title=\"fig : \" ] .1 cm      the property of the neural codeword shown above is still preserved when large populations of neurons are considered . in fig .",
    "[ retinaln ] , we show the theoretical result computed on a network sample of @xmath69",
    ". as the network size grows , the number of lem detected by gdd method also increases .",
    "accordingly , the internal structure of the codewords becomes more complicated ( a rough visualization is given by the mds map , see supplementary fig .",
    "[ mdsln ] ( b ) ) . as shown in fig .",
    "[ retinaln ] ( a ) , there exist two hysteresis loops separated by another monostable branch ( two curves for increasing and decreasing @xmath1 coincide with each other ) .",
    "these two successive hysteresis loops naturally arise if there exist three deep minima in the energy landscape , where sweeping @xmath1 shifts a dominant contribution from one to another .",
    "[ retinaln ] ( b ) shows that @xmath56 grows with spike - counts ( distance from the as state ) .",
    "the growth is likely nonlinear in this case , perhaps induced by the complexity of the state space . the fraction of neural codewords that can reach the as state without in - between gaps reduces from the result of the previous section to about @xmath70 .",
    "note that this number is still dominant compared to the reachability of other detected lem .",
    "again , the as state has the densest surrounding core , characterized by the rapid growth of the entropy with distance ( fig .",
    "[ retinaln ] ( c ) ) .",
    "the entropy landscape surrounding the as state does not have a second monostable branch beyond the first entropy gap , except at a biologically implausible distance close to @xmath67 .",
    "this might be because there is no deep enough minima around the as state .",
    "in contrast , the entropy landscape surrounding some other reference codewords , e.g. , state @xmath66 , exhibits a second monostable branch beyond the first entropy gap ( see fig .",
    "[ retinaln ] ( a ) ) , likely indicating that there is another deep minimum around them .    to demonstrate the implication of the entropy landscape , we study how distance from a local energy minimum changes with time when the neural system explores the state space .",
    "we use the local dynamics rule characterized by the transition probability @xmath71 where @xmath72 denotes the effective spiking bias of neuron @xmath9 . under this dynamics",
    ", states are sampled from the original distribution @xmath73 .",
    "note that the gdd rule to obtain lem allows only monotonically decreasing energy on the energy surface .",
    "in contrast , the current dynamics rule allows the energy to increase occasionally .",
    "sampled distance from a reference local energy minimum @xmath74 is denoted by @xmath75 where @xmath76 denotes the time step .",
    "the mean field prediction @xmath77 of a typical codeword - distance is given by setting @xmath78 and initializing the iteration equation ( see methods ) at @xmath21 .",
    "note that @xmath78 corresponds to the case without distance - constraint , and thus takes into account all codewords in the cluster that @xmath21 belongs to . as expected",
    ", this calculation predicts the fluctuation plateau of @xmath79 close to the reference , as shown in fig .",
    "[ dynamics ] ( a ) and ( b ) .",
    "note that the local dynamics escapes fast from the as state ( see the inset of fig .  [ dynamics ] ( a ) ) , which may be related to its very small core ( fig .  [ retinaln ] ( c ) ) .",
    "the same qualitative behavior holds for the smaller network ( @xmath39 , see supplementary fig .",
    "[ dynn60 ] ) and when the neural dynamics is simulated starting from a non - lem codeword .",
    "( a ) typical trajectory observed for reference as ( inset ) , state 1 and 2 in simulations .",
    "the ( solid , dashed , dotted ) line is the theoretical prediction computed at @xmath78 for each reference .",
    "( b ) the fluctuation plateau of @xmath80 is predicted by the mean field theory ( @xmath81 ) .",
    "five trials from the same reference are considered for each data point .",
    "each trial lasts for @xmath82 steps .",
    "each step corresponds to @xmath7 proposed flips .",
    "note that in the inset of ( a ) , one step corresponds to one possible flip .",
    "lq : lower quartile ; med : median ; uq : upper quartile .",
    ", title=\"fig : \" ] .1 cm ( b ) ) .",
    "( a ) typical trajectory observed for reference as ( inset ) , state 1 and 2 in simulations .",
    "the ( solid , dashed , dotted ) line is the theoretical prediction computed at @xmath78 for each reference .",
    "( b ) the fluctuation plateau of @xmath80 is predicted by the mean field theory ( @xmath81 ) .",
    "five trials from the same reference are considered for each data point .",
    "each trial lasts for @xmath82 steps .",
    "each step corresponds to @xmath7 proposed flips .",
    "note that in the inset of ( a ) , one step corresponds to one possible flip .",
    "lq : lower quartile ; med : median ; uq : upper quartile .",
    ", title=\"fig : \" ] .1 cm",
    "in this work , we have established the resemblance of codeword organization between the retinal network and the hopfield network . in previous studies , the memory retrieval function of hopfield network",
    "was empirically compared to the behavior of real networks  @xcite .",
    "however , no theoretical framework was proposed to build a solid relationship between these artificial and real biological networks .",
    "in fact , they are naturally distinct in terms of detailed parameters .",
    "surprisingly , we have found that the two networks both similarly organize their codewords .",
    "the clustering of codewords has been identified by the first - order phase transition in the codeword - distance .",
    "this transition is accompanied by hysteresis loops , which becomes increasingly complex as the network size grows .",
    "we have also revealed that the as state has a distinct role from other codewords . the number of codewords surrounding the as state always grows much more rapidly as a function of distance compared to that surrounding other codewords .",
    "interestingly , despite the presence of entropy gaps , most codewords even far away from the as state could still have easy access to it because of their surrounding dense cores of codewords typically extending beyond the as state .",
    "thus , the most frequently observed as state plays a key role in serving as a hub facilitating neural exploration of the codeword - space .",
    "the only knowledge a neuronal population can have comes from the population activity of interacting neurons .",
    "as shown in our study , there exists well - designed structure of codewords in the neural state space .",
    "the codewords are partitioned into multiple clusters separated by entropy gaps .",
    "moreover , this emergent property remains even if one - fourth of our data is used to learn the model ( see supplementary fig .",
    "[ overf ] ) .",
    "thus the revealed organization structure is most likely an intrinsic property of the retinal network , and downstream brain areas may benefit from this structure for decoding purpose .",
    "the clustering is functionally advantageous and intimately related to the network function , i.e. , pattern completion ( error - correction ) and pattern separation ( discrimination ability ) . upon repeated presentations of the same visual stimulus ,",
    "the neural responses show strong trial - to - trial variability  @xcite .",
    "however , all codewords belonging to the same core perhaps encode the same feature of the semantic information  @xcite .",
    "this property also allows the neural code to be robust against the ubiquitous noise in nervous systems  @xcite . in an analogous way to error - correcting codes",
    "@xcite , even if the neural codeword is corrupted by a small amount of noise , the dense core structure still allows population coding of stimulus features .",
    "therefore , the non - trivial internal structure of the neural codeword - space is useful for the neural population not only to discriminate different neural activity pattern , but also to carry out error - correction  @xcite .",
    "the retina as an early visual system should adapt to the visual stimulus distribution to efficiently transmit relevant information to downstream brain areas .",
    "the energy landscape shaped by the neural interactions likely depends on the natural scene statistics .",
    "it is therefore interesting to study their relationship under the current context .",
    "the codeword - space structure quantitatively predicts the fluctuation plateau of the simulated neural dynamics starting from lem .",
    "hence , our analytic framework establishes the relationship between the simulated neural dynamics and clustering of codewords . in previous studies , the match between spontaneous neural activity and the stimulus - evoked activity increases during development especially for natural stimuli  @xcite , and the spontaneous activity outlines the regime of evoked neural responses  @xcite .",
    "our analysis might further reveal how spontaneous neural activity is related to the vocabulary of neural codewords a neural circuit learns to internally represent external worlds .",
    "overall , our study provides an important step to understand the stationary distribution of neural spiking patterns and its functional relevance , which also sheds light on future studies of the sensory processing in other brain areas .",
    "we are grateful to michael j. berry for sharing us the retinal data .",
    "this work was supported by the program for brain mapping by integrated neurotechnologies for disease studies ( brain / minds ) from japan agency for medical research and development , amed .",
    "the spiking activity of @xmath83 retinal ganglion cells was collected from a @xmath84 @xmath85 patch of the salamander retina , when a repeated naturalistic movie was presented .",
    "the visual stimulus consists of @xmath86 repeats of a @xmath87s long movie clip being a gray movie of swimming fish and swaying water plants in a tank ( data courtesy of michael j. berry ii , see experimental details in the original paper  @xcite ) .",
    "the spike train data is binned with the bin size @xmath88 reflecting the temporal correlation time scale , yielding about @xmath89 binary neural codewords for model analysis .      for a neuronal population of size @xmath7",
    ", the neural spike trains of duration @xmath90 are binned at temporal resolution @xmath91 , producing @xmath92 samples of @xmath7-dimensional binary neural codewords .",
    "we use @xmath93 to indicate spiking activity of neuron @xmath9 , and @xmath10 for silent activity .",
    "the neural responses to repeated stimulus are highly variable ( so - called trial - to - trial variability , see fig .",
    "[ evod ] ) . to model the neural codeword statistics , we assign each codeword",
    "@xmath20 a cost function ( energy in statistical physics jargon ) @xmath3 , then the probability of observing one codeword @xmath20 is written as @xmath94 , where @xmath95 the spiking bias @xmath96 and neuronal coupling @xmath97 are constructed from the spike train data such that the spiking rate @xmath98 and the pairwise correlation @xmath99 under the model match those computed from the data .",
    "high energy state @xmath20 corresponds to low probability of observation .",
    "this is a low dimensional representation of the original high dimensional neural codewords , since we need only @xmath100 model parameters .    to find the model parameters , we apply the maximum likelihood learning principle corresponding to maximizing the log - likelihood @xmath101 with respect to the parameters .",
    "the learning equation is given by    [ le ] @xmath102    where @xmath76 and @xmath103 denote the learning step and learning rate , respectively .",
    "the maximum likelihood learning shown here has a simple interpretation of minimizing the kullback - leibler divergence between the empirical probability and the model probability  @xcite . in the learning equation ( eq .  ( [ le ] ) ) ,",
    "the data dependent terms can be easily computed from the binned neural data . however , the model expectation of the spiking rate ( magnetization in statistical physics ) and pairwise correlation is quite hard to evaluate without any approximations .",
    "here we propose the mean field method to tackle this difficulty .",
    "the statistical properties of the model ( eq .  ( [ energyising ] ) ) can be analyzed by the cavity method in the mean field theory  @xcite .",
    "the self - consistent equations are written in the form of message passing ( detailed derivation is given in refs  @xcite ) as    [ bp ] @xmath104    where @xmath105 denotes the member of interaction @xmath106 expect @xmath9 , and @xmath107 denotes the interaction set @xmath9 is involved in with @xmath58 removed . @xmath108 and @xmath109 .",
    "@xmath110 is interpreted as the message passing from the neuron @xmath9 to the interaction @xmath58 it participates in , while @xmath111 is interpreted as the message passing from the interaction @xmath106 to its member @xmath9 .",
    "iteration of the message passing equation on the inferred model would converge to a fixed point corresponding to a global ( local ) minimum of the free energy ( in the cavity method approximation  @xcite ) @xmath112 where @xmath113 is the normalization constant ( partition function ) of the model probability @xmath101 .",
    "the free energy contribution of one neuron @xmath114 where @xmath115 , and the free energy contribution of one interaction @xmath116 . at the same time",
    ", the model spiking rate and multi - neuron correlation can also be estimated as    [ magcorre ] @xmath117    we have defined @xmath118 and @xmath119 . note that the iteration converges in a few steps at each learning stage , and estimated magnetizations as well as correlations are used in the gradient ascent learning step . here the multi - neuron correlation is calculated directly from the cavity method approximation  @xcite and expected to be accurate enough for current neural data analysis .",
    "another advantage is the low computational cost .",
    "a more accurate expression could be derived from linear response theory  @xcite with much more expensive computational cost .",
    "finally , one can also estimate the entropy of the model from the fixed point of the message passing equation .",
    "the entropy is defined as @xmath120 , and it measures the capacity of the neural population for information transmission .",
    "more obvious variability of the neural responses implies larger entropy value .",
    "based on the standard thermodynamic relation , @xmath121 , where @xmath122 is the energy of the neural population and given by    [ energ ] @xmath123\\\\ & \\times\\prod_{a\\in\\partial i\\backslash b}\\cosh\\gamma_a(1+x\\hat{m}_{a\\rightarrow i } ) .",
    "\\end{split}\\end{aligned}\\ ] ]      to uncover the internal structure of the neural codeword - space , we introduce a modified probability measure  @xcite @xmath124 where @xmath13 is the inverse temperature or neural reliability , and the coupling field @xmath1 is introduced to control the overlap between the neural codeword @xmath20 and a reference one @xmath21 .",
    "the partition function @xmath113 can be approximated by a saddle point analysis , i.e. , @xmath125 , from which the free energy per neuron @xmath126 ( density ) is given by @xmath127 , where @xmath30 is the energy density ( @xmath128 ) , @xmath129 the entropy density ( @xmath130 ) and @xmath31 the typical value of the overlap ( @xmath131 ) .",
    "note that the hamming distance per neuron is related to the overlap by @xmath35 . according to the double legendre transform ,",
    "the entropy density is calculated via @xmath132 .",
    "@xmath133 counts the number of valid configurations around the reference satisfying both the distance constraint ( @xmath0 ) and the energy density ( @xmath30 ) . here",
    "@xmath13 controls the energy level and @xmath1 selects the overlap or hamming distance .",
    "the overlap @xmath31 is given by @xmath134 with @xmath98 being calculated under the modified probability measure .",
    "@xmath135 obeys the following equations : @xmath136 and @xmath137 . in this",
    "setting , the above iteration equations ( eq .  ( [ bp ] ) )",
    "remain unchanged except that the bias is changed to @xmath138 and the coupling is rescaled as @xmath139 . for the real neuronal network , the neural reliability @xmath14 ,",
    "since the constructed biases and couplings reflect the neural noise observed in the spike train data .",
    "for the hopfield model , higher @xmath13 implies weaker thermal fluctuation and may correspond to a retrieval phase for pattern completion .",
    "note that to compute the entropy curve for metastable or unstable branches of distance - coupling field curve , one has to fix @xmath0 by searching for compatible coupling field @xmath1 , e.g. , by the secant method  @xcite .      to search for a local energy minimum starting from",
    "any given neural activity pattern , we use greedy descent dynamics ( gdd ) in the energy landscape  @xcite . to be more precise , for each neuron",
    ", we flip its activity if the flip will decrease the energy .",
    "if we could not decrease the energy by flipping any neuron s activity , then a local energy minimum is identified .",
    "such minima are also called single - flip stable attractors , i.e. , their energy can not be decreased by flipping any single neuron s activity .",
    "we choose randomly a pattern set of size @xmath64 from the neural data to ensure that any two patterns are rarely identical . by applying the gdd method",
    ", we identify a lem set whose size is much smaller than that of the pattern set , with a large portion of patterns evolving to the all - silent state . the number of lem increases with the network size .",
    "these lem are then expressed in a low dimensional space ( called multidimensional scaling analysis ( mds )  @xcite ) .",
    "mds represents the proximity between lem in the high dimensional space with some degree of fidelity by the distance between points in the low dimensional space .      in the case of fitting only the first moments ( mean spiking activity ) ,",
    "the distance entropy can be computed exactly .",
    "the result is given by @xmath140 where @xmath141 and @xmath142 .",
    "fig .  [ mdsln ] corresponds to fig .",
    "[ retina ] and fig .",
    "[ retinaln ] in the main text . fig .",
    "[ retina02 ] shows another typical example of a network of @xmath143 neurons .",
    "the qualitative properties do not change .",
    "[ dynn60 ] shows the neural dynamics result for smaller networks ( @xmath39 ) .",
    "[ overf ] shows that the problem structure is not affected by the finite sampling of the data .",
    "[ evod ] shows the role of the as state with temporal information included .",
    "the time - dependent hamming distance is defined as @xmath144 , and the time - dependent spike - counts @xmath145 .",
    "\\(a )   serve as coordinates ) .",
    "state @xmath146 is the as state .",
    "( a ) mds map for a population of @xmath143 neurons ( see fig .  [ retina ] ) .",
    "( b ) mds map for a population of @xmath82 neurons ( see fig .  [ retinaln ] ) , in which @xmath147 lem are identified by gdd method .",
    "it becomes difficult to represent faithfully these lem in a low dimensional space ( some information are lost ) , nevertheless , the map still shows how they are distributed .",
    ", title=\"fig : \" ] .1 cm ( b )   serve as coordinates ) .",
    "state @xmath146 is the as state .",
    "( a ) mds map for a population of @xmath143 neurons ( see fig .  [ retina ] ) .",
    "( b ) mds map for a population of @xmath82 neurons ( see fig .  [ retinaln ] ) , in which @xmath147 lem are identified by gdd method .",
    "it becomes difficult to represent faithfully these lem in a low dimensional space ( some information are lost ) , nevertheless , the map still shows how they are distributed .",
    ", title=\"fig : \" ]    \\(a ) , another typical example ) .",
    "( a ) a first order phase transition in hamming distance when the coupling field is tuned .",
    "the transition disappears for an independent model ( ind ) .",
    "( b ) distance entropy from reference neural codewords of different spike - counts .",
    "( c ) maximum distance @xmath56 at which the low-@xmath0 branch in the hysteresis loop terminates versus spike - counts of the reference ( distance from as state ) .",
    "five references for each spike - count are considered .",
    "the line is a linear fit ( slope=@xmath148 ) .",
    "( d ) distance entropy from neural codewords and their corresponding lem .",
    "( e ) low dimensional representation of lem corresponding to ( d ) by multidimensional scaling ( mds ) analysis ( @xmath149 serve as coordinates ) .",
    "state @xmath146 is the as state .",
    ", title=\"fig : \" ] .1 cm ( b ) , another typical example ) .",
    "( a ) a first order phase transition in hamming distance when the coupling field is tuned .",
    "the transition disappears for an independent model ( ind ) .",
    "( b ) distance entropy from reference neural codewords of different spike - counts .",
    "( c ) maximum distance @xmath56 at which the low-@xmath0 branch in the hysteresis loop terminates versus spike - counts of the reference ( distance from as state ) .",
    "five references for each spike - count are considered .",
    "the line is a linear fit ( slope=@xmath148 ) .",
    "( d ) distance entropy from neural codewords and their corresponding lem .",
    "( e ) low dimensional representation of lem corresponding to ( d ) by multidimensional scaling ( mds ) analysis ( @xmath149 serve as coordinates ) .",
    "state @xmath146 is the as state .",
    ", title=\"fig : \" ] .1 cm ( c ) , another typical example ) .",
    "( a ) a first order phase transition in hamming distance when the coupling field is tuned .",
    "the transition disappears for an independent model ( ind ) .",
    "( b ) distance entropy from reference neural codewords of different spike - counts .",
    "( c ) maximum distance @xmath56 at which the low-@xmath0 branch in the hysteresis loop terminates versus spike - counts of the reference ( distance from as state ) .",
    "five references for each spike - count are considered .",
    "the line is a linear fit ( slope=@xmath148 ) .",
    "( d ) distance entropy from neural codewords and their corresponding lem .",
    "( e ) low dimensional representation of lem corresponding to ( d ) by multidimensional scaling ( mds ) analysis ( @xmath149 serve as coordinates ) .",
    "state @xmath146 is the as state .",
    ", title=\"fig : \" ] .1 cm ( d ) , another typical example ) .",
    "( a ) a first order phase transition in hamming distance when the coupling field is tuned .",
    "the transition disappears for an independent model ( ind ) .",
    "( b ) distance entropy from reference neural codewords of different spike - counts .",
    "( c ) maximum distance @xmath56 at which the low-@xmath0 branch in the hysteresis loop terminates versus spike - counts of the reference ( distance from as state ) .",
    "five references for each spike - count are considered .",
    "the line is a linear fit ( slope=@xmath148 ) .",
    "( d ) distance entropy from neural codewords and their corresponding lem .",
    "( e ) low dimensional representation of lem corresponding to ( d ) by multidimensional scaling ( mds ) analysis ( @xmath149 serve as coordinates ) .",
    "state @xmath146 is the as state .",
    ", title=\"fig : \" ] .1 cm ( e ) , another typical example ) .",
    "( a ) a first order phase transition in hamming distance when the coupling field is tuned .",
    "the transition disappears for an independent model ( ind ) .",
    "( b ) distance entropy from reference neural codewords of different spike - counts .",
    "( c ) maximum distance @xmath56 at which the low-@xmath0 branch in the hysteresis loop terminates versus spike - counts of the reference ( distance from as state ) .",
    "five references for each spike - count are considered .",
    "the line is a linear fit ( slope=@xmath148 ) .",
    "( d ) distance entropy from neural codewords and their corresponding lem .",
    "( e ) low dimensional representation of lem corresponding to ( d ) by multidimensional scaling ( mds ) analysis ( @xmath149 serve as coordinates ) .",
    "state @xmath146 is the as state .",
    ", title=\"fig : \" ] .1 cm    \\(a ) ) .",
    "( a , b ) the network with @xmath143 neurons in the main text .",
    "( c , d ) the network with @xmath143 neurons corresponding to fig .",
    "[ retina02 ] .",
    "( a , c ) typical trajectory observed in simulations .",
    "the ( solid , dashed , dotted ) line is the theoretical prediction computed at @xmath78 for each reference .",
    "( b , d ) the fluctuation plateau of @xmath80 is predicted by the mean field theory ( @xmath81 ) .",
    "five trials from the same reference are considered for each data point .",
    "each trial lasts for @xmath82 steps .",
    "lq : lower quartile ; med : median ; uq : upper quartile .",
    ", title=\"fig : \" ] .1 cm ( b ) ) .",
    "( a , b ) the network with @xmath143 neurons in the main text .",
    "( c , d ) the network with @xmath143 neurons corresponding to fig .",
    "[ retina02 ] .",
    "( a , c ) typical trajectory observed in simulations .",
    "the ( solid , dashed , dotted ) line is the theoretical prediction computed at @xmath78 for each reference .",
    "( b , d ) the fluctuation plateau of @xmath80 is predicted by the mean field theory ( @xmath81 ) .",
    "five trials from the same reference are considered for each data point .",
    "each trial lasts for @xmath82 steps .",
    "lq : lower quartile ; med : median ; uq : upper quartile .",
    ", title=\"fig : \" ] .1 cm ( c ) ) .",
    "( a , b ) the network with @xmath143 neurons in the main text .",
    "( c , d ) the network with @xmath143 neurons corresponding to fig .",
    "[ retina02 ] .",
    "( a , c ) typical trajectory observed in simulations .",
    "the ( solid , dashed , dotted ) line is the theoretical prediction computed at @xmath78 for each reference .",
    "( b , d ) the fluctuation plateau of @xmath80 is predicted by the mean field theory ( @xmath81 ) .",
    "five trials from the same reference are considered for each data point .",
    "each trial lasts for @xmath82 steps .",
    "lq : lower quartile ; med : median ; uq : upper quartile .",
    ", title=\"fig : \" ] .1 cm ( d ) ) .",
    "( a , b ) the network with @xmath143 neurons in the main text .",
    "( c , d ) the network with @xmath143 neurons corresponding to fig .  [ retina02 ] .",
    "( a , c ) typical trajectory observed in simulations .",
    "the ( solid , dashed , dotted ) line is the theoretical prediction computed at @xmath78 for each reference .",
    "( b , d ) the fluctuation plateau of @xmath80 is predicted by the mean field theory ( @xmath81 ) .",
    "five trials from the same reference are considered for each data point .",
    "each trial lasts for @xmath82 steps .",
    "lq : lower quartile ; med : median ; uq : upper quartile .",
    ", title=\"fig : \" ] .1 cm      \\(a ) , typical example shown in the main text ) .",
    "( a ) the profile for four repeats .",
    "trial - to - trial variability is observed .",
    "( b ) the profile for only one repeat .",
    "the as state is frequently visited , and the neural network seems to explore the state space by local moves .",
    ", title=\"fig : \" ] .1 cm ( b ) , typical example shown in the main text ) .",
    "( a ) the profile for four repeats .",
    "trial - to - trial variability is observed .",
    "( b ) the profile for only one repeat .",
    "the as state is frequently visited , and the neural network seems to explore the state space by local moves .",
    ", title=\"fig : \" ] .1 cm                a.  tang , d.  jackson , j.  hobbs , w.  chen , j.  l. smith , h.  patel , a.  prieto , d.  petrusca , m.  i. grivich , a.  sher , p.  hottowy , w.  dabrowski , a.  m. litke , and j.  m. beggs .",
    "a maximum entropy model applied to spatial and temporal correlations from cortical networks in vitro .",
    ", 28:505 , 2008 ."
  ],
  "abstract_text": [
    "<S> a network of neurons in the central nervous system collectively represents information by its spiking activity states . typically observed states , </S>",
    "<S> i.e. , codewords , occupy only a limited portion of the state space due to constraints imposed by network interactions . </S>",
    "<S> geometrical organization of codewords in the state space , critical for neural information processing , is poorly understood due to its high dimensionality . here </S>",
    "<S> , we explore the organization of neural codewords using retinal data by computing the entropy of codewords as a function of hamming distance from a particular reference codeword . specifically , we report that the retinal codewords in the state space are divided into multiple distinct clusters separated by entropy - gaps , and that this structure is shared with well - known associative memory networks in a recallable phase . </S>",
    "<S> our analysis also elucidates a special nature of the all - silent state . </S>",
    "<S> the all - silent state is surrounded by the densest cluster of codewords and located within a reachable distance from most codewords . </S>",
    "<S> this codeword - space structure quantitatively predicts typical deviation of a state - trajectory from its initial state . </S>",
    "<S> altogether , our findings reveal a non - trivial heterogeneous structure of the codeword - space that shapes information representation in a biological network . </S>"
  ]
}