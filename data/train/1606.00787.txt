{
  "article_text": [
    "there are many cases in bayesian modeling where a certain choice of prior distribution allows for computationally simple or tractable inference .",
    "for example ,    * conjugate priors yield posteriors with a known parametric form and therefore allow for non - iterative , exact inference @xcite . *",
    "certain priors yield models with tractable conditional or marginal distributions , which allows efficient approximate inference algorithms to be applied ( e.g. gibbs sampling @xcite , sampling in collapsed models @xcite , or mean - field variational methods @xcite ) .",
    "* simple parametric priors ( e.g. the normal distribution ) allow for computationally cheap density queries , maximization , and sampling @xcite , which can allow for easier use in iterative inference algorithms ( e.g. metropolis - hastings @xcite , gradient - based mcmc @xcite , or sequential monte carlo @xcite ) . *",
    "certain priors mitigate issues of identifiability , and allow for simpler posteriors without multiple modes @xcite .",
    "however , more sophisticated priors that provide a better depiction of observed or expert knowledge do not , in general , allow for the above inference techniques . instead , researchers must resort to more general and computationally expensive inference methods .",
    "this can encourage the use of convenient priors in practice , rather than priors that might yield a more realistic or accurate inference , which is a criticism of bayesian methods @xcite .    in this paper , we investigate the following question : for a given model ,",
    "is it possible to use _ any convenient prior _ to infer a false posterior , and afterwards , given some true prior of interest , quickly transform this result into the true posterior ?    intuitively , our strategy is the following : for a given model , we first choose any computationally convenient false prior , and perform inference , which returns the false posterior .",
    "we then use our inferred false posterior , a true prior of interest , and the false prior , to efficiently produce samples from the true posterior via a method we call _ prior swapping_.    one key attribute of prior swapping is that it runs without touching any data .",
    "existing general inference algorithms are iterative and _ data - dependent _ : parameter updates at each iteration involve data , and the computational complexity or quality of each update depends on the amount of data used .",
    "thus , inference on larger datasets takes more time .",
    "furthermore , in practice , we often want to perform inference for a number of different priors or hyperparameter settings ; for each , some data - dependent inference algorithm must be run to compute a new result .",
    "in contrast , prior swapping algorithms are _ data - independent_i.e .",
    "parameter updates do not involve data , and neither the complexity nor quality of each update depends on data size .",
    "we therefore advocate doing difficult inference in two steps : first , perform data - dependent inference using the most computationally convenient prior for a given model , and then , for all future priors of interest ( e.g. complex priors or a range of prior hyperparameter settings ) , perform quick , data - independent prior swapping .",
    "we summarize the advantages of this approach :    1 .",
    "there often exists some false posterior that can be computed more efficiently than the true posterior ; in some cases , we can therefore apply significantly less - costly inference procedures to more - sophisticated models than previously possible .",
    "2 .   the prior swapping procedure runs independently of the data ; this can allow for considerable speed - ups when doing inference on large datasets or ranges of hyperparameter values .",
    "3 .   we can often maintain theoretical guarantees of existing approximate inference algorithms ; e.g. when inferring the false posterior with an asymptotically - exact method ( such as sampling ) , prior swapping can return asymptotically - exact true posterior samples .",
    "we can update our prior , or incorporate new prior information , in an online fashion , without redoing data - dependent inference .",
    "this is a black box procedure , which can be run directly on the output of most existing inference methods to add richer prior information to inference results .",
    "suppose we have a dataset of @xmath0 real , finite - dimensional vectors @xmath1 @xmath2 @xmath3 @xmath4 , and we are interested in a family of models specified by the likelihood function @xmath5 , parameterized by a real , d - dimensional vector @xmath6 .",
    "suppose we have a prior distribution over the space of model parameters @xmath7 , with probability density function ( pdf ) @xmath8 .",
    "the likelihood and prior define a joint model given by the pdf @xmath9 @xmath2 @xmath10 . in bayesian inference , we are interested in computing the posterior distribution",
    " i.e . a conditional distribution of this joint model  with pdf @xmath11 suppose we ve chosen a different prior distribution @xmath12 , which we refer to as a _ false prior _",
    "( while we refer to @xmath8 as the _ true prior _ ) .",
    "we can now can define a new joint model @xmath13 @xmath2 @xmath14 , with posterior ( conditional ) pdf @xmath15 .",
    "we refer to this second posterior distribution as a _",
    "false posterior_.    the goal of our method is to first infer a false posterior and then leverage it to infer a true posterior . to carry out this transformation",
    ", we use the _ prior swap function _",
    "@xmath16 , which we define as the false posterior density multiplied by the true prior density and divided by the false prior density , i.e. @xmath17    note that @xmath16 is proportional to the true posterior density @xmath18 .",
    "however , depending on how we represent the false posterior @xmath19 , @xmath16 can have a much simpler analytic representation than @xmath18 , which is typically defined via a likelihood function ( which is a function of the data ; see eq .",
    "[ trueposterior ] ) .",
    "it is therefore often possible to apply existing inference algorithms to @xmath16 , instead of @xmath18 , to perform quicker inference .",
    "we will use @xmath20 to denote the _ prior swap density _",
    ", i.e. @xmath20 @xmath2 @xmath21 .",
    "our strategy is to first compute some expression for @xmath19 , and then infer the true posterior using @xmath20 as a target density .",
    "this can be done with various types of inference for @xmath19 , including closed form ( exact ) computations , sampling - based methods ( such as markov chain monte carlo ) , and other approximate inference strategies such as variational inference and expectation propagation methods .",
    "we illustrate prior swapping in fig .",
    "[ fig : priorswappingillustration ] .",
    "[ [ fixed - complexity - false - posterior . ] ] fixed - complexity false posterior .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    as a first case , suppose that we have computed an analytic expression @xmath22 for the false posterior density with a fixed - complexity parametric form ( i.e. the complexity of the expression does not grow as inference proceeds ) .",
    "for example , this is the case if we    * compute @xmath19 exactly ( in a closed form ) via a conjugate prior . *",
    "generate samples from @xmath19 , via mcmc or other sampling methods , then use these samples to compute a parametric density estimate @xmath23 . *",
    "apply optimization - based approximate inference methods , such as variational inference or expectation propagation , that return analytic approximate posteriors @xmath24 or @xmath25 .    in each case , while we use some data - dependent algorithm to infer the false posterior , our resulting expression @xmath22 is not a function of the data . given this expression ,",
    "consider the functions    [ cols= \" < , < \" , ]     we can use these to draw posterior samples extremely efficiently with a variety of standard mcmc algorithms . at each iteration in mcmc , to draw a new parameter , we must evaluate a target function associated with the posterior density .",
    "for example , we evaluate a function proportional to @xmath26 in metropolis - hastings ( mh ) @xcite and @xmath27 in gradient - based mcmc methods ( such as langevin dynamics ( ld ) @xcite , hamiltonian monte carlo ( hmc ) @xcite , and their stochastic variants @xcite ) . due to the likelihood function in @xmath18 ,",
    "these target functions are data - dependent .    in prior swapping",
    ", we instead evaluate @xmath28 ( eq .  [ psfixedcomplexity01 ] ) in mh , or @xmath29 ( eq .  [ psfixedcomplexity02 ] ) in ld , hmc , or gradient ascent optimization to a map point estimate ( see appendix for detailed algorithm pseudocode ) . here",
    ", each iteration only requires evaluating a few simple analytic expressions , and thus has @xmath30 complexity with respect to data size .",
    "[ [ consistent - false - posterior . ] ] consistent false posterior .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    suppose our false posterior inference procedure yields a set of samples @xmath31 . above",
    ", we proposed the strategy of computing a fixed - complexity parametric density estimate @xmath23 , and plugging this into the prior swap function ( which we will denote @xmath32 ) .",
    "this @xmath33 is , in general , an inconsistent estimate of @xmath19 , and using it in prior swapping yields asymptotically-_biased _ samples from @xmath18 . here",
    ", we aim to answer the question : given samples from @xmath19 , can we develop an efficient method that returns asymptotically-_exact _ samples from @xmath18 ?",
    "suppose we instead use a consistent false posterior density estimate , such as a nonparametric @xcite or semiparametric @xcite estimate .",
    "we will prove ( sec .",
    "[ theoreticalguarantees ] ) that plugging a consistent estimate for @xmath19 into the prior swap function yields asymptotically - exact samples . however , the cost of these consistent estimates grows with the number of samples @xmath34 ; typically , evaluating their pdf or gradient has a complexity of @xmath35 . using these estimates for @xmath19 in the above prior swapping procedure",
    "therefore has a complexity of @xmath35 _ per iteration _ , which is costly for large @xmath34 .",
    "we instead propose the following prior swapping method , which still yields asymptotically - exact samples from @xmath18 , yet does not require a significant increase in computation : first generate approximate posterior samples using @xmath32 ( as above ) , and then _ correct _ these samples via algorithms from the parallel mcmc literature ( described below ) , designed to sample from the product of densities .    to motivate this method , we choose a a consistent semiparametric false posterior estimate @xmath36 ( see @xcite for background and consistency guarantees ) , which can be viewed as the product of a parametric density estimate @xmath23 and a nonparametric correction function .",
    "this is written @xmath37 , \\end{aligned}\\ ] ] where we use @xmath38 to denote a probability density kernel , with bandwidth @xmath39 , where @xmath40 as @xmath41 ( see @xcite for details on probability density kernels and bandwidth selection ) .",
    "a general parametric family for @xmath23 that we can use in nearly all cases is the family of false prior distributions .",
    "these are typically simple parametric distributions over the correct parameter space , which contain parameterizations in a broad vicinity of @xmath19 , and for which there exist efficient parameter estimation algorithms .",
    "given @xmath42 , we write the semiparametric prior swap function as @xmath43       \\left [ \\frac{1}{t } \\sum_{t=1}^t       \\frac{k \\left(\\frac{\\|\\theta - \\widetilde{\\theta}_t\\|}{h}\\right ) }      { \\hat{f}_{\\phi|x}^p(\\widetilde{\\theta}_t ) h^d }      \\right ]      =      \\left [ \\hat{f}_{\\widetilde{ps}}^p ( \\theta ) \\right ]      \\left [ \\frac{1}{t } \\sum_{t=1}^t w_t      k \\left(\\frac{\\|\\theta - \\widetilde{\\theta}_t\\|}{h}\\right )      \\right ] .",
    "\\nonumber\\end{aligned}\\ ] ] hence , the prior swap function is proportional to the product of two densities : the parametric prior swap density , and a correction density .",
    "we can easily generate samples from both of the densities that comprise @xmath44the former with ( fixed - complexity ) parametric prior swapping and the latter by sampling from components in the correction density with frequency proportional to the components weights .",
    "r0.59    estimate @xmath23 using @xmath45 @xmath46 parametric estimate .",
    "+ sample @xmath47 @xmath46 parametric prior swapping .",
    "+ sample @xmath48 .",
    "+ sample @xmath49 via @xmath50 .",
    "we therefore turn to sample combination methods for efficiently generating samples from the product of densities @xcite . given two sets of samples",
    "@xmath51 @xmath52 @xmath53 and @xmath54 @xmath52 @xmath55 , these algorithms aim to return @xmath56 @xmath2 @xmath57 .",
    "these methods are efficient because each density product sample can be generated without iterating through either set of @xmath34 input samples ; typically , only a single sample from each input density is required",
    ". each sample can therefore be produced with constant @xmath30 complexity .",
    "we summarize the full asymptotically - exact prior swapping procedure in alg .",
    "[ alg : priorswapping ] . in the appendix",
    ", we give pseudocode for the density product sample combination algorithms , summarize the complexity of all methods , and also discuss how this semiparametric prior swapping method allows for easy incorporation of observed prior information .",
    "this setting also has some relation to importance sampling ( is ) @xcite .",
    "for example , one could use the false posterior as an is proposal ( i.e. simply reweight and then use samples from @xmath19 ) .",
    "however , in practice , performance would only be adequate for false posteriors that are very similar to the true posterior ( and not for arbitrary @xmath58 , especially in high dimensions @xcite .",
    "we show this empirically by comparing against this strategy in sec .",
    "[ empiricalresults ] .      here",
    "we give theoretical guarantees about the correctness of the samples generated via prior swapping .",
    "first , note that if we have an exact analytic expression @xmath59 for the false posterior , the prior swap function is proportional to the true posterior , i.e. @xmath60 . using @xmath61 in standard mcmc algorithms therefore carries out mcmc on an exact true posterior target and comes with existing guarantees , such as producing asymptotically - exact posterior samples @xcite .    in the second setting",
    ", to prove that we generate asymptotically - exact samples given a consistent false posterior estimator , we need to show , as @xmath41 , that @xmath62 is consistent for @xmath20 and that alg .  [",
    "alg : priorswapping ] indeed draws samples from @xmath62 .",
    "given false posterior samples @xmath31 and @xmath63 , the estimator @xmath64 ( eq .  [ eq : semiparest ] ) is consistent for @xmath16 , i.e. its mean - squared error satisfies @xmath65      < \\frac{c}{t^{4/(4+d ) } } \\nonumber\\end{aligned}\\ ] ] for some @xmath66 and @xmath67 .",
    "the procedure given in alg .",
    "[ alg : priorswapping ] generates samples from @xmath68 ( eq .",
    "[ eq : semiparest ] ) as @xmath41 .",
    "proofs for both theorems and definition of @xmath69 are given in the appendix .",
    "we show empirical results on bayesian generalized linear models ( including linear and hierarchical logistic regression ) with sparsity and heavy tailed priors , on latent factor models ( including mixture models and topic models ) with relational priors over factors ( e.g. diversity - encouraging , agglomerate - encouraging , etc . ) , and feedforward neural networks , where we show hyperparameter tuning of weight - decay l2 regularization via prior swapping with normal priors .",
    "we aim to demonstrate empirically that prior swapping allows us to apply less - costly inference algorithms to more - complex models than was previously possible , and that it efficiently yields correct samples .",
    "we compare the following inference procedures :    * * mcmc on the true posterior : * mcmc sampling algorithms run directly on @xmath18 .",
    "* * vi on the true posterior : * variational inference algorithms run directly on @xmath18 . *",
    "* false posterior : * using the inferred false posterior @xmath19 . * * importance sampling with false posterior proposal : * using @xmath19 as a proposal density and running importance sampling . * * prior swapping ( fixed - complexity ) : * prior swapping using a fixed - complexity expression @xmath22 , including exactly - computed @xmath19 , parametric estimates @xmath23 , and approximations @xmath24 . * * prior swapping ( consistent ) : * prior swapping via alg .",
    "[ alg : priorswapping ] using a semiparametric @xmath36 . * * prior swapping to map : * optimization , using the prior swap function , to a map point estimate . *",
    "* false - true hybrid prior * prior swapping on @xmath70 without dividing out the false prior . *",
    "* normal approximation : * prior swapping using normal approximations for @xmath19 , @xmath71 , and @xmath72 .    to assess performance , we use two metrics : posterior error and test error . to compute posterior error",
    ", we run a single chain of mcmc on the true posterior for one million steps , and use these samples as groundtruth ( after removing the first quarter as burn in ) .",
    "we then compare these groundtruth samples with those returned by our inference methods . specifically , we compute the euclidean distance between the means of both sets of samples , the euclidean distance between the variances of both sets of samples , and the estimated @xmath73 distance between the groundtruth density @xmath74 and a sampled posterior density @xmath75 : @xmath76 .",
    "posterior error is the normalized average of these three computations .",
    "test error , which we report for classification models , is the proportion of misclassified held - out ( test ) data . for timing plots , to assess performance at a given time point , we collect samples drawn before this time point , remove the first quarter as burn in , and compute the above metrics .",
    "sparsity - encouraging regularizers have gained a high level of popularity over the past decade due to their ability to produce models with greater interpretability , predictive accuracy , and parsimony .",
    "for example , the @xmath77 norm has been used to induce sparsity with great effect @xcite , and has been shown to be equivalent to a mean - zero independent laplace prior @xcite . in a bayesian setting , inference given a sparsity prior can be difficult , and researchers often resort to computationally intensive methods ( such as hamiltonian monte carlo ) or biased posterior approximations ( e.g. expectation propagation @xcite ) that make factorization or parametric assumptions @xcite .",
    "we provide a cheap yet accurate solution : first use a more - tractable prior ( such as a normal prior , in which closed form inference is possible ) , and then use prior swapping to quickly convert the result to the posterior given a sparsity prior .",
    "our first set of experiments are on sparse bayesian linear regression models @xcite , which we can write as @xmath78 . for @xmath71 , we compute results on on laplace , student s t , and verysparse ( with pdf @xmath79 @xmath2 @xmath80 @xcite ) priors . here , a normal @xmath72 is conjugate and allows for exact inference .",
    "our second set of experiments are on a hierarchical bayesian logistic regression model @xcite , which we write as @xmath81 .",
    "we will consider a hierarchical @xmath82 . here",
    ", we also use a normal @xmath72 ( see @xcite for examples of convenient inference in bayesian logistic regression under normal priors ) . in these experiments",
    ", we generate synthetic data from the models in order to show results under varying @xmath0 and @xmath83 . for comparison methods , we use mh for mcmc , and a mean field approximation @xcite for vi .    in fig .",
    "[ fig : lmall ] we show results for sparse linear models , with @xmath0=10,000 observations , and @xmath83=20 dimensions . in ( a ) and ( b )",
    "we show convergence plots and see that prior swapping performs faster inference ( by a few orders of magnitude ) than mcmc .",
    "we also see that semiparametric prior swapping ( alg .",
    "[ alg : priorswapping ] ) achieves nearly identical performance as prior swapping on the exactly computed @xmath19 .",
    "the other comparison methods yield posteriors that are incorrect or very slow to converge . in ( b )",
    "we halve the variance of our false prior , which hurts performance of the comparison methods , but leaves prior swapping unchanged . in ( c )",
    "we show 1-d density marginals as we increase the prior sparsity , and in ( d ) we show prior swapping results for different sparsity priors .    in fig .",
    "[ fig : lrall ] , we show results for hierarchical logistic regression . in ( a ) and",
    "( b ) we vary the number of observations ( @xmath0=10 - 120,000 ) and see that prior swapping has a constant wall time while the wall times of both mcmc and vi increase with @xmath0 . in ( b ) we see that the prior swapping methods achieve the same test error as the standard inference methods . in ( c ) and ( d )",
    "we vary the number of dimensions ( @xmath83=1 - 40 ) . in this case , all methods have increasing wall time , and again the test errors match . in ( e ) , ( f ) , and ( g ) , we vary the prior hyperparameter ( @xmath84=1 - 1.05 ) . for prior swapping ,",
    "we infer a single @xmath19 ( using @xmath85 ) with both mcmc and vi , and compute _ all other _ hyperparameter results using this @xmath19 .",
    "this demonstrates that prior swapping can quickly infer correct results over a range of hyperparameters . here",
    ", the asymptotically - exact semiparametric prior swapping method matches the test error of mcmc slightly better than the parametric method .",
    "many latent variable models in machine learning  such as mixture models , topic models , probabilistic matrix factorization , and various others ",
    "involve a set of latent factors ( e.g. components or topics )",
    ". often , we d like to use priors that encourage interesting behaviors among the factors .",
    "for example , we might want dissimilar factors via a diversity - promoting prior @xcite or for the factors to show some sort of sparsity pattern @xcite .",
    "inference in such models is often computationally expensive or designed on a case - by - case basis @xcite .",
    "however , when conjugate priors are placed over the factor parameters , collapsed gibbs sampling can be applied . in this method ,",
    "the factor parameters are integrated out , leaving only a subset of variables ; on these , the conditional distributions can be computed analytically , which allows for gibbs sampling over these variables .",
    "afterwards , samples of the collapsed factor parameters can be computed .",
    "hence , we propose the following strategy : first , assign a prior for the factor parameters that allows for collapsed gibbs sampling ; afterwards , reconstruct the factor samples and apply prior swapping for more complex relational priors over the factors .",
    "we can thus perform convenient inference in the collapsed model , yet apply more - sophisticated priors to variables in the uncollapsed model .",
    "we first show results on a gaussian mixture model ( gmm ) @xcite , written @xmath86 .",
    "using a normal @xmath72 over @xmath87 allows for collapsed gibbs sampling .",
    "we also show results on a topic model ( latent dirichlet allocation ( lda ) @xcite ) for text data ( for the form of this model , see @xcite ) . here , using a dirichlet @xmath72 over topics allows for collapsed gibbs sampling . for mixture models ,",
    "we generate synthetic data from the above model ( @xmath0=10,000 , @xmath83=2 , @xmath88=9 ) , and for topic models , we use the simple english wikipedia corpus ( n=27,443 documents , vocab=10,192 words ) @xcite , and set @xmath88=400 topics .    in fig .",
    "[ fig : latentfactor ] we show results for mixture and topic models . in ( a )",
    "we show inferred posteriors over gmm components for a number of relational priors , which we define in ( b ) . in ( c )",
    ", we apply the diversity - promoting prior to lda , to separate redundant topics . here , we show two topic - clusters ( `` geography '' and `` family '' ) in @xmath19 , which are separated into thematically - similar yet distinct topics after prior swapping . in ( a ) and",
    "( c ) we also show wall times of the inference methods .      learning neural networks with weight decay ( l2 regularization )",
    "can be viewed as finding the map point estimate of a bayesian neural network model with a normal prior @xcite .",
    "since the prior swap function can be used as an objective for optimization to a map estimate , we aim to use prior swapping for quick hyperparameter tuning of weight decay in neural networks .",
    "we will compare this to finding the optimal weight decay via stochastic gradient descent @xcite and stochastic gradient langevin dynamics @xcite , two popular methods for learning and inference in neural networks .",
    "these stochastic gradient methods have only a weak dependence on data at each iteration ; however , their updates may be noisy or suboptimal @xcite , while prior swapping updates involve exact gradients ( of the prior swap function ) without any stochasticity .",
    "furthermore , note that we can use stochastic gradient inference methods in conjunction with prior swapping ( i.e. to compute @xmath19 , which we will do here ) .",
    "r0.7    we run our experiment on an eight - layer fully connected deep neural network with 400 nodes per layer , yielding a model with 1,280,410 dimensions . for data , we use the mnist handwritten digits classification dataset ( @xmath0=60,000 , @xmath83=784 ) @xcite .",
    "we consider a family of true priors @xmath89 over neural network parameters ( for weight decay ) , and aim to compute map point estimates over a range of hyperparameters @xmath90 . due to the high dimensionality",
    ", we make a parametric approximation @xmath33 for the false posterior , which we assume to be normal with a diagonal covariance matrix .",
    "we use sgld to infer @xmath33 ( choosing a single @xmath90 to parameterize @xmath72 ) . for prior swapping ,",
    "we use this @xmath33 to learn map estimates for all other @xmath90 values in the range .    in fig  [ fig : nnall ]",
    "we show wall time and test error for the comparison methods over the set of weight decay parameters ( @xmath90=50 - 10,000 ) . for prior swapping ,",
    "we perform gradient ascent optimization using the gradient log prior swap function ( eq .  [ psfixedcomplexity02 ] ) .",
    "we see in ( c ) that all methods yield the same optimal parameter ( @xmath90=550 ) with the lowest test error .",
    "however , in ( a ) and ( b ) we see that prior swapping takes less time than sgd and sgld .",
    "we have presented procedures to carry out the task of prior swapping : given any false posterior ( computed using some convenient false prior ) and a true prior of interest , our algorithms generate samples from the true posterior .",
    "empirically , we have demonstrated prior swapping on a number of models and priors , and have shown that it can ( 1 ) quickly generate correct samples , ( 2 ) allow for less - costly data - dependent inference algorithms to be applied to more - complex models than previously possible , ( 3 ) allow for quick model selection or hyperparameter tuning , and ( 4 ) be used directly on top of existing inference algorithms to add richer prior information to pre - computed inference results , without having to revisit the data .",
    "theoretically , we have shown that , given a stream of false posterior samples , this strategy can be used to generate asymptotically exact samples from the true posterior .    we furthermore hope that prior swapping can be successfully implemented as a black box method , and paired with existing automatic inference engines or probabilistic programming languages @xcite . since prior swapping does not require tuning and can be applied directly to the output of inference yielded by these frameworks , it has potential to aid in general purpose automated inference with more - realistic and difficult priors .",
    "* appendix for `` prior swapping for data - independent inference '' *",
    "here , we prove the two theorems stated in sec .",
    "[ theoreticalguarantees ] .    throughout this analysis",
    ", we assume that we have @xmath34 samples @xmath91 @xmath92 @xmath93 @xmath92 @xmath94 from the false - posterior @xmath19 , and that @xmath95 denotes the bandwidth of our semiparametric false - posterior density estimator @xmath36 .",
    "let hlder class @xmath96 on @xmath93 be defined as the set of all @xmath97 times differentiable functions @xmath98 whose derivative @xmath99 satisfies @xmath100    we also define the class of densities @xmath101 to be @xmath102 we also assume that the false - posterior density @xmath19 is bounded , i.e. that there exists some @xmath103 such that @xmath104 for all @xmath6",
    ".    given false posterior samples @xmath105 and @xmath63 , the estimator @xmath64 ( eq .  [ eq : semiparest ] ) is consistent , i.e. its mean - squared error satisfies @xmath65      < \\frac{c}{t^{4/(4+d ) } } \\nonumber\\end{aligned}\\ ] ] for some @xmath66 and @xmath67 .",
    "to prove mean - square consistency of our semiparametric prior swap estimator @xmath106 , we give a bound on the mean - squared error ( mse ) , and show that it tends to zero as we increase the number of samples drawn from the false - posterior . to prove this , we bound the bias and variance of the estimator , and use this to bound the mse .",
    "we first bound the bias of our semiparametric prior swap estimator . for any @xmath107 , we can write the bias as @xmath108           - f_{\\widetilde{ps } } \\right|           & =           \\left| \\mathbb{e } \\left [ \\hat{f}_{\\phi|x}^s          \\frac{f_\\theta}{f_\\phi } \\right ] - f_{\\phi|x }           \\frac{f_\\theta}{f_\\phi } \\right|   \\\\          & =           \\left| \\frac{f_\\theta}{f_\\phi }           \\mathbb{e } \\left [ \\hat{f}_{\\phi|x}^s \\right ] - f_{\\phi|x }           \\right|   \\\\          & = \\tilde{c } \\left| \\mathbb{e } \\left [ \\hat{f}_{\\phi|x}^s          \\right ] - f_{\\phi|x } \\right| \\\\          & \\leq ch^2\\end{aligned}\\ ] ] for some @xmath66 , where we have used the fact that @xmath109 - f_{\\phi|x } \\right| \\leq   \\tilde{c } h^2 $ ] for some  @xmath110 ( given in @xcite ) .",
    "we next bound the variance of our semiparametric prior swap estimator . for any @xmath107 , we can write the variance of our estimator as @xmath111      & = \\text{var } \\left [ \\hat{f}_{\\phi|x}^s          \\frac{f_\\theta}{f_\\phi } \\right ] \\\\      & = \\frac{f_\\theta^2}{f_\\phi^2 } \\text{var }      \\left [ \\hat{f}_{\\phi|x}^s \\right ] \\\\      & \\leq \\frac{c}{th^d}\\end{aligned}\\ ] ] for some @xmath66 , where we have used the facts that @xmath112      \\leq \\frac{c}{th^d}$ ] for some @xmath66 and @xmath113 ^ 2       \\leq \\tilde{c}$ ] for some @xmath114 ( given in @xcite ) .",
    "next , we will use these two results to bound the mean - squared error of our semiparametric prior swap estimator , which shows that it is mean - square consistent .",
    "we can write the mean - squared error as the sum of the variance and the bias - squared , and therefore , @xmath115           & \\leq c_1 h^2 + \\frac{c_2}{th^d } \\\\          & = \\frac{c}{t^{4/(4+d)}}\\end{aligned}\\ ] ] for some @xmath66 , using the fact that @xmath63 .    the procedure given in alg .",
    "[ alg : priorswapping ] generates samples from @xmath68 ( eq .  [ eq : semiparest ] ) as @xmath41 .    in alg .",
    "[ alg : priorswapping ] , note that line 2 is equivalent to standard mcmc ( such as metropolis - hastings or hamiltonian monte carlo ) run on a posterior target function proportional to @xmath116 , and thus yields samples from @xmath117 as @xmath41 ( see @xcite for mcmc guarantees ) .    in line 3",
    ", we can sample from @xmath118 exactly via the following procedure : for each sample @xmath119 ,    1 .",
    "draw an index @xmath120 : @xmath121 .",
    "2 .   draw the sample @xmath122 : @xmath123 .",
    "we thus have two streams of asymptotically - exact samples , which is exactly the setting of the product density sample combination procedures @xcite , which run on the output of mcmc algorithms to generate asymptotically - exact samples from the distribution with pdf proportional to the product of the pdfs of the markov chains stationary distributions ( see @xcite , sec .  5 ) .",
    "we give pseudocode for these sample combination procedures in the `` sample combination algorithm pseudocode '' section of the appendix .",
    "therefore , as @xmath41 , line 4 of alg .",
    "[ alg : priorswapping ] yields samples from @xmath124      \\left [      \\frac{1}{t } \\sum_{t=1}^t      \\hat{f}_{\\phi|x}^p(\\widetilde{\\theta}_t)^{-1 }      k \\left(\\frac{\\|\\theta - \\widetilde{\\theta}_t\\|}{h }      \\right )      \\right ] $ ] .",
    "here we summarize the complexity of our prior swapping algorithms . assume that we have run an existing inference algorithm to compute either a fixed - complexity analytic expression for the false - posterior density , or to draw @xmath34 false - posterior samples , given some dataset of @xmath0 observations .",
    "suppose that we wish to compute @xmath125 samples from the true - posterior .",
    "given a fixed - complexity analytic expression for the false - posterior , each step ( i.e. generating each sample ) in an mcmc algorithm requires a constant @xmath30 number of operations , and drawing @xmath125 samples via our procedure therefore requires @xmath126 operations",
    ".    given @xmath34 samples from the false - posterior , the asymptotically - exact ( semiparametric ) procedure in alg .",
    "[ alg : priorswapping ] must perform the fixed - complexity prior swapping procedure on a parametric false posterior @xmath19 ( @xmath126 operations for @xmath125 samples ) , draw @xmath125 samples from the correction density ( @xmath35 operations to compute component weights and @xmath126 operations to draw @xmath125 samples , for a total complexity of @xmath127 ) , then apply a density product sample combination algorithm @xcite ( @xmath126 operations for @xmath125 samples ) .",
    "therefore , the asymptotically - exact ( semiparametric ) procedure has a total complexity of @xmath127 to draw @xmath125 samples .    in general ,",
    "given the false - posterior inference result , none of the prior swapping algorithms depend on the number of data points @xmath0 .",
    "here we give pseudocode for the prior swapping procedure , given a fixed - complexity ( parametric ) false posterior expression @xmath59 , using the prior swap functions @xmath61 and @xmath128 ( eqs .",
    "[ psfixedcomplexity01 ] and [ psfixedcomplexity02 ] ) . in alg .",
    "[ alg : ps_mh ] , we show prior swapping via the metropolis - hastings algorithm , which makes use of @xmath61 . in alg .",
    "[ alg : ps_hmc ] we show prior swapping via hamiltonian monte carlo , which makes use of @xmath128 . a special case of alg .",
    "[ alg : ps_hmc ] , which occurs when we set the number of simulation steps to @xmath129 ( in line 6 ) , is prior swapping via langevin dynamics .",
    "initialize @xmath130 .",
    "@xmath46 initialize markov chain .",
    "+    initialize @xmath130 .",
    "@xmath46 initialize markov chain .",
    "here we give pseudocode for a product density sample combination algorithm @xcite .",
    "we will write this algorithm for our setting ( i.e. for generating samples from the product of _ two _ densities ) though these algorithms are typically more general .",
    "we mainly follow alg .  1 from @xcite .",
    "the intuitive idea behind these algorithms is the following : given two sets of samples @xmath51 @xmath52 @xmath53 and @xmath54 @xmath52 @xmath55 , they aim to return @xmath56 @xmath2 @xmath57 . at each iteration in this algorithm , four steps are carried out :    1 .",
    "choose one of the two input sample sets ( uniformly at random ) .",
    "2 .   re - draw a sample from the chosen sample set .",
    "3 .   accept this drawn sample , or reject it and keep the previously drawn sample .",
    "4 .   save a noisy average of the two current samples ( one from each sample set ) .",
    "we give the pseudocode for this procedure in alg .",
    "[ alg : samplecombination ] .",
    "draw @xmath131 .",
    "@xmath46 initialize sample indices . + set @xmath132",
    "the asymptotically - exact method for prior swapping allows for a related way to more easily incorporate observed prior information .",
    "often , instead of ( or in addition to ) an analytic expression for our prior beliefs , we have prior observations , such as previously observed outcomes of what we aim to infer .",
    "incorporating information such as this can lead to better data - driven priors . here",
    ", we will extend our asymptotically - exact prior swapping procedure to this setting .",
    "suppose that we only have access to samples @xmath133 from the true - prior @xmath71 , in addition to false - posterior samples @xmath134 from @xmath19 . in this case",
    ", we can apply a density product sample combination algorithm @xcite ( described in the `` sample combination algorithm pseudocode '' section of the appendix ) to generate samples @xmath135 from @xmath136 @xmath137 @xmath138 , and then construct a semiparametric estimate @xcite of this density ( the same one used in defining the asympotically - exact prior swapping method ) , written @xmath139 we then define the prior swap function for observed prior samples , @xmath140 , to be @xmath141       \\left [ \\frac{1}{t } \\sum_{t=1}^t       w'_t k \\left(\\frac{\\|\\theta - \\widetilde{\\theta}_t\\|}{h}\\right )       \\right],\\end{aligned}\\ ] ] where we ve defined @xmath142 . as in the asymptotically - exact prior swapping procedure , @xmath140 is proportional to the product of two densities , both of which we can easily sample from ( note that we can sample from @xmath143 using the fixed - complexity parametric prior swapping method ) .",
    "as before , we then apply the density product sample combination methods to generate samples from @xmath144 .",
    "we summarize this full procedure in alg  [ alg : priorswappingobserved ] .",
    "sample @xmath145 via @xmath146",
    ". + estimate @xmath147 using @xmath45 @xmath46 parametric estimate .",
    "+ sample @xmath148 @xmath46 parametric prior swapping .",
    "+ sample @xmath149 .",
    "+ sample @xmath150 via @xmath151 ."
  ],
  "abstract_text": [
    "<S> while bayesian methods are praised for their ability to incorporate useful prior knowledge , in practice , priors that allow for computationally convenient or tractable inference are more commonly used . in this paper , we investigate the following question : _ for a given model , is it possible to use any convenient prior to infer a false posterior , and afterwards , given some true prior of interest , quickly transform this result into the true posterior ? _    we present a procedure to carry out this task : given an inferred false posterior and true prior , our algorithm generates samples from the true posterior . </S>",
    "<S> this transformation procedure , which we call `` prior swapping '' works for arbitrary priors . </S>",
    "<S> notably , its cost is independent of data size . </S>",
    "<S> it therefore allows us , in some cases , to apply significantly less - costly inference procedures to more - sophisticated models than previously possible . </S>",
    "<S> it also lets us quickly perform any additional inferences , such as with updated priors or for many different hyperparameter settings , without touching the data . </S>",
    "<S> we prove that our method can generate asymptotically exact samples , and demonstrate it empirically on a number of models and priors . </S>"
  ]
}