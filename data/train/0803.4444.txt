{
  "article_text": [
    "given a probability distribution @xmath9[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}{\\bf x}-{\\bf b}\\cdot{\\bf x}\\right)\\right]\\ ] ] a generic heatbath algorithm can be described as a stochastic process in which the vector @xmath10 is related to the vector at the previous step @xmath11 by @xmath12 where @xmath13 is a direction in the @xmath3 space and @xmath14[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}{\\bf x}-{\\bf b}\\right)}{{\\bf d}{\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}{\\bf d}}+ \\left(\\beta { \\bf d}{\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}{\\bf",
    "d}\\right)^{-1/2 } r \\label{eq : hb - tau}\\ ] ] where @xmath15 is a gaussian random number with zero mean and unitary spread @xmath8 , and @xmath16 is the inverse temperature at which the sampling is performed .",
    "the application of this algorithm does not require inversion of the matrix @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] .",
    "the sequence of directions @xmath13 is rather arbitrary , and could be a random sequence or a predefined deterministic sequence . strictly speaking , detailed balance is satisfied only if the directions are randomly chosen at each step .",
    "nevertheless it has been shown in ref@xcite that correct sampling can be achieved if every monte carlo move leaves the equilibrium distribution unchanged . in appendix",
    "[ sec : stationary ] we show that this is the case , provided that direction @xmath13 is chosen independently from position @xmath3 .",
    "nevertheless , different choices of directions can lead to different sampling efficiency .",
    "our final choice will be to select for @xmath13 a sequence of conjugate directions ( section  [ sub : conj - dir ] ) .",
    "however , we shall first analyze the choice of random , uncorrelated directions , and a sequential sweep along a set of orthogonal directions .    for the sake of simplicity",
    ", we take @xmath17 and we choose the basis into which @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] is diagonal , @xmath18 .",
    "since these properties are subsequently never used , no loss of generality is implied . to compare the efficiency of the different choices of directions we shall consider the autocorrelation matrix for the components along the eigenmodes @xmath19",
    ". a quantitative measure of the speed of decorrelation of @xmath19 can be obtained from its slope at the origin . since in monte",
    "carlo one progresses in discrete steps , this quantity is given by @xmath20\\label{eq : slopefirst}\\ ] ] in eq .",
    "( [ eq : slopefirst ] ) we have introduced the normalized slope tensor @xmath1[0ex]{\\uuline{\\mbox{$\\boldsymbol\\delta$}}}\\,}$ ] , which can be expressed as a function of the eigenvalues of @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] and of the components of @xmath13 , using equations ( [ eq : step ] ) and ( [ eq : hb - tau ] ) : @xmath21 therefore , depending on the choice of direction @xmath13 , the different components of the vector @xmath3 decorrelate at different speeds . however , since @xmath22[0ex]{\\uuline{\\mbox{$\\boldsymbol\\delta$}}}\\,}=1 $ ] , the sum of these normalized speeds does not depend on the direction chosen .",
    "the same quantity @xmath1[0ex]{\\uuline{\\mbox{$\\boldsymbol\\delta$}}}\\,}$ ] also enters a recursion relation for the autocorrelation functions at a generic monte carlo step @xmath23 , @xmath24\\label{eq : auto - induction}\\end{aligned}\\ ] ] use of this equation requires that one appropriately averages over the direction @xmath13 , as we shall discuss in the following .",
    "we will begin our analysis from the simpler case , in which the direction @xmath13 is chosen at every step to be equal to a stochastic vector @xmath7 , whose components are distributed as gaussian random numbers with zero mean and standard deviation one . the normalized slope tensor ( [ eq : slope - tens ] ) in this case results from an average over the possible directions , @xmath25[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}}. \\label{eq : random - slope}\\ ] ] the limit expression holds for the size @xmath4 of the matrix going to infinity ( see appendix  [ sec : rnd - asymptotic ] ) , under the hypothesis that the largest eigenvalue of @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] does not grow with @xmath4 and that @xmath22[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] is @xmath26 , hypotheses which are relevant to many physical problems . since in this case the direction chosen at every step is independent of all the previous choices , the same average enters equation ( [ eq : auto - induction ] ) at any time , so that proceeding by induction one can easily obtain the entire autocorrelation function , @xmath27^t \\label{eq : random - full}\\ ] ] where @xmath28 is the quantity obtained in equation  ( [ eq : random - slope ] ) . from ( [ eq : random - full ] )",
    "we can calculate the autocorrelation time for mode @xmath29 , @xmath30^{-1 } \\overset{n\\rightarrow\\infty}{\\approx } \\frac{{\\ensuremath{\\operatorname{tr}}}{\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}}{{a}_i}.\\ ] ] in the case of large @xmath4 , the decorrelation speed of the components along normal modes is directly proportional to the corresponding eigenvalue , so that in ill - conditioned cases a critical slowing down for the softer normal modes will be present .",
    "let us now consider moves along a predefined set of orthogonal directions @xmath31 .",
    "this is done to mimic the case in which one performs a sweep along cartesian directions . in our reference frame , where @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] is taken to be diagonal , this would be trivial , hence the choice of an arbitrarily oriented set of orthogonal directions . as in standard local heatbath , the outcome will depend on the orientation of the @xmath32 relative to the eigenvectors of @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] . averaging over all the possible choices of initial direction , we find the slope at @xmath33 , @xmath34 obviously , it is not possible to reduce this result to an expression which does not depend on the particular set of orthogonal directions .",
    "however , the following inequality holds @xmath35 equation  ( [ eq : localuneq ] ) does not put rigid constraints on the value of @xmath28 , but demonstrates that also in this case @xmath1[0ex]{\\uuline{\\mbox{$\\boldsymbol\\delta$}}}\\,}$ ] is diagonal and suggests that in real life the convergence will be faster for the higher eigenvalues , and that the spread in the relaxation speed for different modes is larger when the condition number @xmath36 is higher .    in the case",
    "where directions @xmath32 are swept sequentially we have not been able to derive a closed expression for @xmath19 because of the dependence of @xmath37 on the previous history .",
    "if , on the other hand , a random direction is drawn from @xmath32 at every step , @xmath19 is given by expression ( [ eq : random - full ] ) where @xmath28 has the value in equation  ( [ eq : localdelta ] ) .",
    "it is clear from equation ( [ eq : random - full ] ) that a random choice of the directions @xmath13 leads to fast decorrelation of the components relative to the eigenvectors with high eigenvalues .",
    "on the other hand , the components relative to the eigenvectors with low eigenvalues will decorrelate more slowly .",
    "similar behavior is expected for the local heatbath method , unless particular relations hold between the eigenvectors and the cartesian axes . if the operator @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] is ill - conditioned , the practical consequence is that the slow modes will be accurately sampled only after a very large number of steps .",
    "as we have already discussed , the sum of the decorrelation slopes of the different components does not depend on the choice of the directions @xmath13 .",
    "however , with a proper choice of the directions @xmath13 this sum could be spread in a uniform way among the different modes .",
    "a similar problem arises in minimization algorithms based on directional search , and is often solved choosing a sequence of conjugated directions@xcite . in the same spirit , we can compute the decorrelation speed of the different modes when the @xmath13 s are chosen to be conjugated directions .",
    "let us consider a set of conjugated directions @xmath38 , such that @xmath39[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}{\\bf h}^{(j)}=\\delta_{ij}$ ] .",
    "the set @xmath38 can be generated with various algorithms , such as a gram - schmidt orthogonalization that uses the positive definite @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] matrix as a metric , or a conjugate gradient procedure , as described in section  [ sub : tricks ] .",
    "using the fact that @xmath40 , the slope at @xmath33 is @xmath41[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}{\\bf h}^{(m)}}=\\\\ = \\frac{1}{n}\\sqrt{\\frac{{a}_j}{{a}_i}}\\sum_m \\frac{{a}_i h^{(m)}_i h^{(m)}_j}{{\\bf h}^{(m)}{\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}{\\bf h}^{(m)}}=\\frac{\\delta_{ij}}{n}\\end{aligned}\\ ] ] with this choice , the decorrelation slopes of the different modes are independent of the eigenvalue .",
    "if one chooses one conjugate direction at random at each step it is straightforward to show that overall the autocorrelation function decays exponentially as @xmath42^t\\ ] ]    this derivation shows that if matrix @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] is ill - conditioned and one wishes to decorrelate the slow modes , then the choice of performing the heatbath using a sequence of conjugated directions can improve the sampling quality dramatically .",
    "of course , the slow modes are accelerated and the fast modes are decelerated . however , it is clear that a completely independent vector @xmath3 is obtained only when all the modes are decorrelated .",
    "a heatbath on conjugate directions allows all the modes to be decorrelated with the same efficiency , irrespective of their stiffness .",
    "even better efficiency can be obtained by sequentially sweeping a set of conjugated directions . at first sight",
    "it would appear that the dependence of @xmath43 on @xmath44 would make it very difficult if not impossible to obtain the autocorrelation function in a closed form .",
    "however , conjugate directions have a redeeming feature . if we expand the position vector on the non - orthogonal basis @xmath45 , @xmath46 , and we evaluate the correlation matrix between the contravariant components @xmath47 , we find that @xmath48 .",
    "this property can be easily demonstrated taking into account that the ensemble average @xmath49 , and that conjugacy implies @xmath39[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}{\\bf h}^{(j)}=\\delta_{ij}$ ] .",
    "thus , effectively , every time we perform a heatbath move along direction @xmath50 the component @xmath47 is randomized , without affecting the others . after a complete sweep across the set of directions",
    "a completely independent state is obtained .    a more formal proof is provided in appendix  [ sec : cd - formal ] , where it is also demonstrated that the autocorrelation function is @xmath51   & t < n\\\\ 0                & t\\ge n \\end{array } \\right.\\label{eq : cd - autofun}\\ ] ] therefore the corresponding autocorrelation time is @xmath52 .",
    "a remarkable feature of equation ( [ eq : cd - autofun ] ) is that the autocorrelation function is linear , and that after @xmath4 moves a completely independent vector is obtained .",
    "this property holds also for the global heatbath method . in section  [ sec : comp - global ] we shall discuss the relation between our approach and global heatbath sampling .      in the last section we have shown how a heatbath algorithm based on conjugate directions can dramatically improve the sampling of the slow modes for an ill - conditioned action . an efficient strategy to generate these directions is the application of the conjugate gradient procedure@xcite . for the sake of completeness and to introduce a consistent notation we give here an outline of the cg algorithm .",
    "one starts from a random configuration and search direction , @xmath53 , so that the directions obtained and the sample vector @xmath3 are independent as required .",
    "then , a series of directions @xmath54 and residuals @xmath55 are generated using the recurrence relations @xmath56[0ex]{\\uuline{\\mbox{$\\bf",
    "a$}}}\\,}\\cdot{\\bf h}^{(i)}\\quad { \\bf h}^{(i+1)}={\\bf g}^{(i+1)}+\\gamma_i \\cdot{\\bf h}^{(i)}\\ ] ] @xmath57[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}{\\bf h}^{(i ) } } \\quad   \\gamma_i=\\frac{{\\bf g}^{(i+1)}\\cdot { \\bf g}^{(i+1)}}{{\\bf g}^{(i)}\\cdot { \\bf g}^{(i)}}\\ ] ] this procedure generates at every step a new direction @xmath50 , conjugated to all the previous ones , and it can be used to perform a directional heatbath move on @xmath3 .",
    "it should be stressed that there is no need to store all the @xmath50 if the heatbath moves are performed concurrently with the cg minimization .",
    "the `` force '' @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}{\\bf h}^{(i)}$ ] can be reused for performing the heatbath update ( cfr .",
    "( [ eq : step ] ) ) . at a certain point",
    "the cg procedure will be over , with the residual @xmath58 dropping to zero .",
    "the sequential sweep algorithm described inte the previous section can be implemented starting again from the same @xmath59 .",
    "in contrast to the global heatbath method , numerical stability is not a major issue , since the accuracy of the sampling does not depend on the search directions being exactly conjugated .",
    "the only effect of imperfect conjugation would be to slightly reduce the decorrelation efficiency .",
    "there is however a drawback to this approach . in order to be ergodic ,",
    "the set of directions must span the whole space .",
    "the problem arises when there are degenerate eigenvalues , as cg converges to zero in a number @xmath5 of iterations equal to the number of distinct eigenvalues .",
    "if we keep reusing the same set of @xmath60 directions , only a part of the subspaces corresponding to degenerate eigenvalues will be explored , and the sampling will not be ergodic .",
    "scheme of the block algorithm described in paragraph [ sub : tricks ] ; squares represent eigenvectors of the action matrix , which need to be refreshed in order to obtain a statistically independent sample point ; modes on the same column correspond to the same , degenerate eigenvalue . at every step ,",
    "one of the vectors of a set with the same size as the biggest degenerate subspace is used in a conjugate gradient minimization , while the remaining ones are made orthogonal to the search directions that are generated in the process .",
    "when the first vector approaches zero , one can start back on the second one ( figure  b ) ) , and the process can be continued ( figures  c ) and d ) ) until the refresh is complete . ]",
    "we have considered two possible ways of recovering ergodicity .",
    "the simplest consists in drawing a new random point @xmath61 every time we reset the cg search .",
    "this causes a deviation from the linear behavior of the autocorrelation functions for @xmath62 .",
    "non - degenerate eigenvalues will initially converge with @xmath63 instead of @xmath64 slope , but degenerate ones will converge more slowly , and with exponential trend , as we are sampling random directions within every degenerate subspace .    in order to improve the efficiency",
    ", we mix cg with gram - schmidt orthogonalization of a small set of vectors , ideally of the same size @xmath65 of the largest degeneracy present . as discussed earlier , here gram - schmidt orthogonalization has to be performed using the metric of @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] , which amounts to imposing conjugacy .",
    "the procedure is illustrated in figure  [ fig : hybrid ] .",
    "we start from @xmath65 random vectors , @xmath66 .",
    "we set @xmath67 and begin a cg minimization . at each step",
    "we obtain a search direction @xmath50 , and make each of the other @xmath68 vectors conjugate to @xmath50 with a gram - schmidt procedure .",
    "this does not require any matrix - vector product other than the one necessary for the heatbath step .",
    "after @xmath5 iterations the conjugate gradient will have converged and @xmath58 will be close to zero .",
    "we can start again from the second vector in the pool , which meanwhile has become @xmath69 , and is conjugate to all the directions visited so far .",
    "thus , we set @xmath70 and start again the cg procedure , orthogonalizing the @xmath71 remaining vectors to @xmath50 , and so on and so forth .",
    "after @xmath4 steps the procedure will be converged . at the successive sweep",
    ", one can generate again a set of random initial @xmath72 .",
    "this can make the method more stable , at the cost of some loss in performance .",
    "some savings can be made if one stores the conjugated @xmath73 , and uses them in the subsequent sweeps , avoiding the need to repeat the gs orthogonalizations ( see figure  [ fig : hybrid ] ) . in practice , where more than one complete sweep is affordable , it is easy to devise adaptive variations of this scheme , in which the pool of vectors @xmath72 is enlarged whenever the cg minimization converges in less than @xmath4 steps , so that in a few sweeps the optimal size to guarantee ergodicity is attained .",
    "in the previous section we have discussed a collective modes heatbath method that could outperform standard local heatbath techniques when the hamiltonian has a very large condition number and sampling along the slower eigenmodes is required . in this section",
    "we illustrate the efficiency of our algorithm using numerical experiments on a simple model for @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] , @xmath74[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}={\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf 1$}}}\\,}+\\left(\\begin{array}{cccccc } -2b      & b & 0 & \\cdots & 0 & b \\\\ b    & -2b     & b & 0 & \\cdots & 0 \\\\ 0    & b      & -2b     & b & \\ddots      & \\vdots \\\\ \\vdots   & 0 & b & -2b     & \\ddots & 0 \\\\ 0    & \\vdots & \\ddots&\\ddots & \\ddots & b \\\\ b    & 0 & \\cdots & 0 & b & -2b      \\end{array}\\right ) \\label{eq : mat - phonon}\\ ] ] this matrix corresponds to the dynamical matrix of a linear chain of spring - connected masses , with periodic boundary conditions and an additional diagonal term to make the acoustic mode nonzero .",
    "@xmath75 can be chosen so as to obtain the desired condition number .",
    "eigenmodes and eigenvalues for such a matrix are easily obtained , @xmath76 @xmath77 and projection of a state on the eigenvectors is quickly done via fast - fourier transform . in figure",
    "[ fig : curve ] we compare the the autocorrelation functions obtained with different algorithms for a matrix of the form ( [ eq : mat - phonon ] ) . figure  [ fig : curve ] also highlights the ergodicity problems connected with the naive use of the conjugate gradient algorithm to generate the search directions , and shows how both the suggestions of paragraph  [ sub : tricks ] can help in solving this problem . in general , a conjugate directions search speeds up decorrelation for the slower modes , but is less efficient than local heatbath for the modes with a high eigenvalue .",
    "this is a direct consequence of the fact that @xmath22[0ex]{\\uuline{\\mbox{$\\boldsymbol{\\delta}$}}}\\,}=1 $ ] .",
    "an additional advantage of our method is the linear rate of decorrelation , which allows complete decorrelation just like the direct inversion of @xmath1[0ex]{\\uuline{\\mbox{$\\bf m$}}}\\,}$ ] , whereas moves along the cartesian axes lead to approximatively exponential autocorrelation functions .",
    "autocorrelation functions for a ) the projection along the mode @xmath78 ; b ) the projection along the mode @xmath79 for a matrix of the form ( [ eq : mat - phonon ] ) with @xmath80 and condition number @xmath81 .",
    "line * a * corresponds to local heatbath moves ( one step stands for a complete sweep of the @xmath4 coordinates ) , lines * b * to * d * to conjugate directions moves : * b * is the hybrid conjugate gradient / gram - schmidt block algorithm ; * c * corresponds to cg sweeps , with the search direction randomized at the beginning of every sweep ; curve * d * corresponds to cg sweeps starting from the same initial vector .",
    "conjugate direction moves decorrelate faster than local heatbath for the slow mode , but are less efficient for modes with higher eigenvalue . for degenerate eigenmodes , the method used for curve *",
    "d * is not ergodic ( and thus gives incorrect values for @xmath82 ) , and random restarts ( curve * c * ) are much less efficient than the hybrid ( curve * b * ) algorithm .",
    ", title=\"fig : \" ] +   + autocorrelation functions for a ) the projection along the mode @xmath78 ; b ) the projection along the mode @xmath79 for a matrix of the form ( [ eq : mat - phonon ] ) with @xmath80 and condition number @xmath81 .",
    "line * a * corresponds to local heatbath moves ( one step stands for a complete sweep of the @xmath4 coordinates ) , lines * b * to * d * to conjugate directions moves : * b * is the hybrid conjugate gradient / gram - schmidt block algorithm ; * c * corresponds to cg sweeps , with the search direction randomized at the beginning of every sweep ; curve * d * corresponds to cg sweeps starting from the same initial vector .",
    "conjugate direction moves decorrelate faster than local heatbath for the slow mode , but are less efficient for modes with higher eigenvalue . for degenerate eigenmodes , the method used for curve *",
    "d * is not ergodic ( and thus gives incorrect values for @xmath82 ) , and random restarts ( curve * c * ) are much less efficient than the hybrid ( curve * b * ) algorithm .",
    ", title=\"fig : \" ]    we stress again that the relative efficiency of the two methods depends strongly on the observable being calculated and on the actual spectrum of the hamiltonian of the system . as a more realistic benchmark we will consider the evaluation of the trace of the inverse matrix , i.e. @xmath83[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}^{-1}\\right)=\\left<{\\bf x}^2\\right >",
    "\\label{eq : omega}\\ ] ] this observable is strongly dependent on the slow modes .",
    "( color online ) comparison of the efficiency of local heatbath versus conjugate - gradient moves .",
    "the graph represents @xmath84 , the ratio of the autocorrelation times for the observable @xmath85 ( [ eq : omega ] ) ; @xmath86 corresponds to the value obtained from standard local heatbath moves ( one unit of monte carlo time corresponds to a whole coordinates sweep ) , while @xmath87 corresponds to the value obtained with moves along conjugate directions , as obtained from our block algorithm with random restarts .",
    "the data plotted results from a linear interpolation of some simulations ( labeled by @xmath88 ) performed for an action of the form ( [ eq : mat - phonon ] ) , with varying size @xmath4 and condition number @xmath89 . ]    in figure  [ fig : benchmark ] we plot the ratios of the autocorrelation times @xmath90 $ ] as obtained with local heatbath moves and with the block conjugate gradient version of our algorithm , as a function of changing condition number and system size .",
    "it remains for us to discuss how our method fares in comparison with global heatbath .",
    "the latter requires that matrix @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] be decomposable in the form @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}={\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf m$}}}\\,}^t{\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf m$}}}\\,}$ ] .",
    "this is the case in many fields@xcite , but in principle if it were necessary to decompose @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] this would add extra cost . here",
    "we make our comparison assuming that @xmath1[0ex]{\\uuline{\\mbox{$\\bf m$}}}\\,}$ ] is already available .",
    "in such a case , the two algorithms are on paper equally efficient in producing statistically independent samples . the global heatbath might offer some numerical advantages when the spectrum of @xmath1[0ex]{\\uuline{\\mbox{$\\bf m$}}}\\,}$ ] is highly degenerate , since the number of cg iterations needed to solve the @xmath1[0ex]{\\uuline{\\mbox{$\\bf m$}}}\\,}x={\\bf r}$ ] linear system is @xmath60 , as discussed earlier .",
    "whenever a good preconditioner for the linear system is available , other inversion algorithms such as the stabilized bi - conjugate gradient@xcite or the generalized conjugate residual may allow to solve the linear system with a sufficient accuracy more efficiently than using cg . in this paper",
    "we make the comparison with conjugate gradient because of the close analogy with our scheme and because our method is aimed at problems where ill - conditioning can not be otherwise relieved .    in this respect ,",
    "our method displays significant advantages .",
    "firstly , it is more stable , because every move preserves the probability distribution , and the conjugate gradient procedure ( which is known to be quite delicate in problems with large condition number ) is only used to generate search directions .",
    "instabilities in the procedure , which would cause incorrect sampling in the global heatbath , affect only the efficiency , and not the accuracy .",
    "moreover , dividing the @xmath4 steps of an iterative inversion process into separate heatbath moves greatly improves the flexibility of the sampling scheme . to give some examples , if one needs to perform an average on a slowly varying @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] , it is possible to perform only a partial sweep with fixed action , then continue with the new @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] , assuming that eigenmodes will change slowly .",
    "it is also straightforward to tailor the choice of directions in order to optimize the convergence speed for the observable or interest .",
    "adler s overrelaxation@xcite can be included naturally , and can help in further optimizing the autocorrelation time . as an example of possible fine - tunings ,",
    "let us recall the observable @xmath85 introduced in the previous section ( equation ( [ eq : omega ] ) ) .",
    "this observable depends strongly on the softer eigenvector of @xmath1[0ex]{\\uuline{\\mbox{$\\bf{a}$}}}\\,}$ ] .",
    "we have then modified our algorithm in the following way : we perform block conjugate gradient sweeps , with random resets , and we monitor the curvature along the direction being thermalized , @xmath91[0ex]{\\uuline{\\mbox{$\\bf",
    "a$}}}\\,}{\\bf h}/{\\bf h}\\cdot{\\bf h}$ ] .",
    "we save the direction of minimum curvature encountered along the sweep , @xmath92 ; during the following sweep , every @xmath93 moves along the cg directions , one move is performed along @xmath92 . as is evident from figure  [ fig : omegatails ] , this trick considerably reduces the autocorrelation time for @xmath85 .",
    "even smarter combinations of moves can be devised , and the one we suggest is just an example of how the additional flexibility gained through subdividing the inversion process in @xmath4 exact sampling moves can be exploited . in table  [ tab : refvalues ] we report some numerical extimates of the error in the evaluation or @xmath85 , which can serve as a reference to compare our method to other approaches .",
    ".[tab : refvalues ] percentual errors in the evaluation of @xmath94 ( equation ( [ eq : omega ] ) ) , extimated using a blocking analysis , for different sampling methods . *",
    "a * corresponds to local heatbath , * b * corresponds to `` hybrid '' versions of our cg algorithm , with a pool of two vectors with random restarts , while curve * c * is obtained including the tricks described in section  [ sec : comp - global ] with @xmath95 .",
    "different tests are performed with varying matrix size @xmath4 , number of sampling steps @xmath96 and condition number @xmath89 . due to the large autocorrelation time",
    ", the values of the error for local heatbath with @xmath80 and @xmath97 could not be extimated as reliably as in the other cases , and are only indicative . [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     autocorrelation function for the observable ( [ eq : omega ] ) for an action of the form ( [ eq : mat - phonon ] ) , with size @xmath80 and condition number @xmath98 .",
    "line * a * corresponds to local heatbath , line * b * to the `` hybrid '' versions of our cg algorithm , with a pool of two vectors with random restarts , while curve * c * is obtained including the tricks described in section  [ sec : comp - global ] with @xmath99",
    "we have presented an algorithm for performing collective modes heatbath along conjugate directions for a quadratic action , which allows the components of the sampling vector along all modes to be decorrelated in @xmath4 steps , with a linear decay to zero .",
    "this method is more computationally demanding than local updates , but becomes competitive for ill - conditioned actions , when one needs to compute observables which depend on modes with low eigenvalues , or when the spectrum of the action matrix has only a few high eigenvalue modes which would slow down cartesian moves .",
    "in fact , this method has an efficiency comparable with that of direct inversion of the matrix , but presents various advantages , such as improved stability , as the numerical issues connected with conjugate gradient method do not affect the accuracy of the sampling , and the possibility of exploiting some additional flexibility to improve the sampling on a case - by - case basis .",
    "lastly , global heatbath requires the knowledge of the square root of the action @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] , so our scheme should be considered whenever the square root is difficult to compute or its use is inefficient with respect to the original action .",
    "the geometrical simplicity of this approach , with its close analogy with minimization methods , also suggests that it might be extended to the sampling of anharmonic systems .",
    "we report here a simple demonstration of the fact that heatbath moves along a generic direction @xmath13 leave an equilibrium probability distribution unchanged .",
    "we will use the fact that if @xmath7 , @xmath100 and @xmath101 are vectors distributed as gaussians with zero mean and standard deviation one , then @xmath1[0ex]{\\uuline{\\mbox{$\\bf b$}}}\\,}{\\bf r}+{\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf c$}}}\\,}{\\bf r}'$ ] is distributed as @xmath1[0ex]{\\uuline{\\mbox{$\\bf d$}}}\\,}{\\bf r}''$ ] where @xmath1[0ex]{\\uuline{\\mbox{$\\bf d$}}}\\,}^t{\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf d$}}}\\,}={\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf b$}}}\\,}^t{\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf b$}}}\\,}+{\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf c$}}}\\,}^t{\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf c$}}}\\,}$ ] . since",
    "@xmath3 is drawn from the equilibrium distribution , i.e. @xmath102[0ex]{\\uuline{\\mbox{$\\bf m$}}}\\,}^{-1}{\\bf r}$ ] , we can cast eq .",
    "( [ eq : hb - tau ] ) and ( [ eq : step ] ) into the form @xmath103[0ex]{\\uuline{\\mbox{$\\bf m$}}}\\,}^{-1}\\right)_{jm}-d_j \\sum_k m_{km}d_k \\quad\\quad q_{jm}=d_j \\delta_{m0}\\nonumber\\end{aligned}\\ ] ] where we have put @xmath17 into eq .",
    "( [ eq : hb - tau ] ) and normalized the direction so that @xmath104[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}{\\bf d}=1 $ ] in order to simplify the notation .",
    "we can then compute @xmath105[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}^{-1}_{jl}-d_j d_l \\quad\\quad \\sum_m q_{jm}q_{lm}=d_j d_l\\ ] ] so that @xmath1[0ex]{\\uuline{\\mbox{$\\bf p$}}}\\,}^t{\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf p$}}}\\,}+{\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf q$}}}\\,}^t{\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf q$}}}\\,}= \\left({\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf m$}}}\\,}^{-1}\\right)^t{\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf m$}}}\\,}^{-1}$ ] , i.e. also @xmath106 may be written as @xmath1[0ex]{\\uuline{\\mbox{$\\bf m$}}}\\,}^{-1}{\\bf r}$ ] , and is therefore correctly distributed .",
    "we shall here discuss briefly the derivation of the asymptotic form of equation ( [ eq : random - slope ] ) when the size @xmath4 of the action matrix tends to infinity .",
    "the quantity to be computed is @xmath107\\ ] ] the integral can be transformed as follows : @xmath108=\\\\ = \\int_0^{\\infty}\\mathrm{d}t \\frac{1}{{a}_i t+1}\\prod_k \\frac{1}{\\sqrt{{a}_k t + 1}},\\end{aligned}\\ ] ] and the resulting expression , including the correct normalization , is @xmath109 let us focus on @xmath110 , since all the @xmath111 can be computed as @xmath112 .",
    "we perform the change of variables @xmath113 , so that @xmath114 under the physically reasonable assumption that @xmath22[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\ , } =   \\mathcal{o}\\left(n\\right)$ ] , and that the maximum eigenvalue does not scale with the system size , we can use @xmath115 as a small parameter . expanding @xmath116 one finds @xmath117^n = \\sum_k\\frac{{a}_k}{n}\\frac{t}{2 } + \\sum_{n=1 } t^{n+1 } \\mathcal{o}\\left(\\frac{1}{n^n}\\right).\\end{aligned}\\ ] ] all but",
    "the leading term become negligible for @xmath118 .",
    "this suggests separating out from @xmath119 the term order zero in @xmath115 , and writing for @xmath120 the expression @xmath121[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}}{n}\\right)\\times \\nonumber\\\\ \\times\\left[1+\\frac{1}{4}\\sum_k\\left(\\frac{{a}_k}{n}\\right)^2 t^2 +   \\mathcal{o}\\left(\\frac{1}{n^2}\\right)t^3+\\ldots \\right ] { \\rm d } t\\end{aligned}\\ ] ] which leads to the asymptotic result @xmath122[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}}+\\mathcal{o}\\left(n^{-2}\\right)$ ] .",
    "correspondingly , dropping the higher order terms in @xmath115 , we have @xmath123[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}}+\\mathcal{o}\\left(n^{-2}\\right)$ ] , which is the desired result .",
    "we obtain here the autocorrelation function for the components along the eigenmodes of the action matrix @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] , when performing heatbath sweeps along a set of conjugate directions @xmath124 . in this section",
    ", the indices of the directions are defined modulo @xmath4 , i.e. @xmath125 . in this case",
    ", one can write eq .",
    "( [ eq : auto - induction ] ) as @xmath126 .",
    "\\label{eq : cg - induction}\\end{aligned}\\ ] ] explicit calculations for small values of @xmath23 suggest for @xmath127 the ansatz @xmath128.\\label{eq : cg - ansatz}\\ ] ] since the first term in eq .",
    "( [ eq : cg - induction ] ) does not contain the new direction , we can substitute the ansatz without concern . on the other hand ,",
    "the second term contains reference to @xmath54 , so that the average that led to ( [ eq : cg - ansatz ] ) can not be performed separately , and one should rather write : @xmath129.\\end{aligned}\\ ] ] which is split into @xmath130 , \\label{eq : cg - one}\\\\ \\frac{1}{n}\\sum_{mkk'}\\left[\\left <",
    "x_i\\left(0\\right ) x_{k'}\\left(t-1\\right)\\right > \\sqrt{\\frac{{a}_{k'}}{{a}_i } } \\delta_{k'k}\\left({\\bf h}^{(m-1)}\\right ) \\delta_{ki}\\left({\\bf h}^{(m)}\\right)\\right]\\label{eq : cg - two}\\end{aligned}\\ ] ] the term ( [ eq : cg - two ] ) goes to zero , since @xmath131 while ( [ eq : cg - one ] ) can be expanded again , giving rise to the @xmath132 analogue and to a term containing @xmath133 .",
    "one iterates this process recursively until it reaches @xmath134 , thus contributing another @xmath64 to the autocorrelation function .",
    "things are different for @xmath135 , since terms involving products of the slopes for the same direction will enter the procedure at a certain point in the iteration .",
    "because of these terms , for @xmath135 autocorrelation functions will be identically zero .",
    "it is a pleasure to acknowledge useful discussion with fulvio ricci and nazario tantalo , whose suggestions have helped improving the manuscript ."
  ],
  "abstract_text": [
    "<S> we present a method for performing sampling from a boltzmann distribution of an ill - conditioned quadratic action . </S>",
    "<S> this method is based on heatbath thermalization along a set of conjugate directions , generated via a conjugate - gradient procedure . </S>",
    "<S> the resulting scheme outperforms local updates for matrices with very high condition number , since it avoids the slowing down of modes with lower eigenvalue , and has some advantages over the global heatbath approach , compared to which it is more stable and allows for more freedom in devising case - specific optimizations .    </S>",
    "<S> a common problem in many branches of statistical physics is the sampling of distributions of the type @xmath0[0ex]{\\uuline{\\mbox{$a$}}}\\ , } x}\\right)$ ] where @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] is a positive definite @xmath2 matrix and the random variable @xmath3 an @xmath4-dimensional vector . </S>",
    "<S> areas in which such sampling is needed are for instance qcd@xcite and a recently developed linear scaling electronic structure method@xcite . in principle sampling @xmath5 </S>",
    "<S> is straightforward , if diagonalizing @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] is an option . </S>",
    "<S> however , in many cases , @xmath4 is so large that circumventing the @xmath6 diagonalization step becomes mandatory . </S>",
    "<S> different approaches have been proposed . in the so - called global heatbath method one </S>",
    "<S> writes @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}={\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf m$}}}\\,}^t{\\,\\raisebox{0ex}[0ex][0ex]{\\uuline{\\mbox{$\\bf m$}}}\\,}$ ] , and obtains a series of statistically independent vectors by solving the linear system @xmath1[0ex]{\\uuline{\\mbox{$\\bf m$}}}\\,}{\\bf x}={\\bf r}$ ] , where @xmath7 is a vector whose components are distributed according to a gaussian with zero mean and unit variance @xmath8 . </S>",
    "<S> the advantage of this method is that the algorithmic complexity of the problem can be reduced by using an iterative solver for the linear system . in order to expedite sampling a metropolis - like criterion </S>",
    "<S> has been suggested that leads to correct sampling without having to bring the iterative process to full convergence@xcite . </S>",
    "<S> unfortunately , when the ratio between the largest and smallest eigenvalues is large ( ill - conditioned matrices ) the acceptance of this scheme drops to zero unless full convergency is achieved . </S>",
    "<S> an alternative approach is the local heatbath algorithm , in which at every step one single component of the state vector @xmath3 is thermalized in turn , keeping the others fixed . </S>",
    "<S> it has been pointed out elsewhere@xcite that there is a close analogy between this second method and the gauss - seidel minimization technique . </S>",
    "<S> this approach is relatively inexpensive , but becomes very inefficient when the condition number of @xmath1[0ex]{\\uuline{\\mbox{$\\bf a$}}}\\,}$ ] is large , and even more inefficient when the observable of interest depends strongly on the eigenvectors corresponding to smaller eigenvalues .    in this paper </S>",
    "<S> we propose a heatbath algorithm in which moves are performed along mutually conjugated directions . </S>",
    "<S> this choice is based on the analogy between various heatbath methods ( see e.g. ref .  </S>",
    "<S> @xcite ) and directional minimization techniques . </S>",
    "<S> we show both analytically and numerically that the choice of conjugate directions allows all the degrees of freedom to become decorrelated on the same time scale , independent of their associated eigenvalue . </S>",
    "<S> we also discuss the cases in which the improved efficiency outbalances the additional computational cost . </S>",
    "<S> our method can be interpreted as the subdivision of the global heatbath matrix inversion process into @xmath4 intermediate steps , all of which guarantee an exact sampling of the probability distribution .    in section  [ sec : cartaepenna ] we introduce a simple formalism to treat heatbath moves along general directions , discuss the properties of a sweep through a set of conjugate directions , and describe a couple of algorithms to obtain such a set with reasonable effort . in section  [ sec : numerico ] we present some numerical tests on a model action and compare the efficiency of conjugate directions heatbath with local moves for a model observable . in section  [ sec : comp - global ] we compare our method with global heatbath , and in section  [ sec : conclusions ] we present our conclusions . </S>"
  ]
}