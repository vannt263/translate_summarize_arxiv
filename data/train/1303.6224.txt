{
  "article_text": [
    "we study in this paper the distributed solution of a problem of relative localization in a network of sensors .",
    "we assume to have a group of agents organized in a graph and a vector , indexed over the agents and unknown to them : the agents are allowed to take relative noisy measurements of their vector entries with respect to their neighbors in the graph .",
    "the estimation problem consists in reconstructing the original vector , up to an additive constant . we refer to this problem as the problem of _ relative localization_.      in our previous work  @xcite",
    ", we studied the performance of a distributed algorithm , obtained as a gradient descent solution after of a least - squares formulation of the localization problem .",
    "the mean square estimation error of this algorithm has a minimum at a finite time , after which the performance worsens . this non - monotonic behavior ,",
    "although very interesting from a theoretical point of view , may be seen as a potential drawback of the algorithm .",
    "for this reason , in the present paper we build on the insights gained from our previous work to present an algorithm with monotonic mean square error performance .    as the main contribution of this work",
    ", we define an @xmath1-convergence time for the algorithm and we find an upper bound on it , which has the remarkable feature of being independent of the network and even of number of sensors .",
    "notably , also the minimum time of the algorithm in  @xcite has an upper bound which is independent of the graph .",
    "both these observations suggest that cooperation provides limited benefit in reconstructing estimates from measurements which are affected by noise .",
    "indeed , a bounded optimal time means that there is no advantage for a node in obtaining data from outside a certain neighborhood .",
    "intuitively , communication with sensors which are far away in the network does not contribute enough significant information : then , the noise which corrupts the data makes it useless ( in the algorithm below ) or even misleading ( in the more nave algorithm in  @xcite ) .      the problem of relative localization has been brought to our attention in the formulation of  @xcite , which is slightly different from ours , as these authors assume to have an anchor node , in order to avoid the uncertainty about the additive constant .",
    "the natural applications of this estimation problem include spacial localization and clock synchronization  @xcite .",
    "distributed algorithms have been proposed in several papers , including  @xcite , and contemporary work is focusing on randomized algorithms  @xcite .",
    "in section  [ sect : relative - localization ] we define the problem of relative localization , and our novel algorithm for its solution is derived in section  [ sect : algo - def ] .",
    "then in section  [ sect : analysis ] we analytically study the convergence and the mean square error of the algorithm , while simulations are described in section  [ sect : simulations ] .",
    "we conclude with a short section which summarizes our contribution and points to future research .",
    "vectors are denoted with boldface letters , and matrices with capital letters . by the symbols @xmath2 and @xmath3 we denote vectors having all entries equal to @xmath4 and @xmath5 , respectively . given a matrix @xmath6 , we denote by @xmath7 its trace , by @xmath8 its transpose and by @xmath9 its moore - penrose pseudo - inverse .",
    "we consider a set of @xmath10 agents , and we endow each of them with a scalar quantity @xmath11 , for @xmath12 . the @xmath13th agent does not know the value @xmath14 , but has an estimate @xmath15 .",
    "we shall denote by @xmath16 and @xmath17 the @xmath10-dimensional vectors whose components are @xmath14 and @xmath18 , respectively .",
    "we suppose that each agent @xmath13 can take relative measurements @xmath19 with respect to some neighbors @xmath20 .",
    "an undirected graph @xmath21 is used to represent the available measurements .",
    "the set of vertices is constituted by the @xmath10 agents , and the edges ( pairs of agents ) in @xmath22 correspond to the available measurements .",
    "we assume that there are @xmath6 available measurements , and that measurements are symmetrical , meaning that both agents of a pair know the measurement , with a reversed sign .",
    "furthermore , we assume that the graph @xmath23 is connected . on each edge ,",
    "we choose an orientation , that is , we define a starting node and an ending node , in order to encode the measurements by using the incidence matrix @xmath24 defined as follows @xmath25 measurements are affected by errors , which can be modeled by independent and identically distributed noises .",
    "let @xmath26 be the vector of the measurements and @xmath27 that of noises .",
    "then , in matrix notation we have @xmath28 with @xmath29 = { \\mathbf{0}}$ ] and @xmath30= \\sigma^2 i$ ] where @xmath31 is the identity matrix .",
    "it is also useful to define the laplacian of @xmath23 as @xmath32 .",
    "the laplacian @xmath33 is a symmetric matrix , and being @xmath23 connected , @xmath33 has eigenvalues @xmath34 and @xmath35 for @xmath36 , with @xmath37 denoting the maximum degree of the nodes .",
    "in view of the statistical assumptions on the noise affecting the measurements , a natural approach to the relative localization problem involves solving the least - squares problem @xmath38 this approach has already been taken in the literature , and leads to design the distributed algorithm studied in  @xcite . in this paper",
    ", we additionally assume that each node @xmath13 has an _ a priori _ information on @xmath39 , which is known to be a random vector independent from @xmath40 and such that @xmath41=   { \\mathbf{x}}_0 $ ] and @xmath42= \\nu^2 i$ ] . in order to exploit this statistical information",
    ", we choose to minimize the functional @xmath43 which includes both the information obtained by the measurements and the _ a priori _ knowledge about @xmath39 , weighted according to their significance , _",
    "i.e. _ , the inverse of their variances . compared to @xmath44 , the extra term in this functional can be seen as a tikhonov regularization term , which turns the estimation problem at hand into a problem of maximum a posteriori probability ( map ) estimation .",
    "we refer the reader to  ( * ? ? ? * and  7.1.2 ) for a broad introduction to these concepts .",
    "as @xmath45 is convex , it is natural to consider gradient descent algorithms for its minimization .",
    "provided we define @xmath46 , the gradient of the objective function is @xmath47 , so that a gradient descent iterate can be defined as @xmath48 & = { \\mathbf{x}}[t ] - \\tau \\frac{2}{\\sigma^2}\\nabla \\phi \\left({\\mathbf{x}}[t]\\right ) \\\\               & = { \\mathbf{x}}[t ] - \\tau \\left [ \\left(a^\\top a { \\mathbf{x}}[t ] - a^\\top { \\mathbf{b}}\\right )                       + \\gamma \\left ( { \\mathbf{x}}[t ] - { \\mathbf{x}}_0 \\right ) \\right ] \\\\               & = ( i - \\tau l - \\tau \\gamma i ) { \\mathbf{x}}[t ] + \\tau a^\\top b + \\tau \\gamma { \\mathbf{x}}_0 \\end{aligned}\\ ] ] for a suitable @xmath49 .",
    "equivalently , we may write the algorithm as @xmath50 = q { \\mathbf{x}}[t ] + { \\mathbf{w}}\\\\          { \\mathbf{x}}[0 ] = { \\mathbf{x}}_0          \\end{array } \\right.\\ ] ] where @xmath51 and @xmath52    remarkably , this algorithm is distributed , in the following sense . the matrix @xmath53 is adapted to the graph @xmath23 , _",
    "i.e , _ , @xmath54 if @xmath55 : then , in order to update a component as @xmath56 = \\sum_j q_{ij } x_j[t ] + w_i $ ] , the algorithm requires communication and measurements only with the nodes which are neighbors of @xmath13 in the graph .",
    "in the analysis of algorithm   and from here on in this paper , we shall make the following assumption , which is sufficient to our results .    [ ass : on - tau ] the graph @xmath23 is connected and @xmath57      we begin our analysis by studying the convergence properties of the proposed algorithm .    [",
    "prop : convergence ] if assumption  [ ass : on - tau ] is satisfied , then the algorithm   converges at exponential rate to @xmath58 which is the optimal solution to the problem @xmath59    first , we show that @xmath60 is the optimal solution of the optimization problem . to this goal",
    ", we equate to the zero of the gradient @xmath61 and solve the normal equation @xmath62 since @xmath63 , the matrix @xmath64 is invertible , and hence the optimal solution is unique and equal to @xmath65 .",
    "second , we show that the algorithm converges to @xmath60 . by solving the recursion we have @xmath66 & = q^t { \\mathbf{x}}_0 + \\sum_{n=0}^{t-1 } q^n { \\mathbf{w}}\\end{aligned}\\ ] ] since @xmath67 , also @xmath53 is diagonalizable with real eigenvalues @xmath68 . using assumption  [ ass : on - tau ]",
    ", w we have @xmath69 therefore , given the assumptions on @xmath70 , all the eigenvalues of @xmath53 belong to the interval @xmath71 $ ] ( note that @xmath72 ) , the algorithm is exponentially convergent , and @xmath73 then , we can compute @xmath74 & = q^t { \\mathbf{x}}_0 + ( i - q)^{-1 } ( i - q ) \\sum_{n=0}^{t-1 } q^n { \\mathbf{w}}\\\\       & = q^t { \\mathbf{x}}_0 + ( i - q)^{-1 } ( i - q^t ) { \\mathbf{w}}\\end{aligned}\\ ] ] and consequently @xmath75 & = ( i - q ) { \\mathbf{w}}\\\\          & = ( l + \\gamma i)^{-1 } ( a^\\top { \\mathbf{b}}+\\gamma{\\mathbf{x}}_0 ) = { \\mathbf{x}}^*.      \\end{aligned}\\ ] ]    the algorithm preserves the barycenter ( or average ) of the state , namely @xmath76 = \\frac{1}{n } { \\mathbf{1}}^\\top { \\mathbf{x}}_0 $ ] . remarkably , this property holds even if @xmath53 is not stochastic . notice indeed that @xmath77 and that @xmath78 = ( 1-\\tau\\gamma ) { \\mathbf{1}}^\\top { \\mathbf{x}}[t ] + \\tau \\gamma { \\mathbf{1}}^\\top { \\mathbf{x}}_0 $ ] . since @xmath79 = { \\mathbf{x}}_0 $ ] , by induction the barycenter is preserved .",
    "this property is also consistent with the intuition that the optimal solution must satisfy @xmath80 .      to evaluate the algorithm performance , we follow the approach in  @xcite and define the performance metric as the mean square error between the current estimate @xmath81 $ ] and the true configuration @xmath16 , that is , @xmath82 - \\bar{{\\mathbf{x } } } \\|_2 ^ 2,\\ ] ] where the expectation is taken on both the noise @xmath40 and the initial condition @xmath83 .",
    "this performance metric can be computed in terms of the eigenvalues of the matrix @xmath53 .",
    "[ prop : time - performance ] if assumption  [ ass : on - tau ] is satisfied , then the following equality holds @xmath84 where @xmath85 s are the eigenvalues of @xmath53 .",
    "we express @xmath86 in terms of @xmath40 and @xmath87 as @xmath88 now , we compute @xmath81 - \\bar{{\\mathbf{x}}}$ ] , given @xmath86 and as @xmath89 - \\bar{{\\mathbf{x } } } =   q^t ( { \\mathbf{x}}_0 - \\bar{{\\mathbf{x } } } ) & +                     \\tau \\gamma \\sum_{n=0}^{t-1 } q^n ( { \\mathbf{x}}_0 - \\bar{{\\mathbf{x } } } ) \\\\                    & +     \\tau \\sum_{n=0}^{t-1 } q^n a^\\top { \\mathbf{n}}.       \\end{aligned}\\ ] ] from the definition of @xmath90 we have @xmath91 - \\bar{{\\mathbf{x}}})({\\mathbf{x}}[t ] - \\bar{{\\mathbf{x}}})^\\top \\right ] \\right ]      \\end{aligned}\\ ] ] by using the above formula for @xmath81 - \\bar{{\\mathbf{x}}}$ ] , we get @xmath92 ,      \\end{aligned}\\ ] ] through some algebraic manipulations which we omit involving the properties of the trace operator , the linearity of expectation and the symmetry of @xmath53 .",
    "now , given that @xmath46 , we obtain @xmath93 \\\\               & = \\frac{1}{n } { \\mathop{\\mathrm{tr}}}\\left [ \\nu^2 q^{2 t } + \\tau \\sigma^2 ( i + q^t ) ( i - q^t ) ( i - q)^{-1 } \\right ] \\\\               & = \\frac{1}{n } { \\mathop{\\mathrm{tr}}}\\left [ \\nu^2 q^{2 t } + \\tau \\sigma^2 ( i - q^{2t})(i - q)^{-1 } \\right ] .",
    "\\end{aligned}\\ ] ] notice that the matrix @xmath94 is invertible since it is proportional to @xmath95 .",
    "the result follows immediately as @xmath85s are the eigenvalues of @xmath53 .",
    "the key property of monotonicity of @xmath90 is stated in the next result .",
    "if assumption  [ ass : on - tau ] is satisfied , then @xmath90 is strictly decreasing and @xmath96    let us recall the definition of @xmath97 and define a new constant @xmath98 , according to what done in  @xcite : @xmath99 note that , given assumption  [ ass : on - tau ] , @xmath100 . keeping this inequality in mind we can rewrite @xmath90 as @xmath101 .",
    "\\end{aligned}\\ ] ] we will show that @xmath90 is decreasing in @xmath102 , since the @xmath103 term in the sum is either a constant or decreasing sequence . let us compute the finite increment @xmath104 \\\\                  & = \\frac{\\tau \\sigma^2}{n } \\sum_{i=0}^{n-1 } \\left [ \\xi_i^{2 t } h(\\xi_i ) \\right ] ,      \\end{aligned}\\ ] ] with @xmath105 .",
    "note that @xmath106 for all @xmath107 whereas @xmath108 when @xmath109 . since @xmath110 when @xmath111 , the corresponding contribution in @xmath90 is a decreasing sequence ( unless @xmath112 ) .",
    "the contribution of @xmath113 in @xmath90 is constant , since @xmath114 .",
    "this corresponds to the invariance of the barycenter .    the sequence @xmath90 is bounded and monotonic , so it has a limit that we can also compute explicitly as @xmath115 where @xmath116 are the eigenvalues of the laplacian of the graph .",
    "it is worth to recall that the asymptotical error , which we can also write as @xmath117 , only depends on the properties of @xmath60 as the solution of the regularized least - squares problem : hence it does not depend on the algorithm .      for every @xmath118 ,",
    "we can define a near - optimal stopping time , after which the estimation error is only a @xmath119 factor larger than the optimal one : @xmath120    the following estimate shows that the algorithm can be stopped , with a guaranteed loss of accuracy with respect to the regularized least - squares optimum , after a time which does not depend the graph or even on the number of sensors .",
    "[ prop : bound - t - eps ] if assumption  [ ass : on - tau ] is satisfied , then it holds @xmath121 where @xmath98 is defined in  .    from the definition we immediately deduce that @xmath122 by taking an upper bound on the second term of the left - hand side of the inequality , we have @xmath123 since assumption  [ ass : on - tau ] implies @xmath124 and @xmath125",
    ", we have @xmath126 by solving for @xmath102 in the above inequality we get @xmath127 and then the result follows .",
    "we have simulated algorithm   and numerically evaluated the related performance metrics , assuming the graph to be a cycle .",
    "figure  [ fig : t - epsilon ] compares the simulated and expected performance of the algorithm .",
    "notice that , although the expected error @xmath128 is monotonic , single realizations need not to be monotonic , and indeed some of them show a minimum .",
    "the figure also shows the actual near - optimal time @xmath129 , in comparison with its estimate obtained in proposition  [ prop : bound - t - eps ] .",
    "we notice that the estimate is significantly larger than the true value : this looseness is not surprising , as our bound does not exploit any information about the topology of the sensing and communication graph , which is likely to have a role .",
    "hence , future research may improve upon our bounds by a careful use of information about the spectrum of @xmath53 , _",
    "i.e. _ , on the graph .     on a cycle graph with @xmath130 , @xmath131 , @xmath132 , @xmath133 . ]",
    "the second goal of our simulations is to compare algorithm   with the analogous algorithm defined in  ( * ? ? ?",
    "* eq .  ( 1 ) ) , based on based on minimizing @xmath44 . hence , figure  [ fig : compare - algos ] plots for both algorithms the mean square error , together with the mean square error of a few single realizations .",
    "we can see that the performance of the two algorithms is roughly similar ( in expectation ) until the algorithm in  @xcite reaches a time at which its mean square error is minimal . from that time on , the behavior of the two algorithms becomes different , as algorithm  ( * ? ? ?",
    "* eq .  ( 1 ) ) accumulates an increasingly larger mean square error , whereas the error of algorithm   decreases further .",
    "we leave to future research a more detailed comparison of the two algorithms , which should include a discussion on the behavior of single realizations , as opposed to the average performance which has been studied so far .    , @xmath131 , and @xmath132 .",
    "in this paper , we have studied a distributed algorithm to solve the relative localization problem in sensor networks . compared to algorithms available on literature",
    ", the proposed algorithm has an improved performance for large times : moreover , the algorithm is guaranteed to reach ( on average ) an @xmath1-approximation of the optimal solution within a time which only grows logarithmically in @xmath1 and does not depend on either the topology of the sensor network or the number of sensors .",
    "we interpret this feature as an inherent limitation on the benefit of cooperation .",
    "future research should put our results in a broader context , investigating the fundamental issue of quantifying the benefit of cooperation ( if any ) , depending on the `` cooperation task '' which is assigned to the agents , as well as on the available communication and the measurement models : a recent example of work in this direction is  @xcite .",
    "a.  giridhar and p.  r. kumar , `` distributed clock synchronization over wireless networks : algorithms and analysis , '' in _ ieee conference on decision and control _ , san diego , ca , usa , dec . 2006 , pp . 49154920 .",
    "s.  bolognani , s.  d. favero , l.  schenato , and d.  varagnolo , `` consensus - based distributed sensor calibration and least - square parameter identification in wsns , '' _ international journal of robust and nonlinear control _ , vol .",
    "20 , no .  2 , pp . 176193 , 2010 .",
    "r.  carli and l.  schenato , `` exponential - rate consensus - based algorithms for estimation from relative measurements , '' university of padova , tech .",
    ", sep . 2012 .",
    "[ online ] .",
    "available : http://automatica.dei.unipd.it/people/schenato/publications.html      f.  garin and s.  zampieri , `` mean square performance of consensus - based distributed estimation over regular geometric graphs , '' _ siam journal on control and optimization _ , vol .",
    "50 , no .  1 ,",
    "pp . 306333 , 2012 ."
  ],
  "abstract_text": [
    "<S> important applications in robotic and sensor networks require distributed algorithms to solve the so - called relative localization problem : a node - indexed vector has to be reconstructed from measurements of differences between neighbor nodes . in a recent note , </S>",
    "<S> we have studied the estimation error of a popular gradient descent algorithm showing that the mean square error has a minimum at a finite time , after which the performance worsens . </S>",
    "<S> this paper proposes a suitable modification of this algorithm incorporating more realistic _ a priori _ information on the position . </S>",
    "<S> the new algorithm presents a performance monotonically decreasing to the optimal one . </S>",
    "<S> furthermore , we show that the optimal performance is approximated , up to a @xmath0 factor , within a time which is independent of the graph and of the number of nodes . </S>",
    "<S> this convergence time is very much related to the minimum exhibited by the previous algorithm and both lead to the following conclusion : in the presence of noisy data , cooperation is only useful till a certain limit . </S>"
  ]
}