{
  "article_text": [
    "basic random optimization(bro ) was proposed by matyas@xcite in 1965 , and it is proved that the bro can ensures convergence to a global minimum with probability one(@xcite,@xcite ) . to enhance the performance of the random optimization ,",
    "various strategies have been introduced . in @xcite , to adjust the parameters of mean and standard deviation in a gaussian vector , a heuristic random optimization ( hro ) war presented , utilizing two different mechanisms based on gradient information and reinforcement , respectively . in ( @xcite,@xcite ) , two approaches named adaptive random search technique(arset ) and dynamic random search technique(darset ) were put forward by coskun hamzacebi and fevzi kutay , to facilitate the determination of the global minimum .",
    "then , another new random search algorithm named random selection walk(rsw ) was raised through integrating the random selection and walk algorithms@xcite .",
    "+ based on the concepts of state and state transition , a metaheuristic random search method called state transition algorithm(sta ) was emerged , and experimental results have shown that the proposed algorithm is advantageous for most of the test problems@xcite . on the other hand , especially for optimization functions with independent variables , it is easy to get trapped into local optimum . in this study ,",
    "a new operation call axesion transformation is raised to promote its search ability .",
    "comparisons with other methods on several benchmark problems are presented in the paper , and the outcome is satisfactory , which shows that the proposed strategy is effective .",
    "in general , the form of state transition algorithm can be described as the following @xmath0 where @xmath1 stands for a state , corresponding to a solution of the optimization problem ; @xmath2 and @xmath3 are state transition matrixes , which are usually transformation operators ; @xmath4 is the function with variables @xmath1 and history states ; @xmath5 is the objective function or evaluation function .",
    "+ using various types of space transformation for reference , three special state transformation operators are defined to solve continuous function optimization problems .",
    "+ ( 1 ) rotation transformation @xmath6 where @xmath1 @xmath7 @xmath8 , @xmath9 is a positive constant , called rotation factor ; @xmath10 @xmath7 @xmath11,is random matrix with its elements belonging to the range of [ -1 , 1 ] and @xmath12 is 2-norm of a vector .",
    "it has proved that the rotation transformation has the function of searching in a hypersphere@xcite . + ( 2 )",
    "translation transformation + @xmath13 where @xmath14 is a positive constant , called translation factor ; @xmath15 @xmath16 is a random variable with its elements belonging to the range of [ 0,1 ] .",
    "it has illustrated the translation transformation has the function of searching along a line from @xmath17 to @xmath1 at the starting point @xmath1 , with the maximum length of @xmath14@xcite .",
    "+ ( 3 ) expansion transformation @xmath18 where @xmath19 is a positive constant , called expansion factor ; @xmath20is a random diagonal matrix with its elements obeying the gaussian distribution .",
    "it has also stated the expansion transformation has the function of expanding the elements in @xmath1 to the range of [ -@xmath21 , + @xmath21 ] , searching in the whole space@xcite .",
    "+ the procedures of the original state transition algorithm can be outlined in the following pseudocode .",
    "+    initialize feasible solution @xmath23 randomly , set @xmath24 , and @xmath25 @xmath26 @xmath27 updating @xmath28 @xmath29 updating @xmath28 @xmath30 @xmath31 updating @xmath28 @xmath29 updating @xmath28 @xmath32     + where ( _ se _ ) is search enforcement , which means the times of the transformation .",
    "operators such as @xmath33 and @xmath34 correspond to the rotation , translation , and expansion , respectively .",
    "_ fc _ is a constant coefficient used for lessening the @xmath9 . by the way , the translation operator will only be performed when",
    "a better solution is obtained .",
    "in the proposed sta , three different state transformation operators are designed , aiming for exploration(global search ) and exploitation(local search ) as well as the equilibrium between them .",
    "it is beneficial for optimization functions with relevant variables ; however , for functions with independent variables , it is necessary to intensify the single dimension search .",
    "+ to simplify the one dimensional search , a new operator called axesion is added to the sta , which , in its meaning , aims to search along each axes .",
    "+ ( 4 ) axesion transformation @xmath35 where @xmath36 is a positive constant , called axesion factor ; @xmath37 @xmath38 is a random diagonal matrix with its elements obeying the gaussian distribution and only one random index has value .",
    "for example , @xmath37 @xmath39 can be the following styles : +    @xmath40 @xmath41 @xmath42     + to illustrate the functions of the axesion transformation , let suppose the @xmath43 $ ] and @xmath44 ; then , after 1e3 times of independent axesion transformation , the distribution of @xmath45 can be illustrated on the space as described in figure.[tab : axesion ] .",
    "+ when the new transformation is introduced into the original sta , it will follow the same procedures as rotation and expansion , while translation transformation will only be performed when a better solution is gained by axesion . in the meanwhile , to reduce the computational complexity , the rotation transformation will be executed in an outer loop instead of an inner loop , that is to say , the @xmath9 factor will vary in a periodic way .",
    "experiments are divided into two groups . the first group consist of five benchmark problems which have been tested by hro , arset , and rsw , while the second group benchmark problems have been tested by darset and rsw . in the proposed original sta and new sta ,",
    "all of the problems will be carried out .",
    "parameters of the stas are described in table [ tab : stas ] , and the lessening coefficient _ fc _ will be 4 in original sta and 2 in new sta .",
    "cc _ parameter _ & _ value _ + epoch number & 1000 + @xmath46(search enforcement ) & 32 + @xmath9 & 1 @xmath47 1e-4 + @xmath14 & 1 + @xmath19 & 1 + @xmath36 & 1 + _ fc _ & 4(2 ) +      \\(1 ) problem @xmath48 + the first problem is taken from [ 4,5,7 ] , objective function of the problem is given as follows @xmath49 as can be seen from fig . [",
    "tab : f1 ] , the function has two minimums , one lying on @xmath50 and the other one on @xmath51 .",
    "their results of the hro , arset , rsw , and stas are given in table [ tab : rf1 ] .",
    "both stas can achieve the global minimum in the end , as a matter of fact , stas can meet the optimum in no more than 10 epoches .    ,",
    "width=226,height=226 ]    cccc _ algorithms _ & best @xmath52 & best @xmath53 + hro & 3.000324 & -3 + arset & 3 & -3 + rsw & 3 & -3 + sta(original ) & 3 & -3 + sta(new ) & 3 & -3 +     + ( 2 ) problem @xmath54 + the second problem is taken from [ 4,5,7 ] , objective function of the problem is given as follows @xmath55 ^ 4 + [ x cos(\\frac{1}{x})]^4 , f(0)= \\lim_{x\\rightarrow 0 } f(x)=0 \\label{tab : eqf2}\\ ] ] as can be seen from fig . [",
    "tab : f2 ] , the function has numerous local minimums . their results of the hro , arset , rsw , and stas are given in table [ tab : rf2 ] .",
    "both stas can achieve the global minimum in the end , as a matter of fact , stas can meet the optimum in no more than 50 epoches .    , width=226,height=226 ]",
    "cccc _ algorithms _ & best @xmath52 & best @xmath53 + hro & 2.4000e-005 & 2.8595e-019 + arset & -2.53e-011 & 2.21e-043 + rsw & 8.17e-82 & 0 + sta(original ) & 2.0447e-082 & 0 + sta(new ) & 3.5197e-084 & 0 +     + ( 3 ) problem @xmath56 + the third problem is taken from [ 5,7 ] , objective function of the problem is given as follows @xmath57 as can be seen from fig .",
    "[ tab : f3 ] , the function also has numerous local minimums .",
    "their results of the arset , rsw , and stas are given in table [ tab : rf3 ] .",
    "compared with arset and rsw , stas can get better solution than them , in the same time , stas can meet the specified precision in no more than 100 epoches .    , width=226,height=226 ]",
    "cccc _ algorithms _ & best @xmath52 & best @xmath58 & best @xmath59 + arset & 3.0015 & 3 & 5.04e-023 + rsw & 2.9996 & 3 & 3.43e-28 + sta(original ) & 3.0000 & 3.0000 & 5.8715e-033 + sta(new ) & 3.0000 & 3.0000 & 1.0335e-035 +     + ( 4 ) problem @xmath60 + the fourth problem is taken from [ 5,7 ] , objective function of the problem is given as follows @xmath61 fig .",
    "[ tab : f4 ] shows the graph of the function , which is widely used for testing because there is a valley in the landscape , making it hard to optimize .",
    "their results of the arset , rsw , and stas are given in table [ tab : rf4 ] .",
    "compared with arset and rsw , stas seem a little deficient for the function .    , width=226,height=226 ]    cccc _ algorithms _ & best @xmath52 & best @xmath58 & best @xmath59 + arset & 1 & 1 & 4.02e-016 + rsw & 1 & 1 & 1.97e-31 + sta(original ) & 1.0000 & 1.0000 & 8.2040e-012 + sta(new ) & 1.0000 & 1.0000 & 3.7678e-012 +     + ( 5 ) problem @xmath62 + the fifth problem is taken from [ 5,7],objective function of the problem is given as follows @xmath63 fig .",
    "[ tab : f5 ] shows the graph of the function , which is indifferentiable at the minimum point .",
    "their results of the arset , rsw , and stas are given in table [ tab : rf5 ] .",
    "compared with arset and rsw , stas are much better than them , because only they can meet the global minimum .    ,",
    "width=226,height=226 ]    cccc _ algorithms _ & best @xmath52 & best @xmath58 & best @xmath59 + arset & -10 & 6.67e-008 & -10 + rsw & -9.9996 & -6.57e-17 & -9.9996 + sta(original ) & -10.0000 & 0.0000 & -10 + sta(new ) & -10.0000 & 0.0000 & -10 +      problems @xmath64 to @xmath65 are taken from [ 6 ] and [ 7 ] , which have been tested by darset and rsw .",
    "the detailed information of the test functions are listed in table [ tab : grp2 ] . in darset and rsw ,",
    "epoch number varies from 250200 to 2508000 ; however , in original sta and new sta , both the epoch number is fixed at 1000 , and 10 independent times are run .",
    "+ as described in table [ tab : rgrp ] , the original sta can get the same results as darset and rsw in @xmath66 and @xmath65 . for",
    "@xmath64 and @xmath67 , the results of original sta are better than darset but a little inferior to bsw , for @xmath68 and @xmath69 , the original sta can get better results than bsw but a little inferior to darset , and for @xmath70 , the original sta can gain better solution than both darset and bsw .",
    "+ on the other hand , the new sta can achieve the best results of all functions except for @xmath71 and @xmath72 .",
    "even so , it is obvious to find that the new sta actually can achieve the global minimums in a specified precision .",
    "+ through the numerical experiments , it has proved that the new sta has better performance than the original sta in terms of the global search ability and the proposed axesion transformation is beneficial for sta . + the reasons of the efficiency of the new strategy can be explained as : ( 1 ) axesion transformation enlarge the search space .",
    "not only rotation but also expansion , both of them search in the neighborhood of the best solution , of which , one search in a unit hypersphere , the other search in a relative broader space but with low probability , while the new operator can also search in global space , that is to say , the probability of finding the global optimum is increased .",
    "( 2 ) the new transformation has the function of single dimensional search , which is advantageous for functions with independent variables . ( 3 ) the new transformation can achieve good quality solution with high precision .",
    "when searching in a single direction , the axesion can enhance the depth of search .    lccc function & dimension & variable range & theoretical best + @xmath73 & 2 & [ -1.28,1.28 ] & 0 + @xmath74[2.1 - cos(3\\pi y ) + cos(3.5\\pi y)]$ ] & 2 & [ -1,1 ] & -16.0917 + @xmath75^{-1}$ ] & 2 & [ -65.536,65.536 ] & 0.9980 + @xmath76 & & & + @xmath77 & 2 & @xmath78\\\\ y \\in [ 0,15 ] \\end{array } $ ] & 0.3979 + @xmath79 & 2 & @xmath80\\\\ y \\in [ -2,2 ] \\end{array } $ ] & -1.0316 + @xmath81 \\\\ \\times [ 30+(2x-3y)^2(18 - 32x+12x^2 + 48y-36xy+27y^2 ) ] \\end{array } $ ] & 2 & [ -5,5 ] & 3 + @xmath82\\times[\\sum_{i=1}^5icos((i+1)y+i)]$ ] & 2 & [ -10,10 ] & -186.7309 + @xmath83 & 3 & @xmath84 $ ] & 8.0128 + @xmath85 & & & + @xmath86 + 19.8(x_2 - 1)(x_4 - 1 ) \\end{array } $ ] & 4 & [ -10,10 ] & 0 + @xmath87 $ ] & 20 & [ -1,4 ] & 0 + @xmath88 $ ] & 20 & [ -10,10 ] & 0 + @xmath89 & 2 & [ -10,10 ] & 0 + @xmath90 & 2 & [ -5,5 ] & 1 + @xmath91 $ ] & 2 & [ 0,10 ] & 1.74 + @xmath92 & 4 & [ -5,5 ] & 0 +    ccccccccc function & & & & + & best & average & best & average & best & average & best & average + @xmath64 & 0 & 9.10e-016 & 0 & 0 & 0 & 5.3147e-012 & 0 & 0 + @xmath68 & -16.0917 & -16.0917 & -16.0917 & -15.7399 & -16.0917 & -16.0917 & -16.0917 & -16.0917 + @xmath69 & 0.998 & 1.5885 & 0.998 & 6.3728 & 0.9980 & 3.9354 & 0.9980 & 0.9980 + @xmath93 & 0.3979 & 0.3979 & 0.3979 & 0.3979 & 0.3979 & 0.3979 & 0.3979 & 0.3979 + @xmath94 & -1.0316 & -1.0316 & -1.0316 & -1.0316 & -1.0316 & -1.0316 & -1.0316 & -1.0316 + @xmath95 & 3 & 3 & 3 & 3 & 3.0000 & 3.0000 & 3.0000 & 3.0000 + @xmath96 & -186.7309 & -186.7309 & -186.7309 & -186.7309 & -186.7309 & -186.7309 & -186.7309 & -186.7309 + @xmath97 & 8.0128 & 8.0128 & 8.0128 & 8.0128 & 8.0128 & 8.0128 & 8.0128 & 8.0128 + @xmath98 & 3.72e-12 & 9.30e-06 & 1.28e-28 & 2.15e-28 & 2.8718e-010 & 1.1802e-009 & 8.3086e-011 & 1.1344e-009 + @xmath67 & 2.45e-16 & 4.02e-15 & 0 & 0 & 0 & 0 & 4.9783e-094 & 2.7247e-084 + @xmath70 & 5.93e-12 & 26.227 & 2.36e-32 & 3.3927 & 7.2021e-011 & 1.0417 & 2.6223e-011 & 3.8022e-011 + @xmath99 & 3.91e-15 & 4.28e-14 & 2.84e-29 & 6.07e-28 & 8.9683e-014 & 3.8771e-012 & 9.5239e-014 & 9.9002e-012 + @xmath72 & 1 & 1.0077 & 1.0091 & 1.0091 & 1.0000 & 1.0375 & 1.0000 & 1.0225 + @xmath100 & 1.7442 & 1.7442 & 1.7442 & 1.7442 & 1.7442 & 1.7442 & 1.7442 & 1.7442 + @xmath65 & 8.17e-09 & 1.68e-07 & 1.02e-11 & 1.71e-11 & 2.1942e-014 & 6.4995e-009 & 9.9870e-014 & 1.0542e-007 +",
    "as a random search method , the original sta has shown the great ability in optimizing continuous functions . to enhance the global search capability of the sta",
    ", axesion transformation is introduced into the original sta , which aims to search along a single dimension in depth .",
    "the results of the numerical experiments have testified the efficiency and reliability of the new sta .",
    "comparisons with other random optimization methods , the outcome of the experiments has also revealed the advantages of the stas . by the way , other functions with independent variables have also been tested , and results are more satisfactory .    1 matyas , j ,  random optimization , \" _ automation and remote control _ ,",
    "246 - 253 , 1965 .",
    "francisco j. solis and roger j - b .",
    "wets ,  minimization by random search techniques , \" _ mathematics of operations research _ , vol . 6 , no .",
    "19 - 30 , 1981 .",
    "n.baba , t.shoman,y.sawaragi ,  a modified convergence therorem for a random optimization method , \" _ information science _ , vol .",
    "159 - 166 , 1977 .",
    "junyi li and r. russell rhinehart ,  heuristic random optimization , \" _ computers and chemical engineering _ , vol .",
    "427 - 444 , 1998 .",
    "coskun hamzacebi , fevzi kutay ,  a heuristic approach for finding the global minimum : adaptive random search technique , \" _ applied mathematics and computation _ , vol .",
    "1323 - 1333 , 2006 .",
    "coskun hamzacebi , fevzi kutay ,  continous functions minimization by dynamic random search technique , \" _ applied mathematical modeling _",
    "2189 - 2198 , 2007 .",
    "tunchan cura ,  a random search approach to finding the global minimum , \" _",
    "int.j.contemp.math.science_ , vol . 5 , no .",
    "179 - 190 , 2010 .",
    "xiaojun zhou ,  chunhua yang and weihua gui , initial version of state transition algorithm , \" _ the 2nd international conference on digital manufacturing & automation _ ,",
    "2011(to be published ) ."
  ],
  "abstract_text": [
    "<S> to promote the global search ability of the original state transition algorithm , a new operator called axesion is suggested , which aims to search along the axes and strengthen single dimensional search . </S>",
    "<S> several benchmark minimization problems are used to illustrate the advantages of the improved algorithm over other random search methods . </S>",
    "<S> the results of numerical experiments show that the new transformation can enhance the performance of the state transition algorithm and the new strategy is effective and reliable . </S>"
  ]
}