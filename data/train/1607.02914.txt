{
  "article_text": [
    "there have been various techniques to evaluate performance of machine learning methods theoretically .",
    "taking lasso @xcite as an example , lasso has been analyzed by nonparametric statistics @xcite , empirical process @xcite , statistical physics @xcite and so on . in general ,",
    "most of these techniques require either asymptotic assumption ( sample number @xmath0 and/or feature number @xmath1 go to infinity ) or various technical assumptions like boundedness of features or moment conditions .",
    "some of them are much restrictive for practical use . in this paper",
    ", we try to develop another way for performance evaluation of machine learning methods with as few assumptions as possible .",
    "an important candidate for this purpose is barron and cover s theory ( bc theory ) , which is one of the most famous results for the minimum description length ( mdl ) principle .",
    "the mdl principle @xcite claims that the shortest description of a given set of data leads to the best hypotheses about the data source . a famous model selection criterion based on the mdl principle was proposed by rissanen @xcite .",
    "this criterion corresponds to a codelength of a two - stage code in which one encodes a statistical model to encode data and then the data are encoded with the model . in this case ,",
    "an mdl estimator is defined as the minimizer of the total codelength of this two - stage code .",
    "bc theory @xcite guarantees that a risk of the mdl estimator in terms of the rnyi divergence @xcite is tightly bounded from above by redundancy of the corresponding two - stage code .",
    "because this result means that the shortest description of the data by the two - stage code yields the smallest risk upper bound , this result gives a mathematical justification of the mdl principle .",
    "furthermore , bc theory holds for finite @xmath0 without any complicated technical conditions .",
    "however , bc theory has been applied to supervised learning only approximately or limitedly .",
    "the original bc theory seems to be widely recognized that it can be applicable to both unsupervised and supervised learning .",
    "though it is not false , bc theory actually can not be applied to supervised learning without a certain condition ( condition [ criticalcondition ] defined in section [ bctheory ] ) .",
    "this condition is critical in a sense that lack of this condition breaks a key technique of bc theory .",
    "the literature @xcite is the only example of application of bc theory to supervised learning to our knowledge .",
    "his work assumed a specific setting , where condition [ criticalcondition ] can be satisfied .",
    "however , the risk bound may not be sufficiently tight due to imposing condition [ criticalcondition ] forcedly , which will be explained in section [ bctheory ] .",
    "another well - recognized disadvantage is the necessity of quantization of parameter space .",
    "barron et al . proposed a way to avoid the quantization and derived a risk bound of lasso @xcite as an example .",
    "however , their idea can not be applied to supervised learning in general .",
    "the main difficulty stems from condition [ criticalcondition ] as explained later .",
    "it is thus essentially difficult to solve .",
    "actually , their risk bound of lasso was derived with fixed design only ( i.e. , essentially unsupervised setting ) .",
    "the fixed design , however , is not satisfactory to evaluate generalization error of supervised learning . in this paper",
    ", we propose an extension of bc theory to supervised learning without quantization in random design cases .",
    "the derived risk bound inherits most of advantages of the original bc theory .",
    "the main term of the risk bound has again a form of redundancy of two - stage code .",
    "thus , our extension also gives a mathematical justification of the mdl principle in supervised learning .",
    "it should be remarked that , however , an additional condition is required for an exact redundancy interpretation .",
    "we also derive new risk and regret bounds of lasso with random design as its application under normality of features .",
    "this application is not trivial at all and requires much more effort than both the above extension itself and the derivation in fixed design cases .",
    "we will try to derive those bounds in a manner not specific to our setting but rather applicable to several other settings .",
    "interestingly , the redundancy and regret interpretation for the above bounds are exactly justified without any additional condition in the case of lasso .",
    "the most advantage of our theory is that it requires almost no assumptions : neither asymptotic assumption ( @xmath3 is also allowed ) , bounded assumptions , moment conditions nor other technical conditions .",
    "especially , it is remarkable that our risk evaluation holds for finite @xmath0 without necessity of boundedness of features though the employed loss function ( the rnyi divergence ) is not bounded .",
    "behavior of the regret bound will be investigated by numerical simulations .",
    "it may be worth noting that , despite we tried several other approaches in order to extend bc theory to supervised learning , we can hardly derive a risk bound of lasso as tight as meaningful by using them .",
    "we believe that our proposal is currently the unique choice that could give a meaningful risk bound .",
    "this paper is organized as follows .",
    "section [ setting ] introduces an mdl estimator in supervised learning .",
    "we briefly review bc theory and its recent progress in section [ bctheory ] .",
    "the extension of bc theory to supervised learning will appear in section [ extension ] .",
    "we derive new risk and regret bounds of lasso in section [ lasso ] .",
    "all proofs of our results are given in section [ proofs ] .",
    "section [ simulation ] contains numerical simulations .",
    "a conclusion will appear in section [ conclusion ] .",
    "suppose that we have @xmath0 training data @xmath4 generated from @xmath5 , where @xmath6 is a domain of feature vector @xmath7 and @xmath8 could be @xmath9 ( regression ) or a finite set ( classification ) according to target problems . here ,",
    "the sequence @xmath10 is not necessarily independently and identically distributed ( i.i.d . ) but can be a stochastic process in general .",
    "we write the @xmath11th component of the @xmath12th sample as @xmath13 . to define an mdl estimator according to the notion of two - stage code @xcite",
    ", we need to describe data itself and a statistical model used to describe the data too . letting @xmath14",
    "be the codelength of the two - stage code to describe @xmath15 , @xmath14 can be decomposed as @xmath16 by the chain rule . since",
    "a goal of supervised learning is to estimate @xmath17 , we need not estimate @xmath18 . in view of the mdl principle , this implies that @xmath19 ( the description length of @xmath20 ) can be ignored .",
    "therefore , we only consider the encoding of @xmath21 given @xmath20 hereafter .",
    "this corresponds to a description scheme in which an encoder and a decoder share the data @xmath20 . to describe @xmath21 given @xmath20",
    ", we use a parametric model @xmath22 with parameter @xmath23 .",
    "the parameter space @xmath24 is a certain continuous space or a union of continuous spaces .",
    "note that , however , the continuous parameter can not be encoded .",
    "thus , we need to quantize the parameter space @xmath24 as @xmath25 . according to the notion of the two - stage code",
    ", we need to describe not only @xmath21 but also the model used to describe @xmath21 ( or equivalently the parameter @xmath26 ) given @xmath20 .",
    "again by the chain rule , such a codelength can be decomposed as @xmath27 here , @xmath28 expresses a codelength to describe @xmath21 using @xmath29 , which is , needless to say , @xmath30 . on the other hand",
    ", @xmath31 expresses a codelength to describe the model @xmath29 itself . note that @xmath32 must satisfy kraft s inequality @xmath33 the mdl estimator",
    "is defined by the minimizer of the above codelength : @xmath34 let us write the minimum description length attained by the two - stage code as @xmath35 because @xmath36 also satisfies kraft s inequality with respect to @xmath21 for each @xmath20 , it is interpreted as a codelength of a prefix two - stage code .",
    "therefore , @xmath37 is a conditional sub - probability distribution corresponding to the two - stage code .",
    "we briefly review barron and cover s theory ( bc theory ) and its recent progress in view of supervised learning though they discussed basically unsupervised learning ( or supervised learning with fixed design ) . in bc theory ,",
    "the rnyi divergence @xcite between @xmath38 and @xmath39 with order @xmath40 @xmath41 is used as a loss function .",
    "the rnyi divergence converges to kullback - leibler ( kl ) divergence @xmath42 as @xmath43 , i.e. , @xmath44 for any @xmath45 .",
    "we also note that the rnyi divergence at @xmath46 is equal to bhattacharyya divergence @xcite @xmath47 we drop @xmath0 of each divergence like @xmath48 if it is defined with a single random variable , i.e. , @xmath49 bc theory requires the model description length to satisfy a little bit stronger kraft s inequality defined as follows .",
    "let @xmath50 be a real number in @xmath51 .",
    "we say that a function @xmath52 satisfies @xmath50-stronger kraft s inequality if @xmath53 where the summation is taken over a range of @xmath54 in its context .",
    "the following condition is indispensable for application of bc theory to supervised learning .",
    "[ criticalcondition ] both the quantized space and the model description length are independent of @xmath20 , i.e. , @xmath55    under condition [ criticalcondition ] , bc theory @xcite gives the following two theorems for supervised learning .",
    "though these theorems were shown only for the case of hellinger distance in the original literature @xcite , we state these theorems with the rnyi divergence .",
    "[ bctheorem1 ] let @xmath50 be a real number in @xmath51 .",
    "assume that @xmath56 satisfies @xmath50-stronger kraft s inequality .",
    "under condition [ criticalcondition ] , @xmath57\\label{origbound } \\\\   & = & e_{\\bar{p}_*(x^n , y^n)}\\log\\frac{p_*(y^n|x^n)}{{\\tilde{p}_{2}}(y^n|x^n)}\\label{origbound2}\\end{aligned}\\ ] ] for any @xmath58 $ ] .",
    "[ bctheorem2 ] let @xmath50 be a real number in @xmath51 .",
    "assume that @xmath56 satisfies @xmath50-stronger kraft s inequality . under condition",
    "[ criticalcondition ] , @xmath59 for any @xmath58 $ ] .",
    "since the right side of ( [ origbound2 ] ) is just the redundancy of the prefix two - stage code , theorem [ bctheorem1 ] implies that we obtain the smallest upper bound of the risk by compressing the data most with the two - stage code .",
    "that is , theorem [ bctheorem1 ] is a mathematical justification of the mdl principle .",
    "we remark that , by interchanging the infimum and the expectation of ( [ origbound ] ) , the right side of ( [ origbound ] ) becomes a quantity called `` index of resolvability '' @xcite , which is an upper bound of redundancy .",
    "it is remarkable that bc theory requires no assumption except condition [ criticalcondition ] and @xmath50-stronger kraft s inequality .",
    "however , condition [ criticalcondition ] is a somewhat severe restriction .",
    "both the quantization and the model description length can depend on @xmath20 in the definitions . in view of the mdl principle , this is favorable because the total description length can be minimized according to @xmath20 flexibly .",
    "if we use the model description length that is uniform over @xmath60 in contrast , the total codelength must be longer in general .",
    "hence , data - dependent model description length is more desirable .",
    "actually , this observation suggests that the bound derived in @xcite may not be sufficiently tight .",
    "in addition , the restriction by condition [ criticalcondition ] excludes a practically important case ` lasso with column normalization ' ( explained below ) from the scope of application .",
    "however , it is essentially difficult to remove this restriction as noted in section [ introduction ] .",
    "another concern is quantization .",
    "the quantization for the encoding is natural in view of the mdl principle .",
    "our target , however , is an application to usual estimators or machine learning algorithms themselves including lasso .",
    "a trivial example of such an application is a penalized maximum likelihood estimator ( pmle ) @xmath61 where @xmath62 is a certain penalty .",
    "similarly to the quantized case , let us define @xmath63 that is , @xmath64 note that , however , @xmath65 is not necessarily a sub - probability distribution in contrast to the quantized case , which will be discussed in detail in section [ extension ] .",
    "pmle is a wide class of estimators including many useful methods like ridge regression @xcite , lasso , dantzig selector @xcite and any maximum - a - posteriori estimators of bayes estimation . if we can accept @xmath66 as an approximation of @xmath67 ( by taking @xmath68 ) , we have a risk bound by direct application of bc theory .",
    "however , the quantization is unnatural in view of machine learning application . besides",
    ", we can not use any data - dependent @xmath69 .",
    "barron et al . proposed an important notion ` _ risk validity _ ' to remove the quantization @xcite .",
    "[ riskvalidity ] let @xmath50 be a real number in @xmath51 and @xmath70 be a real number in @xmath71 $ ] . for fixed @xmath20",
    ", we say that a penalty function @xmath72 is risk valid if there exist a quantized space @xmath73 and a model description length @xmath31 satisfying @xmath50-stronger kraft s inequality such that @xmath25 and @xmath31 satisfy @xmath74 where @xmath75    note that their original definition in @xcite was presented only for the case where @xmath76 . here , @xmath77 is the rnyi divergence for fixed design ( @xmath20 is fixed ) .",
    "hence , @xmath78 does not depend on @xmath18 in contrast to the rnyi divergence for random design @xmath79 defined by ( [ renyidiv ] ) .",
    "barron et al . proved that @xmath67 has bounds similar to theorems [ bctheorem1 ] and [ bctheorem2 ] for any risk valid penalty in the fixed design case .",
    "their way is excellent because it does not require any additional condition other than the risk validity .",
    "however , the risk evaluation only for a particular @xmath20 like @xmath80 $ ] is unsatisfactory for supervised learning . in order to evaluate the so - called ` generalization error ' of supervised learning , we need to evaluate the risk with random design , i.e. , @xmath81 $ ] .",
    "however , it is essentially difficult to apply their idea to random design cases as it is .",
    "let us explain this by using lasso as an example .",
    "the readers unfamiliar to lasso can refer to the head of section [ lasso ] for its definition . by extending the definition of risk validity to random design straightforwardly ,",
    "we obtain the following definition .    [ riskvalidityinrd ]",
    "let @xmath50 be a real number in @xmath51 and @xmath70 be a real number in @xmath71 $ ] .",
    "we say that a penalty function @xmath72 is risk valid if there exist a quantized space @xmath82 and a model description length @xmath83 satisfying @xmath50-stronger kraft s inequality such that @xmath84    in contrast to the fixed design case , ( [ rv ] ) must hold not only for a fixed @xmath85 but also for all @xmath86 .",
    "in addition , @xmath87 and @xmath83 must be independent of @xmath20 due to condition [ criticalcondition ] .",
    "the form of rnyi divergence @xmath88 also differs from @xmath89 of the fixed design case in general .",
    "let us rewrite ( [ rvrd ] ) equivalently as @xmath90 for short , we write the inside part of the minimum of the left side of ( [ rv2 ] ) as @xmath91 .",
    "we need to evaluate @xmath92 in order to derive risk valid penalties",
    ". however , it seems to be considerably difficult . to our knowledge , the technique used by chatterjee and barron @xcite is the best way to evaluate it , so that we also employ it in this paper .",
    "a key premise of their idea is that taking @xmath93 close to @xmath94 is not a bad choice to evaluate @xmath95 .",
    "regardless of whether it is true or not , this premise seems to be natural and meaningful in the following sense . if we quantize the parameter space finely enough , the quantized estimator @xmath66 is expected to behave almost similarly to @xmath67 with the same penalty and is expected to have a similar risk bound . if we take @xmath96 , then @xmath91 is equal to @xmath97 , which implies that @xmath97 is a risk valid penalty and has a risk bound similar to the quantized case .",
    "note that , however , we can not match @xmath93 to @xmath94 exactly because @xmath93 must be on the fixed quantized space @xmath87 .",
    "so , chatterjee and barron randomized @xmath93 on the grid points on @xmath87 around @xmath94 and evaluate the expectation with respect to it .",
    "this is clearly justified because @xmath98 $ ] .",
    "by using a carefully tuned randomization , they succeeded in removing the dependency of @xmath99 $ ] on @xmath21 .",
    "let us write the resultant expectation as @xmath100 $ ] for convenience .",
    "any upper bound @xmath72 of @xmath101 is a risk valid penalty . by this fact ,",
    "risk valid penalties should basically depend on @xmath20 in general . if not ( @xmath102 ) , @xmath103 must bound @xmath104 , which makes @xmath105 much larger .",
    "this is again unfavorable in view of the mdl principle . in particular",
    ", @xmath101 includes an unbounded term in linear regression cases with regard to @xmath20 , which originates from the third term of the left side of ( [ rv2 ] ) .",
    "this can be seen by checking section iii of @xcite .",
    "though their setting is fixed design , this fact is also true for the random design .",
    "hence , as long as we use their technique , derived risk valid penalties must depend on @xmath20 in linear regression cases .",
    "however , the @xmath106 norm used in the usual lasso does not depend on @xmath20 .",
    "hence , the risk validity seems to be useless for lasso .",
    "however , the following weighted @xmath106 norm @xmath107 plays an important role here .",
    "the lasso with this weighted @xmath106 norm is equivalent to an ordinary lasso with column normalization such that each column of the design matrix has the same norm .",
    "the column normalization is theoretically and practically important .",
    "hence , we try to find a risk valid penalty of the form @xmath108 , where @xmath109 and @xmath110 are real coefficients .",
    "indeed , there seems to be no other useful penalty dependent on @xmath20 for the usual lasso .",
    "in contrast to fixed design cases , however , there are severe difficulties to derive a meaningful risk bound with this penalty .",
    "we explain this intuitively .",
    "the main difficulty is caused by condition [ criticalcondition ] .",
    "as described above , our strategy is to take @xmath93 close to @xmath94 .",
    "suppose now that it is ideally almost realizable for any choice of @xmath111 .",
    "this implies that @xmath91 is almost equal to @xmath112 . on the other hand , for each fixed @xmath94 , the weighted @xmath106 norm of @xmath94 can be arbitrarily small by making @xmath20 small accordingly .",
    "therefore , the penalty @xmath113 is almost equal to @xmath110 in this case .",
    "this implies that @xmath110 must bound @xmath114 , which is infinity in general .",
    "if @xmath56 depended on @xmath20 , we could resolve this problem .",
    "however , @xmath56 must be independent of @xmath20 .",
    "this issue does not seem to be specific to lasso .",
    "another major issue is the rnyi divergence @xmath88 . in the fixed design case ,",
    "the rnyi divergence @xmath89 is a simple convex function in terms of @xmath94 , which makes its analysis easy .",
    "in contrast , the rnyi divergence @xmath88 in case of random design is not convex and more complicated than that of fixed design cases , which makes it difficult to analyze .",
    "we will describe why the non - convexity of loss function makes the analysis difficult in section [ someremarks ] .",
    "the difficulties that we face when we use the techniques of @xcite in the random design case are not limited to them .",
    "we do not explain them here because it requires the readers to understand their techniques in detail .",
    "however , we only remark that these difficulties seem to make their techniques useless for supervised learning with random design .",
    "we propose a remedy to solve these issues in a lump in the next section .",
    "in this section , we propose a way to extend bc theory to supervised learning and derive a new risk bound of lasso .",
    "there are several possible approaches to extend bc theory to supervised learning .",
    "a major concern is how tight a resultant risk bound is .",
    "below , we propose a way that gives a tight risk upper bound for at least lasso .",
    "a key idea is to modify the risk validity condition by introducing a so - called typical set of @xmath20 .",
    "we postulate that a probability distribution of stochastic process @xmath115 is a member of a certain class @xmath116 .",
    "furthermore , we define @xmath117 by the set of marginal distribution of @xmath118 of all elements of @xmath116 .",
    "we assume that we can define a typical set @xmath119 for each @xmath120 , i.e. , @xmath121 as @xmath122 .",
    "this is possible if @xmath123 is stationary and ergodic for example .",
    "see @xcite for detail . for short ,",
    "@xmath124 is written as @xmath125 hereafter .",
    "we modify the risk validity by using the typical set .",
    "[ epsilonriskvalid ] let @xmath126 be real numbers in @xmath51 and @xmath70 be a real number in @xmath71 $ ] .",
    "we say that @xmath72 is @xmath127-risk valid for @xmath128 if for any @xmath129 , there exist a quantized subset @xmath130 and a model description length @xmath131 satisfying @xmath50-stronger kraft s inequality such that @xmath132    note that both @xmath87 and @xmath56 can depend on the unknown distribution @xmath18 .",
    "this is not problematic because the final penalty @xmath69 does not depend on the unknown @xmath18 .",
    "a difference from ( [ rv2 ] ) is the restriction of the range of @xmath20 onto the typical set . from here to the next section , we will see how this small change solves the problems described in the previous section .",
    "first , we show what can be proved for @xmath127-risk valid penalties .",
    "[ mytheorem1 ] define @xmath133 as a conditional expectation with regard to @xmath134 given that @xmath135 .",
    "let @xmath126 be arbitrary real numbers in @xmath51 .",
    "for any @xmath58 $ ] , if @xmath72 is @xmath127-risk valid for @xmath136 , @xmath137    [ mytheorem2 ] let @xmath126 be arbitrary real numbers in @xmath51 . for any @xmath58 $ ] ,",
    "if @xmath72 is @xmath127-risk valid for @xmath136 , @xmath138    a proof of theorem [ mytheorem1 ] is described in section [ mytheorem1proof ] , while a proof of theorem [ mytheorem2 ] is described in section [ mytheorem2proof ] .",
    "note that both bounds become tightest when @xmath76 because the rnyi divergence @xmath79 is monotonically increasing in terms of @xmath70 ( see @xcite for example ) .",
    "we call the quantity @xmath139 in theorem [ mytheorem2 ] ` regret ' of the two - stage code @xmath140 on the given data @xmath15 in this paper , though the ordinary regret is defined as the codelength difference from @xmath141 , where @xmath142 denotes the maximum likelihood estimator . compared to the usual bc theory",
    ", there is an additional term @xmath143 in the risk bound ( [ finriskbound ] ) . due to the property of the typical set",
    ", this term decreases to zero as @xmath144 .",
    "therefore , the first term is the main term , which has a form of redundancy of two - stage code like the quantized case .",
    "hence , this theorem gives a justification of the mdl principle in supervised learning .",
    "note that , however , @xmath145 needs to satisfy kraft s inequality in order to interpret the main term as a conditional redundancy exactly . a sufficient conditions for this was introduced by @xcite and is called ` codelength validity ' .",
    "[ codelengthvalid ] we say that @xmath72 is codelength valid if there exist a quantized subset @xmath73 and a model description length @xmath31 satisfying kraft s inequality such that @xmath146 for each @xmath20 .    we note that both the quantization and the model description length on it depend on @xmath20 in contrast to the @xmath127-risk validity .",
    "this is because the fixed design setting suffices to justify the redundancy interpretation .",
    "let us see that @xmath147 can be exactly interpreted as a codelength if @xmath72 is codelength valid .",
    "first , we assume that @xmath8 , the range of @xmath148 , is discrete . for each @xmath20",
    ", we have @xmath149 hence , @xmath145 can be exactly interpreted as a codelength of a prefix code .",
    "next , we consider the case where @xmath8 is a continuous space .",
    "the above inequality trivially holds by replacing the sum with respect to @xmath21 with an integral .",
    "thus , @xmath65 is guaranteed to be a sub - probability density function .",
    "needless to say , @xmath145 can not be interpreted as a codelength as itself in continuous cases .",
    "as is well known , however , a difference @xmath150 can be exactly interpreted as a codelength difference by way of quantization .",
    "see section iii of @xcite for details .",
    "this indicates that both the redundancy interpretation of the fist term of ( [ finriskbound ] ) and the regret interpretation of the ( negative ) second term in the left side of the inequality in the first line of ( [ regretbound ] ) are justified by the codelength validity .",
    "note that , however , the @xmath127-risk validity does not imply the codelength validity and vice versa in general .",
    "we discuss about the conditional expectation in the risk bound ( [ finriskbound ] ) .",
    "this conditional expectation seems to be hard to be replaced with the usual ( unconditional ) expectation .",
    "the main difficulty arises from the unboundedness of the loss function .",
    "indeed , we can immediately show a similar risk bound with unconditional expectation for bounded loss functions .",
    "as an example , let us consider a class of divergence , called @xmath151-divergence @xcite @xmath152 the @xmath151-divergence approaches kl divergence as @xmath153 @xcite .",
    "more exactly , @xmath154 we also note that the @xmath151-divergence with @xmath155 is four times the squared hellinger distance @xmath156 which has been studied and used in statistics for a long time .",
    "we focus here on the following two properties of @xmath151-divergence :    * the @xmath151-divergence is always bounded : @xmath157 \\label{boundedalpha}\\ ] ] for any @xmath45 and @xmath158 . *",
    "the @xmath151-divergence is bounded by the rnyi divergence as @xmath159 for any @xmath45 and @xmath158 .",
    "see @xcite for its proof .    as a corollary of theorem [ mytheorem1 ]",
    ", we obtain the following risk bound .",
    "[ coro1 ] let @xmath126 be arbitrary real numbers in @xmath51 . define a function @xmath160 . for any @xmath161 , if @xmath72 is @xmath127-risk valid for @xmath162 and @xmath65 is a sub - probability distribution , @xmath163&\\le &     \\frac{1}{\\lambda(\\alpha ) }     e_{\\bar{p}_*}\\left [       \\log \\frac{p_*(y^n|x^n)}{{p_{2}}(y^n|x^n ) }      \\right]\\\\   & & + \\frac{{p_{\\epsilon}^{n}}}{\\lambda(\\alpha)\\beta}\\log \\frac{1}{{p_{\\epsilon}^{n } } }    + \\frac{(1-{p_{\\epsilon}^{n}})}{\\lambda(\\alpha)(\\lambda(\\alpha)+\\alpha)},\\end{aligned}\\ ] ] in particular , taking @xmath164 yields the tightest bound @xmath163&\\le &     \\frac{1}{\\lambda(\\alpha ) }     e_{\\bar{p}_*}\\left [       \\log \\frac{p_*(y^n|x^n)}{{p_{2}}(y^n|x^n ) }      \\right]\\nonumber \\\\   & & + \\frac{{p_{\\epsilon}^{n}}}{\\lambda(\\alpha)(\\lambda(\\alpha)+\\alpha)}\\log \\frac{1}{{p_{\\epsilon}^{n}}}\\nonumber\\\\ & & + \\frac{(1-{p_{\\epsilon}^{n}})}{\\lambda(\\alpha)(\\lambda(\\alpha)+\\alpha)}.\\label{alphabound}\\end{aligned}\\ ] ]    its proof will be described in section [ coro1proof ] .",
    "though it is not so obvious when the condition `` @xmath65 is a sub - probability distribution '' is satisfied , we remark that the codelength validity of @xmath72 is its simple sufficient condition . the second and",
    "the third terms of the right side vanish as @xmath144 due to the property of the typical set .",
    "the boundedness of loss function is indispensable for the proof . on the other hand",
    ", it seems to be impossible to bound the risk for unbounded loss functions .",
    "our remedy for this issue is the risk evaluation based on the conditional expectation on the typical set .",
    "because @xmath20 lies out of @xmath119 with small probability , the conditional expectation is likely to capture the expectation of almost all cases . in spite of this fact ,",
    "if one wants to remove the unnatural conditional expectation , theorem [ mytheorem2 ] offers a more satisfactory bound .",
    "note that the right side of ( [ regretbound ] ) also approaches to zero as @xmath144 .",
    "we remark the relationship of our result with kl divergence @xmath165 .",
    "because of ( [ renyikl ] ) or ( [ alphakl ] ) , it seems to be possible to obtain a risk bound with kl divergence .",
    "however , it is impossible because taking @xmath166 in ( [ finriskbound ] ) or @xmath167 in ( [ alphabound ] ) makes the bounds diverge to the infinity .",
    "that is , we can not derive a risk bound for the risk with kl divergence by bc theory , though we can do it for the rnyi divergence and the @xmath151-divergence .",
    "it sounds somewhat strange because kl divergence seems to be related the most to the notion of the mdl principle because it has a clear information theoretical interpretation .",
    "this issue originates from the original bc theory and has been casted as an open problem for a long time .",
    "finally , we remark that the effectiveness of our proposal in real situations depends on whether we can show the risk validity of the target penalty and derive a sufficiently small bound for @xmath168 and @xmath169 .",
    "actually , much effort is required to realize them for lasso .      in this section ,",
    "we apply the approach in the previous section to lasso and derive new risk and regret bounds . in a setting of lasso",
    ", training data @xmath170 obey a usual regression model @xmath171 for @xmath172 , where @xmath173 is a true parameter and @xmath174 is a gaussian noise having zero mean and a known variance @xmath175 . by introducing @xmath176 , @xmath177 and an @xmath178 matrix @xmath179^t$ ]",
    ", we have a vector / matrix expression of the regression model @xmath180 .",
    "the parameter space @xmath24 is @xmath181 .",
    "the dimension @xmath1 of parameter @xmath94 can be greater than @xmath0 .",
    "the lasso estimator is defined by @xmath182 where @xmath109 is a positive real number ( regularization coefficient ) .",
    "note that the weighted @xmath106 norm is used in ( [ lassoestimator ] ) , though the original lasso was defined with the usual @xmath106 norm in @xcite . as explained in section [ bctheory ]",
    ", @xmath67 corresponds to the usual lasso with ` column normalization ' .",
    "when @xmath20 is gaussian with zero mean , we can derive a risk valid weighted @xmath106 penalty by choosing an appropriate typical set .",
    "[ erv4lasso ] for any @xmath183 , define @xmath184 where @xmath185 is a gaussian distribution with a mean vector @xmath186 and a covariance matrix @xmath187 .",
    "here , @xmath188 denotes the @xmath11th diagonal element of @xmath187 and @xmath13 denotes the @xmath11th element of @xmath189 .",
    "assume a linear regression setting : @xmath190 let @xmath50 be a real number in @xmath51 and @xmath70 be a real number in @xmath71 $ ] .",
    "the weighted @xmath106 penalty @xmath191 is @xmath127-risk valid for @xmath136 if @xmath192",
    "we describe its proof in section [ erv4lassoproof ] .",
    "the derivation is much more complicated and requires more techniques , compared to the fixed design case in @xcite .",
    "this is because the rnyi divergence is a usual mean square error ( mse ) in the fixed design case , while it is not in the random design case in general .",
    "in addition , it is important for the risk bound derivation to choose an appropriate typical set in a sense that we can show that @xmath125 approaches to one sufficiently fast and we can also show the @xmath127-risk validity of the target penalty with the chosen typical set . in case of lasso with normal design ,",
    "the typical set @xmath119 defined in ( [ tset ] ) satisfies such properties .",
    "let us compare the coefficient of the risk valid weighted @xmath106 penalty with the fixed design case in @xcite .",
    "they showed that the weighted @xmath106 norm satisfying @xmath193 is risk valid in the fixed design case .",
    "the condition for @xmath110 is the same , while the condition for @xmath109 in ( [ rvcondition ] ) is more strict than that of the fixed design case .",
    "we compare them by taking @xmath194 ( the tightest choice ) and @xmath195 in ( [ rvcondition ] ) because @xmath127 can be negligibly small for sufficiently large @xmath0 .",
    "the minimum @xmath109 for the risk validity in the random design case is @xmath196 times that for the fixed design case .",
    "hence , the smallest value of regularization coefficient @xmath109 for which the risk bound holds in the random design is always larger than that of the fixed design case for any @xmath40 but its extent is not so large unless @xmath70 is extremely close to @xmath197 ( see fig .",
    "[ compfixed ] ) .     against @xmath70.,width=259 ]    next , we show that @xmath125 exponentially approaches to one as @xmath0 increases .",
    "[ expbound ] suppose that @xmath198 independently . for any @xmath183",
    ", @xmath199    see section [ expboundproof ] for its proof . in the lasso case",
    ", it is often postulated that @xmath1 is much greater than @xmath0 .",
    "due to lemma [ expbound ] , @xmath169 is @xmath200 , which also implies that the second term in ( [ finriskbound ] ) can be negligibly small even if @xmath2 . in this sense ,",
    "the exponential bound is important for lasso . combining lemmas [ erv4lasso ] and [ expbound ] with theorems [ mytheorem1 ] and [ mytheorem2 ] , we obtain the following theorem .",
    "[ theorem4lasso ] for any @xmath183 , define @xmath201 assume a linear regression setting : @xmath190 let @xmath50 be a real number in @xmath51 .",
    "for any @xmath58 $ ] , if @xmath202 the lasso estimator @xmath203 in ( [ lassoestimator ] ) has a risk bound @xmath204\\le } \\nonumber\\\\ & & \\hskip-7mm{e^{n}_{\\epsilon}}\\biggl[\\inf_{\\theta\\in    \\theta}\\bigg\\{\\frac{\\left(\\|y - x\\theta\\|_2 ^ 2-\\|y - x\\theta^*\\|_2 ^ 2\\right)}{2n\\sigma^2 }    + \\mu_1\\|\\theta\\|_{w,1}+\\mu_2\\bigg\\}\\biggr]\\nonumber\\\\   & & \\hskip-7mm-\\frac{p\\log\\left(1\\!-\\!2\\exp\\left(-\\frac{n}{2}(\\epsilon-\\log(1+\\epsilon))\\right)\\right)}{n\\beta},\\label{riskboundlasso}\\end{aligned}\\ ] ] and a regret bound @xmath205 with probability at least @xmath206 which is bounded below by @xmath207 with @xmath208 .    since @xmath20 and @xmath21 are i.i.d .",
    "now , @xmath209 .",
    "hence , we presented the risk bound as a single - sample version in ( [ riskboundlasso ] ) by dividing the both sides by @xmath0 . finally , we remark that the following interesting fact holds for the lasso case .",
    "[ cv4lasso ] assume a linear regression setting : @xmath190 if @xmath109 and @xmath110 satisfy ( [ rvcondition ] ) , then the weighted @xmath106 norm @xmath210 is codelength valid .",
    "that is , the weighted @xmath106 penalties derived in lemma [ erv4lasso ] are not only @xmath127-risk valid but also codelength valid .",
    "its proof will be described in section [ proofcv4lasso ] . by this fact , the redundancy and regret interpretation of the main terms in ( [ riskboundlasso ] ) and ( [ regretboundlasso ] ) are justified .",
    "it also indicates that we can obtain the unconditional risk bound with respect to @xmath151-divergence for those weighted @xmath106 penalties by corollary [ coro1 ] without any additional condition .",
    "we give all proofs to the theorems , the lemmas and the corollary in the previous section .      here , we prove our main theorem .",
    "the proof proceeds along with the same line as @xcite though some modifications are necessary .",
    "define @xmath211 by the @xmath127-risk validity , we obtain @xmath212}\\nonumber\\\\ & \\hspace{-9mm}\\le & \\hspace{-5mm}{e^{n}_{\\epsilon}}\\big[\\exp\\big(\\beta\\max_{{\\tilde{\\theta}}\\in   { \\widetilde{\\theta}}}\\big\\{f_{\\lambda}^{{\\tilde{\\theta}}}(x^n , y^n)-{\\tilde{l}}({\\tilde{\\theta}}|q_*)\\big\\}\\big)\\big]\\nonumber\\\\ & \\hspace{-9mm}\\le & \\hspace{-5mm}\\sum_{{\\tilde{\\theta}}\\in { \\widetilde{\\theta}}(q_*)}{e^{n}_{\\epsilon}}\\big[\\exp\\big(\\beta\\big(f_{\\lambda}^{{\\tilde{\\theta}}}(x^n , y^n)-{\\tilde{l}}({\\tilde{\\theta}}|q_*)\\big)\\big)\\big]\\nonumber\\\\ & \\hspace{-9mm}= & \\hspace{-5mm}\\sum_{{\\tilde{\\theta}}\\in   { \\widetilde{\\theta}}(q_*)}\\exp(-\\beta{\\tilde{l}}\\big({\\tilde{\\theta}}|q_*)\\big){e^{n}_{\\epsilon}}\\big[\\exp\\big(\\beta   f_{\\lambda}^{{\\tilde{\\theta}}}(x^n , y^n)\\big)\\big].\\label{intermediate4 }    \\end{aligned}\\ ] ] the following fact is an extension of the key technique of bc theory : @xmath213}\\\\   & = & \\exp\\left(\\beta d^n_{\\lambda}(p_*,p_{{\\tilde{\\theta}}})\\right )   { e^{n}_{\\epsilon}}\\left[\\left(\\frac{p_{{\\tilde{\\theta}}}(y^n|x^n)}{p_*(y^n|x^n)}\\right)^{\\beta}\\right]\\\\   & \\le&\\frac{1}{{p_{\\epsilon}^{n}}}\\exp\\left(\\beta d^n_{\\lambda}(p_*,p_{{\\tilde{\\theta}}})\\right )   e_{\\bar{p } _ * } \\left[\\left(\\frac{p_{{\\tilde{\\theta}}}(y^n|x^n)}{p_*(y^n|x^n)}\\right)^{\\beta}\\right]\\\\   & = & \\frac{1}{{p_{\\epsilon}^{n}}}\\exp\\left(\\beta                  d^n_{\\lambda}(p_*,p_{{\\tilde{\\theta}}})\\right)\\exp\\left(-\\beta                  d_{1-\\beta}^n(p_*,p_{{\\tilde{\\theta}}})\\right )   \\\\   & \\le & \\frac{1}{{p_{\\epsilon}^{n}}}\\exp\\left(\\beta                     d^n_{\\lambda}(p_*,p_{{\\tilde{\\theta}}})\\right)\\exp\\left(-\\beta                     d^n_{\\lambda}(p_*,p_{{\\tilde{\\theta}}})\\right)=\\frac{1}{{p_{\\epsilon}^{n}}}.    \\end{aligned}\\ ] ] the first inequality holds because @xmath214\\ge    { p_{\\epsilon}^{n}}{e^{n}_{\\epsilon}}\\left[a\\right]$ ] for any non - negative random variable @xmath215 .",
    "the second inequality holds because of the monotonically increasing property of @xmath88 in terms of @xmath70 .",
    "thus , the right side of ( [ intermediate4 ] ) is bounded as @xmath216\\\\   & & \\le \\frac{1}{{p_{\\epsilon}^{n } } } \\sum_{{\\tilde{\\theta}}\\in   { \\widetilde{\\theta}}(q_*)}\\exp(-\\beta{\\tilde{l}}\\big({\\tilde{\\theta}}|q_*)\\big)\\le \\frac{1}{{p_{\\epsilon}^{n}}}.\\end{aligned}\\ ] ] hence , we have an important inequality @xmath217.\\label{coroused}\\end{aligned}\\ ] ] applying jensen s inequality to ( [ coroused ] ) , we have @xmath218\\right)\\nonumber \\\\ & \\ge & \\!\\!\\!\\!\\exp\\left({e^{n}_{\\epsilon}}\\left[\\beta\\left(f_{\\lambda}^{\\hat{\\theta}}(x^n , y^n)-{l}(\\hat{\\theta}|x^n)\\right)\\right]\\right).\\nonumber   \\end{aligned}\\ ] ] thus , we have @xmath219.\\end{aligned}\\ ] ] rearranging the terms of this inequality , we have the statement .",
    "it is not necessary to start from scratch .",
    "we reuse the proof of theorem [ mytheorem1 ]",
    ".    we can start from ( [ coroused ] ) . for convenience ,",
    "we define @xmath220 by markov s inequality and ( [ coroused ] ) , @xmath221 hence , we obtain @xmath222 the proof completes by noticing that @xmath223 for any @xmath20 and @xmath21 .      the proof is obtained immediately from theorem [ mytheorem1 ] .",
    "let again @xmath133 denote a conditional expectation with regard to @xmath134 given that @xmath224 .",
    "let further @xmath225 be an indicator function of a set @xmath226 .",
    "the unconditional risk is bounded as @xmath227}\\\\     & & \\hskip-7 mm      = e_{\\bar{p}_*}[i_{{a_{\\epsilon}^{n}}}(x^n){\\mathrsfs{d}}^n_{\\alpha}(p_*,p_{\\hat{\\theta } } ) ]       \\!+\\!e_{\\bar{p}_*}[(1-i_{{a_{\\epsilon}^{n}}}(x^n)){\\mathrsfs{d}}^n_{\\alpha}(p_*,p_{\\hat{\\theta } } ) ]       \\\\     & & \\hskip-7mm\\le { p_{\\epsilon}^{n}}{e^{n}_{\\epsilon}}[{\\mathrsfs{d}}^n_{\\alpha}(p_*,p_{\\hat{\\theta } } ) ]      + ( 1-{p_{\\epsilon}^{n}})\\cdot \\frac{4}{1-\\alpha^2}\\\\     & & \\hskip-7mm\\le \\frac{{p_{\\epsilon}^{n}}}{\\lambda(\\alpha)}{e^{n}_{\\epsilon}}[d^n_{\\lambda(\\alpha)}(p_*,p_{\\hat{\\theta } } ) ]      + \\frac{(1-{p_{\\epsilon}^{n}})}{\\lambda(\\alpha)(\\lambda(\\alpha)+\\alpha)}\\\\     & & \\hskip-7mm\\le \\frac{{p_{\\epsilon}^{n}}}{\\lambda(\\alpha)}\\left({e^{n}_{\\epsilon}}\\log \\frac{p_*(y^n|x^n)}{{p_{2}}(y^n|x^n ) } + \\frac{1}{\\beta } \\log \\frac{1}{{p_{\\epsilon}^{n } } }                             \\right)\\\\     & & + \\frac{(1-{p_{\\epsilon}^{n}})}{\\lambda(\\alpha)(\\lambda(\\alpha)+\\alpha)}\\\\     & & \\hskip-7mm= \\frac{1}{\\lambda(\\alpha)}e_{\\bar{p}_*}\\left [ i_{{a_{\\epsilon}^{n}}}(x^n ) \\log \\frac{p_*(y^n|x^n)}{{p_{2}}(y^n|x^n)}\\right ]     + \\frac{{p_{\\epsilon}^{n}}}{\\lambda(\\alpha)\\beta}\\log \\frac{1}{{p_{\\epsilon}^{n}}}\\\\ & &      + \\frac{(1-{p_{\\epsilon}^{n}})}{\\lambda(\\alpha)(\\lambda(\\alpha)+\\alpha)}\\\\     & & \\hskip-7mm\\le \\frac{1}{\\lambda(\\alpha)}e_{\\bar{p}_*}\\left [                        \\log \\frac{p_*(y^n|x^n)}{{p_{2}}(y^n|x^n)}\\right ]     + \\frac{{p_{\\epsilon}^{n}}}{\\lambda(\\alpha)\\beta}\\log \\frac{1}{{p_{\\epsilon}^{n}}}\\\\     & & + \\frac{(1-{p_{\\epsilon}^{n}})}{\\lambda(\\alpha)(\\lambda(\\alpha)+\\alpha)}.    \\end{aligned}\\ ] ] the first and second inequalities follow from the two properties of @xmath151-divergence in ( [ boundedalpha ] ) and ( [ alpharenyi ] ) respectively .",
    "the third inequality follows from theorem [ mytheorem1 ] because @xmath228 by the assumption .",
    "the last inequality holds because of the following reason . by the decomposition of expectation , we have @xmath229}\\\\ & & \\hskip-7mm= e_{q_*(x^n)}\\bigg [ i_{{a_{\\epsilon}^{n}}}(x^n)e_{p_*(y^n|x^n)}\\bigg [ \\log \\frac{p_*(y^n|x^n)}{{p_{2}}(y^n|x^n)}\\bigg]\\bigg ] .      \\end{aligned}\\ ] ] since @xmath230 is a sub - probability distribution by the assumption , the conditional expectation part is non - negative",
    ". therefore , removing the indicator function @xmath231 can not decrease this quantity .",
    "the final part of the statement follows from the fact that taking @xmath76 makes the bound in ( [ finriskbound ] ) tightest because of the monotonically increasing property of rnyi divergence with regard to @xmath70 .",
    "again , we remark that the sub - probability condition of @xmath65 can be replaced with a sufficient condition `` @xmath232 is codelength valid . '' in addition , the sub - probability condition can be relaxed to @xmath233 under which the bound increases by @xmath234 .      in this section and the next section ,",
    "we prove a series of lemmas , which will be used to derive risk valid penalties for lasso .",
    "first , we show that the rnyi divergence can be understood by defining @xmath235 in lemma [ renyiview ] .",
    "then , their explicit forms in the lasso setting are calculated in lemma [ bqlemma ] .",
    "[ renyiview ] define a probability distribution @xmath235 by @xmath236 where @xmath237 is a normalization constant .",
    "then , the rnyi divergence and its first and second derivatives are written as @xmath238,\\label{firstderivative}\\\\   \\frac{\\partial^2d_{\\lambda}(p_*,p_{\\theta})}{\\partial \\theta\\partial    \\theta^t}\\!\\!&=&\\!\\!-e_{\\bar{p}^{\\lambda}_{\\theta}}\\left [ g_{\\theta}(x , y ) \\right]\\nonumber\\\\ & & -(1-\\lambda){\\rm var}_{\\bar{p}^{\\lambda}_{\\theta}}\\left(s_{\\theta}(y|x)\\right),\\label{hessiangeneral}\\end{aligned}\\ ] ] where @xmath239 denotes a covariance matrix of @xmath215 with respect to @xmath1 and @xmath240    the normalizing constant is rewritten as @xmath241.\\end{aligned}\\ ] ] thus , the rnyi divergence is written as @xmath242 next , we calculate the partial derivative of @xmath243 as @xmath244\\\\     & = &     \\frac{1-\\lambda}{z^{\\lambda}_{\\theta } }    e_{\\bar{p}_*}\\left[\\left(\\frac{p_{\\theta}(y|x)}{p_*(y|x)}\\right)^{1-\\lambda}\\frac{\\partial \\log p_{\\theta}(y|x)}{\\partial                   \\theta }                   \\right]\\\\     & = &     \\frac{1-\\lambda}{z^{\\lambda}_{\\theta } }    \\int q_*(x)p_*(y|x)^{\\lambda}p_{\\theta}(y|x)^{1-\\lambda}s_{\\theta}(y|x)dxdy\\\\     & = &     ( 1-\\lambda)e_{\\bar{p}^{\\lambda}_{\\theta}}[s_{\\theta}(y|x ) ] .",
    "\\end{aligned}\\ ] ] therefore , the first derivative is @xmath245.\\end{aligned}\\ ] ] furthermore , we have @xmath246\\\\      & = & ( 1-\\lambda ) \\left(s_{\\theta}(y|x)-e_{\\bar{p}^{\\lambda}_{\\theta}}[s_{\\theta}(y|x)]\\right ) .",
    "\\end{aligned}\\ ] ] hence , @xmath247\\right)^t\\\\ & &   + \\frac{\\partial                     ^2\\log p_{\\theta}(y|x)}{\\partial \\theta\\partial\\theta^t}\\bigg]\\\\ & \\hskip-4mm=&\\hskip-3mm - e_{\\bar{p}^{\\lambda}_{\\theta}}\\left[\\frac{\\partial                     ^2\\log p_{\\theta}(y|x)}{\\partial                     \\theta\\partial\\theta^t}\\right]-(1-\\lambda)\\\\ & \\hskip-4mm&\\hskip-7mm\\cdot e_{\\bar{p}^{\\lambda}_{\\theta}}\\bigg[\\!\\ ! \\left(s_{\\theta}(y|x)\\!-\\!e_{\\bar{p}^{\\lambda}_{\\theta}}\\left[s_{\\theta}(y|x)\\right]\\right)\\left(s_{\\theta}(y|x)\\!-\\!e_{\\bar{p}^{\\lambda}_{\\theta}}[s_{\\theta}(y|x)]\\right)^t\\!\\bigg]\\\\ & \\hskip-4mm=&\\hskip-3mm - e_{\\bar{p}^{\\lambda}_{\\theta}}\\left[\\frac{\\partial                     ^2\\log p_{\\theta}(y|x)}{\\partial                     \\theta\\partial\\theta^t}\\right]\\!\\!-\\!(1-\\lambda)\\mbox{var}_{\\bar{p}^{\\lambda}_{\\theta}}\\left(s_{\\theta}(y|x)\\right).\\end{aligned}\\ ] ]    [ bqlemma ] let @xmath248 if we assume that @xmath249 ( i.e. , linear regression setting ) , @xmath250\\bar{\\theta},\\nonumber\\\\   \\frac{\\partial^2d_{\\lambda}(p_*,p_{\\theta})}{\\partial \\theta\\partial    \\theta^t }    \\!\\!\\!\\!&=&\\!\\!\\!\\ ! \\frac{\\lambda}{\\sigma^2}e_{q^{\\lambda}_{\\theta}}[xx^t]-\\frac{\\lambda}{\\sigma^2c}{\\rm var}_{q^{\\lambda}_{\\theta}}\\!\\left(xx^t\\bar{\\theta}\\right).\\label{genhessianlasso}\\end{aligned}\\ ] ] if we additionally assume that @xmath251 with a non - singular covariance matrix @xmath187 , @xmath252 where @xmath253    by completing squares , we can rewrite @xmath235 as @xmath254 hence , @xmath255 is @xmath256 .",
    "integrating @xmath148 out , we also have @xmath257 when @xmath258 , @xmath259 since @xmath187 is strictly positive definite by the assumption , @xmath260 is non - singular . hence , by the inverse formula ( lemma [ inverseformula ] in appendix ) , @xmath261 therefore , @xmath262 .",
    "the score function and hessian of @xmath263 are @xmath264 using ( [ firstderivative ] ) , the first derivative is obtained as @xmath265\\\\   & = & -e_{q^{\\lambda}_{\\theta}}\\left[e_{p^{\\lambda}_{\\theta}}[s_{\\theta}(y|x)]\\right]\\\\ & = & -e_{q^{\\lambda}_{\\theta}}\\left[e_{p^{\\lambda}_{\\theta}}\\left[\\frac{1}{\\sigma^2}x(y - x^t\\theta)\\right]\\right]\\\\   & = & -e_{q^{\\lambda}_{\\theta}}\\left[\\frac{1}{\\sigma^2}xx^t(\\theta(\\lambda)-\\theta)\\right]\\\\    & = & \\frac{\\lambda}{\\sigma^2}e_{q^{\\lambda}_{\\theta}}\\left[xx^t\\right]{\\bar{\\theta}}\\end{aligned}\\ ] ] because @xmath266 . when @xmath267 , @xmath268 from ( [ qsigma ] ) , we have @xmath269 which gives ( [ normalfirstderiv ] ) .",
    "though ( [ normalhessianlasso ] ) can be obtained by differentiating ( [ normalfirstderiv ] ) , we derive it by way of ( [ hessiangeneral ] ) here . to calculate the covariance matrix of @xmath270 in terms of @xmath271",
    ", we decompose @xmath270 as @xmath272 note that the covariance of @xmath273 and @xmath274 vanishes since @xmath275}\\\\    & = & e_{q^{\\lambda}_{\\theta}}\\left[xx^t(x^t{\\bar{\\theta}})e_{p^{\\lambda}_{\\theta}}\\left[(y - x^t\\theta(\\lambda))\\right]\\right ]     = 0 .",
    "\\end{aligned}\\ ] ] therefore , we have @xmath276    + \\frac{\\lambda^2}{\\sigma^4}\\mbox{var}_{q^{\\lambda}_{\\theta}}\\left(xx^t{\\bar{\\theta}}\\right)\\\\   & = &    \\frac{1}{\\sigma^2}e_{q^{\\lambda}_{\\theta}}\\left[xx^t\\right ]    + \\frac{\\lambda^2}{\\sigma^4}\\mbox{var}_{q^{\\lambda}_{\\theta}}\\left(xx^t{\\bar{\\theta}}\\right)\\end{aligned}\\ ] ] by ( [ hessiangeneral ] ) combined with ( [ secondderiv ] ) , the hessian of rnyi divergence is calculated as @xmath277-(1-\\lambda)\\left(\\frac{1}{\\sigma^2}e_{q^{\\lambda}_{\\theta}}[xx^t]+\\frac{\\lambda^2}{\\sigma^4}\\mbox{var}_{q^{\\lambda}_{\\theta}}\\left(xx^t\\bar{\\theta}\\right)\\right)\\\\ & = & \\!\\!\\!\\!\\frac{\\lambda}{\\sigma^2}e_{q^{\\lambda}_{\\theta}}[xx^t]-\\frac{\\lambda^2(1-\\lambda)}{\\sigma^4}\\mbox{var}_{q^{\\lambda}_{\\theta}}\\left(xx^t\\bar{\\theta}\\right ) \\\\ & = & \\!\\!\\!\\!\\frac{\\lambda}{\\sigma^2}e_{q^{\\lambda}_{\\theta}}[xx^t]-\\frac{\\lambda}{\\sigma^2c}\\mbox{var}_{q^{\\lambda}_{\\theta}}\\left(xx^t\\bar{\\theta}\\right ) .",
    "\\end{aligned}\\ ] ] when @xmath258 , @xmath278 is calculated as follows .",
    "note that @xmath279 -({\\sigma^{\\lambda}_{\\theta}}{\\bar{\\theta}})({\\sigma^{\\lambda}_{\\theta}}{\\bar{\\theta}})^t.\\end{aligned}\\ ] ] the @xmath280 element of @xmath281 $ ] is calculated as @xmath282 \\!=\\!\\!\\!\\!\\sum_{j_3,j_4=1}^p\\!\\!\\!{\\bar{\\theta}}_{j_3}{\\bar{\\theta}}_{j_4}e_{q^{\\lambda}_{\\theta}}\\left[x_{j_1}x_{j_2}x_{j_3}x_{j_4}\\right],\\end{aligned}\\ ] ] where @xmath283 denotes the @xmath11th element of @xmath7 only here .",
    "thus , we need all the fourth - moments of @xmath284 .",
    "we rewrite @xmath285 as @xmath286 to reduce notation complexity hereafter . by the formula of moments of gaussian distribution , we have @xmath287=s_{j_1j_2}s_{j_3j_4}+s_{j_1j_3}s_{j_2j_4}+s_{j_2j_3}s_{j_1j_4}.\\ ] ]",
    "therefore , the above quantity is calculated as @xmath288}\\\\ & = & \\!\\!\\!\\!\\sum_{j_3,j_4=1}^p{\\bar{\\theta}}_{j_3}{\\bar{\\theta}}_{j_4}(s_{j_1j_2}s_{j_3j_4}+s_{j_1j_3}s_{j_2j_4}+s_{j_2j_3}s_{j_1j_4})\\\\ & = & { \\bar{\\theta}}^ts{\\bar{\\theta}}s_{j_1j_2}+2(s{\\bar{\\theta}})_{j_1}(s{\\bar{\\theta}})_{j_2}.\\end{aligned}\\ ] ] summarizing these as a matrix form , we have @xmath289 & = & ( { \\bar{\\theta}}^ts{\\bar{\\theta}})s+2s{\\bar{\\theta}}(s{\\bar{\\theta}})^t.\\end{aligned}\\ ] ] as a result , @xmath290 is obtained as @xmath291 using ( [ sthetabar ] ) , the first and second terms of ( [ varst ] ) are calculated as @xmath292 combining these , @xmath293      using lemma [ bqlemma ] in section [ bqlemmasec ] , we show that the negative hessian of the rnyi divergence is bounded from above .",
    "[ hessianlemma ] assume that @xmath251 and @xmath249 , where @xmath187 is non - singular . for any @xmath294 , @xmath295 where @xmath296 implies that @xmath297 is positive semi - definite .    by lemma [ bqlemma ]",
    ", we have @xmath298 for any nonzero vector @xmath299 , @xmath300 by cauchy - schwartz inequality .",
    "hence , we have @xmath301 thus , @xmath302 define @xmath303 for @xmath304 . checking the properties of @xmath305 , we have @xmath306 therefore , @xmath307 . as a result , we obtain @xmath308      we are now ready to derive @xmath127-risk valid weighted @xmath106 penalties .    similarly to the rewriting from ( [ rv ] ) to ( [ rv2 ] ) ,",
    "we can rewrite the condition for @xmath127-risk validity as @xmath309 we again write the inside part of the minimum in ( [ erv2 ] ) as @xmath310 . as described in section [ bctheory ]",
    ", the direct minimization of @xmath310 seems to be difficult . instead of evaluating the minimum explicitly",
    ", we borrow a nice randomization technique introduced in @xcite with some modifications .",
    "their key idea is to evaluate not @xmath95 directly but its expectation @xmath311 $ ] with respect to a dexterously randomized @xmath54 because the expectation is larger than the minimum .",
    "let us define @xmath312 , where @xmath313 and @xmath314 .",
    "we quantize @xmath24 as @xmath315 where @xmath316 is a quantization width and @xmath317 is the set of all integers . though @xmath87 depends on @xmath20 in fixed design cases @xcite , we must remove the dependency to satisfy the @xmath127-risk validity as above . for each @xmath94 , @xmath54 is randomized as @xmath318 where @xmath319 and each component of @xmath54 is statistically independent of each other .",
    "its important properties are @xmath320=\\theta,\\quad \\mbox{(unbiasedness)}\\nonumber\\\\   & & e_{{\\tilde{\\theta}}}[|{\\tilde{\\theta}}|]=|\\theta| , \\label{absoluteunbiasedness}\\\\   & & e_{{\\tilde{\\theta}}}[({\\tilde{\\theta}}_j-\\theta_j)({\\tilde{\\theta}}_{j'}-\\theta_{j'})]\\le i(j = j')\\frac{\\delta}{w_j^*}|\\theta_j| ,",
    "\\nonumber \\end{aligned}\\ ] ] where @xmath321 denotes a vector whose @xmath11th component is the absolute value of @xmath322 and similarly for @xmath323 .",
    "using these , we can bound @xmath311 $ ] as follows .",
    "the loss variation part in ( [ erv2 ] ) is the main concern because it is more complicated than squared error of fixed design cases .",
    "let us consider the following taylor expansion @xmath324 where @xmath325 is a vector between @xmath94 and @xmath54 . the first term in the right side of ( [ type1expansion ] )",
    "vanishes after taking expectation with respect to @xmath54 because @xmath326=0 $ ] . as for the second term , we obtain @xmath327 by lemma [ hessianlemma ] .",
    "thus , expectation of the loss variation part with respect to @xmath54 is bounded as @xmath328\\le \\frac{\\delta n\\lambda}{16\\sigma^2}\\|\\theta\\|_{w^*,1}. \\label{elvp}\\ ] ] the codelength validity part in ( [ erv2 ] ) have the same form as that for the fixed design case in its appearance . however , we need to evaluate it again in our setting because both @xmath87 and @xmath56 are different from those in @xcite . the likelihood term is calculated as @xmath329 taking expectation with respect to @xmath54 , we have @xmath330 \\!\\!\\!\\!\\!&=&\\!\\!\\!\\!\\ ! \\frac{n}{2\\sigma^2}e_{{\\tilde{\\theta}}}\\!\\left[{{\\rm tr}}\\!\\left(w^2({\\tilde{\\theta}}-\\theta)({\\tilde{\\theta}}-\\theta)^t\\right)\\right]\\nonumber\\\\ \\!\\!\\!\\!\\!&\\le&\\!\\!\\!\\!\\!\\frac{\\delta n}{2\\sigma^2}\\sum_{j=1}^p\\frac{w_j^2}{w_j^*}|\\theta_j|,\\nonumber\\end{aligned}\\ ] ] where @xmath331 .",
    "we define a codelength function @xmath332 over @xmath333 .",
    "note that @xmath334 satisfies kraft s inequality .",
    "let us define a codelength function on @xmath335 as @xmath336 by this definition , @xmath56 satisfies @xmath50-stronger kraft s inequality and does not depend on @xmath20 but depends on @xmath337 through @xmath338 . by taking expectation with respect to @xmath54",
    ", we have @xmath339=\\frac{\\log 4p}{\\beta\\delta}\\|\\theta\\|_{w^*,1}+\\frac{\\log 2}{\\beta}\\end{aligned}\\ ] ] because of ( [ absoluteunbiasedness ] ) .",
    "thus , the codelength validity part is bounded above by @xmath340 combining with the loss variation part , we obtain an upper bound of @xmath311 $ ] as @xmath341 since @xmath135 , we have @xmath342 thus , we can bound @xmath311 $ ] by the data - dependent weighted @xmath106 norm @xmath343 as @xmath344}\\nonumber\\\\ & \\hskip-3mm\\le & \\hskip-2mm\\frac{\\delta n\\lambda}{16\\sigma^2 } \\frac{\\|\\theta\\|_{w,1}}{\\sqrt{1-\\epsilon } } + \\frac{\\delta n\\sqrt{1+\\epsilon}}{2\\sigma^2}\\sum_{j=1}^p\\frac{w_j^{2}}{w_j}|\\theta_j| + \\frac{\\log 4p}{\\beta\\delta}\\frac{\\|\\theta\\|_{w,1}}{\\sqrt{1-\\epsilon}}\\nonumber\\\\ & & + \\frac{\\log 2}{\\beta}\\nonumber\\\\ & \\hskip-3mm=&\\hskip-2mm\\left (    \\frac{\\delta n}{\\sigma^2 }    \\left(\\frac{\\lambda}{16\\sqrt{1-\\epsilon}}\\!+\\!\\frac{\\sqrt{1+\\epsilon}}{2}\\right)\\!+\\ !    \\frac{\\log",
    "4p}{\\delta \\beta\\sqrt{1-\\epsilon}}\\right ) \\|\\theta\\|_{w,1}\\!+\\!\\frac{\\log 2}{\\beta}.\\nonumber\\end{aligned}\\ ] ] because this holds for any @xmath316 , we can minimize the upper bound with respect to @xmath345 , which completes the proof .",
    "the main difference of the proof from the fixed design case is in the loss variation part . in the fixed design case , the rnyi divergence @xmath346 is convex in terms of @xmath94 . when the rnyi divergence is convex , the negative hessian is negative semi - definite for all @xmath94",
    "hence , the loss variation part is trivially bounded above by zero . on the other hand",
    ", @xmath347 is not convex in terms of @xmath94 .",
    "this can be intuitively seen by deriving the explicit form of @xmath347 instead of checking the positive semi - definiteness of its hessian . from ( [ qcalc ] ) , we have @xmath348 where @xmath349 is the identity matrix of dimension @xmath1 . prof .",
    "a. r. barron suggested in a private discussion that @xmath237 can be simplified more as follows .",
    "let @xmath350 $ ] be an orthogonal matrix such that @xmath351 . using this ,",
    "we have @xmath352 hence , the resultant @xmath237 is obtained as @xmath353 thus , we have a simple expression of the rnyi divergence as @xmath354 from this form , we can easily know that the rnyi divergence is not convex .",
    "when the rnyi divergence is non - convex , it is unclear in general whether and how the loss variation part is bounded above .",
    "this is one of the main reasons why the derivation becomes more difficult than that of the fixed design case .",
    "we also mention an alternative proof of lemma [ erv4lasso ] based on ( [ andrewsuggestion ] ) .",
    "we provided lemma [ renyiview ] to calculate hessian of the rnyi divergence .",
    "however , the above simple expression of the rnyi divergence is somewhat easier to differentiate , while the expression based on ( [ oldrenyi ] ) is somewhat hard to do it .",
    "therefore , we can twice differentiate the above rnyi divergence directly in order to obtain hessian instead of lemma [ bqlemma ] in our gaussian setting . however , there is no guarantee that such a simplification is always possible in general setting . in our proof , we tried to give a somewhat systematic way which is easily applicable to other settings to some extent .",
    "suppose now , for example , we are aim at deriving @xmath127-risk valid @xmath106 penalties for lasso when @xmath337 is subject to non - gaussian distribution . by ( [ genhessianlasso ] ) in lemma [ bqlemma ]",
    ", it suffices only to bound @xmath290 in the sense of positive semi - definiteness because @xmath355 $ ] is negative semi - definite . in general",
    ", it seemingly depends on a situation which is better , the direct differential or using ( [ genhessianlasso ] ) . in our gaussian setting , we imagine that the easiest way to calculate hessian for most readers is to calculate the first derivative by the formula ( [ firstderivative ] ) and then to differentiate it directly , though this depends on readers background knowledge . for other settings",
    ", we believe that providing lemmas [ renyiview ] and [ bqlemma ] would be useful in some cases .      here",
    ", we show that @xmath20 distributes out of @xmath119 with exponentially small probability with respect to @xmath0 .",
    "the typical set @xmath119 can be decomposed covariate - wise as @xmath356 where @xmath357 and the above @xmath358 denotes a direct product of sets . from its definition ,",
    "@xmath359 is subject to a gamma distribution @xmath360 when @xmath361 .",
    "we write @xmath359 as @xmath362 and @xmath363 as @xmath364 ( the index @xmath11 is dropped for legibility ) .",
    "we rewrite the gamma distribution @xmath365 in the form of exponential family : @xmath366 where @xmath367 that is , @xmath368 is a natural parameter and @xmath362 is a sufficient statistic , so that the expectation parameter @xmath369 is @xmath370 $ ] .",
    "the relationship between the variance parameter @xmath364 and natural / expectation parameters are summarized as @xmath371 for exponential families , there is a useful sanov - type inequality ( lemma [ exp2bound ] in appendix ) . using this lemma",
    ", we can bound @xmath372 as follows . for this purpose",
    ", it suffices to bound the probability of the event @xmath373 .",
    "when @xmath374 and @xmath375 , @xmath376 where @xmath377 is the single data version of the kl - divergence defined by ( [ kldiv ] ) .",
    "it is easy to see that @xmath378 for any @xmath379 . by lemma [ exp2bound ] , we obtain @xmath380 hence @xmath125 can be bounded below as @xmath381 the last inequality follows from @xmath382 for any @xmath383 $ ] and @xmath384 . to simplify the bound , we can do more .",
    "the maximum positive real number @xmath385 such that , for any @xmath386 $ ] , @xmath387 is @xmath388 .",
    "then , the maximum integer @xmath389 such that @xmath390 is @xmath391 , which gives the last inequality in the statement .      we can prove this lemma by checking the proof of lemma [ erv4lasso ] .",
    "let @xmath392 similarly to the rewriting from ( [ rvrd ] ) to ( [ rv2 ] ) , we can restate the codelength validity condition for @xmath393 as  there exist a quantize subset @xmath25 and a model description length @xmath31 satisfying the usual kraft s inequality , such that @xmath394 recall that ( [ rvcondition ] ) is a sufficient condition for the @xmath127-risk validity of @xmath395 , in fact , it was derived as a sufficient condition for the proposition that @xmath393 bounds from above @xmath396= & \\!\\!\\!\\!\\underbrace{e_{{\\tilde{\\theta}}}\\left [ d^n_{\\lambda}(p_*,p_{\\theta})- d^n_{\\lambda}(p_*,p_{{\\tilde{\\theta } } } )             \\right ] } _ { \\mbox{(i ) } } \\nonumber\\\\         & & \\!\\!\\!\\!+ \\underbrace { e_{{\\tilde{\\theta}}}\\left [ \\log \\frac{p_{\\theta}(y^n|v^n)}{p_{{\\tilde{\\theta}}}(y^n|v^n ) } + { \\tilde{l}}({\\tilde{\\theta}}|q_*)\\right]}_{\\mbox{(ii ) } } \\label{eh }        \\end{aligned}\\ ] ] for any @xmath397 , @xmath398 , @xmath399 , @xmath23 , where @xmath54 was randomized on @xmath335 and @xmath400 were defined by ( [ quantizationrd ] ) and ( [ ctpen4lasso ] ) , in particular , @xmath131 satisfies @xmath50-stronger kraft s inequality .",
    "recall that @xmath310 is the inside part of the minimum in ( [ erv2 ] ) . here , we used @xmath401 instead of @xmath20 so as to discriminate from the above fixed @xmath20 . to derive the sufficient condition",
    ", we obtained upper bounds on the terms ( i ) and ( ii ) of ( [ eh ] ) respectively , and shown that @xmath402 with @xmath403 is not less than the sum of both upper bounds if ( [ rvcondition ] ) is satisfied .",
    "a point is that the upper bound on the term ( i ) we derived is a non - negative function of @xmath94 ( see ( [ elvp ] ) ) .",
    "hence , if @xmath403 and ( [ rvcondition ] ) hold , @xmath402 is an upper bound on the term ( ii ) , which is not less than @xmath404    now , assume ( [ rvcondition ] ) and let us take @xmath120 given @xmath20 , such that @xmath188 is equal to @xmath405 for all @xmath11 .",
    "then we have @xmath406 , which implies @xmath407 since @xmath408 is determined by @xmath20 and @xmath409 satisfies kraft s inequality , the codelength validity condition holds for @xmath395 .",
    "we investigate behavior of the regret bound ( [ regretboundlasso ] ) . in the regret bound ,",
    "we take @xmath194 with which the regret bound becomes tightest .",
    "furthermore , @xmath109 and @xmath110 are taken as their smallest values in ( [ rvcondition ] ) .",
    "as described before , we can not obtain the exact bound for kl divergence which gives the most famous loss function , the mean square error ( mse ) , in this setting .",
    "this is because the regret bound diverges to the infinity as @xmath43 unless @xmath0 is accordingly large enough .",
    "that is , we can obtain only the approximate evaluation of the mse .",
    "the precision of that approximation depends on the sample size @xmath0 .",
    "we do not employ the mse here but another famous loss function , squared hellinger distance @xmath410 ( for a single data ) .",
    "the hellinger distance was defined in ( [ hellinger ] ) as @xmath0 sample version ( i.e. , @xmath411 ) .",
    "we can obtain a regret bound for @xmath412 by ( [ regretboundlasso ] ) because two times the squared hellinger distance @xmath413 is bounded by bhattacharyya divergence ( @xmath414 ) in ( [ bhattacharyya ] ) through the relationship ( [ alpharenyi ] ) .",
    "we set @xmath415 , @xmath416 and @xmath417 to mimic a typical situation of sparse learning .",
    "the lasso estimator is calculated by a proximal gradient method @xcite . to make the regret bound tight , we take @xmath418 that is close to zero compared to the main term ( regret ) . for this @xmath419 , fig .",
    "[ figprob ] shows the plot of ( [ lowerboundp ] ) against @xmath127 .",
    "we should choose the smallest @xmath127 as long as the regret bound holds with large probability .",
    "our choice is @xmath420 at which the value of ( [ lowerboundp ] ) is @xmath421 .",
    "we show the results of two cases in figs .",
    "[ fig1]-[fig3 ] .",
    "these plots express the value of @xmath414 , @xmath413 and the regret bound that were obtained in a hundred of repetitions with different signal - to - noise ratios ( snr ) @xmath422/\\sigma^2 $ ] ( that is , different @xmath175 ) . from these figures and other experiments",
    ", we observed that @xmath423 almost always equaled @xmath414 ( they were almost overlapped ) .",
    "as the sn ratio got larger , then the regret bound became looser , for example , about six times larger than @xmath423 when snr is @xmath424 .",
    "one of the reasons is that the @xmath127-risk validity condition is too strict to bound the loss function when snr is high .",
    "hence , a possible way to improve the risk bound is to restrict the parameter space @xmath24 used in @xmath127-risk validity to a range of @xmath67 , which is expected to be considerably narrower than @xmath24 due to high snr .",
    "in contrast , the regret bound is tight when snr is 0.5 in fig .",
    "finally , we remark that the regret bound dominated the rnyi divergence over all trials , though the regret bound is probabilistic .",
    "one of the reason is the looseness of the lower bound @xmath425 of the probability for the regret bound to hold .",
    "this suggests that @xmath127 can be reduced more if we can derive its tighter bound .    ) against @xmath183 when @xmath426 and @xmath418 .",
    "the dotted vertical line indicates @xmath420.,width=259 ]     ( bhattacharyya div . ) , @xmath423 ( hellinger dist . ) and the regret bound with @xmath418 in case that snr=1.5.,width=259 ]     ( bhattacharyya div . ) , @xmath423 ( hellinger dist . ) and the regret bound with @xmath418 in case that snr=10.,width=259 ]     ( bhattacharyya div . ) , @xmath423 ( hellinger dist . ) and the regret bound with @xmath418 in case that snr=0.5.,width=259 ]",
    "we proposed a way to extend the original bc theory to supervised learning by using a typical set . similarly to the original bc theory , our extension also gives a mathematical justification of the mdl principle for supervised learning . as an application",
    ", we derived a new risk and regret bounds of lasso .",
    "the derived bounds still retains various advantages of the original bc theory .",
    "in particular , it requires considerably few assumptions .",
    "our next challenges are applying our proposal to non - normal cases for lasso and other machine learning methods .",
    "[ sanovsection ] the following lemma is a special case of the result in @xcite .",
    "below , we give a simpler proof . in the lemma",
    ", we denote a random variable of one dimension by @xmath427 and denote its corresponding one dimensional variable by @xmath7 .",
    "[ exp2bound ] let @xmath428 where @xmath7 and @xmath94 are of one dimension .",
    "then , @xmath429 where @xmath430 is the expectation parameter corresponding to the natural parameter @xmath94 and similarly for @xmath431 .",
    "the symbol @xmath377 denotes the single sample version of the kl - divergence defined by ( [ kldiv ] ) .    in this setting",
    ", the kl divergence is calculated as @xmath432=(\\theta-\\theta')\\eta-\\psi(\\theta)+\\psi(\\theta').\\ ] ] assume @xmath433 .",
    "because of the monotonicity of natural parameter and expectation parameter of exponential family , @xmath434 by markov s inequality , we have @xmath435}{\\exp\\left((\\theta'-\\theta)\\eta'\\right)}\\\\ & = & \\int \\exp(\\theta x-\\psi(\\theta))\\exp((\\theta'-\\theta)x)dx\\cdot \\exp(-(\\theta'-\\theta)\\eta')\\\\ & = & \\int \\exp(\\theta ' x-\\psi(\\theta))dx\\cdot \\exp(-(\\theta'-\\theta)\\eta')\\\\ & = & \\exp(\\psi(\\theta'))\\exp(-\\psi(\\theta))\\cdot \\exp(-(\\theta'-\\theta)\\eta')\\\\ & = & \\exp(-\\left((\\theta'-\\theta)\\eta'-\\psi(\\theta')+\\psi(\\theta)\\right)).\\end{aligned}\\ ] ] the other inequality can also be proved in the same way .",
    "[ inverseformula ] let @xmath215 be a non - singular @xmath436 matrix .",
    "if @xmath437 and @xmath438 are both @xmath439 vectors and @xmath440 is non - singular , then @xmath441    see , for example , corollary 1.7.2 in @xcite for its proof .",
    "we thank professor andrew barron for fruitful discussion .",
    "the form of rnyi divergence ( [ andrewsuggestion ] ) is the result of simplification suggested by him .",
    "furthermore , we learned the simple proof of lemma [ exp2bound ] from him .",
    "we also thank mr .",
    "yushin toyokihara for his support ."
  ],
  "abstract_text": [
    "<S> the minimum description length ( mdl ) principle in supervised learning is studied . </S>",
    "<S> one of the most important theories for the mdl principle is barron and cover s theory ( bc theory ) , which gives a mathematical justification of the mdl principle . </S>",
    "<S> the original bc theory , however , can be applied to supervised learning only approximately and limitedly . though barron et al . </S>",
    "<S> recently succeeded in removing a similar approximation in case of unsupervised learning , their idea can not be essentially applied to supervised learning in general . to overcome this issue , an extension of bc theory to supervised learning </S>",
    "<S> is proposed . </S>",
    "<S> the derived risk bound has several advantages inherited from the original bc theory . </S>",
    "<S> first , the risk bound holds for finite sample size . </S>",
    "<S> second , it requires remarkably few assumptions . </S>",
    "<S> third , the risk bound has a form of redundancy of the two - stage code for the mdl procedure . </S>",
    "<S> hence , the proposed extension gives a mathematical justification of the mdl principle to supervised learning like the original bc theory . </S>",
    "<S> as an important example of application , new risk and ( probabilistic ) regret bounds of lasso with random design are derived . </S>",
    "<S> the derived risk bound holds for any finite sample size @xmath0 and feature number @xmath1 even if @xmath2 without boundedness of features in contrast to the past work . </S>",
    "<S> behavior of the regret bound is investigated by numerical simulations . </S>",
    "<S> we believe that this is the first extension of bc theory to general supervised learning with random design without approximation .    </S>",
    "<S> lasso , risk bound , random design , mdl principle </S>"
  ]
}