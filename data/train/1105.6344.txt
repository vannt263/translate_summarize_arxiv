{
  "article_text": [
    "in astronomy and cosmology one is often challenged by the complexity of the relationship between the physical parameters to be estimated and the distribution of the observed data . in a typical application the mapping from the _ parameter space _ to the _ observed data space _ is built on sophisticated physical theory or simulation models or both .",
    "these scientifically motivated models are growing ever more complex and nuanced as a result of both increased computing power and improved understanding of the underlying physical processes . at the same time , data are progressively more abundant and of higher dimensionality as a result of more sophisticated detectors and greater data collection capacity .",
    "these challenges create opportunities for statisticians to make a large impact in these fields .    in this paper",
    "we address one such challenge in the field of astrophysics .",
    "informally , the setup can be described as follows .",
    "the observed data vector from each source is appropriately modeled as a constrained linear combination of a set of physical components , plus some nonlinear distortion and noise to account for observational effects .",
    "call this the signal model .",
    "one also has a computer model capable of generating a dictionary of physical components under different settings of the physical parameters . using this dictionary of components",
    ", the signal model can be fitted to observed data .",
    "the parameters of interest  which we will refer to as _ target _",
    "parameters  are , however , not the parameters explicitly appearing in the signal model , but are derived from them .",
    "the target parameters capture the physical essence of each object under study .",
    "our goal is to find accurate estimates of these parameters given observed data and theoretic models of the basic components .",
    "see ( [ eqngenmodel ] ) for the formal problem statement .",
    "our proposed methods choose small sets of prototypes from a large dictionary of physical components to fit the signal model to the observed data from each object of interest .",
    "even though the data are truly generated as combinations of curves from a continuous ( or fine ) grid of parameters , we obtain more accurate maximum likelihood estimates of the target parameters by using a smaller , principled choice of prototype basis .",
    "this result is partially due to the fact that maximum likelihood estimation ( mle ) often fails when the parameters take values in an infinite - dimensional space . in @xcite",
    ", the authors suggest salvaging mle for continuous parameter spaces by a method of sieves [ @xcite ] , where one maximizes over a constrained subspace of the parameter space and then relaxes the constraint as the sample size grows .",
    "quantization is one such method for constraining the parameter space , and the optimal number of quanta or prototypes is then determined by the sample size ; see @xcite for an example of quantized density estimation with mle .",
    "our approach is based on similar ideas but our final goal is parameter estimation rather than density estimation .",
    "although we do not directly tie the number of quanta to the sample size , we do observe a similar phenomenon : in the face of limited , noisy data , gains can be made by reducing the parameter space further prior to finding the mle . by deriving a small set of prototypes that effectively cover the support of the signal model , we obtain a marked decrease in the variance of the final parameter estimates , and only a slight increase in bias . furthermore , by choosing a smaller set of prototypes , the fitting procedure becomes computationally tractable .",
    "our principal motivation for developing this methodology is to understand the process of star formation in galaxies .",
    "specifically , researchers in this field seek to improve the physical models of galaxy evolution so that they more accurately explain the observed patterns of galaxy star formation history ( sfh ) in the universe .",
    "the principal idea is that each galaxy consists of a  mixture of subpopulations of stars with different ages and compositions . by estimating the proportion of each constituent stellar subpopulation present",
    ", we can reconstruct the star formation rate and composition as a function of time , throughout the life of that galaxy .",
    "this is the approach of _ galaxy population synthesis _",
    "[ @xcite , @xcite , @xcite ] , whereby the observed data from each galaxy are modeled as linear combinations of a set of idealized simple stellar populations ( ssps , groups of stars having the same age and composition ) plus some parametrized , nonlinear distortions . equation ( [ cfmodel ] ) shows one such galaxy population synthesis model .",
    "the fitted parameters from this signal model allow us to estimate the sfh target parameters of each galaxy , which are simple functions of the parameters in this model .",
    "astrophysicists can use the estimated sfhs of a large sample of galaxies to better understand the physics governing the evolution of galaxies and to constrain cosmological models .",
    "this modeling approach has produced compelling estimates of cosmological parameters such as the cosmic star formation rate , the evolution of stellar mass density , and the stellar initial mass function , which describes the initial distribution of stellar masses in a population of stars [ see @xcite and @xcite for examples of such results ] .",
    "sfh target parameter estimates from galaxy population synthesis are highly dependent on the choice of ssp basis .",
    "astronomers have the ability to theoretically model simple stellar populations from fine parameter grids , but much care needs to be taken to determine an appropriate basis to achieve accurate sfh parameter estimates .",
    "in @xcite it was shown that better parameter estimates are achieved by exploiting the underlying geometry of the ssp disribution than by using ssps from regular parameter grids . in this paper",
    "we will further explore this problem .",
    "our main contributions are the following :    to introduce prototyping as an approach to estimating parameters derived from the signal model parameters and to show the effectiveness of quantizing the vector space or support of the model data ,    to demonstrate that sparse coding does not work as a prototyping method without the appropriate constraints and that constrained sparse coding methods do not perform well for target parameter estimation , and    to work out the details of the star formation history estimation problem and obtain more accurate estimates of sfh for galaxies than the approaches used in the astronomy and statistics literature .",
    "there are several other fields where observed data are commonly modeled as linear combinations of dictionaries of theoretical or idealized components ( plus some parametrized distortions ) , for example : _ remote sensing _ , both of the earth [ @xcite ] and other planets [ @xcite ] , where the observed spectrum of each area of land is modeled as a mixture of pure spectral `` endmembers ; '' _ computer vision and computational anatomy _ [ @xcite , @xcite ] , where data are modeled as mixtures of deformable templates ; and _ compositional modeling of asteroids _",
    "[ @xcite , @xcite ] , where observed asteroids are described as mixtures of pure minerals to determine their composition .",
    "these applications can benefit from the methodology proposed here .",
    "a  related and important problem in theoretical physics is _ gravitational wave modeling _ [ @xcite , @xcite ] , where large template banks are used to estimate the parameters of observed compact binary systems ( such as neutron stars and black holes ) . in this particular problem ,",
    "one is interpolating between runs of the computer model , and not modeling the observed data as superpositions of the model output , as we do in this paper .",
    "there are strong connections between this work and ongoing research into the design of _ computer experiments _ ; see @xcite and @xcite for an overview of the topic . the fundamental challenge in that setting is to adequately characterize the relationship between input parameters to a simulation model and the output that the model produces .",
    "the term `` simulation model '' should be interpreted broadly to mean computer code which produces output as a function of input parameters ; in situations of interest , this code is a computationally - intensive model for a complex physical phenomenom .",
    "hence , one must carefully `` design the computer experiment '' by choosing the set of input parameter vectors for which runs of the simulator will be made .",
    "regression methods are then used to approximate the output of the simulator for other values of the input parameters . as is the case in our application , the ultimate objective is to compare observed data with the simulated output to constrain these input parameters .",
    "research has largely focused on situations in which the output of interest is scalar , but there has been recent work on functional outputs ; see , for instance , @xcite .",
    "here , we have the same goal of parameter estimation , but instead of seeking to reduce the number of times the computer code must be run , we instead work with the scientific details of the problem at hand and simplify the code in a principled manner to reduce the computational burden .      to elucidate the challenges of this type of modeling problem , we begin with a simple example .",
    "imagine our dictionary consists of @xmath0 gaussian functions generated over a fine grid of @xmath1 , such as those in figure  [ fignormcurv ] .",
    "we observe a set of objects , each producing data from a  different function constructed as a sparse linear combination of the dictionary of gaussian functions .",
    "the data from each object are sampled across a fixed grid with additive i.i.d .",
    "gaussian noise .",
    "the component weights are constrained to be nonnegative and sum to  1 , ensuring that all parameters are physically - plausible ( e.g. , @xmath2 ) .    .",
    "simulated data are generated as noisy random sparse linear combinations of these curves .",
    "as @xmath1 increases , it becomes more difficult to distinguish the curves , especially in the presence of noise .",
    "a basis of prototypes for estimation of the target parameter , @xmath3 , should include a higher proportion of low-@xmath1 gaussian curves . ]",
    "our ultimate goal is to estimate a set of target parameters for each observed data point . in this example , our target is @xmath3 , the weighted average @xmath1 of the component gaussian curves of each observed data vector . to this end , we model each observed curve as a linear superposition of a set of prototypes and use the estimated prototype weights to estimate @xmath3 .    if our goal were to reconstruct each data point with as small of error as possible , then a prototyping approach that samples along the boundary of the convex hull of the dictionary of gaussian functions ( such as archetypal analysis ,",
    "see section  [ secaa ] ) would be optimal . in this paper ,",
    "the goal is to achieve small errors in the _ target parameter estimates_. a common approach for this problem is to sample prototypes uniformly over the parameter space .",
    "however , this often leads to the inclusion of many prototypes with nearly identical curves . consider the gaussian curve example : for high values of @xmath1 , the curves do not change considerably with respect to changes in @xmath1 . under the presence of noise , curves with large @xmath1 are not distinguishable .",
    "we are better off including a higher proportion of prototypes in the low-@xmath1 range , where curves change more with respect to changes in @xmath1 .",
    "this intuition leads us to a different approach : choose prototypes by quantizing the space of _ curves _",
    "( see section [ ssquant ] ) .",
    "we show in section [ secnorm ] that a  method that selects prototypes by quantizing the vector space of theoretical components outperforms the method of choosing prototypes from a  uniform grid of @xmath1 in the estimation of @xmath3 ( see figure [ figsigmse ] ) .",
    "additionally , judicious selection of a reduced prototype basis is an effective regularization of an estimation problem that is subject to large variance when the full range of theoretical components are utilized without any smoothing .",
    "the simulation results shown below will display markedly reduced variances in the estimates of the parameters of interest relative to the same procedures using larger libraries of basis functions .",
    "additionally , smaller prototype bases yield better parameter estimates than the approach of using _ all _ of the theoretical components to model observed data , a  phenomenon that can be explained by the markedly reduced variance of parameter estimates found by smaller , judiciously - chosen bases .",
    "the paper is organized as follows . in section  [ secsfh ]",
    "we detail the problem of estimating star formation history parameters for galaxies and explain how prototyping methods can be used to obtain accurate parameter estimates . in section [ secproblem ]",
    "we formalize the problem of prototype selection for target parameter estimation and in section [ secmeth ] describe several approaches .",
    "we apply those methods to simulated data in section [ secsim ] to compare their performances . in section  [ secsdss ]",
    "we return to the astrophysics example , applying our methods to galaxy data from the sloan digital sky survey .",
    "we end with some concluding remarks in section [ secconc ] .",
    "galaxies are gravitationally - bound objects containing @xmath4@xmath5 stars , gas , dust and dark matter .",
    "the characteristics of the light we detect from each galaxy primarily depend on the physical parameters ( e.g. , age and composition ) of its component stars as well as distortions due to dust that resides in our line of sight to that galaxy , spectral distortions due to the line - of - sight component of the orbital velocities of its component stars , and the distance to the galaxy .",
    "the physical mechanisms that govern galaxy formation and evolution are complicated and poorly understood .",
    "galaxies are complex , dynamic objects .",
    "the star formation rate ( sfr ) of each galaxy tends to change considerably throughout its lifetime and the patterns of sfr vary greatly between different galaxies .",
    "the sfr for each galaxy depends on a countless number of factors , such as merger history , the galaxy s local environment ( e.g. , the matter density of its neighborhood , and the properties of surrounding galaxies ) and chemical composition .",
    "astronomers are interested in refining galaxy evolution models so that they match the observed patterns of galaxy sfh in the universe .",
    "it is imperative that we first have accurate estimates of the star formation history parameters for each observed galaxy .",
    "these sfh estimates are necessary to test competing physical models , alert to possible shortcomings in current models , and estimate cosmological parameters [ for an example of such an analysis , see @xcite ] .",
    "a common technique in the astronomy literature , called empirical population synthesis , is to model each galaxy as a  mixture of stars from different simple stellar populations ( ssps ) , defined as groups of stars with the same age and metallicity ( @xmath6 , defined as the fraction of mass contributed by any element heavier than helium ) .",
    "the principle behind this method is that each galaxy consists of multiple subpopulations of stars of different age and composition so that the integrated observed light from each galaxy is a mixture of the light contributed by each ssp . describing the data from each galaxy as a combination of ssps allows us to reconstruct the star formation and metallicity history of each galaxy",
    "this is because , for each galaxy , the component weight on an ssp captures the proportion of that galaxy s stars that was created at the specific epoch corresponding to the age of that ssp .",
    "therefore , the full vector of ssp component weights for each galaxy describes the star formation throughout the galaxy s lifetime .",
    "theoretical ssps can be produced by physical models , that are in turn constrained by observational studies .",
    "these models typically start with a  set of initial conditions and evolve the system forward in time based on sets of physically motivated differential equations .",
    "the output produced by these models can be extremely detailed . in our study",
    ", we use a set of high - resolution , broad - band spectra from the ssp models of @xcite . see figure [ figssps ] for an example of some ssp spectra , plotted over the optical portion of the electromagnetic spectrum .    , colored by @xmath7 .",
    "each spectrum is normalized to 1 at @xmath8  .",
    "top : basis of regular ( @xmath9 ) grid used in cid fernandes et al .",
    "( @xcite ) .",
    "bottom : diffusion @xmath10-means basis used in richards et al .",
    "( @xcite ) .",
    "the diffusion @xmath10-means basis shows a more gradual sampling of spectral space than the regular grid basis , which over - samples spectra from young stellar populations . ]    the galaxy data we use to estimate sfh parameters are high - resolution , broad - band spectra from the sloan digital sky survey [ sdss , @xcite ] which consist of light flux measurements over thousands of wavelength bins . to model the data from each galaxy",
    ", we adopt the empirical population synthesis generative model of a galaxy spectrum introduced in @xcite : @xmath11 where @xmath12 is the light flux at wavelength @xmath13 .",
    "the components of model  ( [ cfmodel ] ) are the following :    * @xmath14 is the @xmath15th ssp spectrum normalized at wavelength @xmath16 .",
    "each ssp has age @xmath17 and metallicity @xmath18 . in the true generative model",
    ", @xmath19 contains an infinite number of ssp spectra over the continuous parameters of age and metallicity . *",
    "@xmath20 $ ] , the component proportion of the @xmath15th ssp .",
    "the vector @xmath21 is the _ population vector _ of the galaxy , the principal parameter of interest for calculating derived parameters describing the sfh of a galaxy .",
    "* @xmath22 , the observed flux at wavelength @xmath16 . * @xmath23 accounts for the wavelength - dependent fraction of light",
    "that is either absorbed or scattered out of the line of sight by foreground dust .",
    "@xmath24 parametrizes the amount of this dust extinction that occurs .",
    "we adopt the reddening model of @xcite .",
    "* convolution , in wavelength , by the gaussian kernel @xmath25 describes spectral distortions from doppler shifts caused by the movement of stars within the observed galaxy with respect to our line - of - sight , and is parametrized by a central velocity @xmath26 and dispersion @xmath27 .",
    "previous to the analysis , care was taken to properly resample all spectra  both the observed and model spectra  to 1 measurement per ngstrm .",
    "this was done to ensure the reliability of the spectral errors when used by the ` starlight ` spectral fitting software .",
    "more details are available at ` http://www.starlight .",
    "ufsc.br/papers/manual_stcv04.pdf ` .      for each galaxy ,",
    "we observe a flux , @xmath28 , at each spectral wavelength , @xmath13 , with corresponding standard error , @xmath29 , estimated from photon counting statistics and characteristics of the telescope and detector .",
    "to estimate the target sfh parameters for each galaxy , we use the ` starlight ` software of @xcite , fitting model ( [ cfmodel ] ) using maximum likelihood .",
    "the code uses a metropolis algorithm with simulated annealing to minimize @xmath30 where @xmath12 is the model flux in ( [ cfmodel ] ) .",
    "the optimization routine searches for the maximum likelihood solution for the model @xmath31 , i.i.d . for each @xmath13 .",
    "the minimization of ( [ chisq ] ) is performed over @xmath32 parameters : @xmath33 , @xmath26 , and @xmath27 .",
    "the speed of the algorithm scales as @xmath34 , so it is imperative to pick a ssp basis with a small number of spectra .",
    "in practice , we use a basis of @xmath35 _ prototype _ ssp spectra , @xmath36which can be a carefully chosen subset or a nontrivial combination of the @xmath37s  and model each galaxy spectrum as @xmath38 where each prototype , @xmath39 , has age @xmath40 and metallicity @xmath41 , and@xmath42 .",
    "our goal in this analysis is to choose a suitable ssp basis to estimate a  set of physical parameters for each galaxy . some of the commonly - used sfh parameters are as follows :    * @xmath43 , the luminosity - weighted average log age of the stars in the galaxy , * @xmath44 , the log luminosity - weighted average metallicity of the stars in the galaxy , * @xmath45 , a time - binned version of the population vector , @xmath21 , and * @xmath46 , mass - weighted versions of the average age and metallicity of the stars in the galaxy .",
    "we estimate each of these parameters using the maximum likelihood parameters from model ( [ cfmodel1 ] ) . in @xcite",
    ", we introduced a method of choosing a ssp prototype basis and compared it to bases of regular @xmath47 grids that were used in previous analyses .",
    "see figure [ figssps ] for a plot of two such ssp spectral bases .",
    "we begin with a large , fixed set of @xmath48 theoretical components , each with known parameters @xmath49 ( these are the physical properties of each component ) .",
    "we refer to this set as the model data .",
    "these data can be thought of as a sample from some distribution @xmath50 in @xmath51 .",
    "the model data are stored in an @xmath52 by @xmath48 matrix @xmath53 $ ] , where @xmath52 is the total wavelength range of the ssp spectra .",
    "we assume that each observed data point @xmath54 , @xmath55 , is generated from the linearly separable nonlinear model @xmath56 where , for each @xmath15 , the coefficients , @xmath57 , are nonnegative and sum to 1 .",
    "the functional @xmath58 is a known , problem - dependent ( possibly nonlinear ) function of the linear combination of the components @xmath19 and some unknown parameters , @xmath59 .",
    "each @xmath60 is a vector of random errors .",
    "the set of target parameters for each observed data vector , @xmath54 , is @xmath61 , where @xmath62 is a function of the model weights , @xmath21 , and intrinsic parameters , @xmath63 , of the theoretical components .    for large @xmath48",
    ", it is impossible to use model ( [ eqngenmodel ] ) to estimate each @xmath64 due to the large computational cost .",
    "our goal is to find a set of prototypes @xmath65 $ ] , where @xmath35 , that can accurately estimate the target parameters @xmath64 for each observed @xmath54 , using the model @xmath66 where @xmath67 are nonnegative component weights such that @xmath68 for all @xmath15 .",
    "naturally , our estimate of @xmath69 is @xmath70 where the @xmath71 are estimated using the model ( [ eqnmodel ] ) , and @xmath72 is an @xmath48 by @xmath10 matrix of nonnegative coefficients that defines the prototypes from the dictionary of components by @xmath73 the coefficients @xmath72 are constrained such that each of the prototypes , @xmath74 , resides in a region of the theoretical component space , @xmath75 , with nonzero probability , @xmath76 , over all plausible values of the physical parameters used to generate  @xmath19 .",
    "this constraint is enforced to ensure the physical plausibility of the prototypes , @xmath77 , and their parameters . if our prototype basis were to include components that are disallowed by the physical models that generated @xmath19 , then the parameter estimates for the observed data would be uninterpretable .",
    "the usual method used to choose a basis for estimating target parameters from the signal model is to select prototypes from a regular grid in the physical parameter space .",
    "examples of such bases are those found in @xcite and @xcite , both of whom employ ssps on regular grids of age and metallicity to estimate sfh parameters . in this section",
    "we propose methods that use the set of physical components , @xmath19 , to construct a prototype basis in a principled manner . in section [ secsim ]",
    "we compare the proposed basis selection methods via simulations , and show that regular parameter grids tend to yield suboptimal parameter estimates .",
    "@xmath78for problems of interest , practical of theoretical models to noisy data requires a finite set of prototypes .",
    "the question becomes how to best choose this set of prototypes , that is ,  how to _ quantize the model space_. here , instead of quantizing the parameter space  by choosing uniform parameter grids , we propose methods that quantize the vector space @xmath79 of theoretical model - produced data .",
    "the idea behind this  approach is that under the presence of noise , components with similar functional forms will be indistinguishable , so that it is better to choose prototypes that are approximately evenly spaced in @xmath79 ( rather than evenly spaced in the parameter space ) . by replacing the theoretical models in each neighborhood by their local average , the model quantization approach is optimal for treating degeneracies because it allows a slight increase in bias to achieve a large decrease in variance of the target parameter estimates .",
    "the increase in estimator bias should be small because more prototypes are included in parameter regions where we can better discern the theoretical data curves of the components , allowing for precise parameter estimates in those regions and coarser average estimates in degenerate regions . if , instead , multiple components in our dictionary were to have very similar theoretical data curves but different parameter values , then , in the absence of any other method of regularization , we would have difficulty breaking the degeneracy no matter how many prototypes we include in that region of the parameter space , causing increased parameter estimator variance and higher statistical risk.=-1      the basic idea here is to quantize the vector space or support of model - produced data with respect to an appropriate metric and prior distribution .",
    "the vector quantization approach can be formalized as follows :    suppose that @xmath80 is a sample from some distribution @xmath50 with support @xmath81 .",
    "the support @xmath79 often has some lower - dimensional structure , which we refer to as the lower - dimensional _ geometry _ of @xmath79 .",
    "fix an integer @xmath82 . to any dictionary @xmath83 of prototypes",
    ", we can assign a  cost @xmath84 let @xmath85 denote all sets of the form @xmath86 with @xmath87 .",
    "define the optimal dictionary of @xmath10 prototypes as the cluster centers @xmath88 in practice , we estimate @xmath77 from model - produced data @xmath80 according to @xmath89 where @xmath90 is the empirical distribution . this estimate is found by lloyd s @xmath10-means ( km ) algorithm . to simplify the notation",
    ", we will henceforth skip the hat symbol on all estimates .",
    "the empirical @xmath10-means solution corresponds to allocating each @xmath91 into subsets @xmath92 , where the @xmath10 centroids define the prototypes . in the definition of the prototypes in ( [ eqnproto ] ) , this reduces to @xmath93 potential problems to this approach are the following : ( 1 ) the km prototypes will adhere to the design density on @xmath79 , and ( 2 ) for small @xmath10 , estimated prototypes could fall in areas that @xmath50 assigns probability zero .",
    "the first issue can be corrected using a weighted @xmath10-means approach or a method such as uniform subset selection ( section [ sssuss ] )",
    ". however , often the density on  @xmath79 corresponds to a prior distribution on the physical parameters , meaning it is often desirable to adhere to its design density . to remedy the latter issue , we could select as prototypes the @xmath10 data points that are closest to each of the centroids .",
    "we see in simulations that this approach tends to yield slightly worse parameter estimates than the original @xmath10-means formulation .",
    "we attribute this to the smoother sampling of parameter space achieved by the original km formulation , which averages the parameters of components with similar theoretical data , effectively decreasing the variability of the parameter estimates .",
    "if the theoretical data are high dimensional , we might choose to first learn the low - dimensional structure of @xmath19 and then employ @xmath10-means in this reduced space .",
    "this would permit us to avoid quantizing high - dimensional data , where @xmath10-means can be problematic due to the curse of dimensionality .",
    "this failure occurs because the theoretical data are extremely sparse in high dimensions , causing the distances between similar components to approach the distances between unrelated objects . to remedy this , we suggest the use of the diffusion map method for nonlinear dimensionality reduction [ @xcite , @xcite ] .",
    "in other words , we transform the model data into a lower - dimensional representation where we apply @xmath10-means ( diffusion @xmath10-means , dkm ) .",
    "formally , this corresponds to substituting ( [ eqkmeanscost ] ) with the cost function @xmath94 where @xmath95 is a data transformation defined by diffusion maps",
    ".- means is available in the ` diffusionmap  r ` package , which can be downloaded from ` http://cran.r-project.org/web/packages/ diffusionmap / index.html ` . ]      in the theoretical model data quantization approach the goal is to have prototypes regularly spaced in  @xmath79 , where  @xmath79 is the support of  @xmath50 . with this heuristic in mind",
    ", we devise the uniform subset selection ( uss ) method , which sequentially chooses the component @xmath96 that is furthest away from the closest component that has already been chosen .",
    "because the choice of distance metric is flexible , uss can be tailored to deal with many data types and high - dimensional data . unlike @xmath10-means ,",
    "uss is not influenced by differences in the density of components across  @xmath79 .",
    "however , uss typically chooses extreme components as prototypes because in each successive selection it picks the furthest theoretical data curve from the active set . in simulations",
    ", uss produces poor parameter estimates due to its tendency to select extreme components .",
    "most standard sparse coding techniques do not apply for the prototyping problem . without the appropriate constraints ,",
    "the prototype basis elements will be nonphysical and the subsequent parameter estimates will be nonsensical ( see section [ sssother ] ) .",
    "there are methods related to sparse coding that enforce the proper constraints to ensure that prototype basis elements reside within the native data space ( see sections [ secaa ] and [ ssssss ] ) , but these generally do not perform well for target parameter estimation because their objective of optimal data reconstruction  and not estimation of the target parameters  forces these methods to choose extreme prototypes .",
    "archetypal analysis ( aa ) was introduced by cutler and breiman ( @xcite ) as a method of representing each data point as a  linear mixture of archetypal examples , which themselves are linear mixtures of the original component dictionary .",
    "the method searches for the set of archetypes @xmath97 that satisfy ( [ eqnproto ] ) and minimize the residual sum of squares ( rss ) @xmath98 where @xmath99 for all @xmath100 and @xmath101 for all @xmath100 and @xmath102 . to minimize the rss criterion , an alternating nonnegative least squares algorithm is employed , alternating between finding the best @xmath103 s for a set of prototypes and finding the best prototypes ( @xmath72 s ) for a set of @xmath103 s .",
    "this computation scales linearly in the number of dimensions of the original theoretical data , with computational complexity becoming prohibitive for dimensionality more than 500 [ @xcite ] .",
    "once there are as many prototypes , @xmath10 , as the number of data points that define the boundary of the convex hull , any element in the dictionary can be fit perfectly with a linear mixture of the prototypes , yielding a rss of 0 . if we try to pick more prototypes than the number of data points that define the boundary of the convex hull , then the aa algorithm will fail to converge because @xmath104 becomes noninvertible , preventing the iterative algorithm to find the optimal set of prototypes , @xmath105 , given the current @xmath104 .",
    "we have experimented with using the moore ",
    "penrose pseudoinverse to perform this operation , but it is usually ill - behaved when @xmath104 is noninvertible .",
    "this upper bound on the number of aa prototypes is a serious drawback to using aa as a prototyping method because often the complicated nature of the data generating processes necessitates the use of larger prototype bases .",
    "prototypes found by aa are optimal in the sense that they minimize the rss for fitting noiseless , linear mixtures of the @xmath19 s .",
    "this is the case because  aa prototypes are found along the boundary of the convex hull formed by the @xmath19 s [ see @xcite ] . unlike aa , our objective is not to minimize rss , but to minimize the error in the derived parameter estimates .",
    "archetypal analysis achieves suboptimal results in the estimation of @xmath106 because it only samples prototypes from the boundary of the component space ,  @xmath79 , focusing attention on extreme cases while disregarding large regions of  @xmath79 . in section [ secsim ]",
    "we show using simulated data that aa is outperformed by the model quantization approach for estimating the target parameters from the signal model parameters .",
    "we introduce the method of sparse subset selection ( sss ) , whose goal is to find a subset of the original dictionary , @xmath107 , that can reconstruct @xmath19 in a linear mixture setting .",
    "this method is motivated by sparse coding in that it seeks the basis that minimizes a  regularized reconstruction of @xmath19 , where the regularization is chosen to select a  subset of the columns of @xmath19 .",
    "recently , @xcite introduced a method of variable selection in a high - dimensional multivariate linear regression setting .",
    "their method uses a penalty on the @xmath108 norm , for @xmath109 , of the matrix of regression coefficients in such a way that induces sparsity in the rows of the coefficient matrix .",
    "we can , in a straightforward way , adapt their method to select a  subset of columns of @xmath19 to be used as prototypes .",
    "our objective function  is @xmath110 where is the frobenius norm of a matrix , and the @xmath111 penalty is defined as @xmath112 so that sparsity is induced in the _ rows _ of @xmath113 , the @xmath48 by @xmath48 matrix of nonnegative mixture coefficients .",
    "additionally , @xmath113 is normalized to sum to 1 across columns .",
    "the basis , @xmath77 , is defined as the columns of @xmath19 that correspond to nonzero rows of @xmath113 ( @xmath72 is the corresponding indicator variable ) .",
    "the parameter @xmath114 controls the number of prototypes in our sss set @xmath77 .    to perform the optimization ( [ eqnsss1 ] )",
    ", we use the ` cvx matlab ` package [ @xcite ] . setting @xmath115 , we recast the problem as a second - order cone problem with the additional constraints of nonnegativity and column normalization of @xmath113",
    "[ see @xcite ] .",
    "the current implementation can not solve problems for large @xmath48 .",
    "in section [ sscomp ] we show , for a  small problem , that sss has behavior similar to archetypal analysis in that it selects prototypes from the boundary of the convex hull of @xmath19 . like  aa , sss is not a good method for target parameter estimation .",
    "there are other methods for sparse data representation that fail to work for prototype selection .",
    "these methods are not applicable to this problem because they do not select prototypes that reside in regions of @xmath79 with nonzero probability @xmath50 .",
    "the failure to obey this constraint means that the chosen prototypes in general will not be _ physical _ , meaning that either their theoretical data or intrinsic parameters are disallowed .",
    "for instance , in the sfh problem , this could lead us to use prototypes whose spectra have negative photon fluxes or whose ages are either negative or greater than the age of the universe . using such uninterpretable prototypes to model observed data produces parameter estimates that are nonsensical .",
    "we mention two popular methods for estimating small bases from large dictionaries , @xmath19 , and describe why they are not useful for prototyping :    in _ standard sparse coding _",
    "[ @xcite ] , the goal is to find a  decomposition of the matrix @xmath19 , in which the hidden components are sparse .",
    "sparse coding combines the goal of small reconstruction error along with sparseness , via minimization of @xmath116 where the trade - off between @xmath117 sparsity in the mixture coefficients @xmath118 , and accurate reconstruction of @xmath19 , is controlled by @xmath13 . however , there are no constraints on the sign of the entries of @xmath118 or @xmath77 , meaning that prototypes with nonphysical attributes are allowed",
    ".    _ nonnegative matrix factorization _ ( _ nmf _ )",
    "[ @xcite , @xcite ] is a related technique that includes strict nonnegativity constraints on all coefficients @xmath119 and @xmath120 while minimizing the reconstruction of @xmath19 , @xmath121 this construction is different than our prototype definition in ( [ eqnproto ] ) , where @xmath122 . to reconcile the two ,",
    "we see that , since @xmath123 , @xmath72 is the right inverse of @xmath118 : @xmath124 which exists if @xmath118 is full rank .",
    "however , under this formulation , the @xmath125 are not constrained to be nonnegative and the resultant prototypes are not constrained to reside in @xmath79 .",
    "thus , nmf is not useful for prototyping .",
    "note that archetypal analysis avoids this problem by enforcing the further constraint that the prototypes be constrained linear combinations of @xmath19 .",
    "we apply four prototyping methods to the two - dimensional data set ` toy ` in the ` archetypes r ` package .",
    "we treat each 2-d data point , @xmath126 , as model - produced theoretical data .",
    "plots of this dictionary of data and the selected prototypes for four different prototyping methods , using @xmath127 , are in figure [ figtoyproto ] .",
    "@xmath10-means places prototypes evenly spaced within the convex hull of the data .",
    "uss also evenly allocates the prototypes , but    s ) for four different methods when applied to the 250 theoretical data objects in the ` toy ` data set ( grey @xmath128 s ) .",
    "@xmath10-means evenly samples the native data space while the other methods focus more attention to the boundary of the space . ]",
    "places many along the boundary of the native space .",
    "archetypal analysis and sss place all prototypes on the boundary of the convex hull .",
    "note that for more than 7 prototypes , the archetypal analysis algorithm does not converge to a solution .",
    "in this section we test the effectiveness of the prototyping methods for estimating a set of target parameters using simulated data .",
    "the first test set is the toy example of zero - mean gaussian curves discussed in section  [ secex ] .",
    "the second simulation experiment is a set of realistic galaxy spectra created to mimic the sdss data that we later analyze in section [ secsdss ] .",
    "we begin with the example introduced in section  [ secex ] .",
    "we simulate a database of @xmath129 gaussian curves , @xmath130 , on a fine grid of @xmath131 from 0.2 to 8 in steps of 0.05 ( see figure  [ fignormcurv ] ) .",
    "each @xmath126 is represented as a vector of length 321 . from this database ,",
    "we simulate a set of 100 data vectors , @xmath132 , from the model @xmath133 where the mixture coefficients , @xmath134 , sum to unity for each @xmath15 and have at most 5 nonzero entries for each @xmath15 .",
    "the noise vectors , @xmath135 , are i.i.d .",
    "normal zero - mean with standard deviation 0.05 .    from @xmath136",
    ", we generate bases of prototypes using six different methods described in section [ secmeth ] . to explore the differences in each of these methods , we plot ( figure [ fignormsig ] ) the distribution of @xmath137 prototype @xmath1 values .",
    "the model quantization methods ( km , dkm , uss ) find more prototypes with small @xmath1 values .",
    "the aa and sss methods place more prototypes at the extreme values of @xmath1 ( note that for  sss , we ran the algorithm on a coarser grid of 32 gaussian curves ) .",
    "prototype @xmath1 values for seven different prototyping methods applied to the gaussian curves example .",
    "the methods are the following : grid - regular  @xmath1 grid , log grid - regular @xmath138 grid , km@xmath10-means , dkm ",
    "diffusion @xmath10-means , uss  uniform subset selection , aa ",
    "archetypal analysis , and sss  sparse subset selection . ]    to evaluate each of the methods , we compare their ability to estimate the average @xmath1 for each @xmath54 , defined as @xmath139 for each choice of basis , we fit the observed data using nonnegative least squares .. ] in figure [ figsigmse ] the mse for @xmath3 estimation for @xmath10-means , diffusion @xmath10-means , uss and uniform @xmath1-grid and @xmath138 grid bases is plotted as a function of @xmath10 .",
    "sss is not plotted because it yields parameter estimates with mse @xmath140 .",
    "aa is not plotted because it only converges for @xmath141 , and performs worse than the @xmath1 grid for those values .",
    "km and dkm outperform the regular parameter grids , uss , and aa prototype bases .",
    "km achieves a minimum mse , averaged over 25 trials , of 0.815 at @xmath142 prototypes .",
    "dkm achieves a minimum mse of 0.846 at prototypes , while the uniform @xmath1 grid achieves a minimum mse of 1.378 , 1.7 times higher than the best mse for km .",
    "results for aa and sss are not plotted because aa only converges for @xmath141 prototypes , and sss is too computationally intensive to run on the entire dictionary of curves ; at @xmath137 , neither method outperforms a  uniform @xmath1 grid .     for the gaussian curve example .",
    "plotted is the mse for using a regular parameter grid , @xmath10-means ( km ) , diffusion @xmath10-means ( dkm ) and archetypal analysis ( aa ) prototype bases .",
    "both dkm and km achieve significantly better  @xmath3 estimates than a regular parameter grid and outperform estimates obtained by using all 157 gaussian curves in the original dictionary . for each @xmath10 ,",
    "the mse is averaged across 25 repetitions of the experiment .",
    "point - wise 68% confidence bands are shown as dotted lines . ]",
    "an interesting observation in figure [ figsigmse ] is that the minimum mse for estimating @xmath143 is achieved for @xmath142 km prototypes . as the number of prototypes increases from 10 , the km @xmath144 estimates worsen .",
    "this exemplifies the bias - variance trade - off in the estimation procedure : for @xmath145 , the increased variance of the estimates is larger than the reduction in squared - bias .",
    "estimates of @xmath3 from four of the five prototype bases plotted in figure  [ figsigmse ] outperform the estimates found by fitting each @xmath54 as a mixture of all 157 original component curves . over the 25 repetitions of the simulations , the  @xmath146 which are positive ,",
    "that is , the @xmath91 that receive any weight , vary widely .",
    "these results demonstrate that a single , judiciously chosen , reduced basis can reproduce a wide range of truths and return accurate parameter estimates with reduced variance .",
    "we further test the performance of each prototyping method using realistic simulated galaxy spectra . starting with a database , @xmath19 , of 1,182 ssps from the models of @xcite ( see section [ secsfh ] ) ,",
    "we generate simulated galaxy spectra using the model  ( [ cfmodel ] ) .",
    "the ssps are generated from 6 different metallicities and a fine sampling of 197 ages from 0 to 14 gyrs .",
    "we use a prescription similar to @xcite to choose the physical parameters of the simulations , altered to have higher contribution from younger ssps .",
    "the basic physical components of the simulation are as follows :    a star formation history with exponentially decaying star formation rate ( sfr ) : sfr @xmath147 . here , @xmath148 , so the sfr is exponentially declining with time , as @xmath149 is the age of the ssp today .",
    "we allow @xmath21 to vary between galaxies . for each galaxy",
    "we draw @xmath21 from a uniform distribution between 0.25 and 1 @xmath150 .",
    "the time @xmath151 when a galaxy begins star formation is distributed uniformly between 0 and 5.7 gyr after the big bang , where the universe is assumed to be 13.7 gyr old .",
    "we allow for starbursts , epochs of increased sfr , with equal probability at all times .",
    "the probability a starburst begins at time @xmath149 is constructed so that the probability of no starbursts in the life of the galaxy is 33% .",
    "the length of each burst is distributed uniformly between 0.03 and 0.3 gyr and the fraction of total stellar mass formed in the burst in the past 0.5 gyr is distributed log - uniformly between 0 and 0.5 .",
    "the sfr of each starburst is constant throughout the length of the burst .",
    "each galaxy spectrum is generated as a mixture of ssps of up to 197 time bins , with a uniformly drawn metallicity in each bin .",
    "we draw the reddening parameter ( @xmath24 ) and velocity dispersion ( @xmath152 ) from empirical distributions over a plausible range of each parameter .",
    "we simulate 100 galaxy spectra with i.i.d .",
    "zero - mean gaussian noise with @xmath153 at @xmath154  .",
    "we apply the methods in section [ secmeth ] to choose ssp prototype bases from  @xmath19 . in figure [ figssppar150 ]",
    "the distributions of the ssp prototype ages and metallicities for @xmath155 prototype bases are plotted along with the regular parameter grid used by @xcite .",
    "each method highly samples the older , higher metallicity ssps and typically only includes a few prototypes with low age and low metallicity .",
    "this is reasonable because older , higher metallic ssp spectra change more with respect to changes in age and metallicity .",
    "any method for prototyping based on the model - produced data will detect this difference and sample these regions of the parameter space more highly.=1     of several prototype bases of ssps , @xmath155 .",
    "all bases were derived using a database of 1,182 model - produced ssps .",
    "each of the methods more heavily samples prototypes with large age and large metallicity . ]",
    "each simulated galaxy spectrum is fit using the ` starlight ` software with each prototype basis . to assess the performance of each method",
    ", we compare the accuracy of their parameter estimates . in figure [ figwild2 ]",
    "we plot the mse of the estimates of @xmath156 and @xmath27 and the average error of the coarse - grained population vector estimate ,    -means ( km ) , diffusion @xmath10-means ( dkm ) , centroid @xmath10-means ( km - central ) , uss , aa , and a regular parameter grid .",
    "mses are plotted for bases of size @xmath142 , 25 , 45 , 100 and 150 .",
    "the regular parameter grids are from cid fernandes et al .",
    "( @xcite ) ( @xmath157 ) and asari et al .",
    "( @xcite ) ( @xmath155 ) .",
    "each prototyping method finds more accurate sfh parameter estimates than the two regular parameter grids . ]",
    "@xmath158 , measured by the average  @xmath159 distance to the true @xmath45 .",
    "each prototype method outperforms the regular parameter grid prototype bases , often by large margins , especially for @xmath157 . between the different prototyping methods there",
    "does not appear to be a clear winner , though diffusion @xmath10-means bases achieve the lowest or second - lowest mse for 4 of the 5 parameters .",
    "@xmath10-means also achieves accurate estimates for each of the parameters , and always beats or ties the @xmath10-means - central estimates .",
    "both uss and aa yield inaccurate estimates for all parameters except @xmath160 and @xmath27 .",
    "sss could not be run on such a large dictionary of ssps .",
    "overall , small bases achieve better estimates of @xmath161 and @xmath162 , but this likely will not be the case for real galaxies , whose sfhs are more complicated and diverse than the simulation prescription used .",
    "prototyping methods are used to estimate the sfh parameters from the sdss spectra of a set of 3046 galaxies in sdss data release 6 [ @xcite ] . for more detailed information about the data and preprocessing steps ,",
    "see @xcite . in figure [ figsdss ]",
    "we plot the estimated @xmath163 versus @xmath160 for each galaxy using three basis choices : the regular parameter grid of @xcite ( asa07 , @xmath155 ) , dkm with @xmath157 , and dkm with @xmath155 .     versus @xmath164 for a set of 3046 galaxies observed by the sdss , estimated using ` starlight ` with three different prototype bases . from left to right , bases are as follows : regular parameter grid from asari et al .",
    "( @xcite ) with @xmath155 , diffusion @xmath10-means @xmath157 , and diffusion @xmath10-means @xmath155 .",
    "estimates from diffusion @xmath10-means bases show much less spread in the direction of the well - known age - metallicity degeneracy in galaxy population synthesis studies . ]",
    "there are several differences in the estimated @xmath165 relation for each basis .",
    "first , both diffusion @xmath10-means bases produce estimates that are tightly spread around an increasing trend while the asa07 estimates are more diffusely spread around such a trend .",
    "the direction of discrepancy in the asa07 estimates from the trend corresponds exactly with the direction of a well - known spectral degeneracy between old , metal - poor and young , metal - rich galaxies [ @xcite ] .",
    "this suggests that the observed variability along this direction is not due to the physics of these galaxies , but rather is caused by confusion stemming from the choice of basis [ in @xcite we verified that diffusion @xmath10-means sfh estimates have a  decreased age - metallicity degeneracy , using simulated galaxy spectra ] .",
    "second , the @xmath157 diffusion @xmath10-means basis estimates no young , metal - poor galaxies , whereas the other bases do .",
    "this suggests that this small number of prototypes is not sufficient to cover the parameter space ; particularly , young , metal - poor ssps have been neglected in the @xmath157 diffusion @xmath10-means basis .",
    "finally , the overall trend between @xmath163 versus @xmath166 differs substantially between the regular grid and diffusion @xmath10-means basis , suggesting that sfh parameter estimates are sensitive to the choice of basis and that downstream cosmological inferences will depend heavily on the basis used .",
    "recently , we have estimated the sfh parameters for all 781,692 galaxies in the sdss dr7 [ @xcite ] main sample or lrg sample .",
    "this subset of dr7 galaxies was chosen for analysis because it was targeted for spectroscopic observation , and thus has a well defined selection function [ @xcite ] .",
    "we estimated the parameters using ` starlight ` with a diffusion @xmath10-means basis of size @xmath155 .",
    "the computational routines took nearly 5 cpu years to analyze the entire data set , which includes preprocessing of the data , estimating the sfh parameters for each , and compiling the catalog of estimates .",
    "the computations were performed in parallel on the 1,000-core high - performance flux cluster at the university of michigan",
    ". results of this analysis are in preparation [ @xcite ] and will be published shortly . these sfh estimates will be used to constrain cosmological models that concern the formation and evolution of galaxies and the history and fate of the universe .",
    "there is also ongoing work into approaches to quantifying the statistical uncertainty in the resulting parameter estimates .",
    "this is a critical , but challenging , component .",
    "the basic approach to be employed will exploit the massive amount of data by inspecting the amount of variability in parameter estimates in small neighborhoods in the space of galaxy spectra .",
    "an additional regression model will be fit , with the parameter estimates as the response , and the spectrum as the predictor . in previous work [ @xcite and @xcite ]",
    ", we have fit models of exactly this type , using galaxy spectra or colors to predict redshift . as was the case in that work",
    ", we will smooth the parameter estimates in the high - dimensional space to obtain an estimator with lower variance .",
    "equally important , this will yield a natural way of estimating the uncertainty in the estimator , by inspecting the variance of the residuals of the regression fit .",
    "we have introduced a prototyping approach for the common class of parameter estimation problems where observed data are produced as a constrained linear combination of theoretical model - produced components , and the target parameters are derived from the parameters in the signal model .",
    "the usual approach to this type of problem is to use models on a regular grid in parameter space . in this paper",
    "we have introduced approaches that use the properties of the theoretical data from the dictionary of components to estimate prototype bases .",
    "these approaches include : quantizing the component model data space using @xmath10-means , selecting prototypes uniformly over the space of theoretical component data , and estimating prototype bases that minimize the reconstruction error of the components .",
    "* the quantization methods presented in this paper achieve better parameter estimates than the approach of using prototypes from a regular parameter grid , as shown in multiple simulations .",
    "the regularization that results from a reduced basis leads to reduced variance in the parameter estimates , without sacrificing accuracy .",
    "this is the case because components with similar theoretical data will be indiscernible under the presence of noise , making it crucial that prototypes be spread out evenly in theoretical data space , inducing a large decrease in variance of the target parameter estimates .",
    "if bases are too small , then the parameter estimates suffer from large bias because important regions of model space are neglected .",
    "* standard sparse coding methods are not appropriate for this class of problem . without the proper constraints ,",
    "these methods do not find prototypes that are physically - plausible .",
    "even with these constraints , these methods select prototypes around the boundary of the data distribution , which is good for data reconstruction but not for target parameter estimation . * for a complicated problem in astrophysics  estimating the history of star formation for each galaxy in a large database  we obtain more accurate parameters ( in simulations ) using the model quantization approach than using regular parameter grids . when applied to the real data , these different prototyping approaches produce markedly different results , showing the importance of prototype basis selection ."
  ],
  "abstract_text": [
    "<S> parameter estimation in astrophysics often requires the use of complex physical models . in this paper </S>",
    "<S> we study the problem of estimating the parameters that describe star formation history ( sfh ) in galaxies . here , high - dimensional spectral data from galaxies are appropriately modeled as linear combinations of physical components , called simple stellar populations ( ssps ) , plus some nonlinear distortions . </S>",
    "<S> theoretical data for each ssp is produced for a fixed parameter vector via computer modeling . </S>",
    "<S> though the parameters that define each ssp are continuous , optimizing the signal model over a large set of ssps on a fine parameter grid is computationally infeasible and inefficient . </S>",
    "<S> the goal of this study is to estimate the set of parameters that describes the sfh of each galaxy . </S>",
    "<S> these target parameters , such as the average ages and chemical compositions of the galaxy s stellar populations , are derived from the ssp parameters and the component weights in the signal model . here , we introduce a principled approach of choosing a small basis of ssp _ prototypes _ for sfh parameter estimation . </S>",
    "<S> the basic idea is to quantize the vector space and effective support of the model components . </S>",
    "<S> in addition to greater computational efficiency , we achieve better estimates of the sfh target parameters . in simulations , </S>",
    "<S> our proposed quantization method obtains a substantial improvement in estimating the target parameters over the common method of employing a  parameter grid . </S>",
    "<S> sparse coding techniques are not appropriate for this problem without proper constraints , while constrained sparse coding methods perform poorly for parameter estimation because their objective is signal reconstruction , not estimation of the target parameters .    ,    ,    . </S>"
  ]
}