{
  "article_text": [
    "in reinforcement learning a tradeoff between exploration and exploitation of knowledge is considered .",
    "the multiarmed bandit problem is one formulation of the reinforcement learning and is a model of a gambler playing a slot machine with multiple arms .",
    "a dilemma for the gambler is that he can not know whether the expectation of an arm is high or not without pulling it many times but he suffers a loss if he pulls suboptimal ( i.e. , not optimal ) arms many times .",
    "this problem was formulated by and its theoretical bound was derived by for single parametric models , which was extended to multiparameter models by .",
    "these theoretical bounds show that any suboptimal arm has to be pulled at least logarithmic number of rounds and its coefficient is determined by the distributions of suboptimal arms and the expectation of the optimal arm .",
    "along with the asymptotic bound for this problem , achievability of the bound has also been considered in many models .",
    "proved the asymptotic optimality of a policy based on the notion of _ upper confidence bound _ ( ucb ) for laplace distributions ( which do not belong to exponential families ) and some exponential families including normal distributions with _ known _ variances .",
    "the achievability of the bound was later extended to a subclass of one - parameter exponential families @xcite .    on the other hand in multiparameter or nonparametric models , and proved the achievability for finite - support distributions and bounded - support distributions , respectively .",
    "however , the above two models are both compact and achievability of the bound is not known for non - compact multiparameter models , which include normal distributions with unknown means and variances . since the normal distribution model is one of the most basic settings of stochastic bandits ,",
    "many researches have been conducted for this model @xcite . however , to the authors knowledge , only the ucb - normal policy @xcite assures a ( non - optimal ) logarithmic regret for this model .    in this paper",
    "we discuss the asymptotic optimality of thompson sampling ( ts ) @xcite for this normal distribution model with unknown means and variances .",
    "ts is a bayesian policy which chooses an arm randomly according to the posterior probability with which the arm is the optimal .",
    "this policy was recently rediscovered and is researched extensively because of its excellent empirical performance for many models @xcite .",
    "the theoretical analysis of ts was first given for bernoulli model @xcite and was later extended to general one - parameter exponential families @xcite .",
    "the asymptotic optimality of ts under uniform prior is proved for bernoulli model in , whereas it is proved for a more general model , one - parameter exponential family , under jeffreys prior in .",
    "therefore , tss with uniform prior and jeffreys prior are asymptotically equivalent at least for the bernoulli model .",
    "nevertheless , we prove for the normal distribution model that ts with uniform prior achieves the asymptotic bound whereas ts with jeffreys prior and reference prior can not .",
    "furthermore , ts with jeffreys prior can not even achieve a logarithmic regret and suffers a polynomial regret in expectation .",
    "this result implies that ts may be more sensitive to the choice of priors than expected and non - informative priors are sometimes risky ( in other words , too optimistic ) for multiparameter models .",
    "this paper is organized as follows . in sect.[sect_pre ] , we formulate the bandit problem for the normal distribution model and introduce thompson sampling .",
    "we give the main result on the optimality of ts in sect.[sect_main ] .",
    "the remaining sections are devoted to the proof of the main result . in sect.[sect_prob ] , we derive inequalities for probabilities which appear in the normal distribution model .",
    "we prove the optimality of ts with conservative priors in sect.[sect_regret ] and prove the non - optimality of ts with optimistic priors in sect.[sect_dame ] .",
    "we consider the @xmath0-armed stochastic bandit problem in the normal distribution model .",
    "the gambler pulls an arm @xmath1 at each round and receives a reward independently and identically distributed by @xmath2 , where @xmath3 denotes the normal distribution with mean @xmath4 and variance @xmath5 .",
    "the gambler does not know the parameter @xmath6 .",
    "the maximum expectation is denoted by @xmath7 .",
    "let @xmath8 be the arm pulled at the @xmath9-th round and @xmath10 be the number of times that the arm @xmath11 is pulled before the @xmath9-th round .",
    "then the regret of the gambler at the @xmath12-th round is given for @xmath13 by @xmath14 let @xmath15 be the @xmath16-th reward from the arm @xmath11 .",
    "we define @xmath17 that is , @xmath18 and @xmath19 denote the sample mean and the sum of squares from @xmath16 samples from the arm @xmath11 , respectively .",
    "we denote the sample mean and the sum of squares before the @xmath9-th round by @xmath20 and @xmath21 .",
    "it is well known that @xmath22 where the chi - squared distribution @xmath23 with degree of freedom @xmath24 has the density @xmath25      it is shown in that under any policy satisfying a mild regularity condition the expected regret satisfies @xmath26}{\\log t } } { \\nonumber\\\\}&\\ge \\sum_{i : { \\delta}_i>0}\\frac{{\\delta}_i } { \\inf_{(\\mu,{\\sigma } ) : \\mu>\\mu^ * } d({\\mathcal{n}}(\\mu_i,{\\sigma}_i^2)\\vert{\\mathcal{n}}(\\mu,{\\sigma}^2 ) ) } { \\,,}\\label{bound_burnetas}\\end{aligned}\\ ] ] where @xmath27 is the kl divergence . since the kl divergence between normal distributions is @xmath28 the infimum in",
    "is expressed for @xmath29 as @xmath30 therefore , by letting @xmath31 we can rewrite the theoretical bound in as @xmath32}{\\log t } & \\ge \\sum_{i : { \\delta}_i>0}\\frac{{\\delta}_i } { { d_{\\mathrm{inf}}}({\\delta}_i,{\\sigma}_i^2)}{\\,.}\\label{bound_normal}\\end{aligned}\\ ] ]      thompson sampling is a policy based on the bayesian viewpoint .",
    "we mainly consider the prior @xmath33 , or equivalently , @xmath34 . since the density of the inverse gamma distribution is @xmath35 the above prior for @xmath36 corresponds to this distribution with parameters @xmath37 .",
    "the cases @xmath38 correspond to uniform for parameter @xmath36 , uniform for parameter @xmath39 , reference and jeffreys priors , respectively ( see , e.g.   for results on bayesian theory given in this section ) . under this prior , the posterior distribution is @xmath40 where @xmath41 . since the density of @xmath9-distribution with degree of freedom",
    "@xmath42 is @xmath43 we see that @xmath44 thompson sampling is the policy which chooses an arm randomly according to the probability with which the arm is the optimal when each @xmath45 is distributed independently by the posterior @xmath46 for @xmath47 .",
    "this policy is formulated as algorithm [ alg_thompson ] .",
    "note that we require @xmath48 initial pulls to avoid improper posteriors .",
    "we use @xmath49 for simplicity of the analysis .",
    "pull each arm @xmath49 times .",
    "sample @xmath51 from the posterior @xmath52 under prior @xmath53 for each arm @xmath11 .",
    "2 .   pull an arm @xmath11 maximizing @xmath51 .",
    "in this section we give the main result of this paper .",
    "first we show that ts achieves the asymptotic bound for the prior @xmath54 with @xmath55 .",
    "[ thm_possible ] let @xmath56 be arbitrary and assume that there is a unique optimal arm . under thompson sampling with @xmath55 , the expected regret is bounded as @xmath57 & \\le \\sum_{i:{\\delta}_i>0}\\frac{{\\delta}_i\\log t}{{d_{\\mathrm{inf}}}({\\delta}_i,{\\sigma}_i^2)}+{\\mathrm{o}}((\\log t)^{4/5}){\\,.}{\\nonumber}\\ ] ]    see lemma [ lem_possible ] for the specific representation of the reminder term @xmath58 .",
    "we see from this theorem that ts with @xmath55 is asymptotically optimal in view of .",
    "next we show that ts with @xmath59 can not achieve the asymptotic bound . to simplify the analysis we consider a two - armed setting more advantageous to the gambler in which the full information on the arm 2 is known beforehand , that is , the prior on the arm 2 is the unit point mass measure @xmath60 instead of @xmath61 .",
    "[ thm_impossible ] assume that there are @xmath62 arms such that @xmath63 .",
    "then , under thompson sampling such that @xmath64 with @xmath59 and @xmath65 , there exists a constant @xmath66 independent of @xmath67 such that @xmath32}{\\log t}\\ge \\xi{\\,.}\\label{cannot1}\\end{aligned}\\ ] ] in particular , if @xmath68 then there exist @xmath69 and @xmath70 such that @xmath71}{t^{\\eta}}\\ge \\xi'{\\,.}\\label{cannot2}\\end{aligned}\\ ] ]    eq .",
    "means that ts with @xmath68 suffers a polynomial regret in expectation .",
    "also note that the asymptotic bound in approaches zero for sufficiently small @xmath67 in the above two - armed setting since @xmath72 as @xmath73 .",
    "nevertheless , the lhs of does not go to zero as @xmath74 because @xmath66 is independent of @xmath67 .",
    "therefore ts with @xmath75 also does not achieve the asymptotic bound at least for sufficiently small @xmath67 .",
    "recall that jeffreys and reference priors correspond to @xmath76 and @xmath77 , respectively .",
    "therefore this theorem means that ts with these non - informative priors does not achieve the asymptotic bound .",
    "probability that the sample mean satisfies @xmath78 for any @xmath79 becomes large when @xmath36 is large .",
    "therefore the posterior probability that the true expectation @xmath45 is larger than @xmath80 becomes large when the prior has heavy weight at large @xmath36 , that is , @xmath81 is small . as a result , as @xmath81 decreases , ts becomes a `` conservative '' policy which chooses a seemingly suboptimal arm frequently .",
    "theorems [ thm_possible ] and [ thm_impossible ] mean that the prior should be conservative to some extent and non - informative priors are too optimistic .",
    "although ts with non - informative priors does not achieve the asymptotic bound in the sense of expectation , this fact does not necessarily mean that these priors are `` bad '' ones . as we can see from a close inspection of the proof of theorem [ thm_impossible ] ,",
    "the expected regret of ts with these priors becomes large because an enormously large regret arises with fairly small probability .",
    "therefore this policy performs well except for the case arising with this small probability , and the authors think that ts with these priors also becomes a good policy in the probably approximately correct ( pac ) framework . in any case , we should be aware that these non - informative priors are `` risky '' in the sense of expectation .",
    "in this section we derive fundamental inequalities for distributions appearing in thompson sampling for the normal distribution model . we prove them in appendix .",
    "first we give a simple inequality to evaluate the ratio of gamma functions which appears in the densities of normal , chi - squared and @xmath9-distributions .",
    "[ lem_gamma ] for @xmath82 @xmath83    next we give large deviation probabilities ( see , e.g. , ) for empirical means and variances .    [ lem_ldp ] for any @xmath84 @xmath85\\le { \\mathrm{e}}^{-\\frac{n(\\mu-\\mu_i)^2}{2{\\sigma}_i^2}}\\label{ldp_mean}\\end{aligned}\\ ] ] and for any @xmath86 @xmath87\\le { \\mathrm{e}}^{-nh\\left(\\frac{{\\sigma}^2}{{\\sigma}_i^2}\\right)}\\label{ldp_var}\\end{aligned}\\ ] ] where @xmath88 .",
    "it is well known that mill s ratio ( * ? ? ? * chap.5 ) gives a tighter bound for the tail probability of normal distributions , and similar technique can also be applied to the tail weight of @xmath89 distributions .",
    "however , we use bounds in lemma [ lem_ldp ] based on the large deviation principle because they are simpler and convenient for our analysis .    finally we evaluate the posterior distribution of the mean for thompson sampling .",
    "probability that the sample from the posterior is larger than or equal to @xmath4 , which is formally defined as @xmath90 is bounded as follows .",
    "[ lem_upper ] if @xmath91 and @xmath92 then @xmath93 and @xmath94 where @xmath95",
    "in this section we show that thompson sampling achieves the asymptotic bound if @xmath55 . the main result of this section is given as follows .",
    "[ lem_possible ] fix any @xmath55 and assume that @xmath96 and the arm 1 is the unique optimal arm .",
    "then , for any @xmath97 , @xmath98 } { \\nonumber\\\\}&\\le \\sum_{i : { \\delta}_i>0}{\\delta}_i \\bigg ( \\frac{\\log t}{{d_{\\mathrm{inf}}}({\\delta}_i-2{\\epsilon},{\\sigma}_i^2+{\\epsilon})}+2 - 2{\\alpha}{\\nonumber\\\\}&\\qquad\\qquad+\\frac{\\sqrt{{\\sigma}_i^2+{\\epsilon}}}{{\\delta}_i-2{\\epsilon } } + { \\frac{1}{1-{\\mathrm{e}}^{-\\frac{{\\epsilon}}{2{\\sigma}_i^2}}}}+{\\frac{1}{1-{\\mathrm{e}}^{-h(1+\\frac{{\\epsilon}}{{\\sigma}_i^2 } ) } } } \\bigg ) { \\nonumber\\\\}&\\quad+{\\delta}_{\\max}\\bigg ( { \\frac{1}{1-{\\mathrm{e}}^{-\\frac{{\\epsilon}^2}{2}}}}+{\\frac{1}{1-{\\mathrm{e}}^{-h(2 ) } } } + \\frac{\\mathrm{b}(1/2,-{\\alpha})}{(1-{\\mathrm{e}}^{-\\frac{{\\epsilon}^2}{2}})^2}{\\nonumber\\\\}&\\phantom{wwwwwwwwi}+ \\frac{2\\sqrt{2}}{{\\epsilon } } \\frac{\\left(1+{\\epsilon}^2/8\\right)^{1-{\\alpha}}}{1-\\left(1+{\\epsilon}^2/8\\right)^{-1/2 } } \\bigg ) { \\nonumber\\\\}&= \\sum_{i : { \\delta}_i>0 } \\frac{{\\delta}_i\\log t}{{d_{\\mathrm{inf}}}({\\delta}_i-{\\epsilon},{\\sigma}_i^2+{\\epsilon } ) } + { \\mathrm{o}}({\\epsilon}^{-4}){\\,,}{\\nonumber}\\end{aligned}\\ ] ] where @xmath99 and @xmath100 is the beta function .    [ cor_possible ] under the same assumption as lemma [ lem_possible ] , @xmath101\\le\\sum_{i : { \\delta}_i>0 } \\frac{{\\delta}_i\\log t}{{d_{\\mathrm{inf}}}({\\delta}_i,{\\sigma}_i^2 ) } + { \\mathrm{o}}((\\log t)^{4/5}){\\,.}{\\nonumber}\\end{aligned}\\ ] ]    this corollary is straightforward from lemma [ lem_possible ] with @xmath102 .",
    "note that @xmath103 is invariant under the location and scale transformation , that is , @xmath104 thus theorem [ thm_possible ] easily follows from corollary [ cor_possible ] by the transformation @xmath105 .",
    "define events @xmath106 where @xmath107 .",
    "then the regret at the round @xmath12 is bounded as @xmath108}{\\nonumber\\\\}&\\quad+\\sum_{i=2}^k { \\delta}_i \\left(n_0+\\sum_{t = kn_0 + 1}^t{{\\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[j(t)=i{,\\;}{\\mathcal{a}}(t)\\right]}\\right){\\nonumber\\\\}&\\le { \\delta}_{\\max}\\sum_{t = kn_0 + 1}^t { { \\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[j(t)\\neq 1{,\\;}{\\mathcal{a}}^c(t)\\right]}{\\nonumber\\\\}&\\quad+ \\sum_{i=2}^k{\\delta}_i\\bigg ( \\sum_{t = kn_0 + 1}^t{{\\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[j(t)=i{,\\;}{\\mathcal{a}}(t){,\\;}{\\mathcal{b}}_i(t)\\right]}{\\nonumber\\\\}&\\qquad\\qquad+\\sum_{t = kn_0 + 1}^t{{\\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[j(t)=i{,\\;}{\\mathcal{b}}_i^c(l)\\right]}+n_0\\bigg ) { \\,,}\\label{bunkai}\\end{aligned}\\ ] ] where @xmath109}$ ] is the indicator function and the superscript `` @xmath110 '' denotes the complementary set . in the following lemmas [ lem_difficult][lem_easy ] we bound the expectation of the above three terms and the proof",
    "is completed .",
    "[ lem_difficult ] if @xmath55 then @xmath111}\\right ] } { \\nonumber\\\\}&\\le \\frac{1}{1-{\\mathrm{e}}^{-\\frac{{\\epsilon}^2}{8 } } } + \\frac{1}{1-{\\mathrm{e}}^{-h(2 ) } } + \\frac{2\\sqrt{2}}{{\\epsilon } } \\frac{\\left(1+\\frac{{\\epsilon}^2}{8}\\right)^{1-{\\alpha}}}{1-\\left(1+\\frac{{\\epsilon}^2}{8}\\right)^{-1/2}}{\\nonumber\\\\}&\\quad+ \\frac{\\mathrm{b}(1/2,-{\\alpha})}{\\left(1-{\\mathrm{e}}^{-\\frac{{\\epsilon}^2}{2}}\\right)^2}{\\nonumber\\\\}&={\\mathrm{o}}({\\epsilon}^{-4}){\\,.}{\\nonumber}\\end{aligned}\\ ] ]    [ lem_main ] for any @xmath112 , @xmath113}\\right ] } { \\nonumber\\\\}&\\le \\frac{\\log n}{{d_{\\mathrm{inf}}}({\\delta}_i-2{\\epsilon},{\\sigma}_i^2+{\\epsilon } ) } + 2 - 2{\\alpha}+\\frac{\\sqrt{{\\sigma}_i^2+{\\epsilon}}}{{\\delta}_i-2{\\epsilon}}{\\nonumber\\\\}&=\\frac{\\log n}{{d_{\\mathrm{inf}}}({\\delta}_i-2{\\epsilon},{\\sigma}_i^2+{\\epsilon})}+{\\mathrm{o}}(1 ) { \\,.}{\\nonumber}\\end{aligned}\\ ] ]    [ lem_easy ] for any @xmath112 , @xmath114}\\right ] } { \\nonumber\\\\}&\\le \\frac{1}{1-{\\mathrm{e}}^{-\\frac{{\\epsilon}^2}{2{\\sigma}_i^2 } } } + \\frac{1}{1-{\\mathrm{e}}^{-h\\left(1+\\frac{{\\epsilon}}{{\\sigma}_i^2}\\right ) } } = { \\mathrm{o}}({\\epsilon}^{-2}){\\,.}{\\nonumber}\\end{aligned}\\ ] ]    we prove lemma [ lem_easy ] in appendix and prove lemmas [ lem_difficult ] and [ lem_main ] in this section .",
    "whereas the second term of becomes the main term of the regret , the evaluation of the first term is the most difficult point of the proof , which corresponds to lemma [ lem_difficult ] .",
    "in fact , it is reported in that they were not able to prove the asymptotic optimality of a policy for the normal distribution model because of difficulty of the evaluation corresponding to this term .",
    "also note that this is the term which does not become a constant in the case @xmath59 and is considered in the proof of theorem [ thm_impossible ] .",
    "in this paper we evaluate this term by first bounding this term for a fixed statistic @xmath41 and finally taking its expectation , whereas a probability on this statistic is first evaluated in . by leaving the evaluation on the distribution of @xmath115 to the latter part",
    ", we can significantly simplify the integral by variable transformation .    of lemma [ lem_difficult ]",
    "first we bound the summation as @xmath116 } } { \\nonumber\\\\}&= \\sum_{n = n_0}^{t}\\sum_{t = kn_0 + 1}^{t } { { \\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[j(t)\\neq 1{,\\;}{\\mathcal{a}}^c(t){,\\;}n_1(t)=n\\right]}{\\nonumber\\\\}&= \\sum_{n = n_0}^{t}\\sum_{m=1}^{t}{\\nonumber\\\\}&\\qquad{{\\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[m\\le \\sum_{t = kn_0 + 1}^{t } { { \\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[j(t)\\neq 1{,\\;}{\\mathcal{a}}^c(t){,\\;}n_1(t)=n\\right]}\\right]}.{\\nonumber}\\end{aligned}\\ ] ] note that @xmath117}{\\nonumber}\\end{aligned}\\ ] ] implies that @xmath118 occurred for the first @xmath119 elements of @xmath120 .",
    "therefore , @xmath121 } \\right ] } { \\nonumber\\\\}&\\le ( 1-{p_{n}(-{\\epsilon}|{{\\hat{\\theta}}_{1,n}})})^m{\\nonumber}\\end{aligned}\\ ] ] and we have @xmath111}\\right ] } { \\nonumber\\\\}&\\le { \\mathrm{e}}\\left [ \\sum_{n = n_0}^{t}\\sum_{m=1}^{t } ( 1-{p_{n}(-{\\epsilon}|{{\\hat{\\theta}}_{1,n}})})^m \\right]{\\nonumber\\\\}&\\le \\sum_{n = n_0}^{t } { \\mathrm{e}}\\left[\\frac{1-{p_{n}(-{\\epsilon}|{{\\hat{\\theta}}_{1,n}})}}{{p_{n}(-{\\epsilon}|{{\\hat{\\theta}}_{1,n } } ) } } \\right]{\\,.}\\label{matizikan}\\end{aligned}\\ ] ] since @xmath122 for @xmath123 from the symmetry of @xmath9-distribution , this expectation is partitioned into @xmath124 } { \\nonumber\\\\}&\\le 2{\\mathrm{e}}\\left[{{\\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[-{\\epsilon}\\le{\\bar{x}_{1,n}}\\right]}(1-{p_{n}(-{\\epsilon}|{{\\hat{\\theta}}_{1,n}})})\\right]{\\nonumber\\\\}&\\quad+ { \\mathrm{e}}\\left[\\frac{{{\\,\\mbox{\\rm",
    "1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[{\\bar{x}_{1,n}}\\le -{\\epsilon}\\right]}}{{p_{n}(-{\\epsilon}|{{\\hat{\\theta}}_{1,n}})}}\\right ] { \\nonumber\\\\}&\\le \\pr\\left[-{\\epsilon}<{\\bar{x}_{1,n}}\\le -{\\epsilon}/2\\right ] { \\nonumber\\\\}&\\quad + \\pr\\left[-{\\epsilon}/2<{\\bar{x}_{1,n}}{,\\;}{{s}_{1,n}}\\ge 2n\\right]{\\nonumber\\\\}&\\quad+2{\\mathrm{e}}\\big[{{\\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[-{\\epsilon}/2<{\\bar{x}_{i , n}}{,\\;}{{s}_{1,n}}\\le 2n\\right ] } ( 1-{p_{n}(-{\\epsilon}|{{\\hat{\\theta}}_{i , n}})})\\big]{\\nonumber\\\\}&\\quad + { \\mathrm{e}}\\left[\\frac{{{\\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[{\\bar{x}_{1,n}}\\le -{\\epsilon}\\right]}}{{p_{n}(-{\\epsilon}|{{\\hat{\\theta}}_{1,n}})}}\\right ] .",
    "\\label{partition4}\\end{aligned}\\ ] ] from lemma [ lem_ldp ] , the first and second terms of are bounded as @xmath125 & \\le { \\mathrm{e}}^{-\\frac{n{\\epsilon}^2}{8}}{\\,,}{\\nonumber\\\\}\\pr\\left[-{\\epsilon}/2<{\\bar{x}_{1,n}}{,\\;}{{s}_{1,n}}\\ge 2n\\right ] & \\le{\\mathrm{e}}^{-nh(2)}{\\,,}\\label{term3}\\end{aligned}\\ ] ] respectively . next , recall that @xmath126 .",
    "then , from the symmetry of @xmath9-distribution @xmath127 and the third term of is bounded from as @xmath128 } ( 1-{p_{n}(-{\\epsilon}|{{\\hat{\\theta}}_{i , n}})})\\big ] } { \\nonumber\\\\}&= { \\mathrm{e}}\\big[{{\\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[-{\\epsilon}/2<{\\bar{x}_{i , n}}{,\\;}{{s}_{1,n}}\\le 2\\right ] } { p_{n}(2{\\bar{x}_{i , n}}-{\\epsilon}|{{\\hat{\\theta}}_{i , n}})}\\big]{\\nonumber\\\\}&\\le \\frac{2\\sqrt{2}}{{\\epsilon } } \\left(1+\\frac{{\\epsilon}^2}{8}\\right)^{-\\frac{n}{2}-{\\alpha}+1}{\\,.}\\label{term4}\\end{aligned}\\ ] ] finally we evaluate the fourth term of . from and , we have @xmath129}}{{p_{n}(-{\\epsilon}|{{\\hat{\\theta}}_{1,n}})}}\\right ] } { \\nonumber\\\\}&\\le \\frac1{a_{n,{\\alpha}}}\\int_{-\\infty}^{-{\\epsilon}}\\int_{0}^{\\infty } \\left(1+\\frac{n(x+{\\epsilon})^2}{s}\\right)^{\\frac{n-1}{2}+{\\alpha}}{\\nonumber\\\\}&\\qquad\\cdot \\sqrt{\\frac{n}{2\\pi}}{\\mathrm{e}}^{-\\frac{nx^2}{2 } } \\frac{s^{\\frac{n-3}{2 } } { \\mathrm{e}}^{-\\frac{s}{2}}}{2^{\\frac{n-1}{2}}{\\gamma}(\\frac{n-1}{2 } ) } { \\mathrm{d}}s{\\mathrm{d}}x{\\nonumber\\\\}&\\le \\frac{{\\mathrm{e}}^{-n{\\epsilon}^2/2}}{a_{n,{\\alpha}}}\\int_{-\\infty}^{-{\\epsilon}}\\int_{0}^{\\infty } \\left(1+\\frac{n(x+{\\epsilon})^2}{s}\\right)^{\\frac{n-1}{2}+{\\alpha}}{\\nonumber\\\\}&\\qquad\\cdot \\sqrt{\\frac{n}{2\\pi}}{\\mathrm{e}}^{-\\frac{n(x+{\\epsilon})^2}{2 } } \\frac{s^{\\frac{n-3}{2 } } { \\mathrm{e}}^{-\\frac{s}{2}}}{2^{\\frac{n-1}{2}}{\\gamma}(\\frac{n-1}{2 } ) } { \\mathrm{d}}s{\\mathrm{d}}x\\label{main_sekibun}\\ ] ] by @xmath130 for @xmath131 . by letting @xmath132 we have @xmath133 and is rewritten as @xmath134}}{{p_{n}(-{\\epsilon}|{{\\hat{\\theta}}_{i , n}})}}\\right ] } { \\nonumber\\\\}&\\le \\frac{{\\mathrm{e}}^{-n{\\epsilon}^2/2}}{a_{n,{\\alpha}}}\\int_{0}^{1}\\int_{0}^{\\infty } \\left(1+\\frac{w}{1-w}\\right)^{\\frac{n-1}{2}+{\\alpha}}{\\nonumber\\\\}&\\qquad\\cdot \\sqrt{\\frac{n}{2\\pi}}{\\mathrm{e}}^{-zw } \\frac{(z(1-w))^{\\frac{n-3}{2 } } { \\mathrm{e}}^{-z(1-w)}}{2{\\gamma}(\\frac{n-1}{2 } ) } \\sqrt{\\frac{2z}{nw}}{\\mathrm{d}}z { \\mathrm{d}}w{\\nonumber\\\\}&= \\frac{{\\mathrm{e}}^{-n{\\epsilon}^2/2}}{2\\sqrt{\\pi}a_{n,{\\alpha}}{\\gamma}(\\frac{n-1}{2})}\\int_{0}^{1 } w^{-1/2}(1-w)^{-1-{\\alpha}}{\\mathrm{d}}w { \\nonumber\\\\}&\\qquad\\cdot \\int_{0}^{\\infty } { \\mathrm{e}}^{-z } z^{\\frac{n}{2}-1}{\\mathrm{d}}z { \\nonumber\\\\}&= \\frac{{\\mathrm{e}}^{-n{\\epsilon}^2/2}}{2\\sqrt{\\pi}a_{n,{\\alpha}}{\\gamma}(\\frac{n-1}{2 } ) } \\mathrm{b}(1/2 , -{\\alpha } ) \\gamma(n/2){\\nonumber\\\\}&\\le \\frac{{\\mathrm{e}}^{1/3}\\sqrt{(n+2{\\alpha})(n-1)}}{2 } { \\mathrm{e}}^{-n{\\epsilon}^2/2}\\mathrm{b}(1/2 , -{\\alpha } ) { \\nonumber\\\\}&\\phantom{wwwwwwwwwwwwwwww}(\\mbox{by lemma \\ref{lem_gamma } and \\eqref{eq_an}}){\\nonumber\\\\}&\\le n{\\mathrm{e}}^{-n{\\epsilon}^2/2}\\mathrm{b}(1/2",
    ", -{\\alpha}){\\,.}\\;\\,\\quad \\mbox{(by $ { \\mathrm{e}}^{1/3}\\le 2 $ and $ { \\alpha}<0 $ ) } \\label{term1}\\end{aligned}\\ ] ] by combining , , , with , we obtain @xmath135}\\right ] } { \\nonumber\\\\}&\\le \\sum_{n = n_0}^t\\bigg ( { \\mathrm{e}}^{-\\frac{n{\\epsilon}^2}{8}}+{\\mathrm{e}}^{-nh(2)}+ \\frac{2\\sqrt{2}}{{\\epsilon } } \\left(1+\\frac{{\\epsilon}^2}{8}\\right)^{-\\frac{n}{2}-{\\alpha}+1}{\\nonumber\\\\}&\\phantom{wwwwwwwwwwwwwww}+ n{\\mathrm{e}}^{-n{\\epsilon}^2/2}\\mathrm{b}(1/2 , -{\\alpha})\\bigg){\\nonumber\\\\}&\\le \\frac{1}{1-{\\mathrm{e}}^{-\\frac{{\\epsilon}^2}{8 } } } + \\frac{1}{1-{\\mathrm{e}}^{-h(2 ) } } + \\frac{2\\sqrt{2}}{{\\epsilon } } \\frac{\\left(1+\\frac{{\\epsilon}^2}{8}\\right)^{1-{\\alpha}}}{1-\\left(1+\\frac{{\\epsilon}^2}{8}\\right)^{-1/2}}{\\nonumber\\\\}&\\quad+ \\frac{\\mathrm{b}(1/2,-{\\alpha})}{\\left(1-{\\mathrm{e}}^{-\\frac{{\\epsilon}^2}{2}}\\right)^2}{\\nonumber\\\\}&={\\mathrm{o}}({\\epsilon}^{-2})+{\\mathrm{o}}(1)+{\\mathrm{o}}({\\epsilon}^{-3})+{\\mathrm{o}}({\\epsilon}^{-4})={\\mathrm{o}}({\\epsilon}^{-4 } ) { \\,.}\\!\\!\\dqed{\\nonumber}\\end{aligned}\\ ] ]    let @xmath136 be arbitrary . then @xmath137 } } { \\nonumber\\\\}&\\le \\sum_{t = kn_0 + 1}^t { { \\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[{\\tilde{\\mu}}_i(t)\\ge -{\\epsilon}{,\\;}b_i(l)\\right]}{\\nonumber\\\\}&\\le n_i+ \\sum_{t = kn_0 + 1}^{t } { { \\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[{\\tilde{\\mu}}_i(t)\\ge -{\\epsilon}{,\\;}{\\mathcal{b}}_i(t),\\,n_i(t)\\ge n_i\\right]}{\\,.}\\label{lognterm}\\end{aligned}\\ ] ] under the condition @xmath138 , the probability of the event @xmath139 is bounded from lemma [ lem_upper ] as @xmath140 therefore the expectation of is bounded as @xmath141 } \\right ] } { \\nonumber\\\\}&\\le n_i+ \\sum_{t = kn_0 + 1}^{t } \\pr\\left[{\\tilde{\\mu}}_i(t)\\ge -{\\epsilon}{,\\;}{\\mathcal{b}}_i(t),\\,n_i(t)\\ge n_i\\right]{\\nonumber\\\\}&\\le n_i+ t\\frac{\\sqrt{{\\sigma}_i^2+{\\epsilon}}}{{\\delta}_i-2{\\epsilon } } { \\mathrm{e}}^{-(n_i+2{\\alpha}-2){d_{\\mathrm{inf}}}({\\delta}_i-2{\\epsilon } , { \\sigma}_i^2+{\\epsilon})}{\\nonumber}\\end{aligned}\\ ] ] and we complete the proof by letting @xmath142 .",
    "in this section we prove theorem [ thm_impossible ] . as mentioned before , the evaluation in the proof corresponds to lemma [ lem_difficult ] , in which @xmath55 is required so that @xmath143 becomes finite .",
    "we show in the following proof that this requirement is actually necessary to achieve the asymptotic bound .",
    "we assume @xmath96 without loss of generality .",
    "fix any @xmath92 and let @xmath144 be the first round at which the number of samples from the arm 1 is @xmath16 , that is , we define @xmath145 . since @xmath146 implies that the arm @xmath147 is pulled @xmath148 times through the first @xmath149 rounds , we have @xmath150 } { \\nonumber\\\\}&= { \\delta}_i\\sum_{t=1}^{\\infty}\\pr[t_{1,n}=t]{\\mathrm{e}}\\left [ \\sum_{m=1}^{2t}{{\\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[j(m)=2\\right ] } \\bigg|t_{1,n}=t\\right]{\\nonumber\\\\}&\\ge{\\delta}_i\\sum_{t=1}^{t}\\pr[t_{1,n}=t ] { \\mathrm{e}}\\left[\\sum_{m=1}^{2t}{{\\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[j(m)=2\\right]}\\bigg|t_{1,n}=t\\right]{\\nonumber\\\\}&\\quad+{\\delta}_i\\sum_{t = t+1}^{\\infty}\\pr[t_{1,n}=t](t - n){\\,.}\\label{bunkai_impo}\\end{aligned}\\ ] ]",
    "now we consider the case @xmath151 .",
    "the conditional expectation in is bounded as @xmath152}\\bigg|t_{1,n}=t\\right ] } { \\nonumber\\\\}&\\ge{\\mathrm{e}}\\left[\\sum_{m = t}^{t+t-1 } { { \\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[j(m)=2{,\\;}n_1(m)=n\\right]}\\bigg|t_{1,n}=t\\right]{\\nonumber}\\end{aligned}\\ ] ] note that if @xmath153 then @xmath154 for any @xmath155 .",
    "therefore , for any @xmath156 , @xmath157 since @xmath65 always holds . by defining @xmath158",
    "we have @xmath159}\\bigg|t_{1,n}=t\\right ] } { \\nonumber\\\\}&\\ge{\\mathrm{e}}\\left[\\sum_{m = t}^{t+t-1 } { { \\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[\\bigcup_{k=0}^{m - t } \\{{\\tilde{\\mu}}_1(t+k)<\\mu_2\\}\\right]}\\bigg|t_{1,n}=t\\right]{\\nonumber\\\\}&={\\mathrm{e}}\\left[\\sum_{m = t}^{t+t-1}({\\bar{p}_{n}(\\mu_2|{{\\hat{\\theta}}_{1,n}})})^{m - t+1}\\right]{\\nonumber\\\\}&\\ge{\\mathrm{e}}\\left[\\sum_{m=1}^{t}({\\bar{p}_{n}(\\mu_2|{{\\hat{\\theta}}_{1,n}})})^{m}\\right]{\\nonumber\\\\}&={\\mathrm{e}}\\left [ \\left(1-({\\bar{p}_{n}(\\mu_2|{{\\hat{\\theta}}_{1,n}})})^{t}\\right)\\frac{{\\bar{p}_{n}(\\mu_2|{{\\hat{\\theta}}_{1,n}})}}{{p_{n}(\\mu_2|{{\\hat{\\theta}}_{1,n } } ) } } \\right]{\\nonumber\\\\}&\\ge\\frac12{\\mathrm{e}}\\left [ { { \\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[({\\bar{p}_{n}(\\mu_2|{{\\hat{\\theta}}_{1,n}})})^t\\le 1/2\\right ] } \\frac{{\\bar{p}_{n}(\\mu_2|{{\\hat{\\theta}}_{1,n}})}}{{p_{n}(\\mu_2|{{\\hat{\\theta}}_{1,n } } ) } } \\right]{\\nonumber\\\\}&\\ge\\frac12{\\mathrm{e}}\\left [ \\frac { { { \\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[({\\bar{p}_{n}(\\mu_2|{{\\hat{\\theta}}_{1,n}})})^t\\le 1/2\\right ] } } { { p_{n}(\\mu_2|{{\\hat{\\theta}}_{1,n } } ) } } \\right]-\\frac12{\\,.}\\label{sita_id}\\\\ & \\phantom{wwwwwwwwwwwwww}(\\mbox{by $ ( 1-p)/p= 1/p-1$}){\\nonumber}\\end{aligned}\\ ] ] here we obtain from that @xmath160 therefore the expectation in is bounded from as @xmath161}}{{p_{n}(\\mu_2|{{\\hat{\\theta}}_{1,n } } ) } } \\right ] } { \\nonumber\\\\}&\\ge \\!\\!\\ ! \\operatorname*{\\rm \\iint } _",
    "{ \\begin{array}{c } { \\scriptstyle x\\le \\mu_2,\\,s\\ge 0,}\\\\ { \\scriptstyle \\frac{n(\\mu_2-x)^2}{s}\\le c_t } \\end{array } } \\!\\!\\ ! \\frac{\\mu_2-x}{\\sqrt{s } } \\left(1+\\frac{n(\\mu_2-x)^2}{s}\\right)^{\\frac{n}{2}+{\\alpha}-1}{\\nonumber\\\\}&\\phantom{wwwwwwwwwwwww}\\cdot \\sqrt{\\frac{n}{2\\pi } } { \\mathrm{e}}^{-\\frac{nx^2}{2 } } \\frac{s^{\\frac{n-3}{2}}{\\mathrm{e}}^{-\\frac{s}{2}}}{2^{\\frac{n-1}{2}}{\\gamma}(\\frac{n-1}{2 } ) } { \\mathrm{d}}x { \\mathrm{d}}s{\\nonumber\\\\}&= \\frac{\\sqrt{n}{\\mathrm{e}}^{-\\mu_2 ^ 2/2}}{\\sqrt{\\pi}2^{\\frac{n}{2}}{\\gamma}(\\frac{n-1}{2 } ) } \\!\\!\\ ! \\operatorname*{\\rm \\iint } _ { \\begin{array}{c } { \\scriptstyle x\\le\\mu_2,\\,s\\ge 0,}\\\\ { \\scriptstyle \\frac{n(\\mu_2-x)^2}{s}\\le c_t } \\end{array } } \\!\\!\\ ! { \\mathrm{e}}^{-\\frac{n(\\mu - x)^2}{2}+2\\mu_2(\\mu_2-x ) } { \\nonumber\\\\}&\\phantom{wi}\\cdot ( \\mu_2-x ) \\left(1+\\frac{n(\\mu_2-x)^2}{s}\\right)^{\\frac{n}{2}+{\\alpha}-1 } s^{\\frac{n}{2}-2}{\\mathrm{e}}^{-\\frac{s}{2 } } { \\mathrm{d}}x { \\mathrm{d}}s{\\,.}{\\nonumber}\\ ] ] by letting @xmath162 we obtain in a similar way to that @xmath163 } } { { p_{n}(\\mu_2|{{\\hat{\\theta}}_{1,n } } ) } } \\right ] } { \\nonumber\\\\}&\\ge \\frac{{\\mathrm{e}}^{-\\mu_2 ^ 2/2}}{2\\sqrt{\\pi n}{\\gamma}(\\frac{n-1}{2 } ) } \\int_{0}^{\\frac{1}{1+\\frac{1}{c_t } } } \\int_{0}^{\\infty } { \\mathrm{e}}^{-z}{\\mathrm{e}}^{2\\mu_2\\sqrt{\\frac{2zw}{n } } } { \\nonumber\\\\}&\\qquad\\cdot z^{\\frac{n}{2}-1 } ( 1-w)^{-1-{\\alpha}}{\\mathrm{d}}z { \\mathrm{d}}w{\\nonumber\\\\}&\\ge \\frac{{\\mathrm{e}}^{-\\mu_2 ^ 2/2}}{2\\sqrt{\\pi n}{\\gamma}(\\frac{n-1}{2 } ) } \\int_{0}^{\\infty } { \\mathrm{e}}^{2\\mu_2\\sqrt{\\frac{2z}{n } } } { \\mathrm{e}}^{-z}z^{\\frac{n}{2}-1 } { \\mathrm{d}}z{\\nonumber\\\\}&\\phantom{wi}\\cdot \\int_{0}^{\\frac{1}{1+\\frac{1}{c_t } } } ( 1-w)^{-1-{\\alpha}}{\\mathrm{d}}w{\\,.}\\qquad(\\mbox{by $ \\mu_2<\\mu_1=0 $ } ) { \\nonumber}\\end{aligned}\\ ] ] here note that @xmath164 then there exists a constant @xmath165 such that @xmath163 } } { { p_{n}(\\mu_2|{{\\hat{\\theta}}_{1,n } } ) } } \\right ] } { \\nonumber\\\\}&\\ge \\begin{cases } b_{n,{\\alpha},\\mu_2}\\log ( 1+c_t),&{\\alpha}=0,\\\\ b_{n,{\\alpha},\\mu_2}((1+c_t)^{{\\alpha}}-1),&{\\alpha}>0 . \\end{cases}\\label{impo_owari}\\end{aligned}\\ ] ] finally by putting , and together we obtain for @xmath77 that @xmath166 } { \\nonumber\\\\}&\\ge { \\delta}_2\\min\\left\\ { t - n,\\ , ( 1/2)b_{n,{\\alpha},\\mu_2}\\log ( 1+c_t)-1/2 \\right\\}.{\\nonumber}\\end{aligned}\\ ] ] eq . follows since @xmath92 is fixed and @xmath167 defined in is polynomial in @xmath12 .",
    "eq . for @xmath68 is obtained in the same way .",
    "we considered the stochastic multiarmed bandit problem such that each reward follows a normal distribution with an unknown mean and variance .",
    "we proved that thompson sampling with prior @xmath168 achieves the asymptotic bound if @xmath55 but can not if @xmath59 , which includes reference prior @xmath77 and jeffreys prior @xmath76 .",
    "a future work is to examine whether ts with non - informative priors is risky or not for other multiparameter models where ts is used without theoretical analysis ( see e.g. , ) .",
    "since the analysis of this paper heavily depends on the specific form of normal distributions , it is currently unknown whether the technique of this paper can be applied to other models and this generalization remains as an important open problem .",
    "we prove lemmas [ lem_gamma ] , [ lem_ldp ] , [ lem_upper ] and [ lem_easy ] in this appendix .",
    "let @xmath172 be i.i.d .",
    "random variables on @xmath173 .",
    "then , for @xmath174 and any convex set @xmath175 , @xmath176\\le \\exp\\left(-\\inf_{z\\in c}\\lambda^*(z)\\right){\\,,}{\\nonumber}\\end{aligned}\\ ] ] where @xmath177\\}{\\,.}{\\nonumber}\\end{aligned}\\ ] ]      now we show .",
    "let @xmath179 .",
    "then it is easy to see that the fenchel - legendre transform of the cumulant generating function of @xmath180 is given by @xmath181 eq .",
    "follows from @xmath182 } { \\nonumber\\\\}&= \\pr[\\bar{z}^{(2)}-(\\bar{z}^{(1)})^2\\ge { \\sigma}^2]{\\nonumber\\\\}&\\le \\exp\\left(-n \\inf_{(z^{(1)},z^{(2)}):\\,z^{(2)}-(z^{(1)})^2\\ge { \\sigma}^2}\\lambda^*(z^{(1)},z^{(2 ) } ) \\right){\\nonumber\\\\}&\\le \\exp\\left(-n h\\left(\\frac{{\\sigma}^2}{{\\sigma}_i^2}\\right ) \\right){\\,,}{\\nonumber}\\end{aligned}\\ ] ] where the first and the second inequalities follow because @xmath183 is a convex set and @xmath184 is increasing in @xmath185 , respectively .        on the other hand , the integral is bounded from above by @xmath191_{\\infty}^{x_0}{\\nonumber\\\\}&\\quad-\\tilde{a}\\int_{x_0}^{\\infty } \\frac2{x^2}\\frac{\\frac{n-1}{2}+{\\alpha}}{\\frac{n-2}{2}+{\\alpha } } \\left(1+\\frac{x^2}{n+2{\\alpha}-1}\\right)^{-\\frac{n}{2}-{\\alpha}+1 } { \\mathrm{d}}x{\\nonumber\\\\}&\\le \\frac{\\tilde{a}}{x_0}\\frac{\\frac{n-1}{2}+{\\alpha}}{\\frac{n-2}{2}+{\\alpha } } \\left(1+\\frac{x_0 ^ 2}{n+2{\\alpha}-1}\\right)^{-\\frac{n}{2}-{\\alpha}+1}{\\,.}{\\nonumber}\\end{aligned}\\ ] ] from lemma [ lem_gamma ] @xmath192 and we complete the proof .      of lemma [ lem_easy ]",
    "first we have @xmath193 } } { \\nonumber\\\\}&= \\sum_{n = n_0}^{t}{{\\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left[\\bigcup_{t = kn_0 + 1}^t \\{j(t)=i{,\\;}{\\mathcal{b}}_i^c(t){,\\;}n_i(t)=n\\}\\right]}{\\nonumber\\\\}&\\le \\sum_{n = n_0}^{t}{{\\,\\mbox{\\rm 1}\\hspace{-0.63em}\\mbox{\\rm \\small 1\\,}\\!}\\left [ { \\bar{x}_{i , n}}\\ge \\mu_i+{\\delta}\\mbox { or } { { s}_{i , n}}\\ge n({\\sigma}_i^2+{\\epsilon } ) \\right]}{\\,.}{\\nonumber}\\end{aligned}\\ ] ] therefore , from lemma [ lem_ldp ] , @xmath114}\\right ] } { \\nonumber\\\\}&\\le \\sum_{n = n_0}^{t}\\left ( { \\mathrm{e}}^{-n\\frac{{\\epsilon}^2}{2{\\sigma}_i^2}}+ { \\mathrm{e}}^{-nh\\left(1+\\frac{{\\epsilon}}{{\\sigma}_i^2}\\right ) } \\right){\\nonumber\\\\}&\\le \\frac{1}{1-{\\mathrm{e}}^{-\\frac{{\\epsilon}^2}{2{\\sigma}_i^2 } } } + \\frac{1}{1-{\\mathrm{e}}^{-h\\left(1+\\frac{{\\epsilon}}{{\\sigma}_i^2}\\right)}}{\\nonumber\\\\}&={\\mathrm{o}}({\\epsilon}^{-2})+{\\mathrm{o}}({\\epsilon}^{-2})={\\mathrm{o}}({\\epsilon}^{-2 } ) { \\,.}\\dqed{\\nonumber}\\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> in stochastic bandit problems , a bayesian policy called thompson sampling ( ts ) has recently attracted much attention for its excellent empirical performance . </S>",
    "<S> however , the theoretical analysis of this policy is difficult and its asymptotic optimality is only proved for one - parameter models . in this paper </S>",
    "<S> we discuss the optimality of ts for the model of normal distributions with unknown means and variances as one of the most fundamental example of multiparameter models . </S>",
    "<S> first we prove that the expected regret of ts with the uniform prior achieves the theoretical bound , which is the first result to show that the asymptotic bound is achievable for the normal distribution model . </S>",
    "<S> next we prove that ts with jeffreys prior and reference prior can not achieve the theoretical bound . </S>",
    "<S> therefore the choice of priors is important for ts and non - informative priors are sometimes risky in cases of multiparameter models . </S>"
  ]
}