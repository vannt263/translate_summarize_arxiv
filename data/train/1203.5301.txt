{
  "article_text": [
    "a typical fluid flow is random or chaotic in the turbulent and instability regimes .",
    "therefore we need to employ accurate numerical schemes for simulating such flows .",
    "a pseudospectral algorithm  @xcite is one of the most accurate methods for solving fluid flows , and it is employed for performing direct numerical simulations of turbulent flows , as well as for critical applications like weather predictions and climate modelling .",
    "yokokawa et al .",
    "@xcite , donzis et al .",
    "@xcite , and pouquet et al .",
    "@xcite have performed spectral simulations on some of the largest grids ( e.g. , @xmath2 ) .",
    "we have developed a general - purpose flow solver named tarang ( synonym for waves in sanskrit ) for turbulence and instability studies .",
    "tarang is a parallel and modular code written in object - oriented language c++ . using tarang",
    ", we can solve incompressible flows involving pure fluid , rayleigh - bnard convection , passive and active scalars , magnetohydrodynamics , liquid - metals , etc .",
    "tarang is an open - source code and it can be downloaded from http://turbulence.phy.iitk.ac.in . in this paper",
    "we will describe some details of the code , scaling results , and code validation performed on tarang .",
    "the basic steps of tarang follow the standard procedure of pseudospectral method  @xcite .",
    "the navier - stokes and related equations are numerically solved given an initial condition of the fields .",
    "the fields are time - stepped using one of the time integrators .",
    "the nonlinear terms , e.g. @xmath3 , transform to convolutions in the spectral space , which are very expensive to compute .",
    "orszag devised a clever scheme to compute the convolution in an efficient manner using fast fourier transforms ( fft )  @xcite . in this scheme ,",
    "the fields are transformed from the fourier space to the real space , multiplied with each other , and then transformed back to the fourier space .",
    "note that the spectral transforms could involve fourier functions , sines and cosines , chebyshev polynomials , spherical harmonics , or a combination of these functions depending on the boundary conditions .",
    "for details the reader is referred to standard references , e.g. , the books by boyd  @xcite and canuto et al .  @xcite .",
    "some of the specific choices made in tarang are as follow :    a.   in the turbulent regime , the two relevant time scales , the large - eddy turnover time and the small - scale viscous time , are very different ( order of magnetic apart ) . to handle this feature , we use the  exponential trick \" that absorbs the viscous term using a change of variable  @xcite .",
    "b.   we use the fourth - order runge - kutta scheme for time stepping .",
    "the code however has an option to use the euler and the second - order runge - kutta schemes as well .",
    "c.   the code provides an option for dealiasing the fields .",
    "the 3/2 rule is used for dealiasing  @xcite .",
    "d.   the wavenumber components @xmath4 are @xmath5 where @xmath6 is the box dimension in the @xmath7-th direction , and @xmath8 is an integer .",
    "we use parameters @xmath9 to control the box size , especially for rayleigh - bnard convection . note that typical spectral codes take @xmath10 , or @xmath11 .",
    "the parallel implementation of tarang involved parallelization of the spectral transforms and the input - output operations , as described below .",
    "a pseudospectral code involves forward and inverse transforms between the spectral and real space . in a typical pseudospectral code , these operations take approximately 80% of the total time .",
    "therefore , we use one of the most efficient parallel fft routines , fftw ( fastest fourier transform in the west )  @xcite , in tarang . we adopt fftw s strategy for dividing the arrays . if @xmath12 is the number of available processors , we divide each of the arrays into @xmath12  slabs \" .",
    "for example , a complex array @xmath13 is split into @xmath14 segments , each of which is handled by a single processor . this division is called  slab decomposition \" .",
    "the other time - consuming tasks in tarang are the input and output ( i / o ) operations of large data sets , and the element - by - element multiplication of arrays .",
    "the data sets in tarang are massive , for example , the data size of a @xmath2 fluid simulation is of the order of 1.5 terabytes .",
    "for i / o operations , we use an efficient and parallel library named hdf5 ( hierarchical data format-5 ) . the third operation",
    ", element - by - element multiplication of arrays , is handled by individual processors in a straightforward manner .",
    "tarang has been organized in a modular fashion , so the spectral transforms and i / o operations were easily parallelized . for a periodic - box ,",
    "we use the parallel fftw library itself .",
    "however , for the mixed transforms ( e.g. , sine transform along @xmath15 , and fourier transform along @xmath16 plane ) , we parallelize the transforms ourselves using one- and two - dimensional fftw transforms .",
    "an important aspect of any parallel simulation code is its scalability .",
    "we tested the scaling of fftw and tarang by performing simulations on @xmath0 , @xmath1 , and @xmath2 grids with variable number of processors .",
    "the simulations were performed on the _ hpc system _ of iit kanpur and _ shaheen _ supercomputer of king abdullah university of science and technology ( kaust ) .",
    "the _ hpc system _ has 368 compute nodes connected via a 40 gbps qlogic infiniband switch with each node containing dual intel xeon quadcore c5570 processor and 48 gb of ram .",
    "its peak performance ( rpeak ) is approximately 34 teraflops ( tera floating point operations per second ) . _",
    "shaheen _ on the other hand is a 16-rack ibm bluegene / p system with 65536 cores and 65536 gb of ram . _",
    "shaheen _ s peak performance is approximately 222 teraflops .     and @xmath2 grids with single precision computation .",
    "the straight lines represent the ideal linear scaling . ]    for parallel fft with slab decomposition , we compute the time taken per step ( forward+backward transform ) on _ shaheen _ for several large @xmath17 grids .",
    "the results displayed in fig .",
    "[ fig : bg_slab_fft ] demonstrate an approximate linear scaling ( called  strong scaling \" ) .",
    "using the fact that each forward plus inverse fft involves @xmath18 operations for single precision computations  @xcite , the average fft performance per core on _ shaheen _ is approximately 0.3 gigaflops , which is only 8% of its peak performance .",
    "similar efficiency is observed for the _ hpc system _ as well , whose cores have rating of approximately 12 gigaflops .",
    "the aforementioned loss of efficiency is consistent with the other fft libraries , e.g , p3dfft  @xcite .",
    "also note that an increase in the data size and number of processors ( resources ) by a same amount takes approximately the same time ( see fig .",
    "[ fig : bg_slab_fft ] ) .",
    "for example , fft of a @xmath0 array using 128 processors , as well as that of a @xmath1 array on 1024 processors , takes approximately 4 seconds .",
    "thus our implementation of fft shows good  weak scaling \" as well .     and @xmath1 grids with single precision computation .",
    "the straight lines represent the ideal linear scaling . ]    , @xmath1 , and @xmath2 grids with single precision computation .",
    "the straight lines represent the ideal linear scaling . ]     and @xmath1 grids with single precision computation .",
    "the straight lines represent the ideal linear scaling . ]",
    "we also test the scaling of tarang on _ shaheen _ and the _",
    "hpc system_. figs .",
    "[ fig : bg_tarang_fluid ] and [ fig : hpc_tarang_fluid ] exhibit the scaling results of fluid simulations performed on these systems .",
    "[ fig : bg_tarang_mhd_gnu ] shows the scaling results for magnetohydrodynamics ( mhd ) simulation on shaheen .",
    "these plots demonstrate _ strong scaling _ of tarang , consistent with the aforementioned fft scaling .",
    "sometimes we observe a small loss of efficiency when @xmath19 .",
    "we also observe approximate _",
    "weak scaling _ for tarang on both _",
    "shaheen _ and the _",
    "hpc system_.    a critical limitation of the  slab decomposition \" is that the number of processor can not be more than @xmath20 .",
    "this limitation can be overcome in a new scheme called  pencil decomposition \" in which the array @xmath13 is split into @xmath21 pencils where the total number of processors @xmath22  @xcite .",
    "we are in the process of implementing",
    " pencil decomposition \" on tarang . in this paper",
    "we will focus only on the  slab decomposition \" .    after the above discussion on parallelization of the code",
    ", we will discuss code validation , and time and space complexities for simulations of fluid turbulence , rayleigh - bnard convection , and magnetohydrodynamic turbulence .",
    "the governing equations for incompressible fluid turbulence are @xmath23 where @xmath24 is the velocity field , @xmath12 is the pressure field , @xmath25 is the kinematic viscosity , and @xmath26 is the external forcing . for studies on homogeneous and isotropic turbulence , simulations are performed on high - resolution grids ( e.g , @xmath27 ) with a periodic boundary condition .",
    "the resolution requirement is stringent due to @xmath28 relation ; for @xmath29 , the required grid resolution is approximately @xmath30 , which is quite challenging even for modern supercomputers .",
    "regarding the space complexity of a forced fluid turbulence simulation , tarang requires 15 arrays ( for @xmath31 , and three temporary arrays ) , which translates to approximately 120 gigabytes ( 8 terabytes ) of memory for @xmath0 ( @xmath2 ) double - precision computations . here * k * and * r * represent the wavenumbers and the real space coordinates respectively . the requirement is halved for a simulation with single precision .",
    "regarding the time requirement , each numerical step of the fourth - order runge - kutta ( rk4 ) scheme requires @xmath32 fft operations .",
    "the factor 9 is due to the 3 inverse and 6 forward transforms performed for each of the four rk4 iterates .",
    "therefore , for every time step , all the fft operations require @xmath33 multiplications for a single precision simulation  @xcite , which translates to approximately 2.9 ( 185 ) tera floating - point operations for @xmath0 ( @xmath2 ) grids .",
    "the number of operations for double - precision computation is twice of the above estimate . on 128 processors on _ hpc system _ ,",
    "a fluid simulation with single - precision takes approximately 36 seconds ( see fig .  [",
    "fig : hpc_tarang_fluid ] ) , which corresponds to per core performance of approximately 0.68 gigaflops .",
    "this is only 6% of the peak performance of the cores , which is consistent with the efficiency of fft operations discussed in section  [ sec : pstrategy ] .",
    "also note that the solver also involves other operations , e.g. , element - by - element array multiplication , but these operations take only a small fraction of the total time .",
    "we can also estimate the total time required to perform a @xmath2 fluid simulation .",
    "a typical fluid turbulence would require 5 eddy turnover time with @xmath34 , which corresponds to @xmath35 time steps for the simulations .",
    "so the total floating point operations required for this single - precision simulation is @xmath36 tera floating - point operations for the fft itself . assuming 5% efficiency for fft , and ffts share being 80% of the total time",
    ", the aforementioned fluid simulation will take approximately 128 hours on a 100 teraflop cluster .",
    "we perform code validation of the fluid solver using kolmgorov s theory  @xcite for the third - order structure function , according to which @xmath37 where @xmath38 is the energy flux in the inertial range , and @xmath39 represents ensemble averaging ( here spatial averaging ) .",
    "we compute the structure function @xmath40 , as well as @xmath41 , @xmath42 , and @xmath43 for the steady - state dataset of a fluid simulation on a @xmath0 grid .",
    "the computed values of @xmath44 are illustrated in fig .",
    "[ fig : str_fn ] that shows a good agreement with kolmogorov s theory .     vs. @xmath45 for a fluid simulation using tarang .",
    "here @xmath38 is the energy flux , and @xmath46 is the kolmogorov scale . ]    after the discussion on fluid solver , we move on to the module for solving rayleigh - bnard convection .",
    "rayleigh - bnard convection ( rbc ) is an idealized model of convection in which fluid is subjected between two plates that are separated by a distance @xmath47 , and are maintained at temperatures @xmath48 and @xmath49 .",
    "the equations for the above fluid under boussinesq approximations are @xmath50 where @xmath51 and @xmath52 are the temperature and pressure fluctuations from the steady conduction state ( @xmath53 with @xmath54 as the conduction temperature profile ) , @xmath55 is the buoyancy direction , @xmath56 is the temperature difference between the two plates , @xmath25 is the kinematic viscosity , and @xmath57 is the thermal diffusivity .",
    "we solve the nondimensionalized equations , which are obtained using @xmath47 as the length scale , @xmath58 as the velocity scale , and @xmath56 as the temperature scale : @xmath59 here the two important nondimensional parameters are the rayleigh number @xmath60 , and the prandtl number @xmath61 .",
    "in tarang we can apply the free - slip boundary condition for the velocity fields at the horizontal plates , i.e. , @xmath62 and isothermal boundary condition on the horizontal plates @xmath63 periodic boundary conditions are applied to the vertical boundaries .",
    "the number of arrays required for a rbc simulation is 18 ( 15 for fluids plus three for @xmath64 ) .",
    "thus the memory requirement for rbc is ( 18/15 ) times that for the fluid simulation .",
    "regarding the time complexity , the number of fft operations required per time step is @xmath65 fft operations ( 4 inverse + 9 forward transforms per rk4 step ) . as a result ,",
    "the total time requirement for a rbc simulation is ( 13/9 ) times the respective fluid simulation .    for code validation of tarang s rbc solver",
    ", we compare the nusselt number @xmath66 computed using tarang with that computed by thual  @xcite for two - dimensional free - slip box .",
    "the analysis is performed for the steady - state dataset .",
    "the comparative results shown in table  [ rbc_table ] illustrate excellent agreement between the two runs .",
    "we also compute the nusselt number for a three - dimensional flow with @xmath67 and observe that @xmath68  @xcite , which is in good agreement with earlier experimental and numerical results .",
    ".verification of tarang against thual s  @xcite 2d rbc simulations .",
    "we compare nusselt numbers ( @xmath69 ) computed in our simulations on a @xmath70 grid against thual s simulations on @xmath71 ( thu1 ) , @xmath72 ( thu2 ) , and @xmath70 ( thu3 ) grids .",
    "all @xmath69 values tabulated here are for the prandtl number of @xmath73 . [ cols=\"^,^,^,^,^ \" , ]     [ rbc_table ]    using the rbc module of tarang , we also studied the energy spectra and fluxes of the velocity and temperature fields  @xcite , the nusselt number scaling  @xcite , and chaos and bifurcations near the onset of convection  @xcite .    in the next section",
    "we will discuss the results of the mhd module of tarang .",
    ", 0.05 , 0.1 from top to bottom .",
    "the equations for the incompressible mhd turbulence  @xcite are @xmath74 where @xmath75 , @xmath76 and @xmath12 are the velocity- , magnetic- , and pressure ( thermal+magnetic ) fields respectively , @xmath25 is the kinematic viscosity , and @xmath46 is the magnetic diffusivity .",
    "the @xmath26 and @xmath77 are external forcing terms for the velocity and magnetic fields respectively .",
    "typically , @xmath78 , but tarang implements @xmath77 for generality .",
    "the magnetic field @xmath79 can be separated into its mean @xmath80 and fluctuations @xmath81 : @xmath82 .",
    "the number of nonlinear terms in the above equations is four whose computation requires 27 ffts .",
    "however , the number of fft computations in terms of the elsasser variables @xmath83 is only 15 , thus saving significant computing time .",
    "we use @xmath84 to compute the nonlinear terms .",
    "thus , the time requirement for a mhd simulation would be around 15/9 times that for the fluid simulation . in fig .  [",
    "fig : bg_tarang_mhd_gnu ] we plot the time taken per step for different set of processors on _ shaheen_. the results are consistent with the above estimates .",
    "regarding the space complexity , an mhd simulation requires 27 arrays for storing @xmath85 , @xmath86 and three temporary fields .",
    "hence the memory requirement for a mhd simulation is 27/15 times that of a fluid simulation .",
    "we perform code validation of tarang s mhd module using the results of breyiannis and valougeorgis s  @xcite lattice kinetic simulations of three - dimensional decaying mhd . following breyiannis and valougeorgis ,",
    "we solve the mhd equations inside a cube with periodic boundary conditions on all directions , and with a taylor - green vortex ( given below ) as an initial condition , @xmath87,\\\\%\\quad \\\\",
    "{ \\bf b } & = & \\left[\\sin(x)\\sin(y)\\cos(z ) , \\cos(x)\\cos(y)\\cos(z ) , 0\\right].\\end{aligned}\\ ] ] this taylor - green vortex is then allowed to evolve freely .",
    "the simulation box is discretized using @xmath88 grid points .",
    "the results of this test case for different parameter values ( @xmath89 , 0.05 , 0.1 ) are presented in fig .",
    "[ fig : keme ] .",
    "the top and bottom panels exhibit the time evolution of the total kinetic- and magnetic energies respectively .",
    "tarang s data points , illustrated using blue dots , are in excellent agreement with breyiannis and valougeorgis results  @xcite , which is represented using solid lines .",
    "we thus verify the mhd module of tarang .",
    "we have used tarang to perform extensive simulations of dynamo transition under the taylor - green forcing  @xcite . using tarang ,",
    "we have also computed the magnetic and kinetic energy spectra , various energy fluxes  @xcite , and shell - to - shell energy transfers for mhd turbulence ; these results would be presented in a subsequent paper .",
    "in addition to the fluid , mhd , and rayleigh - bnard convection solvers , tarang has modules for simulating rotating turbulence , passive and active scalars , liquid metal flows , rotating convection  @xcite , and kolmogorov flow .",
    "in this paper we describe salient features and code validation of tarang .",
    "tarang passes several validation tests performed for fluid , rayleigh - bnard convection , and magnetohydrodynamic solvers .",
    "we also report scaling analysis of tarang and show that it exhibits excellent strong- and weak scaling up to several thousand processors .",
    "tarang has been used for studying rayleigh - bnard convection , dynamo , and magnetohydrodynamic turbulence .",
    "it has been ported to various computing platforms including the _ hpc system _ of iit kanpur , _ shaheen _ of kaust , _ param yuva _ of the centre for advanced computing ( pune ) , and _ eka _ of the computational research laboratory ( pune ) .",
    "tarang simulations were performed on _ shaheen _ supercomputer of kaust ( through the project k97 ) and on the _ hpc system _ of iit kanpur , for which we thank the personnels of respective supercomputing centers , especially abhishek and brajesh pande of iit kanpur .",
    "we are grateful to sandeep joshi and late dr .",
    "v. sunderarajan ( cdac ) who encouraged us to run tarang on very large grids .",
    "we also thank daniele carati and his group at ulb brussels for sharing with us the details of a pseudospectral code , and cdac engineers for help at various stages .",
    "mkv acknowledges the support of swaranajayanti fellowship and a research grant 2009/36/81-brns from bhabha atomic research center ."
  ],
  "abstract_text": [
    "<S> tarang is a general - purpose pseudospectral parallel code for simulating flows involving fluids , magnetohydrodynamics , and rayleigh - bnard convection in turbulence and instability regimes . in this paper </S>",
    "<S> we present code validation and benchmarking results of tarang . </S>",
    "<S> we performed our simulations on @xmath0 , @xmath1 , and @xmath2 grids using the _ hpc system _ of iit kanpur and _ shaheen _ of kaust . </S>",
    "<S> we observe good  weak \" and  strong \" scaling for tarang on these systems .    </S>",
    "<S> pseudospectral method ; direct numerical simulations ; high - performance computing </S>"
  ]
}