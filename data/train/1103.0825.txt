{
  "article_text": [
    "a fundamental problem in data management is how to share real data sets without compromising the privacy of the individuals who contribute to the data .",
    "great strides have been made on this problem in recent years , leading to a growing science of `` anonymization '' .",
    "such anonymization enables data owners to share data privately with the public , external collaborators , or other groups within an organization ( the risk of private information leaking guides the amount of masking applied to the data ) .",
    "anonymization also allows businesses to retain detailed information about their customers , while complying with data protection and privacy legislation .",
    "different techniques are relevant to different threat models : ` syntactic ' privacy definitions , such as @xmath0-anonymity and @xmath1-diversity , preserve more detail at the microdata level , but can be susceptible to attack by determined adversaries  @xcite . differential privacy , introduced in a series of papers in the theoretical community ,",
    "is a more semantic definition .",
    "it has gained considerable traction due to its precise privacy guarantees .",
    "see recent tutorials on data anonymization for further background and definitions  @xcite .",
    "the original model for differential privacy assumed an interactive scenario : queries are posed to a `` gatekeeper , '' who computes the true answer , adds random noise and returns the result .",
    "the random noise is drawn from a particular distribution , whose parameter is determined by the impact any individual can have on the answers to a fixed number of queries .",
    "once that number is reached , the gatekeeper stops answering . to avoid this restriction",
    ", we can release many statistics about the data _ en masse_. for example , release data in the form of contingency tables , with appropriate noise added to each entry @xcite .",
    "this is equivalent to the result of various groupby / count ( * ) operations on the original data ( with noise ) .",
    "[ eg1 ] consider a collection of geographic data , which records the commuting patterns of a large population .",
    "this could be collected by a census bureau @xcite , or derived by a cellphone provider based on calling patterns @xcite .",
    "a contingency table , indexed by home and work locations , is a matrix : each cell contains the number of people who commute from the given source to the given destination .",
    "we assume that the locations are sufficiently large areas so that there are many cells with moderately large numbers in them . to release this data in a differentially private way",
    ", we add random noise independently to each cell before publishing the matrix .",
    "the noise is relatively small ( typically single digit perturbations ) , so the large patterns in the data are preserved while ensuring that no individual s contribution is revealed .    * scalability problem . * in theory , the above scheme is a useful approach to privately release data .",
    "but a roadblock emerges , showing this method to be impractical .",
    "as we observe below , the contingency table can be orders of magnitude larger than the original data . to satisfy privacy , we must add noise to _ every cell _ , including those with zero count . because the noise has a low probability of being zero",
    ", the resulting table is both large and very dense : we can not represent it compactly ( such as by a list of non - zero cells ) .",
    "the method is time and space consuming , to the extent where it becomes unfeasible for many datasets .",
    ".examples of datasets and their densities . [ cols=\"<,>,<\",options=\"header \" , ]     consider example  [ eg1 ] for a dataset of 10 million commuters , spread across 1 million locations . the contingency table over home and work locations has @xmath2 entries .",
    "just storing this table requires terabytes , and generate @xmath2 random draws from the noise distribution is extremely slow .",
    "we have gone from an original manageable data set with @xmath3 rows to one that is @xmath4 times larger , making subsequent analysis slow and unwieldy .",
    "table  [ tblsparse ] shows several examples of widely - used datasets ( further details of these datasets are explained in appendix  [ app : datasets ] ) .",
    "for each dataset , we show the _ density _ , @xmath5 , which is the ratio between the original data size , and the size of the noisy contingency table . hence , naively applying differential privacy generates output which is @xmath6 times larger than the data size .",
    "as table  [ tblsparse ] indicates , many natural datasets have low density in the single - digit percentage range , or less .",
    "this is the norm , rather than the exception : any data with several attributes @xmath7 of moderately high cardinality @xmath8 leads to huge contingency matrices of size @xmath9 for a table with @xmath10 rows , the density is at most @xmath11 . in the above examples , @xmath6 ranges from tens to thousands .",
    "this @xmath6 factor applies to the time required to prepare the private data , and to store and process it .",
    "this is clearly not practical for today s large data sizes .",
    "* our contributions .",
    "* in this paper we propose new techniques for making differential privacy scalable over low - density ( sparse ) datasets . our algorithms compute privacy - preserving outputs whose size is controlled by the data owner . usually , this is chosen close to the size of the original data , or smaller . moreover",
    ", their running time is proportional to the size of the output , and independent of the size of the contingency table .",
    "this is a crucial step towards making differential privacy practical in database applications .    as discussed above",
    ", the anonymization process changes originally sparse datasets into dense ones . to meet the strict differential privacy definition",
    ", there must be some probability of creating noise in _ any _ cell , else a powerful analyst could eliminate some possibilities for a targeted individual .",
    "however , once the noisy contingency table is generated , we can safely return only a random sample of it , instead of the entire data .",
    "we refer to this sample as the differentially private _ summary _ of the data .",
    "our approach relies on the well - known property that any post - processing of differentially private data remains differentially private ; see  @xcite .",
    "while post - process sampling alleviates the storage requirements of the output , it does not help the overall computation cost .",
    "hence , this approach is effective for the data recipient , but still very costly for the data owner .",
    "instead , we propose a new technique : we show how to directly generate the summary from the original data , without computing the huge contingency table first .",
    "this requires some care : the ( random ) two - step process of generating noisy intermediate data and then applying some sampling method over it implies a probability distribution over possible summaries . our goal is to provide one - step methods that create summaries with _ the same _ distribution over possible outputs , while being much more efficient .",
    "we design several one - step summary generation methods , which are provably correct and run in time proportional to the output size .",
    "our algorithms depend on the specific sampling method employed .",
    "there is a wealth of work on data reduction techniques .",
    "we focus on some of the best known methods , such as filtering , priority sampling and sketching  @xcite .",
    "they enable vast quantities of data to be reduced to much more manageable sizes , while still allowing a variety of queries to be answered accurately .",
    "to summarize :    = 1em    we formalize the problem of releasing sparse data , and describe appropriate target summaries : filters , samples and sketches .",
    "we show techniques to efficiently generate these summaries , by drawing directly from an implicit distribution over summaries .",
    "we perform an experimental study over a variety of data sets .",
    "we compare the utility of our summaries to that of the large intermediate tables , by running a large number of queries over both , and comparing their relative accuracies .",
    "we conclude that our significantly smaller summaries have similar utility to the large contingency tables , and in some cases , they even improve on it .    for background ,",
    "a primer on ideas and prior work in differential privacy and data summarization is presented in appendix  [ app : background ] .",
    "our goal is to efficiently release a summary of a dataset with privacy guarantees .",
    "formally , let @xmath12 be the original data , represented as a ( potentially huge ) contingency table .",
    "we assume that @xmath12 has low density , i.e. , the number of non - zero entries in @xmath12 , denoted by @xmath10 , is much smaller than the total number of entries in @xmath12 , denoted by @xmath13 : @xmath14 .",
    "the entries in @xmath12 are non - negative integers representing counts ( although our methods can apply to data with arbitrary values ) .",
    "we generate a differentially private table @xmath15 from @xmath12 via the geometric noise mechanism @xcite .",
    "this adds independent random noise to each entry in @xmath12 , with the distribution    @xmath16 = \\frac{1-\\alpha}{1+\\alpha } \\alpha^{|x| } , $ ]    where @xmath17 , @xmath18 , @xmath19 is the differential privacy parameter and @xmath20 is the sensitivity parameter , for disjoint counts @xmath21 .    because the noise can be negative , @xmath15 can contain negative entries . in many applications ,",
    "it is not meaningful to have negative counts .",
    "however , we can adjust for this , e.g. , by rounding negative values up to the smallest meaningful value , 0 .",
    "let @xmath22 denote the anonymized table obtained from @xmath15 via this procedure .",
    "for small range queries or for point queries , using @xmath22 tends to be more accurate than @xmath15 .",
    "however , since the noise is symmetric , retaining negative values is useful for large queries which touch many entries , since the noise cancels .",
    "more precisely , the sum of noise values is zero in expectation ( but has non - trivial variance ) .",
    "our aim is to publish @xmath23 , a compact summary of @xmath15 ( or of @xmath22 ) .",
    "the naive approach is as follows : compute the entire @xmath15 , by generating and adding noise for every entry in the ( large , sparse ) @xmath12 , then summarize @xmath15 to obtain @xmath24 this is costly to the point of impracticality , even though @xmath23 is expected to be relatively small , i.e. , @xmath25 instead , we compute @xmath23 directly from @xmath12 , without materializing the intermediate table @xmath15 .",
    "we illustrate this process schematically in figure [ fig : sampling - model ] , which contrasts the _ laborious _ approach to our proposed _",
    "shortcut _ approach .",
    "the following observation is crucial for designing our algorithms .     directly from @xmath12 . ]",
    "[ binomial ] for all summarization techniques we consider , each zero entry in @xmath12 has the same probability @xmath26 to be chosen in @xmath23 .",
    "this is since the summarization methods depend only on the values of the items in @xmath15 , not their positions ; and since the noise added to each zero entry is drawn from the same distribution .",
    "hence , out of the @xmath27 zero entries of @xmath12 , the number chosen for the summary @xmath23 follows the binomial distribution @xmath28    in the subsequent sections , we develop shortcut approaches for several summarization methods .",
    "each method requires careful analysis , to show that the resulting @xmath23 is distributed as if it were generated via summarization of @xmath15 , and that the shortcut approach can be implemented efficiently .",
    "we also state how to use the resulting summaries to accurately estimate common queries ( range and subset queries ) , and how to choose parameters such as sample size .",
    "to simplify notation , we treat @xmath12 as a ( long ) one - dimensional array .",
    "however , the techniques directly apply to other data layouts , since they consider each entry in the input independently .",
    "we interchangeably refer to entries in @xmath12 as items , consistent with sampling terminology .",
    "thus , we say that @xmath12 has @xmath13 items , of which @xmath10 have non - zero count .",
    "for clarity of exposition , we assume that the values @xmath10 and @xmath29 are not sensitive . otherwise , it is straightforward to add the necessary noise to mask their true values",
    ".      the simplest form of data reduction we consider is to apply a high - pass filter to @xmath15 : this has the effect of retaining the large values , but replacing the small values with 0 .",
    "if we choose an appropriate cut - off to distinguish `` small '' from `` large '' , the result is to zero out a large portion of the data , meaning that the remainder can be described compactly .",
    "let @xmath30 be the cut - off value ( @xmath31 ) .",
    "if an item @xmath32 , then @xmath33 , else we set @xmath34 .",
    "our approach for generating @xmath23 is to consider the non - zero entries of @xmath12 separately from the zero - entries .",
    "first , in @xmath35 time , we perform filtering for the non - zero entries in @xmath12 : generate and add noise , then determine whether to retain them . for the @xmath27 zero entries of @xmath12 ,",
    "this procedure is too slow ( recall that @xmath36 ) .",
    "therefore , we design a statistical process which achieves the same overall distribution over outputs , without explicitly adding noise to each zero entry .",
    "we now describe our statistical process to filter originally zero entries .",
    "let @xmath37 denote the probability that a zero entry in @xmath12 passes the filter @xmath38 we draw the number of entries @xmath0 that pass the filter from the distribution @xmath39 , per remark  [ binomial ] .",
    "we then choose uniformly at random @xmath0 zero entries in @xmath12 .",
    "for each such entry @xmath40 , we generate the value @xmath41 by adding noise , _ conditional _ on the fact that the noise exceeds @xmath30 .",
    "this may seem like a lot of effort to go to in order to generate noise in the output data , but it is a necessary step : we must simulate exactly the output of filtering over the full table @xmath15 , in order to preserve the differential privacy guarantee .",
    "algorithm filter summarizes the algorithm for this shortcut process .",
    "we prove its correctness in appendix  [ app : proofs ] .",
    "* algorithm filter@xmath42 : * generates @xmath23 via high - pass filter .    .=2em",
    "for every non - zero entry @xmath43 , add geometric noise with parameter @xmath44 to get @xmath45 .",
    "if @xmath32 , add @xmath45 to @xmath23 .    for zero entries , sample a value @xmath0 from the binomial distribution @xmath39 , where @xmath46 .",
    "uniformly at random select @xmath0 locations @xmath40 from @xmath12 such that @xmath47 . for each of these @xmath0 locations , include @xmath40 with value @xmath48 in @xmath23 where @xmath48 is sampled according to the distribution @xmath49 = ( 1 - \\alpha^{x - \\theta+1})\\ ] ]    [ thm : highpass ] algorithm filter generates a summary with the same distribution as the laborious approach under high - pass filtering with parameter @xmath50 .",
    "more sophisticated data reduction techniques are based on sampling . in this section and",
    "the next , we discuss how to generate @xmath23 as a random sample of @xmath15 , without explicitly generating @xmath15 .",
    "there are many sampling procedures .",
    "the simplest is to uniformly sample items in @xmath12 , then create @xmath23 by adding noise to them .",
    "this is easy to implement , but unlikely to have much utility : since @xmath12 is sparse , we would sample almost exclusively entries with @xmath47 , making the sample virtually useless .",
    "instead , we extend the intuition from the high - pass filter : items in @xmath15 with high ( noisy ) values are more likely to correspond to non - zero entries in @xmath12 , i.e. , to original data",
    ". we should include them in our sample with higher probability .",
    "the filtering approach achieved this deterministically : items below the threshold had zero chance of being included . when sampling , we now allow every item a chance of being in the summary , but set the probability proportional to its ( noisy ) count",
    ".     * threshold sampling .",
    "* we first consider _ threshold sampling _",
    "@xcite , and define the weight @xmath51 of item @xmath40 to be the absolute value of its noisy count @xmath52 the method samples each entry with probability proportional to its weight .",
    "hence , an item with weight @xmath45 has the same probability of inclusion as an item with weight @xmath53 .",
    "the threshold sampling procedure with parameter @xmath54 is as follows : we include item @xmath40 in the sample with probability @xmath55 .",
    "this means that truly heavy items that have @xmath56 are included in the sample with probability 1",
    ".     * algorithm threshold .",
    "* generate @xmath23 via threshold sampling .    .=2em    for every non - zero entry in @xmath43 ,",
    "add geometric noise to get @xmath45 and add it to @xmath23 with probability @xmath57 .",
    "for the zero entries , sample a number @xmath0 from the binomial distribution @xmath58 , where @xmath59    uniformly at random select @xmath0 entries @xmath40 from @xmath12 such that @xmath47 .",
    "for each of these @xmath0 entries , draw the summary value @xmath41 from the distribution @xmath60 $ ] given by : @xmath61 where @xmath62 is a constant depending on @xmath63 .",
    "the following result is proven in appendix  [ app : proofs ] .",
    "[ thm : threshold ] algorithm threshold generates generates a summary with the same distribution as the laborious approach under threshold sampling with parameter @xmath64",
    ".     * priority sampling . *",
    "[ sec : priority ] _ priority sampling _ has been advocated as a method to generate a sample of fixed size , with strong accuracy properties @xcite .",
    "the sampling scheme is defined as follows .",
    "each entry is assigned a priority @xmath65 , where @xmath66 is a random value chosen uniformly from the range ( 0,1 ] .",
    "the scheme draws a sample of size @xmath67 by picking the items with the @xmath67 largest priorities , and also retaining the @xmath68th largest priority for estimation purposes .    to efficiently build a priority sample of @xmath15 ,",
    "suppose that we knew @xmath69 , the @xmath68th largest priority .",
    "then @xmath45 is sampled if @xmath70    since @xmath66 is uniform over ( 0,1 ] , the probability of this event is @xmath71 . in other words ,",
    "this procedure can be seen as equivalent to threshold sampling with threshold @xmath69 .",
    "the crucial difference is that @xmath69 is data - dependent , so we do not know it in advance .",
    "however , we can _ guess _ a good value : one which will yield a sample of size @xmath72 where the constant hidden by the @xmath73 notation is small ( but @xmath74 ) .",
    "we first use our guess for @xmath69 to draw a corresponding threshold sample .",
    "we then augment each item in the resulting sample with an assigned priority @xmath66 .",
    "that is , conditional on item @xmath40 being in the sample , we draw an @xmath66 consistent with this outcome . for @xmath40 , we must have @xmath75 ( else @xmath40 would not have passed the threshold ) , but beyond this there are no additional constraints , so @xmath66 is picked uniformly in the range @xmath76 ( which is distributed the same as the distribution of the @xmath77 largest priorities over all of @xmath15 ) , we can reduce the sample size to @xmath67 by picking the @xmath67 largest priorities ( and retaining the @xmath68th priority ) .",
    "the running time is @xmath78 ( in expectation ) .",
    "the result has the desired distribution ; see the proof in appendix  [ app : proofs ] .",
    "[ thm : priority ] we can generate a priority sample of @xmath15 via the shortcut approach , in expected time @xmath79 .",
    "we have argued that both sampling and filtering are useful ways to summarize data . in particular , filtering removes small counts which are very likely noise . setting the filter threshold low can remove a lot of noise , but will still pass too many items , while setting it too high will remove too many true data items . a natural compromise is to combine sampling with filtering : consider generating @xmath23 from @xmath15 by first filtering out low frequencies , and then sampling the result .",
    "we expect this to give us the best properties of both summaries : noise removal and bounded output size . in appendix",
    "[ app : samplefilter ] , we analyze the combination of the high - pass filter with threshold sampling , and design a shortcut algorithm which correctly computes this combination .",
    "range queries that touch many entries tend to have much higher error than small queries .",
    "although in expectation the sum of noise values is 0 ( so query answers are expected to be correct ) , its variance is linear in the number of entries touched by the query  @xcite . in practice ,",
    "the observed errors tend to be proportional to the standard deviation ( i.e. , the square root of the number of cells touched ) . a natural way to make range queries more accurate is to publish anonymized data at multiple levels of granularity , so that any range can be decomposed into a small number of probes to the published data .",
    "for one dimensional data , the canonical approach is _ dyadic ranges _ : build a ( binary ) tree over the domain of the data .",
    "for each leaf , count the number of data values in the interval corresponding to that leaf . for each internal node @xmath80 , compute the sum of counts over all the leaves in @xmath80 s subtree .",
    "we can then publish these counts , at all levels , in a privacy - preserving manner .",
    "let @xmath81 denote the height of this tree .",
    "in general , @xmath82 .",
    "each individual s data now affects @xmath81 counts , i.e. , all the node counts on the path from the individual s leaf to the root .",
    "hence , the sensitivity increases by a factor of @xmath81 , and the noise in each count is higher .",
    "it is well known that any range query can be answered as a sum of at most @xmath83 counts ( at most two node counts per level ) . for large enough ranges ,",
    "the higher noise added to each count is countered by the smaller number of counts touched by it .",
    "related ideas were developed in  @xcite : the authors compute the wavelet transform of the data and add noise to its coefficients .",
    "this approach also has a sensitivity of @xmath84 , and answers range queries from @xmath84 counts , but with different constants .",
    "these schemes and their variants are described and analyzed in greater detail in recent work  @xcite . in theory and in practice",
    "the results for dyadic ranges and wavelets are quite similar : the variance of estimators for range queries under both techniques is proportional to @xmath85 .    however , a limitation of these approaches is that they make no attempt to bound the amount of data . if anything , they may blow up the data size . thus , when @xmath86 , we face the same scalability problem .",
    "our solution is to combine these variants of private data publishing with summarization : the counts computed by dyadic ranges or wavelets on the original data become the input data @xmath12 .",
    "we then apply the shortcut approaches from the previous sections .",
    "[ thm : dyadic ] we can generate summaries of size @xmath67 with the same distribution as the laborious approach applied to dyadic ranges or haar wavelets under high - pass filtering and threshold / priority sampling in ( expected ) time @xmath87 .    this approach extends naturally to multiple dimensions of data .",
    "however , care is needed : the sensitivity grows exponentially with the number of dimensions , as @xmath88 ( when we have @xmath89 dimensions divided into @xmath13 ranges each ) .",
    "as each range query is answered by summing @xmath90 counts , there are fewer queries benefiting from dyadic ranges or wavelets , as their sizes must increase rapidly with the dimension .     * consistency checks for dyadic ranges .",
    "* we can further refine results for range queries by using the inherent correlation between counts along the same leaf to root path . for dyadic ranges over the original data ,",
    "a node count is never smaller than the counts in the node s descendants .",
    "therefore , it is natural to try to enforce a similar condition in the summary data , by modifying some entries after publication . for filter summaries over dyadic ranges ,",
    "we impose the following post - processing step .",
    "if a node @xmath80 is selected in the summary @xmath91 but at least one of @xmath80 s ancestors @xmath48 is not selected , then we drop @xmath80 from @xmath23 .",
    "the intuition for this is straightforward : since @xmath92 , @xmath48 had a higher chance to pass the filter after noise addition .",
    "the fact that it did not is strong evidence that its count ( and thus @xmath80 s count ) is small , and likely zero .",
    "the evidence of @xmath48 s absence from the summary trumps the evidence of @xmath80 s presence , so we drop @xmath80 .",
    "it is not clear that such a condition is meaningful on the output of sampling : the summary may omit some nodes @xmath48 as part of the random sampling , so the absence of some node does not give a strong reason for dropping its descendants .",
    "consequently , we do not advocate imposing this condition on the output of sampling .",
    "however it is meaningful to apply for the combination of filter and priority sampling described in section  [ filter - sample ] , if we do so after filtering but before sampling .",
    "this means that we have to work on the output of the filtering , which may still be large , and so we do not obtain a result that directly generates the final summary . however , this may be an acceptable quality / efficiency tradeoff for some data .",
    "we first evaluate on synthetic data , where we can observe the impact of varying data characteristics on the quality of summaries produced .",
    "we control parameters including the mean @xmath93 and standard deviation @xmath94 of the ( non - zero ) data values , the data density ( @xmath95 ) , and the distribution of locations of non - zero data values in the input space , e.g. , either uniformly scattered or skewed and concentrated in particular regions .",
    "our experiments use an input size of @xmath96 as a default , so it is feasible to compare to the solution of adding noise at each location .",
    "the non - zero data entries are drawn from a gaussian distribution with mean @xmath97 , standard deviation @xmath98 , and they are then rounded to the nearest positive integer .",
    "the default data density is @xmath99 , a moderately dense data set by our standards , and the data values are uniformly distributed over the data domain .",
    "the default privacy requirement is set to @xmath100 .",
    "we use the standard geometric mechanism as the baseline to which we compare our techniques , noting that this is not a practical solution for the sparse data settings we consider .",
    "we measure utility by computing the absolute errors for various queries , i.e. the @xmath101 difference between the true query answer ( on @xmath12 ) and that estimated from the published data ( @xmath23 ) .",
    "this dimensionless quantity varies depending on the size of the range and the nature of the data , but allows us to compare the relative performance .",
    "we also report median relative errors over the query sets .",
    "the queries are to compute the sum of a set of values specified as either a contiguous range of values in the data space ( range query ) , or as a subset of locations in the domain ( subset sum query ) .",
    "figure [ fig : parameter ] shows absolute error as we vary data parameters ( @xmath30 and @xmath54 ) . for these summaries , range queries and subset queries are effectively identical , so we do not distinguish them .",
    "* filtering .",
    "* we compare the performance of the two high - pass filters : one - sided ( pass only positive values above @xmath30 ) and two - sided ( pass all items whose absolute value is above @xmath30 ) .",
    "we fix the range size @xmath102 and vary the filtering threshold @xmath30 to get different summary sizes .",
    "figure [ fig : exp1_filtering ] shows the accuracy on a log scale , indicating that the accuracy varies by several orders of magnitude .",
    "two - sided filtering is generally more accurate than one - sided filtering . when most of the ( originally ) non - zero data points are in the sample , query answers using two - sided filtering estimates are unbiased .",
    "in this case the relative error of this technique is very low : consistently around 1% .",
    "one - sided filtering is more likely to over - estimate because some data points which were originally zero are now positive entries in the sample . when the summary size is small , we expect that the summary is dominated by the non - zeros and hence both techniques have similar accuracy .",
    "the minimal error occurs when the sample contains most of the true data points and few originally zero entries .",
    "but this may require a @xmath30 value that results in a larger than desired summary . as we saw the same behavior across a variety of range sizes and data types , we henceforth only use two - sided filtering , since it is uniformly more robust .",
    "* threshold and priority sampling . *",
    "figure  [ fig : exp1_priority ] shows the accuracy of threshold sampling and priority sampling when we fix the range size @xmath102 and set the parameter @xmath54 for threshold sampling to get a desired sample size @xmath67 ( see section  [ app : threshold ] ) .",
    "this corresponds to relative errors in the range 1% to 7% . for priority sampling ,",
    "the sample size @xmath67 is the input parameter . the behavior of threshold sampling is very similar to that of priority sampling , as expected from our analysis .",
    "the only difference is that priority sampling returns a sample with fixed size @xmath67 as required , but needs more computation to find the @xmath68th priority and adjust the weights accordingly , as is observed in figure  [ fig : exp2_throughput ] .",
    "since threshold and priority sampling behaved similarly across all other range sizes tested , we show only the latter from now on .",
    "* filter - priority sampling*. figure  [ fig : exp1_thresholdpriority ] shows the accuracy for the combination of two - sided filtering with priority sampling ( which we dub `` filter - priority sampling '' ) for @xmath30 between 5 and 70 .",
    "we fix the priority sample size to @xmath103 , which is equal to the cardinality of the original input data .",
    "the relative errors measured are within [ 0.3% , 6% ] . for this setting , we find that @xmath104 is about optimal .",
    "this is in the region of half of the mean @xmath93 of the non - zero data values . in general",
    ", the threshold @xmath30 should maximally filter most of the ( upgraded ) zeros but still retain a large number of the original non - zero data , hence it should be neither too large nor small . for larger range queries ,",
    "we want to set @xmath30 smaller to make sure most of the non - zeros appear in the sample , since on average a range query contains more non - zeros and this technique relies on the priority sampling to give unbiased estimations to query answers .",
    "the choice of @xmath30 is also affected by the data density .",
    "we next create summaries of size @xmath103 , and compare performance to the baseline geometric mechanism in figure [ fig : subset ] .     * range query accuracy .",
    "* figures [ fig : exp2_compare1_small ] and [ fig : exp2_compare1_large ] show the accuracy of high - pass filtering , priority sampling , filter - priority sampling , and the naive geometric mechanism , for both medium and large range queries using the default dataset .",
    "apart from priority sampling , all techniques have relative error around 5% for subsets of size 100 , and this decreases as the subset size increases .",
    "note that the geometric mechanism publishes the entire noisy data domain , which is typically infeasible , so we see how well we can do against this benchmark when releasing a much smaller summary . for small and medium range queries , filtering , with @xmath105 ,",
    "performs best while threshold and priority sampling is preferable when the range size is large enough .",
    "this also exhibits the same behavior as in figure [ fig : exp1_thresholdpriority ] .",
    "filter - priority sampling ( here we use @xmath104 ) combines the advantage of the two techniques , having consistent accuracy comparable to that of the geometric mechanism .",
    "this is a somewhat surprising outcome : at best , we hoped to equal the accuracy of the ( unfeasible ) geometric mechanism with a compact summary ; but here and elsewhere , we see examples with _",
    "better _ accuracy better than this benchmark .",
    "the reason is that summarization is helping us : the noise introduced by the privacy mechanism is less likely to reach the output summary , so we get somewhat more accurate answers .",
    "this does not alter privacy guarantees  as the user is free to do any post - processing of the output of the geometric mechanism , such as apply a filter to it ",
    "rather , this indicates that the summarization techniques maintain the same privacy and can help improve utility of the released data .",
    "figure [ fig : exp2_compare2 ] shows the corresponding plot with higher standard deviation @xmath106 .",
    "filtering now performs much worse as it is harder to separate the true data from the noise introduced in @xmath15 .",
    "threshold and priority sampling start to outperform filtering over shorter ranges , around a thousand items in length .",
    "filter - priority sampling , with a smaller threshold @xmath107 , is better than individual filtering or sampling techniques . in this experiment ,",
    "the summary techniques are less accurate than the geometric mechanism .",
    "the gap is narrowed for larger summaries : picking a sample size of @xmath108 ( twice as large ) is sufficient to make the filter - priority summary equal to the geometric mechanism in accuracy",
    ".     * impact of data density .",
    "* next , we reduce the data density to @xmath109 while using the same input size @xmath13 and setting the sample size to be the number of non - zeros , i.e. , @xmath110 . figure  [ fig : exp2_compare3 ] shows that filtering outperforms sampling across a range of sample sizes , because with more zero entries , the total probability of a zero appearing in the sample increases , and the proportion of true data points sampled decreases .",
    "but filter - thresholding ( with @xmath105 ) has yet higher accuracy and outperforms the geometric mechanism across the whole spectrum of subset sizes due to efficiently canceling much noise from zero entries .",
    "this suggests that , across a range of data types and query sizes , this hybrid method has the best of both worlds : it uses filtering to prune away pure noise , and sampling to favor more significant data entries without bias , producing a summary of fixed size .     * throughput . * recall that our motivation is that data release via methods like the geometric mechanism is not scalable when the data is sparse : it produces a very large output ( of size @xmath13 ) , and may be very slow to create the summary .",
    "we now justify our effort in creating compact summaries by observing that they can be created very efficiently .",
    "we measure the throughput , or the number of ( non - zero ) entries processed per second , of all techniques under different data density values , @xmath111 .",
    "this spans the range of densities observed in table  [ tblsparse ] .",
    "the size of input data domain , @xmath13 , is held fixed , and sample size @xmath67 is set equal to the cardinality of the original data , @xmath10 .",
    "figure [ fig : exp2_throughput ] shows that filtering and threshold sampling are the fastest since they require less computation .",
    "priority sampling has a higher cost due to the extra effort to find the @xmath68th priority and adjusting the weights . under the very densest setting , @xmath99",
    ", its running time is about the same as the basic geometric mechanism .",
    "when the data is more sparse , all of these techniques are faster than the basic geometric mechanism , by orders of magnitude .",
    "this demonstrates that the laborious approach to generating the samples , which requires materializing the ( assumed huge ) @xmath15 , is infeasible in most cases .      as noted in section [ sec : dyadic ] , answering range queries via dyadic ranges has proven successful when it is feasible to publish the full output of the laborious approach .",
    "we next combine the dyadic range technique with the filtering and sampling methods to produce a bounded output size , and evaluate the performance on range queries . specifically , we consider filtering , priority sampling , and filter - priority sampling on dyadic ranges , as well as the geometric mechanism both with and without dyadic ranges .",
    "the size of the sampled dyadic ranges is set equal to number of non - zeros in the input , @xmath103 , and we vary the range sizes .",
    "figure [ fig : exp3_dyadic1 ] shows that priority sampling is worst of the filtering and sampling techniques .",
    "this is since the sensitivity of the dyadic ranges is large ( i.e. , @xmath84 ) , hence the noise added is large , giving zeros in dyadic ranges a high probability to be in the sample .",
    "filtering is preferable since it can filter most of original zeros ; even if we drop some entries with very small magnitude , this does not dramatically affect the accuracy of the overall answer on the whole range .",
    "when range size @xmath112 ( corresponding to 1% of the data space ) , using dyadic ranges has better accuracy than without them .",
    "after this point , the accuracy depends only very weakly on the range size , since each query probes about the same number of entries ( i.e. , twice the height of the dyadic range tree ) .",
    "we also measured the running time of these techniques while varying the data densities from @xmath113 to @xmath114 .",
    "we observe the same trend as in figure [ fig : exp2_throughput]sampling and filtering are always faster than the geometric mechanism . for smaller density values ,",
    "the gain in throughput for our techniques is less pronounced , i.e. , about 2 to 4 times , since all methods incur the same cost for building the dyadic ranges .",
    "figure [ fig : exp3_dyadic2 ] shows the same experiment when the data is skewed , i.e. the non - zeros in the data have a higher probability to be placed close together in the input array .",
    "this effectively changes the sparsity of the resulting dyadic ranges , which have more zeros in certain tree nodes than others . now filtering the dyadic ranges improves over its geometric mechanism equivalent since it can eliminate more noise from these `` light '' nodes in the dyadic range tree .",
    "overall , we conclude that the approach of dyadic ranges ( and similarly , wavelet transforms ) is compatible with the shortcut summarization approach , and is effective when we anticipate that queries will touch any more than a small fraction of the data space .",
    "* consistency checks . *",
    "we next experiment with applying consistency checks to the dyadic ranges , as discussed in section  [ sec : dyadic ] .",
    "this is done for plain filtering , and on the output of filtering prior to priority sampling .",
    "note that applying these checks on the simple geometric mechanism is not helpful , since there are very few entries set to zero which can be used to filter descendant nodes .",
    "figure  [ fig : exp3_dyadic_consistency ] shows that applying consistency checks reduces the errors of our two techniques by 30% to 60% .",
    "this confirms that consistency checks help improve accuracy when the data is highly sparse and non - uniform , since they further eliminate noise from originally zero entries . for more uniform datasets ,",
    "the same trend is present , but is less pronounced : the improvement is closer to 10% .",
    "we evaluate performance on the `` onthemap '' data described in appendix  [ app : datasets ] .",
    "the data is actually synthesized from real individuals but is designed to represent realistic patterns .",
    "for the purposes of our study , we treat it as the ground truth @xmath12 and compare our techniques for protecting the privacy of these ( synthetic ) individuals .",
    "we first apply filtering and sampling over this data set , and show results for a summary of size @xmath115 .",
    "figure  [ fig : exp4_map1 ] shows that applying filtering to this data is inaccurate , partly due to the high variance of the data : deleting too many small values actually wipes out a lot of the original signal . priority sampling and filter - priority sampling with a small threshold",
    "give better accuracy : not much worse than the geometric mechanism ( whose output size is more than 5 times larger ) for most queries , and actually better for large queries which cover 20% or more of the data space . on these large subset queries ,",
    "priority sampling outperforms the other techniques , but filter - priority is not far behind . in terms of relative errors ,",
    "all queries for subsets covering 5% or above of the data size have errors less than 0.8% .    to compare dyadic ranges , we linearize the data by sorting by the work location i d .",
    "the size of the published dyadic ranges used is @xmath116 , smaller than original data cardinality .",
    "figure  [ fig : exp4_map2 ] shows the benefit of dyadic ranges for even small - medium range sizes ( around 1% of the data size ) .",
    "filtering improves the accuracy over just applying the geometric mechanism .",
    "this can be explained as a case where the data has a very high variance , and most of the small - valued entries are masked with noise , hence we do not need to publish a large sample .",
    "we further apply consistency checks on this data .",
    "figure [ fig : exp4_map_consistency ] shows that having these checks helps improve the accuracy about 10% , similarly to the above setting where the non - zeros are placed uniformly .",
    "we also compared to the method of machanavajjhala _ et al . _ to anonymize data via synthetic data generation @xcite .",
    "we applied their method using parameters @xmath117 , @xmath118 , to satisfy @xmath119-probabilistic differential privacy . while this approach has been shown to approximately preserve features such as the distribution of commuting distance in the data @xcite",
    ", it does not seem to help to accurately answer range queries .",
    "for example , over the same set of queries we observe absolute error over three times that of the geometric mechanism with the same parameters .",
    "further , it does not seem possible to find a setting of the parameters @xmath120 required by the method to obtain stricter privacy guarantees ( i.e. @xmath121 ) such as those we focus on here ( our experiments are carried out with the stronger guarantee of @xmath100 ) .",
    "differential privacy represents a powerful mechanism for releasing data without compromising the privacy of the data subjects .",
    "we have shown that our shortcut approach is an effective way to release data under differential privacy without overwhelming the data user or the data owner .",
    "the accuracy of query answering from the released summary compares favorably to the laborious approach of publishing vast data with geometric noise , and can actually help the user to improve their utility by removing a lot of the noise without compromising the privacy .",
    "both filtering and sampling are effective in different cases , but the combined filter - priority method seems generally useful across a wide variety of settings . on data which is sparse , as is the case for most realistic examples , the cost creating the summary is low , and the benefit only improves as the data dimensionality increases .",
    "when we expect that range queries are prevalent , the summaries can be applied to techniques such as dyadic ranges .",
    "the benefits of the summary still hold ( compact , accurate , fast to compute ) and the benefit compared to the base method is felt for moderately short queries , corresponding to a small fraction of the input space .",
    "10    b.  barak , k.  chaudhuri , c.  dwork , s.  kale , f.  mcsherry , and k.  talwar .",
    "privacy , accuracy , and consistency too : a holistic solution to contingency table release . ,  2007 .",
    "m.  charikar , k.  chen , and m.  farach - colton .",
    "finding frequent items in data streams . , 2002 .",
    "chen , d.  kifer , k.  lefevre , and a.  machanavajjhala . .",
    "now publishers , 2009 .",
    "g.  cormode and d.  srivastava .",
    "anonymized data : generation , models , usage . , 2009 .",
    "n.  duffield , c.  lund , and m.  thorup .",
    "estimating flow distributions from sampled flow statistics . , 2003 .",
    "n.  duffield , c.  lund , and m.  thorup .",
    "priority sampling for estimation of arbitrary subset sums . , 54(6 ) , 2007 .",
    "c.  dwork . differential privacy . , pages 112 , 2006 .",
    "m.  garofalakis , j.  gehrke , and r.  rastogi . querying and mining data streams",
    ": you only get one look . , 2002 .",
    "j.  gehrke , d.  kifer , and a.  machanavajjhala .",
    "privacy in data publishing . ,",
    "a.  ghosh , t.  roughgarden , and m.  sundararajan .",
    "universally utility - maximizing privacy mechanisms . , 2009 .",
    "m.  hay , v.  rastogi , g.  miklau , and d.  suciu . boosting the accuracy of differentially - private histograms through consistency . ,",
    "s.  isaacman , r.  becker , r.  cceres , s.  kobourov , j.  rowland , and a.  varshavsky . a tale of two cities . , 2010",
    "d.  kifer .",
    "attacks on privacy and definetti s theorem .",
    ", 2009    d.  kifer and b .-",
    "r . lin . towards an axiomatization of statistical privacy and utility . , 2010 .    c.  li , m.  hay , v.",
    "rastogi , g.  miklau , and a.  mcgregor . optimizing linear counting queries under differential privacy . , 2010 .",
    "a.  machanavajjhala , d.  kifer , j.  m. abowd , j.  gehrke , and l.  vilhuber .",
    "privacy : theory meets practice on the map . , 2008 .",
    "f.  mcsherry .",
    "privacy integrated queries : an extensible platform for privacy - preserving data analysis . ,",
    "f.  mcsherry and k.  talwar .",
    "mechanism design via differential privacy . , 2007 .",
    "s.  muthukrishnan . .",
    "now publishers , 2005 .",
    "f.  olken . .",
    "phd thesis , berkeley , 1997 .",
    "x.  xiao , g.  wang , and j.  gehrke .",
    "differential privacy via wavelet transforms . ,",
    "y.  xiao , l.  xiong , and c.  yuan .",
    "differentially private data release through multidimensional partitioning .",
    "* census income data . * census income data has attributes ( _ age , birthplace , occupation , income _ ) .",
    "samples can be downloaded from www.ipums.org . to represent in a contingency table , we considered income at the granularity of $ 10,000 multiples , and age to multiples of 5 years . under various settings",
    ", we observed data density of 3% to 5% .     * onthemap data . *",
    "the us census bureau makes available data describing commuting patterns us residents , as the number of people for each work - home combination ( at the census block level ) , together with other information such as age ranges , salary ranges , and job types .",
    "we consider the 47 states available in version 4 of the 2008 data from http://lehdmap.did.census.gov/. we take the location data as the first 2 digits of the census tracts in each county ; so each location is identified by `` county i d + 2-digit tract i d '' .",
    "there are 4001 such locations , so the size of the resulting frequency matrix is @xmath122 .",
    "the number of non - zeros is @xmath123 , so the data density is @xmath124 . ] .",
    "the mean value in each non - zero cell is approximately 150 , but with very high variance : many cells have frequency 1 or 2 ( and hence should be masked by the addition of noise ) .     * adult data . *",
    "the uci adult data from the machine learning repository at http://archive.ics.uci.edu/ml/datasets/adult has been widely used in data mining , and prior work on privacy  @xcite .",
    "the full data has 14 attributes , but to avoid gridding issues , we projected the data on categorical attributes , i.e , _ workclass , education , maritalstatus , occupation , relationship , race , sex_. this generated data with a density of @xmath125 , and an average value in each ( non - zero ) cell of 9",
    ".     * telecom warehouse data . * at&t records measurements on the performance of devices in its network in a data warehouse .",
    "these measurements include attributes _ deviceid , timestamp , val _ , representing a measurement _ val _ of each device at a given time stamp . for each day",
    ", many gigabytes of data are added to the warehouse . for several natural bucketings of the numerical attributes ,",
    "the observed density ranges from 0.5% to 2% .",
    "therefore , the output of reporting all differentially private counts is 50 - 200 times larger .",
    "generating , storing and processing data in this form would increase the associated costs by the same factors , making this vastly too expensive .",
    "nevertheless , given the company s data protection policies , and the need for various internal groups to analyze this data , a practical anonymization solution is highly desirable .",
    "differential privacy was introduced over the course of a series of papers in the 2000s , culminating in dwork s paper which coined the term @xcite .",
    "a randomized algorithm achieves @xmath19 differential privacy if the probability of output falling in some set is at most @xmath126 times the probability of the output falling in the same set , given input which differs in the records of at most one individual .",
    "a randomized algorithm @xmath127 gives @xmath19-differential privacy if for all data sets @xmath128 and @xmath129 differing on at most one element , and all @xmath130 : @xmath131    the choice of @xmath126 makes it easy to reason about composing results : when two pieces of information about a data set are released with @xmath132 and @xmath133 differential privacy respectively , their combination is ( at most ) @xmath134 differentially private .",
    "there is a simple recipe for generating differentially private numerical data : compute the true answer exactly , and then output this value with additive noise drawn from a laplacian distribution with parameter @xmath135 , where @xmath136 is the `` sensitivity '' of the query , the total influence that any individual can have on the output @xcite . for discrete data ,",
    "the laplacian can be replaced with a symmetric geometric distribution @xcite . in this paper , we focus collections of ( disjoint ) count queries , where the conditions partition the input ( as in the case of contingency tables and count - cubes ) : in this case @xmath137 , as adding or removing one individual affects at most one value in the collection of counts .    although often introduced in the context of allowing queries to be answered interactively with privacy , differential privacy naturally applies to data publication .",
    "effectively , the data owner chooses a collection of representative queries to ask , and answers them all with appropriate noise .",
    "this has been promoted particularly in the case of histograms or contingency tables of data @xcite .",
    "when the data is not numeric , other approaches are possible . the exponential mechanism @xcite gives a generic approach : the probability of producing a particular output ( amongst a set of possible outputs ) should depend exponentially on how closely it resembles the original input .",
    "directly instantiating the exponential mechanism is costly in general , since often a huge number of possible outputs must be considered .",
    "however , certain special cases have been shown to be efficient . in recent years",
    ", there has been interest in the database community in how to incorporate differential privacy into data management .",
    "mcsherry s pinq system adds privacy as an overlay by automating the addition of noise to query outputs @xcite .",
    "et al _ observe that directly applying the laplacian mechanism to contingency tables yields poor answers to range queries , since the noise grows with the size of the range @xcite .",
    "their solution was to add noise in the wavelet domain , which has the same privacy properties , but reduces the error for large range queries due to cancellations .",
    "similar techniques are suggested by hay _",
    "a more general approach is given by li _",
    "@xcite , by considering queries in the form of arbitrary collections of linear sums of the input data ( represented as a matrix ) .",
    "a key difference of this prior work is that it considers publishing results which are at least as big as the underlying domain size of the data .",
    "in contrast , we consider the case where the total domain cardinality of the data in contingency table form is too large to be published in full .",
    "xiao _ et al _ @xcite publish a summary in the form of a kd - tree of counts .",
    "the construction algorithm examines all cells in the contingency table of the data : a natural question is to extend our methods to produce similar summaries over high domain cardinality data .",
    "lastly , there has been some work on synthetic data generation based on parameters derived privately from real data sets",
    ". specifically , machanavajjhala _ et al . _",
    "propose a technique tailored for certain types of census data @xcite .",
    "this approach produces an output data size comparable to the input size when direct application of noise mechanisms would generate a dense , noisy output .",
    "the guarantees produced are that the synthetic data satisfies a weaker ( @xmath19 , @xmath138)-guarantee , for a larger value of @xmath19 than we tolerate .",
    "instead , we aim to produce output which is based directly on the original microdata , and offers high levels of privacy .",
    "the topic of data summarization is vast , generating many surveys and books on different approaches to the problem @xcite . in this paper",
    ", we consider some key techniques .",
    "filters are deterministic procedures which prune away parts of the data which are thought to contribute little to the overall query results .",
    "the canonical filter is the high - pass filter : data elements with low frequencies ( below the filter threshold ) are dropped , while the elements with high frequencies ( aka the ` heavy hitters ' ) are retained .",
    "sampling is perhaps the most commonly used data reduction technique .",
    "a random process determines a subset of elements from the input data to retain , along with weights to assign each element in the sample .",
    "queries over the full data are then approximated by applying the same query to the ( weighted ) data in the sample , and reporting the sample result . in our setting",
    ", we have initial weights attached to the input elements . over such data ,",
    "threshold sampling is based on a parameter @xmath54 @xcite .",
    "the sample includes each element @xmath139 with weight @xmath140 with probability @xmath141 .",
    "if sampled , the sample s weight is assigned to be @xmath142 .",
    "one can verify answers to subset sum queries ( sum of elements passing a given predicate ) are unbiased , i.e. correct in expectation .",
    "a limitation of threshold sampling is that there is not a strong control over the size of the resulting sample : the size is @xmath143 in expectation , but may vary a lot .",
    "priority sampling fixes the sample size to be @xmath67 , and offers strong guarantees about the quality of the sample relative to all methods which generate samples of size exactly @xmath67 @xcite .",
    "each element is assigned a priority @xmath144 , where @xmath145 is drawn uniformly in the range [ 0,1 ] .",
    "the @xmath67 elements with the largest @xmath146 values are retained to form the sample .",
    "weights are defined based on the @xmath68th priority value , denoted @xmath69 : the adjusted weight of element @xmath139 is @xmath147 .",
    "lastly , sketching techniques summarize the whole data set .",
    "the count sketch hashes each element @xmath139 to one of @xmath148 possible buckets as @xmath149 , and applies a second hash function @xmath150 to map each element to either + 1 or -1 .",
    "each bucket maintains the sum of all counts @xmath140 mapped there , each multiplied by @xmath150 .",
    "given a target element @xmath139 , the count of the bucket @xmath149 multiplied by @xmath150 is an unbiased estimate for @xmath140 with bounded variance .",
    "taking the mean or median of @xmath89 independent repetitions of the sketching reduces the variance further @xcite .",
    "section [ sec : sketch ] analyzes sketches under differential privacy .",
    "clearly , filter has the same action as the laborious approach and hence has the same distribution , on the @xmath10 non - zero entries of @xmath12 .",
    "we therefore focus on the distribution of entries which are zero in @xmath12 that are represented in @xmath23 . for entries @xmath40",
    "such that @xmath47 , the probability they pass the filter is : @xmath151 & = & \\sum_{x \\geq \\theta }   \\frac{1-\\alpha}{1+\\alpha } \\alpha^{|x| } \\\\   & = & \\frac{1-\\alpha}{1+\\alpha } \\alpha^{\\theta } \\sum_{x \\geq 0 }   \\alpha^{x } \\\\   & = & \\frac{1-\\alpha}{1+\\alpha } \\alpha^{\\theta } \\frac{1}{1-\\alpha } \\\\   & = & \\frac{\\alpha^\\theta}{1+\\alpha }    \\triangleq p_\\theta \\end{aligned}\\ ] ]    hence , the number of zero entries upgraded , i.e. , which pass the filter after noise is added , follows a binomial distribution @xmath152 .",
    "we need to select this many locations @xmath40 in @xmath12 such that @xmath47 .",
    "assuming @xmath153 , we can just perform rejection sampling : randomly pick a location uniformly from the @xmath13 possibilities , and accept it if @xmath47 ; otherwise , reject and repeat .",
    "the distribution of values which were zero but whose noise exceeds @xmath30 is : @xmath154 & = & \\frac{{\\mathsf{pr } } [ m'(i ) = b]}{{\\mathsf{pr}}[m'(i ) \\geq \\theta ] } \\\\   & = & ( 1-\\alpha ) \\alpha^{b - \\theta } \\end{aligned}\\ ] ] the cdf of this distribution is : @xmath155 & = & \\sum_{m'(i ) \\leq x } { \\mathsf{pr}}[m'(i ) | m'(i ) \\geq \\theta ] \\\\   & = & \\sum_{b=\\theta}^x ( 1-\\alpha ) \\alpha^{b-\\theta } \\\\   & = & 1 - \\alpha^{x- \\theta + 1 } ,   \\forall x \\geq \\theta   \\end{aligned}\\ ] ] hence the algorithm which uses this distribution operates as claimed .    given this form of the cdf , it is straightforward to draw from it : we draw a uniform random value @xmath145 in @xmath156 , and invert the equation to find for which @xmath157 is @xmath158 = r$ ] .",
    "this value of @xmath157 is then distributed according to the required distribution , and can be used as the value for @xmath41 .    the two - sided filter , which passes values @xmath159 , is very similar .",
    "the only differences are that @xmath37 is now equal to @xmath160 , and the cdf now includes both positive and negative values .",
    "the new cdf sets @xmath161 = 1-\\alpha^{x-\\theta+1}$ ] .",
    "so we draw from the same distribution as before , but now flip a fair coin to determine the sign of @xmath45",
    ".     * time and space analysis . *",
    "the running time of this algorithm is composed of the time to process the @xmath10 non - zero entries , and the time to upgrade the @xmath0 zero entries . since adding noise and drawing a value for each upgraded zero both take constant time , the overall cost is @xmath162 .",
    "the value @xmath0 depends on the parameter @xmath26 : in expectation , @xmath163=np_\\theta$ ] , which in turn depends on @xmath30 .",
    "suppose we have a target output size of @xmath164 tuples ( chosen , perhaps , roughly proportional to the original data size @xmath10 ) .",
    "how should we choose @xmath30 ?",
    "first , we focus on the zeros : if we pick @xmath165 , then we expect to have around @xmath164 zeros upgraded .",
    "this leads us to pick @xmath166 for one - sided filters .",
    "we can apply this threshold , and consider the output of the filter algorithm : if it is sufficiently close to @xmath164 , we can accept the output .",
    "otherwise , if the output is too large we need to choose a higher @xmath30 . rather than re - running the algorithm , we can simply take its output and apply a higher filter @xmath167 on it until we reach the desired sample size .",
    "if the output of the algorithm is too small , we can re - run the filter algorithm with the same @xmath30 value until we obtain a sufficiently large summary .",
    "* query answering .",
    "* naively , the filtered output can be used directly to answer queries .",
    "this is somewhat biased , in that all small values in @xmath15 have been replaced with zero .",
    "however , it may work well in practice if @xmath30 is chosen so that values below @xmath30 are mostly noise , while most of the original data values in @xmath12 were comfortably above @xmath30 .",
    "one can also consider other heuristic corrections to the output , such as assuming all values absent from @xmath23 are , e.g. , @xmath168 or some other calibrated value .",
    "the subsequent methods we consider avoid this issue by providing unbiased estimators for query answering .",
    "using equations and , the probability that @xmath40 is in the sample is given by : @xmath173 & = \\sum_\\nu { \\mathsf{pr } } [ i \\in s | m'(i ) = \\nu ] { \\mathsf{pr } } ( m'(i ) = \\nu )   \\\\      = & \\sum_{|\\nu| \\leq \\tau } \\frac{|\\nu|}{\\tau }          \\frac{1-\\alpha}{1+\\alpha } \\alpha^{|\\nu| } + \\sum_{|\\nu| > \\tau }          \\frac{1-\\alpha}{1+\\alpha } \\alpha^{|\\nu| }   \\\\",
    "= & 2 \\left ( \\frac{1- \\alpha}{\\tau(1+\\alpha ) } \\sum_{\\nu            = 0}^\\tau   \\nu \\alpha^\\nu +   \\sum_{\\nu > \\tau }          \\frac{1-\\alpha}{1+\\alpha } \\alpha^{\\nu } \\right )      \\\\       = &   2 \\left ( \\frac{\\alpha}{\\tau(1- \\alpha^2 ) } ( 1 - ( \\tau+1 )          \\alpha^{\\tau } + \\tau \\alpha^{\\tau+1 } )   +          \\frac{\\alpha^{\\tau+1}}{1 + \\alpha } \\right )   \\\\       = & \\frac{2\\alpha ( 1 - \\alpha^\\tau)}{\\tau(1- \\alpha^2 ) }       \\triangleq     p_\\tau\\end{aligned}\\ ] ]    since each zero has the same , equal chance of being sampled by the threshold sampling , the number sampled follows a binomial distribution @xmath174 .",
    "given that @xmath40 is picked for the sample , its value @xmath45 is conditioned on the fact that it was sampled : @xmath175 & = & \\frac{{\\mathsf{pr}}[i \\in s| m'(i ) = \\nu ] { \\mathsf{pr}}[m'(i ) = \\nu ] } { { \\mathsf{pr}}[(i ) \\in s ] } \\\\      & = & \\frac { \\min ( \\frac{|\\nu|}{\\tau } , 1 ) \\cdot   \\frac{1-\\alpha}{1+\\alpha } \\alpha^{|\\nu|}}{p_\\tau } \\\\\\end{aligned}\\ ] ] @xmath176   & = \\frac{\\frac{1-\\alpha}{1+\\alpha }    \\alpha^{|\\nu|}}{\\frac{2\\alpha(1 - \\alpha^\\tau)}{\\tau(1-\\alpha^2 ) } } \\\\       & = \\frac{\\tau(1-\\alpha)^2 \\alpha^{|\\nu|}}{2\\alpha ( 1-\\alpha^\\tau)}\\end{aligned}\\ ] ]      let @xmath62 ; then the cdf of this distribution , at any @xmath178 , can be computed by summing the probability of @xmath179 $ ] , @xmath180 , given by the above two expressions .",
    "it can be broken into four pieces and written as follows .",
    "@xmath181 & =    \\tau \\alpha^{-\\nu}c_\\tau(1-\\alpha )   \\\\[6pt ]    { \\mathsf{pr}}[-\\tau < x = \\nu \\leq 0 ] & = c_\\tau ( \\tau \\alpha^\\tau ( 1-\\alpha ) -\\nu    \\alpha^{-\\nu } + ( \\nu + 1 ) \\alpha^{-\\nu+1 } \\\\    & \\qquad \\qquad - \\tau \\alpha^\\tau + ( \\tau-1)\\alpha^{\\tau+1 } ) \\\\      & = c_\\tau ( - \\nu \\alpha^{-\\nu }   + ( \\nu + 1 ) \\alpha^{-\\nu + 1 } -    \\alpha^{\\tau+1 } )   \\\\[6pt ] { \\mathsf{pr } } [ 0 < x = \\nu \\leq \\tau ]     & =   \\frac{1}{2 } +   \\alpha c_\\tau(1 - ( \\nu+1)\\alpha^{\\nu } + \\nu\\alpha^{\\nu+1 } )      \\\\[6pt ] { \\mathsf{pr}}[\\tau < x = \\nu ]   & =       \\frac{1}{2 } + c_\\tau ( \\alpha ( 1 - ( \\tau+1)\\alpha^{\\tau } + \\tau \\alpha^{\\tau+1})\\\\       & \\qquad \\qquad + \\tau\\alpha^{\\tau+1 } ( 1 - \\alpha^{\\nu-\\tau})(1-\\alpha ) ) \\\\    & = \\frac{1}{2 } + \\alpha c_\\tau(1 - \\alpha^\\tau - \\tau\\alpha^\\nu ( 1-\\alpha ) ) \\end{array}\\ ] ]       * time and space analysis .",
    "* it follows immediately from the description of the algorithm that the sample can be generated in time @xmath183 .",
    "we now analyze how to choose @xmath54 given a desired sample size @xmath164 .",
    "we can do the same as for estimating @xmath30 in high - pass filters , i.e. , choosing @xmath54 so that the number of zeros sampled is about @xmath164 . or , we can also make a more accurate estimation by taking into account the statistics of the non - zeros as follows",
    ". the expected size of the threshold sample is @xmath184 . on high cardinality data , where @xmath54 will be large so that the sample size is bounded",
    ", we may approximate this by @xmath185 where @xmath186 is the geometric distribution with parameter @xmath44 .",
    "using , we have that @xmath187 = 2\\sum_{j=0}^{\\infty } \\frac{1-\\alpha}{1+\\alpha } j \\alpha^j = \\frac{1-\\alpha}{1+\\alpha } \\frac{2\\alpha}{(1-\\alpha)^2 } = \\frac{2\\alpha}{1-\\alpha^2}.\\ ] ] hence , in expectation this approximation is @xmath188/\\tau =     ( \\|m\\|_1 + 2m\\alpha/(1-\\alpha^2))/\\tau\\ ] ] rearranging , this suggests that we should pick @xmath54 in the region of @xmath189 for a desired sample size @xmath164 . as with the high - pass filter , we can draw a sample based on an estimate of @xmath164 , then refine our choice of @xmath54 to achieve a desired sample size . however , this is taken care of more directly when we build a priority sample of fixed size , discussed in the next section",
    ".     * query answering . * each element is sampled with probability @xmath190 .",
    "if we scale each sampled item s weight by this probability , we obtain an unbiased horvitz - thompson estimator for the weight @xcite .",
    "the data owner can scale all values by this weight before release .",
    "this allows queries to be answered by evaluating the query over the sample , using the adjusted weights .",
    "however , for very small queries ( e.g. point queries ) , it can more accurate to use the ( biased ) estimator of the unadjusted weight .",
    "* query answering .",
    "* as with threshold sampling , there is an unbiased way to adjust the weights : letting @xmath69 be the @xmath68th priority , we assign each item a weight with magnitude @xmath191 while keeping the sign of @xmath41 .",
    "this is unbiased , and has low variance for arbitrary subset queries compared to any other possible sampling scheme which builds a sample of @xmath67 items @xcite .",
    "we have two parameters : @xmath30 for the filter , and @xmath54 for the sampling . if we set @xmath192 , then every item which passes the filter enters the sample , i.e. we have just filtering .",
    "if we set @xmath193 , then every item passes the filter , i.e. we have just sampling .",
    "the interesting range occurs when we have @xmath194 : in this regime , items in @xmath15 with absolute weights below @xmath30 are suppressed , above @xmath54 are guaranteed to be included in @xmath23 , and in between have a chance proportional to their weight to be included .",
    "as in previous cases , we can handle the non - zero elements of @xmath12 directly : we add noise drawn from the appropriate geometric distribution to obtain @xmath45 , and first filter then sample the element .",
    "it is the large number of zero elements which we must treat more carefully to keep the process efficient .",
    "we can follow the analysis in previous sections to help us determine how to proceed .",
    "for any zero entry , the distribution of its noisy value after threshold filtering , @xmath195 is : @xmath196 , @xmath197 & = & \\frac{1-\\alpha}{1+\\alpha}\\alpha^{|\\nu| } \\\\[6pt ] { \\mathsf{pr}}[m'_\\theta(i ) = 0 ] & = & { \\mathsf{pr}}[m'(i ) < \\theta ]   = \\sum_{|x| < \\theta } \\frac{1-\\alpha}{1 + \\alpha } \\alpha^{|x| } \\\\   & = & \\frac{1-\\alpha}{1+\\alpha } 2 \\sum_{x < \\theta } \\alpha^x    = 1 - \\frac{2\\alpha^\\theta}{1+\\alpha}\\end{aligned}\\ ] ]    given the parameter @xmath54 , the value of @xmath198 , the probability of the element being sampled , is : @xmath199 { \\mathsf{pr } } [ m_\\theta'(i ) = \\nu ] \\\\     & = & \\sum_{|\\nu| >",
    "\\tau } \\frac{1-\\alpha}{1+\\alpha }     \\alpha^{|\\nu| } + \\sum_{\\theta \\leq |\\nu| \\leq \\tau }     \\frac{1-\\alpha}{1+\\alpha } \\alpha^{|\\nu| } \\frac{|\\nu|}{\\tau } \\\\      & = & 2 \\left ( \\sum_{\\nu > \\tau } \\frac{1-\\alpha}{1+\\alpha }     \\alpha^{\\nu } + \\sum_{\\theta \\leq \\nu \\leq \\tau }     \\frac{1-\\alpha}{1+\\alpha } \\alpha^{\\nu } \\frac{\\nu}{\\tau } \\right ) \\\\    & = & 2 \\left ( \\frac{\\alpha^{\\tau+1}}{1+\\alpha } +     \\frac{\\alpha}{\\tau(1-\\alpha^2)}(\\theta \\alpha^{\\theta-1 } -     ( \\theta-1)\\alpha^\\theta \\right .",
    "\\left . - ( \\tau+1 ) \\alpha^\\tau",
    "+ \\tau \\alpha^{\\tau+1 } ) \\right ) \\\\   &",
    "= & \\frac{2}{\\tau(1-\\alpha^2 ) } ( \\theta \\alpha^{\\theta } - ( \\theta-1 ) \\alpha^{\\theta+1 } - \\alpha^{\\tau+1})\\end{aligned}\\ ] ] observe that for the case @xmath193 , this simplifies to @xmath200 , the expression for @xmath201 from the previous section . as before , we can use @xmath198 to determine the number of zero locations from @xmath12 to sample .",
    "given such a location , conditioned on it being included in the sample , the distribution of its value is now @xmath202 & = & \\frac{{\\mathsf{pr}}[i \\in s| m_\\theta'(i ) = v ] { \\mathsf{pr}}[m_\\theta'(i ) = v]}{{\\mathsf{pr } } [ i \\in s ] } \\\\   & = & \\tau c_{\\theta,\\tau }   ( 1-\\alpha)^2 \\alpha^{|v| } \\min ( \\frac{|\\nu|}{\\tau } , 1)\\end{aligned}\\ ] ] where the constant @xmath203 .",
    "since we can answer threshold samples on the filtered data , we can also support priority samples via the observation of section [ sec : priority ] , that we can extract a priority sample from a threshold sample .",
    "* query answering . *",
    "the standard sampling estimation schemes allow us to provide unbiased estimates over the underlying data .",
    "in this case , the estimates are of the filtered version of @xmath15 .",
    "since filtering with a low threshold removes much of the noise , being unbiased over the filtered @xmath15 may be a good approximation of the original @xmath12 data , while remaining private .",
    "the approach is quite direct : from @xmath12 , we produce the ( exact ) transform into either wavelets or dyadic ranges , then apply the above algorithms to this transform .",
    "the theorem follows by observing that the total number of non - zero values in the transform of @xmath12 can be bounded in terms of @xmath10 and @xmath13 .",
    "we outline the case for dyadic ranges ; wavelets are similar .",
    "consider each non - zero in @xmath12 as a leaf in a binary tree .",
    "the number of non - zeros in the dyadic range transform of @xmath12 is the number of distinct nodes touched by following the paths from each non - zero to the root .",
    "each non - zero can touch at most one node in any level , giving a crude bound of @xmath206 .",
    "this can be tightened by observing that for levels close to the root , there may be fewer than @xmath10 nodes . more precisely , for the top @xmath207 levels , there are at most @xmath10 nodes in total .",
    "thus we bound the total number of nodes touched , and hence the number of non - zeros in the transform as :      both filtering and sampling keep information about a selected subset of locations in the original data space , and drop information about the remaining locations .",
    "in contrast , sketch techniques bring together information about every point in the original data .",
    "a first approach is to generate the noisy data @xmath15 , and produce the sketch of this : using the count sketch ( described in section [ sec : reduction ] ) , we would have significant noise in each bucket in the sketch .",
    "we can improve on this considerably by observing that , viewing the sketch as a query , the sketch has low sensitivity .",
    "that is , we can view sketching as a mechanism for publishing data similar to histograms : once we fix the hash function @xmath81 , each bucket contains the sum of counts of a set of individuals , and this forms a partition of the input data .",
    "thus , over @xmath89 rows of the sketch , the sensitivity is @xmath208 , and we can achieve privacy by adding the appropriate amount of random noise to each entry in the sketch . consequently :      the time cost follows from the fact that we have to map each non - zero location in the input to @xmath89 rows of the sketch , and then add noise to each entry .",
    "the variance of each estimate , due to sketching , is proportional to the euclidean norm of the data scaled by the number of buckets , @xmath212 @xcite .",
    "the noise added for privacy simply adds to this variance , in the amount of @xmath213 for noise with parameter @xmath214 . taking the average of @xmath89 repetitions scales the variance down by a factor of @xmath89 to @xmath211 .",
    "thus there is a tradeoff for setting the parameter @xmath89 : increasing @xmath89 reduces the sketching error , but increases the privacy error . in an experimental study ,",
    "we able to use large values of @xmath148 , so the second term dominated , meaning that the optimal setting was to pick small values of @xmath89 , such as @xmath215 .",
    "we observed that the error was considerably higher than for sampling / filtering , so we omit a detailed study of sketching from this presentation ."
  ],
  "abstract_text": [
    "<S> the problem of privately releasing data is to provide a version of a dataset without revealing sensitive information about the individuals who contribute to the data . </S>",
    "<S> the model of differential privacy allows such private release while providing strong guarantees on the output . </S>",
    "<S> a basic mechanism achieves differential privacy by adding noise to the frequency counts in the contingency tables ( or , a subset of the count data cube ) derived from the dataset . </S>",
    "<S> however , when the dataset is sparse in its underlying space , as is the case for most multi - attribute relations , then the effect of adding noise is to vastly increase the size of the published data : it implicitly creates a huge number of dummy data points to mask the true data , making it almost impossible to work with .    </S>",
    "<S> we present techniques to overcome this roadblock and allow efficient private release of sparse data , while maintaining the guarantees of differential privacy . </S>",
    "<S> our approach is to release a compact summary of the noisy data . generating the noisy data and then summarizing it would still be very costly , so </S>",
    "<S> we show how to shortcut this step , and instead directly generate the summary from the input data , without materializing the vast intermediate noisy data . </S>",
    "<S> we instantiate this outline for a variety of sampling and filtering methods , and show how to use the resulting summary for approximate , private , query answering . </S>",
    "<S> our experimental study shows that this is an effective , practical solution , with comparable and occasionally improved utility over the costly materialization approach . </S>"
  ]
}