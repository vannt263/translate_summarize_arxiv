{
  "article_text": [
    "this methodology to codify and extract symbolic knowledge from a nn is very simple and efficient for the extraction of comprehensible rules from medium - sized data sets .",
    "it is , moreover , very sensible to attribute relevance .    in the theoretical point of view",
    "it is particularly interesting that restricting the values assumed by neurons weights restrict the information propagation in the network , thus allowing the emergence of patterns in the neuronal network structure . for the case of linear neuronal networks , having by activation function the identity truncate to 0 and 1 ,",
    "these structures are characterized by the occurrence of patterns in neuron configuration directly presentable as formulas in logic ."
  ],
  "abstract_text": [
    "<S> this work describes a methodology that combines logic - based systems and connectionist systems . </S>",
    "<S> our approach uses finite truth - valued ukasiewicz logic , wherein every connective can be defined by a neuron in an artificial network @xcite . </S>",
    "<S> this allowed the injection of first - order formulas into a network architecture , and also simplified symbolic rule extraction . </S>",
    "<S> for that we trained a neural networks using the levenderg - marquardt algorithm , where we restricted the knowledge dissemination in the network structure . </S>",
    "<S> this procedure reduces neural network plasticity without drastically damaging the learning performance , thus making the descriptive power of produced neural networks similar to the descriptive power of ukasiewicz logic language and simplifying the translation between symbolic and connectionist structures . </S>",
    "<S> we used this method for reverse engineering truth table and in extraction of formulas from real data sets .    -    </S>",
    "<S> [ [ section ] ]    there are essentially two representation paradigms , usually taken very differently . on one hand </S>",
    "<S> , symbolic - based descriptions are specified through a grammar that has fairly clear semantics . on the other hand , the usual way to see information presented using a connectionist description is its codification on a neural network ( nn ) . </S>",
    "<S> artificial nns , in principle , combine - among other things - the ability to learn and robustness or insensitivity to perturbations of input data . </S>",
    "<S> nns are usually taken as black boxes , thereby providing little insight into how the information is codified . </S>",
    "<S> it is natural to seek a synergy integrating the _ white - box _ </S>",
    "<S> character of symbolic base representation and the learning power of artificial neuronal networks . </S>",
    "<S> such neuro - symbolic models are currently a very active area of research : for the extraction of logic programs from trained networks see @xcite @xcite .    our approach to neuro - symbolic models and knowledge extraction is based on a comprehensive language for humans , representable directly in a nn topology and able to be used . </S>",
    "<S> this is done on knowledge - based networks @xcite @xcite , to generate the initial network architecture from crude symbolic domain knowledge . in the other direction , the hardest problem , </S>",
    "<S> neural language can be translated into a symbolic language . </S>",
    "<S> however in @xcite @xcite @xcite this processes is used by identifing the most significant determinants of decision or classification . </S>",
    "<S> hence , any individual unit must be associated with a single concept or feature of the problem domain . in this work </S>",
    "<S> we used a first - order language wherein formulas are interpreted as nns . in this framework formulas are simple to inject into a multilayer feed - forward network , and the system is free from the need of giving interpretation to hidden units in the problem domain .    </S>",
    "<S> our approx to the generation of neuro - symbolic models used ukasiewicz logic . </S>",
    "<S> this type of many - valued logic has a very useful property motivated by the  linearity  of logic connectives . </S>",
    "<S> every logic connective can be defined by a neuron in an artificial network having , by activation function , the identity truncated to zero and one @xcite . </S>",
    "<S> this allows the direct codification of formulas into network architecture , and simplifies the extraction of rules . </S>",
    "<S> multilayer feed - forward nn , having this type of activation function , can be trained efficiently using the levenderg - marquardt ( lm ) algorithm @xcite , and the generated network can be simplified quickly using the `` optimal brain surgeon '' algorithm proposed by b. hassibi , d. g. stork and g.j . </S>",
    "<S> stork @xcite .    </S>",
    "<S> this strategy has good performance when applied to the reconstruction of formulas from truth tables . in this type of reverse engineering problem </S>",
    "<S> , we presuppose no noise </S>",
    "<S> . however , the process is stable for the introduction of gaussian noise . </S>",
    "<S> this motivates its application to extract comprehensible symbolic rules from real data .    </S>",
    "<S> [ [ section-1 ] ]      classical propositional logic is one of the earliest formal systems of logic . </S>",
    "<S> the algebraic semantics of this logic are given by boolean algebra . </S>",
    "<S> both , the logic and the algebraic semantics have been generalized in many directions . </S>",
    "<S> many - valued logics , is one of this generalizations , and can be conceived as a set of formal representation languages that proven to be useful for both real world and computer science applications . in applications of many - valued logic , like fuzzy logic , the properties of boolean conjunction are too rigid , this is overtake extending a new binary connective , @xmath0 , usually called _ </S>",
    "<S> fusion_. the generalization of boolean algebra can be based in the relationship between conjunction and implication given by @xmath1 these equivalences , can be used to present implication as a generalized inverse for conjunction .    </S>",
    "<S> these two operators are defined in a partially ordered set of truth values , @xmath2 , thereby extending the two - valued set of an boolean algebra . </S>",
    "<S> if @xmath3 has more than two values , the associated logics are called a _ many - valued logics_. a many - valued logic having @xmath4 $ ] as set of truth values is called a _ fuzzy logic_. in this type of logics a continuous fusion operator @xmath0 is known as a _ t_-norm . </S>",
    "<S> the following are example of continuous @xmath5-norms :    1 .   </S>",
    "<S> _ ukasiewicz _ @xmath5-norm : @xmath6 . </S>",
    "<S> 2 .   </S>",
    "<S> product @xmath5-norm : @xmath7 usual product between real numbers . 3 .   </S>",
    "<S> gdel @xmath5-norm : @xmath8 .    </S>",
    "<S> the fuzzy logic defined using _ </S>",
    "<S> ukasiewicz _ </S>",
    "<S> @xmath5-norm is called ukasiewicz logic ( logic ) and the corresponding propositional calculus has a nice complete axiomatization @xcite . in this type of logic </S>",
    "<S> the implication , is called _ residuum _ operator , and is given by @xmath9 .    like first - order languages , in logic , sentences are usually built from ( countable ) set of propositional variables , @xmath0 the fusion operator , implication @xmath10 , and the truth constant 0 . </S>",
    "<S> further connectives are defined as follows :    [ cols=\"<,^ , < \" , ]     the extracted formula    @xmath11    is @xmath12-similar , with @xmath13 to the original nn . </S>",
    "<S> formula @xmath14 misses the classification for 40 cases . </S>",
    "<S> note that the symbolic model is stable , the bad performance of @xmath15 representation do not affect the model . </S>"
  ]
}