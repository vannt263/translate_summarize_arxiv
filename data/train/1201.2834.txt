{
  "article_text": [
    "we consider games played between two players on graphs . at every round of the game , each of the two players selects a move ; the moves of the players then determine the transition to the successor state .",
    "a play of the game gives rise to a path in the graph .",
    "we consider the two basic objectives for the players : _ reachability _ and _ safety_. the reachability goal asks player  1 to reach a given set of target states or , if randomization is needed to play the game , to maximize the probability of reaching the target set .",
    "the safety goal asks player  2 to ensure that a given set of safe states is never left or , if randomization is required , to minimize the probability of leaving the target set .",
    "the two objectives are dual , and the games are determined : the supremum probability with which player  1 can reach the target set is equal to one minus the supremum probability with which player  2 can confine the game to the complement of the target set  @xcite .    these games on graphs can be divided into two classes : _ turn - based _ and _ concurrent_. in turn - based games , only one player has a choice of moves at each state ; in concurrent games , at each state both players choose a move , simultaneously and independently , from a set of available moves . for turn - based games ,",
    "the solution of games with reachability and safety objectives has long been known .",
    "if each move determines a unique successor state , then the games are p - complete and can be solved in linear time in the size of the game graph .",
    "if , more generally , each move determines a probability distribution on possible successor states , then the problem of deciding whether a turn - based game can be won with probability greater than a given threshold @xmath2 $ ] is in np @xmath3 co - np @xcite , and the exact value of the game can be computed by a strategy - improvement algorithm  @xcite , which works well in practice .",
    "these results all depend on the fact that in turn - based reachability and safety games , both players have optimal deterministic ( i.e. , no randomization is required ) , memoryless strategies .",
    "these strategies are functions from states to moves , so they are finite in number , and this guarantees the termination of the strategy - improvement algorithm .",
    "the situation is very different for concurrent games .",
    "the player-1 _ value _ of the game is defined , as usual , as the sup - inf value : the supremum , over all strategies of player  1 , of the infimum , over all strategies of player  2 , of the probability of achieving the reachability or safety goal . in concurrent reachability games ,",
    "player  1 is guaranteed only the existence of @xmath4-optimal strategies , which ensure that the value of the game is achieved within a specified tolerance @xmath5 @xcite . moreover , while these strategies ( which depend on @xmath4 ) are memoryless , in general they require randomization  @xcite ( even in the special case in which the transition function is deterministic ) . for player  2 ( the safety player ) , _ optimal _ memoryless strategies exist  @xcite , which again require randomization ( even when the transition function is deterministic ) .",
    "all of these strategies are functions from states to probability distributions on moves .",
    "the question of deciding whether a concurrent game can be won with probability greater than @xmath6 is in pspace ; this is shown by reduction to the theory of the real - closed fields @xcite .    to summarize :",
    "while strategy - improvement algorithms are available for turn - based reachability and safety games  @xcite , so far no strategy - improvement algorithms or even approximation schemes were known for concurrent games .",
    "if one wanted to compute the value of a concurrent game within a specified tolerance @xmath5 , one was reduced to using a binary search algorithm that approximates the value by iterating queries in the theory of the real - closed fields .",
    "value - iteration schemes were known for such games , but they can be used to approximate the value from one direction only , for reachability goals from below , and for safety goals from above  @xcite .",
    "the value - iteration schemes are not guaranteed to terminate .",
    "worse , since no convergence rates are known for these schemes , they provide no termination criteria for approximating a value within  @xmath4 .",
    "* our results for concurrent reachability games .",
    "* concurrent reachability games belong to the family of stochastic games @xcite , and they have been studied more specifically in @xcite .",
    "our contributions for concurrent reachability games are two - fold .",
    "first , we present a simple and combinatorial proof of the existence of memoryless @xmath1-optimal strategies for concurrent games with reachability objectives , for all @xmath0 .",
    "second , using the proof techniques we developed for proving existence of memoryless @xmath1-optimal strategies , for @xmath0 , we obtain a strategy - improvement ( a.k.a .",
    "policy - iteration ) algorithm for concurrent reachability games .",
    "unlike in the special case of turn - based games the algorithm need not terminate in finitely many iterations .",
    "it has long been known that optimal strategies need not exist for concurrent reachability games , and for all @xmath7 , there exist @xmath1-optimal strategies that are memoryless  @xcite .",
    "a proof of this fact can be obtained by considering limit of discounted games .",
    "the proof considers _ discounted _ versions of reachability games , where a play that reaches the target in @xmath8 steps is assigned a value of @xmath9 , for some discount factor @xmath10 .",
    "it is possible to show that , for @xmath11 , memoryless optimal strategies exist .",
    "the result for the undiscounted ( @xmath12 ) case followed from an analysis of the limit behavior of such optimal strategies for @xmath13 .",
    "the limit behavior is studied with the help of results from the field of real puisieux series  @xcite .",
    "this proof idea works not only for reachability games , but also for total - reward games with nonnegative rewards ( see  ( * ? ? ?",
    "* chapter  5 ) for details ) .",
    "a more recent result @xcite establishes the existence of memoryless @xmath1-optimal strategies for certain infinite - state ( recursive ) concurrent games , but again the proof relies on results from analysis and properties of solutions of certain polynomial functions .",
    "another proof of existence of memoryless @xmath1-optimal strategies for reachability objectives follows from the result of  @xcite and the proof uses induction on the number of states of the game .",
    "we show the existence of memoryless @xmath1-optimal strategies for concurrent reachability games by more combinatorial and elementary means .",
    "our proof relies only on combinatorial techniques and on simple properties of markov decision processes @xcite . as our proof is more combinatorial ,",
    "we believe that the proof techniques will find future applications in game theory .",
    "our proof of the existence of memoryless @xmath1-optimal strategies , for all @xmath7 , is built upon a value - iteration scheme that converges to the value of the game @xcite .",
    "the value - iteration scheme computes a sequence @xmath14 of valuations , where for @xmath15 each valuation @xmath16 associates with each state @xmath17 of the game a lower bound @xmath18 on the value of the game , such that @xmath19 converges to the value of the game at @xmath17 .",
    "the convergence is monotonic from below , but no rate of convergence was known . from each valuation @xmath16 , we can extract a memoryless , randomized player-1 strategy , by considering the ( randomized ) choice of moves for player  1 that achieves the maximal one - step expectation of @xmath16 . in general ,",
    "a strategy @xmath20 obtained in this fashion is not guaranteed to achieve the value @xmath16 .",
    "we show that @xmath20 is guaranteed to achieve the value @xmath16 if it is _ proper _ , that is , if regardless of the strategy adopted by player  2 , the play reaches with probability  1 states that are either in the target , or that have no path leading to the target .",
    "next , we show how to extract from the sequence of valuations @xmath21 a sequence of memoryless randomized player-1 strategies @xmath22 that are guaranteed to be proper , and thus achieve the values @xmath14 .",
    "this proves the existence of memoryless @xmath1-optimal strategies for all @xmath7 .",
    "our proof is completely different as compared to the proof of  @xcite : the proof of  @xcite uses induction on the number of states , whereas our proof is based on the notion of ranking function obtained from the value - iteration algorithm .",
    "we then apply the techniques developed for the above proof to design a _ strategy - improvement _ algorithm for concurrent reachability games .",
    "strategy - improvement algorithms , also known as _ policy - iteration _ algorithms in the context of markov decision processes  @xcite , compute a sequence of memoryless strategies @xmath23 such that , for all @xmath24 , ( i )  the strategy @xmath25 is at all states no worse than @xmath26 ; ( ii )  if @xmath27 , then @xmath28 is optimal ; and ( iii )  for every @xmath7 , we can find a @xmath8 sufficiently large so that @xmath29 is @xmath1-optimal . computing a sequence of strategies @xmath22 on the basis the value - iteration scheme from above",
    "does not yield a strategy - improvement algorithm , as condition ( ii ) may be violated : there is no guarantee that a step in the value iteration leads to an improvement in the strategy .",
    "we will show that the key to obtain a strategy - improvement algorithm consists in recomputing , at each iteration , the values of the player-1 strategy to be improved , and in adopting a particular strategy - update rule , which ensures that all generated strategies are proper . unlike previous proofs of strategy - improvement algorithms for concurrent games @xcite , which rely on the analysis of discounted versions of the games , our analysis is again more combinatorial .",
    "hoffman - karp  @xcite presented a strategy improvement algorithm for the special case of concurrent games with ergodic property ( i.e. , from every state @xmath17 any other state @xmath30 can be guaranteed to reach with probability  1 ) ( also see algorithm for discounted games in  @xcite ) .",
    "observe that for concurrent reachability games , with the ergodic assumption the value at all states is trivially  1 , and thus the ergodic assumption gives us the trivial case .",
    "our results give a combinatorial strategy improvement algorithm for the whole class of concurrent reachability games .",
    "the results of  @xcite presents a strategy improvement algorithm for recursive concurrent games with termination criteria : the algorithm of  @xcite is more involved ( depends on properties of certain polynomial functions ) and works for the more general class of recursive concurrent games . differently from turn - based games @xcite , for concurrent games we can not guarantee the termination of the strategy - improvement algorithm .",
    "however , for turn - based stochastic games we present a detailed analysis of termination criteria . our analysis is based on bounds on the precision of values for turn - based stochastic games . as a consequence of our analysis",
    ", we obtain an improved upper bound for termination for turn - based stochastic games .    * our results for concurrent safety games . *",
    "we present for the first time a strategy - improvement scheme that approximates the value of a concurrent safety game _ from below_. together with the strategy improvement algorithm for reachability games , or the value - iteration scheme , to approximate the value of such a game from above , we obtain a termination criterion for computing the value of concurrent reachability and safety games within any given tolerance @xmath5 .",
    "this is the first termination criterion for an algorithm that approximates the value of a concurrent game .",
    "several difficulties had to be overcome in developing our scheme .",
    "first , while the strategy - improvement algorithm that approximates reachability values from below is based on locally improving a strategy on the basis of the valuation it yields , this approach does not suffice for approximating safety values from below : we would obtain an increasing sequence of values , but they would not necessarily converge to the value of the game ( see example  [ examp : conc - safety ] ) .",
    "rather , we introduce a novel , non - local improvement step , which augments the standard valuation - based improvement step .",
    "each non - local step involves the solution of an appropriately constructed turn - based game .",
    "the turn - based game constructed is polynomial in the state space of the original game , but _",
    "exponential _ in the number of actions .",
    "it is an interesting open question whether the turn - based game can be also made polynomial in the number of the actions .",
    "second , as value - iteration for safety objectives converges from above , while our sequences of strategies yield values that converge from below , the proof of convergence for our algorithm can not be derived from a connection with value - iteration , as was the case for reachability objectives .",
    "we had to develop new proof techniques both to show the monotonicity of the strategy values produced by our algorithm , and to show their convergence to the value of the game .",
    "* added value of our algorithms . *",
    "the new strategy improvement algorithms we present in this paper has two important contributions as compared to the classical value - iteration algorithms .    1 .   _",
    "termination for approximation . _",
    "the value - iteration algorithm for reachability games converges from below , and the value - iteration for safety games converges for above . hence given desired precision @xmath0 for approximation , there is no termination criteria to stop the value - iteration algorithm and guarantee @xmath1-approximation .",
    "the sequence of valuation of our strategy improvement algorithm for concurrent safety games converges from below , and along with the value - iteration or strategy improvement algorithm for concurrent reachability games we obtain the _ first _ termination criteria for @xmath1-approximation of values in concurrent reachability and safety games . using a result of  @xcite on the bound on @xmath8-uniform memoryless @xmath1-optimal strategies , for @xmath0 , we also obtain a bound on the number of iterations of the strategy improvement algorithms that guarantee @xmath1-approximation of the values .",
    "moreover a recent result of  @xcite provide a nearly tight double exponential upper and lower bound on the number of iterations required for @xmath1-approximation of the values .",
    "approximation of strategies .",
    "_ our strategy improvement algorithms are also the first approach to approximate memoryless @xmath1-optimal strategies in concurrent reachability and safety games .",
    "the witness strategy produced by the value - iteration algorithm for concurrent reachability games is not memoryless ; and for concurrent safety games since the value - iteration algorithm converges from above it does not provide any witness strategies .",
    "our strategy improvement algorithms for concurrent reachability and safety games yield sequence of memoryless strategies that ensure for convergence to the value of the game from below , and yield witness memoryless strategies to approximate the value of concurrent reachability and safety games .",
    "* notation . * for a countable set  @xmath31 , a _ probability distribution _ on @xmath31 is a function @xmath32 $ ] such that @xmath33 .",
    "we denote the set of probability distributions on @xmath31 by @xmath34 . given a distribution @xmath35 , we denote by @xmath36 the support set of @xmath37 .",
    "( concurrent games ) .",
    "a ( two - player ) _ concurrent game structure _ @xmath38 consists of the following components :    * a finite state space @xmath39 and a finite set @xmath40 of moves or actions . * two move assignments @xmath41 . for @xmath42 , assignment @xmath43 associates with each state @xmath44 a nonempty set @xmath45 of moves available to player @xmath46 at state @xmath17 . * a probabilistic transition function @xmath47 that gives the probability @xmath48 of a transition from @xmath17 to @xmath30 when player  1 chooses at state @xmath17 move @xmath49 and player  2 chooses move @xmath50 , for all @xmath51 and @xmath52 , @xmath53 .",
    "we denote by @xmath54 the size of transition function , i.e. , @xmath55 .",
    "we denote by @xmath56 the size of the game graph , and @xmath57 . at every state @xmath58 ,",
    "player  1 chooses a move @xmath59 , and simultaneously and independently player  2 chooses a move @xmath60 .",
    "the game then proceeds to the successor state @xmath30 with probability @xmath61 , for all @xmath62 .",
    "a state @xmath17 is an _ absorbing state _ if for all @xmath52 and @xmath53 , we have @xmath63 . in other words , at an absorbing state @xmath17 for all choices of moves of the two players , the successor state is always @xmath17 .",
    "( turn - based stochastic games ) . a _",
    "turn - based stochastic game graph _",
    "( _ @xmath64-player game graph _ ) @xmath65 consists of a finite directed graph @xmath66 , a partition @xmath67 , @xmath68 , @xmath69 of the finite set @xmath39 of states , and a probabilistic transition function @xmath37 : @xmath70 , where @xmath71 denotes the set of probability distributions over the state space  @xmath39 .",
    "the states in @xmath72 are the _",
    "player-@xmath73 _ states , where player  @xmath73 decides the successor state ; the states in @xmath68 are the _",
    "player-@xmath74 _ states , where player  @xmath74 decides the successor state ; and the states in @xmath75 are the _ random or probabilistic _ states , where the successor state is chosen according to the probabilistic transition function  @xmath37 .",
    "we assume that for @xmath76 and @xmath62 , we have @xmath77 iff @xmath78 , and we often write @xmath79 for @xmath80 . for technical convenience we assume that every state in the graph @xmath66 has at least one outgoing edge . for a state @xmath58",
    ", we write @xmath81 to denote the set @xmath82 of possible successors .",
    "we denote by @xmath54 the size of the transition function , i.e. , latexmath:[$|\\trans|=\\sum_{s\\in s_r , t\\in s }     bits required to specify the transition probability @xmath80 .",
    "we denote by @xmath56 the size of the game graph , and @xmath84 .    * plays . *",
    "a _ play _ @xmath85 of @xmath86 is an infinite sequence @xmath87 of states in @xmath39 such that for all @xmath88 , there are moves @xmath89 and @xmath90 with @xmath91 .",
    "we denote by @xmath92 the set of all plays , and by @xmath93 the set of all plays @xmath94 such that @xmath95 , that is , the set of plays starting from state  @xmath17 .    * selectors and strategies .",
    "* a _ selector _",
    "@xmath96 for player @xmath97 is a function @xmath98 such that for all states @xmath44 and moves @xmath99 , if @xmath100 , then @xmath101 .",
    "a selector @xmath96 for player @xmath46 at a state @xmath17 is a distribution over moves such that if @xmath102 , then @xmath101 . we denote by @xmath103 the set of all selectors for player  @xmath97 , and similarly , we denote by @xmath104 the set of all selectors for player  @xmath46 at a state @xmath17 .",
    "the selector @xmath96 is _ pure _ if for every state @xmath44 , there is a move @xmath99 such that @xmath105 .",
    "a _ strategy _ for player @xmath106 is a function @xmath107 that associates with every finite , nonempty sequence of states , representing the history of the play so far , a selector for player  @xmath46 ; that is , for all @xmath108 and @xmath44 , we have @xmath109 . the strategy @xmath110 is _ pure _ if it always chooses a pure selector ; that is , for all @xmath111 , there is a move @xmath99 such that @xmath112 . a _ memoryless _ strategy is independent of the history of the play and depends only on the current state .",
    "memoryless strategies correspond to selectors ; we write @xmath113 for the memoryless strategy consisting in playing forever the selector @xmath96 .",
    "a strategy is _ pure memoryless _ if it is both pure and memoryless . in a turn - based stochastic game ,",
    "a strategy for player  1 is a function @xmath114 , such that for all @xmath108 and for all @xmath115 we have @xmath116 .",
    "memoryless strategies and pure memoryless strategies are obtained as the restriction of strategies as in the case of concurrent game graphs .",
    "the family of strategies for player  2 are defined analogously .",
    "we denote by @xmath117 and @xmath118 the sets of all strategies for player @xmath73 and player @xmath74 , respectively .",
    "we denote by @xmath119 and @xmath120 the sets of memoryless strategies and pure memoryless strategies for player  @xmath46 , respectively .",
    "* destinations of moves and selectors .",
    "* for all states @xmath44 and moves @xmath52 and @xmath53 , we indicate by @xmath121 the set of possible successors of @xmath17 when the moves @xmath49 and @xmath50 are chosen . given a state @xmath17 , and selectors @xmath122 and @xmath123 for the two players , we denote by @xmath124 the set of possible successors of @xmath17 with respect to the selectors @xmath122 and @xmath123 .",
    "once a starting state @xmath17 and strategies @xmath125 and @xmath126 for the two players are fixed , the game is reduced to an ordinary stochastic process . hence , the probabilities of events are uniquely defined , where an _ event _ @xmath127 is a measurable set of plays . for an event @xmath127 ,",
    "we denote by @xmath128 the probability that a play belongs to @xmath129 when the game starts from @xmath17 and the players follows the strategies @xmath125 and  @xmath126 .",
    "similarly , for a measurable function @xmath130 , we denote by @xmath131 the expected value of @xmath132 when the game starts from @xmath17 and the players follow the strategies @xmath125 and  @xmath126 . for @xmath133 , we denote by @xmath134 the random variable denoting the @xmath46-th state along a play .    * valuations . *",
    "a _ valuation _ is a mapping @xmath135 $ ] associating a real number @xmath136 $ ] with each state @xmath17 . given two valuations @xmath137 , we write @xmath138 when @xmath139 for all states @xmath44 . for an event @xmath129 , we denote by @xmath140 the valuation @xmath141 $ ] defined for all states @xmath44 by @xmath142 . similarly , for a measurable function",
    "@xmath143 $ ] , we denote by @xmath144 the valuation @xmath141 $ ] defined for all @xmath44 by @xmath145 .    * the @xmath146 operator . * given a valuation @xmath147 , and two selectors @xmath148 and @xmath149 , we define the valuations @xmath150 , @xmath151 , and @xmath152 as follows , for all states @xmath44 : @xmath153 intuitively , @xmath154 is the greatest expectation of @xmath147 that player  1 can guarantee at a successor state of @xmath17 . also note that given a valuation @xmath147 , the computation of @xmath152 reduces to the solution of a zero - sum one - shot matrix game , and can be solved by linear programming .",
    "similarly , @xmath155 is the greatest expectation of @xmath147 that player  1 can guarantee at a successor state of @xmath17 by playing the selector @xmath122 .",
    "note that all of these operators on valuations are monotonic : for two valuations @xmath156 , if @xmath138 , then for all selectors @xmath148 and @xmath149 , we have @xmath157 , @xmath158 , and @xmath159 .",
    "* reachability and safety objectives . * given a set @xmath160 of _ safe _ states , the objective of a safety game consists in never leaving @xmath161 .",
    "therefore , we define the set of winning plays as the set @xmath162 . given a subset @xmath163 of _ target _ states , the objective of a reachability game consists in reaching @xmath164 .",
    "correspondingly , the set winning plays is @xmath165 of plays that visit @xmath164 . for all @xmath160 and @xmath163 , the sets",
    "@xmath166 and @xmath167 is measurable .",
    "an objective in general is a measurable set , and in this paper we consider only reachability and safety objectives . for an objective @xmath168 ,",
    "the probability of satisfying @xmath168 from a state @xmath44 under strategies @xmath125 and @xmath126 for players  1 and  2 , respectively , is @xmath169 .",
    "we define the _ value _ for player  1 of game with objective @xmath168 from the state @xmath44 as @xmath170 i.e. , the value is the maximal probability with which player  1 can guarantee the satisfaction of @xmath168 against all player  2 strategies .",
    "given a player-1 strategy @xmath125 , we use the notation @xmath171 a strategy @xmath125 for player  1 is _ optimal _ for an objective @xmath168 if for all states @xmath44 , we have @xmath172 for @xmath7 , a strategy @xmath125 for player  1 is _ @xmath1-optimal _ if for all states @xmath44 , we have @xmath173 the notion of values and optimal strategies for player  2 are defined analogously .",
    "reachability and safety objectives are dual , i.e. , we have @xmath174 . the quantitative determinacy result of",
    "@xcite ensures that for all states @xmath44 , we have @xmath175",
    "to develop our arguments , we need some facts about one - player versions of concurrent stochastic games , known as _ markov decision processes _ ( mdps ) @xcite . for @xmath97 , a _",
    "player-@xmath46 mdp _",
    "( for short , @xmath46-mdp ) is a concurrent game where , for all states @xmath44 , we have @xmath176 . given a concurrent game @xmath177 , if we fix a memoryless strategy corresponding to selector @xmath122 for player  1 , the game is equivalent to a 2-mdp @xmath178 with the transition function @xmath179 for all @xmath44 and @xmath53 .",
    "similarly , if we fix selectors @xmath122 and @xmath123 for both players in a concurrent game @xmath177 , we obtain a markov chain , which we denote by @xmath180",
    ".    * end components . * in an mdp , the sets of states that play an equivalent role to the closed recurrent classes of markov chains ( * ? ? ?",
    "* chapter  4 ) are called `` end components '' @xcite .",
    "( end components ) .",
    "an _ end component _ of an @xmath46-mdp @xmath177 , for @xmath106 , is a subset @xmath181 of the states such that there is a selector @xmath96 for player  @xmath46 so that @xmath182 is a closed recurrent class of the markov chain @xmath183 .",
    "it is not difficult to see that an equivalent characterization of an end component @xmath182 is the following . for each state @xmath184",
    ", there is a subset @xmath185 of moves such that :    1 .   _",
    "( closed ) _ if a move in @xmath186 is chosen by player @xmath46 at state @xmath17 , then all successor states that are obtained with nonzero probability lie in @xmath182 ; and 2 .   _ ( recurrent ) _ the graph @xmath187 , where @xmath188 consists of the transitions that occur with nonzero probability when moves in @xmath189 are chosen by player @xmath46 , is strongly connected .",
    "given a play @xmath190 , we denote by @xmath191 the set of states that occurs infinitely often along @xmath85 . given a set @xmath192 of subsets of states , we denote by @xmath193 the event @xmath194 .",
    "the following theorem states that in a 2-mdp , for every strategy of player  2 , the set of states that are visited infinitely often is , with probability  1 , an end component .",
    "corollary  [ coro : prob1 ] follows easily from theorem  [ theo - ec ] .",
    "( @xcite ) .",
    "[ theo - ec ] for a player-1 selector @xmath122 , let @xmath195 be the set of end components of a 2-mdp @xmath178 . for all player-2 strategies",
    "@xmath126 and all states @xmath44 , we have @xmath196 .",
    "[ coro : prob1 ] for a player-1 selector @xmath122 , let @xmath195 be the set of end components of a 2-mdp @xmath178 , and let @xmath197 be the set of states of all end components .",
    "for all player-2 strategies @xmath126 and all states @xmath44 , we have @xmath198 .",
    "[ [ subsec : mdpreach ] ] mdps with reachability objectives .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    given a 2-mdp with a reachability objective @xmath167 for player  2 , where @xmath199 , the values can be obtained as the solution of a linear program  @xcite ( see section 2.9 of  @xcite where linear program solution is given for mdps with limit - average objectives and reachability objective is a special case of limit - average objectives ) .",
    "the linear program has a variable @xmath200 for all states @xmath44 , and the objective function and the constraints are as follows : @xmath201 @xmath202 the correctness of the above linear program to compute the values follows from  @xcite ( see section 2.9 of  @xcite , and also see  @xcite for the correctness of the linear program ) .",
    "in this section we present an elementary and combinatorial proof of the existence of memoryless @xmath1-optimal strategies for concurrent reachability games , for all @xmath7 ( optimal strategies need not exist for concurrent games with reachability objectives  @xcite ) .",
    "consider a reachability game with target @xmath163 , i.e. , objective for player  1 is @xmath167 .",
    "let @xmath203 be the set of states from which player  1 can not reach the target with positive probability . from @xcite , we know that this set can be computed as @xmath204 , where @xmath205 , and for all @xmath24 , @xmath206 the limit is reached in at most @xmath207 iterations .",
    "note that player  2 has a strategy that confines the game to @xmath208 , and that consequently all strategies are optimal for player  1 , as they realize the value  0 of the game in @xmath208 .",
    "therefore , without loss of generality , in the remainder we assume that all states in @xmath208 and @xmath164 are absorbing .",
    "our first step towards proving the existence of memoryless @xmath1-optimal strategies for reachability games consists in considering a value - iteration scheme for the computation of @xmath209 .",
    "let @xmath210 : s \\to [ 0,1]$ ] be the indicator function of @xmath164 , defined by @xmath210(s ) = 1 $ ] for @xmath211 , and @xmath210(s ) = 0 $ ] for @xmath212 .",
    "let @xmath213 $ ] , and for all @xmath88 , let @xmath214 note that the classical equation assigns",
    "@xmath215 \\lor pre_1(u_k)$ ] , where @xmath216 is interpreted as the maximum in pointwise fashion . since we assume that all states in @xmath164 are absorbing , the classical equation reduces to the simpler equation given by ( [ eq - valiter ] ) . from the monotonicity of @xmath217",
    "it follows that @xmath218 , that is , @xmath219 , for all @xmath24 .",
    "the result of @xcite establishes by a combinatorial argument that @xmath220 , where the limit is interpreted in pointwise fashion . for all @xmath24 ,",
    "let the player-1 selector @xmath221 be a _ value - optimal _",
    "selector for @xmath222 , that is , a selector such that @xmath223 .",
    "an @xmath1-optimal strategy @xmath224 for player  1 can be constructed by applying the sequence @xmath225 of selectors , where the last selector , @xmath226 , is repeated forever .",
    "it is possible to prove by induction on @xmath8 that @xmath227 .\\ , \\randpath_j \\in t ) \\geq u_k.\\ ] ] as the strategies @xmath224 , for @xmath24 , are not necessarily memoryless , this proof does not suffice for showing the existence of memoryless @xmath1-optimal strategies .",
    "on the other hand , the following example shows that the memoryless strategy @xmath228 does not necessarily guarantee the value @xmath222 .",
    "[ examp - nonopt ] consider the @xmath73-mdp shown in fig  [ fig1 ] . at all states except @xmath229 ,",
    "the set of available moves for player  1 is a singleton set . at @xmath229 ,",
    "the available moves for player  1 are @xmath230 and @xmath231 .",
    "the transitions at the various states are shown in the figure .",
    "the objective of player  1 is to reach the state @xmath232 .",
    "we consider the value - iteration procedure and denote by @xmath222 the valuation after @xmath8 iterations .",
    "writing a valuation @xmath233 as the list of values @xmath234 , we have : @xmath235 the valuation @xmath236 is thus a fixpoint .",
    "now consider the selector @xmath122 for player  1 that chooses at state @xmath229 the move @xmath230 with probability  1 .",
    "the selector @xmath122 is optimal with respect to the valuation @xmath236 .",
    "however , if player  1 follows the memoryless strategy @xmath237 , then the play visits @xmath229 and @xmath238 alternately and reaches @xmath232 with probability  0 .",
    "thus , @xmath122 is an example of a selector that is value - optimal , but not optimal .    on the other hand , consider any selector @xmath239 for player  1 that chooses move @xmath231 at state @xmath229 with positive probability . under the memoryless strategy @xmath240",
    ", the set @xmath241 of states is reached with probability  1 , and @xmath232 is reached with probability @xmath242 .",
    "such a @xmath239 is thus an example of a selector that is both value - optimal and optimal .",
    "( 48,28)(15,-5 ) ( n0)(40,12)@xmath243 ( n1)(70,0)@xmath232 ( n2)(70,24)@xmath244    ( n3)(20,12)@xmath229 ( n4)(0,12)@xmath238    ( n1 ) ( n2 ) ( n0,n1)@xmath245 ( n0,n2)@xmath245 ( n3,n0)@xmath231 ( n3,n4)@xmath230 ( n4,n3 )    in the example , the problem is that the strategy @xmath246 may cause player  1 to stay forever in @xmath247 with positive probability .",
    "we call `` proper '' the strategies of player  1 that guarantee reaching @xmath248 with probability  1 .",
    "( proper strategies and selectors ) . a player-1 strategy @xmath125 is _ proper _ if for all player-2 strategies @xmath126 , and for all states @xmath249 , we have @xmath250 . a player-1 selector @xmath122 is _ proper _ if the memoryless player-1 strategy @xmath246 is proper .",
    "we note that proper strategies are closely related to condon s notion of a _ halting game _",
    "@xcite : precisely , a game is halting iff all player-1 strategies are proper .",
    "we can check whether a selector for player  1 is proper by considering only the pure selectors for player  2 .",
    "given a selector @xmath122 for player  1 , the memoryless player-1 strategy @xmath246 is proper iff for every pure selector @xmath123 for player  2 , and for all states @xmath44 , we have @xmath251 .",
    "we prove the contrapositive .",
    "given a player-1 selector @xmath122 , consider the 2-mdp @xmath178 . if @xmath246 is not proper , then by theorem  [ theo - ec ] , there must exist an end component @xmath252 in @xmath178 . then , from @xmath182 , player  2 can avoid reaching @xmath253 by repeatedly applying a pure selector @xmath123 that at every state @xmath184 deterministically chooses a move @xmath53 such that @xmath254 .",
    "the existence of a suitable @xmath255 for all states @xmath184 follows from the definition of end component .",
    "the following lemma shows that the selector that chooses all available moves uniformly at random is proper .",
    "this fact will be used later to initialize our strategy - improvement algorithm .",
    "[ lemm : proper2 ] let @xmath256 be the player-1 selector that at all states @xmath249 chooses all moves in @xmath257 uniformly at random",
    ". then @xmath256 is proper .",
    "assume towards contradiction that @xmath256 is not proper . from theorem  [ theo - ec ] ,",
    "in the 2-mdp @xmath258 there must be an end component @xmath252 .",
    "then , when player  1 follows the strategy @xmath259 , player  2 can confine the game to  @xmath182 . by the definition of @xmath256 ,",
    "player  2 can ensure that the game does not leave @xmath182 regardless of the moves chosen by player  1 , and thus , for _ all _ strategies of player  1 .",
    "this contradicts the fact that @xmath208 contains all states from which player  2 can ensure that @xmath164 is not reached .",
    "the following lemma shows that if the player-1 selector @xmath221 computed by the value - iteration scheme ( [ eq - valiter ] ) is proper , then the player-1 strategy @xmath228 guarantees the value @xmath222 , for all @xmath88 .",
    "[ lem - selector ] let @xmath147 be a valuation such that @xmath260 and @xmath261 for all states @xmath262 .",
    "let @xmath122 be a selector for player  1 such that @xmath263 .",
    "if @xmath122 is proper , then for all player-2 strategies @xmath126 , we have @xmath264 .",
    "consider an arbitrary player-2 strategy @xmath126 , and for @xmath24 , let @xmath265 be the expected value of @xmath147 after @xmath8 steps under @xmath246 and @xmath126 .",
    "by induction on @xmath8 , we can prove @xmath266 for all @xmath24 .",
    "in fact , @xmath267 , and for @xmath24 , we have @xmath268 for all @xmath24 and @xmath44 , we can write @xmath269 as @xmath270 since @xmath271 when @xmath211 , the first term on the right - hand side is at most @xmath272 . for the second term",
    ", we have @xmath273 by hypothesis , because @xmath274 and every state @xmath275 is absorbing . finally , the third term on the right hand side is  0 , as @xmath261 for all states @xmath262 .",
    "hence , taking the limit with @xmath276 , we obtain @xmath277 where the last inequality follows from @xmath266 for all @xmath24 .",
    "note that @xmath278 , and since @xmath164 is absorbing it follows that @xmath269 is non - deccreasing ( monotonic ) and is bounded by  1 ( since it is a probability measure ) .",
    "hence the limit of @xmath269 is defined .",
    "the desired result follows .      in this section",
    "we show how to obtain memoryless @xmath1-optimal strategies from the value - iteration scheme , for @xmath0 . in the following section the existence such strategies",
    "would be established using a strategy - iteration scheme .",
    "the strategy - iteration scheme has been used previously to establish existence of memoryless @xmath1-optimal strategies , for @xmath0 ( for example see  @xcite and also results of condon  @xcite for turn - based games ) .",
    "however our proof which constructs the memoryless strategies based on value - iteration scheme is new .",
    "considering again the value - iteration scheme ( [ eq - valiter ] ) , since @xmath279 , for every @xmath0 there is a @xmath8 such that @xmath280 at all states @xmath44 .",
    "lemma  [ lem - selector ] indicates that , in order to construct a memoryless @xmath1-optimal strategy , we need to construct from @xmath281 a player-1 selector @xmath122 such that :    1 .",
    "@xmath122 is value - optimal for @xmath281 , that is , @xmath282 ; and 2 .",
    "@xmath122 is proper .    to ensure the construction of a value - optimal , proper selector , we need some definitions . for @xmath283 ,",
    "the _ value class _ @xmath284 consists of the states with value @xmath285 under the valuation @xmath286 .",
    "similarly we define @xmath287 , for @xmath288 . for a state @xmath44 ,",
    "let @xmath289 be the _ entry time _ of @xmath17 in @xmath290 , that is , the least iteration @xmath291 in which the state @xmath17 has the same value as in iteration @xmath8 . for @xmath24 , we define the player-1 selector @xmath292 as follows : if @xmath293 , then @xmath294 otherwise , if @xmath295 , then @xmath296 ( this definition is arbitrary , and it does not affect the remainder of the proof ) . in words , the selector @xmath297 is an optimal selector for @xmath17 at the iteration @xmath298 .",
    "it follows easily that @xmath299 , that is , @xmath292 is also value - optimal for @xmath281 , satisfying the first of the above conditions .    to conclude the construction , we need to prove that for @xmath8 sufficiently large ( namely , for @xmath8 such that @xmath300 at all states @xmath301 ) , the selector @xmath292 is proper",
    "to this end we use theorem  [ theo - ec ] , and show that for sufficiently large @xmath8 no end component of @xmath302 is entirely contained in @xmath303 .",
    ", even though our proof , for the sake of a simpler argument , does not show it .",
    "] to reason about the end components of @xmath302 , for a state @xmath44 and a player-2 move @xmath53 , we write @xmath304 for the set of possible successors of state @xmath17 when player  1 follows the strategy @xmath305 , and player  2 chooses the move @xmath50 .    [ lem-1 ] let @xmath306 and @xmath24 , and consider a state @xmath307 such that @xmath308",
    ". for all moves @xmath53 , we have :    1 .",
    "either @xmath309 , 2 .   or @xmath310 , and there is a state @xmath311 with @xmath312 .    for convenience ,",
    "let @xmath313 , and consider any move @xmath53 .",
    "* consider first the case that @xmath314 .",
    "then , it can not be that @xmath315 ; otherwise , for all states @xmath311 , we would have @xmath316 , and there would be at least one state @xmath311 such that @xmath317 , contradicting @xmath318 and @xmath319 .",
    "so , it must be that @xmath320 .",
    "* consider now the case that @xmath310 .",
    "since @xmath321 , due to the monotonicity of the @xmath217 operator and ( [ eq - valiter ] ) , we have that @xmath322 for all states @xmath311 . from @xmath323",
    ", it follows that @xmath324 for all states @xmath311 , implying that @xmath325 for all states @xmath311 .",
    "the above lemma states that under @xmath292 , from each state @xmath326 with @xmath327 we are guaranteed a probability bounded away from  0 of either moving to a higher - value class @xmath328 , or of moving to states within the value class that have a strictly lower entry time .",
    "note that the states in the target set @xmath164 are all in @xmath329 : they have entry - time  0 in the value class for value  1 .",
    "this implies that every state in @xmath330 has a probability bounded above zero of reaching @xmath164 in at most @xmath331 steps , so that the probability of staying forever in @xmath247 is  0 . to prove this fact formally",
    ", we analyze the end components of @xmath302 in light of lemma  [ lem-1 ] .",
    "[ lem-2 ] for all @xmath24 , if for all states @xmath332 we have @xmath333 , then for all player-2 strategies @xmath126 , we have @xmath334 .    since every state @xmath275 is absorbing , to prove this result , in view of corollary  [ coro : prob1 ] , it suffices to show that no end component of @xmath302 is entirely contained in @xmath247 . towards the contradiction ,",
    "assume there is such an end component @xmath335 .",
    "then , we have @xmath336}$ ] with @xmath337 , for some @xmath338 , where @xmath339 } = u^k_{\\geq r_1 } \\inters u^k_{\\leq r_2}$ ] is the union of the value classes for all values in the interval @xmath340 $ ] .",
    "consider a state @xmath341 with minimal @xmath342 , that is , such that @xmath343 for all other states @xmath344 . from lemma  [ lem-1 ]",
    ", it follows that for every move @xmath53 , there is a state @xmath311 such that ( i )  either @xmath344 and @xmath312 , ( ii )  or @xmath345 . in both cases ,",
    "we obtain a contradiction .",
    "the above lemma shows that @xmath292 satisfies both requirements for optimal selectors spelt out at the beginning of section  [ sec - optisel ] .",
    "hence , @xmath292 guarantees the value @xmath222 .",
    "this proves the existence of memoryless @xmath1-optimal strategies for concurrent reachability games .",
    "( memoryless @xmath1-optimal strategies ) . for every @xmath0 ,",
    "memoryless @xmath1-optimal strategies exist for all concurrent games with reachability objectives .",
    "consider a concurrent reachability game with target @xmath163 . since @xmath346 , for every @xmath7 we can find @xmath347 such that the following two assertions hold : @xmath348 by construction , @xmath349",
    "hence , from lemma  [ lem - selector ] and lemma  [ lem-2 ] , for all player-2 strategies @xmath126 , we have @xmath350 , leading to the result .",
    "in the previous section , we provided a proof of the existence of memoryless @xmath1-optimal strategies for all @xmath7 , on the basis of a value - iteration scheme . in this section",
    "we present a strategy - improvement algorithm for concurrent games with reachability objectives .",
    "the algorithm will produce a sequence of selectors @xmath351 for player 1 , such that :    1 .",
    "[ l - improve-1 ] for all @xmath133 , we have @xmath352 ; 2 .",
    "[ l - improve-3 ] if there is @xmath133 such that @xmath353 , then @xmath354 ; and 3 .",
    "[ l - improve-2 ] @xmath355 .",
    "condition  [ l - improve-1 ] guarantees that the algorithm computes a sequence of monotonically improving selectors .",
    "condition  [ l - improve-3 ] guarantees that if a selector can not be improved , then it is optimal .",
    "condition  [ l - improve-2 ] guarantees that the value guaranteed by the selectors converges to the value of the game , or equivalently , that for all @xmath7 , there is a number @xmath46 of iterations such that the memoryless player-1 strategy @xmath356 is @xmath1-optimal .",
    "note that for concurrent reachability games , there may be no @xmath357 such that @xmath353 , that is , the algorithm may fail to generate an optimal selector .",
    "this is because there are concurrent reachability games that do not admit optimal strategies , but only @xmath1-optimal strategies for all @xmath7 @xcite . for _ turn - based _ reachability games , our algorithm terminates with an optimal selector and we will present bounds for termination .",
    "we note that the value - iteration scheme of the previous section does not directly yield a strategy - improvement algorithm .",
    "in fact , the sequence of player-1 selectors @xmath358 computed in section  [ sec - valitersel ] may violate condition  [ l - improve-3 ] : it is possible that for some @xmath133 we have @xmath359 , but @xmath360 for some @xmath361 .",
    "this is because the scheme of section  [ sec - valitersel ] is fundamentally a value - iteration scheme , even though a selector is extracted from each valuation .",
    "the scheme guarantees that the valuations @xmath14 defined as in ( [ eq - valiter ] ) converge , but it does not guarantee that the selectors @xmath358 improve at each iteration .",
    "the strategy - improvement algorithm presented here shares an important connection with the proof of the existence of memoryless @xmath1-optimal strategies presented in the previous section . here , also , the key is to ensure that all generated selectors are proper .",
    "again , this is ensured by modifying the selectors , at each iteration , only where they can be improved .",
    "* ordering of strategies .",
    "* we let @xmath208 be as in section  [ sec - valitersel ] , and again we assume without loss of generality that all states in @xmath362 are absorbing .",
    "we define a preorder @xmath363 on the strategies for player 1 as follows : given two player 1 strategies @xmath125 and @xmath364 , let @xmath365",
    "if the following two conditions hold : ( i )  @xmath366 ; and ( ii )  @xmath367 for some state @xmath58 .",
    "furthermore , we write @xmath368 if either @xmath365 or @xmath369 .",
    "* informal description of algorithm  [ algorithm : strategy - improve ] . *",
    "we now present the strategy - improvement algorithm ( algorithm  [ algorithm : strategy - improve ] ) for computing the values for all states in @xmath370 .",
    "the algorithm iteratively improves player-1 strategies according to the preorder @xmath363 .",
    "the algorithm starts with the random selector @xmath371 . at iteration @xmath372",
    ", the algorithm considers the memoryless player-1 strategy @xmath373 and computes the value @xmath374 .",
    "observe that since @xmath373 is a memoryless strategy , the computation of @xmath374 involves solving the 2-mdp @xmath375 .",
    "the valuation @xmath374 is named @xmath376 . for all states @xmath17 such that @xmath377 , the memoryless strategy at @xmath17 is modified to a selector that is value - optimal for @xmath376 .",
    "the algorithm then proceeds to the next iteration .",
    "if @xmath378 , the algorithm stops and returns the optimal memoryless strategy @xmath373 for player  1 . unlike strategy - improvement algorithms for turn - based games ( see @xcite for a survey ) , algorithm  [ algorithm : strategy - improve ] is not guaranteed to terminate , because the value of a reachability game may not be rational .",
    "aaa = aaa = aaa = aaa = aaa = aaa = aaa = aaa + a concurrent game structure @xmath177 with target set @xmath164 . + a strategy @xmath379 for player  1 .",
    "compute @xmath203 .",
    "let @xmath380 and @xmath381 .",
    "compute @xmath382 .",
    "do \\ { *   + 3.1 .",
    "let @xmath383 .",
    "let @xmath122 be a player-1 selector such that for all states @xmath384 , + we have @xmath385 .",
    "the player-1 selector @xmath386 is defined as follows : for each state @xmath58 , let + @xmath387 +   + 3.4 .",
    "compute @xmath388 .",
    "let @xmath389 .",
    "+ @xmath390 .",
    "* return * @xmath391 .",
    "[ lemm : proper3 ] let @xmath392 and @xmath386 be the player-1 selectors obtained at iterations @xmath46 and @xmath372 of algorithm  [ algorithm : strategy - improve ] . if @xmath392 is proper , then @xmath386 is also proper .",
    "assume towards a contradiction that @xmath392 is proper and @xmath386 is not .",
    "let @xmath123 be a pure selector for player  2 to witness that @xmath386 is not proper .",
    "then there exist a subset @xmath393 such that @xmath182 is a closed recurrent set of states in the markov chain @xmath394 .",
    "let @xmath395 be the nonempty set of states where the selector is modified to obtain @xmath386 from @xmath392 ; at all other states @xmath392 and @xmath386 agree .",
    "since @xmath392 and @xmath386 agree at all states other than the states in @xmath395 , and @xmath392 is a proper strategy , it follows that @xmath396 .",
    "let @xmath397 be the value class with value @xmath285 at iteration @xmath46 .",
    "for a state @xmath398 the following assertion holds : if @xmath399 , then @xmath400 . let @xmath401 , that is , @xmath402 is the greatest value class at iteration @xmath46 with a nonempty intersection with the closed recurrent set @xmath182 .",
    "it easily follows that @xmath403 . consider any state @xmath384 , and let @xmath404 .",
    "since @xmath377 , it follows that @xmath405 .",
    "hence we must have @xmath406 , and therefore @xmath407 .",
    "thus , for all states @xmath408 , we have @xmath409 .",
    "recall that @xmath410 is the greatest value class at iteration @xmath46 with a nonempty intersection with @xmath182 ; hence @xmath411 .",
    "thus for all states @xmath412 , we have @xmath413 .",
    "it follows that @xmath414 .",
    "however , this gives us three statements that together form a contradiction : @xmath396 ( or else @xmath392 would not have been proper ) , @xmath407 , and @xmath415 .",
    "[ lemm : proper4 ] for all @xmath416 , the player-1 selector @xmath392 obtained at iteration @xmath46 of algorithm  [ algorithm : strategy - improve ] is proper .",
    "by lemma  [ lemm : proper2 ] we have that @xmath417 is proper .",
    "the result then follows from lemma  [ lemm : proper3 ] and induction .",
    "[ lemm : stra - improve ] let @xmath392 and @xmath386 be the player-1 selectors obtained at iterations @xmath46 and @xmath372 of algorithm  [ algorithm : strategy - improve ] .",
    "let @xmath418 .",
    "let @xmath419 and @xmath420 .",
    "then @xmath421 for all states @xmath58 ; and therefore @xmath422 for all states @xmath58 , and @xmath423 for all states @xmath424 .",
    "consider the valuations @xmath376 and @xmath425 obtained at iterations @xmath46 and @xmath372 , respectively , and let @xmath426 be the valuation defined by @xmath427 for all states @xmath44 . since @xmath386 is proper ( by lemma  [ lemm : proper4 ] ) , it follows that the counter - optimal strategy for player  2 to minimize @xmath425 is obtained by maximizing the probability to reach @xmath208 .",
    "in fact , there are no end components in @xmath428 in the 2-mdp @xmath429 .",
    "let @xmath430 in other words , @xmath431 , and we also have @xmath432 .",
    "we now show that @xmath433 is a feasible solution to the linear program for mdps with the objective @xmath434 , as described in section  [ sec : mdp ] .",
    "since @xmath435 , it follows that for all states @xmath58 and all moves @xmath53 , we have @xmath436 for all states @xmath437 , we have @xmath409 and @xmath438 , and since @xmath432 , it follows that for all states @xmath437 and all moves @xmath53 , we have @xmath439    since for @xmath384 the selector @xmath440 is obtained as an optimal selector for @xmath441 , it follows that for all states @xmath424 and all moves @xmath53 , we have @xmath442 in other words , @xmath443 .",
    "hence for all states @xmath424 and all moves @xmath53 , we have @xmath444 since @xmath432 , for all states @xmath424 and all moves @xmath53 , we have @xmath445 hence it follows that @xmath433 is a feasible solution to the linear program for mdps with reachability objectives .",
    "since the reachability valuation for player  2 for @xmath434 is the least solution ( observe that the objective function of the linear program is a minimizing function ) , it follows that @xmath446 .",
    "thus we obtain @xmath422 for all states @xmath44 , and @xmath423 for all states @xmath384 .",
    "( strategy improvement).[thrm : stra - improve ] the following two assertions hold about algorithm  [ algorithm : strategy - improve ] :    1 .   for all @xmath133",
    ", we have @xmath447 ; moreover , if @xmath448 , then @xmath373 is an optimal strategy .",
    "2 .   @xmath449 .",
    "we prove the two parts as follows .    1 .",
    "the assertion that @xmath450 follows from lemma  [ lemm : stra - improve ] .",
    "if @xmath451 , then @xmath452 .",
    "let @xmath453 , and since @xmath147 is the least solution to satisfy @xmath454 ( i.e. , the least fixpoint )  @xcite , it follows that @xmath455 . from lemma  [ lemm : proper4 ] it follows that @xmath373 is proper .",
    "since @xmath373 is proper by lemma  [ lem - selector ] , we have @xmath456 .",
    "it follows that @xmath373 is optimal for player  1 .",
    "let @xmath457 $ ] and @xmath213 $ ] .",
    "we have @xmath458 . for all @xmath459 , by lemma  [ lemm : stra - improve ] , we have @xmath460 \\lor \\pre_1(v_k)$ ] . for all @xmath24 , let @xmath461 \\lor \\pre_1(u_k)$ ] . by induction",
    "we conclude that for all @xmath24 , we have @xmath462 .",
    "moreover , @xmath463 , that is , for all @xmath459 , we have @xmath464 since @xmath346 , it follows that @xmath465    the theorem follows .",
    "if the input game structure to algorithm  [ algorithm : strategy - improve ] is a turn - based stochastic game structure , then if we start with a proper selector @xmath417 that is pure , then for all @xmath133 we can choose the selector @xmath392 such that @xmath392 is both proper and pure : the above claim follows since given a valuation @xmath147 , if a state @xmath17 is a player  1 state , then there is an action @xmath230 at @xmath17 ( or choice of an edge at @xmath17 ) that achieves @xmath154 at @xmath17 .",
    "since the number of pure selectors is bounded , if we start with a pure , proper selector then termination is ensured .",
    "hence we present a procedure to compute a pure , proper selector , and then present termination bounds ( i.e. , bounds on @xmath46 such that @xmath466 ) . the construction of a pure",
    ", proper selector is based on the notion of _ attractors _ defined below .",
    "_ attractor strategy .",
    "_ let @xmath467 , and for @xmath416 we have @xmath468 since for all @xmath469 we have @xmath470 , it follows that from all states in @xmath471 player  1 can ensure that @xmath164 is reached with positive probability .",
    "it follows that for some @xmath133 we have @xmath472 .",
    "the pure _ attractor _",
    "selector @xmath473 is as follows : for a state @xmath474 we have @xmath475 , where @xmath476 ( such a @xmath30 exists by construction ) .",
    "the pure memoryless strategy @xmath477 ensures that for all @xmath416 , from @xmath478 the game reaches @xmath479 with positive probability . hence there is no end - component @xmath182 contained in @xmath480 in the mdp @xmath481 .",
    "it follows that @xmath473 is a pure selector that is proper , and the selector @xmath473 can be computed in @xmath482 time .",
    "we now present the termination bounds .",
    "_ termination bounds . _",
    "we present termination bounds for binary turn - based stochastic games .",
    "a turn - based stochastic game is binary if for all @xmath483 we have @xmath484 , and for all @xmath76 if @xmath485 , then for all @xmath486 we have @xmath487 , i.e. , for all probabilistic states there are at most two successors and the transition function @xmath37 is uniform .    [ lemm : mc - bound ] let @xmath177 be a binary markov chain with @xmath207 states with a reachability objective @xmath167 .",
    "then for all @xmath44 we have @xmath488 , with @xmath489 and @xmath490 .",
    "the results follow as a special case of lemma  2 of  @xcite .",
    "lemma  2 of  @xcite holds for halting turn - based stochastic games , and since markov chains reaches the set of closed connected recurrent states with probability  1 from all states the result follows .",
    "[ lemm : tb - bound ] let @xmath177 be a binary turn - based stochastic game with a reachability objective @xmath167 . then for all @xmath44 we have @xmath488 , with @xmath489 and @xmath491 .",
    "since pure memoryless optimal strategies exist for both players ( existence of pure memoryless optimal strategies for both players in turn - based stochastic reachability games follows from  @xcite ) , we fix pure memoryless optimal strategies @xmath125 and @xmath126 for both players .",
    "the markov chain @xmath492 can be then reduced to an equivalent markov chains with @xmath493 states ( since we fix deterministic successors for states in @xmath494 , they can be collapsed to their successors ) .",
    "the result then follows from lemma  [ lemm : mc - bound ] .    from lemma  [ lemm : tb - bound ]",
    "it follows that at iteration  @xmath46 of the reachability strategy improvement algorithm either the sum of the values either increases by @xmath495 or else there is a valuation @xmath16 such that @xmath466 .",
    "since the sum of values of all states can be at most @xmath207 , it follows that algorithm terminates in at most @xmath496 iterations .",
    "moreover , since the number of pure memoryless strategies is at most @xmath497 , the algorithm terminates in at most @xmath497 iterations .",
    "it follows from the results of  @xcite that a turn - based stochastic game structure @xmath177 can be reduced to a equivalent binary turn - based stochastic game structure @xmath498 such that the set of player  1 and player  2 states in @xmath177 and @xmath498 are the same and the number of probabilistic states in @xmath498 is @xmath499 , where @xmath54 is the size of the transition function in @xmath177 .",
    "thus we obtain the following result .",
    "let @xmath177 be a turn - based stochastic game with a reachability objective @xmath167 , then the reachability strategy improvement algorithm computes the values in time @xmath500 where @xmath501 is polynomial function .",
    "the results of  @xcite presented an algorithm for turn - based stochastic games that works in time @xmath502 .",
    "the algorithm of  @xcite works only for turn - based stochastic games , for general turn - based stochastic games the complexity of the algorithm of  @xcite is better .",
    "however , for turn - based stochastic games where the transition function at all states can be expressed with constantly many bits we have @xmath503 . in these cases",
    "the reachability strategy improvement algorithm ( that works for both concurrent and turn - based stochastic games ) works in time @xmath504 as compared to the time @xmath505 of the algorithm of  @xcite .",
    "a proof of the existence of memoryless optimal strategies for safety games can be found in @xcite : the proof uses results on martingales to obtain the result . for sake of completeness",
    "we present ( an alternative ) proof of the result : the proof we present is similar in spirit with the other proofs in this paper and uses the results on mdps to obtain the result .",
    "the proof is very similar to the proof presented in  @xcite .",
    "( memoryless optimal strategies).[thrm : memory - safe ] memoryless optimal strategies exist for all concurrent games with safety objectives .",
    "consider a concurrent game structure @xmath177 with an safety objective @xmath166 for player  1 .",
    "then it follows from the results of  @xcite that @xmath506 , \\pre_1(x ) } \\big),\\ ] ] where @xmath507 $ ] is the indicator function of the set @xmath161 and @xmath508 denotes the greatest fixpoint .",
    "let @xmath509 , and for all states @xmath211 we have @xmath510 , and hence any memoryless strategy from @xmath164 is an optimal strategy .",
    "thus without loss of generality we assume all states in @xmath164 are absorbing .",
    "let @xmath511 , and since we assume all states in @xmath164 are absorbing it follows that @xmath512 ( since @xmath147 is a fixpoint ) .",
    "let @xmath513 be a player  1 selector such that for all states @xmath17 we have @xmath514 .",
    "we show that @xmath515 is an memoryless optimal strategy .",
    "consider the player-2 mdp @xmath516 and we consider the maximal probability for player  2 to reach the target set @xmath164 .",
    "consider the valuation @xmath517 defined as @xmath518 .",
    "for all states @xmath211 we have @xmath519 . since @xmath520 it follows that for all states @xmath521 and all @xmath53 we have @xmath522 in other words , for all @xmath521 we have @xmath523 .",
    "hence for all states @xmath524 and all moves @xmath53 , we have @xmath525 hence it follows that @xmath517 is a feasible solution to the linear program for mdps with reachability objectives , i.e. , given the memoryless strategy @xmath515 for player  1 the maximal probability valuation for player  2 to reach @xmath164 is at most @xmath517 .",
    "hence the memoryless strategy @xmath515 ensures that the probability valaution for player  1 to stay safe in @xmath161 against all player  2 strategies is at least @xmath511 .",
    "optimality of @xmath515 follows .",
    "in this section we present a strategy improvement algorithm for concurrent games with safety objectives . we consider a concurrent game structure with a safe set @xmath161 , i.e. , the objective for player  1 is @xmath166 .",
    "the algorithm will produce a sequence of selectors @xmath351 for player 1 , such that condition  [ l - improve-1 ] , condition  [ l - improve-3 ] and condition  [ l - improve-2 ] of section  [ sec - stra - improve - reach ] are satisfied .",
    "note that for concurrent safety games , there may be no @xmath357 such that @xmath353 , that is , the algorithm may fail to generate an optimal selector , as the value can be irrational  @xcite .",
    "we start with a few notations    * optimal selectors . * given a valuation @xmath147 and a state @xmath17 , we define by @xmath526 the set of optimal selectors for @xmath147 at state @xmath17 . for an optimal selector @xmath527",
    ", we define the set of counter - optimal actions as follows : @xmath528 observe that for @xmath527 , for all @xmath529 we have @xmath530 .",
    "we define the set of optimal selector support and the counter - optimal action set as follows : @xmath531 i.e. , it consists of pairs @xmath532 of actions of player  1 and player  2 , such that there is an optimal selector @xmath122 with support @xmath31 , and @xmath533 is the set of counter - optimal actions to @xmath122 .",
    "* turn - based reduction . * given a concurrent game @xmath534 and a valuation @xmath147 we construct a turn - based stochastic game @xmath535 as follows :    1 .",
    "the set of states is as follows : @xmath536 2 .",
    "the state space partition is as follows : @xmath537 ; @xmath538 ; and @xmath539 . in other words",
    ", @xmath540 is a partition of the state space , where @xmath541 are player  1 states , @xmath542 are player  2 states , and @xmath543 are random or probabilistic states",
    "the set of edges is as follows :",
    "@xmath544 4 .",
    "the transition function @xmath545 for all states in @xmath543 is uniform over its successors .",
    "intuitively , the reduction is as follows . given the valuation @xmath147 , state @xmath17 is a player  1 state where player  1 can select a pair @xmath532 ( and move to state @xmath546 ) with @xmath547 and @xmath548 such that there is an optimal selector @xmath122 with support exactly @xmath31 and the set of counter - optimal actions to @xmath122 is the set @xmath533 . from a player  2 state @xmath546 ,",
    "player  2 can choose any action @xmath231 from the set @xmath533 , and move to state @xmath549 .",
    "a state @xmath549 is a probabilistic state where all the states in @xmath550 are chosen uniformly at random . given a set @xmath551 we denote by @xmath552 .",
    "we refer to the above reduction as @xmath553 , i.e. , @xmath554 .    * value - class of a valuation . * given a valuation @xmath147 and a real @xmath555 , the _ value - class _ @xmath556 of value @xmath285 is the set of states with valuation @xmath285 , i.e. , @xmath557      * ordering of strategies .",
    "* let @xmath177 be a concurrent game and @xmath161 be the set of safe states .",
    "let @xmath558 . given a concurrent game structure @xmath177 with a safety objective @xmath166 , the set of _ almost - sure winning _ states is the set of states @xmath17 such that the value at @xmath17 is  @xmath73 , i.e. , @xmath559 is the set of almost - sure winning states .",
    "an optimal strategy from @xmath560 is referred as an almost - sure winning strategy .",
    "the set @xmath560 and an almost - sure winning strategy can be computed in linear time by the algorithm given in  @xcite .",
    "we assume without loss of generality that all states in @xmath561 are absorbing .",
    "we recall the preorder @xmath363 on the strategies for player 1 ( as defined in section  [ subsec : reach - stra ] ) as follows : given two player 1 strategies @xmath125 and @xmath364 , let @xmath365",
    "if the following two conditions hold : ( i )  @xmath562 ; and ( ii )  @xmath563 for some state @xmath58 .",
    "furthermore , we write @xmath368 if either @xmath365 or @xmath369 .",
    "we first present an example that shows the improvements based only on @xmath217 operators are not sufficient for safety games , even on turn - based games and then present our algorithm .",
    "[ examp : conc - safety ] consider the turn - based stochastic game shown in fig  [ fig : example - tbs ] , where the @xmath564 states are player  1 states , the @xmath565 states are player  2 states , and @xmath566 states are random states with probabilities labeled on edges . the safety goal is to avoid the state @xmath238 .",
    "consider a memoryless strategy @xmath125 for player  1 that chooses the successor @xmath567 , and the counter - strategy @xmath126 for player  2 chooses @xmath568 .",
    "given the strategies @xmath125 and @xmath126 , the value at @xmath569 and @xmath243 is @xmath570 , and since all successors of @xmath232 have value @xmath570 , the value can not be improved by @xmath217 .",
    "however , note that if player  2 is restricted to choose only value optimal selectors for the value @xmath570 , then player  1 can switch to the strategy @xmath571 and ensure that the game stays in the value class @xmath570 with probability  1 . hence switching to @xmath572",
    "would force player  2 to select a counter - strategy that switches to the strategy @xmath573 , and thus player  1 can get a value @xmath574 .",
    "( 75,45)(15,0 ) ( n2)(10,50)@xmath243 ( n4)(10,10)@xmath238 ( n3)(100,50)@xmath229 ( n5)(100,10)@xmath575 ( n0)(40,50)@xmath232 ( n1)(70,50)(4,5.0)@xmath244 ( n4 ) ( n5 ) ( n0,n2 ) ( n1,n3 ) ( n2,n4)@xmath574 ( n2,n5)@xmath570 ( n3,n5)@xmath574 ( n3,n4)@xmath570 ( n0,n1 ) ( n1,n0 )    * informal description of algorithm  [ algorithm : strategy - improve - safe ] .",
    "* we first present the basic strategy improvement algorithm ( algorithm  [ algorithm : strategy - improve - safe ] ) and will later present a convergent version ( algorithm  [ algorithm : strategy - convergent ] ) for computing the values for all states in @xmath576 . the algorithm ( algorithm  [ algorithm : strategy - improve - safe ] ) iteratively improves player-1 strategies according to the preorder @xmath363 .",
    "the algorithm starts with the random selector @xmath371 that plays at all states all actions uniformly at random . at iteration @xmath372",
    ", the algorithm considers the memoryless player-1 strategy @xmath373 and computes the value @xmath577 .",
    "observe that since @xmath373 is a memoryless strategy , the computation of @xmath577 involves solving the 2-mdp @xmath375 .",
    "the valuation @xmath577 is named @xmath376 . for all states @xmath17 such that @xmath377 , the memoryless strategy at @xmath17 is modified to a selector that is value - optimal for @xmath376 .",
    "the algorithm then proceeds to the next iteration .",
    "if @xmath378 , then the algorithm constructs the game @xmath578 , and computes @xmath579 as the set of almost - sure winning states in @xmath580 for the objective @xmath581 .",
    "let @xmath582 .",
    "if @xmath583 is non - empty , then a selector @xmath386 is obtained at @xmath583 from an pure memoryless optimal strategy ( i.e. , an almost - sure winning strategy ) in @xmath580 , and the algorithm proceeds to iteration @xmath372 .",
    "if @xmath452 and @xmath583 is empty , then the algorithm stops and returns the memoryless strategy @xmath373 for player  1 . unlike strategy improvement algorithms for turn - based games ( see @xcite for a survey ) , algorithm  [ algorithm : strategy - improve - safe ]",
    "is not guaranteed to terminate ( see example  [ ex : counter - soda ] ) .",
    "we will show that algorithm  [ algorithm : strategy - improve - safe ] has both the monotonicity and optimality on termination properties , however , as we will illustrate in example  [ ex : counter - soda ] , the valuations of algorithm  [ algorithm : strategy - improve - safe ] need not necessarily converge to the values .",
    "however , for turn - based stochastic games algorithm  [ algorithm : strategy - improve - safe ] correctly converges to the values .",
    "we will show that algorithm  [ algorithm : strategy - convergent ] has all the desired properties ( i.e. , monotonicity , optimality on termination , and convergence to the values ) .",
    "aaa = aaa = aaa = aaa = aaa = aaa = aaa = aaa + a concurrent game structure @xmath177 with safe set @xmath161 .",
    "+ a strategy @xmath379 for player  1 .",
    "compute @xmath584 .",
    "let @xmath380 and @xmath381 .",
    "compute @xmath585 .",
    "do \\ { *   + 3.1 .",
    "let @xmath586 .",
    "+ 3.2 * if * @xmath587 , * then * + 3.2.1 let @xmath122 be a player-1 selector such that for all states @xmath384 , + we have @xmath385 .",
    "+ 3.2.2 the player-1 selector @xmath386 is defined as follows : for each state @xmath58 , let + @xmath387 + 3.3 * else * + 3.3.1 let@xmath578 + 3.3.2 let @xmath579 be the set of almost - sure winning states in @xmath580 for @xmath581 and + @xmath588 be a pure memoryless almost - sure winning strategy from the set @xmath579 .",
    "+ 3.3.3 * if * ( @xmath589 ) + 3.3.3.1 let @xmath590 + 3.3.3.2 the player-1 selector @xmath386 is defined as follows : for @xmath58 , let + @xmath591 + 3.4 .",
    "compute @xmath592 .",
    "let @xmath389 .",
    "+ @xmath390 and @xmath593 .",
    "* return * @xmath391 .",
    "aaa = aaa = aaa = aaa = aaa = aaa = aaa = aaa + a concurrent game structure @xmath177 with safe set @xmath161 , and number @xmath8 .",
    "+ a strategy @xmath379 for player  1 .",
    "compute @xmath584 ; and @xmath594 .",
    "let @xmath380 and @xmath381 .",
    "+ 2 . compute @xmath585 .",
    "do \\ { *   + 3.1 .",
    "let @xmath595 .",
    "+ 3.2 * if * @xmath596 , * then * + 3.2.1 let @xmath122 be a @xmath8-uniform player-1 selector such that for all states @xmath384 , + we have @xmath597 .",
    "+ 3.2.2 the player-1 selector @xmath386 is defined as follows : for each state @xmath58 , let + @xmath598 + 3.3 * else * + 3.3.1 let@xmath599 + 3.3.2 let @xmath600 be the set of almost - sure winning states in @xmath601 for @xmath581 and + @xmath588 be a pure memoryless almost - sure winning strategy from the set @xmath600 .",
    "+ 3.3.3 * if * ( @xmath602 ) + 3.3.3.1 let @xmath603 + 3.3.3.2 the player-1 selector @xmath386 is defined as follows : for @xmath58 , let + @xmath604 + 3.4 .",
    "compute @xmath592 .",
    "let @xmath389 .",
    "+ @xmath605 and @xmath606 .",
    "return * @xmath391 .",
    "[ lemm : stra - improve - safe1 ] let @xmath392 and @xmath386 be the player-1 selectors obtained at iterations @xmath46 and @xmath372 of algorithm  [ algorithm : strategy - improve - safe ] .",
    "let @xmath607 .",
    "let @xmath608 and @xmath609 .",
    "then @xmath421 for all states @xmath58 ; and therefore @xmath422 for all states @xmath58 , and @xmath423 for all states @xmath424 .",
    "the proof is essentially similar to the proof of lemma  [ lemm : stra - improve ] , and we present the details for completeness . consider the valuations @xmath376 and @xmath425 obtained at iterations @xmath46 and @xmath372 , respectively , and let @xmath426 be the valuation defined by @xmath427 for all states @xmath44 .",
    "the counter - optimal strategy for player  2 to minimize @xmath425 is obtained by maximizing the probability to reach @xmath164 .",
    "let @xmath430 in other words , @xmath431 , and we also have @xmath432 .",
    "we now show that @xmath433 is a feasible solution to the linear program for mdps with the objective @xmath167 , as described in section  [ sec : mdp ] .",
    "since @xmath610 , it follows that for all states @xmath58 and all moves @xmath53 , we have @xmath436 for all states",
    "@xmath437 , we have @xmath409 and @xmath438 , and since @xmath432 , it follows that for all states @xmath437 and all moves @xmath53 , we have @xmath611    since for @xmath384 the selector @xmath440 is obtained as an optimal selector for @xmath441 , it follows that for all states @xmath424 and all moves @xmath53 , we have @xmath442 in other words , @xmath443 .",
    "hence for all states @xmath424 and all moves @xmath53 , we have @xmath444 since @xmath432 , for all states @xmath424 and all moves @xmath53 , we have @xmath445 hence it follows that @xmath433 is a feasible solution to the linear program for mdps with reachability objectives .",
    "since the reachability valuation for player  2 for @xmath167 is the least solution ( observe that the objective function of the linear program is a minimizing function ) , it follows that @xmath446 . thus we obtain @xmath422 for all states @xmath44 , and @xmath423 for all states @xmath384 .",
    "recall that by example  [ examp : conc - safety ] it follows that improvement by only step  3.2 is not sufficient to guarantee convergence to optimal values .",
    "we now present a lemma about the turn - based reduction , and then show that step 3.3 also leads to an improvement .",
    "finally , in theorem  [ thrm : safe - termination ] we show that if improvements by step 3.2 and step 3.3 are not possible , then the optimal value and an optimal strategy is obtained .",
    "[ lemm : stra - improve - safetb ] let @xmath177 be a concurrent game with a set @xmath161 of safe states .",
    "let @xmath147 be a valuation and consider @xmath554 .",
    "let @xmath612 be the set of almost - sure winning states in @xmath613 for the objective @xmath581 , and let @xmath588 be a pure memoryless almost - sure winning strategy from @xmath612 in @xmath613 .",
    "consider a memoryless strategy @xmath125 in @xmath177 for states in @xmath614 as follows : if @xmath615 , then @xmath616 such that @xmath617 and @xmath618 .",
    "consider a pure memoryless strategy @xmath126 for player  2 . if for all states @xmath619 , we have @xmath620 , then for all @xmath619 , we have @xmath621 .",
    "we analyze the markov chain arising after the player fixes the memoryless strategies @xmath125 and @xmath126 . given the strategy @xmath126 consider the strategy @xmath622 as follows : if @xmath615 and @xmath623 , then at state @xmath546 choose the successor @xmath549 . since @xmath588 is an almost - sure winning strategy for @xmath581 , it follows that in the markov chain obtained by fixing @xmath588 and @xmath622 in @xmath613 , all closed connected recurrent set of states that intersect with @xmath612 are contained in @xmath612 , and from all states of @xmath612 the closed connected recurrent set of states within @xmath612 are reached with probability  1 .",
    "it follows that in the markov chain obtained from fixing @xmath125 and @xmath126 in @xmath177 all closed connected recurrent set of states that intersect with @xmath624 are contained in @xmath625 , and from all states of @xmath614 the closed connected recurrent set of states within @xmath625 are reached with probability  1 .",
    "the desired result follows .",
    "[ lemm : stra - improve - safe2 ] let @xmath392 and @xmath386 be the player-1 selectors obtained at iterations @xmath46 and @xmath372 of algorithm  [ algorithm : strategy - improve - safe ] .",
    "let @xmath626 , and @xmath627 .",
    "let @xmath608 and @xmath609 .",
    "then @xmath422 for all states @xmath58 , and @xmath423 for some state @xmath628 .",
    "we first show that @xmath629 .",
    "let @xmath630 .",
    "let @xmath427 for all states @xmath44 .",
    "since @xmath610 , it follows that for all states @xmath58 and all moves @xmath53 , we have @xmath436 the selector @xmath631 chosen for @xmath386 at @xmath632 satisfies that @xmath633 .",
    "it follows that for all states @xmath58 and all moves @xmath53 , we have",
    "@xmath634 it follows that the maximal probability with which player  2 can reach @xmath164 against the strategy @xmath635 is at most @xmath426 .",
    "it follows that @xmath636 .",
    "we now argue that for some state @xmath632 we have @xmath637 .",
    "given the strategy @xmath635 , consider a pure memoryless counter - optimal strategy @xmath126 for player  2 to reach @xmath164 . since the selectors @xmath440 at states @xmath638 are obtained from the almost - sure strategy @xmath639 in the turn - based game @xmath580 to satisfy @xmath581 , it follows from lemma  [ lemm : stra - improve - safetb ] that if for every state @xmath632 , the action @xmath640 , then from all states @xmath632 , the game stays safe in @xmath161 with probability  1 .",
    "since @xmath635 is a given strategy for player  1 , and @xmath126 is counter - optimal against @xmath635 , this would imply that @xmath641 .",
    "this would contradict that @xmath559 and @xmath642 .",
    "it follows that for some state @xmath643 we have @xmath644 , and since @xmath645 we have @xmath646 in other words , we have @xmath647 define a valuation @xmath410 as follows : @xmath648 for @xmath649 , and @xmath650 . given the strategy @xmath635 and the counter - optimal strategy @xmath126 , the valuation @xmath410 satisfies the inequalities of the linear - program for reachability to @xmath164 .",
    "it follows that the probability to reach @xmath164 given @xmath635 is at most @xmath410 .",
    "thus we obtain that @xmath422 for all @xmath58 , and @xmath651 .",
    "this concludes the proof .",
    "we obtain the following theorem from lemma  [ lemm : stra - improve - safe1 ] and lemma  [ lemm : stra - improve - safe2 ] that shows that the sequences of values we obtain is monotonically non - decreasing .",
    "( monotonicity of values).[thrm : safe - mono ] for @xmath416 , let @xmath652 and @xmath386 be the player-1 selectors obtained at iterations @xmath46 and @xmath372 of algorithm  [ algorithm : strategy - improve - safe ] . if @xmath653 , then ( a )  for all @xmath44 we have @xmath654 ; and ( b )  for some @xmath655 we have @xmath656 .",
    "( optimality on termination).[thrm : safe - termination ] let @xmath376 be the valuation at iteration @xmath46 of algorithm  [ algorithm : strategy - improve - safe ] such that @xmath657 .",
    "if @xmath658 , and @xmath659 , then @xmath356 is an optimal strategy and @xmath660 .",
    "we show that for all memoryless strategies @xmath125 for player  1 we have @xmath661 . since memoryless optimal strategies exist for concurrent games with safety objectives ( theorem  [ thrm : memory - safe ] ) the desired result follows .",
    "let @xmath622 be a pure memoryless optimal strategy for player  2 in @xmath580 for the objective complementary to @xmath581 , where @xmath662 .",
    "consider a memoryless strategy @xmath125 for player  1 , and we define a pure memoryless strategy @xmath126 for player  2 as follows .    1 .   if @xmath663 , then @xmath664 , such that @xmath665 ; ( such a @xmath231 exists since @xmath663 ) .",
    "2 .   if @xmath666 , then let @xmath667 , and consider @xmath533 such that @xmath668 .",
    "then we have @xmath669 , such that @xmath670 .",
    "observe that by construction of @xmath126 , for all @xmath671 , we have @xmath672 . we first show that in the markov chain obtained by fixing @xmath125 and @xmath126 in @xmath177 , there is no closed connected recurrent set of states @xmath182 such that @xmath673 .",
    "assume towards contradiction that @xmath182 is a closed connected recurrent set of states in @xmath674 .",
    "the following case analysis achieves the contradiction .    1 .",
    "suppose for every state @xmath184 we have @xmath666 .",
    "then consider the strategy @xmath588 in @xmath580 such that for a state @xmath184 we have @xmath615 , where @xmath675 , and @xmath668 .",
    "since @xmath182 is closed connected recurrent states , it follows by construction that for all states @xmath184 in the game @xmath580 we have @xmath676 , where @xmath677 .",
    "it follows that for all @xmath184 in @xmath580 we have @xmath678 .",
    "since @xmath622 is an optimal strategy , it follows that @xmath679 .",
    "this contradicts that @xmath680 .",
    "2 .   otherwise for some state @xmath681 we have @xmath682 .",
    "let @xmath683 , i.e. , @xmath285 is the least value - class with non - empty intersection with @xmath182 .",
    "hence it follows that for all @xmath684 , we have @xmath685 . observe that since for all @xmath184 we have @xmath672 , it follows that for all @xmath686 either ( a )  @xmath687 ; or ( b )  @xmath688 , for some @xmath684 .",
    "since @xmath689 is the least value - class with non - empty intersection with @xmath182 , it follows that for all @xmath686 we have @xmath690 .",
    "it follows that @xmath691 .",
    "consider the state @xmath681 such that @xmath692 . by the construction of @xmath693",
    ", we have @xmath694 .",
    "hence we must have @xmath695 , for some @xmath696 .",
    "thus we have a contradiction .",
    "it follows from above that there is no closed connected recurrent set of states in @xmath697 , and hence with probability  1 the game reaches @xmath698 from all states in @xmath674 . hence the probability to satisfy",
    "@xmath166 is equal to the probability to reach @xmath560 . since for all states",
    "@xmath699 we have @xmath672 , it follows that given the strategies @xmath125 and @xmath126 , the valuation @xmath376 satisfies all the inequalities for linear program to reach @xmath560 .",
    "it follows that the probability to reach @xmath560 from @xmath17 is atmost @xmath700 .",
    "it follows that for all @xmath699 we have @xmath701 . the result follows .    * @xmath8-uniform selectors and strategies . * for concurrent games",
    ", we will use the result that for @xmath0 , there is a _ @xmath8-uniform memoryless _ strategy that achieves the value of a safety objective within @xmath1 .",
    "we first define @xmath8-uniform selectors and @xmath8-uniform memoryless strategies . for a positive integer @xmath702 ,",
    "a selector @xmath96 for player  1 is _",
    "if for all @xmath703 and all @xmath704 there exists @xmath705 such that @xmath706 and @xmath707 , i.e. , the moves in the support are played with probability that are multiples of @xmath708 with @xmath709 .",
    "we denote by @xmath710 the set of @xmath8-uniform selectors .",
    "a memoryless strategy is @xmath8-uniform if it is obtained from a @xmath8-uniform selector .",
    "we denote by @xmath711 the set of @xmath8-uniform memoryless strategies for player  1 .",
    "we first present a technical lemma ( lemma  [ lemm - expl - constr ] ) that will be used in the key lemma ( lemma  [ lemm : kuniform ] ) to prove the convergence result .",
    "[ lemm - expl - constr ] let @xmath712 be @xmath713 real numbers such that ( 1 ) for all @xmath714 , we have @xmath715 ; and ( 2 ) @xmath716 .",
    "let @xmath717 .",
    "for @xmath718 , there exists @xmath719 and @xmath713 real numbers @xmath720 such that ( 1 ) for all @xmath714 , we have @xmath721 is a multiple of @xmath722 and @xmath723 ; ( 2 ) @xmath724 ; and ( 3 ) for all @xmath714 , we have @xmath725 and @xmath726 .",
    "let @xmath727 . for @xmath714",
    ", define @xmath728 such that @xmath728 is a multiple of @xmath708 and @xmath729 ( basically define @xmath728 as the least multiple of @xmath708 that is at least the value of @xmath730 ) . for @xmath714 ,",
    "let @xmath731 ; i.e. , @xmath721 is defined from @xmath728 with normalization .",
    "clearly , @xmath724 , and for all @xmath732 , we have @xmath723 and @xmath721 can be expressed as a multiple of @xmath722 , for some @xmath733 .",
    "we have the following inequalities : for all @xmath714 , we have @xmath734 the first inequality follows since @xmath735 and @xmath736 .",
    "the second inequality follows since @xmath737 and @xmath738 .",
    "hence for all @xmath714 , we have @xmath739 @xmath740 the desired result follows .",
    "[ lemm : kuniform ] for all concurrent game structures @xmath177 , for all safety objectives @xmath166 , for @xmath551 , for all @xmath0 , there exist @xmath702 and @xmath8-uniform selectors @xmath96 such that @xmath113 is an @xmath1-optimal strategy .",
    "our proof uses a result of solan @xcite and the existence of memoryless optimal strategies for concurrent safety games ( theorem  [ thrm : memory - safe ] ) .",
    "we first present the result of solan specialized for mdps with reachability objectives .    _",
    "the result of  @xcite .",
    "_ let @xmath741 and @xmath742 be two player-2 mdps defined on the same state space @xmath39 , with the same move set @xmath40 and the same move assignment function @xmath743 , but with two different transition functions @xmath37 and @xmath744 , respectively .",
    "let @xmath745 where by convention @xmath746 for @xmath747 , and @xmath748 ( compare with equation ( 9 ) of  @xcite : @xmath749 is obtained as a specialization of ( 9 ) of  @xcite for mdps ) .",
    "let @xmath750 .",
    "for @xmath44 , let @xmath751 and @xmath752 denote the value for player  2 for the reachability objective @xmath167 from @xmath17 in @xmath177 and @xmath498 , respectively . then from theorem  6 of  @xcite ( also see equation ( 10 ) of  @xcite ) it follows that @xmath753 where @xmath754 .",
    "we first explain how specialization of theorem  6 of  @xcite yields ( [ eq - sol - sp ] ) .",
    "theorem  6 of  @xcite was proved for value functions of discounted games with costs , even when the discount factor @xmath755 .",
    "since the value functions of limit - average games are obtained as the limit of the value functions of discounted games as the discount factor goes to @xmath756  @xcite , the result of theorem  6 of  @xcite also holds for concurrent limit - average games ( this was the main result of  @xcite ) .",
    "since reachability objectives are special case of limit - average objectives , theorem  6 of  @xcite also holds for reachability objectives . in the special case of reachability objectives with the same target set , the different cost functions used in equation ( 10 ) of  @xcite coincide , and the maximum absolute value of the cost is  1 .",
    "thus we obtain ( [ eq - sol - sp ] ) as a specialization of theorem  6 of  @xcite .",
    "we now use the existence of memoryless optimal strategies in concurrent safety games , and ( [ eq - sol - sp ] ) to obtain our desired result . consider a concurrent safety game @xmath757 with safe set @xmath161 for player  1 .",
    "let @xmath125 be a memoryless optimal strategy for the objective @xmath166 .",
    "let @xmath758 be the minimum positive transition probability given by @xmath125 .",
    "given @xmath0 , let @xmath759 .",
    "we define a memoryless strategy @xmath364 satisfying the following conditions : for @xmath44 and @xmath52 we have    1 .   if @xmath760 , then @xmath761 ; 2 .",
    "if @xmath762 , then following conditions are satisfied : 1 .",
    "@xmath763 ; 2 .",
    "@xmath764 ; 3 .",
    "@xmath765 ; and 4 .",
    "@xmath766 is a multiple of @xmath722 , for an integer @xmath702 ( such a @xmath8 exists for @xmath767 .    for @xmath768 ,",
    "such a strategy @xmath364 exists ( follows from the construction of lemma  [ lemm - expl - constr ] ) .",
    "let @xmath769 and @xmath770 be the two player-2 mdps obtained from @xmath177 by fixing the memoryless strategies @xmath125 and @xmath364 , respectively . then by definition of @xmath364 we have @xmath771 .",
    "let @xmath509 .",
    "for @xmath44 , let the value of player  2 for the objective @xmath167 in @xmath769 and @xmath770 be @xmath751 and @xmath752 , respectively .",
    "by ( [ eq - sol - sp ] ) we have @xmath772 observe that by choice of @xmath773 we have ( a )  @xmath774 and ( b )  @xmath775 .",
    "hence we have @xmath776 .",
    "since @xmath125 is a memoryless optimal strategy , it follows that @xmath364 is a @xmath8-uniform memoryless @xmath1-optimal strategy",
    ".    * turn - based stochastic games convergence .",
    "* we first observe that since pure memoryless optimal strategies exist for turn - based stochastic games with safety objectives ( the results follows from results of  @xcite ) , for turn - based stochastic games it suffices to iterate over pure memoryless selectors .",
    "since the number of pure memoryless strategies is finite , it follows for turn - based stochastic games algorithm  [ algorithm : strategy - improve - safe ] always terminates and yields an optimal strategy . in other words",
    ", we can restrict the selectors used in algorithm  [ algorithm : strategy - improve - safe ] in steps 3.2.2 and 3.3.2.2 to be pure memoryless selectors .",
    "then the local improvement steps of algorithm  [ algorithm : strategy - improve - safe ] with pure memoryless selectors terminates , and by theorem  [ thrm : safe - termination ] yield a globally optimal pure memoryless strategy ( which is an optimal strategy ) .",
    "we will use the argument for turn - based stochastic games to a variant of algorithm  [ algorithm : strategy - improve - safe ] restricted to @xmath8-uniform selectors .",
    "* strategy improvement with @xmath8-uniform selectors . * we now present the variant of algorithm  [ algorithm : strategy - improve - safe ] where we restrict the algorithm to @xmath8-uniform selectors .",
    "the notations are essentially the same as used in algorithm  [ algorithm : strategy - improve - safe ] , but restricted to @xmath8-uniform selectors and presented as algorithm  [ algo : k - uniform ] .",
    "( for example , @xmath601 is similar to @xmath580 but restricted to @xmath8-uniform selectors , and similarly @xmath777 are the optimal @xmath8-uniform selectors , see section  [ sec : appendix ] for complete details ) .",
    "we first argue that if we restrict algorithm  [ algorithm : strategy - improve - safe ] such that every iteration yields a @xmath8-uniform selector , for @xmath702 , then the algorithm terminates , i.e. , algorithm  [ algo : k - uniform ] terminates . the basic argument that if algorithm  [ algorithm : strategy - improve - safe ] is restricted to @xmath8-uniform selectors for player  1 , for @xmath702 , then the algorithm terminates , follows from the facts that ( i )  the sequence of strategies obtained are monotonic ( theorem  [ thrm : safe - mono ] ) ( i.e. , the algorithm does not cycle among @xmath8-uniform selectors ) ; and ( ii )  the number of @xmath8-uniform selectors for a given @xmath8 is finite",
    "given @xmath702 , let us denote by @xmath778 the valuation of algorithm  [ algo : k - uniform ] at iteration @xmath46 .",
    "[ lemm - conv - terminate ] for all @xmath779 , there exists @xmath133 such that @xmath780 .    * convergence to optimal @xmath8-uniform memoryless strategies .",
    "* we now argue that the valuation algorithm  [ algo : k - uniform ] converges to is optimal for @xmath8-uniform selectors .",
    "the argument is as follows : if we restrict player  1 to chose between @xmath8-uniform selectors , then a concurrent game structures @xmath177 can be converted to a turn - based stochastic game structure , where player  1 first chooses a @xmath8-uniform selector , then player  2 chooses an action , and then the transition is determined by the chosen @xmath8-uniform selector of player  1 , the action of player  2 and the transition function @xmath37 of the game structure @xmath177 . then by termination of turn - based stochastic games it follows that the algorithm will terminate .",
    "it follows from theorem  [ thrm : safe - termination ] that upon termination we obtain optimal strategy for the turn - based stochastic game . in other words ,",
    "as discussed above for turn - based stochastic game , the local iteration converges to a globally optimal strategy .",
    "hence the valuation obtained upon termination is the maximal value obtained over all @xmath8-uniform memoryless strategies .",
    "this gives us the following lemma ( also see appendix for a detailed proof ) .",
    "[ lemm - conv - optimal ] for all @xmath702 , let @xmath416 be such that @xmath780 .",
    "then we have @xmath781 .",
    "[ lemm - conv - bound ] for all concurrent game structures @xmath177 , for all safety objectives @xmath166 , for @xmath551 , for all @xmath0 , there exist @xmath779 and @xmath133 such that for all @xmath44 we have @xmath782 .    by lemma  [ lemm : kuniform ] ,",
    "for all @xmath0 , there exists @xmath702 such that there is a @xmath8-uniform memoryless @xmath1-optimal strategy for player  1 . by lemma",
    "[ lemm - conv - terminate ] , for all @xmath702 , there exists an @xmath133 such that @xmath783 , and by lemma  [ lemm - conv - optimal ] it follows that the valuation @xmath784 represents the maximal value obtained by @xmath8-uniform memoryless strategies .",
    "hence it follows that there exists @xmath702 and @xmath133 such that for all @xmath44 we have @xmath782 .",
    "the desired result follows .",
    "we now present the convergent strategy improvement algorithm for safety objectives as algorithm  [ algorithm : strategy - convergent ] that iterates over @xmath8-uniform strategy values .",
    "the algorithm iteratively calls algorithm  [ algo : k - uniform ] with larger @xmath8 , unless the termination condition of algorithm  [ algorithm : strategy - improve - safe ] is satisfied .",
    "aaa = aaa = aaa = aaa = aaa = aaa = aaa = aaa + a concurrent game structure @xmath177 with safe set @xmath161 .",
    "+ a strategy @xmath379 for player  1 .",
    "@xmath785 and @xmath381 .",
    "do \\ { *   + 1.1 @xmath786 algorithm  [ algo : k - uniform](@xmath787 ) + 1.2 compute @xmath592 + 1.3 let @xmath586 .",
    "+ 1.4 let@xmath578 + 1.4.1 let @xmath579 be the set of almost - sure winning states in @xmath580 for @xmath581 .",
    "+ 1.5 let @xmath389 and @xmath788 .",
    "+ @xmath390 and @xmath593 .",
    "return * @xmath391 .",
    "( monotonicity , optimality on termination and convergence ) .",
    "let @xmath376 be the valuation obtained at iteration @xmath46 of algorithm  [ algorithm : strategy - convergent ] .",
    "then the following assertions hold .    1 .   for all @xmath133 we have @xmath629 .",
    "if the algorithm terminates , then @xmath660 .",
    "3 .   for all @xmath0",
    ", there exists @xmath46 such that for all @xmath17 we have @xmath789 .",
    "4 .   @xmath790 .",
    "we prove the results as follows .    1 .",
    "let @xmath376 is the valuation of algorithm  [ algorithm : strategy - convergent ] at iteration @xmath46 . for @xmath702 , we consider @xmath791 to denote the valuation of algorithm  [ algo : k - uniform ] with the restriction of @xmath8-uniform selector at iteration @xmath46 , and let @xmath792 denote the least fixpoint ( i.e.",
    ", @xmath793 is the least value of @xmath46 such that @xmath794 ) .",
    "since @xmath8-uniform selectors are a subset of @xmath795-uniform selectors , it follows that the maximal value obtained over strategies that uses @xmath795-uniform selectors is at least the maximal value obtained over @xmath8-uniform selectors .",
    "since @xmath792 denote the maximal value obtained over @xmath8-uniform selectors ( follows from lemma  [ lemm - conv - optimal ] ) , we have that @xmath796 ( note that we do not require that @xmath797 , i.e. , the algorithm with @xmath795-uniform selectors may require more iterations to terminate ) .",
    "we have @xmath798 and hence the first result follows .",
    "2 .   the result follows from theorem  [ thrm : safe - termination ] .",
    "3 .   from lemma",
    "[ lemm - conv - bound ] it follows that for all @xmath0 , there exists a @xmath702 such that for all @xmath17 we have @xmath799 .",
    "hence @xmath800 .",
    "hence we have that for all @xmath0 , there exists @xmath24 , such that for all @xmath44 we have @xmath801 .",
    "4 .   by part ( 1 ) for all @xmath133 we have @xmath629 . by part ( 3 ) , for all @xmath0 , there exists @xmath416 such that for all @xmath44 we have @xmath789 .",
    "hence it follows that for all @xmath0 , there exists @xmath416 such that for all @xmath802 and for all @xmath44 we have @xmath803 .",
    "it follows that @xmath790 .",
    "this gives us the following result .",
    "* discussion on convergence of algorithm  [ algorithm : strategy - improve - safe ] .",
    "* we will now present an example to illustrate that ( contrary to the claim of theorem  4.3 of  @xcite ) algorithm  [ algorithm : strategy - improve - safe ] need not converge to the values in concurrent safety games .",
    "however , as discussed before algorithm  [ algorithm : strategy - improve - safe ] satisfies the monotonicity and optimality on termination , and for turn - based stochastic games ( and also when restricted to @xmath8-uniform strategies ) converges to the values as termination is guaranteed . in the example",
    "we will also argue how algorithm  [ algorithm : strategy - convergent ] converges to the values of the game .",
    "[ ex : counter - soda ] our example consists of two steps . in the first step we will present a gadget where the value is irrational and with probability  1 absorbing states are reached .",
    "( 48,28)(-5,-5 ) ( n0)(10,12)@xmath232 ( n1)(40,0)@xmath244 ( n2)(40,24)@xmath243    ( n1 ) ( n2 ) ( n0)@xmath804 ( n0,n1)@xmath805 ( n0,n1)@xmath804 ( n0,n2)@xmath806    * step 1 .",
    "* we first consider the game shown in fig  [ fig : ex1 ] with three states @xmath807 with two actions @xmath808 for player  1 and @xmath809 for player  2 .",
    "the states @xmath569 are safe states , and @xmath243 is a non - safe state .",
    "the transitions are as follows : ( 1 )  @xmath244 and @xmath243 are absorbing ; and ( 2 )  in @xmath232 we have the following transitions , ( a )  given action pairs @xmath810 and @xmath806 the next state is @xmath244 , ( b )  given action pair @xmath811 the next state is @xmath243 , and ( c )  given action pair @xmath812 the next states are @xmath232 and @xmath244 with probability @xmath245 each . in this game ,",
    "the state @xmath232 is transient , as given any action pairs , the set @xmath813 of absorbing states is reached with probability at least @xmath245 in one step .",
    "hence the set @xmath813 is reached with probability  1 , irrespective of the choice of strategies of the players . hence in this game the objective for player  1 is equivalently to reach @xmath244 .",
    "let us denote by @xmath814 the value of the game at @xmath232 , and let us consider the following matrix @xmath815 then @xmath816 .",
    "in other words , consider the valuation @xmath817 for states @xmath569 and @xmath243 , respectively , and @xmath818 describes that @xmath819 , and it is the least fixpoint of valuations satisfying @xmath820 .",
    "we now analyze the value @xmath814 at @xmath232 .",
    "the solution of @xmath814 is achieved by solving the following optimization problem @xmath821 intuitively , @xmath822 denotes the probability to choose move @xmath230 in an optimal strategy .",
    "the solution to the optimization problem is achieved by setting @xmath823 .",
    "hence we have @xmath824 , i.e. , @xmath825 .",
    "since @xmath822 must lie in the interval @xmath826 $ ] , we have @xmath827 , and thus we have @xmath828 .",
    "we now analyze algorithm  [ algorithm : strategy - improve - safe ] on this example .",
    "let @xmath376 denote the valuation of the @xmath46-th iteration , and let @xmath829 be the value at state @xmath232 .",
    "then we have @xmath830 and in the limit it converges to value @xmath831 .",
    "we observe that on this example algorithm  [ algorithm : strategy - improve - safe ] exactly behaves as algorithm  [ algorithm : strategy - improve ] ( strategy improvement for reachability ) as the objective for player  1 is equivalently to reach @xmath244 , since @xmath232 is transient .",
    "the reason of the strict inequality @xmath830 is as follows : if the valuation at state @xmath232 in @xmath46-th and @xmath372-th iteration is the same , then by correctness of algorithm  [ algorithm : strategy - improve ] it follows that the values would have been achieved in finitely many steps , implying convergence to a rational value at @xmath232 .",
    "the convergence to the values in the limit is due to correctness of algorithm  [ algorithm : strategy - improve ] .",
    "( 48,28)(-5,-5 ) ( n0)(40,12)@xmath232 ( n1)(70,0)@xmath244 ( n2)(70,24)@xmath243    ( n3)(20,12)@xmath229 ( n4)(0,12)@xmath238 ( n5)(-20,12)@xmath575    ( n1 ) ( n2 ) ( n0)@xmath804 ( n0,n1)@xmath805 ( n0,n1)@xmath804 ( n0,n2)@xmath806 ( n3,n0)@xmath832 ( n4,n5)@xmath833 ( n3,n4)@xmath834 ( n4,n3)@xmath835 ( n4,n5)@xmath833 ( n4,n5)@xmath833    * step  2 . * we will now augment the game of step  1 to construct an example to show that algorithm  2 does not necessarily converge to the values . consider the game shown in fig  [ fig : ex2 ] augmenting the game of fig  [ fig : ex1 ] with some additional states ( states @xmath836 and @xmath575 ) and transitions ( we only show the interesting transitions in the figure for simplicity ) .",
    "all the additional states shown are safe states .",
    "the value of state @xmath575 is @xmath837 ( consider it as a probabilistic state going to state @xmath244 with probability @xmath837 and @xmath243 with probability @xmath838 , and these edges are not shown in the figure ) .",
    "the transitions at state @xmath229 and @xmath238 are as follows : in state @xmath229 , player  1 can goto state @xmath232 or @xmath238 by choosing actions @xmath230 and @xmath231 , respectively ( at @xmath229 player  2 has only one action @xmath839 ) ; and in state @xmath238 , player  2 can goto state @xmath229 or @xmath575 by choosing actions @xmath840 and @xmath841 , respectively ( at @xmath238 player  1 has only one action @xmath839 ) .",
    "we analyze algorithm  [ algorithm : strategy - improve - safe ] on the example shown in fig  [ fig : ex2 ] . in this game , at @xmath229 player  1 starts by playing actions @xmath230 and @xmath231 uniformly , and player  2 responds by chosing action @xmath840 . in the iterations of the algorithm it follows by the argument of step  1 , that the set @xmath395 of step  3.1 of algorithm  [ algorithm : strategy - improve - safe ] is always non - empty as @xmath842 . hence in every iteration the value at @xmath232 improves , and",
    "the strategy in @xmath229 and @xmath238 does not change .",
    "hence the valuation at @xmath229 converges to the valuation at @xmath232 , i.e. , to @xmath843 .",
    "however , by switching to action @xmath231 at @xmath229 , player  1 can enforce player  2 to play action @xmath841 at @xmath238 and ensure value @xmath837 . in other words ,",
    "the value at @xmath229 is @xmath837 , whereas algorithm  [ algorithm : strategy - improve - safe ] converges to @xmath844 .",
    "the switching to action @xmath231 would have been ensured by the turn - based construction of step  3.3 . for turn - based stochastic games or @xmath8-uniform memoryless strategies , since convergence to values is guaranteed , the turn - based construction of step  3.3 is also ensured to get executed .",
    "however , as the convergence to values in concurrent games is in the limit , step  3.3 of algorithm  [ algorithm : strategy - improve - safe ] may not get executed as shown by this example .",
    "however , we now illustrate that the valuations of algorithm  [ algorithm : strategy - convergent ] converges to the values .",
    "we consider algorithm  [ algorithm : strategy - convergent ] : consider @xmath8-uniform strategies , for a finite @xmath845 , then the value at @xmath232 for @xmath8-uniform strategies converges in finitely many steps to a value smaller than @xmath837 ( as it converges to a value smaller than the value at @xmath232 ) , and then step  3.3 of algorithm  [ algo : k - uniform ] would get executed , and the value at @xmath229 would be assigned to  @xmath837 . in other words , for algorithm  [ algorithm : strategy - convergent ] the values at @xmath836 and @xmath575 are always set to @xmath837 , and the value at @xmath232 converges in the limit to @xmath831 . thus with the example we show that though algorithm  [ algorithm : strategy - improve - safe ] does not necessarily converge to the values , algorithm  [ algorithm : strategy - convergent ] correctly converges to the values .",
    "* retraction of theorem  4.3 of  @xcite . * in  @xcite , the convergence of algorithm  [ algorithm : strategy - improve - safe ] was claimed .",
    "unfortunately the theorem is incorrect ( with irreparable error ) as shown by example  [ ex : counter - soda ] and we retract the claim of theorem  4.3 of  @xcite of convergence of algorithm  [ algorithm : strategy - improve - safe ] for concurrent games .    *",
    "* algorithm  [ algorithm : strategy - improve - safe ] may not terminate in general ; we briefly describe the complexity of every iteration .",
    "given a valuation @xmath376 , the computation of @xmath846 involves the solution of matrix games with rewards @xmath376 ; this can be done in polynomial time using linear programming .",
    "given @xmath376 , if @xmath452 , the sets @xmath847 and @xmath848 can be computed by enumerating the subsets of available actions at @xmath17 and then using linear - programming .",
    "for example , to check whether @xmath849 it suffices to check both of these facts :    1 .   _",
    "( @xmath31 is the support of an optimal selector @xmath122 ) .",
    "_ there is an selector @xmath122 such that ( i )  @xmath122 is optimal ( i.e. for all actions @xmath850 we have @xmath851 ) ; ( ii )  for all @xmath852 we have @xmath853 , and for all @xmath854 we have @xmath855 ; 2 .   _",
    "( @xmath533 is the set of counter - optimal actions against @xmath122 ) .",
    "_ for all @xmath856 we have @xmath857 , and for all @xmath858 we have @xmath859 .",
    "all the above checks can be performed by checking feasibility of sets of linear equalities and inequalities .",
    "hence , @xmath860 can be computed in time polynomial in size of @xmath177 and @xmath376 and exponential in the number of moves .",
    "we observe that the construction is exponential only in the number of moves at a state , and not in the number of states .",
    "the number of moves at a state is typically much smaller than the size of the state space .",
    "we also observe that the improvement step 3.3.2 requires the computation of the set of almost - sure winning states of a turn - based stochastic safety game : this can be done both via linear - time discrete graph - theoretic algorithms @xcite , and via symbolic algorithms  @xcite .",
    "both of these methods are more efficient than the basic step 3.4 of the improvement algorithm , where the quantitative values of an mdp must be computed .",
    "thus , the improvement step 3.3 of algorithm  [ algorithm : strategy - improve - safe ] is in practice should not be inefficient , compared with the standard improvement steps 3.2 and 3.4 .",
    "we now discuss the above steps for algorithm  [ algo : k - uniform ] .",
    "the argument is similar as above , but in case of @xmath8-uniform selectors , we need to ensure that the witness selectors are @xmath8-uniform which can be achieved with integer constraints . in other words , for algorithm  [ algo : k - uniform ] the above checks are performed by checking feasibility of sets of integer linear equalities and inequalities ( which can be achieved in exponential time ) . again , the construction is exponential in the number of moves at a state , and not in the number of states .",
    "hence we enumerate over sets of moves at a state ( exponential in number of moves ) , and then need to solve integer linear constraints ( the size of the integer linear constraints is polynomial in the number of moves , and is achieved in time exponential in the number of moves ) .",
    "thus again the improvement step 3.3 of algorithm  [ algo : k - uniform ] is polynomial in the size of the game , and exponential in the number of moves .      in this subsection",
    "we present termination criteria for strategy improvement algorithms for concurrent games for @xmath1-approximation .    *",
    "termination for concurrent games .",
    "* we apply the reachability strategy improvement algorithm ( algorithm  [ algorithm : strategy - improve ] ) for player  2 , for a reachability objective @xmath167 , we obtain a sequence of valuations @xmath861 such that ( a ) @xmath862 ; ( b ) if @xmath466 , then @xmath863 ; and ( c ) @xmath864 . given a concurrent game @xmath177 with @xmath160 and @xmath865",
    ", we apply algorithm  [ algorithm : strategy - improve ] to obtain the sequence of valuation @xmath861 as above , and we apply algorithm  [ algorithm : strategy - convergent ] to obtain a sequence of valuation @xmath866 .",
    "the termination criteria are as follows :    1 .",
    "if for some @xmath46 we have @xmath466 , then we have @xmath863 , and @xmath867 , and we obtain the values of the game ; 2 .   if for some @xmath46 we have @xmath868 , then we have @xmath869 , and @xmath660 , and we obtain the values of the game ; and 3 .   for @xmath0 , if for some @xmath416 , we have @xmath870 , then for all @xmath44 we have @xmath871 and @xmath872 ( i.e. , the algorithm can stop for @xmath1-approximation ) .",
    "observe that since @xmath861 and @xmath866 are both monotonically non - decreasing and @xmath873 , it follows that if @xmath870 , then forall @xmath874 we have @xmath875 and @xmath876 .",
    "this establishes that @xmath877 and @xmath878 ; and the correctness of the stopping criteria ( 3 ) for @xmath1-approximation follows .",
    "we also note that instead of applying the reachability strategy improvement algorithm , a value - iteration algorithm can be applied for reachability games to obtain a sequence of valuation with properties similar to @xmath879 and the above termination criteria can be applied .",
    "let @xmath177 be a concurrent game structure with a safety objective @xmath166 .",
    "algorithm  [ algorithm : strategy - convergent ] and algorithm  [ algorithm : strategy - improve ] for player  2 for the reachability objective @xmath880 yield two sequences of monotonic valuations @xmath866 and @xmath879 , respectively , such that ( a )  for all @xmath416 , we have @xmath881 ; and ( b )  @xmath882 .    * bounds for approximation .",
    "* we now discuss the bounds for approximation for concurrent games with reachability objectives , which follows from the results of  @xcite .",
    "it follows from the results of  @xcite that for all @xmath0 , there exist @xmath8-uniform memoryless optimal strategies for concurrent reachability and safety games @xmath177 , where @xmath8 is bounded by @xmath883 .",
    "it follows that for all @xmath0 , if we consider our strategy improvement algorithm ( restricted to @xmath8-uniform selectors ) for reachability games , then upon termination the valuation obtained is an @xmath1-approximation of the value function of the game , where @xmath8 is bounded by @xmath883 . using the restriction to @xmath8-uniform memoryless strategies , along with the reduction of concurrent games to turn - based stochastic game for @xmath8-uniform memoryless strategies and the termination bound for turn - based stochastic games we obtain a double exponential bound on the number of iterations required for termination (",
    "note that if @xmath884 , then the total number of @xmath8-uniform memoryless strategies is @xmath885 , which is double exponential ) ( also see  @xcite for details ) .",
    "moreover , the recent result of  @xcite shows that the double exponential bound is near optimal for the strategy improvment algorithm for concurrent games with reachability objectives .",
    "* approximation of strategies . * the previous method to solve concurrent reachability and safety games was the value - iteration algorithm .",
    "the witness strategy produced by the value - iteration algorithm for concurrent reachability games is not memoryless ; and for concurrent safety games since the value - iteration algorithm converges from above it does not provide any witness strategies .",
    "the only previous algorithm to approximate memoryless @xmath1-optimal strategies , for @xmath0 , for concurrent reachability and safety games is the naive algorithm that exhaustively searches over the set of all @xmath8-uniform memoryless strategies ( such that the @xmath8-uniform memoryless strategies suffices for @xmath1-optimality and @xmath8-depends in @xmath1 ) .",
    "our strategy improvement algorithms for concurrent reachability and safety games are the first strategy search based approach to approximate @xmath1-optimal strategies",
    ".    * acknowledgements . *",
    "we thanks anonymous reviewers for many helpful and insightful comments that significantly improved the presentation of the paper , and help us fix gaps in the results of the conference versions .",
    "we warmly acknowlede their help .",
    "this work was partially supported by erc start grant graph games ( project no 279307 ) , fwf nfn grant s11407-n23 ( rise ) and a microsoft faculty fellowship .    10    d.p .",
    "bertsekas . .",
    "athena scientific , 1995 .",
    "volumes i and ii .",
    "k.  chatterjee , l.  de  alfaro , and t.  a. henzinger .",
    "termination criteria for solving concurrent safety and reachability games . in _ soda _ , pages 197206 .",
    "acm - siam , 2009 .",
    "k.  chatterjee , l.  de  alfaro , and t.a .",
    "strategy improvement in concurrent reachability games . in _",
    "ieee , 2006 .",
    "k.  chatterjee , m.  jurdziski , and t.a .",
    "simple stochastic parity games . in _ csl03 _ , volume 2803 of _ lncs _ , pages 100113 .",
    "springer , 2003 .",
    "a.  condon .",
    "the complexity of stochastic games .",
    "96(2):203224 , 1992 .    a.  condon . on algorithms for simple stochastic games .",
    "in _ advances in computational complexity theory _ , volume  13 of _ dimacs series in discrete mathematics and theoretical computer science _ ,",
    "pages 5173 .",
    "american mathematical society , 1993 .",
    "c.  courcoubetis and m.  yannakakis . the complexity of probabilistic verification . , 42(4):857907 , 1995 .",
    "l.  de  alfaro . .",
    "phd thesis , stanford university , 1997 .",
    "technical report stan - cs - tr-98 - 1601 .",
    "l.  de  alfaro and t.a .",
    "concurrent omega - regular games . in _ proceedings of the 15th annual symposium on logic in computer science _ , pages 141154 .",
    "ieee computer society press , 2000 .",
    "l.  de  alfaro , t.a .",
    "henzinger , and o.  kupferman .",
    "concurrent reachability games . , 386(3):188217 , 2007 .",
    "l.  de  alfaro and r.  majumdar . quantitative solution of omega - regular games .",
    ", 68:374397 , 2004 .    c.  derman . .",
    "academic press , 1970 .",
    "k.  etessami and m.  yannakakis .",
    "recursive concurrent stochastic games . in _",
    "icalp 06 : automata , languages , and programming_. springer , 2006 .",
    "h.  everett .",
    "recursive games . in _ contributions to the theory of games",
    "iii _ , volume  39 of _ annals of mathematical studies _ ,",
    "pages 4778 , 1957 .    j.  filar and k.  vrieze . .",
    "springer - verlag , 1997 .",
    "h.  gimbert and f.  horn .",
    "simple stochastic games with few random vertices are easy to solve . in _ fossacs08 _ , 2008 .",
    "k.  a. hansen , r.  ibsen - jensen , and p.  b. miltersen .",
    "the complexity of solving reachability games using value and strategy iteration . in _",
    "csr _ , pages 7790 , 2011 .",
    "k.  a. hansen , m.  kouck , and p.  .",
    "winning concurrent reachability games requires doubly - exponential patience . in _ lics _ , pages 332341 , 2009 .",
    "a.j . hoffman and r.m . karp .",
    "on nonterminating stochastic games . , 12(5):359370 , 1966 .",
    "r.  a. howard . .",
    "mit press , 1960 .",
    "kemeny , j.l .",
    "snell , and a.w .",
    "d. van nostrand company , 1966 .",
    "t.  a. liggett and s.  a. lippman .",
    "stochastic games with perfect information and time average payoff . , 11:604607 , 1969 .",
    "mertens and a.  neyman .",
    "stochastic games . , 10:5366 , 1981 .",
    "t.  parthasarathy .",
    "discounted and positive stochastic games . , 77(1):134136 , 1971 .",
    "s.  s. rao , r.  chandrasekaran , and k.  p.  k. nair .",
    "algorithms for discounted games . , pages 627637 , 1973 .",
    "stochastic games . , 39:10951100 , 1953 .",
    "e.  solan .",
    "continuity of the value of competitive markov decision processes .",
    ", 16:831845 , 2003 .",
    "u.  zwick and m.s .",
    "the complexity of mean payoff games on graphs .",
    ", 158:343359 , 1996 .",
    "we now present the details of restriction to @xmath8-uniform selectors , and the details of the notations used in algorithm  [ algo : k - uniform ] .",
    "the definitions are essentially same as for selectors and optimal selectors , but restricted to @xmath8-uniform selectors",
    ".    * optimal @xmath8-uniform selectors . *",
    "for @xmath702 , a valuation @xmath147 and a state @xmath17 , let @xmath886 denote the optimal one - step value among @xmath8-uniform selectors .",
    "for @xmath702 , given a valuation @xmath147 and a state @xmath17 , we define by @xmath887 the set of optimal selectors among @xmath8-uniform selectors for @xmath147 at state @xmath17 . for a @xmath8-uniform optimal selector @xmath888",
    ", we define the set of counter - optimal actions as follows : @xmath889 observe that for @xmath888 , for all @xmath890 we have @xmath891 .",
    "we define the set of @xmath8-uniform optimal selector support and the counter - optimal action set as follows : @xmath892 i.e. , it consists of pairs @xmath532 of actions of player  1 and player  2 , such that there is a @xmath8-uniform optimal selector @xmath122 with support @xmath31 , and @xmath533 is the set of counter - optimal actions to @xmath122 .      1 .",
    "the set of states is as follows : @xmath894 2 .",
    "the state space partition is as follows : @xmath537 ; @xmath895 ; and @xmath896 .",
    "in other words , @xmath540 is a partition of the state space , where @xmath541 are player  1 states , @xmath542 are player  2 states , and @xmath543 are random or probabilistic states . 3 .",
    "the set of edges is as follows : @xmath897 4 .",
    "the transition function @xmath545 for all states in @xmath543 is uniform over its successors .",
    "intuitively , the reduction is as follows . given the valuation @xmath147 , state @xmath17 is a player  1 state where player  1 can select a pair @xmath532 ( and move to state @xmath546 ) with @xmath547 and @xmath548 such that there is a @xmath8-uniform optimal selector @xmath122 with support exactly @xmath31 and the set of counter - optimal actions to @xmath122 is the set @xmath533 . from a player  2 state @xmath546 ,",
    "player  2 can choose any action @xmath231 from the set @xmath533 , and move to state @xmath549 .",
    "a state @xmath549 is a probabilistic state where all the states in @xmath550 are chosen uniformly at random .",
    "given a set @xmath551 we denote by @xmath552 .",
    "we refer to the above reduction as @xmath553 , i.e. , @xmath898 .    *",
    "( of lemma  [ lemm - conv - optimal ] ) . *",
    "the proof of the result is essentially identical as the proof of theorem  [ thrm : safe - termination ] , and we present the details for completeness .",
    "let @xmath899 .",
    "we show that for all @xmath8-uniform memoryless strategies @xmath125 for player  1 we have @xmath661 .",
    "let @xmath622 be a pure memoryless optimal strategy for player  2 in @xmath601 for the objective complementary to @xmath581 , where @xmath900 .",
    "consider a @xmath8-uniform memoryless strategy @xmath125 for player  1 , and we define a pure memoryless strategy @xmath126 for player  2 as follows .    1 .   if @xmath901 , then @xmath664 , such that @xmath665 ; ( such a @xmath231 exists since @xmath901 ) .",
    "2 .   if @xmath902 , then let @xmath667 , and consider @xmath533 such that @xmath903 .",
    "then we have @xmath669 , such that @xmath670 .",
    "observe that by construction of @xmath126 , for all @xmath671 , we have @xmath672 . we first show that in the markov chain obtained by fixing @xmath125 and @xmath126 in @xmath177 , there is no closed connected recurrent set of states @xmath182 such that @xmath673 .",
    "assume towards contradiction that @xmath182 is a closed connected recurrent set of states in @xmath674 .",
    "the following case analysis achieves the contradiction .    1 .",
    "suppose for every state @xmath184 we have @xmath902 .",
    "then consider the strategy @xmath588 in @xmath601 such that for a state @xmath184 we have @xmath615 , where @xmath675 , and @xmath903 .",
    "since @xmath182 is closed connected recurrent states , it follows by construction that for all states @xmath184 in the game @xmath601 we have @xmath676 , where @xmath677 .",
    "it follows that for all @xmath184 in @xmath601 we have @xmath678 .",
    "since @xmath622 is an optimal strategy , it follows that @xmath904 .",
    "this contradicts that @xmath905 .",
    "otherwise for some state @xmath681 we have @xmath906 .",
    "let @xmath683 , i.e. , @xmath285 is the least value - class with non - empty intersection with @xmath182 .",
    "hence it follows that for all @xmath684 , we have @xmath685 .",
    "observe that since for all @xmath184 we have @xmath672 , it follows that for all @xmath686 either ( a )  @xmath687 ; or ( b )  @xmath688 , for some @xmath684 .",
    "since @xmath689 is the least value - class with non - empty intersection with @xmath182 , it follows that for all @xmath686 we have @xmath690 .",
    "it follows that @xmath691 .",
    "consider the state @xmath681 such that @xmath907 . by the construction of @xmath693",
    ", we have @xmath694 .",
    "hence we must have @xmath695 , for some @xmath696 .",
    "thus we have a contradiction .",
    "it follows from above that there is no closed connected recurrent set of states in @xmath697 , and hence with probability  1 the game reaches @xmath698 from all states in @xmath674 . hence the probability to satisfy",
    "@xmath166 is equal to the probability to reach @xmath560 .",
    "since for all states @xmath699 we have @xmath672 , it follows that given the strategies @xmath125 and @xmath126 , the valuation @xmath376 satisfies all the inequalities for linear program to reach @xmath560 .",
    "it follows that the probability to reach @xmath560 from @xmath17 is atmost @xmath700 .",
    "it follows that for all @xmath699 we have @xmath701 .",
    "this completes the proof ."
  ],
  "abstract_text": [
    "<S> we consider concurrent games played on graphs . at every round of a game , </S>",
    "<S> each player simultaneously and independently selects a move ; the moves jointly determine the transition to a successor state . </S>",
    "<S> two basic objectives are the safety objective to stay forever in a given set of states , and its dual , the reachability objective to reach a given set of states . </S>",
    "<S> first , we present a simple proof of the fact that in concurrent reachability games , for all @xmath0 , memoryless @xmath1-optimal strategies exist . </S>",
    "<S> a memoryless strategy is independent of the history of plays , and an @xmath1-optimal strategy achieves the objective with probability within @xmath1 of the value of the game . </S>",
    "<S> in contrast to previous proofs of this fact , our proof is more elementary and more combinatorial . </S>",
    "<S> second , we present a strategy - improvement ( a.k.a .  </S>",
    "<S> policy - iteration ) algorithm for concurrent games with reachability objectives . </S>",
    "<S> we then present a strategy - improvement algorithm for concurrent games with safety objectives . </S>",
    "<S> our algorithms yield sequences of player-1 strategies which ensure probabilities of winning that converge monotonically to the value of the game . </S>",
    "<S> our result is significant because the strategy - improvement algorithm for safety games provides , for the first time , a way to approximate the value of a concurrent safety game _ from below_. previous methods could approximate the values of these games only from one direction , and as no rates of convergence are known , they did not provide a practical way to solve these games .    _ </S>",
    "<S> keywords . </S>",
    "<S> _ concurrent games ; reachability and safety objectives ; strategy improvement algorithms . </S>"
  ]
}