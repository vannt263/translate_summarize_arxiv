{
  "article_text": [
    "in many applications , one obtains measurements @xmath0 for which the response @xmath1 is related to @xmath2 via some mixture of known kernel functions @xmath3 , and the goal is to recover the mixture parameters @xmath4 and their associated weights :    @xmath5    where @xmath3 is a known kernel function parameterized by @xmath6 , and @xmath7 are model parameters to be estimated , @xmath8 are unknown nonnegative weights to be estimated , and @xmath9 is additive noise . the number of components @xmath10 is also unknown , hence , this is a _",
    "nonparametric model_. one example of a domain in which mixture models are useful is the analysis of data from diffusion - weighted magnetic resonance imaging ( dwi ) .",
    "this biomedical imaging technique is sensitive to the direction of water diffusion within millimeter - scale voxels in the human brain _ in vivo_. water molecules freely diffuse along the length of nerve cell axons , but is restricted by cell membranes and myelin along directions orthogonal to the axon s trajectory .",
    "thus , dwi provides information about the microstructural properties of brain tissue in different locations , about the trajectories of organized bundles of axons , or fascicles within each voxel , and about the connectivity structure of the brain .",
    "mixture models are employed in dwi to deconvolve the signal within each voxel with a kernel function , @xmath11 , assumed to represent the signal from every individual fascicle [ 1 , 2 ] ( figure [ fig : illus1]b ) , and @xmath12 provide an estimate of the fiber orientation distribution function ( fodf ) in each voxel , the direction and volume fraction of different fascicles in each voxel . in other applications of",
    "mixture modeling these parameters represent other physical quantities .",
    "for example , in chemometrics , @xmath6 represents a chemical compound and @xmath13 its spectra . in this paper",
    ", we focus on the application of mixture models to the data from dwi experiments and simulations of these experiments .",
    "hereafter , we restrict our attention to the use of squared - error loss ; resulting in penalized least - squares problem @xmath14    minimization problems of the form can be found in the signal deconvolution literature and elsewhere : some examples include super - resolution in imaging [ 6 ] , entropy estimation for discrete distributions [ 7 ] , x - ray diffraction [ 8 ] , and neural spike sorting [ 3 ] . here",
    ", @xmath15 is a _ convex _",
    "penalty function of @xmath16 .",
    "examples of such penalty functions given in section [ s : regularization ] ; a formal definition of convexity in the nonparametric setting can be found in the supplementary material , but will not be required for the results in the paper .",
    "technically speaking , the objective function is convex in @xmath17 , but since its domain is of infinite dimensionality , for all practical purposes is a nonconvex optimization problem .",
    "one can consider fixing the number of components in advance , and using a descent method ( with random restarts ) to find the best model of that size .",
    "alternatively , one could use a stochastic search method , such as simulated annealing or mcmc [ 9 ] , to estimate the size of the model and the model parameters simultaneously .",
    "however , as one begins to consider fitting models with increasing number of components @xmath18 and of high dimensionality , it becomes increasingly difficult to apply these approaches [ 3 ] .",
    "hence a common approach to obtaining an approximate solution to is to limit the search to a discrete grid of candidate parameters @xmath19 .",
    "the estimated weights and parameters are then obtained by solving an optimization problem of the form @xmath20 where @xmath21 has the @xmath22th column @xmath23 , where @xmath24 is defined by @xmath25 .",
    "examples applications of this non - negative least - squares - based approach ( nnls ) include [ 10 ] and [ 1 , 2 , 7 ] .",
    "in contrast to descent based methods , which get trapped in local minima , nnls is guaranteed to converge to a solution which is within @xmath26 of the global optimum , where @xmath26 depends on the scale of discretization . in some cases",
    ", nnls will predict the signal accurately ( with small error ) , but the parameters resulting will still be erroneous .",
    "figure [ fig : illus1 ] illustrates the worst - case scenario where discretization is misaligned relative to the true parameters / kernels that generated the signal .    .",
    "this problem arises in deconvolving multi - dimensional signals , such as the 3d dwi signal ( b ) , as well . here",
    ", the dwi signal in an individual voxel is presented as a 3d surface ( top ) .",
    "this surface results from a mixture of signals arising from the fascicles presented on the bottom passing through this single ( simulated ) voxel . due to the signal generation process",
    ", the kernel of the diffusion signal from each one of the fascicles has a minimum at its center , resulting in dimples in the diffusion signal in the direction of the peaks in the fascicle orientation distribution function . ]    in an effort to improve the discretization error of nnls , ekanadham et al [ 3 ] introduced continuous basis pursuit ( cbp ) .",
    "cbp is an extension of nonnegative least squares in which the points on the discretization grid @xmath27 can be continuously moved within a small distance ; in this way , one can reach any point in the parameter space .",
    "but instead of computing the actual kernel functions for the perturbed parameters , cbp uses linear approximations , e.g. obtained by taylor expansions .",
    "depending on the type of approximation employed , cbp may incur large error .",
    "the developers of cbp suggest solutions for this problem in the one - dimensional case , but these solutions can not be used for many applications of mixture models ( e.g dwi ) .",
    "the computational cost of both nnls and cbp scales exponentially in the dimensionality of the parameter space .",
    "in contrast , using stochastic search methods or descent methods to find the global minimum will generally incur a computational cost scaling which is exponential in the sample size times the parameter space dimensions .",
    "thus , when fitting high - dimensional mixture models , practitioners are forced to choose between the discretization errors inherent to nnls , or the computational difficulties in the descent methods . we will show that our boosting approach to mixture models combines the best of both worlds : while it does not suffer from discretization error , it features computational tractability comparable to nnls and cbp .",
    "we note that for the specific problem of super - resolution , cndes derived a deconvolution algorithm which finds the global minimum of without discretization error and proved that the algorithm can recover the true parameters under a minimal separation condition on the parameters [ 6 ] .",
    "however , we are unaware of an extension of this approach to more general applications of mixture models .",
    "the model appears in an entirely separate context , as the model for learning a regression function as an ensemble of weak learners @xmath13 , or boosting [ 4 ] .",
    "however , the problem of fitting a mixture model and the problem of fitting an ensemble of weak learners have several important differences . in the case of learning an ensemble ,",
    "the family @xmath28 can be freely chosen from a universe of possible weak learners , and the only concern is minimizing the prediction risk on a new observation .",
    "in contrast , in the case of fitting a mixture model , the family @xmath28 is specified by the application . as a result , boosting algorithms , which were derived under the assumption that @xmath28 is a suitably flexible class of weak learners , generally perform poorly in the signal deconvolution setting , where the family @xmath28 is inflexible . in the context of regression , @xmath29boost , proposed by buhlmann",
    "et al [ 4 ] produces a path of ensemble models which progressively minimize the sum of squares of the residual .",
    "@xmath29boost fits a series of models of increasing complexity .",
    "the first model consists of the single weak learner @xmath24 which best fits @xmath1 .",
    "the second model is formed by finding the weak learner with the greatest correlation to the residual of the first model , and adding the new weak learner to the model , without changing any of the previously fitted weights . in this way",
    "the size of the model grows with the number of iterations : each new learner is fully fit to the residual and added to the model .",
    "but because the previous weights are never adjusted , @xmath29boost fails to converge to the global minimum of in the mixture model setting , producing suboptimal solutions . in the following section ,",
    "we modify @xmath29boost for fitting mixture models .",
    "we refer to the resulting algorithm as _ elastic basis pursuit_.",
    "our proposed procedure for fitting mixture models consists of two stages . in the first stage , we transform a @xmath30 penalized problem to an equivalent _ non regularized _ least squares problem . in the second stage , we employ a modified version of @xmath29boost , _ elastic basis pursuit _ , to solve the transformed problem .",
    "we will present the two stages of the procedure , then discuss our fast convergence results .      for most mixture problems",
    "it is beneficial to apply a @xmath30-norm based penalty , by using a modified input @xmath31 and kernel function family @xmath32 , so that @xmath33    we will use our modified @xmath29boost algorithm to produce a path of solutions for objective function on the left side , which results in a solution path for the penalized objective function .    for example , it is possible to embed the penalty latexmath:[$p_{\\boldsymbol{\\theta}}(w ) =    obtained by using the penalty function latexmath:[$p_{\\boldsymbol{\\theta}}(w ) =    using the usual @xmath30 penalty @xmath35 . the penalty @xmath36 is implemented by using the transformed input : @xmath37 and using modified kernel vectors @xmath38 .",
    "other kinds of regularization are also possible , and are presented in the _",
    "supplemental material_.      motivated by the connection between boosting and mixture modelling , we consider application of @xmath29boost to solve the transformed problem ( the left side of ) .",
    "again , we reiterate the _ nonparametric _ nature of the model space ; by minimizing , we seek to find the model with _ any _ number of components which minimizes the residual sum of squares .",
    "in fact , given appropriate regularization , this results in a well - posed problem . in each iteration of our algorithm a subset of the parameters , @xmath39 are considered for adjustment . following lawson and hanson",
    "[ 11 ] , we refer to these as the _ active set_. as stated before , @xmath29boost can only grow the active set at each iteration , converging to inaccurate models .",
    "our solution to this problem is to modify @xmath29boost so that it grows _ and _ contracts the active set as needed ; hence we refer to this modification of the @xmath29boost algorithm as _",
    "elastic basis pursuit_. the key ingredient for any boosting algorithm is an oracle for fitting a weak learner : that is , a function @xmath40 which takes a residual as input and returns the parameter @xmath6 corresponding to the kernel @xmath32 most correlated with the residual .",
    "ebp takes as inputs the oracle @xmath40 , the input vector @xmath31 , the function @xmath32 , and produces a path of solutions which progressively minimize . to initialize the algorithm",
    ", we use nnls to find an initial estimate of @xmath17 . in the @xmath41th iteration of the boosting algorithm ,",
    "let @xmath42 be residual from the previous iteration ( or the nnls fit , if @xmath43 ) .",
    "the algorithm proceeds as follows    1 .",
    "call the oracle to find @xmath44 , and add @xmath45 to the active set @xmath39 .",
    "2 .   refit the weights @xmath46 , using nnls , to solve : @xmath47 where @xmath48 is the matrix formed from the regressors in the active set , @xmath32 for @xmath49 .",
    "this yields the residual @xmath50 .",
    "3 .   prune the active set @xmath39 by removing any parameter @xmath6 whose weight is zero , and update the weight vector @xmath46 in the same way .",
    "this ensures that the active set @xmath39 remains sparse in each iteration .",
    "let @xmath51 denote the values of @xmath17 at the end of this step of the iteration .",
    "stopping may be assessed by computing an estimated prediction error at each iteration , via an independent validation set , and stopping the algorithm early when the prediction error begins to climb ( indicating overfitting ) .",
    "psuedocode and matlab code implementing this algorithm can be found in the supplement .    in the boosting context",
    ", the property of refitting the ensemble weights in every iteration is known as the _ totally corrective _",
    "property ; lpboost [ 12 ] is a well - known example of a totally corrective boosting algorithm . while we derived ebp as a totally corrective variant of @xmath29boost , one",
    "could also view ebp as a generalization of the classical lawson - hanson ( lh ) algorithm [ 11 ] for solving nonnegative least - squares problems . given mild regularity conditions and appropriate regularization",
    ", elastic basis pursuit can be shown to deterministically converge to the global optimum : we can bound the objective function gap in the @xmath52th iteration by @xmath53 , where @xmath54 is an explicit constant ( see [ s : theory ] ) . to our knowledge ,",
    "fixed iteration guarantees are unavailable for all other methods of comparable generality for fitting a mixture with an unknown number of components .      _",
    "( detailed proofs can be found in the supplementary material . ) _    for our convergence results to hold , we require an oracle function @xmath55 which satisfies    @xmath56    for some fixed @xmath57 . our proofs can also be modified to apply given a stochastic oracle that satisfies with fixed probability @xmath58 for every input @xmath59 . recall that @xmath31 denotes the transformed input , @xmath32 the transformed kernel and @xmath60 the dimensionality of @xmath31 .",
    "we assume that the parameter space @xmath61 is compact and that @xmath32 , the transformed kernel function , is continuous in @xmath6 .",
    "furthermore , we assume that either @xmath30 regularization is imposed , _ or _",
    "the kernels satisfy a positivity condition , i.e. @xmath62 for @xmath63 .",
    "proposition 1 states that these conditions imply the existence of a maximally saturated model @xmath64 of size @xmath65 with residual @xmath66 .",
    "the existence of such a saturated model , in conjunction with existence of the oracle @xmath40 , enables us to state fixed - iteration guarantees on the precision of ebp , which implies asymptotic convergence to the global optimum . to do so",
    ", we first define the quantity @xmath67 , see above .",
    "proposition 2 uses the fact that the residuals @xmath68 are orthogonal to @xmath69 , thanks to the nnls fitting procedure in step 2 .",
    "this allows us to bound the objective function gap in terms of @xmath70 .",
    "proposition 3 uses properties of the oracle @xmath40 to lower bound the progress per iteration in terms of @xmath70 .    * proposition 2 * _ assume the conditions of proposition 1 .",
    "take saturated model @xmath71 .",
    "then defining _ @xmath72 _ the @xmath52th residual of the ebp algorithm @xmath68 can be bounded in size by _",
    "@xmath73    in particular , whenever @xmath74 converges to 0 , the algorithm converges to the global minimum .",
    "* proposition 3 * _ assume the conditions of proposition 1 . then @xmath75 for @xmath76 defined above in .",
    "this implies that the sequence @xmath77 is decreasing .",
    "_    combining propositions 2 and 3 yields our main result for the non - asymptotic convergence rate .",
    "* proposition 4 * _ assume the conditions of proposition 1 . then for all @xmath78 , @xmath79 where @xmath80 for @xmath81 defined in",
    "_    hence we have characterized the non - asymptotic convergence of ebp at rate @xmath82 with an explicit constant , which in turn implies asymptotic convergence to the global minimum .",
    "to demonstrate the utility of ebp in a real - world application , we used this algorithm to fit mixture models of dwi .",
    "different approaches are taken to modeling the dwi signal .",
    "the classical diffusion tensor imaging ( dti ) model [ 5 ] , which is widely used in applications of dwi to neuroscience questions , is not a mixture model .",
    "instead , it assumes that diffusion in the voxel is well approximated by a 3-dimensional gaussian distribution .",
    "this distribution can be parameterized as a rank-2 tensor , which is expressed as a 3 by 3 matrix . because the dwi measurement has antipodal symmetry , the tensor matrix is symmetric , and only 6 independent parameters need to be estimated to specify it .",
    "dti is accurate in many places in the white matter , but its accuracy is lower in locations in which there are multiple crossing fascicles of nerve fibers . in addition , it should not be used to generate estimates of connectivity through these locations .",
    "this is because the peak of the fiber orientation distribution function ( fodf ) estimated in this location using dti is not oriented towards the direction of any of the crossing fibers .",
    "instead , it is usually oriented towards an intermediate direction ( figure [ fig : results_sim]b ) . to address these challenges ,",
    "mixture models have been developed , that fit the signal as a combination of contributions from fascicles crossing through these locations .",
    "these models are more accurate in fitting the signal",
    ". moreover , their estimate of the fodf is useful for tracking the fascicles through the white matter for estimates of connectivity .",
    "however , these estimation techniques either use different variants of nnls , with a discrete set of candidate directions [ 2 ] , or with a spherical harmonic basis set [ 1 ] , or use stochastic algorithms [ 9 ] . to overcome the problems inherent in these techniques ,",
    "we demonstrate here the benefits of using ebp to the estimation of a mixture models of fascicles in dwi .",
    "we start by demonstrating the utility of ebp in a simulation of a known configuration of crossing fascicles .",
    "then , we demonstrate the performance of the algorithm in dwi data .",
    "the dwi measurements for a single voxel in the brain are @xmath83 for directions @xmath84 on the three dimensional unit sphere , given by @xmath85,\\ ] ] the kernel functions @xmath86 each describe the effect of a single fascicle traversing the measurement voxel on the diffusion signal , well described by the stejskal - tanner equation [ 13 ] . because of the non - negative nature of the mri signal ,",
    "@xmath87 is generated from a rician distribution [ 14 ] .",
    "where @xmath88 is a scalar quantity determined by the experimenter , and related to the parameters of the measurement ( the magnitude of diffusion sensitization applied in the mri instrument ) .",
    "@xmath89 is a positive definite quadratic form , which is specified by the direction along which the fascicle represented by @xmath90 traverses the voxel and by additional parameters @xmath91 and @xmath92 , corresponding to the axial and radial diffusivity of the fascicle represented by @xmath90 .",
    "the oracle function @xmath40 is implemented by newton - raphson with random restarts . in each iteration of the algorithm ,",
    "the parameters of @xmath89 ( direction and diffusivity ) are found using the oracle function , @xmath93 , using gradient descent on @xmath59 , the current residuals . in each iteration ,",
    "the set of @xmath90 is shrunk or expanded to best match the signal .            in a simulation with a complex configuration of fascicles , we demonstrate that accurate recovery of the true fodf can be achieved . in our simulation model , we take @xmath94 , and generate @xmath95 as uniformly distributed vectors on the unit sphere and weights @xmath96 as i.i.d . uniformly distributed on the interval @xmath97 $ ] .",
    "each @xmath98 is associated with a @xmath99 between 0.5 and 2 , and setting @xmath100 to 0 .",
    "we consider the signal in 150 measurement vectors distributed on the unit sphere according to an electrostatic repulsion algorithm .",
    "we partition the vectors into a training partition and a test partition to minimize the maximum angular separation in each partition .",
    "@xmath101 we generate a signal    we use cross - validation on the training set to fit nnls with varying l1 regularization parameter @xmath102 , using the regularization penalty function : @xmath103 .",
    "we choose this form of penalty function because we interpret the weights @xmath46 as comprising partial volumes in the voxel ; hence @xmath102 represents the total volume of the voxel weighted by the isotropic component of the diffusion .",
    "we fix the regularization penalty parameter @xmath104 .",
    "the estimated fodfs and predicted signals are obtained by three algorithms : dti , nnls , and ebp .",
    "each algorithm is applied to the training set ( 75 directions ) , and error is estimated , relative to a prediction on the test set ( 75 directions ) .",
    "the latter two methods ( nnls , ebp ) use the regularization parameters @xmath105 and the @xmath102 chosen by cross - validated nnls .",
    "figure [ fig : polar ] illustrates the first two iterations of ebp applied to these simulated data .",
    "the estimated fodf are compared to the true fodf by the antipodally symmetrized earth mover s distance ( emd ) [ 15 ] in each iteration .",
    "figure [ fig : iterations ] demonstrates the progress of the internal state of the ebp algorithm in many repetitions of the simulation . in the simulation results ( figure [ fig : results_sim ] ) , ebp clearly reaches a more accurate solution than dti , and a sparser solution than nnls .    ) .",
    "ebp estimates a much sparser solution with weights concentrated around the true peaks ( d ) . ]    the same procedure",
    "is used to fit the three models to dwi data , obtained at 2x2x2 @xmath106 , at a b - value of 4000 @xmath107 . in the these data ,",
    "the true fodf is not known .",
    "hence , only test prediction error can be obtained .",
    "we compare rmse of prediction error between the models in a region of interest ( roi ) in the brain containing parts of the corpus callosum , a large fiber bundle that contains many fibers connecting the two hemispheres , as well as the centrum semiovale , containing multiple crossing fibers ( figure [ fig : results_data ] ) .",
    "nnls and ebp both have substantially reduced error , relative to dti .",
    "we developed an algorithm to model multi - dimensional mixtures .",
    "this algorithm , _ elastic basis pursuit _ ( ebp ) , is a combination of principles from boosting , and principles from the lawson - hanson _ active set _ algorithm .",
    "it fits the data by iteratively generating and testing the match of a set of candidate kernels to the data .",
    "kernels are added and removed from the set of candidates as needed , using a totally corrective backfitting step , based on the match of the entire set of kernels to the data at each step .",
    "we show that the algorithm reaches the global optimum , with fixed iteration guarantees .",
    "thus , it can be practically applied to separate a multi - dimensional signal into a sum of component signals .",
    "for example , we demonstrate how this algorithm can be used to fit diffusion - weighted mri signals into nerve fiber fascicle components .",
    "the authors thank brian wandell and eero simoncelli for useful discussions .",
    "cz was supported through an nih grant 1t32gm096982 to robert tibshirani and chiara sabatti , ar was supported through nih fellowship f32-ey022294 .",
    "fp was supported through nsf grant bcs1228397 to brian wandell      [ 1 ] tournier j - d , calamante f , connelly a ( 2007 ) . robust determination of the fibre orientation distribution in diffusion mri : non - negativity constrained super - resolved spherical deconvolution .",
    "_ neuroimage _ 35:145972    [ 2 ] dellacqua f , rizzo g , scifo p , clarke ra , scotti g , fazio f ( 2007 ) . a model - based deconvolution approach to solve fiber crossing in diffusion - weighted mr imaging .",
    "_ ieee trans biomed eng _",
    "54:46272    [ 3 ] ekanadham c , tranchina d , and simoncelli e. ( 2011 ) .",
    "recovery of sparse translation - invariant signals with continuous basis pursuit .",
    "_ ieee transactions on signal processing _",
    "( 59):4735 - 4744 .",
    "[ 4 ] bhlmann p , yu b ( 2003 ) .",
    "boosting with the l2 loss : regression and classification .",
    "_ jasa _ , 98(462 ) , 324 - 339 .",
    "[ 5 ] basser , p .",
    "j. , mattiello , j. and le - bihan , d. ( 1994 ) .",
    "mr diffusion tensor spectroscopy and imaging . _ biophysical journal _ , 66:259 - 267 .    [ 6 ]",
    "cands , e. j. , and fernandezgranda , c. ( 2013 ) . towards a mathematical theory of superresolution .",
    "_ communications on pure and applied mathematics . _",
    "[ 7 ] valiant , g. , and valiant , p. ( 2011 ,",
    "june ) . estimating the unseen : an n / log ( n)-sample estimator for entropy and support size , shown optimal via new clts . in _ proceedings of the 43rd annual acm symposium on theory of computing _",
    "685 - 694 ) .",
    "[ 8 ] snchez - bajo , f. , and cumbrera , f. l. ( 2000 ) .",
    "deconvolution of x - ray diffraction profiles by using series expansion",
    ". _ journal of applied crystallography _ , 33(2 ) , 259 - 266 .    [ 9 ]",
    "behrens tej , berg hj , jbabdi s , rushworth mfs , and woolrich mw ( 2007 ) .",
    "probabilistic diffusion tractography with multiple fiber orientations : what can we gain ?",
    "_ neuroimage _",
    "( 34):144 - 45 .",
    "[ 10 ] bro , r. , and de jong , s. ( 1997 ) . a fast non - negativity - constrained least squares algorithm .",
    "_ journal of chemometrics _",
    ", 11(5 ) , 393 - 401 .",
    "[ 11 ] lawson cl , and hanson rj .",
    "_ solving least squares problems_. siam .    [ 12 ] demiriz , a. , bennett , k. p. , and shawe - taylor , j. ( 2002 ) .",
    "linear programming boosting via column generation .",
    "_ machine learning _ , 46(1 - 3 ) , 225 - 254 .",
    "[ 13 ] stejskal eo , and tanner je .",
    "spin diffusion measurements : spin echoes in the presence of a time - dependent gradient field .",
    "_ j chem phys_(42):288 - 92 .",
    "[ 14 ] gudbjartsson , h. , and patz , s. ( 1995 ) . the rician distribution of noisy mr data . _ magn reson med_. 34 : 910914 .    [ 15 ] rubner , y. , tomasi , c. guibas , l.j .",
    "the earth mover s distance as a metric for image retrieval .",
    "_ intl j. computer vision _",
    ", 40(2 ) , 99 - 121 .",
    "continuous basis pursuit , introduced by ekanadham et al . [ 3 ] , can be viewed as an extension of nonnegative least squares where we are given the liberty of perturbing the points on the discretization grid @xmath108 to adjusted versions @xmath109 where the perturbations are constrained to lie within voronoi cells @xmath110 generated by @xmath108 .",
    "the idea of cbp is to linearly approximate the resulting kernel functions @xmath111 . in particular , in first - order cbp ( focbp )",
    ", one uses the approximation @xmath112 where @xmath89 is the dimensionality of the parameter space . defining @xmath113 and @xmath114 for @xmath115 , and convex constraint sets @xmath116 one writes the focbp objective function as @xmath117 subject to @xmath118 yielding estimates @xmath119 , @xmath120 @xmath121 where @xmath122 is the @xmath123th standard basis vector .",
    "ekanadham et al . suggested using solvers for semidefinite programs ( sdp ) to solve instances of cbp problems , like the objective function above .",
    "however , we found that focbp can be transformed into a nonnegative least squares problem , generally resulting in speedups and improvements in stability .",
    "the key observation is that any pertubed parameter @xmath124 can be represented as a positive linear combination of the finite set of vertices of @xmath125 , @xmath126 . yet",
    ", this implies that the corresponding approximated kernel function @xmath127 can also be represented as a positive linear combination of @xmath128 .        before discussing our proposed method , true continuous basis pursuit",
    ", we discuss the unique properties of the lawson - hanson algorithm [ 11 ] for solving nonnegative least squares problems of the form @xmath135 where @xmath136 is a @xmath137 matrix , in the special case of @xmath136 with nonnegative entries .",
    "the algorithm begins with an active set @xmath138 initialized to the null set and estimate @xmath139 intialized to 0 , and uses a tolerance @xmath140 . letting @xmath141 represent the columns of @xmath136 corresponding to the indices included in @xmath138 and @xmath142 be the entries of @xmath139 corresponding to the indices of @xmath138 .",
    "the lh algorithm is as follows [ charles will summarize ]      1 .",
    "initialize set @xmath138 of indices to the empty set .",
    "2 .   intiialize @xmath139 to be a @xmath143 vector of zeroes 3 .",
    "initialize @xmath144 4 .",
    "run main loop 5 .",
    "return @xmath139 , the solution to the least - squares problem      1 .",
    "_ while _ @xmath145 : 2 .",
    "letting @xmath22 be the smallest index such that @xmath146 , set @xmath147 3 .",
    "let @xmath148 be a @xmath143 vector of zeros .",
    "4 .   set @xmath149 5 .",
    "begin * inner loop*. 6 .",
    "set @xmath150 . 7",
    ".   set @xmath151 8 .",
    "_ end while _        since the lh algorithm was proposed in 1974 , a number of improvements have been proposed for solving large - scale nonnegative least squares problem .",
    "efron s least - angle procedure is especially suitable for solving the lasso - regularized nnls problem @xmath159 but can also be applied to the original nnls problem .",
    "kim , sra and dhillon proposed an interior - point based method for solving nnls problems using conjugate gradients .",
    "potluru propose using coordinate descent to solve nnls .",
    "the fista algorithm of beck can also be modified to solve nnls .",
    "firstly , the @xmath139 vector remains sparse in every iteration of the lh algorithm , even for noisy data .",
    "this means that the lh algorithm gains a substantial advantage over coordinate descent methods by computing the true least - sqaures solution for the current active set .",
    "secondly , the nature of the basis set renders gradient - descent based approaches , like the kim sra dhillon algorithm , much less effective .",
    "due to the high degree of collinearity in the basis set , the function has high curvature in the direction of the gradient , which often reduces the maximum step size at each iteration to below working precision .",
    "fourthly , the geometry of the basis set , which resembles a high - dimensional connected , curved surface with a spike at @xmath161 , poses special difficulties for efron s lars algorithm , which aggresively adds variables to the active set as it continuously adjusts the coefficients of the solution vector .",
    "the lars algorithm is hampered by the frequency at which the active set must change along the solution path .",
    "on the other hand , since the lars algorithm recovers the entire l1 regularized solution path , it may still be useful for tuning the l1 regularization parameter .",
    "* proof*. form the matrix @xmath170 .",
    "then @xmath171 for any @xmath172 .",
    "but if we minimize @xmath173 over @xmath139 nonnegative , we can find a solution @xmath174 with @xmath175 or fewer nonzero entries , as proved in lawson and hanson [ 7 ] .",
    "taking @xmath176 to be the nonnegative entries of @xmath174 and taking @xmath177 to be the corresponding parameters @xmath6 , we have @xmath178 .",
    "@xmath179          then @xmath169 is continuous .",
    "since the squared norm of any vector is nonnegative , we know that @xmath189 . by the compactness of @xmath185 , there exists @xmath190 such that @xmath191 .",
    "take @xmath192 and take @xmath193 to be the sequence of nonnegative entries of @xmath46 , and @xmath194 to be the sequence of nonnegative entries of @xmath39 to complete the proof .",
    "@xmath179    * proposition . *",
    "_ suppose there exists @xmath71 satisfying .",
    "then for any oracle @xmath40 satisfying condition there exists @xmath195 and @xmath196 such that for all iterations @xmath197 of the lh algorithm , we have _ @xmath198    * proof . * for @xmath199 define @xmath200 first we show that @xmath70 produces an upper bound on @xmath201 .",
    "define @xmath202 note that @xmath203 is jointly convex in @xmath204 , and verify that @xmath205 and @xmath206 .",
    "further note that @xmath207 due to the fact that the residual @xmath208 is orthogonal to the columns of @xmath209 ( see [ 7 ] ) .",
    "meanwhile , note that @xmath210 , which implies @xmath211 now due to the convexity of @xmath203 , we have @xmath212 where @xmath213 the next major step is to see that @xmath214 which implies @xmath215 include @xmath216 by also all of the columns of @xmath209 for which @xmath217 is nonzero .",
    "next , is obtained by an application of the pythagorean theorem , and by applying the definitions of @xmath70 and the condition on @xmath40 .",
    "finally , follows from observing that @xmath218 is nondecreasing in @xmath52 , hence @xmath219 .",
    "but since @xmath221 , this implies that @xmath222 is convergent .",
    "hence , there exists a constant @xmath223 , @xmath140 and @xmath224 such that for all @xmath225 , @xmath226 since we bounded the objective function gap in terms of @xmath70 in , this yields the desired result ."
  ],
  "abstract_text": [
    "<S> diffusion - weighted magnetic resonance imaging ( dwi ) and fiber tractography are the only methods to measure the structure of the white matter in the living human brain . </S>",
    "<S> the diffusion signal has been modelled as the combined contribution from many individual fascicles of nerve fibers passing through each location in the white matter . </S>",
    "<S> typically , this is done via _ basis pursuit _ , but estimation of the exact directions is limited due to discretization [ 1 , 2 ] . the difficulties inherent in modeling dwi data are shared by many other problems involving fitting non - parametric mixture models . </S>",
    "<S> ekanadaham et al . </S>",
    "<S> [ 3 ] proposed an approach , _ continuous basis pursuit _ , to overcome discretization error in the 1-dimensional case ( e.g. , spike - sorting ) . here </S>",
    "<S> , we propose a more general algorithm that fits mixture models of any dimensionality without discretization . </S>",
    "<S> our algorithm uses the principles of l2-boost [ 4 ] , together with refitting of the weights and pruning of the parameters . </S>",
    "<S> the addition of these steps to l2-boost both accelerates the algorithm and assures its accuracy . </S>",
    "<S> we refer to the resulting algorithm as _ </S>",
    "<S> elastic basis pursuit _ , or ebp , since it expands and contracts the active set of kernels as needed . </S>",
    "<S> we show that in contrast to existing approaches to fitting mixtures , our boosting framework ( 1 ) enables the selection of the optimal bias - variance tradeoff along the solution path , and ( 2 ) scales with high - dimensional problems . in simulations of dwi </S>",
    "<S> , we find that ebp yields better parameter estimates than a non - negative least squares ( nnls ) approach , or the standard model used in dwi , the tensor model , which serves as the basis for diffusion tensor imaging ( dti ) [ 5 ] . </S>",
    "<S> we demonstrate the utility of the method in dwi data acquired in parts of the brain containing crossings of multiple fascicles of nerve fibers .    </S>",
    "<S> = 1 </S>"
  ]
}