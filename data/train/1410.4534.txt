{
  "article_text": [
    "extreme value theory ( evt ) can be seen as a branch of probability theory which studies the stochastic behaviour of extremes associated to a set of random variables with a common probability distribution . in recent years",
    ", several statistical techniques capable of better quantifying the probability of occurence of rare events have grown in popularity , especially in areas such as finance , actuaries and environmental sciences ( see for example , , ) . for a good review of both theory and interesting applications of evt",
    "the main reference is still .",
    "natural phenomena like river flows , wind speed and rain are subject to extreme values that can imply in great material and financial losses . financial markets where large amounts of money invested can have an impact in the economy of a country need to have their risks of large losses and gains quantified . in risk analysis , estimating future losses by modelling events associated to default is of fundamental importance . in insurance , the potencial risk of high value claims needs to be quantified and associated to possible catastrofic events due to the large amount of money involved in payments .",
    "the usual approach for the analysis of extreme data is based on the generalized extreme value ( gev ) distribution which distribution function is given by ,    @xmath0    where @xmath1 , @xmath2 and @xmath3 are location , scale and shape parameters respectively .",
    "the @xmath4 sign denotes the positive part of the argument .",
    "we use the notation @xmath5 .",
    "the value of the shape parameter @xmath3 defines the tail behaviour of the distribution . if @xmath6 the distribution is defined for @xmath7 and is called a gumbel distribution ( exponentially decaying tail ) .",
    "if @xmath8 the distribution is defined for values @xmath9 , has a lower bound and is called a frchet distribution ( slowly decaying tail ) .",
    "if @xmath10 the distribution is defined for values @xmath11 , has an upper bound and is called a negative weibull distribution ( upper bounded tail ) .",
    "the density function of the gev distribution is given by , @xmath12 which is illustrated in figure [ fig1 ] for @xmath13 , @xmath14 and @xmath15 .",
    "+    figure [ fig1 ] about here .",
    "+    now suppose that we have observed data @xmath16 and assume that they are realizations from independent and identically distributed random variables @xmath17 with @xmath18 .",
    "we wish to make inferences about the unknown parameters @xmath1 , @xmath2 and @xmath3 .",
    "the likelihood function is given by , @xmath19^{-1/\\xi-1 } \\exp\\left\\{-\\sum_{i=1}^n\\left(1+\\xi~\\dfrac{y_i-\\mu}{\\sigma}\\right)^{-1/\\xi}\\right\\}\\ ] ] for @xmath20 when @xmath10 and for @xmath21 when @xmath8 .",
    "otherwise the likelihood function is undefined . a bayesian analysis",
    "is then carried out by assigning prior distributions on @xmath1 , @xmath2 and @xmath3 .",
    "simulation methods , in particular markov chain monte carlo ( mcmc ) methods , are now routinely employed to produce a sample of simulated values from the posterior distribution which can in turn be used to make inferences about the parameters . in gev models ,",
    "the random walk metropolis algorithm is usually employed where a proposal distribution must be chosen and tuned , for which a poor choice will considerably delay convergence towards the posterior distribution .",
    "our main motivation to investigate alternative algorithms is computational and we hope that our findings are useful for the applied user of this class of models .    in the next section",
    "we describe an alternative algorithm to generate these posterior samples in a much more efficient way .",
    "this is compared with the traditional mcmc methods in section [ sec : app ] in terms of computational efficiency through a real dataset and a simulation study . in section",
    "[ sec : ar ] a time series ingredient is included in the model to analyse time series of extreme values .",
    "some final comments are given in section [ sec : conclusion ] .",
    "hamiltonian monte carlo ( hmc ) was originaly proposed by for simulating molecular dynamics under the name of hybrid monte carlo . in what follows",
    "we present the hmc method in a compact form which will be used in the context of gev models .",
    "the reader is referred to for an up to date review of theoretical and practical aspects of hamiltonian monte carlo methods .",
    "let @xmath22 denote a @xmath23-dimensional vector of parameters , @xmath24 denote the posterior density of @xmath25 and @xmath26 denote a vector of auxiliary parameters independent of @xmath25 and distributed as @xmath27 .",
    "if @xmath25 is interpreted as the position of a particle and @xmath28 describes its potential energy while @xmath29 is the momentum with kinetic energy @xmath30 then the total energy of a closed system is the hamiltonian function ,    @xmath31 where @xmath32 .",
    "+ the ( unormalized ) joint density of @xmath33 is then given by , @xmath34 .",
    "\\end{aligned}\\ ] ]    for continuous time @xmath35 , the deterministic evolution of a particle that keeps the total energy constant is given by the hamiltonian dynamics equations , @xmath36 where @xmath37 is the gradient of @xmath38 with respect to @xmath25 .",
    "so , the idea is that introducing the auxiliary variables @xmath29 and using the gradients will lead to a more efficient exploration of the parameter space .",
    "+ however these differential equations can not be solved analytically and numerical methods are required .",
    "one such method is the strmer - verlet ( or leapfrog ) numerical integrator ( ) which discretizes the hamiltonian dynamics as the following steps , @xmath39 for some user specified small step - size @xmath40 . after a given number of time steps this results in a proposal @xmath41 . in appendix",
    "[ appendix ] we provide details on the required expressions of partial derivatives for hmc .    a metropolis acceptance probability",
    "must then be employed to correct the error introduced by this discretization and ensure convergence to the invariant distribution . since the joint distribution of @xmath33 is our target distribution , the transition to a new proposed value @xmath41",
    "is accepted with probability , @xmath42     & = &    \\min\\left[\\frac{f({\\hbox{\\boldmath$\\theta$}}^*,{\\hbox{\\boldmath$p$}}^*)}{f({\\hbox{\\boldmath$\\theta$}},{\\hbox{\\boldmath$p$}})},1\\right]\\\\\\\\    & = &    \\min\\left[\\exp[h({\\hbox{\\boldmath$\\theta$}},{\\hbox{\\boldmath$p$ } } ) - h({\\hbox{\\boldmath$\\theta$}}^*,{\\hbox{\\boldmath$p$}}^*)],1\\right].\\end{aligned}\\ ] ]    in the distribution of the auxiliary parameters , @xmath43 is a symmetric positive definite mass matrix which is typically diagonal with constant elements , i.e.@xmath44 .",
    "the hmc algorithm in its simplest form taking @xmath45 is given by ,    1 .",
    "give an initial position @xmath46 and set @xmath47 , 2 .",
    "[ step2 ] draw @xmath48 and @xmath49 , 3 .",
    "set @xmath50 and @xmath51 , 4 .",
    "repeat the strmer - verlag solution @xmath52 times,.2 cm * @xmath53 * @xmath54 * @xmath53 + .2 cm 5 .",
    "set @xmath55 and @xmath56 , 6 .",
    "compute @xmath57 $ ] = @xmath58 $ ] , 7 .",
    "set @xmath59 if @xmath57 > u$ ] and @xmath60 otherwise .",
    "set @xmath61 and return to step [ step2 ] until convergence .",
    "since the algorithm is making use of first derivatives of the ( unormalized ) log - posterior densities it tends to propose moves to regions of higher probabilities and the chains are expected to reach stationarity faster .",
    "also , in order to employ this algorithm all sampling must be done on an unconstrained space , so we need to implement a transformation of @xmath25 to the real line .",
    "then prior distributions are assigned and derivatives are taken for the transformed parameters .      developed a modification in the proposal mechanism in which the moves are according to a riemann metric instead of the standard euclidean distance .",
    "this procedure explores geometric properties of the posterior distribution and is referred to as riemann manifold hmc or rmhmc .",
    "the idea is to redefine the hamiltonian function as , @xmath62 where the position dependent matrix @xmath63 adapts to the local geometry of the posterior distribution ( see also ) . in this paper",
    "we adopt the form proposed in where , @xmath64 i.e. the expected fisher information matrix plus the negative hessian of the log - prior . the hamiltonian dynamics becomes , @xmath65 + \\frac{1}{2}{\\hbox{\\boldmath$p$}}'{\\hbox{\\boldmath$g$}}({\\hbox{\\boldmath$\\theta$}})^{-1}\\frac{\\partial{\\hbox{\\boldmath$g$}}({\\hbox{\\boldmath$\\theta$}})}{\\partial\\theta_i}{\\hbox{\\boldmath$p$}}.\\end{aligned}\\ ] ] and in order to simulate values in discrete time we adopt the generalized strmer - verlet solution ( ) .",
    "expressions for the expected fisher information matrix and the hessian of the log - prior are provided in appendix [ appendix ] .",
    "this example is taken from page 59 and refers to the annual maximum sea levels ( in metres ) from 1923 to 1987 at port pirie , south australia ( see figure [ fig : data ] ) .",
    "the objective is to fit a generalized extreme value distribution to this data .",
    "the prior distribution adopted is a trivariate normal on @xmath66 with mean vector zero and diagonal variance covariance matrix ( i.e. assuming prior independence ) with prior variances equal to 25 .",
    "the complete conditional distributions are not of any standard form and metropolis steps are used to yield the required realizations from the posterior distribution .",
    "figure [ fig : data ] about here .    for comparison purposes we also used the r package evdbayes ( ) which is freely available from the website http://cran.r-project.org/web/packages/evdbayes and provides functions for the bayesian analysis of extreme value models using mcmc methods .",
    "this package uses the metropolis - hastings algorithm .",
    "figure [ fig : mh1 ] shows the trace plots of the sampled values of @xmath1 , @xmath2 and @xmath3 using the evdbayes package with @xmath67 simulations discarding the first @xmath68 as burn - in .",
    "we note that even after discarding the first @xmath68 iterations the chains are far from convergence and sample autocorrelations are still high .",
    "figure [ fig : mh1 ] about here .",
    "the hmc algoritm was implemented in r. after some pilot tunning the parameter @xmath69 was taken as 0.12 and the strmer - verlet solution was replicated 27 times .",
    "the results appear in figure [ fig : hmc ] which shows the trace plots of sampled values of @xmath1 , @xmath2 and @xmath3 using hmc .",
    "we note that the hmc algorithm had an acceptance rate around 0.95 and reachs a stationary regime much faster than the metropolis - hastings . besides , there is practically no autocorrelation in the output chains .    in order to compare the relative efficiency of these methods",
    "we calculate the effective sample size ( ess ) using the posterior samples for each parameter .",
    "this measure is defined as @xmath70 where @xmath71 is the number of posterior samples and @xmath72 are the monotone lag @xmath73 sample autocorrelations ( ) .",
    "it can thus be interpreted as the number of effectively independent samples . for a fair comparison ,",
    "first we discarded another 1500 iterations from the samples generated by mh and hmc algorithms .",
    "the ess is easily obtained from any mcmc output using the functionality from the r package coda ( ) which provides tools for output analysis and diagnostics .",
    "table [ tab1 ] shows the effective samples sizes for the parameters using both algorithms based on the last 3500 iterations from which we can see a much lower degree of autocorrelation in the hmc output .",
    "table [ tab1 ] about here .      in order to evaluate and compare the performances of hmc and mh algorithms two simulation studies were conducted for parameter estimation in a gev model . in both studies we generated @xmath74 replications of @xmath75 observations from a gev model with parameters @xmath76 , @xmath77 and @xmath78 .",
    "location and scale parameters are usually not too difficult to estimate but according to the value @xmath78 is not common in practice as it leads to distributions with too heavy tails .",
    "this makes the inferences for this parameter more problematic .",
    "let @xmath79 the estimate of a parameter @xmath80 for the @xmath81-th replication , @xmath82 . to evaluate the estimation method ,",
    "two criteria were considered : the bias and the mean square error ( mse ) , which are defined as , @xmath83    for each replication and each sample size a gev model was fitted using the hmc and metropolis algorithm ( using evdbayes package ) based on 20000 iterations discarding 10000 as burn - in . in this",
    "study the posterior modes were taken as parameter point estimates in ( [ eq1 ] ) and ( [ eq2 ] ) since the marginal posterior distributions are skewed .",
    "the results in terms of bias and mean square errors for each parameter appear in table [ ressim ] .",
    "overall , both measures are pretty small for both algorithms although they tend to be slightly smaller for the hmc .",
    "this was expected since after the 10000 iterations discarded the metropolis algorithm is as close to the invariant distribution as the hmc algorithm .    in a second experiment",
    ", we generated only 1100 samples from the posterior distribution discarding the first 100 as burn - in .",
    "the main objetive here is to see whether the hmc algorithm tends to get close enough to the stationary distribution so as to provide good estimates with such a small number of iterations .",
    "the results are shown in table [ ressim1 ] from which we can see that both bias and mean square error are still relatively small for the hmc algorithm while the metropolis algorithm appears to be definetely far from the stationary distribution .",
    "therefore , the advantage of adopting the hmc algorithm instead of metropolis seems clear at least in terms of speed of convergence .",
    "this comes at a price of obtaining and evaluating first derivatives which are really easy to obtain and code as shown in appendix [ appendix ] . finally , the computational times for each iteration were not too large in this application after some pilot tunning for the step - size .",
    "of course each iteration of hmc takes more time than in the metropolis algorithm but this is more than compensated by the faster convergence ( we need many less iterations ) .",
    "table [ ressim ] about here .",
    "table [ ressim1 ] about here .",
    "in this section we extend the gev model by allowing the location parameter to vary across observations through an autoregressive process of order @xmath84 ( ar(@xmath84 ) ) .",
    "the model is given by , @xmath85 where @xmath86 are independent identically distributed random errors distributed as @xmath87 .",
    "assuming second order stationarity and restricting @xmath88 it follows that , @xmath89=\\mu_{y_t } & = & \\dfrac{\\mu_{e_t } + \\mu}{1-\\sum_{j=1}^p \\theta_j } , \\forall t \\nonumber \\\\ e[e_t]=\\mu_{e_t } & = & - \\dfrac{\\sigma}{\\xi } + \\dfrac{\\sigma}{\\xi}\\gamma(1-\\xi ) , \\nonumber \\\\ var[e_t]=\\sigma^2_{e_t } & = & \\dfrac{\\sigma^{2}}{\\xi^{2}}\\left[\\gamma(1 - 2\\xi)-\\gamma^{2}(1-\\xi)\\right].\\end{aligned}\\ ] ]    the likelihood function is given by , @xmath90 where @xmath91 and @xmath92 . denoting @xmath93 then @xmath94 and @xmath95 .",
    ".5 cm    prior distributions are then assigned to the parameters @xmath25 , @xmath1 , @xmath2 and @xmath3 . these are assumed to be a priori independent with relatively vague prior distributions defined in the original parameter space , except for @xmath3 which is constrained to the interval @xmath96 so that both the mean and the variance of the autoregressive process exist . in what follows , we adopt the prior specifications @xmath97 , @xmath98 , @xmath99 , @xmath100 and @xmath101 .",
    ".5 cm      in this simulation study , the main objective is to investigate the behaviour of the hmc and rmhmc algorithms in terms of speed to reach the stationary distribution .",
    "therefore , in this experiment we performed only 600 mcmc iterations discarding the first 100 as burn - in .",
    "we generated @xmath74 replications of @xmath102 time series observations from gev - ar(@xmath84 ) models with @xmath103 .",
    "the artificial time series were simulated from the following stationary models , @xmath104 where the error terms @xmath86 are independent and identicaly distributed as @xmath105 , @xmath106 .    for the hmc algorithm we set @xmath107 and repeated the strmer - verlet solution 13 times . for the rmhmc",
    ", we used a fixed metric given by the model information matrix evaluated at the map estimate . for the @xmath108-@xmath109 and @xmath108-@xmath110 models the elements",
    "@xmath111 $ ] and @xmath112 $ ] are determined in closed form for all @xmath35 . for the @xmath108-@xmath113 model we used the approximation @xmath114",
    "\\approx \\mu_{y_t}^2 + \\widehat{c}(y_t , y_{t+i})$ ] , @xmath115 , where @xmath116 is the sample covariance matrix .",
    "we set @xmath117 and repeated the strmer - verlet solution 13 times .",
    "the simulation results are reported in table [ ressim2 ] as bias and mean square errors as defined in expressions ( [ eq1 ] ) and ( [ eq2 ] ) . for models of orders 1 and 2 and",
    "the three sample sizes considered the performances in terms of bias are barely similar but these are in general smaller for the rmhmc algorithm .",
    "this is also true for the model of order 3 and sample sizes 60 and 150 , but for samples of size 300 the hmc algorithm underestimates @xmath1 and @xmath2 more severely and , except for @xmath118 , the biases are smaller for the rmhmc algorithm .",
    "when we look at the mean square errors , the comparison is in general more favorable to the rmhmc specially for larger sample sizes . in particular , for the @xmath108-@xmath113 model the mean square error tends to decrease ( sometimes dramatically ) for all sample sizes . at this point ,",
    "an explanation for the large values of mse for @xmath1 and @xmath2 in the @xmath108-@xmath113 model is in order .",
    "recall that we comparing the performances of the two algorithms based on relatively few mcmc iterations .",
    "so , for samples of size 300 the initial values where probably far from regions of higher posterior probabilities and the hmc would require more iterations while for the rmhmc these initial values were much less influencial .",
    "all in all , we consider that this simulation study provides empirical evidence of a better performance of the rmhmc algorithm and we would recommend this approach to the applied user dealing with time series of extreme values .",
    "table [ ressim2 ] about here .      in this application",
    ", each observation represents the maximum annual level of lake michigan , which is obtained as the highest mean monthly level , 1860 to 1955 ( @xmath119 observations ) .",
    "the time series data can be obtained from the time series data library repository at https://datamarket.com/data/set/22p3/    based on the autocorrelation and partial autocorrelation functions of the data we propose a @xmath108-@xmath109 model for this dataset . to assess the quality of predictions",
    ", we removed the last three observations from estimation .",
    "the predictions are then compared with the actual data .",
    "the rmhmc algorithm was applied with a fixed metric evaluated at the map estimate to simulate values from the posterior distribution of @xmath120 .",
    "after a short pilot tunning a step - size @xmath121 was taken and the strmer - verlet solution was repeated 11 times at each iteration . a total of 21000 values were simulated discarding the first 1000 as burn - in .",
    "table [ tablear1 ] shows the approximations for the marginal posterior mean , standard deviation , mode , median and credible interval for the model parameters . from table",
    "[ tablear1 ] we note that the estimated model is stationary with high probability and the point estimate of @xmath3 is about @xmath122 with a small standard deviation thus characterizing a distribution with moderate asymetry .",
    "convergence of the markov chains was assessed by visual inspection of trace and autocorrelation plots ( not shown ) and all indicated that the chains reached stationarity relatively fast with low autocorrelations .    in the bayesian approach , given @xmath123 ,",
    "the @xmath124-steps ahead predictions are obtained from the predictive density of @xmath125 which is given by , @xmath126.\\end{aligned}\\ ] ] here we propose to compute a point prediction @xmath127 of @xmath125 as a monte carlo approximation of the predictive expectation , @xmath128 = e[e[y_{_{t+j}}|\\mu,\\theta,\\sigma,\\xi,{\\hbox{\\boldmath$y$}}]]$ ] .",
    "so , given a sample of @xmath71 simulated parameter values we sample values @xmath129 given @xmath130 , @xmath131 which allow us to use the following approximation , @xmath132 for @xmath133 .",
    "in figure [ par1 ] we can see how the predictions behave relative to the actual values .",
    "all observed values are within the credible intervals of the predictive distributions which tend to follow the time series .",
    "in this paper we evaluated bayesian mcmc methods to estimate the parameters in a generalized extreme value model both for independent and time series data .",
    "we employed the bayesian approach using both traditional mcmc ( metropolis - hastings ) methods and ( riemann manifold ) hamiltonian monte carlo methods to obtain the approximations to the posterior marginal distributions of interest .",
    "applications to real datasets of maxima illustrated how ( rm)hmc can be much more efficient computationally than traditional mcmc . in a simulation study for independent data we noticed that parameter estimation is relatively robust to the choice of algorithm for a large number of iterations and discarding a lot of initial values as burn - in although bias and mean square error tend to be slightly smaller for hmc .",
    "however , hmc was much faster to reach the stationary distribution and this was observed by repeating the simulations with a small number of iterations .",
    "another simulation study for time series data has shown that rmhmc is to be recommended for the applied user .",
    "as in any simulation study , our results are limited to our particular selection of sample sizes , prior distributions and gev parameters .",
    "in particular , the choice @xmath78 in section 3.2 was intended to compare the algorithms in a more difficult scenario in terms of estimation ( ) .",
    "we hope that our findings are useful to the practitioners .",
    "the first author received financial support from capes - brazil .",
    "in this appendix we present the expressions of gradients needed for the implementation of hmc and rmhmc in the gev model . in what follows ,",
    "let @xmath134 . denoting @xmath135 and @xmath136",
    "then ,      the partial derivatives of this log - density with respect to the transformed parameters @xmath66 are given by , @xmath138",
    "\\\\ \\frac{dl_{y|\\theta}}{d\\delta } & = &   -n+(1+\\xi)\\sum_{t=1}^n \\frac{y_t-\\mu}{\\sigma}z_t^{-1 } -\\sum_{t=1}^n \\frac{y_t-\\mu}{\\sigma}z_t^{-1/\\xi-1}\\\\ \\frac{dl_{y|\\theta}}{d\\xi }   & = & \\sum_{t=1}^n\\frac{\\log z_t}{\\xi^2}- \\left(\\frac{1}{\\xi}+1\\right)\\left(\\frac{y_t-\\mu}{\\sigma}\\right)z_t^{-1}+ \\frac{1}{\\xi}\\left(\\frac{y_t-\\mu}{\\sigma}\\right)z_t^{-1/\\xi-1}- \\frac{\\log z_t}{\\xi^2}z_t^{-1/\\xi}.\\end{aligned}\\ ] ]    now letting @xmath139 and since the ( transformed ) parameters are assumed a priori independent and normally distributed with mean zero then , @xmath140 where @xmath141 , @xmath142 and @xmath143 are the prior variances .",
    ".5 cm      to obtain the fisher information matrix we use the fact that @xmath147 = e[e[g(y_t)|d_{t-1 } ] ] , ~\\forall t$ ] .",
    "the nonzero elements are given by , @xmath148 = ( t - p)\\dfrac{a}{\\sigma^{2 } } \\\\",
    "-e \\left ( \\dfrac{\\partial^2 \\ell}{\\partial \\mu \\partial \\theta_j }   \\right ) & = & ( t - p)\\dfrac{a}{\\sigma^{2 } } e[y_{t - j } ] = \\mu_{y_t}(t - p)\\dfrac{a}{\\sigma^{2}}\\\\ -e \\left(\\dfrac{\\partial^2 \\ell}{\\partial \\mu \\partial \\sigma } \\right ) & = & - e \\left [ e\\left ( \\dfrac{\\partial^2 \\ell}{\\partial \\sigma      \\partial \\mu_t } \\middle| d_{t-1 }   \\right ) \\right ]    = -(t - p ) \\dfrac{1}{\\sigma^{2}\\xi}[a - \\gamma(2+\\xi ) ] \\\\",
    "-e \\left ( \\dfrac{\\partial^2 \\ell}{\\partial \\mu \\partial \\xi } \\right ) & = & - e \\left [ e\\left ( \\dfrac{\\partial^2 \\ell}{\\partial \\xi \\partial      \\mu_t }   \\middle| d_{t-1 }   \\right ) \\right ]   = - ( t - p)\\dfrac{1}{\\sigma\\xi}\\left(b - \\dfrac{a}{\\xi } \\right)\\\\ -e \\left ( \\dfrac{\\partial^2 \\ell}{\\partial \\theta_i \\partial \\theta_j } \\right ) & = & - e \\left [ e\\left ( \\dfrac{\\partial^2\\ell}{\\partial      \\mu_t^2 } y_{t - i } y_{t - j } \\middle| d_{t-1}\\right)\\right ] = ( t - p)\\dfrac{a}{\\sigma^{2 } } e[y_{t - i}y_{t - j } ] \\\\ -e \\left ( \\dfrac{\\partial^2 \\ell}{\\partial \\sigma \\partial \\theta_j }   \\right ) & = & - e \\left [ e\\left ( \\dfrac{\\partial^2\\ell}{\\partial \\sigma       \\partial \\mu_t } y_{t - j }",
    "\\middle| d_{t-1 }   \\right ) \\right]\\\\    & = & -(t - p ) \\dfrac{1}{\\sigma^{2}\\xi}[a - \\gamma(2+\\xi ) ] e[y_{t - j}]\\\\   & = & -(t - p ) \\dfrac{1}{\\sigma^{2}\\xi}[a - \\gamma(2+\\xi ) ] \\mu_{y_t}\\\\   -e \\left ( \\dfrac{\\partial^2 \\ell}{\\partial \\xi \\partial \\theta_j }   \\right ) & = & - e \\left [ e\\left ( \\dfrac{\\partial^2\\ell}{\\partial \\xi       \\partial \\mu_t } y_{t - j } \\middle| d_{t-1 }   \\right ) \\right]\\\\    & = & -(t - p ) \\dfrac{1}{\\sigma\\xi}\\left(b - \\dfrac{a}{\\xi } \\right ) e[y_{t - j } ] \\\\    & = & -(t - p ) \\dfrac{1}{\\sigma\\xi}\\left(b - \\dfrac{a}{\\xi } \\right)\\mu_{y_t}\\\\ -e\\left ( \\dfrac{\\partial^2 \\ell}{\\partial \\xi \\partial \\sigma } \\right ) & = & - ( t - p ) \\dfrac{1}{\\sigma\\xi^{2}}\\left[1 - \\gamma + \\dfrac{1      - \\gamma(2+\\xi)}{\\xi } - b + \\dfrac{a}{\\xi } \\right]\\end{aligned}\\ ] ] where @xmath149 , @xmath150 $ ] , @xmath151 is the gamma function , @xmath152 is the digamma function and @xmath153 is the euler s constant ( @xmath154 ) ."
  ],
  "abstract_text": [
    "<S> in this paper we propose to evaluate and compare markov chain monte carlo ( mcmc ) methods to estimate the parameters in a generalized extreme value model . </S>",
    "<S> we employed the bayesian approach using traditional metropolis - hastings methods , hamiltonian monte carlo ( hmc ) and riemann manifold hmc ( rmhmc ) methods to obtain the approximations to the posterior marginal distributions of interest . </S>",
    "<S> applications to real datasets of maxima illustrate illustrate how hmc can be much more efficient computationally than traditional mcmc and simulation studies are conducted to compare the algorithms in terms of how fast they get close enough to the stationary distribution so as to provide good estimates with a smaller number of iterations . </S>",
    "<S> .5 cm    key words : extreme value ; bayesian approach ; hamiltonian monte carlo ; markov chain monte carlo . </S>"
  ]
}