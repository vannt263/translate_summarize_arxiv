{
  "article_text": [
    "neutral hydrogen ( hi ) is the raw fuel for star formation in galaxies , and an important ingredient in understanding galaxy formation and evolution through cosmic time . in the interstellar medium ( ism ) of the milky way , hi is predicted to exist in two thermally stable states : the cold neutral medium ( cnm ) with temperature between 40@xmath0 , and the warm neutral medium ( wnm ) with temperature between 4100@xmath1 @xcite .",
    "the 21 cm hyperfine transition of hi is a convenient tracer of hi .",
    "one technique for measuring the excitation temperature of hi is to fit 21 cm emission and absorption data to a collection of independent iso - thermal gaussian components . with this technique ,",
    "hi spin temperatures have been measured in the range of @xmath2@xmath3 ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "although hi surveys have detected hundreds of components with temperatures consistent with the predictions for the cnm , few have been detected with temperatures consistent with those of the wnm .",
    "the absence of wnm - temperature gas is surprising because the wnm contains @xmath4 of the mass in the neutral ism @xcite . surveys also find a significant fraction of thermally _ unstable _ gas ( with temperature between @xmath5@xmath6 ) , up to 47% of detections in @xcite . although the missing wnm could be explained in terms of sub - thermal excitation of the 21 cm line in low density environments ( e.g. , * ? ? ?",
    "* ) , recent results from @xcite point instead toward a lack of absorption observations with enough sensitivity to detect wnm - temperature gas , which has an absorption strength @xmath7 less than cnm - temperature gas .",
    "additionally , numerical simulations have shown that magnetic fields and non - equilibrium physics like bulk flows and turbulence can affect the expected relative fractions of wnm , cnm , and intermediate temperature ( unstable ) gas @xcite , although observational data can not yet distinguish between these scenarios .",
    "the main reason it has been difficult to make progress in understanding the neutral ism is that observational surveys have sample sizes of only 10100 sightlines , leaving large statistical errors in the measurements of the hi spin temperature distribution .",
    "the square kilometer array ( ska ) and its pathfinder telescopes , the australian ska pathfinder ( askap ) , the recently expanded karl g. jansky very large array , and meerkat , will push radio astrophysics into a new era of  big spectral data \" by providing scientists with millions of high resolution , high - sensitivity radio emission and absorption spectra probing lines of sight through the milky way and neighboring galaxies .",
    "this infusion of data promises to revolutionize our understanding of the neutral ism .",
    "however , these new data will bring new challenges in data interpretation .",
    "modelling a 21 cm emission or absorption spectrum as a superposition of @xmath8 independent gaussian components requires solving a non - linear optimization problem with @xmath9 parameters . because gaussian functions do not form an orthogonal basis ( solutions are not unique )",
    ", the parameter space is non - convex ( contains local optima instead of a single , global optimum ) , and therefore the final solutions sensitively depend on the initial guesses of the components positions , widths , and amplitudes , and especially on the total number of components . to minimize the chances of getting stuck in local optima during model fitting",
    ", researchers choose the initial parameter guesses to lie as close to the global optimum as possible . in previous and current surveys ,",
    "these initial guesses are provided manually , effectively using the pattern - recognition skills of humans to identify the individual components within the blended spectra .",
    "this manual selection process is time consuming and subjective , rendering it ineffective for the large data volumes in the ska era .",
    "automatic line finding and gaussian decomposition algorithms can solve these problems",
    ".    however , the available algorithms for automatic line detection are either unlikely to scale to the data volumes of ska , or lack the flexibility to fit complex spectra .",
    "the bayesian line finder by @xcite searches parameter space using the nested sampling algorithm @xcite , and uses bayesian inference to discover the optimal number of spectral components .",
    "however , it has only been applied to simple spectra with few components , and has not been tested on complex galactic 21 cm data .",
    "procedural algorithms like those of @xcite or @xcite iteratively add , subtract , or merge components based on the effects these decisions have on the resulting residuals of least - squares fits , and have been used to interpret large datasets from , e.g. , the leiden - argentina - bonn ( lab ) all - sky hi survey @xcite .",
    "however , the initial parameters for each fit are adopted from previous solutions in adjacent sky positions , thereby limiting the use these algorithms to densely - sampled emission surveys .",
    "topology - based algorithms like clumpfind @xcite and duchamp @xcite are too limited for efficient gaussian decomposition because they can only detect components that are strong enough to produce local maxima in their spectra , do not allow components to overlap , and do not provide estimates of spectral shape .",
    "similarly , gaussclumps @xcite only locates strong components that produce local optima in 3d space .",
    "while the above algorithms operate successfully in the data for which they were designed , they are not suited for rapid objective decomposition of millions of complex absorption spectra .    in this paper , we present a new algorithm , called autonomous gaussian decomposition ( agd ) , which uses computer vision and machine learning to quickly provide optimized guesses for the initial parameters of a multi - component gaussian model .",
    "agd allows for the interpretation of large volumes of spectral data and for the ability to objectively compare observations to numerical simulations in a statistically robust way . while the development of agd was motivated by radio astrophysics , specifically the 21-sponge survey ( *",
    "* murray et al . 2014b , in prep .",
    ") , the algorithm can be used to search for one - dimensional gaussian ( or any other single - peaked spectral profile)-shaped components in any data set .    in section [ s :",
    "agd ] , we explain the algorithm ; in section [ s : code ] we describe the python implementation of agd called gausspy ; in section [ s : performance ] , we discuss agd s performance in decomposing real 21 cm absorption spectra ; and in section [ s - conclude ] , we present a discussion of results and conclusions . +",
    "( red dot ) , third @xmath10 ( orange dash ) , and @xmath11 ( pink dot - dash ) numerical derivatives .",
    "the locations in the data satisfying the conditions from equations [ e - c1]-[e - c4 ] are identified with blue circles , with blue line segments showing the guessed @xmath12 widths from equation [ e - widths ] .",
    "the positions and widths indicated by the blue circle and line segments represent the guesses that agd would produce for this example spectrum . ]",
    "agd approaches the problem of gaussian decomposition by focusing on the task of choosing the parameters initial guesses , where human input has been most needed in the past . by quickly producing high quality initial guesses ,",
    "most of the work in interpreting the spectrum has been done , and the resulting least - squares fit converges quickly to the global optimum .    in the following , @xmath13 and @xmath14 represent an example spectrum .",
    "for example , @xmath13 might have units of frequency and @xmath14 units of flux density .",
    "where relevant , all one - dimensional variables are to be interpreted as column vectors .",
    "the variables @xmath15 , @xmath16 , and @xmath17 represent the amplitude , `` @xmath18 '' width ( hereafter referred to as just the `` width '' ) , and position of a gaussian function @xmath19 according to @xmath20      derivative spectroscopy is the technique of analyzing a spectrum s derivatives to gain understanding about the data . it is used in computer vision applications because derivatives can respond to shapes in images like gradients , curvature , and edges .",
    "it has a long history of use in biochemistry ( see , e.g. , * ? ? ?",
    "* ) , and has been recently used to analyze the spectral features of hi self - absorption in two galactic molecular clouds @xcite .",
    "agd uses derivative spectroscopy to decide how many gaussian components a spectrum contains , and also to decide where they are located .",
    "the algorithm places one guess at the location of every _ local minimum of negative curvature _ in the data , where the curvature of @xmath14 is defined as the second derivative , @xmath21 .",
    "this criterion finds `` bumps '' in the data , and has the sensitivity to detect weak and blended components .",
    "mathematically , this condition corresponds to locations in the data which satisfy the four conditions : @xmath22 in ideal , noise - free data , we could set @xmath23 ; however , observational noise produces random curvature fluctuations and a signal threshold should be applied to avoid placing guesses in signal - free regions .",
    "equation [ e - c2 ] enforces that the curvature is negative , while equations [ e - c3][e - c4 ] ensure the location is a local minimum of the curvature .",
    "the @xmath8 discrete values of @xmath13 satisfying equations [ e - c1][e - c4 ] serve as the guesses for the component positions @xmath24 where @xmath25 .",
    "figure [ f - example - dspec ] shows an example of applying equations [ e - c1][e - c4 ] to find the component locations in an ideal noise - free spectrum .",
    "next , agd guesses the components widths by exploiting the relation between a component s width and the maximum of its second derivative . for an isolated component ,",
    "the peak of the @xmath26 derivative is located at @xmath27 , and has a value of @xmath28\\bigg|_{x=\\mu } & = -\\frac{a}{\\sigma^2}.\\end{aligned}\\ ] ] agd applies this single - component solution to provide estimates for the widths of all @xmath29 components @xmath30 by approximating @xmath31 to obtain @xmath32    finally , agd guesses the components amplitudes , @xmath33 .",
    "naive estimates for the amplitudes of the @xmath8 components are simply the values of the original data evaluated at the component positions . however ,",
    "if the components are highly blended , then the naive guesses can significantly over estimate the true amplitudes .",
    "agd compensates for this overestimate by attempting to `` de - blend '' the amplitude guesses using the information in the already - produced position and width guesses ( see appendix [ a - deblend ] for details on the deblending process ) .    , @xmath34 , and @xmath35 in gaussian - distributed noise with @xmath36 .",
    "_ middle panel : _ the black dashed line shows the ideal derivative of the underlying gaussian function , and the green line shows the results of a finite - difference - based numerical derivative applied to the green data from the top panel .",
    "the amplified noise makes it impossible to locate local optima reliably . _",
    "bottom panel : _ regularized derivatives ( section [ s - reg ] ) of the green data from the top panel using different values of the regularization parameter @xmath37 .",
    "larger or smaller values of @xmath37 trade smoothness for data fidelity , respectively .",
    "+ ]      in order to identify components in @xmath14 using equations [ e - c1][e - c4 ] , the derivatives of @xmath14 must be accurate and smoothly varying .",
    "any noise in the derivatives of the spectra will produce spurious component guesses .",
    "computing derivatives using finite - difference techniques greatly amplifies noise in the data , thereby rendering finite - difference techniques unusable for our needs of computing derivatives up to the fourth order .",
    "we regularize the differentiation process using tikhonov regularization @xcite , where the derivative is fit to the data under the constraint that it remains smooth by following the technique presented in @xcite and @xcite .    ) during agd s two - phase training process ( appendix [ a - gd ] ) are represented by black circles , black lines , and white `` x ' 's , respectively .",
    "the dashed white line marks the @xmath38 boundary .",
    "tracks that begin too far away from the global best solution ( @xmath39 , @xmath40 ) can converge into local optima with lower resulting accuracy .",
    "multiple training runs with different starting positions are therefore recommended to find the global optimum .",
    "additionally , physical considerations like the expected width of components can help guide the choice of starting value . the background image shows a densely sampled representation of the underlying parameter space , and was generated using the htcondor cluster at the university of wisconsin s center for high - throughput computing . ]",
    "we define the regularized derivative of the data as @xmath41)$ ] , where @xmath42 = \\alpha \\int \\sqrt { \\left(d_x u\\right)^2+\\beta^2 } +           \\int |a_x u - f|^2 .",
    "\\end{aligned}\\ ] ] the derivative operator @xmath43 and the anti - derivative operator @xmath44 .",
    "the first term on the right - hand - side ( rhs ) of equation [ e - tv ] is the regularization term , and is an approximation to total - variation ( tv ) regularization .",
    "when @xmath45 is zero , this term becomes the @xmath46 norm of @xmath47 , pushing @xmath48 to be piecewise constant .",
    "when @xmath45 is @xmath49 , the regularization term behaves like the @xmath50 norm of @xmath47 , constraining @xmath48 instead to be smoothly varying . to produce smoothly - varying solutions for our derivatives , we ( a ) set @xmath51 , and ( b ) rescale the bin widths to unity and peak - normalize the data ; these scale factors are remembered and reapplied when optimization has completed .",
    "the second term of the rhs of equation [ e - tv ] is the data fidelity term , enforcing that the integral of @xmath48 closely follows the data @xmath52 .",
    "the parameter @xmath37 controls the relative balance between smoothness and data fidelity in the solution , i.e. , between variance and bias . when @xmath53 , @xmath54 is equal to the finite difference derivative .",
    "figure [ f - example ] displays how the regularization parameter @xmath37 affects the shape of the resulting regularized derivative of a single gaussian component within gaussian distributed noise .",
    "larger values of @xmath37 effectively ignore variations in the data on increasingly larger spatial scales . because of the large range that @xmath37 can span",
    ", we hereafter refer to the regularization parameter as @xmath55 .      in supervised machine learning , the computer is given a collection input / output pairs , known as a training set , and then `` learns '' a general rule for mapping inputs to outputs .",
    "after this `` training '' process is completed , the algorithm can be used to predict the output values for new inputs ( see , e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "the regularization procedure of section [ s - reg ] allows us to take smooth derivatives at the expense of introducing the free parameter @xmath56 , which controls the degree of regularization .",
    "supervised machine learning is used to train agd and pick the optimal value of @xmath56 which maximizes the accuracy of component guesses on a training set of spectra with known decompositions .",
    "one can obtain the training set by manually decomposing a subset of the data , or by generating new synthetic spectra using components that are drawn from the same distribution as the science data . in the latter case , there is a risk that the training data are different from the science data , but also the benefit that the decompositions are guaranteed to be `` correct '' while the manual decompositions are not .",
    "given @xmath57 component guesses @xmath58 , produced by running agd with fixed @xmath59 on data containing @xmath60 true components @xmath61 , the `` accuracy '' @xmath62 of the guesses is defined using the balanced f - score .",
    "the balanced f - score is a measure of classification accuracy that depends on both precision ( fraction of guesses that are correct ) and recall ( fraction of true components that were found ) , thus penalizing component guesses which are incorrect , missing , or spurious .",
    "the accuracy is given by @xmath63 where @xmath64 represents the number of `` correct '' guesses .",
    "we consider a single guessed component ( @xmath65 , @xmath66 , @xmath67 ) to be a `` correct '' match to a true component ( @xmath68 , @xmath69 , @xmath70 ) if its amplitude , position , and width are all within the limits specified by the following equations : @xmath71 the analysis in section [ s : performance ] uses @xmath72 .",
    "the final solution is least sensitive to the initial amplitudes , so we choose the values @xmath73 and @xmath74 to bracket a large relative range ; it is more sensitive to the guessed widths , so we chose a narrower relative range in @xmath75 and @xmath76 ; finally , we find that the positions are the most important parameters for fitting the data in the end , motivating the relatively strict value of @xmath77 .",
    "we impose the additional restriction that matches between guessed and true components must be one - to - one , and therefore match consideration proceeds in order of decreasing amplitude .",
    "the optimal value of @xmath56 is that which maximizes the accuracy ( equation [ e - acc ] ) between agd s guessed components and the true answers in the training data .",
    "this non - linear optimization process is performed using gradient descent and is described in detail in appendix [ a - gd ] .",
    "gausspy is the name of our python / c implementation of the agd algorithm .",
    "this lightweight python module is easy to deploy on high - throughput computing solutions like htcondor ( see figure [ f - tracks ] ) or hadoop / mapreduce @xcite , allowing for rapid decomposition of very large spectral datasets , e.g. , the spectral data products of the ska .",
    "agd may also be suitable for deployment on the scalable source finding framework @xcite .",
    "gausspy is maintained by the author and will be publicly available through the python package index upon publication of this manuscript .    [ cols=\"^,^ \" , ]     the agd algorithm as explained in section [ s : agd ] is optimized for finding components spanning only a modest range in width .",
    "this is the cost we pay for the ability to compute smooth derivatives using regularization . in order to search for gaussian components on widely different scales , e.g. , to search for components with widths near @xmath78@xmath79 and @xmath80-@xmath81 in the same spectra",
    ", we can iteratively apply agd to search for components with widths at each of these scales .",
    "this capability is included in gausspy and is referred to as `` two - phase '' decomposition ( for details , see appendix [ a - two - phase ] ) .",
    "gausspy uses agd to produce the initial guesses for parameters in a multi - component gaussian fit , and also carries out the final least - squares fit on the data . in this final optimization",
    ", gausspy uses unconstrained minimization with the levenberg - marquardt @xcite algorithm , which has been used in previous surveys ( e.g. , * ? ? ?",
    "if negative amplitudes are found , then the fit is remade using the limited - memory bound constrained broyden - fletcher - goldfarb - shanno ( l - bfgs - b ) @xcite algorithm for which we can enforce the constraint that all amplitudes be non - negative .    in gausspy",
    ", we minimize the functional @xmath82 $ ] ( equation [ e - tv ] ) using the quasi - newton algorithm `` bfgs2 '' from the gnu scientific library ` multimin ` package and achieve computation - time scalings of @xmath83 , where @xmath29 is the number of channels in the data , and @xmath84 . the relative scaling between @xmath56 and the minimum preserved scale in the data",
    "is found numerically to be approximately @xmath85 where @xmath86 is the spatial scale in channels ( see figure [ f - example ] ) . by plugging in an estimate of the expected component widths to equation [ e - scale ] ,",
    "one obtains a rough estimate of the appropriate regularization parameter @xmath56 . however , to find the value which maximizes the accuracy of the decompositions , one should solve for @xmath56 using the machine learning technique of section [ s - ml ] .",
    "we test agd by comparing its results to human - derived answers for 21 spectra from the 21 cm spectral line observations of neutral gas with the evla ( 21-sponge ) survey ( murray et al .",
    "2014b , in prep . ) .",
    "21-sponge spectra cover a velocity range from @xmath87 to @xmath88 , tracing galactic hi gas .",
    "21-sponge s @xmath89",
    "absorption spectra are among the most sensitive ever observed with typical optical - depth root - mean - square ( rms ) sensitivities of @xmath90 per @xmath91 channel ( murray et al .",
    "2014b , in prep . ) .",
    "this combination of sensitivity and spectral resolution will stay among the best obtainable through the ska era .",
    "the survey data come natively in units of fractional absorption ( @xmath92 ) , we transform the data into optical depth units ( @xmath93 ) for the agd analysis because only in @xmath94-space will a single component produce a single peak in curvature ( i.e , strong absorption signals will produce _ dual _ peaks in the curvature of @xmath95 ) .",
    "we begin by constructing the training data set , which is based on independent 21 cm absorption observations from the millennium arecibo 21 centimeter absorption - line survey @xcite .",
    "we produce 20 synthetic spectra by randomly - selecting gaussian components from the @xcite catalog .",
    "the number of components per spectrum is chosen to be a uniform random integer between the mean value ( three ) and the maximum value ( eight ) from the observations . only components with peak optical depth @xmath96",
    "are included in the training data because beyond this , the absorption signal saturates and the component properties are poorly constrained .",
    "we next add gaussian - distributed noise with @xmath97 per @xmath91 channel to the spectra ( in observed @xmath95 space ) to mimic real observational noise from the millenium survey @xcite , and re - sample the data at @xmath98 to avoid aliasing the narrowest components ( with fwhms of @xmath99 ) in the training set .",
    "we set the global threshold , ( parameter @xmath100 in equation [ e - c1 ] ) , to be @xmath101 the rms for individual spectra .    ) and velocity fwhm for each recovered gaussian component .",
    "the contours represent @xmath102 and @xmath103 containment regions .",
    "the side panels show marginalized histograms of peak optical depth ( top ) and velocity fwhm ( right ) for agd ( dashed ) and human ( solid ) results .",
    "there are 118 human - detected components , and 120 agd - detected components in the 21 spectra . ]",
    "we next train agd for both one- and two - phase decompositions and compare their performances . for one - phase agd",
    "we use the initial value @xmath104 and agd converged to @xmath105 .",
    "the resulting accuracy was @xmath106 on the training data , and @xmath107 on an independent test - set of 100 newly - generated ( out - of - sample ) synthetic spectra . testing the performance on similar but independent out - of - sample `` test '' data prevents against `` over - fitting '' the training data . for two - phase agd ,",
    "we use initial values of @xmath108 and @xmath109 and agd converged to @xmath39 , @xmath40 , returning @xmath110 on the training data and @xmath111 on the independent test data from above .",
    "figure [ f - tracks ] shows the convergence tracks of @xmath112 when the two - phase training process is initialized with different initial values for @xmath113 and @xmath114 .",
    "the @xmath56 values between one- and two - phase decompositions generally follow the trend @xmath115 , and this property can be used to help choose initial values during training .",
    "we next apply the trained algorithm to the 21-sponge data .",
    "we find that two - phase agd performs better than one - phase in decomposing the 21-sponge data , which contain absorption signatures from two distinct populations of ism clouds : cold clouds with narrow absorption features and warm clouds with broad absorption features .",
    "we compare the performance of agd to human decompositions using the average difference in the number of modelled components @xmath116 : @xmath117 and the average fractional change in the residual rms , @xmath118 : @xmath119 we find that @xmath120 and for one - phase agd and @xmath121 and for two phase agd . both one - phase and two - phase agd guessed comparable numbers of components , but two - phase agd resulted in lower residual errors compared to human - decomposed spectra , consistent with two - phase agd s higher accuracy ( i.e. , @xmath111 vs. @xmath107 , for two and one - phase agd , respectively ) . a comparison between the resulting number of components and rms residuals between two - phase agd and human results for the individual spectra is shown in figure [ f - correlations ] .                in figure [ f-21sponge ] , we show a scatter plot of the best - fit fwhms and peak amplitudes for all agd and human - derived gaussian components for the 21-sponge data . there are 118 and 120 components detected by agd and human , respectively .",
    "we performed a 2-sample kolmogorov - smirnov test on the amplitudes , fwhms , and derived equivalent widths ( @xmath122 ) of the resulting components from agd vs. human results and find that in each case , the agd and human distributions are consistent with being drawn from identical distributions .",
    "thus , agd results are statistically indistinguishable to human - derived decompositions in terms of the numbers on components , the residual rms values , and the component shapes .",
    "figure [ fig : spectra ] shows the agd guesses , agd - best fits , and human - derived best fits for all 21 spectra in our data set .",
    "observational noise can scatter the measured signals of weak spectral lines below a survey s detection threshold , effectively modifying the measured component distribution by a `` completeness '' function .",
    "the effect of completeness needs to be taken into account in order to make high - precision comparisons between the measured distributions of hi absorption / emission profiles and the predictions of physical models .",
    "s speed and autonomy allows for easy reconstruction of a survey s completeness function , and this information can be used to correct the number counts of observed line components so that one can infer the true component distribution to lower column densities .     per @xmath91 channel .",
    "the black circles represent the 21-sponge detections from agd . detected components with amplitudes @xmath123 have @xmath124 completeness .",
    "+ ]    we demonstrate this procedure by measuring agd s line completeness of 21-sponge hi absorption profiles as a function of amplitude and velocity width using a monte carlo simulation .",
    "we inject a single gaussian component with fixed parameters into sythetic spectra containing realistic observational noise ( @xmath125 per @xmath91 channel ) and then run agd to measure the completeness , which we define as the fraction of successfully - detected components out of 50 trials .",
    "agd s completeness function for the 21-sponge data is shown in figure [ f - completeness ] .",
    "agd obtains @xmath124 completeness for components with @xmath126 and @xmath123 .",
    "regularized derivatives ( section [ s - reg ] ) are insensitive to noise on spatial scales less than that set by the regularization parameter @xmath56 ( equation [ e - scale ] ) . because the observational sensitivity of 21-sponge data is uniform and very high , we next demonstrate that agd is robust to varying noise properties by characterizing the guessed position and fwhm of a gaussian component with fixed shape in data with increasing noise intensity .",
    "figure [ f - stability ] shows that @xmath127 of component guesses remain within @xmath12 distance of the true component positions for noise intensities ranging from 1@xmath128 . over the same range in noise , the guesses fwhms varied by @xmath129 .",
    "therefore , varying the noise properties has little effect on agds performance , making agd a robust tool to analyze heterogeneous datasets with varying sensitivities .    , @xmath130 , and @xmath131 .",
    "different line thicknesses and colors represent different rms noise values , ranging from @xmath132 to @xmath133 .",
    "the horizontal bracket displays the @xmath134 width of the injected component . ]",
    "we have presented an algorithm , named autonomous gaussian decomposition ( agd ) , which produces optimized initial guesses for the parameters of a multi - component gaussian fit to spectral data .",
    "agd uses derivative spectroscopy and bases its guesses on the properties of the first four numerical derivatives of the data .",
    "the numerical derivatives are calculated using regularized optimization , and the smoothness of the derivatives is controlled by the regularization parameter @xmath56 .",
    "superivsed machine learning is then used to train the algorithm to choose the optimal value of @xmath56 which maximizes the accuracy of component identification on a given training set of data with known decompositions .",
    "we test agd by comparing its results to human - derived gaussian decompositions for 21 spectra from the 21-sponge survey ( murray et al .",
    "2014 , in prep . ) . for this test ,",
    "we train the algorithm on results from the independent millenium survey @xcite .",
    "we find that agd performs comparably to humans when decomposing spectra in terms of number of components guessed , the residuals in the resulting fit , and the shape parameters of the resulting components .",
    "agd s performance is affected little by varying observational noise intensity until the point where components fall below the s / n threshold ( i.e. , completeness ) .",
    "combined with monte carlo simulation , we use agd to measure the hi line completeness of 21-sponge data as a function of hi peak optical depth and velocity width .",
    "thus , agd is well suited for helping to interpret the big spectral data incoming from the ska and ska - pathfinder radio telescopes .    the time required for gausspy to decompose a spectrum varies with the number of channels and complexity of components . for data consisting of between 100 to a few thousand channels , and containing between 1 and 15 components ,",
    "the time required ( for initial guesses + final fit ) is between 0.1 to a few seconds for each spectrum on a single @xmath135 computer core .",
    "agd is distinct from bayesian spectral line finding algorithms ( e.g. , * ? ? ?",
    "* ) in terms of the criteria used in deciding the number of components . where the bayesian approach chooses the number based on the bayesian evidence , agd uses machine learning and is motivated by the answers in the training set",
    "this machine learning approach requires one to produce a training set , but allows for more flexibility in telling the algorithm how spectra should be decomposed .    in section [",
    "s : performance ] , we used agd to decompose spectra into gaussian components which correspond to physical clouds in the ism . however , agd can provide a useful parametrization of spectral data even when there is no _ physical _ motivation to represent the data as independent gaussian functions . for example , agd could potentially be used to compress the data volume of wide - bandwidth spectra for easy data transportation , or on - the - fly viewing .",
    "for example , if a @xmath136 channel spectrum contains signals which can be represented by @xmath2 gaussian components , then by recasting the data into gaussian component lists one could achieve a data compression factor of @xmath137 .",
    "this work was supported by the nsf early career development ( career ) award ast-1056780 and by nsf grant no .",
    "c. m. acknowledges support by the national science foundation graduate research fellowship and the wisconsin space grant institution .",
    "we thank elijah bernstein - cooper , matthew turk , and james foster for useful discussions .",
    "we thank lauren michael and the university of wisconsin s center for high - throughput computing for their help and support with htcondor .",
    "cv - c expresses his appreciation towards the aspen center for physics for their hospitality .",
    "the national radio astronomy observatory is a facility of the national science foundation operated under cooperative agreement by associated universities , inc .",
    "the arecibo observatory is operated by sri international under a cooperative agreement with the national science foundation ( ast-1100968 ) , and in alliance with ana g. mndez - universidad metropolitana , and the universities space research association .",
    "agd `` de - blends '' the naive amplitude guesses using the fact that when the parameters @xmath30 and @xmath24 are fixed , the multi - component gaussian model becomes a _ linear _ function of the component amplitudes .",
    "therefore , the naive amplitude estimates can be written as a linear combination of true deblended amplitudes @xmath138 , weighted by the overlap from each neighboring component .",
    "this system of linear equations is expressed in matrix form ( see , e.g. , * ? ? ?",
    "* ) as @xmath139 where @xmath140 the elements of matrix @xmath141 represent the overlap of component @xmath142 onto the center of component @xmath143 . when components are negligibly blended , @xmath141 is equal to the identity matrix and @xmath144 .",
    "the `` true '' de - blended amplitude estimates @xmath145 are then found using the normal equations of linear least squares minimization to be @xmath146 in practice , we compute the solution for @xmath138 through numerical optimization to avoid inverting a possibly singular matrix @xmath147 . if all the de - blended amplitude estimates are greater than zero ( physically valid ) , then they are adopted as the amplitude guesses ; if any are @xmath148 ( caused by errors in the estimates of @xmath24 , @xmath30 , or the number of components ) , the naive amplitudes are retained . therefore , @xmath149",
    "the regularization parameter @xmath56 ( which is generally a multi - dimensional vector ; see , e.g. , appendix [ a - two - phase ] ) is tuned to maximize the accuracy of component guesses ( equation [ e - acc ] ) using gradient descent with momentum .",
    "we define the cost function @xmath150 that we wish to minimize in order to find this solution as @xmath151    in traditional gradient descent , updates to the parameter vector @xmath56 are made by moving in the direction of greatest decrease in the cost function , i.e. , @xmath152 , and the learning rate @xmath153 controls the step size .",
    "our cost function @xmath154 is highly non - convex , so we use gradient descent ( see , e.g. , * ? ? ?",
    "* ) with added momentum to push through local noise valleys",
    ". therefore , at the @xmath155 iteration , our parameter update is given by @xmath156 where the `` momentum '' @xmath157 controls the degree to which the previous update influences the current update .    because the decision function ( i.e. , equations [ e - d1][e - d3 ] ) representing the success or failure for individual component guesses is binary in nature , the cost function @xmath158 is a piecewise - constant surface on small scales ( see figure [ f - tracks ] ) .",
    "therefore , in order to probe the large - scale slope of the cost function surface , we use a relatively large value for the finite difference step size when computing the gradient .",
    "for example , the @xmath159 component of the gradient in equation [ e - update ] is defined according to @xmath160 where @xmath161 is the finite - difference step size which we set to @xmath162 .",
    "figure [ f - tracks ] shows example tracks of @xmath163 when using gradient descent with momentum during agd s two - phase training on the 21-sponge data .",
    "we find that small - scale local optima are ignored effectively during the search for large - scale optima .",
    "two - phase decompositions allow researchers to decompose spectra which contain components that are drawn from two distributions with very different widths .",
    "gausspy performs two - phase decomposition by first applying the usual agd algorithm but with a non - zero threshold used in equation [ e - c2 ] : @xmath164 , which locates only the narrowest components in the data so that they can be removed .",
    "the parameters of just these narrow components are next found by minimizing the sum of squared residuals @xmath165 between the second derivative of the data and the second derivative of a model consisting of only these narrow components , @xmath166 , given by @xmath167 the narrow components are fit to the data on the basis of their second derivatives so that the signals from wider components , which they may be superposed on , are attenuated by a factor @xmath168 . the residual spectrum is then fed back into agd to search for broader components using a larger value of @xmath56 and setting @xmath169 .",
    "ivezic , v. , connolly , a.  j. , vanderplas , j.  t. , & gray , a. 2014 , statistics , data mining , and machine learning in astronomy , stu - student edition edn . ,",
    "princeton series in modern observational astronomy ( princeton university press ) , 544"
  ],
  "abstract_text": [
    "<S> we present a new algorithm , named autonomous gaussian decomposition ( agd ) , for automatically decomposing spectra into gaussian components . </S>",
    "<S> agd uses derivative spectroscopy and machine learning to provide optimized guesses for the number of gaussian components in the data , and also their locations , widths , and amplitudes . </S>",
    "<S> we test agd and find that it produces results comparable to human - derived solutions on 21 cm absorption spectra from the 21 cm spectral line observations of neutral gas with the evla ( 21-sponge ) survey . </S>",
    "<S> we use agd with monte carlo methods to derive the hi line completeness as a function of peak optical depth and velocity width for the 21-sponge data , and also show that the results of agd are stable against varying observational noise intensity . </S>",
    "<S> the autonomy and computational efficiency of the method over traditional manual gaussian fits allow for truly unbiased comparisons between observations and simulations , and for the ability to scale up and interpret the very large data volumes from the upcoming square kilometer array and pathfinder telescopes . </S>"
  ]
}