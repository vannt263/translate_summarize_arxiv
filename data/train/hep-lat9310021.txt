{
  "article_text": [
    "during the last years , monte carlo simulations of scientific problems have turned out to be of outstanding importance @xcite .",
    "it is now a general belief within the community of computational scientists that multiprocessor systems of the mimd type with distributed memory are the most appropriate computer systems to provide the computational resources needed to solve the most demanding problems e.g.  in high energy physics , material sciences or meteorology .",
    "+ implementing a synchronous parallel algorithm on a heterogeneous mimd system with distributed memory ( e.g.  on a cluster of different workstations ) , a load balancing between the processors of the system ( taking into account the actual resources being available on each node ) turns out to be crucial , because the processor with the least resources determines the speed of the complete algorithm .",
    "+ the heterogenity of the mimd system may not only result because of heterogeneous hardware resources , but also due to a _ heterogeneous use _ of homogeneous hardware resources ( e.g.  on a workstation cluster , there may exist several serial tasks running on some of the workstations of the cluster for some time in addition to the parallel application ; this results in a temporary heterogenity of the cluster , even if the workstations of the cluster are identical ) .",
    "this kind of heterogenity can in general only be detected during the runtime of the parallel algorithm .",
    "+ therefore , the usual approach of _ geometric parallelization _",
    "@xcite to parallelize algorithms by a _ static _ decomposition of a domain into subdomains and associating each subdomain with a processor of the multiprocessor system is not appropriate for a heterogeneous multiprocessor system . instead , on a heterogeneous system",
    "this _ geometric parallelization _ should be done _",
    "dynamic_. + in the sequel , we consider geometrically parallelized monte carlo simulations consisting of update algorithms ( e.g. metropolis or heatbath algorithms ) defined on e.g. spins at the sites of a lattice ( e.g. in ising models ) , matrices defined on the links of the lattice ( e.g. in lattice gauge theories ) , etc .. in general , a synchronization of the parallel processes takes place after each sweep through the lattice ( each iteration ) .",
    "+ for this class of simulations , we will introduce an algorithm for dynamic load balancing . implementing and testing the algorithm for the two  dimensional ising model",
    ", it will be shown , that this algorithm may drastically improve the performance of the monte carlo simulation on a heterogeneous multiprocessor system .",
    "+ the paper consist of basically three parts . in the first part we will introduce a simple model for analyzing the performance of synchronous monte carlo simulations on multiprocessor systems with distributed memory , in the second part we will present our algorithm for the dynamic load balancing and finally we will present our numerical results . +",
    "we consider a heterogeneous multiprocessor system consisting of @xmath0 processors . for a parallelized monte carlo simulation we measure at times @xmath1 the times @xmath2",
    "the simulation has taken for a fixed number of sweeps on each of the processors denote `` wall clock times '' measured in seconds and @xmath1 denotes the `` internal '' time of the simulation , measured in numbers of monte carlo sweeps . ] .",
    "the parallelization is done by _ geometric parallelization _ , associating a sublattice @xmath3 with a characteristic scale @xmath4 ( e.g. a characteristic side of the sublattice , its volume , etc .. ) with each of the processors .",
    "these scales are choosen such that    @xmath5    holds . here",
    "@xmath6 denotes the scale of the complete lattice and the @xmath7 are real numbers with @xmath8 and @xmath9 respectively of the @xmath10 is quite arbitrary , one could choose e.g. @xmath11 for all @xmath3 . ] . using these parameters , we can calculate quantities @xmath12 , characterizing the computing resources of processor @xmath3 at time @xmath1 :    @xmath13    assuming , that the resources of the processors vary slowly compared with the time the simulation takes for one sweep    @xmath14    we set    @xmath15    now we reinterpret formula ( [ pidef ] ) : for _ fixed _",
    "@xmath16 we want to calculate a set of @xmath17 , such that the time for the next sweep ( _ excluding _ the time spent for communication ) , @xmath18 , etc .",
    "and the coefficients @xmath19 to be taken at @xmath20 . for the sake of clarity",
    "we therefore drop this index throughout this section . ]",
    "@xmath21    for @xmath22 has a minimal value    @xmath23    a necessary condition for this solution is obviously , that all @xmath18 must be equal",
    ". then we could easily make @xmath24 smaller by a redefinition of @xmath25 and @xmath26 with @xmath27 . ] . remembering the normalization condition on the constants @xmath28 , we arrive at    @xmath29    with @xmath30 . using ( [ nsweept ] ) this results in    @xmath31    for a homogenous system ( all @xmath16 equal ) ( [ cisol ] ) would give    @xmath32    for a heterogenous system this choice of @xmath19 results in ( using ( [ nsweept ] ) )    @xmath33    as the _ homogenity _ @xmath34 of the multiprocessor system we define therefore the ratio of @xmath35 with ( [ tninsol ] ) :    @xmath36    with @xmath37 .",
    "the speedup @xmath38 that can be obtained by the dynamic resizing of the sublattices is the inverse of @xmath34 :    @xmath39    rewriting ( [ tminsol ] ) in terms of @xmath34 , we arrive at    @xmath40    for later comparison with our experimental data , let us look at the special case @xmath41 . in this case",
    "we have for @xmath35 :    @xmath42    with @xmath43 . ] . without load balancing ,",
    "the time for the total simulation will be determined by the time spent on processor @xmath0 : @xmath44 figure [ theory ] shows the `` optimal '' curve ( using dynamic load balancing ) : @xmath45 and the one obtained without any load balancing : @xmath46 for @xmath47 .",
    "these curves will be compared with our experimental results .",
    "in this section we describe our algorithm to perform the dynamic load balancing , based on the performance model described above .",
    "+      * a characteristic scale @xmath6 of the lattice ( e.g.  a side length of the lattice ) . *",
    "the number @xmath0 of processors of the multiprocessor system . * the total number of iterations @xmath48 to be done by the simulation . *",
    "the number of iterations @xmath49 after which a resizing of the sublattices may take place . *",
    "a control parameter @xmath50 with @xmath51 to determine if a resizing should be done .",
    "+      * a dynamic resizing of the domains associated with each processor of the multiprocessor system , taking into account the actual resources of the processors .",
    "+      * read the input .",
    "* introduce characteristic scales @xmath4 of the sublattices with @xmath52 and @xmath53 , where @xmath3 denotes the processors and @xmath1 counts the number of resizings that have been done . *",
    "calculate the initial characteristic sizes of the sublattices @xmath54 for all processors according to @xmath55 .",
    "* associate each of the sublattices with one of the processors .",
    "* do on each processor @xmath52 ( in parallel ) * *   * * set @xmath56 . * * for @xmath57 : * * *   * * * perform iteration of the monte carlo update algorithm on the sublattice . * * * if @xmath58 then * * * *   * * * * measure the wall  clock time @xmath59 spent on processor @xmath3 for doing the calculations _ excluding _ the time spent for communications . * * * * calculate @xmath60 to measure the actual resources of each node of the multiprocessor system . * * * * communicate the results to all processors of the multiprocessor system . * * * * calculate the new characteristic sizes @xmath61 of the sublattices with @xmath62 and @xmath63 * * * * resize the sublattices if @xmath64 + this step may include the communication of parts of the sublattices between the processors and is certainly the critical part of the algorithm .",
    "we will introduce an algorithm for this resizing for a special case below . * * * * set @xmath65 . * * * *   * * *   * *     in the sequel we present an algorithm for resizing the sublattices for the special case that the splitting of the sublattices takes place only in one dimension .",
    "we use the _ host  node _ ( respectively _ client  server _ ) parallel programming paradigm ( see @xcite ) , associating each sublattice with a server process and leaving the more administration oriented tasks ( like reading the global parameters of the simulation , starting the server processes , etc . ) to the host process .",
    "let us assume the size of the lattice in the direction of the splitting to be @xmath6 and that the host process holds arrays @xmath66 $ ] with ( @xmath67 ) for the `` monte carlo times '' @xmath1 and @xmath68 containing the first coordinate in that direction of the `` slice '' of the lattice associated with each processor : @xmath69 = 1 \\leq a[2 ] \\leq ... \\leq a[n+1 ] = l+1 . \\label{arraydef}\\ ] ] ( in terms of the constants @xmath19 this would mean @xmath70-a[i]}{l}$ ] . )",
    "now the host process sends messages containing instructions to the node processes in two passes :    * for @xmath71 * *   * * if ( @xmath72 - a_{t_{mc}-1}[i ] ) > 0 $ ] ) * * *   * * * send message to server @xmath3 telling it to send its `` first '' @xmath73 slices to processor @xmath74 * * *   * *   * for @xmath75 * *   * * if @xmath76 - a_{t_{mc}-1}[i+1 ] ) < 0 $ ] * * *   * * * send message to server @xmath3 telling it to send its `` last '' @xmath73 slices to processor @xmath77 * * *   * *     the node processes wait for messages from either the host process or from neighbouring node processes . if there are not enough slices available on a node process to be sent , the node process waits for a message from a neighbour node process to receive additional slices .",
    "the two pass algorithm prevents deadlocks .",
    "+ if the resources of the processors of the multiprocessor system change very rapidly , a multiple communication of data may be necessary and will drastically reduce the efficiency of this algorithm .",
    "but this is consistent with the fact , that our complete approach to dynamic load balancing is anyhow only valid for systems with moderatly varying resources , as was already pointed out at the beginning of section 2 , see ( [ psimp ] ) .",
    "the above described algorithm has been implemented for the parallelized simulation of the two  dimensional ising model on a cluster of four ibm risc system/6000  550 workstations @xcite , using the pvm programming environment @xcite .",
    "here we have a two  dimensional lattice which is divided into stripes .",
    "the objects defined on the lattice sites are spins ( i.e. binary variables ) and an iteration defined on these objects consists e.g. of a metropolis algorithm to generate a new spin configuration on the lattice .",
    "each stripe is associated with one workstation .",
    "the characteristic scales of the stripes are their widths and the characteristic scale of the lattice is the sum of all widths .",
    "+ the cluster being completely homogeneous , the heterogeneous situation has been simulated by starting independent processes on one or several nodes of the cluster .",
    "this allows the heterogenity of the multiprocessor system to be introduced in a controlled manner , i.e. to vary the homogenity @xmath34 and measure ( [ optcurve ] ) resp .",
    "( [ nooptcurve ] ) as functions of @xmath34 .",
    "our results are presented in figure [ measurements ] for a @xmath78 and a @xmath79 lattice .",
    "one clearly sees the qualitative agreement with the prediction of our performance model , see figure [ theory ] . ] .    a different point of view consists of looking at the ( mega ) updates done by the metropolis algorithm on each spin per second ( `` mups '' ) .",
    "these are presented for a @xmath78 and a @xmath79 lattice as a function of @xmath34 in figures [ mups1000 ] and [ mups2000 ] with the dynamic load balancing being done after a certain number of sweeps .",
    "it turns out , that the optimal number of sweeps between the load balancing to be performed depends on the size of the problem . +",
    "we have introduced an algorithm for dynamic load balancing for synchronous monte carlo simulations on a heterogenous multiprocessor system with distributed memory . implementing this algorithm for the two  dimensional ising model ,",
    "we have shown , that it may result in a speedup of a factor 5 - 6 for the above described class of geometrically parallelized algorithms . in many cases , the implementation of the algorithm is straight forward with only little overhead in calculation and communication . for homogeneous systems ,",
    "almost no performance is lost because the algorithm detects that no resizing is necessary by applying ( [ resizecond ] ) . for systems with slowly changing heterogenity",
    ", the algorithm converges very fast and the requirements of the algorithm concerning the computing environment are minimal : the system only has to provide a routine to measure the wall ",
    "clock time ; such a routine should be available on all operating systems .",
    "+ considering the generality of the algorithm introduced above , it may also be useful applied to problems other than monte carlo simulations , e.g. in parallel iterative methods for solving linear or nonlinear equations appearing in engineering problems .    99 m.creutz ( ed . ) , quantum fields on the computer ( world scientific publishing co. pte .",
    "singapore 1992 ) .",
    "k. binder ( ed . ) , the monte carlo method in condensed matter physics , topics in applied physics , vol .",
    "71 , ( springer  verlag , berlin , heidelberg 1992 ) . k. binder ( ed . ) , monte carlo methods in statistical physics , topics in current physics , vol . 7 , 2nd edition ( springer  verlag , berlin , heidelberg 1986 )",
    ". k. binder , d.w.heermann , monte carlo simulation in statistical physics : an introduction , springer series in solid  state sciences , vol .",
    "80 ( springer  verlag , berlin , heidelberg 1988 ) .",
    "j.p.mesirov ( ed . ) , very large scale computation in the 21st century d.w . heermann and a.n .",
    "burkitt , parallel algorithms in computational science , springer series in information sciences ( springer  verlag , berlin , heidelberg 1991 ) .",
    "p. altevogt , a. linke , parallelization of the two  dimensional ising model on a cluster of ibm risc system/6000 workstations , parallel comp .",
    "19 , vol.9 ( 1993 ) .",
    "akl , the design and analysis of parallel algorithms , prentice  hall international editions ( prentice  hall , inc . 1989 ) .",
    "a. beguelin , j.j .",
    "dongarra , g.a .",
    "geist , r.manchek and v.s .",
    "sunderam , a user s guide to pvm parallel virtual machine .",
    "technical report ornl / tm-118 26 , oak ridge national laboratory , july 1991 . v.s .",
    "sunderam , pvm : a framework for parallel distributed computing , concurrency : practice&experience vol.2 no.4 , dec ."
  ],
  "abstract_text": [
    "<S> we describe an algorithm for dynamic load balancing of geometrically parallelized synchronous monte carlo simulations of physical models . </S>",
    "<S> this algorithm is designed for a ( heterogeneous ) multiprocessor system of the mimd type with distributed memory . </S>",
    "<S> the algorithm is based on a dynamic partitioning of the domain of the algorithm , taking into account the actual processor resources of the various processors of the multiprocessor system . </S>",
    "<S> +   + _ keywords : _ monte carlo simulation ; geometric parallelization ; synchronous algorithms ; dynamic load balancing ; dynamic resizing    peter altevogt ,    andreas linke + institute for supercomputing and applied mathematics ( isam ) + heidelberg scientific center + ibm deutschland informationssysteme gmbh + vangerowstr . </S>",
    "<S> 18 + 69115 heidelberg + tel . : 06221594471 + germany    ibm preprint 75.93.08 , hep - lat/9310021 </S>"
  ]
}