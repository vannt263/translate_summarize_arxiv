{
  "article_text": [
    "it is well known  ( see e.g. @xcite and @xcite ) , that the capacity of a discrete memoryless channel ( dmc ) is not increased by feedback .",
    "nevertheless , feedback can help in at least two ways : for a fixed target error probability , feedback can be used to reduce the sender / receiver complexity and/or to reduce the expected decoding delay .",
    "an example is the binary erasure channel , where feedback makes it possible to implement a communication strategy that is extremely simple and also minimizes the delay .",
    "the strategy is simply to send each information bit repeatedly until it is received unerased .",
    "this strategy is capacity achieving , results in zero probability of error , and reproduces each information bit with the smallest delay among all possible strategies .",
    "the reliability function  also called the error exponent  is a natural way to quantify the benefit of feedback . for block codes on channels without feedback the reliability function is defined as @xmath0 where @xmath1 is the smallest possible error probability of length @xmath2 block codes with @xmath3 codewords .",
    "the decoding time @xmath2 in a communication system with feedback may depend on the channel output sequence .",
    "if it does , the decoding time @xmath2 becomes a random variable and the notions of rate and reliability function need to be redefined .",
    "following burnashev  @xcite , in this case we define the rate as @xmath4}},\\ ] ] where @xmath3 is the size of the message set .",
    "similarly we define the reliability function as @xmath5 where @xmath6 is the smallest error probability of a variable - length block code with feedback that transmits one of @xmath3 equiprobable messages by means of @xmath7 or fewer channel uses on average .",
    "as we remark below , the limit exists for all rates from zero to capacity .",
    "burnashev showed that for a dmc of capacity @xmath8 , the reliability function @xmath9 equals @xmath10 where @xmath11 is determined by the two `` most distinguishable '' channel input symbols as @xmath12 where @xmath13 is the probability distribution of the channel output when the input is @xmath14 , and @xmath15 denotes the kullback - liebler divergence between two probability distributions .",
    "it is remarkable that ( [ eq : bexp ] ) determines the reliability function exactly for all rates .",
    "in contrast , the reliability function without feedback is known exactly only for rates above a critical rate . below the critical rate",
    "only upper and lower bounds to the reliability function without feedback are known . for a binary symmetric channel",
    "the situation is depicted in fig .",
    "[ fig : exponents ] .",
    "( 220,270 ) ( 0,0).,title=\"fig : \" ] ( 20,0)(0,0)@xmath16 ( 84,0)(0,0)@xmath17 ( 200,0)(0,0)@xmath8 ( -45,150)(0,0 ) ( 0,20)(0,0)0 ( -7,145)(0,0)0.26    burnashev showed that @xmath18 by showing that for every communication scheme @xmath19\\geq \\biggl(\\frac{\\ln m}{c } - \\frac{\\ln    p_e}{c_1}\\biggr ) \\bigl(1-o(1)\\bigr)\\ ] ] where @xmath20 represents positive terms that tend to zero as @xmath21 tends to infinity , and that there exists schemes with @xmath22\\leq \\biggl(\\frac{\\ln m}{c } - \\frac{\\ln    p_e}{c_1}\\biggr ) \\bigl(1+o(1)\\bigr),\\ ] ] where @xmath20 now represents positive terms that tend to zero as both @xmath3 and @xmath21 tend to infinity .    for a plausibility argument that justifies ( [ eq : ach ] ) it suffices to summarize the achievability construction by yamamoto and itoh  @xcite .",
    "their scheme relies on two distinct transmission phases that we shall call the communication and the confirmation phase , respectively . in the communication",
    "phase the message is encoded using a fixed - length block code and the codeword is transmitted over the forward channel .",
    "the decoder makes a tentative decision based on the corresponding channel output .",
    "the encoder knows the channel output and can run the algorithm used by the receiver to determine the tentative decision . if the tentative decision is correct , in the confirmation phase the encoder sends ack .",
    "otherwise it sends nack .",
    "acks and nacks are sent via a fixed - length repetition code .",
    "( the code consists of two codewords ) . during the confirmation",
    "phase the decoder performs a binary hypothesis test to decide if ack or nack was transmitted .",
    "if ack is decoded , the tentative decision becomes final and the transmission of the current message ends , leaving the system free to restart with a new message .",
    "if nack is decoded , the tentative decision is discarded and the two phase scheme restarts with the same message .",
    "the overhead caused by retransmissions is negligible if the probability of decoding nack is small .",
    "this is the case if both the error probability of the communication phase as well as that of the confirmation phase are small . assuming that this is the case , the number of channel uses for the communication phase ( including repetitions ) is slightly above @xmath23 .",
    "the probability of error is the probability that nack is sent and ack is decoded . in the asymptotic regime of interest",
    "this probability is dominated by the probability that ack is decoded given that nack is sent . in a straightforward application of stein s lemma",
    "@xcite one immediately sees that we can make this probability to be slightly less than @xmath24 ( thus achieve error probability @xmath24 ) by means of a confirmation code of length slightly above @xmath25 . summing up",
    ", we see that we can make the error probability arbitrarily close to @xmath24 by means of slightly more than @xmath26 channel uses on average .",
    "this confirms ( [ eq : ach ] ) .    to obtain the converse ( [ eq : whatweprove ] ) , burnashev investigated the entropy of the _ a posteriori _ probability distribution over the message set .",
    "he showed that the average decrease of this entropy due to an additional channel output observation , as well as the average decrease of the logarithm of this entropy , are bounded .",
    "he uses these bounds to form two submartingales , one based on the entropy of the _ a posteriori _ distribution and the other based on the logarithm of this entropy .",
    "he then constructs a single submartingale by patching these two together .",
    "then doob s optional stopping theorem is applied to this submartingale and the desired bound on the expected decoding time , which is a stopping time , is obtained .",
    "burnashev s proof is an excellent example of the power of martingales , however both the sophistication of the martingale construction and the use of the logarithm of entropy leaves the reader with little insight about some of the terms in the converse bound . while it is easy to see that @xmath23 channel uses are needed in average , it was not as clear why one needs an additional @xmath25 channel uses .",
    "the connection of the latter term to binary hypothesis testing suggested the existence of an operational justification . the work presented in this paper started as an attempt to find this operational justification .",
    "our converse somewhat parallels the yamamoto ",
    "itoh achievability scheme .",
    "this suggests that that a communication and confirmation phase may be implicit components of any scheme for which the probability of error decreases with the largest possible exponent .",
    "our approach has been generalized by como , yksel and tatikonda in @xcite to prove a similar converse for variable - length block codes on finite state markov channels .",
    "we consider a discrete memoryless channel , with finite input alphabet @xmath27 , finite output alphabet @xmath28 , and transition probabilities @xmath29 .",
    "we will denote the channel input and output symbols at time @xmath30 by @xmath31 and @xmath32 , and denote the corresponding vectors @xmath33 and @xmath34 by @xmath35 and @xmath36 , respectively .",
    "a perfect causal feedback link is available , i.e. , at time @xmath30 the encoder knows @xmath37 .",
    "( following common practice , random variables are represented by capital letters and their realizations are denoted by the corresponding lowercase letters . )    we will assume , without loss of generality , that the channel has no `` useless outputs symbols '' , i.e. , no symbols @xmath38 for which @xmath39 for every @xmath14 . note that for channels for which @xmath11 is infinite , the lower bound to the expected decoding time is a restatement of the fact that feedback does not increase capacity .",
    "we will therefore restrict our attention to channels for which @xmath40 .",
    "for such channels , @xmath41 for every @xmath14 and @xmath38 ; if not , there exists an @xmath14 and @xmath38 for which @xmath42 . since @xmath38 is reachable from some input , there also exists an @xmath43 for which @xmath44 .",
    "but then @xmath45 contradicting the finiteness of @xmath11 . the fact that both @xmath46 and @xmath47 are finite sets lets us further conclude that for the channels of interest to this paper , there is a @xmath48 for which @xmath49 for every @xmath14 and @xmath38 .",
    "a variable - length block code is defined by two maps : the encoder and the decoder .",
    "the encoder functions @xmath50 , where @xmath51 is the set of all possible messages , determine the channel input @xmath52 based on the message @xmath53 and on past channel outputs @xmath54 . the decoder function @xmath55 , where @xmath56 is the receiver observation space until the decoding time @xmath2 , i.e. , @xmath57 takes values in @xmath58 .",
    "the decoding time @xmath2 should be a stopping time with respect to the receiver observation @xmath36 otherwise the decision of when to decode would depend on future channel outputs and the decoder would no longer be causal .",
    "we treat the case when @xmath59 } < \\infty$ ] , and point out that what we are setting out to prove , namely ( [ eq : whatweprove ] ) , is trivially true when @xmath59 } = \\infty$ ] .",
    "the codes we consider here differ from non - block ( also called sequential ) codes with variable delay , such as those studied in @xcite and  @xcite . in sequential coding , the message ( typically as an infinite stream of bits ) is introduced to the transmitter and decoded by the receiver in a progressive fashion .",
    "delay is measured separately for each bit , and is defined as the time between the introduction and decoding of the bit .",
    "this is in contrast to the codes considered in this paper , where the entire message is introduced to the transmitter at the start of communication , and @xmath2 measures the duration of the communication .",
    "due to their different problem formulation , sequential codes with feedback have reliability functions that differ from those for variable - length block codes , just as fixed constraint length convolutional codes have reliability functions that differ from those of fixed - length block codes .    the observation space @xmath56 is a collection of channel output sequences and for a dmc with feedback the length of these sequences may vary .",
    "( the length of the channel input itself may depend on the channel realization ) . nevertheless , these sequences have the property of being prefix - free ( otherwise the decision to stop would require knowledge of the future ) .",
    "thus , @xmath58 can be represented as the leaves of a complete @xmath60-ary tree @xmath61 ( complete in the sense that each intermediate node has @xmath60 descendants ) , and has expected depth @xmath59 } < \\infty$ ] .",
    "note that the decision time @xmath2 is simply the first time the sequence @xmath62 of channel outputs hits a leaf of @xmath61 .",
    "furthermore we may label each leaf of @xmath63 with the message decoded by the receiver when that leaf is reached . this way the decoder is completely specified by the labeled tree @xmath63 .",
    "the message statistics , the code , and the transition probabilities of the channel determine a probability measure on the tree @xmath63 .",
    "the binary case ( @xmath64 ) will play a key role in our main proof . in this section",
    "we assume that the message set contains only two elements .",
    "we will arbitrarily denote the two hypotheses by @xmath65 and @xmath66 ( ack and nack , respectively ) .",
    "we denote by @xmath67 and @xmath68 the corresponding probability distributions on the leaves of @xmath63 .",
    "the following proposition bounds the kullback - leibler divergence @xmath69 .",
    "it will be used in the main result of this section to bound the error probability of binary hypothesis testing with feedback .",
    "the reader familiar with stein s lemma will not be surprised by the fact that the kullback - leibler divergence @xmath69 plays a key role in binary hypothesis testing with feedback .",
    "the steps here closely parallel those in and ( * ? ? ?",
    "[ prop : expkl ] for any binary hypothesis testing scheme for a channel with feedback @xmath70}}\\ ] ] where @xmath2 is the decision stopping time , @xmath59 } < \\infty$ ] , and @xmath71}}$ ] denotes the expectation of @xmath2 conditioned on hypothesis @xmath65 .    in the following ,",
    "we will denote probability under hypothesis @xmath65 by @xmath72 and probability under hypothesis @xmath66 by @xmath73 .",
    "let @xmath74 so that @xmath75}}$ ] , and the proposition is equivalent to the statement @xmath76}}\\leq 0 $ ] .",
    "observe that @xmath77 note now that @xmath78 } } & = { { e\\left [ { \\left.\\ ! { \\ln \\frac {          \\operatorname{p_{a}}(y_k|y^{k-1 } ) } { \\operatorname{p_{n}}(y_k|y^{k-1 } ) } } \\right| { a , y^{k-1 } } } \\right ] } }   \\nonumber\\\\          & = { { e\\left [ { \\left.\\ ! { \\ln \\frac {          \\operatorname{p_{a}}(y_k|x_k = f_k(a , y^{k-1}),y^{k-1})}{\\operatorname{p_{n}}(y_k|x_k = f_k(n , y^{k-1}),y^{k-1 } ) } } \\right| { a ,          x_k = f_k(a , y^{k-1 } ) , y^{k-1 } } } \\right ] } } \\nonumber\\\\ & = \\sum_{y \\in { { \\cal y } } }          { { \\pr\\left\\ { { \\left.\\ ! { y_k = y } \\right| { x_k = f_k(a , y^{k-1 } ) } } \\right\\ } } }   \\ln\\frac {          { { \\pr\\left\\ { { \\left.\\ ! { y_k = y } \\right| { x_k = f_k(a , y^{k-1 } ) } } \\right\\ } } } } { { { \\pr\\left\\ { { \\left.\\ ! { y_k = y }",
    "\\right| { x_k = f_k(n , y^{k-1 } ) } } \\right\\ } } } }          \\nonumber\\\\   & \\leq { c_1 } , \\end{aligned}\\ ] ] where @xmath79 is the encoder function at time @xmath80 .",
    "consequently , @xmath81 is a supermartingale under hypothesis @xmath65 .",
    "observe that the existence of a @xmath82 for which @xmath83 for all @xmath84 implies that @xmath85 .",
    "we can now use doob s optional - stopping theorem ( see e.g.  ( * ? ? ?",
    "* sec .  10.10 ) ) to conclude that @xmath86 } }          \\leq 0 $ ]",
    ".    we can apply proposition  [ prop : expkl ] to find a lower bound on the error probability of a binary hypothesis testing problem with feedback .",
    "the bound is expressed in terms of the expected decision time .",
    "[ prop : pe - bound ] the error probability of a binary hypothesis test performed across a dmc with feedback and variable - length codes is lower bounded by @xmath87}}\\ ] ] where @xmath88 and @xmath89 are the _ a priori _ probabilities of the hypotheses .",
    "each decision rule corresponds to a tree where each leaf @xmath57 is associated with a decoded hypothesis @xmath90 .",
    "thus we can partition the leaves into two sets corresponding to the two hypotheses .",
    "@xmath91 where @xmath92 is the decision region for hypothesis @xmath65 .",
    "the log sum inequality @xcite ( or data processing lemma for divergence ) implies @xmath93 by proposition  [ prop : expkl ] , @xmath94 } } \\ge { d\\left ( \\left.\\ ! { { { \\cal q}_{a } } } \\right\\|{{{\\cal q}_{n } } } \\right)}$ ] , thus  ( [ eq : kl - logsum ] ) can be re - arranged to give @xmath95}}\\geq - { { \\cal q}_{a}}({{\\cal s } } ) \\ln { { \\cal q}_{n}}({{\\cal s } } )      -\\operatorname{h}({{\\cal q}_{a}}(\\bar{{{\\cal s}}})),\\ ] ] where @xmath96 is the binary entropy function . writing the overall probability of error in terms of marginal error probabilities yields @xmath97 which allows us to bound @xmath98 as @xmath99 substituting back into  ( [ eq : depth_conda ] ) yields a bound on the expected depth of the decision tree conditioned on @xmath65 just in terms of @xmath67 and the _ a priori _ message probabilities @xmath100 } } \\geq -{{\\cal q}_{a}}({{\\cal s } } ) \\ln \\frac{{{\\it p_{e } } } }        { \\min \\{p_a , p_n\\}}- \\operatorname{h}({{\\cal q}_{a}}(\\bar{{{\\cal s}}})).\\ ] ] following identical steps with the roles of @xmath65 and @xmath66 swapped yields @xmath101 } } \\geq -{{\\cal q}_{n}}(\\bar{{\\cal s } } )    \\ln \\frac{{{\\it p_{e } } } } { \\min \\{p_a , p_n\\}}- \\operatorname{h}({{\\cal q}_{n}}({{\\cal s}})).\\ ] ] we can now average both sides of  ( [ eq : treedeptha ] ) and  ( [ eq : treedepthr ] ) by weighting with the corresponding _ a priori _ probabilities . if we do so and use the facts that @xmath102 is the probability of making the correct decision and @xmath103 is the probability of making an error together with the concavity of the binary entropy function , we obtain the following unconditioned bound on the depth of the decision tree @xmath104 } & \\geq -(1-{{\\it p_{e } } } ) \\ln \\frac{{{\\it p_{e}}}}{\\min\\{p_a , p_n\\}}-   \\operatorname{h}({{\\it p_{e } } } ) \\\\ & \\geq      -\\ln { { \\it p_{e}}}-2\\operatorname{h}({{\\it p_{e}}})+\\ln\\min\\{p_a , p_n\\}\\\\ & \\ge -\\ln { { \\it p_{e}}}-2\\ln 2 +      \\ln\\min\\{p_a , p_n\\}.   \\end{aligned}\\ ] ]    solving for @xmath105 completes the proof .",
    "it is perhaps worthwhile pointing out why the factor @xmath106 arises : if one of the hypotheses has small _ a priori _ probability , one can achieve an equally small error probability by always deciding for the other hypothesis , irrespective of the channel observations .",
    "given the channel observations @xmath107 , one can calculate the _ a posteriori _",
    "probability @xmath108 of any message @xmath109 . recall that a maximum _ a posteriori _ ( map ) decoder asked to decide at time @xmath30 when @xmath110 will chose ( one of ) the message(s ) that has the largest _ a posteriori _",
    "probability @xmath111 .",
    "the probability of error will then be @xmath112 .",
    "similarly , we can define the probability of error of a map decoder for each leaf of the observation tree @xmath63 .",
    "let us denote by @xmath113 the probability of error given the observation @xmath114 .",
    "the unconditioned probability of error is then @xmath115}$ ] .    for any fixed @xmath116",
    "we can define a stopping time @xmath117 as the first time that the error probability goes below @xmath118 , if this happens before @xmath2 , and as @xmath2 otherwise : @xmath119    if @xmath120 exceeds @xmath118 , then we are certain that @xmath121 , and @xmath122 for all @xmath123 , so the event @xmath124 is included in the event @xmath125 .",
    "( we have inclusion instead of equality since @xmath126 does not exclude @xmath125 . )",
    "thus @xmath127 where the second inequality is an application of markov s inequality .",
    "given a particular realization @xmath107 we will denote the entropy of the _ a posteriori _",
    "distribution @xmath128 as @xmath129 .",
    "then @xmath130 is a random variable is commonly written as @xmath131 .",
    "we can not use the standard notation since it becomes problematic when we substitute @xmath36 for @xmath107 as we just did . ] and @xmath132 } = h(w|y^n)$ ] . if @xmath133 , then from fano s inequality it follows that @xmath134 the expected value of @xmath135 can be bounded by conditioning on the event @xmath136 and its complement then applying  ( [ eq : htau_bound ] ) and then  ( [ eq : peexdelta ] ) as follows @xmath137 } & = { { e\\left [ { \\left.\\ ! { { { { \\mathcal h}(w|y^{{\\tau } } ) } } } \\right| { { { \\it p_{e}}}(y^{\\tau})\\leq    \\delta } } \\right]}}{\\pr\\left\\{{{{\\it p_{e}}}(y^{\\tau})\\leq \\delta}\\right\\ } } +   { { e\\left [ { \\left.\\ ! { { { { \\mathcal h}(w|y^{{\\tau } } ) } } } \\right| { { { \\it p_{e}}}(y^{\\tau } ) >    \\delta } } \\right]}}{\\pr\\left\\{{{{\\it p_{e}}}(y^{\\tau } ) > \\delta}\\right\\ } } \\\\ & \\leq \\left(\\operatorname{h}(\\delta ) + \\delta \\ln m    \\right){\\pr\\left\\{{{{\\it",
    "p_{e}}}(y^{\\tau})\\leq \\delta}\\right\\ } } + ( \\ln m ) { \\pr\\left\\{{{{\\it p_{e}}}(y^{\\tau } ) > \\delta)}\\right\\ } } \\\\    & \\leq \\operatorname{h}(\\delta )   + \\left ( \\delta+\\frac{{{\\it p_{e}}}}{\\delta } \\right ) \\ln m.\\end{aligned}\\ ] ] this upper bound on the expected posterior entropy at time @xmath117 can be turned into a lower bound on the expected value of @xmath117 by using the channel capacity as an upper bound to the expected change of entropy .",
    "this notion is made precise by the following lemma ,    [ prop : exp_tau ] for any @xmath138 @xmath139 } \\ge    \\left(1-\\delta-\\frac{{{\\it p_{e}}}}{\\delta}\\right)\\frac { \\ln    m}{{c}}-\\frac{\\operatorname{h}(\\delta)}{{c}}.\\ ] ]    observe that @xmath140 is a submartingale ( an observation already made in  ( * ? ? ?",
    "* lemma 2 ) ) . to see this , @xmath141 } }   & = i(w;y_{n+1}|y^{n}=y^{n})\\\\ & \\stackrel{(a)}{\\leq}i(x_{n+1};y_{n+1}|y^{n}=y^{n})\\\\   & \\leq { c}\\end{aligned}\\ ] ] where @xmath142 follows from the data processing inequality and the fact that @xmath143@xmath144@xmath145 forms a markov chain given @xmath146 .",
    "hence @xmath140 is indeed a submartingale .",
    "since @xmath129 is bounded between @xmath16 and @xmath147 for all @xmath30 , and the expected stopping time @xmath148 } \\le { e\\left[{t}\\right ] } < \\infty$ ] , doob s optional - stopping theorem allows us to conclude that at time @xmath117 the expected value of the submartingale must be greater than or equal to the initial value , @xmath147 .",
    "hence @xmath149 } \\\\ & = { e\\left [ { { { { \\mathcal h}(w|y^{{\\tau}})}}}\\right]}+{e\\left[{\\tau}\\right ] } { c}\\\\ & \\le    \\operatorname{h}(\\delta ) + \\left ( \\delta + \\frac{{\\it p_{e}}}\\delta \\right ) \\ln m + { e\\left[{\\tau}\\right ] } { c}.\\end{aligned}\\ ] ] solving for @xmath150 $ ] yields @xmath139 } \\ge \\left(1-\\delta-\\frac{{{\\it p_{e}}}}{\\delta}\\right ) \\frac { \\ln m}{{c}}-\\frac{\\operatorname{h}(\\delta)}{{c}}.\\ ] ]",
    "in this section we will combine the two bounds we have established in the preceding sections to obtain a bound on the overall expected decoding time .",
    "lemma  [ prop : exp_tau ] provides a lower bound on @xmath148}$ ] as a function of @xmath3 , @xmath118 and @xmath105 .",
    "we will show that a properly constructed binary hypothesis testing problem allows us to use lemma  [ prop : pe - bound ] to lower bound the probability of error in terms of @xmath151}}$ ] .",
    "this in turn will lead us to the final bound on @xmath59}$ ] .",
    "the next proposition states that a new channel output symbol can not change the _ a posteriori _ probability of any particular message by more than some constant factor when @xmath152 is finite .",
    "[ prop : postent ] @xmath153 implies @xmath154 where @xmath155 .    using bayes rule , the posterior may be written recursively as @xmath156 the quotient may be upper and lower bounded using @xmath157 and @xmath158 , which yields the statement of the proposition .    our objective is to lower bound the probability of error of a decoder that decides at time @xmath2 .",
    "the key idea is that a binary hypothesis decision such as deciding whether or not @xmath143 lies in some set @xmath159 can be made at least as reliably as a decision on the value of @xmath143 itself .    given a set @xmath159 of messages , consider deciding between @xmath160 and @xmath161 in the following way : given access to the original decoder s estimate @xmath162 , declare that @xmath163 if @xmath164 , and declare @xmath165 otherwise .",
    "this binary decision is always correct when the original decoder s estimate @xmath162 is correct .",
    "hence the probability of error of this ( not necessarily optimal ) binary decision rule can not exceed the probability of error of the original decoder , for any set @xmath159 .",
    "thus the error probability of the optimal decoder deciding at time @xmath2 whether or not @xmath166 is a lower bound to the error probability of any decoder that decodes @xmath143 itself at time @xmath2 .",
    "this fact is true even if the set @xmath159 is chosen at a particular stopping time @xmath117 and the error probabilities we are calculating are conditioned on the observation @xmath167 .    for every realization of @xmath167 , the message set",
    "can be divided into two parts , @xmath168 and its complement @xmath169 , in such a way that both parts have an _ a posteriori _ probability greater than @xmath170 .",
    "the rest of this paragraph describes how this is possible . from the definition of @xmath117 , at time @xmath171 the _ a posteriori _ probability of",
    "every message is smaller than @xmath172 .",
    "this implies that the sum of the _ a posteriori _ probabilities of any set of @xmath173 messages is greater than @xmath118 at time @xmath174 , and by proposition  [ prop : postent ] , greater than @xmath170 at time @xmath117 .",
    "in particular , @xmath175 .",
    "we separately consider the cases @xmath176 and @xmath177 . in the first case , @xmath178 ,",
    "let @xmath168 be the set consisting of only the message with the highest _ a posteriori _ probability at time @xmath117 .",
    "the _ a posteriori _ probability of @xmath168 then satisfies @xmath179 . as argued above",
    ", its complement ( the remaining @xmath173 messages ) also has _",
    "a posteriori _ probability greater than @xmath170 , thus for this @xmath168 , @xmath180 $ ] . in the second case , namely when @xmath181 , the _ a posteriori _ probability of each message is smaller than @xmath172 . in this case",
    "the set @xmath168 may be formed by starting with the empty set and adding messages in arbitrary order until the threshold @xmath182 is exceeded .",
    "this ensures that the _ a priori _ probability of @xmath168 is greater than @xmath170 .",
    "notice that the threshold will be exceeded by at most @xmath172 , thus the complement set has an _ a posteriori _ probability of at least @xmath183 .",
    "thus @xmath180 $ ] .    for any realization of @xmath167",
    "we have the binary hypothesis testing problem , running from @xmath117 until @xmath2 , deciding whether or not @xmath184 .",
    "notice that the _ a priori _ probabilities of the two hypotheses of this binary hypothesis testing problem are the _ a posteriori _ probabilities of @xmath168 and @xmath169 at time @xmath117 each of which is shown to be greater than @xmath185 in the paragraph above .",
    "we apply lemma  [ prop : pe - bound ] with @xmath186 and @xmath187 to lower bound the probability of error of the binary decision made at time @xmath2 and , as argued above , we use the result to lower bound the probability that @xmath188 .",
    "initially everything is conditioned on the channel output up to time @xmath117 , thus @xmath189}}}.\\ ] ] taking the expectation of the above expression over all realizations of @xmath190 yields the unconditional probability of error @xmath191}\\geq { e\\left [ { \\frac{\\lambda\\delta}{4 } e^{-{c_1}{{e\\left [ { \\left.\\ ! { t-\\tau } \\right| { y^{\\tau } } } \\right ] } } }    } \\right ] } .\\ ] ] using the convexity of @xmath192 and jensen s inequality , we obtain @xmath193}}.\\ ] ] solving for @xmath194}$ ] yields @xmath195 } \\geq \\frac{-\\ln { { \\it p_{e}}}- \\ln 4 + \\ln ( \\lambda \\delta ) } { { c_1}}.\\ ] ] combing lemma  [ prop : exp_tau ] and  ( [ eq : boundon2 ] ) yields :    the expected decoding time @xmath2 of any variable - length block code for a dmc used with feedback is lower bounded by @xmath196 } \\geq      \\left(1-\\delta-\\frac{{{\\it p_{e}}}}{\\delta}\\right)\\frac { \\ln m}{{c } } + \\frac{-\\ln      { { \\it p_{e}}}}{{c_1 } }   -\\frac{\\operatorname{h}(\\delta)}{{c } } + \\frac{\\ln ( \\lambda \\delta ) - \\ln 4      } { { c_1}},\\ ] ] where @xmath3 is the cardinality of the message set , @xmath24 the error probability , @xmath197 , and @xmath118 is any number satisfying @xmath198 .",
    "choosing the parameter @xmath118 as @xmath199 achieves the required scaling for ( [ eq : whatweprove ] ) .",
    "we have presented a new derivation of burnashev s asymptotically tight lower bound to the average delay needed for a target error probability when a message is communicated across a dmc used with ( channel output ) feedback .",
    "our proof is simpler than the original , yet provides insight by clarifying the role played by the quantities that appear in the bound .",
    "specifically , from the channel coding theorem we expect it to take roughly @xmath200 channel uses to reduce the probability of error of a map decision to some small ( but not too small ) value . at this point",
    "we can partition the message set in two subsets , such that neither subset has too small an _ a posteriori _ probability . from now on it takes ( asymptotically ) @xmath201 channel uses to decide with probability of error @xmath24 which of the two sets contains the true message .",
    "it takes at least as many channel uses to decide which message was selected and incur the same error probability .    for obvious reasons we may call the two phases the communication and the binary hypothesis testing phase , respectively .",
    "these two phases exhibit a pleasing similarity to the communication and confirmation phase of the optimal scheme proposed and analyzed by yamamoto and itoh in @xcite .",
    "the fact that these two phases play a key role in proving achievability as well as in proving that one can not do better suggests that they are an intrinsic component of an optimal communication scheme using variable - length block codes over dmcs with feedback .",
    "the authors would like to thank r.  g.  gallager for his help in pointing out an error in an earlier version of this paper and the reviewers for their helpful comments .",
    "m.  v. burnashev , `` data transmission over a discrete channel with feedback , '' _ problemy peredai informacii _ ,",
    "12 , no .  4 , pp .",
    "1030 , 1976 .",
    "translated in problems of information transmission , pp .",
    "250265 , 1976 .",
    "b.  d. kudryashov , `` message transmission over a discrete channel with noiseless feedback , '' _ problemy peredai informacii _ ,",
    "15 , no .  1 , pp .",
    "313 , 1979 . translated in problems of information transmission , pp .",
    "19 , 1979 ."
  ],
  "abstract_text": [
    "<S> in a remarkable paper published in 1976 , burnashev determined the reliability function of variable - length block codes over discrete memoryless channels with feedback . </S>",
    "<S> subsequently , an alternative _ achievability _ proof was obtained by yamamoto and itoh via a particularly simple and instructive scheme . </S>",
    "<S> their idea is to alternate between a communication and a confirmation phase until the receiver detects the codeword used by the sender to acknowledge that the message is correct . </S>",
    "<S> we provide a _ converse _ that parallels the yamamoto - itoh achievability construction . </S>",
    "<S> besides being simpler than the original , the proposed converse suggests that a communication and a confirmation phase are implicit in any scheme for which the probability of error decreases with the largest possible exponent . </S>",
    "<S> the proposed converse also makes it intuitively clear why the terms that appear in burnashev s exponent are necessary .    </S>",
    "<S> burnashev s error exponent , discrete memoryless channels ( dmcs ) , feedback , variable - length communication </S>"
  ]
}