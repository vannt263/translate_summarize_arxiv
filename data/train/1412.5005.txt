{
  "article_text": [
    "preconditioning is a technique developed originally for the iterative solution of linear systems that aims at the acceleration of convergence of the iterations . in its simplest form ,",
    "the system @xmath0 is multiplied by a matrix @xmath1 such that the spectral condition number of @xmath2 , the ratio of the largest to the smallest singular value thereof , is considerably smaller than that of @xmath3 , which generally leads to faster convergence .    0",
    "we define a preconditioner of a matrix @xmath3 as a matrix @xmath1 such that their product @xmath2 has a smaller condition number than @xmath3 . instead of solving a linear system @xmath4 , one may solve a preconditioned system @xmath5 .",
    "the small condition number of @xmath2 is expected to lead to fast convergence of iterations .",
    "iterative methods for solving linear systems normally do not require @xmath3 and @xmath1 to be explicitly formed as matrices : it is sufficient that matrix - vector multiplications are implemented and performed via user - defined procedures .",
    "the same is true for iterative methods that compute eigenvalues and eigenvectors of a very large matrix , as , e.g.,in @xcite , calculating one eigenvector of a 100-billion size matrix , or in  @xcite .",
    "a classical application area for preconditioned solvers is the discretized boundary value problems for elliptic partial differential operators ; see , e.g. ,  @xcite . with multigrid preconditioning , preconditioned solvers may achieve linear complexity on problems from this area ; see , e.g. ,  @xcite and references there for symmetric eigenvalue problems .",
    "dyakonov seminal work , summarized in @xcite , proposes `` spectrally equivalent '' preconditioning for elliptic operator eigenvalue problems in order to guarantee convergence that does not deteriorate with the increasing dimension of the discretized problem .",
    "owing to this , for large enough problems such preconditioners outperform direct solvers , which factorize the original sparse matrix @xmath3 .",
    "inevitable matrix fill - ins , especially prominent in discretized differential problems in more than two spatial dimensions , destroy the matrix sparsity , resulting in computer memory overuse and non - optimal performance .",
    "preconditioning has also long since been a key technique in _ ab initio _ calculations in material sciences ; see , e.g. ,   @xcite and references therein . in the last decade , preconditioning for graphs is attracting growing attention as a tool for achieving an optimal complexity for large data mining problems , e.g. , for graph bisection and image segmentation using graph laplacian and fiedler vectors since @xcite ; for recent work see , e.g. ,  @xcite .",
    "preconditioned iterative methods for the original linear system @xmath6 are in many cases mathematically equivalent to standard iterative methods applied for the preconditioned system @xmath7 .",
    "for example , the classical richardson iteration step applied to the preconditioned system becomes @xmath8 where @xmath9 is a suitably chosen scalar .",
    "turning now to eigenvalue problems , let us consider the computation of an eigenvector of a real symmetric positive definite matrix @xmath3 corresponding to its smallest eigenvalue .",
    "borrowing an argument from @xcite , suppose that the targeted eigenvalue @xmath10 , or a sufficiently good approximation thereof , is known .",
    "then the corresponding eigenvector can be computed by solving a homogeneous linear system @xmath11 , or , equivalently , the system @xmath12 , where @xmath13 is an identity .",
    "the richardson iteration step now becomes @xmath14 theoretically , the best preconditioners for @xmath6 and @xmath15 are , correspondingly , @xmath16 and @xmath17 , where @xmath18 denotes a pseudo - inverse , making both richardson iteration schemes , and , converge in a single step with @xmath19 . under the standard assumption @xmath20 , both in and ,",
    "convergence theory is straightforward , e.g. ,  in terms of the spectral radius @xmath21 of @xmath22 .",
    "sharp explicit convergence bounds , not relying on generic constants , can be derived in the form of inequalities that allow one to determine whether the convergence deteriorates with the increasing problem size by analyzing every term in the bound .",
    "for some classes of eigenvalue problems , the efficiency of choosing @xmath20 has been demonstrated , both numerically and theoretically , in @xcite .",
    "this choice allows the easy adaptation of a vast variety of preconditioners already developed for linear systems to the eigensolvers .    in practice",
    ", the theoretical value @xmath10 in the richardson iteration above has to be replaced with its approximation .",
    "a standard choice for @xmath10 is a rayleigh quotient function @xmath23 , leading to @xmath24    it is well known that the rayleigh quotient @xmath25 gives a high quality ( quadratic ) approximation of the eigenvalue @xmath10 , if the sequence @xmath26 converges to the corresponding eigenvector .",
    "thus , asymptotically as @xmath27 where @xmath28 , methods and are equivalent , and so may be their asymptotic convergence rate bounds .",
    "however , asymptotic convergence rate bounds naturally contain generic constants , which are independent of @xmath29 , but may depend on the problem size . due to the changing value @xmath25 , a non - asymptotic theoretical convergence analysis is much more difficult , compared to the case for linear systems , even for the simplest methods , such as the richardson iteration .",
    "dyakonov pioneering work from the eighties , summarized in ( * ? ? ?",
    "* chapter 9 ) , includes the first non - asymptotic convergence bounds for preconditioned eigensolvers proving their linear convergence with a rate , which can be bounded above independently of the problem size .",
    "just a few of the known bounds are sharp .",
    "one of them is proved for the simplest preconditioned eigensolver with a fixed step size in a series of papers by neymeyr over a decade ago ; see @xcite and references therein .",
    "the original proof has been greatly simplified and shortened in @xcite by using a gradient flow integration approach .",
    "in this paper we present a new self - contained proof of a sharp convergence rate bound from @xcite for the preconditioned eigensolver , theorem [ t.1 ] . following the geometrical approach of @xcite , we reformulate the problem of finding the convergence bound for as a constrained optimization problem for the rayleigh quotient .",
    "the main novelty of the proof is that here we use inequality constraints , which brings to the scene the karush - kuhn - tucker ( kkt ) theory ; see , e.g.  @xcite .",
    "kkt conditions allow us to reduce our convergence analysis to the simplest scenario in two dimensions , which is the key step in the proof .",
    "we have also found several simplifications in the two dimensional convergence analysis , compared to that of @xcite .",
    "we believe that the new proof will greatly enhance the understanding of the convergence behavior of increasingly popular preconditioned eigensolvers , whose application area is quickly expanding : see , e.g. ,  @xcite .",
    "we consider a real generalized eigenvalue problem @xmath30 with real symmetric positive definite matrices @xmath3 and @xmath31 . the objective is to approximate iteratively the smallest eigenvalue @xmath32 by minimizing the rayleigh quotient @xmath33 .",
    "a direct formulation of the convergence analysis with respect to this form of the eigenvalue problem has some disadvantages . instead , the inverted form @xmath34 with @xmath35 results in more compact representation of the problem and the proof ( many inverses like @xmath36 and @xmath37 can be avoided ) , cf . @xcite . for this inverted form",
    "the objective is to approximate the largest eigenvalue @xmath38 of @xmath34 by maximizing the rayleigh quotient @xmath39 .",
    "we denote the eigenvalues by @xmath40 , which can have arbitrary multiplicity .",
    "corresponding eigenspaces are denoted by @xmath41 .",
    "the increase of @xmath42 can be achieved by correcting the current iterate @xmath43 along the preconditioned gradient of the rayleigh quotient , i.e. @xmath44 see @xcite and references therein . if @xmath45 , then @xmath46 and method turns into with @xmath19 , discussed in the introduction .    in all our prior work on preconditioned eigensolvers for symmetric eigenvalue problems , including @xcite , we have always assumed that the preconditioner @xmath1 is a symmetric and positive definite matrix , typically satisfying conditions @xmath47 or equivalent , up to the scaling of @xmath1 .",
    "recently , the authors of @xcite have noticed and demonstrated that @xmath1 does not have to be symmetric positive definite , and a less restrictive assumption @xmath48 can be used instead , where @xmath49 denotes the matrix largest singular value , and @xmath50 is the symmetric positive definite square root of @xmath3 .",
    "it is verified in @xcite that and are equivalent if @xmath1 is symmetric and positive definite .    in what follows",
    ", we give a complete and concise proof of the following convergence rate bound , first proved in @xcite ,    [ t.1 ] if @xmath51 and @xmath1 satisfies , then for @xmath52 given by it holds that either @xmath53 or @xmath54    the first step , lemma [ l.1 ] , of the proof of theorem [ t.1 ] is the same as that in @xcite , where we characterize a set of possible next step iterates @xmath52 in varying the preconditioner @xmath1 constrained by assumption , aiming at eliminating the preconditioner @xmath1 from consideration .",
    "the only difference is that in @xcite we start with changing an original coordinate basis to an @xmath3-orthogonal basis , which transforms @xmath3 into the identity @xmath13 , resulting in a one - line proof of lemma [ l.1 ] . here",
    ", we choose to present a detailed proof of lemma [ l.1 ] , for a general @xmath3 , demonstrating that the transformation of @xmath3 into the identity @xmath13 , made after lemma [ l.1 ] , is well justified .",
    "[ l.1 ] let us denote @xmath55 and @xmath56 and define a closed ball @xmath57 centered at @xmath58 .",
    "let @xmath1 satisfy , then for @xmath52 given by it holds that @xmath59    left - multiplying by @xmath60 gives @xmath61 or , in our new notation , @xmath62 resulting @xmath63 . since @xmath64 by , we get @xmath65    the second step of the proof is traditional  reducing the generalized symmetric eigenvalue problem @xmath34 to the standard eigenvalue problem for the symmetric positive definite matrix @xmath66 by making the change of variables as hinted by lemma [ l.1 ] .",
    "we use the standard inner product in @xmath67 variables , i.e. @xmath68 and the corresponding vector norm @xmath69 , so , e.g.,@xmath70 we later use @xmath71 and @xmath72 -based scalar products and norms defined as follows , e.g. ,   @xmath73 for brevity we drop the subscript @xmath67 in the rest of the paper . in the following @xmath1",
    "refers to @xmath74 , @xmath31 refers to @xmath66 , @xmath43 refers to @xmath75 , and so on , cf .",
    "lemma [ l.1 ] .",
    "furthermore , @xmath76 , and method is @xmath77 .",
    "the new form of condition is @xmath78 .",
    "this means that @xmath79 approximates the identity matrix with respect to the notation used in lemma [ l.1 ] .",
    "the closed ball has the form @xmath80 with the radius @xmath81 centered at @xmath82 . since @xmath83 and @xmath84",
    ", we can estimate @xmath85 by using a minimizer of @xmath86 in @xmath87 ( i.e. by considering the worst case ) .",
    "we observe that , effectively , we set @xmath88 without loss of generality .",
    "[ s3 ] the main idea of the geometrical approach of @xcite , which we also employ in this paper , is that the convergence rate of iterations is slowest , in terms of the rayleigh quotient , if @xmath43 is a linear combination of two eigenvectors , which makes the further convergence analysis trivial .",
    "a new proof of this fact actually occupies a major part of our paper . in order to illustrate how such a dramatic reduction in dimension becomes possible , in this section we apply our technique to a simplified case @xmath90 corresponding to @xmath91 .",
    "it is not difficult to see that under this assumption turns into one iteration @xmath92 of the power method , 0 our proof of theorem [ t.1 ] in the next section uses powerful tools , which are uncommon in numerical linear algebra and thus may catch an unprepared reader unguarded .",
    "the role of this section is to serve as a gentle introduction to the main proof .",
    "here we assume @xmath93 so that @xmath91 in order to simplify the analysis , then relation turns into the power method , @xmath92 , and bound holds with @xmath91 and thus @xmath94 .",
    "let us make a historic note that exactly this result has apparently first appeared in @xcite .",
    "the left - hand side of bound is monotone in @xmath95 .",
    "one way to find out at which @xmath43 the behavior of is the worst is to minimize @xmath96 for all @xmath43 that satisfy @xmath97 for some fixed @xmath98 . slightly abusing the notation in the proof",
    ", we keep denoting by @xmath43 both the initial approximation in and the vector in the minimization problem .",
    "we notice that @xmath97 is equivalent to @xmath99 . therefore , at a stationary point we have , using lagrangian multipliers , that @xmath100 where @xmath101 is some constant .",
    "this yields @xmath102 which can be rewritten as @xmath103 where @xmath104 .",
    "since @xmath105 implies @xmath106 , we obtain @xmath107 which shows that @xmath108 .",
    "thus , equation can be viewed as a polynomial equation @xmath109 , where @xmath110 is a third degree polynomial with positive first and last coefficients , specifically @xmath111 and @xmath112 , correspondingly .",
    "inserting @xmath113 , where @xmath114 are the projections of @xmath43 onto the eigenspaces @xmath115 , leads to @xmath116 .",
    "since the eigenspaces are orthogonal to each other , the products @xmath117 must be zero for each @xmath118 .",
    "owing to the positiveness of the first and last coefficients , the polynomial @xmath119 must have a non - positive root , and thus at most two positive roots , i.e. @xmath120 can be zero for some two indexes @xmath121 and @xmath122 at most , allowing the only possibly nonzero @xmath123 and @xmath124 from all projections @xmath114 .",
    "we conclude that @xmath43 is a linear combination of at most two normalized eigenvectors @xmath125 and @xmath126 , corresponding to distinct eigenvalues @xmath127 and @xmath128 of the matrix @xmath31 .    we assume without loss of generality that @xmath129 , then @xmath130 similarly , since @xmath131 , we obtain @xmath132 let @xmath133 , then @xmath134 implies @xmath135 . by using monotonicity of the ratio of the quotients in @xmath127 and @xmath128 and the fact that the vector @xmath43 here corresponds to the worst - case scenario , i.e. minimizing @xmath95 over all @xmath43 with the fixed value @xmath97 , we obtain with @xmath91 .",
    "since is an equality , we also prove that the upper bound in with @xmath91 is sharp , turning into an equality if the initial approximation in satisfies @xmath136 .    in the next section ,",
    "we apply the described dimensionality reduction technique to the general case .",
    "we formulate the conditions that `` the worst case '' @xmath43 must satisfy , which yield the generalization of equation , and rewrite this equation as a cubic equation @xmath109 .",
    "we show that the first and last coefficients of this equation are positive , which , as we have just seen , implies that @xmath43 is a linear combination of two eigenvectors .",
    "a simple two - dimensional analysis completes the proof of theorem  [ t.1 ] .",
    "[ s4 ] next the proof of theorem [ t.1 ] is given : let us denote @xmath138 and define @xmath80 , a closed ball with the radius @xmath81 centered at @xmath82 .",
    "on the one hand , it holds that @xmath139 for any vector @xmath140 since @xmath43 is not an eigenvector and @xmath141 . indeed , taking into account @xmath142 ,",
    "we have @xmath143 so that @xmath144 . on the other hand , @xmath145 , since @xmath146 , see lemma [ l.1 ] .",
    "this proves @xmath147 and , thus , the left inequality in , provided that @xmath148    in the previous proof with @xmath91 , the ball @xmath87 shrinks to a single point @xmath82 and the only choice of @xmath149 is possible .",
    "the present case @xmath150 is significantly more difficult for the worst - case scenario analysis , involving a minimization problem with two variables , @xmath43 and @xmath151 . in our previous work ,",
    "see @xcite and references therein , we first vary @xmath140 intending to minimize @xmath152 for a given @xmath43 , and then vary @xmath43 fixing @xmath97 .",
    "the first minimization problem defines @xmath151 as an implicit function of @xmath43 , and then lagrangian multipliers are used , as in section [ s3 ] , to analyze the second minimization problem , in @xmath43 .",
    "it turns out that the proof is much simpler if we vary both @xmath43 and @xmath151 at the same time and attack the required two - parameter minimization problem in @xmath43 and @xmath151 directly by using the kkt arguments as provided below .",
    "[ t.reduc ] for @xmath153 and a fixed value @xmath154 that is not an eigenvalue of @xmath31 , let a pair of vectors @xmath155 denote a solution of the following constrained minimization problem : @xmath156 if @xmath157 is not an eigenvector of @xmath31 , then both @xmath157 and @xmath158 belong to a two - dimensional invariant subspace of @xmath31 corresponding to two distinct eigenvalues , and @xmath159 where @xmath160 denotes an angle between two vectors defined by @xmath161 .",
    "we consider the equivalent problem @xmath162 we first notice that the assumption @xmath163 implies @xmath164 because of the first constraint and @xmath165 .",
    "thus , @xmath152 is correctly defined .",
    "next , let us temporarily consider a stricter constraint @xmath166 , instead of @xmath163 .",
    "combined with the other constraints , this results in minimization of the smooth function @xmath167 on a compact set , so there exists a solution @xmath155 .",
    "finally , let us remove the artificial constraint @xmath166 and notice that any nonzero multiple of @xmath155 is also a solution .",
    "thus we can consider the karush - kuhn - tucker ( kkt ) conditions , e.g. , ( * ? ? ?",
    "* theorem 9.1.1 ) , @xcite , in any neighborhood of @xmath155 , which does not include the origin .",
    "next we show that the gradients of @xmath168 and @xmath169 are linearly independent . for the gradient of @xmath169",
    ", it holds that @xmath170 with @xmath141 , since @xmath43 is not an eigenvector of @xmath31 , and it holds that @xmath171 , since @xmath169 does not depend on @xmath151 . assuming the linear dependence of the gradients of @xmath168 and @xmath169 implies that @xmath172 , so that @xmath173 and @xmath149 . by using @xmath149",
    ", it holds that @xmath174 while @xmath170 , i.e. ( using again the assumed linear dependence ) the vector @xmath175 is an eigenvector of @xmath176 , and , hence @xmath43 is an eigenvector of @xmath31 , contradicting the lemma assumption .",
    "therefore , the gradients of @xmath168 and @xmath169 are linearly independent .",
    "all functions involved in our constrained minimization are smooth .",
    "we conclude that the stationary point @xmath155 is regular , i.e. ,  the kkt conditions are valid .",
    "the kkt stationarity condition states that there exist constants @xmath101 and @xmath177 such that @xmath178 at the critical point @xmath155 .",
    "the independent variables @xmath179 no longer appear , so to simplify the notation , in the rest of the proof we drop the superscript @xmath180 and substitute @xmath179 for @xmath155 .",
    "we separately write the partial derivatives with respect to @xmath43 , @xmath181 and with respect to @xmath151 , @xmath182 the kkt complementary slackness condition @xmath183 must be satisfied , implying @xmath184    if @xmath151 is an eigenvector then @xmath185 in condition , leading to @xmath149 , i.e. vector @xmath43 is also an eigenvector of @xmath31 , thus we are done .",
    "now we consider a nontrivial case , where neither @xmath43 nor @xmath151 is an eigenvector .",
    "condition then implies @xmath186 , so identity holds unconditionally , condition turns into @xmath187 and taking the inner product of with @xmath151 gives @xmath188    taking the inner products of both sides of with @xmath189 results in @xmath190 therein we use and .    denoting @xmath191 , we rewrite as @xmath192 taking the inner products of both sides of with @xmath193 yields @xmath194 where the orthogonality @xmath195 has been used again .",
    "therefore , we obtain @xmath196 , which implies @xmath197 .    substituting @xmath175 and multiplying through by @xmath31 in results in @xmath198 multiplying through by @xmath199 and substituting @xmath200 , which follows from ,",
    "we obtain @xmath201 , where @xmath202 is a third degree polynomial with @xmath203 and @xmath204 , which can not have more than two positive roots . 0",
    "inserting the expansion @xmath205 with the projections @xmath114 of @xmath151 on the eigenspaces @xmath115 leads to @xmath206 for every @xmath207 , cf .",
    "section 3 . since @xmath208 and @xmath209 , the polynomial @xmath119 must have a non - positive root , and thus at most two positive roots .",
    "hence @xmath210 can be nonzero for at most two @xmath207 ( since @xmath211 ) and thus , @xmath151 is a linear combination of two normalized eigenvectors @xmath125 and @xmath126 corresponding to two distinct eigenvalues ( cf .",
    "section  3 ) , i.e. @xmath212 . since @xmath213 , by so is @xmath43 .",
    "furthermore , the orthogonality @xmath195 from shows that @xmath214 and @xmath215 .",
    "this leads to @xmath216 , since the angles between vectors have the range @xmath217 $ ] ( due to @xmath218 ) .",
    "similarly , @xmath219 together with @xmath220 implies @xmath221 .",
    "then we have @xmath222 by using .",
    "we now derive bound in a two - dimensional @xmath31-invariant subspace .",
    "[ t.2d ] let @xmath157 and @xmath158 belong to a two - dimensional invariant subspace of @xmath31 corresponding to the eigenvalues @xmath223 and satisfy , where @xmath157 is not an eigenvector .",
    "it holds that @xmath224    in this proof we drop the superscript @xmath180 upon @xmath43 and @xmath151 .",
    "the vectors @xmath43 , @xmath151 and @xmath82 can be represented by the coefficient vectors @xmath225 with respect to an orthonormal basis @xmath226 , where @xmath125 and @xmath126 are eigenvectors associated with @xmath127 and @xmath128 .",
    "evidently , it holds that @xmath227 , @xmath228 , @xmath229 by using the orthonormal basis .",
    "therefore , @xmath230 , and similarly @xmath231 .",
    "this allows us to rewrite in the form @xmath232 . using the geometric property of the cross products @xmath233 we have @xmath234 which yields @xmath235 further",
    ", we use the equalities @xmath236 which can be derived in a similar way to section [ s.cc ] .",
    "then @xmath237 , so that @xmath238 since @xmath239 , we have @xmath240 which proves by using @xmath241 .    the proof of theorem [ t.1 ] is completed by deriving convergence bound from its two - dimensional version .",
    "we restate the assumption @xmath242 . according to lemma [ t.reduc ] , there exists a minimizer @xmath151 with @xmath243 , which satisfies with @xmath244 , and the interval @xmath245 is a subset of @xmath246 . using monotonicity arguments , , and the same arguments as section [ s.cc ] , we obtain @xmath247 & = \\frac{\\mu_k-\\mu(y)}{\\mu(y)-\\mu_l}\\ ;     \\frac{\\mu(x)-\\mu_l}{\\mu_k-\\mu(x ) }     \\le\\left(\\gamma+(1-\\gamma)\\frac{\\mu_l}{\\mu_k}\\right)^2 \\le\\left(\\gamma+(1-\\gamma)\\frac{\\mu_{i+1}}{\\mu_i}\\right)^2 . \\end{split}\\ ] ] this proves since @xmath248 .",
    "we have presented a succinct proof of the standard sharp convergence rate bound for the simplest fixed step size preconditioned eigensolver . the key argument of",
    "the new proof is the characterization of the case of poorest convergence as a constrained optimization problem for the rayleigh quotient . employing the karush - kuhn - tucker conditions and some elementary matrix algebra , we dramatically simplify the convergence analysis by reducing it to a subspace spanned by two eigenvectors .",
    "we expect the analytical framework developed in this paper to be a valuable tool in the convergence analysis of a variety of preconditioned eigensolvers ; see , e.g. , the analysis of the preconditioned steepest descent method in @xcite .",
    "* i*. an alternative estimate for the left - hand side of , which is sharp with respect to all variables , can be derived as follows : with @xmath249 and @xmath250 , a new representation of @xmath251 is given by @xmath252 this results in a quadratic equation for @xmath253 with the roots @xmath254 since @xmath255 , a strictly sharp bound for the estimate in is given by @xmath256 .",
    "we note that in the limit case @xmath257 it holds that @xmath258 , and @xmath259 turns into @xmath260 .",
    "this coincides with the known bound in . + * ii*. the bound in contains a convex combination of @xmath111 and @xmath261 .",
    "interestingly , this bound can also be derived by using a convex function as follows : without loss of generality , we assume that @xmath43 has a positive @xmath125 coordinate . then @xmath262 is an acute angle . since @xmath211 , @xmath263 and @xmath264 are also acute angles .",
    "the equality together with @xmath165 shows further @xmath265 .",
    "since @xmath264 and @xmath266 are acute angles , the vectors @xmath125 and @xmath151 are located in a half - plane whose boundary line is orthogonal to @xmath82 .",
    "a simple case differentiation shows that @xmath267 is either equal to @xmath268 or equal to @xmath269 .",
    "further , we use the equalities @xmath270 which can be derived in a similar way to section [ s.cc ] .",
    "the last equality proves @xmath271 , since the tangent is an increasing function for acute angles , and @xmath272 .",
    "this leads to @xmath273 , since @xmath274 are all in the same quadrant . in summary",
    ", it holds that @xmath275 i.e. , @xmath267 is a further acute angle . using these acute angles , we write equivalently as @xmath276 in order to prove , we use again , together with @xmath277 , @xmath278 and the first inequality in .",
    "it holds that @xmath279=:f(\\gamma).\\ ] ] because of @xmath280,\\ ] ] @xmath281 is a monotonically increasing function in @xmath282 $ ] .",
    "the numerator of @xmath283 is also a monotonically increasing function and its denominator is monotonically decreasing in @xmath284 $ ] .",
    "these two functions are positive so that @xmath283 is also a monotonically increasing function .",
    "thus @xmath281 is a convex function in @xmath282 $ ] , and @xmath285 which proves and hence .",
    ", nonsymmetric preconditioning for conjugate gradient and steepest descent methods , _ procedia computer science _ , 51 ( 2015 ) ,",
    "276285 . . a preliminary version available at arxiv:1212.6680 [ cs.na ] , 2012",
    "http://arxiv.org/abs/1212.6680                , modern preconditioned eigensolvers for spectral image segmentation and graph bisection , workshop on clustering large data sets third ieee international conference on data mining ( icdm 2003 ) , 2003 .",
    "http://math.ucdenver.edu/~aknyazev/research/conf/icdm03.pdf    knn2003 a.  v. knyazev and k.  neymeyr , a geometric theory for preconditioned inverse iteration .",
    "iii : a short and sharp convergence estimate for generalized eigenvalue problems , _ linear algebra appl .",
    "_ , 358 ( 2003 ) , pp .",
    "95114 .",
    "kn2003 a.  v. knyazev and k.  neymeyr , efficient solution of symmetric eigenvalue problems using multigrid preconditioners in the locally optimal block conjugate gradient method , _ electronic transactions on numerical analysis _ , 15 ( 2003 ) , pp .",
    "/ vol.15.2003/pp38 - 55.dir / pp38 - 55.pdf                  , preconditioned eigensolvers for large - scale nonlinear hermitian eigenproblems with variational characterizations .",
    "i. conjugate gradient methods , research report 14 - 08 - 26 , department of mathematics , temple university , august 2014 .",
    "revised april 2015 . to appear in",
    "_ mathematics of computation_. https://www.math.temple.edu/~szyld/reports/nlpcg.report.rev.pdf    , preconditioned eigensolvers for large - scale nonlinear hermitian eigenproblems with variational characterizations .",
    "interior eigenvalues , research report 15 - 04 - 10 , department of mathematics , temple university , april 2015 . to appear in _ siam journal on scientific computing_. http://arxiv.org/abs/1504.02811        , high - performance computing for exact numerical approaches to quantum many - body problems on the earth simulator , in proceedings of the 2006 acm / ieee conference on supercomputing ( sc 06 ) .",
    "acm , new york , ny , usa , article 47 , 2006 ."
  ],
  "abstract_text": [
    "<S> preconditioned iterative methods for numerical solution of large matrix eigenvalue problems are increasingly gaining importance in various application areas , ranging from material sciences to data mining . </S>",
    "<S> some of them , e.g. , those using multilevel preconditioning for elliptic differential operators or graph laplacian eigenvalue problems , exhibit almost optimal complexity in practice , i.e. , their computational costs to calculate a fixed number of eigenvalues and eigenvectors grow linearly with the matrix problem size </S>",
    "<S> . theoretical justification of their optimality requires convergence rate bounds that do not deteriorate with the increase of the problem size . </S>",
    "<S> such bounds were pioneered by e.  dyakonov over three decades ago , but to date only a handful have been derived , mostly for symmetric eigenvalue problems . </S>",
    "<S> just a few of known bounds are sharp . </S>",
    "<S> one of them is proved in [ ] for the simplest preconditioned eigensolver with a fixed step size . </S>",
    "<S> the original proof has been greatly simplified and shortened in [ ] by using a gradient flow integration approach . in the present work , </S>",
    "<S> we give an even more succinct proof , using novel ideas based on karush - kuhn - tucker theory and nonlinear programming .    </S>",
    "<S> symmetric ; preconditioner ; eigenvalue ; eigenvector ; rayleigh quotient ; gradient ; iterative method ; karush  kuhn  tucker theory .    </S>",
    "<S> 65f15 65k10 65n25    _ dedicated to the memory of evgenii g.  dyakonov , moscow , russia , 19352006 . _ </S>"
  ]
}