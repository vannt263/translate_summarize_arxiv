{
  "article_text": [
    "proposed by @xcite , cca is a technique for finding pairs of vectors that maximises the correlation between a set of paired variables .",
    "the set of paired variables can be considered as two views of the same object , a perspective we adopt throughout the paper . since the debut of cca , a multitude of analyses , adaptations and applications have been proposed @xcite . +   +",
    "the potential disadvantage of cca and similar statistical methods , such as principle component analysis ( pca ) and partial least squares ( pls ) , is that the learned projections are a linear combination of all the features in the primal and dual representations respectively . this makes the interpretation of the solutions difficult .",
    "studies by @xcite and the more recent @xcite have addressed this issue for pca and pls by learning only the relevant features that maximise the variance for pca and covariance for pls . a previous application of sparse cca has been proposed in @xcite where the authors imposed sparsity on the semantic space by penalising the cardinality of the solution vector @xcite . the scca presented in this paper is novel to the extent that instead of working with covariance matrices @xcite , which may be computationally intensive to compute when the dimensionality of the data is large , it deals directly with the training data .",
    "+   + in the machine learning ( ml ) community it is common practice to refer to the input space as the primal - representation and the kernel space as the dual - representation . in order to avoid confusion with the meanings of the terms primal and dual commonly used in the optimisation literature , we will use ml - primal to refer to the input space and ml - dual to refer to the kernel space for the remainder of the paper , though note that the references to primal and dual in the abstract refer to ml - primal and ml - dual .",
    "+   + we introduce a new convex least squares variant of cca which seeks a semantic projection that uses as few relevant features as possible to explain as much correlation as possible . in previous studies ,",
    "cca had either been formulated in the ml - primal ( input ) or ml - dual ( kernel ) representation for both views .",
    "these formulations , coupled with the need for sparsity , could prove insufficient when one desires or is limited to a ml primal - dual representation , i.e. one wishes to learn the correlation of words in one language that map to documents in another .",
    "we address these possible scenarios by formulating scca in a ml primal - dual framework in which one view is represented in the ml - primal and the other in the ml - dual ( kernel defined ) representation .",
    "we compare scca with kcca on a bilingual english - french and english - spanish data - set for a mate retrieval task .",
    "we show that in the mate retrieval task scca performs as well as kcca when the number of original features is small and scca outperforms kcca when the number of original features is large .",
    "this emphasises scca s ability to learn the semantic space from a small number of relevant features .",
    "+   + in section [ sec1 ] we give a brief review of cca , and section [ sec2 ] formulates and defines scca . in section [ sec4 ]",
    "we derive our optimisation problem and show how all the pieces are assembled to give the complete algorithm .",
    "the experiments on the paired bilingual data - sets are given in section [ sec5 ] .",
    "section [ sec6 ] concludes this paper .",
    "we briefly review canonical correlation analysis and its ml - dual ( kernel ) variant to provide a smooth understanding of the transition to the sparse formulation .",
    "first , basic notation representation used in the paper is defined @xmath0 + the correlation between @xmath1 and @xmath2 can be computed as @xmath3 where @xmath4 and @xmath5 are the within - set covariance matrices and @xmath6 is the between - sets covariance matrix , @xmath7 is the matrix whose columns are the vectors @xmath8 , @xmath9 from the first representation while @xmath10 is the matrix with columns @xmath8 from the second representation .",
    "we are able to observe that scaling @xmath11 does not effect the quotient in equation ( [ math : sim ] ) , which is therefore equivalent to maximising @xmath12 subject to @xmath13 . +   + the kernelising of cca @xcite offers an alternative by first projecting the data into a higher dimensional feature space @xmath14 before performing cca in the new feature spaces .",
    "the kernel variant of cca is useful when the correlation is believed to exist in some non linear relationship .",
    "given the kernel functions @xmath15 and @xmath16 let @xmath17 and @xmath18 be the linear kernel matrices corresponding to the two representations of the data , where @xmath7 is now the matrix whose columns are the vectors @xmath19 , @xmath9 from the first representation while @xmath10 is the matrix with columns @xmath20 from the second representation .",
    "the weights @xmath21 and @xmath22 can be expressed as a linear combination of the training examples @xmath23 and @xmath24 .",
    "substitution into the ml - primal cca equation ( [ math : sim ] ) gives the optimisation @xmath25 which is equivalent to maximising @xmath26 subject to @xmath27 .",
    "this is the ml - dual form of the cca optimisation problem given in equation ( [ math : sim ] ) which can be cast as a generalised eigenvalue problem and for which the first @xmath28 generalised eigenvectors can be found efficiently .",
    "both cca and kcca can be formulated as symmetric eigenproblems .",
    "+   + a variety of theoretical analyses have been presented for cca @xcite .",
    "a common conclusion of some of these analyses is the need to regularise kcca .",
    "for example the quality of the generalisation of the associated pattern function is shown to be controlled by the sum of the squares of the weight vector norms in @xcite .",
    "although there are advantages in using kcca , which have been demonstrated in various experiments across the literature , we clarify that when using a linear kernel in both views , regularised kcca is the same as regularised cca ( since the former and latter are linear ) .",
    "nonetheless using kcca with a linear kernel can have advantages over cca , the most important being speed when the number of features is larger than the number of samples .",
    "the motivation for formulating a ml primal - dual scca is largely intuitive when faced with real - world problems combined with the need to understand or interpret the found solutions .",
    "consider the following examples as potential case studies which would require ml primal - dual sparse multivariate analysis methods , such as the one proposed .",
    "* enzyme prediction ; in this problem one would like to uncover the relationship between the enzyme sequence , or more accurately the sub - sequences within each enzyme sequence that are highly correlated with the possible combination of the enzyme reactants .",
    "we would like to find a sparse ml - primal weight representation on the enzyme sequence which correlates highly to sparse ml - dual feature vector of the reactants .",
    "this will allow a better understanding of the enzyme structure relationship to reactions . *",
    "bilingual analysis ; when learning the semantic relationship between two languages , we may want to understand how one language maps from the word space ( ml - primal ) to the contextual document ( ml - dual ) space of another language . in both cases",
    "we do not want a complete mapping from all the words to all possible contexts but to be able to extract an interpretable relationship from a sparse word representation from one language to a particular and specific context ( or sparse combination of ) in the other language .",
    "* brain analysis ; here , one would be interested in finding a ( ml - primal ) sparse voxelmm@xmath29 mm . ] activation map to some ( ml - dual ) non - linear stimulus activation ( such as musical sequences , images and various other multidimensional input ) .",
    "the potential ability to find only the relevant voxels in the stimuli would remove the particularly problematic issue of thresholding the full voxel activation maps that are conventionally generated .",
    "for the scope of this paper we limit ourselves to experiments with the bilingual texts problems .",
    "+   + throughout the paper we only consider the setting when one is interested in a ml - primal representation for the first view and a ml - dual representation for the second view , although it is easily shown that the given derivations hold for the inverted case ( i.e. a ml - dual representation for the first view and a ml - primal representation for the second view ) which is therefore omitted .",
    "+   + consider a sample from a pair of random vectors ( i.i.d assumptions hold ) of the form @xmath30 each with zero mean ( i.e. centred ) where @xmath9 .",
    "let @xmath7 and @xmath10 be matrices whose columns are the corresponding training samples and let @xmath31 be the kernel matrix of the second view and @xmath22 be expressed as a linear combination of the training examples @xmath32 ( note that @xmath33 is a general vector and should not be confused with notation sometimes used for unit coordinate vectors ) .",
    "the primal - dual cca problem can be expressed as a primal - dual rayleigh quotient @xmath34 where we choose the primal weights @xmath21 of the first representation and dual features @xmath33 of the second representation such that the correlation @xmath35 between the two vectors is maximised . as we are able to scale @xmath21 and @xmath33 without changing the quotient , the maximisation in equation ( [ eig : origin ] ) is equal to maximising @xmath36 subject to @xmath37 .",
    "for simplicity let @xmath38 , @xmath39 and @xmath40 . +   + having provided the initial primal - dual framework we proceed to reformulate the problem as a convex sparse least squares optimisation problem .",
    "we are able to show that maximising the correlation between the two vectors @xmath41 and @xmath42 can be viewed as minimising the angle between them .",
    "since the angle is invariant to rescaling , we can fix the scaling of one vector and then minimise the norm to be the @xmath43norm . ] between the two vectors @xmath44 subject to @xmath45 .",
    "this intuition is formulated in the following theorem ,    [ theo1 ] vectors @xmath46 are an optimal solution of equation ( [ eig : origin ] ) if and only if there exist @xmath47 such that @xmath48 are an optimal solution of equation ( [ eigsec ] ) .",
    "theorem [ theo1 ] is well known in the statistics community and corresponds to the equivalence between one form of alternating conditional expectation ( ace ) and cca @xcite . for an exact proof",
    "see theorem @xmath49 on page @xmath50 in @xcite .",
    "+   + constraining the @xmath43norm of @xmath41 ( or @xmath42 ) will result in a non convex problem , i.e we will not obtain a positive / negative - definite hessian matrix",
    ". motivated by the rayleigh quotient solution for optimising cca , whose resulting symmetric eigenproblem does _ not _ enforce the @xmath45 constraint , i.e. the optimal solution is invariant to rescaling of the solutions .",
    "therefore we replace the scaling of @xmath45 with the scaling of @xmath33 to be @xmath51 .",
    "we will address the resulting convexity when we achieve the final formulation .",
    "+   + after finding an optimal cca solution , we are able to re - normalise @xmath33 so that @xmath45 holds .",
    "we emphasis that even though @xmath52 has been removed from the constraint the link to kernels ( kernel tricks and rkhs ) is represented in the choice of kernel @xmath52 used for the dual - view , otherwise the presented method is a sparse linear cca .",
    "we can now focus on obtaining an optimal sparse solution for @xmath46 .",
    "+   + it is obvious that when starting with @xmath53 further minimising is impossible . to avoid this trivial solution and to ensure that the constraints hold in our starting condition ,",
    "therefore there must be at least one @xmath54 for some @xmath55 that is equal to @xmath56 .",
    "] we set @xmath51 by fixing @xmath57 for some fixed index @xmath58 so that @xmath59 $ ] . to further obtain a sparse solution on @xmath33",
    "we constrain the @xmath60norm of the remaining coefficients @xmath61 , where we define @xmath62 $ ] .",
    "the motivation behind isolating a specific @xmath28 and constraining the @xmath60norm of the remaining coefficients , other than ensuring a non - trivial solution , follows the intuition of wanting to find similarities between the samples given some basis for comparison . in the case of documents ,",
    "this places the chosen document ( indexed by @xmath28 ) in a semantic context defined by an additional ( sparse ) set of documents .",
    "this captures our previously stated goal of wanting to be able to extract an interpretable relationship from a sparse word representation from one language to a particular and specific context in the other language .",
    "the @xmath63 choices of @xmath28 correspond to the @xmath64 projection vectors .",
    "we discuss the selections of @xmath28 and the ensuring of orthogonality of the sparse projections in section [ sccaalgsec ] . +   + we are also now able to constrain the @xmath60norm of @xmath65 without effecting the convexity of the problem .",
    "this gives the final optimisation as @xmath66 subject to @xmath51 .",
    "the expression @xmath67 is quadratic in the variables @xmath65 and @xmath33 and is bounded from below @xmath68 and hence is convex since it can be expressed as @xmath69 h [ { \\mathbf{w } } ' { \\mathbf{e}}']'$ ] . if @xmath70 were not positive definite taking multiple @xmath71 of the eigenvector @xmath72 $ ] with negative eigenvalue @xmath73 would give @xmath74 creating arbitrarily large negative values . when minimising subject to linear constraints ( 1-norms are linear ) this makes the whole optimisation convex .",
    "+   + while equation ( [ eqn : stacca1 ] ) is similar to least absolute shrinkage and selection operator ( lasso ) @xcite , it is not a standard lasso problem unless @xmath33 is fixed .",
    "the problem in equation ( [ eqn : stacca1 ] ) could be considered as a double - barreled lasso where we are trying to find sparse solutions for _ both _ @xmath46 .",
    "we propose a novel method for solving the optimisation problem represented in equation ( [ eqn : stacca1 ] ) , where the suggested algorithm minimises the gap between the primal and dual lagrangian solutions using a greedy search on @xmath75 .",
    "the proposed algorithm finds a sparse @xmath76 vectors , by iteratively solving between the ml primal and dual formulation in turn .",
    "we give the proposed algorithm as the following high - level pseudo - code .",
    "a more complete description will follow later ;    * repeat 1 .",
    "use the dual lagrangian variables to solve the ml - primal variables 2 .",
    "check whether all constraints on ml - primal variables hold 3 .",
    "use ml - primal variables to solve the dual lagrangian variables 4 .",
    "check whether all dual lagrangian variable constraints hold 5 .",
    "check whether 2 .",
    "holds , if not go to 1 .",
    "* end    we have yet to address how to determine which elements in @xmath46 are to be non - zero .",
    "we will show that from the derivation given in section [ primald ] a lower and upper bound is computed .",
    "combining the bound with the constraints provides us with a criterion for selecting the non - zero elements for both @xmath65 and @xmath33 .",
    "the criteria being that only the respective indices which violate the bound and the various constraints need to be updated .",
    "+   + we proceed to give the derivation of our problem .",
    "the minimisation @xmath77 subject to @xmath51 can be written as @xmath78 subject to @xmath51 , where @xmath71 , @xmath79 are fixed positive parameters .",
    "+   + to simplify our mathematical notation we revert to uniformly using @xmath33 in place of @xmath80 , as @xmath28 will be fixed in an outer loop so that the only requirement is that no update will be made for @xmath81 , which can be enforced in the actual algorithm .",
    "we further emphasis that we are only interested in the positive spectrum of @xmath33 , which again can be easily enforced by updating any @xmath82 to be @xmath83 constraint by updating any @xmath84 to be @xmath85 . ] .",
    "therefore we could rewrite the constraint @xmath86 as @xmath87 +   + we are able to obtain the corresponding lagrangian @xmath88 subject to @xmath89 where @xmath90 is the dual lagrangian variable on @xmath33 and @xmath47 are positive scale factors as discussed in theorem [ theo1 ] and @xmath91 is the all ones vector .",
    "we note that as we algorithmically ensure that @xmath92 we are able to write @xmath93 as @xmath94 .",
    "+   + the constants @xmath47 can also be considered as the hyper - parameters ( or regularisation parameters ) common in the lasso literature , controlling the trade - off between the function objective and the level of sparsity .",
    "we show that the scale parameters can be treated as a type of dual lagrangian parameters to provide an underlying automatic determination of sparsity .",
    "this potentially sub - optimal setting still obtains very good results and is discussed in section [ sec : hyper ] .",
    "+   + to simplify the @xmath60norm derivation we express @xmath65 by its positive and negative components will only have the positive / negative values of @xmath65 and zero elsewhere . ]",
    "such that @xmath95 subject to @xmath96 .",
    "we limit ourselves to positive entries in @xmath33 as we expect to align with a positive subset of articles .",
    "+   + this allows us to rewrite the lagrangian as @xmath97 the corresponding lagrangian in equation ( [ eqnlarg ] ) is subject to @xmath98 the two new dual lagrangian variables @xmath99 are to uphold the positivity constraints on @xmath100 .      in this section",
    "we will show that the constraints on the dual lagrangian variables will form the criterion for selecting the non - zero elements from @xmath65 and @xmath33 .",
    "first we define further notations used . given the data matrix @xmath101 and kernel matrix @xmath102 as defined in section [ sec2 ] , we define the following vectors @xmath103\\\\ { \\mathbf{w}}^- & = & \\left[w^-_1,\\ldots , w^-_m\\right]\\\\ \\bm{\\alpha}^+ & = & \\left [ \\alpha^+_1,\\ldots , \\alpha^+_m \\right]\\\\ \\bm{\\alpha}^- & = & \\left [ \\alpha^-_1,\\ldots , \\alpha^-_m \\right]\\\\ { \\mathbf{e } } & = & \\left[e_1,\\ldots , e_\\ell \\right]\\\\ \\bm{\\beta } & = & \\left[\\beta_1,\\ldots,\\beta_\\ell \\right].\\end{aligned}\\ ] ] throughout this section let @xmath55 be the index of either @xmath46 that needs to be updated .",
    "we use the notation @xmath104 or @xmath105_i$ ] to refer to the @xmath55th index within a vector and @xmath106 to refer to the @xmath55th element on the diagonal of a matrix .",
    "+   +   + taking derivatives of equation ( [ eqnlarg ] ) in respect to @xmath107 , @xmath108 , @xmath109 and equating to zero gives @xmath110 adding the first two equations gives @xmath111 implying a lower and upper component - wise bound on @xmath112 of @xmath113 we use the bound on @xmath114 to indicate which indices of the vector @xmath65 need to be updated by only updating the @xmath115 s whose corresponding @xmath116 violates the bound .",
    "similarly , we only update @xmath54 that has a corresponding @xmath117 value smaller than @xmath118 .",
    "+   + we are able to rewrite the derivative with respect to @xmath107 in terms of @xmath119 @xmath120 we wish to compute the update rule for the selected indices of @xmath65 .",
    "taking the second derivatives of equation ( [ eqnlarg ] ) in respect to @xmath107 and @xmath108 , gives @xmath121 so for the @xmath122 , the unit vector with entry @xmath56 , we have an exact taylor series expansion @xmath123 and @xmath124 respectively for @xmath125 and @xmath126 as @xmath127 giving us the exact update for @xmath125 by setting @xmath128_i.\\end{aligned}\\ ] ] therefore the update for @xmath125 is @xmath129 .",
    "we also compute the exact update for @xmath126 as @xmath130_i,\\end{aligned}\\ ] ] so that the update for @xmath126 is @xmath131 . recall that @xmath132 , hence the update rule for @xmath115 is @xmath133",
    "therefore we find that the new value of @xmath115 should be @xmath134_i.\\end{aligned}\\ ] ] we must also consider the update of @xmath115 when @xmath116 is within the constraints and @xmath135 , i.e. previously @xmath116 had violated the constraints triggering the updated of @xmath115 to be non zero .",
    "notice from equation ( [ sccdev1 ] ) that @xmath136 it is easy to observe that the only component which can change is @xmath137 , therefore as we need to update @xmath115 towards zero .",
    "hence when @xmath138 the absolute value of the update is @xmath139 else when @xmath140 then the update is the negation of @xmath141 so that the update rule is @xmath142 . in the updating of @xmath115",
    "we ensure that @xmath143 do not have opposite signs ,",
    "i.e. we will always stop at zero before updating in any new direction .",
    "+   + we continue by taking second derivatives of the lagrangian in equation ( [ eqnlarg ] ) with respect to @xmath109 , which gives @xmath144 so for @xmath122 , the unit vector with entry @xmath56 , we have an exact taylor series expansion @xmath145 giving us the following update rule for @xmath146 @xmath147_i,\\end{aligned}\\ ] ] the update for @xmath33 is @xmath148 .",
    "the new value of @xmath54 will be @xmath149_i,\\ ] ] again ensuring that @xmath150 .",
    "input : data matrix @xmath151 , kernel matrix @xmath152 and the value @xmath28 .",
    "@xmath153 initialisation : @xmath154 , @xmath155 , @xmath156 , @xmath57 @xmath157 , @xmath158 @xmath159 @xmath160 @xmath153 update the found weight values : converge over @xmath161 using algorithm [ alg : sccaw ] @xmath153 find the dual values that are to be updated @xmath162 @xmath163 @xmath153 update the found dual projection values converge over @xmath109 using algorithn [ alg : sccae ] @xmath153 find the weight values that are to be updated @xmath164 @xmath160 @xmath165 , @xmath166    * output * : feature directions @xmath167    @xmath168 @xmath169 $ ] @xmath170 @xmath171 $ ] @xmath172 @xmath173 @xmath174 @xmath175    @xmath176 $ ] @xmath177 @xmath178      observe that in the initial condition when @xmath179 from equations ( [ sccdev1 ] ) we are able to treat the scale parameters @xmath180 as dual lagrangian variables and set them to @xmath181 we emphasise that this is to provide an underlying automatic determination of sparsity and may not be the optimal setting although we show in section [ sec : hyper ] that this method works well in practice . combining all the pieces we give the scca algorithm as pseudo - code in algorithm [ alg : scca ] , which takes @xmath28 as a parameter . in order to choose the optimal value of @xmath28 we would need to run the algorithm with all values of @xmath28 and select the one giving the best objective value .",
    "this would be chosen as the first feature .",
    "+   + to ensure orthogonality of the extracted features @xcite for each @xmath182 and corresponding @xmath183 , we compute the residual matrices @xmath184 , @xmath185 by projecting the columns of the data onto the orthogonal complement of @xmath186 , a procedure known as deflation , @xmath187 where @xmath188 is a matrix with columns @xmath189 and @xmath190 is a matrix with columns @xmath191",
    ". the extracted projection directions can be computed ( following @xcite ) as @xmath192 .",
    "similarly we deflate for the dual view @xmath193 where @xmath194 and compute the projection directions as @xmath195 where @xmath196 is a matrix with columns @xmath197 and @xmath198 has columns @xmath199 . the deflation procedure is illustrated in pseudocode in algorithm [ completealgo ] , for a detailed review on deflation we refer the reader to @xcite . +   + checking each value of @xmath28 at each iteration is computationally impractical . in our experiments",
    "we adopt the very simplistic strategy of picking the values of @xmath28 in numerical order @xmath200 .",
    "clearly , there exists intermediate options of selecting a small subset of values at each stage , running the algorithm for each and selecting the best of this subset .",
    "this and other extension of our work will be focused on in future studies .",
    "input : data matrix @xmath151 , kernel matrix @xmath152 .",
    "@xmath201 , @xmath202 @xmath203 @xmath204 = $ ] scca_algorithm@xmath205 @xmath194 @xmath189 @xmath191 @xmath206 @xmath207",
    "in the following experiments we use two paired english - french and english - spanish corpora .",
    "the english - french corpus consists of @xmath208 samples with @xmath209 english features and @xmath210 french features while the english - spanish corpus consists of @xmath211 samples with @xmath212 english features and @xmath213 spanish features .",
    "the features represent the number of words in each language .",
    "both corpora are pre - processed into a term frequency inverse document frequency ( tfidf ) representation followed by zero - meaning ( centring ) and normalisation .",
    "the linear kernel was used for the dual view . the best test performance for the kcca regularisation parameter for the paired corpora",
    "was found to be @xmath214 .",
    "we used this value to ensure that kcca was not at a disadvantage since scca had no parameters to tune .      in the following section",
    "we demonstrate that the proposed approach for automatically determining the regularisation parameter ( hyper - parameter ) @xmath71 ( or alternatively @xmath79 ) is sufficient for our purpose .",
    "the scca problem @xmath215 subject to @xmath51 can be simplified to a general lasso solver by removing the optimisation over @xmath33 , resulting in @xmath216 where , given our paired data , @xmath217 is the inner product between the query and the training samples and @xmath218 is the second paired data samples",
    ". this simplified formulation is trivially solved by algorithm [ alg : scca ] by ignoring the loops that adapt @xmath33 .",
    "the simplification of equation ( [ eqn : stacca ] ) allows us to focus on showing that @xmath71 is close to optimal , which is also true for @xmath79 , and therefore omitted .",
    "+   + the hyper - parameters control the level of sparsity .",
    "therefore , we test the level of sparsity as a function of the hyper - parameter value .",
    "we proceed by creating a new document @xmath219 from a paired language that best matches our query and observe how the change in @xmath71 affects the total number of words being selected .",
    "an  ideal \" @xmath71 would generate a new document , in the paired language , and select an equal number of words in the query s actual paired document . recall that the data has been mean corrected ( centred ) and therefore no longer sparse .",
    "+   + we set @xmath71 to be in the range of @xmath220 $ ] with an increment of @xmath221 and use a leave - paired document - out routine for the english - french corpus , which is repeated for all @xmath208 documents . figure [ gif : larschange ] illustrates , for a single query , the effective change in @xmath71 on the level of sparsity .",
    "we plot the ratio of the total number of selected words to the total number of words in the original document .",
    "an ideal choice of @xmath71 would choose a ratio of @xmath56 ( the horizontal lines ) i.e. create a document with exactly the same number of words as the original document or in other words select a @xmath71 such that the cross would lie on the plot .",
    "we are able to observe that the method for automatically choosing @xmath71 ( the vertical line ) is able to create a new document with a close approximation to the total number of words in the original document .        in table",
    "[ tab : rat ] we are able to show that the average ratio of total number of selected words for each document generated in the paired language is very close to the  ideal \" level of sparsity , while a non - sparse method ( as expected ) generates a document with an average of @xmath222 times the number of words from the original document . now that we have established the automatic setting of the hyper - parameters , we proceed in testing how ` good ' the selected words are in the form of a mate - retreiveal experiment .",
    ".french - english corpus : the ratio of the total number of selected words to the actual total number of words in the paired test document , averaged over all queries . the optimal average ratio if we always generate an ` ideal ' document is @xmath56 . [ cols=\"^,^\",options=\"header \" , ]     [ tab : rat ]      our experiment is of mate - retrieval , in which a document from the test corpus of one language is considered as the query and only the mate document from the other language is considered relevant . in the following experiments the results are an average of retrieving the mate for both english and french ( english and spanish ) and have been repeated @xmath223 times with a random train - test split .",
    "+   + we compute the mate - retrieval by projecting the query document as well as the paired ( other language ) test documents into the learnt semantic space where the inner product between the projected data is computed .",
    "let @xmath224 be the query in one language and @xmath225 the kernel matrix of the inner product between the second language s testing and training documents @xmath226 the resulting inner products @xmath227 are then sorted by value .",
    "we measure the success of the mate - retrieval task using average precision , this assesses where the correct mate within the sorted inner products @xmath227 is located .",
    "let @xmath228 be the index location of the retrieved mate from query @xmath229 , the average precision @xmath230 is computed as @xmath231 where @xmath232 is the number of query documents .",
    "+   + we start by giving the results for the english - french mate - retrieval as shown in figure [ fig : enfr ] .",
    "the left plot depicts the average precision ( @xmath233 standard deviation ) when @xmath234 documents are used for training and the remaining @xmath235 are used as test queries .",
    "the right plot in figure [ fig : enfr ] gives the average precision ( @xmath233 standard deviation ) when @xmath236 documents are used for training and the remaining @xmath237 for testing .",
    "it is interesting to observe that even though scca does not learn the common semantic space using all the features ( average plotted in figure [ fig : enfrp ] ) for either ml primal or dual views ( although scca will use full dual features when using the full number of projections ) its error is extremely similar to that of kcca and in fact converges with it when a sufficient number of projections are used .",
    "it is important to emphasise that kcca uses the full number of documents ( @xmath234 and @xmath236 ) and the full number of words ( an average of @xmath238 for both languages ) to learn the common semantic space .",
    "for example , following the left plot in figure [ fig : enfr ] and the additional plots in figure [ fig : enfrp ] we are able to observe that when @xmath239 projections are used kcca and scca show a similar error . however , scca uses approximately @xmath240 words and @xmath241 documents to learn the semantic space , while kcca uses @xmath238 words and @xmath234 documents . +   +",
    "the second mate - retrieval experiment uses the english - spanish paired corpus . in each run",
    "we randomly split the @xmath242 samples into @xmath236 training and @xmath243 testing paired documents .",
    "the results are plotted in figure [ fig : enes ] where we are clearly able to observe scca outperforming kcca throughout .",
    "we believe this to be a good example of when too many features hinder the learnt semantic space , also explaining the difference in the results obtained from the english - french corpus as the number of features are significantly smaller in that case .",
    "the average level of scca sparsity is plotted in figure [ fig : enesp ] . in comparison",
    "to kcca which uses all words ( @xmath244 ) scca uses a maximum of @xmath245 words .",
    "+   + the performance of scca , especially in the latter english - spanish experiment , shows that we are indeed able to extract meaningful semantics between the two languages , using only the relevant features .                    despite these already impressive results",
    "our intuition is that even better results are attainable if the hyper - parameters would be tuned to give optimal results .",
    "the question of hyper - parameter optimality is left for future research .",
    "although , it seems that the main gain of scca is sparsity and interpretability of the features .",
    "despite being introduced in @xmath246 , cca has proven to be an inspiration for new and continuing research . in this paper",
    "we analyse the formulation of cca and address the issues of sparsity as well as convexity by presenting a novel scca method formulated as a convex least squares approach .",
    "we also provide a different perspective of solving cca by using a ml primal - dual formulation which focuses on the scenario when one is interested in ( or limited to ) a ml - primal representation for the first view while having a ml - dual representation for the second view . a greedy optimisation algorithm is derived . +   +",
    "the method is demonstrated on a bi - lingual english - french and english - spanish paired corpora for mate retrieval .",
    "the true capacity of scca becomes visible when the number of features becomes extremely large as scca is able to learn the common semantic space using a very sparse representation of the ml primal - dual views .",
    "+   + the papers raison dtre was to propose a new efficient algorithm for solving the sparse cca problem .",
    "we believe that while addressing this problem new and interesting questions which need to be addressed have surfaced    * how to automatically compute the hyperparameters @xmath47 values so to achieve optimal results ? * how do we set @xmath28 for each @xmath182 when we wish to compute less than @xmath247 projections ? * extending scca to a ml primal - primal ( ml dual - dual ) framework .",
    "we believe this work to be an initial stage for a new sparse framework to be explored and extended .",
    "david r. hardoon is supported by the epsrc project le strum , ep - d063612 - 1 .",
    "we would like to thank zakria hussain and nic schraudolph for insightful discussions .",
    "this publication only reflects the authors views .",
    "dhanjal , c. , gunn , s.  r. & shawe - taylor , j. ( 2006 ) .",
    "sparse feature extraction using generalised partial least squares . in _ proceedings of the ieee international workshop on machine learning for signal processing _ ( pp .  2732 ) .",
    "friman , o. , borga , m. , lundberg , p. & knutsson , h. ( 2001a ) . a correlation framework for functional mri data analysis . in _ proceedings of the 12th scandinavian conference on image analysis_. bergen , norway .",
    "hardoon , d.  r. & shawe - taylor , j. ( 2003 ) . for different level precision in content - based image retrieval . in _ proceedings of third international workshop on content - based multimedia indexing_. irisa , rennes , france .",
    "sriperumbudur , b.  k. , torres , d. & lanckriet , g. ( 2007 ) .",
    "sparse eigen methods by d.c . programming . in _ in proceedings of 2nd international conference on machine learning _",
    "( pp .  831838 ) .",
    "morgan kaufmann , san francisco , ca .",
    "szedmak , s. , bie , t.  d. & hardoon , d.  r. ( 2007 ) .",
    "a metamorphosis of canonical correlation analysis into multivariate maximum margin learning . in _",
    "15th european symposium on artificial neural networks ( esann)_."
  ],
  "abstract_text": [
    "<S> we present a novel method for solving canonical correlation analysis ( cca ) in a sparse convex framework using a least squares approach . </S>",
    "<S> the presented method focuses on the scenario when one is interested in ( or limited to ) a primal representation for the first view while having a dual representation for the second view . </S>",
    "<S> sparse cca ( scca ) minimises the number of features used in both the primal and dual projections while maximising the correlation between the two views . </S>",
    "<S> the method is demonstrated on two paired corpuses of english - french and english - spanish for mate - retrieval . </S>",
    "<S> we are able to observe , in the mate - retreival , that when the number of the original features is large scca outperforms kernel cca ( kcca ) , learning the common semantic space from a sparse set of features . </S>"
  ]
}