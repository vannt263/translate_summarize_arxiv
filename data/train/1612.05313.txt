{
  "article_text": [
    "a polynomial homotopy is a family of polynomial systems which depend on one parameter .",
    "numerical continuation methods to track solution paths defined by a homotopy are classical , see e.g. :  @xcite and  @xcite .",
    "our goal is to improve the algorithms to track solution paths in two ways :    1 .",
    "polynomial homotopies define deformations of polynomial systems starting at generic instances and moving to specific instances . tracking solution paths that start at singular solutions is not supported by current polynomial homotopy software systems .",
    "2 .   to predict the next solution along a path , the current path trackers apply extrapolation methods on each coordinate of the solution separately , without taking the interdependencies between the variables into account .    * problem statement . *",
    "we want to define an efficient , numerically stable , and robust algorithm to compute a power series expansion for a solution curve of a polynomial system .",
    "the input is a list of polynomials in several variables and a point on a solution curve which vanishes when evaluated at each polynomial in the list .",
    "the output of the algorithm is a series in a parameter @xmath0 , where @xmath0 equals one of the variables in the given list of polynomials .",
    "* background and related work . * as pointed out in  @xcite , polynomials , power series , and toeplitz matrices are closely related . a direct method to solve block banded toeplitz systems",
    "is presented in  @xcite .",
    "the book  @xcite is a general reference for methods related to approximations and power series .",
    "we found inspiration for the relationship between higher - order newton - raphson iterations and hermite interpolation in  @xcite .",
    "the computation of power series is a classical topic in computer algebra  @xcite .",
    "the authors of  @xcite propose new algorithms to manipulate polynomials by values via lagrange interpolation .",
    "* our contributions .",
    "* via linearization , rewriting matrices of series into series with matrix coefficients , we formulate the problem of computing the updates in newton s method as a block structured linear algebra problem . for matrix series where the leading coefficient is regular , the solution of the block linear system satisfies the hermite interpolation problem . for general matrix series , where several of the leading matrix coefficients may be rank deficient , hermite - laurent interpolation applies . to solve the block diagonal linear system",
    ", we propose to reduce the coefficient matrix to a lower triangular echelon form .",
    "our proposed gauss - newton method on power series runs completely numerically .",
    "the source code for the algorithm presented in this paper is archived at github via our accounts nbliss and janverschelde .",
    "* acknowledgments .",
    "* we thank the organizers of the ilas 2016 minisymposium on multivariate polynomial computations and polynomial systems , bernard mourrain , vanni noferini , and marc van barel , for giving the second author the opportunity to present this work .",
    "the solving of linear systems over the field of complex coefficients extends naturally to the field of truncated power series . by naturally , mathematically speaking , we consider arithmetical operations as in any field . as computer programmers we apply operator overloading to extend the same code over any field . in numerical analysis",
    "we are concerned with efficiency and accuracy . to answer questions about the cost overhead and the propagation of roundoff errors , we propose to apply linearization .      instead of working with vectors of power series",
    ", we work with series where the coefficients are vectors . instead of computing with matrices of power series , we propose to compute with power series where the coefficients are matrices .",
    "we formalize the linearization in the following definitions .    given an @xmath1-dimensional vector of power series @xmath2 , truncated to degree  @xmath3 : @xmath4     =     \\left [        \\begin{array}{c }           v_{1,0 } + v_{1,1 } t + \\cdots + v_{1,d } t^d \\\\           v_{2,0 } + v_{2,1 } t + \\cdots + v_{2,d } t^d \\\\ \\vdots \\\\",
    "v_{n,0 } + v_{n,1 } t + \\cdots + v_{n , d } t^d \\\\",
    "\\end{array }     \\right].\\ ] ] rewritten as @xmath5}_{{\\mathbf{v}}_0 }     +     \\underbrace{\\left [        \\begin{array}{c }           v_{1,1 } \\\\",
    "v_{2,1 } \\\\ \\vdots \\\\",
    "v_{n,1 } \\\\",
    "\\end{array }     \\right]}_{{\\mathbf{v}}_1 } t     + \\cdots +     \\underbrace{\\left [        \\begin{array}{c }           v_{1,d } \\\\",
    "v_{2,d } \\\\ \\vdots \\\\",
    "v_{3,d } \\\\",
    "\\end{array }     \\right]}_{{\\mathbf{v}}_d } t^d.\\ ] ] is a _",
    "power series vector _ @xmath2 , truncated to degree @xmath3 , in abbreviated form denoted by @xmath6 the series @xmath2 is then represented by a column vector @xmath7^t$ ] , @xmath8 .",
    "the same rewriting of a vector of power series into a power series vector applies to matrices .",
    "we rewrite a matrix of truncated power series into a power series of matrices , summarized formally as follows .",
    "a _ matrix series @xmath9 _ is a series with matrices as coefficients :    @xmath10    a matrix series , truncated to degree @xmath3 , is represented by a row vector @xmath11 , a_k \\in { { \\mathbb c}}^{n \\times n}$ ] .    as data structures ,",
    "a truncated power series vector is a vector polynomial and a truncated matrix series vector is a matrix polynomial .",
    "however , because of the computational differences we will not use the terminology of vector and matrix polynomials , but refer consistently to vector and matrix series .      given a row vector @xmath11 , a_k \\in { { \\mathbb c}}^{n \\times n}$ ] , which represents a matrix series , truncated to degree  @xmath3 ; and given a column vector @xmath12^t$ ] , @xmath13 , which represents a vector series , truncated to degree  @xmath3 , we denote @xmath14 , where @xmath15 .",
    "then @xmath16     \\left [        \\begin{array}{l }           { \\mathbf{x}}_0 \\\\ { \\mathbf{x}}_1 t",
    "\\\\ { \\mathbf{x}}_2 t^2 \\\\ ~\\vdots \\\\ { \\mathbf{x}}_d t^d        \\end{array }     \\right ]     = { \\mathbf{b}}_0 + { \\mathbf{b}}_1 t + { \\mathbf{b}}_2 t^2 + \\cdots + { \\mathbf{b}}_d t^d\\ ] ] is the expanded form of the linear system @xmath14 .    to solve @xmath14 ,",
    "consider its expansion along the powers of @xmath0 , ignoring powers higher than  @xmath3 :    @xmath17    this expansion results in a block triangular linear system .",
    "assuming that @xmath18 is invertible , we can write : @xmath19 in case @xmath18 is invertible , the solving of @xmath14 is reduced to the solving of @xmath20 regular linear systems , using lu , qr , or svd .",
    "we can then apply the following numerical algorithm , suggested by the equations below : @xmath21 because of roundoff , solving @xmath22 does not give the exact @xmath23 but an approximate @xmath24 .",
    "the approximation errors are propagated to the right hand sides of the other linear systems , so we compute not the exact @xmath25 , @xmath26 , @xmath27 , @xmath28 but approximate @xmath29 , @xmath30 , @xmath27 , @xmath31 .    the condition number of @xmath18 predicts the size of the error . if the condition number of @xmath18 is low and the degree @xmath3 is small , then we can afford the computation of the terms of the solution series @xmath32 in a staggered manner , computing one term after the other .",
    "[ propcost ] in case the leading coefficient @xmath18 of the matrix series @xmath33 is regular , the equation in  ( [ eqregproblem ] ) can be solved with @xmath34 operations , where @xmath1 is the dimension of the matrix coefficients in  @xmath33 and where @xmath3 is the degree of truncated series in  @xmath33 .",
    "the cost of the staggered solver is @xmath34 :    1 .",
    "@xmath35 for the decomposition of the matrix @xmath18 ; and 2 .",
    "@xmath36 for the back substitutions using the decomposition of @xmath18 and the convolutions to compute the right hand sides .",
    "adding the cost of the back substitutions to the cost of decomposition gives the formula for the total cost .",
    "if the condition number of @xmath18 is high and the degree @xmath3 is large , then it may be advantageous to solve the following linear system : @xmath37    \\left [      \\begin{array}{c }         { \\mathbf{x}}_0 \\\\         { \\mathbf{x}}_1 \\\\         { \\mathbf{x}}_2 \\\\ \\vdots \\\\         { \\mathbf{x}}_d      \\end{array }    \\right ]    =     \\left [      \\begin{array}{c }         { \\mathbf{b}}_0 \\\\         { \\mathbf{b}}_1 \\\\         { \\mathbf{b}}_2 \\\\ \\vdots \\\\         { \\mathbf{b}}_d      \\end{array }    \\right].\\ ] ] however , ignoring the triangular matrix structure , the cost of solving the larger linear system will be @xmath38 .    instead of considering the larger system above , the alternative solution to an ill - conditioned @xmath18 is to apply double double or quad double arithmetic to the staggered solver .",
    "the cost overhead of double double arithmetic is a fixed factor , experimentally ranging between 5 and 8 , which may be lower than the larger linear system .",
    "the solution to  ( [ eqbiglinsys ] ) corresponds to the solution of the hermite interpolation problem at @xmath39 for @xmath32 satisfying the conditions @xmath40    for the zero - th derivative , we solve @xmath41 which is equivalent to solving @xmath22 , which is the first equation of  ( [ eqbiglinsys ] ) . for the first derivative , application of the product rule",
    "gives @xmath42 which for @xmath43 simplifies to @xmath44 , which is the left hand side of the second equation in  ( [ eqbiglinsys ] ) .",
    "because @xmath45 at @xmath39 equals @xmath46 , we have that @xmath25 is the solution of @xmath47 .",
    "in general , if @xmath48 denotes the @xmath49^th^ derivative of @xmath50 with respect to @xmath0 , we have @xmath51 which at @xmath43 becomes @xmath52 likewise , for the right hand side we have @xmath53 .",
    "so the factorials cancel from both sides and we are left with the @xmath1^th^ equation of  ( [ eqbiglinsys ] ) ; solving these simultaneously is thus the same as solving  ( [ eqbiglinsys ] ) .",
    "the relationship with hermite interpolation is interesting when the leading coefficient matrix @xmath18 is singular .",
    "a power series with positive exponents is invertible when the leading constant coefficient is invertible .",
    "if the leading constant coefficient is zero , then we can define an inverse if we allow negative leading exponents .",
    "for example , the inverse of @xmath54 is @xmath55 . we compute this via @xmath56 the inverse @xmath57 of @xmath58",
    "is the solution to the equation @xmath59 which , when expanded along powers of @xmath0 , gives the triangular linear system @xmath60 so the inverse of @xmath58 is @xmath61 . to get the inverse of @xmath54 ,",
    "we multiply @xmath61 by @xmath62 .",
    "the verification gives @xmath63    the generalization of zero constant coefficients for matrix series are series where the leading coefficients are singular matrices .",
    "we introduce the computation of the inverse of such series by example .",
    "[ ex2dim2order ] consider a 2-dimensional second order matrix series @xmath64     =      \\underbrace {       \\left [         \\begin{array}{cc }            0 & 4 \\\\            0 & 0         \\end{array }       \\right ]     } _ { a_0 }     +      \\underbrace {       \\left [         \\begin{array}{cc }            4 & 0 \\\\            4 & 0         \\end{array }       \\right ]     } _ { a_1 } t     + o(t^2).\\ ] ] both @xmath18 and @xmath65 are singular .",
    "yet , there is a well defined inverse to this series : @xmath66 \\\\     & = &      \\underbrace {       \\left [         \\begin{array}{cc }          0    & 0 \\\\          1/4 & -1/4       \\end{array }     \\right ]     } _ { b_{-1 } } t^{-1 }     +     \\underbrace {       \\left [         \\begin{array}{cc }          0 & 1/4 \\\\          0 & 0       \\end{array }     \\right ]     } _ { b_0 } + o(t)\\end{aligned}\\ ] ] denote the identity matrix by @xmath67 .",
    "then the condition @xmath68 is equivalent to the equation @xmath69 expanded along the powers of @xmath0 gives @xmath70 where @xmath71 denotes the 2-dimensional zero matrix . in matrix form",
    ", we have the following block matrix linear system : @xmath72     \\left [        \\begin{array}{c }           b_{-1 } \\\\ b_0        \\end{array }     \\right ]     =     \\left [       \\begin{array}{c }          0 \\\\ i \\\\ 0       \\end{array }     \\right],\\ ] ] which has as solutions the matrices @xmath73 and @xmath74 from above .",
    "to solve a linear system where the coefficient matrix is a matrix series with singular matrices as coefficients , the explicit computation of the inverse is not needed .",
    "we obtain a similar structured block matrix as in the above example .",
    "( example  [ ex2dim2order ] continued . )",
    "the equation @xmath75 for a second order matrix series is @xmath76 which , expanding along powers of @xmath0 , is equivalent to @xmath77 which in matrix form becomes @xmath78     \\left [        \\begin{array}{c }           { \\mathbf{x}}_0 \\\\           { \\mathbf{x}}_1        \\end{array }     \\right ]     =     \\left [       \\begin{array}{c }          { \\mathbf{b}}_0 \\\\ { \\mathbf{b}}_1 \\\\ 0       \\end{array }     \\right].\\ ] ] the system can be solved in the same manner as the computation of the inverse , since the coefficient matrix is the same as in  ( [ eqblockinverse ] ) .",
    "but we could use the inverse as well to solve the system .    for this particular example , the relations in  ( [ eqexpowersexpanded ] ) commute .",
    "therefore , we not only have a left inverse , but also a right inverse for the coefficient matrix . in particular :",
    "@xmath79     \\left [       \\begin{array}{cc }          a_0 & 0",
    "\\\\          a_1 & a_0 \\\\           0   & a_1   \\\\",
    "\\end{array }     \\right ]     =     \\left [       \\begin{array}{cc }          i & 0 \\\\          0 & i \\\\",
    "\\end{array }     \\right].\\ ] ] therefore , using @xmath73 and @xmath74 we can explicit expressions for the solutions @xmath23 and @xmath25 : @xmath80     =     \\left [       \\begin{array}{ccc }          b_0 & b_{-1 } & 0 \\\\          0 & b_0 & b_{-1 }        \\end{array }     \\right ]     \\left [       \\begin{array}{c }          { \\mathbf{b}}_0 \\\\ { \\mathbf{b}}_1 \\\\ 0       \\end{array }     \\right ]     =     \\left [       \\begin{array}{l }          b_{0 } { \\mathbf{b}}_0 + b_{-1 } { \\mathbf{b}}_1 \\\\ b_0 { \\mathbf{b}}_1       \\end{array }     \\right].\\ ] ]    in the example above , the right inverse of the block matrix leads to a left inverse . for matrix series of order  @xmath3",
    ", we have the following equations .",
    "@xmath81    \\left [      \\begin{array}{c }         b_{-d } \\\\         \\vdots \\\\",
    "b_{-1 } \\\\",
    "b_0      \\end{array }    \\right ]    =    \\left [      \\begin{array}{c }         0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\         i \\\\         0 \\\\ \\vdots \\\\ 0      \\end{array }    \\right]\\ ] ]    with the matrices @xmath74 , @xmath73 , @xmath27 , @xmath82 , we can write a left inverse : @xmath83    \\left [      \\begin{array}{cccc }          a_0   &     0    & \\cdots & 0 \\\\          a_1   &    a_0   & \\cdots & 0 \\\\        \\vdots &    a_1   & \\ddots & \\vdots \\\\",
    "a_d   & \\vdots & \\ddots & a_0 \\\\           0    &    a_d   & \\ddots & a_1 \\\\        \\vdots & \\vdots & \\ddots & \\vdots \\\\           0    &     0    & \\cdots & a_d      \\end{array }    \\right].\\ ] ] the product above",
    "will lead to the identity matrix if the relations commute .",
    "in particular , we must have that @xmath84 and the additional off diagonal relations that lead to zero must hold as well .    the block matrix with the matrices @xmath18 , @xmath65 , @xmath27 , @xmath85 is a block toeplitz matrix .",
    "there are efficient algorithms to solve linear systems which have a toeplitz matrix as their coefficient matrix .",
    "the toeplitz structure applies only to the outer blocks . in our application , the blocks inside the block matrix are not necessarily structured matrices .    given a matrix series @xmath86 and a right hand side vector @xmath87 , the solution to @xmath88    \\left [      \\begin{array}{c }         { \\mathbf{x}}_{-d } \\\\ { \\mathbf{x}}_{-d+1 }",
    "\\\\ \\vdots \\\\",
    "{ \\mathbf{x}}_0      \\end{array }    \\right ]    =    \\left [      \\begin{array}{c }         { \\mathbf{0}}\\\\ { \\mathbf{0}}\\\\ \\vdots \\\\ { \\mathbf{0}}\\\\          { \\mathbf{b}}_0 \\\\ { \\mathbf{b}}_1 \\\\ \\vdots \\\\ { \\mathbf{b}}_d      \\end{array }    \\right]\\ ] ] corresponds to the hermite - laurent interpolation problem for the series @xmath89 satisfying the conditions @xmath90    we derive the matrix formulation of the problem from the hermite - laurent interpolation problem for @xmath91 . the condition @xmath92 is equivalent to @xmath93 the interpolation condition on the zero - th derivative is @xmath94 , which gives the first equation in  ( [ eqbigsys2 ] ) .",
    "the condition on the first derivative is @xmath95 which at @xmath39 evaluates to @xmath96 .",
    "the derivative of @xmath97 at @xmath39 evaluates to @xmath98 .",
    "taking @xmath96 as the left hand side and @xmath98 as the right hand side corresponds to the first equation in  ( [ eqbigsys2 ] ) with a nonzero right hand side .",
    "evaluating the second derivative at @xmath43 leaves with @xmath99 at the left and @xmath100 at the right , which corresponds to the last equation of  ( [ eqbigsys2 ] ) .",
    "for any @xmath101 , the interpolating conditions can be rewritten in the matrix formulation of  ( [ eqbigsys2 ] ) .",
    "the system  ( [ eqbigsys2 ] ) is not guaranteed to have a solution and there may not be a unique solution .",
    "we propose to solve the system  ( [ eqbigsys2 ] ) in the least squares sense .",
    "unlike the cost estimate in proposition  [ propcost ] for matrix series with a regular leading coefficient , the cost of solving the system  ( [ eqbigsys2 ] ) is @xmath102 . in the next section",
    "we show how to reduce this cost .",
    "we can generalize the linear systems in in  ( [ eqbiglinsys ] ) and in  ( [ eqbigsys2 ] ) by the following :    @xmath103    \\left [      \\begin{array}{c }         { \\mathbf{x}}_{-d } \\\\ { \\mathbf{x}}_{-d+1 }",
    "\\\\ \\vdots \\\\ { \\mathbf{x}}_0 \\\\ { \\mathbf{x}}_1 \\\\ \\vdots \\\\ { \\mathbf{x}}_d      \\end{array }    \\right ]    =    \\left [      \\begin{array}{c }         { \\mathbf{0}}\\\\ { \\mathbf{0}}\\\\ \\vdots \\\\ { \\mathbf{0}}\\\\          { \\mathbf{b}}_0 \\\\ { \\mathbf{b}}_1 \\\\ \\vdots \\\\ { \\mathbf{b}}_d      \\end{array }    \\right].\\ ] ]    if @xmath18 is regular , then all coefficients @xmath104 , @xmath105 , @xmath27 , @xmath106 with the negative exponents in the power series are zero and the coefficient matrix of  ( [ eqbigsys3 ] ) can be reduced to the coefficient matrix of  ( [ eqbiglinsys ] ) by removing the first @xmath107 rows and columns . if @xmath18 and eventually also other matrices @xmath108 are singular , then all coefficients @xmath25 , @xmath25 , @xmath27 , @xmath28 with the positive exponents in the power series may be zero and the coefficient matrix of  ( [ eqbigsys3 ] ) can be reduced to the coefficient matrix of  ( [ eqbigsys2 ] ) by removing the last @xmath107 columns .",
    "figure  [ figechelon ] shows the structure of the coefficient matrix of  ( [ eqbigsys3 ] ) for the regular case , when @xmath18 is regular and all block matrices are dense . with column operations we can reduce the matrix to a lower triangular echelon form as shown at the right of figure  [ figechelon ] .",
    "( 400,100)(0,0 ) ( 0,0 ) at the left , with at the right its lower triangular echelon form.,title=\"fig : \" ] ( 170,0 ) at the left , with at the right its lower triangular echelon form.,title=\"fig : \" ]    the lower triangular echelon form of a matrix is a lower triangular matrix with zero elements above the diagonal . if the matrix is regular , then all diagonal elements are nonzero . for a singular matrix ,",
    "the zero rows of its echelon form are on top ( have the lowest row index ) and the zero columns are at the right ( have the highest column index ) .",
    "every nonzero column has one pivot element , which is the nonzero element with the smallest row index in the column .",
    "all elements at the right of a pivot are zero .",
    "columns may need to be swapped so that the row indices of the pivots of columns with increasing column indices are sorted in decreasing order .",
    "( example  [ ex2dim2order ] continued . ) for the matrix series in  ( [ eq2dim2order ] ) , we have the following reduction : @xmath109    \\rightarrow    \\left [       \\begin{array}{cccccc }          0 & 0 &   0 & 0 &   0 & 0 \\\\          4 & 0 &   0 & 0 &   0 & 0 \\\\          0 & 4 &   0 & 0 &   0 & 0 \\\\          0 & 4 & -4 & 0 &   0 & 0 \\\\          0 & 0 &   0 & 4 &   0 & 0 \\\\          0 & 0 &   0 & 4 & -4 & 0       \\end{array }    \\right].\\ ] ] because of the singular matrix coefficients in the series , we find zeros on the diagonal in the echelon form .    given a general @xmath1-by-@xmath110 dimensional matrix  @xmath111 , the lower triangular echelon form @xmath112 can be described by one @xmath1-by-@xmath1 row permutation matrix @xmath113 which swaps the zero rows of  @xmath111 and a sequence of @xmath110 column permutation matrices @xmath114 ( of dimension @xmath110 ) and multiplier matrices @xmath115 ( also of dimension @xmath110 ) .",
    "the matrices @xmath114 define the column swaps to bring the pivots with lowest row indices to the lowest column indices .",
    "the matrices @xmath115 contain the multipliers to reduce what is at the right of the pivots to zero .",
    "then the construction of the lower triangular echelon form can be summarized in the following matrix equation :    @xmath116    similar to solving a linear system with a lu factorization , the multipliers are applied to the solution of the lower triangular system which has @xmath112 as its coefficient matrix .",
    "we end this section with a couple of remarks concerning the efficiency and generality of this lower triangular echelon form .",
    "if the leading coefficient @xmath18 in the matrix series is regular ( as illustrated by figure  [ figechelon ] ) , we may copy the lower triangular echelon form @xmath117 of @xmath18 to all blocks on the diagonal and apply the permutation @xmath118 and column operations as defined by @xmath119 to all other column blocks in  @xmath111 .",
    "if only interested in the solution , the regularity of  @xmath18 implies that all @xmath120 , for @xmath121 , and thus we may use the lower triangular echelon form of @xmath122 to solve  ( [ eqbiglinsys ] ) with substitution . in the regular case , with this quick optimization we obtain the same cost as in proposition  [ propcost ] .    in general , in case @xmath18 and several other matrix coefficients",
    "may be rank deficient , the diagonal of nonzero pivot elements will shift towards the bottom of  @xmath112 .",
    "we then find as solutions vectors in the null space of the upper half of the matrix  @xmath111 .",
    "given a polynomial system @xmath123 and an approximation @xmath124 for a solution of the system , the update @xmath125 to @xmath124 is computed as the solution of the linear system @xmath126 where @xmath127 denotes the jacobian matrix of @xmath128 . if @xmath124 is given as a truncated power series in @xmath0 , then we could formally substitute the series into @xmath129 and @xmath130 and then solve the linear system @xmath131 . instead of this formal substitution",
    ", we propose to apply hermite interpolation on  ( [ eqnewton ] ) .",
    "observe that for a series @xmath132 its maclaurin expansion is @xmath133 where @xmath134 denotes the @xmath49-th derivative of @xmath32 evaluated at zero . then : @xmath135 so we may as well look directly for @xmath134 .    abbreviating the expression in  ( [ eqnewton ] ) as @xmath136 , we take derivatives with respect to @xmath0 : @xmath137 which in matrix format becomes @xmath138     \\left [        \\begin{array}{l }           \\delta \\\\",
    "\\delta ' \\\\ \\delta '' \\\\ \\delta ' ''        \\end{array }     \\right ]     =      -     \\left [        \\begin{array}{l }           { \\mathbf{f}}\\\\ { \\mathbf{f } } ' \\\\ { \\mathbf{f } } '' \\\\ { \\mathbf{f } } ' ''        \\end{array }     \\right].\\ ] ] we now see that  ( [ eqnewtonhermite ] ) has the same structure as the linear system in  ( [ eqbiglinsys ] ) .",
    "techniques of algorithmic differentiation  @xcite provide an efficient evaluation of the derivatives .",
    "our power series methods have been implemented in phcpack  @xcite and are available to the python programmer via phcpy  @xcite .",
    "the examples illustrate the application of our methods to polynomial homotopy continuation .",
    "viviani s curve is an example of a natural parameter homotopy , where one of the original variables in the system is taken as the continuation parameter . in the second example",
    ", we shift one of the circles in the apollonius problem with an artificial parameter homotopy .",
    "the third example develops a solution curve from an initial form system , as an illustration of a generalized polyhedral homotopy .",
    "we start with the example of viviani s curve , which is defined as the intersection of the sphere @xmath139 and the cylinder @xmath140 ; see figure  [ vivianicurve ] .",
    "we wish to expand the curve where @xmath141 , using @xmath142 as the free variable for the series expansion .",
    "we choose the point @xmath143 as the base point of our expansion .",
    "however , the jacobian matrix of the system with respect to the non - free variables , @xmath144 drops rank at @xmath145 , or in other words the variety is singular there .",
    "thus we must use other methods to obtain a better guess to begin newton s method .",
    "using one step of the newton - puiseux method we find @xmath146 for the first term of the power series expansion at @xmath145 .",
    "since we chose @xmath142 as the free variable , the @xmath147 means that our series should be a puiseux expansion in powers of @xmath148 .",
    "to avoid this , we substitute @xmath149 , and now may set @xmath150 . in general",
    ", we can always perform such a substitution for the component we ve chosen as the free variable to avoid computations with fractional exponents .",
    "following section  [ sec : newtonhermite ] , we compute @xmath130 at @xmath151 and find @xmath152 for @xmath153 we have @xmath154 which when evaluated at @xmath151 is @xmath155 higher derivatives of @xmath130 vanish , as does @xmath156 since we know the first term of the series , so the block matrix we wish to solve is    @xmath157     \\left [        \\begin{array}{l }           \\delta ' \\\\ \\delta '' \\\\ \\delta ' '' \\\\",
    "\\delta '' ''        \\end{array }     \\right ]     =      -     \\left [        \\begin{array}{l }           { \\mathbf{f } } ' \\\\ { \\mathbf{f } } '' \\\\ { \\mathbf{f } } ' '' \\\\ { \\mathbf{f } } '' ''        \\end{array }     \\right]\\ ] ]    which becomes @xmath158     \\left [        \\begin{array}{l }           \\delta ' \\\\ \\delta '' \\\\ \\delta ' '' \\\\",
    "\\delta '' ''        \\end{array }     \\right ]    =     \\left [      \\begin{array}{c }          0    \\\\          0    \\\\          8    \\\\          16   \\\\          0    \\\\          0    \\\\",
    "-96 \\\\",
    "-96      \\end{array }    \\right].\\ ] ] solving this in the least squares sense , we obtain another term of the series and have @xmath159 .",
    "the classical problem of apollonius consists in finding all circles that are simultaneously tangent to three given circles .",
    "a special case is when the three circles are mutually tangent and have the same radius ; see figure  [ appolfig ] . here",
    "the solution variety is singular  the circles themselves are in fact double solutions . in this figure",
    ", all have radius 3 and centers @xmath160 , @xmath161 , and @xmath162 .",
    "we can in fact study this configuration with power series techniques .",
    "we introduce a parameter @xmath0 to represent a vertical shift of the third circle , and then examine the solutions as we vary @xmath0 .",
    "this is represented algebraically as a solution to @xmath163 we are interested in power series solutions of the above near @xmath43 , so we use @xmath0 as our free variable . to simplify away the @xmath164 , we substitute @xmath165 , @xmath166 , and the system becomes @xmath167",
    "now we can expand at @xmath168 .",
    "the jacobian @xmath130 is @xmath169 which is singular at this point .",
    "one step of the newton - puiseux method yields @xmath170 , at which @xmath130 is again singular .",
    "another step gives two options : @xmath171 and @xmath172 , which are rounded for readability .",
    "we will continue with the second ; call it @xmath23 . at @xmath151 , @xmath130 and @xmath153 are @xmath173 respectively , and @xmath174 vanishes . without evaluating at @xmath43 , we find that @xmath175 , @xmath176 , @xmath177 . from these",
    "we can construct the matrix in the left hand side of equation  ( [ eqnewtonhermite ] ) : @xmath178\\ ] ] and the right side is @xmath179^t.\\ ] ] solving in the least squares sense , we obtain two more terms of the series , so in total we have @xmath180 from this , we can get a better idea of what happens near @xmath43 .",
    "for example , in figure  [ shiftedappol ] one can see the solutions of the system at @xmath181 .",
    "the series help to explain why some solution circles of the perturbation of the special configuration grow faster than others .",
    "we note that we get one extra , but erroneous term .",
    "it is necessary to solve as though the fifth term were obtainable in order to correctly find the fourth , but as newton s method is at best quadratic we know a priori that the fifth can not be found on this step .",
    "cyclic 8-roots has solution curves not reported by backelin  @xcite . in  @xcite , the vector @xmath188 gives the leading exponents of the series .",
    "the corresponding unimodular coordinate transformation @xmath189 is @xmath190   \\quad   \\begin{array}{l }      x_{0 } = z_{0 } \\\\",
    "x_{1 } = z_{1 } z_{0}^{-1 } \\\\",
    "x_{2 } = z_{2 } \\\\",
    "x_{3 } = z_{3 } z_{0 } \\\\",
    "x_{4 } = z_{4 } \\\\",
    "x_{5 } = z_{5 } \\\\",
    "x_{6 } = z_{6 } z_{0}^{-1 } \\\\",
    "x_{7 } = z_{7}. \\end{array}\\ ] ]            .... ( 7.12500000000000e+00 + 7.12500000000000e+00*i)*z0 ^ 4 + ( -1.52745512076048e-16 - 4.25000000000000e+00*i)*z0 ^ 3 + ( -1.25000000000000e+00 + 1.25000000000000e+00*i)*z0 ^ 2 + ( 5.00000000000000e-01 - 1.45255178343636e-17*i)*z0 + ( -5.00000000000000e-01 - 5.00000000000000e-01*i ) ; ....    bounds on the degree of the puiseux series expansion to decide whether a point is isolated or not are are derived in  @xcite . for efficiency of this test",
    ", the quadratic convergence of the computation of the terms in the expansion is relevant .",
    "d.  adrovic and j.  verschelde .",
    "computing puiseux series for algebraic surfaces . in j.",
    "van  der hoeven and m.  van hoeij , editors , _ proceedings of the 37th international symposium on symbolic and algebraic computation ( issac 2012 ) _ , pages 2027 .",
    "acm , 2012 .",
    "d.  adrovic and j.  verschelde .",
    "polyhedral methods for space curves exploiting symmetry applied to the cyclic @xmath1-roots problem . in v.p .",
    "gerdt , w.  koepf , e.w .",
    "mayr , and e.v .",
    "vorozhtsov , editors , _ computer algebra in scientific computing , 15th international workshop , casc 2013 , berlin , germany _ , volume 8136 of _ lecture notes in computer science _ , pages 1029 , 2013 .",
    "d.  a. bini and b.  meini .",
    "solving block banded block toeplitz systems with structured blocks : algorithms and applications . in d.",
    "a. bini , e.  tyrtyshnikov , and p.  yalamov , editors , _ structured matrices _ , pages 2141 .",
    "nova science publishers , inc . ,",
    "commack , ny , usa , 2001 ."
  ],
  "abstract_text": [
    "<S> we consider the extension of the method of gauss - newton from complex floating - point arithmetic to the field of truncated power series with complex floating - point coefficients . </S>",
    "<S> with linearization we formulate a linear system where the coefficient matrix is a series with matrix coefficients . </S>",
    "<S> the structure of the linear system leads in the regular case to a block triangular system . </S>",
    "<S> when one or several of the matrix coefficients in the series are singular , the linear system has a block banded structure . </S>",
    "<S> we show that in the regular case , the solution of the linear system satisfies the conditions of the hermite interpolation problem . in general , we solve a hermite - laurent interpolation problem , via a lower triangular echelon form on the coefficient matrix . </S>",
    "<S> hermite interpolation also applies to newton s method where we compute the coefficients of the power series via its maclaurin expansion . with a couple of illustrative examples </S>",
    "<S> , we demonstrate the application to polynomial homotopy continuation .    * </S>",
    "<S> key words and phrases . * linearization , gauss - newton , hermite interpolation , polynomial homotopy , power series . </S>"
  ]
}