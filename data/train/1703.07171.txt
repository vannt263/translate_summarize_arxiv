{
  "article_text": [
    "sparsity penalties are important priors for regularizing linear systems .",
    "typically one tries to solve a formulation that minimizes a trade - off between sparsity and residual error such as @xmath1 where @xmath2 is the number of non - zero elements in @xmath3 , and the matrix @xmath4 is of size @xmath5 .",
    "direct minimization of is generally considered difficult because of the properties of the @xmath6 function , which is non - convex and discontinuous .",
    "the method that has by now become the standard approach is to replace @xmath2 with the convex @xmath0 norm @xmath7 @xcite .",
    "this choice can be justified with the @xmath0 norm being the convex envelope of the @xmath6 function on the set @xmath8 .",
    "furthermore , strong performance guarantees can be derived @xcite if @xmath4 obeys a rip @xmath9 for all vectors @xmath3 with @xmath10 , where @xmath11 is a bound on the number of non - zero terms in the sought solution .",
    "the @xmath0 approach does however suffer from a shrinking bias .",
    "in contrast to the @xmath6 function the @xmath0 norm penalizes both small elements of @xmath3 , assumed to stem from measurement noise , and large elements , assumed to make up the true signal , equally . in some sense the suppression of noise also requires an equal suppression of signal .",
    "therefore non - convex alternatives able to penalize small components proportionally harder have been considered @xcite . on the downside convergence to the global optimum",
    "is not guaranteed .",
    "this paper considers the non - convex relaxation @xmath12 where @xmath13 .",
    "figure  [ fig : threeregterms ] shows one dimensional illustrations of the @xmath6-function , @xmath0-norm and @xmath14 term .",
    "it can be shown @xcite that the convex envelope of @xmath15 where @xmath16 is some given vector , is @xmath17 note that similarly to the @xmath6 function @xmath14 does not penalize elements that are larger than @xmath18 .",
    "in fact it is easy to show that the minimizer @xmath19 of both and is given by thresholding of @xmath16 , that is , @xmath20 if @xmath21 and @xmath22 if @xmath23 . if there is an @xmath24 such that @xmath25 then the minimizer is not unique",
    ". in @xmath26 can take either the value @xmath27 or @xmath18 and in any convex combination of these .",
    "the regularization term @xmath14 is by itself not convex , see figure  [ fig : threeregterms ] .",
    "however when combined with a quadratic term @xmath28 , non - convexities are canceled and the result is a convex objective function .    ) .,width=226 ]    assuming that @xmath4 fulfills it is natural to wonder about convexity properties of .",
    "intuitively @xmath29 seems to behave like @xmath30 which combined with @xmath31 only has one local minimum . in this paper",
    "we make this reasoning formal and study the stationary points of .",
    "we show that if @xmath32 is a stationary point of and the elements of the vector @xmath33 fulfill @xmath34 $ ] then for any other stationary point @xmath35 we have @xmath36 .",
    "a simple consequence is that if we for example find such a local minimizer with @xmath37 then this is the sparsest possible one .",
    "the meaning of the vector @xmath16 can in some sense be understood by seeing that the stationary point @xmath32 fulfills @xmath38 , which we prove in section [ sec : sparsity ] .",
    "hence @xmath32 can be obtained through thresholding of the vector @xmath16 .",
    "our results then essentially state that if the elements of @xmath16 are not to close to the thresholds @xmath39 then @xmath36 holds for all other stationary points @xmath35 .    in two very recent papers @xcite the relationship between ( both local and global ) minimizers of and is studied . among other things",
    "@xcite shows that if @xmath40 then any local minimizer of is also a local minimizer of , and that their global minimizers coincide .",
    "hence results about the stationary points of are also relevant to the original discontinuous objective .",
    "the theory of rank minimization largely parallels that of sparsity with the elements @xmath41 of the vector @xmath3 replaced by the singular values @xmath42 of the matrix @xmath43 . typically we want to solve a problem of the type @xmath44 where @xmath45 is some linear operator on the set of @xmath46 matrices",
    ". in this context",
    "the standard approach is to replace the rank function with the convex nuclear norm @xmath47 @xcite .",
    "it was first observed that this is the convex envelope of the rank function over the set @xmath48 in @xcite . in @xcite",
    "the notion of rip was generalized to the matrix setting requiring that @xmath49 is a linear operator @xmath50 fulfilling @xmath51 for all @xmath43 with @xmath52 .",
    "since then a number of generalizations that give performance guarantees for the nuclear norm relaxation have appeared @xcite .",
    "non - convex alternatives have also been shown to improve performance @xcite .",
    "analogous to the vector setting it was recently shown @xcite that the convex envelope of @xmath53 is given by @xmath54 where @xmath55 is the vector of singular values of @xmath43 . in @xcite",
    "an efficient fixed - point algorithm is developed for objective functions of the type @xmath56 with linear constraints .",
    "the approach is illustrated to work well even when @xmath57 which gives a non - convex objective .    in this paper",
    "we consider @xmath58 where @xmath49 obeys .",
    "our main result states that if @xmath59 is a stationary point of and @xmath60 has no singular values in the interval @xmath61 $ ] then for any other stationary point we have @xmath62 .",
    "in this section we introduce some preliminary material and notation . in general we will use boldface to denote a vector @xmath3 and its @xmath24th element @xmath41 . by @xmath63",
    "we denote the standard euclidean norm @xmath64 .",
    "we use @xmath42 to denote the @xmath24th singular value of a matrix @xmath43 .",
    "the vector of all singular values is denoted @xmath55 .",
    "a diagonal matrix with diagonal elmenents @xmath3 will be denoted @xmath65 .",
    "the scalar product is defined as @xmath66 , where @xmath67 is the trace function , and the frobenius norm @xmath68 .",
    "the adjoint of a the linear matrix operator @xmath49 is denoted @xmath69 . for functions taking values in @xmath70 such as @xmath14 we will frequently use the convention that @xmath71 .",
    "the function @xmath72 will be useful when considering stationary points , since it is convex with a well defined sub - differential .",
    "we can write @xmath73 as @xmath74 its sub - differential is given by @xmath75 & x = 0 \\end{cases}. \\label{eq : vectorsubgrad}\\ ] ] note that the sub - differential consists of a single point for each @xmath76 .",
    "by @xmath77 we mean the set of vectors @xmath78 .",
    "figure  [ fig : gfunk ] illustrates @xmath73 and its sub - differential .",
    "( left ) and its sub - differential @xmath79 ( right ) .",
    "note that the sub - differential contains a unique element everywhere except at @xmath80.,title=\"fig:\",width=151 ]   ( left ) and its sub - differential @xmath79 ( right ) .",
    "note that the sub - differential contains a unique element everywhere except at @xmath80.,title=\"fig:\",width=151 ]    for the matrix case we similarly define @xmath81 .",
    "it can be shown @xcite that a matrix @xmath82 is in the sub - differential of @xmath83 at @xmath43 if and only if @xmath84 where @xmath85 is the svd of @xmath43 . roughly speaking the sub - differential at @xmath43",
    "is obtained by taking the sub - differential at each singular value .    in section  [ sec : rank ] we utilize the notion of doubly sub - stochastic ( dss ) matrices @xcite .",
    "a matrix @xmath86 is dss if its rows and columns fulfill @xmath87 and @xmath88 .",
    "the dss matrices are closely related to permutations .",
    "let @xmath89 denote a permutation and @xmath90 the matrix with elements @xmath91 and zeros otherwise .",
    "it is shown in @xcite ( lemma 3.1 ) that an @xmath92 matrix is dss if and only if it lies in the convex hull of the set @xmath93 .",
    "the result is actually proven for matrices with complex entries , but the proof is identical for real matrices .",
    "in this section we consider stationary points of the proposed sparsity formulation .",
    "let @xmath94 the function @xmath95 can equivalently be written @xmath96 taking derivatives we see that the stationary points solve @xmath97 the following lemma clarifies the connection between a stationary point @xmath32 and the vector @xmath98 .",
    "[ lemma : lowrankstatpt ] the point @xmath32 is stationary in @xmath95 if and only if @xmath99 and if and only if @xmath100    by we know that @xmath32 is stationary in @xmath95 if and only if @xmath99 . similarly , inserting @xmath101 and @xmath102 in shows that @xmath32 is stationary in @xmath103 if and only if @xmath99 . since @xmath103 is convex in @xmath3 ,",
    "this is equivalent to solving .",
    "the above result shows that stationary points of @xmath95 are sparse approximations of @xmath16 in the sense that small elements are suppressed .",
    "the elements of @xmath32 are either zero or have magnitude larger than @xmath18 assuming that the vector @xmath16 has no elements that are precisely @xmath39 .",
    "the term @xmath104 can also be seen as a local approximation of @xmath105 @xmath106 replacing @xmath107 with its first order taylor expansion @xmath108 ( ignoring the constant term ) reduces to @xmath109 , where @xmath110 is a constant .",
    "we now assume that @xmath4 is a matrix fulfilling the rip .",
    "we can equivalently write @xmath111 where @xmath112 the term @xmath113 is constant with respect to @xmath3 and we can therefore drop it without affecting the optimizers .",
    "the point @xmath3 is a stationary point of @xmath95 if @xmath114 , that is there is a vector @xmath115 such that @xmath116 .",
    "our goal is now to find constraints that assure that this system of equations have only one sparse solution . before getting into the details we outline the overall idea . for simplicity",
    "consider two differentiable strictly convex functions @xmath117 and @xmath118 .",
    "their sum is minimized by the stationary point @xmath32 fulfilling @xmath119 .",
    "since @xmath118 is strictly convex its directional derivative @xmath120 is increasing for all directions @xmath121 .",
    "similarly @xmath122 is decreasing for all @xmath123 since @xmath124 is strictly concave .",
    "therefore @xmath125 which means that @xmath32 is the only stationary point . in what follows",
    "we will estimate the growth of the directional derivatives of the functions involved in in order to show a similar contradiction .",
    "for the function @xmath126 we do not have convexity , however due to we shall see that it behaves essentially like a convex function for sparse vectors @xmath3 . additionally , because of the non - convex perturbation @xmath127 we need somewhat sharper estimates than just growth of the directional derivatives of @xmath73 .",
    "we fist consider the estimate for the derivatives of @xmath126 .",
    "note that @xmath128 , and therefore @xmath129 applying now shows that @xmath130 when @xmath131 .    next we need a similar bound on the sub - gradients of @xmath73 . in order to guarantee uniqueness of a sparse stationary point",
    "we need to show that they grow faster than @xmath132 .",
    "the following three lemmas show that provided that the vector @xmath16 , where @xmath133 is the sub - gradient , has elements that are not too close to the thresholds @xmath39 this will be true .",
    "[ lemma : bnd1 ] assume that @xmath115 . if @xmath134 then for any @xmath135 with @xmath136 we have @xmath137 and @xmath138    we first assume that @xmath139 . because of and we have @xmath140",
    ". there are now two possibilities :    * if @xmath141 then @xmath142 and by we therefore must have that @xmath143 . *",
    "if @xmath144 we consider the line @xmath145 see the left graph of figure  [ fig : linefig1 ] . we will show that this line is an upper bound on the sub - gradients for all @xmath144 .",
    "+    + we note that for @xmath146 we have @xmath147 furthermore @xmath148 the right hand side is clearly larger than both @xmath149 for @xmath150 . for @xmath151",
    "we have @xmath152 .",
    "this shows that the line @xmath153 is an upper bound on the subgradients of @xmath73 for every @xmath146 , that is @xmath154 for all @xmath144 and since @xmath155 we get @xmath156 .",
    "the proof for the case @xmath157 is similar .",
    "[ lemma : bnd2 ] assume that @xmath115 .",
    "if @xmath158 then for any @xmath135 with @xmath136 we have @xmath137 and @xmath138    by we see that @xmath159 .",
    "we first assume that @xmath141 and consider the line @xmath160 , see the right graph of figure  [ fig : linefig1 ] .",
    "we have that @xmath161 the right hand side is less than @xmath162 when @xmath163 and less than @xmath164 when @xmath165 . therefore @xmath153 is a lower bound on the subgradients of @xmath73 for all @xmath166 which gives @xmath167 for @xmath141 and since @xmath168 we get @xmath169 .",
    "the case @xmath144 is similar .",
    "[ lemma : subgradbnd ] assume that @xmath115 .",
    "if the elements @xmath170 fulfill @xmath171 $ ] for every @xmath24 , then for any @xmath135 with @xmath136 we have @xmath172 as long as @xmath173 .",
    "the is an immediate consequence of the previous two results .",
    "we have according to lemmas [ lemma : bnd1 ] and [ lemma : bnd2 ] that @xmath174 for all @xmath24 with @xmath175 .",
    "since @xmath176 gives @xmath177 summing over @xmath24 gives @xmath172 as long as @xmath173 .",
    "we are now ready to consider the distribution of stationary points . set @xmath178 and recall that @xmath99 for stationary points @xmath32 ( lemma [ lemma : lowrankstatpt ] ) .",
    "[ thm : statpoint ] assume that @xmath32 is a stationary point of @xmath95 and that each element @xmath170 fulfills @xmath171 $ ] .",
    "if @xmath35 is another stationary point of @xmath95 then @xmath179 .",
    "assume that @xmath180 .",
    "we first note that @xmath181 since @xmath32 and @xmath35 are both stationary points we have @xmath182 and @xmath183 , where @xmath99 and @xmath184 . taking the difference between the two equations",
    "gives @xmath185 which implies @xmath186 where @xmath187 .",
    "however , according to the left hand side is less than @xmath188 if @xmath189 which contradicts lemma [ lemma : subgradbnd ] .",
    "[ [ a - one - dimensional - example . ] ] a one dimensional example .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + +    we conclude this section with a simple one dimensional example which shows that the bounds and can not be made sharper .",
    "figure  [ fig : simpleex ] shows the function @xmath190 for different values of @xmath191 .",
    "it is not difficult to verify that this function can have three stationary points ( when @xmath191 ) .",
    "the point @xmath80 is stationary if @xmath192 , @xmath193 if @xmath194 and @xmath195 if @xmath196 , see figure  [ fig : simpleex ] .",
    "for this example @xmath197 and therefore @xmath198 holds with @xmath199 .    now suppose that @xmath192 and that we , using some algorithm , find the stationary point @xmath80 .",
    "we then have @xmath200 theorem 3.3 now tells us that @xmath80 is the unique stationary point if @xmath201 \\leftrightarrow b \\notin \\left[\\frac{1}{\\sqrt{2 } } , 2\\sqrt{2 } \\right].\\ ] ] note that the lower interval bound @xmath202 is precisely when @xmath80 is unique , see the leftmost graph in figure  [ fig : simpleex ] .    similarly suppose that @xmath203 .",
    "for the point @xmath195 we get @xmath204 theorem 3.3 now shows that @xmath195 is unique if @xmath205 \\leftrightarrow b \\notin \\left[\\frac{1}{2\\sqrt{2 } } , \\sqrt{2 } \\right].\\ ] ] here the upper interval bound @xmath206 is precisely when @xmath195 is unique , see rightmost graph in figure  [ fig : simpleex ] .",
    "hence for this example theorem 3.3 is tight in the sense that it would be able to verify uniqueness of the stationary point for every @xmath207 where this holds .",
    "[ cols=\"^,^,^,^,^ \" , ]      we conclude the paper with a synthetic experiment on sparse recovery . for figure  [ fig : sparse_results ] ( a)-(c )",
    "we randomly generated problem instances for sparse recovery .",
    "each instance uses a matrix @xmath4 of size @xmath208 with @xmath209 which was generated by first randomly sampling the elements of a matrix @xmath210 a gaussian @xmath211 distribution .",
    "the matrix @xmath4 was then constructed from @xmath210 by modifying the singular values to be evenly distributed between @xmath212 and @xmath213 . to generate a ground truth solution and a @xmath214 vector",
    "we then randomly select values for @xmath215 nonzero elements of @xmath3 and computed @xmath216 , where all elements of @xmath217 are @xmath218 .",
    "the averaged results ( over 50 random instances for each @xmath219 setting ) are shown in figure  [ fig : sparse_results ] ( a)-(c ) .",
    "similar to the matrix case it is quite clear that the @xmath0 norm ( a ) suffers from shrinking bias .",
    "it consistently gives the best agreement with the ground truth data for values of @xmath220 that are not big enough to generate low cardinality .",
    "in contrast , gives the best fit at the correct cardinality for all noise levels .",
    "this fit was consistently better than that of for all noise levels . in figure",
    "[ fig : sparse_results ] ( c ) we show the fraction of problem instances that could be verified to be optimal .    in figure",
    "[ fig : sparse_results ] ( d ) and ( e ) we tested the case where the elements of an @xmath5 matrix @xmath4 are sampled from @xmath221 @xcite .",
    "here we let @xmath4 be random @xmath222 matrices and generated the ground truth solution and @xmath214 vector as described previously . here",
    "consistently outperformed which exhibits the same tendency to achieve a better fit for non - sparse solutions .",
    "in this paper we studied the local minima of a non - convex rank / sparsity regularization approach . or",
    "main results show that if a rip property holds then the stationary points are often well separated .",
    "this gives an explanation as to why many non - convex approaches such as @xcite can be observed to work well in practice .",
    "our experimental evaluation verifies that the proposed approach often recovers better solutions than standard convex counterparts , even when the rip constraint fails .",
    "pinghua gong , changshui zhang , zhaosong lu , jianhua huang , and jieping ye . a general iterative shrinkage and thresholding algorithm for non - convex regularized optimization problems . in",
    "the 30th international conference on machine learning ( icml ) _ , pages 3745 , 2013 .",
    "vladimir jojic , suchi saria , and daphne koller .",
    "convex envelopes of complexity controlling penalties : the case against premature envelopment . in _",
    "international conference on artificial intelligence and statistics _ , 2011 .",
    "karthik mohan and maryam fazel .",
    "iterative reweighted least squares for matrix rank minimization . in _",
    "communication , control , and computing ( allerton ) , 2010 48th annual allerton conference on _ , pages 653661 .",
    "ieee , 2010 .",
    "samet oymak , karthik mohan , maryam fazel , and babak hassibi . a simplified approach to recovery conditions for low rank matrices . in _ information theory proceedings ( isit ) , 2011 ieee international symposium on _ , pages 23182322 .",
    "ieee , 2011 ."
  ],
  "abstract_text": [
    "<S> this paper considers the problem of recovering either a low rank matrix or a sparse vector from observations of linear combinations of the vector or matrix elements . </S>",
    "<S> recent methods replace the non - convex regularization with @xmath0 or nuclear norm relaxations . </S>",
    "<S> it is well known that this approach can be guaranteed to recover a near optimal solutions if a so called restricted isometry property ( rip ) holds . on the other hand </S>",
    "<S> it is also known to perform soft thresholding which results in a shrinking bias which can degrade the solution .    in this paper </S>",
    "<S> we study an alternative non - convex regularization term . </S>",
    "<S> this formulation does not penalize elements that are larger than a certain threshold making it much less prone to small solutions . </S>",
    "<S> our main theoretical results show that if a rip holds then the stationary points are often well separated , in the sense that their differences must be of high cardinality / rank . </S>",
    "<S> thus , with a suitable initial solution the approach is unlikely to fall into a bad local minima . </S>",
    "<S> our numerical tests show that the approach is likely to converge to a better solution than standard @xmath0/nuclear - norm relaxation even when starting from trivial initializations . in many cases our results </S>",
    "<S> can also be used to verify global optimality of our method .    </S>",
    "<S>     [ section ] [ theorem]lemma [ theorem]proposition [ theorem]corollary [ theorem]remark </S>"
  ]
}