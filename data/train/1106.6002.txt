{
  "article_text": [
    "we study the distribution of thresholding estimators such as hard - thresholding , soft - thresholding , and adaptive soft - thresholding in a linear regression model when the number of regressors can be large .",
    "these estimators can be viewed as penalized least - squares estimators in the case of an orthogonal design matrix , with soft - thresholding then coinciding with the lasso ( introduced by frank and friedman ( 1993 ) , alliney and ruzinsky ( 1994 ) , and tibshirani ( 1996 ) ) and with adaptive soft - thresholding coinciding with the adaptive lasso ( introduced by zou ( 2006 ) ) .",
    "thresholding estimators have of course been discussed earlier in the context of model selection ( see bauer , ptscher and hackl ( 1988 ) ) and in the context of wavelets ( see , e.g. , donoho , johnstone , kerkyacharian , picard ( 1995 ) ) .",
    "contributions concerning distributional properties of thresholding and penalized least - squares estimators are as follows : knight and fu ( 2000 ) study the asymptotic distribution of the lasso estimator when it is tuned to act as a conservative variable selection procedure , whereas zou ( 2006 ) studies the asymptotic distribution of the lasso and the adaptive lasso estimators when they are tuned to act as consistent variable selection procedures . fan and li ( 2001 ) and fan and peng ( 2004 ) study the asymptotic distribution of the so - called smoothly clipped absolute deviation ( scad ) estimator when it is tuned to act as a consistent variable selection procedure . in the wake of fan and li ( 2001 ) and fan and peng ( 2004 ) a large number of papers have been published that derive the asymptotic distribution of various penalized maximum likelihood estimators under consistent tuning ; see the introduction in ptscher and schneider ( 2009 ) for a partial list .",
    "except for knight and fu ( 2000 ) , all these papers derive the asymptotic distribution in a fixed - parameter framework .",
    "as pointed out in leeb and ptscher ( 2005 ) , such a fixed - parameter framework is often highly misleading in the context of variable selection procedures and penalized maximum likelihood estimators .",
    "for that reason , ptscher and leeb ( 2009 ) and ptscher and schneider ( 2009 ) have conducted a detailed study of the finite - sample as well as large - sample distribution of various penalized least - squares estimators , adopting a moving - parameter framework for the asymptotic results .",
    "[ related results for so - called post - model - selection estimators can be found in leeb and ptscher ( 2003 , 2005 ) and for model averaging estimators in ptscher ( 2006 ) ; see also sen ( 1979 ) and ptscher ( 1991 ) . ]",
    "the papers by ptscher and leeb ( 2009 ) and ptscher and schneider ( 2009 ) are set in the framework of an orthogonal linear regression model with a fixed number of parameters and with the error - variance being known .",
    "in the present paper we build on the just mentioned papers ptscher and leeb ( 2009 ) and ptscher and schneider ( 2009 ) .",
    "in contrast to these papers , we do not assume the number of regressors @xmath3 to be fixed , but let it depend on sample size  thus allowing for high - dimensional models .",
    "we also consider the case where the error - variance is unknown , which in case of a high - dimensional model creates non - trivial complications as then estimators for the error - variance will typically not be consistent .",
    "considering thresholding estimators from the outset in the present paper allows us also to cover non - orthogonal design .",
    "while the asymptotic distributional results in the known - variance case do not differ in substance from the results in ptscher and leeb ( 2009 ) and ptscher and schneider ( 2009 ) , not unexpectedly we observe different asymptotic behavior in the unknown - variance case if the number of degrees of freedom @xmath2 is constant , the difference resulting from the non - vanishing variability of the error - variance estimator in the limit .",
    "less expected is the result that  under consistent tuning  for the variable selection probabilities ( implied by all the estimators considered ) as well as for the distribution of the hard - thresholding estimator , estimation of the error - variance still has an effect asymptotically even if @xmath2 diverges , but does so only slowly .",
    "to give some idea of the theoretical results obtained in the paper we next present a rough summary of some of these results . for simplicity of exposition",
    "assume for the moment that the @xmath4 design matrix @xmath5 is such that the diagonal elements of @xmath6 are equal to @xmath7 , and that the error - variance @xmath8 is equal to @xmath7 .",
    "let @xmath9 denote the hard - thresholding estimator for the @xmath10-th component @xmath11 of the regression parameter , the threshold being given by @xmath12 , with @xmath13 denoting the usual error - variance estimator and with @xmath14 denoting a tuning parameter .",
    "an infeasible version of the estimator , denoted by @xmath15 , which uses @xmath16 instead of @xmath17 , is also considered ( known - variance case ) .",
    "we then show that the uniform rate of convergence of the hard - thresholding estimator is @xmath18 if the threshold satisfies @xmath19 and @xmath20 ( `` conservative tuning '' ) , but that the uniform rate is only @xmath14 if the threshold satisfies @xmath19 and @xmath21 ( `` consistent tuning '' ) . the same result also holds for the soft - thresholding estimator @xmath22 and the adaptive soft - thresholding estimator @xmath23 , as well as for infeasible variants of the estimators that use knowledge of @xmath16 ( known - variance case ) .",
    "furthermore , all possible limits of the centered and scaled distribution of the hard - thresholding estimator @xmath9 ( as well as of the soft- and the adaptive soft - thresholding estimators @xmath22 and @xmath23 ) under a moving parameter framework are obtained .",
    "consider first the case of conservative tuning : then all possible limiting forms of the distribution of @xmath24 as well as of @xmath25 for arbitrary parameter sequences @xmath26 are determined .",
    "it turns out that  in the known - variance case ",
    "these limits are of the same functional form as the finite - sample distribution , i.e. , they are a convex combination of a pointmass and an absolutely continuous distribution that is an excised version of a normal distribution . in the unknown - variance case ,",
    "when the number of degrees of freedom @xmath2 goes to infinity , exactly the same limits arise . however , if @xmath2 is constant , the limits are `` averaged '' versions of the limits in the known - variance case , the averaging being with respect to the distribution of the variance estimator @xmath13 . again these limits",
    "have the same functional form as the corresponding finite - sample distributions .",
    "consider next the case of consistent tuning : here the possible limits of @xmath27 as well as of @xmath28 have to be considered , as @xmath14 is the uniform convergence rate . in the known - variance case",
    "the limits are convex combinations of ( at most ) two pointmasses , the location of the pointmasses as well as the weights depending on @xmath29 and @xmath14 . in the unknown - variance case",
    "exactly the same limits arise if @xmath2 diverges to infinity sufficiently fast ; however , if @xmath2 is constant or diverges to infinity sufficiently slowly , the limits are again convex combinations of the same pointmasses , but with weights that are typically different .",
    "the picture for soft - thresholding and adaptive soft - thresholding is somewhat different : in the known - variance case , as well as in the unknown - variance case when @xmath2 diverges to infinity , the limits are ( single ) pointmasses .",
    "however , in the unknown - variance case and if @xmath2 is constant , the limit distribution can have an absolutely continuous component .",
    "it is furthermore useful to point out that in case of consistent tuning the sequence of distributions of @xmath24 is not stochastically bounded in general ( since @xmath14 is the uniform convergence rate ) , and the same is true for soft - thresholding @xmath22 and adaptive soft - thresholding @xmath23 .",
    "this throws a light on the fragility of the oracle - property , see section [ oracle ] for more discussion .",
    "while our theoretical results for the thresholding estimators immediately apply to lasso and adaptive lasso in case of orthogonal design , this is not so in the non - orthogonal case . in order to get some insight into the finite - sample distribution of the latter estimators also in the non - orthogonal case",
    ", we numerically compare the distribution of lasso and adaptive lasso with their thresholding counterparts in a simulation study .",
    "the main take - away messages of the paper can be summarized as follows :    * the finite - sample distributions of the various thresholding estimators considered are highly non - normal , the distributions being in each case a convex combination of pointmass and an absolutely continuous ( non - normal)component .",
    "* the non - normality persists asymptotically in a moving parameter framework . *",
    "results in the unknown - variance case are obtained from the corresponding results in the known - variance case by smoothing with respect to the distribution of @xmath17 . in line with this",
    ", one would expect the limiting behavior in the unknown - variance case to coincide with the limiting behavior in the known - variance whenever the degrees of freedom @xmath2 diverge to infinity .",
    "this indeed turns out to be so for some of the results , but not for others where we see that the speed of divergence of @xmath2 matters . * in case of conservative tuning the estimators have the expected uniform convergence rate , which is @xmath18 under the simplified assumptions of the above discussion , whereas under consistent tuning the uniform rate is slower , namely @xmath14 under the simplified assumptions of the above discussion .",
    "this is intimately connected with the fact that the so - called ` oracle property ' paints a misleading picture of the performance of the estimators . *",
    "the numerical study suggests that the results for the thresholding estimators @xmath22 and @xmath23 qualitatively apply also to the ( components of ) the lasso and the adaptive lasso as long as the design matrix is not too ill - conditioned .",
    "the paper is organized as follows .",
    "we introduce the model and define the estimators in section [ model ] .",
    "section [ variable ] treats the variable selection probabilities implied by the estimators .",
    "consistency , uniform consistency , and uniform convergence rates are discussed in section [ minimax ] .",
    "we derive the finite - sample distribution of each estimator in section [ fs ] and study the large - sample behavior of these in section ls .",
    "a numerical study of the finite - sample distribution of lasso and adaptive lasso can be found in section [ numstudy ] .",
    "all proofs are relegated to section [ prfs ] .",
    "consider the linear regression model    @xmath30    with @xmath31 an @xmath32 vector , @xmath5 a nonstochastic @xmath4 matrix of rank @xmath33 , and @xmath34 , @xmath35 .",
    "we allow @xmath3 , the number of columns of @xmath5 , as well as the entries of @xmath31 , @xmath5 , and @xmath36 to depend on sample size @xmath1 ( in fact , also the probability spaces supporting @xmath31 and @xmath36 may depend on @xmath1 ) , although we shall almost always suppress this dependence on @xmath1 in the notation .",
    "note that this framework allows for high - dimensional regression models , where the number of regressors @xmath3 is large compared to sample size @xmath1 , as well as for the more classical situation where @xmath3 is much smaller than @xmath1 .",
    "furthermore , let @xmath37 denote the nonnegative square root of @xmath38 , the @xmath10-th diagonal element of @xmath6 .",
    "now let@xmath39@xmath40denote the least - squares estimator for @xmath41 and the associated estimator for @xmath8 , the latter being defined only if @xmath42 .",
    "the hard - thresholding estimator @xmath43 is defined via its components as follows @xmath44where the tuning parameters @xmath14 are positive real numbers and @xmath45 denotes the @xmath10-th component of the least - squares estimator .",
    "we shall also need to consider its infeasible counterpart @xmath46 given by @xmath47the soft - thresholding estimator @xmath48 and its infeasible counterpart @xmath49  are given by@xmath50and @xmath51where @xmath52 .",
    "finally , the adaptive soft - thresholding estimator @xmath53 and its infeasible counterpart @xmath54 are defined via @xmath55and @xmath56    note that @xmath43 , @xmath48 , and @xmath53 as well as their infeasible counterparts are equivariant under scaling of the columns of @xmath57 by non - zero column - specific scale factors .",
    "we have chosen to let the thresholds @xmath58 ( @xmath59 , respectively ) depend explicitly on @xmath17 ( @xmath16 , respectively ) and @xmath37 in order to give @xmath60 an interpretation independent of the values of @xmath16 and @xmath5 .",
    "furthermore , often @xmath14 will be chosen independently of @xmath10 , i.e. , @xmath61 where @xmath62 is a positive real number . clearly , for the feasible versions we always need to assume @xmath42 , whereas for the infeasible versions @xmath63 suffices .",
    "we note the simple fact that@xmath64holds on the event that @xmath65 , and that@xmath66holds on the event that @xmath67 .",
    "analogous inequalities hold for the infeasible versions of the estimators .",
    "[ lasso]_(lasso ) _ ( i ) consider the objective function @xmath68where @xmath69 are positive real numbers .",
    "it is well - known that a unique minimizer @xmath70 of this objective function exists , the lasso - estimator .",
    "it is easy to see that in case @xmath71 is diagonal we have @xmath72hence , in the case of diagonal @xmath71 , the components @xmath73 of the lasso reduce to soft - thresholding estimators with appropriate thresholds ; in particular , @xmath73 coincides with @xmath22 for the choice @xmath74 .",
    "therefore all results derived below for soft - thresholding immediately give corresponding results for the lasso as well as for the dantzig - selector in the diagonal case .",
    "we shall abstain from spelling out further details .",
    "\\(ii ) sometimes @xmath69 in the definition of the lasso is chosen independently of @xmath10 ; more reasonable choices seem to be ( a ) @xmath75 ( where @xmath76 denotes the nonnegative square root of the @xmath10-th diagonal element of @xmath77 ) , and ( b ) @xmath78 where @xmath60 are positive real numbers ( not depending on the design matrix and often not on @xmath10 ) as then @xmath14 again has an interpretation independent of the values of @xmath16 and @xmath5 .",
    "note that in case ( a ) or ( b ) the solution of the optimization problem is equivariant under scaling of the columns of @xmath57 by non - zero column - specific scale factors .",
    "\\(iii ) similar results obviously hold for the infeasible versions of the estimators .",
    "[ alasso]_(adaptive lasso ) _ consider the objective function @xmath79where @xmath69 are positive real numbers .",
    "this is the objective function of the adaptive lasso ( where often @xmath80 is chosen independent of @xmath10 ) . again",
    "the minimizer @xmath81 exists and is unique ( at least on the event where @xmath82 for all @xmath10 ) .",
    "clearly , @xmath81 is equivariant under scaling of the columns of @xmath57 by non - zero column - specific scale factors provided @xmath69 does not depend on the design matrix .",
    "it is easy to see that in case @xmath71 is diagonal we have @xmath83hence , in the case of diagonal @xmath71 , the components @xmath84 of the adaptive lasso reduce to the adaptive soft - thresholding estimators @xmath23 ( for @xmath85 ) . therefore all results derived below for adaptive soft - thresholding immediately give corresponding results for the adaptive lasso in the diagonal case .",
    "we shall again abstain from spelling out further details .",
    "similar results obviously hold for the infeasible versions of the estimators .    _",
    "( other estimators ) _",
    "( i ) the adaptive lasso as defined in zou ( 2006 ) has an additional tuning parameter @xmath86 .",
    "we consider adaptive soft - thresholding only for the case @xmath87 , since otherwise the estimator is not equivariant in the sense described above .",
    "nonetheless an analysis for the case @xmath88 , similar to the analysis in this paper , is possible in principle .",
    "\\(ii ) an analysis of a scad - based thresholding estimator is given in ptscher and leeb ( 2009 ) in the known - variance case .",
    "[ these results are given in the orthogonal design case , but easily generalize to the non - orthogonal case .",
    "] the results obtained there for scad - based thresholding are similar in spirit to the results for the other thresholding estimators considered here .",
    "the unknown - variance case could also be analyzed in principle , but we refrain from doing so for the sake of brevity .",
    "\\(iii ) zhang ( 2010 ) introduced the so - called minimax concave penalty ( mcp)to be used for penalized least - squares estimation . apart from the usual tuning parameter",
    ", mcp also depends on a shape parameter @xmath86 .",
    "it turns out that the thresholding estimator based on mcp coincides with hard - thresholding in case @xmath89 , and thus is covered by the analysis of the present paper . in case @xmath90 , the mcp - based thresholding estimator could similarly be analyzed , especially since the functional form of the mcp - based thresholding estimator is relatively simple ( namely , a piecewise linear function of the least - squares estimator ) .",
    "we do not provide such an analysis for brevity .",
    "_ for all asymptotic considerations in this paper we shall always assume without further mentioning that _ @xmath91 _ _",
    "satisfies__@xmath92__for every fixed _",
    "_ @xmath93 _ _  satisfying",
    "_ _ @xmath94 _ _  for large enough _ _ @xmath1__. _ _ the case excluded by assumption ( [ xi ] ) seems to be rather uninteresting as unboundedness of @xmath95  means that the information contained in the regressors gets weaker with increasing sample size ( at least along a subsequence ) ; in particular , this implies ( coordinate - wise ) inconsistency of the least - squares estimator .",
    "[ in fact , if @xmath3 as well as the elements of @xmath5 do not depend on @xmath1 , this case is actually impossible as @xmath95 is then necessarily monotonically nonincreasing . ]",
    "the following notation will be used in the paper : let @xmath96 denote the extended real line @xmath97 endowed with the usual topology .",
    "on @xmath98 we shall consider the topology it inherits from @xmath96 .",
    "furthermore , @xmath99 and @xmath100 denote the cumulative distribution function ( cdf ) and the probability density function ( pdf ) of a standard normal distribution , respectively . by @xmath101",
    "we denote the cdf of a non - central @xmath102-distribution with @xmath103 degrees of freedom and non - centrality parameter @xmath104 . in the central case , i.e. , @xmath105 , we simply write @xmath106 .",
    "we use the convention @xmath107 , @xmath108 with a similar convention for @xmath101 .",
    "the estimators @xmath43 , @xmath48 , and @xmath53 can be viewed as performing variable selection in the sense that these estimators set components of @xmath41 exactly equal to zero with positive probability . in this section",
    "we study the variable selection probability @xmath109 , where @xmath110 stands for any of the estimators @xmath9 , @xmath22 , and @xmath23 . since these probabilities are the same for any of the three estimators considered we shall drop the subscripts @xmath111 , @xmath112 , and @xmath113 in this section .",
    "we use the same convention also for the variable selection probabilities of the infeasible versions .",
    "since @xmath114 it suffices to study the variable deletion probability@xmath115    as can be seen from the above formula , @xmath116 depends on @xmath41 only via @xmath11 .",
    "we first study the variable selection / deletion probabilities under a `` fixed - parameter '' asymptotic framework .",
    "[ select_prob_pointwise]let @xmath35 be given .",
    "for every @xmath93 satisfying @xmath117 for large enough @xmath1 we have :    \\(a ) a necessary and sufficient condition for @xmath118 as @xmath119 for all @xmath41 satisfying @xmath120 ( @xmath11 not depending on @xmath1 ) is @xmath121 .",
    "\\(b ) a necessary and sufficient condition for @xmath122 as @xmath119 for all @xmath41 satisfying @xmath123 is @xmath124 .",
    "\\(c ) a necessary and sufficient condition for @xmath125 as @xmath119 for all @xmath41 satisfying @xmath123 is @xmath126 , @xmath127 .",
    "the constant @xmath128 is then given by @xmath129 .",
    "part ( a ) of the above proposition gives a necessary and sufficient condition for the procedure to correctly detect nonzero coefficients with probability converging to @xmath7 .",
    "part ( b ) gives a necessary and sufficient condition for correctly detecting zero coefficients with probability converging to @xmath7 .",
    "[ uninteresting]if @xmath130 does not converge to zero , the conditions on @xmath14 in parts ( a ) and ( b ) are incompatible ; also the conditions in parts ( a ) and ( c ) are then incompatible ( except when @xmath131 ) .",
    "however , the case where @xmath130 does not converge to zero is of little interest as the least - squares estimator @xmath45 is then not consistent .    _",
    "( speed of convergence in proposition select_prob_pointwise ) _",
    "( i ) the speed of convergence in ( a ) is @xmath132 in case @xmath133 is bounded ( an uninteresting case as noted above ) ; if@xmath134 , the speed of convergence in ( a ) is not slower than @xmath135 for some suitable @xmath136 depending on @xmath137 .",
    "\\(ii ) the speed of convergence in ( b ) is @xmath138 . in ( c )",
    "the speed of convergence is given by the rate at which @xmath139 approaches @xmath140 .",
    "[ for the above results we have made use of lemma vii.1.2 in feller ( 1957 ) .",
    "]    for @xmath141 let @xmath142 . then ( i ) for every @xmath143 @xmath144suppose now that the entries of @xmath41 do not change with @xmath1 (",
    "although the dimension of @xmath41 may depend on @xmath1 ) .",
    "is made up of the initial @xmath145 elements of a fixed element of @xmath146 .",
    "] then , given that @xmath147 is bounded ( this being in particular the case if @xmath145 is bounded ) , the probability of incorrect non - detection of at least one nonzero coefficient converges to @xmath148 if and only if @xmath149 as @xmath119 for every @xmath143 .",
    "[ if @xmath147 is unbounded then this probability converges to @xmath148 , e.g. , if @xmath121 and @xmath150 as @xmath119 for every @xmath151 and @xmath152 and @xmath153 as @xmath119 for a suitable @xmath154 that is determined by @xmath155 . ]",
    "\\(ii ) for every @xmath156 we have@xmath157 .\\end{aligned}\\]]suppose again that the entries of @xmath41 do not change with @xmath1 . then , given that @xmath158 is bounded ( this being in particular the case if @xmath145 is bounded ) , the probability of incorrectly classifying at least one zero parameter as a non - zero one converges to @xmath148 as @xmath119 if and only if @xmath159 for every @xmath151 .",
    "[ if @xmath158 is unbounded then this probability converges to @xmath148 , e.g. , if @xmath160 as @xmath119 . ]",
    "\\(iii ) in case @xmath71 is diagonal , the relevant probabilities @xmath161 as well as @xmath162 can be directly expressed in terms of products of @xmath163 or @xmath164 , and proposition [ select_prob_pointwise ] can then be applied .    since the fixed - parameter asymptotic framework often gives a misleading impression of the actual behavior of a variable selection procedure ( cf .",
    "leeb and ptscher ( 2005 ) , ptscher and leeb ( 2009 ) ) we turn to a `` moving - parameter '' framework next , i.e. , we allow the elements of @xmath41 as well as @xmath16 to depend on sample size @xmath1 . in the proposition to follow ( and all subsequent large - sample results )",
    "we shall concentrate only on the case where @xmath121 as @xmath165 , since otherwise the estimators @xmath166 are not even consistent for @xmath11 as a consequence of proposition _ _  _ _ select_prob_pointwise , cf .",
    "also theorem [ thresh_consistency ] below . given the condition @xmath121",
    ", we shall then distinguish between the case @xmath167 , @xmath168 , and the case @xmath169 , which in light of proposition [ select_prob_pointwise ] we shall call the case of `` conservative tuning '' and the case of `` consistent tuning '' , respectively . to a ( finite or infinite ) limit , in the sense that this convergence can , for any given sequence @xmath139 , be achieved along suitable subsequences in light of compactness of the extended real line . ]",
    "[ select_prob_moving_par]suppose that for given @xmath93 satisfying @xmath117 for large enough @xmath1 we have @xmath170 and @xmath167 where @xmath171 .",
    "\\(a ) assume @xmath172 .",
    "suppose that the true parameters @xmath173 and @xmath174 satisfy @xmath175 .",
    "then @xmath176    \\(b ) assume @xmath177 .",
    "suppose that the true parameters @xmath178 and @xmath174 satisfy @xmath179 .",
    "then    \\1 .",
    "@xmath180 implies @xmath181 .",
    "@xmath182 implies @xmath183 .",
    "\\3 . @xmath184 and @xmath185 , for some @xmath186 ,",
    "imply @xmath187    in a fixed - parameter asymptotic analysis , which in proposition select_prob_moving_par corresponds to the case @xmath188 and @xmath189 , the limit of the probabilities @xmath116 is always @xmath148 in case @xmath120 , and is @xmath7 in case @xmath123 and consistent tuning ( it is @xmath190 in case @xmath123 and conservative tuning ) ; this does clearly not properly capture the finite - sample behavior of these probabilities .",
    "the moving - parameter asymptotic analysis underlying proposition select_prob_moving_par better captures the finite - sample behavior and , e.g. , allows for limits other than @xmath148 and @xmath7 even in the case of consistent tuning . in particular , proposition [ select_prob_moving_par ]",
    "shows that the convergence of the variable selection / deletion probabilities to their limits in a fixed - parameter asymptotic framework is not uniform in @xmath11 , and this non - uniformity is local in the sense that it occurs in an arbitrarily small neighborhood of @xmath123 ( holding the value of @xmath191 fixed ) . in a neighborhood of zero . ]",
    "furthermore , the above proposition entails that under consistent tuning deviations from @xmath123 of larger order than under conservative tuning go unnoticed asymptotically with probability 1 by the variable selection procedure corresponding to @xmath166 . for more discussion in a special case ( which in its essence also applies here ) see ptscher and leeb ( 2009 ) .    _",
    "( speed of convergence in proposition select_prob_moving_par ) _",
    "( i ) the speed of convergence in ( a ) is given by the slower of the rate at which @xmath139 approaches @xmath140 and @xmath192 approaches @xmath193 provided that @xmath194 ; if @xmath195 , the speed of convergence is not slower than @xmath196for any @xmath197 .",
    "\\(ii ) the speed of convergence in ( b1 ) is not slower than @xmath198 where @xmath154 depends on @xmath199 .",
    "the same is true in case ( b2 ) provided @xmath200 ; if @xmath201 , the speed of convergence is not slower than @xmath202 for every @xmath197 . in case ( b3 )",
    "the speed of convergence is not slower than the speed of convergence of@xmath203for any @xmath204 in case @xmath205 ; in case @xmath206 it is not slower than@xmath207for any @xmath204 .",
    "the preceding remark corrects and clarifies the remarks at the end of section 3 in ptscher and leeb ( 2009 ) and section 3.1 in ptscher and schneider ( 2009 ) .      in the unknown - variance case",
    "the finite - sample variable selection / deletion probabilities can be obtained as follows:@xmath208 \\rho _ { n - k}(s)ds   \\notag \\\\ & = t_{n - k , n^{1/2}\\theta _",
    "{ i}/(\\sigma \\xi _ { i , n})}\\left ( n^{1/2}\\eta _ { i , n}\\right ) -t_{n - k , n^{1/2}\\theta",
    "_ { i}/(\\sigma \\xi _ { i , n})}\\left ( -n^{1/2}\\eta _ { i , n}\\right ) .",
    "\\label{select_prob_unknown}\\end{aligned}\\]]here we have used ( [ select_prob ] ) , and independence of @xmath17 and @xmath45 allowed us to replace @xmath17 by @xmath209 in the relevant formulae , cf .",
    "leeb and ptscher ( 2003 , p.  110 ) .",
    "in the above @xmath210 denotes the density of @xmath211 times the square root of a chi - square distributed random variable with @xmath2 degrees of freedom .",
    "it will turn out to be convenient to set @xmath212 for @xmath213 , making @xmath210 a bounded continuous function on @xmath214 .",
    "we now have the following fixed - parameter asymptotic result for the variable selection / deletion probabilities in the unknown - variance case that perfectly parallels the corresponding result in the known - variance case , i.e. , proposition [ select_prob_pointwise ] :    [ select_prob_pointwise_unknown]let @xmath35 be given . for every @xmath93",
    "satisfying @xmath117 for large enough @xmath1 we have :    \\(a ) a necessary and sufficient condition for @xmath215 as @xmath119 for all @xmath41 satisfying @xmath120 ( @xmath11 not depending on @xmath1 ) is @xmath121 .",
    "\\(b ) a necessary and sufficient condition for @xmath216 as @xmath119 for all @xmath41 satisfying @xmath123 is @xmath159 .",
    "\\(c ) a necessary and sufficient condition for @xmath217 as @xmath119 for all @xmath41 satisfying @xmath123 and with @xmath218 satisfying @xmath219 is @xmath167 , @xmath127 .",
    "proposition [ select_prob_pointwise_unknown ] shows that the dichotomy regarding conservative tuning and consistent tuning is expressed by the same conditions in the unknown - variance case as in the known - variance case .",
    "furthermore , note that @xmath220 appearing in part ( c ) of the above proposition converges to @xmath221 in the case where @xmath222 , the limit thus being the same as in the known - variance case .",
    "this is different in case @xmath2 is constant equal to @xmath223 , say , eventually , the sequence @xmath220 then being constant equal to @xmath224 eventually .",
    "we finally note that remark [ uninteresting ]  also applies to proposition select_prob_pointwise_unknown above .",
    "for the same reasons as in the known - variance case we next investigate the asymptotic behavior of the variable selection / deletion probabilities under a moving - parameter asymptotic framework .",
    "we consider the case where @xmath2 is ( eventually ) constant and the case where @xmath222 .",
    "there is no essential loss in generality in considering these two cases only , since by compactness of @xmath225 we can always assume ( possibly after passing to subsequences ) that @xmath2 converges in @xmath225 .",
    "[ select_prob_moving_par_unknown]suppose that for given @xmath93 satisfying @xmath117 for large enough @xmath1 we have @xmath226 and @xmath167 where @xmath227 .",
    "\\(a ) assume @xmath172 .",
    "suppose that the true parameters @xmath173 and @xmath174 satisfy @xmath175 .",
    "( a1 ) if @xmath2 is eventually constant equal to @xmath223 , say , then@xmath228",
    "( a2 ) if @xmath222 holds , then@xmath229    \\(b ) assume @xmath177 .",
    "suppose that the true parameters @xmath178 and @xmath174 satisfy @xmath179 .",
    "( b1 ) if @xmath2 is eventually constant equal to @xmath223 , say , then @xmath230    ( b2 ) if @xmath222 holds , then    \\1 .",
    "@xmath180 implies @xmath231 .",
    "@xmath182 implies @xmath232 .",
    "@xmath184 and @xmath233 imply @xmath234provided @xmath235 for some @xmath236 .",
    "\\4 . @xmath184 and @xmath237 with @xmath238 imply @xmath239provided @xmath240 for some @xmath186 .",
    "[ note that the integral in the above display reduces to @xmath7 if @xmath241 , and to @xmath148 if @xmath242 . ]    \\5 . @xmath184 and @xmath243 imply @xmath244provided @xmath245 for some @xmath246 .",
    "theorem [ select_prob_moving_par_unknown ] shows , in particular , that also in the unknown - variance case the convergence of the variable selection / deletion probabilities to their limits in a fixed - parameter asymptotic framework is not locally uniform in @xmath11 . in the case of conservative tuning",
    "the theorem furthermore shows that the limit of the variable selection / deletion probabilities in the unknown - variance case is the same as in the known - variance case if the degrees of freedom @xmath2 go to infinity ( entailing that the distribution of @xmath247 concentrates more and more around @xmath7 ) ; if @xmath2 is eventually constant , the limit turns out to be a mixture of the known - variance case limits ( with @xmath16 replaced by @xmath209 ) , the mixture being with respect to the distribution of @xmath247 .",
    "[ we note that in the somewhat uninteresting case @xmath131 this mixture also reduces to the same limit as in the known - variance case . ]",
    "while this result is as one would expect , the situation is different and more subtle in the case of consistent tuning : if @xmath222 the limits are the same as in the known - variance case if @xmath180 or @xmath248 holds , namely @xmath7 and @xmath148 , respectively .",
    "however , in the `` boundary '' case @xmath184 the rate at which @xmath2 diverges to infinity becomes relevant .",
    "if the divergence is fast enough in the sense that @xmath249 , again the same limit as in the known - variance case , namely @xmath250 , is obtained ; but if @xmath2 diverges to infinity more slowly , a different limit arises ( which , e.g. , in case 4 of part ( b2 ) is obtained by averaging @xmath251 with respect to a suitable distribution ) .",
    "the case where the degrees of freedom @xmath2 is eventually constant looks very much different from the known - variance case and again some averaging with respect to the distribution of @xmath247 takes place .",
    "note that in this case the limiting variable deletion probabilities are @xmath7 and @xmath148 , respectively , only if @xmath252 and @xmath201 , respectively , which is in contrast to the known - variance case ( and the unknown - variance case with @xmath222 ) .",
    "[ costfree](i ) for later use we note that proposition select_prob_moving_par and theorem [ select_prob_moving_par_unknown ] also hold when applied to subsequences , as is easily seen .",
    "\\(ii ) the convergence conditions in proposition [ select_prob_moving_par ] on the various quantities involving @xmath29 and @xmath253 are essentially cost - free in the sense that given any sequence @xmath254 we can , due to compactness of @xmath96 , select from any subsequence @xmath255 a further subsubsequence @xmath256 such that along this subsubsequence all relevant quantities such as @xmath192 ( or @xmath257 and @xmath258 ) converge in @xmath96 . since proposition [ select_prob_moving_par ] also holds when applied to subsequences as just noted , an application of this proposition to the subsubsequence @xmath256 then results in a characterization of all possible accumulation points of the variable selection / deletion probabilities in the known - variance case .",
    "\\(iii ) in a similar manner , the convergence conditions in theorem select_prob_moving_par_unknown ( including the ones on @xmath2 ) are essentially cost - free , and thus this theorem provides a full characterization of all possible accumulation points of the variable selection / deletion probabilities in the unknown - variance case .",
    "as just discussed , in the case of conservative tuning we get the same limiting behavior under moving - parameter asymptotics in the known - variance and in the unknown - variance case along any sequence of parameters if @xmath222 or @xmath131 ( which in the conservatively tuned case can equivalently be stated as @xmath259 ) . in the case of",
    "consistent tuning the same coincidence of limits occurs if @xmath222 fast enough such that @xmath249 .",
    "this is not accidental but a consequence of the following fact :    [ closeness_prob]suppose that for given @xmath93 satisfying @xmath260 for large enough @xmath1 we have @xmath261 as @xmath119 .",
    "then @xmath262    [ weekend]suppose that @xmath121 holds as @xmath119 , the other case being of little interest as noted earlier . if @xmath263 does not converge to zero as @xmath119 , it can be shown from proposition select_prob_moving_par and theorem [ select_prob_moving_par_unknown ] that the limits of the variable deletion probabilities ( along appropriate ( sub)sequences @xmath264 ) for the known - variance and the unknown - variance case do not coincide .",
    "this shows that the condition @xmath265 in the above proposition can not be weakened ( at least in case @xmath121 holds ) .",
    "for purposes of comparison we start with the following obvious proposition , which immediately follows from the observation that @xmath45 is @xmath266-distributed .",
    "[ ls_consistency]for every @xmath93 satisfying @xmath117 for large enough @xmath1 we have the following :    \\(a ) @xmath267 is a necessary and sufficient condition for @xmath45 to be consistent for @xmath11 , the convergence rate being @xmath130 .",
    "\\(b ) suppose @xmath267 .",
    "then @xmath45 is uniformly consistent for @xmath11 in the sense that for every @xmath268@xmath269    in fact , @xmath45 is uniformly @xmath270-consistent for @xmath11 in the sense that for every @xmath268 there exists a real number @xmath271 such that @xmath272[note that the probabilities in the displays above in fact neither depend on @xmath41 nor @xmath16 . in particular ,",
    "the l.h.s .  of the above displays equal @xmath273 and @xmath274 ,",
    "respectively . ]",
    "the corresponding result for the estimators @xmath9 , @xmath22 , or @xmath23 and their infeasible counterparts @xmath15 , @xmath275 , or @xmath276 is now as follows .",
    "[ thresh_consistency]let @xmath110 stand for any of the estimators @xmath9 , @xmath22 , or @xmath23 .",
    "then for every @xmath93 satisfying @xmath117 for large enough @xmath1 we have the following :    \\(a ) @xmath110 is consistent for @xmath11 if and only if @xmath149 and @xmath267 .",
    "\\(b ) suppose @xmath121 and @xmath277 .",
    "then @xmath110 is uniformly consistent in the sense that for every @xmath268@xmath278furthermore , @xmath110 is uniformly @xmath279-consistent with @xmath280 in the sense that for every @xmath268 there exists a real number @xmath271 such that @xmath281    \\(c ) suppose @xmath121 and @xmath277 and @xmath282 . if for every @xmath283 there exists a real number @xmath271 such that @xmath284holds , then @xmath285 necessarily holds .",
    "\\(d ) let @xmath166 stand for any of the estimators @xmath15 , @xmath275 , or @xmath276 .",
    "then the results in ( a)-(c ) also hold for @xmath166 .",
    "the preceding theorem shows that the thresholding estimators @xmath9 , @xmath22 , and @xmath23 ( as well as their infeasible versions ) are uniformly @xmath279-consistent and that this rate is sharp and can not be improved",
    ". in particular , if the tuning is conservative these estimators are uniformly @xmath270-consistent , which is the usual rate one expects to find in a linear regression model as considered here . however ,",
    "if consistent tuning is employed , the preceding theorem shows that these thresholding estimators are then only uniformly @xmath286-consistent , i.e. , have a slower uniform convergence rate than the least - squares ( maximum likelihood ) estimator ( or the conservatively tuned thresholding estimators for that matter ) . for a discussion of the pointwise convergence rate see section [ oracle ] .",
    "[ asy - equiv]if @xmath287 , then @xmath110 is asymptotically equivalent to @xmath45 in the sense that for every @xmath268@xmath288a similar statement holds for @xmath166 . for @xmath110",
    "this follows immediately from ( [ closeness_h_s_as_ls_unknown ] ) in section [ prfs ] and the fact that the family of distributions corresponding to @xmath210 is tight ; for @xmath166 this follows from the relation @xmath289 .",
    "\\(i ) a variation of the proof of theorem [ thresh_consistency ] shows that in case of consistent tuning for the infeasible estimators additionally also@xmath290holds for every @xmath291 , and that for the feasible estimators@xmath292holds for every @xmath291 provided that @xmath222 .",
    "\\(ii ) inspection of the proof shows that the conclusion of theorem thresh_consistency(c ) continues to hold if the supremum over @xmath293 is replaced by the supremum over an arbitrarily small neighborhood of @xmath148 and @xmath16 is held fixed at an arbitrary positive value .",
    "\\(iii ) if @xmath294 and @xmath295 are replaced by @xmath296 and @xmath297 , respectively , in the displays in proposition [ ls_consistency ] and theorem [ thresh_consistency ] as well as in remark [ asy - equiv ] , the resulting statements remain true provided the suprema over @xmath298 are replaced by suprema over @xmath299 , where @xmath136 is an arbitrary real number .",
    "we next present the finite - sample distributions of the infeasible thresholding estimators .",
    "it will turn out to be convenient to give the results for scaled versions , where the scaling factor @xmath300 is a positive real number , but is otherwise arbitrary . _",
    "note that below we suppress the dependence of the distribution functions of the thresholding estimators on the scaling sequence _ @xmath300 _ _  in the notation .",
    "_ _ furthermore , observe that the finite - sample distributions depend on @xmath41 only through @xmath11 .",
    "[ 1]the cdf @xmath301 of @xmath302 is given by @xmath303or , equivalently , @xmath304where @xmath305 denotes pointmass at @xmath306 .    [ 2]the cdf @xmath307 of @xmath308 is given by@xmath309or , equivalently , @xmath310    [ 3]the cdf @xmath311 of @xmath312 is given by @xmath313where @xmath314 are defined by @xmath315or , equivalently , @xmath316where @xmath317    the finite - sample distributions of @xmath15 , @xmath275 , and @xmath276 are seen to be non - normal .",
    "they are made up of two components , one being a multiple of pointmass at @xmath318 and the other one being absolutely continuous with a density that is generally bimodal . for more discussion and some graphical illustrations in a special case see ptscher and leeb ( 2009 ) and ptscher and schneider ( 2009 ) .",
    "[ diag]in the case where @xmath71 is diagonal , the estimators of the components @xmath11 and @xmath319 for @xmath320 are independent and hence the above results immediately allow one to determine the finite - sample distributions of the entire vectors @xmath46 , @xmath49 , and @xmath54 .",
    "in particular , this provides the finite - sample distribution of the lasso @xmath321 and the adaptive lasso @xmath54 in the diagonal case ( cf .",
    "remarks lasso and [ alasso ] ) .",
    "the finite - sample distributions of @xmath9 , @xmath22 , @xmath23 are obtained next . the same remark on the scaling as in the previous section applies here .",
    "[ 4]the cdf @xmath322 of @xmath323 is given by @xmath324or , equivalently , @xmath325    [ 5]the cdf @xmath326 of @xmath327 is given by@xmath328or , equivalently,@xmath329    [ 6]the cdf @xmath330 of @xmath331 is given by @xmath332or , equivalently , @xmath333    as in the known - variance case the distributions are a convex combination of pointmass and an absolutely continuous part . in case of hard - thresholding , the averaging with respect to the density @xmath210 smoothes the indicator functions leading to a continuous density function for the absolutely continuous part ( while in the known - variance case the density function is only piece - wise continuous , cf .",
    "figure 1 in ptscher and leeb ( 2009 ) ) .",
    "this is not so for soft - thresholding and adaptive soft - thresholding , where the averaging with respect to the density @xmath334 does not affect the indicator functions involved ; here the shape of the distribution is qualitatively the same as in the known - variance case ( figure 2 in ptscher and leeb ( 2009 ) and figure 1 in ptscher and schneider ( 2009 ) ) .    in the case where @xmath71 is diagonal , the finite - sample distributions of the entire vectors @xmath43",
    ", @xmath335 , and @xmath53 can be found from the distributions of @xmath46 , @xmath49 , and @xmath54 ( see remark [ diag ] ) by conditioning on @xmath336 and integrating with respect to @xmath337 . in particular , this provides the finite - sample distributions of the lasso @xmath70 and the adaptive lasso @xmath53 in the diagonal case ( cf",
    ".  remarks [ lasso ] and alasso ) .",
    "we next derive the asymptotic distributions of the thresholding estimators under a moving - parameter ( and not only under a fixed - parameter ) framework since it is well - known that asymptotics based only on a fixed - parameter framework often lead to misleading conclusions regarding the performance of the estimators ( cf .",
    "also the discussion in section [ oracle ] ) .",
    "we first consider the infeasible versions of the thresholding estimators .",
    "[ lsdk_h]suppose that for given @xmath93 satisfying @xmath117 for large enough @xmath1 we have @xmath121 and @xmath167 where @xmath171 .",
    "\\(a ) assume @xmath172",
    ". set the scaling factor @xmath338 .",
    "suppose that the true parameters @xmath173 and @xmath174 satisfy @xmath175 .",
    "then @xmath339 converges weakly to the distribution with cdf@xmath340the corresponding measure being@xmath341[this distribution reduces to a standard normal distribution in case @xmath342 or @xmath131 . ]",
    "\\(b ) assume @xmath177",
    ". set the scaling factor @xmath343 .",
    "suppose that the true parameters @xmath178 and @xmath174 satisfy @xmath179 .",
    "\\1 . if @xmath180 , then @xmath339 converges weakly to @xmath344 .",
    "\\2 . if @xmath182 , then @xmath339 converges weakly to @xmath345 .",
    "if @xmath184 and @xmath346 , for some @xmath186 , then @xmath339 converges weakly to@xmath347    [ lsdk_s]suppose that for given @xmath93 satisfying @xmath117 for large enough @xmath1 we have @xmath121 and @xmath167 where @xmath171 .",
    "\\(a ) assume @xmath172",
    ". set the scaling factor @xmath338 .",
    "suppose that the true parameters @xmath173 and @xmath174 satisfy @xmath175 .",
    "then @xmath348 converges weakly to the distribution with cdf@xmath349the corresponding measure being@xmath350[this distribution reduces to a @xmath351-distribution in case @xmath342 or @xmath131 . ]",
    "\\(b ) assume @xmath177",
    ". set the scaling factor @xmath343 .",
    "suppose that the true parameters @xmath178 and @xmath174 satisfy @xmath179 .",
    "then @xmath352 converges weakly to @xmath353 .",
    "[ lsdk_as]suppose that for given @xmath93 satisfying @xmath117 for large enough @xmath1 we have @xmath121 and @xmath167 where @xmath171 .",
    "\\(a ) assume @xmath172",
    ". set the scaling factor @xmath338 .",
    "suppose that the true parameters @xmath173 and @xmath174 satisfy @xmath175 .",
    "then @xmath354 converges weakly to the distribution with cdf@xmath355 in case @xmath194 , the corresponding measure being@xmath356where @xmath357 . in case @xmath342 ,",
    "the cdf @xmath358 converges weakly to @xmath359 , i.e. , to a standard normal distribution .",
    "[ in case @xmath131 the limit always reduces to a standard normal distribution . ]",
    "\\(b ) assume @xmath177 .",
    "set the scaling factor @xmath343 .",
    "suppose that the true parameters @xmath178 and @xmath174 satisfy @xmath179 .",
    "\\1 . if @xmath180 , then @xmath354 converges weakly to @xmath344 .",
    "\\2 . if @xmath360 , then @xmath358 converges weakly to @xmath361 .",
    "\\3 . if @xmath362 , then @xmath358 converges weakly to @xmath345 .",
    "observe that the scaling factors @xmath300 used in the above propositions are exactly of the same order as @xmath279 in the case of conservative as well as in the case of consistent tuning and thus correspond to the uniform rate of convergence in both cases . in the case of conservative tuning",
    "the limiting distributions have essentially the same form as the finite - sample distributions , demonstrating that the moving - parameter asymptotic framework captures the finite - sample behavior of the estimators in a satisfactory way .",
    "in contrast , a fixed - parameter asymptotic framework , which corresponds to setting @xmath363 and @xmath189 in the above propositions , misrepresents the finite - sample properties of the thresholding estimators whenever @xmath120 but small , as the fixed - parameter limiting distribution is  in case of hard - thresholding and adaptive soft - thresholding  then always @xmath364 , regardless of the size of @xmath365 . for soft - thresholding",
    "we also observe a strong discrepancy between the finite - sample distribution and the fixed - parameter limit for @xmath366 which is given by @xmath367 .",
    "in particular , the above propositions demonstrate non - uniformity in the convergence of finite - sample distributions to their limit in a fixed - parameter framework .    in the case of",
    "consistent tuning we observe an interesting phenomenon , namely that the limiting distributions now correspond to pointmasses ( but not always located at zero ! ) , or are convex combinations of two pointmasses in some cases when considering the hard - thresholding estimator .",
    "this essentially means that consistently tuned thresholding estimators are plagued by a bias - problem in that the `` bias - component '' is the dominant component and is of larger order than the `` stochastic variability '' of the estimator .",
    ", where we can achieve a limiting probability for @xmath368 that is strictly between @xmath148 and @xmath7 .",
    "that this randomness does not survive for the other two estimators in the limit seems to be connected to the fact that these estimators are continuous functions of the data , whereas @xmath15 is not . ] in a fixed - parameter framework we get the trivial limits @xmath345 for every value of @xmath11 in case of hard - thresholding and adaptive soft - thresholding . at first glance",
    "this seems to suggest that we have used a scaling sequence that does not increase fast enough with @xmath1 , but recall that the scaling used here corresponds to the uniform convergence rate .",
    "we shall take this issue further up in section [ oracle ] .",
    "the situation is different for the soft - thresholding estimator where the fixed - parameter limit is @xmath369 , which reduces to @xmath370 only for @xmath123 ; this is a reflection of the well - known fact that soft - thresholding is plagued by bias problems to a higher degree than are hard - thresholding and adaptive soft - thresholding .",
    "we next show that the finite - sample cdfs of @xmath9 , @xmath22 , and @xmath23 and of their infeasible counterparts @xmath15 , @xmath275 , and @xmath276 , respectively , are uniformly ( with respect to the parameters ) close in the total variation distance ( or the supremum norm ) provided the number of degrees of freedom @xmath2 diverges to infinity fast enough .",
    "apart from being of interest in their own right , these results will be instrumental in the subsequent section . we note that the results in theorem [ closeness ] below hold for any choice of the scaling factors @xmath300 .    [ closeness ] suppose that for given @xmath93 satisfying @xmath117 for large enough @xmath1 we have @xmath265 as @xmath119 .",
    "then@xmath371@xmath372and @xmath373hold .",
    "[ n - k]in case of conservative tuning , the condition @xmath265 is always satisfied if @xmath222 .",
    "[ in fact it is then equivalent to @xmath374 or @xmath131 . ] in case of consistent tuning @xmath222 is clearly a weaker condition than @xmath261 .",
    "however , in general , a sufficient condition for @xmath265 is that @xmath375 and @xmath376 .",
    "[ scaleinv]suppose that @xmath121 holds as @xmath119 .",
    "if @xmath263 does not converge to zero as @xmath119 , remark [ weekend ] shows that none of the convergence results in theorem [ closeness ] holds .",
    "[ to see this note that the variable deletion probabilities constitute the weight of the pointmass in the respective distribution functions .",
    "] this shows that the condition @xmath265 in the above theorem can not be weakened ( at least in case @xmath226 holds ) .",
    "we next obtain the limiting distributions of @xmath9 , @xmath22 , and @xmath23 in a moving - parameter framework under conservative tuning .",
    "[ htconservative](hard - thresholding with conservative tuning ) suppose that for given @xmath93 satisfying @xmath117 for large enough @xmath1 we have @xmath121 and @xmath126 where @xmath127 . set the scaling factor @xmath338 .",
    "suppose that the true parameters @xmath173 and @xmath174 satisfy @xmath175 .",
    "\\(a ) if @xmath2 is eventually constant equal to @xmath223 , say , then @xmath377 converges weakly to the distribution with cdf@xmath378the corresponding measure being@xmath379[the distribution reduces to a standard normal distribution in case @xmath342 or @xmath131 . ]",
    "\\(b ) if @xmath222 holds , then @xmath380 converges weakly to the distribution given in proposition [ lsdk_h](a ) .",
    "[ stconservative](soft - thresholding with conservative tuning ) suppose that for given @xmath93 satisfying @xmath117 for large enough @xmath1 we have @xmath121 and @xmath126 where @xmath127",
    ". set the scaling factor @xmath338 .",
    "suppose that the true parameters @xmath173 and @xmath174 satisfy @xmath175 .",
    "\\(a ) if @xmath2 is eventually constant equal to @xmath223 , say , then @xmath381 converges weakly to the distribution with cdf@xmath382the corresponding measure being@xmath383[the atomic part in the above expression is absent in case @xmath195 .",
    "furthermore , the distribution reduces to a standard normal distribution if @xmath131 . ]",
    "\\(b ) if @xmath222 holds , then @xmath384 converges weakly to the distribution given in proposition [ lsdk_s](a ) .",
    "[ astconservative](adaptive soft - thresholding with conservative tuning ) suppose that for given @xmath93 satisfying @xmath117 for large enough @xmath1 we have @xmath121 and @xmath385 where @xmath127 . set the scaling factor @xmath386 .",
    "suppose that the true parameters @xmath173 and @xmath174 satisfy @xmath175 .",
    "\\(a ) suppose @xmath2 is eventually constant equal to @xmath223 , say .",
    "then @xmath387 converges weakly to the distribution with cdf@xmath388 in case @xmath194 , the corresponding measure being given by@xmath389where @xmath390 . in case @xmath195 , the cdf @xmath391 converges weakly to @xmath99 , i.e. , a standard normal distribution .",
    "[ if @xmath131 , the limit always reduces to a standard normal distribution . ]",
    "\\(b ) if @xmath222 , then @xmath391 converges weakly to the distribution given in proposition [ lsdk_as](a ) .",
    "it transpires that in case of conservative tuning and @xmath392 we obtain exactly the same limiting distributions as in the known - variance case and hence the relevant discussion given at the end of section lsdkvc applies also here .",
    "[ that one obtains the same limits does not come as a surprise given the results in section [ uniform_close ] and the observation made in remark [ n - k ] . ] in the case , where @xmath2 is eventually constant , the limits are obtained from the limits in the known - variance case ( with @xmath16 replaced by @xmath393 ) by averaging with respect to the distribution of @xmath247 . again",
    "the limiting distributions essentially have the same structure as the corresponding finite - sample distributions .",
    "the fixed - parameter limiting distributions ( corresponding to setting @xmath394 and @xmath189 in the above theorems ) again misrepresent the finite - sample properties of the thresholding estimators whenever @xmath366 but small , as the fixed - parameter limiting distribution is  in case of hard - thresholding and adaptive soft - thresholding  then always @xmath364 , regardless of the size of @xmath11 .",
    "for soft - thresholding we also observe a strong discrepancy between the finite - sample distribution and the fixed - parameter limit especially for @xmath120 but small , which is given by the distribution with pdf @xmath395 regardless of the size of @xmath11 . as a consequence",
    ", we again observe non - uniformity in the convergence of finite - sample distributions to their limit in a fixed - parameter framework also in the case where the number of degrees of freedom is ( eventually ) constant .",
    "we next derive the limiting distributions of @xmath9 , @xmath22 , and @xmath23 in a moving - parameter framework under consistent tuning .",
    "[ htconsistent](hard - thresholding with consistent tuning ) suppose that for given @xmath93 satisfying @xmath117 for large enough @xmath1 we have @xmath121 and @xmath159 . set the scaling factor @xmath396 .",
    "suppose that the true parameters @xmath178 and @xmath174 satisfy @xmath179 .",
    "\\(a ) if @xmath2 is eventually constant equal to @xmath223 , say , then @xmath377 converges weakly to@xmath397[the above display reduces to @xmath345 for @xmath398 . ]",
    "\\(b ) if @xmath222 holds , then    \\1 .",
    "@xmath180 implies that @xmath377 converges weakly to @xmath344 .",
    "@xmath182 implies that @xmath377 converges weakly to @xmath345 .",
    "\\3 . @xmath184 and @xmath233",
    "imply that @xmath377 converges weakly to@xmath399provided @xmath400 for some @xmath236 .",
    "\\4 . @xmath184 and @xmath237 with @xmath238 imply that @xmath401 converges weakly to@xmath402provided @xmath240 for some @xmath186 .",
    "[ note that the above display reduces to @xmath344 if @xmath403 , and to @xmath345 if @xmath242 . ]    \\5 . @xmath184 and @xmath243 imply that @xmath401 converges weakly to@xmath404provided @xmath245 for some @xmath246 .",
    "[ stconsistent](soft - thresholding with consistent tuning ) suppose that for given @xmath93 satisfying @xmath117 for large enough @xmath1 we have @xmath121 and @xmath159 . set the scaling factor @xmath396 .",
    "suppose that the true parameters @xmath178 and @xmath174 satisfy @xmath179 .",
    "\\(a ) if @xmath2 is eventually constant equal to @xmath223 , say , then @xmath381 converges weakly to the distribution given by@xmath405where we recall the convention that @xmath406 for @xmath407 .",
    "[ in case @xmath362 , the atomic part in ( soft_large_sample_unknown_density_c ) is absent and ( soft_large_sample_unknown_density_c ) reduces to @xmath408 . ]",
    "\\(b ) if @xmath222 holds , then @xmath384 converges weakly to @xmath409 .",
    "[ astconsistent](adaptive soft - thresholding with consistent tuning ) suppose that for given @xmath93 satisfying @xmath117 for large enough @xmath1 we have @xmath121 and @xmath21 . set the scaling factor @xmath343 .",
    "suppose that the true parameters @xmath178 and @xmath174 satisfy @xmath179 .",
    "\\(a ) suppose @xmath2 is eventually constant equal to @xmath223 , say .",
    "then @xmath387 converges weakly to the distribution with cdf@xmath410 in case @xmath411 , and to the distribution with cdf@xmath412 in case @xmath413 .",
    "furthermore , @xmath391 converges weakly to @xmath345 if @xmath398 .",
    "[ in case @xmath414 , the distribution has a jump of height @xmath415 at @xmath416 and is otherwise absolutely continuous .",
    "in particular , it reduces to @xmath345 in case @xmath252 . ]",
    "\\(b ) if @xmath222 holds , then    \\1 .",
    "@xmath417 implies that @xmath387 converges weakly to @xmath418 ,    \\2 .",
    "@xmath419 implies that @xmath387 converges weakly to @xmath361 ,    \\3 .",
    "@xmath362 implies that @xmath387 converges weakly to @xmath370 .    we know from theorem [ closeness ] that we obtain the same limiting distributions for @xmath9 , @xmath22 , and @xmath23 as for @xmath15 , @xmath275 , and @xmath276 , respectively , provided @xmath2 diverges to infinity sufficiently fast in the sense that @xmath261 .",
    "the theorems in this section now show that for the soft - thresholding as well as for the adaptive soft - thresholding estimator we actually get the same limiting distribution as in the unknown - variance case whenever @xmath2 diverges even if @xmath261 is violated",
    ". however , for the hard - thresholding estimator the picture is different , and in case @xmath2 diverges but @xmath265 is violated , limit distributions different from the known - variance case arise ( these limiting distributions still being convex combinations of two pointmasses , but with weights different from the known - variance case ) .",
    "it seems that this is a reflection of the fact that the hard - thresholding estimator is a discontinuous function of the data , whereas the other two estimators considered depend continuously on the data .",
    "the fixed - parameter limiting distributions for all three estimators are again the same as in the known - variance case .    in the case where the degrees of freedom @xmath2 are eventually constant",
    ", the limiting distribution of the hard - thresholding estimator is again a convex combination of two pointmasses , with weights that are in general different from the known - variance case .",
    "however , for the soft - thresholding as well as for the adaptive soft - thresholding estimator the limiting distributions can also contain an absolutely continuous component .",
    "this component seems to stem from an interaction of the more pronounced `` bias - component '' ( as compared to hard - thresholding ) with the nonvanishing randomness in the estimated variance .",
    "the fixed - parameter limiting distributions for hard - thresholding and adaptive soft - thresholding are again given by @xmath370 for all values of @xmath11 as in the known - variance case , whereas for soft - thresholding the fixed - parameter limiting distribution is @xmath370 only for @xmath123 and otherwise has a pdf given by @xmath420 ( as compared to a limit of @xmath369 in the known - variance case ) .",
    "as already mentioned at the end of sections [ lsdkvc ] and [ consistent ] , under consistent tuning the _ fixed - parameter _ limiting distributions of the hard - thresholding and of the adaptive soft - thresholding estimator  in the known - variance as well as in the unknown - variance case  always degenerate to pointmass at zero .",
    "recall that in these results the estimators ( after centering at @xmath11 ) are scaled by @xmath421 , which corresponds to the uniform convergence rate .",
    "we next show that if the estimators are scaled by @xmath422 instead , a limit distribution under _ fixed - parameter _ asymptotics arises that is not degenerate in general ( under an additional condition on the tuning parameter in case of adaptive soft - thresholding ) .",
    "in fact , we show that the hard - thresholding as well as the adaptive soft - thresholding estimators then satisfy what has been called the `` oracle - property '' .",
    "however , it should be kept in mind that  with this faster scaling sequence @xmath423  the centered estimators are no longer stochastically bounded in a moving - parameter framework ( for certain sequences of parameters ) , cf .",
    "theorem thresh_consistency .",
    "this shows the fragility of the `` oracle - property '' , which is a fixed - parameter concept , and calls into question the statistical significance of this notion .",
    "for a more extensive discussion of the `` oracle - property '' and its consequences see leeb and ptscher ( 2008 ) , ptscher and leeb ( 2009 ) , and ptscher and schneider ( 2009 ) .",
    "[ oracle_1]let @xmath35 be given .",
    "suppose that for given @xmath93 satisfying @xmath117 for large enough @xmath1 we have @xmath149 and @xmath169 .",
    "\\(a ) @xmath424 as well as @xmath425 converge in distribution to @xmath364 when @xmath120 , and to @xmath426 when @xmath123 .",
    "\\(b ) @xmath427 as well as @xmath428 converge in distribution to @xmath364 when @xmath120 , and to @xmath426 when @xmath123 , provided the tuning parameter additionally satisfies @xmath429 for @xmath119 .",
    "inspection of the proof of part ( b ) given in section prfs_ls shows that the condition @xmath430 is used for the result only in case @xmath120 .",
    "if now @xmath431 with @xmath432 , inspection of the proof shows that then in case @xmath366 we have that @xmath433 , where @xmath434 is standard normal and is independent of @xmath247 .",
    "hence , we see that the distribution of @xmath435 asymptotically behaves like the convolution of an @xmath364-distribution and the distribution of @xmath436 times a chi - square distributed random variable with @xmath2 degrees of freedom ( if @xmath222 this reduces to an @xmath437-distribution ) . if @xmath438 , then @xmath439 is stochastically unbounded .",
    "note that this shows that the consistently tuned adaptive soft - thresholding estimator  even in a fixed - parameter setting  has a convergence rate slower than @xmath133 if @xmath120 and if the tuning parameter is `` too large '' in the sense that @xmath438 .",
    "the same conclusion applies to the infeasible estimator @xmath276 ( with the simplification that one always obtains an @xmath440-distribution in case @xmath441 with @xmath432 ) .",
    "we further illustrate the fragility of the fixed - parameter asymptotic results under a @xmath423-scaling obtained above by providing the moving - parameter limits under this scaling .",
    "let @xmath442 denote the cdf of @xmath443 , and define @xmath444 and @xmath445 analogously .",
    "the proofs of the subsequent propositions are completely analogous to the proofs of theorem 9 in ptscher and leeb ( 2009 ) and theorem 5 in ptscher and schneider ( 2009 ) , respectively .",
    "[ oracle_h](hard - thresholding ) suppose that for given @xmath93 satisfying @xmath117 for large enough @xmath1 we have @xmath226 and @xmath169 .",
    "suppose that the true parameters @xmath446 and @xmath174 satisfy @xmath447 and @xmath448 .",
    "[ note that in case @xmath449 the convergence of @xmath192 already follows from that of @xmath450 , and @xmath193 is then given by @xmath451 . ]",
    ". then @xmath452converges weakly to @xmath453 if @xmath194 ; if @xmath454 the total mass of @xmath455 escapes to @xmath456 , in the sense that @xmath457 for every @xmath458 if @xmath459 , and that @xmath460 for every @xmath458 if @xmath461 .",
    "suppose @xmath182 . then @xmath452converges weakly to @xmath99 .",
    "suppose @xmath184 and @xmath346 for some @xmath186 .",
    "then  @xmath462 converges to@xmath463for every @xmath458 .",
    "[ in case @xmath242 the limit reduces to a standard normal distribution . ]",
    "[ oracle_as](adaptive soft - thresholding ) suppose that for given @xmath464 satisfying @xmath117 for large enough @xmath1 we have @xmath226 and @xmath169 .",
    "suppose that the true parameters @xmath446 and @xmath174 satisfy @xmath465 .",
    "\\1 . if @xmath252 and @xmath466 , then @xmath467converges weakly to @xmath453 .    \\2 .",
    "the total mass of @xmath468 escapes to @xmath469 or @xmath470 in the following cases : if @xmath413 , or if @xmath252 and @xmath471 , or if @xmath472 and @xmath473 , then @xmath474 for every @xmath475 .",
    "if @xmath476 , or if @xmath252 and @xmath477 , or if @xmath478 and @xmath479 , then @xmath480 for every @xmath458 .",
    "\\3 . if @xmath362 and @xmath481 , then @xmath468converges weakly to @xmath482 .",
    "it is easy to see that setting @xmath394 and @xmath483 in proposition [ oracle_h ] immediately recovers the `` oracle - property '' for @xmath15 .",
    "similarly , we recover the `` oracle property '' for @xmath276 from proposition [ oracle_as ] provided @xmath484 .",
    "the propositions also characterize the sequences of parameters along which the mass of the distributions of the hard - thresholding and the adaptive soft - thresholding estimator escapes to infinity ; loosely speaking these are sequences along which the bias of the estimators exceeds all bounds .",
    "the theorems in section [ uniform_close ] also show that the last two propositions above carry over immediately to the unknown - variance case whenever @xmath222 sufficiently fast such that @xmath261 holds . to save space",
    ", we do not extend these two propositions to the case where the latter condition fails to hold .",
    "the situation is somewhat different for the soft - thresholding estimator .",
    "it follows from theorem [ stconsistent ] that the distribution of @xmath485 does not degenerate to pointmass at zero ( in fact , has no mass at zero ) if @xmath120 and is held fixed .",
    "consequently , @xmath486 is also the fixed - parameter convergence rate of @xmath22 , in the sense that scaling with a faster rate ( e.g. , @xmath133 ) leads to the escape of the total mass of the finite - sample distribution of the so - scaled ( and centered ) estimator to @xmath487 .",
    "for @xmath123 we get with the same argument as for hard - thresholding that @xmath488 converges to @xmath345 . for the infeasible version @xmath275",
    "the situation is identical .",
    "we conclude by a result analogous to propositions oracle_h and [ oracle_as ] .",
    "the proof of this result is completely analogous to the proof of theorem 10 in ptscher and leeb ( 2009 ) .",
    "( soft - thresholding )  suppose that for given @xmath93 satisfying @xmath260 for large enough @xmath1 we have @xmath121 and @xmath169 .",
    "suppose that the true parameters @xmath489 and @xmath174 satisfy @xmath490 .",
    "then @xmath491converges weakly to @xmath492 if @xmath194 ; and if @xmath342 , the total mass of @xmath493 escapes to @xmath456 , in the sense that @xmath494 for every @xmath475 if @xmath459 , and that @xmath495 for every @xmath458 if @xmath461 .",
    "again , this proposition immediately extends to the unknown - variance case whenever @xmath222 sufficiently fast such that @xmath261 holds .",
    "we abstain from extending the result to the case where the latter condition fails to hold .",
    "[ costfree2](i ) the convergence conditions on the various quantities involving @xmath29 and @xmath253 ( and on @xmath2 ) in the propositions in sections [ lsdkvc ] and [ oracle ] as well as in the theorems in section [ lsdukvc ] are essentially cost - free for the same reason as explained in remark [ costfree ] .",
    "\\(ii ) we note that all possible forms of the moving - parameter limiting distributions in the results in this section already arise for sequences @xmath29 belonging to an arbitrarily small neighborhood of zero ( and with @xmath191 fixed ) .",
    "consequently , the non - uniformity in the convergence to the fixed - parameter limits is of a local nature .",
    "ptscher and leeb ( 2009 ) and ptscher and schneider ( 2009 ) present impossibility results for estimating the finite - sample distribution of the thresholding estimators considered in these papers .",
    "in the present context , corresponding impossibility results could be derived under appropriate assumptions .",
    "we abstain from presenting such results .",
    "as has been discussed in remarks [ lasso ] and [ alasso ] in section model , the soft - thresholding estimator coincides with the lasso , and the adaptive soft - thresholding estimator coincides with the adaptive lasso in case of orthogonal design .",
    "a natural question now is if the distributional results for the ( adaptive ) soft - thresholding estimator derived in this paper are in any way indicative for the distribution of the ( adaptive ) lasso in case of non - orthogonal design . in order to gain some insight into this we provide a simulation study to compare the finite - sample distributions of the respective estimators .",
    "we simulate the lasso estimator as defined in remark [ lasso ] ( with @xmath496 and @xmath61 not depending on @xmath10 ) and the adaptive lasso estimator as defined in remark alasso ( with @xmath497 not depending on @xmath10 ) and show histograms of @xmath498 where @xmath499 stands for the @xmath10-th component of lasso or adaptive lasso .",
    "[ the scaling used here is chosen on the basis that with this scaling the @xmath10-th component of the least - squares estimator is standard normally distributed . ]",
    "we set @xmath500 and @xmath501 , resulting in @xmath502 degrees of freedom .",
    "two different types of designs are considered : for design i we use @xmath503 with @xmath504 .",
    "more concretely , @xmath5 is partitioned into @xmath505 blocks of size @xmath506 and each of these blocks is set equal to @xmath507 with @xmath508 , the cholesky factorization of @xmath509 .",
    "the value of @xmath510 is set equal to @xmath511 , @xmath512 , and @xmath513 , implying condition numbers for @xmath71 of @xmath514 , @xmath515 , and @xmath516 , respectively .",
    "design ii is an `` equicorrelated '' design . here",
    "we set the matrix comprised of the first @xmath3 rows of @xmath5 equal to @xmath517 , where @xmath518 is the @xmath506 matrix with all components equal to @xmath7 and @xmath154 is a real number greater than @xmath519 . the remaining entries of @xmath5 are all set equal to @xmath148 .",
    "we choose three values for @xmath154 : first , @xmath520 which implies a correlation of @xmath521 between any two regressors and a condition number of @xmath522 for @xmath71 ; second , @xmath523 which implies a correlation of @xmath524 and a condition number of @xmath525 ; and @xmath526 which implies a correlation of @xmath527 and a condition number of @xmath528 . for either type of design",
    "we proceed as follows : for the given parameters @xmath529 and @xmath530 , we simulate @xmath531 data vectors @xmath31 and compute the corresponding estimator , i.e. , the lasso and adaptive lasso as specified above .",
    "we set @xmath532 , implying that the thresholding estimators delete a given irrelevant variable with probability @xmath533 .    for the non - zero outcomes of the estimators , we plot the histogram of @xmath534 which is normalized such that its mass corresponds to the proportion of the non - zero values .",
    "the zero values are accounted for by plotting `` pointmass '' with height representing the proportion of zero values , i.e. , the simulated variable selection probability . for the purpose of comparison the graph of the distribution of the corresponding ( centered and scaled ) thresholding estimator ( using the same @xmath61 ) as derived analytically in section [ fs ] is then superimposed in red color .",
    "the results of the simulation study are presented in figures 1 - 12 below .    in comparing the adaptive lasso with the adaptive soft - thresholding estimator",
    ", we find remarkable agreement between the respective marginal distributions in all cases where the design matrix is not too multicollinear , see figures 1 , 2 , and 4 .",
    "for the cases where the design matrix is no longer well - conditioned a difference between the respective marginal distributions emerges but seems to be surprisingly moderate , see figures 3 , 5 , and 6 .    turning to the lasso and its thresholding counterpart",
    ", we find a similar situation with a somewhat stronger disagreement between the respective marginal distributions . again in the cases where the design matrix is well - conditioned ( figures 7 , 8 , and 10 ) the difference is less pronounced than in the case of an ill - conditioned design matrix ( figures 9 , 11 , and 12 ) .",
    "we have also experimented with other values of @xmath1 , @xmath3 , @xmath41 , @xmath535 , @xmath154 , and @xmath62 and have found the results to be qualitatively the same for these choices .    ,",
    "scaledwidth=95.0% ]    , scaledwidth=95.0% ]    , scaledwidth=95.0% ]    , scaledwidth=95.0% ]    , scaledwidth=95.0% ]    , scaledwidth=95.0% ]    , scaledwidth=95.0% ]    , scaledwidth=95.0% ]    , scaledwidth=95.0% ]    , scaledwidth=95.0% ]    , scaledwidth=95.0% ]    , scaledwidth=95.0% ]",
    "* proof of proposition [ select_prob_pointwise ] : * we first prove part ( a ) .",
    "rewrite @xmath116 as @xmath536assume first that @xmath121 and fix @xmath366 . by a standard subsequence argument",
    "we may assume without loss of generality that @xmath133 converges to a constant @xmath537 which by our maintained assumption ( [ xi ] ) must satisfy @xmath538 .",
    "now @xmath539 both converge to @xmath540 , which is non - zero , and consequently both arguments in ( [ fi ] ) converge to @xmath541 .",
    "since @xmath99 is continuous on @xmath96 , the expression ( [ fi ] ) converges to zero . to prove the converse , now assume that ( [ fi ] ) converges to zero for all @xmath120 . by a standard subsequence argument",
    ", we may assume without loss of generality that @xmath542 converges to a constant @xmath543 satisfying @xmath544 .",
    "suppose @xmath545 holds .",
    "choose @xmath11 such that @xmath546 holds .",
    "it follows that @xmath547 and @xmath548 eventually have opposite signs and are bounded away from zero . by our maintained assumption ( [ xi ] ) , the same is then true for the arguments in ( [ fi ] ) leading to a contradiction .",
    "hence @xmath549 must hold , completing the proof of part ( a ) .",
    "parts ( b ) and ( c ) are obvious since @xmath550 @xmath551 @xmath552 whenever @xmath123 . @xmath553    * proof of proposition [ select_prob_moving_par ] : * part ( a ) follows immediately from ( [ select_prob ] ) and the assumptions . to prove part ( b ) we use ( [ select_prob ] ) to write@xmath554the first and the second claim then follow immediately . for the third claim , assume first that @xmath555 .",
    "then@xmath556the case @xmath557 is handled analogously .",
    "@xmath553    * proof of proposition [ select_prob_pointwise_unknown ] : * we prove part ( b ) first . observe that @xmath558 \\rho _ { n - k}(s)ds \\\\ & = t_{n - k}\\left ( n^{1/2}\\eta _ { i , n}\\right ) -t_{n - k}\\left ( -n^{1/2}\\eta _ { i , n}\\right ) .\\end{aligned}\\]]by a subsequence argument it suffices to prove the result under the assumption that @xmath559 converges in @xmath560 . if the limit is finite , then @xmath561 is eventually constant and the result follows since every @xmath562-distribution has unbounded support . if @xmath222 then @xmath563where @xmath564 denotes the supremum norm . since @xmath565 if @xmath222 by polya s theorem , the result follows .",
    "part ( c ) is proved analogously .",
    "we next prove part ( a ) .",
    "observe that the collection of distributions corresponding to @xmath566 is tight on @xmath567 , meaning that for every @xmath568 there exist @xmath569 such that @xmath570 and @xmath571 .",
    "note that the map @xmath572 is monotonically nondecreasing .",
    "hence,@xmath573since @xmath574 ( @xmath575 , respectively ) converges to zero if and only if @xmath132 does so , part ( a ) follows from proposition select_prob_pointwise applied to the estimators @xmath576and @xmath577 .",
    "@xmath553    * proof of theorem [ select_prob_moving_par_unknown ] : * ( a ) set @xmath578 for @xmath579 . by proposition [ select_prob_moving_par ]",
    "we have that @xmath580 converges to @xmath581 for all @xmath579 , where @xmath582 for @xmath579 .",
    "since @xmath580 as well as @xmath581 are continuous functions of @xmath583 , are monotonically nondecreasing in @xmath583 , and have the property that their limits for @xmath584 are @xmath148 while the limits for @xmath585 are @xmath7 , it follows from polya s theorem that the convergence is uniform in @xmath583 .",
    "but then using ( [ select_prob_unknown ] ) gives@xmath586as @xmath119 .",
    "this completes the proof in case @xmath587 eventually ; in case @xmath222 observe that @xmath588 then converges to @xmath589 as the distribution corresponding to @xmath210 converges weakly to pointmass at @xmath590 and the integrand is bounded and continuous .",
    "\\(b ) observe that @xmath591 converges to @xmath7 for @xmath592 and to @xmath148 for @xmath593 by proposition [ select_prob_moving_par ] applied to the estimator @xmath594 .",
    "now ( [ select_prob_unknown ] ) and dominated convergence deliver the result in ( b1 ) .",
    "next consider ( b2 ) : suppose first that @xmath180 .",
    "choose @xmath268 small enough such that @xmath595 .",
    "then , recalling that @xmath596 is monotonically nondecreasing in @xmath583 , eq .",
    "( [ select_prob_unknown ] ) gives@xmath597now the integral on the r.h.s",
    ".  converges to @xmath7 since @xmath595 , and the probability on the r.h.s",
    ".  converges to @xmath7 by proposition [ select_prob_moving_par ] applied to the estimator @xmath598 .",
    "this completes the proof for the case @xmath180 .",
    "next assume that @xmath248 .",
    "choose @xmath268 small enough such that @xmath599 holds .",
    "then from ( select_prob_unknown ) we have@xmath600since @xmath580 is monotonically nondecreasing in @xmath583 and @xmath601 is not larger than @xmath7 . since @xmath602 and @xmath222 the second term on the r.h.s .",
    "goes to zero , while the first term goes to zero by proposition [ select_prob_moving_par ] applied to the estimator @xmath603 .",
    "next we prove 3.&4 .  and assume @xmath555 first . then using eq .",
    "( select_prob_unknown ) and performing the substitution @xmath604 we obtain ( recalling that @xmath210 is zero for negative arguments and using the abbreviations @xmath605 and @xmath606)@xmath607 \\\\ & & \\times \\left ( 2\\left ( n - k\\right ) \\right ) ^{-1/2}\\rho _ { n - k}(\\left ( 2\\left ( n - k\\right ) \\right ) ^{-1/2}t+1)dt \\\\ & = & \\int_{-\\infty } ^{\\infty } \\left [ \\phi \\left ( r_{i , n}+n^{1/2}\\eta _ { i , n}\\left ( 2\\left ( n - k\\right ) \\right ) ^{-1/2}t\\right ) -\\phi \\left ( r_{i , n}^{\\ast } -n^{1/2}\\eta _ { i , n}\\left ( 2\\left ( n - k\\right ) \\right ) ^{-1/2}t\\right ) \\right ] \\\\ & & \\times \\phi ( t)dt+o(1).\\end{aligned}\\]]the indicated term in the above display is @xmath608 by the lemma in the appendix and because the expression in brackets inside the integral is bounded by @xmath7 .",
    "since @xmath240 and @xmath609 , the integrand converges to @xmath610 under 3.@xmath611and to @xmath612 under 4 .",
    "the dominated convergence theorem then completes the proof .",
    "the case @xmath613 is treated similarly .",
    "it remains to prove 5 .",
    "again assume @xmath555 first .",
    "define @xmath614 and @xmath615 and rewrite the above display as@xmath616 \\\\ & & \\times \\phi ( t)dt+o(1).\\end{aligned}\\]]observe that @xmath617 and @xmath618 . the expression in brackets inside the integral hence converges to @xmath7 for @xmath619 and to @xmath620 for @xmath621 . by dominated convergence",
    "the integral converges to @xmath622 .",
    "the case @xmath557 is treated similarly .",
    "@xmath553    * proof of proposition [ closeness_prob ] : * observe that@xmath623by a trivial modification of lemma 13 in ptscher and schneider ( 2010 ) we conclude that for every @xmath268 there exists a real number @xmath624 such that @xmath625for every @xmath42 .",
    "using the fact , that @xmath99 is globally lipschitz with constant @xmath626 , this gives @xmath627which proves the result since @xmath296 can be made arbitrarily small .",
    "@xmath553      * proof of theorem [ thresh_consistency ] : * ( a ) observe that @xmath628holds for any of the estimators .",
    "hence , consistency of @xmath110 under @xmath121 and @xmath277 follows immediately from proposition ls_consistency(a ) since the distributions of @xmath247 are tight .",
    "conversely , suppose @xmath110 is consistent .",
    "then clearly @xmath629 whenever @xmath120 must hold , which implies @xmath226 by proposition [ select_prob_pointwise_unknown](a ) .",
    "this then entails consistency of @xmath45 by ( closeness_h_s_as_ls_unknown ) and tightness of the distributions of @xmath247 ; this in turn implies @xmath267 by proposition [ ls_consistency](a ) .",
    "\\(b ) since @xmath630 , it suffices to prove the second claim in ( b ) .",
    "now for every real @xmath271 we have@xmath631this gives@xmath632where the first term on the r.h.s .",
    "can be made arbitrarily small in view of proposition [ ls_consistency](b ) by choosing @xmath297 large enough .",
    "the second term on the r.h.s .  can be written as ( cf .",
    "( [ select_prob_unknown ] ) ) @xmath633for @xmath268 choose @xmath634 as in the proof of proposition [ select_prob_pointwise_unknown ] . using continuity of @xmath99 and the fact that the probability appearing on the r.h.s .",
    "above is monotonically increasing as @xmath635 approaches @xmath636 from above , this can be further bounded by@xmath637the last inequality holding for @xmath638 and since @xmath639 and @xmath640 .",
    "choosing @xmath297 sufficiently large ( depending on @xmath296 ) completes the proof for @xmath9 .",
    "next observe that @xmath641and similarly @xmath642 hold .",
    "since the set of distributions of @xmath247 ( i.e. , the set of distributions corresponding to @xmath210 ) is tight as already noted , this proves ( b ) then also for @xmath275 and @xmath276 .",
    "\\(c ) by a subsequence argument we can reduce the argument to the case where @xmath643 and @xmath2 converges in @xmath560 .",
    "suppose first that @xmath177 : observe that then @xmath644 eventually .",
    "choose @xmath26 and @xmath253 such that @xmath645 , where @xmath199 does not depend on @xmath1 and @xmath646 holds , and set the other coordinates of @xmath647 to arbitrary values ( e.g. , equal to zero ) . observe that there exists a constant @xmath648 such that @xmath649holds : if @xmath2 converges to a finite limit , i.e. , is eventually constant , the claim follows from theorem [ select_prob_moving_par_unknown](b1 ) ; if @xmath222 , then use theorem select_prob_moving_par_unknown(b2 ) . by ( [ nec ] )",
    "we have for @xmath650 and a suitable @xmath297 that@xmath651for all @xmath1 sufficiently large .",
    "but this is only possible if @xmath652 holds eventually , implying that @xmath285 .",
    "next consider the case where @xmath653 : observe that then @xmath279 is of the same order as @xmath270 . then define @xmath29 and @xmath253 such that @xmath654 , where @xmath193 does not depend on @xmath1 and @xmath655 holds , and set the other coordinates of @xmath647 to arbitrary values ( e.g. , equal to zero ) . observe that then ( [ delta ] ) also holds , in view of theorem [ select_prob_moving_par_unknown](a1 ) in case @xmath2 is eventually constant , and in view of theorem select_prob_moving_par_unknown(a2 ) in case @xmath222 .",
    "the rest of the proof is then similar as before .",
    "it remains to consider the case @xmath131 : it follows from ( [ closeness_h_s_as_ls_unknown ] ) , the assumptions on @xmath37 and @xmath14 , from @xmath131 , and from the observation that @xmath45 is @xmath656-distributed , that @xmath657 converges in distribution to a standard normal distribution for each fixed @xmath11 and @xmath16 .",
    "hence , stochastic boundedness of @xmath658 for each @xmath11 ( and a fortiori ( nec ) ) necessarily implies that @xmath659 .",
    "\\(d ) the proof for @xmath166 is similar and in fact simpler : note that now @xmath660 holds and that in the proof of ( b ) the integration over @xmath583 can simply be replaced by evaluation at @xmath590 . for ( c )",
    "one uses proposition [ select_prob_moving_par ] instead of theorem select_prob_moving_par_unknown .",
    "@xmath553      * proofs of propositions [ 1 ] , [ 2 ] , and [ 3 ] : * observe that @xmath661and that @xmath662 is @xmath663 .",
    "furthermore , we have @xmath664identifying @xmath662 and @xmath665 with @xmath666 and @xmath41 in ptscher and leeb ( 2009 ) and making use of eq .",
    "( 4 ) in that reference immediately gives the result for @xmath667 .",
    "the result for @xmath668 then follows from elementary calculations .",
    "the result for @xmath669 follows similarly by making use of eq .",
    "( 5 ) instead of eq .",
    "( 4 ) in ptscher and leeb ( 2009 ) .",
    "the result for @xmath670 then follows from elementary calculations .",
    "the results for @xmath671 and @xmath672 follow similarly by making use of eqs .",
    "( 9)-(11 ) in ptscher and schneider ( 2009 ) . @xmath553",
    "* proofs of propositions [ 4 ] , [ 5 ] , and [ 6 ] : * we have @xmath673where we have used independence of @xmath17 and @xmath45 allowing us to replace @xmath17 by @xmath209 in the relevant formulae , cf .",
    "leeb and ptscher ( 2003 , p.  110 ) .",
    "substituting ( hard_finite_sample ) , with @xmath14 replaced by @xmath674 , into the above equation gives ( [ hard_finite_sample_unknown ] ) .",
    "representing @xmath675 as an integral of @xmath676 given in ( [ hard_finite_sample_density ] ) and applying fubini s theorem then gives ( hard_finite_sample_unknown_density ) .",
    "similarly , we have @xmath677substituting ( [ soft_finite_sample ] ) , with @xmath14 replaced by @xmath674 , into the above equation and noting that @xmath678 gives ( soft_finite_sample_unknown ) .",
    "elementary calculations then yield ( soft_finite_sample_unknown_density ) .    finally ,",
    "we have @xmath679substituting ( [ adaptive_finite_sample ] ) , with @xmath14 replaced by @xmath674 , into the above equation gives ( adaptive_finite_sample_unknown ) .",
    "elementary calculations then yield ( adaptive_finite_sample_unknown_density ) .",
    "@xmath553      * proof of proposition [ lsdk_h ] :* the proof of ( a ) is completely analogous to the proof of theorem 4 in ptscher and leeb ( 2009 ) , whereas the proof of ( b ) is analogous to the proof of theorem 17 in the same reference .",
    "@xmath553    * proof of proposition [ lsdk_s ] :* the proof of ( a ) is completely analogous to the proof of theorem 5 in ptscher and leeb ( 2009 ) , whereas the proof of ( b ) is analogous to the proof of theorem 18 in the same reference .",
    "@xmath553    * proof of proposition [ lsdk_as ] :* the proof of ( a ) is completely analogous to the proof of theorem 4 in ptscher and schneider ( 2009 ) , whereas the proof of ( b ) is analogous to the proof of theorem 6 in the same reference . @xmath553    * proof of theorem [ closeness ] : * observe that the total variation distance between two cdfs is bounded by the sum of the total variation distances between the corresponding discrete and continuous parts .",
    "furthermore , recall that the total variation distance between the absolutely continuous parts is bounded from above by the @xmath680-distance of the corresponding densities .",
    "hence , from ( [ hard_finite_sample_density ] ) and ( [ hard_finite_sample_unknown_density ] ) we obtain    @xmath681    where@xmath682and@xmath683 \\right .",
    "\\\\ & & + \\left .",
    "\\left [ \\phi \\left ( n^{1/2}\\left ( -\\theta _ { i}/(\\sigma \\xi _ { i , n})-\\eta _ { i , n}(s\\wedge 1)\\right ) \\right ) -\\phi \\left ( n^{1/2}\\left ( -\\theta _ { i}/(\\sigma \\xi _ { i , n})-\\eta _ { i , n}(s\\vee 1)\\right ) \\right ) \\right ] \\right\\ } \\rho _ { n - k}(s)ds,\\end{aligned}\\]]where we have made use of fubini s theorem and performed an obvious substitution . by a trivial modification of lemma 13 in ptscher and schneider ( 2010 ) we conclude that for every @xmath268 there exists a real number @xmath624 such that @xmath684for every @xmath685 .",
    "using the fact , that @xmath99 is globally lipschitz with constant @xmath626 , this gives@xmath686the r.h.s .",
    "now converges to @xmath687 because @xmath261 .",
    "since @xmath268 was arbitrary , this shows that @xmath688 converges to zero .",
    "note also that @xmath689 has already been shown to converge to zero in proposition closeness_prob .",
    "this completes the proof for the hard - thresholding estimator .    with the same argument as above we obtain@xmath690where",
    "@xmath691and@xmath692where we have used ( [ soft_finite_sample_density ] ) and ( soft_finite_sample_unknown_density ) .",
    "now,@xmath693where@xmath694and where we have used fubini s theorem and an obvious substitution .",
    "it is elementary to verify that@xmath695and that @xmath696 holds .",
    "consequently , using ( [ c ] ) we obtain@xmath697where we have again used the fact that @xmath99 is globally lipschitz with constant @xmath626 .",
    "since @xmath698 and @xmath268 was arbitrary , the proof for soft - thresholding is complete , because @xmath689 goes to zero by proposition [ closeness_prob ] .",
    "finally , from ( [ adaptive_finite_sample ] ) and ( adaptive_finite_sample_unknown ) we obtain@xmath699observe that on the one hand @xmath700 and @xmath701 are bounded by @xmath7 , and that on the other hand , using the lipschitz - property of @xmath99 and the mean - value theorem,@xmath702where @xmath703 is a mean - value between @xmath583 and @xmath7 which may depend on @xmath704 .",
    "the supremum over @xmath704 on the r.h.s .",
    "is now clearly assumed for @xmath705 , resulting in the bound@xmath706the same bound is obtained for @xmath707 in exactly the same way .",
    "consequently , using ( [ c ] ) we obtain@xmath708 .\\end{aligned}\\]]since @xmath265 and @xmath268 was arbitrary , the proof is complete .",
    "* proof of theorem [ htconservative ] : * ( a ) the atomic part of @xmath709 as given in ( hard_finite_sample_unknown_density ) clearly converges weakly to the atomic part of ( [ hard_large_sample_unknown_density_a ] ) in view of theorem select_prob_moving_par_unknown(a1 ) and the fact that @xmath710 by assumption ; also note that the atomic part converges to the zero measure in case @xmath342 or @xmath131 as then the total mass of the atomic part converges to zero .",
    "we turn to the absolutely continuous part next . for later use",
    "we note that what has been established so far also implies that the total mass of the absolutely continuous part converges to the total mass of the absolutely continuous part of the limit , since it is easy to see that the limiting distribution given in the theorem has total mass @xmath7 .",
    "the density of the absolutely continuous part of ( [ hard_finite_sample_unknown_density ] ) takes the form@xmath711observe that for given @xmath458 , the indicator function in the above display converges to @xmath712 for lebesgue almost all @xmath583 . [ if @xmath131 , this is necessarily true only for @xmath458 with @xmath713 . ] since @xmath587 eventually , we get from the dominated convergence theorem that the above display converges to @xmath714 for every @xmath458 ( for every @xmath458 with @xmath713 in case @xmath131 ) , which is the density of the absolutely continuous part in ( [ hard_large_sample_unknown_density_a ] ) . since the total mass of the absolutely continuous part is preserved in the limit as shown above , the proof is completed by scheff s lemma .",
    "\\(b ) follows immediately from proposition [ lsdk_h ] and theorem closeness . @xmath553",
    "* proof of theorem [ stconservative ] : * ( a ) the atomic part of @xmath715 as given in ( soft_finite_sample_unknown_density ) converges weakly to the atomic part of ( [ soft_large_sample_unknown_density_a ] ) in view of theorem select_prob_moving_par_unknown(a1 ) and the fact that @xmath710 by assumption ; also note that the atomic part converges to the zero measure in case @xmath342 or @xmath131 as then the total mass of the atomic part converges to zero .",
    "we turn to the absolutely continuous part next . for later use",
    "we note that what has been established so far also implies that the total mass of the absolutely continuous part converges to the total mass of the absolutely continuous part of the limit , since it is easy to see that the limiting distribution given in the theorem has total mass @xmath7 .",
    "the density of the absolutely continuous part of ( [ soft_finite_sample_unknown_density ] ) takes the form@xmath716observe that for given @xmath458 , the functions @xmath717 converge to @xmath718 , respectively , for all @xmath583 .",
    "since @xmath587 eventually , we then get from the dominated convergence theorem that the above display converges to @xmath719for every @xmath720 ; the last display is precisely the density of the absolutely continuous part in ( soft_large_sample_unknown_density_a ) . since the total mass of the absolutely continuous part is preserved in the limit as shown above , the proof is completed by scheff s lemma .",
    "\\(b ) follows immediately from proposition [ lsdk_s ] and theorem closeness .",
    "@xmath553    * proof of theorem [ astconservative ] : * ( a ) observe that @xmath721where @xmath722 and @xmath723 reduce to @xmath724clearly , @xmath725 as well as @xmath726 converge for every @xmath727 to @xmath728and @xmath729respectively , if @xmath194 , and the dominated convergence theorem shows that the weights of the indicator functions in ( [ above ] ) converge to the corresponding weights in ( adaptive_soft_large_sample_unknown_cdf_a ) .",
    "since @xmath730 converges to @xmath193 by assumption , it follows that for every @xmath713 we have convergence of @xmath387 to the cdf given in ( adaptive_soft_large_sample_unknown_cdf_a ) .",
    "this proves part ( a ) in case @xmath194 . in case @xmath461",
    ", we have that @xmath723 converges to @xmath704 by an application of proposition 15 in ptscher and schneider ( 2009 ) .",
    "consequently , the limit of @xmath726 is now @xmath731 .",
    "again applying the dominated convergence theorem and observing that for each @xmath475 we have that @xmath732 is eventually zero , shows that @xmath733 converges to @xmath734 .",
    "the case @xmath459 is proved analogously .",
    "\\(b ) follows immediately from proposition [ lsdk_as ] and theorem closeness .",
    "@xmath553    * proof of theorem [ htconsistent ] : * observe that@xmath735where @xmath434 is standard normally distributed .",
    "the expressions in front of the indicator functions now converge to @xmath736 and @xmath148 , respectively , in probability as @xmath119 .",
    "inspection of the cdf of @xmath737 then shows that this cdf converges weakly to@xmath738if @xmath739 .",
    "part ( b ) of theorem select_prob_moving_par_unknown  completes the proof of both parts of the theorem in case @xmath739 .",
    "if @xmath740 the same theorem shows that the weak limit is now @xmath345 .",
    "@xmath553    * proof of theorem [ stconsistent ] : * ( a ) the atomic part of @xmath715 as given in ( soft_finite_sample_unknown_density ) converges weakly to the atomic part given in ( [ soft_large_sample_unknown_density_c ] ) by theorem select_prob_moving_par_unknown(b1 ) .",
    "the density of the absolutely continuous part of @xmath715 can be written as@xmath741recalling the convention that @xmath742 for @xmath743 .",
    "note that with this convention @xmath744 is then a bounded continuous function on the real line .",
    "since @xmath745 and @xmath746 clearly converge weakly to @xmath747 and @xmath748 , respectively , the density of the absolutely continuous part of @xmath715 is seen to converge to @xmath749 for every @xmath750 .",
    "an application of scheff s lemma then completes the proof , noting that the total mass of the absolutely continuous part of @xmath715 converges to the total mass of the absolutely continuous part of ( soft_large_sample_unknown_density_c ) as the same is true for the atomic part in view of theorem [ select_prob_moving_par_unknown](b1 ) ( and since the distributions involved all have total mass @xmath7 ) .",
    "\\(b ) rewrite @xmath751 as@xmath752where @xmath753 is a sequence of @xmath754-distributed random variables .",
    "observe that @xmath755 converges to @xmath199 and that @xmath753 converges to zero in @xmath756-probability . now ,",
    "if @xmath757 , then @xmath758 by theorem select_prob_moving_par_unknown(b2 ) , and hence @xmath759 converges to @xmath736 in @xmath756-probability .",
    "this proves the result in case @xmath180 . in case",
    "@xmath248 we have that @xmath760and@xmath761clearly , also @xmath762 converges to @xmath7 in @xmath763-probability since @xmath222 .",
    "consequently , @xmath751 converges to @xmath764 in @xmath763-probability , which proves the case @xmath248 . finally , if @xmath184 , then ( [ sign ] ) continues to hold and we can write@xmath765where @xmath766 refers to a term that converges to zero in @xmath763-probability",
    "this then completes the proof of part ( b ) . @xmath553",
    "* proof of theorem [ astconsistent ] : * ( a ) assume first that @xmath767 holds .",
    "note that @xmath768 and @xmath769 now reduce to @xmath770 .\\]]first , for @xmath771 we see that @xmath772 eventually reduces to @xmath773furthermore , for @xmath774 we see that @xmath775 for all @xmath579 whereas for @xmath776 we have that @xmath777 for @xmath778 and @xmath779 for @xmath780 .",
    "as a consequence , we obtain from the dominated convergence theorem that @xmath733 converges to @xmath7 for @xmath774 and to @xmath781 for @xmath776 .",
    "second , for @xmath782 note that @xmath733 eventually reduces to @xmath783and that @xmath784 for all @xmath579 in this case .",
    "this shows that for @xmath782 we have that @xmath733 converges to @xmath148 .",
    "but this proves the result for the case @xmath411 . in case",
    "@xmath478 the same reasoning shows that now @xmath785 eventually reduces to @xmath786for all @xmath704 , and that now for @xmath787 we have @xmath775 for all @xmath579 whereas for @xmath407 we have that @xmath788 for all @xmath579 .",
    "this shows that @xmath789 converges weakly to @xmath345 in case @xmath478 .",
    "the proof for the case @xmath790 is completely analogous .",
    "\\(b ) rewrite @xmath791 as@xmath792where @xmath753 is a sequence of @xmath754-distributed random variables .",
    "note that @xmath793 converges to @xmath199 by assumption .",
    "now , if @xmath757 , then @xmath794 by theorem select_prob_moving_par_unknown(b2 ) , hence @xmath795 converges to @xmath736 in @xmath756-probability , establishing the result in this case .",
    "furthermore , for @xmath796 rewrite the above display as@xmath797with the convention that @xmath798 in case @xmath398 .",
    "if @xmath182 ( including the case @xmath362 ) then @xmath799 by theorem [ select_prob_moving_par_unknown](b2 ) , and hence the last display shows that @xmath795 converges to @xmath800 in @xmath756-probability , establishing the result in this case . finally , if @xmath184 holds , then the last line in the above display reduces to @xmath801 , completing the proof of part ( b ) .",
    "@xmath553    * proof of proposition [ oracle_1 ] : * ( a ) by a subsequence argument we may assume that @xmath2 converges in @xmath225 .",
    "applying theorem [ select_prob_moving_par_unknown](b ) we obtain that @xmath802 converges to @xmath7 in case @xmath123 , and to @xmath148 in case @xmath120 .",
    "observe that @xmath803holds on the event @xmath804 , while @xmath805holds on the event @xmath806 .",
    "the result then follows in view of the fact that @xmath434 is standard normally distributed .",
    "the proof for @xmath15 is similar using proposition select_prob_moving_par(b ) instead of theorem select_prob_moving_par_unknown(b ) ( it is in fact simpler as the subsequence argument is not needed ) .",
    "\\(b ) again we may assume that @xmath2 converges in @xmath225 .",
    "by the same reference as in the proof of ( a ) we obtain that @xmath807 converges to @xmath7 in case @xmath123 , and to @xmath148 in case @xmath120 .",
    "now @xmath808holds on the event @xmath809 and the claim for @xmath810 follows immediately . on the event",
    "@xmath811 we have from the definition of the estimator @xmath812now , if @xmath120 , then the event @xmath811 has probability approaching @xmath7 as shown above .",
    "hence , we have on events that have probability tending to @xmath7@xmath813since @xmath814 and @xmath815 by the assumption and since @xmath816 ; also note that @xmath247 is stochastically bounded since the collection of distributions corresponding to @xmath744 with @xmath817 is tight on @xmath567 as was noted earlier .",
    "the proof for @xmath276 is again similar ( and simpler ) by using proposition select_prob_moving_par(b ) instead of theorem select_prob_moving_par_unknown(b ) . @xmath553",
    "alliney , s. & s.  a. ruzinsky ( 1994 ) : an algorithm for the minimization of mixed @xmath818 and @xmath819 norms with applications to bayesian estimation . _ _ ieee transactions on signal processing  _ _ 42 , 618 - 627 .",
    "bauer , p. , ptscher , b.  m. & p.  hackl ( 1988 ) : model selection by multiple test procedures . _ _ statistics  _ _ 19 , 3944 .",
    "donoho , d.  l. , johnstone , i.  m. , kerkyacharian , g. , d. picard ( 1995 ) : wavelet shrinkage : asymptopia ?",
    "with discussion and a reply by the authors . _ journal of the royal statistical society series b _  57 , 301369 .",
    "fan , j. & r. li ( 2001 ) : variable selection via nonconcave penalized likelihood and its oracle properties .",
    "_ journal of the american statistical association _  96 , 1348 - 1360 .",
    "fan , j. & h. peng ( 2004 ) : nonconcave penalized likelihood with a diverging number of parameters . _",
    "annals of statistics _  32 , 928961 .",
    "feller , w. ( 1957 ) : _ an introduction to probability theory and its applications , volume 1 .",
    "_ 2nd ed . ,",
    "wiley , new york .",
    "frank , i.  e. & j.  h. friedman ( 1993 ) : a statistical view of some chemometrics regression tools ( with discussion ) . _ _ technometrics  _ _ 35 , 109 - 148 .",
    "ibragimov , i.  a. ( 1956 ) : on the composition of unimodal distributions .",
    "_ _ theory of probability and its applications  _ _ 1 , 255 - 260 .",
    "knight , k. & w. fu ( 2000 ) : asymptotics for lasso - type estimators .",
    "_ _ annals of statistics  _ _ 28 , 1356 - 1378 .",
    "leeb , h. & b.  m. ptscher ( 2003 ) : the finite - sample distribution of post - model - selection estimators and uniform versus nonuniform approximations .",
    "_ econometric theory _  19 , 100142 .",
    "leeb , h. & b.  m. ptscher ( 2005 ) : model selection and inference : facts and fiction .",
    "_ econometric theory _  21 , 2159 .",
    "leeb , h. & b.  m. ptscher ( 2008 ) : sparse estimators and the oracle property , or the return of hodges estimator .",
    "_ _ journal of econometrics  _ _ 142 , 201 - 211 .",
    "ptscher , b.  m. ( 1991 ) : effects of model selection on inference .",
    "_ econometric theory _  7 , 163185 .",
    "ptscher , b.  m. ( 2006 ) : the distribution of model averaging estimators and an impossibility result regarding its estimation . _ ims lecture notes - monograph series _  52 , 113129 .",
    "ptscher , b.  m. & h. leeb ( 2009 ) : on the distribution of penalized maximum likelihood estimators : the lasso , scad , and thresholding . _ _ journal of multivariate analysis  _ _ 100 , 2065 - 2082 .",
    "ptscher , b.  m. & u. schneider ( 2009 ) : on the distribution of the adaptive lasso estimator .",
    "_ _ journal of statistical planning and inference  _ _ 139 , 2775 - 2790 .",
    "ptscher , b.  m. & u. schneider ( 2010 ) : confidence sets based on penalized maximum likelihood estimators in gaussian regression .",
    "_ electronic journal of statistics _  10 , 334 - 360 .",
    "sen , p.  k. ( 1979 ) : asymptotic properties of maximum likelihood estimators based on conditional specification .",
    "_ _ annals of statistics  _ _ 7 , 1019 - 1033 .",
    "tibshirani , r. ( 1996 ) : regression shrinkage and selection via the lasso .",
    "_ journal of the royal statistical society series b _  58 , 267 - 288 .",
    "zhang , c .- h .",
    "( 2010 ) : nearly unbiased variable selection under minimax concave penalty .",
    "_ _ annals of statistics  _ _ 38 , 894 - 942 .",
    "zou , h. ( 2006 ) : the adaptive lasso and its oracle properties .",
    "_ _ journal of the american statistical association  _ _ 101 , 1418 - 1429 .",
    "recall that @xmath406 for @xmath407 .      observe that @xmath820 is the density of @xmath823 where @xmath824 denotes a chi - square distributed random variable with @xmath223 degrees of freedom . by the central limit theorem and the delta - method @xmath825 converges in distribution to a standard normal random variable . with @xmath826being the density of @xmath827 we have for @xmath787 @xmath828and we have @xmath406 for @xmath829 .",
    "since the cdf associated with @xmath830 is unimodal , this shows that the same is true for the cdf associated with @xmath744 .",
    "but then convergence in distribution of @xmath825 implies convergence of @xmath831 to @xmath821 in the @xmath680-sense by a result of ibragimov ( 1956 ) , scheff s lemma , and a standard subsequence argument ."
  ],
  "abstract_text": [
    "<S> we study the distribution of hard- , soft- , and adaptive soft - thresholding estimators within a linear regression model where the number of parameters @xmath0 can depend on sample size @xmath1 and may diverge with @xmath1 . </S>",
    "<S> in addition to the case of known error - variance , we define and study versions of the estimators when the error - variance is unknown . </S>",
    "<S> we derive the finite - sample distribution of each estimator and study its behavior in the large - sample limit , also investigating the effects of having to estimate the variance when the degrees of freedom @xmath2 does not tend to infinity or tends to infinity very slowly . </S>",
    "<S> our analysis encompasses both the case where the estimators are tuned to perform consistent variable selection and the case where the estimators are tuned to perform conservative variable selection . </S>",
    "<S> furthermore , we discuss consistency , uniform consistency and derive the uniform convergence rate under either type of tuning .    </S>",
    "<S> msc subject classification : 62f11 , 62f12 , 62j05 , 62j07 , 62e15 , 62e20    keywords and phrases : thresholding , lasso , adaptive lasso , penalized maximum likelihood , variable selection , finite - sample distribution , asymptotic distribution , variance estimation , uniform convergence rate , high - dimensional model , oracle property </S>"
  ]
}