{
  "article_text": [
    "information flow , or information transfer as it may be referred to in the literature , has long been recognized as the appropriate measure of causality between dynamical events@xcite .",
    "it possesses the needed asymmetry or directionalism for a cause - effect relation , and , moreover , provides a quantitative characterization of the otherwise statistical test , e.g. , the granger causality test@xcite .",
    "for this reason , the past decades have seen a surge of interest in this arena of research .",
    "measures of information flow proposed thus far include , for example , the time - delayed mutual information@xcite , transfer entropy@xcite , momentary information transfer@xcite , causation entropy@xcite , etc . , among which transfer entropy has been proved to be equivalent to granger causality up to a factor 2 for linear systems@xcite .",
    "recently , liang and kleeman find that the notion of information flow actually can be put on a rigorous footing within a given deterministic system@xcite .",
    "the basic idea can be best illustrated with a system of two components , say , @xmath0 and @xmath1 .",
    "the problem here essentially deals with how the marginal entropies of @xmath0 and @xmath1 , written respectively as @xmath2 and @xmath3 , evolve .",
    "take @xmath2 for an example",
    ". its evolution could be due to @xmath0 its own and/or caused by @xmath1 .",
    "that is to say , @xmath4 can be split exclusively into two parts : @xmath5 if we write the contribution from the former mechanism as @xmath6 and that from the latter as @xmath7 .",
    "this @xmath7 is the very time rate of information flowing from @xmath1 to @xmath0 .    to find the information flow @xmath7",
    ", it suffices to find @xmath8 , since , for each deterministic system , there is a liouville equation for the density of the state and , accordingly , @xmath4 can be obtained . in ref .",
    "@xcite , @xmath8 is acquired through an intuitive argument based on an entropy evolutionary law established therein .",
    "the same result is later on rigorously proved ; see @xcite for a review . for stochastic systems which we will be considering in this study , the trick ceases to work , but in @xcite liang manages to circumvent the difficulty and find the result , which we will be briefly reviewing in the following .",
    "consider a two - dimensional ( 2d ) stochastic system @xmath9 where @xmath10 is the vector of drift coefficients ( differentiable vector field ) , @xmath11}}$ ] the matrix of stochastic perturbation coefficients , and @xmath12 a 2d standard wiener process .",
    "let @xmath13 , and @xmath14 be the marginal probability density function of @xmath15 .",
    "liang ( 2008)@xcite proves that the time rate of information flowing from @xmath1 to @xmath0 is @xmath16 where @xmath17 signifies the operator mathematical expectation .",
    "this measure of information flow is asymmetric between the two parties and , particularly , if the process underlying @xmath0 does not depend on @xmath1 , then the resulting causality from @xmath1 to @xmath0 vanishes .",
    "this is the so - called _ property of causality _ , which asserts , in the above language , that @xmath7 vanishes if @xmath18 and @xmath19 are independent of @xmath20 .",
    "when @xmath7 is nonzero , it may take positive or negative values .",
    "a positive @xmath7 means @xmath1 causes @xmath0 to be more uncertain , while a negative @xmath7 reduces the entropy of @xmath0 , and hence functions to stabilize the latter . for more details ,",
    "referred to ref .",
    "@xcite .",
    "the above theorem is recently applied to time series analysis . under the assumption of a linear model with additive noise",
    ", liang@xcite shows that the maximal likelihood estimate of the information flow in ( [ eq : t21 ] ) turns out to be very tight in form , involving only the common statistics namely sample covariances .",
    "take two series @xmath0 and @xmath1 for example .",
    "the rate of information flowing ( units : nats per unit time ) from @xmath1 to @xmath0 is shown to be @xmath21 where @xmath22 is the sample covariance between @xmath23 and @xmath24 , and @xmath25 is that between @xmath23 and @xmath26 , @xmath26 being the difference approximation of @xmath27 using the euler forward scheme : @xmath28 here @xmath29 is usually 1 , but for highly chaotic and densely sampled series , @xmath30 should be chosen to avoid getting spuriously large @xmath26 due to possible shock structures that make the differencing highly sensitive to the error in @xmath24 .",
    "this formula involves only sample covariances , and is hence very convenient to evaluate .",
    "in addition , it is easy to see that if @xmath31 then @xmath32 , but when @xmath32 the correlation @xmath33 does not need to vanish .",
    "that is to say , contrapositively , _",
    "causation implies correlation , but correlation does not imply causation_. in an explicitly quantitative way , this corollary resolves the long - standing debate over causation versus correlation .     and @xmath1 ) of the autoregressive process ( [ eq : ar1 ] ) for ( a ) @xmath34 , @xmath35 , and ( b ) @xmath36 @xmath35",
    ". , scaledwidth=75.0% ]    in general , the rates of information flow differ from case to case .",
    "one would like to normalize them in real applications , just as that with correlation coefficients , in order to assess the importance of the flow identified , if any .",
    "for example , consider two series generated from two autoregressive processes :    [ eq : ar1 ] @xmath37    where the errors @xmath38 and @xmath39 are independent .",
    "generate a pair of series with 80000 steps on matlab , and perform the causality analysis ( with @xmath40 ) , one obtains ( units are in nats per iteration ; same below in this section )    * for @xmath41 , @xmath42 , @xmath43 ; * for @xmath44 :   @xmath45 , @xmath46 .",
    "( the results may differ slightly with different series due to the random number generation . ) for the case @xmath34 , @xmath47 , one may then conclude that this is a one - way causality from @xmath1 to @xmath0 , as is indeed true . for the latter",
    ", however , one actually can not say much from the numbers .",
    "although they are small , they tell no more than that the information flows in both directions are of equal importance . from this particular example , one sees that , though theoretically the information flows in both directions should be precisely zero , in reality the rates evaluated from two time series , albeit very small , generally do not precisely vanish .",
    "one then can not tell whether the causality indeed exists .",
    "we need to normalize the obtained information flow for one to see the relative magnitude .",
    "of course , one may perform statistical test for the results . in the first example , at a 90% significance level , @xmath48 @xmath49",
    "so @xmath50 is insignificant . for the second example , @xmath51 @xmath52 that is to say , at a 90% level , these flow rates are not significantly different from zero . in this sense",
    ", it seems that we could indeed infer rather accurately the true causality .",
    "however , a statistical test just tells how precise the estimate is ; it does not tell how the information flow may weigh in the entropy balance of the series .",
    "besides , it depends on the length of the series which is irrelevant to the parameter to be estimated . to see this more clearly ,",
    "consider the following case : @xmath53 obviously , the information flows , albeit existing , make only tiny contributions to their respective series , as the coupling coefficients are over an order smaller ; in classical perturbation analysis , they can be dropped to the first order approximation .",
    "the computed information flows , at a 90% significance level , are @xmath54 @xmath55 these results indicate that they are significantly different from zero  this from one aspect testifies to the success of the formalism .",
    "however , the small numbers can not tell how important they are , since , with a slowly varying series , even the dominant flow could be very small . on the other hand , if we cut the series by half and pick the first 40000 points for the analysis , then the result will be @xmath56 @xmath57 so one finds that @xmath7 is insignificant while @xmath50 is .",
    "( again , these small numbers may show large fluctuation if series of different lengths are used , because in reality they are insignificant . )",
    "can one thus conclude that there is a one - way causality , or can he / she thus asserts that this shortened series yields a more reliable estimation ?",
    "surely this is absurd .",
    "the problem here is that we do need a normalized flow to evaluate its importance relative to other factors .",
    "the normalization is by no means as simple as it appears with correlation analysis , which is in an inner product form based on the cauchy - schwarz inequality . in the following",
    "we will see that we need to get down to the fundamentals of information flow before arriving at a logically and physically sound normalizer .",
    "as what we did before in @xcite , this normalizer is estimated with the method of maximal likelihood estimation , using the given time series ( section  [ sect : estimation ] ) .",
    "the resulting formula is then validated ( section  [ sect : validation ] ) with the autoregression example as shown above .",
    "to demonstrate its diverse applicability , presented subsequently are two real world examples , one from climate science ( section  [ sect : climate ] ) , another from financial economics ( section  [ sect : finance ] ) .",
    "this study is summarized in section  [ sect : summary ] .",
    "as mentioned in the introduction , the normalization is not as simple as it seems to be . a natural normalizer that comes to mind , at the hint of correlation coefficient ,",
    "might be the information of a series transferred from itself .",
    "a snag is , however , that this quantity may turn out to be zero , just as that in the hnon map , a benchmark problem we have examined before ( see the references in @xcite ) .",
    "another snag is , the two way causality actually can not be normalized together , as that in correlation analysis based on the cauchy - schwarz inequality .",
    "that is to say , two information flows of equal size may have different relative importances in their respective series .",
    "since in our framework the information flow from @xmath1 to @xmath0 is the contribution of @xmath1 that makes to the time rate of change of the marginal entropy of @xmath0 , written @xmath2 , one may ask whether the rate of marginal entropy change @xmath4 can be the normalizer .",
    "this might be appealing , but there is a third snag . as information flow",
    "can be positive or negative , @xmath4 may turn out to be smaller than the flow in absolute value  the so - obtained relative flow would exceed 100% , a case which we do not want to see .",
    "all the above tells that information flow normalization is by no means a trivial task .",
    "we need to get to the basics and analyze how an information flow within a system is derived . by ref .",
    "@xcite , the time rate of change of the marginal entropy of @xmath0 is @xmath58 it is actually a result of two mutually exclusive mechanisms : the first is the information flow @xmath7 as shown in ( [ eq : t21 ] ) ; the second is the complement , i.e. , the rate of entropy increase without taking into account of the effect of @xmath1 .",
    "denote this latter as @xmath59 , liang ( 2008)@xcite has proved that @xmath60 the right hand side has three terms .",
    "the first term is precisely the time rate of change of @xmath2 due to @xmath0 itself in the absence of stochasticity .",
    "this is the starting point which we have shown in 2005@xcite in establishing the rigorous formalism and proved later on ( cf .",
    "hence through a careful analysis , the increase in the marginal entropy @xmath2 is decomposed into three parts : @xmath61 as well as @xmath7 in ( [ eq : t21 ] ) , which correspond to , respectively , the contribution due to @xmath0 itself , the stochastic effect , and the information flowing from @xmath1 .",
    "note this decomposition does not appear explicitly in the marginal entropy evolution equation  ( [ eq : dh1 ] ) , as the two stochastic terms cancel out .",
    "the normalization is now made easy .",
    "let @xmath62 obviously it is no less than @xmath7 in magnitude , and can not be zero unless @xmath0 does not change , a situation that is excluded in time series analysis .",
    "we may therefore pick @xmath63 as the normalizer , and define @xmath64 this way if @xmath65 , the variation of @xmath2 is 100% due to the information flow from @xmath1 ; if @xmath66 is approximately zero , @xmath1 is not causal",
    ". therefore , @xmath66 assesses the importance of the influence of @xmath1 to @xmath0 relative to other processes .",
    "it should be pointed out that , the above normalizer applies to @xmath7 only . for @xmath50 ,",
    "it is @xmath67 which may be quite different in value .",
    "this from another aspect reflects the asymmetry between @xmath7 and @xmath50 .",
    "as in ref .",
    "@xcite , consider a linear version of the stochastic differential equation ( sde ) ( [ eq : sde ] ) @xmath68 where @xmath69 is a constant vector , and @xmath70 and @xmath71 are constant matrices .",
    "initially if @xmath72 obeys a gaussian distribution , then it is a gaussian for ever , i.e. , @xmath73 with the mean @xmath74 and covariance matrix @xmath75 governed by equations @xmath76 so eqs .",
    "( [ eq : dh1star ] ) and ( [ eq : dh1noise ] ) can be explicitly evaluated : @xmath77 and @xmath78 }        -\\frac12 \\iint_{\\r^2 } \\rho_{2|1 } { \\frac { { \\partial}^2 g_{11}\\rho_1 } { { \\partial}x_1 ^ 2 } } dx_1dx_2 \\cr      & = & \\frac12 \\frac { g_{11 } } { \\sigma_1 ^ 2 }        - \\frac12 \\int_\\r { \\frac { { \\partial}^2 g_{11}\\rho_1 } { { \\partial}x_1 ^ 2 } }          { \\left(\\int_\\r \\rho_{2|1}(x_2 | x_1 ) dx_2\\right ) } dx_1 ,      \\end{aligned}\\ ] ] since neither @xmath19 nor @xmath79 depends on @xmath20 . but @xmath80 , and @xmath79 is compactly supported , the whole second term on the right hand side then vanishes . hence @xmath81 where for notational symmetry @xmath82 has been written as @xmath83 .",
    "these , together with the information flow from @xmath1 to @xmath0 as we have obtained before@xcite@xcite , @xmath84 form the three constituents that account for the evolution of the marginal entropy of @xmath0 .",
    "an observation about @xmath85 , where @xmath86 is that it is always positive . that is to say",
    ", the noise always contributes to increase the marginal entropy of @xmath0 , conforming to our common sense . in financial economics ,",
    "this reflects the volatility of , say , a stock . on the other hand , for a stationary series , i.e. , when @xmath87 , the balance on the right hand side of eq .",
    "( [ eq : sigma ] ) requires that @xmath88 .",
    "so this quantity is also related to the noise - to - signal ratio .",
    "the above results need to be estimated if what we are given are just a pair of time series .",
    "that is to say , what we know is a single realization of some unknown system , which , if known , can produce infinitely many realizations . the problem now is turned into estimating ( [ eq : dh1star_lin ] ) and ( [ eq : dh1noise_lin ] ) with the available statistics of the given time series .    we use maximum likelihood estimation ( e.g. , @xcite ) to achieve the goal .",
    "the procedure follows precisely that of @xcite , which for easy reference we briefly summarize here . as established before , a further assumption that @xmath89 , and hence @xmath90 , will much simplify the result , while in practice this is quite reasonable .",
    "suppose that the series are equal - distanced with a time stepsize @xmath91 , and let @xmath92 be the sample size .",
    "consider an interval @xmath93 $ ] , and let the transition probability function ( pdf ) be @xmath94 , where",
    "@xmath95 stands for the vector of parameters to be estimated .",
    "so the log likelihood is @xmath96 as @xmath92 is usually large , the term @xmath97 can be dropped without causing much error .",
    "the transition pdf is , with the euler - bernstein approximation ( see @xcite ) , @xmath98^{1/2 } }        \\times e^{-\\frac12 ( { { \\bf x}}_{n+1 } - { { \\bf x}}_n - { { \\bf f}}{{\\delta t}})^t                    ( { { \\bf { b}}}{{\\bf { b}}}^t{{\\delta t}})^{-1 }                    ( { { \\bf x}}_{n+1 } - { { \\bf x}}_n - { { \\bf f}}{{\\delta t } } ) } ,      \\end{aligned}\\ ] ] where @xmath99 .",
    "this results in a log likelihood functional @xmath100 where @xmath101 and @xmath102 is the euler forward differencing approximation of @xmath103 : @xmath104 with @xmath105 .",
    "usually @xmath40 should be used to ensure accuracy , but in some cases of deterministic chaos and the sampling is at the highest resolution , one needs to choose @xmath30 . maximizing @xmath106 , we find@xcite that the maximizer @xmath107 satisfies the following algebraic equation : @xmath108 } }        { { \\left[\\begin{array}{l }                  \\hat f_1 \\\\",
    "\\hat a_{11 } \\\\",
    "\\hat a_{12 }                  \\end{array}\\right ] } }      =      { { \\left[\\begin{array}{l }                  \\overline{\\dot x_1 } \\\\                  \\overline{x_1\\dot x_1 } \\\\",
    "\\overline{x_2 \\dot x_1 }                  \\end{array}\\right ] } } ,      \\end{aligned}\\ ] ] where the overline signifies sample mean .",
    "after some manipulations ( see @xcite ) , this yields the mle estimators : @xmath109 where @xmath110 are the sample covariances , and @xmath111}^2\\cr      & = & \\sum_{n=1}^n { \\left [          ( \\dot x_{1,n } - \\overline{\\dot x_{1,n } } )          - \\hat a_{11 } ( x_{1,n } - \\bar x_1 )           - \\hat a_{12 } ( x_{2,n } - \\bar x_2 ) \\right]}^2       \\cr      & = & n ( c_{d1,d1 } + \\hat a_{11}^2 c_{11 } + \\hat a_{12}^2 c_{22 }          - 2\\hat a_{11 } c_{d1,1 } - 2\\hat a_{12 } c_{d1,2 }          + 2\\hat a_{11 } \\hat a_{12 } c_{12 } ) .",
    "\\end{aligned}\\ ] ]    on the other hand , the population covariance matrix @xmath75 can be rather accurately estimated by the sample covariance matrix @xmath112 .",
    "so ( [ eq : dh1star_lin])-([eq : dh1noise_lin ] ) become @xmath113 as that in ref .",
    "@xcite with @xmath7 , here @xmath114 and @xmath115 should bear a hat , since they are the corresponding estimators .",
    "we abuse the notation a little bit to avoid notational complexity ; from now on they should be understood as their respective estimators .",
    "with these the normalizer is @xmath116 and hence we have the relative information flow from @xmath1 to @xmath0 : @xmath117",
    "back to the autoregressive process exemplified in the beginning . when @xmath118 , @xmath35 , the computed relative information flow rates are : @xmath119 clearly both are negligible in comparison to the contributions from the other processes in their respective series .",
    "this is in agreement with what one would conclude based on the absolute information flow computation and statistical testing . for the case",
    "@xmath120 , in which one may encounter difficulty due to the ambiguous small numbers , the computed relative information flow rates are : @xmath121 again they are essentially negligible , just as one would expect .    on the other hand , when @xmath34 , @xmath35 , @xmath122 to @xmath0 , the influence from @xmath1 is large , contributing to more than 1/6 of the total entropy change .",
    "in contrast , the influence from @xmath0 to @xmath1 is negligible .",
    "it should be pointed out that the relative information flow , say , @xmath66 , makes sense only with respect to @xmath0 , since the comparison is within the series itself .",
    "here comes the following situation : for a two - way causal system with absolute information flows @xmath7 and @xmath50 of equal importance , their relative importances within their respective series could be quite different .",
    "for example , @xmath123 where @xmath124 and @xmath125 are identical independent normals ( @xmath126n(0,1 ) ) .",
    "initialize them with random values between [ 0,1 ] and generate 80000 data points on matlab .",
    "the computed information flow rates ( in nats per iteration ) @xmath127 which are almost the same .",
    "the relative information flows , however , are quite different : @xmath128 in terms of relative contribution in their respective series , the former is way more below the latter .    generally speaking , the above imbalance is a rule , not an exception , reflecting the asymmetry of information flow .",
    "one may reasonably imagine that , in some extreme situation , a flow might be dominant while its counterpart is negligible within their respective series , although the two are of the same order in absolute value .",
    "el nio , also known as el nio - southern oscillation , or enso for short , is a long known and extensively studied climate mode in the tropical pacific ocean due to its relation to the global disasters like the droughts in southeast asia , southern africa , and northern australia , the floods in ecuador , the increasing number of typhoons , the death of birds and dolphins in peru , and the famine and epidemic diseases in far - flung regions of the world@xcite . a correct forecast of an el  nio ( or its cold counterpart , la nia ) a few months earlier will not only help issue in - advance warnings of potential disastrous impacts , but also make the subsequent seasonal weather forecasting much easier .",
    "however , this aperiodic leading mode in the tropical pacific seems to be extraordinarily difficult to predict .",
    "a good example is the latest `` super el  nio '' or `` monster el  nio '' , which has been predicted to arrive in 2014 in a lot of portentous forecasts , turns out to be a computer artifact .    for more reliable predictions",
    ", it is imperative to clarify the source of its uncertainty or unpredictability . in ref .",
    "@xcite , we have presented an application of eq .",
    "( [ eq : t21_est ] ) to the relation study between el  nio and the indian ocean dipole ( iod ) , another major climate mode in the indian ocean@xcite , and found that the indian ocean is a source of uncertainty that keeps enso from being accurately predicted . since in that study",
    "there is no relative importance assessment , we do not know whether the information flows , albeit significant , do weigh much in the modal variabilities .",
    "we hence redo the computation using the relative information flow formula ( [ eq : tau21 ] ) .",
    "we use for this study the same data as that used in @xcite , which include the nio4 index series and the sea surface temperature ( sst ) series downloaded from the noaa esrl physical sciences division@xcite , and the iod index namely dmi series from the jamstec site@xcite .    shown in fig .",
    "[ fig : indian ] is the relative information flow rate from the indian ocean sst to el nio , @xmath129 . from it",
    "one can see that the information flow accounts for more than 10% of the uncertainties of nio4 , the maximum reaching 27% .",
    "this number is very large .",
    "besides , all the values are positive , indicating that the indian ocean sst functions to make el  nio more uncertain .",
    "no wonder recently researchers find that assimilation of the indian ocean data helps the prediction of el  nio ( e.g. , @xcite ) , although traditionally the indian ocean is mostly ignored in el  nio modeling .    besides the relative importance we have just obtained , fig .",
    "[ fig : indian ] also reveals some difference in structure from its counterpart , i.e. , the fig .",
    "5b of @xcite .",
    "a conspicuous difference is that now there are clearly two centers , residing on either side of indian .",
    "note this structure is different from the traditional dipolar pattern as one would expect ; here both centers are positive .",
    "this means that the northern indian ocean sst anomalies , both the positive phase and negative phase , as an integral entity influence the el  nio variabilities , and , in particular , make el  nio more unpredictable .",
    "the dipolar structure implies that most probably this entity is iod , not others like iobm ( indian ocean basin mode ; see @xcite ) .    .",
    ", scaledwidth=60.0% ]    to see more about this , we look at the information flow from the index dmi to the tropic pacific sst .",
    "the absolute rates are referred to the fig .",
    "4a of @xcite ; shown in fig .",
    "[ fig : pacific ] are @xmath130 .",
    "indeed the computed flow rates are significant , and all are positive .",
    "the largest @xmath131 , which occupies a large swathe of the equatorial region between @xmath132 through @xmath133 , reaches 10% .",
    "moreover , the structure reminds one of the el  nio pattern .",
    "it is generally the same as that in its counter part , i.e. , fig .",
    "4a of @xcite , save for two changes : ( 1 ) the maximum center moves westward ; ( 2 ) the small center of a secondary maximum near @xmath134 at the equator disappears .",
    "this clear el nio - like structure attests to the above conjecture that iod is indeed a major source of uncertainty for the el  nio forecast .    .",
    ", scaledwidth=100.0% ]    we have also computed the relative information flows from el  nio to the indian ocean sst , and that from the pacific sst to the iod , using the same datasets .",
    "the results are also significant , though only approximately half as shown above .",
    "we now look at the causal relations between several financial time series . here",
    "it is not our intention to conduct an financial economics research or study the market dynamics from an econophysical point of view ; our purpose is to demonstrate a brief application of the aforementioned formalism for time series analysis .",
    "nonetheless , this topic is indeed of interest to both physicists and economists in the field of macroscopic econophysics ; see , for example , @xcite .",
    "we pick nine stocks in the united states and download their daily prices from @xmath135 .",
    "these stocks are : msft ( microsoft corporation ) , aapl ( apple inc . ) , ibm ( international business machines corporation ) , intc ( intel corporation ) , ge ( general electric company ) , wmt ( wal - mart stores inc . ) , xom ( exxon mobil corporation ) , cvs ( cvs health corporation ) , f ( ford motor corporation ) . among these are high - tech companies ( msft , aapl , ibm , intc ) , retail trade companies",
    "[ e.g. , the drugstore chains ( cvs ) and discount stores ( wmt ) ] , automotive industry ( f ) , oil and gas industry ( xom ) , and the multinational conglomerate corporation ge which operates through the segments of energy , technology infrastructure , capital finance , etc . here by `` daily '' we mean on a trading day basis ,",
    "excluding , say , holidays and weekends .",
    "since stock prices are generally nonstationary , we check the series of daily return , i.e. , @xmath136 / p(t),\\ ] ] or log - return @xmath137 where @xmath138 are the adjusted closing prices in the yahoo spreadsheet , and @xmath139 is one trading day .",
    "following most people we use the series of log - returns @xmath140 for our purpose .",
    "in fact , return and log - return series are approximately equivalent , particularly in the high - frequency regime , as indicated by @xcite .",
    "since the most recent stock msft started on march 13 , 1986 , all the series are chosen from that date through december 26 , 2014 , when we started to examine these series .",
    "this amounts to 7260 data points , and hence 7259 points for the log - return series .    using eq .",
    "( [ eq : t21_est ] ) , we compute the information flows between the nine stocks and form a matrix of flow rates ; see table  [ tab : stocks ] . a flow direction is represented with the matrix indices ; more specifically , it is from the row index to the column index .",
    "for example , listed at the location ( 2,4 ) is @xmath141 , i.e. , @xmath142 , the flow rate from apple to intel , while ( 4,2 ) stores the rate of the reverse flow , @xmath143 . also listed in the table",
    "are the respective confidence intervals at the 90% level .    from table  [ tab : stocks ] , most of the information flow rates are significant at the 90% level , as highlighted .",
    "their values vary from 4 to 22 ( units : @xmath144  nats / day ; same below in this section ) .",
    "the maximum is @xmath145 , and second to it are @xmath146 and @xmath147 , both being @xmath148 .",
    "the influence of ibm to exxon is not a surprise , considering the dependence of the oil industry on high - tech equipments .",
    "the mutual causality between the retail stores wmt and cvs are also understandable .",
    "the information flow from cvs to ge could be through the sales of ge products ; after all , ge makes household appliances . for the rest in the table",
    ", they can be summarized from the following two aspects .",
    "* companies as sources .",
    "+ look at the table row by row .",
    "perhaps the most conspicuous feature is that the whole cvs row is significant .",
    "next to it is xom , with only three entries insignificant .",
    "that is to say , cvs has been found causal to all other stocks , though the causality magnitudes are yet to be assessed ( see below ) .",
    "this does make sense . as a chain of convenience stores ,",
    "cvs connects most of the general consumers and commodities and hence the corresponding industries . for xom , it is also understandable that why it makes a source of causality .",
    "oil or gas is for sure one of the most fundamental component in the american economy .",
    "* companies as recipients .",
    "+ examining column by column , the most outstanding stock is again cvs , with only one entry insignificant .",
    "that is to say , cvs is influenced by all other stocks except xom .",
    "following cvs is xom , wmt , and intc .",
    "the ibm and msft columns form the third tier .",
    "a few words regarding the stock f. as a cause to other stocks ( though causality maybe tiny ) , xom has not been identified to be causal to f. in fact , f has not been found causal to xom , either . this is a little surprising ; the reason(s ) can be found only after a careful analysis of ford , which is beyond the scope here .",
    "( in fact , computation does reveal information flows between xom and toyota . )",
    "interestingly , @xmath149 .",
    "this is easy to understand , as we rely on our motor vehicles to shop at wal - mart , while cvs stores could be just somewhere in the neighborhood !",
    ".the rates of absolute information flow between the 9 chosen stocks ( in @xmath144 nats per trading day ) . at each entry",
    "the direction is from the row index to the column index of the matrix .",
    "also listed are the standard errors at a 90% significance level , and highlighted are the significant flows . [ cols=\"^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     it should be noted that the causal relations generally change with time .",
    "if the series are long enough , we may look at how these information flows may vary from period to period . pick the pair ( ibm ,  ge ) as an example . for the duration ( march 1986 through present )",
    "considered above , @xmath150 , while @xmath151 is not significant .",
    "neither @xmath152 nor @xmath153 reaches 1% . since from the yahoo site both ge and ibm",
    "can be dated back to january 2 , 1962 , we can extend the time series a lot up to 13338 data points . shown in fig .",
    "[ fig : ibm_ge]a are the series of their historic prices , and in fig .",
    "[ fig : ibm_ge]b and [ fig : ibm_ge ] c are the corresponding log - returns .        computation of information flows with the whole series ( 13338 points ) results in @xmath154 and @xmath155 , and @xmath156 , @xmath157  nats / day , both being significant at the 90% level .",
    "this is very different from what are shown in tables  [ tab : stocks ] and [ tab : stocks_norm ] , with the causal structure changed from a weak two - way causality to a stronger and more or less one - way causality .",
    "since in the above only the data of the recent 30 years are used , we expect that in the early years this causal structure could be much enhanced .",
    "choose the first 7000 points ( from january 1962 through november 1989 ) , the computed relative information flow rates are : @xmath158 where the units for the latter pair are in @xmath144  nats / day , same below in this section .",
    "further narrow down the period to 2250 - 3250 ( corresponding to the period 1971 - 1975 ) , then @xmath159 attaining the maximum of @xmath153 , in contrast to the insignificant flow in table  [ tab : stocks ] .",
    "obviously , during this period , the causality can be viewed as one - way , i.e. , from ibm to ge .",
    "and the relative flow makes more than 5% , much larger than those in table  [ tab : stocks_norm ] .    the above remarkable causal structure for that particular period actually can trace its reason back in the history of ge@xcite .",
    "there is such a period in 1960 s when `` seven dwarfs '' ( burroughs , sperry rand , control data , honeywell , general electric , rca and ncr ) competed with ibm the giant for computer business , and , particularly , to build mainframes . in 1965",
    ", ge had only a 3.7-percent market share of the industry , though it was then dubbed as the `` king of the dwarfs '' , while ibm had 65.3% share .",
    "historically ge was once the largest computer user outside the us federal government ; it got into computer manufacturing to avoid dependency on others . and , indeed ,",
    "throughout the 60s , the causalities between ge and ibm are not significant . then , why , as time entered 70s , was the information flow from ibm to ge suddenly increased to its highest level ?",
    "it turned out that ge sold its computer division to honeywell in 1970 ; in the following years ( starting from 1971 ) , it relied much on the ibm products .",
    "this ge computer era , which has almost gone to oblivion , does substantiate the existence of a causation between ge and ibm , and , to be more precise , an essentially one - way causation from ibm to ge . in this sense",
    ", our formalism is well validated .",
    "to assess the importance of a flow of information from a series , say , @xmath160 , to another , say , @xmath161 , it needs to be normalized .",
    "the normalization can not follow a way as that in computing a correlation coefficient , since there is no such a theorem like the cauchy - schwarz inequality for it to base .",
    "getting down to the fundamentals , we were able to distinguish three types of mechanisms that contribute to the evolution of the marginal entropy of @xmath0 , @xmath2 ,      similarly there are three such quantities for @xmath1 , as schematized in fig .",
    "[ fig : schem_info_flow ] .",
    "we hence proposed that the normalization can be fulfilled as follows : @xmath163 obviously , a normalized flow tells its importance relative to other mechanisms within its own series .",
    "in other words , the two flows are normalized differently , echoing the property of asymmetry which makes information flow analysis distinctly different from those such as correlation analysis or mutual information analysis .",
    "the above normalizer can be accurately obtained in the framework of a dynamical system .",
    "when only two equi - distanced series , say , @xmath0 and @xmath1 , are given , with a linear model its constituents can be estimated as follows , in a nutshell , @xmath164 } ,   \\\\      & & t_{2\\to1 } = \\frac { c_{11}c_{12}c_{2,d1 } - c_{12}^2c_{1,d1 } }                 { c_{11}^2 c_{22 } - c_{11 } c_{12}^2 } ,      \\end{aligned}\\ ] ] where @xmath91 is the time stepsize , @xmath165 @xmath22 the sample covariance between @xmath23 and @xmath24 , @xmath25 the sample covariance between @xmath23 and @xmath26 , and @xmath166 ( @xmath40 ; but for chaotic series sampled at high resolution , @xmath30 may be needed ) .",
    "it should be noted that a relative information flow is for the comparison purpose within its own series .",
    "the two reverse flows between two series can only be compared in terms of absolute value , since they belong to different series . in this sense ,",
    "absolute and relative information flows should be examined simultaneously .",
    "this is clarified in the schematic diagram in fig .",
    "[ fig : schem_info_flow ] , and has been exemplified in the validations with two autoregressive processes .",
    "it is quite normal that two identical information flows may differ a lot in relative importance with respect to their own series , as testified in our realistic applications . in some extreme situation",
    ", a pair of equal flows may find one dominant but another negligible in their respective entropy balances .",
    "partly for demonstration and partly for verification , we have presented two applications .",
    "the first is a re - examination of the climate science problem previously studied in ref .",
    "@xcite . considering the fadeout of the recent portentous predictions of a `` super '' or `` monster '' el nio , we have particularly focused on the predictability of el nio .",
    "our result reconfirmed that the indian ocean sst is a source of uncertainty to the el  nio prediction .",
    "we further clarified that the information flow from the indian ocean is mainly through the indian ocean dipole ( iod ) .",
    "another realistic problem we have examined regards the causation between a few randomly picked american stocks .",
    "it is shown that many flows ( and hence causalities ) , though significant at a 90% level , their respective importances relative to other mechanisms are mostly negligible .",
    "the resulting matrices of absolute and relative information flows provide us a pattern of causality mostly understandable using our common sense .",
    "for example , ford has a larger influence on wal - mart than on cvs because people rely on motor vehicles to shop at wal - mart , while cvs could be just somewhere within a walking distance .",
    "a particularly interesting case is that we have identified a strong one - way causality from ibm to ge during the early stage of these companies .",
    "this has revealed to us the story of `` seven dwarfs '' competing ibm the giant for computer market . in an era",
    "when this story has almost gone to oblivion ( one even can not find it from ge s website ) , and ge may have left us an impression that it never built any computers , let alone a series of mainframes , this finding is indeed remarkable .      * acknowledgments .",
    "* this study was partially supported by jiangsu provincial government through the `` specially - appointed professor program '' ( jiangsu chair professorship ) to xsl , and by the national science foundation of china ( nsfc ) under grant no .",
    "41276032 .",
    "e.g. , k. hlav@xmath167kov - schindler , m. palu@xmath168 , m. vejmelka , j. bhattacharya , physics reports , 441(1 ) , 1 - 46 ( 2007 ) ; j. pearl , _ causality : models , reasoning , and inference_. mit press , cambridge , ma , 2nd edition ( 2009 ) ; m. lungarella , k. ishiguro , y. kuniyoshi , n. otsu , international journal of bifurcation and chaos , 17(3 ) , 903 - 921 ( 2007 ) ."
  ],
  "abstract_text": [
    "<S> recently , a rigorous yet concise formula has been derived to evaluate the information flow , and hence the causality in a quantitative sense , between time series . to assess the importance of a resulting causality </S>",
    "<S> , it needs to be normalized . </S>",
    "<S> the normalization is achieved through distinguishing three types of fundamental mechanisms that govern the marginal entropy change of the flow recipient . a normalized or relative flow measures its importance relative to other mechanisms . in analyzing realistic series , both absolute and relative information flows need to be taken into account , since the normalizers for a pair of reverse flows belong to two different entropy balances ; it is quite normal that two identical flows may differ a lot in relative importance in their respective balances . </S>",
    "<S> we have reproduced these results with several autoregressive models . </S>",
    "<S> we have also shown applications to a climate change problem and a financial analysis problem . </S>",
    "<S> for the former , reconfirmed is the role of the indian ocean dipole as an uncertainty source to the el  nio prediction . </S>",
    "<S> this might partly account for the unpredictability of certain aspects of el nio that has led to the recent portentous but spurious forecasts of the 2014 `` monster el  nio '' . </S>",
    "<S> for the latter , an unusually strong one - way causality has been identified from ibm ( international business machines corporation ) to ge ( general electric company ) in their early era , revealing to us an old story , which has almost gone to oblivion , about `` seven dwarfs '' competing a giant for the mainframe computer market . </S>"
  ]
}