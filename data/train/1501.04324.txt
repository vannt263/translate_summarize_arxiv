{
  "article_text": [
    "statistical language models estimating the distribution of various natural language phenomena are crucial for many applications . in machine translation",
    ", it measures the fluency and well - formness of a translation , and therefore is important for the translation quality , see  @xcite and  @xcite etc .",
    "common applications of lms include estimating the distribution based on n - gram coverage of words , to predict word and word orders , as in  @xcite and  @xcite .",
    "the independence assumption for each word is one of the simplifying method widely adopted .",
    "however , it does not hold in textual data , and underlying content structures need to be investigated as discussed in  @xcite .",
    "utf8gbsn    [ fig - example ]    we model the prediction of phrase and phrase orders . by considering all word sequences as phrases ,",
    "the dependency inside a phrase is preserved , and the phrase level structure of a sentence can be learned from observations .",
    "this can be considered as an n - gram model on the n - gram of words , therefore word based lm is a special case of phrase based lm if only single - word phrases are considered .",
    "intuitively our approach has the following advantages : 1 ) _ long distance dependency _ : the phrase based lm can capture the long distance relationship easily . to capture the sentence level dependency ,",
    "e.g. between the first and last word of the sentence in table  [ fig - example ] , we need a 7-gram word based lm , but only a 3-gram phrase based lm , if we take `` played the basketball '' and `` the day before yesterday '' as phrases .",
    "2 ) _ consistent translation unit with phrase based mt _ : some words may acquire meaning only in context , such as  day \" , or ",
    "the \" in  the day before yesterday \" in table  [ fig - example ] . considering",
    "the frequent phrases as single units will reduce the entropy of the language model .",
    "more importantly , current mt is performed on phrases , which is taken as the translation unit .",
    "the translation task is to predict the next phrase , which corresponds to the phrased based lm .",
    "3 ) _ fewer independence assumptions in statistical models _ : the sentence probability is computed as the product of the single word probabilities in the word based n - gram lm and the product of the phrase probabilities in the phrase based n - gram lm , given their histories . the less words / phrases in a sentence , the fewer mistakes the lm may contain due to less independence assumption on words / phrases .",
    "once the phrase segmentation is fixed , the number of elements via phrase based lm is much less than that via the word based lm .",
    "therefore , our approach is less likely to obtain errors due to assumptions .",
    "4 ) _ phrase boundaries as additional information _ : we consider different segmentation of phrases in one sentence as a hidden variable , which provides additional constraints to align phrases in translation .",
    "therefore , the constraint alignment in the blocks of words can provide more information than the word based lm .",
    "[ [ comparison - to - previous - work ] ] comparison to previous work + + + + + + + + + + + + + + + + + + + + + + + + + + +    in the dependency or structured lm , phrases corresponding to the grammars are considered , and dependencies are extracted , such as in  @xcite and in  @xcite . however , in the phrase based smt , even phrases violating the grammar structure may help as a translation unit .",
    "for instance , the partial phrase  the day before \" may appear both in  the day before yesterday \" and  the day before spring \" .",
    "most importantly , the phrase candidates in our phrase based lm are same as that in the phrase based translation , therefore are more consistent in the whole translation process , as mentioned in item 2 in section 1 .",
    "some researchers have proposed their phrase based lm for speech recognition . in  @xcite and  @xcite ,",
    "new phrases are added to the lexicon with different measure function . in  @xcite ,",
    "a different lm was proposed which derived the phrase probabilities from a language model built at the lexical level .",
    "nonetheless , these methods do not consider the dependency between phrases and the re - ordering problem , and therefore are not suitable for the mt application .",
    "we are given a sentence as a sequence of words @xmath0 , where @xmath1 is the sentence length . in the word based lm  @xcite , the probability of a sentence @xmath2 to denote general probability distributions with ( almost ) no specific assumptions .",
    "in contrast , for model - based probability distributions , we use the generic symbol @xmath3.]is defined as the product of the probabilities of each word given its previous @xmath4 words : @xmath5    the positions of phrase boundaries on a word sequence @xmath6 is indicated by @xmath7 and @xmath8 , where @xmath9 , and @xmath10 is the number of phrases in the sentence .",
    "we use @xmath11 to indicate that the @xmath12-th phrase segmentation is placed after the word @xmath13 and in front of word @xmath14 , where @xmath15 .",
    "@xmath16 is a boundary on the left side of the first word @xmath17 , which is defined as @xmath18 , and @xmath19 is always placed after the last word @xmath20 and therefore equals @xmath1 .",
    "an example is illustrated in table  [ fig - example ] .",
    "the english sentence ( @xmath6 ) contains seven words ( @xmath21 ) , where @xmath17 denotes  john \" , etc . the first phrase segmentation boundary is placed after the first word , and the second boundary is after the third word ( @xmath22 ) and so on .",
    "the phrase sequence @xmath23 in this sentence have a different order than that in its translation , on the phrase level .",
    "hence , the phrase based lm advances the word based lm in learning the phrase re - ordering .",
    "[ [ model - description ] ] ( 1 ) model description + + + + + + + + + + + + + + + + + + + + +    given a sequence of words @xmath6 and its phrase segmentation boundaries @xmath24 , a sentence can also be represented in the form of a sequence of phrases @xmath25@xmath26 , and each individual phrase @xmath27 is defined as @xmath28 in phrase based lm , we consider the phrase segmentation @xmath24 as hidden variable and the equation  [ eq - wordlm ] can be extended as follows : @xmath29    [ [ sentence - probability ] ] ( 2 ) sentence probability + + + + + + + + + + + + + + + + + + + + + + + +    for the segmentation prior probability , we assume a uniform distribution for simplicity , i.e. @xmath30 , where the number of different @xmath31 , i.e. @xmath32 if not considering the maximum phrase or phrase n - gram length ; to compute the @xmath33 , we consider either two approaches :    * sum model ( baum - welch ) + we consider all @xmath34 segmentation candidates .",
    "equation  [ eq - plm ] is defined as @xmath35 * max model ( viterbi ) + the sentence probability formula of the second model is defined as @xmath36 in practice we select the segmentation that maximizes the perplexity of the sentence instead of the probability to consider the length normalization .",
    "[ [ perplexity ] ] ( 3 ) perplexity + + + + + + + + + + + + + +    sentence perplexity and text perplexity in the sum model use the same definition as that in the word based lm .",
    "sentence perplexity in the max model is defined as @xmath37^{-1/j}\\nonumber\\ ] ] .",
    "[ [ parameter - estimation ] ] ( 4 ) parameter estimation + + + + + + + + + + + + + + + + + + + + + + + +    we apply maximum likelihood to estimate probabilities in both sum model and max model : @xmath38 where @xmath39 is the frequency of a phrase .",
    "the uni - gram phrase probability is @xmath40 , and @xmath41 is the frequency of all single phrases , in the training text .",
    "since we generate exponential number of phrases to the sentence length , the number of parameters is huge .",
    "therefore , we set the maximum n - gram length on the phrase level ( note not the phrase length ) as @xmath42 in experiments .",
    "[ [ smoothing ] ] ( 5 ) smoothing + + + + + + + + + + + + +    for the unseen events , we perform good - turing smoothing as commonly done in word based lms . moreover , we interpolate between the phrase probability and the product of single word probabilities in a phrase using a convex optimization:@xmath43 where phrase @xmath27 is made up of @xmath44 words @xmath45 .",
    "the idea of this interpolation is to make the probability of a phrase consisting of of @xmath44 words smooth with a @xmath44-word unigram probability after normalization . in our experiments",
    ", we set @xmath46 for convenience .",
    "[ [ algorithm - of - calculating - phrase - n - gram - counts ] ] ( 6 ) algorithm of calculating phrase n - gram counts + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the training task is to calculate n - gram counts on the phrase level in equation  [ eq - count1 ] . given a training corpus @xmath47 , where there are @xmath48 sentences @xmath49 ( @xmath50 ) , our goal is to to compute @xmath39 , for all phrase",
    "n - grams that the number of phrases is no greater than @xmath51 .",
    "therefore , for each sentence @xmath6 , we should find out every @xmath52-gram phrases that @xmath53 .",
    "we do dynamic programming to collect the phrase n - grams in one sentence @xmath6 : @xmath54 where @xmath55 is the auxiliary function denoting the multiset of all phrase n - grams or unigram ending at position @xmath56 ( @xmath57 ) .",
    "@xmath58 denotes the starting word position of the last phrase in the multiset .",
    "the @xmath59 is a multiset , and @xmath60 means to append the element to each element in the multiset .",
    "@xmath61 denotes the union of multisets . after appending @xmath62",
    ", we consider all @xmath58 that is no less than @xmath52 and no greater than @xmath56 .",
    "the phrase counts @xmath39 is the sum of all phrase n - grams from all sentences @xmath47 , with each sentence @xmath63 , and @xmath64 is the number of elements in a multiset : @xmath65",
    "this is an ongoing work , and we performed preliminary experiments on the iwslt  @xcite task , then evaluated the lm performance by measuring the lm perplexity and the mt translation performance . because of the computational requirement , we only employed sentences which contain no more than 15 words in the training corpus and no more than 10 words in the test corpora ( dev2010 , on tst2010 and on tst2011 ) , as shown in table  [ tab - data ] .",
    "we took word based lm in equation  [ eq - wordlm ] as the baseline method ( base ) .",
    "we calculated the perplexities of tst2011 with different n - gram orders using both sum model and max model , with and without smoothing ( s. ) as in section  2 .",
    "table  [ tab - ppl ] shows that perplexities in our approaches are all lower than those in the baseline . for mt , we selected the single best translation output based on the lm perplexity of the 100-best translation candiates , using different lms as shown in table  [ tab - bleu ]",
    ". max  model along with smoothing outperforms the baseline method under all three test sets with the bleu score  @xcite increase of 0.3% on dev2010 , 0.45% on tst2010 , and 0.22% on tst2011 , respectively .",
    "table  [ tab - outputs ] shows two examples from the tst2010 , where we can see that our max model generates better selection results than the baseline method in these cases .",
    "we showed the preliminary results that a phrase based lm can improve the performance of mt systems and the lm perplexity .",
    "we presented two phrase based models which consider phrases as the basic components of a sentence and perform exhaustive search .",
    "our future work will focus on the efficiency for a larger data track as well as the improvements on the smoothing methods ."
  ],
  "abstract_text": [
    "<S> we consider phrase based language models ( lm ) , which generalize the commonly used word level models . </S>",
    "<S> similar concept on phrase based lms appears in speech recognition , which is rather specialized and thus less suitable for machine translation ( mt ) . </S>",
    "<S> in contrast to the dependency lm , we first introduce the exhaustive phrase - based lms tailored for mt use . </S>",
    "<S> preliminary experimental results show that our approach outperform word based lms with the respect to perplexity and translation quality . </S>"
  ]
}