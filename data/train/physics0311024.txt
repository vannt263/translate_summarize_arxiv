{
  "article_text": [
    "density functional theory ( dft ) converts many - electron problems into effective one - electron problems .",
    "this conversion is rigorous if the exact exchange - correlation functional is known .",
    "it is thus important to find the accurate dft exchange - correlation functionals .",
    "much progress has been made , primarily due to the development of generalized gradient approximation ( gga )  @xcite and hybrid functionals  @xcite .",
    "existing exchange - correlation functionals include local or nearly local contributions such as local spin density approximation ( lsda )  @xcite and gga  @xcite , and nonlocal terms , for instance , exact exchange functional .",
    "although these local and nonlocal terms account for the bulk contributions to exact exchange - correlation functional , high - order contributions are yet to be identified and taken into account . conceding that it is exceedingly difficult to derive analytically the exact universal exchange - correlation functional , we resort to an entirely different approach .    an important methodology in the development of exchange - correlation functionals",
    "has been established by utilizing highly accurate experimental data to determine exchange - correlation functionals  @xcite .",
    "becke pioneered this semiempirical approach and determined the three parameters in b3lyp functional  @xcite by a least - square fit to 116 molecular and atomic energy data  @xcite . building upon this semiempirical methodology , we propose here a new approach which takes into account of high - order contributions beyond the existing local and nonlocal exchange - correlation functionals .",
    "since its beginning in the late fifties , neural networks has been applied to various engineering problems , such as robotics , pattern recognition , and speech  @xcite .",
    "a neural network is a highly nonlinear system , and is suitable to determine or mimic the complex relationships among relevant physical variables .",
    "recently we developed a combined first principles calculation and neural - networks correction approach to improve significantly the accuracy of calculated thermodynamic properties  @xcite . in this work",
    ", we develop a neural - networks - based approach to construct the dft exchange - correlation functional and apply it to improve the results of the popular b3lyp calculations . in section ii we describe the neural - networks - based methodology and report our work leading to improved b3lyp calculations .",
    "the results of the improved b3lyp calculations and their comparisons to the experimental data are given in section iii .",
    "further discussion is given in section iv .",
    "b3lyp functional is a hybrid functional composed of several local and nonlocal exchange and correlation contributions , and can be expressed as    @xmath3    where e@xmath4 is the local spin density exchange functional  @xcite , e@xmath5 is the exact exchange functional , e@xmath6 is becke s gradient - corrected exchange functional  @xcite , e@xmath7 is the correlation functional of lee , yang and parr  @xcite , and e@xmath8 represents the correlation functional proposed by vosko , wilk and nusair  @xcite .",
    "the values of its three parameters , @xmath9 , @xmath10 and @xmath11 , dictate the contributions of various terms .",
    "they have been determined via the least - square fit to the 116 atomization energies ( aes ) , ionization potentials ( ips ) , proton affinities ( pas ) and total atomic energies ( taes ) by becke  @xcite , and are 0.80 , 0.72 and 0.81 , respectively .",
    "note that @xmath10@xmath12@xmath9@xmath12@xmath11 .",
    "b3lyp functional explicitly consists of the first and second rungs of the jacob s ladder of density functional approximation  @xcite and the partial exact exchange functional  @xcite .",
    "being determined via the least - square fit to the 116 experimental data , b3lyp functional includes implicitly the high - order contributions to the exact functional such as those in the meta - gga functional  @xcite .",
    "these high - order contributions are averaged over the 116 energy data  @xcite , and their functional forms or the values of @xmath9 , @xmath10 and @xmath11 are assumed invariant for all types of atomic or molecular systems . since high - order contributions to the exact exchange - correlation energy are in fact system - dependent , their inclusion in eq .",
    "( [ eq : one ] ) leads to the system - dependence of a@xmath13 , a@xmath14 and a@xmath15 which is in turn dictated by the characteristic properties of the system .",
    "the challenge is to identify these characteristic properties , and more importantly , to determine their quantitative relationships to the values of a@xmath13 , a@xmath14 and a@xmath15 .",
    "these characteristic properties , termed as the physical descriptors of the system , satisfy two criteria : ( 1 ) they must be of purely electronic nature , since the exact exchange - correlation functional is a universal functional of electron density only ; and ( 2 ) they should reflect the electron distribution . after identifying these physical descriptors that are related to the high - order contributions to the exchange - correlation functional , we employ neural networks to determine their quantitative relationships to a@xmath13 , a@xmath14 and a@xmath15 . instead of being taken as a system - dependent semiempirical functional",
    ", the resulting neural network can be viewed as a generalized universal exchange - correlation functional .",
    "it can be systematically improved upon the availability of new experimental data .    beyond the gga , perdew and co - workers",
    "@xcite proposed the meta - gga in which the exchange - correlation functional depends explicitly on the kinetic energy density of the occupied kohn - sham orbitals , @xmath16 where @xmath17 is the wave function of an occupied kohn - sham orbital @xmath18 .",
    "the total kinetic energy of the electronic system , @xmath19 , should relate closely to the high - order contributions to b3lyp functional , and is thus chosen as a key physical descriptor .",
    "the exchange - correlation functional is uniquely determined by the electron density distribution @xmath20 .",
    "@xmath20 can be expanded in terms of the multipole moments .",
    "being the zeroth - order term of the expansion , the total number of electrons @xmath21 is recognized as a natural physical descriptor , and the dipole and quadrupole moments of the system are selected as another two descriptors .",
    "we use the magnitude of the dipole moment @xmath22 for the dipole descriptor , where @xmath23 is a component of the dipole vector . for the quadrupole descriptor , we choose @xmath24 , where @xmath25 is a diagonal element of the quadrupole tensor .",
    "the exchange functional accounts for the exchange interaction among the electrons of the same spin .",
    "spin multiplicity @xmath26 is thus adopted as a physical descriptor as well .",
    "our neural network adopts a three - layer architecture which consists of an input layer , a hidden layer and an output layer  @xcite .",
    "the values of the physical descriptors , @xmath26 , @xmath21 , d , @xmath27 and q , are inputted into the neural network at the input layer .",
    "the values of the modified @xmath9 , @xmath10 and @xmath11 for each atom or molecule , denoted by @xmath28 , @xmath29 and @xmath30 , are obtained at the output layer .",
    "different layers are connected via the synaptic weights  @xcite . the neural network structure such as the number of hidden neurons at the hidden layer is to be determined .",
    "we take the 116 experimental energies that were employed by becke  @xcite as our training set , and they are utilized to determine the structure of our neural network and its synaptic weights . instead of the basis - set - free calculations carried out by becke  @xcite",
    ", we adopt a gaussian - type - function ( gtf ) basis set , 6 - 311+g(3_df_,2_p _ ) , in our calculations .",
    "geometry of every molecule is optimized directly using conventional b3lyp/6 - 311+g(3_df_,2_p _ ) .",
    "the values of @xmath31 , d and q are obtained at the same level of calculations . besides @xmath26 , @xmath21 ,",
    "d , @xmath27 and q , a bias is introduced as another input and its value is set to @xmath32 in all cases .",
    "the values of @xmath28 , @xmath29 and @xmath30 vary from system to system , and are used to modify the b3lyp functional for each atom or molecule . the modified b3lyp functional is subsequently used to evaluate its ae , ip , pa , or tae .",
    "the resulting energies are then compared to their experimental counterparts , and the comparison is used to tune the synaptic weights of our neural network .",
    "the process is iterated until the differences between the calculated and measured energies are small enough for all the molecules or atoms in the training set , and the neural network is then considered as converged , _",
    "i.e. _ , its synaptic weights are determined .",
    "[ fig.1 ]    the structure and synaptic weights of our neural network are optimized via a cross - validation technique  @xcite .",
    "the 116 energy values are randomly partitioned into six subsets of equal size .",
    "five of them are used to train the weights of the neural network , and are termed as the estimation subset .",
    "the sixth is used to compare the prediction of current neural network , and is termed as the validation subset .",
    "this procedure is repeated six times in rotation to assess the performance of current neural network .",
    "the number of neurons in the hidden layer is varied from 1 to 5 to decide the optimal structure of the neural network .",
    "we find that the hidden layer containing two neurons yields the best overall results , _",
    "i.e. _ , the minimal root - mean - square ( rms ) errors and the minimal rms difference between the estimation and validation subsets ( less than 0.2 kcal@xmath1mol@xmath2 ) . minimizing the rms difference between the estimation and validation subsets",
    "helps ensure the predictive capability of our neural network .",
    "therefore , the 6 - 3 - 3 structure is adopted for our neural network , see fig .",
    "[ fig.1 ] .",
    "the input values at the input layer , @xmath33 , @xmath34 , @xmath35 , @xmath36 , @xmath37 and @xmath38 are @xmath26 , @xmath21 , d , @xmath27 , q and bias , respectively .",
    "except for the bias , input values are scaled before being inputted into the neural network as follows , @xmath39 where @xmath40 and @xmath41 are two constants between 0 and 1 that set the upper and lower boundaries , @xmath42 and @xmath43 are the values of the physical descriptor before and after the scaling , and @xmath44 and @xmath45 are the maximum and minimum values of the descriptor ( @xmath46=1 - 5 ) . in our neural network",
    "we adopt @xmath47 and @xmath48 , therefore all the inputs @xmath43 are within the interval [ 0.1 , 0.9 ] .",
    "the biases are introduced at both the input and hidden layers and their value are set to unity . the synaptic weights @xmath49 connect the input layer @xmath50 and the hidden neurons @xmath51 , and @xmath52 connect the hidden neurons and the output .",
    "the corrected @xmath28 , @xmath29 and @xmath30 are given at the output layer , and they are related to the input @xmath50 as @xmath53+w'_{10 }    \\}\\\\ \\tilde{a}_x & = & sigb\\{[\\sum_{j=1}^{2}w'_{2j}{\\cdot}siga ( \\sum_{i=0}^{5}w_{ji } x_i ) ] + w'_{20 }    \\}\\\\ \\tilde{a}_c & = & sigb\\{[\\sum_{j=1}^{2}w'_{3j}{\\cdot}siga ( \\sum_{i=0}^{5}w_{ji } x_i ) ] + w'_{30 }    \\}\\end{aligned}\\ ] ] where @xmath54 and @xmath55 , and @xmath18 and @xmath56 are the parameters that control the switch steepness of sigmoidal functions @xmath57 and @xmath58 .",
    "an error back - propagation learning procedure  @xcite is used to optimize the values of @xmath59 and @xmath60 ( @xmath46=0 - 5 , and @xmath61=0 - 2 .",
    "zero indices are referred to the biases ) .",
    "@p0.2inp0.4inp0.4inp0.4inp0.4inp0.4inp0.4inp0.4inp0.4inp0.4 in     +   + no . & name & g@xmath62 & nt &  d   & @xmath31   & q   & @xmath28 & @xmath29 & @xmath30 + & & & &  ( db ) &  ( a.u . ) &  ( db@xmath1 ) & & & +   + & + no . & name & g@xmath62 & nt &  d   & @xmath31   & q   & @xmath28 & @xmath29 & @xmath30 + & & & &  ( db ) &  ( a.u . ) &  ( db@xmath1 ) & & & + & + 1 & h@xmath63 & 1 & 2 & 0.00 & 1.83 & 3.35 & 0.779 & 0.726 & 0.906 + 2 & lih & 1 & 4 & 5.72 & 8.98 & 10.59 & 0.788 & 0.737 & 0.911 + 3 & beh & 2 & 5 & 0.29 & 16.73 & 14.41 & 0.767 & 0.722 & 0.927 + 4 & ch & 2 & 7 & 1.48 & 41.17 & 12.51 & 0.771 & 0.726 & 0.927 + 5 & ch@xmath63(@xmath64b@xmath65 ) & 3 & 8 & 0.61 & 45.29 & 12.98 & 0.752 & 0.714 & 0.939 + 6 & ch@xmath63(@xmath66a@xmath65 ) & 1 & 8 & 1.81 & 44.98 & 13.80 & 0.789 & 0.737 & 0.909 + 7 & ch@xmath67 & 2 & 9 & 0.00 & 49.28 & 13.80 & 0.771 & 0.727 & 0.927 + 8 & ch@xmath68 & 1 & 10 & 0.00 & 53.68 & 14.70 & 0.789 & 0.737 & 0.908 + 9 & nh & 3 & 8 & 1.54 & 58.63 & 10.93 & 0.753 & 0.715 & 0.939 + 10 & nh@xmath63 & 2 & 9 & 1.82 & 63.23 & 12.10 & 0.773 & 0.729 & 0.927 + 11 & nh@xmath67 & 1 & 10 & 1.53 & 68.24 & 13.14 & 0.791 & 0.739 & 0.909 + 12 &",
    "oh & 2 & 9 & 1.68 & 79.89 & 10.08 & 0.774 & 0.729 & 0.927 + 13 & oh@xmath63 & 1 & 10 & 1.91 & 85.35 & 11.07 & 0.791 & 0.740 & 0.909 + 14 & fh & 1 & 10 & 1.85 & 105.36 & 9.13 & 0.792 & 0.740 & 0.908 + 15 & li@xmath63 & 1 & 6 & 0.00 & 16.62 & 22.29 & 0.786 & 0.735 & 0.911 + 16 & lif & 1 & 12 & 6.22 & 116.17 & 10.65 & 0.797 & 0.746 & 0.911 + 17 & c@xmath63h@xmath63 & 1 & 14 & 0.00 & 101.73 & 20.75 & 0.794 & 0.743 & 0.910 + 18 & c@xmath63h@xmath68 & 1 & 16 & 0.00 & 111.54 & 23.65 & 0.796 & 0.746 & 0.910 + 19 & c@xmath63h@xmath69 & 1 & 18 & 0.00 & 121.45 & 26.44 & 0.798 & 0.748 & 0.911 + 20 & cn & 2 & 13 & 1.38 & 111.45 & 18.85 & 0.779 & 0.736 & 0.928 + 21 & hcn & 1 & 14 & 3.04 & 117.02 & 19.43 & 0.797 & 0.747 & 0.911 + 22 & co & 1 & 14 & 0.10 & 135.49 & 19.13 & 0.795 & 0.744 & 0.910 + 23 & hco & 2 & 15 & 1.67 & 140.01 & 19.86 & 0.782 & 0.739 & 0.928 + 24 & h@xmath63co & 1 & 16 & 2.41 & 145.41 & 20.69 & 0.799 & 0.748 & 0.911 + 25 & h@xmath67coh & 1 & 18 & 1.69 & 155.46 & 22.83 & 0.800 & 0.750 & 0.911 + 26 & n@xmath63 & 1 & 14 & 0.00 & 132.87 & 18.79 & 0.795 & 0.744 & 0.909 + 27 & h@xmath63nnh@xmath63 & 1 & 18 & 1.93 & 152.86 & 22.79 & 0.800 & 0.750 & 0.911 + 28 & no & 2 & 15 & 0.14 & 155.35 & 18.57 & 0.781 & 0.737 & 0.927 + 29 & o@xmath63 & 3 & 16 & 0.00 & 178.05 & 17.78 & 0.766 & 0.727 & 0.939 + 30 & hooh & 1 & 18 & 0.00 & 187.79 & 19.53 & 0.799 & 0.748 & 0.909 + 31 & f@xmath63 & 1 & 18 & 0.00 & 229.74 & 16.40 & 0.800 & 0.749 & 0.909 + 32 & co@xmath63 & 1 & 22 & 0.00 & 246.26 & 28.63 & 0.804 & 0.754 & 0.912 + 33 & sih@xmath63(@xmath66a@xmath65 ) & 1 & 16 & 0.09 & 300.05 & 27.42 & 0.802 & 0.753 & 0.913 + 34 & sih@xmath63(@xmath64b@xmath65 ) & 3 & 16 & 0.07 & 300.26 & 27.00 & 0.773 & 0.735 & 0.940 + 35 & sih@xmath67 & 2 & 17 & 0.00 & 306.45 & 28.06 & 0.790 & 0.747 & 0.930 + 36 & sih@xmath68 & 1 & 18 & 0.00 & 312.63 & 28.84 & 0.803 & 0.754 & 0.913 + 37 & ph@xmath63 & 2 & 17 & 0.52 & 353.40 & 26.07 & 0.791 & 0.749 & 0.930 + 38 & ph@xmath67 & 1 & 18 & 0.57 & 360.14 & 27.16 & 0.805 & 0.756 & 0.913 + 39 & sh@xmath63 & 1 & 18 & 1.00 & 411.71 & 25.00 & 0.806 & 0.757 & 0.914 + 40 & clh & 1 & 18 & 1.12 & 466.75 & 22.63 & 0.807 & 0.758 & 0.914 + 41 & na@xmath63 & 1 & 22 & 0.00 & 344.67 & 33.68 & 0.807 & 0.758 & 0.914 + 42 & si@xmath63 & 3 & 28 & 0.00 & 625.88 & 46.53 & 0.796 & 0.760 & 0.942 + 43 & p@xmath63 & 1 & 30 & 0.00 & 744.63 & 45.05 & 0.817 & 0.771 & 0.919 + 44 & s@xmath63 & 3 & 32 & 0.00 & 866.42 & 44.07 & 0.804 & 0.768 & 0.942 + 45 & cl@xmath63 & 1 & 34 & 0.00 & 994.10 & 42.53 & 0.821 & 0.775 & 0.920 + 46 & nacl & 1 & 28 & 8.74 & 662.67 & 30.03 & 0.817 & 0.772 & 0.919 + 47 & sio & 1 & 22 & 3.21 & 403.18 & 30.97 & 0.809 & 0.761 & 0.915 + 48 & sc & 1 & 22 & 1.99 & 468.48 & 33.25 & 0.810 & 0.763 & 0.916 + 49 & so & 3 & 24 & 1.55 & 518.11 & 31.14 & 0.789 & 0.752 & 0.941 + 50 & clo & 2 & 25 & 1.33 & 579.64 & 30.57 & 0.803 & 0.762 & 0.931 + 51 & fcl & 1 & 26 & 0.91 & 607.86 & 29.71 & 0.813 & 0.766 & 0.915 + 52 & si@xmath63h@xmath69 & 1 & 34 & 0.00 & 671.93 & 55.02 & 0.818 & 0.772 & 0.920 + 53 & ch@xmath67cl & 1 & 26 & 1.95 & 549.86 & 33.84 & 0.813 & 0.766 & 0.916 + 54 & h@xmath67csh & 1 & 26 & 1.55 & 494.08 & 36.34 & 0.812 & 0.765 & 0.916 + 55 & hocl & 1 & 26 & 1.55 & 585.49 & 30.94 & 0.813 & 0.766 & 0.916 + 56 & so@xmath63 & 1 & 32 & 1.71 & 655.49 & 41.21 & 0.817 & 0.771 & 0.918 + 57 & h & 2 & 1 & 0.00 & 0.50 & 2.41 & 0.760 & 0.714 & 0.925 + 58 & he & 1 & 2 & 0.00 & 2.87 & 1.87 & 0.779 & 0.725 & 0.906 + 59 & li & 2 & 3 & 0.00 & 7.43 & 13.87 & 0.764 & 0.720 & 0.927 + 60 & be & 1 & 4 & 0.00 & 14.59 & 13.05 & 0.783 & 0.731 & 0.909 + 61 & b & 2 & 5 & 0.00 & 24.57 & 12.61 & 0.766 & 0.722 & 0.927 + 62 & c & 3 & 6 & 0.00 & 37.76 & 11.06 & 0.749 & 0.710 & 0.939 + 63 & n & 4 & 7 & 0.00 & 54.49 & 9.68 & 0.731 & 0.698 & 0.946 + 64 & o & 3 & 8 & 0.00 & 75.09 & 9.06 & 0.752 & 0.713 & 0.938 + 65 & f & 2 & 9 & 0.00 & 99.53 & 8.28 & 0.772 & 0.727 & 0.926 + 66 & ne & 1 & 10 & 0.00 & 128.66 & 7.55 & 0.791 & 0.738 & 0.907 + 67 & na & 2 & 11 & 0.00 & 161.83 & 19.78 & 0.779 & 0.735 & 0.928 + 68 & mg & 1 & 12 & 0.00 & 199.57 & 21.87 & 0.796 & 0.746 & 0.911 + 69 & al & 2 & 13 & 0.00 & 241.93 & 26.04 & 0.784 & 0.741 & 0.929 + 70 & si & 3 & 14 & 0.00 & 289.39 & 25.10 & 0.770 & 0.733 & 0.940 + 71 & p & 4 & 15 & 0.00 & 340.77 & 23.66 & 0.755 & 0.722 & 0.947 + 72 & s & 3 & 16 & 0.00 & 397.58 & 22.91 & 0.776 & 0.738 & 0.940 + 73 & cl & 2 & 17 & 0.00 & 459.07 & 21.73 & 0.794 & 0.751 & 0.929 + 74 & ar & 1 & 18 & 0.00 & 526.13 & 20.36 & 0.807 & 0.759 & 0.913 + 75 & ph & 3 & 16 & 0.40 & 346.54 & 24.90 & 0.774 & 0.737 & 0.940 + 76 & sh & 2 & 17 & 0.77 & 403.69 & 23.98 & 0.793 & 0.750 & 0.930 + 77 & h@xmath70 & 1 & 0 & 0.00 & 0.00 & 0.00 & 0.777 & 0.723 & 0.905 + 78 & he@xmath70 & 2 & 1 & 0.00 & 1.99 & 0.60 & 0.759 & 0.714 & 0.925 + 79 & li@xmath70 & 1 & 2 & 0.00 & 7.22 & 0.70 & 0.779 & 0.725 & 0.905 + 80 & be@xmath70 & 2 & 3 & 0.00 & 14.26 & 4.99 & 0.763 & 0.717 & 0.926 + 81 & b@xmath70 & 1 & 4 & 0.00 & 24.25 & 6.11 & 0.782 & 0.729 & 0.907 + 82 & c@xmath70 & 2 & 5 & 0.00 & 37.34 & 6.39 & 0.766 & 0.720 & 0.926 + 83 & n@xmath70 & 3 & 6 & 0.00 & 53.96 & 6.17 & 0.748 & 0.710 & 0.938 + 84 & o@xmath70 & 4 & 7 & 0.00 & 74.46 & 5.82 & 0.731 & 0.698 & 0.946 + 85 & f@xmath70 & 3 & 8 & 0.00 & 98.93 & 5.67 & 0.752 & 0.713 & 0.938 + 86 & ne@xmath70 & 2 & 9 & 0.00 & 127.90 & 5.42 & 0.773 & 0.728 & 0.926 + 87 & na@xmath70 & 1 & 10 & 0.00 & 161.63 & 5.09 & 0.791 & 0.739 & 0.907 + 88 & mg@xmath70 & 2 & 11 & 0.00 & 199.29 & 10.60 & 0.778 & 0.734 & 0.927 + 89 & al@xmath70 & 1 & 12 & 0.00 & 241.71 & 13.08 & 0.796 & 0.745 & 0.909 + 90 & si@xmath70 & 2 & 13 & 0.00 & 288.61 & 15.64 & 0.784 & 0.740 & 0.928 + 91 & p@xmath70 & 3 & 14 & 0.00 & 340.42 & 16.27 & 0.770 & 0.733 & 0.940 + 92 & s@xmath70 & 4 & 15 & 0.00 & 397.23 & 16.07 & 0.756 & 0.723 & 0.947 + 93 & cl@xmath70 & 3 & 16 & 0.00 & 458.63 & 16.14 & 0.777 & 0.739 & 0.940 + 94 & ar@xmath70 & 2 & 17 & 0.00 & 525.55 & 15.13 & 0.795 & 0.752 & 0.929 + 95 & ch@xmath68@xmath70 & 2 & 9 & 0.01 & 52.90 & 8.23 & 0.770 & 0.725 & 0.926 + 96 & nh@xmath67@xmath70 & 2 & 9 & 0.00 & 67.71 & 7.32 & 0.771 & 0.725 & 0.926 + 97 & oh@xmath70 & 3 & 8 & 2.02 & 79.19 & 6.24 & 0.754 & 0.715 & 0.939 + 98 & oh@xmath63@xmath70 & 2 & 9 & 2.12 & 84.52 & 6.77 & 0.774 & 0.729 & 0.926 + 99 & fh@xmath70 & 2 & 9 & 2.36 & 104.35 & 6.01 & 0.775 & 0.730 & 0.926 + 100 & sih@xmath68@xmath70 & 2 & 17 & 1.21 & 310.53 & 18.93 & 0.789 & 0.746 & 0.929 + 101 & ph@xmath70 & 2 & 15 & 0.62 & 346.56 & 17.05 & 0.788 & 0.745 & 0.928 + 102 & ph@xmath63@xmath70 & 1 & 16 & 0.80 & 353.00 & 17.79 & 0.803 & 0.753 & 0.911 + 103 & ph@xmath67@xmath70 & 2 & 17 & 0.35 & 359.87 & 18.05 & 0.790 & 0.747 & 0.928 + 104 & sh@xmath70 & 3 & 16 & 1.08 & 404.05 & 16.69 & 0.776 & 0.738 & 0.940 + 105 & sh@xmath63@xmath70(@xmath71b@xmath65 ) & 2 & 17 & 1.37 & 411.17 & 17.29 & 0.793 & 0.750 & 0.929 + 106 & sh@xmath63@xmath70(@xmath71a@xmath65 ) & 2 & 17 & 0.54 & 411.00 & 12.72 & 0.789 & 0.744 & 0.925 + 107 & clh@xmath70 & 2 & 17 & 1.53 & 466.10 & 16.58 & 0.794 & 0.751 & 0.929 + 108 & c@xmath63h@xmath63@xmath70 & 2 & 13 & 0.00 & 100.59 & 14.82 & 0.777 & 0.732 & 0.927 + 109 & c@xmath63h@xmath68@xmath70 & 2 & 15 & 0.00 & 110.31 & 15.16 & 0.779 & 0.734 & 0.926 + 110 & co@xmath70 & 2 & 13 & 2.73 & 135.32 & 12.48 & 0.781 & 0.736 & 0.927 + 111 & n@xmath63@xmath70(@xmath71@xmath72@xmath73 ) & 2 & 13 & 0.00 & 132.02 & 13.12 & 0.778 & 0.733 & 0.926 + 112 & n@xmath63@xmath70(@xmath71@xmath74@xmath75 ) & 2 & 13 & 0.00 & 130.77 & 13.95 & 0.777 & 0.731 & 0.924 + 113 & o@xmath63@xmath70 & 2 & 17 & 0.00 & 180.07 & 13.06 & 0.783 & 0.738 & 0.926 + 114 & p@xmath63@xmath70 & 2 & 29 & 0.00 & 741.28 & 33.44 & 0.808 & 0.767 & 0.932 + 115 & s@xmath63@xmath70 & 2 & 31 & 0.00 & 869.18 & 33.65 & 0.811 & 0.771 & 0.932 + 116 & cl@xmath63@xmath70 & 2 & 33 & 0.00 & 997.85 & 33.40 & 0.814 & 0.773 & 0.933 + 117 & fcl@xmath70 & 2 & 25 & 1.70 & 610.61 & 22.93 & 0.803 & 0.761 & 0.930 + 118 & sc@xmath70 & 2 & 21 & 0.52 & 469.27 & 23.20 & 0.797 & 0.754 & 0.929 + 119 & h@xmath67@xmath70 & 1 & 2 & 0.00 & 3.06 & 1.50 & 0.779 & 0.725 & 0.905 + 120 & c@xmath63h@xmath67@xmath70 & 1 & 14 & 0.98 & 107.01 & 15.19 & 0.794 & 0.743 & 0.909 + 121 & nh@xmath68@xmath70 & 1 & 10 & 0.00 & 72.98 & 7.54 & 0.789 & 0.736 & 0.906 + 122 & h@xmath67o@xmath70 & 1 & 10 & 0.00 & 90.43 & 7.32 & 0.789 & 0.737 & 0.906 + 123 & sih@xmath76@xmath70 & 1 & 18 & 1.30 & 317.11 & 19.69 & 0.803 & 0.754 & 0.911 + 124 & ph@xmath68@xmath70 & 1 & 18 & 0.00 & 366.99 & 18.64 & 0.804 & 0.754 & 0.911 + 125 & h@xmath67s@xmath70 & 1 & 18 & 1.48 & 418.61 & 17.84 & 0.806 & 0.756 & 0.912 + 126 & h@xmath63cl@xmath70 & 1 & 18 & 1.90 & 473.92 & 16.99 & 0.807 & 0.758 & 0.912 + [ table.1 ]    @xmath26 , @xmath21 , d , @xmath27 and q of each molecule or atom in the training set are listed in table  [ table.1 ] .",
    "the conventional b3lyp/6 - 311+g(3_df_,2_p _ ) calculations are carried out to evaluate aes , ips , pas or taes of the molecules and atoms in the training set , and the results are given in tables  [ table.2 ] ,  [ table.3 ] ,  [ table.4 ] and  [ table.5 ] , respectively . compared to the experimental data , the rms deviations are 3.0 , 4.9 , 1.6 and 10.3 kcal@xmath1mol@xmath2 for aes , ips , pas and taes , respectively .",
    "the physical descriptors of each molecule or atom in the training set are inputted to the neural network , and the @xmath28 , @xmath29 and @xmath30 from the output layer are used to construct the b3lyp functional which is used subsequently to calculate ae , ip , pa or tae .",
    "these values are then compared to the 116 energy values in the training set , and the synaptic weights @xmath49 and @xmath52 are tuned accordingly .",
    "the final values of synaptic weights are shown in tables  [ table.6 ] and  [ table.7 ] . in table",
    "[ table.8 ] we list the derivatives of @xmath28 , @xmath29 and @xmath30 with respect to @xmath43 ( @xmath46=0 - 5 ) .",
    "the magnitude of a derivative reflects the influence on @xmath28 , @xmath29 and @xmath30 of the corresponding physical descriptor .",
    "the larger the magnitude is , the more significant the physical descriptor is to determine the values of @xmath28 , @xmath29 and @xmath30 . derivatives in table  [ table.8 ] are obtained at @xmath77 ( @xmath46=1 - 5 ) and @xmath78 .",
    "we find that the spin multiplicity @xmath26 and total kinetic energy @xmath27 have the derivatives of the largest two magnitudes .",
    "similar results are observed at @xmath79 ( @xmath46=1 - 5 ) and @xmath78 , or @xmath80 ( @xmath46=1 - 5 ) and @xmath78 .",
    "hence @xmath81 and @xmath27 are identified as two most significant descriptors to determine the high - order components of @xmath28 , @xmath29 and @xmath30 .",
    "the final or optimal values of @xmath28 , @xmath29 and @xmath30 for each molecule or atom are listed in table  [ table.1 ] .",
    "note that their values are overall shifted from the original b3lyp values , while the order @xmath29@xmath12@xmath28@xmath12@xmath30 is kept for each molecule or atom .",
    "this overall shift is caused by the finite basis set .",
    "more importantly , their values are slightly different from each other .",
    "therefore , the resulting b3lyp functional is system - dependent .",
    "we list the neural - networks - corrected aes , ips , pas and taes in tables  [ table.2 ] ,  [ table.3 ] ,  [ table.4 ] and  [ table.5 ] , respectively . @xmath82 and @xmath83 in these tables are the differences between the calculated values and the experimental counterpart for the conventional b3lyp/6 - 311+g(3_df_,2_p _ ) and the neural - networks - based b3lyp/6 - 311+g(3_df_,2_p _ ) calculations , respectively . compared to their experimental counterparts , the rms deviations of neural - networks - based b3lyp/6 - 311+g(3_df_,2_p _ ) calculations",
    "are 2.4 , 3.7 , 1.6 and 2.7 kcal@xmath1mol@xmath2 for ae , ip , pa and tae , respectively , and are less than those of the conventional b3lyp/6 - 311+g(3_df_,2_p _ ) calculations  ( cf .",
    "table  [ table.9 ] ) .",
    "we note that the neural - networks - based b3lyp/6 - 311+g(3_df_,2_p _ ) calculations yield much improved tae results ( see table  [ table.5 ] ) . in becke s original work  @xcite ,",
    "the rms deviations are 2.9 , 3.9 , 1.9 , and 4.1 kcal@xmath1mol@xmath2 for ae , ip , pa and tae , respectively .",
    "the new b3lyp/6 - 311+g(3_df_,2_p _ ) calculations yield improved results in comparison to becke s work  @xcite ( cf .",
    "table  [ table.9 ] ) .",
    ".atomization energy ( kcal@xmath1mol@xmath2 ) [ cols=\">,<,>,>,>,>,>\",options=\"header \" , ]     [ table.11 ]",
    "there are currently two schools of density functional construction : the reductionist school and the semiempiricist school .",
    "the reductionists attempt to deduce the universal exchange - correlation functional from the first - principles .",
    "the jacob s ladder  @xcite of density functional approximations depicts the approach that the reductionists take towards the universal exchange - correlation functional of chemical accuracy .",
    "becke realized that the existence and uniqueness of exact exchange - correlation functional do not guarantee that the functional is expressible in simple or even not so - simple analytical form , and introduced the semiempirical approach to construct accurate exchange - correlation functionals .",
    "we go beyond the semiempirical approach by constructing the neural - networks - based exchange - correlation functional .",
    "our generalized functional is a neural network whose structure and synaptic weights are determined by accurate experimental data .",
    "it is dynamic , and evolves readily when more accurate experimental data become available .",
    "although the parameters in the resulting functional , such as @xmath28 , @xmath29 and @xmath30 , are system - dependent as compared to the universal functionals adopted by both reductionists and semiempiricists , the neural network is not system - dependent and is regarded as a generalized universal functional .",
    "our approach relies on neural networks to discover automatically the hidden regularities or rules from large amount of experimental data .",
    "it is thus distinct from the semiempirical approach .",
    "we term it as the discovery approach .",
    "compared to the conventional b3lyp/6 - 311+g(3_df_,2_p _ ) calculations , the neural - networks - based b3lyp/6 - 311+g(3_df_,2_p _ ) calculations yield much improved aes , ips , pas and taes ( cf . table  [ table.9 ] ) . however , the improvement over becke s calculation  @xcite is not as significant .",
    "this leaves room for further improvement or investigation .    to summarize",
    ", we have developed a promising new approach , the neural - networks - based approach , to construct the accurate dft exchange - correlation functional .",
    "the improved b3lyp functional developed in this work is certainly not yet the final exchange - correlation functional of chemical accuracy that we seek for .",
    "our work opens the door of an entirely different methodology to develop the accurate exchange - correlation functionals .",
    "the neural - networks - based functional can be systematically improved as more or better experimental data become available .",
    "the introduction of neural networks to the construction of exchange - correlation functionals is potentially a powerful tool in computational chemistry and physics , and may open the possibility for first - principles methods being employed routinely as predictive tools in materials research and development .",
    "we thank prof . yijing yan for extensive discussion on the subject .",
    "support from the hong kong research grant council ( rgc ) and the committee for research and conference grants ( crcg ) of the university of hong kong is gratefully acknowledged ."
  ],
  "abstract_text": [
    "<S> a neural - networks - based approach is proposed to construct a new type of exchange - correlation functional for density functional theory . </S>",
    "<S> it is applied to improve b3lyp functional by taking into account of high - order contributions to the exchange - correlation functional . </S>",
    "<S> the improved b3lyp functional is based on a neural network whose structure and synaptic weights are determined from 116 known experimental atomization energies , ionization potentials , proton affinities or total atomic energies which were used by becke in his pioneer work on the hybrid functionals [ j. chem . </S>",
    "<S> phys . </S>",
    "<S> @xmath0 , 5648  ( 1993 ) ] . </S>",
    "<S> it leads to better agreement between the first - principles calculation results and these 116 experimental data . </S>",
    "<S> the new b3lyp functional is further tested by applying it to calculate the ionization potentials of 24 molecules of the g2 test set . </S>",
    "<S> the 6 - 311+g(3_df_,2_p _ ) basis set is employed in the calculation , and the resulting root - mean - square error is reduced to 2.2 kcal@xmath1mol@xmath2 in comparison to 3.6 kcal@xmath1mol@xmath2 of conventional b3lyp/6 - 311+g(3_df_,2_p _ ) calculation . </S>"
  ]
}