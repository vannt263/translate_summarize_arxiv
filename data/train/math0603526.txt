{
  "article_text": [
    "let @xmath5 be a measurable space .",
    "we consider a random variable @xmath6 with values in @xmath7 and denote by @xmath8 the distribution of @xmath6 .",
    "we denote by @xmath9 the marginal of @xmath8 on @xmath10 and @xmath11 the conditional probability function of @xmath12 given that @xmath13 .",
    "we denote by @xmath14 , @xmath3 i.i.d .",
    "observations of the couple @xmath6 .",
    "we recall some usual notions introduced for the classification framework .",
    "a _ prediction rule _ is a measurable function @xmath15 .",
    "the _ misclassification error _ associated to @xmath16 is @xmath17 it is well known ( see , e.g. , @xcite ) that @xmath18 where the prediction rule @xmath19 is called _ bayes rule _ and is defined by @xmath20the minimal risk @xmath21 is called the _",
    "bayes risk_. a _ classifier _ is a function , @xmath22 , measurable with respect to @xmath23 and @xmath24 with values in @xmath25 , that assigns to the sample @xmath23 a prediction rule @xmath26 .",
    "a key characteristic of @xmath27 is the value of _ generalization error _ @xmath28 $ ] .",
    "$ ] called the _ excess risk _ of @xmath27 .",
    "we say that the classifier @xmath27 learns with the convergence rate @xmath30 , where @xmath31 is a decreasing sequence , if there exists an absolute constant @xmath32 such that for any integer @xmath3 , @xmath33\\leq c\\phi(n)$ ] .",
    "theorem 7.2 of @xcite shows that no classifier can learn with a given convergence rate for arbitrary underlying probability distribution @xmath8 .",
    "in this paper we focus on entropy assumptions which allow us to work with finite sieves .",
    "hence , we first work with a finite model for @xmath19 : it means that we take a finite class of prediction rules @xmath34 . our aim is to construct a classifier @xmath27 which mimics the best one of them w.r.t . to the excess risk and with an optimal residual .",
    "namely , we want to state an oracle inequality @xmath35\\leq a_0\\min_{f\\in{{\\cal f}}}(r(f)-r^*)+c\\gamma(m , n),\\ ] ] where @xmath36 and @xmath32 are some absolute constants and @xmath37 is the residual .",
    "the classical procedure , due to vapnik and chervonenkis ( see , e.g. @xcite ) , is to look for an erm classifier , i.e . ,",
    "the one which minimizes the _ empirical risk _ @xmath38 over all prediction rules @xmath16 in @xmath39 , where @xmath40 denotes the indicator of the set @xmath41 .",
    "this procedure leads to optimal theoretical results ( see , e.g. chapter 12 of @xcite ) , but minimizing the empirical risk ( [ er ] ) is computationally intractable for sets @xmath39 of classifiers with large cardinality ( often depending on the sample size @xmath3 ) , because this risk is neither convex nor continuous . nevertheless , we might base a tractable estimation procedure on minimization of a convex surrogate @xmath42 for the loss ( @xcite , @xcite , @xcite , @xcite , @xcite and @xcite ) .",
    "a wide variety of classification methods in machine learning are based on this idea , in particular , on using the convex loss associated to support vector machines ( @xcite , @xcite),@xmath43 called the _ hinge - loss_. the risk associated to this loss is called the _ hinge risk _ and is defined by @xmath44,\\ ] ] for all @xmath45 .",
    "the _ optimal hinge risk _ is defined by @xmath46 where the infimum is taken over all measurable functions @xmath16 .",
    "the bayes rule @xmath19 attains the infimum in ( [ ohr ] ) and , moreover , denoting by @xmath47 the misclassification error of @xmath48 for all measurable functions @xmath16 with values in @xmath49 , zhang , cf .",
    "@xcite , has shown that,@xmath50for any real valued measurable function @xmath16 .",
    "thus , minimization of the _ excess hinge risk _ @xmath51 provides a reasonable alternative for minimization of the excess risk . in this paper",
    "we provide a procedure which does not need any minimization step .",
    "we use a convex combination of the given prediction rules , as explained in section [ oraclesection ] .",
    "the difficulty of classification is closely related to the behavior of the conditional probability function @xmath52 near @xmath53 ( the random variable @xmath54 is sometimes called the theoretical margin ) .",
    "tsybakov has introduced , in @xcite , an assumption on the the margin , called _ margin ( or low noise ) assumption _ ,    _ * ( ma ) margin ( or low noise ) assumption . *",
    "the probability distribution @xmath8 on the space @xmath55 satisfies the margin assumption ma(@xmath1 ) with margin parameter @xmath56 if there exists @xmath57 such that , @xmath58 for all measurable functions @xmath16 with values in @xmath59 _    under this assumption , the risk of an erm classifier over some fixed class @xmath39 can converge to the minimum risk over the class with _ fast rates _ , namely faster than @xmath60 ( cf .",
    "@xcite ) . on the other hand , with no margin assumption on the joint distribution @xmath8",
    "( but combinatorial or complexity assumption on the class @xmath39 ) , the convergence rate of the excess risk is not faster than @xmath60 ( cf .",
    "@xcite ) .",
    "in this paper we suggest an easily implementable procedure of aggregation of classifiers and prove the following results :    1 .",
    "we obtain an oracle inequality for our procedure and we use it to show that our classifiers are adaptive both to the margin parameter ( low noise exponent ) and to a complexity parameter .",
    "we generalize the lower bound inequality stated in chapter 14 of @xcite , by introducing the margin assumption and deduce optimal rates of aggregation under low noise assumption in the spirit of tsybakov @xcite .",
    "we obtain classifiers with minimax fast rates of convergence on a hlder class of conditional probability functions @xmath52 and under the margin assumption .",
    "the paper is organized as follows . in section",
    "@xmath61 we prove an oracle inequality for our convex aggregate , with an optimal residual , which will be used in section [ example ] to construct minimax classifiers and to obtain adaptive classifiers by aggregation of them .",
    "proofs are given in section [ proofs ] .",
    "we have @xmath2 prediction rules @xmath62 .",
    "we want to mimic the best of them according to the excess risk under the margin assumption .",
    "our procedure is using exponential weights .",
    "similar constructions in other context can be found , e.g. , in @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite .",
    "consider the following aggregate which is a convex combination with exponential weights of @xmath2 classifiers , @xmath63 where @xmath64 since @xmath62 take their values in @xmath25 , we have , @xmath65 for all @xmath66 , where @xmath67 is the empirical analog of the hinge risk . since",
    "@xmath68 for all @xmath69 , these weights can be written in terms of the empirical risks of @xmath70 s , @xmath71 remark that , using the definition ( [ coefrisk ] ) for the weights , we can aggregate functions with values in @xmath49 ( like in theorem [ theooraclea ] ) and not only functions with values in @xmath25 .    the aggregation procedure defined by ( [ aggregate ] ) with weights ( [ coefrisk ] ) , that we can called aggregation with exponential weights ( aew ) , can be compared to the erm one .",
    "first , our aew method does not need any minimization algorithm contrarily to the erm procedure .",
    "second , the aew is less sensitive to the over fitting problem .",
    "intuitively , if the classifier with smallest empirical risk is over fitted ( it means that the classifier fits too much to the observations ) then the erm procedure will be over fitted .",
    "but , if other classifiers in @xmath39 are good classifiers , our procedure will consider their `` opinions '' in the final decision procedure and these opinions can balance with the opinion of the over fitted classifier in @xmath39 which can be false because of its over fitting property .",
    "the erm only considers the `` opinion '' of the classifier with the smallest risk , whereas the aew takes into account all the opinions of the classifiers in the set @xmath39 .",
    "the aew is more temperate contrarily to the erm . understanding why aggregation procedure are often more efficient than the erm procedure from a theoretical point of view is a deep question , on which we are still working at this time this paper is written .",
    "finally , the following proposition shows that the aew has similar theoretical property as the erm procedure up to the residual @xmath72 .",
    "[ propaewerm ] let @xmath73 be an integer , @xmath62 be @xmath2 real valued functions on @xmath10 . for any integers @xmath3 , the aggregate defined in ( [ aggregate ] ) with weights ( [ coefrisk ] )",
    "@xmath75    the following theorem provides first an exact oracle inequality w.r.t .",
    "the hinge risk satisfied by the aew procedure and second shows its optimality among all aggregation procedures .",
    "we deduce from it that , for a margin parameter @xmath76 and a set of @xmath2 functions with values in @xmath77 $ ] , @xmath34 , @xmath78 is an optimal rate of convex aggregation of @xmath2 functions with values in @xmath77 $ ] w.r.t . the hinge risk , in the sense of @xcite .    [ theooraclea ]",
    "let @xmath76 .",
    "we assume that @xmath8 satisfies ma(@xmath1 ) .",
    "we denote by @xmath79 the convex hull of a finite set of functions with values in @xmath77 $ ] , @xmath34 .",
    "the aew procedure , introduced in ( [ aggregate ] ) with weights ( [ coefrisk ] ) ( remark that the form of the weights in ( [ coefrisk ] ) allows to take real valued functions for the @xmath70 s ) , satisfies for any integer @xmath80 the following inequality @xmath81\\leq \\min_{f\\in{{\\cal c}}}(a(f)-a^*)+c_0\\gamma({{\\cal f}},\\pi , n,\\kappa),\\ ] ] where @xmath82 depends only on the constants @xmath1 and @xmath83 appearing in ma(@xmath1 ) .",
    "moreover , there exists a set of prediction rules @xmath34 such that for any procedure @xmath84 with values in @xmath49 , there exists a probability measure @xmath8 satisfying ma(@xmath1 ) such that for any integers @xmath85 with @xmath86 we have @xmath87\\geq \\min_{f\\in{{\\cal c}}}(a(f)-a^*)+c_0'\\gamma({{\\cal f}},\\pi , n,\\kappa),\\]]where @xmath88 depends only on the constants @xmath1 and @xmath83 appearing in ma(@xmath1 ) .",
    "the hinge loss is linear on @xmath77 $ ] , thus , model selection aggregation or convex aggregation are identical problems if we use the hinge risk and if we aggregate function with values in @xmath77 $ ] .",
    "namely , @xmath89 moreover , the result of theorem [ theooraclea ] is obtained for the aggregation of functions with values in @xmath77 $ ] and not only for prediction rules .",
    "in fact , only functions with values in @xmath77 $ ] have to be considered when we use the hinge loss since , for any real valued function @xmath16 , we have @xmath90 for all @xmath91 where @xmath92 is the projection on @xmath77 $ ] , thus , @xmath93 remark that , under ma(@xmath1 ) , there exists @xmath94 such that,@xmath95\\leq c\\left(a(f)-a^ *    \\right)^{1/\\kappa}$]for all functions @xmath16 on @xmath10 with values in @xmath77 $ ] ( cf .",
    "the proof of theorem [ theooraclea ] is not given here by the lack of space . it can be found in @xcite . instead",
    ", we prove here the following slightly less general result that we will be further used to construct adaptive minimax classifiers .",
    "[ oraclea ] let @xmath76 and let @xmath34 be a finite set of prediction rules with @xmath96 .",
    "we denote by @xmath79 the convex hull of @xmath39 .",
    "we assume that @xmath8 satisfies ma(@xmath1 ) .",
    "the aggregate defined in ( [ aggregate ] ) with the exponential weights ( [ coef1 ] ) ( or ( [ coefrisk ] ) ) satisfies for any integers",
    "@xmath97 and any @xmath98 the following inequality @xmath81\\leq ( 1+a ) \\min_{f\\in{{\\cal c}}}(a(f)-a^*)+c\\left(\\frac{\\log m}{n}\\right)^{\\frac{\\kappa}{2\\kappa-1}},\\ ] ] where @xmath32 is a constant depending only on @xmath99 .",
    "[ oracler ] let @xmath76 , @xmath96 and @xmath100 be a finite set of prediction rules .",
    "we assume that @xmath8 satisfies ma(@xmath1 ) .",
    "the aew procedure satisfies for any number @xmath98 and any integers @xmath97 the following inequality , with @xmath32 a constant depending only on @xmath99 , @xmath101\\leq 2(1+a)\\min_{j=1,\\ldots , m}(r(f_j)-r^*)+c\\left(\\frac{\\log m}{n}\\right)^{\\frac{\\kappa}{2\\kappa-1}}.\\ ] ]    we denote by @xmath102 the set of all probability measures on @xmath7 satisfying the margin assumption ma(@xmath1 ) . combining corollary [ oracler ] and the following theorem",
    ", we get that the residual @xmath103 is a near optimal rate of model selection aggregation in the sense of @xcite when the underlying probability measure @xmath8 belongs to @xmath102 .",
    "[ optimalkappa ] for any integers @xmath2 and @xmath3 satisfying @xmath104 , there exists @xmath2 prediction rules @xmath62 such that for any classifier @xmath105 and any @xmath98 , we have @xmath106 - 2(1+a)\\min_{j=1,\\ldots , m}(r(f_j)-r^ * ) \\right]\\geq c_1 \\left(\\frac{\\log m}{n } \\right)^{\\frac{\\kappa}{2\\kappa-1}},\\ ] ] where @xmath107 .",
    "in this section we give two applications of the oracle inequality stated in corollary [ oracler ] .",
    "first , we construct classifiers with minimax rates of convergence and second , we obtain adaptive classifiers by aggregating the minimax ones .",
    "following @xcite , we focus on the regularity model where @xmath52 belongs to the hlder class .    for any multi - index @xmath108 and any @xmath109 , we define @xmath110 and @xmath111 we denote by @xmath112 the differential operator @xmath113    let @xmath114 .",
    "we denote by @xmath115 the maximal integer that is strictly less than @xmath116 for any @xmath117 and any @xmath115-times continuously differentiable real valued function @xmath118 on @xmath119",
    "we denote by @xmath120 its taylor polynomial of degree @xmath115 at point @xmath121 , namely , @xmath122    for all @xmath123 and @xmath114 . the @xmath124^d)-$]_hlder class _ of functions , denoted by @xmath125^d)$ ] , is the set of all real valued functions @xmath118 on @xmath126^d$ ] that are @xmath115-times continuously differentiable on @xmath127 and satisfy , for any @xmath128 the inequality @xmath129    a control of the complexity of hlder classes is given by kolmogorov and tikhomorov ( 1961 ) : @xmath130^d),\\epsilon , l^{\\infty}([0,1]^d ) \\right)\\leq a(\\beta , d)\\epsilon^{-\\frac{d}{\\beta } } , \\forall \\epsilon>0,\\ ] ] where the lhs is the @xmath131entropy of the @xmath124^d)-$]hlder class w.r.t . to the @xmath132^d)-$]norm and @xmath133 is a constant depending only on @xmath134 and @xmath135 .",
    "if we want to use entropy assumptions on the set which @xmath52 belongs to , we need to make a link between @xmath9 and the lebesgue measure , since the distance in ( [ entropy ] ) is the @xmath136norm w.r.t . the lebesgue measure . therefore , introduce the following assumption :    * ( a1)*_the marginal distribution @xmath9 on @xmath10 of @xmath8 is absolutely continuous w.r.t .",
    "the lebesgue measure @xmath137 on @xmath126^d$ ] , and there exists a version of its density which is upper bounded by @xmath138 . _    we consider the following class of models . for all @xmath76 and @xmath114",
    ", we denote by @xmath139 the set of all probability measures @xmath8 on @xmath7 , such that    1 .",
    "ma(@xmath1 ) is satisfied .",
    "2 .   the marginal @xmath9 satisfies ( a1 ) .",
    "the conditional probability function @xmath52 belongs to @xmath140 .",
    "now , we define the class of classifiers which attain the optimal rate of convergence , in a minimax sense , over the models @xmath141 let @xmath142 and @xmath114 . for any @xmath143 , we denote by @xmath144 an @xmath145-net on @xmath125^d)$ ] for the @xmath136norm , such that , its cardinal satisfies @xmath146 .",
    "we consider the aew procedure defined in ( [ aggregate ] ) , over the net @xmath147 @xmath148    [ ratecv ] let @xmath149 and @xmath114 .",
    "let @xmath150 be an absolute constant and consider @xmath151 the aggregate ( [ classifier ] ) with @xmath152 , satisfies , for any @xmath153 and any integer @xmath80 , the following inequality @xmath154\\leq c_2(\\kappa,\\beta , d )    n^{-\\frac{\\beta\\kappa}{\\beta(2\\kappa-1)+d(\\kappa-1)}},\\ ] ] where @xmath155 and @xmath156 is the constant appearing in corollary [ oracler ] .",
    "audibert and tsybakov ( cf .",
    "@xcite ) have shown the optimality , in a minimax sense , of the rate obtained in theorem [ ratecv ] .",
    "note that this rate is a fast rate because it can approach @xmath157 when @xmath1 is close to @xmath158 and @xmath134 is large .",
    "the construction of the classifier @xmath159 needs the knowledge of @xmath1 and @xmath134 which are not available in practice .",
    "thus , we need to construct classifiers independent of these parameters and which learn with the optimal rate @xmath160 if the underlying probability measure @xmath8 belongs to @xmath161 , for different values of @xmath1 and @xmath134 .",
    "we now show that using the procedure ( [ aggregate ] ) to aggregate the classifiers @xmath162 , for different values of @xmath145 in a grid , the oracle inequality of corollary [ oracler ] provides the result .",
    "we use a split of the sample for the adaptation step .",
    "denote by @xmath163 the subsample containing the first @xmath164 observations and @xmath165 the one containing the @xmath166(@xmath167 ) last ones .",
    "subsample @xmath163 is used to construct the classifiers @xmath168 for different values of @xmath145 in a finite grid .",
    "subsample @xmath165 is used to aggregate these classifiers by the procedure ( [ aggregate ] ) .",
    "we take @xmath169 set @xmath170 .",
    "we consider a grid of values for @xmath145 : @xmath171 for any @xmath172 we consider the step @xmath173 the classifier that we propose is the sign of @xmath174}(\\tilde{f}_m^{\\epsilon_m^{(\\phi)}})\\tilde{f}_m^{\\epsilon_m^{(\\phi)}},\\ ] ] where @xmath175 is the classifier associated to the aggregate @xmath168 for all @xmath143 and the weights @xmath176}(f)$ ] are the ones introduced in ( [ coef1 ] ) constructed with the observations @xmath165 for all @xmath177 : @xmath178}(f)=\\frac{\\exp\\left ( \\sum_{i = m+1}^ny_if(x_i)\\right)}{\\sum_{g\\in{{\\cal f}}(n)}\\exp\\left ( \\sum_{i = m+1}^ny_ig(x_i)\\right)}.\\ ] ] the following theorem shows that @xmath179 is adaptive both to the low noise exponent @xmath1 and to the complexity ( or regularity ) parameter @xmath134 , provided that @xmath180 belongs to a compact subset of @xmath181    [ adaptation ] let @xmath182 be a compact subset of @xmath183 .",
    "there exists a constant @xmath184 that depends only on @xmath182 and @xmath135 such that for any integer @xmath80 , any @xmath185 and any @xmath153 , we have , @xmath186\\leq c_3n^{-\\frac{\\kappa\\beta}{\\beta(2\\kappa-1)+d(\\kappa-1)}}.\\ ] ]    classifiers @xmath159 are not easily implementable since the cardinality of @xmath187 is an exponential of @xmath3 .",
    "an alternative procedure which is easily implementable is to aggregate plug - in classifiers constructed in audibert and tsybakov ( cf .",
    "@xcite ) .",
    "we introduce the class of models @xmath188 composed of all the underlying probability measures @xmath8 such that :    1 .",
    "@xmath8 satisfies the margin assumption ma(@xmath1 ) .",
    "2 .   the conditional probability function @xmath189^d).$ ] 3 .   the marginal distribution of @xmath24 is supported on @xmath126^d$ ] and has a lebesgue density lower bounded and upper bounded by two constants .",
    "[ theoat05 ] let @xmath190 .",
    "the excess risk of the plug - in classifier @xmath191 satisfies @xmath192 \\leq c_4n^{-\\frac{\\beta\\kappa}{(\\kappa-1)(2\\beta+d)}},\\ ] ] where @xmath193 is the locally polynomial estimator of @xmath194 of order @xmath115 with bandwidth @xmath195 and @xmath196 a positive constant .    in @xcite",
    ", it is shown that the rate @xmath197 is minimax over @xmath188 , if @xmath198 .",
    "remark that the fast rate @xmath199 can be achieved .",
    "we aggregate the classifiers @xmath200 for different values of @xmath134 lying in a finite grid .",
    "we use a split of the sample to construct our adaptive classifier : @xmath201 the training sample @xmath202 is used for the construction of the class of plug - in classifiers @xmath203 the validation sample @xmath204 is used for the construction of weights @xmath178}(f)=\\frac{\\exp\\left ( \\sum_{i = m+1}^ny_if(x_i)\\right)}{\\sum_{\\bar{f}\\in{{\\cal f}}}\\exp\\left ( \\sum_{i = m+1}^ny_i\\bar{f}(x_i)\\right ) } , \\quad \\forall f\\in{{\\cal f}}.\\ ] ] the classifier that we propose is @xmath205 , where : @xmath206}(f)f.$ ]    [ theoadap2 ] let @xmath182 be a compact subset of @xmath183 .",
    "there exists a constant @xmath207 depending only on @xmath182 and @xmath135 such that for any integer @xmath80 , any @xmath185 , such that @xmath208 , and any @xmath209 , we have , @xmath210\\leq c_5n^{-\\frac{\\beta\\kappa}{(\\kappa-1)(2\\beta+d)}}.\\ ] ]    adaptive classifiers are obtained in theorem ( [ adaptation ] ) and ( [ theoadap2 ] ) by aggregation of only @xmath4 classifiers .",
    "other construction of adaptive classifiers can be found in @xcite . in particular ,",
    "adaptive svm classifiers .",
    "using the convexity of the hinge loss , we have @xmath211 . denote by @xmath212 , we have @xmath213 for all @xmath214 and by averaging over the @xmath215 we get : @xmath216 where we used that @xmath217 where @xmath218 denotes the kullback - leiber divergence between the weights @xmath219 and uniform weights @xmath220 .    *",
    "proof of theorem [ oraclea ] .",
    "* let @xmath98 .",
    "using proposition [ propaewerm ] , we have for any @xmath221 and for the bayes rule @xmath19 : @xmath222 @xmath223 taking the expectations , we get @xmath224 & \\leq & ( 1+a)\\min_{f\\in{{\\cal f}}}(a(f)-a^*)+(1+a)(\\log m)/n\\\\ & & + \\mathbb{e}\\left[a(\\tilde{f}_n)-a^*-(1+a)(a_n(\\tilde{f}_n)-a_n(f^ * ) ) \\right].\\end{aligned}\\ ] ] the following inequality follows from the linearity of the hinge loss on @xmath77 $ ] : @xmath225.\\ ] ] thus , using bernstein s inequality , we have for all @xmath226 @xmath227}\\\\    & \\leq & \\sum_{f\\in{{\\cal f}}}\\mathbb{p}\\left[a(f)-a^*-(a_n(f)-a_n(f^*))\\geq    \\frac{\\delta+a(a(f)-a^*)}{1+a }    \\right]\\\\    & \\leq & \\sum_{f\\in{{\\cal f } } }    \\exp\\left(-\\frac{n(\\delta+a(a(f)-a^*))^2}{2(1+a)^2(a(f)-a^*)^{1/\\kappa}+2/3(1+a)(\\delta+a(a(f)-a^ * ) ) }    \\right).\\end{aligned}\\ ] ] there exists a constant @xmath228 depending only on @xmath99 such that for all @xmath229 and all @xmath221 , we have@xmath230 thus , @xmath231\\leq m\\exp(-nc_1\\delta^{2 - 1/\\kappa}).$ ] observe that an integration by parts leads to @xmath232 , for any @xmath233 and @xmath234 , so for all @xmath235 , we get @xmath236 \\leq 2u + m\\frac{\\exp(-nc_1u^{2 - 1/\\kappa})}{nc_1u^{1 - 1/\\kappa}}.\\ ] ] if we denote by @xmath237 the unique solution of @xmath238 , we have @xmath239 . for @xmath240",
    "such that @xmath241 , we obtain the result .",
    "* proof of corollary [ oracler ] .",
    "* we deduce corollary [ oracler ] from theorem [ oraclea ] , using that for any prediction rule @xmath16 we have @xmath242 and applying zhang s inequality @xmath243 fulfilled by all @xmath118 from @xmath10 to @xmath49 .",
    "* proof of theorem [ optimalkappa ] .",
    "* for all prediction rules @xmath62 , we have @xmath244 - 2(1+a)\\min_{j=1,\\ldots , m}(r(f_j)-r^ * ) \\right)\\ ] ] @xmath245\\right).\\ ] ] thus , we look for a set of cardinality not greater than @xmath2 , of the worst probability measures @xmath246 from our classification problem point of view and choose @xmath62 as the corresponding bayes rules .",
    "let @xmath247 be an integer such that @xmath248 .",
    "let @xmath249 be @xmath247 distinct points of @xmath10 .",
    "let @xmath250 . denote by @xmath9 the probability measure on @xmath10 such that @xmath251 for @xmath252 and @xmath253 .",
    "we consider the set of binary sequences @xmath254 .",
    "let @xmath255 .",
    "for all @xmath256 we consider @xmath257 for all @xmath256 we denote by @xmath258 the probability measure on @xmath7 with the marginal @xmath9 on @xmath10 and with the conditional probability function @xmath259 of @xmath12 knowing @xmath24 .",
    "we denote by @xmath264 the hamming distance on @xmath265 ( cf .",
    "@xcite p.88 ) .",
    "let @xmath266 be such that @xmath267 .",
    "we have @xmath268 we take @xmath269 and @xmath270 such that @xmath271 thus , @xmath272 for any integer @xmath3 .",
    "let @xmath105 be a classifier and @xmath256 . using ma(@xmath1 )",
    ", we have @xmath273\\geq ( c_0w)^\\kappa\\mathbb{e}_{\\pi_\\sigma}\\left [ \\left ( \\sum_{i=1}^{n-1}|\\hat{f}_n(x_i)-\\sigma_i|\\right)^{\\kappa}\\right].\\ ] ] by jensen s lemma and assouad s lemma ( cf .",
    "@xcite ) we obtain :          * proof of theorem [ ratecv ] .",
    "* according to theorem [ oracler ] , where we set @xmath284 , we have , for any @xmath143 : @xmath285\\leq 4\\min_{\\bar\\eta\\in\\sigma_\\epsilon(\\beta)}\\left(r(f_{\\bar\\eta})-r^ * \\right)+c\\left(\\frac{\\log { \\rm card}\\sigma_\\epsilon(\\beta)}{n } \\right)^{\\frac{\\kappa}{2\\kappa-1}}.\\ ] ]    let @xmath286 be a function with values in @xmath126 $ ] and denote by @xmath287 the plug - in classifier associated .",
    "we have @xmath288 , thus : @xmath289 = \\mathbb{e}\\left [ |2\\eta(x)-1|{{\\rm 1}\\kern-0.24em{\\rm i}}_{\\bar{f}\\neq f^*}{{\\rm 1}\\kern-0.24em{\\rm i}}_{\\bar{f}\\neq f^*}\\right]\\ ] ] @xmath290 \\leq    \\left\\vert\\left\\vert    \\right\\vert\\right\\vert_{l^\\infty(p^x ) } c_0 \\left ( r(\\bar{f})-r^*\\right)^{\\frac{1}{\\kappa}},\\ ] ] and assumption ( a1 ) lead to latexmath:[\\[r(f_{\\bar\\eta})-r^*\\leq ( 2c_0\\mu_{max})^{\\frac{\\kappa}{\\kappa-1}}||\\bar\\eta-\\eta    @xmath143 , we have @xmath285\\leq   d\\left(\\epsilon^{\\frac{\\kappa}{\\kappa-1}}+   \\left(\\frac{\\epsilon^{-d/\\beta}}{n}\\right)^{\\frac{\\kappa}{2\\kappa-1}}\\right),\\ ] ] where @xmath292 . for the value",
    "@xmath293we have @xmath294\\leq c_1 n^{-\\frac{\\beta\\kappa}{\\beta(2\\kappa-1)+d(\\kappa-1)}},\\ ] ] where @xmath295      let @xmath301 . for any @xmath302",
    ", there exists @xmath303 such that @xmath304 we denote by @xmath305 the increasing function @xmath306 from @xmath307 to @xmath296 .",
    "we set @xmath308 there exists @xmath309 such that @xmath310    let @xmath311 according to the oracle inequality of corollary [ oracler ] , we have , conditionally to the first subsample @xmath312 : @xmath313\\leq   4   \\min_{\\phi\\in{{\\cal g}}(n)}\\left(r(\\tilde{f}_m^{\\epsilon_m^{(\\phi)}})-r^*\\right )   + c\\left(\\frac{\\log { \\rm card}({{\\cal g}}(n))}{l }   \\right)^{\\frac{\\kappa_0}{2\\kappa_0 - 1}}.\\ ] ] using the definition of @xmath166 and the fact that @xmath314 we get that there exists @xmath315 independent of @xmath3 such that @xmath316\\leq\\tilde{c}\\left (   \\mathbb{e}_\\pi\\left[r(\\tilde{f}_m^{\\epsilon_m^{(\\phi_{k_0})}})-r^ * \\right]+   \\left(\\frac{\\log^2 n}{n } \\right)^{\\frac{\\kappa_0}{2\\kappa_0 - 1}}\\right)\\ ] ]    moreover @xmath317 , hence , @xmath318 .",
    "thus , according to theorem [ ratecv ] , we have @xmath319\\leq c_1(k , d)m^{-\\psi(\\kappa_0,\\beta_{0,n})},\\ ] ] where @xmath320 and @xmath321 by construction , there exists @xmath322 such that @xmath323 moreover for any integer @xmath3 we have @xmath324 , which is a constant .",
    "we conclude that @xmath325\\leq c_2(k , d ) \\left(n^{-\\psi(\\kappa_0,\\beta_0)}+\\left(\\frac{\\log^2 n}{n } \\right)^{\\frac{\\kappa_0}{2\\kappa_0 - 1 } } \\right),\\ ] ] where @xmath326 is independent of @xmath3 .",
    "we achieve the proof by observing that @xmath327        let @xmath311 according to the oracle inequality of corollary [ oracler ] , we have , conditionally to the first subsample @xmath312 : @xmath332\\leq   4 \\min_{f\\in{{\\cal f}}}(r(f)-r^ * )   + c\\left(\\frac{\\log { \\rm card}({{\\cal f}})}{l }   \\right)^{\\frac{\\kappa_0}{2\\kappa_0 - 1}}.\\ ] ] using the proof of theorem [ adaptation ] we get that there exists @xmath315 independent of @xmath3 such that @xmath316\\leq\\tilde{c}\\left (   \\mathbb{e}_\\pi\\left[r(\\hat{f}_m^{(\\beta_{k_0})})-r^ * \\right]+   \\left(\\frac{\\log^2 n}{n } \\right)^{\\frac{\\kappa_0}{2\\kappa_0 - 1}}\\right)\\ ] ]    moreover @xmath333 , hence , @xmath334 . thus , according to theorem [ theoat05 ] , we have @xmath335\\leq c_4(k , d)m^{-\\theta(\\kappa_0,\\beta_{k_0})},\\ ] ] where @xmath336 .",
    "we have @xmath337 by construction .",
    "moreover @xmath338 for any integer @xmath3 .",
    "we conclude that @xmath339\\leq \\tilde{c}_4(k , d ) \\left(n^{-\\theta(\\kappa_0,\\beta_0)}+\\left(\\frac{\\log^2 n}{n } \\right)^{\\frac{\\kappa_0}{2\\kappa_0 - 1 } } \\right),\\ ] ] where @xmath340 is independent of @xmath3 .",
    "we achieve the proof by observing that @xmath341 , if @xmath330    2 audibert , j .- y . and",
    "tsybakov , a.b . : fast learning rates for plug - in classifiers under margin condition .",
    "available at http://www.proba.jussieu.fr/mathdoc/preprints/index.html#2005 ( preprint pma-998 )          bartlett , p. and jordan , m. and mcauliffe , j. : convexity , classification and risk bounds , technical report 638 , department of statistics , u.c .",
    "berkeley , ( 2003 ) .",
    "available at http://stat-www.berkeley.edu/tech-reports/638.pdf .",
    "massart , p. : some applications of concentration inequalities to statistics , ( 2000 ) , probability theory .",
    "annales de la facult des sciences de toulouse , * 2 * , 245303 , volume spcial ddi  michel talagrand .",
    "tsybakov , a.b . : optimal rates of aggregation , ( 2003 ) , computational learning theory and kernel machines .",
    "b.schlkopf and m.warmuth , eds .",
    "lecture notes in artificial intelligence , 2777 , 303313 , springer , heidelberg ."
  ],
  "abstract_text": [
    "<S> we consider the problem of optimality , in a minimax sense , and adaptivity to the margin and to regularity in binary classification . </S>",
    "<S> we prove an oracle inequality , under the margin assumption ( low noise condition ) , satisfied by an aggregation procedure which uses exponential weights . </S>",
    "<S> this oracle inequality has an optimal residual : @xmath0 where @xmath1 is the margin parameter , @xmath2 the number of classifiers to aggregate and @xmath3 the number of observations . </S>",
    "<S> we use this inequality first to construct minimax classifiers under margin and regularity assumptions and second to aggregate them to obtain a classifier which is adaptive both to the margin and regularity . </S>",
    "<S> moreover , by aggregating plug - in classifiers ( only @xmath4 ) , we provide an easily implementable classifier adaptive both to the margin and to regularity . </S>"
  ]
}