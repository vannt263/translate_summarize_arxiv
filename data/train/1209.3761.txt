{
  "article_text": [
    "in the real world , one single object may have different representations in different domains .",
    "for example , the declaration of independence has versions translated into different languages .",
    "let @xmath0 denote the number of objects @xmath1 , and @xmath2 be the number of domains .",
    "then we have @xmath3 where the @xmath4th object @xmath5 has @xmath2 measurements @xmath6 ; @xmath7 is the representation for object @xmath5 in space @xmath8 .",
    "the problem explored in this paper is that for @xmath9 new objects @xmath10 , how to classify their representations @xmath11 given the representations @xmath12 with @xmath13 .",
    "for this task , @xmath14 , described above are needed to learn the relation between @xmath8 and @xmath15 so that we can map data from @xmath8 and @xmath15 to a common space @xmath16 .",
    "thus @xmath17 are the domain relation learning training data . in our scenario",
    ", we are interested in a particular setting that the data to be classified is in separated classes different from the data used to learn the low dimensional manifold .",
    "this is shown in figure [ classification ] , where disks represent the domain relation learning training data @xmath17 and squares denote the classifier training and testing data @xmath18 . a classification rule @xmath19",
    "is trained on @xmath20 and applied on @xmath21 .",
    "we consider one domain relation learning method , canonical correlation analysis ( cca ) @xcite , which can be carried out using reduced - rank regression routines @xcite .",
    "we investigate classification performance in the common space @xmath16 obtained via cca , training the classifier on @xmath20 and testing on @xmath21 .",
    "the focus of this paper is not on optimizing the classifier ; rather , we investigate performance for a given clasifier ( 5-nearest neighbor ) as a function of the number of domain relation learning training data observation @xmath0 used to learn @xmath16 . the main contribution of this paper is an investigation of the notion of supplementing the training data of classifier by using data from other disparate sources / spaces .",
    "the structure of the paper is as follows : section [ background ] talks about related work .",
    "section [ method ] discusses the methods employed , including the manifold matching framework as well as embedding and classification details .",
    "experimental setup and results are presented in section [ results ] .",
    "section [ conclusion ] is the conclusion .",
    "different methods of transfer learning , multitask learning and domain adaptation are discussed in a recent survey @xcite .",
    "there are algorithms developed on unsupervised document clustering where training and testing data are of different kinds @xcite .",
    "the problem explored in this paper can be viewed as a domain adaptation problem , for which the training and testing data of the classifier are from different domains .",
    "when the classification is on the text documents in different languages , as described in the later sections of this paper , it is called cross - language text classification .",
    "there is much work on inducing correspondences between different language pairs , including using bilingual dictionaries @xcite , latent semantic analysis ( lsa ) features @xcite , kernel canonical correlation analysis ( kcca ) @xcite , etc .",
    "machine translation is also involved in the cross - language text classification , which translates the documents into a single domain @xcite .",
    "in this paper , we focus on manifold matching .",
    "the whole procedure can be divided into the following steps :    1 .   for each single space @xmath8 , calculate the dissimilarity matrix for all domain relation learning training data observations @xmath5 .",
    "2 .   for each @xmath22",
    ", use multidimensional scaling ( mds ) on the dissimilarity matrix to get a euclidean representation @xmath23 .",
    "3 .   run cca ( for @xmath24 ) or generalized cca ( @xmath25 ) to map the collection @xmath26 to a common space @xmath16 .",
    "4 .   pursue joint inference ( i.e. classification ) in the common space @xmath16 .",
    "this procedure combines mds and ( generalized ) cca in a sequential way .",
    "firstly mds is applied to learn low - dimensional manifolds , then ( generalized ) cca is used to match those manifolds to obtain a common space .",
    "this paper focuses on manifold matching and it demonstrates the classification improvement via fusing data from additional space to learn the common low dimensional manifold .",
    "it is interesting to investigate how to generate the low dimensional space using all data instead of matching separate manifolds . but this requires calculating the dissimilarity information for the objects representation in different spaces properly for the multi - dimensional scaling purpose .",
    "this issue had been investigated , e.g. , @xcite , but there had not been any clear answer .      the framework structure for manifold matching",
    "is shown in figure [ model ] @xcite .",
    "for each of the @xmath0 objects @xmath27 , there are @xmath2 representations @xmath28 generated by the mappings @xmath29 .",
    "manifold matching works to find @xmath30 to map @xmath31 to a low - dimensional common space @xmath32 : @xmath33    after learning the @xmath34s , we can map a new measurement @xmath35 into the common space @xmath32 via : @xmath36 this allows joint inference to proceed in @xmath37 .",
    "the work described in this paper is based on dissimilarity measures .",
    "let @xmath38 denote the dissimilarity measure in the @xmath22th space @xmath8 , and @xmath39 be the euclidean distance in the common space @xmath37 .",
    "there are two kinds of mapping errors induced by the @xmath34s : fidelity error and commensurability error .",
    "fidelity measures how well the original dissimilarities are preserved in the mapping @xmath40 , and the fidelity error is defined as the within - condition squared error : @xmath41    commensurability measures how well the matchedness is preserved in the mapping , and the commensurability error is defined as the between - condition squared error : @xmath42    multidimensional scaling ( mds ) @xcite works to get a euclidean representation while approximately preserving the dissimilarities . given the @xmath43 dissimilarity matrix @xmath44 $ ] in space @xmath8 , multidimensional scaling generates embeddings @xmath45 for @xmath46 , which attempts to optimize fidelity , that is , @xmath47 .",
    "for the @xmath24 case , multidimensional scaling generates @xmath48 matrices @xmath49 from @xmath50 and @xmath51 from @xmath52 .",
    "the @xmath4th row vector @xmath53 of @xmath54 is the multidimensional scaling embedding for @xmath55 .",
    "canonical correlation analysis is applied to the multidimensional scaling results .",
    "canonical correlation works to find @xmath56 matrices @xmath57 and @xmath58 as the linear mapping method to maximize correlation for the mappings into @xmath59 , where two matices satisfy @xmath60 and @xmath61 .",
    "that is , for the @xmath62th ( @xmath63 ) dimension , the mapping process is defined by @xmath64 and @xmath65 , the @xmath62th column vector of @xmath66 and @xmath67 respectively .",
    "the orthonormal requirement on the columns of @xmath66 ( similarly @xmath67 ) implies that the correlation between different dimensions of the embedding is 0 .",
    "the correlation of the mapping data is calculated as @xmath68 which is equivalent to @xmath69    subject to    @xmath70    and the constraint can be proved to be equivalent to @xmath71    for cca it holds @xmath72 .    for new data @xmath73 , out - of - sample embedding for multidimensional scaling @xcite generates @xmath74 dimensional row vector @xmath75 .",
    "the final embeddings in the common space @xmath37 are given by @xmath76 and @xmath77 .",
    "canonical correlation analysis optimizes commensurability without regard for fidelity @xcite . for our work",
    ", first we use multidimensional scaling to generate a fidelity - inspired euclidean representation , and then we use canonical correlation analysis to enforce low dimensional commensurability .    canonical correlation analysis is developed as a way of measuring the correlation of two multivariate data sets , and it can be formulated as a generalized eigenvalue problem .",
    "the expansion of canonical correlation analysis to more than two multivariate data sets is also available @xcite , which is called generalized canonical correlation analysis ( gcca ) .",
    "generalized canonical correlation analysis simultaneously find @xmath78 to map the multivariate data sets in @xmath2 spaces to the common space @xmath59 .",
    "similarly for the new data @xmath79 , we can get their representations in the common space @xmath37 as @xmath80 .",
    "similar to cca , the correlation of data in the @xmath62th mapping dimension is calculated as @xcite    @xmath81    subject to    @xmath82    gcca can be formulated as a generalized eigenvalue problem .",
    "different algorithms have been developed as the solution , e.g. least square regression .",
    "for the particular dataset used in our experiments , because it is not very large , we can perform eigenvalue decomposition on the respective matrices directly .      given the measurements of @xmath9 new data points @xmath83 , ( generalized ) canonical correlation analysis in section [ embedding ] yields the embeddings @xmath84 in the common space @xmath59 . to classify @xmath84 , instead of using data points from the same space @xmath8 ( i.e. @xmath85 ) ,",
    "we consider the problem in which we must borrow the embeddings from another space @xmath15 for training , that is , @xmath86 .",
    "this problem is motivated by the fact that in many situations there is a lack of training data in the space where the testing data lie .",
    "we investigate the effect of the number of domain relation learning training data observations on the classification performance .",
    "our experiments apply canonical correlation analysis and its generalization to text document classification .",
    "the dataset is obtained from wikipedia , an open - source multilingual web - based encyclopedia with around 19 million articles in more than 280 languages .",
    "each document may have links pointing to other documents in the same language which explain certain terms in its content as well as the documents in other languages for the same subject .",
    "articles of the same subject in different languages are not necessarily the exact translations of one another .",
    "they can be written by different people and their contents can differ significantly .",
    "english articles within a 2-neighborhood of the english article `` algebraic geometry '' are collected .",
    "the corresponding french documents of those english ones are also collected .",
    "so this data set can be viewed as a two space case : @xmath87 is the english space and @xmath88 is the french space .",
    "there are in total 1382 documents in each space .",
    "that is , @xmath89 , and @xmath90 .",
    "note that @xmath91 includes both domain relation learning training data @xmath92 and new data points @xmath93 ( @xmath94 ) used for classification training and testing .",
    "all 1382 documents are manually labeled into 5 disjoint classes ( @xmath95 ) based on their topics .",
    "the topics are category , people , locations , date and math things respectively",
    ". there are 119 documents in class 0 , 372 documents in class 1 , 270 documents in class 2 , 191 documents in class 3 , and 430 documents in class 4 . the documents in classes @xmath96 are the domain relation learning training data @xmath97 .",
    "there are in total @xmath98 documents in those 3 classes ( @xmath99 ) .",
    "the @xmath100 ( @xmath101 ) documents in classes @xmath102 are the new data @xmath103 .",
    "they are used to train a classifier and run the classification test .",
    "the method described in section [ embedding ] starts with the dissimilarity matrix . for our work",
    "two different kinds of dissimilarity measures are considered : text content dissimilarity matrix @xmath104 and graph topology dissimilarity matrix @xmath105 .",
    "both matrices are of dimension @xmath106 , containing the dissimilarity information for all data points @xmath107 .",
    "graphs @xmath108 can be constructed to describe the dataset ; @xmath109 represents the set of vertices which are the 1382 wikipedia documents , and @xmath23 is the set of edges connecting those documents in language @xmath22 .",
    "the ( @xmath110 ) entry @xmath111 is the number of steps on the shortest path from document @xmath4 to document @xmath112 in @xmath113 . in the english space @xmath87 , @xmath114 , where the 4 comes from the 2-neighborhood document collection . in the french space @xmath88",
    ", @xmath115 is the document in french corresponding to the document @xmath116 , and @xmath117 depends on the french graph connections .",
    "it is possible that @xmath118 . at the extreme end , @xmath119 when @xmath120 and @xmath121 are not connected .",
    "we set @xmath122 for @xmath123 .",
    "@xmath124 is based on the text processing features for documents @xmath125 .",
    "given the feature vectors @xmath126 , @xmath127 is calculated by the cosine dissimilarity @xmath128 . for our experiments ,",
    "we consider three different features for @xmath129 : mutual information ( mi ) features @xcite , term frequency - inverse document frequency ( tfidf ) features @xcite and latent semantic indexing ( lsi ) features @xcite .",
    "the wikipedia dataset used in the experiments are available online .",
    "see the paper @xcite for more details / description .      to choose the dimension @xmath130 for the common space @xmath37",
    ", we pick a sufficiently large dimension and embed @xmath104 and @xmath105 via multidimensional scaling . the scree plot for the mds embedding",
    "is shown in fig [ eigplot_all ] ( term frequency - inverse document frequency features are used for the text dissimilarity calculation ) .",
    "based on the plots in figure [ eigplot_all ] , we choose @xmath131 for the dimension of the joint space @xmath16 , which is low but preserves most of the variance @xcite .",
    "this model selection choice of dimension is an important issue in its own right ; for this paper , we fix @xmath131 throughout .    for the canonical correlation analysis step ,",
    "since it requires to multidimensional scale the dissimilarity matrices to @xmath74 at the beginning , as described in section [ embedding ] , when we choose different number @xmath132 of domain relation learning training documents , @xmath74 depends on @xmath132 .",
    "the choice of dimension is once again an important model selection problem ; for this paper , the values of @xmath74 with different @xmath132 are shown in table [ ccadim ] .",
    "we believe that the values of @xmath74 are chosen large enough to preserve most of the structure yet still small enough to avoid dimensions of pure noise which might deteriorate the following ( g)cca step .",
    "the second column indicates what percentage of the total manifold matching training data @xmath55 is used .",
    ".mds dimensions [ cols=\"^,^,^\",options=\"header \" , ]     inferences regarding differences in the relative performance between competing methodologies ( as well as the seemingly non - monotonic performance across @xmath133 for a given methodology ) are clouded by the variability inherent in our performance estimates .",
    "however , these real - data experimental results nonetheless illustrate the general relative performance characteristics of cca and gcca and their regularized versions , as a function of @xmath133 .",
    "canonical correlation analysis and its generalization are discussed in this paper as a manifold matching method .",
    "they can be viewed as reduced rank regression , and they are applied to a classification task on wikipedia documents .",
    "we show their performance with manifold matching training data from different domains and different dissimilarity measures , and we also investigate their efficiency by choosing different amounts of manifold matching training data .",
    "the experiment results indicate that the generalized canonical correlation analysis , which fuses data from disparate sources , improves the quality of manifold matching with regard to text document classification . also , if we use regularized canonical correlation analysis and its generalization , we further improve performance .    finally , increasing the amount of domain relation learning training data from @xmath134 to @xmath135 ( @xmath133 in the figures [ pic_selfidm_stopwordsrm_stem_lsi_cca ] , [ pic_selfidm_stopwordsrm_stem_tfidf_lsidim_cca ] , [ pic_selfidm_stopwordsrm_stem_mi_lsidim_cca ] , [ pic_selfidm_stopwordsrm_stem_lsi_halflsidim_cca ] , [ pic_selfidm_stopwordsrm_stem_tfidf_halflsidim_cca ] and [ pic_selfidm_stopwordsrm_stem_mi_halflsidim_cca ] ) of the available 819 documents yield approximately @xmath134 improvement in classification performance .",
    "this improvement is independent of the amount of training data available for the classifier .",
    "s.  t.  dumais , t.  a.  letsche , m.  l.  littman and t.  k.  landauer .",
    "_ automatic cross - language retrieval using latent semantic indexing_. 1em plus 0.5em minus 0.4emin aaai symposium on cross language text and speech retrieval , 1997 .",
    "b.  fortuna and j.  shawe - taylor . _",
    "the use of machine translation tools for cross - lingual text mining_. 1em plus 0.5em minus 0.4emin proceedings of the icml workshop on learning with multiple views , 2005 .",
    "d.  r.  hardoon , s.  r.  szedmak , and j.  r.  shawe - taylor .",
    "_ canonical correlation analysis : an overview with application to learning methods_.1em plus 0.5em minus 0.4emneural computation 16(12 ) : 2639 , 2004 .",
    "d.  karakos , j.  eisner , s.  khudanpur and c.  e.  priebe .",
    "_ cross - instance tuning of unsupervised document clustering algorithms_. 1em plus 0.5em minus 0.4emproceedings of the main conference human language technology conference of the north american chapter of the association for computational linguistics , 2007 .",
    "d.  lin and p.  pantel . _ concept discovery from text_.1em plus 0.5em minus 0.4emin proceedings of the 19th international conference on computational linguistics , ( morristown , nj , usa ) , pp . 1 - 7 ,",
    "association for computational linguistics , 2002 .",
    "z.  ma , d.  marchette and c.  e.  priebe . _ fusion and inference from multiple data sources in a commensurate space_. 1em plus 0.5em minus 0.4emstatistical analysis and data mining , accepted for publication , january , 2012 .          c.  e.  priebe , d.  j.  marchette , y.  park , e.  j.  wegman , j.  l.  solka , d.  a.  socolinsky , d.  karakos , k.  w.  church , r.  guglielmi , r.  r.  coifman , d.  lin , d.  m.  healy , m.  q.  jacobs , and a.  tsao .",
    "_ iterative denoising for cross - corpus discovery_.1em plus 0.5em minus 0.4emin proceedings of the 2004 symposium on computational statistics ( invited talk ) , prague , august 23 - 27 , 2004 .    c.  e.  priebe , z.  ma , d.  marchette , e.  hohman and g.  coppersmith . _",
    "fusion and inference from multiple data sources_.1em plus 0.5em minus 0.4emthe 57th session of the international statistical institute , durban , august 16 - 22 , 2009 .    c.  e.  priebe , d.  j.  marchette , z.  ma and s.  adali .",
    "_ manifold matching : joint optimization of fidelity and commensurability_.1em plus 0.5em minus 0.4embrazilian journal of probability and statistics , accepted for publication , february , 2012 .",
    "j.  via , i.  santamaria and j.  perez .",
    "_ canonical correlation analysis ( cca ) algorithms for multiple data sets : application to blind simo equalization_. 1em plus 0.5em minus 0.4em13th european signal processing conference , antalya , turkey , 2005 ."
  ],
  "abstract_text": [
    "<S> manifold matching works to identify embeddings of multiple disparate data spaces into the same low - dimensional space , where joint inference can be pursued . </S>",
    "<S> it is an enabling methodology for fusion and inference from multiple and massive disparate data sources . in this paper </S>",
    "<S> we focus on a method called canonical correlation analysis ( cca ) and its generalization generalized canonical correlation analysis ( gcca ) , which belong to the more general reduced rank regression ( rrr ) framework . </S>",
    "<S> we present an efficiency investigation of cca and gcca under different training conditions for a particular text document classification task .    </S>",
    "<S> manifold matching , canonical correlation analysis , reduced rank regression , efficiency , classification . </S>"
  ]
}