{
  "article_text": [
    "inspired by @xcite , in @xcite a reinforcement learning setting has been introduced where the learner does not have explicit information about the state space of the underlying markov decision process ( mdp ) . instead , the learner has a set of _ models _ at her disposal that map histories ( i.e. , observations , chosen actions and collected rewards ) to states .",
    "however , only some models give a correct mdp representation . the first regret bounds in this setting were derived in @xcite .",
    "they recently have been improved in  @xcite and extended to infinite model sets in @xcite . here",
    "we extend and improve the results of  @xcite as follows .",
    "first , we do not assume anymore that the model set given to the learner contains a true model resulting in an mdp representation .",
    "instead , models will only approximate an mdp .",
    "second , we improve the bounds of  @xcite with respect to the dependence on the state space .",
    "for discussion of potential applications and related work on learning state representations in pomdps ( like predictive state representations @xcite ) , we refer to @xcite .",
    "here we only would like to mention the recent work  @xcite that considers a similar setting , however is mainly interested in the question whether the true model will be identified in the long run , a question we think is subordinate to that of minimizing the regret , which means fast learning of optimal behavior .      for each time step @xmath0 , let @xmath1 be the set of histories up to time @xmath2 , where @xmath3 is the set of observations , @xmath4 a finite set of actions , and @xmath5 $ ] the set of possible rewards .",
    "we consider the following reinforcement learning problem : the learner receives some initial observation @xmath6 . then at any time step @xmath7",
    ", the learner chooses an action @xmath8 based on the current history @xmath9 , and receives an immediate reward  @xmath10 and the next observation  @xmath11 from the unknown environment .",
    "thus , @xmath12 is the concatenation of  @xmath13 with @xmath14 .    * state representation models . * a _ state - representation model _",
    "@xmath15 is a function from the set of histories @xmath16 to a finite set of states @xmath17 .",
    "a particular role will be played by state - representation models that induce a _",
    "markov decision process ( mdp)_. an mdp is defined as a decision process in which at any discrete time @xmath2 , given action @xmath18 , the probability of immediate reward @xmath19 and next observation @xmath11 , given the past history @xmath20 , only depends on the current observation  @xmath21 i.e. , @xmath22 , and this probability is also independent of  @xmath2 .",
    "observations in this process are called _ states _ of the environment .",
    "we say that a state - representation model  @xmath15 is a _",
    "markov model _ of the environment , if the process @xmath23 is an mdp .",
    "note that such an mdp representation needs not be unique . in particular , we assume that we obtain a markov model when mapping each possible history to a unique state . since these states are not visited more than once , this model is not very useful from the practical point of view , however . in general ,",
    "an mdp is denoted as @xmath24 , where @xmath25 is the mean reward and @xmath26 the probability of a transition to state @xmath27 when choosing action @xmath28 in state @xmath29 .",
    "we assume that there is an _ underlying true _",
    "markov model @xmath30 that gives a finite and _ weakly communicating _ mdp , that is , for each pair of states @xmath31 there is a @xmath32 and a sequence of actions @xmath33 such that the probability of reaching state @xmath34 when starting in state @xmath35 and taking actions @xmath36 is positive .",
    "in such a weakly communicating mdp we can define the _ diameter _ @xmath37 to be the expected minimum time it takes to reach any state starting from any other state in the mdp  @xmath38 , cf .",
    ". in finite state mdps , the _ poisson equation _ relates the average reward @xmath39 of any policy @xmath40 to the single step mean rewards and the transition probabilities .",
    "that is , for each _ policy _ @xmath40 that maps states to actions , it holds that @xmath41 where @xmath42 is the so - called _ bias _ vector of @xmath40 , which intuitively quantifies the difference in accumulated rewards when starting in different states .",
    "accordingly , we are sometimes interested in the _ span _ of the bias vector @xmath43 of an optimal policy defined as @xmath44 . in the following",
    "we assume that rewards are bounded in @xmath45 $ ] , which implies that @xmath46 is upper bounded by @xmath47 , cf .",
    "@xcite .",
    "* problem setting . *",
    "given a finite set of models @xmath48 ( not necessarily containing a markov model ) , we want to construct a strategy that performs as well as the algorithm that knows the underlying true markov model @xmath30 , including its rewards and transition probabilities . for that purpose",
    "we define for the markov model @xmath30 the _ regret _ of any strategy at time  @xmath49 , cf .",
    "@xcite , as @xmath50 where @xmath19 are the rewards received when following the proposed strategy and @xmath51 is the average optimal reward in @xmath30 , i.e.@xmath52 $ ] where @xmath53 are the rewards received when following the optimal policy @xmath54 on @xmath38 . note that for weakly communicating mdps the average optimal reward does not depend on the initial state .",
    "we consider the case when @xmath48 is finite and the learner has no knowledge of the correct approximation errors of each model in @xmath48 .",
    "thus , while for each model @xmath55 there is an associated @xmath56 which indicates the aggregation error ( cf .  definition [ def : approx ] below ) , this @xmath57 is unknown to the learner for each model .",
    "we remark that we can not expect to perform as well as the unknown underlying markov model , if the model set only provides approximations .",
    "thus , if the best approximation has error @xmath57 we have to be ready to accept respective error of order @xmath58 per step , cf .",
    "the lower bound provided by theorem [ thm : lobo ] below",
    ".    * overview .",
    "* we start with explicating our notion of approximation in section  [ sec : approx ] , then introduce our algorithm in section  [ sec : alg ] , present our regret bounds in section  [ sec : regret ] , and conclude with the proofs in section  [ sec : proof ] .",
    "* approximations . *",
    "before we give the precise notion of _ approximation _ we are going to use , first note that in our setting the transition probabilities @xmath59 for any two histories @xmath60 and an action @xmath61 are well - defined .",
    "then given an arbitrary model  @xmath15 and a state @xmath27 , we can define the aggregated transition probabilities @xmath62 .",
    "note that the true transition probabilities under @xmath30 are then given by @xmath63 for @xmath64 and @xmath65 .",
    "[ def : approx ] a model @xmath15 is said to be an @xmath57-approximation of the true model @xmath66 if : ( i ) for all histories @xmath67 with @xmath68 and all actions @xmath61 @xmath69 and ( ii ) there is a surjective mapping @xmath70 such that for all histories @xmath71 and all actions  @xmath61 it holds that @xmath72    intuitively , condition assures that the approximate model aggregates only histories that are mapped to similar states under the true model .",
    "complementary , condition guarantees that the state space under the approximate model resembles the true state space . so that the total error with respect to the transition probabilities is @xmath57 .",
    "this matches the respective condition for mdp approximations in definition [ def : mdpapprox ] , cf .  also section  [ sec : err - agg ] . ]",
    "note that any model will be an @xmath57-approximation of the underlying true model @xmath30 for sufficiently large @xmath57 .    a particular natural case are approximation models @xmath15 which also satisfy @xmath73 that is , intuitively , states in @xmath74 are aggregated to meta - states in @xmath75 , and holds trivially",
    ".    we may carry over our definition of @xmath57-approximation to mdps .",
    "this will turn out useful , since each approximate model can be interpreted as an mdp approximation , cf .",
    "section  [ sec : err - agg ] below .",
    "[ def : mdpapprox ] an mdp @xmath76 is an _",
    "@xmath57-approximation _ of another mdp @xmath77 if there is a surjective function @xmath78 such that for all  @xmath35  in  @xmath79 : @xmath80    * error bounds for @xmath57-approximations.*[sub : error - bound - app ] the following is an error bound on the error made by an @xmath57-approximation .",
    "it generalizes bounds of @xcite from ergodic to communicating mdps . for a proof see appendix  [ app : proofthm1 ] .",
    "[ thm : aggubo ] let @xmath81 be a communicating mdp and @xmath82 be an @xmath57-approximation of @xmath81 . then @xmath83    the following is a matching lower bound on the error by aggregation .",
    "this is an improvement over the results in @xcite , which only showed that the error approaches 1 when the diameter goes to infinity .",
    "[ thm : lobo ] for each @xmath84 and each @xmath85 there are mdps @xmath81 , @xmath86 such that @xmath86 is an @xmath57-approximation of @xmath81 , @xmath81 has diameter @xmath87 , and @xmath88    consider the mdp @xmath81 shown in figure [ fig : lobo ] ( left ) , where the ( deterministic ) reward in states @xmath89 , @xmath90 is 0 and 1 in state @xmath91 .",
    "we assume that @xmath92 .",
    "then the diameter @xmath93 is the expected transition time from @xmath90 to @xmath89 and equal to @xmath94 .",
    "aggregating states @xmath95 gives the mdp @xmath86 on the right hand side of figure [ fig : lobo ] .",
    "obviously , @xmath86 is an @xmath57-approximation of @xmath81 .",
    "it is straightforward to check that the stationary distribution @xmath96 ( of the only policy ) in @xmath81 is @xmath97 , while the stationary distribution in @xmath86 is @xmath98 .",
    "thus , the difference in average reward is @xmath99    theorems  [ thm : aggubo ] and [ thm : lobo ] compare the optimal policies of two different mdps , however it is straightforward to see from the proofs that the same error bounds hold when comparing on some mdp @xmath81 the optimal average reward @xmath100 to the average reward when applying the optimal policy of an @xmath57-approximation @xmath86 of @xmath81",
    ". thus , when we approximate an mdp  @xmath81 by an @xmath57-approximation  @xmath86 , the respective error of the optimal policy of @xmath86 on  @xmath81 can be of order @xmath101 as well .",
    "hence , we can not expect to perform below this error if we only have an @xmath57-approximation of the true model at our disposal .",
    "the ` oams `  algorithm ( shown in detail as algorithm  [ fig : algo ] ) we propose for the setting introduced in section  [ sec : intro ] is a generalization of the ` oms ` algorithm of @xcite .",
    "application of the original ` oms ` algorithm to our setting would not work , since ` oms ` compares the collected rewards of each model to the reward it would receive if the model were markov .",
    "models not giving sufficiently high reward are identified as non - markov and rejected . in our case , there may be no markov model in the set of given models  @xmath48 .",
    "thus , the main difference to ` oms ` is that ` oams `  for each model estimates and takes into account the possible approximation error with respect to a closest markov model .",
    "` oams `  proceeds in episodes @xmath102 , each consisting of several runs @xmath103 . in each run",
    "@xmath104 of some episode @xmath105 , starting at time @xmath106 , ` oams `  chooses a policy @xmath107 applying the _ optimism in face of uncertainty _ principle twice .",
    "* plausible models . *",
    "first , ` oams `  considers for each model @xmath55 a set of _ plausible _ mdps @xmath108 defined to contain all mdps with state space @xmath109 and with rewards @xmath110 and transition probabilities @xmath111 satisfying @xmath112 where @xmath113 is the estimate for the approximation error of model @xmath15 ( cf .  below ) , @xmath114 and @xmath115 are respectively the empirical state - transition probabilities and the mean reward at time @xmath2 for taking action @xmath61 in state @xmath116 , @xmath117 denotes the number of states under model @xmath15 , @xmath118 is the number of actions , and @xmath119 is the number of times action @xmath61 has been chosen in state  @xmath35 up to time  @xmath2 .",
    "( if @xmath61 hasnt been chosen in @xmath35 so far , we set @xmath119 to 1 . ) the inequalities and are obviously inspired by chernov bounds that would hold with high probability in case the respective model  @xmath15 is markov , cf .  also lemma  [ lem : chernov ] below .",
    "* optimistic mdp for each model @xmath15 . * in line  4 , the algorithm computes for each model @xmath15 a so - called optimistic mdp @xmath120 and an associated optimal policy @xmath121 on @xmath122 such that the average reward @xmath123 is maximized .",
    "this can be done by extended value iteration ( evi )  @xcite . indeed ,",
    "if @xmath124 and @xmath125 denote the optimistic rewards and transition probabilities of @xmath122 , then evi computes optimistic state values @xmath126 such that ( cf .",
    "@xcite ) @xmath127 is an approximation of @xmath128 , that is , @xmath129    * optimistic model selection . * in line  5 , ` oams `  chooses a model @xmath130 with corresponding mdp @xmath131 and policy @xmath132 that maximizes the average reward penalized by the term @xmath133 defined as @xmath134 where we define @xmath135 to be the empirical value span of the optimistic mdp @xmath122 .",
    "intuitively , the penalization term is an upper bound on the per - step regret of the model @xmath15 in the run to follow in case @xmath15 is chosen , cf .",
    "eq .   in the proof of the main theorem .",
    "similar to the regal algorithm of  @xcite this shall prefer simpler models ( i.e. , models having smaller state space and smaller value span ) to more complex ones .",
    "* termination of runs and episodes . * the chosen policy @xmath107 is then executed until either ( i ) run @xmath104 reaches the maximal length of @xmath136 steps , ( ii ) episode  @xmath105 terminates when the number of visits in some state has been doubled ( line  12 ) , or ( iii ) the executed policy @xmath107 does not give sufficiently high rewards ( line  9 ) .",
    "that is , at any time @xmath2 in run @xmath104 of episode @xmath105 it is checked whether the total reward in the current run is at least @xmath137 , where @xmath138 is the ( current ) length of run @xmath104 in episode @xmath105 , and @xmath139 is defined as @xmath140 where @xmath141 , @xmath142 , and @xmath143 are the ( current ) state - action counts of run @xmath104 in episode @xmath105 . that way ,",
    "` oams `  assumes each model to be markov , as long as it performs well .",
    "we will see that @xmath139 can be upper bounded by @xmath144 , cf .",
    "eq .   below .    * guessing the approximation error . *",
    "the algorithm tries to guess for each model @xmath15 the correct approximation error @xmath145 . in the beginning",
    "the guessed value @xmath113 for each model @xmath146 is set to the precision parameter @xmath147 , the best possible precision we aim for . whenever the reward test fails for a particular model @xmath15 ,",
    "it is likely that @xmath113 is too small and it is therefore doubled ( line  10 ) .",
    "set of models @xmath48 , confidence parameter @xmath148 , precision parameter  @xmath149 let @xmath2 be the current time step , and set @xmath150 for all @xmath151 .",
    "@xmath152 , use evi to compute an optimistic mdp @xmath122 in @xmath108 ( the set of _ plausible _ mdps defined via the confidence intervals and for the estimates so far ) , a ( near-)optimal policy @xmath121 on @xmath122 with approximate average reward  @xmath153 , and the empirical value span @xmath154 . choose model @xmath155 such that @xmath156 set @xmath157 , @xmath158 , @xmath159 , and @xmath160 .",
    "choose action @xmath161 , get reward @xmath19 , observe next state @xmath162 .",
    "@xmath163 terminate current episode .",
    "terminate current episode .",
    "the following upper bound on the regret of ` oams `  is the main result of this paper .",
    "[ thm : mainfinite ] there are @xmath164 such that in each learning problem where the learner is given access to a set of models @xmath48 not necessarily containing the true model @xmath30 , the regret @xmath165 of ` oams `  ( with parameters @xmath166 , @xmath147 ) with respect to the true model  @xmath30 after any @xmath167 steps is upper bounded by @xmath168 with probability at least @xmath169 , where @xmath55 is an @xmath145-approximation of the true underlying markov model @xmath30 , @xmath170 , and @xmath171 .",
    "as already mentioned , by theorem  [ thm : lobo ] the second term in the regret bound is unavoidable when only considering models in @xmath48 .",
    "note that theorem  [ thm : mainfinite ] holds for _ all _ models @xmath55 .",
    "for the best possible bound there is a payoff between the size @xmath172 of the approximate model and its precision @xmath145 .",
    "when the learner knows that @xmath48 contains a markov model @xmath30 , the original ` oms ` algorithm of  @xcite can be employed . in case",
    "when the total number @xmath173 of states over all models is large , i.e. , @xmath174 , we can improve on the state space dependence of the regret bound given in @xcite as follows .",
    "the proof ( found in appendix  [ app : stateimprovement ] ) is a simple modification of the analysis in  @xcite that exploits that by the selected models can not have arbitrarily large state space .",
    "[ thm : stateimprovement ] if @xmath48 contains a markov model @xmath30 , with probability at least @xmath169 the regret of ` oms ` is bounded by @xmath175 .    * discussion . * unfortunately , while the bound in theorem  [ thm : mainfinite ] is optimal with respect to the dependence on the horizon @xmath49 , the improvement on the state space dependence that we could achieve in theorem  [ thm : stateimprovement ] for ` oms ` is not as straightforward for ` oams `  and remains an open question just as the optimality of the bound with respect to the other appearing parameters .",
    "we note that this is still an open question even for learning in mdps ( without additionally selecting the state representation ) as well , cf .",
    "@xcite .",
    "another direction for future work is the extension to the case when the underlying true mdp has continuous state space . in this setting ,",
    "the models have the natural interpretation of being discretizations of the original state space .",
    "this could also give improvements over current regret bounds for continuous reinforcement learning as given in  @xcite .",
    "of course , the most challenging goal remains to generate suitable state representation models algorithmically instead of assuming them to be given , cf .",
    "however , at the moment it is not even clear how to deal with the case when an infinite set of models is given .",
    "the proof is divided into three parts and follows the lines of  @xcite , now taking into account the necessary modifications to deal with the approximation error .",
    "first , in section  [ sec : err - agg ] we deal with the error of @xmath57-approximations . then in section  [ sub : test ] , we show that all state - representation models @xmath15 which are an @xmath145-approximation of a markov model pass the test in on the rewards collected so far with high probability , provided that the estimate @xmath176",
    ". finally , in section  [ sub : regret ] we use this result to derive the regret bound of theorem  [ thm : mainfinite ] .",
    "we start with some observations about the empirical rewards and transition probabilities our algorithm calculates and employs for each model @xmath15 . while the estimated rewards @xmath177 and transition probabilities @xmath178 used by the algorithm do in general not correspond to some underlying true values , the expectation values of @xmath177 and @xmath178 are still well - defined , given the history @xmath179 so far . indeed , consider some @xmath179 with @xmath180 , @xmath181 , and an action @xmath61 , and",
    "assume that the estimates @xmath182 and @xmath183 are calculated from samples when action  @xmath61 was chosen after histories @xmath184 that are mapped to the same state  @xmath185 by @xmath15 .",
    "( in the following , we will denote the states of an approximation  @xmath15 by variables with dot , such as @xmath185 , @xmath186 , etc . , and states in the state space @xmath74 of the true markov model  @xmath30 without a dot , such as @xmath35 , @xmath34 , etc . )",
    "since rewards and transition probabilities are well - defined under @xmath30 , we have @xmath187 & = & \\tfrac1n\\sum_{i=1}^n r(\\phi^\\circ(h_i),a ) ,   \\mbox { and }        { \\mathbb{e}}[{\\widehat}p({\\dot{s}}'|{\\dot{s}},a ) ] =              \\tfrac1n\\sum_{i=1}^n \\sum_{h':\\phi(h')={\\dot{s } } ' } \\!\\!\\!\\ !",
    "p(h'|h_i , a ) .",
    "\\qquad \\label{eq : what - p}\\label{eq : what - r } \\end{aligned}\\ ] ]    since @xmath15 maps the histories @xmath188 to the same state @xmath189 , the rewards and transition probabilities in the states @xmath190 of the true underlying mdp are @xmath57-close , cf .  .",
    "it follows that for @xmath64 and @xmath191 @xmath192 - r(s , a ) \\big|                =    \\big|   \\tfrac1n\\sum_{i=1}^n \\big(r(\\phi^\\circ(h_i),a ) - r(\\phi^\\circ(h),a)\\big ) \\big|",
    "< { \\epsilon}(\\phi ) .",
    "\\label{eq : model - r}\\end{aligned}\\ ] ] for the transition probabilities we have by for @xmath193 @xmath194 further , all @xmath195 as well as @xmath71 are mapped to @xmath185 by @xmath15 so that according to and recalling that @xmath64 we have for @xmath193 @xmath196 by and for @xmath193 @xmath197 so that from and we can finally bound @xmath198 -   \\sum_{s'\\in { \\mathcal{s}}^\\circ:\\alpha(s')={\\dot{s } } ' } p(s ' | s , a ) \\big|   }   \\nonumber \\\\              & \\leq &    \\tfrac1n\\sum_{i=1}^n   \\sum_{{\\dot{s } } ' \\in { \\mathcal{s}}_\\phi } \\big| p^{{\\rm agg}}({\\dot{s}}'|h_i , a ) -   \\sum_{s'\\in { \\mathcal{s}}^\\circ:\\alpha(s')={\\dot{s } } ' } p(s ' | s , a )   \\big|   \\,<\\ , { \\epsilon}(\\phi ) .",
    "\\label{eq : model - p } \\end{aligned}\\ ] ] thus , according to and the @xmath57-approximate model @xmath15 gives rise to an mdp @xmath82 on @xmath17 with rewards @xmath199 $ ] and transition probabilities @xmath200 $ ] that is an @xmath57-approximation of the true mdp @xmath38 .",
    "note that @xmath82 actually depends on the history so far .",
    "the following lemma gives some basic confidence intervals for the estimated rewards and transition probabilities .",
    "for a proof sketch see appendix  [ app : proof - chernov ] .",
    "[ lem : chernov ] let @xmath2 be an arbitrary time step and @xmath201 be the model employed at step  @xmath2 . then the estimated rewards @xmath202 and transition probabilities @xmath203 satisfy for all @xmath204 and all @xmath205 @xmath206 & \\leq & \\sqrt{\\tfrac{\\log(48 s_\\phi a t^3/\\delta)}{n_t({\\dot{s}},a ) } } , \\\\   { \\textstyle}\\big\\|{\\widehat}{p}(\\cdot|{\\dot{s}},a ) - { \\mathbb{e}}[{\\widehat}{p}(\\cdot|{\\dot{s}},a)]\\big\\|_1   & \\leq & \\sqrt{\\tfrac{2 s_\\phi \\log(48 s_\\phi a t^3 /\\delta)}{n_t({\\dot{s}},a ) } } , \\end{aligned}\\ ] ] each with probability at least @xmath207 .",
    "the following is a consequence of theorem  [ thm : aggubo ] , see appendix  [ app : lem3 ] for a detailed proof .",
    "[ lem:3 ] let @xmath30 be the underlying true markov model leading to mdp @xmath208 , and @xmath15 be an @xmath57-approximation of @xmath30 .",
    "assume that the confidence intervals given in lemma  [ lem : chernov ] hold at step @xmath2 for all states @xmath204 and all actions  @xmath61 .",
    "then the optimistic average reward @xmath209 defined in satisfies @xmath210      assume that the model @xmath211 employed in run  @xmath104 of episode  @xmath105 is an @xmath212-approximation of the true markov model .",
    "we are going to show that @xmath213 will pass the test on the collected rewards with high probability at any step  @xmath2 , provided that @xmath214 .",
    "[ lem : error - t ] for each step @xmath2 in some run  @xmath104 of some episode  @xmath105 , given that @xmath215 the chosen model @xmath213 passes the test in at step @xmath2 with probability at least @xmath216 whenever @xmath217 .    in the following , @xmath218 and @xmath219 are the states at time step @xmath220 under model @xmath213 and the true markov model @xmath30 , respectively",
    ".    * initial decomposition .",
    "* first note that at time @xmath2 when the test is performed , we have @xmath221 , so that @xmath222 where @xmath223 is the empirical average reward collected for choosing @xmath61 in @xmath185 from time @xmath224 to the current time  @xmath2 in run @xmath104 of episode @xmath105 .",
    "let @xmath225 be the rewards and @xmath226 the transition probabilities of the optimistic model @xmath227 .",
    "noting that @xmath228 when @xmath229 , we get @xmath230 we continue bounding the two terms and separately .    *",
    "bounding the reward term . *",
    "recall that @xmath25 is the mean reward for choosing @xmath61 in @xmath35 in the true markov model @xmath30 .",
    "then we have at each time step @xmath231 @xmath232\\big )   + \\big (   { \\mathbb{e}}[{\\widehat}r_{t'}({\\dot{s}}_\\tau , a ) ]   -    r(s_\\tau , a)\\big )   \\nonumber\\\\     & &     +    \\big(r(s_\\tau ,",
    "a ) - { \\mathbb{e}}[{\\widehat}{r}_{t':t}({\\dot{s}}_\\tau , a)]\\big )    +   \\big({\\mathbb{e}}[{\\widehat}{r}_{t':t}({\\dot{s}}_\\tau , a ) ]   -   { \\widehat}{r}_{t':t}({\\dot{s}}_\\tau ,",
    "a)\\big)\\   \\nonumber\\\\ & \\leq &    { \\textstyle}{\\tilde{{\\epsilon}}}_{kj } + 2 \\sqrt{\\frac{\\log(48 s_{{kj } } a t'^3/\\delta)}{2n_{t'}({\\dot{s}},a ) } } + 2 { \\epsilon}_{kj }         +   \\sqrt{\\frac{\\log(48 s_{{kj } } a t'^3/\\delta)}{2v_{kj}({\\dot{s}},a ) } } ,    \\label{eq : dec}\\end{aligned}\\ ] ] where we bounded the first term in the decomposition by  , the second term by lemma  [ lem : chernov ] , the third and fourth according to , and the fifth by an equivalent to lemma  [ lem : chernov ] for the rewards collected so far in the current run . in summary , with probability at least @xmath233 we can bound as @xmath234 where we used the assumption that @xmath235 as well as @xmath236 .    * bounding the bias term . *",
    "first , notice that we can use to bound @xmath237 where @xmath238 are the state values given by evi .",
    "further , since the transition probabilities @xmath226 sum to  @xmath239 , this is invariant under a translation of the vector @xmath240 . in particular , defining @xmath241 , so that @xmath242 , we can replace @xmath240 with @xmath243 , and can be bounded as @xmath244 now we decompose for each time step @xmath231 @xmath245\\big ) \\ , w_{kj}({\\dot{s } } ' )    \\label{eq : bias2}\\\\ & &   + \\sum_{{\\dot{s}}'\\in{\\mathcal{s}}_{kj } } \\big ( { \\mathbb{e}}[{\\widehat}p_{t'}({\\dot{s}}'|{\\dot{s}}_\\tau , a ) ]   - \\sum_{s ' : \\alpha(s')={\\dot{s } } ' }   p ( s ' | s_\\tau , a ) \\big ) \\ , w_{kj}({\\dot{s } } ' )   \\label{eq : bias3}\\\\ & &   + \\sum_{{\\dot{s}}'\\in{\\mathcal{s}}_{kj}}\\sum_{s ' : \\alpha(s')={\\dot{s } } ' }    p ( s ' | s_\\tau , a ) \\ ,   w_{kj}({\\dot{s } } ' )   -    w_{kj}({\\dot{s}}_\\tau )    \\label{eq : bias4}\\end{aligned}\\ ] ] and continue bounding each of these terms individually .    * bounding :* using @xmath246 , is bounded according to as @xmath247    * bounding :* similarly , by lemma  [ lem : chernov ] with probability at least @xmath248 we can bound at all time steps @xmath220 as @xmath249\\big ) \\ , w_{kj}({\\dot{s } } ' )           \\leq \\tfrac{{\\lambda}_{kj}^+}{2}\\sqrt{\\tfrac{2s_{kj}\\log(48 s_{kj } a t'^3/\\delta)}{n_{t'}(s , a ) } } .",
    "\\label{eq : bias2a}\\end{aligned}\\ ] ]    * bounding :* by and using that @xmath246 , we can bound   by @xmath250   - \\sum_{s ' : \\alpha(s')={\\dot{s } } ' }   p ( s ' | s_\\tau , a ) \\big ) \\ , w_{kj}({\\dot{s } } ' ) < \\tfrac{{\\epsilon}_{kj } { \\lambda}_{kj}^+}{2}.\\ ] ]    * bounding :* we set @xmath251 for @xmath252 and rewrite as @xmath253 summing this term over all steps @xmath231 , we can rewrite the sum as a martingale difference sequence , so that azuma - hoeffding s inequality ( e.g. , lemma  10 of @xcite ) yields that with probability at least @xmath254 @xmath255 since the sequence @xmath256 is a martingale difference sequence with @xmath257 .    * wrap - up . *",
    "summing over the steps @xmath231 , we get from , , , , , , and that with probability at least @xmath233 @xmath258 using that @xmath259 and the assumption that @xmath260 . combining , , and gives the claimed lemma .",
    "summing lemma  [ lem : error - t ] over all episodes gives the following lemma , for a detailed proof see appendix  [ app : error ] .",
    "[ lem : error ] with probability at least @xmath169 , for all runs  @xmath104 of all episodes  @xmath105 the chosen model @xmath213 passes all tests , provided that @xmath217 .",
    "we start with some auxiliary results for the proof of theorem  [ thm : mainfinite ] .",
    "lemma [ lem : bd ] bounds the bias span of the optimistic policy , lemma  [ lem : geps ] deals with the estimated precision of @xmath213 , and lemma  [ lem : episodes ] provides a bound for the number of episodes . for proofs",
    "see appendix  [ app : bd ] , [ app : geps ] , and [ app : episodes ] .    [ lem : bd ] assume that the confidence intervals given in lemma  [ lem : chernov ] hold at some step  @xmath2 for all states @xmath189 and all actions @xmath61 .",
    "then for each @xmath15 , the set of plausible mdps @xmath261 contains an mdp @xmath262 with diameter @xmath263 upper bounded by the true diameter @xmath47 , provided that @xmath264 .",
    "consequently , the respective bias span @xmath154 is bounded by @xmath47 as well .    [",
    "lem : geps ] if all chosen models @xmath213 pass all tests in run  @xmath104 of episode  @xmath105 whenever @xmath265 , then @xmath266 always holds for all models @xmath15 .",
    "[ lem : episodes ] assume that all chosen models @xmath213 pass all tests in run  @xmath104 of episode  @xmath105 whenever @xmath265 .",
    "then the number of episodes @xmath267 after any @xmath167 steps is upper bounded as @xmath268 .",
    "now we can finally turn to showing the regret bound of theorem  [ thm : mainfinite ] .",
    "we will assume that all chosen models @xmath213 pass all tests in run  @xmath104 of episode  @xmath105 whenever @xmath265 . according to lemma [ lem :",
    "error ] this holds with probability at least @xmath169 .",
    "let @xmath211 be the model that has been chosen at time @xmath269 , and consider the last but one step @xmath2 of run  @xmath104 in episode @xmath105 .",
    "the regret @xmath270 of run  @xmath104 in episode @xmath105 with respect to @xmath271 is bounded by @xmath272 where as before @xmath273 denotes the length of run  @xmath104 in episode @xmath105 up to the considered step @xmath2 . by assumption",
    "the test on the collected rewards has been passed at step @xmath2 , so that @xmath274 and we continue bounding the terms of @xmath139 .    * bounding the regret with the penalization term .",
    "* since we have @xmath275 for all @xmath276 and also @xmath277 , by cauchy - schwarz inequality @xmath278 .",
    "applying this to the definition   of @xmath279 , we obtain from and by the definition   of the penalty term that @xmath280    * the key step .",
    "* now , by definition of the algorithm and lemma [ lem:3 ] , for any approximate model @xmath15 we have @xmath281 or equivalently @xmath282 . multiplying this inequality with @xmath136 and noting that @xmath283 then gives @xmath284 combining this with , we get by application of lemma  [ lem : bd ] , i.e. , @xmath285 , and the definition of the penalty term   that @xmath286 by lemma  [ lem : geps ] and using that @xmath287 ( so that @xmath288 ) we get @xmath289    * summing over runs and episodes . *",
    "let @xmath290 be the total number of runs in episode @xmath105 , and let @xmath267 be the total number of episodes up to time @xmath49 .",
    "noting that @xmath291 and summing over all runs and episodes gives @xmath292 as shown in section 5.2 of  @xcite , @xmath293 , @xmath294 and @xmath295 , and we may conclude the proof applying lemma  [ lem : episodes ] and some minor simplifications .",
    "this research was funded by the austrian science fund ( fwf ) : p  26219-n15 , the european community s fp7 program under grant agreements n@xmath296270327 ( complacs ) and 306638 ( suprel ) , the technion , the ministry of higher education and research of france , nord - pas - de - calais regional council , and feder ( contrat de projets etat region cper 2007 - 2013 ) .    1 [ 1]`#1",
    "`    bartlett , p.l . ,",
    "tewari , a. : regal : a regularization based algorithm for reinforcement learning in weakly communicating mdps . in : uai 2009 ,",
    "25th conf .  on uncertainty in artificial intelligence , pp .",
    "auai press ( 2009 )    hallak , a. , castro , d.d .",
    ", mannor , s. : model selection in markovian processes . in : 19th acm sigkdd intl conf .  on knowledge discovery and data mining , kdd 2013 , pp",
    ".  374382 .",
    "acm ( 2013 )    hutter , m. : feature reinforcement learning : part i : unstructured mdps .",
    "j.  artificial general intelligence  1 , 324 ( 2009 )    littman , m , sutton , r. , singh s. : predictive representations of state . in : adv .  neural inf .",
    "process .",
    "15 , pp .  15551561",
    "( 2002 )    jaksch , t. , ortner , r. , auer , p. : near - optimal regret bounds for reinforcement learning . j.  mach .",
    "res .  11 , 15631600 ( 2010 )    maillard , o.a . ,",
    "munos , r. , ryabko , d. : selecting the state - representation in reinforcement learning . in : adv .",
    "neural inf .  process .",
    "24 , pp .",
    "26272635 ( 2012 )    maillard , o.a . , nguyen , p. , ortner , r. , ryabko , d. : optimal regret bounds for selecting the state representation in reinforcement learning . in : proc .",
    "30th intl conf .  on machine learning , icml 2013 , jmlr proc .",
    "28 , pp .  543551",
    "( 2013 )    nguyen , p. , maillard , o.a . ,",
    "ryabko , d. , ortner , r. : competing with an infinite set of models in reinforcement learning . in : proc .",
    "16th intl conf .  on artificial intelligence and statistics , aistats 2013 , jmlr proc .",
    "31 , pp .  463471",
    "( 2013 )    ortner , r. : pseudometrics for state aggregation in average reward markov decision processes . in : alt 2007 .",
    "lncs ( lnai ) , vol .  4754 , pp .",
    "springer ( 2007 )    ortner , r. , ryabko , d. : online regret bounds for undiscounted continuous reinforcement learning , in : adv .",
    "neural inf .  process .",
    "syst .  25 , pp .",
    "17721780 ( 2012 )",
    "we start with an error bound for approximation , assuming we compare two mdps over the same state space .",
    "[ lem:1 ] consider a communicating mdp @xmath77 , and another mdp @xmath297 over the same state - action space which is an @xmath57-approximation of @xmath81 ( for @xmath298 ) .",
    "assume that an optimal policy @xmath299 of @xmath81 is performed on @xmath86 for @xmath300 steps , and let @xmath301 be the number of times state @xmath35 is visited state among these @xmath300 steps .",
    "then @xmath302 with probability at least @xmath169 , where @xmath303 is the diameter of @xmath81",
    ".    we abbreviate @xmath304 and @xmath305 , and use @xmath306 and @xmath307 accordingly . then @xmath308 now , for the first term in we can use the poisson equation ( for the optimal policy @xmath299 on @xmath81 ) to replace @xmath309 , writing @xmath310 for the bias of @xmath311 on @xmath81 . concerning the second term in we can use and the fact that @xmath312 . in summary",
    ", we get @xmath313 by and using that @xmath314 , the last term in is bounded as @xmath315 on the other hand , for the second term in , writing @xmath316 for the state visited at time step @xmath220 we have @xmath317 now @xmath318 , while the sequence @xmath319 is a martingale difference sequence with @xmath320 .",
    "thus , an application of azuma - hoeffding s inequality ( e.g. , lemma 10 of @xcite ) to yields that @xmath321 with probability higher than @xmath169 . summarizing , , , and give the claimed @xmath322    as a corollary to lemma  [ lem:1 ] we can also bound the approximation error in average reward , which we will need below to deal with the error of @xmath57-approximate models .",
    "[ lem:2 ] let @xmath81 be a communicating mdp with optimal policy @xmath311 , and @xmath86 an @xmath57-approximation of @xmath81 over the same state space . then @xmath323    divide the result of lemma  [ lem:1 ] by @xmath300 , choose @xmath324 , and let @xmath325 . since the average reward of a policy is no random value , the result holds surely and not just with probability 1 .",
    "the idea is to define a new mdp @xmath326 on @xmath79 whose rewards  @xmath327 and transition probabilities @xmath328 are @xmath57-close to @xmath81 and that has the same optimal average reward as @xmath86 .",
    "thus , for each state @xmath329 and each action @xmath61 we set @xmath330 and @xmath331 note that @xmath332 is indeed a probability distribution over @xmath79 , that is , in particular it holds that @xmath333 now by definition , the rewards @xmath334 and aggregated transition probabilities @xmath335 in @xmath336 have the same values for all states @xmath35 that are mapped to the same meta - state by @xmath337 .",
    "it follows that @xmath338 .",
    "further by assumption , according to we have @xmath339 and @xmath340 thus , @xmath336 is an @xmath57-approximation of @xmath81 that has the same optimal average reward as @xmath86 so that application of lemma  [ lem:2 ] to @xmath81 and @xmath336 gives the claimed result .",
    "for any given number of observations @xmath341 it holds that ( cf .",
    "appendix c.1 of  @xcite ) for any @xmath342 @xmath343 \\leq \\sqrt{\\tfrac{\\log(2/\\theta)}{n } } \\bigg\\ } < \\theta , \\\\    { \\textstyle}{\\mathbb{p}}\\bigg\\ { \\big\\|{\\widehat}{p}(\\cdot|{\\dot{s}},a ) - { \\mathbb{e}}[{\\widehat}{p}(\\cdot|{\\dot{s}},a)]\\big\\|_1   \\leq \\sqrt{\\tfrac{2 s_\\phi \\log(2 /\\theta)}{n } }   \\bigg\\ } < \\theta.\\end{aligned}\\ ] ] choosing suitable values for @xmath344 , a union bound over all states @xmath185 , all actions @xmath61 and all possible values for @xmath345 shows the lemma .",
    "let @xmath82 be the mdp on @xmath75 whose rewards and transition probabilities are given by the expectation values @xmath346 $ ] and @xmath347 $ ] , respectively .",
    "we have already seen in and that @xmath82 is an @xmath57-approximation of the true mdp @xmath81 , so that by theorem  [ thm : aggubo ] @xmath348    it remains to deal with the difference between @xmath349 and @xmath209 . by assumption ,",
    "the confidence intervals of lemma  [ lem : chernov ] hold for all state - action - pairs so that @xmath82 is contained in the set of plausible mdps @xmath350 ( defined via the empirical rewards and transition probabilities @xmath182 and @xmath351 ) .",
    "it follows together with that @xmath352 which together with proves the claimed inequality .",
    "by lemma  [ lem : error - t ] , at each step  @xmath2 of a run  @xmath104 in an episode  @xmath105 starting at step  @xmath215 the test is passed with probability at least @xmath216 . assuming that @xmath353 is the last step in that run and setting @xmath354 to be the total number of steps in that run",
    ", the test is passed in all steps of the run with error probability bounded by ( using that @xmath355 ) @xmath356 summing over all episodes and runs shows that the test is passed in all time steps with probability at least @xmath357 .",
    "we define an mdp @xmath262 on state space @xmath75 as follows .",
    "first let @xmath358 be an arbitrary mapping that maps states in @xmath75 to some state in @xmath79 such that @xmath359 .",
    "intuitively , @xmath360 is an arbitrary reference state that is mapped to @xmath185 by @xmath337 .",
    "then for @xmath361 in @xmath75 we set the transition probabilities of @xmath262 as @xmath362 then by and lemma  [ lem : chernov ] we obtain @xmath363 \\big|                            +    \\big| { \\mathbb{e } } [ { \\widehat}p_t({\\dot{s}}'|{\\dot{s}},a ) ] - { \\widehat}p_t({\\dot{s}}'|{\\dot{s}},a )   \\big| \\big ) \\\\    & \\leq & { \\epsilon}(\\phi )   +   \\sqrt{\\tfrac{2s_\\phi\\log(48 s_{\\phi } a t^3/\\delta)}{n_{t}(s , a)}},\\end{aligned}\\ ] ] showing that @xmath262 is contained in @xmath261 . to see that @xmath364 , note that @xmath365 maps all transitions in @xmath262 to transitions of the the same or lower probability in the true mdp .",
    "that is , for any @xmath366 it holds that @xmath367 .",
    "thus , each trajectory in @xmath262 can be mapped to a trajectory in the true mdp that can not have higher probability , which proves the first claim of the lemma .",
    "the second claim follows immediately along the lines of section  4.3.1 in @xcite .",
    "by definition of the algorithm , @xmath113 for each model @xmath15 has initial value @xmath147 and is doubled whenever @xmath15 fails a test .",
    "thus , by assumption if @xmath368 , then as soon as @xmath369 the value of @xmath113 will not change anymore , and consequently @xmath370 always holds .    on the other hand ,",
    "if @xmath371 then also @xmath176 for the initial value @xmath372 and again by assumption @xmath372 remains unchanged , so that @xmath373 holds .",
    "first recall that an episode is terminated when either the number of visits in some state - action pair @xmath374 has been doubled ( line 12 of the algorithm ) or when the test on the accumulated rewards has failed ( line 9 ) . by assumption ,",
    "the test is passed provided that @xmath176 . if @xmath375 , then @xmath176 holds trivially",
    "otherwise , @xmath15 will fail the test only @xmath376 times until @xmath176 ( after which the test is passed w.h.p .  and",
    "@xmath113 remains unchanged by lemma  [ lem : error - t ] ) .",
    "therefore , the number of episodes terminated due to failure of the test is upper bounded by @xmath377 .    for the number of episodes terminated since",
    "the number of visits in some state - action pair @xmath374 has been doubled , one can show that it is bounded by @xmath378 , cf .",
    "appendix  c.2 of @xcite or section  5.2 of  @xcite , and the lemma follows .",
    "as the proof of the regret bound for ` oms ` given in @xcite follows the same lines as the proof of theorem  [ thm : mainfinite ] given here , we only sketch the key step leading to the improvement of the bound . note that by and since average rewards are by assumption in @xmath45 $ ] , for the model @xmath213 chosen in some run @xmath104 of some episode  @xmath105 it holds that @xmath379 .",
    "hence , by definition of the penalization term and since @xmath380 , the chosen model @xmath213 always satisfies @xmath381 some simplifications then show that @xmath382 is @xmath383 , so that one can replace the total number of all states @xmath173 in theorem  [ thm : mainfinite ] ( respectively in the regret bound of @xcite ) by the total number of states of models @xmath15 with @xmath384 and consequently by @xmath385 ."
  ],
  "abstract_text": [
    "<S> we consider a reinforcement learning setting introduced in  @xcite where the learner does not have explicit access to the states of the underlying markov decision process ( mdp ) . </S>",
    "<S> instead , she has access to several models that map histories of past interactions to states . </S>",
    "<S> here we improve over known regret bounds in this setting , and more importantly generalize to the case where the models given to the learner do not contain a true model resulting in an mdp representation but only approximations of it . </S>",
    "<S> we also give improved error bounds for state aggregation . </S>"
  ]
}