{
  "article_text": [
    "though typically unnecessary for computational parameter inference in the bayesian framework , the factor , @xmath0 , required to normalize the product of prior and likelihood nevertheless plays a vital role in bayesian model selection and model averaging @xcite . for priors admitting an ` ordinary ' density , @xmath1 , with respect to lebesgue measure ( a `` @xmath2-density '' ) we write for the posterior , @xmath3 and more generally ( e.g.  for stochastic process priors ) we write @xmath4 with the likelihood , @xmath5 , a non - negative , real - valued function supposed integrable with respect to the prior . in this context @xmath0",
    "is generally referred to as either the _ marginal likelihood _",
    "( i.e. , the likelihood of the observed data marginalized [ averaged ] over the prior ) or the _",
    "evidence_. with the latter term though , one risks the impression of over - stating the value of this statistic in the case of limited prior knowledge ( cf .",
    "@xcite , ch .  6 ) .",
    "problematically , few complex statistical problems admit an analytical solution to equations [ bayes ] or [ bayesalt ] , nor span such low dimensional spaces ( @xmath6 - 10 ) that direct numerical integration presents a viable alternative . with errors ( at least in principle )",
    "independent of dimension , monte carlo - based integration methods have thus become the mode of choice for marginal likelihood estimation across a diverse range of scientific disciplines , from evolutionary biology @xcite and cosmology @xcite to quantitative finance @xcite and sociology @xcite .      with the posterior most often",
    "` thinner - tailed ' than the prior and/or constrained within a much diminished sub - volume of the given parameter space the simplest marginal likelihood estimators drawing solely from @xmath1 or @xmath7 can not be relied upon for model selection purposes . in the first case  strictly , that @xmath8 \\pi(\\theta ) d\\theta$ ] diverges  the harmonic mean estimator ( hme ; @xcite ) , @xmath9^{-1 } \\mathrm{\\ for\\ } \\theta_i \\sim \\pi(\\theta|y),\\ ] ] suffers _ theoretically _ from an infinite variance , meaning _ in practice _ that its convergence towards the true @xmath0 as a one - sided @xmath10-stable limit law can be incredibly slow @xcite .",
    "even when ` robustified ' as per @xcite or @xcite , however , the hme remains notably insensitive to changes in @xmath1 , whereas @xmath0 itself is characteristically sensitive @xcite .",
    "( see also @xcite for yet another approach to robustifying the hme . )",
    "though assuredly finite by default the variance of the prior arithmetic mean estimator ( ame ) , @xmath11 on the other hand , will remain impractically large whenever there exists a substantial difference in ` volume ' between the regions of greatest concentration in prior and posterior mass , with huge sample sizes necessary to achieve reasonable accuracy ( e.g.  @xcite ) .",
    "a wealth of more sophisticated integration methods have thus lately been developed for generating improved estimates of the marginal likelihood , as reviewed in depth by @xcite , @xcite , and @xcite .",
    "notable examples include : adaptive multiple importance sampling @xcite , annealed importance sampling @xcite , bridge sampling @xcite , [ ordinary ] importance sampling ( cf .",
    "@xcite ) , path sampling / thermodynamic integration @xcite , nested sampling @xcite , nested importance sampling @xcite , reverse logistic regression @xcite , sequential monte carlo ( smc ; @xcite ) , the savage - dickey density ratio @xcite , and the density of states @xcite .",
    "a common thread running through almost all these schemes is the aim for a superior exploration of the relevant parameter space via ` guided ' transitions across a sequence of intermediate distributions , typically following a bridging path between the @xmath1 and @xmath7 extremes .",
    "( or , more generally , the @xmath12 and @xmath7 extremes if a suitable auxiliary / reference density , @xmath12 , is available to facilitate the integration ; cf .  @xcite . ) however , the nature of this bridging path differs significantly between algorithms .",
    "nested sampling , for instance , evolves its ` live point set ' over a sequence of constrained - likelihood distributions , @xmath13 , transitioning from the prior ( @xmath14 ) through to the vicinity of peak likelihood ( @xmath15 ) ; while thermodynamic integration , on the other hand , draws progressively ( via markov chain monte carlo [ mcmc ] ; @xcite ) from the family of ` power posteriors ' , @xmath16 explicitly connecting the prior at @xmath17 to the posterior at @xmath18 .",
    "another key point of comparison between these rival monte carlo techniques lies in their choice of identity by which the evidence is ultimately computed .",
    "the ( geometric ) path sampling identity , @xmath19 for example , is shared across both thermodynamic integration and smc , in addition to its namesake .",
    "however , smc can also be run with the `` stepping - stone '' solution ( cf .",
    "@xcite ) , @xmath20 with @xmath21 indexing a sequence of ( `` tempered '' ) bridging densities , and indeed this is the mode prefered by experienced practitioners ( e.g.  @xcite ) .",
    "yet another identity for computing the marginal likelihood is that of the recursive pathway explored here .",
    "first introduced within the `` biased sampling '' paradigm @xcite , the recursive pathway is shared by the popular techniques of `` reverse logistic regression '' ( rlr ) and `` the density of states '' ( dos ) . by _ recursive _ we mean that , algorithmically , each may be run such that the desired @xmath0 is obtained through backwards induction of the complete set of intermediate normalizing constants corresponding to the sequence of distributions in the given bridging path by supposing these to be already known .",
    "that is , a stable solution may be found in a gauss - seidel - type manner @xcite by starting with a guess for each normalizing constant as input to a convex system of equations for updating these guesses , returning the new output as input to the same equations , and iterating until convergence . in fact , although the rlr and the dos approaches differ vastly in concept and derivation ",
    "the former emerging from considerations of the reweighting mixtures problem in applied statistics @xcite and the latter from computational strategies for free energy estimation in physics / chemistry / biology @xcite  both may be seen to recover the same algorithmic form in practice . to illustrate this equivalence , and to explain further the recursive pathway to marginal likelihood estimation",
    ", we describe each in detail below ( sectioins [ rlrsection ] and [ dossection ] ) , though we begin with the more general biased sampling algorithm ( section [ bs ] ) .",
    "following this review of the recursive family ( which includes our theoretical contributions concerning the link between the dos and nested sampling in section [ rns ] ) we highlight the potential for efficient prior - sensitivity analysis when using these marginal likelihood estimators ( section [ psa ] ) and discuss issues regarding design and sampling of the bridging sequence ( section [ fisher ] ) .",
    "we then introduce a novel heuristic to help inform the latter by characterizing the connection between the bridging sequences of biased sampling and thermodynamic integration ( section [ tivis ] ) . finally , we present two numerical case studies illustrating the issues and techniques discussed in the previous sections : the first concerns a ` mock ' banana - shaped likelihood function ( section [ banana ] ) and includes the demonstration of a novel synthesis of the recursive pathway with nested sampling ( section [ nestedcomp ] ) ; while the second concerns mixture modeling of the galaxy dataset ( section [ mixtures ] ) and includes a demonstration of importance sample reweighting of an infinite dimensional mixture posterior to recover its finite dimensional counterparts ( section [ immr ] ) .",
    "the archetypal recursive marginal likelihood estimator  from which both the rlr and dos methods may be directly recovered  is that of biased sampling , introduced by @xcite for finite - dimensional parameter spaces and extended to general sample spaces by @xcite .",
    "the basic premise of biased sampling is that one has available @xmath22 sets of @xmath23 iid draws , @xmath24 , from a series of @xmath25-weighted versions of a common , unknown measure , @xmath26 ; that is , @xmath27 the @xmath28 term here represents the normalization constant of the @xmath29-th weighted distribution , typically unknown .",
    "as @xcite demonstrates , provided the drawn @xmath24 obey a certain graphical condition ( discussed later ) then there exists a unique non - parametric maximum likelihood estimator ( npmle ) for @xmath26 , which as a by - product produces consistent estimates of all unknown @xmath28 . if the common measure , @xmath26 , is in fact the parameter prior , @xmath30 , then the choices @xmath31 and @xmath32 describe sampling from the prior and posterior , respectively . hence , we switch to the notation @xmath33 with @xmath34 ( for a proper prior ) and @xmath35 for the above choices of @xmath36 and @xmath37 .    for a given bridging scheme to be amenable to normalization via biased sampling",
    "it is of course necessary that each intermediate sampling distribution be absolutely continuous with respect to the prior ( i.e. , @xmath38 ) such that the weight function corresponds to the radon - nikodym derivative , @xmath39 .",
    "it is easy to verify then the applicability of biased sampling to : e.g. ( i ) importance sampling from a sequence of bridging densities , @xmath40 , with ( at least the union of their ) supports matching but not exceeding that of a @xmath2-density prior , @xmath41 ; and ( ii ) thermodynamic integration over tempered likelihoods , @xmath42 , for both the @xmath2-density and general case .",
    "in fact , if we view the likelihood function as defining a transformation of the prior , @xmath30 , to the measure @xmath43 in univariate `` likelihood space '' , @xmath44 , then such tempering may be seen as directly analogous to vardi s example of `` length biased sampling '' .",
    "accordingly , vardi s case study of @xmath45 with @xmath46 and @xmath47 ( read @xmath48 ) equates to marginal likelihood estimation via defensive importance sampling from the prior and posterior @xcite , while his one sample study with @xmath49 ( @xmath48 ) matches the hme .    for bayesian analysis problems in which the prior measure is explicitly known ( as opposed to being ` known ' only implicitly as the induced measure belonging to a well - defined stochastic process )",
    "the application of the biased sampling paradigm to the task of marginal likelihood estimation is arguably paradoxical since we make the pretence to estimate @xmath30 ( known ) in order to recover an estimate for @xmath0 ( unknown ) .",
    "however , we would propose that an adequate justification for the use of vardi s method in this context is already provided by the same pragmatic reasoning used to adopt _ any _ statistical estimator for the task of marginal likelihood computation in place of the direct approach of numerical integration ( quadrature)namely , that although @xmath0 is defined exactly by our known prior and likelihood function we choose to treat it as if it were an unknown variable simply because the mc integration techniques this brings into play are more computationally efficient ( being relatively insensitve to the dimension of the problem ; cf .",
    "@xcite ) .",
    "vardi s derivation of the nmple for the unknown @xmath26 ( i.e. , @xmath30 ) in biased sampling involves two key steps .",
    "the first is the observation that , as is typical of the nmple method in general , the resulting estimator , @xmath50 , will be strictly atomic with point masses assigned to each of the sampled @xmath51 ( also called a histogram estimate of @xmath26 ) .",
    "the second is that the normalization constants for each @xmath28 corresponding to the atomic @xmath50 can then be learnt via an appropriately weighted summation over _ all _ the observed @xmath51 ( not just those from the corresponding @xmath29-th distribution ) . in the notation for our marginal likelihood estimation scenario",
    ", @xcite shows that the estimation problem for @xmath52 can ultimately be reduced to the maximization of the following log - likelihood function , @xmath53 subject to the constraints , @xmath54 and all @xmath55 ( see vardi s equation 2.2 , where we avoid his explicit treatment of matching @xmath51 draws ; implicitly allowing multiple point mass contributions at the same @xmath51 to give a summed contribution to the atomic @xmath56 ) .",
    "importantly , the resulting biased sampling estimator for the unknown @xmath57 allows for a recursive solution via the iterative updating of initial guesses ( @xmath58 ) as follows , @xmath59 ( adapted from gill et al.s 1988 proposition 1.1 c ) . as discussed by @xcite and @xcite the above system of @xmath60 equations in @xmath60 unknowns ( given @xmath61 ) with gauss - seidel type iterative updates is globally convergent ; although the gradient and hessian of the likelihood function are also accessible , meaning that alternative maximization strategies harnessing this information may prove more efficient within a restricted domain .",
    "the convergence properties of the biased sampling estimator for the unknown @xmath26 ( i.e. , @xmath30 ) and its associated @xmath28 ( @xmath62 ) in general state spaces ( possibly infinite - dimensional ) have been thoroughly characterized by @xcite using the theory of empirical processes indexed by sets and functions ( cf .",
    "in particular , @xcite demonstrate a central limit theorem ( clt ) for convergence of the vector of normalization estimates , @xmath63 , to the truth , @xmath64 , as @xmath65{d } \\mathcal{n}(\\mathbf{0},\\sigma)$ ] , where the covariance matrix , @xmath66 , takes the form given in their proposition 2.3 ( for the case here of @xmath61 known , otherwise their equation 2.24 ) .",
    "the sample - based estimate of this error matrix , @xmath67 , is easily computed from the output of a standard biased sampling simulation , and in our numerical experiments with the banana - shaped pseudo - likelihood function of section [ banana ] it was observed to give ( on average , with an approximate transformation via slutsky s lemma ) a satisfactory , though slightly conservative , match to the sample variance of @xmath68 under repeat simulation , even at relatively small sample sizes .    however , as noted by christian robert in his discussion of kong et al.s ( 2003 ) `` read '' paper , the availability of such formulae ( for the asymptotic covariance matrix ) can sometimes `` give a false confidence in estimators that should not be used '' .",
    "a canonical example is that of the hme , for which the usual importance sampling variance formula applied to the posterior draws may well give a finite result though in fact the theoretical variance is infinite ( meaning that the convergence of the hme is no longer obeying the assumed clt ) . in particular , for finite theoretical variance of the hme ( cf .",
    "section [ intro ] ) we require that the prior is fatter - tailed than the posterior such that @xmath69\\pi(\\theta)d\\theta < \\infty$ ] .",
    "as was recognised by @xcite and @xcite , the same condition effectively holds for the validity of the clt for biased sampling and may be expressed as an inverse mean bias - weighted integrability requirement over the indexing class of functions or sets in its empirical process construction .",
    "important to note in the context of marginal likelihood estimation is that provided the prior itself is contained within the weighting scheme ( e.g. , @xmath70 ) then the above condition is automatically satisfied ; this of course parallels the strategy of defensive importance sampling @xcite .    finally , we observe here the other key prerequisite for successful biased sampling : that the bridging sequence of weighting functions and the random draws from them are such that a unique npmle for @xmath26 ( @xmath30 ) actually exists . to ensure the _ asymptotic _ existence of a unique npmle ( i.e. , with an unlimited number of draws from each weighted distribution ) @xcite gives the following condition on the supports , @xmath71 , of the bridging sequence : that there does not exist a proper subset , @xmath72 , of @xmath73 such that @xmath74 in effect , the set of bridging distributions must overlap in such a way that the relative normalization of each with respect to all others will be inevitably constrained by the data .",
    "this condition is again satisfied automatically if the support of at least one of the bridging distributions encompasses all others , such as that of the prior or an equivalent reference density . in the finite sample sizes of real world simulation",
    "the above must be strengthened to specify that the drawn @xmath24 do in fact cover each critical region of overlap .",
    "formally , @xcite introduces a requirement of strong connectivity on the directed graph , @xmath75 , with @xmath22 vertices and edges @xmath76 to @xmath29 for each @xmath77-pairing , such that @xmath78 for some @xmath79 .",
    "this is equivalent to the finite sample `` inseparability '' condition given by @xcite .      in the reweighting mixtures problem (",
    "cf.@xcite and @xcite ) the aim is to discover an efficient proposal density for use in the importance sampling of an arbitrary target about which little is known _",
    "a priori_. geyer s solution was to suggest sampling not from a single density of standard form , but rather from an _ ensemble _ of different densities , @xmath80 , for @xmath81 with @xmath82 known and @xmath83 typically unknown .",
    "the pooled draws , @xmath84 , are then to be treated as if from a single mixture density , with each free normalizing constant  and hence the appropriate weighting scheme  to be derived recursively . as with biased sampling , if we suppose @xmath85 to be the bayesian prior ( with @xmath86 ) and @xmath87 the ( unnormalized ) posterior ( with @xmath88 ) the relevance of this approach to marginal likelihood estimation becomes readily apparent . in this context",
    "we write the imagined ( i.e. , pseudo- ) mixture density , @xmath89 , in the form , @xmath90 [ q_{j}(\\theta)/z_j],\\ ] ] where @xmath91 .    the recursive normalization scheme introduced by @xcite for this purpose is based on maximization in @xmath92 ( i.e. , @xmath93^{m-1}$ ] ) of the following _ quasi_-log - likelihood function representing the likelihood of each set of @xmath24 having been drawn from its true @xmath94 rather than some other @xmath95}(\\theta)$ ] in the pseudo - mixture : @xmath96 owing to the arithmetic equivalence between equation [ loglx ] and the objective function of logistic regression in the generalized linear modeling framework  but with the ` predictor ' here random and the ` response ' non - random@xcite has dubbed this method , `` reverse logistic regression '' . setting the partial derivative in each unknown @xmath97 to zero yields the series of convex equations defining the rlr marginal likelihood estimator : @xmath98 which , with reference to our definition of the pseudo - mixture density above , may be confirmed equivalent to biased sampling ( equation [ zupdates ] ) in the @xmath2-density case for @xmath99 .",
    "( the @xmath1 term ultimately cancels out from both the numerator and denominator of equation [ zupdates ] , but serve here to establish our connection with the notion of a common unknown distribution , @xmath26 or @xmath30 . )    as @xcite explore in detail , the fact that geyer s rlr derivation via the quasi - log - likelihood function of equation [ loglx ] leads to the same set of recursive update equations as vardi s biased sampling hides a certain weakness of this ` retrospective formulation ' : that the hessian of the _ _ quasi-__log - likelihood does not provide the correct asymptotic covariance matrix for the output @xmath100 ( though the difference in practice is almost negligible : cf .  section [ banana ] ) .",
    "the same applies to a ` nave ' , alternative derivation of the rlr estimator ",
    "relevant to the thermodynamic integration via importance sampling methodology we describe in section [ tivis]given by @xcite in their discussion of kong et al.s `` read '' paper .",
    "that is , treat the pooled @xmath24 as if drawn from the pseudo - mixture density , @xmath89 , with @xmath57 ( @xmath101 ) unknown and apply the ordinary importance sampling estimator  based on the identity , @xmath102to recover the recursive update scheme of equation [ zupdates ] ( but again without a corresponding argument to arrive at the correct variance ) .",
    "an interesting observation often made in connection with rlr is that equation [ eleven ] can in fact be applied without knowledge of which @xmath40 each @xmath51 was drawn from , such that we may rewrite the recursive update scheme , @xmath103 where we have taken the step of ` losing the labels ' , @xmath29 , on our @xmath24 .",
    "this is made possible , as @xcite explains , because `` under the model as specified ...",
    "the association of draws with distribution labels is uninformative .",
    "the reason for this is that all the information in the labels for estimating the ratios is contained in the design constants , @xmath104 '' .",
    "yet another construction of the convex series of @xmath105 updates characterizing the recursive appoach ( cf .",
    "equation [ zupdates ] ) has recently been demonstrated in the context of free energy estimation for molecular interactions by @xcite and @xcite . in this framework rather than aiming directly for estimation of the marginal likelihood one aims instead to reconstruct a closely - related distribution : namely , `` the density of states '' ( dos ) , @xmath106 , defined in the physics literature in terms of a composition of the dirac delta ` function ' , @xmath107 , as @xmath108 important to note from a mathematical perspective , however , is that the composition of the dirac delta ` function'which is itself not strictly a function , being definable only as a measure or a generalized function  lacks an intrinsic definition .",
    "@xcite proposes a version in @xmath109 valid only when the composing function , here @xmath110 , is continuously differentiable and @xmath111 nowhere zero ; clearly problematic whenever the likelihood function holds constant over a set of non - zero measure with respect to @xmath30 ! we therefore begin by suggesting a robust , alternative definition of the dos as a transformation of the likelihood through the prior ; an exercise that also serves to elucidate its connections with skilling s nested sampling .",
    "as briefly noted earlier with respect to characterization of the hme as vardi s `` length biased sampling '' , the likelihood function can serve as the basis for construction of a number of measure theoretic transformations of the prior .",
    "most notably , the mapping @xmath112 gives the prior in likelihood space ( @xmath113 , @xmath114 for @xmath115 ( the borel sets on the extended reals ) following @xcite p.163 , with the notation , @xmath116 , denoting the ( assumed @xmath30-measurable ) set of all @xmath117 transformed through @xmath5 into @xmath72 .",
    "if the domain of @xmath117 is a metric space then continuity ( or at least discontinuity on no more than a countable set ) of @xmath5 is sufficient to ensure the @xmath30-measurability of @xmath72 ( i.e. , the validity of the above ) ; while the continuity of the logarithm in @xmath118 ensures the same for the corresponding transformation of the prior to `` energy '' space ( @xmath119 ) , @xmath120 with @xmath121 . in each case",
    "the appropriate version of the marginal likelihood shares equality with the original ( equation [ bayesalt ] ) wherever @xmath0 is itself finite , owing to the @xmath43- and @xmath122- measurability of @xmath48 and @xmath123 , respectively : @xmath124 ( cf .",
    "@xcite , p.164 ) .",
    "although unnecessary for a straight - forwards application of biased sampling one might choose to further require that @xmath122 admit a @xmath2-density ; equivalent to the requirement that its distribution function , @xmath125 , be everywhere differentiable .",
    "for a continuous likelihood function we can be assured of this provided that @xmath5 at no place holds constant over a set of non - zero measure with respect to @xmath30the same limitation on its @xmath126 ` function ' definition .",
    "if so , we may write the marginal likelihood integral as @xcite , @xmath127    estimation of @xmath128 ( or in fact the general measure , @xmath122 ) can of course be accomplished via biased sampling given iids draws from a series of @xmath129-weighted versions of @xmath130 , and indeed this is the justification of the dos algorithm  seen as the limiting case of the weighted histogram analysis method @xcite with bin size approaching zero  given by @xcite .",
    "the derivation of the recursive update formula ( equation [ zupdates ] ) presented by @xcite for the dos is alternatively via a novel functional analysis procedure for optimization of the log - likelihood of an empirical energy histogram ; however , as with geyer s rlr derivation this approach does not lead to an uncertainty estimate or clt for the output @xmath100 .",
    "the nested sampling identity @xcite , @xmath131 where @xmath132 represents the inverse of the survival function of likelihood with respect to the prior  i.e .",
    ", @xmath133and @xmath134 denotes riemann integration over the ` prior mass cumulant ' , may best be understood by reference to a well - known relation between the expectation of a non - negative random variable and its distribution function : namely , that for @xmath135 with @xmath136 , @xmath137 ( cf .",
    "@xcite , p.223 ) .",
    "importantly , this relation ( which follows from integration by parts ) holds irrespective of whether or not @xmath138 admits a @xmath2-density , and in the marginal likelihood context becomes @xmath139 .",
    "if @xmath140 then this monotonically decreasing , _ cadlag _",
    "function on @xmath141 with bounded range ( between zero and one ) is ( perhaps improper ) riemann integrable , and we may simply ` switch axes ' to obtain equation [ ns ] .",
    "while the uniqueness of the inverse survival function , @xmath132 , can be ensured by requiring @xmath5 to be continuous with connected support @xcite , the weaker condition of @xmath5 discontinuous on a set of measure zero with respect to @xmath43 suffices to ensure an @xmath132 defined uniquely on all but a corresponding set of lebesgue measure zero , negligble also for our riemann integration .",
    "now for differentiable @xmath142 , such that @xmath106 might be defined without our earlier measure theoretic considerations as @xmath143 , the dos version of the marginal likelihood ( equation [ dose ] ) can nevertheless be recovered using the nested sampling identity . observing that @xmath144 ) = x(l)$ ] we have @xmath145)/de = dx(l)/dl    \\times dl / de = dx(l)/dl    \\times -\\exp[-e].\\ ] ] substitution of @xmath146 into equation [ ns ] yields @xmath147 and then by substitution of @xmath148 we recover , @xmath149\\times -g(e)\\exp[e]\\times -\\exp[-e ] \\times de.\\ ] ] that is , consistent with the requirements of @xcite and @xcite , this alternative dos formulation returns the identity , @xmath150    interestingly , the above relationship between the dos and nested sampling identities is mirrored by the existence of a measure theoretic construction for the latter ( cf .",
    "appendix c of @xcite ) .",
    "if we take the survival function , @xmath151 , as defining yet another transformation of the prior through the likelihood  a transformation ensured @xmath43-measurable , and hence @xmath30-measurable , by the right continuity of @xmath146we recover the following distribution in prior cumulant space ( @xmath152 ) : @xmath153 similarly , the marginal likelihood formula equivalent to the nested sampling identity becomes @xmath154 for @xmath146 invertible , i.e. , @xmath5 continuous with connected support @xcite .",
    "more generally though we can view @xmath132 as the conditional probability function of likelihood given prior mass cumulant defined modulo @xmath30 by the relation : @xmath155 ( cf.@xcite ) . for statistical problems on a complete separable metric space there will always exist a unique local version of @xmath156 defined as a weak limit such that @xmath157 is meaningful even for atomic @xmath158 @xcite .",
    "the value of this insight becomes apparent when we examine the nested sampling estimator for posterior functionals ( cf .",
    "@xcite ) , @xmath159 where @xmath160 here represents the nested sampling posterior weight for @xmath51 , @xmath161typically @xmath162 @xcite .",
    "this estimator relies on the relation given by equation [ nsx ] with @xmath5 replaced by @xmath163 , which holds for @xmath164 measurable  a more general condition than that of @xmath165 absolutely continuous given by @xcite .",
    "importantly , this ensures the validity of prior - sensitivity analysis via computation of the posterior functional of @xmath166 in nested sampling ",
    "a powerful technique not previously exploited in nested sampling analyses  as we shall discuss for the case of biased sampling below .      in the bayesian framework",
    "@xcite the ratio of marginal likelihoods under rival hypotheses ( i.e. , the bayes factor ) operates directly on the prior odds ratio for model selection to produce the posterior odds ratio as @xmath167[\\mathrm{p}\\{m_1\\}/\\mathrm{p}\\{m_2\\ } ] \\\\ \\nonumber & = [ z_{m_1}/z_{m_2}][\\mathrm{p}\\{m_1\\}/\\mathrm{p}\\{m_2\\}].\\end{aligned}\\ ] ] a much maligned feature of the marginal likelihood in this context is its possible sensitivity to the choice of the parameter priors , @xmath168 and @xmath169 , through @xmath170 and @xmath171 .",
    "when limited information is available to inform ( or justify ) this choice the resulting bayes factor can appear almost arbitrary .",
    "( on the other hand , viewed as a quantitative implementation of ockham s razor , the key role of prior precision may well serve as strong justification for the use of bayesian model selection in the scientific context ; cf .",
    "@xcite . ) in their influential treatise on this topic @xcite thus argue that some form of prior - sensitivity analysis be conducted as a routine part of all bayesian model choice experiments ; their default recommendation being the recomputation of the bayes factor under a doubling and halving of key hyperparameters .",
    "if the original marginal likelihoods have been estimated under an amenable simulation scheme then , as @xcite point out for the case of nested importance sampling , alternative bayes factors under ( moderate ) prior rescalings may be easily recovered by appropriately reweighting the existing draws without the need to incur further ( computationally expensive ) likelihood function calls ; and indeed the rlr method was conceived specifically to facilitate such computations ( though in the reweighting mixtures context ; @xcite ) . using the @xmath105 from biased sampling under our nominal prior for a given model the pseudo - mixture density , @xmath89 , of equation [ mixtured ] now serves as an efficient ` proposal ' for pseudo - importance sampling of various other targets with mass concentrated near that of the posterior . in particular , for the alternative marginal likelihood , @xmath172 , under some alternative prior density , @xmath173 , we have @xmath174 the stability of this importance sample reweighting procedure may be monitored via the effective sample size , @xmath175 $ ] , following @xcite ; and its asymptotic variance estimated via recomputation of equation [ priorsenseeqn ] under perturbations to the original @xmath100 drawn from the biased sampling covariance matrix with bootstrap resampling of the pooled @xmath51 .    for the general case of biased sampling from @xmath25-weighted versions of a prior distribution , @xmath30 , not necessarily admitting a @xmath2-density , the equivalent formula takes the radon - nikodym derivative of the alternative prior with respect to the original , @xmath176 ( for @xmath177 ) , such that @xmath178 /n.\\ ] ] we demonstrate the utility of this approach to prior - sensitivity analysis in our finite and infinite mixture modeling of the well - known galaxy dataset in section [ mixtures]and we refer the interested reader to our other recent astronomical application concerning a semi - parameteric mixed effects model presented in @xcite . though both these examples are based on the dirichlet process prior one can envisage application of the same technique to investigate prior - sensitivity in many other problems of applied statistics  e.g .  gaussian or ornstein - uhlenbeck process modeling of astronomical time series @xcite .      although the recursive update scheme of biased sampling provides a powerful technique for estimating the marginal likelihood given iid draws from a pre - specified sequence of @xmath25-weighted distributions , the design of this bridging sequence and the choice of an algorithm to sample from it are left to the user . while it is possible from theoretical principles to identify the optimal choice of @xmath25 with respect to the asymptotic variance under perfect sampling for a limited range of problems ",
    "@xcite show the optimality of @xmath179 ( requiring @xmath0 known ! ) for the one sample case with @xmath180 ( in our marginal likelihood notation)the design problem can not easily be solved in general",
    ". moreover , even where a theoretically optimal sequence can be identified it will not necessarily be computationally feasible to sample from such a sequence . of more practical value therefore are heuristic guides for the pragmatic choice of @xmath25 : strategies that will in a wide variety of applied problems produce adequate bridging sequences to ensure manageable uncertainty in the output @xmath181 while remaining accessible to existing posterior sampling techniques .",
    "this topic in various guises is the focus for the remainder of this paper , including our numerical examples .",
    "perhaps the most natural family of bridging sequence for use on the recursive pathway is that of the power posteriors method ( equation [ pp ] ; @xcite ) : this being both the favored approach for past dos - based applications @xcite  where the parameter , @xmath182 , has a physical interpretation as the inverse system temperature  and in geyer s formulation of rlr ",
    "where this particular sampling strategy ties in neatly with his parallel tempering mcmc algorithm ( mc@xmath183 ; @xcite ) .",
    "and indeed in section [ tivis ] below we will describe yet another conceptual connection between these two methods , providing a heuristic justification for the borrowing of thermodynamic integration strategies to this end .",
    "importantly , simulation from the power posterior at an arbitrary @xmath184 is typically no more difficult than simulation from the full posterior ( @xmath185 ) ; the required modifications to a standard mcmc and/or gibbs sampling code being often quite trivial ( e.g.@xcite ) . with biased sampling devised for iid draws though it is important to thin the resulting chains @xcite so as not to bias the corresponding asymptotic covariance estimates .",
    "experience has shown that prior - focussed temperature schedules , such as @xmath186 with @xmath187 - 5 , tend to work well for thermodynamic integration @xcite , and we confirm this also for biased sampling of our banana - shaped likelihood case study in section [ banana ] .",
    "( likewise , for tempering from a normalized auxiliary density , @xmath12 , closer in kullback - leibler divergence to the posterior than the prior ; @xcite and see our section [ tiauxiliary ] . )",
    "another effective choice of bridging sequence for biased sampling , which we demonstrate in our galaxy dataset case study of section [ mixtures ] , is that of partial data posteriors ( cf.@xcite ) : i.e. , @xmath188 where @xmath189 represents a subset of @xmath190 elements of the full dataset with @xmath191 the prior and @xmath192 the full posterior . for iid @xmath193 , with an expected contribution of @xmath190 times the unit fisher information the ` volume ' of highest posterior mass should shrink as roughly @xmath194 , suggesting an automatic choice of roughly @xmath195 with @xmath196 for this method .",
    "( in practice though , the first non - zero @xmath190 may well be limited by sampling / identifiability constraints on the model ; for our mixture model , for instance , we must specify @xmath197 , the number of mixture components . )    finally , as observed by @xcite , the constrained - likelihood bridging sequence of nested sampling can also be represented within the dos framework via @xmath198 with @xmath199 ; although in practice ( as we explore in section [ banana ] ) the non - iid nature of the resulting draws ( with each draw from @xmath200 influencing the placement of the next @xmath201 and its successors ) violates the assumptions of the biased sampling paradigm and ultimately limits the utility of this approach by biasing its asymptotic covariance estimate . in fact , this issue more generally remains an open problem for recursive marginal likelihood estimation theory : how can we best design effective strategies for _ adaptively _ choosing our bridging sequence ( e.g.  @xcite ) , and how can such modifications to the biased sampling paradigm be accounted for theoretically ? given the effectiveness of empirical process theory for characterizing the asymptotics of vardi s biased sampling it seems likely that a solution to the above will require extensive work in this area ( with a focus on the impact of long - range dependencies ) .",
    "a similar problem arises in describing the asymptotics of adaptive multiple importance sampling @xcite , which without its adaptive behaviour could be considered a version of biased sampling with known @xmath28 ; @xcite were recently able to provide a consistency proof for a modified version of this algorithm , but with a clt remaining elusive .",
    "inspired by the recursive pathway of biased sampling , rlr , and the dos , we present here yet another such strategy for marginal likelihood estimation , which we name , `` thermodynamic integration via importance sampling '' ( tivis ) .",
    "although quite novel at face value it is easily shown to be a direct equivalent of the recursive update methodology ; yet by effectively recasting this as a thermodynamic integration procedure we attain insight into the relationship between its error budget and bridging sequence . specifically , the error in the estimation of each @xmath97 may be thought of as dependent on both the @xmath202-divergence @xcite between it and the remainder of the ensemble ( via the thermodynamic identity ) and on the accuracy of our estimates for those other @xmath203 ( @xmath204 ) .    to construct the tivis estimator we once again assume the availability of pooled draws , @xmath84 , from a sequence of bridging densities , @xmath205 ( @xmath81 ) , with each @xmath206 exactly known .",
    "moreover , we suppose that @xmath207 indexes a normalized reference / auxiliary , @xmath1 or @xmath12 , such that @xmath61 is known , but with the remaining @xmath97 typically unknown . despite our subsequent use of the thermodynamic identity , however , we do not necessarily require here that the bridging densities follow the geometric path between these two extremes .",
    "now , rather than seek each @xmath105 via direct importance sampling from @xmath89 as per the rlr , the tivis method is to instead seek each normalization constant via thermodynamic integration from its preceding density in the ensemble , @xmath208 , using the identity , @xmath209 where @xmath210^t[f_{k-1}(\\theta)]^{1-t } \\propto [ q_{k}(\\theta)]^t[q_{k-1}(\\theta)]^{1-t}$ ] . for existence of the log - ratio in equation [ fulltivis ] we must impose the strict condition ( _ not _ necessary for ordinary rlr ) that all @xmath211 share matching supports .",
    "pseudo - importance sampling from @xmath89i.e .",
    ", importance sample reweighting of the drawn @xmath24allows construction of the appropriate ( but unnormalized ) weighting function , @xmath212^t[q_{k-1}(\\theta)]^{1-t}/p(\\theta),\\ ] ] which in substitution to equation [ fulltivis ] yields the tivis estimator , @xmath213 / [ \\sum_{i=1}^n u(\\theta_i , t ) ] dt.\\ ] ] in computational terms , numerical solution of the one - dimension integral in the above may be achieved to arbitrary accuracy by simply evaluating the integrand at sufficiently many @xmath184 on the unit interval , followed by summation with simpson s rule .",
    "if the sequence of bridging densities is well - chosen ( and suitably ordered ) the @xmath202-divergence between each @xmath211 and @xmath214 pairing should be far less than that between prior and posterior , such that a nave regular spacing of the @xmath184 will suffice .    to show the equivalence between this estimator and that of the recursive update scheme defined by equation [ zupdates ] we simply observe that the derivative of the denominator in equation [ tivismain ] equals the numerator , and thus by analogy to @xmath215 we have @xmath216 - \\log [ \\sum_{i=1}^n q_{k-1}(\\theta_i ) /p(\\theta_i)].\\ ] ] given the normalization of the prior ( @xmath61 ) , the above may be arranged into @xmath217 equations in @xmath217 unknowns , @xmath218 - \\log [ \\sum_{i=1}^n q_{1}(\\theta_i ) /p(\\theta_i)],\\ ] ] for @xmath219 .",
    "evidently , the recursive updates of equation [ xxs ] will match those of equation [ rlrx ] ( and hence , equation [ zupdates ] ) provided that @xmath220 = n$ ] , which is identical to the constraint of @xmath221 from the construction of biased sampling ( cf .",
    "section [ bs ] ) .",
    "hence , where vardi s strong connectivity condition holds the tivis estimator is identical to that of biased sampling ( otherwise , neither gives a unique solution ) .    in the following two case studies we further explore by numerical example various issues concerning the design of the bridging sequence ( with particular reference to the efficiency in @xmath5 calls ; section [ banana ] ) , and we highlight the utility of the _ normalized _ bridging sequence for prior - sensitivity analysis ( section [ mixtures ] ) .",
    "for our first case study we consider a ( ` mock ' ; i.e. , data independent ) banana - shaped likelihood function , defined in two - dimensions ( @xmath222 ) as @xmath223 with a uniform prior density of @xmath224 on the rectangular domain , @xmath225\\times [ -0.5,1.5]$ ] .",
    "a simple illustration of this likelihood function as a logarithmically - spaced contour plot is presented in the lefthand panel of figure [ bananafig ] .",
    "brute - force numerical integration via quadrature returns the `` exact '' solution , @xmath226 $ ] ( or @xmath227 $ ] ) .    as a benchmark of the method",
    "we first apply the biased sampling estimator to draws from a sequence of bridging densities following the standard power posteriors path .",
    "though even a cursory inspection of the likelihood function for this simple case study is sufficient to confirm its unimodality and to motivate a family of suitable proposal densities for straight - forward importance sampling of @xmath228 , for demonstrative purposes we have chosen to implement an mc@xmath183 @xcite approach here instead ; the latter being ultimately amenable to more complex posteriors than the former . following standard practice for thermodynamic integration  as per our motiviation from sections [ fisher ] and [ tivis ] above",
    " we adopt a pre - specified tempering schedule spaced geometrically as @xmath186 with @xmath229 and @xmath230 . to illustrate the @xmath231 convergence of biased sampling we run this procedure 100 times at each of five total sample sizes ( @xmath232 ; distributed equally across all five temperatures ) thinned at a rate of 0.25 from their parent mc@xmath183 chains .",
    "the resulting mean and standard error ( se ) at each @xmath233 are marked in the righthand panel of figure [ bananafig ] .",
    "overlaid are ( the means of ) the corresponding `` per simulation '' estimates of this standard error computed from the rival asymptotic covariance matrix forms of @xcite/@xcite and @xcite : the former being originally derived from the empirical process clt applicable to biased sampling and the latter from maximum likelihood theory using the hessian of the quasi - likelihood function for reverse logistic regression . as noted in section [ bs ] @xcite has previously discussed the inadequency of geyer s covariance estimator  though for the present design the difference is negligible .",
    "it is worth noting that both estimates are a little conservative at low @xmath233 , but give an excellent agreement with the repeat simulation se by @xmath234 .    with this power posteriors version of biased sampling as benchmark",
    "we now consider the merits of two alternative schemes for defining , and sampling from , the required sequence of bridging densities , @xmath211 , in sections [ tiauxiliary ] and [ nestedcomp ] below .",
    "as highlighted by @xcite , the error budget of thermodynamic integration over the geometric path depends to first - order upon the @xmath202-divergence between the reference / auxiliary density , @xmath12 , and the target , @xmath7 .",
    "thus , it will generally be more efficient to set a ` data - driven ' @xmath12such as may be recovered from the position and local curvature of the posterior mode  than to integrate ` navely ' from the prior , i.e. , @xmath235 . here",
    "we demonstrate the corresponding improvement to the performance of the biased sampling estimator resulting from the choices , @xmath236 and @xmath237 . here",
    "@xmath238 and @xmath239 denote the two - dimensional normal and student s @xmath182 ( @xmath240 ) distributions ( truncated to our prior support ) , respectively , while @xmath241 denotes the posterior mode and @xmath242 its local curvature ( recovered here analytically , but estimable at minimal cost in many bayesian analysis problems via standard numerical methods ) .",
    "as before we apply mc@xmath183 to explore the tempered posterior and repeat both experiments 100 times at each of our five @xmath233 .",
    "in contrast to the power posteriors case we adopt here a regular temperature grid , @xmath243 , to allow for the imposed / intended similiarity between @xmath7 and @xmath12 .",
    "our results are presented in figure [ tibanana ] and discussed below .    as expected from both theoretical considerations @xcite , and reports of practical experience with other marginal likelihood estimators @xcite , use of a ` data - driven ' auxiliary in this example has indeed reduced markedly the standard error of the biased sampling scheme ( at fixed @xmath233 ) with respect to that of the nave ( power posteriors ) path , i.e. , @xmath235 . in this instance",
    "the ( thinner - tailed ) normal auxiliary has out - performed the ( fatter - tailed ) student s @xmath182 ( with one d.o.f . )",
    "; however , although this result is again consistent with theoretical expectations  as a quick computation using the `` exact '' @xmath244 confirms @xmath245 \\ll j[\\mathcal{t}(\\mu_\\mathrm{mode},\\sigma_\\mathrm{mode}^{-1}),h(\\theta)]$]it should be remembered that the optimal choice of auxiliary from within a standard parameteric family depends on the likelihood function itself , and so will vary from problem to problem .",
    "moreover , without knowledge of the desired @xmath0 it is not possible to optimise @xmath12 _ a priori _ ; and even a crude estimator of the @xmath202-divergence run with , e.g. , the laplace approximation to the marginal likelihood will nevertheless add numerous extra likelihood evaluations to the computational budget .",
    "although ` fatter - tailed ' than a typical likelihood function the student s @xmath182 may well prove a superior choice for some multimodel posterior problems in practice by better facilitating mixing during the mc@xmath183 sampling stage .      recalling the connections between the dos derivation of the recursive pathway and the nested sampling algorithm described in section [ dossection ] it is of some interest to compare directly the performance of these rival techniques .",
    "the present case study with its uniform prior density is in fact well suited to this purpose since in the field of cosmological model selection , where nested sampling has been most extensively - used of late @xcite , it is standard practice to adopt separable priors from which a uniform sample space may be easily constructed under the quantile function transformation ; which , for the discussion below , we assume has been done such that @xmath1 may be taken as strictly uniform on @xmath246^n$ ] ( in the transformed coordinate space ) . given these conditions @xcite",
    "outline a crude - but - effective scheme for exploring the constrained - likelihood shells of nested sampling , in which the new `` live '' particle for each update must be drawn with density proportional to @xmath247 .    under the @xcite scheme , to draw the required @xmath51 one simply identifies the minimum bounding ellipse ( or with @xmath248 , the minimum bounding _ ellipsoid _ ) for the present set of `` live '' particles , expands this ellipse by a small factor @xmath2491.5 - 2 with the aim of enclosing the full support of @xmath250 , and then draws randomly from its interior until a valid @xmath251 is discovered .",
    "supposing the elliptical sampling window thus defined has been enlarged sufficiently to fully enclose the desired likelihood surface ( which it must do to ensure unbiased sampling of @xmath251 , although we can rarely be _ sure _ that it has ) it remains unlikely to match its shape exactly , leading to an overhead of @xmath252 discarded draws , @xmath253 . at each @xmath51 the incurred @xmath252",
    "may be thought of as a single realization of the negative binomial distribution with @xmath254 equal to the fraction of the bounded ellipse for which @xmath255 ; hence , @xmath256 .",
    "the magnitude of this overhead can in general be expected to scale with the geometric volume of the parameter space , potentially limiting the utility of this otherwise dimensionally - insensitive monte carlo - based estimator . however , where applicable the @xcite scheme may nevertheless prove more efficient than the alternative of constrained - mcmc - sampling to find the new @xmath51 ( cf .",
    "@xcite ) in which one must discard at least @xmath24910 - 20 burn - in moves ( each with a necessary @xmath257 call ) per step to achieve approximate stationarity .    applying the ellipse - based approach to nested sampling of the banana - shaped likelihood function of equation [ bananafn ] with @xmath258 live particles evolved over @xmath259 steps in each case ( and a small extrapolation of the mean @xmath260 times @xmath261 at the final step ; cf .",
    "@xcite ) we recover a convergence to the true @xmath244 as shown in the lefthand panel of figure [ nestedbanana ] .",
    "important to note is that with the ellipse scale factor of 1.5 used here the result is an overhead of @xmath262 likelihood calls per accepted @xmath51 , such that nested sampling at @xmath263 corresponds to @xmath264 in the previous examples .",
    "an overhead of this magnitude should be a concern for ` real world ' applications of nested sampling in which the likelihood function may be genuinely expensive to evaluate ; indeed for modern cosmological simulations mcmc exploration of the @xmath265 posterior is effectively a super - computer - only exercise due solely to the cost of solving for @xmath5 .",
    "[ at this point the skeptical reader might object that the distinctly non - elliptical @xmath257 considered in this example be considered a particularly unfair case for testing the @xcite method , but such banana - shaped likelihoods are in fact quite common in higher - order cosmological models ; see , for instance , @xcite .",
    "] we therefore suggest that one might improve upon the efficiency of ellipse - based nested sampling by co - opting its bridging sequence into the biased sampling framework in some manner .",
    "as @xcite has pointed out , the nested sampling pathway can be accommodated roughly within the dos ( and hence biased sampling ) framework : e.g.  by treating the accepted @xmath51 ( pooled with the surviving @xmath266 live particles ) as drawn from the series of weighted distributions , @xmath267 . however , with each @xmath25 ( @xmath268 ) now dependent on past draws  and hence the @xmath24 no longer iid  although we can apply the recursive update scheme of equation [ zupdates ] to normalize the bridging sequence and then importance sample reweight to @xmath0 , the biased sampling clt no longer holds . to demonstrate this we apply the above procedure to the draws from our previous nested sampling runs and plot the mean and repeat simulation se at each @xmath266 in the middle panel of figure [ nestedbanana ] .",
    "while the efficiency of this estimator is almost identical to that of ordinary nested sampling the ` naive ' application of gill et al.s asymptotic covariance matrix does not yield an se estimate matching that of repeat simulation .",
    "a more interesting alternative is to observe that for the ellipse - based nested sampling method ( given uniform priors ) the normalization of each @xmath269 is in fact easily computed from the area / volume of the corresponding ellipse / ellipsoid .",
    "that is , we can simply pool our draws_including _ the @xmath51 with @xmath270 otherwise discarded from nested sampling  and apply the importance sample reweighting procedure of equation [ priorsenseeqn ] with @xmath271 and @xmath272[i(\\theta \\in ell[e_{\\mathrm{live}(k)}])/v_k].\\ ] ] ( with @xmath273 the volume of the @xmath274-th ellipse and @xmath275 $ ] its interior ) .",
    "application of this strategy  which we dub , `` importance nested sampling '' ( ins)to the present example yields @xmath244 estimates with much smaller repeat simulation se than either of the previous summations as shown in the righthand panel of figure [ nestedbanana ] .",
    "bootstrap resampling of the drawn @xmath276 gives a reasonable estimator of this se though we note that ins does not appear to be unbiased in @xmath244 , with a slight tendency towards underestimation at small @xmath233 .",
    "further computational experiments are now underway to better quantify the advantages offered by this approach to harnessing the information content of these otherwise discarded draws in the ellipse - based nested sampling paradigm are presented in @xcite .",
    "the well - known galaxy dataset , first proposed as a test case for kernel density estimation by @xcite , consists of precise recession velocity measurements ( in units of 1000 km s@xmath277 ) for 82 galaxies in the corona borealis region of the northern sky reported by @xcite .",
    "the purpose of the original astronomical study was to search  in light of a then recently discovered void in the neighboring botes field @xcite  for further large - scale inhomogeneities in the distribution of galaxies . given the well - defined selection function of their survey , @xcite were easily able to compute as benchmark the recession velocity density function expected under the null hypothesis of a uniform distribution of galaxies throughout space , and by visual comparison of this density against a histogram of their observed velocities the astronomers were able to establish strong evidence against the null ; thereby boosting support for the ( now canonical ) hierarchical clustering model of cosmological mass assembly @xcite",
    ". however , under the latter hypothesis , as @xcite insightfully observed , one can then ask the more challenging statistical question of ` _ _ how many distinct clustering components are in fact present in the recession velocity dataset ? _ _ ' .",
    "many authors have since attempted to answer this question ( posed for simplicity as a univariate normal mixture modeling problem ) as a means to demonstrate the utility of their preferred marginal likelihood estimation or model space exploration strategy .",
    "notable such contributions to this end include : the infinite mixture model ( dirichlet process prior ) analyses of @xcite and @xcite ; chib s exposition of marginal likelihood estimation from gibbs sampling output ( @xcite ; but see @xcite and @xcite ) ; the reversible jump mcmc approach of @xcite ; and the label switching studies of @xcite and @xcite .",
    "the earliest of these efforts are well summarized by @xcite , who highlights a marked dependence of the inferred number of mixture components on the chosen priors .",
    "for this reason , as much as its historical significance , the galaxy dataset provides a most interesting case study with which to illustrate the potential of prior - sensitivity analysis under the recursive pathway .",
    "the outline of our presentation is as follows . in section [ statmod ]",
    "we set forth the finite and infinite mixture models to be examined here and in section [ gibbssampling ] we describe the mcmc strategies we use to explore their complete and partial data posteriors . in section [ hyperp ]",
    "we discuss various astronomical motivations for our default hyperprior choices , and finally in section [ priorsense ] we present the results of biased sampling run on this problem with importance sample reweighting - based transformations between alternative priors .",
    "following @xcite we write the @xmath274-component normal mixture model with component weights , @xmath278 , in the latent allocation variable form for data vector , @xmath193 , and ( unobserved ) allocation vector , @xmath279 , such that @xmath280 here @xmath281 represents the one - dimensional normal density , which we will reference in mean  precision syntax as @xmath282 , i.e. , @xmath283 .",
    "given priors for the number of components in the mixture , the distribution of weights at a given @xmath274 , and the vector of mean  precisions ",
    ", @xmath284 , @xmath285 , and @xmath286 , respectively  the posterior for the number of mixture components in the _ finite _ mixture case may be recovered by integration over @xmath287 at each @xmath274 , @xmath288 here the likelihood , @xmath289 , is given by a summation over the @xmath233 unobserved , @xmath290 , as @xmath291 that is , for a @xmath284 assigning mass to only a small set of elements one approach to recovering @xmath292 is simply to estimate the `` per component '' marginal likelihood , @xmath293 , at each of these @xmath274 and then reweight by @xmath284 . the full marginal likelihood of the model",
    "can then of course be estimated from the sum , @xmath294 . while this is indeed the strategy adopted here for exposition purposes it is worth noting that such direct marginal likelihood estimation to recover @xmath292 for this model can in fact",
    "be entirely avoided via either the reversible jump mcmc algorithm @xcite or gibbs sampling over the infinite mixture version described below .      rather than specify a maximum number of mixture components _ a priori _ , @xcite and @xcite ( amongst others ) have advocated an infinite dimensional solution based on the dirichet process prior . in particular , one may suppose the data to have been drawn from an infinite mixture of normals with means , variances and weights drawn as the realization , @xmath295 , of a dirichlet process ( dp ) , @xmath296 , on @xmath297 ; the characterization of the dp being via a concentration index , @xmath298 , and reference density , @xmath299 , and with all @xmath295 being both normalized and strictly atomic . for small @xmath298 ( @xmath300 )",
    "the tendency is for these @xmath295 to be dominated by only a few ( mixture ) components , while for large @xmath298 the number of significant components inevitably increases , with the typical @xmath295 thereby becoming closer ( in the metric of weak convergence ) to @xmath299 .",
    "the likelihood of iid @xmath193 for a given @xmath295 requires ( in theory ) an infinite sum over the contribution from each of its components , @xmath301 where each @xmath302 represents the limiting fraction of points in the realization assigned to a particular @xmath303 .",
    "( in practice , however , this summation can generally be truncated with negligible loss of accuracy after accounting for the contributions of only the most dominant components . )",
    "computation of the marginal likelihood for the above model is thus nominally by integration over the infinite dimensional space of @xmath295 . in particular , if we suppose a hyperprior density for the hyperparameters , @xmath304 , of the dp ( i.e. , for @xmath298 and the controlling parameters of @xmath299 ) we have @xmath305 .    as per the finite mixture case",
    "we can simplify our posterior exploration and relevant computations by introducing latent variables , @xmath279 and @xmath117 , for allocation of the @xmath193 and the corresponding mean - precision vectors of the parent components in @xmath295 . in this version",
    "the likelihood takes the form , @xmath306 and the marginal likelihood becomes @xmath307 importantly , existing gibbs sampling methods for the dp allow for collapsed sampling from the posterior for @xmath308 and equation [ vv1 ] can be reduced to @xmath309 . in one further twist , however , we note that since the reduced expression is degenerate across component labellings it is in fact more computationally efficient to estimate @xmath0 from @xmath310 where @xmath311 takes a particularly simple analytic form by the nature of the dp ( cf .",
    "@xcite ) .    finally , it is important to note that since each realization of the dp has always an infinite number of components with probability one ( though usually only a few with significant mass ) the usual interpretation for the posterior , @xmath292 , in this context is the posterior distribution of the number of unique label assignments _ amongst the observed dataset _",
    "( i.e. , the dimension of @xmath117 in @xmath312 ) .",
    "however , although pragmatically useful for such modeling problems as that exhibited by the galaxy dataset , as @xcite note : this estimate is not consistent .",
    "exploration of the posterior for @xmath313 at fixed @xmath274 in this finite mixture model can be accomplished rather efficiently ( modulo the well - known problem of mixing _ between _ modes ; cf.@xcite ) via gibbs sampling given conjugate prior choices , as explained in detail by @xcite . to this end",
    "we suppose @xmath314 where @xmath315 represents the gamma distribution with rate , @xmath316 , and shape , @xmath317 . to simulate from the resulting posterior we use the purpose - built code provided by ` bmmmodel ` and ` jagsrun ` in the ` bayesmix ` package @xcite for ` r ` .",
    "no modifications to this code are necessary for sampling the partial data posterior , and both the partial and full data likelihoods given partial likelihood draws ( at fixed @xmath274 ) may be recovered with equation [ likes ] .",
    "the range of @xmath274 for which we compute marginal likelihoods is here limited by the range of a truncated poisson prior on @xmath274 .",
    "as noted earlier , exploration of the infinite mixture model posterior can also be facilitated through gibbs sampling with the appropriate choice of priors @xcite ; and although contemporary codes typically use the ( more efficient ) alternative algorithm of @xcite the prior forms dictated by the conjugacy necessary for gibbs sampling remain the default . hence , to this end we suppose a fixed concentration index of @xmath318 and a normal - gamma reference density , @xmath319 assigning hyperpriors of @xmath320 and @xmath321 . here",
    "we use the ` dpdensity ` function in the ` dppackage ` @xcite for ` r ` to explore this posterior .",
    "while no modifications to this code are required for sampling the partial likelihood posteriors , the computation of full data likelihoods given the partial likelihood posterior requires that we sample a series of dummy components from the current posterior until some appropriate truncation point , @xmath322 , before applying ( the @xmath323-truncated version of ) equation [ likeq ] .",
    "as noted earlier , by considering the well - defined selection function of their observational campaign the authors of the original astronomical study were able to construct the expected probability density function of recession velocities for their survey under the null hypothesis of a uniform distribution of galaxies throughout space .",
    "in particular , @xcite recognised that the strict _ apparent _ magnitude limit of their spectroscopic targetting strategy ( @xmath324 mag ) would act as a luminosity ( or _ absolute _ magnitude ) limit evolving with recession velocity ( distance ) according to @xmath325 where we have assumed units of 1000 km s@xmath277 for @xmath326 and a `` hubble constant '' of @xmath327 km s@xmath277 mpc@xmath277 . to estimate the form of the resulting selection function , @xmath328",
    ", @xcite considered how the relative number of galaxies per unit volume brighter than this limit would vary with distance given the absolute magnitude distribution function , @xmath329 , for galaxies in the local universe , i.e. , @xmath330 . to approximate the latter the astronomers simply integrated over a previous estimate of the local luminosity density parameterized as a schechter function @xcite with characteristic magnitude , @xmath331 mag , and faint - end slope , @xmath332 , such that @xmath333^{\\alpha_r^\\ast+1 } \\exp[-10^{2/5(m_r^\\ast - m)}],\\ ] ] and @xmath334    an interesting feature of magnitude - limited astronomical surveys is that , although with increasing recession velocity this @xmath328 selection function restricts their sampling to the decreasing fraction of galaxies above @xmath335 , the volume of the universe probed by ( the projection into three - dimensional space of ) their two - dimensional angular viewing window is , in contrast , rapidly increasing .",
    "hence , there exists an important additional selection effect , @xmath336 , operating in competition with , and initially dominating , that on magnitude , and scaling with ( roughly ) the third power of recession velocity such that @xmath337    the product of these two effects therefore returns the net selection function of the galaxy dataset , which we illustrate ( along with each effect in isolation ) in figure [ postman ] ( see also figure 4b from @xcite ) . the point being that there do exist informative astronomical considerations for choosing at least some of the hyperparameters of our priors in this mixture modeling case study , though past analyses have tended to ignore this context ( contributing somewhat to the the apparent ` failure ' of bayesian mixture modeling for this dataset ; @xcite ) .",
    "in particular , the shape of the selection function in velocity space suggests the form for our prior on the component means : a choice of @xmath338 gives a reasonable match to the shape of @xmath339 .",
    "perhaps surprisingly , as we will demonstrate later , the choice of prior on the component means has a substantial influence on the resulting @xmath292 ; changing only these of our hyperparameters to ` data - driven ' values chosen as @xmath340 results in a drastic shift of the posterior .",
    "likewise , we can inform our prior choice for the number of components in the mixture with regard to the original survey design , which featured five separate observational windows placed so as to cover five previously - identified galaxy clusters from the abell catalog .",
    "( the positions of these clusters in bivariate recession velocity  declination space , and its projection to univariate velocity space , are also marked on figure [ postman ] for reference . )",
    "hence , we select a mode of @xmath341 for our truncated poisson prior for @xmath284 . with the @xmath342 and @xmath343 mixture models",
    "already well excluded by previous analyses , and @xmath344 a pragmatic upper bound for exploration given @xmath345 , we therefore truncate our prior to the range , @xmath346 .",
    "this contrasts somewhat with the uniform priors on @xmath347 and @xmath348 assumed by @xcite and @xcite , respectively  though reweighting for alternative @xmath284 ( on this support ) is trivial in any case .",
    "only the precisions of the normal mixture components are not well constrained from astronomical considerations ",
    "although we can at least be confident that any large - scale clustering should occur above the scale of individual galaxy clusters ( @xmath2491 mpc , or @xmath349 ) and ( unless the uniform space - filling hypothesis were correct ) well below the width of our selection function .",
    "thus , we simply adopt a fixed shape hyperparameter of @xmath350 for our gamma prior on the @xmath351 and allow the rate hyperparameter to vary according to its gamma hyperprior form @xmath352 and @xmath353 .",
    "our choice here is thus comparable to that of @xcite who suppose @xmath354not @xmath355 as mis - quoted by @xcite  though we evidently place far less prior weight on exceedingly large precisions ( small variances ) .",
    "the same considerations can also help shape our hyperparameter choices for the priors on our infinite mixture model .",
    "in particular we take @xmath356 for the hyperparameters shaping the normal - gamma reference density , @xmath299 , with the aim of matching as closely as possible to the priors of our finite dimensional model . with the scale parameter of our prior on the component precisions taking an inverse - gamma hyperprior form in the infinite case and a gamma form in the finite case it was not possible to exactly match these distributions : our choice of @xmath357",
    "is intended to at least give comparable 5% and 95% quantiles .",
    "finally , we adopt a fixed value for the concentration parameter of @xmath318 ; this choice coincidentally gives a similar effective prior for the number of unique components amongst the 82 observed galaxies to that of the @xmath284 adopted for our finite mixture model ( see @xcite , for instance ) .",
    "as an initial verification of our code we first run the gibbs sampling procedure outlined above ( section [ fmm ] ) to explore the partial data posteriors of a three component ( unequal variance ) mixture model using the priors from @xcite , with the biased sampling algorithm then applied for marginal likelihood estimation .",
    "@xcite has made public the results of a @xmath358 draw ame calculation providing a precise benchmark for the marginal likelihood under these priors of @xmath359 ( se ) ; though it should be noted that the galaxy dataset used for this purpose is that _ with _ chib s transcription error in the 78th observation ( which we insert explicitly into the public ` r ` version for the present application only ) . given just 200 saved draws from gibbs sampling ( at a thinning rate of 0.9 ) of the partial data posterior at each of 10 steps spaced as @xmath360 ( with @xmath361 reset to 3 to facilitate sampling ) we can confirm the recovery of this benchmark as @xmath362 ( se ) .",
    "estimation of the ( single run ) standard error ( se ) was for this purpose conducted via 1000 repeat simulations .",
    "further repeats of this procedure with both more posterior focussed ( @xmath363 , 1 ) and more prior focussed ( @xmath364 ) partial data schedules confirm the optimality of the @xmath196 choice anticipated from fisher information principles ( section [ fisher ] ) .",
    "the results of this experiment are illustrated in figure [ root ] .",
    "to estimate `` per component ''",
    "marginal likelihoods for each @xmath274 ( @xmath365 ) in our finite mixture model we run the same procedure of partial data posterior exploration followed by biased sampling with @xmath366 draws from each of ten steps on the @xmath196 bridging sequence .",
    "the results of this computation are illustrated in figure [ galaxypost ] ; the uncertainties indicated are gauged from the asymptotic covariance matrix of the biased sampling estimator ( as per @xcite ) . for reference ,",
    "the total marginal likelihood ( @xmath367 ) we recover here is @xmath368 with estimated standard deviation of 0.09 .",
    "we recover a posterior mode of @xmath369 components : the recession velocity density belonging to which at the corresponding mode in @xmath287 is also illustrated in figure [ galaxypost ] for reference . to the eye it appears that @xmath369 may be a slight over - estimate since the third and fourth components ( in order of increasing recession velocity ) are more - or - less on top of each other : suggesting that one is being used to account for a slight non - normality in the shape of this peak .    to demostrate the potential for efficient prior - sensitivity analysis via importance sample reweighting of the pseudo - mixture density of partial data posteriors normalized by biased sampling ( section [ psa ] ) we begin by recovering the @xcite result from the above simulation output .",
    "the results of this reweighting procedure are shown in figure [ priorsensitivity ] .",
    "since the @xcite priors are significantly different to those chosen here from astronomical considerations ( as discussed in [ hyperp ] ) the effective sample sizes provided by our pseudo - mixture of @xmath370 draws range from just 13 to 928 , yet the resulting approximation to the former benchmark is actually rather good .",
    "moreover , the corresponding 95% credible intervals ( recovered via bootstrap resampling from our pseudo - mixture plus estimates of the asymptotic covariance matrix for each @xmath371 ) indeed enclose all eight @xmath292 reference points .    as a second demonstration we also shown in figure [ priorsensitivity ]",
    "the results of reweighting for alternative ` data - driven ' choices for the hyperparameters of our prior on the component means : @xmath340 . to emphasise the large difference this small change in @xmath1 makes to",
    "the `` per component '' log @xmath372 values the comparison presented is between our default and ` data - driven ' priors with @xmath284 removed ( i.e. , treated as uniform ) .",
    "this investigation clearly confirms the remarkable prior - sensitivity of @xmath292 in the galaxy dataset example .",
    "interestingly , the preference under our ` data - driven ' priors is for an even greater number of mixture components ( @xmath373 ) , despite the @xmath369 solution already seeming ( visually ) to be an over - fitting of the available data .      in figure [ imm ]",
    "we present the results of gibbs sampling the posterior of our infinite mixture model . in particular , we show in the lefthand panel of this figure the posterior for the number of unique label assignments amongst the galaxy dataset ; which as we have noted earlier is typically used as a proxy for the number of mixture components present ( although under the dirichet process prior this is formally always infinite ) .",
    "for reference , the marginal likelihood we recover from 4000 draws at each of the ten partial data posteriors on the @xmath196 sequence using the biased sampling estimator is @xmath374 with estimated standard deviation from the gill et al .",
    "asymptotic covariance matrix of 0.08 . in the righthand panel",
    "we demonstrate again the power of importance sample reweighting for prior sensitivity analysis , though for this particular case the stochastic process prior used requires that we apply the appropriate radon - nikodym derivative version given by equation [ rnreweighting ] .    the radon - nikodym derivative , @xmath375 , of the measure on @xmath308 assigned by a @xmath274-component finite mixture model with respect to that assigned by the dirichlet process prior of our infinite mixture model may be computed as follows .",
    "first , we observe that the radon - nikodym derivative between two dirichlet process priors on the ( equivalent ) space of @xmath376 ( with the @xmath51 possibly non - unique ) has been previously derived by @xcite , thereby providing a direct formula for computing @xmath377 where @xmath378 represents a dirichlet process prior with hyperpriors on the @xmath304 of its reference density chosen to be identical to those on the @xmath379 and @xmath380 of our finite mixture model .",
    "that is , we choose @xmath378 such that its projection to @xmath378 for @xmath279 with @xmath274 unique elements is equivalent ( a.e . ) to that of @xmath381 with our hyperparameter on @xmath380 integrated out , allowing @xmath382 to be defined identical to @xmath383 .",
    "the necessary @xmath384 to ensure that @xmath385 is then simply the ratio of the labelling probabilities under our finite mixture model and the intermediate version of our infinite mixture model ( with @xmath386 _ only _ where the number of unique elements in @xmath279 equals @xmath274 ) .",
    "a forumla for the desired @xmath384 can be derived by combining standard properties of the dirichlet - multinomial distribution ( our finite dimensional model prior on @xmath279 ) with results from the work of @xcite on the marginals of the dirichlet process . in each case",
    "the probabilities of a given labelling sequence depends not on its ordering , but rather on its vector of per - label counts . using antoniak s system of writing @xmath387 as the set of labellings with @xmath388 unique elements , @xmath389 pairs , etc . , we have wherever @xmath390 , @xmath391}}},\\ ] ] where @xmath392}$ ] denotes the rising factorial function as per proposition 3 of @xcite . for our case of @xmath393 and @xmath318",
    "this reduces to @xmath394",
    "in this paper we have presented an extensive review of the recursive pathway to marginal likelihood estimation as characterized by biased sampling , reverse logistic regression , and the density of states ; in particular we have highlighed the diversity of bridging sequences amenable to recursive normalization and the utility of the resulting pseudo - mixtures for prior - sensitivity analysis ( in the bayesian context ) .",
    "our key theoretical contributions have included the introduction of a novel heuristic ( `` thermodynamic integration via importance sampling '' ) for guiding design of the bridging sequence , and an elucidation of various connections between these recursive estimators and the nested sampling technique .",
    "our two numerical case studies illustrate in depth the practical implementation of these ideas using both ` ordinary ' and stochastic process priors .",
    "aitkin , m. ( 2001 ) . likelihood and bayesian analyisis of mixtures .",
    "_ statist .",
    "mod . _ * 1 * 287304 .",
    "antoniak , c. e. ( 1974 ) .",
    "mixtures of dirichlet processes with aplications to bayesian nonparametric problems .",
    "statist . _",
    "* 2 * 11521174 .",
    "http://www.ams.org/mathscinet-getitem?mr=0365969[mr0365969 ] arima , s. , tardella , l. ( 2012 ) .",
    "improved harmonic mean estimator for phylogenetic model evidence .",
    "_ j. comput .",
    "* 19 * 418438 .",
    "baele , g. , lemey , p. , bedford , t. , rambaut , a. , suchard , m. a. , alekseyenko , a. v. ( 2012 ) . improving the accuracy of demographic and molecular clock model comparison while accommodating phylogenetic uncertainty .",
    "* 29 * 21572167 .",
    "bailer - jones , c. a. l. ( 2012 ) .",
    "a bayesian method for the analysis of deterministic and stochastic time series .",
    "astrophys . _ * 546 * a89 .",
    "berkhof , j. , van mechelen , i. , gelman , a. ( 2003 ) . a bayesian approach to the selection and testing of mixture models . _",
    "statist . sinica _ * 13 * 423 - 442 .",
    "billingsley , p. ( 1968 ) .",
    "_ convergence of probability measures .",
    "_ , john wiley & sons , inc . , new york .",
    "http://www.ams.org/mathscinet-getitem?mr=0233396[mr0233396 ] brewer , b. j. , stello , d. ( 2009 ) .",
    "gaussian process modelling of astreoseismic data .",
    "_ mon . not .",
    "r. astron .",
    "_ * 395 * 22262233 .",
    "calderhead , b. , girolami , m. ( 2009 ) . estimating bayes factors via thermodynamic integration and population mcmc .",
    "statist . data anal . _ * 53 * 40284045 .",
    "cameron , e. , pettitt , a. n. ( 2013 ) . on the evidence for cosmic variation of the fine structure",
    "constant ( ii ) : a semi - parametric bayesian model selection analysis of the quasar dataset .",
    "[ _ preprint _ ] http://arxiv.org/abs/1309.2737[arxiv:1309.2737 ] caimo , a. , friel , n. ( 2013 ) .",
    "bayesian model selection for exponential random graph models .",
    "_ social networks _ * 35 * 1124 .",
    "capp , o. , guillin , m. , marin , j. m. , robert , c. p. ( 2004 ) .",
    "population monte carlo .",
    "_ j. comput . graph .",
    "statist . _",
    "* 13 * 907930 .",
    "http://www.ams.org/mathscinet-getitem?mr=2109057[mr2109057 ] chen , m .- h .",
    ", shao , q .- m .",
    "on monte carlo methods for estimating ratios of normalizing constants .",
    "statist . _",
    "* 25 * 15631594 .",
    "http://www.ams.org/mathscinet-getitem?mr=1463565[mr1463565 ] chen , m .- h . ,",
    "shao , q .- m . , ibrahim , j. g. ( 2000 ) .",
    "_ monte carlo methods in bayesian computation .",
    "_ springer - verlag , berlin .",
    "chib , s. ( 1995 ) .",
    "marginal likelihood from the gibbs output .",
    "_ j. amer .",
    "assoc . _ * 90 * 13131321 .",
    "http://www.ams.org/mathscinet-getitem?mr=1379473[mr1379473 ] chopin , n. ( 2002 ) . a sequential particle filter method for static models .",
    "_ biometrika _ * 89 * 539552",
    ". http://www.ams.org/mathscinet-getitem?mr=1921961[mr1929161 ] chopin , n. , robert , c. p. ( 2010 ) .",
    "properties of nested sampling .",
    "_ biometrika _ * 97 * 741755",
    ". http://www.ams.org/mathscinet-getitem?mr=2672495[mr2672495 ] cornuet , j .-",
    "marin , j .-",
    "m . , mira , a. , robert c. p. ( 2012 ) . adaptive multiple importance sampling . _",
    "j. stat . _",
    "* 39 * 798812 .",
    "http://www.ams.org/mathscinet-getitem?mr=3000850[mr3000850 ] davis , t. m. , et al .",
    "scrutinizing exotic cosmological models using essence supernova data combined with other cosmological probes .",
    "_ astroph .",
    "j. _ * 666 * 716725 .",
    "del moral , p. , doucet , a. , jasra , a. ( 2006 ) .",
    "sequential monte carlo samplers .",
    "b _ * 68 * 411436 .",
    "diebolt , j. , robert , c. p. ( 1994 ) . estimation of finite mixture distributions through bayesian sampling .",
    "b _ * 56 * 363375 .",
    "http://www.ams.org/mathscinet-getitem?mr=1281940[mr1281940 ] doss , h. ( 2012 ) .",
    "hyperparameter and model selectoin for nonparametric bayes problems via radon - nikodym derivatives .",
    "_ statist .",
    "sinica _ * 22 * 126",
    ". http://www.ams.org/mathscinet-getitem?mr=2933165[mr2933165 ] dudley , r. m. , philipp , w. ( 1983 ) .",
    "invariance principles for sums of banach space valued random elements and empirical processes .",
    "_ z. wahrsch .",
    "gebiete _ * 62 * 509552 .",
    "escobar , m. d. , west , m. ( 1995 ) .",
    "bayesian density estimation and inference using mixtures .",
    "_ j. amer .",
    "assoc . _ * 90 * 577588 .",
    "http://www.ams.org/mathscinet-getitem?mr=1340510[mr1340510 ] evans , m. , robert , c. p. , davison , a. c. , jiang , w. , tanner , m. a. , doss , h. , qin , j. , fokianos , k. , maceachern , s. n. , peruggia , m. , guha , s. , chib , s. , ritov , y. , robins , j. m. , vardi , y. ( 2003 ) .",
    "discussion on the paper by kong , mccullagh , meng , nicolas and tan .",
    "b _ * 65 * 604618",
    ". fan , y. , rui , w. , chen , m .- h . ,",
    "kuo , l. , lewis , p. o. ( 2012 ) .",
    "choosing among partition models in bayesian phylogenetics . _",
    "_ * 28 * 523532 .",
    "feroz , f. , hobson , m. p. ( 2008 ) .",
    "multimodal nested sampling : an efficient and robust alternative to markov chain monte carlo methods for astronomical data analyses . _",
    "mon . not .",
    "r. astron .",
    "soc . _ * 384 * 449463 .",
    "feroz , f. , hobson , m. p. , cameron , e. , pettitt , a. n. ( 2013 ) .",
    "[ _ preprint _ ] http://arxiv.org/abs/1306.2144[arxiv:1306.2144 ] ferrenberg , a. m. , swendsen , r. h. ( 1989 ) .",
    "optimized monte carlo data analysis .",
    "lett . _ * 63 * 11951198 .",
    "friel , n. , pettitt , a. n. ( 2008 ) .",
    "marginal likelihood estimation via power posteriors .",
    "b _ * 70 * 589607 . http://www.ams.org/mathscinet-getitem?mr=2420416[mr2420416 ] friel , n. , wyse , j. ( 2012 ) .",
    "estimating the evidence  a review .",
    "neerl . _ * 66 * 288308 .",
    "friel , n. , hurn , m. , wyse , j. ( 2012 ) .",
    "improving power posterior estimation of statistical evidence .",
    "[ _ preprint _ ] http://arxiv.org/abs/1209.3198[arxiv:1209.3198 ] gelfand , a. e. , dey , d. ( 1994 ) .",
    "bayesian model choice : asymptotic and exact calculations .",
    "b _ * 56 * 501514 .",
    "http://www.ams.org/mathscinet-getitem?mr=1278223[mr1278223 ] gelman , a. , meng , x .-",
    "( 1998 ) . simulating normalizing constants : from importance sampling to bridge sampling to path sampling . _",
    "* 13 * 163185 .",
    "gelman , a. b. , carlin , j. b. , stern , h. s. , rubin , d. b. ( 2004 ) .",
    "_ bayesian data analysis _ , 2nd ed",
    "chapman & hall / crc , boca raton .",
    "http://www.ams.org/mathscinet-getitem?mr=2027492[mr2027492 ] geyer , c. j. ( 1992 ) .",
    "practical markov chain monte carlo .",
    "_ statist .",
    "sci . _ * 7 * 473483 .",
    "geyer , c. j. , thompson , e. a. ( 1992 ) .",
    "constrained monte carlo maximum likelihood for dependent data . _",
    "b _ * 54 * 657699 .",
    "http://www.ams.org/mathscinet-getitem?mr=1185217[mr1185217 ] geyer , c. j. ( 1994 ) .",
    "_ estimating normalizing constants and reweighting mixtures in markov chain monte carlo _ , technical report 568 .",
    "school of statistics , university of minnesota , minneapolis .",
    "gill , r. d. , vardi , y. , wellner , j. a. ( 1988 ) .",
    "large sample theory of empirical distributions in biased sampling models .",
    "_ * 16 * 10691112 .",
    "http://www.ams.org/mathscinet-getitem?mr=0959189    [ mr0959189 ] grn , b. , leisch , f. ( 2010 ) .",
    "bayesmix : an r package for bayesian mixture modeling .",
    "_ technique report_. gunn , j. e. , gott , j. r. iii ( 1972 ) . on the infall of matter into clusters of galaxies and some effects on their evolution",
    "_ astrophys .",
    "j _ * 176 * 119 .",
    "habeck , m. ( 2012 ) .",
    "evaluation of marginal likelihoods via the density of states .",
    "_ j. mach .",
    "_ ( _ proceedings track _ ) * 22 * 486494 .",
    "halmos , p. r. , savage , l. j. ( 1949 ) .",
    "application of the radon - nikodym theorem to the theory of sufficient statistics .",
    "_ ann . math .",
    "statist . _",
    "* 20 * 225241 .",
    "halmos , p. r. ( 1950 ) .",
    "_ measure theory .",
    "_ , van nostrand company , inc .",
    ", new york .",
    "http://www.ams.org/mathscinet-getitem?mr=0033869[mr0033869 ] hesterberg , t. ( 1995 ) .",
    "_ technometrics _ * 37 * 185194 .",
    "hoeting , j. a. , madigan , d. , raftery , a. e. , volinsky , c. t. ( 1999 ) .",
    "bayesian model averaging : a tutorial . with comments by m. clyde , david draper and e. i. george , and a rejoinder by the authors ) .",
    "_ statist .",
    "* 14 * 382417 .",
    "http://www.ams.org/mathscinet-getitem?mr=1765176[mr1765176 ] hrmander , l. ( 1983 ) . _ the analysis of linear partial differential operators .",
    "i. distribution theory and fourier analysis .",
    "_ , grundlehren der mathematischen wissenschaften .",
    "springer - verlag , berlin .",
    "http://www.ams.org/mathscinet-getitem?mr=0717035[mr0717035 ] jara , a. , hanson , t. e. , quintana , f. a. , mller , p. , rosner , g. l. ( 2011 ) .",
    "dppackage : bayesian semi- and nonparametric modelling in r. _ j. statist .",
    "_ * 40 * 130 .",
    "jasra , a. , holmes , c. c. , stephens , d. a. ( 2005 ) .",
    "markov chain monte carlo methods and the label switching problem in bayesian mixture modeling .",
    "_ statist .",
    "* 20 * 5067 .",
    "http://www.ams.org/mathscinet-getitem?mr=[mr ] jaynes , e. t. ( 2003 ) .",
    "_ probability theory : the logic of science",
    ". _ cambridge university press , cambridge .",
    "http://www.ams.org/mathscinet-getitem?mr=1992316[mr1992316 ] jeffreys , h. ( 1961 ) .",
    "_ theory of probability .",
    "clarendon press , oxford .",
    "http://www.ams.org/mathscinet-getitem?mr=0187257[mr0187257 ] jeffreys , w. h. , berger , j. o. ( 1991 ) .",
    "_ sharpening ockham s razor on a bayesian strop . _",
    "technical report # 91 - 44c .",
    "department of statistics , purdue university , indiana .",
    "kass , r. e. , raftery , a. e. ( 1995 ) .",
    "bayes factors .",
    "_ j. amer .",
    "assoc . _ * 90 * 773795 .",
    "kilbinger , m. , wraith , d. , robert , c. p. , benabed , k. , capp , o. , cardoso , j .- f . , fort , g. , prunet , s. , bouchet , f. r. ( 2010 ) .",
    "bayesian model comparison in cosmology with population monte carlo .",
    "_ mon . not .",
    "r. astron .",
    "* 405 * 23812390 .",
    "kirshner , r. p. , oemler , a. jr .",
    ", schechter , p. l. , shectman , s. a. ( 1981 ) . a million cubic megaparsec void in botes ?",
    "_ astrophys .",
    "j _ * 248 * 5760 .",
    "kong , a. , liu , j. s. , wong , w. h. ( 1994 ) .",
    "sequential imputations and bayesian missing data problems . _ j. amer .",
    "assoc . _ * 89 * 278288 .",
    "kong , a. , mccullagh , p. , meng , x .-",
    "l . , nicolae , d. , tan , z. ( 2003 ) . a theory of statistical models of monte carlo integration .",
    "soc . b _ * 65 * 585618 .",
    "kumar , s. , rosenberg , j. m. , bouzida , d. , swendsen , r. h. , kollman , p. a. ( 1992 ) .",
    "the weighted histogram analysis method for free - energy calculations on biomolecules .",
    "i. the method . _ j. comput .",
    "* 13 * 10111021 .",
    "lartillot , n. , phillipe , h. ( 2006 ) .",
    "computing bayes factors using thermodynamic integration .",
    "biol . _ * 55 * 195207 .",
    "lee , k. , marin , j .-",
    "m . , mengersen , k. , robert , c. p. ( 2008 ) .",
    "bayesian inference on mixtures of distributions .",
    "[ _ preprint _ ] http://arxiv.org/abs/0804.2413[arxiv:0804.2413 ] lefebvre , g. , steele , r. , vandal , a. c. ( 2010 ) .",
    "a path sampling identity for computing the kullback - leibler and j divergences .",
    "statist . data anal .",
    "_ * 54 * 17191731 .",
    "http://www.ams.org/mathscinet-getitem?mr=2608968[mr2608968 ] li , y. , ni , z .- x . , lin , j .- g .",
    "a stochastic simulation approach to model selection for stochastic volatility models .",
    "simulation comput .",
    "_ * 40 * 10431056 .",
    "http://www.ams.org/mathscinet-getitem?mr=2792481[mr2792481 ] liu , j. s. ( 2001 ) .",
    "_ monte carlo strategies in scientific computing _ , springer series in statistics .",
    "springer - verlag , new york .",
    "marin , j .-",
    ", robert c. p. ( 2010 ) . on resolving the savage - dickey paradox .",
    "_ electron .",
    "j. statist .",
    "_ * 4 * 643654 .",
    "marin , j .-",
    "m . , pudlo , p. , sedki , m. ( 2012 ) .",
    "consistency of the adaptive multiple importance sampling .",
    "[ _ preprint _ ] http://arxiv.org/abs/1211.2548v1.pdf[arxiv:1301.2548 ] meng , x .-",
    "l . , wong , w. h. ( 1996 ) .",
    "simulating ratios of normalizing constants via a simple identity : a theoretical exploration . _",
    "sinica _ * 6 * 831860",
    ". http://www.ams.org/mathscinet-getitem?mr=1422406[mr1422406 ] miller , j. w. , harrison , m. t. ( 2013 ) .",
    "a simple example of dirichlet process mixture inconsistency for the number of components .",
    "[ _ preprint _ ] http://arxiv.org/abs/1301.2708v1[arxiv:1301.2708v1 ] mukherjee , p. , parkinson , d. , liddle , a. r. ( 2006 ) .",
    "a nested sampling algorithm for cosmological model selection .",
    "_ astrophys .",
    "j. _ * 638 * l51l54 .",
    "neal , r. ( 1999 ) .",
    "erroneous results in `` marginal likelihood from the gibbs output '' .",
    "url(http://www.cs.toronto.edu/@xmath249radford / ftp / chib - letter.pdf ) neal , r. m. ( 2000 ) .",
    "markov chain sampling methods for dirichlet process mixture models .",
    "_ j. comput . graph .",
    "statist . _",
    "* 9 * 249265 .",
    "neal , r. m. ( 2001 ) . annealed importance sampling .",
    "comput . _ * 11 * 125139 .",
    "http://www.ams.org/mathscinet-getitem?mr=1837132[mr1837132 ] newton , m. a. , raftery , a. e. ( 1994 ) .",
    "approximate bayesian inference by the weighted likelihood bootstrap ( with discussion ) .",
    "b _ * 56 * 348",
    ". http://www.ams.org/mathscinet-getitem?mr=1257793[mr1257793 ]",
    "ortega , j. m. , rheinboldt , w. c. ( 1967 ) .",
    "monotone iterations for nonlinear equations with application to gauss - seidel methods .",
    "_ siam j. numer .",
    "* 4 * 171190 .",
    "http://www.ams.org/mathscinet-getitem?mr=0215487[mr0215487 ] phillips , d. b. , smith , a. f. m. ( 1996 ) .",
    "bayesian model comparison via jump diffusions . in _",
    "markov chain monte - carlo in practice _ , w.r.gilks ,",
    "richardson and d.j .",
    "spiegelhalter ( eds . ) 215240 .",
    "postman , m. , huchra , j. p. , geller , m. j. ( 1986 ) .",
    "probes of large - scale structure in the corona borealis region .",
    "_ astrophys .",
    "j. _ * 92 * 12381247 .",
    "pfanzagl , j. ( 1979 ) .",
    "conditial distributions as derivatives .",
    "_ * 7 * 10461050 .",
    "http://www.ams.org/mathscinet-getitem?mr=548898[mr548898 ] raftery , a. e. , newton , m. a. , satagopan , j. m. , krivitsky , p. n. ( 2007 ) . estimating the integrated likelihood via posterior simulation using the harmonic mean identity .",
    "_ bayesian statistics _ * 8 * 371416 .",
    "richardson , s. , green , p. j. ( 2007 ) . on bayesian analysis of mixtures with an unknown number of components ( with discussion ) . _",
    "b _ * 59 * 731792 .",
    "http://www.ams.org/mathscinet-getitem?mr=1483213[mr1483213 ] robert , c. p. , wraith , d. ( 2009 ) computational methods for bayesian model choice . in",
    "bayesian inference and maximum entropy methods in science and engineering : the 29th international workshop on bayesian inference and maximum entropy methods in science and engineering . _",
    "( _ aip conference proceedings _ ) , vol . 1193 , pp .",
    "roeder , k. ( 1990 ) .",
    "density estimation with confidence sets exemplified by superclusters and voids in the galaxies .",
    "assoc . _ * 85 * 617624 .",
    "roeder , k. , wasserman , l. ( 1997 ) .",
    "practical bayesian density estimation using mixtures of normals .",
    "_ j. amer .",
    "assoc . _ * 92 * 894902 .",
    "http://www.ams.org/mathscinet-getitem?mr=1482121[mr1482121 ] schechter , p. ( 1976 ) .",
    "an analytic expression for the luminosity function of galaxies . _ astrophys .",
    "j. _ * 203 * 297306 .",
    "shirts , m. r. , chodera , j. d. ( 2008 ) . statistically optimal analysis of samples from multiple equilibrium states .",
    "_ j. chem .",
    "phys . _ * 129 * 124105 .",
    "skilling , j. ( 2006 ) .",
    "nested sampling for general bayesian computation .",
    "_ bayesian anal . _ * 1 * 833860 .",
    "http://www.ams.org/mathscinet-getitem?mr=2282208[mr2282208 ] stephens , m. ( 2000 ) .",
    "bayesian analysis of mixture models with an unknown number of components  an alternative to reversible jump methods . _ ann .",
    "_ * 28 * 4074 .",
    "[ mr1762903 ] tan , z. , gallicchio , e. , lapelosa , m. , levy , r. m. ( 2012 ) .",
    "theory of binless multi - state free energy estimation with applications to protein - ligand binding .",
    "_ j. chem .",
    "phys . _ * 136 * 144102 .",
    "tierney , l. ( 1994 ) .",
    "markov chains for exploring posterior distributions .",
    "statist . _",
    "* 22 * 17011762 .",
    "http://www.ams.org/mathscinet-getitem?mr=1329166[mr1329166 ] vardi , y. ( 1985 ) .",
    "empirical distributions in selection bias models .",
    "statist . _",
    "* 13 * 178203 .",
    "] weinberg , m. d. ( 2012 ) .",
    "computing the bayes factor from a markov chain monte carlo simulation of the posterior distribution .",
    "_ bayesian anal . _ * 7 * 737770 .",
    "http://www.ams.org/mathscinet-getitem?mr=2981634[mr2981634 ] wolpert , r. l. , schmidler , s. c. ( 2012 ) .",
    "@xmath10-stable limit laws for harmonic mean estimators of marginal likelihoods .",
    "_ statist .",
    "* 22 * 12331251 .",
    "xie , w. , lewis , p. , fan , y. , kuo , l. , chen , m .- h .",
    "improving marginal likelihood estimation for bayesian phylogenetic model selection .",
    "biol . _ * 18 * 10011013 ."
  ],
  "abstract_text": [
    "<S> we investigate the utility to computational bayesian analyses of a particular family of recursive marginal likelihood estimators characterized by the ( equivalent ) algorithms known as `` biased sampling '' or `` reverse logistic regression '' in the statistics literature and `` the density of states '' in physics . through a pair of numerical examples ( including mixture modeling of the well - known galaxy dataset ) we highlight the remarkable diversity of sampling schemes amenable to such recursive normalization , as well as the notable efficiency of the resulting pseudo - mixture distributions for gauging prior - sensitivity in the bayesian model selection context . </S>",
    "<S> our key theoretical contributions are to introduce a novel heuristic ( `` thermodynamic integration via importance sampling '' ) for qualifying the role of the bridging sequence in this procedure , and to reveal various connections between these recursive estimators and the nested sampling technique .    </S>",
    "<S> = 1 </S>"
  ]
}