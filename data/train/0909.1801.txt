{
  "article_text": [
    "in today s information - rich world , different sources are best informers about different topics .",
    "if the topic under consideration is well known beforehand , then one chooses the best source .",
    "otherwise , it is not obvious what source or how many sources one should observe .",
    "this need to identify sensors ( information sources ) to be observed in decision making problems is found in many common situations , e.g. , when deciding which news channel to follow .",
    "when a person decides what information source to follow , she relies in general upon her experience , i.e. , one knows through experience what combination of news channels to follow . in engineering applications , a reliable decision on the underlying hypothesis is made through repeated measurements . given infinitely many observations , decision making can be performed accurately .",
    "given a cost associated to each observation , a well - known tradeoff arises between accuracy and number of iterations .",
    "various sequential hypothesis tests have been proposed to detect the underlying hypothesis within a given degree of accuracy . there",
    "exist two different classes of sequential tests .",
    "the first class includes sequential tests developed from the dynamic programming point of view .",
    "these tests are optimal and , in general , difficult to implement  @xcite .",
    "the second class consists of easily - implementable and asymptotically - optimal sequential tests ; a widely - studied example is the sequential probability ratio test ( sprt ) for binary hypothesis testing and its extension , the multi - hypothesis sequential probability ratio test ( msprt ) .    in this paper , we consider the problem of quickest decision making using sequential probability ratio tests .",
    "recent advances in cognitive psychology  @xcite show that the performance of a human performing decision making tasks , such as `` two - alternative forced choice tasks , '' is well modeled by a drift diffusion process , i.e. , the continuous - time version of sprt . roughly speaking , modeling decision making as an sprt process is somehow appropriate even for situations in which a",
    "human is making the decision .",
    "sequential hypothesis testing and quickest detection problems have been vastly studied  @xcite .",
    "the sprt for binary decision making was introduced by wald in @xcite , and was extended by armitage to multiple hypothesis testing in  @xcite .",
    "the armitage test , unlike the sprt , is not necessarily optimal  @xcite .",
    "various other tests for multiple hypothesis testing have been developed throughout the years ; a survey is presented in  @xcite . designing hypothesis tests , i.e.",
    ", choosing thresholds to decide within a given expected number of iterations , through any of the procedures in  @xcite is infeasible as none of them provides any results on the expected sample size .",
    "a sequential test for multiple hypothesis testing was developed in  @xcite ,  @xcite ,  and  @xcite , which provides with an asymptotic expression for the expected sample size .",
    "this sequential test is called the msprt and reduces to the sprt in case of binary hypothesis .",
    "recent years have witnessed a significant interest in the problem of sensor selection for optimal detection and estimation .",
    "tay et al  @xcite discuss the problem of censoring sensors for decentralized binary detection .",
    "they assess the quality of sensor data by the neyman - pearson and a bayesian binary hypothesis test and decide on which sensors should transmit their observation at that time instant .",
    "gupta et al  @xcite focus on stochastic sensor selection and minimize the error covariance of a process estimation problem .",
    "isler et al  @xcite propose geometric sensor selection schemes for error minimization in target detection .",
    "debouk et al  @xcite formulate a markovian decision problem to ascertain some property in a dynamical system , and choose sensors to minimize the associated cost .",
    "wang et al  @xcite design entropy - based sensor selection algorithms for target localization .",
    "joshi et al  @xcite present a convex optimization - based heuristic to select multiple sensors for optimal parameter estimation .",
    "bajovi et al  @xcite discuss sensor selection problems for neyman - pearson binary hypothesis testing in wireless sensor networks .",
    "a third and last set of references related to this paper are those on linear - fractional programming .",
    "various iterative and cumbersome algorithms have been proposed to optimize linear - fractional functions @xcite , @xcite . in particular , for the problem of minimizing the sum and the maximum of linear - fractional functionals , some efficient iterative algorithms have been proposed , including the algorithms by falk et al  @xcite and by benson  @xcite . in this paper , we analyze the problem of time - optimal sequential decision making in the presence of multiple switching sensors and determine a sensor selection strategy to achieve the same .",
    "we consider a sensor network where all sensors are connected to a fusion center .",
    "the fusion center , at each instant , receives information from only one sensor .",
    "such a situation arises when we have interfering sensors ( e.g. , sonar sensors ) , a fusion center with limited attention or information processing capabilities , or sensors with shared communication resources .",
    "the fusion center implements a sequential hypothesis test with the gathered information .",
    "we consider two such tests , namely , the sprt and the msprt for binary and multiple hypothesis , respectively .",
    "first , we develop a version of the sprt and the msprt where the sensor is randomly switched at each iteration , and determine the expected time that these tests require to obtain a decision within a given degree of accuracy .",
    "second , we identify the set of sensors that minimize the expected decision time .",
    "we consider three different cost functions , namely , the conditioned decision time , the worst case decision time , and the average decision time . we show that the expected decision time , conditioned on a given hypothesis , using these sequential tests is a linear - fractional function defined on the probability simplex .",
    "we exploit the special structure of our domain ( probability simplex ) , and the fact that our data is positive to tackle the problem of the sum and the maximum of linear - fractional functionals analytically .",
    "our approach provides insights into the behavior of these functions .",
    "the major contributions of this paper are :    1 .",
    "we develop a version of the sprt and the msprt where the sensor is selected randomly at each observation .",
    "2 .   we determine the asymptotic expressions for the thresholds and the expected sample size for these sequential tests .",
    "3 .   we incorporate the processing time of the sensors into these models to determine the expected decision time .",
    "we show that , to minimize the conditioned expected decision time , the optimal policy requires only one sensor to be observed .",
    "we show that , for a generic set of sensors and @xmath0 underlying hypotheses , the optimal average decision time policy requires the fusion center to consider at most @xmath0 sensors .",
    "6 .   for the binary hypothesis case ,",
    "we identify the optimal set of sensors in the worst case and the average decision time minimization problems .",
    "moreover , we determine an optimal probability distribution for the sensor selection .",
    "7 .   in the worst case and the average decision time minimization problems , we encounter the problem of minimization of sum and maximum of linear - fractional functionals . we treat these problems analytically , and",
    "provide insight into their optimal solutions .",
    "the remainder of the paper is organized in following way . in section  [ sec : problem - setup ] , we present the problem setup . some preliminaries are presented in section  [ sec : preliminaries ] .",
    "we develop the switching - sensor version of the sprt and the msprt procedures in section  [ sec : switch - sensors ] . in section",
    "[ sec : optimal - sensor - selection ] , we formulate the optimization problems for time - optimal sensor selection , and determine their solution .",
    "we elucidate the results obtained through numerical examples in section  [ sec : numerical - examples ] .",
    "our concluding remarks are in section  [ sec : conclusions ] .",
    "we consider a group of @xmath1 agents ( e.g. , robots , sensors , or cameras ) , which take measurements and transmit them to a fusion center .",
    "we generically call these agents `` sensors . ''",
    "we identify the fusion center with a person supervising the agents , and call it `` the supervisor . ''",
    "the goal of the supervisor is to decide , based on the measurements it receives , which of the @xmath2 alternative hypotheses or `` states of nature , '' @xmath3 , @xmath4 is correct . for doing so",
    ", the supervisor uses sequential hypothesis tests , which we briefly review in the next section .",
    "we assume that only one sensor can transmit to the supervisor at each ( discrete ) time instant .",
    "equivalently , the supervisor can process data from only one of the @xmath5 agents at each time .",
    "thus , at each time , the supervisor must decide which sensor should transmit its measurement .",
    "we are interested in finding the optimal sensor(s ) , which must be observed in order to minimize the decision time .",
    "we model the setup in the following way :    1 .",
    "let @xmath6 indicate which sensor transmits its measurement at time instant @xmath7 .",
    "2 .   conditioned on the hypothesis @xmath3 , @xmath8 , the probability that the measurement at sensor @xmath9 is @xmath10 ,",
    "is denoted by @xmath11 .",
    "3 .   the prior probability of the hypothesis @xmath3 , @xmath4",
    ", being correct is @xmath12 .",
    "4 .   the measurement of sensor @xmath9 at time @xmath13 is @xmath14 .",
    "we assume that , conditioned on hypothesis @xmath3 , @xmath14 is independent of @xmath15 , for @xmath16 .",
    "the time it takes for sensor @xmath9 to transmit its measurement ( or for the supervisor to process it ) is @xmath17 .",
    "the supervisor chooses a sensor randomly at each time instant ; the probability to choose sensor @xmath9 is stationary and given by @xmath18 .",
    "the supervisor uses the data collected to execute a sequential hypothesis test with the desired probability of incorrect decision , conditioned on hypothesis @xmath3 , given by @xmath19 .",
    "we assume that there are no two sensors with identical conditioned probability distribution @xmath11 and processing time @xmath20 .",
    "if there are such sensors , we club them together in a single node , and distribute the probability assigned to that node equally among them .",
    "given parameters @xmath21 , @xmath22 , @xmath23 , and @xmath24 , the function @xmath25 , defined by @xmath26 is called a _ linear - fractional function _ @xcite .",
    "a linear - fractional function is quasi - convex as well as quasi - concave . in particular ,",
    "if @xmath27 , then any scalar linear - fractional function @xmath28 satisfies @xmath29 for all @xmath30 $ ] and @xmath31 .      given two probability distributions functions @xmath32 and @xmath33 , the kullback - leibler distance @xmath34 is defined by @xmath35}}= \\int_{\\real } f_1(x ) \\log\\frac{f_1(x)}{f_2(x ) } dx.\\ ] ] further , @xmath36 , and the equality holds if and only if @xmath37 almost everywhere .",
    "the sprt is a sequential binary hypothesis test that provides us with two thresholds to decide on some hypothesis , opposed to classical hypothesis tests , where we have a single threshold .",
    "consider two hypothesis @xmath38 and @xmath39 , with prior probabilities @xmath40 and @xmath41 , respectively .",
    "given their conditional probability distribution functions @xmath42 and @xmath43 , and repeated measurements @xmath44 , with @xmath45 defined by @xmath46 the sprt provides us with two constants @xmath47 and @xmath48 to decide on a hypothesis at each time instant @xmath13 , in the following way :    1 .",
    "compute the log likelihood ratio : @xmath49 : = log @xmath50 , 2 .   integrate evidence up to time @xmath51 , i.e. , @xmath52 : = @xmath53 , 3 .",
    "decide only if a threshold is crossed , i.e. , @xmath54\\eta_0 , \\eta_1 [ } , &   \\mbox{continue sampling}. \\end{cases}\\end{aligned}\\ ] ]    given the probability of false alarm @xmath55 = @xmath56 and probability of missed detection @xmath57 = @xmath58 , the wald s thresholds @xmath47 and @xmath48 are defined by @xmath59 the expected sample size @xmath51 , for decision using sprt is asymptotically given by @xmath60 } } & \\to -\\frac{(1-\\alpha_0)\\eta_0 + \\alpha_0 \\eta_1-\\lambda_0}{\\dist(f^0,f^1)},\\quad \\text{and}\\\\      { \\ensuremath{\\mathbb{e}\\left[n|h_1\\right ] } } & \\to \\frac{\\alpha_1 \\eta_0+(1-\\alpha_1 ) \\eta_1-\\lambda_0}{\\dist(f^1,f^0 ) } ,    \\end{split}\\ ] ] as @xmath61 .",
    "the asymptotic expected sample size expressions in equation   are valid for large thresholds .",
    "the use of these asymptotic expressions as approximate expected sample size is a standard approximation in the information theory literature , and is known as wald s approximation  @xcite ,  @xcite ,  @xcite .    for given error probabilities ,",
    "the sprt is the optimal sequential binary hypothesis test , if the sample size is considered as the cost function  @xcite .",
    "the msprt for multiple hypothesis testing was introduced in  @xcite , and was further generalized in  @xcite and  @xcite .",
    "it is described as follows .",
    "given @xmath0 hypotheses with their prior probabilities @xmath62 , the posterior probability after @xmath51 observations @xmath63 is given by @xmath64 where @xmath65 is the probability density function of the observation of the sensor , conditioned on hypothesis @xmath66 .",
    "before we state the msprt , for a given @xmath51 , we define @xmath67 by @xmath68    the msprt at each sampling iteration @xmath13 is defined as @xmath69 where the thresholds @xmath70 , for given frequentist error probabilities ( accept a given hypothesis wrongly ) @xmath19 , @xmath4 , are given by @xmath71 where @xmath720,1[}$ ] is a constant function of @xmath65 ( see  @xcite ) .",
    "it can be shown  @xcite that the expected sample size of the msprt , conditioned on a hypothesis , satisfies @xmath73 } } \\to \\frac{-\\log \\eta_k}{\\delta_k } ,    \\quad \\text{as } { \\ensuremath{\\underset{k\\in{{\\{0,\\dots , m-1\\}}}}{\\overset{}{\\max } } } } \\eta_k \\to 0^+,\\ ] ] where @xmath74 .",
    "the msprt is an easily - implementable hypothesis test and is shown to be asymptotically optimal in  @xcite .",
    "consider the case when the fusion center collects data from @xmath5 sensors . at each iteration",
    "the fusion center looks at one sensor chosen randomly with probability @xmath18 , @xmath75 .",
    "the fusion center performs sprt with the collected data .",
    "we define this procedure as sprt with switching sensors .",
    "if we assume that sensor @xmath76 is observed at iteration @xmath13 , and the observed value is @xmath14 , then sprt with switching sensors is described as following , with the thresholds @xmath47 and @xmath48 defined in equation , and @xmath45 defined in equation  :    1 .",
    "compute log likelihood ratio : @xmath77 2 .   integrate evidence up to time @xmath51 , i.e. , @xmath52 : = @xmath53 , 3 .",
    "decide only if a threshold is crossed , i.e. , @xmath78\\eta_0 , \\eta_1 [ } , & \\mbox{continue sampling}. \\end{cases}\\end{aligned}\\ ] ]    [ lem : sample - size - switch - sprt ] for the sprt with switching sensors described above , the expected sample size conditioned on a hypothesis is asymptotically given by : @xmath79 } } & \\to -\\frac{(1-\\alpha_0)\\eta_0 + \\alpha_0          \\eta_1-\\lambda_0}{{\\ensuremath{\\underset{s=1}{\\overset{n}{\\sum}}}}q_s          \\dist(f_s^0,f_s^1)},\\quad \\text{and }        \\\\        { \\ensuremath{\\mathbb{e}\\left[n|h_1\\right ] } } & \\to \\frac{\\alpha_1          \\eta_0+(1-\\alpha_1)\\eta_1-\\lambda_0}{{\\ensuremath{\\underset{s=1}{\\overset{n}{\\sum}}}}q_s\\dist(f_s^1,f_s^0 ) } ,      \\end{split}\\ ] ] as @xmath61 .",
    "similar to the proof of theorem 3.2 in  @xcite .",
    "the expected sample size converges to the values in equation   for large thresholds . from equation",
    ", it follows that large thresholds correspond to small error probabilities . in the remainder of the paper , we assume that the error probabilities are chosen small enough , so that the above asymptotic expression for sample size is close to the actual expected sample size .",
    "[ lem : decision - time - switch - sprt ] given the processing time of the sensors @xmath80 , the expected decision time of the sprt with switching sensors @xmath81 , conditioned on the hypothesis @xmath3 , @xmath82 , is @xmath83}}=   \\frac{{\\ensuremath{\\underset{s=1}{\\overset{n}{\\sum } } } } q_st_s}{{\\ensuremath{\\underset{s=1}{\\overset{n}{\\sum } } } }      q_si^k_s}=\\frac{q\\cdot t}{q\\cdot i^k } ,   \\quad\\mbox{for each }    k\\in\\{0,1\\},\\ ] ] where @xmath84 , are constant vectors for each @xmath85 .    the decision time using sprt with switching sensors is the sum of sensor s processing time at each iteration .",
    "we observe that the number of iterations in sprt and the processing time of sensors are independent .",
    "hence , the expected value of the decision time @xmath81 is @xmath86}}= { \\ensuremath{\\mathbb{e}\\left[n|h_k\\right]}}{\\ensuremath{\\mathbb{e}\\left[t\\right ] } } , \\quad \\text{for each } k\\in\\{0,1\\}.\\end{aligned}\\ ] ] by the definition of expected value , @xmath87 } } = { \\ensuremath{\\underset{s=1}{\\overset{n}{\\sum } } } } q_st_s.\\end{aligned}\\ ] ] from equations , , and it follows that @xmath88}}= \\frac{{\\ensuremath{\\underset{s=1}{\\overset{n}{\\sum } } } }      q_st_s}{{\\ensuremath{\\underset{s=1}{\\overset{n}{\\sum}}}}q_si^k_s}= \\frac{q\\cdot t}{q\\cdot i^k } ,    \\quad\\mbox{for each } k\\in\\{0,1\\},\\ ] ] where @xmath89 is a constant , for each @xmath85 , and @xmath75 .",
    "we call the msprt with the data collected from @xmath5 sensors while observing only one sensor at a time as the msprt with switching sensors .",
    "the one sensor to be observed at each time is determined through a randomized policy , and the probability of choosing sensor @xmath9 is stationary and given by @xmath18 .",
    "assume that the sensor @xmath90 is chosen at time instant @xmath13 , and the prior probabilities of the hypothesis are given by @xmath62 , then the posterior probability after @xmath51 observations @xmath63 is given by @xmath91    before we state the msprt with switching sensors , for a given @xmath51 , we define @xmath92 by @xmath93 for the thresholds @xmath70 , @xmath4 , defined in equation  , the msprt with switching sensors at each sampling iteration @xmath51 is defined by @xmath94    before we state the results on asymptotic sample size and expected decision time , we introduce the following notation . for a given hypothesis @xmath3 , and a sensor @xmath9 , we define @xmath95 by @xmath96 we also define @xmath97 by @xmath98 where @xmath99 represents the probability simplex in @xmath100 .",
    "[ lem : sample - size - msprt ] given thresholds @xmath101 , @xmath102 , the sample size @xmath51 required for decision satisfies @xmath103}}}{-\\log \\eta_k } \\to \\frac{1}{\\mathcal{e}_\\dist(q , f^k , f^{j^*_k})},\\ ] ] as @xmath104 .",
    "the proof follows from theorem 5.1 of  @xcite and the observation that @xmath105    [ decision - time - dmsprt ] given the processing time of the sensors @xmath80 , the expected decision time @xmath81 conditioned on the hypothesis @xmath3 , for each @xmath4 , is given by @xmath106 } } = \\frac{-\\log        \\eta_k}{\\mathcal{e}_\\dist(q , f^k , f^{j^*_k } ) } { \\ensuremath{\\underset{s=1}{\\overset{n}{\\sum } } } }      q_st_s= \\frac{q\\cdot t}{q\\cdot \\bar{i}^k},\\ ] ] where @xmath107 are constants .",
    "similar to the proof of lemma  [ lem : decision - time - switch - sprt ] .",
    "in this section we consider sensor selection problems with the aim to minimize the expected decision time of a sequential hypothesis test with switching sensors .",
    "as exemplified in lemma  [ decision - time - dmsprt ] , the problem features multiple conditioned decision times and , therefore , multiple distinct cost functions are of interest . in scenario",
    "i below , we aim to minimize the decision time conditioned upon one specific hypothesis being true ; in scenarios ii and iii we will consider worst - case and average decision times . in all three scenarios",
    "the decision variables take values in the probability simplex .",
    "minimizing decision time conditioned upon a specific hypothesis may be of interest when fast reaction is required in response to the specific hypothesis being indeed true .",
    "for example , in change detection problems one aims to quickly detect a change in a stochastic process ; the cusum algorithm ( also referred to as page s test )  @xcite is widely used in such problems .",
    "it is known  @xcite that , with fixed threshold , the cusum algorithm for quickest change detection is equivalent to an sprt on the observations taken after the change has occurred .",
    "we consider the minimization problem for a single conditioned decision time in scenario i below and we show that , in this case , observing the best sensor each time is the optimal strategy .",
    "in general , no specific hypothesis might play a special role in the problem and , therefore , it is of interest to simultaneously minimize multiple decision times over the probability simplex .",
    "this is a multi - objective optimization problem , and may have pareto - optimal solutions .",
    "we tackle this problem by constructing a single aggregate objective function . in the binary hypothesis case ,",
    "we construct two single aggregate objective functions as the maximum and the average of the two conditioned decision times .",
    "these two functions are discussed in scenario ii and scenario iii respectively . in the multiple hypothesis setting ,",
    "we consider the single aggregate objective function constructed as the average of the conditioned decision times .",
    "an analytical treatment of this function for @xmath108 , is difficult .",
    "we determine the optimal number of sensors to be observed , and direct the interested reader to some iterative algorithms to solve such optimization problems .",
    "this case is also considered under scenario iii .",
    "before we pose the problem of optimal sensor selection , we introduce the following notation .",
    "we denote the probability simplex in @xmath109 by @xmath99 , and the vertices of the probability simplex @xmath99 by @xmath110 , @xmath111 .",
    "we refer to the line joining any two vertices of the simplex as an _ edge_. finally , we define @xmath112 , @xmath4 , by @xmath113      we consider the case when the supervisor is trying to detect a particular hypothesis , irrespective of the present hypothesis . the corresponding optimization problem for a fixed @xmath4 is posed in the following way : @xmath114    the solution to this minimization problem is given in the following theorem .",
    "[ thm : single ] the solution to the minimization problem is @xmath115 , where @xmath116 is given by @xmath117 and the minimum objective function is @xmath118 } } = \\frac{t_{s^*}}{i_{s^*}^k}.\\ ] ]    we notice that objective function is a linear - fractional function . in the following argument , we show that the minima occurs at one of the vertices of the simplex .",
    "we first notice that the probability simplex is the convex hull of the vertices , i.e. , any point @xmath119 in the probability simplex can be written as @xmath120 we invoke equation , and observe that for some @xmath121 $ ] and for any @xmath122 @xmath123 which can be easily generalized to @xmath124 for any point @xmath119 in the probability simplex @xmath99 .",
    "hence , minima will occur at one of the vertices @xmath125 , where @xmath116 is given by @xmath126      for the binary hypothesis testing , we consider the multi - objective optimization problem of minimizing both decision times simultaneously .",
    "we construct single aggregate objective function by considering the maximum of the two objective functions .",
    "this turns out to be a worst case analysis , and the optimization problem for this case is posed in the following way :    @xmath127    before we move on to the solution of above minimization problem , we state the following results .    [",
    "lem : mon ] the functions @xmath128 , @xmath4 are monotone on the probability simplex @xmath99 , in the sense that given two points @xmath129 , the function @xmath128 is monotonically non - increasing or monotonically non - decreasing along the line joining @xmath130 and @xmath131 .",
    "consider probability vectors @xmath132 .",
    "any point @xmath133 on line joining @xmath130 and @xmath131 can be written as @xmath134 , @xmath1350,1[}$ ] .",
    "we note that @xmath136 is given by : @xmath137    the derivative of @xmath128 along the line joining @xmath130 and @xmath131 is given by @xmath138 we note that the sign of the derivative of @xmath128 along the line joining two points @xmath139 is fixed by the choice of @xmath130 and @xmath131 .",
    "hence , the function @xmath128 is monotone over the line joining @xmath130 and @xmath131 .",
    "moreover , note that if @xmath140 , then @xmath128 is strictly monotone . otherwise , @xmath128 is constant over the line joining @xmath130 and @xmath131 .",
    "[ lem : opt_max ] define @xmath141 by @xmath142 .",
    "a minimum of @xmath28 lies at the intersection of the graphs of @xmath143 and @xmath144 , or at some vertex of the probability simplex @xmath99 .",
    "case 1 : the graphs of @xmath143 and @xmath144 do not intersect at any point in the simplex @xmath99 .    in this case , one of the functions @xmath143 and @xmath144 is an upper bound to the other function at every point in the probability simplex @xmath99 .",
    "hence , @xmath145 , for some @xmath82 , at every point in the probability simplex @xmath99 . from theorem [ thm :",
    "single ] , we know that the minima of @xmath128 on the probability simplex @xmath99 lie at some vertex of the probability simplex @xmath99 .    case 2 : the graphs of @xmath143 and @xmath144 intersect at a set @xmath146 in the probability simplex @xmath99 , and let @xmath147 be some point in the set @xmath146 .",
    "suppose , a minimum of @xmath28 occurs at some point @xmath148 , and @xmath149 , where relint@xmath150 denotes the relative interior . with out loss of generality",
    ", we can assume that @xmath151 .",
    "also , @xmath152 , and @xmath153 by assumption .",
    "we invoke lemma [ lem : mon ] , and notice that @xmath143 and @xmath144 can intersect at most once on a line .",
    "moreover , we note that @xmath154 , hence , along the half - line from @xmath147 through @xmath155 , @xmath156 , that is , @xmath157 . since @xmath153 , @xmath28 is decreasing along this half - line .",
    "hence , @xmath28 should achieve its minimum at the boundary of the simplex @xmath99 , which contradicts that @xmath155 is in the relative interior of the simplex @xmath99 . in summary , if a minimum of @xmath28 lies in the relative interior of the probability simplex @xmath99 , then it lies at the intersection of the graphs of @xmath143 and @xmath144 .",
    "the same argument can be applied recursively to show that if a minimum lies at some point @xmath158 on the boundary , then either @xmath159 or the minimum lies at the vertex .",
    "in the following arguments , let @xmath146 be the set of points in the simplex @xmath99 , where @xmath160 , that is , @xmath161    also notice that the set @xmath146 is non empty if and only if @xmath162 has at least one non - negative and one non - positive entry .",
    "if the set @xmath146 is empty , then it follows from lemma [ lem : opt_max ] that the solution of optimization problem in equation   lies at some vertex of the probability simplex @xmath99 .",
    "now we consider the case when @xmath146 is non empty .",
    "we assume that the sensors have been re - ordered such that the entries in @xmath162 are in ascending order .",
    "we further assume that , for @xmath162 , the first @xmath163 entries , @xmath164 , are non positive , and the remaining entries are positive .",
    "[ lem : vertex ] if the set @xmath146 defined in equation is non empty , then the polytope generated by the points in the set @xmath146 has vertices given by : @xmath165    any @xmath166 satisfies the following constraints @xmath167 , \\label{eq : simplex}\\\\ { \\ensuremath{\\underset{s=1}{\\overset{n}{\\sum } } } } q_s(i^0_s - i^1_s ) = 0 , \\label{eq : null}\\end{gathered}\\ ] ] eliminating @xmath168 , using equation and equation , we get : @xmath169 the equation defines a hyperplane , whose extreme points in @xmath170 are given by @xmath171    note that for @xmath172 , @xmath173 .",
    "hence , these points define some vertices of the polytope generated by points in the set @xmath146 .",
    "also note that the other vertices of the polytope can be determined by the intersection of each pair of lines through @xmath174 and @xmath175 , and @xmath176 and @xmath177 , for @xmath172 , and @xmath178 .",
    "in particular , these vertices are given by @xmath179 defined in equation .",
    "hence , all the vertices of the polytopes are defined by @xmath179 , @xmath172 , @xmath180 .",
    "therefore , the set of vertices of the polygon generated by the points in the set @xmath146 is @xmath181 .",
    "before we state the solution to the optimization problem , we define the following : @xmath182 we also define @xmath183    [ thm : worst - case ] for the optimization problem , an optimal probability vector is given by : @xmath184 and the minimum value of the function is given by : @xmath185    we invoke lemma [ lem : opt_max ] , and note that a minimum should lie at some vertex of the simplex @xmath99 , or at some point in the set @xmath146 .",
    "note that @xmath160 on the set @xmath146 , hence the problem of minimizing max@xmath186 reduces to minimizing @xmath143 on the set @xmath146 . from theorem [ thm : single",
    "] , we know that @xmath143 achieves the minima at some extreme point of the feasible region .",
    "from lemma [ lem : vertex ] , we know that the vertices of the polytope generated by points in set @xmath146 are given by set @xmath181 .",
    "we further note that @xmath187 and @xmath188 are the value of objective function at the points in the set @xmath181 and the vertices of the probability simplex @xmath99 respectively , which completes the proof .      for the multi - objective optimization problem of minimizing all the decision times simultaneously on the simplex , we formulate the single aggregate objective function as the average of these decision times .",
    "the resulting optimization problem , for @xmath2 , is posed in the following way : @xmath189    in the following discussion we assume @xmath190 , unless otherwise stated .",
    "we analyze the optimization problem in equation   as follows :    [ lem : monotone - sum ] the objective function in optimization problem in equation   has no critical point on @xmath99 if the vectors @xmath191 are linearly independent .",
    "the jacobian of the objective function in the optimization problem in equation   is @xmath192\\in\\real^{n\\times ( m+1 ) } , \\text { and } \\\\",
    "{ \\psi : \\delta_{n-1 } & \\rightarrow \\real^{m+1 } } \\text { is defined by } \\\\",
    "\\psi(q ) & =   \\left[\\begin{array } { cccc } { \\ensuremath{\\underset{k=0}{\\overset{m-1}{\\sum } } } } \\frac{1}{q\\cdot i^k } & \\frac{q\\cdot t}{(q\\cdot i^0)^2 } & \\ldots & \\frac{q\\cdot t}{(q\\cdot i^{m-1})^2 } \\end{array}\\right]^{\\text{t}}.\\end{aligned}\\ ] ]    for @xmath190 , if the vectors @xmath193 are linearly independent , then @xmath194 is full rank .",
    "further , the entries of @xmath195 are non - zero on the probability simplex @xmath99 . hence , the jacobian does not vanish anywhere on the probability simplex @xmath99 .",
    "[ lem : indep ] for @xmath196 , if @xmath197 and @xmath198 are linearly independent , and @xmath199 , for some @xmath200 , then the following statements hold :    1 .",
    "if @xmath56 and @xmath58 have opposite signs , then @xmath201 has no critical point on the simplex @xmath99 , and 2 .   for @xmath202",
    ", @xmath201 has a critical point on the simplex @xmath99 if and only if there exists @xmath203 perpendicular to the vector @xmath204 .",
    "we notice that the jacobian of @xmath201 satisfies @xmath205 substituting @xmath206 , equation   becomes @xmath207 since @xmath197 , and @xmath198 are linearly independent , we have @xmath208 hence , @xmath201 has a critical point on the simplex @xmath99 if and only if @xmath209 notice that , if @xmath56 , and @xmath58 have opposite signs , then equation can not be satisfied for any @xmath210 , and hence , @xmath201 has no critical point on the simplex @xmath99 .",
    "if @xmath211 , then equation leads to @xmath212 therefore , @xmath201 has a critical point on the simplex @xmath99 if and only if there exists @xmath203 perpendicular to the vector @xmath213 .    [",
    "lem : optimal - sensors ] for @xmath190 , if each @xmath214 submatrix of the matrix @xmath215\\in\\real^{n\\times ( m+1)}\\ ] ] is full rank , then the following statements hold :    1 .",
    "every solution of the optimization problem   lies on the probability simplex @xmath216 ; and 2 .",
    "every time - optimal policy requires at most @xmath0 sensors to be observed .    from lemma [ lem : monotone - sum ] ,",
    "we know that if @xmath217 are linearly independent , then the jacobian of the objective function in equation   does not vanish anywhere on the simplex @xmath99 .",
    "hence , a minimum lies at some simplex @xmath218 , which is the boundary of the simplex @xmath99 .",
    "notice that , if @xmath190 and the condition in the lemma holds , then the projections of @xmath193 on the simplex @xmath218 are also linearly independent , and the argument repeats .",
    "hence , a minimum lies at some simplex @xmath219 , which implies that optimal policy requires at most @xmath0 sensors to be observed .",
    "[ lem : opt - edge ] given two vertices @xmath176 and @xmath177 , @xmath220 , of the probability simplex @xmath99 , then for the objective function in the problem with @xmath196 , the following statements hold :    1 .",
    "if @xmath221 , and @xmath222 , then the minima , along the edge joining @xmath176 and @xmath177 , lies at @xmath176 , and optimal value is given by @xmath223 ; and 2 .",
    "if @xmath224 , and @xmath222 , or vice versa , then the minima , along the edge joining @xmath176 and @xmath177 , lies at the point @xmath225 , where @xmath2260,1[},\\\\        \\mu&=\\frac{i_r^0\\sqrt{t_s i_r^1 -t_r            i_s^1}-i_r^1\\sqrt{t_ri_s^0-t_si_r^0}}{i_s^1\\sqrt{t_ri_s^0-t_si_r^0}-i^0_s\\sqrt{t_s            i_r^1 -t_r i_s^1}}>0 ,      \\end{aligned}\\ ] ] and the optimal value is given by @xmath227    we observe from lemma [ lem : mon ] that both @xmath143 , and @xmath144 are monotonically non - increasing or non - decreasing along any line .",
    "hence , if @xmath221 , and @xmath222 , then the minima should lie at @xmath176 .",
    "this concludes the proof of the first statement .",
    "we now establish the second statement .",
    "we note that any point on the line segment connecting @xmath176 and @xmath177 can be written as @xmath228 .",
    "the value of @xmath201 at @xmath133 is @xmath229 differentiating with respect to @xmath230 , we get @xmath231    notice that the two terms in equation have opposite sign . setting the derivative to zero , and choosing the value of @xmath230 in @xmath232 $ ] , we get @xmath233 , where @xmath234 is as defined in the statement of the theorem",
    ". the optimal value of the function can be obtained , by substituting @xmath235 in the expression for @xmath236 .",
    "[ thm : average ] for the optimization problem in equation   with @xmath196 , the following statements hold :    1 .   if @xmath197 , @xmath198 are linearly dependent",
    ", then the solution lies at some vertex of the simplex @xmath99 .",
    "if @xmath197 and @xmath198 are linearly independent , and @xmath206 , @xmath200 , then the following statements hold : 1 .",
    "if @xmath56 and @xmath58 have opposite signs , then the optimal solution lies at some edge of the simplex @xmath99 .",
    "2 .   if @xmath211 , then the optimal solution may lie in the interior of the simplex @xmath99 .",
    "if every @xmath237 sub - matrix of the matrix @xmath238\\in\\real^{n\\times 3}$ ] is full rank , then a minimum lies at an edge of the simplex @xmath99 .",
    "we start by establishing the first statement . since , @xmath197 and @xmath198 are linearly dependent , there exists a @xmath239 such that @xmath240 . for @xmath240",
    ", we have @xmath241 .",
    "hence , the minima of @xmath201 lies at the same point where @xmath143 achieves the minima . from theorem  [ thm : single ] , it follows that @xmath143 achieves the minima at some vertex of the simplex @xmath99 .    to prove the second statement",
    ", we note that from lemma  [ lem : indep ] , it follows that if @xmath56 , and @xmath58 have opposite signs , then the jacobian of @xmath201 does not vanish anywhere on the simplex @xmath99 .",
    "hence , the minima lies at the boundary of the simplex .",
    "notice that the boundary , of the simplex @xmath99 , are @xmath5 simplices @xmath218 .",
    "notice that the argument repeats till @xmath242 .",
    "hence , the optima lie on one of the @xmath243 simplices @xmath244 , which are the edges of the original simplex .",
    "moreover , we note that from lemma  [ lem : indep ] , it follows that if @xmath245 , then we can not guarantee the number of optimal set of sensors .",
    "this concludes the proof of the second statement .    to prove the last statement",
    ", we note that it follows immediately from lemma  [ lem : optimal - sensors ] that a solution of the optimization problem in equation   would lie at some simplex @xmath244 , which is an edge of the original simplex .",
    "note that , we have shown that , for @xmath196 and a generic set of sensors , the solution of the optimization problem in equation   lies at an edge of the simplex @xmath99 .",
    "the optimal value of the objective function on a given edge was determined in lemma  [ lem : opt - edge ] .",
    "hence , an optimal solution of this problem can be determined by a comparison of the optimal values at each edge .    for the multiple hypothesis case ,",
    "we have determined the time - optimal number of the sensors to be observed in lemma  [ lem : optimal - sensors ] . in order to identify these sensors",
    ", one needs to solve the optimization problem in equation  .",
    "we notice that the objective function in this optimization problem is non - convex , and is hard to tackle analytically for @xmath108 .",
    "interested reader may refer to some efficient iterative algorithms in linear - fractional programming literature ( e.g. , @xcite ) to solve these problems .",
    "we consider four sensors connected to a fusion center .",
    "we assume that the sensors take binary measurements .",
    "the probabilities of their measurement being zero , under two hypotheses , and their processing times are given in the table  [ tab : prob ] .",
    ".conditional probabilities of measurement being zero [ cols=\"^,^,^,^\",options=\"header \" , ]     we note that the optimal results , we obtained , may only be sub - optimal because of the asymptotic approximations in equations  and  .",
    "we further note that , for small error probabilities and large sample sizes , these asymptotic approximations yield fairly accurate results  @xcite , and in fact , this is the regime in which it is of interest to minimize the expected decision time . therefore , for all practical purposes the obtained optimal scheme is very close to the actual optimal scheme .",
    "in this paper , we considered the problem of sequential decision making .",
    "we developed versions sprt and msprt where the sensor switches at each observation .",
    "we used these sequential procedures to decide reliably .",
    "we found out the set of optimal sensors to decide in minimum time .",
    "for the binary hypothesis case , three performance metrics were considered and it was found that for a generic set of sensors at most two sensors are optimal .",
    "further , it was shown that for @xmath0 underlying hypotheses , and a generic set of sensors , an optimal policy requires at most @xmath0 sensors to be observed . a procedure for identification of the optimal sensor",
    "was developed . in the binary hypothesis case ,",
    "the computational complexity of the procedure for the three scenarios , namely , the conditioned decision time , the worst case decision time , and the average decision time , was @xmath246 , @xmath247 , and @xmath247 , respectively .",
    "many further extensions to the results presented here are possible .",
    "first , the time - optimal scheme may not be energy optimal .",
    "in particular , the time optimal set of sensors may be the most distant sensors from the fusion center . given that the power to transmit the signal to the fusion center is proportional to the distance from the fusion center , the time - optimal scheme is no where close to the energy optimal scheme .",
    "this trade off can be taken care of by adding a term proportional to distance in the objective function .",
    "when we choose only one or two sensors every time , issues of robustness do arise . in case of sensor failure",
    ", we need to determine the next best sensor to switch .",
    "a list of sensors , with increasing decision time , could be prepared beforehand and in case of sensor failure , the fusion center should switch to the next best set of sensors .",
    "d.  bajovi , b.  sinopoli , and j.  xavier .",
    "sensor selection for hypothesis testing in wireless sensor networks : a kullback - leibler based approach . in _",
    "ieee conf . on decision and control",
    "_ , shanghai , china , december 2009 . to appear .",
    "h.  wang , k.  yao , g.  pottie , and d.  estrin .",
    "entropy - based sensor selection heuristic for target localization . in _",
    "symposium on information processing of sensor networks _ , pages 3645 , berkeley , ca , april 2004 .",
    "x.  j. zhang .",
    "extension to multiple hypothesis testing . in m.",
    "thoma and a.  wyner , editors , _ auxiliary signal design in fault detection and diagnosis _ , volume 134 of _ lecture notes in control and information sciences_. springer , 1989 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of sensor selection for time - optimal detection of a hypothesis . </S>",
    "<S> we consider a group of sensors transmitting their observations to a fusion center . </S>",
    "<S> the fusion center considers the output of only one randomly chosen sensor at the time , and performs a sequential hypothesis test . </S>",
    "<S> we consider the class of sequential tests which are easy to implement , asymptotically optimal , and computationally amenable . </S>",
    "<S> for three distinct performance metrics , we show that , for a generic set of sensors and binary hypothesis , the fusion center needs to consider at most two sensors . </S>",
    "<S> we also show that for the case of multiple hypothesis , the optimal policy needs at most as many sensors to be observed as the number of underlying hypotheses .    </S>",
    "<S> sensor selection , decision making , sprt , msprt , sequential hypothesis testing , linear - fractional programming . </S>"
  ]
}