{
  "article_text": [
    "a multi - way relay network in which a group of physically separated nodes exchange data .",
    "direct communication between the nodes is not permitted , and the exchange is only made possible with the help of a relay .",
    "the nodes encode and transmit their data over an _ uplink _ ( a multiple - access channel ) to the relay .",
    "the relay processes this information and transmits over the _ downlink _ ( a broadcast channel ) to every node .",
    "we assume that each node requires a lossless reconstruction of the data of all other nodes .",
    "the above relay network aims to model communication in cellular and satellite networks .",
    "a large body of work has comprehensively studied the network from the perspective of source coding  @xcite , channel capacity  @xcite , and network coding  @xcite . however , despite this intense effort , the information - theoretic limits of the network remain largely unknown .",
    "we study the relay network under two specific assumptions .",
    "the first assumption is that the data is arbitrarily correlated  generated by a discrete - memoryless ( dm ) source  and the communications problem involves joint source - channel ( jsc ) coding .",
    "correlated data might take the form of measurements in a sensor network  @xcite , voice data in a cellular network , and data files in a peer - to - peer network .",
    "we wish to determine when a given source can be reliably communicated ( in the usual shannon sense ) over a given channel .",
    "the second assumption is that the _ downlink _ is an arbitrary dm broadcast channel and the _ uplink _ is an orthogonal dm multiple - access channel . our motivation to study",
    "an orthogonal uplink stems from shannon s classic separation theorem  ( * ? ? ?",
    "7.3 ) , which states that the problem of losslessly transmitting a dm source over a point - to - point dm channel can be divided into two independent problems  source coding and channel coding .",
    "moreover , the individual optimisation of stand - alone source and channel codes is optimal for the overall point - to - point jsc problem .",
    "the separation theorem is important in practice because , for example , systems are rarely restricted to transmitting a single source over a fixed channel with known statistics ; indeed , to quote gallager  ( * ? ? ?",
    "140 ) ( see also  ( * ? ? ?",
    "406 ) ) :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  in many data transmission systems the probabilities with which the messages are to be used are either unknown or unmeaningful . \" _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the modular nature of the separate source - channel coding architecture allows the source and channel codes to be changed as needed , without compromising overall optimality  @xcite .",
    "unfortunately , separation may or may not be optimal for networks in general ; for example , separation is suboptimal for the multiple - access channel  ( * ? ? ?",
    "592 ) and the broadcast channel  ( * ? ? ?",
    "14.2 ) , and it is optimal for the orthogonal multiple - access channel  @xcite . given this state of affairs , it is natural to ask whether separation is optimal for the multi - way relay network .",
    "we prove , in this paper , that separation holds for the special case of two nodes .",
    "_ paper outline : _    * * section  [ sec : jscc ] : * we formally define the jsc - coding multi - way relay problem , and we characterise reliable communication with matching single - letter achievability and converse theorems . the achievability proof employs a jsc random - coding argument , which builds on the _ virtual binning _",
    "idea of tuncel  @xcite and the _ cascaded slepian - wolf binning _",
    "idea of wyner _",
    "_  @xcite . * * section  [ sec : sscc ] : * we formalise a notion of source - channel separation , and we prove a separation theorem for _ two _ nodes ; that is , it is asymptotically ( in blocklength ) optimal to separate the source and channel coding functions . *",
    "* section  [ sec : practical ] : * we use the two - node separation theorem as a basis to design practical low complexity codes .",
    "specifically , we consider source and channel codes based on low - density parity - check ( ldpc ) codes .",
    "we show how the individual ldpc codes for source and channel coding can be represented by a joint factor - graph  @xcite , and we use this graph to provide an alternative view of the separation theorem . finally , we present a numerical example and discuss the differences between joint and separate decoding .    _",
    "notation : _ random variables and their alphabets are identified by uppercase and script letters respectively , e.g. @xmath0 and @xmath1 .",
    "random vectors defined on the cartesian product of a set are identified by boldface font , e.g. , @xmath2 takes values from @xmath3 subsets and strict subsets of an alphabet are identified with @xmath4 and @xmath5 respectively .",
    "set complement is denoted by a superscript @xmath6 ; e.g. , if @xmath7 then @xmath8 if @xmath9 is a singleton , say @xmath10 , then we write @xmath11 .",
    "consider fig .  [ fig : jsccarc ] .",
    "suppose that a _",
    "discrete memoryless source _ emits an i.i.d .",
    "string @xmath12 of arbitrarily distributed random variables @xmath13 @xmath14 .",
    "let @xmath15 denote the alphabet of the @xmath16-th random variable @xmath17 for each index @xmath16 in @xmath18 .",
    "the @xmath17-component of the sequence in   is given to node  @xmath16 .",
    "the orthogonal _ uplink channel _ from node @xmath16 to the relay is discrete and memoryless with input alphabet @xmath19 , output alphabet @xmath20 , and transition probabilities @xmath21.\\ ] ] the _ downlink broadcast channel _ is discrete and memoryless with input alphabet @xmath22 , output alphabet @xmath23 at node  @xmath16 , and transition probabilities @xmath24.\\end{gathered}\\ ] ]    a _ joint source - channel ( jsc ) code _ with blocklength @xmath25 , see fig .",
    "[ fig : jsccarc ] , is a collection of @xmath26-maps : the encoder at node  @xmath16 ,    [ eqn : defjscc ] @xmath27 the encoder at the relay , @xmath28 and the decoder at node @xmath16 , @xmath29    node  @xmath16 observes @xmath30 and transmits @xmath31 the relay observes @xmath32 on the @xmath16-th uplink channel , and it transmits @xmath33 node  @xmath16 observes @xmath34 and decodes @xmath35    the _ average joint decoding error probability _ of a jsc - code is @xmath36.\\end{gathered}\\ ] ]    we say that _ reliable communication is achievable with jsc codes _ if there exists for each @xmath37 a code of the form   with @xmath38 for some sufficiently large integer @xmath25 .",
    "the following notation is required for the next theorem .",
    "denote the _ capacity _  ( * ? ? ?",
    "7.1 ) of the @xmath16-th orthogonal uplink channel by @xmath39 where the maximisation is over distributions for @xmath40 on @xmath19 .",
    "if @xmath41 is a nonempty subset of @xmath18 , then let @xmath42 denote those random variables with indices belonging to @xmath9 .",
    "the next theorem is proved in appendix  [ app : proofofthm : jscc ] .",
    "[ thm : jscc ] reliable communication is achievable with jsc codes if    [ eqn : jscc ] @xmath43 holds for each nonempty _ strict _ subset @xmath9 of @xmath18 , and there exists a distribution for @xmath44 on @xmath22 such that @xmath45 holds for each @xmath16 in @xmath18 .",
    "conversely , if reliable communication is achievable then   or   hold as inequalities  instead of strict inequalities  for some @xmath44 .",
    "_ non - matched symbol rates : _ theorem  [ thm : jscc ] characterises reliable communication for matched source and channel symbol rates ; i.e. , @xmath25 source symbols are mapped to @xmath25 channel symbols .",
    "the proof easily extends to the non - matched symbol rate setting where @xmath25 source symbols map to @xmath46 channel symbols .",
    "_ networks of channels : _ the uplink condition   closely resembles han s generalisation",
    "* sec .  1 ) of the slepian - wolf / cover theorem  @xcite",
    "to networks of noisy orthogonal channels ( see also barros and servetto  @xcite ) . indeed , all but one of the inequalities appearing in  @xcite also appear as uplink constraints in    the exception being a total sum rate constraint of the form @xmath47 although our problem formulation differs from that of  @xcite , the similarity of these results can be understood by comparing the respective achievability proofs .",
    "han  @xcite uses a simple separate source - channel coding argument : he combines an optimal slepian - wolf code with optimal channel codes for each orthogonal uplink . in han s setup ,",
    "reliable communication is possible if   and   both hold .",
    "the uplink part of our proof essentially uses the same argument , except we do not require that   holds ; i.e. , we use fewer bins and , as a consequence , the relay can not decode the sources .",
    "indeed , in our setup , the relay needs only to recover the slepian - wolf bin indices and not the individual source sequences .",
    "the downlink achievability proof requires jsc coding and is discussed next .    _",
    "joint source - channel coding : _ the ( downlink ) achievability proof of theorem  [ thm : jscc ] is based on a jsc random - coding argument that builds upon the virtual binning idea developed by tuncel in  @xcite . to illustrate why the virtual binning approach is useful",
    ", momentarily suppose that the relay is given the entire source @xmath48-tuple @xmath49 and consider the downlink phase in isolation . with the setup of  @xcite in mind",
    ", we can view @xmath49 as a common message that needs to be reliably decoded by every node . applying  ( * ? ? ?",
    "* thm .  6 ) we immediately see that the common message can be reliably decoded by every node whenever   holds .    the basic idea behind the proof of  (",
    "6 ) is to randomly generate a downlink channel codeword ( @xmath25 i.i.d .",
    "symbols @xmath50 ) for each and every jointly typical source tuple @xmath49 . upon observing a typical source tuple",
    ", the relay transmits the corresponding channel codeword .",
    "node  @xmath16 , upon observing the channel output @xmath51 , compiles a list of all those channel codewords that are jointly typical with @xmath51 .",
    "the codeword list corresponds to list of typical source sequences , with the same number of elements .",
    "we may think of the source list as a _ ( virtual ) random bin _ in the sense of the classic slepian - wolf theorem  @xcite .",
    "node  @xmath16 looks within this list for a unique source tuple that is jointly typical with its source @xmath52 ; this search will be successful with high probability whenever   holds .",
    "the above argument assumes that the entire source tuple is made available to the relay , which is not the case in the multi - way relay network .",
    "the key difficulty in proving theorem  [ thm : jscc ] is to overcome the fact that the relay only has partial knowledge of the source tuple .",
    "_ processing broadcast satellite : _ the source coding work of wyner _ et al .",
    "1 ) is a special case of theorem  [ thm : jscc ] .",
    "we now compare the general jsc coding architecture of section  [ sec : jscc ] to a separate source - channel coding architecture .",
    "the channel - coding problem of interest is analogous to the jsc - coding problem in fig .",
    "[ fig : jsccarc ] with one exception : the discrete memoryless source @xmath49 is replaced by @xmath48-independent random variables @xmath53 @xmath54 , where each @xmath55 is uniformly distributed on @xmath56 .",
    "a channel code with blocklength @xmath25 is a collection of maps : the encoder at node  @xmath16 ,    [ eqn : channelcode ] @xmath57 the encoder at the relay , @xmath58 and the decoder at node  @xmath16 , @xmath59    the channel code operates in a manner analogous to the jsc - code : node  @xmath16 sends @xmath60 and decodes @xmath61 the _ average joint error probability _ of a channel code is defined by @xmath62 .",
    "\\end{split}\\ ] ] the _ rate _ at which node  @xmath16 transmits the message @xmath55 is defined by @xmath63    a nonnegative rate tuple @xmath64 is said to be _ achievable _ if the following holds : for each @xmath37 there exists a channel code of the form   with @xmath38 and @xmath65 for some sufficiently large integer @xmath25 .    the _ capacity region _",
    "@xmath66 is the set of all achievable rates .",
    "we now give a single - letter expression for @xmath66 .",
    "let @xmath67 denote those nonnegative rate tuples @xmath64 for which    1 .",
    "the uplink channel capacities satisfy @xmath68 for all @xmath69 ; and 2 .",
    "there is a distribution for @xmath44 on @xmath22 such that @xmath70 holds for all @xmath69 .",
    "[ lem : channelcoding ] @xmath71 .",
    "the lemma can be proved in the same way as theorem  [ thm : jscc ] with @xmath72 @xmath73 replaced by @xmath53 @xmath54 and @xmath74 replaced by @xmath75 .",
    "we omit the technical details .",
    "-indices @xmath76 can be reliably transported over the network by a channel code.,scaledwidth=50.0% ]      the source - coding problem of interest is the multi - source multicast problem shown in fig .",
    "[ fig : sourcecodingarc ] .",
    "the problem is the source coding counterpart of the channel coding problem of section  [ sec : channelcoding ] in the following sense : a source code from this section combined with a channel code from the section  [ sec : channelcoding ] produces a ( separate source - channel ) code for the overall jsc problem .",
    "[ eqn : sourcecodedefinition ] a source code of length @xmath25 is a collection of @xmath77-maps : the compressor at node  @xmath16 , @xmath78 and the decompressor at node  @xmath16 , @xmath79    node  @xmath16 sends @xmath80 and decompresses @xmath81 the _ joint decoding error probability _ is defined in the same way as  , and the _ compression rate _ of node  @xmath16 is defined by @xmath82    a nonnegative rate tuple @xmath64 is said to be _ achievable _ if the following holds : for each @xmath37 there exists a source code of the form   with @xmath25 sufficiently large , @xmath38 and @xmath83 for all @xmath69 .    the _ source - coding rate region _",
    "@xmath84 is defined as the set of all achievable rate tuples .",
    "[ lem : sourcecoding ] @xmath84 is equal to the set of all nonnegative rate tuples @xmath85 @xmath86 for which @xmath87 holds for each nonempty and strict subset @xmath9 of @xmath18 .",
    "the lemma is a simple consequence of the slepian - wolf / cover theorem  @xcite .",
    "the details are omitted .",
    "reliable communication with separate source and channel codes is possible if the intersection of the interior of @xmath84 and the interior of @xmath66 is nonempty .",
    "[ thm : sscc ] reliable communication with separate source and channel codes is possible if there exists nonnegative rates @xmath64 such that    [ eqn : separation ] @xmath88 holds for each nonempty and strict subset @xmath9 of @xmath18 and @xmath89 holds for all @xmath16 in @xmath18 .",
    "the theorem is an immediate consequence of lemmas  [ lem : channelcoding ] and  [ lem : sourcecoding ] .",
    "separate source and channel coding is optimal for two nodes .",
    "specifically , the achievability assertion of theorem  [ thm : sscc ] is equivalent to that of theorem  [ thm : jscc ] : if   holds , then we can find @xmath90 simultaneously satisfying @xmath91 and @xmath92    the situation is more complicated for three or more nodes . indeed ,",
    "the achievability assertion of theorem  [ thm : sscc ] is more restrictive than that of theorem  [ thm : jscc ] in general ; in particular , it is not possible to simultaneously remove all redundancies in the sources for every node , and such redundancies can be exploited by the channel code .",
    "we describe such a situation in appendix  [ app : example ] .",
    "it should be noted that the capacity region @xmath66 is formulated with the requirement that each node reliably communicates a single message to every other node .",
    "a more general setup would permit the use of a _ private _ message from each node to each subset of nodes .",
    "characterising the resultant @xmath93-dimensional capacity region appears to be a formidable task ; in particular , the problem includes the setup of  @xcite as a special case .",
    "theorem  [ thm : sscc ] should therefore be understood as a sufficient condition for separate source and channel coding to be optimal .",
    "finally , it is interesting to juxtapose such difficulties to the relatively simple jsc coding scheme used to prove theorem  [ thm : jscc ] .",
    "we now consider the problem of designing practical , low complexity , codes that can approach those theoretical limits established for jsc coding in sections  [ sec : jscc ] and  [ sec : sscc ] .",
    "iterative error correction codes have been extensively investigated for jsc coding ; for example , see @xcite on joint turbo decoding and estimation of hidden markov sources , or @xcite on distributed jsc coding of video . in this section , we present a jsc coding scheme for the two - way relay network that is based on low - density parity - check ( ldpc ) codes  @xcite .",
    "in particular , we consider multi - edge ldpc codes  @xcite that are widely used in applications such as wiretap and multi - relay channels @xcite . our aim is to show how the individual codes for source and channel coding can be represented by a joint factor - graph @xcite , and we use this graph to provide an alternative view of the separation principle using multi - edge density evolution .    in the following ,",
    "we represent the channel coding message of node @xmath16 ( denoted by @xmath55 in section  [ sec : channelcoding ] ) using the binary notation @xmath94 .",
    "as there are only two nodes , we will denote the node that is not node @xmath16 as node @xmath95 .",
    "the encoder at node @xmath16 uses a linear source code to compress its source vector @xmath96 as @xmath97 here @xmath96 and @xmath94 are binary vectors of length @xmath25 and @xmath98 respectively , and @xmath99 is the parity - check matrix defining the source code of node @xmath16 .",
    "the compressed vector @xmath94 is mapped to a channel codeword @xmath100 , which is transmitted over the uplink channel .",
    "the uplink channel code is defined by a parity - check matrix @xmath101 into a codeword @xmath100 for a particular code @xmath101 see ( * ? ? ?",
    "the relay decodes @xmath94 from the noisy outputs @xmath102 of the @xmath16-th uplink channel",
    ". it then maps the concatenation of @xmath103 and @xmath104 to @xmath105 using a channel encoder defined by the parity - check matrix @xmath106 .",
    "each node uses a channel decoder to recover the other user s index @xmath107 using their own index @xmath94 and the noisy observation @xmath108 from the broadcast channel .",
    "finally , the source code is decoded separately resulting in an estimate of @xmath109 .",
    "this system is represented by the factor - graph in fig .",
    "[ fig : factorgraph ] .          in section  [ sec : separation ]",
    ", we showed that separate source - channel coding is asymptotically ( in blocklength ) optimal in the two - way relay network .",
    "we now provide an alternative view of this result using the factor - graph representation in fig .",
    "[ fig : factorgraph ] .",
    "let us first assume that the relay successfully decodes each bin index  the uplink code does not add to the discussion on separate versus joint decoding of the source and downlink codes .",
    "consider the graph shown in fig .",
    "[ fig : factorgraph ] depicting the source codes and downlink channel code ; these codes are connected via the bin indices .",
    "a separate source - channel decoder will apply the channel decoder to determine the other user s index @xmath107 from @xmath108 and then separately apply the source decoder to estimate @xmath109 from @xmath107 .",
    "a joint source - channel decoder will decode @xmath109 directly from @xmath108 by decoding the source and channel codes on the joint factor graph , and exchanging soft ( extrinsic ) information between the two parts of the factor graph .",
    "consider now the case when the source is compressed with a rate that equals the conditional entropy . in the channel coding setting this corresponds to a capacity achieving code .",
    "it has been shown in @xcite that the extrinsic information about the coded bits of any good ( capacity achieving ) code is zero above capacity .",
    "this implies that a joint decoder can not outperform a separate decoding scheme in such a setting since there is no extrinsic information available to the joint decoder .",
    "therefore , source - channel separation with separate decoding can be seen to be optimal from a factor graph perspective if capacity achieving source codes are applied .    in the following",
    "we will investigate source - channel separation in a practical setting by designing source and channel codes for the two - way relay channel and considering their performance using both density evolution and finite length simulations .",
    "the task of the source code is to map the message @xmath110 of length @xmath25 bits to a message @xmath111 of length @xmath98 bits such that the other node can reconstruct the message using its own message as side information .",
    "this is a slepian - wolf coding problem @xcite with bin index @xmath111 .",
    "an optimal slepian - wolf code can be realised by using the _ syndrome _ of a linear code , which is optimised for a particular symmetric dual channel  @xcite .",
    "the optimisation of the degree distribution of an ldpc code for a symmetric channel with uniform input is well studied  @xcite and we will design our source codes in this way .    for the downlink the relay has to communicate the messages @xmath103 and @xmath104 to both nodes simultaneously by broadcasting a codeword @xmath105 .",
    "our proposed coding scheme is as follows .",
    "first , the relay treats the concatenation of @xmath103 and @xmath104 as the systematic part of an ldpc code .",
    "it then uses an ldpc encoder to determine the message @xmath105 ( the `` parity '' bits ) , which it transmits over the broadcast channel . at each node",
    ", the channel decoder knows its own message @xmath94 and treats the message of the other node as being erased .    the overall structure of the parity - check matrix @xmath106 of our scheme",
    "is shown in fig .",
    "[ fig : bcmatrix ] .",
    "this matrix consists of a concatenation of the matrices @xmath112 , @xmath113 ( corresponding to @xmath103 , @xmath104 ) , and the matrix @xmath114 ( corresponding to @xmath105 ) .",
    "the codebook used by the relay is therefore defined as @xmath115 \\in \\{0,1\\}^{(r{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}}n+r{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}}n+n ) } \\\\      : [ { \\ensuremath{\\mathbf{b}}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2 mm } ) } } { \\ensuremath{\\mathbf{b}}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2 mm } ) } } { \\ensuremath{\\mathbf{x}}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0\\hspace{-0.2 mm } ) } } ] { { \\mathtt{h}}_\\text{c}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0\\hspace{-0.2mm})}}}^{\\mathrm{t } } = { \\ensuremath{\\mathbf{0 } } } \\big\\}.\\end{gathered}\\ ] ] the relay can determine its broadcast message @xmath105 by solving @xmath116 where @xmath117 denotes the element - wise addition over gf(2 ) .",
    "an efficient algorithm to determine @xmath105 is described in ( * ? ? ?",
    "a ) .    to allow successful encoding at the relay and decoding at the nodes :    * the square matrix @xmath114 has to be of full rank ( over gf(2 ) ) ( this enables the relay to determine the vector @xmath105 given the messages @xmath103 and @xmath104 ) , * the matrices @xmath112 and @xmath113 have to be free of stopping sets ( a set of variable nodes that , if erased , can not be resolved by a message - passing decoder ( * ? ? ?",
    "* sec . 3.22 ) ) .",
    "the first condition is obvious since the relay can not determine its broadcast message if the square matrix @xmath114 is rank deficient . to show the necessity of the second condition , assume that the broadcast channel is noiseless , i.e.",
    ", the nodes know @xmath105 without errors .",
    "the nodes now consider their own message @xmath94 as being known and solve for the other message @xmath107 that is treated as being erased .",
    "if this set of erased variables contains a stopping set , then the iterative decoder is unable to solve for all erasures and gets stuck in the largest stopping set .",
    "the absence of stopping sets can also be expressed by requiring that the sub - matrices @xmath112 and @xmath113 can be converted to triangular form using only row and column swaps .",
    "this corresponds to the encoding problem of ldpc codes as described in detail in ( * ? ? ?",
    "a ) which allows us to apply their results . in particular",
    ", we use ( * ? ? ?",
    "a.16 ) to design ldpc codes which can be converted to triangular form .",
    "an optimised code for the downlink phase has to be designed for both nodes simultaneously , i.e. , it has to perform close to the respective theoretical limit for both downlink channels .",
    "such a code can be analysed and optimised using multi - edge type density evolution ( * ? ? ?",
    "* sec . 7 ) .",
    "we apply multi - edge type density evolution to analyse the performance of the coding structure introduced in the previous section .",
    "we make the following assumptions for our numerical examples :    * @xmath118 is a doubly symmetric binary source with cross - over probability @xmath119 , i.e. , @xmath120 the conditional entropy is therefore @xmath121 where @xmath122 denotes the binary entropy function .",
    "* the ( orthogonal ) uplink channels are binary input additive white gaussian noise ( biawgn ) channels with noise variance @xmath123 each . *",
    "the downlink channels are modelled as biawgn channels with noise variance @xmath124 each .",
    "furthermore assume that the noise variance @xmath123 of the uplink channel is fixed and the nodes can communicate their messages reliably to the relay .",
    "for the downlink we are interested in the pairs @xmath125 which define the achievable region @xmath126 where @xmath37 is an arbitrarily small constant and @xmath127 denotes the bit error probability of the estimate @xmath128 after @xmath129 iterations of message passing decoding , i.e. , @xmath130 consists of all pairs of source correlation @xmath131 and noise variance @xmath124 on the downlink channel where the iterative decoder converges to an arbitrarily small error probability for sufficiently large block length .",
    "since we assume that the uplink channels can be decoded by the relay , we focus on the decoding problem at the nodes where we have the constraint : [ equ : con ] @xmath132 where @xmath133 denotes the capacity of a biawgn channel with noise variance @xmath134 .    [ cols=\"^,^,^,^,^,^,^,^,^,^ \" , ]      the decoder at node @xmath16 first decodes the other node s bin index @xmath107 . after performing a hard decision on the bin index ,",
    "the node decodes the slepian - wolf code @xmath135 using the other node s bin index @xmath107 and its own message @xmath96 as side information .",
    "in this example we optimised source and channel codes for two cases",
    "@xmath136 the resulting source and channel codes are given in table  [ table : ldpc_degrees ] . in fig .",
    "[ fig : regionsep ] we have marked the optimised points for both rates ( by a cross and a square respectively ) .",
    "we can see that the ( density evolution ) performance of these optimised codes is close to the theoretical limits .",
    "note that due to practical constraints , for example we considered only ldpc codes with a maximum node degree of @xmath137 , these codes are capacity approaching rather than capacity achieving and so their performance is still bounded away from capacity , as shown by the small gap to the ( 1/2,1/2 ) and ( 1/4,1/4 ) points respectively .",
    "in this example we consider the codes from example 1 when the capacity of the downlink channel is higher and/or when the conditional entropy of the source is lower than the optimised values . in the case of separate source and channel coding",
    "this gives the rectangular regions shown in fig .",
    "[ fig : regionsep ] .",
    "we next consider the same source and channel codes we used in the rate-1/2 case in example 1 , but now apply a joint decoder .",
    "this is achieved by applying the sum - product algorithm @xcite to the joint factor graph consisting of the downlink code and the slepian - wolf source code , i.e. , the graph shown in fig .",
    "[ fig : factorgraph ] assuming error - free uplink channels .",
    "the achievable regions for this joint decoder are shown in fig .",
    "[ fig : regionjoint ] and compared to the separate decoder ( solid and dashed lines , respectively ) .",
    "when the conditional entropy of the source is less than the value for which the system was designed , the source code is of course no longer optimal .",
    "a separate decoder ignores the source when decoding the downlink channel code and therefore it can not exploit any remaining redundancy after decoding the source .",
    "however , a decoder that _ jointly decodes _ the downlink channel code and the source code can exploit this remaining redundancy .",
    "the achievable region of the joint decoder is thus larger than that of the separate decoder due to its improved performance when the conditional entropy of the source is less than 1/2 at the same time that the capacity of the downlink channel is less than 1/2 .",
    "the joint source - channel decoder can therefore be seen to be more robust to variations of the source and channel away from the design entropy and rate .",
    "so while separate decoding is optimal for the particular design rate / entropy ( here @xmath138,@xmath138 ) , joint decoding is more robust when the source and channel vary .",
    "indeed , a close observation of fig .",
    "[ fig : regionjoint ] reveals that in this particular example the joint decoding scheme can decode successfully at lower downlink capacities than the separate scheme even for the ( @xmath138,@xmath138 ) point where the separate scheme is optimised .",
    "this is because the designed source code is not capacity achieving ( see the comment in example 1 ) so it is not compressing at the theoretical limit and there is still some remaining redundancy to be exploited by the joint decoder .      in the previous example we presented the achievable region of a joint decoder but we used the same codes as in example 1 , i.e. , codes that have been optimised for separate decoding of a specific source .",
    "now we use the same source code as above but instead of optimising the channel code for the downlink for one particular source we optimise it for a range of sources . in particular , we chose to minimise the area between the achievable region and the theoretical limit over the entire range of source entropies .",
    "the results of such an optimisation process are shown in fig .",
    "[ fig : regionjoint ] ( dash - dotted line ) .",
    "we observe that such an optimised scheme performs close to the limit over a wide range .",
    "however , this comes with a loss ( at the rate 1/2 point ) compared to a coding scheme which is optimised for that particular setting .",
    "in addition to the asymptotic results of the previous examples we present finite length results . for this purpose",
    "we consider the codes of examples 1 and 2 , i.e. , codes of rate @xmath138 which have been optimised for a separate decoder .",
    "finite length codes have been constructed for source blocks of length @xmath139 using the progressive edge growth ( peg ) algorithm @xcite . for the simulations shown in fig .",
    "[ fig : wer ] we fixed the cross - over probability @xmath131 of the source and varied the signal - to - noise ratio ( @xmath140 ) of the downlink channel ( on the horizontal axis ) .",
    "we repeat this process for three separate sources ( @xmath141 and @xmath142 ) .",
    "results are shown for a separate and joint decoder ( solid and dashed lines , respectively ) .",
    "first , consider the case where @xmath143 .",
    "this cross - over probability is close to the threshold of the source code ( @xmath144 ) which leads to a high probability of error of the finite length source code ( at approximately @xmath145 ) .",
    "these errors are independent of whether a separate or joint decoder is used and are because the finite length source code is far from capacity achieving . however , since the source cross - over probability is still slightly below the threshold , a joint decoder can exploit the remaining redundancy and can decode at a slightly lower @xmath140 for the downlink channel at bit error rates above this error floor .     and @xmath146 in terms of @xmath147 and @xmath148 for rates @xmath138 ( dashed line ) and rates @xmath149 ( solid line ) .",
    "the dotted line represents the theoretical limit .",
    "the uplink from the nodes to the relay is assumed to be error - free . ]     and @xmath146 in terms of @xmath147 and @xmath148 for rates @xmath138 .",
    "the dashed line corresponds to a separate decoder ( example 1 ) and the solid line corresponds to a joint decoder operating on the code designed for a separate scheme ( example 2 ) .",
    "the dash - dotted line represents a joint decoder where the channel code is optimised to perform well over a wide range of sources ( example 3 ) . ]    the error floor caused by the source code is decreased when the sources have a smaller cross - over probability ( i.e. the sources have a greater correlation ) .",
    "for example , lowering the cross - over probability to @xmath150 or @xmath151 shown in fig .",
    "[ fig : wer ] leads to an error probability of the source code which is below our simulation range . in these cases",
    "a joint decoder will exploit the remaining redundancy and is able to decode at a significantly lower @xmath140 for the downlink channel than the separate decoder .",
    "the channel coding part of the separate decoder can not benefit from a lower @xmath131 and so has the same downlink channel performance in all three cases .",
    "this leads to the conclusion that , while source / channel separation is optimal in an asymptotic setting of infinite block length , any practical finite length system will benefit from a joint decoder .",
    "in the usual way , we split the proof of theorem  [ thm : jscc ] into two parts : the _ achievability _ assertion and the _ converse _ assertion .",
    "the main elements of the proof are best understood for the special case where the uplink channels are noiseless . extending the proof to the more general case claimed in theorem",
    "[ thm : jscc ] is straightforward .",
    "the ( noiseless ) uplink random - coding argument uses a slepian - wolf / cover distributed source code , e.g.  ( * ? ? ?",
    "the source sequence at each node is compressed to a _",
    "index that is sent to the relay over the noiseless uplink channel .",
    "the @xmath48 compression rates of the source codes are selected to satisfy all but one inequality in the slepian - wolf theorem  ( * ? ? ?",
    "the exception being the total sum rate inequality , e.g.   is omitted .",
    "the relay has direct access to the @xmath48 bin indices ; it does _ not _ attempt to decode the individual source sequences .",
    "the ( noisy ) downlink random coding argument combines the virtual binning idea of tuncel  @xcite with the cascaded slepian - wolf binning idea of wyner _",
    "et al . _",
    "each node will use a jsc - decoder to recover the source sequences .",
    "we use _ typical sequences _ as , for example , defined in  ( * ? ? ?",
    "* sec .  1 ) and  ( * ? ? ? * chap .  2 ) .",
    "suppose that @xmath152 are random variables on a discrete product space @xmath153 with joint distribution @xmath154 .",
    "let @xmath155 and @xmath156 denote the @xmath25-fold cartesian products of @xmath157 and @xmath158 respectively .",
    "the _ type _ of @xmath159 in @xmath155 and the _ joint type _ of @xmath160 in @xmath161 are the empirical distributions respectively defined by @xmath162 and @xmath163 fix @xmath164 .",
    "the @xmath165-_typical _ , @xmath165-_jointly - typical _ and @xmath165-_conditionally - typical _ sets are respectively defined by @xmath166 @xmath167 and @xmath168 where @xmath169 denotes the @xmath170-marginal of @xmath154 .",
    "we note that if @xmath160 is in @xmath171 , then @xmath159 is in @xmath172 and @xmath173 is in @xmath174 .",
    "the next lemma will be used throughout the proof .",
    "[ lem : condtionallytypical ] fix @xmath175 if @xmath159 belongs to @xmath176 , then the cardinality of the conditionally typical set @xmath177 satisfies @xmath178 moreover , the probability that @xmath179 ( drawn i.i.d . with the @xmath180-marginal of @xmath154 ) belongs to the conditionally - typical set satisfies",
    "@xmath181 \\leq 2^{-n(i(a;b ) - 2 \\delta_2 h(b))}.\\ ] ]      consider node @xmath16 .",
    "randomly partition the source space @xmath182 into @xmath183 bins , labelled as @xmath184 , using an i.i.d .",
    "uniform law ; i.e. , @xmath185 = \\frac{1}{2^{n r{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2 mm } ) } } } } , \\quad b{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2 mm } ) } } \\in \\big\\ { 1,2,\\ldots,2^{nr{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}}}\\big\\}.\\ ] ] the allowable values of @xmath186 will be specified later . with a slight abuse of notation ,",
    "let @xmath187 denote the map from source sequences to bin indices .",
    "node  @xmath16 observes a sequence @xmath188 from @xmath182 , and it sends the corresponding bin index @xmath189 to the relay .",
    "consider the cartesian product set of bin indices , @xmath190 for each bin tuple @xmath191 in @xmath156 , generate a downlink codeword @xmath192 by randomly drawing @xmath25-symbols from @xmath22 using the marginal distribution of @xmath44 .",
    "the relay observes a bin tuple @xmath193 from @xmath156 on the uplink , and it sends @xmath194 over the downlink broadcast channel",
    ".    @xmath195      let @xmath196 denote the joint distribution of the source @xmath197 , and let @xmath198 fix @xmath199 , where @xmath200 is the smallest value in the support sets of @xmath196 and the joint distribution @xmath201 .",
    "the decoding procedure used at each node is identical ; we describe the procedure for node  1 .",
    "the node observes a source sequence @xmath202 from @xmath203 , it computes the corresponding bin index @xmath204 , and it observes a channel output @xmath205 from @xmath206 .",
    "the node compiles a list of bin tuples that contain unique source sequences @xmath165-jointly typical with @xmath202 ; to this end , for each @xmath202 in @xmath203 let @xmath207 be defined as in  .",
    "in addition , the node looks for a _ unique _",
    "tuple @xmath208 @xmath209 in @xmath207 such that the corresponding broadcast channel codeword is @xmath165-jointly typical with the observed channel output , i.e. , @xmath210 if successful , node  1 sets @xmath211 equal to @xmath212 , where @xmath213 @xmath214 is the unique vector identified by @xmath207 .",
    "if unsuccessful , the node declares an error .",
    "_ remarks : _    * _ noisy uplink : _ at the end of the proof , we will adapt the uplink code for noiseless channels to include noisy channels by simply adding a good point - to - point channel code for each orthogonal uplink . * _ separation : _ each node uses standalone source ( and , later , channel ) encoders on the uplink .",
    "similarly , the relay uses standalone channel decoders on the uplink and a standalone channel encoder on the downlink .",
    "however , the decoder at each node is a true jsc - decoder ; for example , node  @xmath215 first exploits its source side information @xmath202 to compile the list @xmath207 , before decoding the transmitted codeword from @xmath207 using @xmath205",
    ". it can be suboptimal to decode the transmitted codeword @xmath216 using only the channel output @xmath205 , which , for example , is the case in separate source - channel coding .",
    "we wish to upper bound the average joint error probability @xmath217 , as defined in  , for the described ensemble of codes . by the union bound for probability , we have @xmath218.\\end{gathered}\\ ] ]",
    "consider the first error event of the sum  : @xmath219 i.e. , the event that node  1 decodes one or more source sequences in error .",
    "we now given an upper bound for @xmath220 $ ] .",
    "fix @xmath221 arbitrarily .",
    "consider the following events .    1 .",
    "the event that the source sequences are not @xmath222-jointly typical : @xmath223 2 .   for each nonempty subset @xmath224 of @xmath225 , define the following event : the source sequences are @xmath222-jointly typical and there exists an @xmath226-tuple of @xmath165-conditionally typical sequences , say @xmath227 in the same bins as the source : @xmath228 3 .",
    "the event that the broadcast channel input and output at node  1 are not @xmath222-jointly typical : @xmath229 4 .",
    "the source sequences are @xmath222-jointly typical , the channel input and output are @xmath222-jointly typical , and there exists another @xmath165-conditionally typical codeword with bin indices in @xmath230 : @xmath231    the error event @xmath232  the event that node  1 decodes a source sequence in error  is a subset of the union of @xmath233 , @xmath234 , @xmath235 and @xmath236 ; hence , @xmath237",
    "\\leq \\mathbb{p}\\big[{\\ensuremath{\\mathcal{e}}}_1\\big ] + \\sum_{{\\ensuremath{\\mathcal{l } } } \\subseteq \\{2,3,\\ldots , l\\ } }   \\mathbb{p}\\big[{\\ensuremath{\\mathcal{e}}}_{2,{\\ensuremath{\\mathcal{l } } } } \\big ] + \\mathbb{p}\\big[{\\ensuremath{\\mathcal{e}}}_3\\big ] + \\mathbb{p}\\big[{\\ensuremath{\\mathcal{e}}}_4\\big].\\ ] ] it follows from the _ law of large numbers _ that @xmath238 = 0\\quad \\text { and } \\quad \\lim_{n\\rightarrow\\infty } \\mathbb{p}\\big[{\\ensuremath{\\mathcal{e}}}_3\\big ] = 0\\ ] ] e.g. , see  ( * ? ? ?",
    "2 ) or  ( * ? ? ?",
    "@xmath239 & \\stackrel{}{= } \\sum_{(\\alpha ) } { \\boldsymbol{q}}_{\\text{s}}({\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2 mm } ) } } ) { \\mathbbm{1}{\\left\\ { { ( { \\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2 mm } ) } } ) \\in { \\ensuremath{\\mathcal{t}}}_{\\delta'}(w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots , w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2 mm } ) } } ) } \\right\\ } } } \\\\ \\notag & \\hspace{75 mm } \\cdot \\sum_{(\\beta ) } \\mathbb{p}\\big [ \\phi{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}}({\\boldsymbol{\\tilde{w}}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2 mm } ) } } ) = \\phi{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}}({\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}})\\ \\forall l \\in { \\ensuremath{\\mathcal{l } } } \\big]\\\\ \\notag & \\stackrel{}{= } \\sum_{(\\gamma ) } { \\boldsymbol{q}}_{\\text{s}}({\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2 mm } ) } } ) \\sum_{(\\beta ) } \\mathbb{p}\\big [ \\phi{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}}({\\boldsymbol{\\tilde{w}}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2 mm } ) } } ) = \\phi{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}}({\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}})\\ \\forall l \\in { \\ensuremath{\\mathcal{l } } } \\big]\\\\ \\notag & { \\stackrel{(\\text{a})}{= } } \\sum_{(\\gamma ) } { \\boldsymbol{q}}_{\\text{s}}({\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2 mm } ) } } ) \\sum_{(\\beta ) } 2^{-n \\sum_{l \\in { \\ensuremath{\\mathcal{l } } } } r{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}}}\\\\ \\notag & { \\stackrel{(\\text{b})}{\\leq } } \\sum_{(\\gamma ) } { \\boldsymbol{q}}_{\\text{s}}({\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}})\\ 2^{n h(w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}{\\ensuremath{\\mathcal{l}}}\\hspace{-0.2mm})}}|w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}{\\ensuremath{\\mathcal{l}}}^c\\hspace{-0.2mm})}})(1+\\delta)}\\ 2^{-n \\sum_{l \\in { \\ensuremath{\\mathcal{l } } } } r{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}}}\\\\ \\label{eqn : bound2el } & \\leq 2^{n h(w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}{\\ensuremath{\\mathcal{l}}}\\hspace{-0.2mm})}}|w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}{\\ensuremath{\\mathcal{l}}}^c\\hspace{-0.2mm})}})(1+\\delta ) } \\ 2^{-n \\sum_{l \\in { \\ensuremath{\\mathcal{l } } } } r{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}}},\\end{aligned}\\ ] ]    the probability of each @xmath240 can be upper bound as shown in  , where    * @xmath241 denotes the indicator function . *",
    "the sum marked with @xmath242 is taken over all @xmath243 * the sums marked with @xmath244 are taken over all @xmath245 * the sums marked with @xmath246 are take over all @xmath247 * equality ( a ) follows because the probability that each sequence @xmath248 in @xmath182 is randomly assigned to the same bin as @xmath188 is independent of all other bin assignments and equal to @xmath249 .",
    "* inequality ( b ) bounds the cardinality of of the conditionally typical set @xmath250 using lemma  [ lem : condtionallytypical ] .",
    "finally , we notice that if @xmath251 then @xmath252 = 0.\\ ] ]    @xmath253 & { \\stackrel{(\\text{a})}{= } } \\sum_{(\\alpha ) } { \\boldsymbol{q}}_\\text{s}({\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2 mm } ) } } ) { \\mathbbm{1}{\\left\\ { { ( { \\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2 mm } ) } } ) \\in { \\ensuremath{\\mathcal{t}}}_{\\delta'}(w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots , w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2 mm } ) } } ) } \\right\\ } } } \\\\ \\notag & \\hspace{10 mm } \\cdot \\sum_{{\\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2 mm } ) } } \\in { \\boldsymbol{{\\ensuremath{\\mathcal{y}}}}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2 mm } ) } } } \\mathbb{p}\\big[{\\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}}={\\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}}\\big|({\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2 mm } ) } } ) = ( { \\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}})\\big]\\\\ \\notag & \\hspace{40 mm }   \\cdot { \\mathbbm{1}{\\left\\ { { \\big({\\boldsymbol{x}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0\\hspace{-0.2mm})}}(b{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},\\ldots , b{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}}),{\\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}}\\big ) \\in { \\ensuremath{\\mathcal{t}}}_{\\delta'}(x{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0\\hspace{-0.2mm})}},y{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2 mm } ) } } ) } \\right\\ } } } \\\\ \\notag & \\hspace{59 mm } \\cdot \\sum_{(\\beta ) } \\mathbb{p}\\big[{\\boldsymbol{x}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0\\hspace{-0.2mm})}}(b{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},\\tilde{b}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,\\tilde{b}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2 mm } ) } } ) \\in { \\ensuremath{\\mathcal{t}}}_\\delta(x{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0\\hspace{-0.2mm})}},y{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}}|{\\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}})\\big]\\\\ \\notag & { \\stackrel{(\\text{b})}{\\leq } } \\sum_{(\\gamma ) } { \\boldsymbol{q}}_\\text{s}({\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}})\\\\ \\notag & \\hspace{10mm}\\cdot \\sum_{{\\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2 mm } ) } } \\in { \\ensuremath{\\mathcal{t}}}_{\\delta'}(y{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2 mm } ) } } ) } \\mathbb{p}\\big[{\\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}}={\\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}}\\big|({\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2 mm } ) } } ) = ( { \\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}})\\big ] \\\\",
    "\\notag & \\hspace{59 mm } \\cdot \\sum_{(\\beta ) } \\mathbb{p}\\big[{\\boldsymbol{x}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0\\hspace{-0.2mm})}}(b{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},\\tilde{b}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,\\tilde{b}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2 mm } ) } } ) \\in { \\ensuremath{\\mathcal{t}}}_\\delta(x{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0\\hspace{-0.2mm})}},y{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}}|{\\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}})\\big]\\\\ \\notag & { \\stackrel{(\\text{c})}{\\leq } } \\sum_{(\\gamma ) } { \\boldsymbol{q}}_\\text{s}({\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}})\\\\ \\notag & \\hspace{10 mm } \\sum_{{\\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2 mm } ) } } \\in { \\ensuremath{\\mathcal{t}}}_{\\delta'}(y{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2 mm } ) } } ) } \\mathbb{p}\\big[{\\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2 mm } ) } } = { \\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2 mm } ) } } \\big| ( { \\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}})=({\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}})\\big ]   \\\\ \\notag & \\hspace{80 mm } \\cdot |{\\ensuremath{\\mathcal{q}}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}}({\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}})|\\ 2^{-n(i(x{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0\\hspace{-0.2mm})}};y{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}})-2\\delta h(x{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0\\hspace{-0.2mm})}}))}\\\\ \\notag \\\\ \\notag & { \\stackrel{(\\text{d})}{\\leq } } \\sum_{(\\gamma ) } { \\boldsymbol{q}}_\\text{s}({\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}})\\\\ \\notag & \\hspace{10 mm } \\sum_{{\\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2 mm } ) } } \\in { \\ensuremath{\\mathcal{t}}}_{\\delta'}(y{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2 mm } ) } } ) } \\mathbb{p}\\big[{\\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}}={\\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}}\\big|({\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2 mm } ) } } ) = ( { \\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}},{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},\\ldots,{\\boldsymbol{w}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}})\\big]\\\\ \\notag & \\hspace{50 mm } \\cdot 2^{n h(w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}3\\hspace{-0.2mm})}},\\ldots , w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}}|w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}})(1 + \\delta)}\\ 2^{-n(i(x{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0\\hspace{-0.2mm})}};y{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}})-2\\delta h(x{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0\\hspace{-0.2mm})}}))}\\\\ \\label{eqn : bounde4l } & \\leq 2^{n h(w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}2\\hspace{-0.2mm})}},w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}3\\hspace{-0.2mm})}},\\ldots , w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}}|w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}})(1 + \\delta)}\\ 2^{-n(i(x{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0\\hspace{-0.2mm})}};y{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}1\\hspace{-0.2mm})}})-2\\delta h(x{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0\\hspace{-0.2mm})}}))},\\end{aligned}\\ ] ]    the probability of @xmath236 can be upper bound as shown in  .",
    "* the sum marked with @xmath242 is taken over all @xmath243 * the sums marked with @xmath244 are taken over all @xmath254 * the sums marked with @xmath246 are taken over all @xmath255    the reasoning behind ( in)equalities ( a ) through ( d ) is as follows .    * we let @xmath256 for @xmath69 , denote the bin index of the @xmath16-th source sequence . *",
    "we have that @xmath257 implies @xmath258 * we bound the probability that an alternate codeword , @xmath259 , is @xmath165-jointly typical with @xmath205 , using lemma  [ lem : condtionallytypical ] and the fact that the @xmath25-symbols of the codeword are drawn i.i.d . with the marginal of @xmath44 .",
    "* the cardinality of @xmath207 must be smaller than the cardinality of the conditionally typical set @xmath260 @xmath261 , which in turn is smaller than the bound of lemma  [ lem : condtionallytypical ] .",
    "finally , it follows that @xmath262 = 0\\ ] ] whenever @xmath263    the above analysis can be repeated for each of the @xmath48 nodes to obtain constraints analogous to   and  .",
    "we are free to choose @xmath222 and @xmath165 arbitrarily small , so @xmath217 ( averaged over the ensemble of codes ) can be made arbitrarily small by increasing @xmath25 if @xmath264 holds for each nonempty and strict subset @xmath9 of @xmath18 and @xmath265 holds for each @xmath69 .",
    "the random coding argument implies that there must exist at least one code from the ensemble with an error probability at least as small as @xmath217 .",
    "consider the setup of theorem  [ thm : jscc ] with noisy ( orthogonal ) uplink channels .",
    "the random - coding argument of sections  [ sec : randomcoding - uplink ] to  [ sec : randomcoding - decoding ] can be extended to this setting by the use of a separate source and channel code architecture on the uplink .",
    "that is , use a ( point - to - point ) capacity - approaching code for each orthogonal uplink channel , choose the source - code compression rates to match the channel coding rates ( @xmath266 , where @xmath267 is sufficiently small ) , and communicate the @xmath48 bin indices to the relay using the point - to - point channel codes .",
    "suppose that we have a jsc - code with @xmath38 .",
    "the next two lemmas will be useful .",
    "[ lem : converse - fano ] for each nonempty and strict subset @xmath9 of @xmath18 , we have @xmath269 where @xmath270 as @xmath271    choose @xmath16 from @xmath272 arbitrarily .",
    "we have @xmath273\\\\ & \\leq \\frac{l\\ h(\\epsilon)}{n }    + \\epsilon l \\max_{l ' \\in \\{1,\\ldots , l\\ } } |{\\ensuremath{\\mathcal{w}}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l'\\hspace{-0.2mm})}}|,\\end{aligned}\\ ] ] where @xmath274 is the binary entropy function and ( a ) follows from fano s inequality .",
    "[ lem : converse - orthogonal - mac ] for each nonempty and strict subset @xmath9 of @xmath275 @xmath276 , we have @xmath277    we have @xmath278\\\\ & \\geq h({\\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0,{\\ensuremath{\\mathcal{l}}}\\hspace{-0.2 mm } ) } } ) - \\sum_{l \\in { \\ensuremath{\\mathcal{l } } } } h({\\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0,l\\hspace{-0.2 mm } ) } } | { \\boldsymbol{x}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}l\\hspace{-0.2mm})}})\\\\ & { \\stackrel{(\\text{a})}{\\geq } } h({\\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0,{\\ensuremath{\\mathcal{l}}}\\hspace{-0.2 mm } ) } } ) - h({\\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0,{\\ensuremath{\\mathcal{l}}}\\hspace{-0.2mm})}}|{\\boldsymbol{x}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}{\\ensuremath{\\mathcal{l}}}\\hspace{-0.2mm})}})\\\\ & = i({\\boldsymbol{x}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}{\\ensuremath{\\mathcal{l}}}\\hspace{-0.2 mm } ) } } ; { \\boldsymbol{y}}{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}0,{\\ensuremath{\\mathcal{l}}}\\hspace{-0.2mm})}}),\\end{aligned}\\ ] ] where ( a ) is a consequence of the markov chain @xmath279    for each nonempty and strict subset @xmath9 of @xmath18 , we can lower bound the right hand side of   by @xmath280\\\\ \\label{eqn : converse1 } & \\stackrel{(e)}{\\geq } h(w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}{\\ensuremath{\\mathcal{l}}}\\hspace{-0.2 mm } ) } } | w{^{\\hspace{-0.2mm}\\scriptscriptstyle ( \\hspace{-0.2mm}{\\ensuremath{\\mathcal{l}}}^c\\hspace{-0.2 mm } ) } } ) - \\varepsilon(n , \\epsilon),\\end{aligned}\\ ] ] where ( a ) follows from the definition of channel capacity ; ( b ) follows from lemma  [ lem : converse - orthogonal - mac ] ; ( c ) follows from the markov chain @xmath281 ; ( d ) follows from the markov chain @xmath282 ; and ( e ) follows from lemma  [ lem : converse - fano ] and that the source is i.i.d .",
    "consider  .",
    "let @xmath283 denote the pmf of the @xmath284-th symbol , @xmath285 , of @xmath286 .",
    "define a new time - averaged random variable @xmath287 on @xmath22 via the pmf @xmath288 for each @xmath16 in @xmath18 , we have @xmath289 where ( a ) follows from jensen s inequality and the fact that @xmath290 is concave in @xmath291 ; ( b ) follows because the broadcast channel is stationary and memoryless ; ( c ) follows from the markov chain @xmath292 ; and ( d ) follows from lemma  [ lem : converse - fano ] .",
    "consider a sequence @xmath293 .",
    "for each @xmath294 in this sequence , we have by definition at jsc - code for which   and   hold for some @xmath44 on @xmath22 . the proof is completed by noting that the sequence of pmfs @xmath295 will converge to some pmf on @xmath22 .",
    "we now describe a situation where the achievability assertion of theorem  [ thm : jscc ] holds , but that of theorem  [ thm : sscc ] fails ; equivalently , reliable communication is achievable with jsc - codes of the form  , and it is not achievable with separate source and channel codes of the form   and  .",
    "suppose we have @xmath296-nodes with the following sources : @xmath297 where each @xmath298 is an independent random variable , for all @xmath299 , and @xmath117 denotes the xor function .",
    "we choose @xmath300 , @xmath301 , @xmath302 , and @xmath303 .",
    "for these choices of probability mass functions , we have        suppose the uplink channels are point - to - point channels with capacities @xmath305 @xmath306 @xmath307 @xmath306 @xmath308 @xmath306 @xmath309 , and the downlink channels are @xmath310 , for each @xmath311 , where each @xmath312 , and each @xmath313 is an independent random variable .",
    "we choose @xmath314 and @xmath315 .",
    "note that the uniform input distribution simultaneously maximises @xmath316 for all @xmath311 , giving @xmath317 and @xmath318 .",
    "these assumptions satisfy the achievability requirements of theorem  [ thm : jscc ] .               defined in  .",
    "each of the three circles represents a source message , and the intersection between two circles represent the mutual information between the sources .",
    "the numbers are the values of the corresponding conditional entropies and mutual informations.,width=151 ]                            r.  ahlswede and t.  han , `` on source coding with side information via a multiple - access channel and related problems in multi - user information theory , '' _ ieee transactions on information theory _ , vol .",
    "29 , no .  3 , pp .",
    "396412 , 1983 .",
    "t.  oechtering , m.  wigger , and r.  timo , `` broadcast capacity regions with three receivers and message cognition , '' in _ in proceedings ieee international symposium on information theory _ , cambridge , ma , 2012 .",
    "v.  rathi , m.  andersson , r.  thobaben , j.  kliewer , and m.  skoglund , `` two edge type ldpc codes for the wiretap channel , '' in _ in proceedings asilomar conference on signals , systems and computers _",
    ", pacific grove , ca , 2009 .",
    "m.  h. azmi , y.  jinhong , g.  lechner , and l.  k. rasmussen , `` design of multi - edge - type bilayer - expurgated ldpc codes for decode - and - forward in relay channels , '' _ ieee transactions on communications _ , vol .",
    "59 , no .  11 , pp . 29933006 , 2011 .",
    "m.  peleg , a.  sanderovich , and s.  shamai , `` on extrinsic information of good codes operating over memoryless channels with incremental noisiness , '' in _ ieee conv .",
    "electrical electronics engineers israel _ , 2006 .",
    "c.  chen , d .- k .",
    "he , and a.  jagmohan , `` the equivalence between slepian - wolf coding and channel coding under density evolution , '' _ ieee transactions on communications _ ,",
    "57 , no .",
    "9 , pp . 25342540 , 2009 .",
    "roy timo ( s06-m09 ) is a research fellow with the institute for telecommunications research at the university of south australia .",
    "dr . timo received the bachelor of engineering ( hons . ) and ph.d .",
    "degrees from the australian national university in july 2005 and december 2009 respectively ; he was a nicta - enhanced ph.d .",
    "candidate at nicta s canberra research laboratory .",
    "he held a visiting postdoctoral research associate position with the department of electrical engineering at princeton university in 2011 and 2012 .",
    "he is a member of ieee and the ieee information theory society .",
    "timo s research interests primarily lie within the fields of information theory and ergodic theory ; in particular , the shannon limits of source coding , channel coding and joint source - channel coding in networks .",
    "gottfried lechner ( s03 , m08 ) was born on august 1 , 1975 in vienna , austria .",
    "he received his dipl .- ing . and",
    "degrees from the vienna university of technology ( vienna , austria ) in 2003 and 2007 , respectively . from 2002 to 2008 , he was a researcher at the telecommunications research centre vienna ( ftw ) in the area of signal- and information processing . since 2008",
    "he is a research fellow at the institute for telecommunications research ( itr ) at the university of south australia ( adelaide , australia ) .",
    "he is a member of the ieee and a member of the ieee information theory and communications societies .",
    "his research interests include sparse graph codes , iterative techniques , source- and channel coding , cooperative communications , and optical communications .",
    "lawrence ong ( s05m10 ) received the beng ( 1st hons ) degree in electrical engineering from the national university of singapore ( nus ) , singapore , in 2001 .",
    "he subsequently received the mphil degree from the university of cambridge , uk , in 2004 and the phd degree from nus in 2008 .",
    "he was with mobileone , singapore , as a system engineer from 2001 to 2002 .",
    "he was a research fellow at nus , from 2007 to 2008 . from 2008 to 2012",
    ", he was a postdoctoral researcher at the university of newcastle , australia . in 2012 , he was awarded the discovery early career researcher award ( decra ) by the australian research council ( arc ) .",
    "he is currently a decra fellow at the university of newcastle .",
    "sarah johnson received the b.e .",
    "( hons ) degree in electrical engineering in 2000 , and phd in 2004 , both from the university of newcastle , australia .",
    "she then held a postdoctoral position with the wireless signal processing program , national ict australia before returning to the university of newcastle where she is now an australian research council future fellow .",
    "sarah s research interests are in the field of error correction and information theory , and in particular the area of codes for iterative decoding .",
    "she is the author of a book on iterative error correction published by cambridge university press ."
  ],
  "abstract_text": [
    "<S> we consider a multi - way relay network with an orthogonal uplink and correlated sources , and we characterise reliable communication ( in the usual shannon sense ) with a single - letter expression . </S>",
    "<S> the characterisation is obtained using a joint source - channel random - coding argument , which is based on a combination of wyner _ </S>",
    "<S> et al.s_ _ cascaded slepian - wolf source coding _ and tuncel s _ slepian - wolf coding over broadcast channels . </S>",
    "<S> _ we prove a separation theorem for the special case of two nodes ; that is , we show that a modular code architecture with separate source and channel coding functions is ( asymptotically ) optimal . </S>",
    "<S> finally , we propose a practical coding scheme based on low - density parity - check codes , and we analyse its performance using multi - edge density evolution . </S>"
  ]
}