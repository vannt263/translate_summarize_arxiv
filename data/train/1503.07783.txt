{
  "article_text": [
    "in the last years the computer vision research community s attention has been driven towards the existence of differences across predefined image datasets and the necessity to recompose these idiosyncrasies .",
    "the main reason behind this need is the increasing amount of available image data sources and the absence of a unique general learning method that can perform well across all of them . in practice training a classifier on a dataset ( e.g. flicker photos ) and testing on another ( e.g. images captured with a mobile phone ) produces very poor results although the task ( i.e. the set of depicted object categories ) is the same .    in this context the notion of _ domain _ already used in machine learning for speech and",
    "language processing has been extended to visual problems . a source domain ( @xmath0 )",
    "usually contains a large amount of labeled images , while a target domain ( @xmath1 ) refers broadly to a dataset that is assumed to have different characteristics from the source , and few or no labeled samples .",
    "formally we can say that two domains differ when for their probability distributions it holds @xmath2 , where @xmath3 indicates the generic image sample and @xmath4 the corresponding class label .",
    "specific annotator tendencies may influence the conditional distributions implying @xmath5 .",
    "other typical causes of visual domain shift include changing in the acquisition device , image resolution , lighting , background , viewpoint and post - processing @xcite .",
    "most of these information are directly encoded in the descriptor space @xmath6 chosen to represent the images and may induce a difference among the marginal distributions @xmath7 .    in 2013 ,",
    "tommasi and caputo showed that by casting the domain adaptation problem into the naive bayes nearest neighbor framework one could achieve a very high level of generalization , thanks to the intrinsic properties of nbnn classifiers @xcite . the proposed approach used distance metric learning to leverage over the source knowledge at the local patch level .",
    "this brought strong results in the semi - supervised and unsupervised domain adaptation scenarios , but the method is computationally expensive and thus not suitable to work on real - time systems , like smatphones or robots .    here",
    "we propose a simple , learning free domain adaptation method that makes it possible to exploit the generalization power of nbnn in the domain adaptation setting .",
    "we leverage over the source patches by randomly selecting a subset of them , and adding them to the target patches . to further increase the descriptive power of the descriptors , we perform data augmentation both on the source and the target data , as it is standard practice in the convolutional neural network literature @xcite .",
    "the combined effect of these two simple actions is remarkable : on commonly used benchmark databases , our approach is on par with the current state of the art when there is a single source from which to adapt , and when the number of classes is limited . in the more challenging and more realistic settings of multiple sources and increasing number of classes ,",
    "our algorithm achieves the state of the art .",
    "the rest of the paper is organized as follows : after reviewing previous work ( section [ rel - work ] ) we revise the basic definitions for domain adaptation ( section [ da ] ) and the nbnn framework ( section [ nbnn ] ) .",
    "section [ rand - da - nbnn ] introduces our approach , while section [ experiments ] presents its thorough experimental evaluation .",
    "we conclude with a summary discussion and outlining possible future avenues for research .",
    "the problem of domain adaptation stems from the fact that supervised learning methods fail to generalize across datasets @xcite .",
    "although this problem exists in various applications @xcite , the visual recognition community has just recently shown interest in dealing with it @xcite .",
    "failure to generalize across datasets has been attributed to the mismatch among various characteristics of the considered databases , and is usually referred to as the ` dataset bias ' problem @xcite .",
    "the fact that different image datasets vary considerably in quality , point of view and image contents , reveals that addressing the domain adaptation problem can significantly improve the performance of visual recognition applications .",
    "several approaches have been adopted for reducing the distance between datasets .",
    "these approaches vary from transferring source data to the target domain @xcite or transferring both source and target to a third space@xcite . unfortunately , despite all efforts , @xcite showed that currently existing selective transfers do not offer significant improvement over random transfers .",
    "as an alternative to the enrichment of the target data through instance based transfer from the source , attempts have been made to modify the classifier in order to resolve the mismatch problem @xcite during training .    while the image to image paradigm is the dominant approach in the above - mentioned methods",
    ", @xcite suggested that one can replace nbnn for bag of words ( bow ) combined with an image - to - image classification paradigm , in favor of an image - to - class recognition framework .",
    "this idea helps release the domain transfer from the known shortcomings of bow representations @xcite .",
    "even though nbnn has been tested on several visual learning applications , the use of this classification paradigm in domain adaptation has been limited . only in 2013",
    "@xcite exploited its potential in a metric learning approach , and showed that using nbnn , one can easily surpass the state of the art among bow - based algorithms presented so far .",
    "be that as it may , the possible usages of this method , called da - nbnn , are restricted due to the computational complexity .",
    "indeed , once the amount of classes , the number of sources and the number of data for each class and source grow , using da - nbnn becomes computationally prohibitive .",
    "our approach overcomes these computational limitations while preserving , and often significantly surpassing , the performances of da - nbnn , proposing the first learning free nbnn - based domain adaptation method in the literature .",
    "in this section we set the scene by introducing formal definitions for the domain adaptation problem ( section [ da ] ) and the nbnn classification framework [ nbnn ] . the notion introduced in this section",
    "will then be used to present our algorithm .",
    "domain adaptation is the problem where knowledge from the source domain @xmath8 is used to enrich and hence improve the performance in the target domain @xmath9 .",
    "this knowledge from the souce might be in the form of instances or data , or model parameters , or metric induced by the source .",
    "it is usually implicitly assumed that the labeled data on the target domain does not exist ( unsupervised setting ) or it is scarce ( semi - supervised setting ) .",
    "although the source and the target domains are different , they use equal label sets @xcite @xmath10 . + the core cause of mismatch between the two domains",
    "is attributed to the difference in the distribution of these labels .",
    "the conditional probability of labels given features are not completely coincident @xmath11 and the marginal data distributions are not equal either @xmath12 . in this paper , we will focus exclusively on the semi - supervised setting .      in the naive bayes nearest neighbor ( nbnn ) classification framework",
    ", it is assumed that for each class there exists a distribution from which local descriptors are drawn independently of one another .",
    "this leads to the use of a naive bayes maximum a posteriori classifier @xcite where each feature @xmath13 votes for one of the classes in @xmath14 .",
    "this voting is realized using the local distance between each feature and its nearest neighbor in class c. @xmath15 .",
    "the generalization of this distance concept to image to class distance is straightforward:@xmath16 .",
    "+ the output of the classifier would then be @xmath17 the distance to this optimum class p is called the positive distance while the distances to the rest of the classes @xmath18 are called the negative distances .",
    "[ diagram ]    [ rand - da - nbnn ] as outlined above , the problem of domain adaptation emerges when the training data for the target task is scarce .",
    "should it not be the case , any supervised learning algorithm would be capable of learning a classifier , according to its learning abilities .",
    "it is also assumed that there exists at least another dataset with enough samples to learn a good classifier ( the source ) , but since the two datasets have been acquired in two different domains , the performance obtained training on the source and testing on the target is not satisfactory .",
    "the nbnn algorithm builds support sets for each class made of the collection of all the features extracted from patches of each of the training examples . due to the scarcity of the data on the target , the support sets that can be built solely using features from the target samples will not contain enough features to guarantee a solid performance . in order to enrich these support sets , _",
    "our proposal is to use features extracted from the patches of the source images_.    how to select such patches - based features ? in @xcite , the authors investigate a domain adaptation approach based on the idea of landmark samples from the source domain , which are relevant for the modeling of the target classifier .",
    "although their approach is theoretically sound , experiments show that the learning method proposed to select such landmark is often statistically on par , and otherwise within a two percent range of performance , with a random selection of the learning samples .",
    "motivated by this result , we apply the same philosophy here to the patches - based features , and we propose to achieve domain adaptation in an nbnn - based framework by randomly sampling a percentage of the patches - based features from the source , adding them to the patches - based features of the target",
    ". we will show with experiments in the next section that this extremely simple and learning free strategy achieves amazingly good results on standard domain adaptation benchmark databases , while being reasonably stable with respect to the amount of features to be samples .    to further improve performance in our approach ,",
    "we have tested the effect of performing data augmentation on the source and target data .",
    "data augmentation is a technique that , since the spectacular success of convolutional neural network in the visual classification arena , has been shown to be very effective in general for any classification algorithm @xcite .",
    "again , our experiments confirm the effectiveness of this strategy , even more so combined with the instance - based domain adaptation approach based on random sampling of patches - based features from the source .",
    "a schematic representation of the overall approach for the class ` cow ' is given in figure [ rand - da - nbnn ] .",
    "note that adding the data augmentation step to our overall approach does not significantly increase the almost non - existent computational load in training .",
    "this characteristic , combined with the remarkably good performances achieved especially as the number of classes and sources grow , makes our approach potentially attractive for applications where computational complexity should be low , like mobile robot or online , wearable systems . to the best of our knowledge ,",
    "there are no previous instance - based , nbnn - based domain adaptation methods in the literature , nor the random sampling strategy has been ever tested in the nbnn learning framework for any learning to learn approach .",
    "in this section we describe the experiments we performed to assess our approach .",
    "we first describe the data , features and experimental setup used ( section [ datasets ] ) , then we report the results obtained ( section [ results ] ) .",
    "we discuss our findings in section [ discussion ] .",
    "we used the office dataset , the standard test bed in domain adaptation which addresses the problem of object categorization between any two datasets of objects usually found in offices @xcite .",
    "this test bed consists of three domains namely amazon , webcam and dslr .",
    "the amazon dataset contains images obtained from online merchants .",
    "the images are centered and usually on a white background .",
    "webcam and dslr are respectively low resolution and high resolution images obtained from web cam and slr cameras . unlike amazon , they could be subject to various environmental disturbances such as lighting or background changes .",
    "the office dataset contain 31 classes of images for each domain .",
    "+ having chosen 10 of the original 31 classes from office , @xcite suggested that we can add images of the same 10 classes from caltech-256 @xcite and form the office+caltech test bed in order to add a fourth domain in the office dataset . +      following the protocol of @xcite ,",
    "images were all resized to a common width ( 256px ) and then converted to grayscale .",
    "surf features were extracted according to @xcite .",
    "the final result was a set of features of length 64 that were consequently fed to a 1-nearest neighbor classifier .",
    "+ the effect of data augmentation on both domains has also been studied . to this end , we have duplicated the exact procedure suggested in @xcite and each image is converted into 10 images through the procedure of cropping and flipping .",
    "+      different pairs of datasets are chosen to act as the source and the target from the office+caltech group . from the source dataset ,",
    "20 images were selected to represent the source data but only 3 were chosen from the target in every class .",
    "when the target was webcam , 15 images were selected instead of 20 as described in @xcite . at this stage , since the dslr dataset behaves very similarly to webcam and it contains a lower number of images , we decided not to include it in our benchmarking .",
    "+ the same sample selection protocol has been adopted for the 31 class adaptation experiments . the third setup that we considered is domain adaptation from more than one source with one target . to this end ,",
    "all possible combinations of two sources to one target have been examined and benchmarked against the existing reported results in the literature .",
    "the first set of experiments was done on a subset of office+caltech consisting of 10 classes as explained in @xcite .",
    "figure [ fig : accuracies ] shows the results in comparison to the state of the art and some baseline algorithms .",
    "+     +   + [ fig : accuracies ]    figures  [ fig : acc_caltech256_classemes ] , [ fig : acc_caltech256_object_bank ] and [ fig : acc_sun09 ] show the changes in the recognition rate with the increase of the percentage of descriptors , randomly transferred to the target from the source . for a better understanding of the effects of different factors , four cases have been demonstrated together .",
    "original data is where there is no augmentation done neither on the target nor on the source domains .",
    "the cases where only the source and only the target domains have been augmented are referred to as source augmented and target augmented respectively .",
    "source and target augmented is where both domains have been over - sampled .",
    "+ the second set of experiments is done on the 31 class office dataset .",
    "the experiments are done exactly inline with what explained and done in @xcite .",
    "table [ 31 class ] shows the results with comparison to the state of the art both using nbnn and the state of the art based on a method other than nbnn .",
    "some further baselines are also included for better comparison .",
    "+    .31 class office dataset experiments , semi - supervised setting [ cols=\"^,^,^,^\",options=\"header \" , ]     the third and last set of experiments are those run using more than one source domain .",
    "the results can be seen in figure [ 2to1 ] .",
    "not all algorithms can be extended to cover the case of several sources and so only those who had this advantage were included in the comparison .",
    "for the experiments the exact test set of @xcite has been used .",
    "the biggest advantage of our proposed method is its simplicity combined with its strong performance over growing number of classes and source domains .",
    "it also performs surprisingly well in comparison to other algorithms .",
    "the results in figure [ fig : accuracies ] show that while different algorithms have varying performances on various test settings , our method is never worse than the second best .",
    "in particular , compared to da - nbnn @xcite ( which is the state of the art among all the methods that exploit an nbnn approach ) , our method outperforms it in 2 cases ( a - w and c - w ) , while da - nbnn performs better in two cases ( c - a and a - c ) . in the remaining two cases ( w - a , w - c ) their performance is close .",
    "in fact , the @xmath19 test shows that in these two experiments there is no statistical evidence of superiority for either of the algorithms .",
    "our method performs significantly better than l2l @xcite where l2l is the state of the art among methods that do not use nbnn . in four of the experiments",
    ", l2l achieves inferior results than ours , while only in one setting shows superiority .",
    "note that the accuracy values reported for l2l have been taken from @xcite , where no result was reported for the c - w experiment .    using the 31 class office setting",
    ", one can study and compare the scalability of the algorithms with respect to the number of classes .",
    "addressing this type of scalability for our method appears very straightforward .",
    "the fact that there is no training , makes things very easy and faster .",
    "table [ 31 class ] shows that , performance - wise , our method scores higher than da - nbnn in all three experiments and better than l2l in two out of three cases .",
    "figure [ 2to1 ] compares the recognition rate for all possible combinations of two sources and one target in the office dataset . for da - nbnn",
    "it is not clear how it could be extended to this case and no experiments of the kind have been reported by its authors .",
    "l2l supports this case and it has been included in the benchmark .",
    "it can be seen that our method outperforms all the others for all three cases of experiments .",
    "an open issue in our method is of course which percentage of the source data should be randomly selected and then added to the target data , in relation to the data augmentation procedure .",
    "results shown in figures [ fig : acc_caltech256_classemes]-[fig : acc_sun09 ] show that in general the combination of source plus target data augmentation and random sampling of around 20% of patches - based features from the source seems to achieve strong performance , always better than the original data .",
    "still , as it can be seen from the figures , the actual optimal performance might vary in terms of percentage of sampling and/or data augmentation strategy for different settings .",
    "although accuracy results are on average quite stable , and therefore the algorithm could be used in online systems even in its current form with good expectations about performance , it would be desirable to explore further the issue of the data selection and find principled ways of selecting the patches to transfer from the source to the target so to have guarantees about the optimality of the procedure .",
    "of course , that would come at the expenses of the current negligible computational cost of the approach .",
    "the contribution of this paper is a learning free naive bayes nearest neighbor based domain adaptation method that is competitive with the current state of the art on the standard office - clatech benchmark database , and that achieves the state of the art when the number of classes and sources grows .",
    "the method consists in performing a random selection of patches - based local features from the source to the target , combined with a data augmentation strategy mutated from the cnn literature .",
    "the resulting algorithm is extremely simple but also remarkably effective , especially when the number of classes and sources grows .",
    "an open challenge is how to select the best percentage of source data to add to the target : even though our experimental evaluation indicates that as a rule of thumb sampling around twenty percent of the overall sample data ( i.e. after data augmentation ) in general leads to very good results , future work will focus on how to determine how much to sample in a principled manner , while at the same time not increasing excessively the computational cost of the approach .",
    "gong b. , shi y. , sha f. , grauman k. : geodesic flow kernel for unsupervised domain adaptation . in cvpr , 2012",
    "yang j. , yan r. , hauptmann a. g. : cross - domain video concept detection using adaptive svms . in acm multimedia,2007"
  ],
  "abstract_text": [
    "<S> as of today , object categorization algorithms are not able to achieve the level of robustness and generality necessary to work reliably in the real world . </S>",
    "<S> even the most powerful convolutional neural network we can train fails to perform satisfactorily when trained and tested on data from different databases . </S>",
    "<S> this issue , known as domain adaptation and/or dataset bias in the literature , is due to a distribution mismatch between data collections . </S>",
    "<S> methods addressing it go from max - margin classifiers to learning how to modify the features and obtain a more robust representation . </S>",
    "<S> recent work showed that by casting the problem into the image - to - class recognition framework , the domain adaptation problem is significantly alleviated @xcite . here </S>",
    "<S> we follow this approach , and show how a very simple , learning free naive bayes nearest neighbor ( nbnn)-based domain adaptation algorithm can significantly alleviate the distribution mismatch among source and target data , especially when the number of classes and the number of sources grow . </S>",
    "<S> experiments on standard benchmarks used in the literature show that our approach ( a ) is competitive with the current state of the art on small scale problems , and ( b ) achieves the current state of the art as the number of classes and sources grows , with minimal computational requirements . </S>"
  ]
}