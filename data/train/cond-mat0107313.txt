{
  "article_text": [
    "the extraction of sensory information by the brain from a stream of multi - dimensional data may be understood as a process of optimisation of mutual information @xmath5 @xcite or of redundancy @xcite .",
    "the @xmath6 measures the statistical dependence between two random variables @xcite . in our case",
    ", they correspond to an @xmath0-dimensional input signal @xmath7 provided by the sensory receptors , and a @xmath1-dimensional output @xmath8 .",
    "more precisely , the @xmath6 indicates the amount of knowledge about @xmath7 that can be extracted from @xmath8 ( see e. g. ref.@xcite ) . with respective probability densities @xmath9 and @xmath10 , this is given by    @xmath11 = <",
    "\\log { p_{v,\\xi}(\\vec{v},\\vec{\\xi})\\over   p_{v}(\\vec{v})p_{\\xi}(\\vec{\\xi})}>_{v,\\xi } , \\label{1.ip}\\ ] ]    here @xmath12 .",
    "the average @xmath13 is over the joint probability distribution @xmath14 .",
    "for instance , if @xmath8 and @xmath7 are independent , we have @xmath15 and so @xmath16 .",
    "the problem of learning the statistical properties of a set of @xmath0-dimensional correlated gaussian inputs is well understood for a linear channel , even in the presence of noise ( which is actually necessary to regularise @xmath6)@xcite-@xcite .",
    "a non - linear continuous channel has also been studied in the low - noise limit for rather general transfer functions @xcite .",
    "it was shown that maximisation of the @xmath6 leads to a factorial code .",
    "threshold - linear networks @xcite ( treated with the replica technique ) have also been considered . on the other hand , the binary channel , where the outputs take discrete values ( say @xmath17 ) ,",
    "is not well understood , although the problem has been studied using replica - symmetric ( @xmath18 ) statistical - mechanical techniques @xcite-@xcite .",
    "most interestingly , an analytical solution has been found , and the existence of a large order phase transition as the number of output neurons increases has been suggested @xcite .",
    "the relevant parameter to describe this transition s occurrence is the ratio between the number of output and of input units , @xmath2 .",
    "the analytical solution holds up to a value of @xmath4 of order one , beyond which it is not longer correct .",
    "on the contrary , the @xmath18 solution does not exhibit any transition and gives good approximations at both the small and large @xmath4 regimes .",
    "it has been proved that below some @xmath19 the analytical and the @xmath18 solutions are very close .",
    "in fact , an expansion in powers of @xmath4 shows that the two solutions are identical up to @xmath20 . in spite of the fact that from the third order the corresponding expansions differ , the numerical agreement up to @xmath21 is excellent ( a relative difference of less than 0.9% up to @xmath22 ) .",
    "this is due to intriguing cancellations between higher orders .",
    "here we present simulations with the aim of providing numerical evidence on the validity of the analytical solution .",
    "in addition , since the order of the transition is large and the @xmath18 solution seems to interpolate well between the small @xmath4 and the asymptotic regimes , we also compare numerical simulations at several values of @xmath4 with the replica theory prediction @xcite .",
    "we consider a single - layer perceptron , or @xmath23 , with @xmath0 continuous input neurons whose states @xmath24 define a vector @xmath25 representing the @xmath26 received from the environment .",
    "the output layer has @xmath1 binary neurons the values @xmath17 of which compose the vector @xmath27 , that represents the @xmath28 . between the signal and the code",
    "there is an encoder , given by a set of synaptic couplings @xmath29 .",
    "the inputs take values drawn from an @xmath0-dimensional gaussian probability distribution , unbiased ( @xmath30 ) and with correlation matrix @xmath31 ,    @xmath32    which using a convenient shorthand can be expressed as @xmath33 .",
    "( here @xmath34 is a correlation parameter between input neurons , to be defined more precisely later ) .",
    "the transfer function is deterministic , so that @xmath35 , where    @xmath36    and the @xmath37 s denote the synaptic weight vectors linking the signal @xmath7 to each output neuron @xmath38 .",
    "they form a set of independent random vectors @xmath39 , each distributed according to an @xmath0-dimensional gaussian probability , with mean @xmath40 and correlation matrix @xmath41 , whose elements are @xmath42 .",
    "this means that @xmath43 .",
    "we are mainly interested in computing the averaged mutual information per input unit in the thermodynamic limit , _",
    "i.e. _ @xmath44>_{\\cal{j}}$ ] .",
    "a useful result to have in mind is that the @xmath6 in eq.([1.ip ] ) is the difference    @xmath11_{\\cal{j } } = h[p_{v } ] - h[p_{v|\\xi } ] , \\label{2.ip}\\ ] ]    where @xmath45= -<\\log p_{v}>_{v}$ ] is the entropy of the output , while @xmath46= -<<\\log p_{v|\\xi}>_{v|\\xi}>_{\\xi}$ ] is the conditional entropy of the output given the input ( averaged over the input ) .",
    "the code @xmath8 , given a fixed signal @xmath7 , has the conditional probability @xmath47 .",
    "since a deterministic channel clearly has zero conditional entropy , in that case the @xmath6 reduces to the output entropy .",
    "as an example we consider a gaussian input distribution with two - point correlation @xmath48 given by @xmath49 .",
    "the input neurons are then less ( more ) spatially correlated if @xmath50 ( @xmath51 ) . for @xmath52 , @xmath48 is a singular matrix , while for @xmath53 @xmath48 tends towards the identity matrix .",
    "the correlations between two synapses converging to the same output , @xmath54 , are chosen to be equal for all output neurons and normalised to @xmath55 , @xmath56 @xmath57 .    for small @xmath4 ,",
    "the mi can be expanded as @xmath58 .",
    "one trivially obtains that @xmath59 and @xmath60 .",
    "both the @xmath18 solution ( see @xcite ) and the analytical solution ( see @xcite ) give the same value of @xmath61 ,    @xmath62    where @xmath63 .",
    "on the other hand , the two techniques disagree in their predictions for @xmath64 .",
    "the @xmath18 method yields    @xmath65    while ( following the methods of @xcite ) one can easily verify that the analytical technique gives :    @xmath66    the @xmath18 solution also gives rise to predictions for strongly correlated inputs . for @xmath51",
    "one obtains :    @xmath67    finally , the same solution shows logarithmic behaviour for large @xmath4 :    @xmath68 . \\label{4.il}\\ ] ]    = 5.cm",
    "the first step of the simulation entails choosing a coupling sample @xmath69 at random .",
    "a given choice of @xmath69 will be labelled as the sample @xmath70 , and the total number of samples will be denoted by @xmath71 .",
    "next , signals @xmath7 are drawn in order to obtain several codewords @xmath8 .",
    "a histogram @xmath72 is then constructed from these states , representing the probability @xmath73 and allowing us to estimate the output entropy :    @xmath74    clearly , this approximation to the true code entropy @xmath45 $ ] will improve as the number of drawn signals increases . in practice , we evaluated @xmath72 using about @xmath75 different input states @xmath7 .",
    "the final step is to calculate the average over an ensemble of @xmath69 , to get @xmath76 .",
    "here we present studies of three relevant regions in parameter space @xmath77 : ( 1 ) @xmath4 small ; ( 2 ) intermediate values of @xmath4 and strong correlations , @xmath51 ; and ( 3 ) @xmath4 large .",
    "the results are compared with the theoretical predictions , eqs.([4.a2])-([4.il ] ) .",
    "the self - averaging property of the mi is also analysed .    in the * small @xmath4 case * ,",
    "we calculated @xmath78 for values of @xmath4 ranging from @xmath79 to @xmath80 : we fixed @xmath81 and took a variable number of inputs in the interval @xmath82 .",
    "we repeated this process for several values of the correlation parameter @xmath83 . using that @xmath84 when @xmath85 , we performed a linear regression on @xmath86 , @xmath87 , and obtained the coefficients @xmath88 as a function of @xmath34 .",
    "for all values of @xmath34 , @xmath89 , which is in agreement with the theory",
    ". the function @xmath90 is plotted in fig.[1ax ] , in comparison with the theoretical prediction ( eq.([4.a2 ] ) ) .",
    "the good agreement indicates that the thermodynamic limit solution is a good estimation even for @xmath0 not very large , as long as one is in the low - loading expansion .    in order to evaluate @xmath91",
    ", we set @xmath92 ( to avoid increasing the error through a larger number of parameters to be fitted ) .",
    "we analysed the mi only for the value @xmath93 . from eq .",
    "[ 4.a2 ] the theoretical value @xmath94 can be obtained , and from eqs .",
    "[ 4.aa ] and [ 4.a3 ] we have @xmath95 and @xmath96 respectively .",
    "the simulation itself was done using several values of @xmath1 , and for each of them a linear regression was performed using @xmath97/\\alpha = a_{2}+a_{3}\\alpha$ ] . in this case the corrections due to finite size effects are noticeable . to observe convergence to the asymptotic regime we considered several values of @xmath1 , namely @xmath98 and @xmath99 .",
    "we averaged for each over , respectively , @xmath100 and @xmath101 samples of @xmath69 .",
    "the number of samples @xmath71 was taken larger as the number of outputs @xmath1 became smaller , to compensate for the lack of statistics .",
    "numerical evaluation of @xmath91 is extremely costly in computational time ; this is the reason why we restricted ourselves to a single value of @xmath34 and did not consider values of @xmath1 larger than @xmath99 .",
    "the result is that our simulation is in agreement with the analytical prediction .",
    "as we see in fig.[2ax]b , @xmath102 converges to @xmath103 as @xmath1 increases .",
    "given the error bars , this result is compatible with @xmath104 but _ excludes _ @xmath105 .",
    "for the sake of comparison we have included fig .",
    "[ 2ax]a where the same numerical analysis is done for @xmath61 .    the results for * strongly correlated inputs * are shown in fig .",
    "[ 3ix , il , is]a .",
    "there is good agreement between the simulation and the exponential behaviour of eq.([4.ik ] ) : @xmath106 .",
    "the results for the * large @xmath4 limit * are presented in fig.[3ix , il , is]b .",
    "the logarithmic behaviour predicted in eq.([4.il ] ) is observed .    to verify that the mi is self - averaging we calculated its mean - square deviation @xmath107 over the samples and made a fit to the form @xmath108 .",
    "the good agreement with this expression can be seen in fig .",
    "[ 3ix , il , is]c .",
    "this shows that the methods of statistical mechanics are appropriate to studies of the information in binary channels .",
    "our main result refers to the comparison between the @xmath18 @xcite and analytical @xcite solutions .",
    "the difference between them can be seen in a small-@xmath4 expansion .",
    "numerical simulation confirms that the two solutions coincide up to second order . at the next order",
    "the solutions are different ( see eqs .",
    "[ 4.a3]-[4.aa ] ) , and the simulation excludes the replica calculation while it is in agreement with the analytical one ( see fig . [ 2ax ] ) .",
    "we have also verified the conjecture that the @xmath18 solution is a good interpolation between the small and the large @xmath4 behaviors @xcite . in particular",
    ", the simulation shows that for intermediate values of @xmath4 and strongly correlated inputs the mi behaves as @xmath109 ( see fig.[3ix , il , is]a ) .",
    "moreover , in fig.[3ix , il , is]b we see that the expected logarithmic behaviour for the large @xmath4 case fits the mi very well .",
    "this work was supported by a spanish grant pb 96 - 47 .",
    "antonio turiel is financially supported by an fpi grant from the comunidad aut ' onoma de madrid , spain .",
    "m. maravall was supported by a beca de colaboracin from the spanish ministry of education .",
    "dominguez thanks the k.u .",
    "leuven for a research fund ( grant ot/94/9 ) ."
  ],
  "abstract_text": [
    "<S> the mutual information of a single - layer perceptron with @xmath0 gaussian inputs and @xmath1 deterministic binary outputs is studied by numerical simulations . </S>",
    "<S> the relevant parameters of the problem are the ratio between the number of output and input units , @xmath2 , and those describing the two - point correlations between inputs . </S>",
    "<S> the main motivation of this work refers to the comparison between the replica computation of the mutual information and an analytical solution valid up to @xmath3 . </S>",
    "<S> the most relevant results are : ( 1 ) the simulation supports the validity of the analytical prediction , and ( 2 ) it also verifies a previously proposed conjecture that the replica solution interpolates well between large and small values of @xmath4 .    </S>",
    "<S> keywords : statistical physics , information theory , neural networks , unsupervised learning .    </S>",
    "<S> pacs :    87.10.+e : general , theoretical , and mathematical biophysics    64.60.cn : order - disorder transformations ; statistical mechanics of model systems    _ europhysics letters _ * 45 * 6 , 739 - 744 ( 1999 ) </S>"
  ]
}