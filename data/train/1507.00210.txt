{
  "article_text": [
    "deep networks have proven extremely successful across a broad range of applications .",
    "while their deep and complex structure affords them a rich modeling capacity , it also creates complex dependencies between the parameters which can make learning difficult via first order stochastic gradient descent ( sgd ) . as long as sgd remains the workhorse of deep learning , our ability to extract high - level representations from data may be hindered by difficult optimization , as evidenced by the boost in performance offered by batch normalization ( bn ) @xcite on the inception architecture @xcite .",
    "though its adoption remains limited , the natural gradient @xcite appears ideally suited to these difficult optimization issues . by following the direction of steepest descent on the probabilistic manifold ,",
    "the natural gradient can make constant progress over the course of optimization , as measured by the kullback - leibler ( kl ) divergence between consecutive iterates .",
    "utilizing the proper distance measure ensures that the natural gradient is invariant to the parametrization of the model .",
    "unfortunately , its application has been limited due to its high computational cost .",
    "natural gradient descent ( ngd ) typically requires an estimate of the fisher information matrix ( fim ) which is square in the number of parameters , and worse , it requires computing its inverse .",
    "truncated newton methods can avoid explicitly forming the fim in memory @xcite , but they require an expensive iterative procedure to compute the inverse .",
    "such computations can be wasteful and do not take into account the smooth change of the fisher during optimization or the highly structured nature of deep models .",
    "inspired by recent work on model reparametrizations @xcite , our approach starts with a simple question : can we devise a neural network architecture whose fisher is constrained to be identity ?",
    "this is an important question , as sgd and ngd would be equivalent in the resulting model .",
    "the main contribution of this paper is in providing a simple , theoretically justified network reparametrization which approximates via first - order gradient descent , a block - diagonal natural gradient update over layers .",
    "our method is computationally efficient due to the local nature of the reparametrization , based on whitening , and the amortized nature of the algorithm .",
    "our second contribution is in unifying many heuristics commonly used for training neural networks , under the roof of the natural gradient , while highlighting an important connection between model reparametrizations and mirror descent @xcite . finally , we showcase the efficiency and the scalability of our method across a broad - range of experiments , scaling our method from standard deep auto - encoders to large convolutional models on imagenet@xcite , trained across multiple gpus .",
    "this is to our knowledge the first - time a ( non - diagonal ) natural gradient algorithm is scaled to problems of this magnitude .",
    "this section provides the necessary background and derives a particular form of the fim whose structure will be key to our efficient approximation . while we tailor the development of our method to the classification setting , our approach generalizes to regression and density estimation .",
    "we consider the problem of fitting the parameters @xmath0 of a model @xmath1 to an empirical distribution @xmath2 under the log - loss .",
    "we denote by @xmath3 the observation vector and @xmath4 its associated label .",
    "concretely , this stochastic optimization problem aims to solve : @xmath5.\\end{aligned}\\ ] ]    defining the per - example loss as @xmath6 , stochastic gradient descent ( sgd ) performs the above minimization by iteratively following the direction of steepest descent , given by the column vector @xmath7 $ ] .",
    "parameters are updated using the rule @xmath8 , where @xmath9 is a learning rate .",
    "an equivalent proximal form of gradient descent @xcite reveals the precise nature of @xmath9 : @xmath10 namely , each iterate @xmath11 is the solution to an auxiliary optimization problem , where @xmath9 controls the distance between consecutive iterates , using an @xmath12 distance .",
    "in contrast , the natural gradient relies on the kl - divergence between iterates , a more appropriate distance measure for probability distributions .",
    "its metric is determined by the fisher information matrix , @xmath13      \\right\\},\\end{aligned}\\ ] ] i.e. the covariance of the gradients of the model log - probabilities wrt .",
    "its parameters .",
    "the natural gradient direction is then obtained as @xmath14 .",
    "see @xcite for a recent overview of the topic .",
    "we start by deriving the precise form of the fisher for a canonical multi - layer perceptron ( mlp ) composed of @xmath15 layers .",
    "we consider the following deep network for binary classification , though our approach generalizes to an arbitrary number of output classes .",
    "@xmath16    the parameters of the mlp , denoted @xmath17 , are the weights @xmath18 connecting layers @xmath19 and @xmath20 , and the biases @xmath21 .",
    "@xmath22 is an element - wise non - linear function .",
    "let us define @xmath23 to be the backpropagated gradient through the @xmath19-th non - linearity .",
    "we ignore the off block - diagonal components of the fisher matrix and focus on the block @xmath24 , corresponding to interactions between parameters of layer @xmath19 .",
    "this block takes the form : @xmath25,\\end{aligned}\\ ] ] where @xmath26 is the vectorization function yielding a column vector from the _ rows _ of matrix @xmath27 .    assuming that @xmath23 and activations @xmath28 are independent random variables , we can write : @xmath29      \\mathbb{e}_{\\pi } \\left [ h_{i-1}(m )   h_{i-1}(n ) \\right],\\end{aligned}\\ ] ] where @xmath30 is the element at row @xmath19 and column @xmath31 of matrix @xmath27 and @xmath32 is the @xmath19-th element of vector @xmath33 .",
    "@xmath34 is the entry in the fisher capturing interactions between parameters @xmath35 and @xmath36 .",
    "our hypothesis , verified experimentally in sec .",
    "[ sec : conditioning ] , is that we can greatly improve conditioning of the fisher by enforcing that @xmath37 = i$ ] , for all layers of the network , despite ignoring possible correlations in the @xmath38 s and off block diagonal terms of the fisher .",
    "[ cols=\"^,^ \" , ]      the model used for our cifar experiments consists of 8 convolutional layers , having @xmath39 receptive fields . @xmath40 spatial max - pooling was applied between stacks of two convolutional layers , with the exception of the last convolutional layer which computes the class scores and is followed by global max - pooling and soft - max non - linearity .",
    "this particular choice of architecture was inspired by the vgg model @xcite and held fixed across all experiments .",
    "the number of filters per layer is as follows : @xmath41 .",
    "the model was trained on @xmath42 random crops with random horizontal reflections .",
    "model selection was performed on a held - out validation set of @xmath43k examples .",
    "results are shown in fig .",
    "[ fig : cifar_imnet_results ] .    with respect to training error , prong and batch normalization seem to offer similar speedups compared to sgd with momentum .",
    "our hypothesis is that the benefits of prong are more pronounced for densely connected networks , where the number of units per layer is typically larger than the number of maps used in convolutional networks .",
    "interestingly , prong generalized better , achieving @xmath44 test error vs. @xmath45 for batch normalization .",
    "this could reflect the findings of @xcite , which showed how ngd can leverage unlabeled data for better generalization : the `` unlabeled '' data here comes from the extra perturbations in the training set when estimating the whitening matrices .",
    "our final set of experiments aims to show the scalability of our method : we thus apply our natural gradient algorithm to the large - scale ilsvrc12 dataset ( 1.3 m images labelled into 1000 categories ) using the inception architecture @xcite . in order to scale to problems of this size , we parallelized our training loop so as to split the processing of a single minibatch ( of size @xmath46 ) across multiple gpus .",
    "note that prong can scale well in this setting , as the estimation of the mean and covariance parameters of each layer is also embarassingly parallel .",
    "eight gpus were used for computing gradients and estimating model statistics , though the eigen decomposition required for whitening was itself not parallelized in the current implementation .    for all optimization algorithms",
    ", we considered initial learning rates in @xmath47 and used a value of @xmath48 as the momentum coefficient .",
    "for prong we tested reparametrization periods @xmath49 , while typically using @xmath50 .",
    "eigenvalues were regularized by adding a small constant @xmath51 before scaling the eigenvectors .",
    "weight decay parameter of @xmath52 , with no dropout @xcite . ] given the difficulty of the task , we employed the enhanced prong@xmath53 version of the algorithm , as simple periodic whitening of the model proved to be unstable .",
    "figure  [ fig : cifar_imnet_results ] ( c - d ) shows that batch normalisation and prong@xmath53 converge to approximately the same top-1 validation error ( @xmath54 vs @xmath55 respectively ) for similar cpu - time . in comparison , sgd achieved a validation error of @xmath56 .",
    "prong@xmath53 however exhibits much faster convergence initially : after @xmath57 updates it obtains around @xmath58 error compared to @xmath59 for bn alone .",
    "we stress that the imagenet results are somewhat preliminary . while our top-1 error is higher than reported in @xcite ( @xmath60 ) , we used a much less extensive data augmentation pipeline .",
    "we are only beginning to explore what natural gradient methods may achieve on these large scale optimization problems and are encouraged by these initial findings .",
    "we began this paper by asking whether convergence speed could be improved by simple model reparametrizations , driven by the structure of the fisher matrix . from a theoretical and experimental perspective",
    ", we have shown that whitened neural networks can achieve this via a simple , scalable and efficient whitening reparametrization .",
    "they are however one of several possible instantiations of the concept of natural neural networks . in a previous incarnation of the idea",
    ", we exploited a similar reparametrization to include whitening of backpropagated gradients , with @xmath61 the whitening matrix for @xmath23 . ] .",
    "we favor the simpler approach presented in this paper , as we generally found the alternative less stable with deep networks . ensuring zero - mean gradients also required the use of skip - connections , with tedious book - keeping to offset the reparametrization of centered non - linearities @xcite .",
    "maintaining whitened activations may also offer additional benefits from the point of view of model compression and generalization . by virtue of whitening , the projection @xmath62 forms an ordered representation , having least and most significant bits . the sharp roll - off in the eigenspectrum of @xmath63",
    "may explain why deep networks are ammenable to compression @xcite .",
    "similarly , one could envision spectral versions of dropout @xcite where the dropout probability is a function of the eigenvalues .",
    "alternative ways of orthogonalizing the representation at each layer should also be explored , via alternate decompositions of @xmath63 , or perhaps by exploiting the connection between linear auto - encoders and pca",
    ". we also plan on pursuing the connection with mirror descent and further bridging the gap between deep learning and methods from online convex optimization .",
    "we are extremely grateful to shakir mohamed for invaluable discussions and feedback in the preparation of this manuscript .",
    "we also thank philip thomas , volodymyr mnih , raia hadsell , sergey ioffe and shane legg for feedback on the paper ."
  ],
  "abstract_text": [
    "<S> we introduce natural neural networks , a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the fisher matrix . </S>",
    "<S> in particular , we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer , while preserving the feed - forward computation of the network . such networks can be trained efficiently via the proposed projected natural gradient descent algorithm ( prong ) , which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the mirror descent online learning algorithm . </S>",
    "<S> we highlight the benefits of our method on both unsupervised and supervised learning tasks , and showcase its scalability by training on the large - scale imagenet challenge dataset . </S>"
  ]
}