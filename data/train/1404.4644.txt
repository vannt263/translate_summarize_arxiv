{
  "article_text": [
    "the study of social networks is becoming increasingly popular . a whole new set of information about an individual",
    "is gained by analyzing the data that is derived from his / her social network .",
    "personal social network of an individual consisting only of neighbors and connections between them , also known as  ego network \" , has recently grabbed significant attention  @xcite .",
    "this new view of the gigantic incomprehensible social network as a collection of small informative overlapping ego networks generates a huge collection of graphs , which leads to a closer and more tractable investigation .",
    "this enormous collection of ego networks , one centered at each user , opens doors for many interesting possibilities which were not explored before .",
    "for instance , consider the scientific collaboration ego network of an individual .",
    "it is known that collaboration follows different patterns across different fields  @xcite .",
    "some scientific communities are more tightly linked among themselves compared to other fields having fewer dependencies among the collaborators .",
    "for instance , people working in experimental high energy physics are very much dependent on specialized labs worldwide ( for example cern ) , and hence it is more likely that scientists in this field have a lot of collaboration among themselves .",
    "collaboration network in such a scientific domain will exhibit more densely connected network compared to other fields where people prefer to work more independently .",
    "the peculiarity in the collaboration network gets reflected in the ego network as well .",
    "for an individual belonging to a more tightly connected field , such as high energy physics , it is more likely that there is collaboration among the individual s coauthors .",
    "thus , we can expect the collaboration ego network of an individual to contain information about the characteristic of his / her research . by utilizing this information",
    ", it should be possible to discriminate ( classify ) between scientists based on the ego networks of their collaboration .",
    "this information can be useful in many applications , for instance , in user based recommendations  @xcite , recommending jobs  @xcite , discovering new collaborations  @xcite , citation recommendations  @xcite .",
    "the focus of this paper is on social network classification or equivalently graph classification .",
    "the first prerequisite for classifying networks is having the  right \" measure of similarity between different graph structures .",
    "finding such a similarity measure is directly related to the problem of computing meaningful mathematical embedding of network structures . in this work",
    ", we address this fundamental problem of finding an appropriate tractable mathematical representation for graphs .",
    "there are many theories that show the peculiarities of social networks  @xcite .",
    "for instance , it is known that the spectrum of the adjacency matrix of a real - world graph is very specific . in particular",
    ", it has been observed that scale - free graphs develop a triangle like spectral density with a power - law tail , while small - world graphs have a complex spectral density consisting of several sharp peaks  @xcite . despite such insight into social graph structures , finding a meaningful mathematical representation for these networks where",
    "various graph structures can be directly compared or analyzed in a common space is an understudied area .",
    "note that the eigenvalues of a graph , which characterize its spectrum , are not directly comparable . moreover , the eigenvalues as feature vector is not a common space because a larger graph will have more number of significant eigenvalues compared to a smaller graph .",
    "recently it was shown that representing graphs as a normalized frequency vector , by counting the number of occurrences of various small @xmath5-size subgraphs ( @xmath6 3 or 4 ) , leads to an informative representation  @xcite .",
    "it was shown that this representation naturally models known distinctive social network characteristics like the  _ triadic closure _ \" .",
    "computing similarity between two graphs as the inner product between such frequency vector representations leads to the state - of - the - art social network classification algorithms .",
    "it is not clear that a histogram based only on counting small subgraphs sufficiently captures all the properties of a graph structure . only counting small @xmath5-subgraphs ( @xmath6 3 or 4 ) loses information .",
    "it is also not very clear what is the right size @xmath5 that provides the right tradeoff between computation and expressiveness .",
    "for instance , we observe that ( see section  [ sec : comp ] ) @xmath7 leads to improvement over @xmath8 but it comes with a significant computational cost .",
    "although , it is known that histograms based on counting subgraphs of size @xmath5 can be reasonably approximated by sampling few induced subgraphs of size @xmath5 , counting subgraphs with @xmath9 is still computationally expensive because it requires testing the given sampled subgraph with the representative set of graphs for isomorphism ( see section  [ sec : comp ] ) .",
    "finding other rich representation for graph , which aptly captures its behavior and is also computationally inexpensive , is an important research direction .",
    "one challenge in meaningfully representing graphs in a common space is the basic requirement that isomorphic graphs should map to the same object .",
    "features based on counting substructures , for example the frequency of subgraphs , satisfy this requirement by default but ensuring this property is not trivial if we take a non - counting based approach .    * our contributions : * we take an alternate route and characterize graph based on the truncated power iteration of the corresponding adjacency matrix @xmath4 , starting with the vector of all ones denoted by @xmath3 .",
    "such a power iteration generates vector @xmath1 in the @xmath10 iteration .",
    "we argue that the covariance between vectors of the form @xmath1 and @xmath2 , given some @xmath11 and @xmath12 , is an informative feature for a given graph .",
    "we show that these covariances are  graph invariants \" .",
    "they also contain information about the spectrum of the adjacency matrix which is an important characteristic of a random graph  @xcite .",
    "in addition , taking an altogether different view , it can be shown that these covariances are also related to the counts of small local structures in the given graph .    instead of a histogram based feature vector representation , we represent graph as a symmetric positive semidefinite covariance matrix @xmath13 whose @xmath0-th entry is the covariance between vectors @xmath1 and @xmath2 . to the best of our knowledge",
    "this is the first representation of its kind .",
    "we further compute similarity between two given graphs as the standard bhattacharya similarity between the corresponding covariance matrix representations .",
    "our proposal follows a simple procedure involving only matrix vector multiplications and summations .",
    "the entire procedure can be computed in time linear in the number of edges which makes our approach scalable in practice .",
    "similarity based on this new representation outperforms exiting methods on the task of real social network classification .",
    "for example , using the similarity based on the histogram based representation , by counting the number of small subgraphs , performs poorly compared to the proposed measure .",
    "these encouraging results provide motivation for studying power iteration of the adjacency matrix for social network analysis .",
    "in addition to the above contributions , this paper provides some interesting insights in the domain of the collaboration networks .",
    "we show that it is possible to distinguish researchers working in different experimental physics sub - domains just based on the ego network of the researcher s scientific collaboration . to the best of our knowledge",
    "this is the first work that explores the information contained in the ego network of scientific collaborations .",
    "the results presented could be of independent interest in itself .",
    "the focus of this work is on undirected , unweighted and connected graphs .",
    "any graph @xmath14 , with @xmath15 and @xmath16 , is represented by an adjacency matrix @xmath17 , where @xmath18 if and only if @xmath19 . for a matrix",
    "a , we use @xmath20 to denote the @xmath10 row of matrix @xmath4 , while @xmath21 denotes its @xmath22 column",
    ". we use @xmath3 to denote the vector with all components being 1 .",
    "dimension of vector @xmath3 will be implicit depending on the operation .",
    "vectors are by default column vectors ( @xmath23 )",
    ". the transpose of a matrix @xmath4 is denoted by @xmath24 , defined as @xmath25 . for a vector @xmath26",
    ", we use @xmath27 to denotes its @xmath10 component .",
    "two graphs @xmath28 and @xmath29 are * _ isomorphic _ * if there is a bijection between the vertex sets of @xmath28 and @xmath29 , @xmath30 , such that any two vertices @xmath31 are adjacent in @xmath28 if and only if @xmath32 and @xmath33 are adjacent in @xmath29 .",
    "every permutation @xmath34 is associated with a corresponding * permutation matrix @xmath35*. the matrix operator @xmath35 left multiplied to matrix @xmath4 shuffles the rows according to @xmath36 while right multiplication with @xmath35 shuffles the columns , i.e. , matrix @xmath37 can be obtained by shuffling the rows of @xmath4 under @xmath36 and @xmath38 can be obtained by shuffling the columns of @xmath4 under @xmath36 . given an adjacency matrix @xmath4 , graphs corresponding to adjacency matrix @xmath4 and @xmath39 are isomorphic , i.e. , they represent the same graph structure . a property of graph ,",
    "which does not change under the transformation of reordering of vertices is called * _ graph invariant_*.    for adjacency matrix @xmath4 , let @xmath40 be the eigenvalues and @xmath41 be the corresponding eigenvectors .",
    "we denote the component - wise sum of the eigenvectors by @xmath42 , i.e. , @xmath43 denotes the component - wise sum of @xmath44 .",
    "a path @xmath45 of length @xmath46 is a sequence of @xmath47 vertices @xmath48 , such that there exists an edge between any two consecutive terms in the sequence , i.e. , @xmath49 @xmath50 .",
    "an edge @xmath51 belongs to a path @xmath52 if there exists @xmath11 such that @xmath53 .    in our analysis",
    ", we can have paths with repeated nodes , i.e. we will encounter paths where @xmath54 for @xmath55 . a path will be called _  simple \" _ if there is no such repetition of nodes .",
    "formally , a * _ simple path _ * of length @xmath46 is a path of length @xmath46 , such that , @xmath56 whenever @xmath55 .",
    "two paths @xmath45 and @xmath57 are different if there exist an edge @xmath3 , such that either of the two conditions @xmath58 or @xmath59 holds , i.e. , there exists one edge which is not contained in one of the paths but contained in the other .",
    "we denote the number of all the different _  simple paths \" _ of length @xmath46 in a given graph by @xmath60 and the total number of triangles by @xmath61 . for clarity we will use [ ] to highlight scalar quantities such as @xmath62",
    "a graph is fully described by its adjacency matrix . a good characterization of a matrix operator is a small history of its _",
    "power iteration_. _ power iteration _ of a matrix @xmath17 on a given starting vector @xmath63 computes normalized @xmath64 in the @xmath10 iteration .    in one of the early results  @xcite",
    ", it was shown that the characteristic polynomial of a matrix can be computed by using the set of vectors generated from its truncated power iterations , i.e. , @xmath65 .",
    "this set of vectors are more commonly known as the `` @xmath5-order krylov subspace '' of matrix @xmath4 .",
    "the  krylov subspace \" leads to some of the fast linear algebraic algorithms for sparse matrices . in web domain , power iteration",
    "are used in known algorithms including _ page - rank _ and _ hits _",
    "it is also known  @xcite that a truncated power iteration of the data similarity matrix leads to informative feature representation for clustering .",
    "thus , the @xmath5-order krylov subspace for some appropriately chosen @xmath5 contains sufficient information to describe the associated matrix .    to represent graphs in a common mathematical space , it is a basic requirement that two isomorphic graphs should map to the same object .",
    "although the @xmath5-order krylov subspace characterizes the adjacency matrix , it can not be directly used as a common representation for the associated graph , because it is sensitive to the reordering of nodes .",
    "given a permutation matrix @xmath35 , the @xmath5-order krylov subspaces of @xmath4 and @xmath39 can be very different .",
    "in other words the mapping @xmath66 is not a _",
    "`` graph invariant '' _ mapping .",
    "note that @xmath4 and @xmath39 represent same graph structure with different ordering of nodes and hence are same entities from graph perspective but not from the matrix perspective .",
    "it turns out that if we use @xmath67 , the vector of all ones , then the covariances between the different vectors in the power iteration are _  graph invariant \" _",
    "( see theorem  [ the : inv ] ) , i.e. , their values do not change with the spurious reordering of the nodes .",
    "we start by defining our covariance matrix representation for the given graph , and the algorithm to compute it . in later sections",
    "we will argue why such a representation is suitable for discriminating between graph structures .",
    "given a graph with adjacency matrix @xmath17 and a fixed number @xmath5 , we compute the first @xmath5 terms of power iteration , which generates normalized vectors of the form @xmath1 @xmath68 . since we start with @xmath3 , we choose to normalize the sum equal to @xmath69 for the ease of analysis .",
    "after generating @xmath5 vectors , we compute matrix @xmath70 where @xmath71 , as summarized in algorithm  [ alg : cov ] .",
    "adjacency matrix @xmath72 , @xmath5 , the number of power iterations .",
    "initialize @xmath73 @xmath74 , @xmath75 @xmath76    @xmath77    @xmath78    algorithm  [ alg : cov ] maps a given graph to a positive semidefinite matrix , which is a graph invariant .",
    "[ the : inv ] @xmath13 is symmetric positive semidefinite . for any given permutation matrix @xmath35 we have @xmath79 , i.e. , @xmath13 is a graph invariant .",
    "@xmath13 is sample covariance matrix of @xmath80 and hence @xmath13 is symmetric positive semidefinite . using the identity@xmath81 , it is not difficult to show that for any permutation matrix p , @xmath82 .",
    "this along with the fact @xmath83 , yields @xmath84 thus , @xmath85 .",
    "the proof follows from the fact that shuffling vectors under same permutation does not change the value of covariance between them , i.e. , @xmath86 which implies @xmath87    note that the converse of theorem  [ the : inv ] is not true",
    ". we can not hope for it because then we would have solved the intractable _ graph isomorphism problem _ by using this tractable matrix representation .",
    "for example , consider adjacency matrix of a regular graph .",
    "it has @xmath3 as one of its eigenvectors with eigenvalue equal to @xmath88 , the constant degree of the regular graph .",
    "so , we have @xmath89 and @xmath90 .",
    "thus , all regular graphs are mapped to the same zero matrix .",
    "perfectly regular graphs never occur in practice , there is always some variation in the degree distribution of real - world graphs . for non regular graphs , i.e. , when @xmath3 is not a eigenvector of the adjacency matrix , we will show in the section  [ sec : proper ] that the proposed @xmath13 representation is informative .    *",
    "alternate motivation : graphs as a set of vectors*. there is an alternate way to motivate this representation and theorem  [ the : inv ] . at time @xmath91 , we start with a value of @xmath92 on each of the nodes . at every time step",
    "@xmath93 we update every value on each node to the sum of numbers , from time @xmath94 , on each of its neighbors .",
    "it is not difficult to show that under this process , for node @xmath11 , at time step @xmath93 we obtain @xmath95 .",
    "these kind of updates are key in many link analysis algorithms including hyper - text induced topic search ( hits )  @xcite .",
    "ignoring normalization the sequence of numbers obtained over time , by such process , on node @xmath11 corresponds to the row @xmath11 of the matrix @xmath96 .",
    "( [ eq : setvec ] ) simply tells us that reordering of nodes under any permutation does not affect the sequence of these numbers generated on each node .",
    "hence , we can associate a set of @xmath69 vectors , the @xmath69 rows of @xmath97 , with graph @xmath28 .",
    "this set of vectors do not change with reordering of nodes , they just shuffle among themselves .",
    "we are therefore looking for a mathematical representation that describes this set of @xmath69 ( @xmath5 dimensional ) vectors .",
    "probability distributions , in particular gaussian , are a natural way to model a set of vectors  @xcite .",
    "the idea is to find the maximum likelihood gaussian distribution fitting the given set of vectors and use this distribution , a mathematical object , as the required representation .",
    "note that this distribution is invariant under the ordering of vectors , and hence we get theorem  [ the : inv ] .",
    "the central component of a multivariate gaussian distribution is its covariance matrix and this naturally motivate us to study the object @xmath13 , which is the covariance matrix of row vectors in @xmath96 associated with the graph .",
    "in this section , we argue that @xmath13 encodes key features of the given graph , making it an informative representation . in particular , we show that @xmath13 contains information about the spectral properties of @xmath4 as well as the counts of small substructures present in the graph .",
    "we assume that the graph is not perfectly regular , i.e. , @xmath3 is not one of the eigenvectors of @xmath4 .",
    "this is a reasonable assumption because in real networks there are always fluctuations in the degree distribution .",
    "we first start by showing connections between the matrix @xmath13 and the spectral properties of @xmath4 .",
    "see section  [ sec_notation ] for the notation , for example , @xmath98 and @xmath99 .",
    "[ theo : cij ] @xmath100    the mean of vector @xmath1 can be written as @xmath101}{n}$ ] . with this observation the covariance between normalized @xmath1 and @xmath2 ( which is equal to @xmath102 )",
    "can be written as @xmath103 } - e\\right)^t\\left(n\\frac{a^je}{[e^ta^je ] } - e\\right)\\\\ & = \\frac{1}{n}\\left(n^2\\frac{[e^ta^{i+j}e]}{[e^ta^ie][e^ta^je ] } - n - n + e^te\\right)\\\\ & =   \\left(n\\frac{[e^ta^{i+j}e]}{[e^ta^ie][e^ta^je]}\\right ) - 1\\end{aligned}\\ ] ] thus , we have @xmath104}{[e^ta^ie][e^ta^je]}\\right ) - 1\\ ] ] to compute @xmath105 $ ] , we use the fact that the vector @xmath1 can be written in terms of eigenvalues and eigenvectors of @xmath4 as @xmath106v_1 + [ s_2\\lambda_2^i]v_2 + ... + [ s_n\\lambda_n^i]v_n.\\ ] ] this follows from the representation of @xmath3 in the eigenbasis of @xmath4 , i.e. , @xmath107 . using the eigenvector property @xmath108 , we have @xmath109 & = \\sum_{t = 1}^n \\lambda_t^i s_t [ e^tv_t ] = \\sum_{t = 1}^n \\lambda_t^i s_t^2\\end{aligned}\\ ] ]    substituting this value for terms @xmath105 $ ] in eq .",
    "( [ eq : cij ] ) leads to the desired expression .    * remarks on theorem  [ theo :",
    "cij ] : * we can see that different elements of matrix @xmath13 are ratios of polynomial expressions in @xmath98 and @xmath99 .",
    "given @xmath13 , recovering values of @xmath98 and @xmath99 @xmath110 boils down to solving a set of nonlinear polynomial equations of the form given in theorem  [ theo : cij ] for different values of @xmath11 and @xmath12 . for a given value of @xmath5 ,",
    "we obtain a set of @xmath111 different such equations . although it may be hard to characterize the solution of this set of equations , but we can not expect many combinations of @xmath98 and @xmath99 to satisfy all such equations , for some reasonably large value of @xmath111 .",
    "thus @xmath13 can be thought of as an almost lossless encoding of @xmath98 and @xmath99 @xmath110 .",
    "it is known that there is sharp concentration of eigenvalues of adjacency matrix @xmath4 for random graphs  @xcite .",
    "the eigenvalues of adjacency matrix for a random erdos - reyni graph follows wigner s semi - circle law  @xcite while for power law graphs these eigenvalues obeys power law  @xcite .",
    "these peculiar distributions of the eigenvalues are captured in the elements of @xmath112 which are the ratios of different polynomials in @xmath113 .",
    "hence we can expect the @xmath13 representations , for graphs having different spectrum , to be very different .",
    "in theorem  [ theo : cij ] , we have shown that the representation @xmath13 is tightly linked with the spectrum of adjacency matrix @xmath4 , which is an important characteristic of the given graph .",
    "it is further known that the counts of various small local substructures contained in the graph such as the number of triangles , number of small paths , etc .",
    ", are also important features  @xcite",
    ". we next show that the matrix @xmath13 is actually sensitive to these counts .",
    "[ theo : traingle ] given the adjacency matrix @xmath4 of an undirected graph with @xmath69 nodes and @xmath114 edges , we have + @xmath115 + where @xmath116 denotes the total number of triangles , @xmath117 is the total number of distinct _ simple paths _ of length 3 , @xmath118 is the total number of distinct _",
    "simple paths _ of length 2 and @xmath119 is the variance of degree .    from eq .",
    "( [ eq : cij ] ) , we have @xmath120}{[e^tae][e^ta^2e]}\\right ) - 1\\ ] ] the term @xmath121 $ ] is the sum of all elements of adjacency matrix @xmath4 , which is equal to twice the number of edges .",
    "so , @xmath122 we need to quantify other terms @xmath123 $ ] and @xmath124 $ ] .",
    "this quantification is provided in the two lemmas below .",
    "[ lem : pat2 ] @xmath125 = 2 m + 2p_2.\\ ] ]    we start with a simple observation that the value of @xmath126 is equal to the number of paths of length 2 between @xmath11 and @xmath12 . thus , @xmath127 $ ] , which is the sum of all the elements of @xmath128 , counts all possible paths of length 2 in the ( undirected ) graph twice",
    ". we should also have to count paths of length 2 with repeated nodes because undirected edges go both ways .",
    "there are two possible types of paths of length 2 as shown in figure  [ fig : path2 ] : i ) node repeated paths of length 2 and ii ) _ simple paths _ of length 2 having no node repetitions .",
    "node repeated paths of length 2 have only one possibility .",
    "it must be a loop of length 2 , which is just an edge as shown in figure  [ fig : path2](a ) .",
    "the total contribution of such node repeated paths ( or edges ) to @xmath123 $ ] is 2 m . by our notation ,",
    "the total number of _ simple paths _ of length 2 ( figure  [ fig : path2](b ) ) in the given graph is @xmath118",
    ". both sets of paths are disjoint .",
    "thus , we have @xmath123 = 2 m + 2p_2 $ ] as required .    [",
    "lem : pat3 ] @xmath129 = 6\\delta + 2p_3 + 2n(var(deg))+ 2m\\left(\\frac{4m}{n } -1\\right),\\\\\\notag & \\text{where } \\hspace{0.2 in } var(deg ) = \\frac{1}{n } \\sum_{i=1}^n deg(i)^2   - \\left(\\frac{1}{n}\\sum_{i = 1}^n deg(i)\\right)^2\\end{aligned}\\ ] ]    on similar lines as lemma  [ lem : pat2 ] , @xmath130 counts number of different paths of length 3 .",
    "there are 3 different kinds of paths of length 3 , as explained in figure  [ fig : path3 ] , which we need to consider .",
    "we can count the contribution from each of these types independently as their contributions do no overlap and so there is no double counting . again",
    "@xmath124 $ ] is twice the sum of the total number of all such paths .    * simple paths : * just like in lemma  [ lem : pat2 ] , any _ simple path _ without node repetition ( figure  [ fig : path3](c ) ) will be counted twice in the term @xmath124 $ ] .",
    "their total contribution to @xmath124 $ ] is @xmath131 .",
    "@xmath117 is the total number of _ simple paths _ with length 3 .",
    "* triangles : * a triangle is the only possible loop of length 3 in the graph and it is counted 6 times in the term @xmath124 $ ] .",
    "there are two orientations in which a triangle can be counted from each of the three participating nodes , causing a factor of 6 .",
    "for instance in figure  [ fig : path3](b ) , from node @xmath35 there are 2 loops of length 3 to itself , @xmath132 and @xmath133 .",
    "there are 2 such loop for each of the contributing nodes @xmath134 and @xmath135 .",
    "thus , if @xmath116 denotes the number of different triangles in the graph , then this type of structure will contribute @xmath136 to the term @xmath124 $ ] .",
    "* node repeated paths : * a peculiar set of paths of length 3 are generated because of an edge @xmath0 . in figure",
    "[ fig : path3](a ) , consider nodes @xmath35 and @xmath134 , there are many paths of length 3 with repeated nodes between @xmath35 and @xmath134 .",
    "to go from @xmath35 to @xmath134 , we can choose any of the neighbors of @xmath134 , say @xmath137 and then there is a corresponding path @xmath138 .",
    "we can also choose any neighbor of @xmath35 , say @xmath135 and we have a path @xmath139 of length 3 .",
    "thus , given an edge @xmath0 , the total number of node repeated paths of length 3 is @xmath140 note that the path @xmath141 , will be counted twice and therefore we subtract 1 .",
    "thus , the total contribution of these kinds of paths in the term @xmath124 $ ] is @xmath142 since the graph is undirected both @xmath143 , so we do not have to use a factor of 2 like we did in other cases .",
    "we have @xmath144 adding contributions of all possible types of paths and using @xmath145 yields lemma  [ lem : pat3 ] after some algebra .    substituting for the terms @xmath123 $ ] and @xmath124 $ ] in eq .",
    "( [ eq : c12 ] ) from lemmas  [ lem : pat2 ] and  [ lem : pat3 ] leads to the desired expression .",
    "* remarks on theorem  [ theo : traingle ] : * from its proof , it is clear that terms of the form @xmath146 $ ] , for small values of @xmath93 like 2 or 3 , are weighted combinations of counts of small sub - structures like triangles and small paths along with global features like degree variance . the key observation behind the proof is that @xmath147 counts paths ( with repeated nodes and edges ) of length @xmath93 , which in turn can be decomposed into disjoint structures over @xmath148 nodes and can be counted separately . extending",
    "this analysis for @xmath149 , involves dealing with more complicated bigger patterns .",
    "for instance , while computing the term @xmath150 $ ] , we will encounter counts of quadrilaterals along with more complex patterns . the representation",
    "@xmath13 is informative in that it captures all such information and is sensitive to the counts of these different substructures present in the graph .",
    "* empirical evidence for theorem  [ theo : traingle ] : * to empirically validate theorem  [ theo : traingle ] , we took publicly available twitter graphs , which consist of around 950 ego networks of users on twitter  @xcite .",
    "these graphs have around 130 nodes and 1700 edges on an average .",
    "we computed the value of @xmath151 for each graph ( and the mean and standard error ) .",
    "in addition , for each twitter graph , we also generated a corresponding random graph with same number of nodes and edges .",
    "to generate a random graph , we start with the required number of nodes and then select two nodes at random and add an edge between them .",
    "the process is repeated until the graph has the same number of edges as the twitter graph .",
    "we then compute the value of @xmath151 for all these generated random graphs .",
    "the mean ( @xmath152 standard error , se ) value of @xmath151 for twitter graphs is 0.6188 @xmath152 0.0099 , while for the random graphs this value is 0.0640 @xmath152 0.0033 .",
    "the mean ( @xmath152 se ) number of triangle for twitter ego network is 14384.16 @xmath152 819.39 , while that for random graphs is 4578.89 @xmath152 406.54 .",
    "it is known that social network graphs have a high value of _ triadic closure probability _ compared to random graphs  @xcite . for any 3 randomly chosen vertices a",
    ", b and c in the graph , triadic closure probability ( common friendships induce new friendships ) is a probability of having an edge ac conditional on the event that the graph already has edges ab and bc . social network graphs have more triangles compared to a random graph .",
    "thus , theorem  [ theo : traingle ] suggests that the value of @xmath151 would be high for a social network graph compared to a random graph with same number of nodes and edges .    combining theorems [ theo : cij ] and [ theo : traingle ]",
    ", we can infer that our proposed representation @xmath13 encodes important information to discriminate between different network structures .",
    "theorem  [ the : inv ] tells us that this object is a graph invariant and a covariance matrix in a fixed dimensional space .",
    "hence @xmath13 is directly comparable between different graph structures .",
    "given a fixed @xmath5 , we have a representation for graphs in a common mathematical space , the space of symmetric positive semidefinite matrices @xmath153 , whose mathematical properties are well understood . in particular , there are standard notions of similarity between such matrices .",
    "we define similarity between two graphs , with adjacency matrices @xmath154 and @xmath155 respectively , as the bhattacharya similarity between corresponding covariance matrices @xmath13 and @xmath156 respectively : @xmath157 here , @xmath158 is the determinant .",
    "note that @xmath78 and @xmath159 are computed using the same value of @xmath5 .",
    "we summarize the procedure of computing similarity between two graphs with adjacency matrices @xmath4 and @xmath160 in algorithm  [ alg : sim ] .",
    "adjacency matrices @xmath161 and @xmath155 , @xmath5 , the number of power iterations .",
    "@xmath162 @xmath163 @xmath164 computed using eq .",
    "( [ eq : kernel ] )    the similarity @xmath164 , defined between graphs with adjacency matrices @xmath4 and @xmath160 , is positive semidefinite and is a valid kernel .",
    "this similarity is positive semidefinite , which follows from the fact that the bhattacharya similarity is positive semidefinite .",
    "thus , the similarity function defined in eq .",
    "( [ eq : kernel ] ) is a valid kernel  @xcite and hence can be directly used in existing machine learning algorithms operating over kernels such as svm .",
    "we will see performance of this kernel on the task of social network classification later in section  [ sec : exp ] .",
    "although @xmath13 is determined by the spectrum of adjacency matrix @xmath4 , we will see in section  [ sec : result ] , that simply taking a feature vector of graph invariants such as eigenvalues and computing the vector inner products is not the right way to compute similarity between graphs .",
    "it is crucial to consider the fact that we are working in the space of positive semidefinite covariance matrices and a similarity measure should utilize the mathematical structure of the space under consideration .    [",
    "cols=\"^,^,^,^,^,<\",options=\"header \" , ]     [ tab : runtime ]    however , even with sampling , subfreq-5 is an order of magnitude slower . to understand this ,",
    "let us review the process of computing the histogram by counting subgraphs .",
    "there are 34 graph structures over 5 nodes unique up to isomorphism .",
    "each of these 34 structures has @xmath165 many isomorphic variants ( one for every permutation ) . to compute a histogram over these 34 structures , we first sample an induced 5-subgraph from the given graph .",
    "the next step is to match this subgraph to one of the 34 structures .",
    "this requires determining which of the 34 graphs is isomorphic with the given sampled subgraph .",
    "the process is repeated 1000 times for every sample .",
    "thus every sampling step requires solving graph isomorphism problem .",
    "even subfreq-4 has the same problem but there are only 11 possible subgraphs and the number of isomorphic structures for each graph is only @xmath166 , which is still efficient .",
    "this scenario starts becoming intractable as we go beyond 5 because of the combinatorially hard graph isomorphism problem .",
    "subfreq-5 , although it is computationally very expensive , improves over subfreq-4 .",
    "the proposed similarity based on @xmath13 is almost as cheap as subfreq-4 but performs better than even subfreq-5 . counting based approaches ,",
    "although they capture information , quickly loose tractability once we start counting bigger substructures .",
    "power iteration of the adjacency matrix is a nice and computationally efficient way of capturing information about the underlying graph .",
    "we embed graphs into a new mathematical space , the space of symmetric positive semidefinite matrices @xmath153 .",
    "we take an altogether different approach of characterizing graphs based on the covariance matrix of the vectors obtained from the power iteration of the adjacency matrix .",
    "our analysis indicates that the proposed matrix representation @xmath13 contains most of the important characteristic information about the networks structure .",
    "since the @xmath13 representation is a covariance matrix in a fixed dimensional space , it naturally gives a measure of similarity ( or distance ) between different graphs .",
    "the overall procedure is simple and scalable in that it can be computed in time linear in number of edges .",
    "experimental evaluations demonstrate the superiority of the @xmath13 representation , over other state - of - the - art methods , in ego network classification tasks . running time comparisons",
    "indicate that the proposed approach provides the right balance between the expressiveness of representation and the computational tractability .",
    "finding tractable and meaningful representations of graph is a fundamental problem , we believe our results as shown will provide motivation for using the new representation in analyzing real networks ."
  ],
  "abstract_text": [
    "<S> finding a new mathematical representations for graph , which allows direct comparison between different graph structures , is an open - ended research direction . </S>",
    "<S> having such a representation is the first prerequisite for a variety of machine learning algorithms like classification , clustering , etc . </S>",
    "<S> , over graph datasets . in this paper </S>",
    "<S> , we propose a symmetric positive semidefinite matrix with the @xmath0-th entry equal to the covariance between normalized vectors @xmath1 and @xmath2 ( @xmath3 being vector of all ones ) as a representation for graph with adjacency matrix @xmath4 . </S>",
    "<S> we show that the proposed matrix representation encodes the spectrum of the underlying adjacency matrix and it also contains information about the counts of small sub - structures present in the graph such as triangles and small paths . </S>",
    "<S> in addition , we show that this matrix is a _  </S>",
    "<S> graph invariant\"_. all these properties make the proposed matrix a suitable object for representing graphs .    the representation </S>",
    "<S> , being a covariance matrix in a fixed dimensional metric space , gives a mathematical embedding for graphs . </S>",
    "<S> this naturally leads to a measure of similarity on graph objects . </S>",
    "<S> we define similarity between two given graphs as a bhattacharya similarity measure between their corresponding covariance matrix representations . </S>",
    "<S> as shown in our experimental study on the task of social network classification , such a similarity measure outperforms other widely used state - of - the - art methodologies . </S>",
    "<S> our proposed method is also computationally efficient . </S>",
    "<S> the computation of both the matrix representation and the similarity value can be performed in operations linear in the number of edges . </S>",
    "<S> this makes our method scalable in practice .    </S>",
    "<S> we believe our theoretical and empirical results provide evidence for studying truncated power iterations , of the adjacency matrix , to characterize social networks . </S>"
  ]
}