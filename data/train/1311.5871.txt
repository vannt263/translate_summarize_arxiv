{
  "article_text": [
    "when faced with an underdetermined system of equations , one typically applies a regularization strategy in order to recover well - posedness .",
    "the choice of regularization depends on the particular application at hand and should be made to drive the solution towards desired properties . in the absence of precise goals , the most popular choice favors solutions with minimum @xmath0-norm .",
    "however , an alternative becoming more and more popular is to search for sparse solutions ( which often have non - minimal @xmath0-norm ) . in the case of a system of linear equations",
    ", this alternative has been investigated in numerous works , see , e.g. , @xcite for a review , and entails many applications of great importance , particularly for signal processing where it goes under the name of compressed sensing / sampling @xcite .",
    "formally , finding the sparsest solutions of linear systems can be written as the minimization , under the constraints of the linear system , of the number of nonzero variables , which is a nonsmooth , nonconvex and np - hard problem .",
    "two main approaches can be distinguished to tackle such problems .",
    "the first one , known as basis pursuit ( bp ) , relies on a convex relaxation based on the minimization of an @xmath1-norm , while the second one applies a greedy strategy to add nonzero variables one - by - one .",
    "many results on the convergence of these methods to the sparsest solution are available in the literature @xcite .",
    "more recently , a few works introduced extensions of this problem to nonlinear equations . in particular , the first greedy approaches appeared in @xcite , while bp algorithms were developed in @xcite to find sparse solutions of systems of quadratic equations and in @xcite for more general nonlinear equations .",
    "formally , these problems can be formulated as @xmath2 where @xmath3 , @xmath4 are nonlinear functions and @xmath5 denotes the @xmath6-pseudo - norm of @xmath7 , i.e , the number of nonzero components @xmath8 .    here , we focus on the case where the @xmath9 s in   are polynomial functions of maximal degree @xmath10 : @xmath11 where @xmath12 is the set of @xmath13 multi - indexes with @xmath14 and @xmath15 are the corresponding monomials .",
    "this includes the particular case considered in @xcite with @xmath16 , while in @xcite this setting is used to deal with the more general case via the taylor expansions of the nonlinear functions @xmath9 .",
    "note that the formulation in   also entails cases outside of the quasi - linear setting considered in @xcite .",
    "the present paper proposes a new bp approach to solve  , which , in comparison with the previous works @xcite , is more simple and amounts to solving an easier optimization problem . more precisely , the method proposed in sect .",
    "[ sec : ell1 ] relies on a simple change of variable sufficient to result in an efficient algorithm implemented as a classical @xmath1-minimization problem .",
    "however , the structure of the polynomials is discarded and the solution may not satisfy the original polynomial constraints . note that this change of variable is also found in @xcite , though with constraints handled in a more complex manner , and closely related to the lifting technique of @xcite for quadratic bp and the one of @xcite for subspace clustering .",
    "the method in @xcite enforces the structure of the quadratic polynomials via rank constraints , which leads to optimization problems with additional levels of relaxation and the introduction of a parameter tuning the trade - off between the minimization of the @xmath1-norm on the one hand and the satisfaction of the rank constraint on the other hand . in @xcite , the structure of higher - degree polynomials",
    "is enforced by a set of quadratic constraints on the monomials , @xmath17 , @xmath18 , @xmath19 , then the structure of @xmath20 and @xmath21 is enforced by @xmath22 and @xmath23 . but note that since these constraints are relaxed in the final formulation , the estimation can yield @xmath24 , which then recursively implies that all monomial constraints involving @xmath20 are meaningless . ] , which are then relaxed as in @xcite .    instead , the method proposed in sect .",
    "[ sec : groupsparse ] implements the structural knowledge via group - sparsity .",
    "this results in an @xmath25-minimization , i.e. , a second - order cone program ( socp ) which can be solved more efficiently .",
    "in addition , this socp formulation is easily extended in sect .",
    "[ sec : weighting ] to benefit from reweighting techniques for improving the sparsity of the solution .",
    "the conditions for exact recovery of the proposed bp methods are analyzed in sect .",
    "[ sec : analysis ] .",
    "in particular , we show that though the simple @xmath1-minimization does not include structural constraints , exact recovery occurs for sufficiently sparse cases .",
    "a similar condition is proved for the @xmath25-minimization based on group - sparsity .",
    "the greedy approach is discussed in sect .",
    "[ sec : greedy ] , where two variants are proposed : an exact algorithm for solving the group - sparsity optimization problem in small - scale cases and an approximate one which remains efficient in higher - dimensional settings .",
    "previous greedy approaches @xcite considered the problem in its sparsity - constrained form , where the sum of squared errors over the equations is minimized subject to @xmath26 for an _ a priori _ fixed @xmath27 .",
    "the iterative hard thresholding of @xcite is a gradient descent algorithm with an additional projection onto the feasible set at each iteration via a simple thresholding operation . in @xcite ,",
    "this is interpreted as a fixed point iteration enforcing a necessary ( but not sufficient ) optimality condition , which requires a well - chosen step - size to converge satisfactorily .",
    "in particular , the step - size must be an upper bound on the lipschitz constant of the gradient of the objective function , which however is not ( globally ) lipschitz continuous for polynomials in with degree @xmath28 .",
    "the sparse - simplex algorithm of @xcite is a coordinate descent method which enjoys similar but less restrictive convergence properties while being parameter - free .",
    "however , for polynomial equations as in , each iteration requires solving several one - dimensional minimizations of a polynomial of degree @xmath29 , which becomes difficult for @xmath30 . on the contrary ,",
    "the proposed greedy algorithms remain simple thanks to the group - sparse and linearized formulation of the problem : each iteration requires only solving least - squares problems .",
    "extensions of the methods are discussed in sections  [ sec : purenl ] and  [ sec : noise ] .",
    "in particular , section  [ sec : purenl ] deals with the issue of purely nonlinear polynomials , for which the solution to  can not be estimated as directly as for polynomials involving linear monomials , but which arise in important applications such as phase retrieval @xcite .",
    "then , the case where the equations   are perturbed by noise is considered in sect .",
    "[ sec : noise ] .",
    "in particular , the analysis provides stable recovery results for polynomial bp denoising in the flavor of the one obtained in @xcite for linear bp denoising .    finally , numerical experiments in sect .",
    "[ sec : exp ] show the efficiency of the proposed methods and extensions .",
    "results are in line with the ones found in classical sparse optimization with linear constraints . in particular",
    ", all methods can recover the sparsest solution in sufficiently sparse cases and the greedy approach is the fastest while the bp methods based on convex relaxations benefit from a slightly higher probability of success .",
    "this section develops two basis pursuit approaches to find sparse solutions of systems of polynomial equations via the minimization of an @xmath1-norm for the first one and of a mixed @xmath25-norm for the second one .",
    "the methods are developed under the following assumption , which will be relaxed in sect .",
    "[ sec : purenl ] .",
    "[ ass : nozerocol1 ] for all @xmath31 such that @xmath32 for @xmath33 determined such that @xmath34 for @xmath35 and @xmath36 for @xmath37 .",
    "in particular , assumption  [ ass : nozerocol1 ] ensures that the polynomials include a linear part , or more precisely , that for any variable @xmath8 , @xmath38 , the monomial @xmath8 has a nonzero coefficient in at least one of the polynomials .",
    "we start by rewriting the constraints in  as @xmath39 where @xmath40^t$ ] and @xmath41 is a mapping that computes all the monomials of degree @xmath42 , @xmath43 , with @xmath44 variables .",
    "note that @xmath7 is embedded in @xmath45 as the components corresponding to the @xmath44 multi - indexes @xmath46 , @xmath38 , such that @xmath47 for @xmath35 and @xmath48 for @xmath49 . assuming that these are the first components of @xmath45 ,",
    "i.e. , @xmath50 , we also define the ( linear ) inverse mapping @xmath51 , such that @xmath52 , as @xmath53 where @xmath54 is the @xmath44-by-@xmath44 identity matrix .",
    "the high - dimensional lifting by @xmath55 allows us to recast  as a standard ( i.e. , linear ) problem in sparse optimization : @xmath56 where @xmath57^t$ ] , @xmath58^t$ ] and @xmath45 is replaced by an unstructured vector @xmath59 .",
    "this constitutes the first level of relaxation in the proposed approach , where the components of @xmath59 are not constrained to be interdependent monomials .",
    "while this yields a rather crude approximation , it will serve as the basis for the refined approach of sect .",
    "[ sec : groupsparse ] , where additional structure will be imposed on @xmath59 .",
    "the second level of relaxation comes from the bp approach , in which problems such as   are typically solved via the convex relaxation @xmath60 where @xmath61 , with @xmath62 the @xmath33th column of @xmath63 , is a diagonal matrix of precompensating weights .",
    "then , for polynomial bp , an estimate of the solution to  can be easily computed under assumption  [ ass : nozerocol1 ] as @xmath64 .",
    "problem   can be formulated as a linear program and solved by generic interior - point algorithms in polynomial time , while it can also be solved by more efficient and dedicated algorithms , see , e.g. , ( * ? ? ?",
    "* chapter  15 ) .",
    "however , the complexity with respect to @xmath44 remains exponential because of the dimension @xmath65 which scales exponentially with @xmath44 .",
    "the literature on basis pursuit and compressed sensing @xcite tells us that the sparser the solution to is , the more likely the convex relaxation is to yield its recovery . here , by construction we know that there is at least a very sparse vector @xmath66 satisfying the constraints : with @xmath67 the sparse solution to  , the sparsity level @xmath68 is better than @xmath69 , as stated by proposition  [ prop : phisparsity ] below .",
    "note that a better bound implying an increased level of sparsity depending on @xmath10 for very sparse cases is derived in appendix  [ app : phisparsity ] .",
    "[ prop : phisparsity ] let the mapping @xmath41 be defined as above .",
    "then , the vector @xmath45 is at least as sparse as the vector @xmath7 in the sense that the inequality @xmath70 holds for all @xmath71 .",
    "proposition  [ prop : phisparsity ] implies that if   has a sparse solution then so does  . to prove proposition  [ prop : phisparsity ] , we first need the following lemma .    [",
    "lem : binoms ] for all triplet @xmath72 such that @xmath73 , the inequality @xmath74 holds .    on the one hand , we have @xmath75 and a similar expression with @xmath76 instead of @xmath77 . on the other hand , with @xmath73 , we have @xmath78 which then yields the sought statement .",
    "we now give the proof of proposition  [ prop : phisparsity ] .    by construction",
    ", the number of nonzeros in @xmath45 is equal to the sum over @xmath42 , @xmath43 , of the number of monomials of degree @xmath42 in @xmath79 variables .",
    "this yields @xmath80 since @xmath81 , lemma  [ lem : binoms ] gives the bound @xmath82 from which the statement follows .",
    "the approach proposed above relies on a rather crude approximation .",
    "thus , the polynomial equations are not guaranteed to be satisfied by the solution @xmath83 , due to the factorization of the polynomial that may not hold for @xmath84 .",
    "in other words , the linearization in together with the direct optimization of @xmath59 discard the desired structure for @xmath59 . in practice",
    ", the solution @xmath83 can be checked a posteriori with @xmath85 , @xmath86 .",
    "but in order to increase the probability of obtaining a satisfactory solution , i.e. , one which satisfies the original polynomial constraints , we must embed the structure of the polynomial in the problem formulation .",
    "let us define the index sets @xmath87 then , the structural information we aim at embedding is given by the following implication : @xmath88 which formalizes the fact that whenever a variable is zero , all monomials involving this variable must be zero .",
    "such a structure can be favored via group - sparsity optimization , as detailed next .",
    "let us define the set of mappings @xmath89 , each computing the subset of components of @xmath55 involving the variable @xmath8 ( see appendix  [ app : m ] for the precise value of @xmath90 ) .",
    "note that these mappings are nonlinear in @xmath7 but linear in @xmath45 : @xmath91 , where @xmath92 is an @xmath93-by-@xmath65 binary matrix filled with zeros except for a 1 on each row at the @xmath33th column for all @xmath94 .",
    "further define the @xmath6-pseudo - norm of a vector - valued sequence as the number of nonzero vectors in the sequence : @xmath95 we call @xmath59 a group - sparse vector if @xmath96 is small . by construction and due to",
    ", a sparse @xmath7 leads to a group - sparse @xmath45 with @xmath97 therefore , in order to solve  we search for a group - sparse solution to the linear system @xmath98 .",
    "such group - sparse solutions can be found by solving @xmath99 where we replaced each @xmath92 by a weight matrix @xmath100 , with @xmath61 , without effect on the @xmath6-pseudo - norm .",
    "the reason for introducing these precompensating weights will become clear in the analysis of sect .",
    "[ sec : analysis ] .",
    "let us define the @xmath101-norm of a vector - valued sequence as @xmath102 problem   can be relaxed by replacing the @xmath6-pseudo - norm with an @xmath101-norm . in this paper we only consider the case @xmath103 and @xmath104 , i.e. , @xmath105 but other norms could be used , such as the @xmath106-norm .",
    "this yields the convex relaxation @xmath107 which is easily reformulated as a second order cone program ( socp ) that can be solved by generic software such as cvx @xcite or mosek @xcite ( more efficient dedicated solvers can also be found , e.g. , @xcite ) .",
    "however , as for the @xmath1-minimization  , the complexity remains exponential with respect to the number @xmath44 of base variables via the dimension @xmath65 .",
    "[ [ adding - structure - via - constraints . ] ] adding structure via constraints .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    all components of the mapping @xmath45 involving only even degrees of base variables @xmath8 are nonnegative .",
    "these structural constraints can help to drive the solution towards one that correctly factorizes and corresponds to a solution of the polynomial equations .",
    "this is obtained by solving @xmath108 this optimization problem has the same complexity as   since we simply added nonnegativity constraints to some variables .",
    "other forms of prior knowledge can be easily introduced in  .",
    "for instance , if ( tight ) box constraints on the base variables are available , then lower and upper bounds on all monomials can be derived . finally , note that these structural constraints can also be added to the @xmath1-minimization  .",
    "the following derives conditions of exact recovery of the sparse solution to the system of polynomial equations via various convex relaxations .",
    "these conditions are based on the mutual coherence of the matrix @xmath63 as defined e.g. in @xcite .",
    "[ def : mu ] the _ mutual coherence _ of a matrix @xmath109 $ ] is @xmath110    in order for the mutual coherence of @xmath63 to be defined , we focus on the case where the following assumption holds .",
    "[ ass : nozerocol ] all columns @xmath62 of the matrix @xmath63 are nonzero , i.e. , @xmath111 , @xmath112 , or , equivalently , for all @xmath113 such that the corresponding polynomial coefficient @xmath32 .",
    "assumption  [ ass : nozerocol ] is slightly more restrictive than assumption  [ ass : nozerocol1 ] , which only constrains the first @xmath44 columns of @xmath63 .",
    "if assumption  [ ass : nozerocol ] does not hold , the following analysis can be reproduced under assumption  [ ass : nozerocol1 ] by considering the submatrix @xmath114 containing the @xmath115 nonzero columns of @xmath63 and a similarly truncated mapping @xmath116 .",
    "adjustments then need to be made where numbers of columns are used , i.e. , by substituting @xmath117 and @xmath118 for @xmath65 and @xmath93 .",
    "however , for the case where both assumptions  [ ass : nozerocol1 ] and  [ ass : nozerocol ] do not hold , i.e. , of zero columns corresponding to base variables , @xmath119 , @xmath120 , @xmath83 can not be obtained by the inverse mapping @xmath121 .",
    "this particular case will be discussed in sect .  [",
    "sec : purenl ] .    note that whenever a condition requires the mutual coherence @xmath122 to be defined , assumption  [ ass : nozerocol ] implicitly holds .",
    "in such cases , assumption  [ ass : nozerocol ] will not be explicitly stated in the theorems below .",
    "the result below characterizes a case where the simple @xmath1-minimization method is sufficient to solve the sparse optimization problem .",
    "let @xmath67 denote the unique solution to . if the inequality @xmath123 holds , then the solution @xmath84 to is unique and equal to @xmath66 , thus providing @xmath124 .",
    "assume  has a unique solution @xmath67 .",
    "then , @xmath98 has a solution @xmath125 with a sparsity bounded by proposition  [ prop : phisparsity ] as @xmath126 but , according to theorem 7 in @xcite , if @xmath127 then , on the one hand , @xmath128 is the sparsest solution to @xmath129 , and on the other hand , it is also the unique solution to .",
    "thus , if holds , @xmath128 is the unique solution to both and , i.e. , @xmath130 .",
    "since @xmath83 is given by the first components of @xmath84 and @xmath67 by the ones of @xmath128 , this completes the proof .",
    "other less conservative conditions can be similarly obtained by considering the exact value of @xmath131 or tighter bounds ( see appendix  [ app : otherconditions ] ) , but these do not take the form of a simple inequality on @xmath132 .",
    "another condition for very sparse cases ( with @xmath133 ) can be similarly obtained by using proposition  [ prop : phisparsity2 ] in appendix  [ app : phisparsity ] instead of proposition  [ prop : phisparsity ] .      the first result below shows that the group - sparse problem   can be used as a proxy for the polynomial problem .    [ thm : grouppoly ] if the solution @xmath134 to is unique and yields @xmath135 such that @xmath136 is a solution to the system of polynomial equations  , then @xmath136 is the sparsest solution to the system of polynomial equations , i.e. , the unique solution to .",
    "assume there is an @xmath137 solution to the polynomial system   and at least as sparse as @xmath136 .",
    "then @xmath138 and @xmath139 which contradicts the fact that @xmath134 is the unique solution to unless @xmath140 and @xmath141 .",
    "the next result provides a condition on the sparsity of the solution to  under which it can be recovered by solving the convex problem   or its variant with nonnegativity constraints  .",
    "this result requires the following lemma .",
    "[ lem : bounddelta2 ] let @xmath109 $ ] be an @xmath142 matrix with mutual coherence @xmath122 as defined in definition  [ def : mu ] .",
    "let @xmath143 be the @xmath144-diagonal matrix of entries @xmath145 .",
    "then , for all @xmath146 and @xmath147 , the bound @xmath148 holds .    for all @xmath146",
    ", we have @xmath149 , which further implies @xmath150 and @xmath151 then , we have @xmath152 note that @xmath153 is a normalized matrix with ones on the diagonal and off diagonal @xmath154-entries equal to @xmath155 and bounded by @xmath122 via definition  [ def : mu ] .",
    "thus , we obtain @xmath156 and , by adding @xmath157 to both sides , the bound @xmath158 which is precisely  .",
    "[ thm : groupsparse ] let @xmath67 be a solution to the polynomial equations   and @xmath159 .",
    "if the condition @xmath160 where @xmath93 is the number of components of @xmath55 associated to a variable , holds , then @xmath128 is the unique solution to both   and  .",
    "the vector @xmath128 is the unique solution to if the inequality @xmath161 holds for all @xmath162 satisfying @xmath163 .",
    "the inequality above can be rewritten as @xmath164 where @xmath165 . by the triangle inequality , @xmath166 , this condition is met if @xmath167 or @xmath168    by defining @xmath169 as the set of indexes corresponding to nonzero columns of @xmath170 , lemma  [ lem : bounddelta2 ] yields @xmath171 where @xmath172 is the number of components of @xmath55 associated to a variable @xmath8 . due to the fact that @xmath173 , we also have @xmath174 which then leads to @xmath175 introducing this result in gives the condition @xmath176 finally , given that @xmath177 , this yields @xmath178 or , after dividing by @xmath179 and rearranging the terms , @xmath180 which can be rewritten as in the statement of the theorem .    it remains to prove that @xmath128 is also the unique solution to   in the case where this condition is satisfied and",
    "@xmath128 is the unique solution to  . to see this , note that @xmath125 is by definition a feasible point of  .",
    "in addition , the feasible set of   is included in the one of  , while the problems   and   share the same cost function .",
    "thus , if @xmath128 is the unique solution to  , there can not be another feasible @xmath181 with lower or equal cost function value for  .",
    "[ cor : uniqueness ] let @xmath67 be a solution to the polynomial equations   and @xmath93 be the number of components of @xmath55 associated to a variable .",
    "if the condition @xmath182 holds , then @xmath67 is the unique solution to the minimization problem   and it can be computed as @xmath183 with @xmath84 the solution to either   or  .",
    "assume there exists another solution @xmath184 to  , and thus with @xmath185 .",
    "then , theorem [ thm : groupsparse ] implies that both @xmath186 and @xmath66 are _ unique _ solutions to either   or  , and thus that @xmath187 ( where @xmath84 is similarly defined as the solution to either   or   in this case ) .",
    "but this contradicts the definition of the mapping @xmath55 implying @xmath188 whenever @xmath184 .",
    "therefore , the assumption @xmath184 can not hold and @xmath67 is the unique solution to  , while @xmath189 .",
    "[ thm : groupsparse2 ] let @xmath84 be a solution to and @xmath93 be the number of components of @xmath55 associated to a variable .",
    "if the condition @xmath190 holds , then @xmath84 is the unique solution to both   and .",
    "if , in addition , @xmath191 satisfies the polynomial constraints  , then @xmath83 is the unique solution to .    by following steps similar to those in the proof of theorem  [ thm : groupsparse ] with @xmath84 instead of @xmath128 , we obtain ( by replacing @xmath132 by @xmath192 in ) that @xmath84 is the unique solution to .",
    "then , let @xmath134 be a solution to .",
    "this implies @xmath193 and , under the condition of the theorem , @xmath194 by following similar steps again , we obtain that @xmath134 is also the unique solution to and thus that @xmath195 is the unique solution to .",
    "finally , since @xmath84 is the unique solution to , if @xmath83 satisfies the polynomial equations  , theorem  [ thm : grouppoly ] implies that @xmath83 is the solution to .    in comparison with theorem  [ thm : groupsparse ] , theorem  [ thm : groupsparse2 ] provides a condition that only depends on the estimate obtained by solving rather than on the sought solution .      in practice ,",
    "convex relaxations such as the ones described above provide a good step towards the solution but might fail to yield the exact solution with sufficient sparsity . in such cases , it is common practice to improve the sparsity of the solution by repeating the procedure with a well - chosen weighting of the variables as described in @xcite .",
    "these techniques can be directly applied to improve the @xmath1-minimization method of sect .",
    "[ sec : ell1 ] while they are adapted below to group - sparsity as considered in sect .",
    "[ sec : groupsparse ] .",
    "the classical reweighting scheme of @xcite for sparse recovery improves the sparsity of the solution by solving a sequence of linear programs .",
    "it can be adapted to the group - sparse recovery problem by iteratively solving @xmath196 with weights @xmath197 initially set to 1 and refined at each iteration by @xmath198 for a given small value of @xmath199 .",
    "the basic idea is to decrease the influence of groups of variables with large @xmath0-norms that are assumed to be nonzero in the solution while increasing the weight of groups with small norms in order to force them towards zero .",
    "the s@xmath1 m algorithm proposed in @xcite is another reweighted @xmath1-minimization mechanism which sets a single weight at zero at each iteration . though typically requiring more computation time than the previous approach due to a number of iterations equal to the number of non - zero elements",
    ", this algorithm can recover sparse solutions in cases where the classical reweighting scheme of @xcite fails .",
    "other advantages include the absence of a tuning parameter and the presence of a convergence analysis @xcite .",
    "the s@xmath200 m algorithm below is an adaptation of s@xmath1 m to group - sparse problems .    1 .",
    "initialize all weights @xmath201 , @xmath38 .",
    "2 .   solve the weighted group - sparse problem .",
    "3 .   find @xmath202 .",
    "4 .   set @xmath203 ( to relax the sparsity constraint on the @xmath33th group ) .",
    "repeat from step 2 until @xmath204 .",
    "note that in this algorithm , the number of iterations is equal to the number of nonzero groups , but if the correct sparsity pattern is recovered then the algorithm stops earlier .",
    "] , which , for polynomial basis pursuit , is @xmath132 and is typically small .",
    "this results in a fast and accurate method for polynomial basis pursuit , as will be seen in sect .",
    "[ sec : exp ] .",
    "as mentioned in the introduction , there are two major techniques to minimize an @xmath6-pseudo - norm : the bp approach and the greedy approach .",
    "we now consider the second one to solve problem  , and more particularly develop two variants of the greedy approach : the exact method and the approximate method .",
    "the exact method is intended for small - scale problems where the number of possible combinations of base variables remains small .",
    "the approximate method is designed to circumvent this limitation and applies to much larger problems .",
    "[ [ exact - greedy - algorithm . ] ] exact greedy algorithm .",
    "+ + + + + + + + + + + + + + + + + + + + + + +    the exact method is implemented as follows , where we let @xmath205 if the polynomial system of equations is assumed to be feasible , and @xmath206 otherwise .",
    "1 .   initialize : @xmath207 and @xmath208 .",
    "2 .   @xmath209 .",
    "3 .   for all combinations @xmath210 of @xmath211 variables among @xmath44 : 1 .   set @xmath212 , where the index sets @xmath213 are defined as in  .",
    "2 .   build the submatrix @xmath214 with the columns of @xmath63 with index in @xmath215 .",
    "3 .   solve @xmath216 4 .",
    "update @xmath217 .",
    "if @xmath218 , compute @xmath84 by setting its components of index in @xmath215 to the values in @xmath219 and the others to 0 .",
    "return @xmath64 .",
    "4 .   if @xmath220 , repeat from step 2 , otherwise return an infeasibility certificate .    in step 3.(a )",
    ", @xmath215 corresponds to the support of @xmath45 when @xmath221 , where @xmath210 is a combination of @xmath211 indexes from 1 to @xmath44 .",
    "the maximal number of least squares ( ls ) problems to solve in step 3.(c ) is @xmath222 .",
    "but many of these are spared by starting with the sparsest combinations and stopping as soon as a solution is found .",
    "thus , if a sparse solution @xmath67 with @xmath223 exists , only @xmath224 ls problems are solved . at iteration @xmath225 , the ls problem is of size @xmath226 , with @xmath227 .",
    "thus , assuming a complexity @xmath228 for an ls problem of size @xmath229 , the overall complexity of the algorithm scales as @xmath230 , which is exponential in @xmath44 , @xmath10 and @xmath132 .    with @xmath231",
    ", the exact greedy algorithm above can be slightly modified to compute all the solutions to  , simply by letting the _ for _ loop in step 3 complete instead of returning as soon as a solution is found . as a result , the algorithm could provide a uniqueness certificate for the solution of   from which theorem  [ thm : grouppoly ] could be applied to conclude that the solution coincides with the unique minimizer of  .    [ [ approximate - greedy - algorithm . ] ] approximate greedy algorithm .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the approximate method is similar except that it explores only a single branch of the tree of possible combinations .",
    "its implementation uses a set @xmath215 of retained variables ( more precisely , @xmath215 contains the indexes of these variables ) :    1 .",
    "initialize the set of nonzero variables : @xmath232 .",
    "2 .   for all @xmath233 , 1 .   set @xmath234 , where the index sets @xmath213 are defined as in  .",
    "2 .   build the submatrix @xmath235 with the columns of @xmath63 with index in @xmath236 .",
    "3 .   solve @xmath237 3 .",
    "select the variable that minimizes the error if added to @xmath215 : @xmath238 4 .",
    "update @xmath239 .",
    "5 .   repeat from step 2 until @xmath240",
    "compute @xmath84 by setting its components of index in @xmath241 to the values in @xmath242 and the others to 0 .",
    "return @xmath64 and the error @xmath243 .",
    "the algorithm starts with an empty set of nonzero variables @xmath215 and adds a single variable to that set at each iteration .",
    "the variable retained at a given iteration is the one that , if added , leads to the minimum sum of squared errors for the equations @xmath244 . in step 2.(a ) , @xmath236 corresponds to the support of @xmath45 when @xmath245 .",
    "note that the value of the minimizer @xmath242 is not retained but re - estimated at the next iteration .",
    "the reason for this is that there is no guarantee that the components of @xmath242 correspond to monomials of base variables .",
    "since one base variable is added at each iteration @xmath225 , the number of iterations equals the sparsity of the returned solution and @xmath246 . in this approximate variant of the algorithm",
    ", each iteration requires solving only @xmath247 ls problems , which yields a total number of ls problems equal to @xmath248 .",
    "then , bounding the complexity of each ls problem as for the exact greedy algorithm leads to an overall complexity bounded by @xmath249 .",
    "since we do not have a result on the convergence of the algorithm to the sparsest solution , we can not bound @xmath250 except by @xmath44 and the worst - case complexity is exponential in @xmath44 .",
    "however , in practice , if the algorithm finds a sparse solution @xmath83 , the complexity is exponential in its sparsity @xmath250 , but only linear in @xmath44 .",
    "this is rather promising given the empirical results to be shown in sect .",
    "[ sec : exp ] which suggest that this occurs with high probability .",
    "we now consider the case of purely nonlinear polynomials @xmath251 without a linear part for some variables , i.e. , with @xmath252 , @xmath86 , for some @xmath253 ( according to the ordering of the multi - indexes , the linear monomials correspond to the first coefficients with @xmath254 ) .",
    "for this specific case where assumption  [ ass : nozerocol1 ] does not hold , some of the first @xmath44 columns of @xmath63 are zero and the corresponding components of @xmath59 are unconstrained , thus set to arbitrary values in @xmath84 . as a result ,",
    "not only does the analysis of sect .",
    "[ sec : analysis ] not hold , but the estimate @xmath83 can not be directly obtained by the inverse mapping @xmath121 as the first components of @xmath84 .",
    "however , the core of the method remains applicable to purely nonlinear polynomials .",
    "more precisely , we can solve , or   ( or apply a reweighting scheme of sect .",
    "[ sec : weighting ] ) to obtain @xmath84 and the corresponding support of @xmath83 as @xmath255 while the greedy algorithms of sect .",
    "[ sec : greedy ] directly estimate @xmath256 .",
    "then , the estimate @xmath83 can be computed from the higher - degree monomials as , e.g. , @xmath257{\\hat{\\phi}_{jq}}$ ] , where the subscript @xmath258 denotes the index such that @xmath259 .",
    "the precise procedure to compute @xmath83 actually depends on the monomials involved in the polynomials .",
    "the most straightforward manner is to compute @xmath260 from the estimate of its smallest nonzero odd power : @xmath261 { \\hat{\\phi}_{j(2\\hat{p}+1)}},\\quad \\mbox{with } \\hat{p } = \\min_{p\\in\\{0,\\dots,(d-1)/2\\ } } p,\\ \\mbox{s.t.}\\ \\hat{\\phi}_{j(2p+1 ) } \\neq 0 .\\ ] ] but for polynomial systems that involve only monomials with even degrees , the minimization computing @xmath262 in has no solution and the procedure is slightly more complex .",
    "for instance , with purely quadratic equations , the absolute value of @xmath260 is given by @xmath263 and the sign must be determined by looking at the signs of the estimates of the bilinear terms @xmath264 , @xmath265 .",
    "in addition , note that for such cases , the solution of  is not unique for symmetry reasons and the method can not be analyzed as in sect .",
    "[ sec : analysis ] in terms of convergence towards the sparsest solution .",
    "then , a different notion of uniqueness is usually considered in the literature dedicated to purely quadratic equations @xcite .",
    "in many applications , the equations @xmath266 need to be relaxed to an error - tolerant form for various reasons , which can for instance be interpreted as having access to noisy measurements , @xmath267 , with unknown noise terms @xmath268 . in this case , we reformulate the general problem   as a denoising one : @xmath269 where @xmath270 is a fixed threshold on the noise @xmath271-norm , with typical choices for @xmath272 being @xmath103 , @xmath273 or @xmath274 .    with polynomial constraints , @xmath275 convex relaxations similar to the ones described in sect .",
    "[ sec : polybp ] can be derived to obtain polynomial bp denoising methods .",
    "this leads to solving @xmath276 for the @xmath1-minimization method and @xmath277 with @xmath25-minimization . for @xmath278 ,",
    "problem   remains a linear program , while @xmath273 leads to a socp .",
    "problem   can still be written as a socp for all @xmath279 and be solved by the generic solvers that apply to  .",
    "thus , the enhancements proposed in sect .  [ sec : polybp ] for the formulations and ( such as reweighting schemes or the addition of structural constraints ) can be easily transposed to the noisy case .    regarding the greedy algorithms of sect .",
    "[ sec : greedy ] , they are already applicable to the noisy case , for which it suffices to set an appropriate threshold @xmath280 on the noise @xmath0-norm .",
    "adaptations of these algorithms to @xmath103 or @xmath274 are straightforward , but require solving a convex optimization problem in sub - step ( c ) without a closed - form solution and thus without the same computational benefit for the approximate greedy approach .      in the noisy case ,",
    "the solution to   is in general not unique and the analysis focuses on stability rather than on exact recovery .",
    "the following theorem provides such a stability result for the @xmath1-minimization .",
    "let @xmath281 denote a solution to  for @xmath273 . if the inequality @xmath282 holds , then @xmath64 with @xmath84 the solution to for @xmath273 must obey @xmath283    assume  has a solution @xmath284 . then , @xmath285 with @xmath125 and @xmath286 . by proposition  [ prop : phisparsity ]",
    ", we have @xmath287 .",
    "but , according to theorem 9 in @xcite , if @xmath288 then @xmath84 must obey @xmath289 thus , if holds , so does   and @xmath290 $ ] . since @xmath291",
    ", this completes the proof .    for the @xmath25-minimization",
    ", we have the following stability result .",
    "[ thm : stabilitygroup ] let @xmath281 denote a solution to  for @xmath273 .",
    "if the inequality @xmath292 holds , then @xmath64 with @xmath84 the solution to for @xmath273 must obey @xmath293 where @xmath294 .",
    "assume  has a solution @xmath284 .",
    "let us define @xmath125 and @xmath295 . to prove the theorem , we will first bound from above the norm @xmath296 , where @xmath143 is the diagonal matrix of precompensating weights .",
    "this part of the proof follows a path similar to that of theorem  3.1 in @xcite , while adapting it to the group - sparse setting and mixed @xmath271/@xmath297 norms .    due to the definition of @xmath84 as a minimizer of",
    ", @xmath298 must satisfy either @xmath299 or @xmath300 , in which case the statement is obvious .",
    "the inequality above can be rewritten as @xmath301 where @xmath165 . by the triangle inequality , @xmath166",
    ", this implies @xmath302 in addition , @xmath298 must satisfy the constraints in as @xmath303 in which @xmath304 can be replaced by @xmath305 , leading to @xmath306 using @xmath307 , this implies @xmath308 , which further gives @xmath309 where we used @xmath310 and the fact that the diagonal entries of @xmath311 are zeros while off - diagonal entries are bounded from above by @xmath122 .    due to @xmath312",
    "being a vector with a subset of entries from @xmath313 , we have @xmath314 , @xmath38 , and thus @xmath315 since the groups defined by the @xmath170 s overlap , @xmath316 , and the squared @xmath1-norm in   can be bounded by @xmath317 introducing the bounds  in yields @xmath318 we will now use this inequality to derive an upper bound on @xmath319 , which will also apply to @xmath320 , since the groups overlap and the squared components of @xmath313 are summed multiple times in @xmath319 .",
    "to derive the upper bound , we first introduce a few notations : @xmath321 and @xmath322 ,      \\quad c_1 =   \\left(\\frac{\\|\\{{\\boldsymbol{w}}_j{\\boldsymbol{\\delta}}\\}_{j\\notin i_0}\\|_{2,2}}{\\|\\{{\\boldsymbol{w}}_j{\\boldsymbol{\\delta}}\\}_{j\\notin i_0}\\|_{1,2}}\\right)^2 \\in \\left[\\frac{1}{n - |i_0| } , 1\\right ] , \\ ] ] where the box bounds are obtained by classical relations between the @xmath1 and @xmath0 norms ( @xmath323 , @xmath324 ) . with these notations ,",
    "the term to bound is rewritten as @xmath325 while the inequality   becomes @xmath326 we further reformulate this constraint by letting @xmath327 : @xmath328 let @xmath329 .",
    "due to  , we have @xmath330 and thus @xmath331 , which , together with the bounds on @xmath332 and @xmath333 , gives the constraints @xmath334 . by setting @xmath335 ,",
    "is rewritten as @xmath336 where @xmath337 since @xmath338 and the positivity is ensured by the condition and the fact that @xmath339 .",
    "thus , @xmath340 and , since @xmath341 , is proved .",
    "this section evaluates the efficiency of the bp and greedy methods in terms of accuracy and computing time for the noiseless case in sect .  [",
    "sec : expnoiseless ] and the noisy case in sect .",
    "[ sec : expnoise ] . here , the definition of accuracy depends on the presence of noise in the equations , while the computing time refers to matlab implementations using mosek and cvx for the convex programs and running on a standard laptop ( except for times reported in fig .",
    "[ fig : time ] ) .",
    "the following methods are compared : the iterative hard thresholding ( iht ) algorithm @xcite as implemented by @xcite , the quadratic ( qbp ) and nonlinear ( nlbp ) bp methods of @xcite , the simple @xmath1-minimization ( @xmath1 m ) solving , the @xmath25-minimization ( @xmath200 m ) solving with its iteratively reweighted counterpart ( ir@xmath200 m ) using 10 iterations as defined in sect .",
    "[ sec : reweighted ] for @xmath342 , the selective @xmath25-minimization ( s@xmath200 m ) of sect .",
    "[ sec : sl1l2 m ] , and the exact ( ega ) and approximate ( aga ) greedy algorithms of sect .",
    "[ sec : greedy ] . for the noisy cases , error tolerant formulations of these methods as described in sect .",
    "[ sec : noise ] are used .",
    ".results on the example from @xcite with quadratic equations .",
    "[ tab : qbp ] [ cols= \" < , < , < , < , < , < , < , < , < \" , ]     [ [ influence - of - the - noise - level - varepsilon . ] ] influence of the noise level @xmath280 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the influence of the noise level @xmath343 on the performance of the proposed methods is evaluated by letting the value of @xmath280 vary between 1 and 10 , which corresponds to a signal - to - noise ratio decreased from 28 db to 9 db .",
    "for all methods except the @xmath1 m , the results plotted in fig .",
    "[ fig : noisevssnr ] indicate that , as expected , e.g. , from the bound  , the approximation error directly depends on the noise level .",
    "in fact , the mean relative error is almost linear with respect to @xmath280 , which shows that the dependence on @xmath280 in the bound   of theorem  [ thm : stabilitygroup ] is of the correct nature .",
    "but even more interestingly , the noise level does not influence the success rate corresponding to the estimation of the support of @xmath67 , thus providing evidence that the methods are robust to noise . here again , the @xmath1 m method , which does not include structural knowledge in its formulation , does not benefit from such satisfactory features .    .",
    "except for the @xmath1 m , the curves are hardly distinguishable and close to @xmath344 for the success rate.[fig : noisevssnr],title=\"fig : \" ] . except for the @xmath1 m ,",
    "the curves are hardly distinguishable and close to @xmath344 for the success rate.[fig : noisevssnr],title=\"fig : \" ]    [ [ influence - of - varepsilon - as - a - tuning - parameter . ] ] influence of @xmath280 as a tuning parameter .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in practical applications , the noise level might be unknown and @xmath280 becomes a tuning parameter .",
    "figure  [ fig : noisevsepsilon ] shows the influence of this parameter on the performance of the methods for a fixed noise level @xmath345 .",
    "all methods except the @xmath1 m perform very well with a slightly overestimated @xmath346 $ ] and , for @xmath280 within a reasonable range around @xmath347 ( @xmath348 $ ] ) , the methods still yield rather accurate estimates of @xmath67",
    ". significantly larger values of @xmath280 lead to an increase of the error for all methods except the greedy algorithms which maintain a mean relative error below @xmath349 for all values of @xmath350 . on the other hand ,",
    "the ir@xmath200 m method is the only one that is not badly affected by the underestimation of @xmath280 in terms of approximation error .    regarding the estimation of the support of @xmath67 , all methods fail with @xmath351 , while overestimating @xmath352 leads to perfect recovery even for much larger values of @xmath280 .",
    "this is in line with results on linear bp denoising , such as theorem  4.1 in @xcite which guarantees that the estimated support is a subset of the sought one for sufficiently sparse cases when using an overestimated @xmath280 .     with data perturbed by a noise of @xmath0-norm equal to 3 .",
    "except for @xmath1 m , the curves of the success rate are hardly distinguishable and close to @xmath344 for @xmath353 . [",
    "fig : noisevsepsilon],title=\"fig : \" ]   with data perturbed by a noise of @xmath0-norm equal to 3 . except for @xmath1 m ,",
    "the curves of the success rate are hardly distinguishable and close to @xmath344 for @xmath353 .",
    "[ fig : noisevsepsilon],title=\"fig : \" ]",
    "the paper proposed several methods for finding the sparsest solution of a system of polynomial equations .",
    "two generic approaches were considered , one based on convex relaxations and one on a greedy strategy . for the convex relaxations ,",
    "sufficient conditions of exact recovery of the sparsest solution were derived .",
    "the methods were also extended to deal with noisy equations , in which case stable recovery bounds for the convex relaxations were obtained .",
    "both the computational efficiency and the accuracy of the proposed methods were shown in numerical experiments , which also emphasized the relationship between the probability of success and the sparsity of the solution for each method .",
    "in addition , these results indicate that the proposed methods accurately recover the sought solution in many cases where the sufficient conditions do not hold . as in classical bp theory",
    ", these conditions suffer from an  excessive pessimism \" and are too restrictive due to their worst - case nature .",
    "remaining open issues include the convergence analysis of the greedy approximation towards the sparsest solution and the derivation of sufficient conditions for the solution of the group - sparsity optimization problem to satisfy the polynomial constraints .",
    "another question of interest is whether we can obtain less restrictive conditions for the variants of the convex relaxations with nonnegativity constraints such as  .",
    "future work will also consider applying the proposed methods to more general nonlinear equations , e.g. , by using taylor expansions as in @xcite .",
    "henrik ohlsson gratefully acknowledges support from the nsf project forces ( foundations of resilient cyber - physical systems ) , the swedish research council in the linnaeus center cadics , the european research council under the advanced grant learn , contract 267381 , a postdoctoral grant from the sweden - america foundation , donated by asea s fellowship fund , and a postdoctoral grant from the swedish research council .",
    "andersen , e.  d. and andersen , k.  d. ( 2000 ) .",
    "the mosek interior point optimizer for linear programming : an implementation of the homogeneous algorithm .",
    ", 33:197232 .",
    "balan , r. , casazza , p. , and edidin , d. ( 2006 ) . on signal reconstruction without phase .",
    ", 20:345356 .    , a.  s. , cahill , j. , mixon , d.  g. , and nelson , a.  a. ( 2014 ) . saving phase : injectivity and stability for phase retrieval .",
    ", 37(1):106125 .",
    "beck , a. and eldar , y.  c. ( 2013 ) .",
    "sparsity constrained nonlinear optimization : optimality conditions and algorithms .",
    ", 23(3):14801509 .",
    "blumensath , t. ( 2013 ) . compressed sensing with nonlinear observations and related nonlinear optimisation problems .",
    ", 59(6):34663474 .",
    "blumensath , t. and davies , m.  e. ( 2008 ) .",
    "gradient pursuit for non - linear sparse signal modelling . in _",
    "european signal processing conference ( eusipco ) _ , pages 2529 .",
    "bruckstein , a.  m. , donoho , d.  l. , and elad , m. ( 2009 ) . from sparse solutions of systems of equations to sparse modeling of signals and images .",
    ", 51(1):3481 .",
    "cands , e.  j. ( 2006 ) .",
    "compressive sampling . in _ proceedings of the international congress of mathematicians",
    ": invited lectures _ , pages 14331452 .",
    "cands , e.  j. , eldar , y.  c. , strohmer , t. , and voroninski , v. ( 2013a ) .",
    "phase retrieval via matrix completion .",
    ", 6(1):199225 .",
    "cands , e.  j. , strohmer , t. , and voroninski , v. ( 2013b ) .",
    "phaselift : exact and stable signal recovery from magnitude measurements via convex programming .",
    ", 66(8):12411274 .",
    "cands , e.  j. , wakin , m.  b. , and boyd , s.  p. ( 2008 ) . .",
    ", 14(5):877905 .",
    "deng , w. , yin , w. , and zhang , y. ( 2011 ) .",
    "group sparse optimization by alternating direction method .",
    "technical report tr11 - 06 , department of computational and applied mathematics , rice university .",
    "donoho , d.  l. ( 2006 ) . compressed sensing .",
    ", 52(4):12891306 .",
    "donoho , d.  l. , elad , m. , and temlyakov , v.  n. ( 2006 ) . stable recovery of sparse overcomplete representations in the presence of noise . , 52(1):618 .",
    "donoho , d.  l. and huo , x. ( 2001 ) .",
    "uncertainty principles and ideal atomic decomposition . , 47(7):28452862 .",
    "ehler , m. , fornasier , m. , and sigl , j. ( 2014 ) .",
    "quasi - linear compressed sensing .",
    ", 12(2):725754 .",
    "eldar , y.  c. and kutyniok , g. , editors ( 2012 ) .",
    ". cambridge university press .",
    "fienup , j. ( 1982 ) .",
    "phase retrieval algorithms : a comparison .",
    ", 21(15):27582769 .",
    "foucart , s. and rauhut , h. ( 2013 ) . .",
    "springer .",
    "gerchberg , r. and saxton , w. ( 1972 ) . a practical algorithm for the determination of phase from image and diffraction plane pictures .",
    ", 35:237246 .",
    "gonsalves , r. ( 1976 ) .",
    "phase retrieval from modulus data .",
    ", 66(9):961964 .",
    "grant , m. and boyd , s. ( 2008 ) .",
    "graph implementations for nonsmooth convex programs . in blondel , v. , boyd , s. , and kimura , h. , editors ,",
    "_ recent advances in learning and control _ , lecture notes in control and information sciences , pages 95110 .",
    "springer - verlag limited .",
    "http://stanford.edu/~boyd/graph_dcp.html .",
    "grant , m. and boyd , s. ( 2013 ) .",
    ": matlab software for disciplined convex programming , version 2.0 beta .",
    "http://cvxr.com/cvx .",
    "kohler , d. and mandel , l. ( 1973 ) . source reconstruction from the modulus of the correlation function : a practical approach to the phase problem of optical coherence theory .",
    ", 63(2):126134 .",
    "le , v.  l. , lauer , f. , and bloch , g. ( 2013 ) .",
    "selective @xmath1 minimization for sparse recovery . .",
    "( to appear ) .",
    "marchesini , s. ( 2007 ) . phase retrieval and saddle - point optimization .",
    ", 24(10):32893296 .",
    "ohlsson , h. and eldar , y.  c. ( 2013 ) . on conditions for uniqueness in sparse phase retrieval .",
    ", abs/1308.5447 .",
    "ohlsson , h. , yang , a.  y. , dong , r. , and sastry , s. ( 2013a ) .",
    "nonlinear basis pursuit . in _",
    "asilomar conf . on signals , systems and computers , pacific grove , ca , usa _ ,",
    "pages 315319 .",
    "ohlsson , h. , yang , a.  y. , dong , r. , verhaegen , m. , and sastry , s. ( 2013b ) .",
    "quadratic basis pursuit . .",
    "ranieri , j. , chebira , a. , lu , y.  m. , and vetterli , m. ( 2013 ) .",
    "phase retrieval for sparse signals : uniqueness conditions .",
    ", abs/1308.3058 .",
    "vidal , r. , ma , y. , and sastry , s. ( 2005 ) .",
    "generalized principal component analysis ( gpca ) .",
    ", 27(12):19451959 .",
    "[ lem : binoms2 ] for all @xmath354 such that @xmath355 , the inequality @xmath356 holds .    for @xmath357 , we can bound the terms in the sum as @xmath358 where we used @xmath355 in the second inequality",
    ". then @xmath359    [ prop : phisparsity2 ] let the mapping @xmath41 be defined as above . then , with @xmath360 and @xmath361 , the vector @xmath45 is sparser than the vector @xmath7 in the sense that the inequality @xmath362 holds for all @xmath71 .    by construction ,",
    "the number of nonzeros in @xmath45 is equal to the sum over @xmath42 , @xmath43 , of the number of monomials of degree @xmath42 in @xmath79 variables : @xmath363\\end{aligned}\\ ] ] the assumption @xmath361 implies that , then , the assumption of the proposition leads to @xmath364 and @xmath365 which is impossible since @xmath366 . ]",
    "@xmath367 . with @xmath367",
    ", we have @xmath368 which yields @xmath369 now , on the one hand we have @xmath370 and on the other hand , lemma  [ lem : binoms2 ] yields @xmath371 thus , @xmath362",
    "the following uses the exact value of @xmath131 .",
    "[ th:2 ] let @xmath67 denote the unique solution to . if the inequality @xmath372 holds , then the solution @xmath84 to is unique and equal to @xmath66 , thus providing @xmath373 .",
    "another more compact but slightly less tight result is as follows .",
    "let @xmath67 denote the unique solution to . if the inequality @xmath374 holds , then the solution @xmath84 to is unique and equal to @xmath66 , thus providing @xmath373 .    since the terms in the sum of theorem  [ th:2 ] form an increasing sequence , we have @xmath375 which yields the sought statement by application of theorem  [ th:2 ] .",
    "let us define @xmath93 as the number of monomials involving a base variable @xmath376 with a degree @xmath377 .",
    "it can be computed as the sum over @xmath42 , @xmath378 of the number of monomials of degree @xmath42 in @xmath379 variables times the remaining degree @xmath380 ( since each monomial in @xmath379 variables can be multiplied by @xmath376 or @xmath381 ... or @xmath382 to produce a monomial of degree at most @xmath10 in @xmath44 variables ) : @xmath383 another technique computes @xmath93 as the total number of all monomials minus the number of monomials not involving @xmath376 which is the number of monomials in @xmath379 variables : @xmath384 \\begin{pmatrix}n + q-2\\\\q\\end{pmatrix}\\\\      & = \\sum_{q=1}^{d } \\frac{q}{n-1 } \\begin{pmatrix}n + q-2\\\\q\\end{pmatrix } .\\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> the paper deals with the problem of finding sparse solutions to systems of polynomial equations possibly perturbed by noise . in particular , we show how these solutions can be recovered from group - sparse solutions of a derived system of linear equations . </S>",
    "<S> then , two approaches are considered to find these group - sparse solutions . the first one is based on a convex relaxation resulting in a second - order cone programming formulation which can benefit from efficient reweighting techniques for sparsity enhancement . for this approach , </S>",
    "<S> sufficient conditions for the exact recovery of the sparsest solution to the polynomial system are derived in the noiseless setting , while stable recovery results are obtained for the noisy case . </S>",
    "<S> though lacking a similar analysis , the second approach provides a more computationally efficient algorithm based on a greedy strategy adding the groups one - by - one . with respect to previous work , </S>",
    "<S> the proposed methods recover the sparsest solution in a very short computing time while remaining at least as accurate in terms of the probability of success . </S>",
    "<S> this probability is empirically analyzed to emphasize the relationship between the ability of the methods to solve the polynomial system and the sparsity of the solution . </S>"
  ]
}