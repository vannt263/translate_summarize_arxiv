{
  "article_text": [
    "peer review serves as an effective and scalable method for performance evaluation in systems where the products to evaluate significantly outnumber the dedicated reviewing experts .",
    "one example of such systems is massive open online courses ( moocs ) , where the number of students enrolled in a course is in the order of tens of thousands and by far exceeds the number of teaching assistants @xcite@xcite .",
    "another example is academic paper review , where the number of papers submitted to a journal by far exceeds the number of ( associate ) editors .",
    "since the proposed mechanism can be applied to general peer review settings , we keep the discussion in this paper general .",
    "peer review systems pose two key challenges .",
    "first , the reviewers have different _ intrinsic capabilities _ ( e.g. , their review quality functions , benefit and cost functions ) , which are _",
    "unknown_. hence , one challenge is how to identify their _ unknown _ intrinsic capabilities ; this is known in the game theory literature as the adverse selection problem .",
    "second , the reviewers can choose to exert different levels of ( costly ) _ effort _",
    "( e.g. , time and energy spent in reviewing ) , which is _",
    "unobservable_. hence , the other challenge is how to incentivize reviewers to exert high effort ; this is known in the game theory literature as the moral hazard problem .",
    "a reviewer s ultimate review quality is determined by her intrinsic capabilities _ and _ effort .",
    "if the capabilities are unknown but the effort is observable ( i.e. pure adverse selection ) , there is hope to identify their capabilities through mechanism design .",
    "if the effort is unobservable but the capabilities are known ( i.e. pure adverse selection ) , there is hope to incentivize high effort through social norms .",
    "however , in the presence of both adverse selection and moral hazard , the problem becomes significantly more challenging .",
    "in fact , no existing work has addressed this problem systematically .    a natural candidate for solving",
    "the pure adverse selection problem is to use matching mechanisms @xcite@xcite .",
    "matching mechanisms aim to efficiently allocate resources ( e.g. , hospitals , or reviewers in our setting ) to agents ( e.g. , medical students ) or their products ( e.g. , assignments or papers in our setting ) .",
    "existing works on matching mechanisms assume that the quality of resources depend only on the types of the agents who provide and receive the resource , but not on the providers effort .",
    "in other words , there is no moral hazard problem . as a result ,",
    "they focus on _ one - shot _ interactions and design _ one - shot _ matching rules ( i.e. , each agent is matched only once ) .",
    "however , their assumption does not hold in peer review systems , where the review quality depends crucially on the reviewers effort .",
    "we prove that under one - shot matching rules , agents will behave myopically by choosing the lowest effort ( i.e. , free - riding ) , because their current effort does not affect future matches and their future payoffs .",
    "hence , the system performance ( in terms of the total review quality ) is the worst since one - shot matching does not address the moral hazard problem .",
    "one way to address the pure moral hazard problem is to use social norms @xcite , where a central agency assigns the agents with ratings that summarize their past behavior and recommends a `` norm '' ( i.e. , desired behavior ) that rewards agents with good ratings and punishes those with bad ratings . in this way ,",
    "the agents are incentivized to conform with the social norm ( e.g. , exert high effort in our setting ) , even when they are randomly matched to each other based on some _ exogenous _ matching rule .",
    "however , existing works on social norms assume that the agents are _ homogenous_. this assumption does not hold in peer review systems , because different reviewers have different intrinsic capabilities .",
    "ideally , the central agency should recommend different norms to agents of different capabilities ; in practice , it can not do this since the capabilities are unknown . in summary , existing works in social norms @xcite@xcite do not deal with the adverse selection problem that is present in peer review .",
    "this paper proposes the _ first _ mechanism to _ simultaneously _ solve the adverse selection and moral hazard problems in peer review .",
    "our proposed mechanism exploits the repeated interaction among agents ( i.e. , by submitting multiple assignments or papers over time ) , and assigns the agents with ratings , which are summaries of their past review quality .",
    "unlike the works on social norms @xcite@xcite , we do not recommend desired behavior ( i.e. , a social norm ) to the agents , because they have unknown , different capabilities and thus computing a recommended social norm is impossible .",
    "instead , we propose rules for _ repeated _ matching that _ endogenously _ depend on agents ratings . unlike existing one - shot @xcite@xcite or exogenous @xcite@xcite matching rules ,",
    "our proposed repeated endogenous matching rules provide strong incentives for agents to exert high effort , because the agents behaviors affect their future ratings and hence , their future matches and future payoffs .",
    "we provide design guidelines for endogenous matching rules that are easy to implement without knowledge of agents private information ( e.g. , their benefit , review quality , and cost functions ) , yet powerful enough to guide the system to desirable equilibria . in particular , in the equilibrium the agents find it in their self - interest to exert high effort , and receive ratings that truly reflect their capabilities .",
    "we also provide case studies on specific matching rules with different reward / punishment schemes .",
    "we show that different reward / punishment schemes lead to different optimal matching rules , which stresses the importance of tailoring matching rules to reward / punishment schemes .",
    "simulation results demonstrate large performance improvement over existing matching rules .    in the following ,",
    "we discuss related works in section  [ sec : related ] .",
    "then we describe the model and formulate the design problem in section  [ sec : model ] .",
    "we study general matching rules in section  [ sec : convergence ] , and the baseline matching rule and its extensions in section  [ sec : design ] .",
    "section  [ sec : simulation ] demonstrates the efficiency of our proposed mechanisms .",
    "finally , section  [ sec : conclusion ] concludes the paper .",
    "the pure adverse selection problem is the focus of a huge literature on matching in resource allocation ( e.g. , allocation of schools to applicants @xcite@xcite ) and exchange ( e.g. , kidney exchange @xcite ) .",
    "these works ignore the moral hazard problem .",
    "in particular , they do not consider `` effort '' . once an agent ( e.g. , an applicant ) is matched to another ( e.g. , a school ) , the benefit ( obtained by this applicant ) is fixed .",
    "in contrast , in our work , the review quality depends crucially on the reviewer s effort .",
    "this additional moral hazard problem , when ignored , will significantly degrade the system performance ( in terms of the total review quality ) .    since these works @xcite@xcite ignore moral hazard , their matching rules are one - shot ( i.e. , match each agent only once ) .",
    "in contrast , our matching rules are repeated and changing over time based on agents ratings .",
    "hence , our matching rules can incentivize agents to exert high effort to obtain better ratings and thus , favorable future matches .",
    "the pure moral hazard problem has been studied in repeated game theory , where anonymous agents are randomly matched to interact with each other @xcite@xcite , as in our work . however , these works @xcite@xcite focus on the _ pure _ moral hazard problem , and ignore the adverse selection problem by assuming _ homogeneous _ agents . in this work ,",
    "we assume _ heterogeneous _ agents and deal with both the moral hazard and adverse selection problems . in @xcite@xcite ,",
    "due to the homogeneity of agents , binary ratings are usually sufficient to identify whether a player has behaved well or badly .",
    "in contrast , in this work the rating is continuous , such that the rating mechanism can identify not only whether a player has behaved well or badly , but also its review quality .",
    "another key difference is that we _",
    "design _ matching rules that _ endogenously _ depend on agents ratings and directly affect their incentives , while the matching rules in @xcite@xcite are _ fixed _ and _ exogenously given_. in our setting , we will prove that the latter type of matching rules will result in the lowest review quality in the equilibrium .",
    "there are other works on peer review systems but with different problems to solve .",
    "for example , there are works focusing on how to aggregate reviewers scores / ratings to obtain a final score / rating that accurately reflects the true quality of the assignments ( in moocs @xcite@xcite ) , the proposals ( in nsf proposal reviewing @xcite ) , or the papers ( in academic peer review @xcite ) .",
    "in contrast , our focus is to incentivize reviewers to exert high effort levels .",
    "consider a peer review system with a set @xmath0 of @xmath1 agents .",
    "each agent has its products reviewed by the other agents .",
    "an agent benefits from the review by its reviewer , and exerts effort in reviewing others products . a designer ( e.g.",
    ", the instructor in moocs ) aims to design a mechanism that incentivizes the reviewers to produce high - quality reviews .",
    "the mechanism includes two parts : _",
    "( i ) _ the rating mechanism that assigns and updates a rating @xmath2 for each agent @xmath3 , and _",
    "( ii ) _ the matching rule that matches agents with reviewers ( possibly based on the ratings ) . in the following ,",
    "we write the rating profile , namely the ratings of every agent , as @xmath4 .",
    "the rating profile is known only to the designer .",
    "we define the * rating distribution * , denoted by a vector @xmath5 , as the ordered ( from high to low ) list of all the ratings .",
    "the rating distribution @xmath5 does not count multiple agents with the same rating .",
    "for example , if the rating profile is @xmath6 , the rating distribution will be @xmath7 .",
    "write @xmath8 as the number of distinct ratings in @xmath9 ( i.e. , the dimension of the vector @xmath5 ) , and @xmath10 as agent @xmath3 s ranking ( i.e. , ordered position ) in the rating distribution . in the above example",
    ", we have @xmath11 , @xmath12 .",
    "although @xmath8 and @xmath10 depend on @xmath9 , we write them simply as @xmath8 and @xmath10 for notational simplicity without causing confusion . denote the @xmath10th element of the rating distribution by @xmath13 .",
    "then we have @xmath14 . finally , notice that the rating distribution does not disclose any information about the identities of the agents .",
    "note , importantly , that an agent s rating indicates its review quality , not the quality of its product .",
    "time is slotted into @xmath15 . in each time slot @xmath16 , the entities in the system moves in the following order : on a function refers to the derivative , and the superscript @xmath17 refers to the variable under consideration at time point @xmath18 . ]    * the designer publishes the rating distribution @xmath5 , and informs agent @xmath3 of its rating @xmath19 and its ranking @xmath10 . *",
    "each agent submits its product to review .",
    "* the designer matches each agent @xmath3 s product to other agent(s ) for review based on a probabilistic * matching rule * @xmath20 $ ] .",
    "the matching rule determines the probability @xmath21 that the agent with the @xmath10th highest rating is matched to the reviewer with the @xmath22th highest rating . from the definition",
    "we can see that the matching does not depend on agents identities . *",
    "each reviewer @xmath23 exerts an * effort level * @xmath24 $ ] , where @xmath25 is @xmath23 s maximum effort level .",
    "reviewer @xmath23 s * review quality * then depends on its effort as @xmath26 , where @xmath27 is the review quality function . *",
    "each agent @xmath3 receives * benefit * @xmath28 from reviewer @xmath23 s review , where @xmath29 is @xmath3 s benefit function , and incurs a * cost * of @xmath30 for reviewing a product , where @xmath31 is @xmath3 s cost function . *",
    "each agent @xmath3 sends a * report * @xmath32 about the reviewer to the designer .",
    "we assume that the report accurately reflects the reviewer s review quality , namely @xmath33 . for examples , in moocs",
    "the report can be made accurate by comparing the grading with the true answers to the assignments ( posted after the submission of the assignments ) . *",
    "the designer updates the agents ratings according to the * rating update rule * @xmath34 . for fairness ,",
    "the rating update rule is identical for all reviewers , and is given by a convex combination of the reviewer s old rating and the report about its review quality with a constant step size @xmath35 : s review quality @xmath36 is perfectly observed , it does not matter how many reports about @xmath23 s review quality are received . ]",
    "@xmath37    we make the following remarks on the agents ratings .",
    "each agent @xmath3 has a maximum effort level @xmath38 , and hence has a maximum review quality @xmath39 . since the new rating is the convex combination of the old rating and the review quality , given any initial rating @xmath40 , agent @xmath3 s rating can only be in the interval @xmath41 $ ] , where @xmath42 . in other words , the possible ratings of each agent @xmath3",
    "are contained in the compact set @xmath43 $ ] .    throughout the paper ,",
    "we make the following reasonable and standard assumptions on the monotonicity , convexity , concavity , and differentiability of our functions .",
    "[ assumption : cost - reviewquality ] each agent @xmath3 s cost function @xmath44 , review quality function @xmath45 , and benefit function @xmath46 satisfy the following :    * the cost @xmath44 is strictly convex , strictly increasing , and twice continuously differentiable in effort @xmath47 .",
    "in addition , @xmath48 . *",
    "the review quality @xmath45 is concave , strictly increasing , and twice continuously differentiable in effort @xmath47 .",
    "in addition , @xmath49 exists and is bounded . *",
    "the benefit @xmath46 is strictly increasing , concave , and continuously differentiable in review quality @xmath50 .",
    "in addition , @xmath51 exists and is bounded . *",
    "we normalize @xmath52 , @xmath53 , and @xmath54 .",
    "the designer receives reports @xmath55 of review quality , and keeps the rating @xmath56 for each agent @xmath3 at each time slot @xmath16 .",
    "hence , the designer knows the identity of the agent at the @xmath10th position of the rating distribution .",
    "however , it does not know the review quality functions @xmath45 , the benefit functions @xmath46 , or the cost functions @xmath44 .",
    "each agent @xmath3 knows its own review quality function @xmath45 , benefit function @xmath46 , and cost function @xmath44 , but does not know the above functions of the other agents .",
    "it knows the matching rule @xmath57 and the rating update rule @xmath58 .",
    "it also knows its own rating @xmath56 , the rating distribution @xmath59 , and its position in the rating distribution @xmath60 , but does not know the others ratings or the identity of its reviewer .      in each time slot @xmath16 ,",
    "agent @xmath3 s expected payoff is its expected benefit from the reviewing of its product minus the expected cost of reviewing other agents products .",
    "we write agent @xmath3 s expected payoff as @xmath61 , which depends on the matching rule @xmath57 , its own rating @xmath19 , the rating distribution @xmath5 , and all the agents effort levels @xmath62 .",
    "the _ expected payoff _ can be calculated as the expected benefit minus the expected cost : @xmath63 \\cdot c_i(e_i ) .",
    "\\end{aligned}\\ ] ]    each agent @xmath3 aims to choose a sequence of effort levels over time to maximize the _ discounted average of expected payoffs _",
    ", i.e. , to solve the _ dynamic _ optimization problem below : @xmath64 \\right\\}_{t=0}^\\infty }   \\!\\!\\!\\!\\mathbb{e } \\left\\ { ( 1-\\delta_i ) \\sum_{t=0}^\\infty \\delta_i^{t }   u_i\\left(m , \\theta_i^t , d(\\bm{\\theta}^t ) , e_i^t , \\bm{e}_{-i}^t\\right ) \\right\\},\\!\\!\\!\\!\\end{aligned}\\ ] ] where @xmath65 is the effort levels chosen by all the agents other than @xmath3 at time @xmath16 , and @xmath66 is agent @xmath3 s _ discount factor_. an agent s discount factor reflects its patience .",
    "we take the expectation @xmath67 because the rating update is random , namely an agent s rating is either updated or kept the same depending on whether it has reviewed a product .",
    "note that the optimization problem is very hard , if not impossible , to solve .",
    "the difficulty lies in the couplings of one agent s decisions over time and with other agents decisions .",
    "first , the agent s current decision ( i.e. , effort level ) affects not only its current payoff ( through the cost ) , but also its future ratings and hence future payoffs .",
    "second , the agent s payoff is affected by the others decisions ( through the benefit ) . however , since an agent has no knowledge about the others , it can not predict the others decisions and the evolution of rating distributions . in summary",
    ", an agent can not solve the optimization problem due to computational complexity and lack of knowledge .",
    "we propose a realistic behavioral model for the agents . to choose the optimal effort level at each time @xmath16 ,",
    "each agent @xmath3 holds a _ conjecture _ that its future value @xmath68 ( i.e. , its discounted average payoff after time @xmath16 ) is the following : @xmath69 where @xmath70 is the conjectured expected benefit of agent @xmath3 in time @xmath71 , assuming that the others ratings remain the same .",
    "we can calculate @xmath70 as @xmath72 , \\end{aligned}\\ ] ] where @xmath73 is the new rating distribution when @xmath3 s rating is updated to @xmath74 and the others ratings remain the same , and @xmath75 is @xmath3 s new ranking of its new rating @xmath76 in the new rating distribution @xmath73 .",
    "note that @xmath3 can compute @xmath73 based on @xmath76 and @xmath77 , without knowing @xmath78 .",
    "each agent @xmath3 holds the conjecture for two reasons .",
    "first , it can not predict the others effort levels or future ratings .",
    "hence , it holds a conjecture that _",
    "the others ratings remain the same _ , and that the others ratings precisely reflect their review quality , namely @xmath79 .",
    "second , it conjectures that its future value is an _ affine function of its expected benefit_. for consistency , both of the above conjectures are required to be true in the equilibrium to be defined later .",
    "the coefficient @xmath80 reflects how `` optimistic '' an agent is about the rating mechanism .",
    "an agent with a larger @xmath80 `` believes in '' the rating mechanism more , because it anticipates a higher future value given the expected benefit .",
    "the coefficient @xmath81 is updated in each time slot by agent @xmath3 , such that the conjectured future value converges to the true future value in the equilibrium .    then at each time @xmath16 ,",
    "each agent @xmath3 simply solves the following static problem for its optimal effort level @xmath82 : @xmath83 } & & \\!\\!\\!\\!\\!\\!\\!\\ ! ( 1-\\delta_i ) \\cdot u_i\\left(m , \\theta_i^t , d(\\bm{\\theta}^t ) , e_i , \\bm{e}_{-i}\\right ) \\nonumber \\\\ & + & \\delta_i \\cdot f_i\\left(\\alpha_i,\\beta_i^t , \\theta_i^t , d(\\bm{\\theta}^t ) , e_i\\right).\\end{aligned}\\ ] ] note that the others current effort levels @xmath84 only affect the benefit term in the current payoff @xmath85 , which does not depend on @xmath3 s effort @xmath47 and can be considered as a constant .",
    "hence , each agent @xmath3 has all the information needed to solve the above static optimization problem .",
    "given any matching rule @xmath57 and any rating update rule @xmath58 , a conjectural equilibrium ( ce ) is a triple @xmath86 that satisfies :    * incentive compatibility constraints : for all @xmath87 , @xmath88 } & &   \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\ !",
    "( 1-\\delta_i ) \\cdot u_i\\left(m , \\theta_i^ * , d(\\bm{\\theta}^ * ) , e_i , \\bm{e}_{-i}^*\\right ) \\nonumber \\\\ & & \\!\\!\\!\\!\\!\\!\\!\\ ! \\!\\!\\!\\!\\!\\!\\!\\ !",
    "+ \\delta_i \\cdot f_i\\left(\\alpha_i,\\beta_i^ * , \\theta_i^ * , d(\\bm{\\theta}^ * ) , e_i\\right),\\end{aligned}\\ ] ] * stable and correct ratings : for all @xmath87 , @xmath89 , * consistent conjectures : for all @xmath87 , @xmath90    in the above definition , the incentive compatibility constraints ensure that the effort level @xmath91 is the best response of each agent @xmath3 . in other words , it will be in agent @xmath3 s self - interest to choose @xmath91 .",
    "a ce also requires that each agent s rating truly reflects its review quality at the equilibrium effort level @xmath91 , and hence each agent s rating is stable , namely @xmath92 . finally , a ce requires that each agent s conjecture about its future value is correct",
    ".    there may be many ces . as a designer ,",
    "it is desirable that the system will converge to a ce from any initial rating profile .",
    "the convergence is important , because the designer can distinguish the true review quality of the reviewers at the equilibrium .",
    "the choice of the matching rule plays an important role in ensuring the convergence to a ce . aside from convergence , certain ces are more desirable than others , as discussed in the next paragraphs .",
    "the designer s problem is to maximize the equilibrium review quality .",
    "we write the designer s objective as a function of the equilibrium review quality @xmath93 .",
    "then the designer problem can be defined as @xmath94    note that the designer does _ not _ maximize the social welfare ( i.e. , the total benefit minus cost of the agents ) , because it is more natural from the designer s perspective to maximize the total review quality .",
    "the designer of the peer review system may not care about the cost of reviewing ; in fact , it would like to elicit more effort from the reviewers , resulting in higher - quality reviews but higher costs .",
    "in this section , we consider general matching rules , and provide important guidelines for designing the matching rules . as discussed before",
    ", we would like to have a matching rule under which the system will converge to a ce from any initial rating profile under the best response dynamics . in this way",
    ", the designer can distinguish the true quality of the reviewers in the equilibrium . before discussing the properties of the matching rules that ensure the convergence , we first describe the best response dynamics .    at each time slot @xmath16 ,",
    "the best response dynamics consist of the following three updates : @xmath95 }   & ( 1-\\delta_i ) \\cdot u_i\\left(m , \\theta_i^t , d(\\bm{\\theta}^t ) , e_i , \\bm{e}_{-i}^{t}\\right )   \\nonumber \\\\ & & + \\delta \\cdot f_i\\left(\\alpha_i,\\beta_i^t , \\theta_i^t , d(\\bm{\\theta}^t ) , e_i\\right);\\end{aligned}\\ ] ]    @xmath96    @xmath97    the update of effort levels in and the update of ratings in are the same as and , respectively .",
    "they are rewritten here for the convenience of reference .",
    "when determining the effort level in , although the current payoff @xmath98 depends on the others effort levels @xmath65 , the current payoff can be separated into the benefit which depends only on the others effort @xmath65 , and the cost which depends only on agent @xmath3 s own effort @xmath47 .",
    "hence , when solving , agent @xmath3 can treat the benefit as a constant , and consider only the cost , which depends on its own effort level and is known to itself .    the update of the parameter @xmath99 in ensures that the conjectured future payoff equals to the current payoff , namely @xmath100 .",
    "when the system converges to a ce @xmath86 , we will have @xmath101 , which fulfills the third requirement of `` consistent conjectures '' in the definition of ce .",
    "next , we will provide the design guidelines on the matching rules , such that the above dynamics  always converge to a ce from any initial ratings .",
    "in fact , the design guideline is simple and intuitive : the matching rule should ensure that each agent s expected benefit is concave and increasing in its own rating .",
    "[ definition : rating - fairness ] a matching rule @xmath57 is _ desirable _ , if under any rating profile @xmath9 ,    * each agent @xmath3 s ( conjectured ) expected benefit from the reviewing of its product , namely @xmath102,\\ ] ] is concave and increasing in its own rating @xmath13 ; * each agent @xmath3 s expected number of products to review is positive and fixed , namely @xmath103    the requirements of concavity and monotonicity are very reasonable .",
    "the expected benefit should be increasing in one s rating , such that one has incentives to exert high effort levels to increase its rating .",
    "in addition , if the expected benefit is concave in one s rating , since the marginal benefit is decreasing , one will not dramatically increase its effort level , which facilitates the convergence .",
    "the requirement of a fixed number of products to review ensures the fairness among the reviewers across time .    despite the simplicity of the requirements for desirable matching rules , we are able to prove its convergence under proper rating update rules .    [",
    "theorem : convergence ] under any desirable matching rule , starting from any initial @xmath104 , there exists @xmath105 such that under any small step size @xmath106 $ ] in the rating update rule , the system will converge to a ce through updates .    see appendix  [ proof : convergence ] .",
    "we illustrate the theoretical results in this paper in fig .",
    "[ fig : illustrationofresults ] .",
    "theorem  [ theorem : convergence ] proves that any desirable matching rule ensures the convergence to a ce .",
    "however , it is difficult to prove the convergence to a particular ce .",
    "the reasons are that there is not a single unique ce and the asymptotically reached ce ( through our matching and update rules ) depends on the history of the system and thus the probabilistic matching assignments and the initial ratings . in our technical analysis",
    "we prove the convergence by showing that under the updates  , the difference ( in terms of @xmath107 - 1 norm ) between two consecutive rating profiles strictly decreases over time .",
    "however , since the best responses are different under different rating profiles , the mappings from the current rating profile to the next one are different over time .",
    "hence , the contraction mapping theorem does not apply here .",
    "in fact , in section  [ sec : design ] , we will show that under several desirable matching rules , there are _ indeed multiple ces _ , and the system converges to different ces under different initial ratings .    for the convergence ,",
    "we require the step size @xmath108 in the rating update rule to be small enough . however , we would like the step size to be as large as possible , subject to convergence , for two reasons .",
    "first , a larger step size results in a fast convergence of the ratings to the true review quality .",
    "second , perhaps less obviously , a larger step size provides higher incentives for agents to exert high effort levels .",
    "this is because with a larger step size , the influence of the current review quality is higher on the next rating . in appendix  [",
    "proof : convergence ] , we derive explicit upper bounds for the eligible step sizes .",
    "the matching rule is the critical component of our design . in this section",
    ", we will first prove that existing matching rules are inefficient .",
    "then we propose a baseline matching rule , and analyze the properties of this baseline matching rule in detail .",
    "finally , we propose and study two extensions .",
    "we show the inefficiency of existing matching rules . in existing matching rules ,",
    "the matching probabilities associated with agent @xmath3 , @xmath109 , do not depend on @xmath3 s ranking @xmath10 or its rating @xmath13 .",
    "this is true for existing matching rules in mechanism design @xcite@xcite , because there is no notion of effort and hence no rating .",
    "it is also true for existing matching rules in social norms @xcite@xcite , which are uniformly random .    under any matching rule that is independent of the rating of the agent",
    "whose product will be reviewed , namely @xmath110 , there is a unique ce , in which @xmath111 and @xmath112 for all @xmath3 .",
    "see appendix ii .",
    "the above proposition shows that the matching rule that does not depend on agents ratings is the worst - case matching rule that results in `` free - riding '' by everyone .",
    "this underlines the importance of designing efficient matching rules that take into account the ratings of both the reviewers and the agents whose products are reviewed .",
    "the baseline matching rule works as follows :    1 .   for the agents with the same rating , match their products among themselves using any one - to - one mapping that does not match one s product to itself .",
    "2 .   for any agent @xmath3 with a distinct rating ( i.e. , no other agent has the same rating ) , 1 .   if it has the highest rating ( i.e. , @xmath113 ) , match its product to a reviewer with the second highest rating with probability @xmath114 .",
    "2 .   if it has the lowest rating ( i.e. , @xmath115 ) , match its product to a reviewer with the second lowest rating with probability @xmath116 .",
    "hence , its product gets no review with probability @xmath117 .",
    "if @xmath118 , match its product to its two `` neighbors '' with the following probabilities ( which sum up to @xmath114 ) : @xmath119 and @xmath120    the above matching rule is illustrated in fig .",
    "[ fig : baselinematchingrule ] . agents with the same rating are matched to each other .",
    "for an agent with a distinct rating , it matches its product with its two nearest  neighbors \" with probabilities that depend on how close its rating is to its neighbors ratings .",
    "we propose this matching rule , because it has the following desirable properties :    * no agent will have to review more than 3 products .",
    "this is because any agent will at most review a product from an agent with the same rating ( if there is any ) , and two products from its neighbors ( if they have distinct ratings ) . *",
    "as we will prove later , this matching rule is a desirable matching rule as defined in definition [ definition : rating - fairness ] .      in the considered system",
    ", it is important to choose the initial ratings correctly , because under different initial ratings , the system may converge to different ces .",
    "since the designer has no knowledge about the agents at the beginning , it is reasonable to assign the same initial rating to all the agents for fairness . in this case , the following proposition tells us that we should not make the initial rating too low .",
    "[ proposition : initial_rating ] there always exists a rating @xmath121 , such that any initial rating profile with the same rating @xmath122 for all agents is the equilibrium rating profile , and that each agent @xmath3 chooses an equilibrium effort level @xmath91 such that @xmath123 .",
    "proposition [ proposition : initial_rating ] implies that we should choose a high enough initial rating . in particular , when the initial rating is too low , it is optimal to choose an effort level @xmath91 that satisfies @xmath123 .",
    "the key reason is that no agent has an incentive to reach a higher rating than the initial one , because in this case it will get a _ distinct _",
    "highest rating , and get the same benefit but a higher cost compared to choosing an effort level such that its rating remains the same as the initial rating . in other words ,",
    "the initial rating determines the highest review quality produced by each agent .",
    "it is useful to classify agents into types based on their cost , review quality , and benefit functions , etc .",
    "we define agents of a certain type as follows .    [",
    "definition : types ] the agents of the same _ type _ have the same normalized marginal benefit to cost ratio , defined as @xmath124 , the same review quality function @xmath45 , and the same marginal benefit function @xmath125 .",
    "[ definition : capability ] an agent @xmath3 is _ more capable _ than an agent @xmath23 , if @xmath126    definition  [ definition : types ] defines `` types '' of agents , in the sense that agents of the same type will always choose the same effort level and hence get the same rating .",
    "definition  [ definition : capability ] gives an ordering of agents in terms of their `` capability '' .",
    "we will prove that a more capable agent indeed gets a higher rating .",
    "in the rest of this section , we make the following assumption about the population size .    [",
    "assumption : population ] there is more than one agent of each type .",
    "assumption  [ assumption : population ] is reasonable in practice , since the number of agents in peer review systems is indeed large .",
    "given the same initial rating , the agents of the same type will choose the same best response effort level , and hence have the same rating .",
    "assumption  [ assumption : population ] ensures that for each agent , there is always another agent with the same rating .",
    "according to property 1 ) in the baseline matching rule , each agent will always have exactly one product to review all the time .",
    "[ theorem : convergence_baseline ] suppose that the large population assumption ( assumption  [ assumption : population ] ) holds .",
    "then we have    * the baseline matching rule is a desirable matching rule ; * starting from any initial rating profile , there exists @xmath105 such that under any small step size @xmath106 $ ] in the rating update rule , the system will converge to a ce through the best response dynamics  ; * if the agents have the same initial rating , at any point in the best response dynamics  , more capable agents will always have no lower ratings than less capable agents .",
    "see appendix ii .    theorem [ theorem : convergence_baseline ] ensures the convergence of the best response dynamics to a ce .",
    "in fact , we can say something stronger about the best response dynamics .",
    "that is , a more capable agent never has lower ratings than a less capable agent _ at any point _ in the best response dynamics .",
    "this means that the rating mechanism can distinguish the agents of different types , and rank them in the correct order .",
    "note that more capable agents produce reviews of high enough quality ( that result in higher ratings ) in their self - interest , as a result of maximizing their own payoffs ; they are not obliged to do so by the designer .",
    "previously , we have focused on the baseline matching rule .",
    "the baseline matching rule is able to incentivize the agents to exert high effort levels by increasing the benefit obtained by an agent when its rating increases .",
    "now we extend the baseline rule in two different ways , both of which result in a class of matching rules that allow us to tune the reward and/or punishment provided by the matching rules .    in the first extension ,",
    "we assign asymmetric probabilities for matching an agent with a distinct rating to its higher and lower neighbors . in particular , the _ asymmetric matching rule _ is parametrized by @xmath127 such that any agent @xmath3 with a distinct rating and with @xmath128 $ ] is matched to its neighbors with the following probabilities : @xmath129_0 ^ 1,\\end{aligned}\\ ] ] and @xmath130_0 ^ 1,\\end{aligned}\\ ] ] where @xmath131_0 ^ 1 \\triangleq \\min\\ { \\max\\{\\cdot , 0\\ } , 1\\}$ ] .",
    "we illustrate this asymmetric extension in fig .",
    "[ fig : matchingrule_extension1 ] .",
    "we can see that when @xmath132 ( @xmath133 ) , the resulting matching rule rewards ( punishes ) the agent by increasing its probability of being matched to the higher - rating ( lower - rating ) neighbor . when @xmath134 , the asymmetric matching rule reduces to the baseline matching rule .    in the second extension , we allow an agent to be matched to a reviewer with even higher or even lower ratings than its two nearest neighbors",
    ". in particular , the matching rule is parametrized by @xmath135 $ ] and @xmath136 $ ] .",
    "then any agent @xmath3 with a distinct rating and with @xmath137 $ ] is matched to its neighbors and neighbors of neighbors with the following probabilities : @xmath138 and @xmath139    we refer to this extension as _ long - range matching rule _ ; see fig .",
    "[ fig : matchingrule_extension2 ] for an illustration .",
    "we can see that the parameters @xmath140 and @xmath141 reflect to what extent the agents are rewarded and punished , respectively .",
    "when @xmath142 , the matching rule reduces to the baseline rule . when @xmath143 ( @xmath144 ) , the agent is rewarded ( punished ) by being matched to a reviewer with the next higher ( lower ) rating .",
    "we summarize the key differences among the baseline matching rule and its extensions in fig .",
    "[ fig : matchingrulecomparison ] .",
    "it is interesting to ask under each class of extended matching rules , which matching rule is optimal in terms of the equilibrium review quality ? we first define the notion that one matching rule is `` better '' than the other .",
    "we say that a matching rule @xmath145 is `` better '' than another matching rule @xmath57 , if for any equilibrium rating profile @xmath146 under @xmath57 , we can find an equilibrium rating profile @xmath147 under @xmath145 that satisfies @xmath148 .",
    "the following theorem tells us how to design an extended matching rule that is better than the baseline rule .",
    "[ theorem : extensions ] suppose that the large population assumption ( assumption  [ assumption : population ] ) holds .",
    "then we have :    * in the first asymetric extension , there exists a @xmath132 ( i.e. , extra reward ) under which the asymmetric matching rule is strictly better than the baseline matching rule . * in the second long - range extension , there exists @xmath149 and @xmath150 ( i.e. , extra punishment ) under which the long - range matching rule is strictly better than the baseline matching rule .",
    "theorem [ theorem : extensions ] tells us if we reward or punish by assigning higher or lower probabilities of being matched to the higher - rating neighbor , it is beneficial to reward . on the contrary , if we reward or punish by creating the possibility of being assigned to the next higher- or lower - rating neighbors , it is beneficial to punish .",
    "note that we can get the benefit only when we set the correct parameters in the two extended matching rules .",
    "the technical reason is that we want to increase the marginal expected benefit when an agent s rating is changed , in order to give more incentive for them to exert high effort levels .",
    "the main message delivered by our result is that we should carefully design the matching rule based on the way we reward and punish .",
    "we consider a system with 10 types of agents .",
    "there are 100 agents of each type .",
    "all the agents have the same patience @xmath151 , the same cost function @xmath152 , and the same benefit function @xmath153 .",
    "different types of agents have different quality functions @xmath154 with @xmath155 .",
    "they also have different @xmath156 in the conjecture functions .",
    "we show the best response dynamics under step sizes @xmath157 and @xmath158 in fig .",
    "[ fig : lowstepsize ] and fig .",
    "[ fig : highstepsize ] , respectively .",
    "note that we only show the ratings of agents of types @xmath159 .",
    "we first observe that under one - shot or exogenous matching rules , the reviewers exert 0 efforts and get 0 ratings all the time .",
    "hence , the proposed endogenous matching greatly improves the performance of the system .",
    "second , we can see that under a larger step size , the agents equilibrium ratings are higher , indicating higher equilibrium effort levels and higher equilibrium review quality .",
    "this is consistent with our intuition in remark 2 .",
    "note that , when @xmath160 , the best response dynamics do not converge ( type-1 agents ratings are oscillating ) .",
    "this stresses the importance of choosing the step size : we want to choose a step size as high as possible for better review quality , subject to the constraint that the best response dynamics converge .",
    "we compare the sum review quality and the social welfare ( i.e. , the total benefit minus cost ) at the equilibrium under different matching rules .    in table",
    "[ table : extension1 ] , we evaluate the first asymmetric extension of matching rules under different parameters @xmath127 .",
    "we can see that in our setting , the optimal @xmath127 should be 0.1 , which results in the highest sum review quality .",
    "this is consistent with our theoretical results : we can find a rewarding matching rule that outperforms the baseline rule .",
    "it is worth mentioning that the matching rule that maximizes the sum review quality may not be the one that maximizes the social welfare .",
    "this is reasonable because higher review quality also results in higher cost .",
    "in fact , in terms of social welfare , the optimal @xmath127 is -0.05 , which results in lower review quality and thus lower cost .",
    "how the review quality and the social welfare are aligned depends on the benefit and cost functions .    in table",
    "[ table : extension2 ] , we evaluate the second long - range extension of matching rules under different parameters @xmath140 and @xmath141 .",
    "we can see that the optimal sum review quality is achieved when @xmath149 and @xmath144 , which is a matching rule that punishes to the most severe extent .",
    "the threat of being matched to an even lower - rating reviewer provides more incentive for agents to exert high effort .",
    "again , such a matching rule does not result in the optimal social welfare .",
    "the optimal social welfare is achieved when @xmath161 and @xmath162 , where the agents are also rewarded .",
    "in this work , we proposed the first rating and repeated endogenous matching mechanisms to address the adverse selection and moral hazard problems simultaneously in peer review .",
    "our proposed rating and matching mechanisms are easy to implement , require no knowledge of agents private information , and ensure the convergence to an equilibrium in which the agents get their review quality revealed by their ratings and are incentivized to produce high - quality reviews .",
    "we thoroughly studied the design of matching rules , in terms of the initial ratings , the requirements for convergence , and the equilibrium ratings and review quality .",
    "we also studied extensions to different classes of matching rules , and proved the optimality of different rewarding / punishing mechanisms under different matching rules .    in future work",
    ", we will investigate the effect of inaccurate and possibly biased reports about the review quality , as well as more detailed modeling frameworks including multiple agents and reviewers associated with each product .",
    "an intriguing problem is the quest for the _ best _ matching rule maximizing the review quality .",
    ".equilibrium review quality and social welfare under the first asymmetric extension of matching rules .",
    "[ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]",
    "for ease of reference , we recall agent @xmath3 s best response @xmath163 in time slot @xmath16 here : @xmath164 } & & ( 1-\\delta_i ) \\cdot u_i(m , \\theta_i^t , d(\\bm{\\theta}^t ) , e_i , \\bm{e}_{-i}^t ) \\nonumber\\\\ & + & \\delta \\cdot f_i(\\alpha_i,\\beta_i^t , \\theta_i^t , d(\\bm{\\theta}^t ) , e_i ) .",
    "\\label{eq : best response}\\end{aligned}\\ ] ] we denote the best response function by @xmath165 .",
    "we need the following lemma summarizing the properties of the best response for future reference .",
    "[ lemma : properties of best response ] consider the current best response as defined in .",
    "the following statements hold :    1 .",
    "the best response is unique for every @xmath166 ; 2 .",
    "the best response function is a continuous and almost everywhere differentiable function ( uniformly in @xmath16 ) of @xmath167 ; 3 .   for any @xmath166",
    ", the best response function satisfies @xmath168 where @xmath169 is a continuous function of the step size @xmath108 that converges to zero as @xmath170 .    the current payoff @xmath171 in the current best response has two terms : @xmath172 where the first term , namely the expected current benefit , does not depend on agent @xmath3 s own effort .",
    "similarly , the conjecture function @xmath173 , where the second term @xmath99 does not depend on agent @xmath3 s own effort .",
    "hence , we can simplify the best response as @xmath174 } & & - ( 1-\\delta_i ) m c_i(e_i ) \\\\ & + & \\delta_i \\alpha_i    \\bar{b}_i\\left(\\theta_i^t , d(\\bm{\\theta}^t ) , e_i^t\\right ) .",
    "\\nonumber\\end{aligned}\\ ] ]    since the cost @xmath175 is strictly convex in @xmath47 , since the grading quality @xmath176 is concave in @xmath47 , and since the conjectured benefit is concave and increasing in @xmath177 , the objective function in is strictly concave in @xmath47 .",
    "thus , the current best response is unique for each @xmath166 .    with regards to statement 2 ) :",
    "due to strict concavity , the objective function in is almost everywhere differentiable with respect to @xmath47 and its derivative is equal almost everywhere to a decreasing function of @xmath47 ( * ? ? ?",
    "* exercise 1.6.42 ) . due to the monotone differentiation theorem ( * ? ? ?",
    "* theorem 1.6.25 ) , the derivative itself is almost everywhere differentiable with respect to @xmath47 as well .",
    "we concluded that the objective function in is almost everywhere twice differentiable with respect to @xmath47 .",
    "as a consequence , the unique best response @xmath163 is almost everywhere differentiable with respect to @xmath178 due to the implicit function theorem ( * ? ?",
    "* ch . 5 , theorem 2.1 ) .",
    "in addition , since the objective function is continuous ( any concave function is continuous ( * ? ? ? * ch . 6 , theorem 2.14 ) ) , and the feasible set does not depend on @xmath178 , the unique best response @xmath47 is continuous in @xmath178 according to berge s theorem of maximum ( * ? ? ?",
    "* ch . 7 , theorem 2.1 ) . in summary ,",
    "the best response @xmath47 is continuous and almost everywhere differentiable of @xmath178 .",
    "notice that all of these properties hold uniformly in @xmath16 .    with regards to statement 3 ) : instead of studying @xmath179 directly , we study a closely related function @xmath180 defined as @xmath181 note that the above optimization problem is almost the same as , except that the feasible set is @xmath182 instead of @xmath183 $ ] . in other words ,",
    "the new optimization problem is unconstrained .",
    "following the same logic , we know that @xmath184 is continuous and almost everywhere differentiable . moreover , since the optimization problem is unconstrained , we do not need to worry about the boundary of the feasible set .",
    "hence , @xmath185 is the unique solution to the first - order condition ( _ whenever the derivative exists _ ) :    @xmath186 , \\nonumber\\end{aligned}\\ ] ]    where @xmath187 .",
    "note that when @xmath188 , @xmath189 does not depend on @xmath76 .",
    "the best response function @xmath179 is related to @xmath184 in the following way : @xmath190_0^{e_i^{\\rm max}},\\ ] ] where @xmath131_0^{e_i^{\\rm max } } = \\min\\left\\{\\max\\left\\{\\cdot , 0\\right\\ } , e_i^{\\rm max } \\right\\}$ ] is the projection of a real number on the interval @xmath191 $ ] .",
    "this is because at any @xmath192 , due to strict concavity of the objective function , the first - order derivative is decreasing and hence must be positive for all @xmath193 . in other words ,",
    "the objective function is increasing in @xmath47 for all @xmath193 . if @xmath194 , then the objective function is increasing in @xmath47 for all @xmath195 . in this case , the maximum of the objective function in is taken at @xmath196 .",
    "similarly , @xmath197 if @xmath198 .",
    "@xmath199   \\right\\ } } { ( 1-\\delta_i ) m c_i^{\\prime\\prime}(\\hat{e}_i )   - \\mu \\delta_i \\alpha_i \\left\\ {   q_i^{\\prime\\prime}(\\hat{e}_i^t ) { \\displaystyle\\sum_{k \\neq k_i^+ } }   \\left [ \\frac{\\partial m_{k_i^+ k}(\\theta_i^{t+1 } , d_{i , k}^+)}{\\partial \\theta_i^{t+1 } } b_i(d_{i , k}^+ ) \\right ] + \\mu \\left [ q_i^{\\prime}(\\hat{e}_i^t ) \\right]^2",
    "\\frac{\\partial^2 m_{k_i^+ k_j } ( \\theta_i^{t+1 } , d_{i , k_j}^+ ) } { \\partial \\left[\\theta_i^{t+1}\\right]^2 } b_i(\\theta^t_j ) \\right\\ } } \\end{aligned}\\ ] ] @xmath200 ^ 2 } b_i ( d_{i , k}^+ ) \\right ] } { ( 1-\\delta_i ) m c_i^{\\prime\\prime}(\\hat{e}_i )   - \\mu \\delta_i \\alpha_i    \\sum_{k \\neq k_i^+ }   \\left [ q_i^{\\prime\\prime}(\\hat{e}_i^t ) \\frac{\\partial m_{k_i^+ k}(\\theta_i^{t+1 } , d_{i , k}^+)}{\\partial \\theta_i^{t+1 } } b_i(d_{i , k}^+ )   + \\mu \\left [ q_i^{\\prime}(\\hat{e}_i^t ) \\right]^2",
    "\\frac{\\partial^2 m_{k_i^+ k } ( \\theta_i^{t+1 } , d_{i , k}^+ ) } { \\partial \\left[\\theta_i^{t+1}\\right]^2 } b_i ( d_{i , k}^+ ) \\right ] } \\end{aligned}\\ ] ]    now we can study @xmath201 .",
    "in particular , we want to know how @xmath185 changes when @xmath192 changes .",
    "to this end , we apply the implicit function theorem to by taking the derivative of the right - hand side of with respect to @xmath202 and keeping in mind that @xmath203 is a function of @xmath202",
    ". then we obtain @xmath204 for @xmath205 in and @xmath206 in shown at the top of the next page , where @xmath207 and @xmath208 for @xmath209 .",
    "based on the above derivations , we prove the following important property about the best response function @xmath179 .",
    "first , we have : @xmath210_0^{e_i^{\\rm max } } - \\left [ \\hat{b}_i(\\bm{\\theta}^t ) \\right]_0^{e_i^{\\rm max } } \\right| \\\\ & \\leq & \\left| \\hat{b}_i(\\bm{\\theta}^{t+1 } ) - \\hat{b}_i(\\bm{\\theta}^t ) \\right|,\\end{aligned}\\ ] ] where the inequality holds because the difference between the projections of two numbers on the interval is no larger than the difference between the two numbers .",
    "next , we write @xmath211 as a `` telescoping sum '' of @xmath1 terms as follows : @xmath212,\\end{aligned}\\ ] ] where the @xmath23th term is the best response under the rating profile where the first @xmath23 agents have ratings in period @xmath71 , minus the best response under the rating profile where the first @xmath213 agents have ratings in period @xmath71 . as a result , we have : @xmath214 } \\textstyle\\left| \\frac{\\partial \\hat{b}_i\\left ( \\theta_1^{t+1},\\ldots,\\theta_{j-1}^{t+1},\\theta_j,\\theta_{j+1}^t,\\ldots,\\theta_n^t \\right)}{\\partial \\theta_j } \\right|,\\end{aligned}\\ ] ] where we take the essential supremum , since the derivative may not exist for all @xmath215 $ ] , but exists almost everywhere except on a subset of measure zero .",
    "we define @xmath216 where we write @xmath217 to emphasize the fact that @xmath218 is a function of the step size @xmath108 .",
    "we further define @xmath219    we can see from and that @xmath217 is continuous in @xmath108 and is @xmath220 when @xmath221 .",
    "hence , due to berge s theorem of maximum ( * ? ? ?",
    "7 , theorem 2.1 ) , @xmath222 is continuous in @xmath108 , and @xmath223 .",
    "similarly , @xmath224 is continuous in @xmath108 , and @xmath225 .",
    "therefore , we have @xmath226",
    "before we prove the convergence of the updates  , we show that _ if _ they converge to a triple @xmath86 , this triple @xmath86 is a ce .",
    "note that when the updates converge , each agent @xmath3 has a fixed rating @xmath227 , a fixed effort level @xmath228 , and a fixed payoff @xmath229 .",
    "first , the best response becomes @xmath88 } & &   \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\ !",
    "( 1-\\delta_i ) \\cdot u_i(m , \\theta_i^ * , d(\\bm{\\theta}^ * ) , e_i , \\bm{e}_{-i}^ * ) \\\\ & + & \\delta_i \\cdot f_i(\\alpha_i,\\beta_i^ * , \\theta_i^ * , d^ * , e_i),\\end{aligned}\\ ] ] which is exactly the first requirement of `` incentive compatibility '' in the definition of ce .",
    "second , the rating update ensures @xmath230 , which fulfills the second requirement of `` stable and correct rating '' in the definition of ce .",
    "finally , the update of @xmath231 in becomes @xmath232 rearranging terms and using the definition of the conjecture function , we have @xmath233 in other words , the conjecture is indeed equal to the true payoff , which fulfills the third requirement of `` correct conjectures '' in the definition of ce .",
    "next , we prove that the updates  _ do _ converge under a small enough step size @xmath108 in the rating update rule .",
    "consider a sequence of rating profiles @xmath234 generated by the updates . our goal is to prove that there exists a @xmath235 such that @xmath236 for all @xmath166 .",
    "if the above is true , the sequence @xmath234 will be a cauchy sequence in @xmath237 , and hence will converge .",
    "we prove the contraction property of the differences between consecutive rating profiles .",
    "note that the rating update rule is asynchronous .",
    "hence , agent @xmath3 s rating will not be updated if it did not grade an assignment .",
    "recall lemma [ lemma : properties of best response ] . for each agent @xmath3 , there are two cases :    * if there is rating update at time @xmath71 , we have @xmath238 where @xmath224 is as in lemma [ lemma : properties of best response ] . *",
    "if there is no rating update at time @xmath71 , we have @xmath239 , and hence @xmath240    hence , the above inequality holds despite the asynchronous rating update .",
    "moreover , since @xmath241 is concave and thus has a decreasing derivative , we have @xmath242    as a result , we have @xmath243 \\\\ & \\leq & ( 1-\\mu ) \\left\\|\\bm{\\theta}^t - \\bm{\\theta}^{t-1}\\right\\|_1 + \\mu \\cdot \\left(\\sum_{i=1}^n q_i^\\prime(0 ) l_i(\\mu)\\right ) \\cdot \\left\\|\\bm{\\theta}^t - \\bm{\\theta}^{t-1}\\right\\|_1 \\\\ & = & \\left [ ( 1-\\mu ) + \\mu \\cdot \\left(\\sum_{i=1}^n q_i^\\prime(0 ) l_i(\\mu)\\right ) \\right ] \\cdot \\left\\|\\bm{\\theta}^t - \\bm{\\theta}^{t-1}\\right\\|_1.\\end{aligned}\\ ] ] due to property 3 of lemma  [ lemma : properties of best response ] , we can find a small enough step size @xmath244 such that @xmath245 , and hence @xmath246 .",
    "_ proof of proposition 1 : _ we analyze the best response defined in . according to",
    ", the best response maximizes an objective function that consists of two terms . under a matching rule that is independent of the author s rating , the second term of the objective function in , namely @xmath247 is independent of @xmath3 s rating , and",
    "hence is independent of its effort level .",
    "hence , @xmath3 s best response is an effort level that maximizes the first term of the objective function , namely @xmath248 since each agent @xmath3 s cost is strictly increasing in its effort level , its best response is then @xmath249 under any rating profile @xmath192 .",
    "since student @xmath3 chooses @xmath249 at any time @xmath16 , its rating will converge to @xmath250 .",
    "@xmath251      based on property 1 ) of the matching rule , agent @xmath3 is matched to a reviewer with the same rating @xmath252 with probability @xmath114 .",
    "hence , agent @xmath3 incurs a current cost of @xmath253 by choosing effort @xmath47 .",
    "next , we look at agent @xmath3 s conjectured future payoff , which depends on the new rating @xmath254 when agent @xmath3 exerts effort @xmath47 .",
    "if @xmath255 , namely @xmath256 , based on properties 2)-a ) and 2)-c ) of the matching rule , agent @xmath3 is matched to a reviewer with rating @xmath252 with probability @xmath114 .",
    "if @xmath257 , namely @xmath258 , based on property 2)-b ) of the matching rule , @xmath3 is matched to a reviewer with rating @xmath252 with probability @xmath259 . in summary ,          to find the solution @xmath262 , we look at the derivative of the objective with respect to @xmath47 for any @xmath265 and @xmath266 : @xmath267 the left derivative of the objective with respect to @xmath47 when @xmath268 is @xmath269    since @xmath176 is strictly increasing in @xmath47 , @xmath270 is strictly increasing in @xmath252 .",
    "due to strict convexity of @xmath271 , @xmath272 is strictly increasing in @xmath252 . since @xmath241 is concave and strictly increasing in @xmath47",
    ", @xmath273 is positive and decreasing in @xmath252 .",
    "since @xmath274 is concave and strictly increasing in @xmath252 , @xmath275 is positive and decreasing in @xmath252 . as a result",
    ", @xmath276 is strictly decreasing in @xmath252 .",
    "since @xmath276 is strictly decreasing and continuous in @xmath252 , and since @xmath279 , there exists a small enough @xmath280 such that @xmath281 for any @xmath282 .",
    "we define @xmath283 .",
    "then for any @xmath122 , we have @xmath281 for all @xmath3 .    for any @xmath122 , due to strict concavity of the objective function ,",
    "the derivative of the objective with respect to @xmath47 , @xmath284 , is decreasing in @xmath47 and thus satisfies @xmath285 for all @xmath286 .",
    "therefore , each agent @xmath3 chooses    effort @xmath287 at time @xmath220 , and gets the same rating @xmath288 at time @xmath114 , under which it chooses the same effort @xmath289 . as a result",
    ", the system will stay the same .",
    "hence , the equilibrium rating is @xmath252 , and the equilibrium effort level is @xmath290 .",
    "@xmath251      _ claims 1 - 2 : _ first , we prove the first claim that the baseline matching rule is a desirable rule . under the large population assumption ( i.e. , assumption  [ assumption : population ] ) ,",
    "there are multiple agents of the same type .",
    "these agents of the same type will choose the same effort level and hence have the same rating . according to property 1 ) in the baseline matching rule",
    ", they will always have exactly one product ( from another student of the same rating ) to review , namely @xmath291 .",
    "it remains to show that the conjectured expected benefit is increasing and concave in the effort level . under the baseline matching rule ,",
    "the conjectured expected benefit can be explicitly computed as follows : @xmath292 \\\\ & = & \\frac{\\theta_i - d(\\bm{\\theta})_{k_i+1}}{d(\\bm{\\theta})_{k_i-1}-d(\\bm{\\theta})_{k_i+1 } } \\cdot b_i(d(\\bm{\\theta})_{k_i-1 } ) \\nonumber \\\\ & + & \\frac{d(\\bm{\\theta})_{k_i-1}-\\theta_i}{d(\\bm{\\theta})_{k_i-1}-d(\\bm{\\theta})_{k_i+1 } } \\cdot b_i(d(\\bm{\\theta})_{k_i+1 } ) \\nonumber \\\\ & = & \\frac{b_i(d(\\bm{\\theta})_{k_i-1})-b_i(d(\\bm{\\theta})_{k_i+1})}{d(\\bm{\\theta})_{k_i-1}-d(\\bm{\\theta})_{k_i+1 } } \\cdot \\theta_i \\nonumber \\\\ & + & \\frac{d(\\bm{\\theta})_{k_i-1 } \\cdot b_i(d(\\bm{\\theta})_{k_i+1})-d(\\bm{\\theta})_{k_i+1 } \\cdot b_i(d(\\bm{\\theta})_{k_i-1})}{d(\\bm{\\theta})_{k_i-1}-d(\\bm{\\theta})_{k_i+1}}. \\nonumber\\end{aligned}\\ ] ] we can see that the expected benefit is a piecewise linear function of @xmath19 . when @xmath3 is ranked at @xmath10 , the slope of the function is @xmath293 .",
    "since @xmath274 is an increasing function of @xmath294 , we have @xmath295 for all @xmath10 .",
    "hence , the expected benefit is increasing in @xmath19 .",
    "since @xmath274 is a concave and increasing function of @xmath294 , we have @xmath296 hence , the slope decreases when @xmath3 is ranked higher .",
    "therefore , the expected benefit is concave in @xmath19 .",
    "since @xmath297 is increasing and concave in @xmath47 , the expected benefit @xmath298\\ ] ] is increasing and concave in the effort level @xmath47 .",
    "_ claim 3 : _ now we prove the third claim that a more capable agent always gets no lower ratings than a less capable agent . more specifically , we will prove that as long as a more capable agent has a no lower current rating , he / she will exert effort high enough such that his / her next rating is no lower than that of a less capable agent . then under the same initial rating for all the agents , the third claim follows .",
    "our proof strategy is to first derive the sufficient and necessary conditions for the best response effort levels , and then show ( by contradiction ) that if a less capable agent exerts a high effort level that results in a next rating higher than that of a more capable agent , this high effort level violates the conditions and can not be the best response .",
    "the main technical difficulty arises since the best responses may be at the smooth or non - smooth points of the objective function , which results in different sufficient and necessary conditions and needs separate treatments .    _ sufficient and necessary conditions for best responses : _ first , we look at @xmath3 s best response at time @xmath16 : @xmath299 } - ( 1-\\delta_i ) c_i(e_i ) + \\\\",
    "\\delta_i \\alpha_i    \\sum_{k_j \\neq k_i }   m_{k_i k_j}\\left ( ( 1-\\mu ) \\theta_i^t + \\mu q_i(e_i ) , d(\\bm{\\theta}^t)_{k_j } \\right ) \\cdot b_i\\left(d(\\bm{\\theta}^t)_{k_j } \\right).\\end{gathered}\\ ] ] under the baseline rule , if the next rating @xmath300 lies between @xmath301 and @xmath302 for some @xmath303 , the expected benefit is calculated as : @xmath304 \\nonumber \\\\ & + & \\frac{d(\\bm{\\theta}^t)_{k } \\cdot b_i(d(\\bm{\\theta}^t)_{k+1})-d(\\bm{\\theta}^t)_{k+1 } \\cdot b_i(d(\\bm{\\theta}^t)_{k})}{d(\\bm{\\theta}^t)_{k}-d(\\bm{\\theta}^t)_{k+1}}. \\nonumber\\end{aligned}\\ ] ] removing all the terms that are unrelated to @xmath47 , the best response can be simplified into : @xmath305 } - ( 1-\\delta_i ) c_i(e_i ) \\\\ & & ~~~~~~~~ + \\delta_i \\alpha_i \\frac{b_i(d(\\bm{\\theta}^t)_{k})-b_i(d(\\bm{\\theta})_{k+1}^t)}{d(\\bm{\\theta}^t)_{k}-d(\\bm{\\theta}^t)_{k+1 } } \\cdot \\mu q_i(e_i ) .",
    "\\nonumber\\end{aligned}\\ ] ]    recall that the expected benefit is piecewise linear in @xmath56 .",
    "hence , the objective function in is a piecewise smooth function of @xmath47 .",
    "there are @xmath1 nonsmooth points where the effort @xmath47 satisfies @xmath306 for some @xmath307 .",
    "since the objective function is strictly concave , the best response @xmath163 should satisfy the following conditions .",
    "when @xmath163 results in a nonsmooth point , namely @xmath308 , the left derivative at @xmath163 must be nonnegative and the right derivative at @xmath163 must be nonpositive , i.e. : @xmath309 and @xmath310      _ proof by contradiction : _ now pick two arbitrary agents @xmath3 and @xmath23 with @xmath3 being more capable than @xmath23 . we prove that if @xmath313 , their best responses must satisfy @xmath314 we distinguish two cases depending on whether @xmath163 results in a smooth point or not .",
    "in each case , we further separate our discussions into two subcases depending on whether @xmath315 results in a smooth point or not .",
    "* case 1 : @xmath163 results in a smooth point , namely @xmath311 .",
    "then @xmath163 must satisfy , which is equivalent to @xmath316 suppose that @xmath23 chooses a best response @xmath315 such that @xmath317 since @xmath318 , and since @xmath319 , we must have @xmath320 , and hence @xmath321 . moreover , for any @xmath322 , we have @xmath323 which leads to @xmath324 for any @xmath322 . * * subcase 1 : if @xmath315 results in any smooth point @xmath325 with @xmath322 , then violates the condition .",
    "* * subcase 2 : if @xmath315 results in any nonsmooth point @xmath326 with @xmath322 , then violates the optimality condition . + in summary , in both subcases , @xmath315 can not be the best response , which leads to contradiction .",
    "hence , in case 1 we must have @xmath327 * case 2 : @xmath163 results in a nonsmooth point , namely @xmath328 .",
    "then @xmath163 must satisfy , which leads to @xmath329 suppose that @xmath23 chooses a best response @xmath315 such that @xmath317 then we must have @xmath320 .",
    "following the same logic that leads to , we have @xmath330 for any @xmath331 .",
    "+ similar to case 1 , since @xmath332 , we have two subcases : @xmath333 , and @xmath334 , where @xmath331 . since the inequality violates and , @xmath315 can not be the best response in either subcase .",
    "+ hence , in case 2 we must have @xmath314    we have shown that if a more capable agent has a current rating no lower than that of a less capable agent , then its next rating is no lower . under the same initial rating , more capable agents always have no lower ratings .",
    "@xmath251    _ proof of theorem  [ theorem : extensions ] : _ a rating profile @xmath146 is an equilibrium rating profile if each student @xmath3 s effort @xmath335 is the best response . under the baseline matching rule , we have derived the sufficient and necessary conditions for @xmath91 to be the best response in . hence , a rating profile @xmath146 and the associated effort profile @xmath336 with @xmath337 are an equilibrium rating profile and the associated equilibrium effort profile if and only if : when @xmath113 , @xmath338 when @xmath339 , @xmath340 @xmath341 and when @xmath342 , @xmath343    following the same procedure , we can show that under the asymmetric extension with @xmath132 , a rating profile @xmath344 and the associated effort profile @xmath345 with @xmath346 are an equilibrium rating profile and the associated equilibrium effort profile if and only if : when @xmath113 , @xmath347\\!\\!\\end{aligned}\\ ] ] when @xmath339 , @xmath348\\!\\!\\end{aligned}\\ ] ] @xmath349\\!\\!\\end{aligned}\\ ] ] and when @xmath342 , @xmath350\\!\\!\\end{aligned}\\ ] ]    observe that the defining inequalities for the baseline matching rule and those for the first extension are identical up to the additive term @xmath127 . for any equilibrium rating profile @xmath146 under the baseline rule @xmath57 , we can define @xmath351 , where @xmath352 is a small constant . in the following we show that a strictly positive and small enough @xmath352 leads to a strictly better equilibrium rating profile @xmath353 .    since @xmath354 and @xmath230 , and",
    "since @xmath241 is strictly increasing in @xmath47 , we have @xmath355 , where @xmath356 .",
    "since @xmath271 is convex and @xmath241 is concave , we have @xmath357 and @xmath358 . since @xmath274 is concave and increasing , we have @xmath359 for any @xmath360 .",
    "therefore , if the inequalities with `` @xmath361 '' , i.e. , and , hold for @xmath146 and @xmath336 , then the inequalities with `` @xmath361 '' , i.e. , and , hold for @xmath344 and @xmath345 as long as @xmath132 is small enough . for any such @xmath127 , if the inequalities with `` @xmath362 '' , i.e. , and , hold for @xmath146 and @xmath336 , then the inequalities with `` @xmath362 '' , i.e. , and hold for @xmath344 and @xmath345 as long as @xmath363 is small enough . in conclusion , we can find an equilibrium rating profile @xmath344 under the extended rule that satisfies @xmath353 .    under the second long - range extension with @xmath149 and @xmath150 , a rating profile",
    "@xmath344 is an equilibrium rating profile iff : @xmath364 , \\end{aligned}\\ ] ] when @xmath113 , @xmath364 , \\\\ & \\!\\!\\!\\!\\!\\ !",
    "0 \\geq -(1-\\delta_i ) c_i^\\prime(\\hat{e}_i^ * ) + \\delta_i \\alpha_i \\mu q_i^\\prime(\\hat{e}_i^ * ) \\cdot \\frac{b_i(d(\\bm{\\hat{\\theta}}^*)_{k_i-1})-b_i(d(\\bm{\\hat{\\theta}}^*)_{k_i}}{d(\\bm{\\hat{\\theta}}^*)_{k_i-1}-d(\\bm{\\hat{\\theta}}^*)_{k_i } } , \\end{aligned}\\ ] ] when @xmath339 , and @xmath365 when @xmath342 .",
    "y.  zhang and m.  van  der schaar , `` peer - to - peer multimedia sharing based on social norms , '' _ elsevier journal signal processing : image communication special issue on `` advances in video streaming for p2p networks '' _ , vol .",
    "27 , no .  5 , pp . 383400 , may 2012"
  ],
  "abstract_text": [
    "<S> peer review ( e.g. , grading assignments in massive open online courses ( moocs ) , academic paper review ) is an effective and scalable method to evaluate the products ( e.g. , assignments , papers ) of a large number of agents when the number of dedicated reviewing experts ( e.g. , teaching assistants , editors ) is limited . </S>",
    "<S> peer review poses two key challenges : 1 ) identifying the reviewers intrinsic capabilities ( i.e. , _ adverse selection _ ) and 2 ) incentivizing the reviewers to exert high effort ( i.e. , _ moral hazard _ ) . some works in mechanism design address _ pure _ adverse selection using _ one - shot _ matching rules , and _ pure _ moral hazard was addressed in repeated games with _ exogenously given _ and _ fixed _ matching rules . </S>",
    "<S> however , in peer review systems exhibiting both adverse selection and moral hazard , one - shot or exogenous matching rules do not link agents current behavior with future matches and future payoffs , and as we prove , will induce myopic behavior ( i.e. , exerting the lowest effort ) resulting in the lowest review quality .    in this paper </S>",
    "<S> , we propose for the first time a solution that _ simultaneously _ solves adverse selection and moral hazard . </S>",
    "<S> our solution exploits the _ repeated _ interactions of agents , utilizes _ ratings _ to summarize agents past review quality , and designs matching rules that _ endogenously _ depend on agents ratings . </S>",
    "<S> our proposed matching rules are easy to implement and require no knowledge about agents private information ( e.g. , their benefit and cost functions ) . yet , they are effective in guiding the system to an equilibrium where the agents are incentivized to exert high effort and receive ratings that precisely reflect their review quality . </S>",
    "<S> using several illustrative examples , we quantify the significant performance gains obtained by our proposed mechanism as compared to existing one - shot or exogenous matching rules . </S>"
  ]
}