{
  "article_text": [
    "1    &    trigger    &    control if new data are transmitted    2    &    data_download    &    if triggered download available data    3    &    commands_parsing    &    parse run options    4    &    local_settings    &    set hosts and data coordinates    5    &    data_fetch    &    load data input for actual run in local buffer    6    &    file_naming    &    creates file names following the standard    7    &    correction    &    perform data correction task    8    &    orbit_rebuilding    &    rebuild data related on a given orbit    8    &    data_reduction    &    data separation , reconstruction and equalization    9    &    photonlist_building    &    analyse data of the reduced photon list    10    &    attitude_correction    &    correction for the attitude wobbling    11    &    imaging    &    image extraction    12    &    filing    &    archive produced data    the sasoa ( super agile standard orbital analysis ) pipeline is composed by a succession of elaboration tasks applied on the sa detected eventlist , the output of a certain task is the input of the following .",
    "macro - tasks launch other subfunctions , some of them are implemented by external scripts and batch programs . in sasoa",
    ", a daemon running an endless loop provides to check if data for the incoming contacts are available and when they are found , the trigger for contact data processing is given .",
    "data for a given contact mostly contain the data related to measurements acquired by the agile instruments during the last orbit .",
    "the failure of a single task of the pipeline may block the production of the final output data for a single run but the failure of a single contact run does not stop the daemon for the next incoming triggers and related data processing run .",
    "different kinds of triggers can be used : the first way used to trigger the sasoa pipeline is to control if a file with extension .ok is generated by the preprocessing system for the incoming contact data , i. e. if a file named `` vc1.002469.009.ok '' is present , data for the contact 2469 are correctly generated and available on the data area of the preprocessing machine .",
    "another trigger modality is to control if a value in a database of the asdc data center is changed . when the system has assured that data are present , then the data are downloaded from the _ gtb _ server in bologna using the _ wget _ utility ( see m. trifoglio et al , `` _ _ archiving the agile level-1 telemetry data _ _ ,",
    "astronomy and astrophysics , 2008 '' , ) , data from asdc are downloaded with _",
    "wget _ also , but they have to be downloaded one by one ( mirror and -a option are disabled ) to avoid transmission band overloading .",
    "data products ( histograms , plots , statistics & reports ) are generated at different levels , hosts coordinates , data directories paths and other configurations of the processing system are set .",
    "the most important input telemetry data used for sa instrument data analysis are eventlist data ( sa tm 3905 ) , attitude data ( sa tm 3914 ) , and ephemerides data ( sa tm 3916 ) .",
    "the file names for i / o file of all tasks are generated following the official naming convention , the correction stage performs time synchronization among input data files and other corrections .",
    "then some actions useful to rebuild the orbital organization of the data are performed : if the contact contains more than 1 orbit , then data are divided in the respective parts and the pipeline is launched on the first part of data . after the run end , the system is triggered on the 2nd part . in the data reduction stage",
    "the eventlist from the previous stage is taken as input ( complete orbit or orbital part ) , with efficiency files and then a reduced output eventlist is generated . in the sources extraction stage ,",
    "the data are corrected removing wobbling effects , then the exposure is evaluated using informations regarding earth occultation and saga time periods .",
    "after these steps the imaging procedure is applied and sources lists are generated , giving position and flux for the detected sources .",
    "telemetry data are preprocessed and archived with the aid of the _ tmpps _ system furnished by the gtb team ( see m. trifoglio et al , astronomy and astrophysics , 2008 ) .",
    "data of superagile measurements have to be integrated over days and weeks to extract science information .",
    "the software utility performing this task is named tds processing ( tds ) .",
    "the working schema of tds is shown in fig .",
    "[ e.16-fig-1 ] . a web graphical user interface ( gui ) is provided to launch data analysis applications through the web , the scientific user can perform its own analysis from elsewhere and using the preferred operating system ( fig .",
    "[ e.16-fig-2 ] ) .",
    "tds processing can be refined with specific source analysis operations , optimizing the results calculating the exposure of the celestial source in the given position , that is an input to the computation , expressed with ( ra , dec ) coordinates .",
    "this has also allowed long integration analysis on 3c 273 ( see l. pacciani et al , astronomy and astrophysics , 2008 ) and on markarian 421 ( see i. donnarumma et al , `` _ _ the june 2008 flare of markarian 421 from optical to tev energies _ _ '' , apj , 2008 ) .",
    "a web based monitor is showing the status of the automatic processing software in figure  [ e.16-fig-3 ] .",
    "the sa scientific processing system is ready , enabling the access from web solve installation & portability problems .",
    "processing stations will be added to fit with the actual number of data analysis software users .",
    "all sa software is portable , we have in use running versions of the whole processing system under linux opensuse ( until 10.3 version , for 32 & 64bit architecture ) .",
    "m. trifoglio et al , `` archiving the agile level-1 telemetry data '' , submitted to astronomy and astrophysics , 2008 l. pacciani et al , `` high energy variability of 3c 273 during the agile multiwavelength campaign of december 2007 - january 2008 '' , submitted to astronomy and astrophysics , 2008 i. donnarumma et al , `` the june 2008 flare of markarian 421 from optical to tev energies '' , submitted to apj , 2008"
  ],
  "abstract_text": [
    "<S> the superagile ( sa ) instrument is a x - ray detector for astrophysics measurements , part of the italian agile satellite for x - ray and gamma - ray astronomy launched at 23/04/2007 from india . </S>",
    "<S> superagile is now studying the sky in the 18 - 60 kev energy band . </S>",
    "<S> it is detecting sources with advanced imaging and timing detection capabilities and good spectral detection capabilities . </S>",
    "<S> several astrophysical sources has been detected and localized , including crab , vela and gx 301 - 2 . </S>",
    "<S> the instrument has the skill to resolve correctly sources in a field of view of [ -40 , + 40 ] degrees interval , with the angular resolution of 6 arcmin , and a spectral analysis with the resolution of 8 kev . </S>",
    "<S> transient events are regularly detected by sa with the aid of its temporal resolution ( 2 microseconds ) and using signal coincidence on different portions of the instrument , with confirmation from other observatories . </S>",
    "<S> the sa data processing scientic software performing at the agile ground segment is divided in modules , grouped in a processing pipeline named sasoa . </S>",
    "<S> the processing steps can be summarized in _ data reduction _ , _ photonlist building _ , _ sources extraction _ and _ sources analysis_. the software services allow orbital data processing ( near real - time ) , daily data set integration , temporal data set ( tds ) processing and tds processing with source target optimization ( tds_src ) . </S>",
    "<S> automatic data processing monitoring and interactive data analysis is possible from an internet connected workstation , with the use of sa data processing web services . </S>",
    "<S> many solutions were implemented in order to achieve fault tolerance . </S>",
    "<S> archive management and data storage are performed with the help of relational database instruments . </S>"
  ]
}