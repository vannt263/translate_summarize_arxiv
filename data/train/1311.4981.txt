{
  "article_text": [
    "high - dimensional data , where the number of covariates @xmath0  greatly exceeds the sample size @xmath1 , arise frequently in modern applications in biology , chemometrics , economics , neuroscience and other scientific fields . to facilitate the analysis , it is often useful and reasonable to assume that only a small number of covariates are relevant for modeling the response variable . under this sparsity assumption ,",
    "a widely used approach for analyzing high - dimensional data is regularized or penalized regression .",
    "this approach estimates the unknown regression coefficients by solving the following penalized regression problem : @xmath2 where @xmath3 is the vector of responses , @xmath4 is an @xmath5 matrix of covariates , @xmath6 is the vector of unknown regression coefficients , @xmath7 denotes the @xmath8 norm ( euclidean norm ) , and @xmath9 is a penalty function which depends on a tuning parameter @xmath10 .",
    "many commonly used variable selection procedures in the literature can be cast into the above framework , including the best subset selection , @xmath11 penalized regression or lasso [ @xcite ] , bridge regression [ @xcite ] , scad [ @xcite ] , mcp [ @xcite ] , among others .    the lasso penalized regression is computationally attractive and enjoys great performance in prediction .",
    "however , it is known that lasso requires rather stringent conditions on the design matrix to be variable selection consistent [ @xcite , @xcite ] . focusing on identifying the unknown sparsity pattern ,",
    "nonconvex penalized high - dimensional regression has recently received considerable attention . @xcite",
    "first systematically studied nonconvex penalized likelihood for fixed finite dimension @xmath0 . in particular , they recommended the scad penalty which enjoys the oracle property for variable selection .",
    "that is , it can estimate the zero coefficients as exact zero with probability approaching one , and estimate the nonzero coefficients as efficiently as if the true sparsity pattern is known in advance .",
    "@xcite extended these results by allowing @xmath0 to grow with @xmath1 at the rate @xmath12 or @xmath13 . for high dimensional nonconvex penalized regression with @xmath14",
    ", @xcite proved that the oracle estimator itself is a local minimum of scad penalized least squares regression under very relaxed conditions ; @xcite proposed a minimax concave penalty ( mcp ) and devised a novel plus algorithm which when used together can achieve the oracle property under certain regularity conditions .",
    "important insight has also been gained through the recent work on theoretical analysis of the global solution [ @xcite ] .",
    "however , direct computation of the global solution to the nonconvex penalized regression is infeasible in high dimensional setting .    for practical data analysis ,",
    "it is critical to find an easy - to - implement procedure which can find a local solution with satisfactory theoretical property even when the number of covariates greatly exceeds the sample size .",
    "two challenging issues remain unsolved .",
    "one is the problem of multiple local minima ; the other is the problem of optimal tuning parameter selection .",
    "a direct consequence of the multiple local minima problem is that the solution path is not unique and is not guaranteed to contain the oracle estimator .",
    "this problem is due to the nature of the nonconvexity of the penalty . to understand it",
    ", we note that the penalized objective function in ( [ model2 ] ) is nonconvex in @xmath15 whenever the convexity of the least squares loss function does not dominate the concavity of the penalty part . in general",
    ", the occurrence of multiple minima is unavoidable unless strong assumptions are imposed on both the design matrix and the penalty function .",
    "the recent theory for scad penalized linear regression [ @xcite ] and for general nonconcave penalized generalized linear models [ @xcite ] indicates that one of the local minima enjoys the oracle property but it is still an unsolved problem how to identify the oracle estimator among multiple minima when @xmath14 .",
    "popularly used algorithms generally only ensure the convergence to a local minimum , which is not necessarily the oracle estimator .",
    "numerical evidence in section  [ sec4 ] suggests that the local minima identified by some of the popular algorithms have a relatively low probability to recover the unknown sparsity pattern although it may have small estimation error .",
    "even if a solution path is known to contain the oracle estimator , identifying such a desirable estimator from the path is itself a challenging problem in ultra - high dimension .",
    "the main issue is to find the optimal tuning parameter which yields the oracle estimator .",
    "the theoretically optimal tuning parameter does not have an explicit representation and depends on unknown factors such as the variance of the unobserved random noise .",
    "cross - validation is commonly adopted in practice to select the tuning parameter but is observed to often result in overfitting . in the case of fixed @xmath0 , @xcite rigorously proved that generalized cross - validation leads to an overfitted model with a positive probability for scad - penalized regression .",
    "effective bic - type criterion for nonconvex penalized regression has been investigated in @xcite and @xcite for fixed @xmath0 ; and in @xcite for diverging @xmath0 ( but @xmath16 ) . however , to the best of our knowledge , there is still no satisfactory tuning parameter selection procedure for nonconvex penalized regression in ultra - high dimension .",
    "the above two main concerns motivate us to consider calibrating nonconvex penalized regression in ultra - high dimension with the goal to identify the oracle estimator with high probability . to achieve this",
    ", we first prove that a calibration of the cccp algorithm [ @xcite ] for nonconvex penalized regression produces a consistent solution path with probability approaching one in merely two steps under conditions much more relaxed than what would be required for the lasso estimator to be model selection consistent . furthermore , extending the recent work of @xcite and @xcite for bayesian information criterion ( bic ) on high dimensional least squares regression , we propose a high - dimensional bic for a nonconvex penalized solution path and prove its validity under more general conditions when @xmath0 grows at an exponential rate .",
    "the recent independent work of @xcite ( @xcite ) devised a multi - stage convex relaxation scheme and proved that for the capped @xmath11 penalty the algorithm can find a consistent solution path with probability approaching one under certain conditions .",
    "despite the similar flavor shared with the algorithm proposed in this paper , his algorithm takes multiple steps ( which can be very large in practice depending on the design condition ) and the paper has not studied the problem of tuning parameter selection .    to deepen our understanding of the nonconvex penalized regression",
    ", we also derive an interesting auxiliary theoretical result of an upper bound on the @xmath8 distance between a sparse local solution of nonconvex penalized regression and the oracle estimator .",
    "this result is new and insightful .",
    "it suggests that under general regularity conditions a sparse local minimum can often have small estimation error even though it may not be the oracle estimator .",
    "overall , the theoretical results in this paper fill in important gaps in the literature , thus substantially enlarge the scope of applications of nonconvex penalized regression in ultra - high dimension . in monte carlo studies , we demonstrate that the calibrated cccp algorithm combined with the proposed high - dimensional bic is effective in identifying the underlying sparsity pattern .",
    "the rest of the paper is organized as follows . in section  [ sec2 ]",
    ", we define the notation , review the cccp algorithm and introduce the new methodology . in section  [ sec3 ] , we establish that the proposed calibrated cccp solution path contains the oracle estimator with probability approaching one under general conditions , and that the proposed high - dimensional bic is able to select the optimal tuning parameter with probability tending to one . in section  [ sec4 ] ,",
    "we report numerical results from monte carlo simulations and a real data example . in section  [ sec5 ]",
    ", we present an auxiliary theoretical result which sheds light on the estimation accuracy of a local minimum of nonconvex penalized regression if it is not the oracle estimator .",
    "the proofs are given in section  [ sec6 ] .",
    "suppose that @xmath17 is a random sample from the linear regression model @xmath18 where @xmath19 , @xmath4 is the @xmath5 nonstochastic design matrix with the @xmath20th row @xmath21 , @xmath22 is the vector of unknown true parameters , and @xmath23 is a vector of independent and identically distributed random errors .",
    "we are interested in the case where @xmath24 greatly exceeds the sample size @xmath1 .",
    "the vector of the true parameters @xmath25 is assumed to be sparse in the sense that the majority of its components are exactly zero .",
    "let @xmath26 be the index set of covariates with nonzero coefficients and let @xmath27 denote the cardinality of @xmath28 .",
    "we use @xmath29[page5 ] to denote the minimal absolute value of the nonzero coefficients . without loss of generality",
    ", we may assume that the first @xmath30 components of @xmath25 are nonzero , thus we can write @xmath31 , where @xmath32 represents a zero vector of length @xmath33",
    ". the oracle estimator is defined as @xmath34 , where @xmath35 is the least squares estimator fitted using only the covariates whose indices are in @xmath28 .    to handle the high - dimensional covariates , we consider the penalized regression in ( [ model2 ] ) .",
    "the penalty function @xmath36 is assumed to be increasing and concave for @xmath37 with a continuous derivative @xmath38 on @xmath39 . to induce sparsity of the penalized estimator",
    ", it is generally necessary for the penalty function to have a singularity at the origin , that is , @xmath40 . without loss of generality ,",
    "the penalty function can be standardized such that @xmath41 .",
    "furthermore , it is required that @xmath42 for some positive constant @xmath43 .",
    "condition ( [ pc2 ] ) plays the key role of not over - penalizing large coefficients , thus alleviating the bias problem associated with lasso .",
    "the above class of penalty functions include the popularly used scad penalty and mcp .",
    "the scad penalty is defined by @xmath44 for some @xmath45 , where the notation @xmath46 stands for the positive part of @xmath47 , that is , @xmath48 .",
    "@xcite recommended to use @xmath49 from a bayesian perspective .",
    "on the other hand , the mcp is defined by @xmath50 for some @xmath51 ( as @xmath52 , it amounts to hard - thresholding , thus in the following we assume @xmath53 ) .",
    "let @xmath54 be the @xmath55th column vector of @xmath4 .",
    "without loss of generality , we assume that @xmath56 for all @xmath55 . throughout this paper , the following notation is used . for an arbitrary index set @xmath57",
    ", @xmath58 denotes the @xmath59 submatrix of @xmath4 formed by those columns of @xmath4 whose indices are in @xmath60 . for a  vector @xmath61",
    ", we use @xmath62 to denote its @xmath8 norm ; on the other hand @xmath63 denotes the @xmath64 norm , @xmath65 denotes the @xmath11 norm and @xmath66 denotes the @xmath67 norm .",
    "we use @xmath68 to represent the size-@xmath69 subvector of @xmath70 formed by the entries @xmath71 with indices in @xmath60 . for a symmetric matrix @xmath72 , @xmath73 and @xmath74 stand for the smallest and largest eigenvalues of @xmath72 , respectively .",
    "furthermore , we let @xmath75 finally , @xmath0 , @xmath30 , @xmath76 and other related quantities are all allowed to depend on @xmath1 , but we suppress such dependence for notational simplicity .",
    "it is challenging to solve the penalized regression problem in ( [ model2 ] ) when the penalty function is nonconvex .",
    "@xcite proposed a fast optimization algorithm called the scad ",
    "cccp ( cccp stands for concave convex procedure ) algorithm for solving the scad - penalized regression .",
    "the key idea is to update the solution with the minimizer of the tight convex upper bound of the objective function obtained at the current solution .",
    "what makes a fast algorithm practical relies on the possibility of decomposing the nonconvexed penalized least squares objective function as the sum of a convex function and a concave function . to be specific ,",
    "suppose we want to minimize an objective function @xmath77 which has the representation @xmath78 for a convex function @xmath79 and a concave function @xmath80 . given a current solution @xmath81 , the tight convex upper bound of @xmath77",
    "is given by @xmath82 where @xmath83 .",
    "we then update the solution by minimizing @xmath84 .",
    "since @xmath84 is a convex function , it can be easily minimized .    for the penalized regression in ( [ model2 ] ) , we consider a penalty function @xmath85 which has the decomposition @xmath86 where @xmath87 is a differentiable concave function .",
    "for example , for the scad penalty , @xmath88 i \\bigl({\\vert}\\beta_j| > a\\lambda\\bigr),\\end{aligned}\\ ] ] while for the mcp penalty , @xmath89i \\bigl({\\vert}\\beta_j| \\geq a\\lambda\\bigr).\\ ] ] hence , using the decomposition in ( [ decom ] ) , the penalized objective function in ( [ model2 ] ) can be rewritten as @xmath90 which is the sum of convex and concave functions . the cccp algorithm is applied as follows .",
    "given a current solution @xmath81 , the tight convex upper bound is @xmath91 we then update the current solution by @xmath92 .",
    "an important property of the cccp algorithm is that the objective function always decreases after each iteration [ @xcite , and @xcite ] , from which it can be deduced that the solution converges to a local minimum .",
    "see , for example , corollary  3.2 of @xcite .",
    "however , there is no guarantee that the local minimum found is the oracle estimator itself because there are multiple local minima and the solution of the cccp algorithm depends on the choice of the initial solution .      in this paper , we propose and study a calibrated cccp estimator .",
    "more specifically , we start with the initial value @xmath93 and a tuning parameter @xmath10 and let @xmath94 be the tight convex upper bound defined in ( [ qfun ] ) .",
    "the calibrated algorithm consists of the following two steps .    1 .",
    "let @xmath95 , where the choice of @xmath96 will be discussed later .",
    "2 .   let @xmath97 .",
    "when we consider a sequence of tuning parameter values , we obtain a solution path @xmath98 .",
    "the calculation of the path is fast even for very high - dimensional @xmath0 as for each of the two steps a convex minimization problem is solved . in step  1 , a smaller tuning parameter @xmath99 is adopted to increase the estimation accuracy , see section  [ sec3.1 ] for discussions on the practical choice of @xmath100 .",
    "we call a solution path `` _ _ path consistent _ _ '' if it contains the oracle estimator . in section  [ sec3.1 ]",
    ", we will prove that the calibrated cccp algorithm produces a consistent solution path under rather weak conditions .",
    "given such a solution path , a critical question is how to tune the regularization parameter @xmath76 in order to identify the oracle estimator .",
    "the performance of a penalized regression estimator is known to heavily depend on the choice of the tuning parameter .",
    "to further calibrate nonconvex penalized regression , we consider the following high - dimensional bic criterion ( hbic ) to compare the estimators from the above solution path : @xmath101 where @xmath102 is the model identified by @xmath103 , @xmath104 denotes the cardinality of @xmath105 , and @xmath106 with @xmath107 .",
    "as we are interested in the case where @xmath0 greatly exceeds @xmath1 , the penalty term also depends on  @xmath0 ; and @xmath108 is a sequence of numbers that diverges to @xmath109 , which will be discussed later .",
    "we compare the value of the above hbic criterion for @xmath110 , where @xmath111 represents a rough estimate of an upper bound of the sparsity of the model and is allowed to diverge to @xmath109 .",
    "we select the tuning parameter @xmath112    the above criterion extends the recent works of @xcite and @xcite on the high - dimensional bic for the least squares regression to tuning parameter selection for nonconvex penalized regression . in sections  [ sec3.1][sec3.3 ] , we study asymptotic properties under conditions such as sub - gaussian random errors , dimension of the covariates growing at the exponential rate and diverging  @xmath113 .",
    "the main theory comprises two parts .",
    "we first show that under some general regularity conditions the calibrated cccp algorithm yields a solution path with the `` _ _ path consistency _ _ '' property .",
    "we next verify that when the proposed high - dimensional bic is applied to this solution path to choose the tuning parameter @xmath76 , with probability tending to one the resulted estimator is the oracle estimator itself .    to facilitate the presentation",
    ", we specify a set of regularity conditions .",
    "there exists a positive constant @xmath114 such that @xmath115 .",
    "the random errors @xmath116 are i.i.d .",
    "mean zero sub - gaussian random variables with a scale factor @xmath117 , that is , @xmath118\\leq",
    "e^{\\sigma^2t^2/2 } , \\forall t. $ ]    the penalty function @xmath36 is assumed to be increasing and concave for @xmath37 with a continuous derivative @xmath38 on @xmath39 .",
    "it admits a convex - concave decomposition as in ( [ decom ] ) with @xmath119 satisfies : @xmath120 for @xmath121 , where @xmath53 is a constant ; and @xmath122 for @xmath123 , where @xmath124 is a positive constant .",
    "the design matrix @xmath4 satisfies : @xmath125 .",
    "assume that @xmath126 and @xmath127 , where @xmath128 is defined on page  , @xmath76  and  @xmath100 are the two parameters in the modified cccp algorithm given in the first paragraph of section  [ sec2.3 ] .",
    "condition ( a1 ) concerns the true model and is a common assumption in the literature on high - dimensional regression .",
    "condition ( a2 ) implies that for a vector @xmath129 , @xmath130 condition ( a3 ) is satisfied by popular nonconvex penalty functions such as scad and mcp .",
    "note that the condition @xmath120 for @xmath121 is equivalent to assuming that @xmath131 , @xmath132 , that is , large coefficients are not penalized , which is exactly the motivation for nonconvex penalties .",
    "condition ( a4 ) , which is given in @xcite , ensures a desirable bound on the @xmath11 estimation loss of the lasso estimator .",
    "note that the cccp algorithm yields the lasso estimator after the first iteration , so the asymptotic properties of the cccp estimator is related to that of the lasso estimator .",
    "condition ( a4 ) holds under the restricted eigenvalue condition which is known to be a relatively mild condition on the design matrix for high - dimensional estimation .",
    "in particular , it is known to hold in some examples where the covariates are highly dependent , and is much weaker than the irrepresentable condition [ @xcite ] which is almost necessary for lasso to be model selection consistent .",
    "we first state a useful lemma that characterizes a nonasymptotic property of the oracle estimator in high dimension .",
    "the result is an extension of that in @xcite under the more general sub - gaussian random error condition .",
    "[ lem1 ] for any given @xmath133 and @xmath134 , consider the events @xmath135 where @xmath136 . then under conditions and , @xmath137}-2(p - q)\\exp \\bigl[-nb_2 ^ 2\\lambda^2/ \\bigl(2 \\sigma^2 \\bigr ) \\bigr].\\ ] ]    the proof of lemma  [ lem1 ] is given in the online supplementary material [ @xcite ] .",
    "theorem [ main ] below provides a nonasymptotic bound of the probability the solution path contains the oracle estimator . under general conditions , this probability tends to one .",
    "[ main ] assume that conditions hold .",
    "if @xmath138 , then for all @xmath1 sufficiently large , @xmath139    assume that conditions hold .",
    "if @xmath140 , @xmath141 and @xmath138 , then @xmath142 as @xmath143 .",
    "@xcite considered thresholding lasso , which has the oracle property under an incoherent design condition in the ultra - high dimension . @xcite",
    "further proposed and investigated a multi - step thresholding procedure which can accurately estimate the sparsity pattern under the restricted eigenvalue condition of @xcite .",
    "these theoretical results are derived by assuming the initial lasso is obtained using a theoretical tuning parameter value , which depends on the unknown random noise variance @xmath144 .",
    "estimating @xmath144 is a difficult problem in high - dimensional setting , particularly when the random noise is non - gaussian . on the other hand ,",
    "if the true value of @xmath144 is known a priori , then it is possible to derive variable selection consistency under somewhat more relaxed conditions on the design matrix than those in the current paper .",
    "adaptive lasso , originally proposed by @xcite for fixed dimension , was extended to high dimension by @xcite under a rather strong mutual incoherence condition .",
    "@xcite derived the consistency of adaptive lasso in high dimension under similar conditions on @xmath145 , but still requires complex conditions on @xmath146 and @xmath128 .",
    "some favorable empirical performance of the multi - step thresholded lasso versus the adaptive lasso was reported in @xcite .",
    "a theoretical comparison of these two procedures in high dimension was considered by @xcite and chapter  7 of @xcite . for both adaptive and thresholded lasso ,",
    "if a covariate is deleted in the first step , it will be excluded from the final selected model .",
    "@xcite proved that selection consistency holds for the mcp solution at the universal penalty level @xmath147 .",
    "the lla algorithm , which @xcite originally proposed for fixed dimensional models , alleviates this problem and has the potential to be extended to the ultra - high dimension under conditions similar as those in this paper .",
    "needless to say , the performances of the above procedures all depend on the choice of tuning parameter . however , the important issue of tuning parameter selection has not been addressed .",
    "we proved that the calibrated cccp algorithm which involves merely two iterations is guaranteed to yield a solution path that contains the oracle estimator with high probability under general conditions . to provide some intuition on this theory , we first note that the first step of the algorithm yields the lasso estimator , albeit with a small penalty level @xmath99 . if we denote the first step estimator by @xmath148 , then based on the optimization theory , the oracle property is achieved when @xmath149 the proof of theorem [ main ] relies on the following condition : @xmath150 for the given @xmath53 .",
    "the proof proceeds by bounding the first part of ( [ key ] ) using a result of @xcite via @xmath151 . in section  [ sec3.3 ]",
    ", we considered an alternative approach using the recent result of @xcite , which leads to weaker requirement on the minimal signal strength under slightly stronger assumptions on the design matrix .",
    "we also noted that theorem [ main ] holds for any @xmath53 , although in the numerical studies we use the familiar @xmath49 .",
    "how fast the probability that our estimator is equal to the oracle estimator approaches one depends on the sparsity level , the magnitude of the smallest signal , the size of the tuning parameter and the condition of the design matrix .",
    "corollary  [ main3 ] below confirms that the path - consistency can hold in ultra - high dimension .",
    "[ main3 ] assume that conditions hold .",
    "suppose there are two positive constants @xmath152 and @xmath153 such that @xmath154 and @xmath155 . if @xmath156 for some @xmath157 and @xmath158 for some @xmath159 , then @xmath160 provided @xmath161 for some @xmath162 , @xmath163 and @xmath127 .",
    "the above corollary indicates that if the true model is very sparse and the design matrix behaves well ( i.e. , @xmath164 ) , then we can take @xmath100 to be a sequence that converges to 0 slowly , for example , @xmath165 .",
    "on the other hand , if one is concerned that the true model may not be very sparse ( @xmath166 ) and the design matrix may not behave very well ( @xmath167 ) , then an alternative choice is to take @xmath168 which works also quite well in practice .",
    "the following corollary establishes that under some general conditions , the choice of @xmath169 yields a consistent solution path under ultra high - dimensionality .",
    "[ main4 ] assume that conditions hold . if @xmath170 for some @xmath171 , @xmath172 for some @xmath173 , @xmath174 for some @xmath175 , @xmath176 for some @xmath177 , @xmath178 for some @xmath179 and @xmath169 , then @xmath180      theorem [ bic ] below establishes the effectiveness of the hbic defined in ( [ hbic ] ) for selecting the oracle estimator along a solution path of the calibrated cccp .",
    "[ bic ] assume that the conditions of theorem  [ main](2 ) hold , and there exists a positive constant @xmath181 such that @xmath182 where @xmath183 denotes the @xmath184 identity matrix and @xmath185 denotes the projection matrix onto the linear space spanned by the columns of @xmath58 . if @xmath186 , @xmath187 and @xmath188 , then @xmath189 as @xmath190 .",
    "condition ( [ iden ] ) is an asymptotic model identifiability condition , similar to that in @xcite .",
    "this condition states that if we consider any model which contains at most @xmath113 covariates , it can not predict the response variable as well as the true model does if it is not the true model . to give some intuition of this condition , as in @xcite",
    ", one can show that for @xmath191 , @xmath192 the theorem confirms that the bic criterion for shrinkage parameter selection investigated in @xcite , @xcite and @xcite can be modified and extended to ultra - high dimensionality . carefully examining the proof",
    ", it is worth noting that the consistency of the hbic only requires a consistent solution path but does not rely on the particular method used to construct the path .",
    "hence , the proposed hbic has the potential to be generalized  to other settings with ultra - high dimensionality .",
    "the sequence @xmath108 should diverge to @xmath109 slowly , for example , @xmath193 , which is used in our numerical studies .",
    "theorem  [ main ] , which is the main result of the paper , implies that the oracle property of the calibrated cccp estimator requires the following lower bound on the magnitude of the smallest nonzero regression coefficient : @xmath194 where @xmath195 means @xmath196 , and @xmath197 is a constant that depends on the design matrix @xmath4 and other unknown factors such as @xmath144 .",
    "when the true model dimension @xmath30 is fixed , the lower bound for @xmath128 is arbitrarily close to the optimal lower bound @xmath198 for nonconvex penalized approaches [ e.g. , @xcite ] .",
    "however , when @xmath30 is diverging , this bound is suboptimal . in general",
    ", there is a tradeoff between the conditions on @xmath128 and the conditions on the design matrix .",
    "comparing to the results in the literature , theorem  [ main ] imposes weak conditions on the design matrix and the algorithm we investigate is transparent . in this section",
    ", we will prove that the optimal lower bound of @xmath128 can be achieved by the calibrated cccp procedure under a set of slightly stronger conditions on the design matrix .",
    "note that the calibrated cccp estimator depends on @xmath199 , which is the lasso estimator obtained after the first iteration of the cccp algorithm .",
    "in fact , the lower bound of @xmath128 is proportional to the @xmath200 convergence rate of @xmath199 to  @xmath25 , and condition  ( a4 ) only implies that @xmath201 is proportional to @xmath202 .  if @xmath203 we can show that @xmath204 for any @xmath127 , and hence we can achieve almost the optimal lower bound for @xmath128 .",
    "now , the question is under what conditions inequality ( [ eq : linfty ] ) holds .",
    "let @xmath205 be the @xmath206 entry of @xmath207 .",
    "@xcite derived the convergence rate ( [ eq : linfty ] ) under the condition of mutual coherence : @xmath208 for some constant @xmath209 .",
    "however , the mutual coherence condition would be too strong for practical purposes when @xmath30 is diverging , since it requires that the pairwise correlations between all possible pairs are sufficiently small . in this subsection",
    ", we give an alternative condition for ( [ eq : linfty ] ) based on the @xmath210 operation norm of @xmath207 .",
    "we replace condition ( a4 ) with the slightly stronger condition ( a4@xmath211 ) below .",
    "we also introduce an additional condition ( a6 ) based on the matrix @xmath210 operational norm .",
    "for a given @xmath212 matrix @xmath213 , the @xmath210 operational norm @xmath214 is defined by latexmath:[$\\|\\mathbf{a}\\|_1=\\max_{i=1,\\ldots , m } \\sum_{j=1}^m    entry of @xmath213 .",
    "let @xmath216 condition ( a4@xmath211 ) : there exist positive constants @xmath217 and @xmath218 such that @xmath219 and @xmath220 where @xmath221 .    condition ( a6 ) : let @xmath222 . there exist finite positive constants @xmath223 and @xmath224 such that @xmath225 and @xmath226    similar conditions to condition ( a4@xmath211 ) were considered by @xcite and @xcite for the @xmath227 convergence of the lasso estimator .",
    "however , ( [ eq : ximax ] ) of condition ( a4@xmath211 ) , which essentially assumes that @xmath228 is sufficiently small , is weaker , at least asymptotically , than the corresponding condition in @xcite and @xcite , which assumes that @xmath229 is bounded . @xcite proved that @xmath230 under condition ( a4@xmath211 ) .",
    "in addition , condition ( a4@xmath211 ) implies condition ( a4 ) [ see @xcite ] .",
    "condition ( a6 ) is not too restrictive .",
    "assume the @xmath231 s are randomly sampled from a distribution with mean @xmath232 and covariance matrix @xmath233 .",
    "if the @xmath210 operational norm of @xmath233 and @xmath234 are bounded , then we have @xmath235 and @xmath236 provided that @xmath30 does not diverge too fast . here",
    "@xmath237 is the @xmath238 submatrix whose entries consist of @xmath239 , the @xmath240th entry of @xmath233 , for @xmath241 and @xmath242",
    ". see proposition  a.1 in the online supplementary material [ @xcite ] of this paper .",
    "an example of @xmath233 satisfying @xmath243 and @xmath244 is a block diagonal matrix where each block is well posed and of finite dimension .",
    "moreover , condition ( a6 ) is almost necessary for the @xmath200 convergence of the lasso estimator .",
    "suppose that @xmath0 is small and @xmath128 is large so that all coefficients of the lasso coefficients are nonzero .",
    "then , @xmath245 where @xmath246 is the least square estimator , and @xmath247 with @xmath248 .",
    "hence , for the sup norm between @xmath249 to be the order of @xmath99 , the @xmath210 operational norm of @xmath250 should be bounded .",
    "[ tiger ] assume that conditions , , and hold .    if @xmath127 , then for all @xmath1 sufficiently large , @xmath251}.\\ ] ]    if @xmath127 and @xmath141 , then @xmath142 as @xmath143 .",
    "assume that the conditions of and ( [ iden ] ) hold .",
    "let @xmath252 be the tuning parameter selected by hbic .",
    "if @xmath186 , @xmath187 , @xmath188 , then @xmath253 , as @xmath190 .",
    "we only need @xmath127 in theorem [ tiger ] for the probability bound of the calibrated cccp estimator , while theorem  [ main ] requires @xmath254 . under the conditions of theorem [ tiger ] , the oracle property of @xmath255 holds",
    "when @xmath256 since @xmath100 can converge to 0 arbitrarily slowly ( e.g. , @xmath257 ) , the lower bound of @xmath128 given by ( [ eq:3 ] ) , @xmath258 , is almost optimal .",
    "we now investigate the sparsity recovery and estimation properties of the proposed estimator via numerical simulations .",
    "we the following estimators : the oracle estimator which assumes the availability of the knowledge of the true underlying model ; the lasso estimator ( implemented using the r package glmnet ) ; the adaptive lasso estimator [ denoted by alasso , @xcite , section  2.8 of @xcite ] , the hard - thresholded lasso estimator [ denoted by hlasso , section  2.8 , @xcite ] , the scad estimator from the original cccp algorithm without calibration ( denoted by scad ) ; the mcp estimator with @xmath259 and @xmath260 . for lasso and scad , 5-fold cross - validation is used to select the tuning parameter ; for alasso , sequential tuning as described in chapter  2 of @xcite is applied . for hlasso , following a referee s suggestion , we first used @xmath76 as the tuning parameter to obtain the initial lasso estimator",
    ", then thresholded the lasso estimator using thresholding parameter @xmath261 for some @xmath262 and refitted least squares regression .",
    "we denote the solution path of hlasso by @xmath263 , and apply hbic to select @xmath76 .",
    "we consider @xmath264 and set @xmath265 in the hbic as it is found they lead to overall good performance for hlasso .",
    "the mcp estimator is computed using the r package plus with the theoretical optimal tuning parameter value @xmath266 , where the standard deviation @xmath267 is taken to be known . for the proposed calibrated cccp estimator ( denoted by new ) , we take @xmath165 and set @xmath265 in the hbic .",
    "we observe that the new estimator performs similarly if we take @xmath169 . in the following , we report simulation results from two examples .",
    "results of additional simulations can be found in the online supplemental file .",
    "[ exa1 ] we generate a random sample @xmath268 , @xmath269 from the following linear regression model : @xmath270 where @xmath271 with @xmath272 denoting a @xmath273-dimensional vector of zeros , the @xmath0-dimensional vector @xmath231 has the @xmath274 distribution with covariance matrix @xmath275 , @xmath276 is independent of @xmath231 and has a normal distribution with mean zero and standard deviation @xmath277 .",
    "this simulation setup was considered in @xcite for a small @xmath0 case . in this example",
    ", we consider @xmath278 and the following choices of @xmath275 : ( 1 ) case  1a : the @xmath206th entry of @xmath275 is equal to @xmath279 , @xmath280 ; ( 2 ) case  1b : the @xmath206th entry of @xmath275 is equal to @xmath281 , @xmath280 ; ( 3 ) case  1c : the @xmath206th entry of @xmath275 equal to 1 if @xmath282 and @xmath283 if @xmath284 .",
    "[ exa2 ] we consider a more challenging case by modifying example  [ exa1 ] case  1a .",
    "we divide the @xmath0 components of @xmath25 into continuous blocks of size 20 .",
    "we randomly select 10 blocks and assign each block the value @xmath285 .",
    "hence , the number of nonzero coefficients is 30 .",
    "the entries in other blocks are set to be zero .",
    "we consider @xmath286 .",
    "two different cases are investigated : ( 1 ) case  2a : @xmath287 and @xmath278 ; ( 2 ) case  2b : @xmath288 and @xmath289 .    in the two examples , based on 100 simulation runs we report the average number of nonzero coefficients correctly estimated to be nonzero ( i.e. , true positive , denoted by tp ) and average number of zero coefficients incorrectly estimated to be nonzero ( i.e. , false positive , denoted by fp ) and the proportion of times the true model is exactly identified ( denoted by tm ) .",
    "these three quantities describe the ability of various estimators for sparsity recovery . to measure the estimation accuracy , we report the mean squared error ( mse ) , which is defined to be @xmath290 , where @xmath291 is the estimator from the @xmath292th simulation run .",
    "@lccd2.2cc@ * case * & * method * & * tp * & & * tm * & * mse * + 1a & oracle & 3.00 & 0.00 & 1.00 & 0.146 + & lasso & 3.00 & 28.99 & 0.00 & 1.101 + & alasso & 3.00 & 11.47 & 0.01 & 1.327 + & hlasso & 3.00 & 0.49 & 0.79 & 0.383 + & scad & 3.00 & 10.12 & 0.08 & 1.496 + & mcp ( @xmath259 ) & 2.89 & 0.28 & 0.76 & 0.561 + & mcp ( @xmath293 ) & 2.91 & 0.42 & 0.68 & 1.292 + & new & 2.99 & & * 0.91 * & * 0.222 * + 1b & oracle & 3.00 & 0.00 & 1.00 & 0.314 + & lasso & 3.00 & 20.64 & 0.00 & 1.248 + & alasso & 3.00 & 8.84 & 0.02 & 1.527 + & hlasso & 2.79 & 0.50 & 0.56 & 1.244 + & scad & 2.99 & 7.42 & 0.17 & 1.598 + & mcp ( @xmath259 ) & 2.02 & 0.51 & 0.06 & 5.118 + & mcp ( @xmath293 ) & 1.99 & 0.60 & 0.02 & 5.437 + & new & 2.77 & & * 0.66 * & * 1.150",
    "* + 1c & oracle & 3.00 & 0.00 & 1.00 & 0.195 + & lasso & 2.99 & 28.22 & 0.00 & 2.987 + & alasso & 2.96 & 10.09 & 0.02 & 2.433 + & hlasso & 2.84 & 0.77 & 0.56 & 1.361 + & scad & 2.96 & 18.09 & 0.01 & 3.428 + & mcp ( @xmath259 ) & 2.67 & & * 0.72 * & 1.636 + & mcp ( @xmath293 ) & 2.77 & 0.22 & 0.68 & 1.677 + & new & 2.79 & 0.46 & 0.58 & * 1.244 * +    @lccd3.2cc@ * case * & * method * & * tp * & & * tm * & * mse * + 2a & oracle & 30.00 & 0.00 & 1.00 & 0.223 + & lasso & 30.00 & 143.14 & 0.00 & 3.365 + & alasso & 29.98 & 7.50 & 0.00 & 0.393 + & hlasso & 29.97 & 1.09 & 0.74 & 0.312 + & scad & 29.98 & 46.15 & 0.00 & 2.495 + & mcp ( @xmath293 ) & 29.83 & 0.50 & * 0.92 * & 0.807 + & new & 29.99 & & 0.89 & * 0.247 * + 2b & oracle & 30.00 & 0.00 & 1.00 & 0.137 + & lasso & 30.00 & 133.65 & 0.00 & 1.089 + & alasso & 30.00 & 1.32 & 0.29 & 0.165 + & hlasso & 30.00 & & * 1.00 * & 0.137 + & scad & 30.00 & 21.83 & 0.00 & 0.599 + & mcp ( @xmath293 ) & 30.00 & 0.08 & 0.92 & 0.137 + & new & 30.00 & & 0.99 & * 0.135 * +    the results are summarized in tables [ table1 ]  and  [ table2 ] .",
    "it is not surprising that lasso always overfits .",
    "other procedures improve the performance of lasso by reducing the false positive rate .",
    "the scad estimator from the original cccp algorithm without calibration has no guarantee to find a good local minimum and has low probability of identifying the true model .",
    "the best overall performance is achieved by the calibrated new estimator : the probability of identifying the true model is high and the mse is relatively small .",
    "the hlasso ( with thresholding parameter selected by our proposed hbic ) and mcp ( using plus algorithm and the theoretically optimal tuning parameter ) also have overall fine performance .",
    "we do not report the results of the mcp with @xmath259 for example  [ exa2 ] since the plus algorithm sometimes runs into convergence problems .      to demonstrate the application",
    ", we analyze the gene expression data set of @xcite , which contains expression values of  31,042 probe sets on 120 twelve - week - old male offspring of rats .",
    "we are interested in identifying genes whose expressions are related to that of gene trim32 ( known to be associated with human diseases of the retina ) corresponding to probe 1389163_at .",
    "we first preprocess the data as described in @xcite to exclude genes that are either not expressed or lacking sufficient variation .",
    "this leaves 18,957 genes .",
    "@lcd2.2c@@xmath294 & * method * & & * prediction error * + 1000 & lasso & 31.17 & * 0.586 * + & alasso & 11.76 & 0.646 + & hlasso & 12.04 & 0.676 + & scad & 4.81 & 0.827 + & mcp ( @xmath259 ) & 11.79 & 0.668 + & mcp ( @xmath293 ) & 7.02 & 0.768 + & new & 8.50 & 0.689 + 2000 & lasso & 32.01 & * 0.604 * + & alasso & 11.01 & 0.661 + & hlasso & 10.82 & 0.689 + & scad & 4.57 & 0.850 + & mcp ( @xmath259 ) & 11.33 & 0.700 + & mcp ( @xmath293 ) & 6.78 & 0.788 + & new & 7.91 & 0.736 +    for the analysis , we select 3000 genes that display the largest variance in expression level .",
    "we further analyze the top @xmath0 ( @xmath295 and 2000 ) genes that have the largest absolute value of marginal correlation with gene trim32 .",
    "we randomly partition the 120 rats into the training data set ( 80 rates ) and testing data set ( 40 rats ) .",
    "we use the training data set to fit the model and select the tuning parameter ; and use the testing data set to evaluate the prediction performance .",
    "we perform 1000 random partitions and report in table  [ table3 ] the average model sizes and the average prediction error on the testing data set for @xmath295 and 2000 .",
    "for the mcp estimators , the tuning parameters are selected by cross - validation since the standard deviation of the random error is not known .",
    "we observe that the lasso procedure yields the smallest prediction error .",
    "however , this is achieved by fitting substantially more complex models .",
    "the calibrated cccp algorithm as well as alasso and hlasso result in much sparser models with still small prediction errors .",
    "the performance of the mcp procedure is satisfactory but its optimal performance depends on the parameter @xmath296 . in screening or diagnostic applications , it is often important to develop an accurate diagnostic test using as few features as possible in order to control the cost .",
    "the same consideration also matters when selecting target genes in gene therapies .",
    "we also applied the calibrated cccp procedure directly to the 18,957 genes and evaluated the predicative performance based on 100 random partitions .",
    "the calibrated cccp estimator has an average model size 8.1 and an average prediction error 0.58 .",
    "note that the model size and predictive performance are similar to what we obtain when we first select 1000 ( or 2000 ) genes with the largest variance and marginal correlation .",
    "this demonstrates the stability of the calibrated cccp estimator in ultra - high dimension .",
    "when a probe is simultaneously identified by different variable selection procedures , we consider it as evidence for the strength of the signal .",
    "probe 1368113_at is identified by both lasso and the calibrated cccp estimator .",
    "this probe corresponds to gene tff2 , which was found to up - regulate cell proliferation in developing mice retina [ @xcite ] . on the other hand , the probes identified by the calibrated cccp but not by lasso also merit further investigation .",
    "for instance , probe 1371168_at was identified by the calibrated cccp estimator but not by lasso .",
    "this probe corresponds to gene mpp2 , which was found to be related to protein metabolism abnormalities in the development of retinopathy in diabetic mice [ @xcite ] .",
    "regularized logistic regression is known to automatically result in a sparse set of features for classification in ultra - high dimension [ @xcite , @xcite ] .",
    "we consider the representative two - class classification problem , where the response variable @xmath297 takes two possible values 0 or 1 , indicating the class membership .",
    "it is assumed that @xmath298 the penalized logistic regression estimator minimizes @xmath299+\\sum_{j=1}^pp_{\\lambda } \\bigl({\\vert}\\beta_j{\\vert}\\bigr).\\ ] ] when a nonconvex penalty is adopted , it is easy to see that the cccp algorithm can be extended to this case without difficulty as the penalized log - likelihood naturally possesses the convex - concave decomposition discussed in section  [ sec2.2 ] of the main paper , because of the convexity of the negative log - likelihood for the exponential family . for easy implementation , the cccp algorithm can be combined with the iteratively reweighted least squares algorithm for ordinary logistic regression , thus taking advantage of the cccp algorithm for linear regression . denote the nonconvex penalized logistic regression estimator by @xmath300 , then for a new feature vector @xmath301 , the predicted class membership is @xmath302 .",
    "we demonstrate the performance of nonconvex penalized logistic regression for classification through the following example : we generate @xmath231 as in example  [ exa1 ] of the main paper , and the response variable @xmath297 is generated according to ( [ binary ] ) with @xmath303 .",
    "we consider sample size @xmath288 and feature dimension @xmath304 .",
    "furthermore , an independent test set of size 1000 is used to evaluate the misclassificaiton error .",
    "the simulation results are reported in table  [ table4 ] .",
    "the results demonstrate that the calibrated cccp estimator is effective in both accurate classification and identifying the relevant features .",
    "we expect that the theory we derived for the linear regression case continues to hold for the logistic regression under similar conditions due to the convexity of the negative log - likelihood function and the fact that the bernoulli random variables automatically satisfies the sub - gaussian tail assumption .",
    "the latter is essential for obtaining the exponential bounds in deriving the theory .    @lcd2.2cc@",
    "* method * & * tp * & & * tm * & * misclassification rate * + oracle & 3.00 & 0.00 & 1.00 & 0.116 + lasso & * 3.00 * & 46.48 & 0.00 & 0.134 + scad & 2.08 & 4.02 & 0.04 & 0.161 + alasso & 2.02 & 4.58 & 0.00 & 0.188 + hlasso & 2.87 & & 0.87 & 0.120 + mcp ( @xmath293 ) & 2.96 & 0.56 & 0.54 & 0.128 + new & 2.99 & & * 0.99 * & * 0.116 * +",
    "in the following , we shall revisit the issue of multiple local minima of nonconvex penalized regression .",
    "we derive an @xmath8 bound of the distance between a sparse local minimum and the oracle estimator .",
    "the result indicates that a local minimum which is sufficiently sparse often enjoys fairly accurate estimation even when it is not the oracle estimator .",
    "this result , to our knowledge , is new in the literature on high - dimensional nonconvex penalized regression .",
    "our theory applies the necessary condition for the local minimizer as in @xcite for convex differencing problems .",
    "let @xmath305 and @xmath306 where @xmath307 if @xmath308 and @xmath309 $ ] otherwise , @xmath310 . as @xmath311 can be expressed as the difference of two convex functions , a necessary condition for @xmath15 to be a local minimizer of @xmath312 is @xmath313 where @xmath314 , where @xmath87 is defined in section  [ sec2.2 ] for scad and mcp penalty functions .    to facilitate our study",
    ", we introduce below a new concept .",
    "[ de5.1 ] the relaxed sparse riesz condition ( src ) in an neighborhood of the true model is satisfied for a positive integer @xmath292 ( @xmath315 ) if @xmath316 where @xmath317 is defined in ( [ ximin ] ) .",
    "the _ relaxed src condition _ is related to , but generally weaker than the _ sparse reisz condition _ [ @xcite , @xcite ] , the _ restricted eigenvalue condition _ of @xcite and the _ partial orthogonality condition _ of @xcite .",
    "the theorem below unveils that for a given sparse estimator which is a local minimum of ( [ model2 ] ) , its @xmath8 distance to the oracle estimator @xmath318 has an upper bound , which is determined by three key factors : tuning parameter @xmath76 , the sparsity size of the local solution , and the magnitude of the smallest sparse eigenvalue as characterized by the relaxed src condition . to this end",
    ", we consider any local minimum @xmath319 corresponding to the tuning parameter @xmath76 .",
    "assume that the sparsity size of this local solution satisfies : @xmath320 for some @xmath321 .",
    "[ local ] consider scad or mcp penalized least squares regression .",
    "assume that conditions and hold , and that the relaxed src condition in an neighborhood of the true model is satisfied for @xmath322 where @xmath323 .",
    "then if @xmath126 , then for all @xmath1 sufficiently large , @xmath324 \\\\ & & \\quad\\qquad { } -2(p - q)\\exp \\bigl[-n\\lambda^2/ \\bigl(2 \\sigma^2 \\bigr ) \\bigr],\\nonumber\\end{aligned}\\ ] ] where @xmath325 is defined in ( [ ximin ] ) and the positive constant @xmath114 is defined in  .    [ col_local ] under the conditions of theorem [ local ] ,",
    "if we take @xmath326 , then we have @xmath327 - 2(p - q)\\exp \\bigl[-n\\lambda^2/ \\bigl(2 \\sigma^2 \\bigr ) \\bigr].\\end{aligned}\\ ] ]    the simple form in the above corollary suggests that if a local minimum is sufficiently sparse , in the sense that @xmath328 diverge to @xmath109 very slowly , this bound is nevertheless quite tight as the rate @xmath329 is near - oracle .",
    "the factor @xmath330 is expected to go to infinity at a relatively slow rate if the local solution is sufficiently sparse .",
    "our experience with existing algorithms for solving nonconvex penalized regression is that they often yield a sparse local minimum , which however has a low probability to be the oracle estimator itself .",
    "we will provide here proofs for the main theoretical results in this paper .",
    "proof of theorem [ main ] by definition , @xmath331 where @xmath332 .",
    "since @xmath333 is a convex function of @xmath15 , the kkt condition is necessary and sufficient for characterizing the minimum . to verify that @xmath318 is the minimizer of @xmath333 , it is sufficient to show that @xmath334 and @xmath335    we first verify ( [ eq : one ] ) . note that with the initial value @xmath232 , we have @xmath336 .",
    "let @xmath337 , where @xmath338 denotes the @xmath11 norm . by modifying the proof of theorem  7.2 of @xcite",
    ", we can show that under the conditions of the theorem , @xmath339 by the assumption of the theorem , on the event @xmath340 , @xmath341 for all @xmath1 sufficiently large .",
    "furthermore , we consider the event @xmath342 defined in lemma [ lem1 ] with @xmath343 . by lemma [ lem1 ] , we have @xmath344 $ ] . by the assumption @xmath126 , for all",
    "@xmath1 sufficiently large , on the event @xmath345 , we have @xmath346 , for @xmath347 and @xmath348 .",
    "hence , by condition  ( a3 ) , on the event @xmath345 , @xmath349 .",
    "furthermore , @xmath350 , for @xmath347 , following the definition of the oracle estimator . therefore , ( [ eq : one ] )  holds with probability at least @xmath351}- 2p\\exp ( -n\\tau^2\\lambda^2/ ( 8\\sigma^2 ) ) $ ] .",
    "next , we verify ( [ eq : two ] ) . on the event @xmath340 , we have @xmath352 , for all @xmath1 sufficiently large .",
    "we consider the event @xmath353 defined in lemma  [ lem1 ] with @xmath354 .",
    "lemma [ lem1 ] implies that @xmath355 $ ] . on the event",
    "@xmath353 we have @xmath356 . by condition ( a3 ) ,",
    "on the event @xmath357 , ( [ eq : two ] ) holds , and this occurs with probability at least @xmath358 - 2p\\exp ( -n\\tau^2\\lambda^2/(8\\sigma^2 ) ) $ ] .",
    "proof of theorem [ bic ] recall that @xmath359 .",
    "we define the following three index sets : @xmath360 , @xmath361 , and @xmath362 . in other words ,",
    "@xmath363 , @xmath364 and @xmath365 denote the sets of @xmath76 values which lead to underfitted , exactly fitted and overfitted models , respectively . for a given model ( or equivalently an index set ) @xmath366 , let @xmath367 .",
    "that is , @xmath368 is the sum of squared residuals when the least squares method is used to estimate model @xmath366 .",
    "also , let @xmath369 . from the definition , we always have @xmath370 .",
    "consider an arbitrary @xmath375 , that is , the model corresponding to @xmath105 is underfitted : @xmath376>0 \\bigr ) \\\\ & & \\qquad = p \\bigl(\\inf_{{\\lambda\\in\\lambda_{n- } } } \\bigl[\\mathrm{hbic}(\\lambda)- \\mathrm{hbic}(\\lambda_n ) \\bigr]>0 , m_{\\lambda_n}=a_0 \\bigr ) \\\\ & & \\quad\\qquad { } + p \\bigl(\\inf_{{\\lambda\\in\\lambda_{n- } } } \\bigl[\\mathrm{hbic}(\\lambda)- \\mathrm{hbic}(\\lambda_n ) \\bigr]>0 , m_{\\lambda_n}\\neq a_0 \\bigr ) \\\\ & & \\qquad \\geq p \\biggl(\\inf_{{\\lambda\\in\\lambda_{n- } } } \\biggl[\\log \\bigl(\\widehat { \\sigma } { } ^2_{m_{\\lambda}}/\\widehat{\\sigma } { } ^2_{a_0 } \\bigr)+\\bigl({\\vert}m_{\\lambda}|-q\\bigr ) \\frac{c_n\\log(p)}{n } \\biggr]>0 \\biggr)+o(1),\\end{aligned}\\ ] ] where the inequality uses theorem  [ main](2 ) .",
    "furthermore , we observe that @xmath377 } { \\bolds{\\varepsilon}^t(\\mathbf{i}_n-\\mathbf{p}_{a_0})\\bolds { \\varepsilon } } \\biggr).\\ ] ] applying the inequality @xmath378 , @xmath379 , we have @xmath376>0 \\bigr ) \\\\ & & \\qquad \\geq p \\biggl(\\min \\biggl\\{\\inf_{{\\lambda\\in\\lambda_{n-}}}\\frac { n ( \\widehat{\\sigma}{}^2_{m_{\\lambda}}-\\widehat{\\sigma } { } ^2_{a_0 } ) } { 2\\bolds{\\varepsilon}^t(\\mathbf{i}_n-\\mathbf{p}_{a_0})\\bolds { \\varepsilon } } , \\log(2 ) \\biggr\\}-\\frac{qc_n\\log(p)}{n}>0 \\biggr)+o(1).\\end{aligned}\\ ] ]    to evaluate @xmath380 , we apply corollary  1.3 of @xcite with their @xmath381 , @xmath382 , @xmath383 and @xmath384 , we have @xmath385 as @xmath143 .",
    "thus @xmath376>0 \\bigr ) \\\\ & & \\qquad \\geq p \\biggl(\\min \\biggl\\{\\frac{\\inf_{{\\lambda\\in\\lambda _ { n-}}}n ( \\widehat{\\sigma}{}^2_{m_{\\lambda}}-\\widehat{\\sigma } { } ^2_{a_0 } ) } { 4(n - q)\\sigma^2},\\log(2 ) \\biggr\\}- \\frac{qc_n\\log(p)}{n}>0 \\biggr)+o(1).\\end{aligned}\\ ] ] in what follows , we will prove that @xmath386 , which combining with the assumption @xmath187 leads to the conclusion @xmath387>0 ) \\rightarrow1 $ ] .",
    "we have @xmath388 where @xmath389 , @xmath390 is the projection matrix into the space spanned by the columns of @xmath391 , and the definition of @xmath392 , @xmath393 , should be clear from the context .",
    "let @xmath394 .",
    "note that @xmath395 is nonempty since @xmath105 underfits .    by assumption ( [ iden ] ) , @xmath396 , for all @xmath1 sufficiently large . to evaluate @xmath397 , we have @xmath398 where @xmath399 with @xmath400 .",
    "note that @xmath401 and @xmath402 . applying the sub - gaussian tail property in ( [ dog1 ] )",
    ", we have @xmath403 as @xmath404 .",
    "hence , @xmath405 . to evaluate @xmath406 ,",
    "let @xmath407 .",
    "it follows from proposition  3 of @xcite that for the sub - gaussian random variables @xmath276 , @xmath408 , @xmath409",
    "^ 2_{+ } } \\biggr\\ } \\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\qquad \\leq \\exp \\biggl(-\\frac{r(\\lambda)t}{2 } \\biggr ) ( 1+t)^{(r(\\lambda))/2}.\\nonumber\\end{aligned}\\ ] ] we take @xmath410 in the above inequality .",
    "then @xmath411 by the assumptions of the theorem .",
    "thus for all @xmath1 sufficiently large , @xmath412 ^ 2_{+ } } \\biggr ) \\\\ & & \\qquad \\leq   2p^{k_n}\\exp \\bigl(-n/ \\bigl(8\\sigma^2k_n \\log(n ) \\bigr ) \\bigr ) \\bigl(n/ \\bigl(2\\sigma^2k_n\\log(n ) \\bigr ) \\bigr)^{k_n/2 } \\\\ & & \\qquad \\leq   2\\exp \\bigl(k_n\\log(p)-n/ \\bigl(8\\sigma^2k_n \\log(n ) \\bigr)+k_n\\log \\bigl(n/ \\bigl(2\\sigma^2k_n \\log(n ) \\bigr ) \\bigr )",
    "\\bigr ) \\\\ & & \\qquad \\rightarrow 0,\\end{aligned}\\ ] ] since @xmath188 . finally , @xmath413 does not depend on @xmath76 .",
    "similarly as above , @xmath414 by the sub - gaussian tail condition . therefore , with probability approaching one , @xmath415 is dominated by @xmath416 .",
    "this finishes the proof for the first case as @xmath187 .",
    "consider an arbitrary @xmath417 , that is , the model corresponding to @xmath105 is overfitted . in this case",
    ", we have @xmath418 .",
    "therefore , @xmath419 let @xmath420 , then @xmath421 by the fact @xmath422 , @xmath423 .",
    "similarly as in case  i , @xmath424>0 \\bigr ) \\\\ & & \\qquad = p \\biggl ( \\inf_{{\\lambda\\in\\lambda_{n+ } } } \\biggl[-\\log \\biggl ( \\frac{\\widehat{\\sigma}{}^2_{a_0}}{\\widehat{\\sigma } { } ^2_{m_{\\lambda } } } \\biggr)+\\bigl({\\vert}m_{\\lambda}|-q\\bigr ) \\frac{c_n\\log(p)}{n } \\biggr]>0 \\biggr)+o(1 ) \\\\ & & \\qquad \\geq p \\biggl(\\inf_{{\\lambda\\in\\lambda_{n+ } } } \\biggl[\\bigl({\\vert}m_{\\lambda}|-q\\bigr ) \\frac{c_n\\log(p)}{n}-\\frac{\\bolds { \\varepsilon}^t(\\mathbf{p } _",
    "{ m_{\\lambda}}-\\mathbf{p}_{a_0})\\bolds{\\varepsilon}}{\\widehat{\\bolds{\\varepsilon}}{}^t\\widehat{\\bolds{\\varepsilon } } -\\bolds{\\varepsilon}^t(\\mathbf{p}_{m_{\\lambda}}-\\mathbf { p}_{a_0})\\bolds{\\varepsilon } } \\biggr]>0 \\biggr ) \\\\ & & \\quad\\qquad { } + o(1 ) \\\\ & & \\qquad = p \\biggl(\\inf_{{\\lambda\\in\\lambda_{n+ } } } \\biggl\\{\\bigl({\\vert}m_{\\lambda}|-q\\bigr ) \\biggl [ \\frac{c_n\\log(p)}{n}-\\frac{\\bolds{\\varepsilon}^t(\\mathbf { p}_{m_{\\lambda}}-\\mathbf{p } _ { a_0})\\bolds{\\varepsilon}/({\\vert}m_{\\lambda}|-q ) } { \\widehat{\\bolds{\\varepsilon}}{}^t\\widehat{\\bolds{\\varepsilon } } -\\bolds{\\varepsilon}^t(\\mathbf{p } _ { m_{\\lambda}}-\\mathbf{p}_{a_0})\\bolds{\\varepsilon } } \\biggr ] \\biggr\\ } \\biggr ) \\\\ & & \\quad\\qquad { } + o(1).\\end{aligned}\\ ] ] it suffices to show that @xmath425>0 \\biggr)\\rightarrow1,\\ ] ] which is implied by @xmath426 note that @xmath427 , hence @xmath428 .",
    "similarly as in case  i , we can show that @xmath429 , since @xmath188 .",
    "thus , @xmath430 . furthermore ,",
    "applying ( [ dog2 ] ) by letting @xmath431 , we have for all @xmath1 sufficiently large , @xmath432 thus with probability approaching one , for all @xmath1 sufficiently large , @xmath433 since @xmath186 .",
    "this finishes the proof .",
    "proof of theorem [ tiger ] we will first prove that there exists a constant such that for @xmath434 , we have @xmath435 let @xmath436 .",
    "since @xmath437 we have @xmath438 hence to prove ( [ f4 ] ) , it suffices to show that @xmath439 .",
    "let @xmath440 corollary  2 of @xcite proves that on the event @xmath441 , @xmath442 , where @xmath443 , provided @xmath444 since @xmath445 [ see ( 7 ) of @xcite ] , where @xmath446 is defined in ( a4 ) and @xmath447 [ see @xcite ] , condition ( a4@xmath211 ) implies that @xmath448    let @xmath449 .",
    "then we have @xmath450 let @xmath451 be the projection of @xmath452 onto @xmath453 , the linear subspace spanned by the column vectors of @xmath58 .",
    "we define the @xmath0-dimensional vector @xmath454 such that @xmath455 and @xmath456 for @xmath457 .",
    "we have @xmath458 therefore , we can write @xmath459 hence @xmath460 , where @xmath461 such that @xmath462 for @xmath457 and @xmath463 for @xmath464 . on @xmath441 , @xmath465 )",
    "implies that on the event @xmath441 , @xmath466 it follows from ( [ eq : a ] ) that inequality ( [ f4 ] ) holds if we show that @xmath467 , in which case @xmath468 .",
    "we will prove this by contradiction .",
    "assume @xmath469 is nonempty .",
    "let @xmath470 be the projection of @xmath54 onto @xmath453 and let @xmath471 , @xmath472 .",
    "then , we can write @xmath473 let @xmath474 . by lemma [ le : low ] below , there exists @xmath475 such that @xmath476 by the kkt condition , we have @xmath477 however we can write @xmath478 the inequalities ( [ eq : low ] ) and ( [ eq : a ] ) with condition ( a6 ) imply that on @xmath441 @xmath479 if @xmath480 , which contradicts the kkt condition . hence , we eventually have @xmath481 on @xmath441 and this proves ( [ f4 ] ) .",
    "we now slightly modify the proof of ( 1 ) of theorem  [ main ] .",
    "more specifically , replacing @xmath340 by @xmath482 , we can show that @xmath483 , and this proves ( 1 ) .",
    "the result in ( 2 ) follows immediately from ( 1 ) .",
    "the proof of ( 3 ) can be done similarly to that of theorem [ bic ] .",
    "proof of theorem [ local ] by ( [ locnec ] ) , a local minimizer @xmath15 necessarily satisfies : @xmath484 where @xmath485 , with @xmath486 if @xmath308 and @xmath309 $ ] otherwise ,",
    "it is easy to see that @xmath488 , @xmath489 .",
    "although the objective function is nonconvex , abusing the notation a little , we refer to the collection of all vectors in the form of the left - hand side of ( [ nce1 ] ) as the subdifferential @xmath490 and refer to a specific element of this set a subgradient . then the necessary condition stated above can be considered as an extension of the classical kkt condition .",
    "alternatively , minimizing @xmath311 can be expressed as a constrained smooth minimization problem [ e.g. , @xcite ] . by the corresponding second - order sufficiency of kkt condition [ e.g. , @xcite , page  320 ]",
    ", @xmath300 is a  local minimizer of @xmath311 if @xmath491 consider the event @xmath492 , where @xmath353 is defined in lemma [ lem1 ] with @xmath493 , and @xmath494 .",
    "since @xmath495 and @xmath126 , similarly as in the proof for lemma [ lem1 ] , we can show that for all @xmath1 sufficiently large , @xmath496 .",
    "$ ] by lemma [ lem1 ] , for all @xmath1 sufficiently large , @xmath497 - 2(p - q)\\exp[-n\\lambda ^2/(2\\sigma^2)]$ ] .",
    "it is apparent that on the event @xmath498 , the oracle estimator @xmath318 satisfies the above sufficient condition .",
    "therefore , by ( [ nce1 ] ) , there exist @xmath499 , @xmath310 , such that @xmath500 abusing notation a little , we denote this zero vector by @xmath501 .",
    "now for any local minimizer @xmath300 which satisfies the sparsity constraint @xmath320 , we will prove by contradiction that under the conditions of the theorem we must have @xmath502 , where @xmath323 .",
    "more specifically , we will derive a contradiction by showing that none of the subgradients of @xmath311 can be zero at @xmath503 .",
    "assume instead that @xmath504 .",
    "let @xmath505 or @xmath506 , then @xmath507 .",
    "let @xmath508 be an arbitrary subgradient in the subdifferential @xmath509 .",
    "let @xmath510 , then @xmath511 satisfies @xmath512 , @xmath310 .",
    "we use @xmath513 to denote the size-@xmath514 subvector of @xmath515 , that is , @xmath516 . and @xmath517 is defined similarly .",
    "we have @xmath518 where the second equality follows from the expression of subgradient , the second last inequality applies the cauchy ",
    "schwarz inequality , and the last inequality follows from the relaxed src condition in an @xmath64-neighborhood of the true model .",
    "thus , this contradicts with the fact that at least one of the subgradients is zero if @xmath300 is a local minimizer and the theorem is proved ."
  ],
  "abstract_text": [
    "<S> we investigate high - dimensional nonconvex penalized regression , where the number of covariates may grow at an exponential rate . </S>",
    "<S> although recent asymptotic theory established that there exists a local minimum possessing the oracle property under general conditions , it is still largely an open problem how to identify the oracle estimator among potentially multiple local minima . </S>",
    "<S> there are two main obstacles : ( 1 ) due to the presence of multiple minima , the solution path is nonunique and is not guaranteed to contain the oracle estimator ; ( 2 ) even if a solution path is known to contain the oracle estimator , the optimal tuning parameter depends on many unknown factors and is hard to estimate . to address these two challenging issues , we first prove that an easy - to - calculate calibrated cccp algorithm produces a consistent solution path which contains the oracle estimator with probability approaching one . </S>",
    "<S> furthermore , we propose a high - dimensional bic criterion and show that it can be applied to the solution path to select the optimal tuning parameter which asymptotically identifies the oracle estimator . the theory for a general class of nonconvex penalties in the ultra - high dimensional setup </S>",
    "<S> is established when the random errors follow the sub - gaussian distribution . </S>",
    "<S> monte carlo studies confirm that the calibrated cccp algorithm combined with the proposed high - dimensional bic has desirable performance in identifying the underlying sparsity pattern for high - dimensional data analysis .    , </S>"
  ]
}