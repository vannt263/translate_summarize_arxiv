{
  "article_text": [
    "past wf attacks employed new combinations of ml classifiers and feature sets to defeat defenses .",
    "we showed that a bayes error lower bound estimate bounds the error of a wf adversary using a certain feature set , regardless his choice of a classifier . on this basis",
    "we introduced privacy metric for wf defenses , based on , that indicates how far a defense is from a perfectly private one .",
    "we discuss open questions and future directions .",
    "our work justifies wf research to shift its focus towards features .",
    "it further gives a tool to help future studies in this direction : the error estimate , a lower bound of the error of any adversary using a certain feature set , can also be used to evaluate feature sets independently of a classifier ( ) .",
    "whilst our results suggest that finding better performing features is becoming harder , we recognize that one limitation of is its dependence on a set of features .",
    "future research may attempt to formulate a similar guarantee independently of features .",
    "we call such a guarantee , which is defined as follows .",
    "let @xmath0 be the range of a defense .",
    "let @xmath1 be the set of all the features that can be extracted from @xmath2 . then is if it is and for any @xmath3 , which guarantees @xmath4-privacy , holds : @xmath5 for an arbitrary @xmath6 ; that is , is if the value of @xmath7 achieved by a feature set is proven to be the smallest up to some negligible additive term @xmath8 .",
    "whilst recent wf research attempted to optimize the computational time and storage required to perform attacks , researchers have not imposed computational bounds to wf adversaries .",
    "we suggest that time and memory computational bounds should be imposed to feature extraction and classification .",
    "we remark although that the bayes error is optimal , for any computationally unbounded adversary .    in the context of feature sets",
    ", we suggest future research to direct its attention towards _ efficient _ feature sets , as we shall now define .",
    "let @xmath9 be the bayes error committed when using full information .",
    "an efficient feature set @xmath10 is a feature set that satisfies both :            we opted for a well - studied asymptotic lower bound estimate of the bayes error , which is based on the error of the nn classifier .",
    "future work may explore other estimates in order to obtain tighter bounds .",
    "unfortunately , under the relaxed i.i.d .",
    "assumption on data , it is not possible to prove any convergence rate result for a bayes error estimate  @xcite ; we will thus need to evaluate convergence experimentally for any estimate .    when the true distributions are known , however , it is possible to compute the bayes error directly .",
    "for example , a recent probabilistic defense , alpaca , samples web pages characteristics from known distributions . in this case",
    ", it may be possible to estimate the bayes error @xmath14 directly from such distributions  @xcite .      both and are valid metrics for evaluating the privacy of a defense against wf attacks .",
    ", which is related to the concept of advantage widely used in cryptography , gives a more precise idea of how far a defense is from a perfect one ( that is , one that forces an adversary into random guessing ) , and should thus be preferred to evaluate the security of defenses . may be used for comparing features .",
    "it is common to engineer wf attacks with respect to their false positives ( fp ) and false negatives ( fn ) rates .",
    "these quantities provide insight about the precision of attacks . whilst we acknowledge that fp - fn are essential for evaluating wf attacks in real - world scenarios , we suggest that the privacy of a defense is best described by just one security parameter . nevertheless , future research may extend our method to these notions .",
    "the simplicity of our approach suggests that other applications in traffic analysis may benefit from it .",
    "in fact , the method evaluates the ability of a defense to protect the underlying behavior of any source of data : provided that an attack is based on an ml classifier , we can bound the success rate of any adversary .    more generally , future research may apply this method to evaluate the security against other ml - based attacks . if an adversary uses an ml classifier to violate security properties , we can immediately estimate security bounds for the proposed countermeasures .",
    "furthermore , we remark that if the information available to the adversary in this context was limited ( i.e. , data comes from a low dimensional space ) , the estimated bounds would be feature - independent .",
    "we showed that there exist natural bounds on any wf adversary , and that we can estimate and use them to evaluate the privacy of any wf defense in a black - box manner .",
    "we expect this will be used to evaluate the privacy of padding schemes in protocols such as tor , ssh , and tls .",
    "we hope our work inspires further research towards formalizing and defining security for wf and ta attacks , for even more realistic adversaries ( e.g. , computationally bounded adversaries ) .",
    "we suspect this will help moving from to .",
    "nonetheless , we reiterate that our framework already provides security guarantees , and that it allows future proposals and evaluations of wf defenses to only focus on the state - of - the - art features in order to show their privacy ."
  ],
  "abstract_text": [
    "<S> website fingerprinting ( wf ) attacks raise major concerns about users privacy . </S>",
    "<S> they employ machine learning ( ml ) to allow a local passive adversary to uncover the web browsing behavior of a user , even if she browses through an encrypted tunnel ( e.g. tor , vpn ) . </S>",
    "<S> numerous defenses have been proposed in the past ; however , it is typically difficult to have formal guarantees on their security , which is most often evaluated empirically against state - of - the - art attacks . in this paper </S>",
    "<S> , we present a practical method to derive security bounds for any wf defense , which depend on a chosen feature set . </S>",
    "<S> this result derives from reducing wf attacks to an ml classification task , where we can determine the smallest achievable error ( the bayes error ) ; such error can be estimated in practice , and is a lower bound for a wf adversary , for any classification algorithm he may use . </S>",
    "<S> our work has two main consequences : i ) it allows determining the security of wf defenses , in a black - box manner , with respect to the state - of - the - art feature set and ii ) it favors shifting the focus of future wf research to the identification of optimal feature sets . </S>",
    "<S> the generality of the approach further suggests that the method could be used to define security bounds for other ml - based attacks .    [ cols=\"<,^,^,^,^,^\",options=\"header \" , ] </S>"
  ]
}