{
  "article_text": [
    "the field of transiting extrasolar planets and especially the study of their atmospheres is one of the youngest and most dynamic subjects in current astrophysics .",
    "permanently at the edge of technical feasibility , we have come from the first radial velocity and transit detections @xcite , via the first detections of molecular features in hot - jupiter atmospheres @xcite to ever more detailed characterisations of multitudes of systems @xcite . with",
    "over 700 exoplanets discovered @xcite and over 1200 exoplanetary candidates that await confirmation @xcite , the focus of interest shifts from the detection to the characterisation of smaller and smaller targets .",
    "the governing factor of this progression is the precision at which we can control our instrument and/or stellar systematics , hence the accuracy with which we can analyse the data .    to minimise the impact of the systematic noise components , different approaches have been proposed in the past . for space and ground - based observations , eg .",
    "spitzer and hubble ( eg . * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , instrumental systematic noise has been approximated using parametric models , often based on auxiliary information ( optical state vectors ) such as instrumental temperature , orbital inclination , inter and intra - pixel positions of the point - spread - function .",
    "using optical state vectors to de - correlate one s data is an effective technique @xcite .",
    "however for instruments that lack a calibration plan at the precision of 10@xmath0 , the accuracy of the retrieved optical state vectors ( e.g. sensor sampling rates ) and the adequacy of the instrument model s definition itself become difficult to determine .",
    "some of the recent controversy over results reported by various teams can be attributed to this circumstance @xcite .",
    "the situation is even further complicated by brightness variability of the planet hosting star , in particular for small , very active m - stars .",
    "hence , it is important to work towards an alternative route to quantify or remove systematic noise using non - parametric models that work as independent confirmations of existing results .",
    "@xcite , @xcite , @xcite and @xcite have progressed towards non - parametric noise models and signal separation using wavelets , principal component , gaussian processes and fourier based analysis .    in this publication",
    ", we propose a new non - parametric method to separate systematic noise from the desired lightcurve signal . given multiple lightcurves , observed simultaneously using spectrographs for example",
    ", we can disentangle our desired astrophysical signal from other time - correlated or non - gaussian systematic noise sources using un - supervised machine learning algorithms .",
    "we furthermore explore the de - correlation of individual time series spanning several consecutive eclipse events .",
    "the importance of this work lies with the fact that no additional knowledge of the system or the star is required , besides the observations themselves .",
    "such non - parametric methods provide a potential new route to de - correlate one s date in the case where the instrument does not feature an adequate calibration plan , the quality of the auxiliary information of the instrument is non - optimal or the host star shows significant activity .",
    "such blind de - convolution techniques provide new insight and powerful validation of the established parametric instrumental models reported in the literature .    here",
    "we will briefly introduce the more general theory of blind - source separation and proceed with a description of the algorithm proposed .",
    "the efficiency of said algorithm is tested with two synthetic models and two hst / nicmos data sets available in the public domain and one kepler ( q2  &  q3 data release ) time series featuring strong time - correlated stellar variability .",
    "future publications will focus on in - depth discussion of the proposed algorithm to specific data sets .",
    "in this section we will briefly describe the fundamental concepts on which the following analysis is based .",
    "readers familiar with independent component analysis may proceed straight to section  [ method ] .",
    "let us consider the analogy of three people talking simultaneously in one room .",
    "the speech signals of these people are denoted by @xmath1 , @xmath2 and @xmath3 . in the same room",
    "are three microphones recording the observed signals @xmath4 , @xmath5 and @xmath6 . the observed signals can be expressed in terms of the original speech signals :    @xmath7    instead of assuming @xmath8 and @xmath9 to be proper time signals , we drop the time dependence and assume them to be random variables    @xmath10    where @xmath11 is a weighting factor ( in this case the square of the distance of the speakers to the microphone ) and @xmath12 are some real coefficients with @xmath13 being the maximum number of observed signals .",
    "the individual time series can also be expressed in terms of vectors where bold lower - case letters denote column vectors and upper - case letter denote matrices :    @xmath14    where the rows of @xmath15 comprise the individual time series , @xmath16 , and similarly @xmath17 is the signal matrix of the individual source signals @xmath18 .",
    "@xmath19 is the mixing matrix consisting of the weights @xmath20 .",
    "equation [ timeseries3 ] is also known as the instantaneous mixing model and often referred to as the classical cocktail party problem @xcite .",
    "the challenge is to estimate the mixing matrix , @xmath19 and its ( pseudo)inverse the de - mixing matrix , @xmath21 ,    @xmath22    given the observations contained in @xmath15 without any additional prior knowledge of either @xmath19 or @xmath17 , or for some methods without restrictions of @xmath19 & @xmath17 .",
    "several algorithms have been proposed to find the linear transformation of equation  [ timeseries3 ] . amongst these are principal component analysis ( pca ) @xcite , factor analysis ( fa ) @xcite , projection pursuit ( pp ) @xcite and the more recently developed independent component analysis ( ica ) @xcite .",
    "the underlying differences between pca and fa on one hand and ica and pp on the other are the underling assumptions on the probability distribution functions ( pdfs ) of the signals comprising @xmath15 .",
    "the former group assumes the signals to follow : 1 ) a gaussian distribution whilst the latter assume the signals to be , 2 ) predominantly non - gaussian or sparse with specific signatures in the spectral domain ( e.g. smica * ? ? ?",
    "this results in significant differences in the way we estimate our signal components .",
    "\\1 ) if the observed signals comprising @xmath15 follow gaussian distributions , we can define their statistics by the first and second statistical moments ( mean and covariance ) only .",
    "algorithms such as pca and fa find a linear transformation from the correlated observed signals , @xmath15 , to a set of uncorrelated signals , @xmath17 . in this case",
    ", uncorrelatedness is equivalent to mutual independence and the source signals are at their most separated ( please see appendix  [ appendix : independent ] for a more in - depth discussion on independence ) .",
    "such a linear transformation is always possible and easily achieved using , for example , single value decompositions ( svd ) @xcite .",
    "an application of pca to exoplanetary light curve de - trending is given by @xcite .",
    "\\2 ) in the case of the observed signals following non - gaussian distributions , significant information is contained in the higher statistical moments ( skew & kurtosis ) and it can be shown that uncorrelated signals ( as produced by pca & fa ) are not necessarily mutually independent and hence not optimally separated from one another . here uncorrelatedness is a weaker constraint than independence and it can be said that independent signals are always uncorrelated but not vice versa ( see appendix  [ appendix : independent ] ) . as a consequence using pca or fa algorithms may only yield a partially separated result for non - gaussian sources .",
    "it is important to note that most observed signals , astrophysical or stellar / instrumental noise , are predominantly non - gaussian by nature .",
    "we can also state that most of these signals should be statistically independent from one another ( e.g. an exoplanet light curve signal is independent of the systematic noise of the instrument with which it was recorded ) .",
    "these properties have led to a surge in ica based analysis methods in current cosmology and extra - galactic astronomy . here",
    "ica is used to separate the cosmic microwave background ( cmb ) or signatures of distant galaxies from their galactic foregrounds ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "@xcite furthermore separates instrumental noise from the desired astrophysical signal .",
    "other applications include data - compression of sparse , large data sets to improve model fitting efficiencies ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "in this publication , we focus on the application of ica to exoplanetary lightcurve analysis .",
    "let us consider multiple time series observations of the same exoplanetary eclipse signal either in parallel , by performing spectrophotometry with a spectrograph , or consecutive in time ( as explained in section  [ kepler ] ) .    without excluding the most general case ,",
    "let us focus on a time - resolved spectroscopic measurement of an exoplanetary eclipse .",
    "for most observations , the signal recorded is a mixture of astrophysical signal , gaussian ( white ) noise and systematic noise components originating from instrumental defects and other sources such as stellar activity and telluric fluctuations .",
    "we can therefore write the individual time series as sum of the desired astrophysical signal , @xmath23 , systematic ( non - gaussian ) noise components , @xmath24 , and gaussian noise , @xmath25 . we can now define the underlying linear model of our time series data to be    @xmath26    or @xmath27    where @xmath28 and @xmath29 are the number of systematic and white noise components respectively and @xmath30 assuming only one component is astrophysical .",
    "the basic assumptions of ica are that the elements comprising @xmath17 , @xmath31 , are mutually independent random variables with probability densities , @xmath32 .",
    "we further assume that all ( or at least one ) of the probability densities , @xmath33 , are non - gaussian .",
    "this non - gaussianity is key since it allows the de - mixing matrix , @xmath21 , to be estimated . from the central limit theorem , we know that a convolution of any arbitrary probability distribution functions ( pdfs ) that feature a formal mean and variance , asymptotically approaches a gaussian distribution in the limit of large @xmath13 @xcite . in other words , the sum of any two non - gaussian pdfs ( ie . @xmath33 and @xmath34 ) is more gaussian than the respective original pdfs .",
    "therefore by maximising the non - gaussianity of the individual signals , we maximise their statistical independence . @xcite .",
    "although several measures of non - gaussianity exist ( we refer the reader to @xcite , @xcite , @xcite and @xcite for detailed summaries ) , we here use the concept of negentropy @xcite .",
    "negentropy is derived from the basic information - theoretical concept of differential entropy . in information theory , in close analogy to thermodynamics , the entropy of a system is at its maximum when all data points are at its most random . in thermodynamics we measure the distribution of particles , in information theory",
    "it is the probability distribution of a random variable . from information theory",
    "we can derive the fundamental result that a gaussian distribution has the largest entropy among all random variables of equal variance and known mean .",
    "hence , by minimising the entropy of a variable , we maximise its non - gaussianity . for a random vector @xmath35 , with random variables @xmath36 , @xmath37 , the entropy is given by    @xmath38    where h@xmath39 is the differential or shannon entropy @xcite and @xmath40 is the pdf of the random vector @xmath35 .",
    "h@xmath39 is at its minimum when @xmath41 is at its most non - gaussian .",
    "we can now normalise equation  [ shannon ] to yield the definition of negentropy :    @xmath42    where @xmath43 is a random gaussian vector with the same covariance matrix as @xmath35 .",
    "now @xmath35 is at its most non - gaussian when @xmath44 is at its maximum .",
    "it is important to note that negentropy is insensitive to a multiplication by a scalar constant .",
    "this has the important consequence of introducing a sign and scaling ambiguity into the retrieved signal components of @xmath17 .",
    "we will discussed the implications of this limitation in section  [ simu ] .      in practice",
    "it is very difficult to calculate the negentropy of a system and various methods were devised to approximate @xmath44 .",
    "the classic method is to measure the kurtosis of mean - subtracted @xmath35 with unit variance .",
    "however , kurtosis is very prone to distortions by outliers in the data and hence lacks the robustness required as measure of negentropy @xcite . to overcome this limitation , more robust measures of negentropy",
    "have been devised .",
    "one can approximate negentropy by equation [ negentropy2 ] @xcite    @xmath45 - \\text{e}[g(\\nu)])^{2 } \\label{negentropy2}\\ ] ]    where @xmath46 is a random gaussian variable with zero mean and unit variance and @xmath47 is a non - quadratic contrast function chosen to approximate the underlying probability distribution .",
    "there are usually three types of contrast functions : @xmath48 as general purpose function , @xmath49 optimised for super - gaussian ( leptokurtic ) distributions and @xmath50 optimised for sub - gaussian ( platykurtic ) distributions @xcite :    @xmath51 \\\\\\nonumber g_{2}(y ) & = -\\text{exp}(-y^{2}/2 ) \\\\\\nonumber g_{3}(y ) & = \\frac{1}{4}y^{4}\\end{aligned}\\ ] ]    the choice of contrast function is only important if one wants to optimise the performance of the ica algorithm as it is done for the efica @xcite algorithm where choices of contrast functions are tried iteratively .      finally , we can maximise the negentropy given in equation [ negentropy2 ] by finding a unit vector @xmath52 that maximises the non - gaussianity of the projection @xmath53 , so that @xmath54 is at its maximum . for the fixed - point fastica algorithm ,",
    "this can be achieved by the simple iterations scheme @xcite :    1 .",
    "choose initial ( i.e. random ) weight vector @xmath52 2 .",
    "increment : @xmath55 - \\text e [ g^ { ' } ( { \\bf w}^{t } { \\bf x } ) ] { \\bf w}$ ] 3 .",
    "normalise : @xmath56 4 .",
    "repeat steps 2 & 3 until converged    where @xmath57 and @xmath58 are the first and second derivatives of the contrast function @xmath59 .",
    "the iteration scheme above estimates only one weight vector at a time , for estimating all non - gaussian components in parallel the iteration scheme and derivates of @xmath59 are given in appendix [ efica ] and in the standard literature ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "for a comprehensive summary of other ica algorithms please refer to @xcite . throughout this paper , we use a variant of the fastica algorithm .",
    "projection pursuit and independent component analysis are inherently linked as both algorithms try to represent the most non - gaussian components in an multidimensional data set . in this sense",
    ", one can think of ica as a variant of pp with two major differences : 1 ) pp only estimates one non - gaussian component at a time whilst ica is the multivariate definition of pp and estimates all non - gaussian components simultaneously , 2 ) as opposed to ica , pp does not need an underlying data model and no assumptions about independent components are made .",
    "if the ica model holds , optimizing the ica non - gaussianity measures produce independent components ; if the model does not hold , then what we get are the projection pursuit directions @xcite .",
    "this is an important point to make with regards to time - consecutive transit observations , which break the underlying ica assumptions otherwise .",
    "following from the discussion above , we can understand the signal de - mixing to be a two step process : de - correlation of the gaussian components in the observed data using pca , followed by the de - correlation of non - gaussian components using ica and wasobi algorithms .",
    "the de - correlation of gaussian components to form a new uncorrelated vectors set can be understood as pre - processing step to the ica procedure .",
    "the algorithm proposed here consists of five main parts : 1 ) pre - processing of the observed data , 2 ) signal separation , 3 ) signal reconstruction 4 ) lightcurve fitting and 4 ) post - analysis .",
    "figure [ flow ] lays out the individual processing steps of the algorithm .      similarly to section  [ independent ] , the observed signals can be expressed as @xmath60 dimensional matrix * x * where each row constitutes an individual time series , @xmath16 , with @xmath61 number of data points .",
    "multiple time series observations are needed to separate the instantaneously mixed non - gaussian components .",
    "the process of identifying statistical independent components is greatly simplified if the input signals to any ica algorithm have previously been whitened ( also referred to as sphering ) . whitening is essentially a transformation of our input data matrix * x * into a mean subtracted , @xmath62 , orthogonal matrix @xmath63 , where its auto - covariance matrix , @xmath64 , equals the identity matrix , @xmath65 = i$ ] . the instantaneous mixing model for the whitened data is now given by    @xmath66    where @xmath67 is the inverse square root of @xmath68 and @xmath69 the corresponding mixing matrix of @xmath63 . for a more detailed explanation see appendix .",
    "this whitening is easily achieved by performing a principal component analysis on the data ( see appendix ) .",
    "this step has two distinct advantages :    \\1 ) it reduces the complexity of the un - whitened mixing matrix , @xmath19 , from @xmath70 parameters , to @xmath71 for a whitened , orthogonal matrix @xmath69 @xcite .",
    "2 ) using whitening by principal components , we can reduce the dimensionality of the data - set by only maintaining a sub - set of eigenvectors .",
    "this reduces possible redundancies of the components comprising the data and prevents the later to be employed ica algorithm from over - learning for over - complete sets .",
    "we also like to note that any type of additional linear signal cleaning or pre - processing step , such as those described by @xcite , are allowed .",
    "linear data filtering or cleaning can be understood as multiplying equation [ timeseries3 ] from the left with a linear transformation @xmath72 to get : @xmath73 .",
    "the underlying data model assumed in this paper is hence not affected .",
    "after the observed signals have successfully been whitened ( @xmath63 ) , we estimate the mixing matrix of the whitened signal , @xmath69 , using the multi - combi algorithm @xcite .",
    "multi - combi comprises two complimentary algorithms , efica @xcite and wasobi @xcite .",
    "efica , an asymptotically efficient variant of the fastica algorithm @xcite , is designed to separate non - gaussian , instantaneously mixed signals .",
    "wasobi , on the other hand , is an asymptotically efficient version of the sobi algorithm @xcite , and is geared towards separating gaussian auto - regressive ( ar ) and time - correlated components .",
    "it uses second - order statistics and can be understood to be similar to principal component analysis .",
    "the use of both algorithms is necessary since a real life data set will always contain a mixture of both , non - gaussian and gaussian ar processes . for a more in - depth discussion of the algorithms employed here , we like to refer the interested reader to the appendices  [ efica ] & [ wasobi ] and the original publications .",
    "the efica and wasobi algorithm can be shown to be asymptotically efficient , i.e. the estimators approach the cramr - rao lower bound @xcite . in other words ,",
    "the algorithms employed here can be shown to converge to the correct solution given the original source signals and in the limit of @xmath74 iterations . in reality",
    "the number of iterations is finite and and imperfect convergence results in traces of other sources to remain in the individual signals comprising @xmath75 .",
    "we can hence state that equation [ demix ] becomes    @xmath76    a measure of this error is the deviation of @xmath77 ( or @xmath78 for the whitened case ) from the unity matrix by inspecting the variance of its elements @xcite .",
    "this leads to the concept of the interference - over - signal ratio ( isr ) matrix .",
    "the isr is the standard measure in signal processing of how well a given signal has been transmitted or de - convolved from a mixture of signals .",
    "it can be understood as the inverse of the signal - to - noise ratio ( snr ) .",
    "the higher the isr for a specific signal , the less well has it been separated from the original mixture . for a real case example , @xmath19 is unknown and the isr needs to be estimated .",
    "an analytic approximation to the isrs for the efica and wasobi algorithms are found in the appendices  [ efica ] & [ wasobi ] .",
    "finally , we check the stability of the signal separation by perturbing the input matrix @xmath63 by a random and known matrix @xmath79 to give    @xmath80    we now re - run the multi - combi procedure using @xmath81 as input and estimate @xmath82 . knowing @xmath79 we can work backwards to obtain @xmath69 as we are dealing with a linear transformation .",
    "this step is repeated several times to check the convergence of the algorithm by inspecting the variation on the mean isr values of each separation attempt and the variations in consecutive estimations of @xmath69 directly .      once the mixing matrix ,",
    "@xmath69 is estimated , we need to identify which signals are astrophysical , which ones are white and which are systematic noise .",
    "this is done in a two step process :    \\1 ) we construct the estimated signal matrix , @xmath83 , and for its individual components @xmath84 compute the pearson correlation coefficient between @xmath84 and the first principal component of the pca decomposition in section [ pca ] . for medium",
    "signal to noise ( snr ) observations , the first principal component ( pc ) , ie .",
    "the one with the highest eigenvalue associated to it , will contain the predominant lightcurve shape .",
    "as previously discussed , the first pc is not perfectly separated from the systematic signals and hence can not be used directly for further analysis but it is good enough to use it as lightcurve identification .",
    "the identified lightcurve signal is labeled @xmath85 .",
    "\\2 ) once the lightcurve signal is identified , we exclude this row from @xmath83 and proceed to classify the remaining signals with respect to their non - gaussianity ( ie .",
    "systematic noise sources ) . here",
    "we use the ljung - box portmanteau test ( see appendix and * ? ? ?",
    "* ) to test for the hypothesis that the time series is statistically white ( ie .",
    "gaussian ) .",
    "this test was originally designed to check the residuals of auto - regressive moving - average ( arma ) models for significant departures from gaussianity .",
    "it is hence ideally suited for our need to identify which estimated signal components are the desired non - gaussian ones .",
    "the identified non - gaussian , systematic noise , signals are hence labeled @xmath86 and the remaining white noise signals @xmath87 to give    @xmath88    where @xmath83 has dimensions @xmath89 and @xmath85 , @xmath86 , @xmath87 , have dimensions of @xmath90 , @xmath91 and @xmath92 respectively where @xmath93 .",
    "the de - mixing matrix is given by @xmath94 .",
    "as previously mentioned , the components of @xmath83 have ambiguities in scaling and sign and can be thought to be similar to the eigenvectors of a principal component analysis with missing eigenvalues .",
    "fortunately there exist two approaches to resolving this degeneracy :    1 .",
    "in the case of @xmath85 being well separated as individual component , we can take @xmath85 and the de - mixing matrix @xmath95 and only retain the row containing the astrophysical signal component forming the row - vector @xmath96 .",
    "we then reconstruct the original data @xmath63 using only the separated signal component : + @xmath97 + where @xmath98 is the reconstructed whitened data with all but the astrophysical signal components removed .",
    "using equation [ whitening ] , we can now calculate the un - whitened matrix @xmath99 .",
    "+ @xmath100 + hence we can think of @xmath101 as a linear , optimal filter for the signal component in @xmath102 .",
    "please note that this linear filtering does not impair the scaling information as this is re - instated going from @xmath85 to @xmath103 .",
    "2 .   in the case of @xmath85 not being well separated but other systematic noise components are , a different , more indirect approach can be used . here , the systematic noise components , @xmath86 which do not contain sign or scaling information , are simultaneously fitted to the time series data ( preferably out - of - transit data ) , @xmath16 .",
    "we therefore define the systematic noise model for an individual time series by @xmath104 , + @xmath105 + where @xmath106 is a k @xmath107 k diagonal scaling matrix of @xmath86 , which needs to be fitted iteratively as free parameters in the following section .",
    "having either filtered the data to obtain @xmath99 or constructed the noise model @xmath104 , we can now fit the original time series , @xmath16 using the standard analytical lightcurve models @xcite in addition to the diagonal matrix @xmath106 , if necessary .",
    "for the purpose of this paper , which focuses on blind - source - separation , we will restrict ourselves to demonstrating the feasibility of estimating @xmath106 and only leave the transit depth as variable lightcurve parameter .",
    "we use the analytic lightcurve model by @xcite and a nelder - mead minimisation algorithm @xcite . for real data applications ,",
    "we advise the reader to use markov chain monte carlo methods , or similar , which have become standard in the field of exoplanets and allow the estimation of the posterior probability distributions and their associated errors @xcite .",
    "once the model fitting stage has been completed , we are left with fitting residual , @xmath108 , i.e. @xmath109 .",
    "several tests are useful to determine how well the signals have been removed from the original time series , @xmath16 . in the case of a full markov chain monte carlo fitting , the posterior distributions of the fitting parameters",
    "may be used to asses the impact of the remaining systematic noise in the data when compared to a simulated data set only containing white noise .",
    "portmanteau tests on individual time series are useful to test for non - white noise signals and allow a measure of the overall performance of the algorithm @xcite .",
    "additionally , we can determine the kullback - leibler divergence @xcite of our residual s probability distribution function ( pdf ) to an idealised gaussian case .",
    "for the simulations and real - data examples presented in the following section , we have merely plotted the autocorrleation functions ( acf ) of the residuals obtained to determine whether for a given lag , these are within the 3@xmath110 confidence limit of the residual being dominated by white - noise @xcite . here the acf is given by :    @xmath111    where @xmath61 is the number of data points in the time series ,",
    "@xmath112 the specific lag and the confidence intervals are given by @xmath113 .",
    "in order to test the behaviour and efficiency of the algorithm described above , we produced a toy model simulation with five observed signals : 1 ) a secondary eclipse @xcite lightcurve ; 2 ) a sinusoidal signal ; 3 ) a sawtooth function ; 4 ) a fourth order auto - regressive signal to simulate time - correlated signals ; 5 ) gaussian noise with a full width half maximum ( fwhm ) of 0.01 magnitudes .",
    "the premixed signals are displayed in figure [ ex1premix ] .",
    "this gives us our signal matrix , @xmath75 , which needs to be recovered later on .",
    "we have then proceeded to mix the signals in figure [ ex1premix ] using a random mixing matrix , @xmath19 , to obtain our observed signals , @xmath102 , in figure [ ex1mix ] . for the sake of comparability",
    "we keep the mixing matrix @xmath19 to be the same for all simulations .",
    "we now subdivide the simulations to illustrate the two possible methods of the signal reconstruction .",
    "_ method _  1 computes @xmath114 using equation [ sigcomp2 ] whilst _ method _  2 fits the noise model @xmath104 ( equation [ noisemodel2 ] ) simultaneously with the @xcite lightcurve .",
    "these two examples demonstrate that both techniques work equally well for a well behaved data set .",
    "in this example , we use the observed signals in figure [ ex1mix ] as input to the algorithm .",
    "we however do not perform a dimensionality reduction using pca since we are not dealing with an over - complete set in this example .",
    "the results of the separation are shown in figure [ ex1postmix ] . here the top three , red lightcurves are the estimated systematic noise components as identified by the algorithm .",
    "the fourth component is gaussian noise and the bottom is an inverse of the lightcurve signal .",
    "it should again be noted here that the blind - source separation does not preserve the scaling nor the signs of the signals in @xmath83 .",
    "however , when the original data is reconstructed using only the signal component , @xmath85 , to obtain @xmath114 ( equation [ sigcomp2 ] ) , the scaling and sign informations are re - instated . for a well behaved data set , i.e. one that obeys the instantaneous mixing model and has negligible gaussian noise in their signal components , it is therefore possible to re - construct the lightcurve signal from the raw data as explained in section [ nmodel ] .",
    "figure [ ex1filter ] shows the top lightcurve of figure [ ex1mix ] ( blue circles ) and overplotted the retrieved signal component ( red crosses ) and offset below the systematic noise component ( black squares ) .    as a useful by - product of the algorithm",
    ", we obtain the interference over signal matrices ( isr , equations [ isref ] @xmath115 [ isrwa ] in the appendix [ bss ] ) for both the efica and wasobi algorithms .",
    "these give us valuable information on the efficiency at which the signals have been separated .",
    "figure [ ex1isrs ] shows the hinton diagrams of the efica and wasobi @xmath116 matrices . here , the smaller the off - diagonal elements of the matrix , the better the signal separation .",
    "in this example , the efica algorithm outperforms the wasobi one , which is to be expected since all signals but one are non - gaussian .      in the previous section",
    ", we have shown that in the case that the astrophysical component @xmath85 is well separated as individual signal , we can create a filter for the raw data that directly filters the lightcurve signal from the noise . however , in most real data applications , @xmath85 , is not perfectly separated but the components of @xmath86 may be more so . in this case",
    "we can construct the noise model @xmath104 given by equation [ noisemodel2 ] and the diagonal elements of @xmath106 are fitted as described in section [ fitting ] .",
    "the starting position of the algorithm is the same as for the previous example ( figure [ ex1mix ] ) .",
    "the model fit of the first lightcurve in figure [ ex1mix ] and its residuals are shown in figure [ ex1fitted ] . the autocorrelation function for 100 lags",
    "is plotted in figure [ ex1auto ] .",
    "all but two lags are within the 3@xmath110 confidence limit that the residual is white noise dominated , indicating that all signals have been removed effectively .",
    "finally we simulate the convergence properties of both efica and wasobi under varying white noise conditions . here",
    "we repeatedly run the algorithm until signal separation is completed and record the mean isrs of the source separation .",
    "we performed this simulation 300 times for gaussian noise fwhms varying from 0.0 - 0.3 magnitudes ( figure [ ex1fitted ] has a fwhm@xmath117 = 0.01 ) and every isr measurement reported is the mean of 10 iterations .",
    "figure [ ex1isr2 ] summarises the results . here",
    ", the red circles represent the mean isr or the efica algorithm and the blue crosses that of wasobi .",
    "it can clearly be seen that for this example the efica algorithm outperforms wasobi and on average reaches lower isr values .",
    "we can further note that the blind source separation is not significantly affected by the magnitude of the white noise and performs well under difficult signal to noise conditions .",
    "the previous examples illustrated the two methods available to correct the observed data : _ method 1 _ : filtering the astrophysical signal out of the systematic noise or _ method 2 _ : fitting a systematic noise model to the raw data .",
    "here we apply these techniques to two primary eclipse data sets of hd  189733b and xo1b recorded by the nicmos instrument on the hubble space telescope as well as a single time - correlated time series obtained by the kepler spacecraft .",
    "first presented by @xcite , this data set of the primary eclipse of hd  189733b was recorded using hst / nicmos in the g206 grism setting spanning five consecutive orbits .",
    "the hst - pipeline calibrated data were downloaded from the mast archive and the spectrum was extracted using both standard iraf routines as well as a custom built routine for optimal spectral extraction .",
    "both extractions are in good accord with each other but the custom built routine was found to yield a better signal to noise and was subsequently used for all further analysis .",
    "a binning of 10 spectral channels ( @xmath118 0.08@xmath119 m ) was used resulting in 10 light curves across the g206 grism band .",
    "figure [ hd189rawlc ] shows the obtained time series which serve as input to the algorithm .",
    "it can be seen that each time series is strongly affected by instrument systematics propagating from the blue side of the spectrum ( bottom light curve ) to the red with varying intensity and even sign .",
    "@xcite showed that these systematics are correlated to instrument state vectors such as orbital phase , relative positions and angles of the spectrum on the detector , instrument temperature , etc .",
    "we can hence expect that these systematics are statistically independent from the recorded astrophysical signal ( the light curve ) and it should therefore be in principle possible to de - correlate the signal .",
    "we here demonstrate the de - trending on an individual light curve at @xmath1181.694@xmath119 m ( 8@xmath120 one down in figure [ hd189raw ] ) .",
    "all time series in figure [ hd189raw ] were taken as input to the algorithm described above to estimate the de - mixing matrix @xmath95 , the astrophysical signal vectors , @xmath121 and the systematic noise vectors , @xmath122 .",
    "the interference over signal ( isr ) matrix indicated the good separation of four main components figure [ hd189hinton ] with the rest of the components being classified as predominantly gaussian or weakly systematic .",
    "the existence of more than one gaussian component ( @xmath123 ) indicates that the set is overcomplete .",
    "however since the data - set is small enough , no pca dimensionality reduction was performed .",
    "after the algorithm has identified the correct astrophysical signal , it proceeded to reconstruct the light curve using both methods described above .",
    "_ method 1 _ : the astrophysical signal was filtered using equations [ sigcomp ] @xmath115 [ sigcomp2 ] .",
    "figure [ hd189componly ] shows the raw light curve ( blue circles ) with the de - trended time series , @xmath114 underneath ( green squares ) .",
    "superimposed light curves were computed using @xcite with orbital parameters were taken from @xcite and limb - darkening parameters from @xcite .",
    "it is clear that the de - trended light curve is an improvement to the raw time series but that systematics still remain in the data .",
    "this is further illustrated by plotting the autocorrelation function of the model - fit residual in figure [ hd189acf ] ( red circles ) . here , residual correlation can be observed in particular at low lags .",
    "this is a consequence of the astrophysical signal , @xmath121 , being well separated but as shown in figure [ hd189hinton ] ( component 1 ) , there remains some weak interference between the @xmath121 and other vectors , which is a consequence of equation [ demix2 ] and to be expected for real data - sets .",
    "_ method 2 _ : the second method is a less direct approach . instead of filtering for the astrophysical signal directly , we try to construct a systematic noise model that is then subtracted off the raw data . using equation [ noisemodel2 ] and a simplex downhill algorithm @xcite we estimated the scaling matrix , @xmath106 , by fitting the the systematic noise vectors , @xmath122 to the four out of transit orbits .",
    "the scaled systematic noise vectors are shown in figure [ hd189snvec ] which combine to form the systematic noise model , @xmath104 , in figure [ hd189raw ] .",
    "it should be noted that @xmath106 is only a scaling matrix of the individual vectors as the scaling information is not preserved by the independent component analysis .",
    "hence , relative intra and inter - orbit variations are preserved .",
    "figure [ hd189corr ] shows the corrected data by subtracting the systematic noise model off the raw data . inspecting the fitting residual s autocorrelation function in figure [ hd189acf ] ( black circles ) indicates the residual to be statistically white and",
    "a maximal de - correlation of the data has been achieved .",
    "originally presented by @xcite , the primary eclipse of xo1b was observed using the hst / nicmos instrument in the g141 grism setting .",
    "the hst - pipeline calibrated data was downloaded and the spectra extracted using the same settings as for section [ hd189sect ] .",
    "this yielded 10 light curves and which serve as input to the algorithm , see figure [ xo1rawlc ] .",
    "similar to the hd  189733b the algorithm retrieved four main components , the light curve signal and three main systematic noise components .",
    "the isr matrix can is shown in figure [ xo1hinton ] .",
    "we now proceeded to de - trending the light curve at the very red end of the spectrum ( first from top in figure [ xo1rawlc ] ) as it , after visual inspection , exhibits the most prominent systematics of the 10 time series .",
    "light curve fits assumed limb - darkening and orbital parameters by @xcite .",
    "_ method  1 _ : figure [ xo1componly ] shows the raw time series and the de - trended light curve using equation [ sigcomp2 ] .",
    "the light curve is significantly de - trended but systematics remain in the data as also shown by the autocorrelation function ( red circles ) in figure [ xo1acf ] .",
    "_ method  2 _ : as described in previous sections , figure [ xo1snvec ] shows the retrieved systematic noise vectors and figure [ xo1raw ] features the raw data with the combined systematic noise model ( red ) underneath . the autocorrelation function of the model fit residual is shown in figure [ xo1acf ] ( black crosses ) and shows a factor 2 improvement on the de - correlation in the lower lags .",
    "figure [ xo1comp2 ] compares the de - trended light curves of _ method  1 _ and _ method  2 _ and shows the residual of _ method  1 _ - _ method  2 _ ( black crosses ) .",
    "there is little difference between both methods indicating that the signal separation for this data - set is close to its maximum with the data being partially de - correlated .",
    "this is in contrast to the hd  189733b example where a near perfect de - correlation was achieved and can be attributed to the systematics being mostly wavelength invariant in the case of xo1b .",
    "in other words , systematic noise components which have a constant weighting throughout the data set can not be de - correlated using ica or pca methods , which is to be expected following equations [ intro1 ] - [ timeseries3 ] .",
    "in the previous examples we have shown that spectroscopic datasets can be de - correlated effectively .",
    "we test here how well the proposed algorithm can de - correlate consecutively observed data - sets .",
    "this is of particular interest in cases where no multi - channel data are available and the time series data are contaminated with time - correlated noise , be it stellar or instrumental . as opposed to the previous examples , where several time series , @xmath16 , were observed simultaneously , here we take a single time series covering several consecutive eclipse features and cut the time series into segments spanning equal lengths over each eclipse event . using these segments as inputs to the algorithm clearly violates the underlying assumptions of the independent component analysis , as the mixing is not instantaneous . in this case , the ica analysis can be understood as a projection pursuit ( pp ) analysis , see section  [ projectionpursuit ] and @xcite . here",
    "the ica algorithm , in the absence of a working ica data - model , will try to extract as many non - gaussian components as possible and return the rest of the data in its original form .",
    "this is very similar to projection pursuit , where the data is not described by an underlying data model at all but only the most non - gaussian component is retrieved . in other words , we can only expect to retrieve the eclipse signal component , @xmath85 , with any degree of accuracy . as a result we will not be able to retrieve systematic noise components , @xmath86 , and we can only use @xmath124 ( in section [ nmodel ] ) to de - trend the data .",
    "we have downloaded data observed by the kepler space telescope @xcite for a planet - hosting candidate star observed over the second and third data - release quarters ( q2 & q3 ) .",
    "the time series , with the kepler i d : 10118816 , exhibits highly variable features and significant time - correlated noise ( see figure [ kepler3 ] , blue crosses ) .",
    "given kepler s superb instrument calibration , we can assume this time - correlated noise to be due to stellar variability .",
    "using the periodogram calculator on the nsted database ] , we identified four main periodically recurring signals in the data - set . choosing the second strongest feature with a period of 0.040915 days , we phase folded the data and cut the time series in 10 equally sized segments .",
    "as for the previous examples we now took these time series segments as input to the algorithm .",
    "we performed our de - correlation as for the previous examples but using _ method  1 _ only .",
    "figure  [ kepler4 ] shows the isr matrix of the separation indicating a relatively poor separation of the components but two ( components 4 & 9 ) . as discussed above , this behaviour is to be expected with the breaking of the instantaneous mixing model .",
    "nonetheless , we obtained a clear feature ( component 4 ) in our analysis which is over plotted ( red circles ) on the mean , phase - folded data ( blue crosses ) in figure  [ kepler1 ] . here",
    "the de - correlated signal has a much reduced scatter compared to the mean of the phase - folded feature , which indicates that much of the unwanted stellar variability has been removed .",
    "it is also clear from this figure that we are not dealing with an exoplanetary light curve but a stellar pulsation feature .",
    "as expected , the remaining components returned from the algorithm ( figure  [ kepler2 ] ) are the residuals of the input data minus the component shown in figure  [ kepler1 ] .",
    "hence we only used the component in figure  [ kepler1 ] to re - construct the original time series .",
    "this was done by using equation [ sigcomp2 ] on each segment of the time series , followed by adding the segments back together in the order they were originally split up .",
    "figure  [ kepler3 ] shows the original input data ( blue crosses ) with the filtered signal ( red circles ) over plotted on top . in the bottom plot ( a zoomed in version of the time series ) , it is clear that the desired feature remains in the filtered time series whilst the contribution of other time - correlated stellar noise is substantially reduced .",
    "in the previous sections we have shown that for a set of simultaneously observed time series data ( e.g. following an exoplanetary eclipse with a spectrograph ) we can describe the data by an instantaneous mixing model ( equation [ timeseries3 ] ) .",
    "this allows the separation of non - gaussian , time and spatially - correlated signals from one another .",
    "the degeneracy caused by not being able to retrieve the component s signs or amplitudes can be circumvented in two ways : _ method 1 _ ) the separated signals are used to construct a linear transformation to filter the astrophysical signal from the originally observed data and hence preserve all scaling information ; _ method 2 _ ) the separated astrophysical signal is not used directly but instead all systematic noise components are combined to form a ` systematic noise model ' which can then be used to correct the original observed data .    we have explored the efficiency of the signal de - trending on two simulated and two hst / nicmos data sets with different types of systematic noise due to different grisms .",
    "the simulations demonstrate the two methods of de - trending the data in an idealised case and explore the efficiency of the signal separation in the presence of varying gaussian noise in the data . in the instantaneous mixing model employed here ,",
    "gaussian noise sources are only indirectly allowed and can interfere with the effectiveness of separating non - gaussian vectors .",
    "we tested this point by adding additional gaussian noise components of variable amplitude to the simulations but did not observe any significant reductions in the signal separation efficiency .",
    "we proceeded to analyse two hst / nicmos data sets : the primary eclipses of hd189733b and xo1b . for both data sets we find the _ method 2 _ to yield better results .",
    "in the case of hd189733b , we can achieve a near perfect de - correlation of astrophysical signal and systematic noise and no further steps are necessary to the de - correlation process . a more in depth discussion of this data set and hst / nicmos systematics",
    "is beyond the scope of this publication . in the case of xo1b",
    "the de - correlation is significant but incomplete .",
    "the difference in maximum de - correlation achievable can be attributed to the systematic noise sources being strong functions of wavelength in the case of hd189733b whilst almost with constant weighting ( @xmath125 in equation [ intro2 ] ) in the case of xo1b .",
    "whenever systematics have constant weighting per channel observed ( @xmath16 ) and/or time , it becomes very difficult for pca or ica based approaches to de - correlate the signal from the systematics .",
    "here auxiliary information of the instrument is needed to proceed with the de - correlation process .",
    "this is very well possible in the case of dedicated instruments such as kepler , as these have been specifically designed with such high precision and stability measurements in mind @xcite . for instruments that do not feature the calibration plan required to further de - correlate with instrument state parameters ,",
    "the solution is far less obvious .",
    "we furthermore explored the de - correlation of eclipse signals observed consecutively rather than in parallel . we demonstrated , using kepler data , that despite the formal violation of the ` instantaneous mixing model ' , the proposed algorithm is able to retrieve the desired signal component with good accuracy .",
    "such an application is particularly important for treating variability of the host - star which can significantly impair the quality of the final science result ( eg .",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "it is furthermore interesting to note that pre and post - processing steps ( e.g. wavelets @xcite , fourier based techniques @xcite , de - correlation using instrument state parameters @xcite ) , do not break the instantaneous mixing model and can be run in conjunction with ica methods .",
    "this makes independent component analysis a very powerful and versatile tool for non - parametric de - correlation of exoplanetary data sets .",
    "in the light of searching and characterising ever smaller and fainter exoplanetary targets , the development of novel de - trending routines becomes increasingly critical . based on the concepts of blind source deconvolution of instantaneously mixed signals ,",
    "we have presented a first step towards non - parametric corrections and data filters that do not require additional information on the systematic noise of the instrument or stellar activity .",
    "such algorithms have two important applications :    \\1 ) for instruments that lack a calibration plan at the accuracy of 10@xmath0 in flux variation , which is required for spectroscopy of exoplanetary atmospheres , the spectroscopic signatures become inherently entangled and dependent on the method used to correct instrument and other systematics in the data .",
    "the de - correlation of spectroscopic data was demonstrated using two hst / nicmos data sets .",
    "\\2 ) detections of faint exoplanetary eclipses are often made difficult by time - correlated activity of the host star .",
    "we demonstrated , using a single kepler time series , that much of the stellar variability can be removed in time series that span several exoplanetary eclipse events .",
    "the algorithm proposed is a powerful tool for lightcurve de - trending , which can be used by its own or in conjunction with any other type of data filtering or cleaning technique .",
    "this becomes an invaluable advantage for data analysis when the instrument s response function is unknown or poorly characterised .",
    "i.p.w . would like to thank prof .",
    "e. feigelson , the referee , dr .",
    "g. tinetti , dr .",
    "f. abdalla and dr .",
    "s. fossey for comments and suggestions that helped to greatly improve this paper .",
    "i.p.w . is supported by an stfc studentship .",
    "aigrain , s. , pont , f. , zucker , s. , 2011 , arxiv : 1110.1034    , e. , cowan , n.  b. , knutson , h.  a. , deming , d. , et al . , 2010 , apj , 721 , 1861    aumont , j. & macas - prez , j. , f. , 2007 , mnras , 376 , 739    bakos , g.   . , shporer , a. , pl , a. , torres , g. , kovcs , g. et al . , 2007 , apjl , 671 , l173    ballerini , p. , micela , g. , lanza , a.  f. , pagano , i. , 2011 , a&a , submitted    bean , j.  l. , , 2011 , nature , 478 , 41    , j.  l. , dsert , j .-",
    "m . , kabath , p. , et al . , 2011 ,",
    "apj , 743 , 92    beaulieu , j. p. ; carey , s. ; ribas , i. and tinetti , g. , 2008 , apj , 677 , 1343    beaulieu , j.  p. , kipping , d.  m. , batista , v. , tinetti , g. , ribas , i. , carey , s. et al . , 2010 , arxiv,0909.0185    belouchrani a. , abed - meraim k. , cardoso j.f . , moulines e. , 1997 , ieee trans .",
    "signal processing , vol .",
    "45 , 434    boisse , i. , bouchy , f. , hbrard , g. , et al . , 2011 ,",
    "a&a , 528 , a4    borucki , w. j. , dunhm , e. w. , koch , d. g. , cochran , w. d. , et al . , 1996 ,",
    "astrophys . & space science 241 , 111    , w.  j. , koch , d. , basri , g. , et al . , 2010 , science , 327 , 977    , w.  j.,koch , d.  g. , basri , g. et al .",
    "2011 , apj , 736 , 19    brillouin , l. , 1953 , j. of applied physics , 24 , 1152    brockwell p.j .",
    ", richard a.d . , 2006 ,",
    "time series : theory and methods ( second edition) , springer verlag , isbn : 1 - 4419 - 0319 - 8    , h. , deleuil , m. , fridlund , m. , alonso , r. , et al .",
    ", 2010 , a&a , 519 , a51    burke , c.  j. , mccullough , p.  r. , valenti , j.  a. , johns - krull , c.  m. , janes , k.  a. et al . , 2007 , apj , 671 , 2115    burke , c.  j. , mccullough , p.  r. , bergeron , l.  e. , et al . , 2010 , apj , 719,1796    , d.  a. , kolodziejczak , j.  j. , van cleve , et al . , 2010 , apjl , 713 , l92    carter j.a .",
    ", winn j.n . ,",
    "apj , 2009 , apj , 704 , 51 t    collier cameron , a. , wilson , d.  m. , west , r.  g. , hebb , l. , et al .",
    ", 2007 , mnras , 380 , 1230    charbonneau , d. , brown , t. , m. , latham , d. , w. , mayor , m. , 2000 , apj , 529 , l45    charbonneau , d. , brown , t.  m. , noyes , r.  w. and gilliland , r.  l. , 2002 , apj , 568 , 377    , d. and allen , l.  e. , megeath , s.  t. , torres , g. , alonso , r. , brown , t.  m. , et al . , a. , 2005 , apj , 626,523    , d. , knutson , h.  a. , barman , t. , allen , l.  e. , mayor , m. , et al . ,",
    "s. , 2008 , apj , 686,1341    cichocki a. , amari s. , 2002 , john wiley & sons inc .",
    ", adaptive blind signal and image processing , isbn : 0471607916    claret , a. , 2000 , a@xmath115a , 363 , 1081    comon , p. , 1994",
    ", signal processing , 36 , 287    comon , p. , & jutten , c. , 2000 , handbook of blind source separation : independent component analysis and applications , academic press    czesla , s. , huber , k. , f. , wolter , u. , schrter , s. , schmitt , h. , m. , m. , 2009 , a&a , 505 , 1277    davison , a.  c. , 2009 , cambridge university press , statistical models , isbn : 9780521734493    , j. , cardoso , j .- f . & patanchon , g. , 2003 , mnras , 346 , 1089    , d. , richardson , l.  j. & harrington , j. , 2007 , mnras , 378 , 148    doron e. , yeredor a. , 2004 , proc . of ica 2004 ,",
    "390    ford e. b. , 2006 , apj , 642 , 505    friedman , j. , h. , 1987 , j. of the american statistical association , 82 , 249    , n.  p.,pont , f. & aigrain , s. , 2011 , mnras , 411 , 2199    gibson , n. p. , aigrain , s. , roberts , s. , et al . , 2011 , arxiv:1109.3251    , m. and lanotte , a.  a. , barman , t. , miller , n. , demory , b .- o . , et al . , j. , 2010 , a&a , 511 , 3    , p.  c. , 2011 , mnras , 410 , 94    grillmair , c.  j. , burrows , a. , charbonneau , d. , armus , l. , stauffer , j. , meadows , v. , van cleve , j. , von braun , k. & levine , d. , 2008 , nature , 456 , 767    harman , h. , h. , 1967 , modern factor analysis , university of chicago press , 2nd edition    , a.  p. , fridlund , m. , nachmani , g. , mazeh , t. , et al .",
    "2010 , arxiv : 1105.3372v1    huber , p. , j. , 1985 , the annals of statistics , 13 , 435    hyvrinen a.,1999 , ieee trans . on neural networks , 10(3 ) ,",
    "626    hyvrinen a. , pajunen , p. , 1999",
    ", neural networks , 12 , 429    hyvrinen a. , oja , e. , 2000 , neural networks , 13 , 411    hyvrinen a. , karhunen j. , oja e. , 2001 , john wiley & sons inc . , independent component analysis , isbn : 0 - 471 - 40540-x    , j.  m. , caldwell , d.  a. , chandrasekaran , et al .",
    ", 2010 , apjl , 713 , l87    jolliffe i.t . , 2002",
    ", springer verlag new york inc .",
    ", isbn : 0 - 387 - 95442 - 2    , h.  a. , charbonneau , d. , allen , l.  e. , fortney , j.  j. , et al . , 2007 , nature , 447,183    , h.  a. , charbonneau , d. , noyes , r.  w. , brown , t.  m. , gilliland , r.  l. , 2007 , apj , 655,564    , h.  a. , madhusudhan , n. , cowan , n.  b. , christiansen , j.  l. , agol , e. , deming , d. , et al .",
    ", 2011 , arxiv : 1104.2901    , d.  g. , borucki , w.  j. , basri , g. , et al . , 2010 , apjl , 713 , l79    koldovsk z. , tichavsk p. , oja e. , ieee trans .",
    "on neural networks , 17(5),1265    koldovsk z. , tichavsk p. , oja e. , 2005 , proc.of ieee / sp 13th workshop on stat .",
    "signal processing    kullback , s. , leibler , r.a . , 1951 , annals of mathematical statistics 22 , 79    lagarias , j.c . ,",
    "j. a. reeds , m. h. wright , & p. e. wright , 1998siam journal of optimization , 9(1 ) , 112    lu , h. , zhou , h. , wang , j. , et al . , 2006 , apj , 131 , 790    , d. , farusi , a. , baccigalupi , c. , et al .",
    ", 2002 , mnras , 334 , 53    maino , d. , donzelli , s. , banday , a. , j. , stivoli , f. , baccigalupi , c. , 2007 , mnras , 374 , 1207    mandel , k.and agol , e. , 2002 , apjl , 580 , l171    manly , b. , j. , f. , 1994 , multivariate statistical methods - a primer , 2nd ed . ,",
    "chapman & hall    marcy , g. , w. , butler , r. , p. , vogt , s. , s. , fischer , d. , lissauer , j. , j. , 1998 , apjl , 505 , 147    mayor , m. , queloz , d. , 1995 , nature , 378 , 355    nadaraya , e. a. , 1964 , prob . & its applic . , 10 , 186    nelder , j.  a.m mead , r. , 1965 , computer journal , 7 , 308    oja , e. , 1992 , neural networks , 5 , 927    pearson , k. , 1901 , phil .",
    ", 2 , 559    , f. , aigrain , s. & zucker , s. , 2010 , mnras , 411 , 1953    press , w.  h. , teukolsky , s.  a. , vetterling , w.  t. , flannery , b.  p. , 2007",
    ", numerical recipes , cambridge uni . press , isbn :",
    "978 - 0 - 521 - 88407 - 5    redfield , s. , endl , m. , cochran , w.  d. and koesterke , l. , 2008 , apjl,673,l87    riley , k.  f. , hobson , m. ,  p. , bence , s.  j. , 2002 , mathematical methods for physics and engineering , cambridge uni . press , isbn : 0 - 521 - 89067 - 5    , j. , dedieu , c. , le sidaner , p. , savalle , r. , zolotukhin , i. , 2011 , a&a , 532 , 79    , s. and malln - ornelas , g. , 2003 , apj , 585 , 1038    shannon , c. , 1948 , bell system tech . j. , 27 , 379    , d.  k. , pont , f. , aigrain , s. , et al . , 2011 , mnras , 416 , 1443    , i.  a.  g. and albrecht , s. and de mooij , e.  j.  w. and le poole , r.  s. , 2008 , a&a , 487 , 357    , i.  a.  g. and de kok , r.  j. and de mooij , e.  j.  w. and albrecht , s. , 2010a , nature , 465 , 1049    , i.  a.  g. and de mooij , e.  j.  w. and burrows , a , 2010b , a&a , 513 , 76    , k.  b. , harrington , j. , nymeyer , s. , madhusudhan , n. , seager , s. , et al .",
    ", 2010 , nature , 464 , 1161    stivoli , f. , baccigalupi , c. , maino , d. , stomper , r. , 2006 , mnras , 372 , 615    stone , j. , v. , 2004 , independent component analysis : a tutorial introduction , a bradford book    swain , m.  r.,vasisht , g. and tinetti , g. , 2008 , nature , 452 , 329    , m.  r. , vasisht , g. , tinetti , g. , bouwman , j. , chen , p. , et al , 2009 , apjl , 690 , l114    , m.  r. , tinetti , g. , vasisht , g. , deroo , p. , griffith , c. , et al . ,",
    "d. , 2009 , apj , 704 , 1616    tichavsk p. , doron e. , yeredor a. , gomez - herrero g. , 2006 , proc .",
    "eusipco-2006    tichavsk p. , doron e. , yeredor a. , nielsen j. , 2006 , proc .",
    "eusipco-2006    , g.,vidal - madjar , a. , liang , m .- c . , beaulieu , j .-",
    "p . , yung , y. , et al . , f. , 2007 , nature , 448 , 169    , g. , deroo , p. , swain , m.  r. , griffith , c.  a. , vasisht , g. , et al . , p. , 2010",
    ", apjl , 712 , l139    , a. and deroo , p. and swain , m.  r. , 2010 , a&a , 523 , 35    waldmann i.p . , drossart p. , tinetti g. , griffith c.a . , swain m. , deroo p. , apj , in press    wang , j. , xu , h. , gu , j. , an , t. , 2010 , arxiv : 1008.3391v1    watson , g.s . , 1964 , sanky series a , 26 , 359    winn , j.  n. , holman , m.  j. , henry , g.  w. , et al . , 2007 , aj , 2007 , 133 , 1828    yeredor a. , 2000 , ieee sig . proc",
    ". letters , 7 , 197    this appendix provides some additional notes to the methods employed in this paper . for a more in - depth discussion of the topics presented here , please refer to the cited publications .",
    "in gaussian statistics , our probability densities are fully defined by the first and second statistical moments , i.e. their means and covariances .",
    "two random vectors , @xmath126 and @xmath127 , are said to be uncorrelated when their covariance ( @xmath128 ) is zero :    @xmath129 ) ( { s_{l+1 } } - e [ { s_{l+1 } } ] ) ]   \\\\\\nonumber & = & e[{s_l } , { s_{l+1 } } ] - e [ { s_l}]e [ { s_{l+1 } } ] = 0\\end{aligned}\\ ] ]    where @xmath130 $ ] is the expectation value of @xmath31 which can be approximated by the mean in this case by    @xmath131 \\approx \\frac{1}{m } \\sum_{t=1}^{m}s_{l}(t ) \\label{expectation}\\ ] ]    with @xmath132 being the number of data points in the time series .",
    "furthermore , we define two random variables ( @xmath31 and @xmath133 ) to be orthogonal , when both their expectation values , in addition to their covariance are zero :    @xmath134 = \\text{e}[{s_{l+1 } } ] = { \\bf c}_{{s}_l , { s}_{l+1 } } = 0\\ ] ]    we can always find an affine , linear transformation from a correlated set of variables to an orthogonal one .    finally , our two random vectors @xmath31 and @xmath133 are independent from one another if and only if the joined probability distribution @xmath135 of both signals are factorizable into the product of their marginal pdfs , @xmath136 and @xmath137 :    @xmath138    and satisfy the property    @xmath139 = \\text{e } [ g ( {   s_{l } } ) ] \\text{e } [ h ( { s_{l+1 } } ) ]   \\label{independence2}\\ ] ]    where @xmath140 and @xmath141 are absolutely integrable functions of @xmath142 and @xmath143 respectively . from the definition of independence in equation [ independence2 ] , we obtain the definition of uncorrelatedness ( equation [ ortho ] ) in the special case where both @xmath144 and @xmath145 are linear and are only defined by their covariances ( i.e. no higher order statistical moments ) @xcite . in other words , uncorrelatedness is a special case of independence .",
    "uncorrelated gaussian random variables are always also independent and the definitions of uncorrelatedness , orthogonality ( for zero mean ) and statistical independence become identical .",
    "the covariance matrix of @xmath102 , @xmath146 , is given by @xmath147 , where @xmath148 is the matrix of eigenvectors and @xmath149 the diagonal matrix of eigenvalues , @xmath150 .",
    "using principal component analysis ( pca ) , we compute * e * and * d * and the whitening matrix is hence the inverse square root covariance matrix @xmath67 is then given by equation [ vwhite ] @xcite .",
    "@xmath151    @xmath152    where @xmath153 and is the de - mixing matrix of the whitened observed signals @xmath63 .",
    "at the heart of the algorithm lies the blind - source separation routine . to attain the demixing matrix @xmath95 ,",
    "many different types and varieties of algorithms are being used in the literature . here",
    "we will use the multi - combi algorithm developed by @xcite combining a fixed point high - order ica algorithm to separate non - gaussian sources with a second - order statistics blind - source - separation ( bss ) algorithm for separating auto - regressive ( ar ) sources .",
    "we will briefly outline these algorithms and explain how it is applied to the whitened data @xmath63 obtained in section [ pca ] .",
    "in section  [ icadef ] we briefly outlined the measures of non - gaussianity , negentropy ( equation  [ negentropy ] ) , used throughout this paper and stated that negentropy can be approximated via the kurtosis of the random vector @xmath35 or via the use of contrast functions , equation  [ negentropy2 ] & [ nonlin1 ] . in section  [ fastica ]",
    "we showed stated the iteration scheme to obtain a single independent component ( ic ) at a time .",
    "this is called a deflationary algorithm where the computed ic is subtracted from the data before the second ic is computed .",
    "such an iteration scheme has the property of finding the ics in the order of decreasing non - gaussianity .",
    "however , the main drawback of a serial computation of ics is that estimation errors in the first ics propagate in the extraction of later ics via the orthogonalization step . this effect is cumulative and may signficantly impair weaker ics .",
    "this predicament can be circumvented by estimating all ics in parallel .    similar to the single unit iteration ,",
    "the whitened demixing matrix @xmath95 is at its most mutually independent when the projection @xmath154 is at its most non - gaussian .",
    "the fastica fixed point iteration step is then given by    @xmath155\\bf{\\tilde{w } }   \\label{ica1}\\ ] ]    where @xmath156 is the unnormalised next iteration of @xmath95 , @xmath157 is an n  x  1 vector of 1 s and @xmath158 and @xmath159 are the first and second order derivatives of the nonlinear function @xmath160 :    @xmath161    this is followed by a symmetric orthogonalisation step :    @xmath162    equations  [ ica1 ] & [ ica2 ] are iterated until the result has converged .    for a full derivation",
    "we refer you to @xcite and @xcite .",
    "whereas the convergence of the fastica algorithm is often dependent on the non - linearity chosen by the user , the efica @xcite algorithm employed here is a variant of the above iteration scheme and allows for different non - linearities to be assigned adaptively to different sources .",
    "@xcite showed that efica is asymptotically efficient , ie .",
    "reaches the cramer - rao lower bound ( crlb ) in an ideal case where the nonlinearity @xmath160 equals the score function .    to assert a good degree of separation",
    ", we can define @xmath163 as the gain matrix . for a perfectly estimated de - mixing matrix , @xmath21 ,",
    "the gain matrix is equal to its identity matrix    @xmath164    in signal processing , the performance of blind - source separation algorithms is usually measured by the interference over signal ratio matrix , @xmath116    @xmath165    where @xmath166 and @xmath167 denote the observed and estimated sources",
    ". the isr for and individual observed signal @xmath166 is given by    @xmath168    however , the original mixing matrix , * a * , is not generally known for real data sets and equations [ isr ] & [ isrk ] are only useful in the case of simulations .",
    "@xcite have shown that the whole * isr * matrix for the efica algorithm can be approximated by    @xmath169    @xmath170 \\\\\\nonumber   \\tau_{k } & = |\\mu_{k } - \\rho_{k}| \\\\",
    "\\nonumber   \\rho_{k } & = e[g_{k}'(\\hat{s}_{k } ] \\\\\\nonumber   \\beta_{k } & = e[g_{k}^{2 } ( \\hat{s}_{k } ] \\label{isref}\\end{aligned}\\ ] ]    where @xmath171 and @xmath172 are the kth and lth observed and estimated signals of * s * in equation [ timeseries3 ] , @xmath173 and @xmath174 the first and second derivative of @xmath160 for signal @xmath166 and @xmath13 is the number of signals estimated . here it should be mentioned that , of course , the true realisation of each * isr * component is unknown and a mean-*isr * is computed leading to the best on average separation of the signals .",
    "whilst efica is optimised for the separation of instantaneously mixed , non - gaussian sources , second - order statistics bss algorithms rely on time - structure in the sources correlation function to estimate @xmath95 .",
    "a variety of algorithms exist in the literature , here we use a derivative of the popular sobi algorithm @xcite , wasobi @xcite to separate gaussian auto - regressive ( ar ) sources in the input data @xmath63 . here",
    ", the blind source separation follows the same linear model as in equation [ timeseries3 ] and the mixing matrix @xmath69 is estimated by a joint diagonalisation of the signals autocorrelation matrices .",
    "the unknown correlation matrices of the observed signals for a given lag @xmath112 , @xmath175 $ ]    @xmath176 \\overset{\\triangle}{= } \\frac{1}{n } \\sum_{n=1}^{n } \\textbf{x}[n ] \\textbf{x}^{t}[n+\\tau ] , ~~ \\tau = 0 , ... , m-1 \\label{wasobi1}\\ ] ]    satisfies the relation      where @xmath177 \\overset{\\triangle}{= } e [ \\textbf{s}[n ] \\textbf{s}^{t}[n+\\tau]]$ ] are the source signals diagonalised correlation matrices @xcite .",
    "hence , if the correlation matrices are diagonal , ie .",
    "the off - diagonal components are zero , the separated signals can be said to be independent from each other .",
    "the sobi & wasobi algorithms estimate @xmath69 as the joint diagonoliser of a set of correlation matrices .",
    "similar to the efica code , we can define an asymptotic estimate of the * isr * matrix    @xmath178}{\\sigma_{l}^{2}r_{k}[0 ] } \\label{isrwa}\\ ] ]    @xmath179 \\label{phi}\\ ] ]    where @xmath166 and @xmath167 denote the observed and the estimated sources , @xmath180\\}^{m-1}_{\\tau=0}$ ] is the covariance sequence of the @xmath166-th source , @xmath181 is the variance of the source and @xmath182 are the auto - regression coefficients of the @xmath167-th source @xcite .",
    "the algorithms introduced above are highly complementary to each other . whilst efica has an asymptotically efficient performance in separating non - gaussian instantaneous mixtures , wasobi is asymptotically efficient in separating gaussian time - correlated signals .",
    "both these properties are necessary since a real data set will have both of the aforementioned properties and its components would hence not be optimally de - mixed if one would only employ one type of algorithm .",
    "multi - combi @xcite uses a clustering technique in which both algorithms are run on the set of unseparated sources @xmath63 and their interference over signal matrices , @xmath183 and @xmath184 , are estimated .",
    "the signals are then clustered depending on whether their specific @xmath185 is lower for the efica or wasobi case .",
    "then , the process is repeated until all clusters are singeltons , ie . only contain one signal per cluster , and the signals are hence optimally separated .",
    "from the multi - combi algorithm , we obtain the estimated signal matrix @xmath83 , an overall * isr * matrix as well as final @xmath183 and @xmath184 . since the algorithms used here use fixed - point convergence techniques , the problem of non - repeatability of the separation process is less than for neural network based approaches .",
    "however , it is common sense to check the stability of the result obtained and to estimate the error on @xmath83 .    in order to estimate the stability of the convergence , we perturb the unknown mixing matrix * a * with a random and known",
    "mixing matrix @xmath79 to give a new mixing matrix @xmath186 and equation [ timeseries3 ] becomes : @xmath187 .",
    "this is equivalent to multiplying the whitened signal @xmath63 with * p *    @xmath188    we re - run the separation step and estimate @xmath189 . since * p * is known , we can reconstruct the original mixing - matrix and compare it with the new result . in the scope of an automated algorithm ,",
    "the sum of all terms of @xmath190 is compared to the sum of @xmath191 and the result is reported .    to identify the stochastic nature of the retrieval we furthermore re - run the separation step with the same whitened signal , @xmath63 , akin to a monte carlo simulation .",
    "we perform @xmath192 realisations ( where @xmath193 typically ) and use the de - mixing matrices @xmath194 to construct mean noise models later on . this way , we propagate the signal separation error to the model - fitting in a coherent manner .",
    "in order to identify the non - white ( i.e. systematic ) signals in our estimated signal matrix @xmath83 , we use the ljung - box portmanteau test @xcite .",
    "the test statistic , usually denoted by @xmath195 , is defined by summing the normalised autocorrelations of the individual time series , @xmath84 over a range of lags :      where @xmath197 is the autocorrelation at lag @xmath112 and @xmath61 is the number of observations in the time series .",
    "the hypothesis of the time series being solely white noise is rejected if @xmath195 is bigger than a pre - specified fraction of the chi - squared distribution"
  ],
  "abstract_text": [
    "<S> the characterisation of ever smaller and fainter extrasolar planets requires an intricate understanding of one s data and the analysis techniques used . </S>",
    "<S> correcting the raw data at the 10@xmath0 level of accuracy in flux is one of the central challenges . </S>",
    "<S> this can be difficult for instruments that do not feature a calibration plan for such high precision measurements . here </S>",
    "<S> , it is not always obvious how to de - correlate the data using auxiliary information of the instrument and it becomes paramount to know how well one can disentangle instrument systematics from one s data , given nothing but the data itself . </S>",
    "<S> we propose a non - parametric machine learning algorithm , based on the concept of independent component analysis , to de - convolve the systematic noise and all non - gaussian signals from the desired astrophysical signal . </S>",
    "<S> such a ` blind ' signal de - mixing is commonly known as the ` cocktail party problem ' in signal - processing . given multiple simultaneous observations of the same exoplanetary eclipse , as in the case of spectrophotometry , we show that we can often disentangle systematic noise from the original light curve signal without the use of any complementary information of the instrument . in this paper , we explore these signal extraction techniques using simulated data and two data sets observed with the hubble - nicmos instrument . another important application is the de - correlation of the exoplanetary signal from time - correlated stellar variability . using data obtained by the kepler mission we show that the desired signal can be de - convolved from the stellar noise using a single time series spanning several eclipse events . </S>",
    "<S> such non - parametric techniques can provide important confirmations of the existent parametric corrections reported in the literature , and their associated results . </S>",
    "<S> additionally they can substantially improve the precision exoplanetary light curve analysis in the future . </S>"
  ]
}