{
  "article_text": [
    "the most familiar metrics in infinite horizon sequential decision problems are discounted cost and long - run average cost respectively .",
    "the latter is useful in the scenarios where discounting is inappropriate or there is no cost - free absorbing state .",
    "however , under the optimal policy the long - run average cost will be minimum but any random long - run cost may be much larger . in order to incorporate risk into our consideration",
    "we need to assign utility function associated with the actual cost . for cost functions ,",
    "exponential function is many times used as utility function .",
    "the reason is that it has important connections with dynamic games and robust control and is popular in certain applications , particularly related to finance where it offers the advantage of ` penalizing all moments ' , so to say , thus capturing the ` risk ' in addition to mean return ( hence the name ) .",
    "then the risk - sensitive cost becomes the certainty equivalent of the expected exponential utility @xcite .",
    "like other cost criterion , one can propose and justify iterative algorithms for solving the dynamic programming equation for risk - sensitive setting @xcite .",
    "the issue we are interested in here is how to do so , even approximately , when the exact model is either unavailable or too unwieldy to afford analysis , but on the other hand simulated or real data is available easily , based on which one may hope to ` learn ' the solution in an incremental fashion .    one important point to note here",
    "is that the usual simulation based technique of calculating average cost does not work when the objective is a risk - sensitive cost .",
    "the reason is that average cost is defined as @xmath0 , \\nonumber \\end{aligned}\\ ] ] where @xmath1 is the cost of state @xmath2 and @xmath3 , @xmath4 is an irreducible finite state markov chain .",
    "therefore the following iterative algorithm will calculate the average cost : @xmath5 , \\end{aligned}\\ ] ] where the step sizes satisfy the robbins - monro conditions .",
    "this follows from the ergodic theorem for irreducible markov chain as well the convergence analysis of stochastic approximation with markov noise @xcite . on the contrary one need to apply multiplicative ergodic theorem ( @xcite ) when the cost is risk - sensitive . however , this does not have any closed - form limit . moreover , one can not even write iterative algorithms like ( [ avg ] ) in this setting because of the non - linear nature of the cost . due to the same reason , methods of @xcite",
    "also wo nt work in this setting when one is solving the full control problem .",
    "this takes us into the domain of reinforcement learning . in @xcite and @xcite q - learning and actor - critic methods",
    "have been proposed respectively for such cost - criterion .",
    "these are ",
    "raw schemes in the sense that there is no further approximation involved .",
    "since complex control problems lead to dynamic programming equations in very large dimensions ( ` curse of dimensionality ' ) , one often looks for an approximate scheme .",
    "one such learning algorithm with function approximation is proposed in @xcite .    in such approximation architectures",
    "an important problem is to obtain a good error bound for the approximation .",
    "this has been pointed out by borkar in the future work section of @xcite . while @xcite provides such a bound when the problem is policy evaluation , it is clearly mentioned there that the bound obtained is not good . in this technical note",
    "we investigate the problems with the existing bound and then improve upon the same .",
    "we begin by recalling the risk - sensitive framework .",
    "consider an irreducible aperiodic markov chain @xmath6 on a finite state space @xmath7 , with transition matrix @xmath8 i , j \\in s$ ] .",
    "while our real concern is a controlled markov chain , we aim at a policy evaluation algorithm for a fixed stationary policy .",
    "thus we have suppressed the explicit control dependence .",
    "let @xmath9 denote a prescribed ` running cost ' function .",
    "the aim is to evaluate @xmath10\\right ) .",
    "\\nonumber \\end{aligned}\\ ] ] function . that this limit exists follows from the multiplicative ergodic theorem for markov chains ( see theorem 1.2 of balaji and meyn ( 2000 ) @xcite , the sufficient condition ( 4 )",
    "therein is trivially verified for the finite state case here ) . associated with this",
    "is the multiplicative poisson equation ( see , e.g. , balaji and meyn ( 2000 ) ( * ? ? ?",
    "* theorem 1.2 ( ii ) ) ) : we know from @xcite that there exists @xmath11 and @xmath12 such that the multiplicative poisson equation holds as follows : @xmath13 for an explicit expression for @xmath14 see ( 5 ) in @xcite .",
    "let @xmath15 with a prescribed @xmath16 .",
    "then @xmath17 is the unique solution to @xmath18 = 1 .",
    "\\nonumber \\end{aligned}\\ ] ]    thus @xmath17 and @xmath19 are respectively the perron - frobenius eigenvalue and eigenvector of the non - negative matrix @xmath20_{i , j \\in s}$ ] , whose existence is guaranteed by the perron - frobenius theorem . furthermore , under our irreducibility assumption , @xmath19 is specified uniquely up to a positive multiplicative scalar and @xmath17 is uniquely specified .",
    "also , the risk - sensitive cost defined as above is @xmath21 .    we know from both @xcite and @xcite that in the case of value iteration ( with both dynamic programming and reinforcement learning ) that the @xmath22-th component of the sequence of the iterates will converge to @xmath17 .",
    "the linear function approximation version in @xcite provides the following parameter update for @xmath23 : @xmath24 where @xmath25^t \\text{~is a vector of coefficients } , \\\\",
    "\\nonumber & \\phi^k ( . ) , 1 \\leq k \\leq m , \\text{~are basis functions or features chosen a priori } , \\\\",
    "\\nonumber & \\phi(i ) = \\left[\\phi^1(i ) , \\dots , \\phi^m(i)\\right]^t , \\\\",
    "\\nonumber & \\phi = \\mbox{an~}s \\times m \\text{~matrix whose~ } ( i , k)\\text{~-th entry is~ }",
    "\\phi^k(i ) \\text{~for } \\\\",
    "\\nonumber    & 1 \\leq i \\leq s \\text{~and~ } 1 \\leq k \\leq m , \\\\",
    "\\nonumber & \\phi(i ) = \\text{~feature of state $ i$ } , \\\\",
    "\\nonumber & a_n = \\sum_{m=0}^n e^{c(x_m , x_{m+1})}\\phi(x_m)\\phi^t(x_{m+1 } ) , \\\\",
    "\\nonumber & b_n = \\sum_{m=0}^n \\phi(x_m)\\phi^t(x_{m } ) .",
    "\\nonumber\\end{aligned}\\ ] ]    we also know from ( * ? ? ?",
    "* theorem 5.3 ) that under a crucial assumption ( see ( @xmath26 ) in p  883 there ) on the feature matrix the iterates @xmath27 satisfy the following : @xmath28 where @xmath29 is a perron - frobenius eigenvalue of the non - negative matrix @xmath30 where @xmath31 and @xmath32 where @xmath33 and ` @xmath34 ' denotes the component - wise product of two matrices with identical row and column dimensions .",
    "therefore @xmath35 serves as an approximation of the original risk - sensitive cost .",
    "our aim is to investigate the difference between these two .",
    "in @xcite the following bound was given : @xmath36 and @xmath37 are two @xmath38 matrices with eigenvalues @xmath39 and @xmath40 respectively then @xmath41 this follows from the observation that if @xmath42 is the leading eigenvalue of @xmath43 , @xmath37 respectively and @xmath44 then @xmath45 .",
    "similar thing happens for the case @xmath46 except the fact that the roles of @xmath47 and @xmath48 and hence the roles of @xmath43 and @xmath37 get reversed thus keeping the right hand side ( r.h.s ) of ( [ spect_bound ] ) the same .",
    "an important point to note is that when @xmath44 , the fact that @xmath49 is the leading eigenvalue of @xmath37 is not used .",
    "same thing happens for the other case where @xmath50 replaces @xmath49 .",
    "another important point above is that for large @xmath51 the bound given above can not differentiate between the cases with two pairs of matrices @xmath52 and @xmath53 such that @xmath54 but @xmath55 and @xmath56 vary dramatically as @xmath57 as @xmath58 as long as @xmath59 .",
    "this will be clear from the next toy example : consider @xmath60 .",
    "suppose @xmath61 for @xmath62 with @xmath63 , @xmath64 and @xmath65 .",
    "it is easy to see that @xmath54 and @xmath66 . clearly , @xmath67 unless @xmath68",
    ". here @xmath69 denotes the perron - frobenius eigenvalue of matrix @xmath43 .    in summary ,",
    "when one is giving a bound between two quantities , the r.h.s should have terms involving the difference .",
    "however this does not occur in case of ( [ spect ] ) as the last term in the r.h.s will converge to @xmath70 as @xmath58 . in sections 5 and 6",
    "the new error bounds that we obtain contain always the difference terms irrespective of the state space size @xmath51 .",
    "recall the assumption ( @xmath26 ) on the feature matrix @xmath71 from @xcite which says that the feature matrix @xmath71 has all non - negative entries and any two column is orthogonal to each other . in this section",
    "we additionally make the following crucial assumption ( the rest of the paper follows the original assumption @xmath72 made in @xcite ) :    @xmath73 every row of the feature matrix @xmath71 has exactly one positive entry .",
    "let @xmath74 denote the column associated with row @xmath2 which has positive entry .",
    "here @xmath75 , where @xmath76 .",
    "clearly , @xmath77 if @xmath78-th column of the matrix @xmath79 is an eigenvector of the matrix @xmath80 with eigenvalue 1 i.e. , @xmath81 where @xmath82 with @xmath83 .    for the special case where the transition probability matrix is doubly stochastic this will require that @xmath84 from ( * ? ? ?",
    "* theorem 1 ) it is easy to see that ( this theorem is applicable due to lemma 5.1 ( ii ) of @xcite and @xmath73 ) the error can be zero even if @xmath85 , namely under the following conditions :    1 .",
    "there exists positive @xmath86 such that @xmath87 2 .",
    "@xmath88    note that if the matrix @xmath71 has a row @xmath2 with all @xmath89s , then @xmath90 for all @xmath91 whereas @xmath92 for at least one @xmath93 which violates the conditions for zero error stated above .",
    "if the transition probability matrix is doubly stochastic then one can also get the following feature selection criterion under the assumption ( @xmath26 ) made in @xcite : @xmath94 note that due to the assumption made on @xmath71 , @xmath95 becomes a diagonal matrix and hence invertible .",
    "motivated by the discussion in section [ relate ] and the fact that risk - sensitive cost is @xmath21 rather than @xmath17 we need to find an upper bound for @xmath96 .",
    "let @xmath69 denote the perron - frobenius eigenvalue of matrix @xmath97 . in the following we obtain three different bounds for the same quantity under the assumptions that a ) @xmath98",
    "b ) the matrix @xmath99 has positive entries and impose conditions under which one is better than the other .",
    "suppose @xmath43 admits left and right perron eigenvectors @xmath100 respectively , with @xmath101 ( this is satisfied , for example , if @xmath43 is irreducible ) .",
    "the three upper bounds of @xmath102 are ( [ first ] ) -([third ] ) .",
    "@xmath103 \\label{first}. \\\\ & \\ln\\left(\\max_i \\sum_j e^{c(i , j)}p(j|i)\\right ) -\\ln \\left(\\min_i \\sum_j e^{c(i , j)}p(j|i ) - \\sum_{i=1}^{s}x_i y_i \\left(e^{c(i , i)}p(i|i)- \\frac{\\phi^{k(i)}(i)\\sum_{l=1}^s \\phi^{k(i)}(l)\\pi_le^{c(l , i)}p(i|l)}{\\sum_{m=1}^s{\\phi^{k(i)}(m)}^2\\pi_m}\\right ) - \\right .",
    "\\nonumber \\\\   & \\left .",
    "\\sum_{i \\neq j } e^{c(i , j)}p(j|i)x_i y_j \\left(c(i , j ) + \\ln p(j|i ) + \\ln \\left(\\frac{\\sqrt{\\pi_j}}{\\sqrt{\\pi_i}}\\right ) + \\ln\\left(\\frac{\\sum_{m=1}^{s } { \\phi^{k(i)}(m)}^2 \\pi_m}{\\sum_{l=1}^s \\phi^{k(i)}(l ) \\pi_l e^{c(l , j)}p(j|l)}\\right ) - \\ln \\phi^{k(i)}(i ) \\right)\\right ) \\label{second}. \\\\ & \\ln \\left(1 + \\frac{1}{\\min_i \\sum_j \\frac{\\sqrt{\\pi_i}\\phi^{k(i)}(i)\\sum_{l=1}^s \\phi^{k(i)}(l)\\pi_l e^{c(l , j)}p(j|l)}{\\sqrt{\\pi_j}\\sum_{m=1}^s{\\phi^{k(i)}(m)}^2\\pi_m}}\\left(\\sum_{i=1}^{s}x_i y_i \\left(e^{c(i , i)}p(i|i)- \\frac{\\phi^{k(i)}(i)\\sum_{l=1}^s \\phi^{k(i)}(l)\\pi_le^{c(l , i)}p(i|l)}{\\sum_{m=1}^s{\\phi^{k(i)}(m)}^2\\pi_m}\\right ) + \\right.\\right . \\nonumber \\\\   & \\left",
    "\\sum_{i \\neq j } e^{c(i , j)}p(j|i)x_i y_j \\left(c(i , j ) + \\ln p(j|i ) + \\ln \\left(\\frac{\\sqrt{\\pi_j}}{\\sqrt{\\pi_i}}\\right ) + \\ln\\left(\\frac{\\sum_{m=1}^{s } { \\phi^{k(i)}(m)}^2 \\pi_m}{\\sum_{l=1}^s \\phi^{k(i)}(l ) \\pi_l e^{c(l , j)}p(j|l)}\\right)- \\ln \\phi^{k(i)}(i ) \\right)\\right)\\right ) \\label{third}.\\end{aligned}\\ ] ]    the inequalities ( [ first ] ) -([third ] ) follow from ( [ ineq1])-([ineq4 ] ) . @xmath104 \\label{ineq3}. \\\\ & \\min_i \\sum_j a_{ij } \\leq r(a ) \\leq \\max_i \\sum_j a_{ij}. \\label{ineq4 } \\end{aligned}\\ ] ]    for the example mentioned in section [ relate ] , ( [ first ] ) will be a much better bound compared to the spectral variation bound under the following condition : @xmath105    here ( [ second ] ) holds under ( [ as1 ] ) .",
    "@xmath106    ( [ ineq1 ] ) immediately follows from ( * ? ? ?",
    "* theorem 1 ) .",
    "( [ ineq2 ] ) and ( [ ineq3 ] ) also easily follow from ( * ? ? ?",
    "* theorem 2 ) . in (",
    "* theorem 3 ) it is shown that under one condition on matrix entries ( [ first ] ) is better than ( [ second ] ) whereas under some other condition it is opposite . in the following we impose conditions under which ( [ third ] ) compares to the other two .",
    "in this section we describe some inequalities .",
    "they are sufficient conditions under which ( [ first])-([third ] ) compares with each other .",
    "they will be referred in the next two lemmas .",
    "@xmath107    assume the condition mentioned in ( * ? ? ?",
    "* theorem 3 ( i ) ) . therefore under the sufficient condition ( [ as2 ] ) and ( [ as3 ] ) , ( [ first ] ) is better than ( [ second ] ) and ( [ second ] ) is better than ( [ third ] ) .",
    "let @xmath108 . under the condition mentioned in (",
    "* theorem 3 ( i ) ) , @xmath109 therefore ( [ first ] ) is better than ( [ second ] ) .",
    "now , ( [ second ] ) is better than ( [ third ] ) if @xmath110 which implies that @xmath111 which implies that @xmath112 .",
    "assume the condition mentioned in ( * ? ? ?",
    "* theorem 3 ( ii ) ) . then under the sufficient condition ( [ as5 ] ) or",
    "( [ as6 ] ) , ( [ third ] ) is better than ( [ second ] ) and hence ( [ first ] ) .",
    "let @xmath113 . under the condition mentioned in (",
    "* theorem 3 ( i ) ) , @xmath114 therefore ( [ second ] ) is better than ( [ first ] ) .",
    "now , ( [ third ] ) is better than ( [ second ] ) if @xmath115 which implies that @xmath116 which implies that @xmath117 or @xmath118 .    ideally , we tried our best to remove the perron - frobenius eigenvectors from the conditions stated in ( [ as1])- ( [ as6 ] ) as calculating them can be hard .",
    "however , we could not remove them in case of ( [ as2 ] ) , ( [ as5])- ( [ as6 ] ) .    similar bounds can be derived in the same way if @xmath119 .",
    "let @xmath120 be the operator norm of a matrix defined by @xmath121 where @xmath122 .",
    "additionally , @xmath123 where @xmath124 is perron - frobenius eigenvector of @xmath43 which has positive components if @xmath43 is irreducible .    under the assumption that @xmath79 is invertible , @xmath125 and @xmath126 .",
    "if @xmath17 is an eigenvalue of matrix @xmath43 with eigenvector @xmath124 , then @xmath127 is an eigenvalue of the adjoint @xmath128 with the same eigenvector .",
    "@xmath129 moreover , @xmath130 then the proof follows from the observation that @xmath131 .",
    "note that @xmath37 need not be irreducible under the assumption @xmath72 in @xcite .",
    "therefore , @xmath132 need not have all the components positive .",
    "in this short technical note we give several new bounds on the function approximation error for policy evaluation algorithm in the context of risk - sensitive reinforcement learning .",
    "an important future direction will be design and analyze a learning algorithm to find the optimal policy with the accompanying error bounds ."
  ],
  "abstract_text": [
    "<S> in this paper we obtain new error bounds of function approximation for the policy evaluation algorithm when the aim is to find the risk - sensitive cost represented using exponential utility . </S>",
    "<S> additionally , we obtain conditions involving the feature matrix so that the error is zero .    ,    risk - sensitive cost ; function approximation ; perron - frobenius eigenvalue . </S>"
  ]
}