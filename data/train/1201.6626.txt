{
  "article_text": [
    "robocup simulated soccer has been conceived and is widely accepted as a common platform to address various challenges in artificial intelligence and robotics research . here , we consider a subtask of the full problem , namely the _ keepaway _ problem . in _",
    "keepaway _ we have two smaller teams : one team ( the ` keepers ' ) must try to maintain possession of the ball for as long as possible while staying within a small region of the full soccer field .",
    "the other team ( the ` takers ' ) tries to gain possession of the ball . @xcite",
    "initially formulated keepaway as benchmark problem for reinforcement learning ( rl ) ; the keepers must individually _ learn _ how to maximize the time they control the ball as a team against the team of opposing takers playing a fixed strategy .",
    "the central challenges to overcome are , for one , the high dimensionality of the state space ( each observed state is a vector of 13 measurements ) , meaning that conventional approaches to function approximation in rl , like grid - based tilecoding , are infeasible ; second , the stochasticity due to noise and the uncertainty in control due to the multi - agent nature imply that the dynamics of the environment are both unknown and can not be obtained easily .",
    "hence we need model - free methods .",
    "finally , the underlying soccer server expects an action every 100 msec , meaning that efficient methods are necessary that are able to learn in real - time .",
    "@xcite successfully applied rl to _ keepaway _ , using the textbook approach with online sarsa(@xmath0 ) and tilecoding as underlying function approximator @xcite .",
    "however , tilecoding is a local method and places parameters ( i.e.  basis functions ) in a regular fashion throughout the entire state space , such that the number of parameters grows exponentially with the dimensionality of the space . in @xcite",
    "this very serious shortcoming was adressed by exploiting problem - specific knowledge of how the various state variables interact . in particular ,",
    "each state variable was considered independently from the rest . here",
    ", we will demonstrate that one can also learn using the full ( untampered ) state information , without resorting to simplifying assumptions . in this paper",
    "we propose a ( non - parametric ) kernel - based approach to approximate the value function .",
    "the rationale for doing this is that by representing the solution through the data and not by some basis functions chosen before the data becomes available , we can better adapt to the complexity of the unknown function we are trying to estimate . in particular , parameters are not ` wasted ' on parts of the input space that are never visited .",
    "the hope is that thereby the exponential growth of parameters is bypassed . to solve the rl problem of optimal control we consider the framework of approximate policy iteration with the related least - squares based policy evaluation methods lspe(@xmath0 ) proposed by @xcite and lstd(@xmath0 ) proposed by @xcite .",
    "least - squares based policy evaluation is ideally suited for the use with linear models and is a very sample - efficient variant of rl . in this paper",
    "we provide a unified and concise formulation of lspe and lstd ; the approximated value function is obtained from a regularization network which is effectively the mean of the posterior obtained by gp regression @xcite .",
    "we use the subset of regressors method @xcite to approximate the kernel using a much reduced subset of basis functions . to select this subset",
    "we employ greedy online selection , similar to @xcite , that adds a candidate basis function based on its distance to the span of the previously chosen ones .",
    "one improvement is that we consider a _ supervised _",
    "criterion for the selection of the relevant basis functions that takes into account the reduction of the cost in the original learning task in addition to reducing the error incurred from approximating the kernel . since the per - step complexity during training and prediction depends on the size of the subset , making sure that no unnecessary basis functions are selected ensures more efficient usage of otherwise scarce resources . in this way learning in real - time ( a necessity for _ keepaway _ ) becomes possible .",
    "this paper is structured in three parts : the first part ( section  [ sec : background ] ) gives a brief introduction on reinforcement learning and carrying out general regression with regularization networks .",
    "the second part ( section  [ sec : pe with rn ] ) describes and derives an efficient recursive implementation of the proposed approach , particularly suited for online learning .",
    "the third part describes the robocup - keepaway problem in more detail ( section  [ sec : robocup ] ) and contains the results we were able to achieve ( section  [ sec : experiments and results ] )",
    ". a longer discussion of related work is deferred to the end of the paper ; there we compare the similarities of our work with that of @xcite .",
    "in this section we briefly review the subjects of rl and regularization networks .",
    "reinforcement learning ( rl ) is a simulation - based form of approximate dynamic programming , e.g. see @xcite . consider a discrete - time dynamical system with states @xmath1 ( for ease of exposition we assume the finite case ) . at each time step @xmath2 ,",
    "when the system is in state @xmath3 , a decision maker chooses a control - action @xmath4 ( again , selected from a finite set @xmath5 of admissible actions ) which changes probabilistically the state of the system to @xmath6 , with distribution @xmath7 .",
    "every such transition yields an immediate reward @xmath8 .",
    "the ultimate goal of the decision - maker is to choose a course of actions such that the long - term performance , a measure of the cumulated sum of rewards , is maximized .",
    "let @xmath9 denote a decision - rule ( called the policy ) that maps states to actions .",
    "for a fixed policy @xmath9 we want to evaluate the state - action value function ( q - function ) which for every state @xmath10 is taken to be the expected infinite - horizon discounted sum of rewards obtained from starting in state @xmath10 , choosing action @xmath11 and then proceeding to select actions according to @xmath9 : @xmath12 where @xmath13 and @xmath14 . the parameter @xmath15 denotes a discount factor .",
    "ultimately , we are not directly interested in @xmath16 ; our true goal is optimal control , i.e.  we seek an optimal policy @xmath17 . to accomplish that",
    ", policy iteration interleaves the two steps policy evaluation and policy improvement : first , compute @xmath18 for a fixed policy @xmath19 .",
    "then , once @xmath18 is known , derive an improved policy @xmath20 by choosing the greedy policy with respect to @xmath18 , i.e. by by choosing in every state the action @xmath21 that achieves the best q - value . obtaining",
    "the best action is trivial if we employ the q - notation , otherwise we would need the transition probabilities and reward function ( i.e.  a ` model ' ) .    to compute the q - function , one exploits the fact that @xmath16 obeys the fixed - point relation @xmath22 , where @xmath23 is the bellman operator @xmath24 in principle , it is possible to calculate @xmath16 exactly by solving the corresponding linear system of equations , provided that the transition probabilities @xmath25 and rewards",
    "@xmath26 are known in advance and the number of states is finite and small .    however , in many practical situations this is not the case . if the number of states is very large or infinite , one can only operate with an approximation of the q - function , e.g. a linear approximation @xmath27 , where @xmath28 is an @xmath29-dimensional feature vector and @xmath30 the adjustable weight vector . to approximate the unknown expectation value one employs simulation ( i.e.  an agent interacts with the environment ) to generate a large number of observed transitions .",
    "figure  [ fig : api ] depicts the resulting approximate policy iteration framework : using only a parameterized @xmath31 and sample transitions to emulate application of @xmath23 means that we can carry out the policy evaluation step only approximately .",
    "also , using an approximation of @xmath18 to derive an improved policy from does not necessarily mean that the new policy actually is an improved one ; oscillations in policy space are possible . in practice however , approximate policy iteration is a fairly sound procedure that either converges or oscillates with bounded suboptimality @xcite .",
    "inferring a parameter vector @xmath32 from sample transitions such that @xmath33 is a good approximation to @xmath18 is therefore the central problem addressed by reinforcement learning .",
    "chiefly two questions need to be answered :    1 .   by what method",
    "do we choose the parametrisation of @xmath31 and carry out regression ? 2 .",
    "by what method do we learn the weight vector @xmath30 of this approximation , given sample transitions ?",
    "the latter can be solved by the family of temporal difference learning , with td(@xmath0 ) , initially proposed by @xcite , being its most prominent member . using a linearly parametrized value function",
    ", it was in shown in @xcite that td(@xmath0 ) converges against the true value function ( under certain technical assumptions ) .          in what follows",
    "we will discuss three related algorithms for approximate policy evaluation that share most of the advantages of td(@xmath0 ) but converge much faster , since they are based on solving a least - squares problem in closed form , whereas td(@xmath0 ) is based on stochastic gradient descent .",
    "all three methods assume that an ( infinitely ) long to zero and make a zero - reward transition from the terminal state to the start state of the next ( following ) episode . ]",
    "trajectory of states and rewards is generated using a simulation of the system ( e.g. an agent interacting with its environment ) .",
    "the trajectory starts from an initial state @xmath34 and consists of tuples @xmath35 and rewards @xmath36 where action @xmath37 is chosen according to @xmath9 and successor states and associated rewards are sampled from the underlying transition probabilities . from now on , to abbreviate these state - action tuples , we will understand @xmath38 as denoting @xmath39 .",
    "furthermore , we assume that the q - function is parameterized by @xmath40 and that @xmath30 needs to be determined .",
    "[ [ the - lspelambda - method . ] ] the lspe(@xmath0 ) method .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the method @xmath0-least squares policy evaluation lspe(@xmath0 ) was proposed by @xcite and proceeds by making incremental changes to the weights @xmath30 .",
    "assume that at time @xmath2 ( after having observed @xmath2 transitions ) we have a current weight vector @xmath41 and observe a new transition from @xmath38 to @xmath42 with associated reward @xmath43 .",
    "then we compute the solution @xmath44 of the least - squares problem @xmath45 where @xmath46 the new weight vector @xmath47 is obtained by setting @xmath48 where @xmath49 is the initial weight vector and @xmath50 is a diminishing step size .    [ [ the - lstdlambda - method . ] ] the lstd(@xmath0 ) method .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the least - squares temporal difference method lstd(@xmath0 ) proposed by @xcite for @xmath51 and by @xcite for general @xmath52 $ ] does not proceed by making incremental changes to the weight vector @xmath30 . instead , at time @xmath2 ( after having observed @xmath2 transitions ) ,",
    "the weight vector @xmath47 is obtained by solving the fixed - point equation @xmath53 for @xmath54 , where @xmath55 and setting @xmath47 to this unique solution .    [",
    "[ comparison - of - lspe - and - lstd . ] ] comparison of lspe and lstd .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    the similarities and differences between lspe(@xmath0 ) and lstd(@xmath0 ) are listed in table  [ tab : vergleich_von_lspe_lstd ] .",
    "both lspe(@xmath0 ) and lstd(@xmath0 ) converge to the same limit ( see * ? ? ?",
    "* ) , which is also the limit to which td(@xmath0 ) converges ( the initial iterates may be vastly different though ) .",
    "both methods rely on the solution of a least - squares problem ( either explicitly as is the case in lspe or implicitly as is the case in lstd ) and can be efficiently implemented using recursive computations . computational experiments in @xcite or @xcite indicate that both approaches can perform much better than td(@xmath0 ) .    both methods lspe and lstd differ as far as their role in the approximate policy iteration framework is concerned .",
    "lspe can take advantage of previous estimates of the weight vector and can hence be used in the context of optimistic policy iteration ( opi ) , i.e.  the policy under consideration gets improved following very few observed transitions . for lstd",
    "this is not possible ; here a more rigid actor - critic approach is called for .",
    "both methods lspe and lstd also differ as far as their relation to standard regression with least - squares methods is concerned .",
    "lspe directly minimizes a quadratic objective function .",
    "using this function it will be possible to carry out ` supervised ' basis selection , where for the selection of basis functions the reduction of the costs ( the quantity we are trying to minimize ) is taken into account . for lstd this is not possible ; here in fact we are solving a fixed point equation that employs least - squares only implicitly ( to carry out the projection ) .",
    ".comparison of least - squares policy evaluation [ cols= \" < , < , < \" , ]      + * loop : * * for * @xmath56 +    p0.8 execute action @xmath4 ( simulate a transition ) .",
    "+ observe next state @xmath6 and reward @xmath43 .",
    "+ choose action @xmath57 .",
    "let @xmath58 .",
    "+ check , if @xmath42 should be added to the set of basis functions .",
    "+ unsupervised basis selection : return true if @xmath59 .",
    "+ supervised basis selection : return true if @xmath59 + and additionally if either or ( [ eq : xitmm])@xmath60 .",
    "+   + obtain @xmath61 from either , ( [ eq : normal pittm] ) , or ( [ eq : normal pittm ]  ) . +",
    "obtain @xmath62 from either , ( [ eq : normal wttm] ) , or ( [ eq : normal wttm ]  ) .",
    "+ ( only when step 1 returned true ) + obtain @xmath63 from either , ( [ eq : pitmm] ) , or ( [ eq : pitmm ]  ) . +",
    "obtain @xmath64 from either , ( [ eq : bbetatmm] ) , or ( [ eq : bbetatmm ]  ) .",
    "+ add @xmath65 to @xmath66 and obtain @xmath67 from .",
    "+ @xmath68 + @xmath69 , @xmath70 , @xmath71 +     +      let @xmath2 be the current time step ,",
    "@xmath72 the currently observed input - output pair and assume that from the past @xmath2 examples @xmath73 the @xmath29 examples @xmath74 were selected into the dictionary @xmath66 .",
    "consider the penalized least - squares problem that is brm ( restated here for clarity ) @xmath75 with @xmath76 being the @xmath77 data matrix and @xmath78 being the @xmath79 vector of the observed output values from . defining the @xmath80",
    "cross product matrix @xmath81 , the solution to ( [ eq : brm4 ] ) is given by @xmath82 finally , introduce the costs @xmath83 . assuming that @xmath84 are known from previous computations , every time a new transition @xmath72 is observed , we will perform one or both of the following update operations :      with @xmath87 defined as @xmath88 , one gets @xmath89 thus @xmath90 and we obtain from ( [ eq : smw ] ) the well - known rls updates @xmath91 with scalar @xmath92 and @xmath93 with scalar @xmath94 .",
    "the costs become @xmath95 .",
    "the set of basis functions @xmath66 is not altered during this step .",
    "operation complexity is @xmath96 .",
    "[ [ how - to - add - a - mathcalbv .",
    "] ] how to add a @xmath66 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    when adding a basis function ( centered on @xmath65 ) to the model , we augment the set @xmath66 with @xmath98 ( note that @xmath98 is the same as @xmath65 from above ) .",
    "define @xmath99 , @xmath100 , and @xmath101 . adding a basis function means appending a new @xmath102 vector @xmath103 to the data matrix and appending @xmath104 as row / column to the penalty matrix @xmath105 , thus @xmath106 invoking ( [ eq : pmi ] ) we obtain the updated inverse @xmath63 via @xmath107 where simple vector algebra reveals that @xmath108 without sparse online approximation this step would require us to recall all @xmath2",
    "past examples and would come at the undesirable price of @xmath109 operations .",
    "however , we are going to get away with merely @xmath110 operations and only need to access the @xmath29 past examples in the memorized @xmath66 . due to the sparse online approximation ,",
    "@xmath103 is actually of the form @xmath111 with @xmath112 and @xmath113 ( see section  [ sect : sog ] ) . hence new information is injected only through the last component . exploiting this special structure of @xmath103 equation ( [ eq : wbdeltab ] ) becomes @xmath114 where @xmath115 .",
    "if we cache and reuse those terms already computed in the preceding step ( see section  [ sect : normalstep ] ) then we can obtain @xmath116 in @xmath110 operations .    to obtain the updated coefficients @xmath64 we postmultiply ( [ eq : pitmm ] ) by @xmath117 , getting @xmath118 where scalar @xmath119 is defined by @xmath120 .",
    "again we can now exploit the special structure of @xmath103 to show that @xmath119 is equal to @xmath121 and again we can reuse terms computed in the previous step ( see section   [ sect : normalstep ] ) .    skipping the computations , we can show that the reduced ( regularized ) cost @xmath122 is recursively obtained from @xmath123 via the expression : @xmath124 finally , each time we add an example to the @xmath66 set we must also update the inverse kernel matrix @xmath125 needed during the computation of @xmath126 and @xmath127 .",
    "this can be done using the formula for partitioned matrix inverses ( [ eq : pmi ] ) : @xmath128    [ [ when - to - add - a - mathcalbv . ] ] when to add a @xmath66 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to decide whether or not the current example @xmath65 should be added to the @xmath66 set , we employ the supervised two - part criterion from @xcite . the first part measures the ` novelty ' of the current example : only examples that are ` far ' from those already stored in the @xmath66 set are considered for inclusion . to this end",
    "we compute as in @xcite the squared norm of the residual from projecting ( in rkhs ) the example onto the span of the current @xmath66 set , i.e.   we compute , restated from ( [ eq : ald - test ] ) , @xmath129 .",
    "if @xmath130 for a given threshold @xmath131 , then @xmath65 is well represented by the given @xmath66 set and its inclusion would not contribute much to reduce the error from approximating the kernel by the reduced set .",
    "on the other hand , if @xmath132 then @xmath65 is not well represented by the current @xmath66 set and leaving it behind could incur a large error in the approximation of the kernel .    aside from novelty , we consider as second part of the selection criterion the ` usefulness ' of a basis function candidate .",
    "usefulness is taken to be its contribution to the reduction of the regularized costs @xmath133 , i.e.  the term @xmath134 from ( [ eq : xitmm ] ) .",
    "both parts together are combined into one rule : only if @xmath135 and @xmath136 , then the current example will become a new basis function and will be added to @xmath66 .",
    "the experimental work we carried out for this article uses the publicly available keepaway framework from @xcite , which is built on top of the standard robocup soccer simulator also used for official competitions @xcite .",
    "agents in robocup are autonomous entities ; they sense and act independently and asynchronously , run as individual processes and can not communicate directly .",
    "agents receive visual perceptions every 150 msec and may act once every 100 msec .",
    "the state description consists of relative distances and angles to visible objects in the world , such as the ball , other agents or fixed beacons for localization .",
    "in addition , random noise affects both the agents sensors as well as their actuators .    in keepaway , one team of ` keepers '",
    "must learn how to maximize the time they can control the ball within a limited region of the field against an opposing team of ` takers ' . only the keepers are allowed to learn , the behavior of the takers is governed by a fixed set of hand - coded rules .",
    "however , each keeper only learns _ individually _ from its own ( noisy ) actions and its own ( noisy ) perceptions of the world .",
    "the decision - making happens at an intermediate level using multi - step macro - actions ; the keeper currently controlling the ball must decide between holding the ball or passing it to one of its teammates .",
    "the remaining keepers automatically try to position themselves such to best receive a pass .",
    "the task is episodic ; it starts with the keepers controlling the ball and continues as long as neither the ball leaves the region nor the takers succeed in gaining control .",
    "thus the goal for rl is to maximize the overall duration of an episode .",
    "the immediate reward is the time that passes between individual calls to the acting agent .    for our work ,",
    "we consider as in @xcite the special 3vs2 keepaway problem ( i.e.  three learning keepers against two takers ) played in a 20x20 m field . in this case",
    "the continuous state space has dimensionality 13 , and the discrete action space consists of the three different actions _ hold , pass to teammate-1 , pass to teammate-2 _",
    "( see figure  [ fig : keepaway ] ) . more generally , larger instantiations of keepaway would also be possible , like e.g. 4vs3 , 5vs4 or more , resulting in even larger state- and action spaces .",
    "in this section we are finally ready to apply our proposed approach to the keepaway problem .",
    "we implemented and compared two different variations of the basic algorithm in a policy iteration based framework : ( a ) optimistic policy iteration using lspe(@xmath0 ) and ( b ) actor - critic policy iteration using lstd(@xmath0 ) . as baseline method we used sarsa(@xmath0 ) with tilecoding , which we re - implemented from @xcite as faithfully as possible .",
    "initially , we also tried to employ brm instead of lstd in the actor - critic framework .",
    "however , this set - up did not fare well in our experiments because of the stochastic state - transitions in keepaway ( resulting in highly variable outcomes ) and brm s inability to deal with this situation adequately .",
    "thus , the results for brm are not reported here .",
    "[ [ optimistic - policy - iteration . ] ] optimistic policy iteration .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    sarsa(@xmath0 ) and lspe(@xmath0 ) paired with optimistic policy iteration is an on - policy learning method , meaning that the learning procedure estimates the q - values from and for the current policy being executed by the agent . at the same time",
    ", the agent continually updates the policy according to the changing estimates of the q - function . thus policy evaluation and improvement",
    "are tightly interwoven .",
    "optimistic policy iteration ( opi ) is an online method that immediately processes the observed transitions as they become available from the agent interacting with the environment @xcite .",
    "[ [ actor - critic . ] ] actor - critic .",
    "+ + + + + + + + + + + + +    in contrast , lstd(@xmath0 ) paired with actor - critic is an off - policy learning method adhering with more rigor to the policy iteration framework . here",
    "the learning procedure estimates the q - values for a fixed policy , i.e.  a policy that is not continually modified to reflect the changing estimates of q. instead , one collects a large number of state transitions under the same policy and estimates q from these training examples . in opi , where the most recent version of the q - function is used to derive the next control action , only one network is required to represent q and make the predictions .",
    "in contrast , the actor - critic framework maintains two instantiations of regularization networks : one ( the actor ) is used to represent the q - function learned during the previous policy evaluation step and which is now used to represent the current policy , i.e.  control actions are derived using its predictions .",
    "the second network ( the critic ) is used to represent the current q - function and is updated regularly .",
    "one advantage of the actor - critic approach is that we can reuse the same set of observed transitions to evaluate different policies , as proposed in @xcite .",
    "we maintain an ever - growing list of all transitions observed from the learning agent ( irrespective of the policy ) , and use it to evaluate the current policy with lstd(@xmath0 ) .",
    "to reflect the real - time nature of learning in robocup , where we can only carry out a very small amount of computations during one single function call to the agent , we evaluate the transitions in small batches ( 20 examples per step ) .",
    "once we have completed evaluating all training examples in the list , the critic network is copied to the actor network and we can proceed to the next iteration , starting anew to process the examples , using this time a new policy .    [ [ policy - improvement - and - varepsilon - greedy - action - selection . ] ] policy improvement and @xmath137-greedy action selection .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to carry out policy improvement , every time we need to determine a control action for an arbitrary state @xmath138 , we choose the action @xmath139 that achieves the maximum q - value ; that is , given weights @xmath32 and a set of basis functions @xmath140 , we choose @xmath141 sometimes however , instead of choosing the best ( greedy ) action , it is recommended to try out an alternative ( non - greedy ) action to ensure sufficient exploration .",
    "here we employ the @xmath137-greedy selection scheme ; we choose a random action with a small probability @xmath137 ( @xmath142 ) , otherwise we pick the greedy action with probability @xmath143 . taking a random action usually means to choose among all possible actions with equal probability .    under the standard assumption for errors in bayesian regression ( e.g. ,",
    "see * ? ? ?",
    "* ) , namely that the observed target values differ from the true function values by an additive noise term ( i.i.d .",
    "gaussian noise with zero mean and uniform variance ) , it is also possible to obtain an expression for the ` predictive variance ' which measures the uncertainty associated with value predictions .",
    "the availability of such confidence intervals ( which is possible for the direct least - squares problems lspe and also brm ) could be used , as suggested in @xcite , to guide the choice of actions during exploration and to increase the overall performance . for the purpose of solving the keepaway problem however",
    ", our initial experiments showed no measurable increase in performance when including this additional feature .",
    "[ [ remaining - parameters . ] ] remaining parameters .",
    "+ + + + + + + + + + + + + + + + + + + + +    since the kernel is defined for state - action tuples , we employ a product kernel @xmath144,[s',a'])=k_s(s , s')k_a(a , a')$ ] as suggested by @xcite .",
    "the action kernel @xmath145 is taken to be the kronecker delta , since the actions in keepaway are discrete and disparate . as state kernel @xmath146 we chose the gaussian rbf @xmath147 with uniform length - scale @xmath148 .",
    "the other parameters were set to : regularization @xmath149 , discount factor for rl @xmath150 , @xmath151 , and lspe step size @xmath152 .",
    "the novelty parameter for basis selection was set to @xmath153 . for the usefulness part we tried out different values to examine the effect supervised basis selection has",
    "; we started with @xmath154 corresponding to the unsupervised case and then began increasing the tolerance , considering alternatively the settings @xmath155 and @xmath156 . since in the case of lstd",
    "we are not directly solving a least - squares problem , we use the associated brm formulation to obtain an expression for the error reduction in the supervised basis selection . due to the very long runtime of the simulations",
    "( simulating one hour in the soccer server roughly takes one hour real time on a standard pc ) we could not try out many different parameter combinations .",
    "the parameters governing rl were set according to our experiences with smaller problems and are in the range typically reported in the literature .",
    "the parameters governing the choice of the kernel ( i.e.  the length - scale of the gaussian rbf ) was chosen such that for the unsupervised case ( @xmath154 ) the number of selected basis functions approaches the maximum number of basis functions the cpu used for these the experiments was able to process in real - time .",
    "this number was determined to be @xmath157 ( on a standard 2 ghz pc ) .",
    "[ [ results . ] ] results .",
    "+ + + + + + + +    we evaluate every algorithm / parameter configuration using 5 independent runs .",
    "the learning curves for these runs are shown in figure  [ fig : results ] .",
    "the curves plot the average time the keepers are able to keep the ball ( corresponding to the performance ) against the simulated time the keepers were learning ( roughly corresponding to the observed training examples ) . additionally , two horizontal lines indicate the scores for the two benchmark policies random behavior and optimized hand - coded behavior used in @xcite .",
    "the plots show that generally rl is able to learn policies that are at least as effective as the optimized hand - coded behavior .",
    "this is indeed quite an achievement , considering that the latter is the product of considerable manual effort . comparing the three approaches",
    "sarsa , lspe and lstd we find that the performance of lspe is on par with sarsa .",
    "the curves of lstd tell a different story however ; here we are outperforming sarsa by 25% in terms of performance ( in sarsa the best performance is about @xmath158 seconds , in lstd the best performance is about @xmath159 seconds ) .",
    "this gain is even more impressive when we consider the time scale at which this behavior is learned ; just after a mere 2 hours we are already outperforming hand - coded control .",
    "thus our approach needs far fewer state transitions to discover good behavior .",
    "the third observation shows the effectiveness of our proposed supervised basis function selection ; here we show that our supervised approach performs as well as the unsupervised one , but requires significantly fewer basis functions to achieve that level of performance ( @xmath160 700 basis functions at tol2@xmath161 against 1400 basis functions at tol2@xmath162 ) .",
    "regarding the unexpectedly weak performance of lspe in comparison with lstd , we conjecture that this strongly depends on the underlying architecture of policy iteration ( i.e.  opi vs. actor - critic ) as well as the specific learning problem . on a related number of experiments carried out with the octopus arm benchmark we made exactly the opposite observation ( not discussed here in more detail ,",
    "see * ? ? ?",
    "we have presented a kernel - based approach for least - squares based policy evaluation in rl using regularization networks as underlying function approximator .",
    "the key point is an efficient supervised basis selection mechanism , which is used to select a subset of relevant basis functions directly from the data stream .",
    "the proposed method was particularly devised with high - dimensional , stochastic control tasks for rl in mind ; we prove its effectiveness using the robocup keepaway benchmark .",
    "overall the results indicate that kernel - based online learning in rl is very well possible and recommendable .",
    "even the rather few simulation runs we made clearly show that our approach is superior to convential function approximation in rl using grid - based tilecoding .",
    "what could be even more important is that the kernel - based approach only requires the setting of some fairly general parameters that do not depend on the specific control problem one wants to solve . on the other hand ,",
    "using tilecoding or a fixed basis function network in high dimensions requires considerable manual effort on part of the programmer to carefully devise problem - specific features and manually choose suitable basis functions .",
    "@xcite initially advocated using kernel - based methods in rl and proposed the related gptd algorithm .",
    "our method using regularization networks develops this idea further .",
    "both methods have in common the online selection of relevant basis functions based on @xcite . as opposed to the unsupervised selection in gptd , we use a supervised criterion to further reduce the number of relevant basis functions selected .",
    "a more fundamental difference is the policy evaluation method addressed by the respective formulation ; gptd models the bellman residuals and corresponds to the brm approach ( see section 2.1.2 ) .",
    "thus , in its original formulation gptd can be only applied to rl problems with deterministic state transitions .",
    "in contrast , we provide a unified and concise formulation of lstd and lspe which can deal with stochastic state transitions as well .",
    "another difference is the type of benchmark problem used to showcase the respective method ; gptd was demonstrated by learning to control a simulated octopus arm , which was posed as an 88-dimensional control problem @xcite .",
    "controlling the octopus arm is a deterministic control problem with known state transitions and was solved there using model - based rl .",
    "in contrast , 3vs2 keepaway is only a 13-dimensional problem ; here however , we have to deal with stochastic and unknown state transitions and need to use model - free rl .",
    "the authors wish to thank the anonymous reviewers for their useful comments and suggestions .",
    "let @xmath163 be the next state - action tuple and @xmath43 be the reward assiociated with transition from the previous state @xmath3 to @xmath6 under @xmath4 .",
    "define the abbreviations :          we want to test if @xmath65 is well represented by the current basis functions in the dictionary or if we need to add @xmath65 to the basis elements .",
    "compute @xmath165 if @xmath166 , then add @xmath65 to the dictionary , execute the growing step ( see below ) and update @xmath167      * normal step @xmath168 : 1 .",
    "@xmath169 with @xmath92 .",
    "2 .   @xmath170 with @xmath94 .",
    "* growing step @xmath171 1 .",
    "@xmath172 where @xmath173 2 .",
    "@xmath174 where @xmath175 .",
    "* reduction of regularized cost when adding @xmath65 ( supervised basis selection ) : @xmath176 for supervised basis selection we additionally check if @xmath177 .      *",
    "normal step @xmath168 : 1 .",
    "@xmath178 2 .",
    "@xmath179 with @xmath180 .",
    "3 .   @xmath181 with @xmath94 .",
    "* growing step @xmath171 1 .",
    "@xmath182 where @xmath183 .",
    "2 .   @xmath184 where + w_b^(1 ) & = a_t+1 + p_tm^-1z_t+1,m & ^(1)&=h^*_t+1 - a_t+1^h_t+1 + w_b^(2 ) & = a_t+1^+ h_t+1^p_tm^-1 & ^(2)&=z_t+1,m^ * - a_t+1^z_t+1,m + + and @xmath185 .",
    "@xmath186 where @xmath187 .",
    "* normal step @xmath168 : 1 .",
    "@xmath188 2 .",
    "@xmath189 with @xmath190 .",
    "3 .   @xmath191",
    "* growing step @xmath171 1 .",
    "@xmath192 + where @xmath183 .",
    "@xmath193 where @xmath194 + and @xmath185 .",
    "3 .   @xmath195 where @xmath187 .",
    "* reduction of regularized cost when adding @xmath65 ( supervised basis selection ) : @xmath196 where @xmath197 and @xmath198 . for supervised basis selection",
    "we additionally check if @xmath199 ."
  ],
  "abstract_text": [
    "<S> we apply kernel - based methods to solve the difficult reinforcement learning problem of 3vs2 keepaway in robocup simulated soccer . </S>",
    "<S> key challenges in keepaway are the high - dimensionality of the state space ( rendering conventional discretization - based function approximation like tilecoding infeasible ) , the stochasticity due to noise and multiple learning agents needing to cooperate ( meaning that the exact dynamics of the environment are unknown ) and real - time learning ( meaning that an efficient online implementation is required ) . </S>",
    "<S> we employ the general framework of approximate policy iteration with least - squares - based policy evaluation . as </S>",
    "<S> underlying function approximator we consider the family of regularization networks with subset of regressors approximation . </S>",
    "<S> the core of our proposed solution is an efficient recursive implementation with automatic supervised selection of relevant basis functions . </S>",
    "<S> simulation results indicate that the behavior learned through our approach clearly outperforms the best results obtained with tilecoding by @xcite .    </S>",
    "<S> reinforcement learning , least - squares policy iteration , regularization networks , robocup </S>"
  ]
}