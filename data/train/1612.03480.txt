{
  "article_text": [
    "dimensionality reduction plays an important role in both artificial and natural signal processing systems . in man - made data analysis pipelines",
    "it denoises the input , simplifies further processing and identifies important features .",
    "dimensionality reduction algorithms have been developed for both the offline setting where the whole dataset is available to the algorithm from the outset and the online setting where data are streamed one sample at a time @xcite . in the brain ,",
    "online dimensionality reduction takes place , for example , in early processing of streamed sensory inputs as evidenced by a high ratio of input to output nerve fiber counts @xcite .",
    "therefore , dimensionality reduction algorithms may help model neuronal circuits and observations from neuroscience may inspire the future development of artificial signal processing systems .",
    "recently , a novel principled approach to online dimensionality reduction in neuronal circuits has been developed @xcite . this approach is based on the principle of similarity matching : the similarity of outputs must match the similarity of inputs under certain constraints . mathematically , pairwise similarity is quantified by the inner products of the data vectors and matching is enforced by the classical multidimensional scaling ( cmds ) cost function .",
    "dimensionality is reduced by constraining the number of output degrees of freedom , either explicitly or by adding a regularization term .    to formulate similarity matching mathematically",
    ", we represent each centered input data sample received at time @xmath0 by a column vector @xmath1 and the corresponding output by a column vector , @xmath2 .",
    "we concatenate the input vectors into an @xmath3 input data matrix @xmath4 $ ] and the output vectors into an @xmath5 output data matrix @xmath6 $ ] .",
    "then we match all pairwise similarities by minimizing the summed squared differences between all pairwise similarities ( known as the cmds cost function , @xcite ) : @xmath7 to avoid the trivial solution , @xmath8 , we restrict the number of degrees of freedom in the output by setting @xmath9 . then the solution to the minimization problem ( [ eq_cmds_cost ] ) is the projection of the input data on the @xmath10-dimensional principal subspace of @xmath11 @xcite , i.e. the subspace spanned by the eigenvectors corresponding to the top @xmath10 eigenvalues of the input similarity matrix @xmath11 .    in @xcite ,",
    "the similarity matching objective ( [ eq_cmds_cost ] ) was optimized in the online setting , where input data vectors arrive sequentially , one at a time , and the corresponding output is computed prior to the arrival of the next input . remarkably , the derived algorithm can be implemented by a single - layer neural network ( figure  [ fig : network ] , left ) , where the components of input ( output ) data vectors are represented by the activity of input ( output ) neurons at the corresponding time .",
    "the algorithm proceeds in alternating steps where the neuronal dynamics computes the output @xmath12 and the synaptic weights are updated according to local learning rules , meaning that a weight update of each synapse depends on the activity of only its pre- and postsynaptic neurons .",
    "the family of similarity matching neural networks @xcite is unique among dimensionality reduction networks in combining biological plausibility with the derivation from a principled cost function .    in real - world signal - processing applications and neuroscience",
    "context the desired number of output dimensions is often unknown to the algorithm a priori and varies with time because of input non - stationarity .",
    "because the number of output dimensions in the neural circuit solution of ( 1 ) is the number of output neurons it can not be adjusted quickly .    to circumvent this problem , we proposed to penalize the rank of the output by adding a regularizer @xmath13 where @xmath14 is a nuclear norm of a matrix known to be a convex relaxation of matrix rank . from the regularized cost function we derived adaptive algorithms @xcite in which the number of output dimensions is given by the number of input singular values exceeding a threshold that depends on the parameter @xmath15 . because singular values scale with the number of time steps @xmath16 the threshold scales with @xmath16 .",
    "however , choosing a regularization parameter is hard because it requires knowing the exact scale of input singular values in advance .",
    "furthermore , a scale - dependent threshold is not adaptive to non - stationary inputs with changing singular values .    in this paper",
    ", we introduce _ self - calibrating _",
    "regularizers for the cost function ( [ eq_cmds_cost ] ) which do not depend on time explicitly and are designed to automatically adjust to the variation in singular values of the input . specifically , we propose @xmath17 and @xmath18 .",
    "we solve the cost function with these regularizers in both offline ( section ii ) and online ( section iv ) settings .",
    "these two online algorithms also map onto a single - layer neuronal network but , importantly , with different learning rules . in section iii",
    ", we mathemathically illustrate the difference among the three regularizers in a very simple data generation scenario . in section",
    "v , we propose the corresponding algorithms for the non - stationary input scenario by introducing discounting or forgetting .",
    "in this section , we first summarize previous derivation of adaptive dimensionality reduction from cost function ( [ eq_cmds_cost ] ) with a scale - dependent regularizer , @xmath19 , @xcite and discuss its potential shortcomings .",
    "then we introduce two self - calibrating adaptive dimension reduction methods , involving solving cost function ( [ eq_cmds_cost ] ) offline with two new regularizers @xmath20 and @xmath21 .      in order to adaptively choose the output dimension",
    ", @xcite proposed to modify the objective function ( [ eq_cmds_cost ] ) by adding a scale - dependent regularizer : @xmath22 with @xmath23 .",
    "such a regularizer corresponds to the trace norm of @xmath24 which is a convex relaxation of rank .",
    "its impact on the solution can be better understood by rewriting the cost as a full square which has the same @xmath25-dependent terms : @xmath26 where @xmath27 is a @xmath28 identity matrix .",
    "the optimal output matrix @xmath25 is a projection of the input data @xmath29 onto its principal subspace @xcite , with soft - thresholding on the input singular values . indeed , suppose the eigen - decomposition of @xmath11 is @xmath30 , where @xmath31 with @xmath32 are ordered eigenvalues of @xmath33 .",
    "then the solution to the offline problem ( [ eq_offline_ty ] ) is @xmath34 where @xmath35 @xmath36 is the soft - thresholding function , @xmath37 .",
    "@xmath38 consists of the columns of @xmath39 corresponding to the top @xmath10 eigenvalues , i.e. @xmath40 $ ] and @xmath41 is any @xmath42 orthogonal matrix , i.e. @xmath43",
    ".    equation ( 4 ) shows that the regularization coefficient @xmath15 sets the threshold on the eigenvalues of input covariance .",
    "input modes with eigenvalues above @xmath44 are included in the output , albeit with a eigenvalue shrunk by @xmath44 .",
    "modes below @xmath44 are rejected by setting corresponding output singular values to zero .",
    "the scaling of regularization coefficient with time , @xmath16 , ensures that the threshold occupies the same relative position in the spectrum of eigenvalues , which grow linearly with time for a stationary signal .",
    "the algorithm can separate signal from the noise if the signal eigenvalues are greater than the noise eigenvalues and @xmath15 is set in between .",
    "however , setting such value of @xmath15 requires knowing the variance of the input signal and noise . in the offline setting , @xmath15 can be computed from the data .",
    "however , if @xmath15 has to be chosen a priori , e.g. in the online setting , choosing the value that suits various inputs with different signal variance may be difficult .",
    "in particular , when the noise variance of one possible input exceeds the signal variance of another input , a universal value of @xmath15 does not exist .",
    "is it possible to regularize the problem so that the regularization coefficient is chosen only once and applies universally to inputs of arbitrary variance ?      a regularizer that applies a relative , rather than absolute , threshold to input singular values would be able to deal with various input setting .",
    "rather than using the threshold depending on time @xmath16 explicitly , we set the threshold value proportional to the sum of eigenvalues of @xmath11 .",
    "formally , this leads to the following optimization problem with input - output regularizer @xmath20 : @xmath45 which is equivalent to : @xmath46 in the offline setting , the change relative to previous method is minor because we can always have a good choice of @xmath15 to make the former formulation similar to the later after observing the whole @xmath29 . as a consequence , the offline solution of eq .",
    "( [ eq_offline_xy ] ) is very similar to that of the previous method .",
    "@xmath25 is a projection of the input data onto its principal subspace with a different eigenvalue cutoff based on the input eigenvalue sum . in turn",
    ", the coefficient , @xmath15 , sets the threshold relative to the sum of eigenvalues of @xmath47 : @xmath48 even though this solution looks very similar to the scale - dependent adaptive dimension reduction s offline solution ( [ eq_sol_offline_ty ] ) , as we will see in section iv , the fact that the thresholding depends on the input singular values will allow corresponding online algorithms to calibrate to various input statistics .",
    "an alternative way to apply a relative threshold to input singular values is to deploy a regularization proportional to the sum of eigenvalues of @xmath49 . when doing dimension reduction , the sum of eigenvalues of @xmath49 is reflective of the sum of top eigenvalues of @xmath50 .",
    "this reasoning leads us to the following optimization problem with squared - output regularizer @xmath21 : @xmath51 ^ 2.\\ ] ]    this optimization problem is not as simple as in the previous case . yet , the optimal output @xmath25 is still a projection of input data but with an adaptively thresholded singular values of @xmath29 : @xmath52 where @xmath53 and @xmath54 the interger @xmath55 decides how many singular values of the input @xmath29 are cut off .",
    "it is chosen as the largest integer in @xmath56 such that all diagonal elements of @xmath57 are nonnegative .",
    "more details about the derivation of this closed - form offline solution can be found in appendix a. intuitively , the amount of shrinkage still depends approximately on the sum of input eigenvalues but the sum is computed only on the top @xmath55 eigenvalues .",
    "similar to the input - output regularizer , the regularization coefficient @xmath15 sets the threshold relatively to the input statistics .",
    "to gain intuition about the three similarity matching algorithms , we compare their offline solutions for the input covariance with only two sets of degenerate eigenvalues .",
    "suppose that the eigenvalues of the normalized input similarity matrix @xmath58 are @xmath59 with @xmath60 and @xmath61 .",
    "this kind of scenario models a situation when signal and noise eigenvalues are separated by a gap .",
    "we ask when the output similarity matrix keeps track of the @xmath62 signal eigenmodes but rejects the @xmath63 noise eigenmodes .",
    "we first derive , for each method , the range of @xmath15 s achieving this goal .    1 .   for the scale - dependent regularizer @xmath64 , according to eq .",
    "( [ eq_sol_offline_ty ] ) , it is sufficient and necessary to choose the regularization coefficient @xmath15 between @xmath65 and @xmath66 . note that this regularization coefficient @xmath15 , unlike the following two , depends on the absolute scale of the noise level @xmath66 .",
    "2 .   for the input - output regularizer @xmath20 , @xmath67 . according to eq .",
    "( [ eq_sol_offline_xy ] ) , it is sufficient and necessary to choose @xmath15 such that @xmath68 3 .   for the squared - output regularizer @xmath21",
    ", we are aiming at choosing @xmath69 . according to eq .",
    "( [ eq_sol_offline_yy ] ) , it is sufficient and necessary to choose @xmath15 such that the @xmath70st output similarity matrix s eigenvalue is non - positive for @xmath71 .",
    "this results in @xmath72    table [ tab : summary_reg ] summarizes the different ranges of regularization coefficients @xmath15 with which three methods can keep track of the first @xmath62 eigenmodes and the resulting output similarity matrix s top eigenvalue .",
    "[ cols=\"<,^,^\",options=\"header \" , ]     [ tab : summary_reg ]    to illustrate the difference between the known scale - dependent regularizer and the two newly proposed regularizers we compute for what fraction of various pairs of @xmath65 and @xmath66 ( see appendix ) each algorithm achieves the goal for values of @xmath15 from @xmath73 to @xmath74 .",
    "[ fig : signal2noise ] shows the fraction of @xmath65 and @xmath66 pairs for which the signal is transmitted vs. the fraction for which the noise is transmitted as @xmath15 varies along each curve .",
    "the curve corresponding to the scale - dependent regularizer does not reach the point @xmath75 in fig .",
    "[ fig : signal2noise ] indicating that no value of @xmath15 achieves the goal for all pairs of @xmath65 and @xmath66 .",
    "yet , the input - output and the squared - output regularizers pass through the point @xmath76 indicating that universal values of @xmath15 exist for which these algorithms transmit all signal while discarding all noise .     and @xmath66 pairs for which all signal is transmitted vs. fraction of @xmath65 and @xmath66 paris for which all noise is transmitted .",
    "each curve is computed for @xmath15 varying from @xmath73 to @xmath74 for each of the three regularizers . ]",
    "nex , to illustrate the difference between the input - output and the squared - output regularizer we plot a phase diagram ( fig .",
    "[ fig : phase_diag ] ) illustrating the range of parameters where each algorithm transmits all signal and rejects all noise .",
    "specifically , fig .",
    "[ fig : phase_diag ] shows the range of regularization coefficient @xmath15 for different noise - to - signal ratio @xmath77 of the input .",
    "the range of @xmath15 for which the squared - output regularizer achieves the goal is much larger than that for the input - output regularizer indicating its robustness .     for which all signal and no noise is transmitted . ]",
    "in this section , we first formulate online versions of the dimensionality reduction optimization problems presented in section ii .",
    "then we derive corresponding online algorithms and map them onto the dynamics of neural networks with biologically plausible local learning rules .",
    "our derivations follow @xcite . at time @xmath16",
    ", the algorithm minimizes the cost depending on the previous inputs and outputs up to time @xmath16 with respect to @xmath12 , while keeping the previous @xmath78 fixed .",
    "@xmath79 in the unregularized formulation , the output dimensionality is determined by the column dimensition of @xmath25 , @xmath10 .",
    "online adaptive dimension reduction methods adaptively choose the output dimensionality based on the trade - off of the cmds cost with regularizers .",
    "consider the following optimization problem in the online setting : @xmath80 by expanding the squared norm and keeping only the terms that depend on @xmath81 , the problem is equivalent to : @xmath82.\\end{aligned}\\ ] ] in the large-@xmath16 limit , the last two terms can be ignored since the first two terms of order @xmath16 dominates .",
    "the remaining cost function is a positive quadratic form in @xmath12 and we could find the minimum by solving the system of linear equations : @xmath83 out of various ways to solve eq .",
    ", we choose the weighted jacobi iteration because it leads to an algorithm implementable by a biologically plausible neural network @xcite . in this algorithm ,",
    "@xmath84 where @xmath85 is the weight parameter , and @xmath86 and @xmath87 are normalized input - output and output - output covariances : @xmath88 when the jacobi iteration converges to a fixed point , it obtains the solution of the quadratic program .",
    "such algorithm can be implemented by the dynamics of neural activity in a single - layer network . @xmath86 and @xmath87 represent the weights of feedforward @xmath89 and lateral @xmath90 synaptic connections . at each time step @xmath16 , we first iterate   until convergence , then update the weights online according to the following learning rules : @xmath91 where we introduce scalar variables @xmath92 representing cumulative activity of neuron @xmath93 up to time @xmath94 .",
    "the left figure of fig .",
    "[ fig : network ] illustrates this network implementation .",
    "the online optimization problem with input - output regularizer is similar to the previous one . at every time step , the amount of thresholding is given by the cumulative sum of input eigenvalues .",
    "after slight modifications , we arrive at the following neural network algorithm . at time",
    "step @xmath16 , @xmath12 is iterated until convergence based on the following jacobi iteration @xmath95 the online synaptic updates are : @xmath96 this differs from eq . in that a new scalar variable @xmath97 is needed that sums up the current input amplitude across all input neurons .",
    "biologically , in an hebbian / anti - hebbian neural network , such summation could be implemented via extracellular space or glia ( fig .",
    "[ fig : network ] , middle ) .",
    "finally , we consider the following online optimization problem : @xmath98 in the large - t limit , @xmath99 , we could instead solve a simplified version to the problem : @xmath100 after this simplification , the update is similar to the previous input - output regularizer except that the learning rate depends on the norm of current output vector , @xmath101 .",
    "since this is a scalar , such summation can be easily implemented in biology using summation in extracellular space or glia ( fig .",
    "[ fig : network ] , right ) . at time",
    "step @xmath16 , @xmath12 the neural network dynamics iterates until convergence of the jacobi iteration  .",
    "after convergence , synaptic weights are updated online as follows , @xmath102",
    "the online algorithms we proposed assume that the input has stationary statistics .",
    "a truly adaptive algorithm , in addition to self - calibrating the number of dimensions to transmit , should be able to adapt to temporal statistics changes . to address this issue",
    ", we introduce discounting into the cost function which reduces the contribution of older data samples @xcite , or equivalently  forgets \" them , in order to react to changes in input statistics .",
    "following @xcite , we discount past inputs , @xmath103 , and past outputs , @xmath104 , with @xmath105 , where @xmath106 . with such discounting , the effective time scale of forgetting is @xmath107 .",
    "this procedure leads to a modified online cost function , @xmath108 where @xmath109 is a diagonal matrix with @xmath110 on the diagonal .",
    "the term @xmath111 takes the place of the time variable @xmath16 in the original online formulation [ eq_online_ty ] .",
    "when @xmath112 is @xmath113 , reduces to . to derive an online algorithm , we follow the same steps as before . by keeping only the terms that depend on current output @xmath12",
    ", we again arrive at a quadratic function of @xmath12 , which is solved as in by a weighted jacobi iteration .",
    "the online synaptic learning rules get modified : @xmath114 the difference from the non - discounted learning rules ( eq . )",
    "is in how @xmath115 gets updated .",
    "the @xmath116 decay in @xmath115 update prevents @xmath115 from growing indefinitely .",
    "consequently , the learning rate , @xmath117 does not steadily decrease with @xmath16 , but saturates , allowing the synaptic weights to react to changes in input statistics .",
    "we can implement forgetting in this case again by discounting past inputs and outputs : @xmath118 following the same steps as before , a weighted jacobi iteration   is still deployed .",
    "the following learning rules can be derived : @xmath119 again , @xmath116 decay in @xmath92 update allows the network to react to non - stationarity .      discounting past inputs and outputs , we arrive at the online cost function : @xmath120 following the same steps as before ,",
    "a weighted jacobi iteration   is still deployed .",
    "the following learning rules can be derived : @xmath121",
    "we evaluate the performance of three online algorithms on a synthetic dataset . we first generate a @xmath122 dimensional colored gaussian process with a specified covariance matrix . in this covariance matrix , the eigenvalues , @xmath123 and",
    "the remaining @xmath124 are chose uniformly from the interval @xmath125 $ ] .",
    "correlations are introduced in the covariance matrix by generating random orthonormal eigenvectors .",
    "we set @xmath85 to be @xmath126 .",
    "synaptic weight matrices were initialized randomly .",
    "initially , we choose different @xmath15 s for three algorithms such that the thresholding has the approximately the same effect on the original data : the output keeps track of the top three principal components , while discarding the rest principal components .",
    "additionally , the top three eigenvalues of the input similarity matrix are soft - thresholded by @xmath127 .      as shown in fig.[fig : eigen_track ] , with appropriately chosen @xmath15 , all three online algorithms are able to keep track of the top three input eigenvalues correctly .        to quantify the performance of these algorithms more precisely than looking at individual eigenvalues , we use two different metrics .",
    "the first metric , eigenvalue error , measures the deviation of output covariance eigenvalues from their optimal offline values derived in section ii .",
    "the eigenvalue error at iteration @xmath16 is calculated by summing squared differences between the eigenvalues of @xmath128 .",
    "the second metric , subspace error , quantifies the deviation of the learned subspace from the input principal subspace .",
    "the exact formula for the subspace error metric can be found in  @xcite .",
    "fig  [ fig : eigen_subspace_error ] shows that three algorithms perform similarly in terms of these two metrics . both errors for each algorithm decrease as a function of iterations @xmath16 .",
    "[ fig : eigen_error ]     [ fig : subspace_error ]      we evaluate the performance of three online algorithms with forgetting on non - stationary input .",
    "the non - stationary input we use here has a sudden change of input statistics .",
    "we first use the original data generation process for @xmath129 iterations .",
    "@xmath15 is chosen for each algorithm such that the top three principal components are retained and the rest are discarded .",
    "then we change the input data generation by multiplying the eigenvalues of @xmath11 by @xmath127 , in order to see whether the algorithms can still keep track of only the top three principal components . finally at @xmath130 iteration",
    ", we change back to the original statistics .",
    "since the input statistics changes over time , @xmath131 is not reflective of the eigenvalues our online algorithms are tracking at time @xmath16 .",
    "thus we the eigenvalues over a short period @xmath132 of data before @xmath16 .    for the first @xmath129 iterations ,",
    "all three online algorithms keep track of the top three principal components ( see fig . [",
    "fig : sudden2 ] ) .",
    "the fourth output singular value ( fourth red line ) is kept zero all the time , while the top three singular values ( top three red lines ) are above zero . at @xmath129 iteration ,",
    "there is sudden change of input data generation .",
    "the fourth output singular value for input - output regularizer and squared - output regularizer remains zero , however , the fourth output singular value for scale - dependent regularizer becomes larger than zero ( see fig . [",
    "fig : sudden2 ] ) .",
    "scale - dependent regularizer now has an output with effective dimension four rather than three . the other",
    "two are doing a better job in keeping track of only three principal components .    when the input data generation is changed back to the original one at iteration @xmath130 , because of the forgetting mechanism we introduced , all three regularizers are able to keep track of the top three principal components like during the first @xmath129 iterations .",
    "we have introduced online dimensionality reduction algortihms with self - calibrating regularizers . unlike the scale - dependent adaptive dimensionality reduction algorithm @xcite , these self - calibrating algorithms are designed to automatically adjust to the variation in singular values of the input . as a consequence",
    ", they may be more appropriate for modeling neuronal circuits or any related artificial signal processing systems .",
    "first , we cite a lemma from @xcite :    * lemma 1 * : let @xmath133 , where @xmath134 are real numbers , and let @xmath135 , where @xmath136 are real numbers",
    ". then , @xmath137 where @xmath138 is the set of @xmath139 orthogonal matrices .",
    "this lemma states that identity belongs to the optimal orthogonal transformations for diagonal matrix alignment .",
    "a complete proof of the lemma can be found in @xcite .",
    "offline adaptive soft - thresholding optimization problem has the following form : @xmath140 ^ 2.\\ ] ] suppose an eigen - decomposition of @xmath24 is @xmath141 .",
    "since the frobenius norm is invariant to multiplication of unitary matrices , we could multiply on left @xmath142 and on right @xmath143 to obtain an equivalent objective @xmath144 ^ 2.\\ ] ] according to * lemma 1 * , we conclude that @xmath145 could be take @xmath27 at optimum . observing that @xmath146",
    "can be written as a linear transform of its diagonal elements .",
    "then the remaining optimization on the diagonal matrix @xmath147 could be written as @xmath148 where @xmath149 and @xmath150 are diagonals of @xmath151 and @xmath147 respectively .",
    "we have an extra constraint that less than @xmath152 coordinates of @xmath150 could be nonzero .",
    "the problem could be written equivalently , @xmath153 where @xmath154 are the first @xmath152 elements of @xmath150 .",
    "this is a nonnegative least squares problem ( nnls ) , which has the general form @xmath155 for general form of @xmath156 , this nnls does not allow for closed form solution . in general , it is solved by an active - set type optimization algorithm  @xcite , and the number of iterations in the worse case could be exponential on the input dimension . in our case , @xmath157 and it almost allows for an closed form solution .    1 .",
    "since @xmath156 is a diagonal matrix plus a constant matrix , when the values of @xmath149 is ordered , the values of @xmath150 is also ordered .",
    "once the support of @xmath150 is known , the problem is a unconstrained positive definite quadratic program , which always allows for a closed form solution .",
    "3 .   combining ( 1 ) and ( 2 ) , the support of the solution is always the first @xmath55 elements .",
    "it is sufficient to try @xmath152 different supports and find the best feasible solution .",
    "now suppose that we have found that the support of solution is of size @xmath55 , we could obtain a closed form solution of the offline problem .",
    "given the support , the nnls problem is equivalent to the unconstrained quadratic problem . @xmath158 solving the unconstrained quadratic problem , we obtain @xmath159 since @xmath156 is invertible with inverse @xmath160 .",
    "the set of signal and noise setups @xmath161 corresponds to the following @xmath162 cases @xmath163",
    "we would like to thank anirvan sengupta for discussions .",
    "j.  yan , b.  zhang , n.  liu , s.  yan , q.  cheng , w.  fan , q.  yang , w.  xi , and z.  chen , `` effective and efficient dimensionality reduction for large - scale and streaming data preprocessing , '' _ ieee transactions on knowledge and data engineering _ , vol .",
    "18 , no .  3 , pp .",
    "320333 , 2006 .",
    "a.  hyvrinen , j.  hurri , and p.  o. hoyer , _",
    "_ natural image statistics : a probabilistic approach to early computational vision.__1em plus 0.5em minus 0.4emspringer science & business media , 2009 , vol",
    ".  39 .    c.  pehlevan , t.  hu , and d.  b. chklovskii , `` a hebbian / anti - hebbian neural network for linear subspace learning : a derivation from multidimensional scaling of streaming data , '' _ neural computation _ , vol .  27 ,",
    "pp . 14611495 , 2015 .",
    "t.  hu , c.  pehlevan , and d.  b. chklovskii , `` a hebbian / anti - hebbian network for online sparse dictionary learning derived from symmetric matrix factorization , '' in _ 2014 48th asilomar conference on signals , systems and computers_.1em plus 0.5em minus 0.4emieee , 2014 , pp .",
    "613619 .    c.  pehlevan and d.  b. chklovskii , `` a hebbian / anti - hebbian network derived from online non - negative matrix factorization can cluster and discover sparse features , '' in _ 2014 48th asilomar conference on signals , systems and computers_.1em plus 0.5em minus 0.4emieee , 2014 , pp .",
    "769775 .",
    "j.  carroll and j.  chang , `` idioscal ( individual differences in orientation scaling ) : a generalization of indscal allowing idiosyncratic reference systems as well as an analytic approximation to indscal , '' in _ psychometric meeting , princeton , nj _ , 1972 ."
  ],
  "abstract_text": [
    "<S> recently , a novel family of biologically plausible online algorithms for reducing the dimensionality of streaming data has been derived from the similarity matching principle . in these algorithms , </S>",
    "<S> the number of output dimensions can be determined adaptively by thresholding the singular values of the input data matrix . however , setting such threshold requires knowing the magnitude of the desired singular values in advance . here </S>",
    "<S> we propose online algorithms where the threshold is self - calibrating based on the singular values computed from the existing observations . </S>",
    "<S> to derive these algorithms from the similarity matching cost function we propose novel regularizers . </S>",
    "<S> as before , these online algorithms can be implemented by hebbian / anti - hebbian neural networks in which the learning rule depends on the chosen regularizer . </S>",
    "<S> we demonstrate both mathematically and via simulation the effectiveness of these online algorithms in various settings . </S>"
  ]
}