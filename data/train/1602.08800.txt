{
  "article_text": [
    "this work was envisioned as application of the _ multilevel aggregation method _",
    "@xcite developed by the author back in 90s to pca problems .",
    "multilevel aggregation method was an extension of well - known _ multigrid _ methods@xcite from boundary value problems to general structural analysis problems which brought it to the class of _ algebraic multigrid _ methods .",
    "the idea of the aggregation method was to use some naturally constructed course model of the original finite element approximation of a structure which provides a fast convergence for iterative methods for solving large algebraic systems of equations .",
    "one of applications of this method was an iterative solution of large eigenvalue problems arising in structural natural vibration and buckling analyses @xcite . in these problems",
    "a sought set of lowest vibration modes can be thought of as _ principal components _ of structure behavior .",
    "an obvious similarity with pca was a turning point to start looking for a proper way to create an aggregation model for data matrix approximation and use it for efficient solution of pca problems .    in this study pca@xcite is applied to and the method is tested on text analysis problems .",
    "a tested data set consists of documents each of which produces an n - dimensional vector stored as a column of a data matrix which values are term frequencies .",
    "our raw data comes in the form of text files from data sets such as medical abstracts and news groups .",
    "the purpose of pca is to iteratively compute a set of highest eigenvalues and corresponding eigenvectors of the covariance matrix .",
    "covariance matrix is never formed explicitly .",
    "the main operation is multiplication of large sparse data matrix or its transpose by a vector .",
    "the course aggregation model of the original covariance matrix is used in the iterative solution of the eigenvalue problem .",
    "original covariance matrix and its approximation of small size assumes similarity of leading eigenvalues and eigenvectors .",
    "this fact allows fast convergence of subspace iterations at minimal additional computational cost .    for numerical experiments we use r language which is rich of linear algebra , statistical and graphical packages .",
    "pca in multivariate statistics is widely used as an effective way to perform unsupervised dimension reduction .",
    "the essence of this method lies in using singular value decomposition ( svd ) which provides the best low rank approximation to original data according to eckart - young theorem @xcite .",
    "let @xmath0 data points in @xmath1 dimensional space be contained in the data matrix which is assumed already centered around the origin for computational stability @xmath2 then covariance matrix is    @xmath3    let ( @xmath4 , @xmath5 ) be an eigenpair of @xmath6 , where eigenvectors @xmath5 define principal directions .",
    "in order to create an aggregation model we divide the entire set of data vectors @xmath7 into @xmath8 clusters using some similarity criteria where @xmath9 .",
    "we will explain later how we do clustering .",
    "we assume that all vectors within the cluster are similar and a single representative of a cluster is an average of all vectors @xmath7 where @xmath10 or for cluster @xmath11 we have @xmath12 transformation of matrix @xmath13 to @xmath14 is done using matrix @xmath15 which we call _ aggregator _",
    "@xmath16 where @xmath17 = $ ] if @xmath10 then @xmath18 else @xmath19 .",
    "@xmath14 is of size @xmath20 .",
    "approximation @xmath21 of covariance matrix @xmath6 is @xmath22 formally matrix @xmath21 is of the same size as @xmath6 but has a much lower rank .",
    "we do not need to use form for computations . for matrix vector multiplication",
    "we rather use sparse matrix @xmath14 which according to is constructed by simple averaging of vectors inside a cluster and @xmath23 therefore @xmath24 requires @xmath25 operations which is much lower than @xmath26 operations required for @xmath27 .",
    "we also expect and this is confirmed by numerical experiments that convergence of iterative methods for solving partial eigenvalue problem for @xmath21 is faster than that for @xmath6 .",
    "there are quite a few clustering techniques known as computationally efficient . besides since we need clustering as an auxiliary procedure we do not need highly accurate clustering results . in this study",
    "we use k - means clustering algorithm @xcite which became very popular in data mining , unsupervised classification , etc . and which converges quickly to a local optimum .",
    "our experience says that the aggregated problem with a small number of clusters provides a good resemblance of the original and approximated covariance matrices in terms of first ( highest ) eigenvalues which is important for the iterative method described below . in figure 1",
    "this resemblance is demonstrated where we show distribution of first 10 eigenvalues of both matrices where the data matrix @xmath13 was obtained by processing 2014 documents of `` cardiovascular diseases abstracts '' corpus .",
    "matrix @xmath14 was obtained by k - means method with 10 clusters .",
    "we use power iteration method @xcite for for solving auxiliary aggregated eigenvalue problem and a modified power method for solving the original eigenvalue problem .",
    "this method is also known as subspace iteration when used to simultaneously iterate a set of eigenvectors .",
    "one iteration of the power algorithm consists of the following steps : @xmath28 which starts with a set of @xmath29 initial approximations of first eigenvectors @xmath30 .",
    "the key property of the power method is that if approximation @xmath31 is spanned by matrix @xmath6 eigenvectors subspace , then after @xmath11 multiplications of matrix @xmath6 by this vector the linear combination of eigenvectors will be weighted by @xmath32 to the power @xmath11 which gives boost to terms corresponding to highest eigenvalues :    @xmath33    in the method proposed for the first @xmath29 principal directions of pca we will need first @xmath11 orthonormal eigenvectors of @xmath21 @xmath34 where @xmath35 .",
    "these vectors can be obtained by algorithms .",
    "we will also need matrix @xmath36 @xmath37 since @xmath38 and @xmath39 , it is a projector to the subspace of @xmath40-th eigenvector of @xmath21",
    ". we will modify method using this projector in the following manner :    @xmath41    this approach can be thought of as `` help '' to the power iteration method to converge on the subspace of eigenvectors of the aggregated problem .",
    "the intuition for that is similarity of first eigenvectors and eigenvalues of the original and aggregated problem if clustering is done properly .",
    "let @xmath42 where @xmath43 are eigenvectors of the original covariance matrix @xmath6 and @xmath44 be a projector on subspace of @xmath5 .",
    "then    @xmath45    if @xmath46 is chosen big then the second term of this expression dominates over the first term thus providing convergence for @xmath5 in one iteration step .",
    "@xmath46 can be derived from the condition stated in :    @xmath47    this equation leads to the quadratic equation for @xmath46 .",
    "omitting indexes and skipping details we arrive at the following expression for @xmath46    @xmath48    where @xmath49 .",
    "we note that as you can see from @xmath46 is chosen from the previous step to simplify computations .",
    "this can also be justified by the fact that eigenvalues converge faster than eigenvectors .",
    "detailed algorithm discussion is out of scope of this paper .",
    "we just mention here that all operations with matrix @xmath6 are reduced to the matrix vector multiplications of the sparse data matrix @xmath13 or its transpose @xmath50 .",
    "for numerical experiments we used two data sets . the fist one is `` cardiovascular diseases abstracts '' which is a set where each abstract is an individual document .",
    "the data matrix @xmath13 size is 16058 by 2014 where the first value is the total number of terms and the second one is the number of documents .",
    "we searched for 10 first eigenvalues of the covariance matrix @xmath51 and used 10 clusters for constructing auxiliary aggregation problem @xmath52 .",
    "so the size of this problem is more than 201 times lower than that for the original problem .",
    "the problem is solved using algorithm .",
    "figure 2 shows changes of parameter @xmath46 for the first three eigenvectors . as expected the biggest contribution of projectors",
    "is observed in first iterations to suppress errors caused by initial eigenvector guesses .",
    "after some number of iteration contribution of projectors is getting smaller while eigenvectors are getting more accurate .",
    "we measure convergence of eigenvalues through @xmath56 and convergence of eigenvectors by the residual matrix through @xmath57 where @xmath58 is a matrix frobenius norm , @xmath59 consists of orthonormal vectors @xmath60 which are approximations of the eigenvectors and @xmath61 is a diagonal matrix of approximations of eigenvalues .",
    "errors graph is demonstrated in figure 3 .",
    "the second corpus was  talk politics  set from the news groups .",
    "size of this problem is 13511 ( terms ) by 1171 ( documents ) .",
    "we searched for 10 first eigenvalues of the covariance matrix and used 10 clusters again .",
    "the quality of the clustering aggregated model can be viewed by comparing eigenvalues of the original and aggregated covariance matrices .",
    "figure 4 demonstrates a good resemblance of eigenvalues distribution .",
    "convergence graph is demonstrated in figure 5 .",
    "after 40 iterations we got @xmath65 and @xmath66 .",
    "99 v.bulgakov and g.kuhn,high-performance multi - level iterative aggregation solver for large finite - element structural analysis problems , int . j. numer .",
    "methods eng .",
    ", 38 , 3529 - 3544 ( 1995 ) .",
    "w. hackbush , ` multi - grid methods and applications ' , springer , berlin , 1985 .",
    "v.bulgakov , m.belyi , k.mathisen , ` multilevel aggregation method for solving large - scale generalized eigenvalue problems in structural dynamics ' , int . j. numer .",
    "methods eng . , 40 , 453 - 471 ( 1997 ) .",
    "jolliffe i.t .",
    "principal component analysis , series : springer series in statistics , 2nd ed . ,",
    "springer , ny , 2002 , xxix , 487 p. 28",
    "isbn 978 - 0 - 387 - 95442 - 4 c. eckart , g. young , the approximation of one matrix by another of lower rank .",
    "psychometrika , volume 1 , 1936 macqueen , j. b. ( 1967 ) . some methods for classification and analysis of multivariate observations .",
    "proceedings of 5th berkeley symposium on mathematical statistics and probability .",
    "university of california press .",
    ". 281297 .",
    "mr 0214227 .",
    "zbl 0214.46201 . retrieved 2009 - 04 - 07 .",
    "h. rutishauser , simultaneous iteration method for symmetric matrices , numer .",
    ", 16 ( 1970 ) , pp .",
    "reprinted in : linear algebra , j.h .",
    "wilkinson , c. reinsch ( eds . ) , pp .",
    "284301 , springer , berlin , 1971 ."
  ],
  "abstract_text": [
    "<S> motivated by the previously developed multilevel aggregation method for solving structural analysis problems a novel two - level aggregation approach for efficient iterative solution of principal component analysis ( pca ) problems is proposed . </S>",
    "<S> the course aggregation model of the original covariance matrix is used in the iterative solution of the eigenvalue problem by a power iterations method . </S>",
    "<S> the method is tested on several data sets consisting of large number of text documents .    </S>",
    "<S> * keywords : * principal component analysis , clustering method , power iteration method , aggregation method , eigenvalue problem    * vitaly bulgakov *    bulgakov_v@yahoo.com </S>"
  ]
}