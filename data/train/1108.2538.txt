{
  "article_text": [
    "noise is ubiquitous in nature , and virtually all signals carry some amount of random noise . in addition , even the simplest systems can be represented as a set of smaller subsystems interconnected with each other .",
    "there have been numerous studies on how noise affects specific functions ( e.g. @xcite , @xcite and references therein ) , but none of them has looked at how noise propagates in general networks , and how various network structures impact the robustness of each system to noise . although there is evidence that it may degrade the system performance , noise is sometimes necessary for specific functions @xcite . networks in which information is transmitted through a means that is accessible by all the individual units of the network are prone to unwanted crosstalk interactions between various unrelated subsystems @xcite .",
    "both noise and crosstalk have been treated as something unwanted in engineering systems .",
    "however , they do not seem to be a problem in the cell , or in natural biological systems in general , despite the large number of noise sources , the variety of molecules , and the intricate patterns of interactions .",
    "we present a new method to quantify the noise propagation in a system , and the vulnerability of each of its subsystems .",
    "we use results from graph theory and control systems theory to quantify noise propagation in networks , and use them to evaluate various network structures in terms of how well they filter out noise .",
    "we study how crosstalk can help suppress noise , when the noise sources are independent or correlated .",
    "we show that perturbations that depend on the state of the system ( for example , feedback loops that are prone to noise or noisy degradation rates ) have a fundamentally different effect on the system output , compared to noise in the inputs .",
    "finally , we study noise propagation in chemical reaction networks where all reactants may introduce noise , and analytically find that noise correlations may affect the expected behavior of such systems .",
    "in this section , we will briefly revisit some basic tools from control systems theory . consider a linear time invariant system with impulse response @xmath0 @xcite .",
    "the general form of the output when the input signal is @xmath1 is @xmath2 where @xmath3 is the impulse response of the dynamical system .",
    "a system with @xmath4 inputs , @xmath5 states and @xmath6 outputs can be written in the form @xmath7 where the dimensions of matrices @xmath8 and @xmath9 are @xmath10 , @xmath11 and @xmath12 respectively .",
    "the output of the system at time @xmath13 when the input is an impulse applied at time @xmath14 is @xmath15 and equation can be simplified to @xmath16    when the network in question is comprised of elements whose outputs obey linear time - invariant differential equations , we can also find the fourier transform of the network output : @xmath17 where @xmath18 is the impulse response of the system and @xmath19 is the angular frequency .",
    "if the system is causal ( @xmath20 for @xmath21 ) , then the expression above can be simplified by replacing the lower limit of the integral with zero .",
    "when the input is a stochastic process , its output will be a stochastic process as well .",
    "we are interested in the mean , the variance , and occasionally the higher central moments of the system output once the system has reached its equilibrium state .",
    "the mean @xmath22 $ ] and the variance @xmath23 $ ] of the output @xmath24 will be denoted as @xmath25 $ ] and @xmath26 $ ] respectively : @xmath27=\\lim _ { t\\rightarrow \\infty } \\mathbb{e}[y(t ) ] \\qquad",
    "\\textrm{and}\\qquad   \\mathbb{v}[y]=\\lim _ { t\\rightarrow \\infty } \\mathbb{v}[y(t ) ] . \\label{asymptoticexpectationandvariance}\\ ] ] if we know the impulse response of the system , the mean of the output vector can be expressed as @xmath28 & = \\mathbb{e } \\left [ \\int _ { -\\infty}^{t } h ( t - s)u(s)ds \\right ] \\\\ & = \\int _ { -\\infty}^{t } h ( t - s )   \\cdot \\mathbb{e } \\left [ u(s)\\right ] ds , \\\\",
    "\\label{generaloutputexpectedvalue } \\end{aligned}\\ ] ] where in the last equation we have interchanged the expectation with the integration operator , assuming that the input functions are non - pathological , and the quantities are finite , such that all the integrands are measurable in the respective measure space ( fubini s theorem , @xcite ) . in",
    "what follows , we will always assume that all such conditions are satisfied .",
    "the covariance matrix of the outputs , when applying the same input is @xmath29&=\\mathbb{e}[y(t ) \\cdot y^{t}(t)]\\\\ & = \\int _ { -\\infty}^{t } \\int _ { -\\infty}^{t } h ( t - r ) \\left ( \\mathbb{e}\\left [ u(r)u^{t}(s)\\right ] -\\mathbb{e}\\left [ u(r ) \\right ] \\mathbb{e}\\left[u^{t}(s)\\right ] \\right )   h^{t } ( t - s )   dr ds.\\\\ \\end{aligned } \\label{generaloutputvariance}\\ ] ] if in addition @xmath30 for @xmath21 , then according to equation , @xmath31=\\lim _ { t\\rightarrow \\infty}\\int _ { 0}^{t } \\int _ { 0}^{t }",
    "h ( t - r ) \\left ( \\mathbb{e}\\left [ u(r)u^{t}(s)\\right ] -\\mathbb{e}\\left [ u(r ) \\right ] \\mathbb{e}\\left[u^{t}(s)\\right ] \\right)h^{t } ( t - s )   dr ds.\\\\ \\end{aligned}\\ ] ]      in this subsection , we will be describing some elementary properties of the wiener process that will be used in the following analysis .",
    "let @xmath32 , @xmath33 be a sequence of independent identically distributed random variables with zero mean and unit standard deviation .",
    "their sum is @xmath34 we now define the piecewise constant function @xmath35 according to the central limit theorem , the distribution of @xmath36 is _ independent _ of the distribution of the sequence of @xmath37 , as long as they have finite variance , are identically distributed and independent of each other .",
    "the random process @xmath36 is normally distributed with variance equal to the time interval it which it is measured : @xmath38 the difference of two sums @xmath39 with @xmath40 has the same distribution of the random variable @xmath41 and as a result @xmath42",
    "lastly , the random variables @xmath43 and @xmath44 are independent when @xmath45 , since the respective sums consist of independent random variables .",
    "more details on the properties of the wiener process can be found in @xcite .",
    "a _ graph _ ( also called a _ network _ ) is an ordered pair @xmath46 comprised of a set @xmath47 of _ vertices _ together with a set @xmath48 of _ edges _ that are unordered 2-element subsets of @xmath49 .",
    "two vertices @xmath50 and @xmath51 are called _ neighbors _ if they are connected through an edge ( @xmath52 ) and we write @xmath53 , otherwise we write @xmath54 .",
    "the _ neighborhood _",
    "@xmath55 of a vertex @xmath50 is the set of its neighbors .",
    "the _ degree _ of a vertex is the number of its neighbors .",
    "the _ order _ @xmath56 of a graph is the number of its vertices , @xmath57 .",
    "a graph s _ size _ ( denoted by @xmath58 ) , is the number of its edges .",
    "we will denote a graph @xmath59 of order @xmath56 and size @xmath4 as @xmath60 or simply @xmath61 .",
    "a _ path _ is a sequence of consecutive edges in a graph and the length of the path is the number of edges traversed .",
    "the _ distance _ between two vertices @xmath50 and @xmath51 , usually denoted by @xmath62 , is the length of the shortest path that connects these two vertices .",
    "full cycle _ is a cycle that includes all the vertices of the network .",
    "a graph is _ connected _ if for every pair of vertices @xmath50 and @xmath51 , there is a path from @xmath50 to @xmath51 . otherwise the graph is called _",
    "disconnected_. we will be focusing exclusively on connected graphs , because every disconnected graph can be analyzed as the sum of its connected _",
    "components_. a _ tree _ is a graph in which any two vertices are connected by exactly one path . a _ path graph _ is a tree with two or more vertices that has two vertices with degree 1 , while all other vertices have degree 2 .",
    "a thorough treatment of the graph theory notions used in this article can be found in @xcite .",
    "in the state space , when the parameters of the system are deterministic and the input consists of a deterministic and a random component ( white noise ) , then the system is defined by the stochastic differential equation : @xmath63 where @xmath64 is the standard vector wiener process in the time interval @xmath65 and @xmath66 is a deterministic input .",
    "we will denote the value of a function @xmath67 at time @xmath13 as @xmath68 or @xmath69 interchangeably .",
    "the matrix @xmath70 consists of nonnegative entries , possibly time - varying , each of which is proportional to the strength of the corresponding disturbance input .",
    "note that the only difference with the system is that now the infinitesimal state difference @xmath71 depends not only on the current state and the deterministic input , but also a random term @xmath72 .",
    "it should be noted that the fraction @xmath73 does not exist as @xmath74 , so dividing both sides of equation by @xmath75 would not make sense . but",
    "this notation also helps us to intuitively understand the effect of randomness in the system , when we know how the state of the system is affected by the randomness in the inputs .",
    "it also helps us to easily generalize these results when the randomness is a product of many noise sources as we will see in the last section .",
    "the different wiener processes may be correlated with each other but since each input may consist of a weighted sum of all of the different processes through multiplication by matrix @xmath76 , the analysis is simplified if we assume that they are independent .",
    "the output of the system is the superposition of the deterministic output , and the response to the random input : @xmath77 the expected value for the output , according to equation will be @xmath28 & = \\int _ { -\\infty}^{t } h ( t - s)\\mathbb{e } \\left [ u(s)ds+ \\sigma_{s } dw_{s } \\right ] \\\\ & = \\int _ { -\\infty}^{t } h ( t - s ) u(s)ds , \\\\ \\end{aligned}\\ ] ] since brownian motion is a martingale @xcite .    applying equation when the input is white noise , the covariance matrix can be written as @xmath31 & = \\lim _ { t\\rightarrow \\infty } \\mathbb{v}[y(t ) ] \\\\ & = \\lim _ { t\\rightarrow \\infty}\\int _",
    "{ -\\infty}^{t } \\int _ { -\\infty}^{t } h ( t - r ) \\mathbb{e}\\left[dw_{r } \\sigma_{r } \\sigma_{s}^{t } dw^{t}_{s } \\right ]   h^{t } ( t - s ) .",
    "\\\\ \\end{aligned } \\label{linearsystemvariance}\\ ] ] but since the inputs are assumed to be white noise processes , the covariance among all of them is nonzero only if they take place during the same interval , and in that case , the covariance is proportional to the length of this interval . @xmath31 & = \\int _ { -\\infty}^{t } \\int _ { -\\infty}^{t } h ( t - r ) \\left ( \\sigma_{r } \\sqrt{dr }   \\delta(r - s ) \\sqrt{ds } \\sigma_{s}^{t } \\right ) h^{t } ( t - s ) \\\\   & = \\int _ { -\\infty}^{t}h ( t - s ) \\cdot v(s ) \\cdot h^{t } ( t - s ) ds , \\\\ \\end{aligned } \\label{outputvariancefromimpulseresponse}\\ ] ] where @xmath78 is the covariance matrix of the input random vector . for the linear time invariant system and white noise inputs of constant variance @xmath79",
    "is a constant matrix , and we can write @xmath31 & = \\int _ { -\\infty}^{t } ( c e^{a ( t - s ) } b ) \\cdot v \\cdot ( c e^{a(t - s ) } b)^{t } ds \\\\ & = c \\left ( \\int _ { 0}^{+\\infty } e^{a x } b v b^{t } e^{a^{t}x } dx \\right ) c^{t}. \\end{aligned } \\label{timedomainltinoiseresponse}\\ ] ]    the mean and the variance of the output signal in the steady state can be written as a function of the fourier transforms of the input signal and the network transfer function . from equation @xmath80&=\\mathbb{e } \\left [ \\int _ { -\\infty}^{t }   h(t - s ) u(s)\\right]ds \\\\ & = h(t)*\\mathbb{e } \\left [ u(t ) \\right ] \\end{aligned}\\ ] ] where @xmath81 denotes the convolution of two functions @xmath68 and @xmath82 given that it exists .",
    "when the input is constant with time , the expected value of the input is constant as well ( @xmath83=\\mu_{x}$ ] ) and the last expression can be simplified to @xmath27=\\mu _ { x } \\int_{0}^{+\\infty}h(u)du = \\mu_{x } h(0).\\ ] ]    if the input itself is not known , but its frequency content can be estimated , we can find the variance of the output using parseval s theorem : @xmath31&=\\mathbb{e}[y\\cdot y^{t}]=\\lim _",
    "{ t\\rightarrow \\infty } \\int_{-\\infty}^{t } y(t ) y^{t}(t)dt \\\\ & = \\int _ { -\\infty}^{+\\infty } |y(f)|^{2}df = \\int _ { -\\infty}^{+\\infty } y(f)\\cdot y^{*}(f)df \\\\ & = \\int _ { -\\infty}^{+\\infty } h(f ) x(f ) x^{*}(f )   h^{*}(f ) df .",
    "\\end{aligned } \\label{frequencydomainvarianceinputfrequency}\\ ] ]    the formula above is useful if we know or we can estimate the various frequencies of the input random processes . more generally , if we know the autocorrelation function of the random processes in the input , we may find the expected autocorrelation in the output , and then estimate the output variance .",
    "@xmath84    we will be focusing on wiener processes exclusively , because this is the most general approach for sums of random disturbances .",
    "the central limit theorem shows that the sum of a large number of independent identically distributed random variables with finite mean and variance always approaches the normal distribution ( see also equation ) .",
    "the only assumption in the case of additive disturbances is that the inputs at every time are sums of independent random variables of arbitrary distribution of finite standard deviation .",
    "this is a reasonable assumption in most settings .",
    "for example , in biology the poisson distribution is frequently used to model random disturbances @xcite .",
    "the poisson distribution can be well approximated by a gaussian when the event rate is greater than @xmath85 ( see @xcite ) , and the same can be said for small sums of poisson random variables . when the input disturbance at each time is correlated with the disturbances during earlier times , the correlation structure can be emulated by passing white noise through a filter that produces it .",
    "also , in some applications , noise can not be expected to have equal frequency content for all frequencies up to infinity .",
    "we can still use white noise as an input , which we can pass through a filter with zero response for all the frequencies outside the desired range .",
    "tree networks are a special case of networks where there is a unique path among every pair of vertices . in other words , there are no cycles , which makes the analysis of such networks easier .",
    "many natural networks have been found to be locally tree - like @xcite . when analyzing the behavior of a network around an equilibrium point , or if the network is linear , then the analysis can be significantly simplified .",
    "since there is a unique path from any vertex to another , it suffices to analyze path networks , which consist of all their vertices connected in series . for each output ,",
    "the total response of the system is the superposition of the signals caused for all the individual inputs .",
    "first , we will show that in the case of random signals , the order of the nodes in the network does not matter in the case of linear pathways .",
    "then , we will find the variance of a linear path graph assuming that every node is a first order filter .",
    "the result can easily be generalized for the case of arbitrary tree graphs .",
    "finally , we are going to find the optimal placement of poles so that the noise suppression is maximized .",
    "the noise response of a linear pathway is independent of the relative position of its nodes .    without loss of generality",
    ", we can assume that the linear pathway has one input and one output .",
    "otherwise , since the system is linear , we can repeat the process each time considering only the respective subtree . under the last assumption ,",
    "the output is the state of the last node , and all inputs affect only the first node . from equation : @xmath31&=\\int _ { -\\infty}^{+\\infty } h(f ) x(f ) x^{*}(f )   h^{*}(f ) df \\\\ & = \\int _ { -\\infty}^{+\\infty } h(f ) ( x_{1}(f ) + \\ldots + x_{n}(f))(x^{*}_{1}(f ) + \\ldots + x^{*}_{n}(f ) )   h^{*}(f ) df \\\\ & = \\sum _ { k=1}^{n } \\sum _ { m=1}^{n } \\int _ { -\\infty}^{+\\infty } x_{k}(f)x^{*}_{m}(f )   h(f ) h^{*}(f ) df \\\\ & = \\sum _ { k=1}^{n } \\sum _ { m=1}^{n } \\int _ { -\\infty}^{+\\infty } x_{k}(f)x^{*}_{m}(f )   ( h_{1}(f)\\cdot \\ldots \\cdot h_{n}(f ) ) ( h^{*}_{n}(f ) \\cdot \\ldots \\cdot h^{*}_{1}(f ) )    df \\\\ & = \\sum _ { k=1}^{m } \\sum _ { m=1}^{m } \\int _ { -\\infty}^{+\\infty } x_{k}(f)x^{*}_{m}(f )   \\prod _ { n=1}^{n}|h_{n}(f)|^{2 } df .",
    "\\end{aligned}\\ ] ] it is evident that we can interchange the transfer functions inside the product in the integral , without changing its value .",
    "assume that we have a linear pathway such that the system is linear , described by the equation where the dynamical and input matrices are @xmath86 for simplicity , we assume that there is only one noise source and only one output , but since there are no cycles , there is a unique path from each node to every other , which means we can use the result for a linear pathway repeatedly , in order to find the total variance .",
    "the variance is independent of the deterministic input that is applied to the pathway , since the system is linear .    using equation , and after performing all calculations , the variance at the output will be @xmath87    the expression above holds even if there exist two vertices @xmath88 and @xmath89 such that their reaction rates are equal , according to the next lemma .",
    "the output variance of a linear pathway does _ not _ depend on the difference of any of the reaction rates .",
    "we pick two rates @xmath90 and @xmath91 and show that the @xmath92 does not depend on their difference . if we denote @xmath93 the difference @xmath94 appears only in the terms @xmath95 and @xmath96 .",
    "their sum @xmath97 is equal to @xmath98 we set @xmath99 so that sum above can be written as @xmath100 expanding the nominator of @xmath97 and grouping the relevant terms together : @xmath101 it is easy to see that both terms in the nominator of the last fraction have a factor of order @xmath102 , and the lemma is proved , and the fraction does not depend on the square difference @xmath102 .",
    "assume that the same noise source is applied to two different pathways with impulse responses @xmath103 and @xmath104 respectively .",
    "the covariance of the signals in their output will be equal to @xmath105 \\\\ & = \\int _ { 0}^{\\infty } h_{1}(r)h_{2}(r+\\tau ) dr . \\end{aligned}\\ ] ]    the two outputs @xmath106 and @xmath107 are equal to @xmath108 where @xmath36 is the wiener process that drives both systems simultaneously .",
    "taking the expected value of the product of the first and a delayed version of the second , @xmath109 \\\\ & = \\lim_{t\\rightarrow \\infty } \\int _ { -\\infty}^{t } \\int _ { -\\infty}^{t+\\tau } h_{1}(t - x )   h_{2}(t+\\tau - y ) \\mathbb{e } \\left [   dw_{x }   dw_{y } \\right ] \\\\ & = \\lim_{t\\rightarrow \\infty } \\int _ { -\\infty}^{t } h_{1}(t - s )   h_{2}(t+\\tau - s ) ds \\\\ & = \\int _ { 0}^{\\infty } h_{1}(r )   h_{2}(r+\\tau ) dr . \\\\ \\end{aligned}\\ ] ]    assume that noise from a single noise source with standard deviation @xmath110 enters a network , and propagates through @xmath56 independent pathways to reach the output . if the impulse response of each of the independent pathways is @xmath111 respectively , the mean of the output @xmath112 will be zero , and its variance equal to @xmath113    the output vertex will receive a weighted sum of the outputs of the two independent pathways @xmath114 its expected value is equal to zero at all times : @xmath115 & = \\mathbb{e}\\left [ \\sum _ { k=1}^{n } a_{k}y_{k}(t ) \\right ] \\\\ & = \\sum _ { k=1}^{n } \\mathbb{e}\\left [ a_{k}y_{k}(t ) \\right ] \\\\ & = \\sum _ { k=1}^{n }   a_{k } \\int _ { -\\infty}^{t } h_{k}(t - x )   \\sigma \\mathbb{e}\\left [ dw_{x }    \\right ] \\\\ & = 0 . \\end{aligned}\\ ] ]    the variance is equal to : @xmath116 \\\\ & = \\lim _ { t\\rightarrow \\infty }   \\mathbb{e } \\left [ \\left ( \\int _ { -\\infty}^{t } \\sum _ { k=1}^{n } a_{k}h_{k}(t - x)dw_{x } \\right ) \\cdot \\left(\\int _ { -\\infty}^{t } \\sum _ { k=1}^{n } a_{k}h_{k}(t - y ) dw_{y}\\right )   \\right]\\\\ & = \\lim _ { t\\rightarrow \\infty } \\int _ { -\\infty}^{t } \\int _ { -\\infty}^{t }   \\left ( \\sum _ { k=1}^{n } a_{k}h_{k}(t - x)\\right ) \\cdot \\left(\\sum _ { k=1}^{n } a_{k}h_{k}(t - y)\\right )   \\mathbb{e } \\left[dw_{x } dw_{y } \\right ] \\\\ & = \\lim _ { t\\rightarrow \\infty } \\int _ { -\\infty}^{t } \\sigma^{2 } \\left ( \\sum _ { k=1}^{n } a_{k}h_{k}(t - s)\\right)^{2 } ds= \\sigma^{2 } \\int _ { 0}^{\\infty }   \\left ( \\sum _ { k=1}^{n } a_{k}h_{k}(x)\\right)^{2 } dx . \\end{aligned}\\ ] ]    suppose we have a linear pathway with each element representing a single - pole linear filter , and we need to pick the position of the poles such that the variance in the output is minimized .",
    "the next lemma shows an easy way to find the pathway if all its vertices are identical and subject to the symmetric constraints .",
    "a symmetric multivariable function @xmath117 is a function for which @xmath118 where @xmath119 is an arbitrary permutation of the input vector @xmath120 .",
    "assume that a symmetric multivariable function @xmath117 is nowhere constant and has a sign definite hessian matrix .",
    "then it has a unique extremum under symmetric constraints , such that all the elements of the input vector @xmath120 are equal .",
    "[ symmetricmultivariableoptimization ]    since the hessian has the same sign everywhere , the function @xmath67 is strictly convex or strictly concave .",
    "we will assume that @xmath67 is strictly convex , noting that the proof is the similar when @xmath67 is concave .",
    "assume that the extremum of the function @xmath67 is equal to @xmath121 , and the argument that achieves this is @xmath122 .",
    "further assume that @xmath123 and @xmath124 are the minimum and maximum elements of the vector @xmath122 respectively . since @xmath67 is symmetric , @xmath125 where the arguments still satisfy the symmetric constraints .",
    "but since @xmath67 is strictly convex , every convex combination of these values will be @xmath126 generalizing the last argument , it is straightforward to see that @xmath127 therefore , @xmath128 needs to be constant in that area , which contradicts the assumption that the function has sign definite hessian .",
    "when the constraints are convex but not necessarily symmetric , then we can use the lagrangian to find the optimal parameters .",
    "coming back to the linear pathway network , and assuming that the input is white noise , if the poles of the different nodes are placed at @xmath129 , the total variance in the output is equal to ( see equation ):",
    "@xmath130 the function @xmath92 is convex with respect to all its arguments @xmath131 , as an ( infinite ) sum of products of convex functions .",
    "consequently , it has a unique minimum under convex constraints .",
    "the lagrangian of the function for @xmath92 is @xmath132 differentiating with respect to @xmath133 , under the leibnitz integral rule : @xmath134 for every @xmath135 . differentiating with respect to all the parameters will give us @xmath56 equations , and we have one more equation by requiring @xmath136 .",
    "so we can solve the system of @xmath137 equations and @xmath137 unknowns @xmath138 , which is guaranteed to have a unique solution as all functions are convex .    in conclusion , we can find the unique minimum of the variance of a linear pathway , when each node is a single pole linear filter with real negative poles .",
    "given that a linear tree network with independent noise inputs can be decomposed to many linear pathways , this method can be applied to any arbitrary network without cycles .",
    "in a serial pathway where each vertex acts as a filter , the output at each node has a different frequency content as the noise propagates through the network , being filtered at each step .",
    "the variance at each node is decreasing as we move further from the noise source , as is shown in figure [ variancevsserialpathwaylength ] .",
    "as the serial pathway becomes longer , the input and the output become less correlated since their distance increases .",
    "in addition , every node changes the phase of its inputs , which also contributes to the decreased correlation . therefore ,",
    "applying negative feedback or setting up a feedforward cycle can only have a measurable effect if the cycle length is relatively small .",
    "figure [ covarianceandcorrelationserialpathway ] shows the covariances and correlations among the vertices of two simple linear pathways , one unidirectional one bidirectional , as they are depicted in figure [ variancevsserialpathwaylength ] .",
    "cycles can significantly increase the effect of noise in the system .",
    "there are two reasons for this : first the noise can now reach more vertices since the average distance among nodes decreases , and second , every node now receives the same disturbance from at least two different paths , and the two signals are correlated , contributing to larger variance .",
    "an example is shown in figure [ averagevariancepathwayvscycle ] , where we compare the average variance of two systems whose only difference is the connection between the first and the last node .",
    "both networks receive the same inputs , but in the cycle network , the variance is much larger .",
    "the result of the noise is even more pronounced when there is correlation among the noise inputs to different nodes .",
    "the effect of cycles on the output noise can be reduced if we make sure that each independent pathway also changes the phase of its input by different amounts .",
    "different phases in the output ( for at least a relatively large frequency spectrum ) will ensure that the various frequencies partially cancel each other , reducing the output variation .",
    "when a pathway significantly reduces the frequency content , or has small gain for most frequencies , then correlations do not play a significant role .",
    "this behavior is clearly shown in figure [ twopathsdifferentlengthsunidirectional ] for a unidirectional cycle and in figure [ twopathsdifferentlengthsbidirectional ] for a bidirectional cycle .",
    "phase shifts in a pathway are equivalent to time delays , as we will see in the next section .    similarly , negative feedback carefully applied to a network contributes to better disturbance rejection .",
    "when the disturbance is white noise , the effect of feedback is smaller as the feedback cycle gets longer .",
    "the correlation and covariance among vertices decreases with distance and the variance of each node decreases as the length of the pathway increases .",
    "furthermore , as we move towards the end of the pathway , the covariance of nodes of a given distance decreases but the correlation of nodes of a given distance increases .",
    "the last observation is easily justified taking into account that each new node introduces a virtual filter , and the output of nodes will tend to have very similar frequency content the more filters it has gone through .",
    "moreover , from the bode plot of a filter , we can easily see that for the frequencies that are not affected by the filter , their phase is also relatively unaffected , which does not decrease their correlation .",
    "the previous analysis hints to the fact that feedback cycles have limited utility when applied to long pathways .",
    "figure [ feedbackstatisticalanalysis ] shows the variance of the output after we apply negative feedback to a linear pathway .",
    "the darkness of each element @xmath139 of the upper triangular matrix shows the standard deviation of the pathway output when we apply feedback from node @xmath5 to node @xmath4 .",
    "as one would expect , the effect of feedback is directly proportional to the correlation between the source and target vertices .",
    "the same holds for feedforward loops , both positive and negative .    in the case of negative feedforward loop , the variance in the output increases as the loop length increases .",
    "when the feedforward interaction is positive , the variance decreases at first , since the correlation among the different states also decreases , but then goes up , partly because when it affects a node towards the end of the pathway , it does not pass through successive filters , so the variance does not have the chance to decrease ( see figure [ feedforwardstatisticalanalysis ] ) .      as one would expect , adding delay to the interactions among any nodes in a network driven by noise decreases their correlation , meaning that any feedforward or feedback cycles will have a smaller effect .",
    "the covariance of a white noise process with a delayed version of the same signal can be computed the same way as in equation : @xmath140 & = \\lim _ { t\\rightarrow \\infty } \\mathbb{e}[y(t)y(t+\\tau ) ] \\\\ & = \\lim _ { t\\rightarrow \\infty } \\mathbb{e } \\left[\\left(\\int _ { -\\infty}^{t } h ( t - r ) \\sigma_{r}dw_{r}\\right ) \\left(\\int _ { -\\infty}^{t+\\tau } h ( t+\\tau - s ) \\sigma _ { s } dw_{s}\\right ) ^{t } \\right ] \\\\ & = \\lim _ { t\\rightarrow \\infty } \\int _ { -\\infty}^{t } \\int _ { -\\infty}^{t+\\tau } h ( t - s ) \\sigma_{r } \\mathbb{e } \\left[dw_{r}dw_{s}^{t } \\right ] \\sigma^{t}_{s } h^{t } ( t+\\tau - s)\\\\ & = \\lim _ { t\\rightarrow \\infty } \\int _ { -\\infty}^{t } \\int _ { -\\infty}^{t+\\tau } h ( t - r ) \\sigma _ { r}\\sqrt{dr } \\delta(s - r ) \\sqrt{ds } \\sigma _ { s } h^{t } ( t+\\tau - s)\\\\ & = \\lim _ { t\\rightarrow \\infty } \\int _ { -\\infty}^{t}h ( t - s ) v_{s } h^{t } ( t+\\tau - s ) ds\\\\ & = \\lim _ { t\\rightarrow \\infty } \\int _ { 0}^{t}h ( t - s ) v_{s } h^{t } ( t+\\tau - s ) ds . \\\\ \\end{aligned}\\ ] ] if the system is causal , linear and time invariant , and the disturbance is white noise of constant strength added to the input , @xmath140 & = \\int _ { 0}^{\\infty}h ( u ) v h^{t } ( u+\\tau ) du.\\\\ \\end{aligned}\\ ] ] as a specific example , if the impulse response is @xmath141 and the covariance matrix is constant : @xmath140 & = \\int _ { 0 } ^{\\infty } ce^{as}b   v   b^{t } e^{(s+\\tau)a^{t}}c^{t }   ds\\\\ & = c \\left ( \\int _ { -\\infty}^{\\infty}e^{as } b   v b^{t}e^{s a^{t}}ds \\right ) e^{\\tau a^{t } } c^{t}. \\\\ \\end{aligned}\\ ] ] note that the last equation is similar to equation , except for the exponential delay term in the end .",
    "we assume that the dynamical matrix @xmath142 has negative eigenvalues , otherwise the system is not stable .",
    "if the delay is @xmath143 ,    @xmath144    the matrix norm used here is the first order elementwise norm , since we are usually interested in the average variance of all parts of the network .",
    "@xmath145    if we only know the autocorrelation function of the disturbance , we can compute the output variance by moving to the frequency domain .",
    "@xmath146 the shape of the autocorrelation function is a good indicator of how a feedback or feedforward loop will affect the output variation . a correlation function that quickly goes to zero as @xmath147 increases shows that the feedback cycle will not change the variance of the output by a lot",
    "conversely , a random signal with a correlation structure can be easily filtered out by applying an appropriate feedback mechanism .      in a general network ,",
    "signals are propagated from one node to its neighbors .",
    "every vertex receives a filtered version of the noise signal , since every node acts as a single pole filter .",
    "the pole is always real , and proportional to the degree of each vertex , if we assume that each node receives input proportional to the differences of concentrations among its neighbors and itself , or that nodes that interact with many others have proportionally large degradation rates . in this case , we can model the dynamics of a first order linear network through its laplacian matrix . in such a network ,",
    "the state of each node @xmath148 follows the differential equation @xmath149 where @xmath150 for every @xmath151 .",
    "the laplacian of a matrix has been used to model a wide range of systems , including formation stabilization for groups of agents , collision avoidance of swarms and synchronization of coupled oscillators @xcite .",
    "it can also be used in biological and chemical reaction networks , if the degradation rate of each species is equal to the sum of the rates with which it is produced . in this section , we will model the dynamics of each network with its laplacian matrix , where each node is affected by a noise source which is independent of all other nodes , but has the same standard deviation . given that each vertex contributes equally to the overall noise measure of the graph , and since the noise entering each node propagates towards all its neighbors , we can use lemma [ symmetricmultivariableoptimization ] to see that the degrees of the network vertices have to be as similar as possible ( see also @xcite and @xcite ) .",
    "in addition , figure [ averagevariancepathwayvscycle ] shows that the cycles need to be as long as possible in order to avoid any correlations of signals through two different paths . for longer cycles ,",
    "the noise inputs go through more filters before they are combined .",
    "moreover , the phase shift is larger for all their frequencies , which reduces their correlation .",
    "on the other hand , there are bounds on how long a cycle can be given the network s order and size .",
    "networks with long cycles tend to have large radius and larger average distance , as shown in @xcite , which makes noise harder to propagate , having to pass through many filters . by the same token",
    ", networks with a small clustering coefficient will tend to be more immune to noise in their output , since these networks tend to create cliques or densely connected subnetworks @xcite , which will facilitate noise propagation , especially if the noise sources that affect the nodes are correlated , as shown in previous sections . a method to find these graphs",
    "is first to determine their degree sequence , and then determine which one has the largest average cycle length .",
    "this procedure can be simplified by working recursively , building networks with progressively larger order and size .",
    "there is always a connected graph of order @xmath56 and size @xmath4 in which there are @xmath135 vertices with degree @xmath152 and @xmath153 vertices with degree @xmath154 where @xmath155    we will prove the existence of such a graph by starting with its degree distribution and , by successive transformations , convert it to a graph that is known to exist . specifically , at each step we will remove one vertex along with its edges , repeating the process until we end up having a cycle graph .",
    "assume that the degree sequence of the graph @xmath156 is as above , and we arrange the degrees of the vertices in a decreasing order .",
    "@xmath157 according to the havel - hakimi theorem @xcite , the above sequence is a graph sequence if and only if the graph sequence in which the largest degree vertex is connected to vertices @xmath158 is also a graph sequence .",
    "the new graph will have a degree sequence of @xmath159 the key observation is that the transformation above preserves the property of degree homogeneity , in other words , in the new graph @xmath160 , the minimum and maximum vertex degrees are @xmath161 and @xmath162 repeating the process , there will be a graph @xmath163 with at least one vertex of degree @xmath164 .",
    "it follows from the analysis above that the graph @xmath163 will include either one or two vertices of degree @xmath164 .",
    "if it has two vertices with degree one , it is the path graph .",
    "if it has only one vertex with degree one , its degree sequence is not a graph sequence . but",
    "this would mean that the sum of all the degrees is an odd number , which is not possible , since at every transformation , we remove @xmath165 from the sum of degrees .",
    "the graph @xmath163 is a connected graph , and implementing the inverse transforms , we connect new vertices to an already connected network , which guarantees that the final graph is connected .    for networks with a small number of vertices",
    ", we can find all graphs with the desired degree sequence , and among them , exhaustively search for the ones with the largest average cycle length that have the smallest average variance .",
    "for @xmath166 nodes , all connected networks ( with @xmath167 edges ) with most homogeneous degree distribution and longest average cycles are shown in figure [ all6minaveragenoisevariance ] .    to summarize this section ,",
    "positive correlations increase the output variance , and cycles create correlations that make the system more prone to random inputs .",
    "the longer the cycles , the smaller their effect .",
    "the immunity to noise is increased when pathways with the same output introduce different phase shifts , so that the different noise contributions cancel each other at least partially .",
    "this result holds both for feedforward and feedback loops .",
    "when we have some convex constraint on the strength of the various filters , placing the poles , we can find the optimal placement such that the output noise is reduced . specifically , for a linear network where all nodes act as single pole filters and the dynamics of the network are described by its laplacian matrix , there is a systematic way to find the network with the smallest average variance .",
    "the optimal networks have homogeneous degree distribution , and cycles that are as long as possible .",
    "assume that we have a resistor without any external voltage source .",
    "if we measure the voltage between its endpoints , we will find that in any infinitesimal frequency interval @xmath168 there is thermal noise @xmath169 with @xmath170=0 \\qquad \\textrm{and } \\qquad \\mathbb{e}\\left [ v^{2}_{t } \\right ] = 4ktr df\\ ] ] where @xmath171 is the resistance .",
    "the above equation shows that the noise increases as temperature and resistance increase .",
    "we connect a capacitor in parallel with the resistor , and measure the voltage between its endpoints .",
    "we are interested in the total amount of variance of the voltage in the output of the parallel combination of the resistor and the capacitor .",
    "when the switch is open , each of the two subcircuits operate independently , and the output variance for both of them is @xmath172    if we close the switch , the output variance is @xmath173 if the capacitor that connects the two subcircuits has capacitance @xmath174 and the two noise sources are uncorrelated , then both outputs have smaller variances .    in biology , there are countless sources of noise , and the noise is often larger than the signal itself .",
    "it is possible that the cell needs to employ the same technique for reducing noise , distributing it among many different and unrelated components .",
    "crosstalk between different elements of a biological network couples the behavior of different parts of the network , introducing more poles in the network dynamics , as we will see next .",
    "this is equivalent to introducing capacitances between random parts of an electrical network .",
    "the new system filters out noise much more effectively , but on the other hand may be slower to react to various inputs , so there seems to be a tradeoff between how fast a network can respond to changes and how well it filters out noise . the next section studies the effect of crosstalk on the behavior of a small network .",
    "we analyze the four simple subgraphs of figure [ singlenodextalkinteractions ] .    for simplicity , we may disregard any deterministic inputs , since we assume these are linear systems , and any deterministic inputs only affect the output mean , but not its variance .",
    "the stochastic differential equations for all the systems are shown next .    system @xmath175 obeys a simple stochastic differential equation , with one noise input , and it has no other interactions with any other parts of the network .",
    "@xmath176 we have found the solution to this equation in the first section , and the variance in the output is found is equal to @xmath177 this is the trivial case without any crosstalk , and will be used for comparison to the performance of the other subnetworks .",
    "subsystem @xmath178 consists of one vertex that interacts with another node which may also be prone to other noise sources .",
    "crosstalk is modeled through a new vertex in the network , with which the studied node exchanges flows . in chemical reaction networks for example",
    ", the species of interest @xmath179 may be forming a complex @xmath180 with species @xmath181 , whose concentration is supposed to be constant :    @xmath182{c } y.\\ ] ]    we also expect @xmath179 to have a constant degradation rate @xmath88 . the equations for the concentrations of @xmath179 and @xmath180 are    @xmath183    and the output variation is @xmath184    the next step is to see what happens if we increase the crosstalk intensity .",
    "we can distinguish two cases .",
    "the first is when there is crosstalk with one other node ( figure [ multiplextalksinglenode ] ) . in the chemical reaction network analogy ,",
    "@xmath185{n\\cdot c } y.\\ ] ] it is straightforward to find the new differential equations , and the variance in the output .",
    "@xmath186 @xmath187 finally , we consider the case where one node has crosstalk interactions with many different nodes , each of which is affected by a different noise process ( figure [ unitxtalkmultiplenodes ] ) .",
    "the equations that the nodes obey are @xmath188 and the output variance can be computed as @xmath189 when no noise is introduced from the crosstalk nodes ( @xmath190 ) , crosstalk reduces the output variance .",
    "figure [ singlenodextalkcomparisonnoiselessnodes ] compares the last three cases , as the strength of crosstalk interactions among the nodes increases . the crosstalk strength in this case is quantified by the ratio @xmath191 which is equal to the concentration of the crosstalk product @xmath180 in equation in the absence of degradation rates and noise inputs .",
    "it is shown that distributing the crosstalk among many nodes ( equation ) decreases the effect of noise noticeably more compared to the single node case .",
    "this is even more pronounced when we normalize by the variance in the base case ( equation ) .",
    "when crosstalk introduces additional noise , it may increase the variance in the output of any given node if crosstalk is not strong enough to make up for the introduced noise ( figure [ singlenodextalkcomparisonnoisynodes ] ) .",
    "we consider two pathways with crosstalk among more than one of their nodes .",
    "we distinguish two cases , when the two pathways have different or the same outputs . in the first case , since the two outputs are independent , it is easier to reduce the noise variance in both of them , by `` exchanging '' their noise through each node , assuming that the different noise sources are independent .",
    "when the output is the same , there is little reduction in the output variance from crosstalk , since every disturbance eventually reaches the output , and is combined with other correlated versions of the same signal , as shown in figure [ parallelpathwaysxtalknoisereduction ] .",
    "the variance reduction in this case is caused by the increase of the effective pathway lengths , since they follow on average a longer path towards the output .",
    "suppose we have a simple decomposed system : @xmath192 the two outputs of the system are completely independent , since they do not interact in any way , and therefore are uncorrelated .",
    "the variance of each output is :    @xmath193=\\sigma^{2}_{y}=\\frac{\\sigma^{2}}{2\\pi}\\int _ { -\\infty}^{+\\infty } \\frac{1}{\\omega^{2}+ a^{2}}d\\omega = \\frac{\\sigma^{2}}{2a}.\\ ] ]    the system is symmetric , thus @xmath194=\\mathbb{v}[y_{2}]=\\sigma^{2}_{y}$ ] .",
    "if there is crosstalk , then the different states of the system are correlated .",
    "if we model crosstalk as a positive conversion rate from one state to another , with the conversion rates being equal among every pair of states , the @xmath195state system above becomes : @xmath196 the variance of each of the outputs now becomes : @xmath197&=\\int _ { -\\infty}^{+\\infty } ( |h_{11}(f)|^{2}+|h_{21 } ( f)|^{2})df \\\\ & = \\frac{\\sigma^{2}}{2\\pi}\\int _ { -\\infty}^{+\\infty } \\left ( \\left| \\frac{a+c+j\\omega}{(a+j\\omega ) ( a+2c+j\\omega)}\\right|^{2}+\\left| \\frac{c}{(a+j\\omega ) ( a+2c+j\\omega)}\\right|^{2 }    \\right)d\\omega\\\\ & = \\frac{\\sigma^{2}}{2a}\\cdot \\frac{a+c}{a+2 c } , \\end{aligned}\\ ] ] where @xmath198 and @xmath199 are the impulse responses of the first node when the input is an impulse response to the first and second node respectively .",
    "the symmetry is preserved , so @xmath194=\\psi^{2}_{y}=\\mathbb{v}[y_{2}]$ ] .",
    "the variance when crosstalk is present ( @xmath200 ) is always smaller than the initial variance of the outputs . generalizing the equations above for @xmath56 nodes",
    "( see figure [ singlestepcompletegraphxtalk ] ) , we find that @xmath201 and as a result , @xmath202 which tends to zero as @xmath56 becomes large .    alternatively , we can model crosstalk interactions as two species being converted to an intermediate complex , as has been done in the previous sections .",
    "a very simple example of a chemical reaction network which demonstrates this type of behavior is    @xmath203 { } z. \\end{aligned}\\ ] ]    crosstalk is defined by the presence of the last reaction .",
    "we are interested in the variance in the concentration of the output products @xmath142 and @xmath204 , which are directly affected by the variance of @xmath205 and @xmath206 .",
    "the two pathways will interact through an intermediate vertex .",
    "the system can be written as @xmath207    we assume that there is a new `` crosstalk vertex '' @xmath208 among each pair of original vertices .",
    "after linearizing around an equilibrium point @xmath209 , these equations become @xmath210 we find that this network is now more capable of reducing the effect of noise in the output ( figure [ crosstalknoisedirectindirectcomparison ] ) .",
    "there are cases where the noise intensity is proportional to a state of the system . in biological networks for example ,",
    "the degradation of various proteins depends on specific enzymes , whose concentration may be subject to random fluctuations .",
    "this makes the degradation of a protein prone to noise whose source is independent of the protein concentration , but makes the rate at which it degrades proportional to it .",
    "the noise intensity is also proportional to the state of the system when a state is autoregulated , either with positive or negative feedback , where the rate at which the concentration of that particular state changes is subject to random noise .",
    "we will call this type of noise multiplicative , because it is multiplied by the state of the system . as a specific example , consider a gene that is regulated by a single regulator @xcite .",
    "the transcription interaction can be written as @xmath211 when @xmath212 is in its active form , gene @xmath179 starts being transcribed and the mrna is translated , resulting in accumulation of protein @xmath179 at a constant rate @xmath89 .",
    "the production of @xmath179 is balanced by protein degradation ( by other specialized proteins ) and cell dilution during growth with rate @xmath88 . a differential equation that describes this simple system is @xmath213",
    "if there is noise in the concentration of the aforementioned degradation proteins , or the cell growth , the rate @xmath214 is not constant , but it consists of a deterministic component , and a random component .",
    "we will now show that noise in the production rate @xmath215 has a fundamentally different effect in system behavior compared to the effect of noise in the degradation rate @xmath214 , because the latter is multiplied by the concentration of the protein itself .",
    "we will first study the homogenous version of the differential equation , and then we will add the constant production term . ignoring the constant production term , and multiplying by @xmath75 , equation becomes @xmath216 after adding a random component in the degradation rate , the last equation becomes @xmath217 where @xmath36 is the regular wiener process and @xmath218 represents the noise term .",
    "note that the degradation rate and the noise intensity are allowed to be time - dependent .",
    "we will first find the differential of the logarithm of @xmath179 using it s lemma .",
    "we will again require that all the input functions are continuous and non - pathological , so that we can always change the order of taking the limit and the expectation operator .",
    "we will additionally assume that all integrals are finite , so that we can also change the order of integration .",
    "the technical details mentioned above are covered in more detail in @xcite and @xcite .",
    "we apply it s lemma on the logarithm of the random variable @xmath179 , which obeys equation : @xmath219 and applying it s lemma , we get @xmath220 the last two terms can be neglected , since @xmath221 and @xmath222 as @xmath74 . on the other hand , as @xmath75 becomes small , @xmath223=dt.\\ ] ] applying the rules above to equation , @xmath224 we can now solve for @xmath225 : @xmath226 the above derivation is valid only when the equilibrium state ( concentration ) is equal to zero and we start from a state @xmath227 .",
    "if the rate @xmath88 and the noise strength @xmath110 are constant , it simplifies to @xmath228 when the equilibrium is positive ( which is the case for most systems ) , the following differential equation is more relevant : @xmath229 one way to view the terms on the right hand side of equation is that the concentration of species @xmath179 depends on a deterministic input , and is regulated by a negative feedback mechanism which is subject to random disturbances .",
    "it has been shown in @xcite that when feedback is also noisy , there are fundamental limits on how much the noise in the output can be reduced , because there are bounds on how well we can estimate the state of the system . in @xcite",
    "the authors focus on discrete random events ( birth - death processes ) as the source of noise , and the result is that feedback noise makes it harder to control the noise in the output .",
    "we will also show that in our setting multiplicative noise results in larger variance than additive noise of equal strength , and in the next section we will show it propagates in a cascade of linear filters .    using it s lemma once more , and the solution to the homogeneous equation",
    ", we find that the solution to the nonhomogeneous case is @xmath230 where @xmath225 is the solution of the homogeneous equation with initial condition @xmath231 .",
    "if the initial state is equal to zero ( or when @xmath13 is large ) , and the all the parameters are constant , then we can simplify the last expression as @xmath232 note that the form of the last equation is fundamentally different from the response of linear systems to input noise , because here the wiener process input depends on the same time variable as the kernel of the integral .",
    "in other words , the output is not a convolution of the impulse response of the system with the input . in order to see how the noise propagates through the network , and given that we can not use the solution , it is helpful to find the correlation of two versions of this stochastic process , so that we find its frequency content .    as a first step",
    ", we will compute the correlation of the exponential of brownian motion .",
    "the expected value is @xmath233 & = \\mathbb{e}\\left[e^{\\sigma w_{t } } \\right ]   \\\\ & = \\int _ { -\\infty}^{+\\infty}e^{\\sigma\\sqrt{t } x } \\frac{1}{\\sqrt{2\\pi t } } e^{-\\frac{x^{2}}{2 t}}dx \\\\ &",
    "= e^{\\frac{1}{2}\\sigma^{2}t}. \\end{aligned}\\ ] ] the expected value of the square of the exponential wiener process is @xmath234 & = \\mathbb{e}\\left[e^{2 \\sigma w_{t } } \\right ]   \\\\ & = \\int _ { -\\infty}^{+\\infty}e^{2\\sigma\\sqrt{t } x } \\frac{1}{\\sqrt{2\\pi t } } e^{-\\frac{x^{2}}{2 t}}dx \\\\ & = e^{2\\sigma^{2}t}. \\end{aligned}\\ ] ] combining the last two equations : @xmath235 & = \\mathbb{e}\\left[z_{t}^{2 } \\right]-\\left ( \\mathbb{e}\\left[z_{t } \\right ]   \\right ) ^{2 } \\\\ & = e^{2\\sigma^{2}t}-e^{\\sigma^{2}t } \\\\ & = e^{\\sigma^{2}t } \\left ( e^{\\sigma^{2}t}-1 \\right ) .",
    "\\end{aligned}\\ ] ] the expected value of @xmath236 in equation can now be computed : @xmath237 & = b \\int _ { 0}^{t}e^{-\\left ( a+\\frac{1}{2}\\sigma^{2}\\right)u}\\cdot \\mathbb{e } \\left [ e^{\\sigma w_{u } } \\right ] du \\\\ & = b \\int _ { 0}^{t}e^{-\\left ( a+\\frac{1}{2}\\sigma^{2}\\right)x}\\cdot e^{\\frac{1}{2}\\sigma^{2}x } dx \\\\ & = \\frac{b}{a } ( 1-e^{-at } ) \\end{aligned}\\ ] ] which means that @xmath238=\\frac{b}{a}.\\ ] ] as one would expect , it the same as when the system is completely deterministic .",
    "next , we need to compute the covariance of two realizations of the random process @xmath239 : @xmath240 & = \\mathbb{e}\\left [ z_{s}\\cdot z_{t}\\right ] - \\mathbb{e}\\left [ z_{s}\\right ] \\cdot \\mathbb{e}\\left [ z_{t}\\right ] \\\\ & = \\mathbb{e}\\left [ e^{\\sigma w_{s } } e^{\\sigma w_{t } } \\right ]   - \\mathbb{e}\\left [ e^{\\sigma   w_{s}}\\right ] \\cdot \\mathbb{e}\\left [ e^{\\sigma w_{t}}\\right ] \\\\ & = \\mathbb{e}\\left [ e^ { \\sigma w_{s\\wedge t } } e^{\\sigma w_{s\\vee t } } \\right ] - e^{\\frac{1}{2}\\sigma^{2 } ( s+t ) } \\\\ & = \\mathbb{e}\\left [ e^{2\\sigma w_{s\\wedge t } } e^{\\sigma ( w_{s\\vee t}-w_{s\\wedge t } ) } \\right]- e^{\\frac{1}{2}\\sigma^{2 } ( s+t ) }   \\\\ & = \\mathbb{e}\\left [ e^{2\\sigma   w_{s\\wedge t}}\\right ] \\cdot \\mathbb{e}\\left [   e^{\\sigma ( w_{s\\vee t}-w_{s\\wedge t } ) } \\right]- e^{\\frac{1}{2}\\sigma^{2 } ( s+t ) }   \\\\ & = e^{2\\sigma^{2}s\\wedge t } \\cdot e^ { \\frac{1}{2 } \\sigma ^{2 } ( s\\vee t - s\\wedge t ) } - e^{\\frac{1}{2}\\sigma^{2 } ( s+t ) }   \\\\ \\end{aligned}\\ ] ] where we follow the standard notation @xmath241 and @xmath242 . combining all the equations above",
    ", we can find the correlation for the geometric brownian motion : @xmath243 \\\\ & = \\frac{\\text{cov } \\left[z_{s},z_{t } \\right]}{\\sigma _ { z_{s}}\\cdot \\sigma_{z_{t } } } \\\\ & = \\frac{e^{2\\sigma^{2}s\\wedge t } \\cdot e^ { \\frac{1}{2 } \\sigma ^{2 } ( s\\vee t - s\\wedge t ) } - e^{\\frac{1}{2}\\sigma^{2 } ( s+t ) }   } { \\sqrt{e^{\\sigma^{2}s } \\left ( e^{\\sigma^{2}s}-1 \\right ) } \\sqrt { e^{\\sigma^{2}t } \\left ( e^{\\sigma^{2}t}-1 \\right ) } } \\\\ & = \\sqrt{\\frac{e^{\\sigma^{2}s \\wedge",
    "t}-1}{e^{\\sigma^{2}s \\vee t}-1 } } . \\end{aligned}\\ ] ] we now define the covariance and correlation of two such processes with time lag @xmath147 in the equilibrium state as : @xmath244 applying this definition to the general correlation formula of geometric brownian motion , @xmath245 so the correlation is exponentially decreasing as a function of the time lag .",
    "we can now follow the same procedure in order to find the correlation of the stochastic process defined by equation .",
    "its second moment is equal to @xmath246 & = b^{2 } \\int _ { 0}^{t } \\int _ { 0}^{t } e^{-\\left(a+\\frac{\\sigma^{2}}{2}\\right)(x+y ) } \\cdot \\mathbb{e } \\left [   e^{\\sigma w_{x } } e^{\\sigma w_{y } }   \\right]dx dy \\\\ & = b^{2}\\int _ { 0}^{t } \\int _ { 0}^{t } e^{-\\left(a+\\frac{\\sigma^{2}}{2}\\right)(x+y ) } e^{2\\sigma^{2}x\\wedge y } e^ { \\frac{1}{2 } \\sigma ^{2 } ( x\\vee y - x\\wedge",
    "y ) } dx dy \\\\ & = b^{2}\\int _ { 0}^{t } \\int _ { 0}^{x } e^{-\\left(a+\\frac{\\sigma^{2}}{2}\\right)(x+y ) } e^{2\\sigma^{2}y } e^ { \\frac{1}{2 } \\sigma ^{2 } ( x - y ) } dx dy \\\\ &",
    "\\qquad \\qquad + b^{2}\\int _ { 0}^{t } \\int _ { x}^{t } e^{-\\left(a+\\frac{\\sigma^{2}}{2}\\right)(x+y ) } e^{2\\sigma^{2}x } e^ { \\frac{1}{2 } \\sigma ^{2 } ( y - x ) } dx dy \\\\ &",
    "= \\frac{2 \\left(a \\left(1 - 2 e^{-a t}+e^{t \\left(-2 a+\\sigma ^2\\right)}\\right)+\\left(-1+e^{-a t}\\right ) \\sigma",
    "^2\\right)}{a \\left(2 a^2 - 3 a \\sigma ^2+\\sigma ^4\\right ) } \\end{aligned}\\ ] ] where we have assumed that all integrals are finite , which means that the rate @xmath88 has to be greater than the input variance @xmath247 . as",
    "@xmath13 goes to infinity , we can ignore all the decaying exponentials .",
    "@xmath248= \\left\\ { \\begin{array}{ll } \\infty & \\textrm{if $ a\\leq \\frac{\\sigma^{2}}{2}$}\\\\ \\frac{b^{2}}{a(a-\\frac{\\sigma^{2}}{2 } ) } & \\textrm{if $ a>\\frac{\\sigma^{2}}{2}$}.\\\\ \\end{array } \\right .",
    "\\label{geometricnoisesecondmoment}\\ ] ]    in what follows , we will only be interested in the behavior of the system when @xmath249 , because it only makes sense to compute the correlation when the standard deviation is finite .",
    "based on equation , the standard deviation ( when it is defined ) is equal to @xmath250    the standard deviation is proportional to the average value of @xmath180 , since the larger the value of @xmath180 , the larger the strength of the disturbance .",
    "assume that a pathway consists of two nodes .",
    "the first one is affected by multiplicative noise , and it is used as an input to the second node .",
    "we first analyze a system where each state has a single real pole , and later on we will generalize it for an arbitrary number of poles .",
    "the equations of the system are @xmath251 combining the forms for the multiplicative noise and the output of a single pole filter , @xmath252 the mean is equal to : @xmath253&=b c e^{-at } \\int _ { 0}^{t } e^{as } \\left (   \\int _ { 0}^{s } e^{-\\left(f+\\frac{\\sigma^{2}}{2 } \\right)u } \\mathbb{e}\\left[e^{\\sigma w_{u}}\\right ] du\\right ) ds \\\\ & = b c e^{-at } \\int _ { 0}^{t } e^{as } \\left (   \\int _ { 0}^{s } e^{-fu } du\\right ) ds \\\\ & = \\frac{b c \\left(a - a e^{-f t}+\\left(-1+e^{-a t}\\right ) f\\right)}{a ( a - f ) f}. \\end{aligned}\\ ] ] the last equation also holds when @xmath254 , and we can find the expected value by finding the limit as @xmath255 . letting the time @xmath13 go to infinity , @xmath256=\\lim",
    "_ { t\\rightarrow \\infty } \\mathbb{e}[y(t)]=\\frac{bc}{af } \\label{multiplicativeadditivemean}\\ ] ] which is exactly the same as an equivalent system without any noise .",
    "the second moment is @xmath257=b^{2}c^{2}e^{-2at } \\int_{0}^{t } e^{ar}dr \\int_{0}^{t}e^{as}ds \\int_{0}^{r } \\int_{0}^{s } e^{-(f+\\frac{\\sigma^{2}}{2})(x+y ) } \\mathbb{e}\\left [ e^{\\sigma(w_{x}+w_{y})}\\right]dx dy .",
    "\\end{aligned}\\ ] ] we break the integral above in five parts , in order to compute the expected value inside it : @xmath258&=\\int _ { 0}^{t } e^{a r } dr \\int _",
    "{ r}^{t } e^{a s } ds \\int_{0}^{r } \\left(\\int_0^x e^{-f x } e^{-f y } e^{\\sigma ^2 y } \\ ,",
    "dy\\right ) dx\\\\ & \\qquad + \\int _",
    "0^te^{a r } dr \\int _",
    "r^te^{a s } ds \\int_0^r \\left(\\int _ x^se^{-f x } e^{-f y } e^{\\sigma ^2 x}dy\\right ) dx \\\\ & \\qquad + \\int",
    "_ 0^te^{a r } dr \\int _ 0^r e^{a s } ds \\int_0^s \\left(\\int _ 0^xe^{-f x } e^{-f y } e^{\\sigma ^2",
    "y}dy\\right ) dx \\\\ & \\qquad + \\int _",
    "0^te^{a r } dr \\int _ 0^re^{a s } ds\\int_0^s \\left(\\int _ x^se^{-f x } e^{-f y } e^{\\sigma ^2 x}dy\\right ) dx \\\\ & \\qquad + \\int _ 0^te^{a",
    "r } dr\\int _ 0^re^{a s } ds \\int_s^r \\left(\\int _ 0^se^{-f x } e^{-f y } e^{\\sigma",
    "^2 y}dy\\right )   dx . \\\\ \\end{aligned}\\ ] ] after performing all the algebraic calculations , @xmath259=\\lim _",
    "{ t\\rightarrow \\infty } \\mathbb{e}[y^{2}(t)]=\\frac{b^{2}c^{2}}{a^{2}f(f-\\frac{\\sigma^{2}}{2 } ) } \\label{multiplicativeadditivevariance } \\end{aligned}\\ ] ] given that the second moment is finite , which happens when @xmath260 .",
    "the variance is @xmath261=b^{2}c^{2}\\frac{\\sigma^{2}}{a^{2}f^{2}(2f-\\sigma^{2})}.\\ ] ] we can write the above equation as a constant times the variance of the first state : @xmath262&=\\left(\\frac{b}{a}\\right)^{2}\\frac{c^{2}\\sigma^{2}}{f^{2}(2f-\\sigma^{2 } ) } \\\\ & = \\left(\\frac{b}{a}\\right)^{2 } \\mathbb{v}[x ] . \\end{aligned}\\ ] ]    the variance of @xmath180 is fundamentally different from the variance in the case when white noise is added directly to the input , in which case , it would be equal to @xmath263=\\frac{b^{2}}{2a } \\sigma_{in}^{2}.\\ ] ]    the time evolution of the variance is shown in figure [ geometricvsadditivevarianceevolution ] .",
    "when the noise is multiplicative , it takes longer for the variance to settle to its steady state value , which is also an indication that the output variance consists of lower frequencies than in the case of additive noise .",
    "more generally , if we pass the output of the multiplicative noise through an arbitrary linear filter with impulse response @xmath3 then the output is defined as the convolution of the impulse response and the input : @xmath264 the mean is @xmath253&=c\\int _ { 0}^{t } h(t - s ) \\left (   \\int _ { 0}^{s } e^{-fu } du\\right ) ds   \\\\ & = \\frac{c}{f } \\left (   \\int _ { 0}^{t } ( 1-e^{-fs})h(t - s ) ds \\right ) .",
    "\\end{aligned}\\ ] ] the variance is equal to @xmath265&=\\mathbb{e}[y^{2}(t)]-\\left ( \\mathbb{e}[y(t ) ] \\right)^{2 } \\\\ & = c\\int_{0}^{t } h(t - r)dr \\int_{0}^{r } h(t - s)ds \\int_{0}^{s } \\int_{0}^{y } e^{-f(x+y)}e^{\\sigma^{2}x } dx dy \\\\ & \\qquad + c\\int_{0}^{t } h(t - r)dr   \\int_{0}^{r } h(t - s)ds \\int_{0}^{s } \\int_{y}^{r } e^{-f(x+y)}e^{\\sigma^{2}y } dx dy \\\\ & \\qquad + c\\int_{0}^{t } h(t - r)dr   \\int_{r}^{t } h(t - s)ds \\int_{0}^{r } \\int_{0}^{x } e^{-f(x+y)}e^{\\sigma^{2}y } dy dx \\\\ & \\qquad + c\\int_{0}^{t } h(t - r)dr   \\int_{r}^{t } h(t - s)ds \\int_{0}^{r } \\int_{x}^{s } e^{-f(x+y)}e^{\\sigma^{2}x } dy dx \\\\ & \\qquad -\\frac{c^{2}}{f^{2 } } \\left (   \\int _ { 0}^{t } ( 1-e^{-fs})h(t - s ) ds \\right)^{2}. \\end{aligned}\\ ] ] for example , if the filter has one pole at @xmath266 with @xmath267 , then @xmath268 , we can verify that the mean and the variance are equal to the ones found in equations and .",
    "if we have n identical single - pole filters in series , with the same pole at @xmath266 , with @xmath269 , and their input is multiplied by @xmath89 , then the mean is @xmath270 & = \\lim _ { t\\rightarrow \\infty } \\frac{b^{n}c}{f } \\int _ { 0}^{t } ( 1-e^{-fs } ) \\frac{(t - s)^{n-1}}{(n-1)!}e^{-as } ds \\\\ & = \\frac{b^{n}}{a^{n}}\\cdot \\frac{c}{f } \\end{aligned}\\ ] ] and the variance is equal to @xmath262=\\left ( \\frac{b}{a } \\right)^{2n } \\left (   \\frac{c}{f}\\right)^{2}\\frac{\\sigma^{2}}{(2f-\\sigma^{2})}. \\end{aligned}\\ ] ]    the above results show how variation that enters the system through noisy degradation rates affects the output of a given pathway .",
    "for example , in the two - step cascade @xmath271 described by , species @xmath180 is affected by multiplicative noise , and then is used as an input to the next reaction that produces @xmath208 .",
    "the second reaction acts as a first order linear filter , and the noise propagates to the pathway output @xmath208 .",
    "the analysis can be used for any system that can be described by linear differential equations .",
    "if a linear time invariant system is described by then , if there is noise in the input @xmath50 or its input matrix @xmath204 , then we can consider noise a new additional input as in equation , and solve it accordingly .",
    "the same holds for the off - diagonal elements of the dynamical matrix @xmath142 . but",
    "noise in the diagonal elements of @xmath142 is multiplicative noise , and needs to be considered separately from all other noise sources , and it leads to qualitatively different behavior than the previous kinds of input noise .",
    "in this section , we will examine how noise propagates in general linear chemical reaction networks .",
    "noise in chemical reaction networks that do not involve bimolecular or higher order reactions has been studied extensively ( see for example @xcite ) and chemical reactions have also been analyzed as analog signal processing systems @xcite . in this section , we will study reactions where two or more reactants are noisy , and their disturbances may be correlated with each other .      consider the following reaction : @xmath272 further assume that the concentration of @xmath179 and @xmath180 is subject to random white noise fluctuations around a deterministic mean value : @xmath273 and @xmath208 degrades with a rate proportional to its concentration .",
    "the corresponding stochastic differential equation is @xmath274 \\end{aligned } \\label{bimolecularreactionwithtwonoiseterms}\\ ] ] where @xmath275 and @xmath36 are standard brownian motions .",
    "equation is a natural generalization of the case where we have only one or more noise terms that are added to the deterministic differential equation . in",
    "all stochastic differential equations so far , we multiply the deterministic factors that contribute to the infinitesimal change in the state of the system by @xmath75 , and then we add the noise terms",
    ". when we have a product of two noisy inputs , we will first consider the noiseless case , and then add all the noise terms , and their products as well . in equation",
    "the deterministic term is equal to @xmath276 and the noise terms that are added are equal to @xmath277 .",
    "the term @xmath278 $ ] is the differential of the quadratic covariation process of @xmath275 and @xmath36 . if the two processes have correlation @xmath279 , then @xmath280=\\rho dt.\\ ] ] simplifying the last expression for @xmath281 , @xmath282 which is the familiar ornstein@xmath283uhlenbeck process with two noise sources .",
    "the final expression for the concentration of @xmath208 is @xmath284 as the effect of the initial conditions diminishes , the mean is @xmath285=\\frac{1}{a}(x_{0}y_{0}+\\rho \\sigma_{x}\\sigma_{y})\\ ] ] and the variance is equal to @xmath286&=\\lim _",
    "{ t\\rightarrow \\infty } \\mathbb{v}[z(t)]=\\frac{y_{0}^{2}\\sigma_{x}^{2}+x_{0}^{2}\\sigma_{y}^{2}+2x_{0}y_{0}\\rho \\sigma_{x}\\sigma_{y}}{2a}. \\end{aligned}\\ ] ] an important consequence of correlations in the input noise ( @xmath287 ) is that the mean is different from the case where there is no noise , even if both noise terms in have themselves zero mean . if the correlation is negative , the mean is lower and vice versa .",
    "in addition , the variance is larger when there are positive correlations in the two input noise terms , as expected .",
    "when the correlation is negative , the two noise processes partially cancel each other , resulting in lower variance .",
    "we can generalize the above results to general reactions of the form @xmath288 where each of the elements of the left - hand side is assumed to be a random variable that consists of a deterministic mean @xmath289 and a standard white noise process @xmath290 multiplied by the standard deviation of its concentration . @xmath291",
    "the concentration of the product @xmath292 is described by a stochastic differential equation : @xmath293 the last equation is derived by using it s box rule , and the fact that higher order products of wiener processes have variance that tends to zero faster than @xmath75 as @xmath74 . as in the bimolecular case , we multiply the noiseless input by @xmath75 , as in the corresponding ordinary differential equation , and then we add all the noise terms , and their products .",
    "the mean ( disregarding initial conditions ) is @xmath294=\\frac{b_{j}}{f_{j}}\\left(\\prod _",
    "{ u=1}^{n}\\bar{x}_{u } + \\sum _ { k=1}^{n } \\sum _ { m=1}^{n}\\sigma_{k}\\sigma_{m } \\left(\\prod_{\\substack{u=1 \\\\ u \\neq k , m } } ^{n}\\bar{x}_{u } \\right ) \\rho_{k , m }   \\right)\\ ] ] which is different from the case when there is no noise , if there are correlations among the noise terms .",
    "the last equation clearly shows that noisy inputs can have an effect in the average of the concentration of the output , even if their mean is zero .",
    "the amount by which they shift the mean depends on their own variances , their correlations , and the product of concentrations of all _ other _ reactants .",
    "the variance is equal to @xmath295=\\frac{b_{j}^{2}}{2f_{j}}\\left (   \\sum_{k=1}^{n } \\sigma_{k}^{2 }    \\prod_{\\substack{u=1 \\\\ u \\neq k } } ^{n}\\bar{x}_{u}^{2}+ \\sum_{k < m}2\\rho_{km}\\sigma_{k}\\sigma_{m}\\sigma_{k}^{2 }    \\prod_{\\substack{u=1 \\\\ u \\neq k , m } } ^{n}\\bar{x}_{u}^{2 } \\right).\\ ] ]    as before , positive correlations increase variance , negative correlations reduce it , and the extent by which the correlations affect it depends on the concentrations of the other species in the reaction .",
    "suppose we have the following simple reaction : @xmath296 where @xmath297 fluctuate around an average value , but the noise has already passed through a linear filter . in this case ,",
    "we can write the equation that @xmath180 satisfies as an ordinary differential equation : @xmath298 where once again @xmath299 is the standard wiener process corresponding to species @xmath135 . expanding the last equation , @xmath300 we have omitted all the terms whose order is larger than @xmath75 as @xmath74 , gathering them under the term @xmath301 . by using it s box rule again , we can replace the products of wiener processes by their correlation times the infinitesimal time interval @xmath75 .",
    "@xmath302    note that the second sum of integrals is deterministic and does not depend on any wiener process . setting @xmath303",
    "the solution to the last differential equation ( with zero initial conditions ) is @xmath304 more generally , if the differential equation for the output has impulse response @xmath82 , and initial condition @xmath305 , @xmath306 where all terms except for the last sum are deterministic .",
    "the last equation nicely decomposes the factors that drive the output @xmath236 .",
    "the first term is the effect of the initial condition , the second term denotes the effect of the mean value of the inputs , the third results from the noise correlations of the inputs , and the last term corresponds to the sum of the random fluctuations of all input sources .",
    "if the output of reaction receives inputs that are affected by both filtered and unfiltered disturbances , then we can use the same methods to find the mean and standard deviation of the output .",
    "we will analyze the case where we have two inputs , one of each type , since the generalization to an arbitrary number of inputs is straightforward .",
    "suppose that the chemical species @xmath180 depends on species @xmath307 and @xmath308 @xmath309 where the inputs @xmath307 and @xmath308 are defined by the following differential equations : @xmath310 and @xmath311 where @xmath275 and @xmath36 are standard wiener processes .",
    "the stochastic differential equation for @xmath180 is @xmath312 since @xmath313 the output is equal to @xmath314 the mean is @xmath256=\\frac{1}{a } \\left ( \\bar{x}_{1}\\bar{x}_{2}+ \\rho h_{0}\\sigma_{1}\\sigma_{2 } \\right)\\ ] ] which differs from the noiseless case by the last term , which is proportional to the correlation and the standard deviation of the noise inputs .",
    "similarly , the variance is found to be equal to @xmath315=v_{1}(t)+v_{2}(t)+v_{12}(t)\\ ] ] where @xmath316 @xmath317 and @xmath318 the first component @xmath319 is the variance because of the noise in the first input @xmath320 , @xmath321 the variance because of noise in the second input , and the last term @xmath322 is the variance emanating from their correlation .",
    "when the inputs @xmath307 and @xmath308 in both have a filtered multiplicative noise component , then the differential equation becomes @xmath323 in order to account for the possibly nonzero correlation between processes @xmath275 and @xmath36 , we write each of them as a sum of two uncorrelated standard processes : @xmath324 the processes @xmath325 and @xmath326 have correlation zero , and @xmath327 is the correlation between @xmath275 and @xmath36 : @xmath328    we are interested in finding the mean and variance of @xmath180 .",
    "first , we will compute the expected value of the product of the two exponential wiener processes @xmath275 and @xmath36 .",
    "@xmath329 & = \\mathbb{e}\\left[e^{\\sigma_{1}\\left(aa_{x}+\\sqrt{1-a^{2}}b_{x}\\right ) }   e^{\\sigma_{2 } \\left(ba_{y}+\\sqrt{1-b^{2}}c_{y}\\right ) } \\right ]   \\\\ & = \\mathbb{e}\\left[e^{a\\sigma_{1 } a_{x}+b\\sigma_{2}a_{y}}\\right ] \\cdot \\mathbb{e}\\left[e^{\\sigma_{1}\\sqrt{1-a^{2}}b_{x}}\\right ] \\cdot \\mathbb{e}\\left[e^{\\sigma_{2}\\sqrt{1-b^{2}}c_{y}}\\right ] \\\\ & = e^{\\frac{1}{2 } \\left(a\\sigma_{1}+b\\sigma_{2}\\right)^{2 } x\\wedge y } e^{\\frac{1}{2 } \\left(\\left(a\\sigma_{1}\\right)^{2}\\delta_{x}+\\left(b\\sigma_{2}\\right)^{2}\\delta_{y } \\right)(x\\vee y - x\\wedge y ) }   e^{\\frac{1}{2 } \\sigma_{1}^{2}\\left(1-a^{2 } \\right)x } e^{\\frac{1}{2 } \\sigma_{2}^{2}\\left(1-b^{2 } \\right)y } \\\\ \\end{aligned}\\ ] ] where @xmath330 denotes the kronecker delta with @xmath331 and @xmath332 .",
    "the expected value of the input of the differential equation is @xmath333 & = \\mathbb{e}\\left[\\left(\\bar{x}_{1 } \\int_{0}^{t}e^{-(\\lambda_{1}+\\frac{\\sigma_{1}^{2}}{2})x}e^{\\sigma_{1}u_{x}}dx \\right)\\left ( \\bar{x}_{2}\\int_{0}^{t}e^{-(\\lambda_{2}+\\frac{\\sigma_{2}^{2}}{2})y}e^{\\sigma_{2}w_{y}}dy \\right )   \\right ] \\\\ & = \\lambda_{1}\\lambda_{2}\\bar{x}_{1}\\bar{x}_{2 }   \\int_{0}^{t } e^{-(\\lambda_{1}+\\frac{\\sigma_{1}^{2}}{2})x }   \\left ( \\int_{0}^{t } e^{-(\\lambda_{2}+\\frac{\\sigma_{2}^{2}}{2})y } \\mathbb{e}\\left[e^{\\sigma_{1}u_{x } } e^{\\sigma_{2}w_{y}}\\right ] dy\\right ) dx   \\\\ & = \\lambda_{1 } \\lambda_{2}\\bar{x}_{1}\\bar{x}_{2 }   \\int_{0}^{t } e^{-\\lambda_{1}x }   \\left ( \\int_{0}^{x } e^{-(\\lambda_{2}-\\rho \\sigma_{1}\\sigma_{2})y } dy\\right ) dx   \\\\ & \\qquad + \\lambda_{1 } \\lambda_{2 } \\bar{x}_{1}\\bar{x}_{2 }   \\int_{0}^{t } e^{-(\\lambda_{1}+\\rho \\sigma_{1}\\sigma_{2})x }   \\left ( \\int_{x}^{t } e^{-\\lambda_{2}y}dy\\right ) dx   \\\\ & = \\lambda_{2}\\bar{x}_{1}\\bar{x}_{2 } \\frac{e^{-t \\lambda _ 1 } \\left(\\left(1-e^{\\rho t \\sigma_{2 }   \\sigma_{1 } -t \\lambda _ 2}\\right ) \\lambda _",
    "1+\\left(-1+e^{t \\lambda _ 1}\\right ) \\left(\\rho \\sigma_{2 }   \\sigma_{1 } -\\lambda _ 2\\right)\\right)}{\\left(\\rho \\sigma_{2 }   \\sigma_{1 } -\\lambda _ 2\\right ) \\left(-\\rho \\sigma_{2 }   \\sigma_{1 } + \\lambda _ 1+\\lambda _ 2\\right ) } \\\\ & \\qquad + \\lambda_{1}\\bar{x}_{1}\\bar{x}_{2 } \\frac{e^{-t \\lambda",
    "_ 2 } \\left(\\left(1-e^{t \\lambda _",
    "2}\\right ) \\rho \\sigma_{2 }   \\sigma_{1 } -\\left(1-e^{t \\lambda _ 2}\\right ) \\lambda _ 1-\\left(1-e^{\\rho t \\sigma_{2 }   \\sigma_{1 } -t \\lambda _ 1}\\right ) \\lambda _ 2\\right)}{\\left(\\rho \\sigma_{2 }   \\sigma_{1 } -\\lambda _ 1\\right ) \\left(\\rho \\sigma_{2 }   \\sigma_{1 } -\\lambda _ 1-\\lambda _ 2\\right ) } \\end{aligned}\\ ] ] where we assume that @xmath334 the inequalities above guarantee that the inputs have finite variances , as shown in equation . in the equilibrium state ,",
    "@xmath335 = \\bar{x}_{1}\\bar{x}_{2 } \\frac{\\lambda _ 1+\\lambda _ 2}{\\left(\\lambda _ 1+\\lambda _ 2 -\\rho \\sigma_{1 }   \\sigma_{2 } \\right)}.\\ ] ] the output average is then equal to @xmath256=\\lim_{t \\rightarrow \\infty } \\mathbb{e}\\left [ y(t ) \\right ] =   \\frac{\\bar{x}_{1}\\bar{x}_{2}}{a}\\cdot \\frac{\\lambda _ 1+\\lambda _ 2}{\\left(\\lambda _ 1+\\lambda _ 2 -\\rho \\sigma_{1 }   \\sigma_{2 } \\right)}.\\ ] ]    the last equation clearly shows that if the input noise sources are correlated ( @xmath287 ) , the average value of the output will be different from the value when there is no correlation ( @xmath336 ) .",
    "as shown in the other types of noise , positive correlations increase the mean , and negative correlations reduce it .",
    "the variance can be computed using the same methods .",
    "first , we will calculate the expected value of a product of different instances of a standard wiener process .    if @xmath337 is an ordered set of times such that @xmath338 and @xmath339 are arbitrary positive numbers denoting standard deviations , then @xmath340 = \\exp \\left [ \\frac{1}{2 } \\sum_{k=1}^{n }   \\left ( \\sum_{m = k}^{n } \\sigma_{m}\\right)^{2 } ( t_{k}-t_{k-1 } ) \\right]\\ ] ] where @xmath36 is the standard wiener process .    for each @xmath341",
    ", we decompose the wiener process @xmath342 as a sum of independent processes : @xmath343 based on the sum above , we can write @xmath344 \\\\ & = \\exp \\left [ \\sum_{k=1}^{n}\\sigma_{k } \\sum _ { m=1}^{k } \\left (   w_{t_{m}}-w_{t_{m-1 } }   \\right)\\right ] \\\\ & = \\exp \\left [ \\sum_{k=1}^{n } \\left (   w_{t_{k}}-w_{t_{k-1 } }   \\right)\\sum _ { m = k}^{n } \\sigma_{k } \\right ] \\\\ & = \\prod_{k=1}^{n } \\exp\\left[\\left (   w_{t_{k}}-w_{t_{k-1 } }   \\right ) \\displaystyle \\sum _ { m = k}^{n } \\sigma_{k}\\right ] \\\\ \\end{aligned}\\ ] ] where in the last equation , we changed the order of summation making use of the triangle rule . all terms in the last product are independent : @xmath345&=\\mathbb{e}\\left [ \\prod_{k=1}^{n } \\exp\\left[\\left (   w_{t_{k}}-w_{t_{k-1 } }   \\right ) \\displaystyle \\sum _ { m = k}^{n } \\sigma_{k}\\right ] \\right ] \\\\ &",
    "= \\prod_{k=1}^{n } \\mathbb{e}\\left[\\exp\\left[\\left (   w_{t_{k}}-w_{t_{k-1 } }   \\right ) \\displaystyle \\sum _ { m = k}^{n } \\sigma_{k}\\right ] \\right ] \\\\ & = \\prod_{k=1}^{n } \\exp\\left[\\frac{1}{2}\\left(t_{k}-t_{k-1}\\right ) \\displaystyle \\left(\\sum _ { m = k}^{n } \\sigma_{k}\\right)^{2 } \\right ] \\\\ & = \\exp\\left[\\frac{1}{2 } \\sum_{k=1}^{n } \\displaystyle \\left(\\sum _ { m = k}^{n } \\sigma_{k}\\right)^{2}\\left(t_{k}-t_{k-1}\\right )   \\right ] .\\\\",
    "\\end{aligned}\\ ] ]    when one of the inputs is affected by multiplicative noise , and the other by additive noise , the mean value of the output is not affected , even if the driving noise is the same in both cases . if consider again the chemical reaction , the differential equation in that case becomes @xmath346 the input is equal to @xmath347 and its expected value is @xmath333 & = \\lambda_{1}\\bar{x}_{1}\\bar{x}_{2 } \\int_{0}^{t}e^{-(\\lambda_{1}+\\frac{\\sigma^{2}}{2})x}\\mathbb{e } \\left [ e^{\\sigma w_{x}}\\right]dx   \\\\ & \\qquad + \\sigma \\lambda_{1 } \\bar{x}_{1 } \\int_{0}^{t }   \\int_{0}^{t } e^{-(\\lambda_{1}+\\frac{\\sigma^{2}}{2})x }   e^{-\\lambda_{2}(t - y ) } \\mathbb{e } \\left [ e^{\\sigma w_{x } } dw_{y } \\right ]   dx .",
    "\\end{aligned } \\label{productadditivegeometricexpectedvalueofinput}\\ ] ] in order to compute the second term of the last equation , we will need the following lemma about the expected value of the product an exponential wiener process with an infinitesimal difference of the same process .    if @xmath36 is a standard wiener process , then @xmath348 = \\left\\ { \\begin{array}{ll } 0 & \\textrm{if $ s\\leq t$}\\\\ \\sigma^{2 } e^{\\frac{\\sigma^2}{2}s } dt & \\textrm{if $ s > t$}. \\end{array } \\right.\\ ] ]    if @xmath349 , then @xmath350 and @xmath64 are uncorrelated , so @xmath351=\\mathbb{e}\\left[e^{\\sigma w_{s}}\\right]\\mathbb{e}\\left[dw_{t}\\right]=0.\\ ] ] now , if @xmath352 , then @xmath353 & = \\mathbb{e } \\left[e^{\\sigma w_{a } }   \\right ] \\mathbb{e } \\left[e^{\\sigma ( w_{b}-w_{a})}(w_{b}-w_{a } )   \\right ] \\mathbb{e } \\left[e^{\\sigma(w_{s}-w_{b } ) }   \\right ] \\\\ & = e^{\\frac{1}{2}\\sigma^{2}a } e^{\\frac{1}{2 } \\sigma^{2}(b - a ) } \\sigma^{2}(b - a ) e^{\\frac{1}{2}\\sigma^{2 } ( s - b ) }   \\\\ & = \\sigma^{2 } e^{\\frac{1}{2}\\sigma^{2}s}(b - a ) .",
    "\\\\ \\end{aligned}\\ ] ] setting @xmath354 and @xmath355 , we get the desired result .    recalling equation , @xmath333 & = \\lambda_{1}\\bar{x}_{1}\\bar{x}_{2 } \\int_{0}^{t}e^{-\\lambda_{1}x}dx+\\sigma^{3 } \\lambda_{1 } \\bar{x}_{1 }   e^{-\\lambda_{2}t }   \\int_{0}^{t }   \\left(\\int_{y}^{t } e^{-\\lambda_{1 } x }   e^{\\lambda_{2}y } dx \\right ) ds \\\\ & = \\bar{x}_{1}\\bar{x}_{2 } \\left(1-e^{-\\lambda_{1}t }   \\right)+   \\sigma^{3}\\bar{x}_{1 } \\frac{e^{-t \\left(\\lambda _ 1+\\lambda _ 2\\right ) }   \\left(\\lambda _ 1 \\left(1-e^{t \\lambda _ 2}\\right ) -\\left(1-e^{t \\lambda _ 1}\\right )",
    "\\lambda _ 2\\right)}{\\lambda _ 2\\left(\\lambda _ 1-\\lambda _ 2\\right)}. \\end{aligned}\\ ] ] as time @xmath13 grows large , @xmath356=\\bar{x}_{1}\\bar{x}_{2}\\ ] ] and the mean of the output is @xmath256=\\frac{1}{a}\\bar{x}_{1}\\bar{x}_{2}\\ ] ] which is exactly the same as in the case where the two noise inputs are completely uncorrelated .",
    "so , input noise correlation does not affect the average concentration of the output in this case .",
    "this section has analyzed how noise propagates in an arbitrary chemical reaction network where one or more inputs include a random component .",
    "the different noise sources may have arbitrary correlations with each other .",
    "we have studied the propagation of both additive and multiplicative noise .",
    "one of the main results is that even if all noise sources have mean equal to zero , their correlations shift the mean of the outputs , for both types of noise . if there is positive correlation , the mean of the output increases , and when the correlation is negative , it shifts lower , and the same is true for the output variance .",
    "we have shown how noise propagates in networks and how a network s noisy parameters can affect its output .",
    "since many biological networks are locally tree - like , we have studied how noise propagates in the absence of feedforward or feedback cycles .",
    "tree networks are relatively easy to quantitatively analyze , since there is only one path from each node to another .",
    "we have derived a method to compute the variance of the output of any tree network , and shown that the variance is minimized when there are no `` bottlenecks '' in each pathway , in other words when there is no rate limiting step",
    ". when a network is not a tree , there are cycles , which means that a signal ( along with its noise ) can propagate through two or more paths towards the output .",
    "feedback cycles typically reduce the output variance , and feedforward cycles increase it . when the noise sources are correlated , the variance in the output is larger , and small cycles have a stronger influence on the output , compared to longer cycles in both cases .",
    "delays contribute to the decrease of the output noise when we have two or more noise sources , since their correlation is diminished .",
    "crosstalk is also shown to decrease the output variance , but the tradeoff is that the output mean is lowered , or the concentration of the inputs needs to be proportionally higher in order to ensure the same output . in biological and chemical reaction networks , the reaction rates are prone to noise , since they depend on the concentration of other species . when the degradation rates are affected by noise , the result is increased output variance , which also depends on the concentration of the respective species , and the form of the output is different from when the noise is in the inputs , in the sense that higher concentrations also correspond to larger deviations from the mean .",
    "finally , we have extensively studied how noise propagates through chemical reaction networks where one or more of the reactants are noisy , and their disturbances may be correlated .",
    "even when the disturbances have zero average , correlations change the output mean , and variance ."
  ],
  "abstract_text": [
    "<S> we describe how noise propagates through a network by calculating the variance of the outputs . using stochastic calculus and dynamical systems theory , </S>",
    "<S> we study the network topologies that accentuate or alleviate the effect of random variance in the network for both directed and undirected graphs . given a linear tree network , </S>",
    "<S> the variance in the output is a convex function of the poles of the individual nodes . </S>",
    "<S> cycles create correlations which in turn increase the variance in the output . </S>",
    "<S> feedforward and feedback have a limited effect on noise propagation when the respective cycles is sufficiently long . </S>",
    "<S> crosstalk between the elements of different pathways helps reduce the output noise , but makes the network slower . </S>",
    "<S> next , we study the differences between disturbances in the inputs and disturbances in the network parameters , and how they propagate to the outputs . finally , we show how noise correlations can affect the steady state of the system in chemical reaction networks with reactions of two or more reactants , each of which may be affected by independent or correlated noise sources . </S>"
  ]
}