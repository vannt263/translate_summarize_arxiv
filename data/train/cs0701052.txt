{
  "article_text": [
    "time series forecasting is a problem encountered in many fields of applications , as finance ( returns , stock markets ) , hydrology ( river floods ) , engineering ( electrical consumption ) , etc .",
    "many methods designed for time series forecasting perform well ( depending on the complexity of the problem ) on a rather short - term horizon but are rather poor on a longer - term one .",
    "this is due to the fact that these methods are usually designed to optimize the performance at short term , their use at longer term being not optimized .",
    "furthermore , they generally carry out the prediction of a single value while the real problem sometimes requires predicting a vector of future values in one step .",
    "for example , in the case of some a priori known periodicity , it could be interesting to predict all values for a period as a whole . but",
    "forecasting a vector requires either more complex models ( with potential loss of performance for some of the vector components ) or many distinct single value predicting models ( with potential loss of the correlation information between the various values ) .",
    "methods able to forecast a whole vector with the same precision for each of its components are thus of great interest .",
    "while enlarging the prediction horizon is of course of primary interest for practitioners , there is of course some limit to the accuracy that can be expected for a long - term forecast .",
    "the limitation is due to the availability of the information itself , and not to possible limitations of the forecasting methods .",
    "indeed , there is no doubt that , whatever forecasting method is used , predicting at long term ( i.e. many time steps in advance ) is more difficult that predicting at short term , because of the missing information in the unknown future time steps ( those between the last known value and the one to predict ) . at some term",
    ", all prediction methods will thus fail .",
    "the purpose of the method presented in this paper is not to enlarge the time horizon for which accurate predictions could be expected , but rather to enlarge the horizon for which we can have insights about the future evolution of the series . by insights ,",
    "we mean some information of interest to the practitioner , even if it does not mean accurate predictions .",
    "for example , are there bounds on the future values ?",
    "what can we expect in average ?",
    "are confidence intervals on future values large or narrow ?",
    "predicting many steps in advance could be realized in a straightforward way , by subsampling the known sequence , then using any short - term prediction method .",
    "however , in this case , the loss of information ( used for the forecast ) is obviously even higher , due to the lower resolution of the known sequence .",
    "furthermore , such solution does not allow in a general way to introduce a stochastic aspect to the method , which is a key issue in the proposed method . indeed , to get insights about the future evolution of a series through some statistics ( expected mean , variance , confidence intervals , quartiles , etc .",
    ") , several predictions should be made in order to extract such statistics .",
    "the predictions should differ ; a stochastic prediction method is able to generate several forecasts by repeated monte - carlo runs . in the method presented in this paper , the stochastic character of the method results from the use of random draws on a probability law .",
    "another attractive aspect of the method presented in this paper is that it can be used to predict scalar values or vectors , with the same expected precision for each component in the case of vector prediction .",
    "having at disposal a time series of values @xmath0 with @xmath1 , the prediction of a vector can be defined as follows : @xmath2 = f(x(t ) , \\ldots , x(t - p+1 ) ) + \\varepsilon_t\\ ] ] where @xmath3 is the size of the vector to be predicted , @xmath4 is the data generating process , @xmath5 is the number of past values that influence the future values and @xmath6 is a centred noise vector .",
    "the past values are gathered in a @xmath5-dimensional vector called _",
    "regressor_.    the knowledge of @xmath7 values of the time series ( with @xmath8 and @xmath9 ) means that relation ( [ prob_def ] ) is known for many ( @xmath10 ) time steps in the past .",
    "the modeling problem then becomes to estimate a function @xmath4 that models correctly the time series for the whole set of past regressors .",
    "the idea of the method is to segment the space of @xmath5-dimensional regressors .",
    "this segmentation can be seen as a way to make possible a local modeling in each segment .",
    "this part of the method is achieved using the self - organizing map ( som ) @xcite .",
    "the prototypes obtained for each class model locally the regressors of the corresponding class .",
    "furthermore , in order to take into account temporal dependences in the series , deformation regressors are built .",
    "those vectors are constructed as the differences between two consecutive regressors .",
    "the set of regressor deformations can also be segmented using the som . once those two spaces are segmented and their dependences characterized , simulations can be performed . using a kind of monte - carlo procedure to repeat the simulations , it is then possible to estimate the distribution of these simulations and to forecast global trends of the time series at long term .",
    "though we could have chosen some other classical vector quantization ( vq ) method as only the clustering property is of interest here , the choice of the som tool to perform the segmentation of the two spaces is justified by the fact that som are efficient and fast compared to other vq methods with a limited complexity @xcite and that they provide an intuitive and helpful graphical representation .    in the following of this paper ,",
    "we first recall some basic concepts about the som classification tool .",
    "then we introduce the proposed forecasting method , the double vector quantization , for scalar time series and then for vector ones .",
    "next we present some experimental results for both scalar and vector forecastings .",
    "a proof of the method stability is given in appendix .",
    "the self - organizing maps ( som ) , developed by teuvo kohonen in the 80 s @xcite , has now become a well - known tool , with established properties @xcite , @xcite .",
    "self - organizing maps have been commonly used since their first description in a wide variety of problems , as classification , feature extraction , pattern recognition and other related applications . as shown in a few previous works @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , the som may also be used to forecast time series at short term .    the kohonen self - organizing maps ( som ) can be defined as an unsupervised classification algorithm from the artificial neural network paradigm .",
    "any run of this algorithm results in a set , with a priori fixed size , of prototypes .",
    "each one of those prototypes is a vector of the same dimension as the input space .",
    "furthermore , physical neighbourhood relation links the prototypes . due to this neighbourhood relation",
    ", we can easily graphically represent the prototypes in a 1- or 2-dimensional grid .    after the learning stage each prototype represents a subset of the initial input set in which the inputs share some similar features . using voronoi s terminology",
    ", the prototype corresponds to a centroid of a region or zone , each zone being one of the classes obtained by the algorithm .",
    "the som thus realizes a vector quantization of the input space ( a voronoi tessellation ) that respects the original distribution of the inputs .",
    "furthermore , a second property of the som is that the resulting prototypes are ordered according to their location in the input space .",
    "similar vectors in the input space are associated either to the same prototype ( as in classical vq ) or to two prototypes that are neighbours on the grid .",
    "this last property , known as the topology preservation , does not hold for other standard vector quantization methods like competitive learning .",
    "the ordered prototypes of a som can easily be represented graphically , allowing a more intuitive interpretation : the 1- or 2-dimensional grid can be viewed as a 1- or 2-dimensional space where the inputs are projected by the som algorithm , even if , in fact , the inputs are rather projected on the prototypes themselves ( with some interpolation if needed in the continuous case ) .",
    "this projection operation for some specific input is proceeded by determining the nearest prototype with respect to some distance metric ( usually the euclidian distance ) .",
    "the method described here aims to forecast long - term trends for a time series evolution .",
    "it is based on the som algorithm and can be divided into two stages : the characterization and the forecasting .",
    "the characterization stage can be viewed as the learning , while the forecasting can be viewed as the use of a model in a generalization procedure .    for the sake of simplicity",
    ", the method is first presented for scalar time series prediction ( i.e. @xmath11 in ( [ prob_def ] ) ) and then detailed later on for vector forecasting .",
    "examples of the method application to scalar and vector time series will be provided in section [ expe ] .",
    "though the determination of an optimal regressor in time series forecasting ( at least in a nonlinear prediction case ) is an interesting and open question @xcite , it is considered here that the optimal , or at least an adequate , regressor of the time series is known .",
    "classically , the regressor can for example be chosen according to some statistical resampling ( cross - validation , bootstrap , etc . )",
    "procedure .    as for many other time",
    "series analysis methods , conversion of the inputs into regressors leads to @xmath12 vectors in a @xmath5-dimensional space , where @xmath5 is the regressor size and @xmath7 the number of values at our disposal in the time series .",
    "the resulting regressors are denoted : @xmath13 where @xmath14 , and @xmath0 is the original time series at our disposal with @xmath1 . in the above @xmath15 notation ,",
    "the subscript index denotes the first temporal value of the vector , while the superscript index denotes its last temporal value .",
    "the obtained vectors @xmath15 are then manipulated and the so - called deformations @xmath16 are created according to : @xmath17 note that , by definition , each @xmath16 is associated to one of the @xmath15 . in order to highlight this link , the same indices have been used .",
    "putting all @xmath16 together in chronological order forms another time series of vectors , the deformations series in the so - called deformation space to be opposed to the original space containing the regressors @xmath15 .",
    "of course , there exist @xmath18 deformations of dimension @xmath5 .",
    "the som algorithm can then be applied to each one of these two spaces , quantizing both the original regressors @xmath15 and the deformations @xmath16 respectively .",
    "note that in practice any kind of som map can be used , but it is assumed that one - dimensional maps ( or strings ) are more adequate in this context .    as a result of the vector quantization by the som on all @xmath15 of the original space , @xmath19",
    "@xmath5-dimensional prototypes @xmath20 are obtained ( @xmath21 ) .",
    "the clusters associated to @xmath20 are denoted @xmath22 .",
    "the second application of the som on all deformations @xmath16 in the deformation space results in @xmath23 @xmath5-dimensional prototypes @xmath24 , @xmath25 .",
    "similarly the associated clusters are denoted @xmath26 .    to perform the forecasting ,",
    "more information is needed than the two sets of prototypes .",
    "we therefore compute a matrix @xmath27 based on the relations between the @xmath15 and the @xmath16 with respect to their clusters ( @xmath22 and @xmath26 respectively ) .",
    "the row @xmath28 for a fixed @xmath29 and @xmath25 is the conditional probability that @xmath16 belongs to @xmath26 , given that @xmath15 belongs to @xmath22 . in practice , those probabilities are estimated by the empirical frequencies : @xmath30 with @xmath21 , @xmath25 .",
    "note that , for a fixed @xmath29 , elements @xmath28 ( @xmath25 ) sum to one ; this justifies the fact that each row of the matrix is an ( empirically estimated ) probability law .",
    "therefore the matrix will be called _ transition matrix _ in the following .",
    "the computation of this transition matrix completes the characterization part of the method .",
    "once the prototypes in the original and deformation spaces together with the transition matrix are known , we can forecast a time series evolution over a rather long - term horizon @xmath31 ( where horizon 1 is defined as the next value @xmath32 for time @xmath33 ) .",
    "the methodology for such forecasting can be described as follows .",
    "first , consider a time value @xmath0 for some time @xmath33 .",
    "the corresponding regressor is @xmath15 .",
    "therefore we can find the associated prototype in the original space , for example @xmath34 ( this operation is in fact equivalent to determining the class @xmath35 of @xmath15 in the som ) .",
    "we then look at row @xmath36 in the transition matrix and randomly choose a deformation prototype @xmath37 among the @xmath24 according to the conditional probability distribution defined by @xmath38 , @xmath25 .",
    "the prediction for time @xmath39 is obtained according to relation ( [ deforms ] ) : @xmath40 where @xmath41 is the estimate of the true @xmath42 given by our time series prediction model .",
    "however @xmath41 is in fact a @xmath5-dimensional vector , with components corresponding to times from @xmath43 to @xmath39 ( see relations ( [ regress ] ) and ( [ deforms ] ) ) . as in the scalar case considered here",
    "we are only interested in a single estimate at time @xmath39 , we extract the scalar prediction @xmath44 from the @xmath5-dimensional vector @xmath41 .",
    "we can iterate the described procedure , plugging in @xmath44 for @xmath0 in ( [ regress ] ) to compute @xmath45 by ( [ preds ] ) and extracting @xmath46 .",
    "we then do the same for @xmath47 , @xmath48 ,  , @xmath49 .",
    "this ends the run of the algorithm to obtain a single simulation of the series at horizon @xmath31 .",
    "next , as the goal of the method is not to perform a single long - term simulation , the simulations are repeated to extract trends . therefore a monte - carlo procedure is used to repeat many times the whole long - term simulation procedure at horizon @xmath31 , as detailed above . as part of the method ( random choice of the deformation according to the conditional probability distributions given by the rows of the transition matrix ) is stochastic , repeating the procedure leads to different simulations .",
    "observing those evolutions allows estimating the simulation distribution and infer global trends of the time series , as the evolution of its mean , its variance , confidence intervals , etc .",
    "it should be emphasized once again that the double quantization method is not designed to determine a precise estimate for time @xmath39 but is more specifically devoted to the problem of longterm evolution , which can only be obtained in terms of trends .",
    "suppose that it is expected to predict vectors @xmath50 of future values of the times series @xmath0 ; @xmath50 is a vector defined as : @xmath51 where @xmath3 is determined according to a priori knowledge about the series . for example",
    "when forecasting an electrical consumption , it could be advantageous to predict all hourly values for one day in a single step instead of predicting iteratively each value separately .",
    "as above regressors of this kind of time series can be constructed according to : @xmath52 where @xmath5 , for the sake of simplicity , is supposed to be a multiple of @xmath3 though this is not compulsory .",
    "the regressor @xmath15 is thus constructed as the concatenation of @xmath3-dimensional vectors from the past of the time series , as it is the concatenation of single past values in the scalar case .",
    "as the @xmath15 regressor is composed of @xmath53 vectors of dimension @xmath3 , @xmath15 is a @xmath5-dimensional vector .",
    "deformation can be formed here according to : @xmath54    here again , the som algorithm can be applied on both spaces , classifying both the regressors @xmath15 and the deformations @xmath16 respectively .",
    "we then have @xmath19 prototypes @xmath20 in the original space , with @xmath21 , associated to classes @xmath22 . in the deformation space , we have @xmath23 prototypes @xmath24 , @xmath25 , associated to classes @xmath26",
    ".    a transition matrix can be constructed as a vector generalisation of relation ( [ freqs ] ) : @xmath55 with @xmath21 , @xmath25 .",
    "the simulation forecasting procedure can also be generalised :    * consider the vector input @xmath56 for time @xmath33 .",
    "the corresponding regressor is @xmath15 ; * find the corresponding prototype @xmath34 ; * choose a deformation prototype @xmath37 among the @xmath24 according to the conditional distribution given by elements @xmath38 of row @xmath36 ; * forecast @xmath57 as @xmath58 * extract the vector @xmath59 from the @xmath3 first columns of @xmath57 ; * repeat until horizon @xmath31 .    for this vector case too , a monte - carlo procedure is used to repeat many times the whole longterm simulation procedure at horizon @xmath31 . then the simulation distribution and its statistics can be observed .",
    "this information gives trends for the long term of the time series .",
    "note that using the som to quantize the vectors @xmath15 and @xmath16 , the method reaches the goal of forecasting vectors with the same precision for each of their components .",
    "indeed each component from regressors @xmath15 and @xmath16 has the same relative weight while the distance between the considered regressor and prototype is computed in the som algorithm",
    ". none of the @xmath15 or @xmath16 components have thus a greater importance in the modification of the prototype weight during the learning of the som .",
    "two important comments must be done .",
    "first , as illustrated in both examples below , it is not mandatory ( in equations ( [ prob_def ] ) , ( [ regress ] ) , ( [ vect_prob ] ) , ( [ vect_regress ] ) ) to consider all successive values in the regressor ; according to the knowledge of the series or to some validation procedure , it might be interesting to select regressors with adequate , but not necessarily successive , scalar values or vectors in the past .",
    "secondly , the vector case has been illustrated in the previous section on temporal vectors ( see equation ( [ vect_prob ] ) ) .",
    "an immediate extension of the method would be to consider spatial vectors , for example when several series must be predicted simultaneously .",
    "the equations in the previous section should be modified , but the principle of the method remains valid .",
    "the predictions obtained by the model described in the previous subsections should ideally be confined in the initial space defined by the learning data set . in that case",
    ", the series of predicted values @xmath16 is said to be stable .",
    "otherwise , if the series tends to infinity or otherwise diverges , it is said to be unstable .",
    "the method has been proven to be stable according to this definition ; a proof is given in appendix .",
    "this section is devoted to the application of the method on two times series .",
    "the first one is the well - known santa fe a benchmark presented in @xcite ; it is a scalar time series .",
    "the second time series is the polish electrical consumption from 1989 to 1996 @xcite .",
    "this real - world problem requires the prediction of a vector of 24 hourly values .      in the method description ,",
    "the numbers @xmath19 and @xmath23 of prototypes have not been fixed .",
    "indeed , the problem is that different values of @xmath19 ( @xmath23 ) result in different segmentations in the original ( deformation ) space and in different conditional distribution in the transition matrix",
    ". the model may thus slightly vary .    selecting the best values for @xmath19 and @xmath23",
    "is an important question too .",
    "traditionally , such hyperparameters are estimated by model selection procedures such as aic , bic or computationally - costly resampling techniques ( leave - one - out , k - fold cross validation , bootstrap ) . as it will be shown further in this paper , exact values of @xmath19 and @xmath23 are not necessary , as the sensitivity of the method around the optimums is low .",
    "a simple validation is then used to choose adequate values for @xmath19 and @xmath23 . for that purpose",
    "the available data are divided into three subsets : the learning , the validation and the test set .",
    "the learning set is used to fix the values of the model parameters , such as the weights of the prototypes in the som and the transition matrix .",
    "the validation set is used to fix meta - parameters , such as the numbers @xmath19 and @xmath23 of prototypes in the som maps .",
    "the validation set is thus used for model selection .",
    "the test set aims to see how the model behaves on unused data that mimic real conditions .",
    "the selection of @xmath19 and @xmath23 is done with regards to an error criterion , in our case a sum of squared error criterion , computed over the validation set @xmath60 : @xmath61    once @xmath19 and @xmath23 have been chosen , a new learning is done with a new learning set obtained from the reassembled learning and validation sets .",
    "this new learning is only performed once with optimal values for @xmath19 and @xmath23 .",
    "note that , hopefully , the sensitivity of the method to specific values of @xmath19 and @xmath23 is not high .",
    "this has been experimentally verified in all our simulations , and will be illustrated on the first example ( santa fe a ) in section [ sfa ] .",
    "another crucial question is the sensitivity of the method to various runs of the som algorithm ( with the same @xmath19 and @xmath23 values ) .",
    "indeed it is well known that initial conditions largely influence the exact final result of the som algorithm ( by final result it is meant the prototype locations , and their neighborhood relations ) @xcite . nevertheless ,",
    "as mentioned above , the neighborhood relations of the som are used for visualization purposes only ; they do not influence the results of the forecast .",
    "moreover , the location of the centroids are used to quantize the space ( therefore allowing the estimation of the empirical conditional frequencies of the clusters ) ; small variations in the centroid location have thus a low influence on each prediction generated by the method , and an even lower one on the statistics ( mean , confidence intervals , etc . )",
    "estimated from the predictions .",
    "this last result has been confirmed experimentally in all our simulations , for which no significant difference was observed after different runs of the two som algorithms .      the santa fe a time series @xcite has been obtained from a far - infrared - laser in a chaotic state .",
    "this time series has become a well - known benchmark in time series prediction since the santa fe competition in 1991 .",
    "the completed data set contains 10 000 data .",
    "this set has been divided here as follows : the learning set contains 6000 data , the validation set 2000 data , and test set 100 data .",
    "note that the best neural network models described in @xcite do not predict much more than 40 data , making a 100-data test set a _ very _ long - term forecasting .    here",
    ", the regressors @xmath15 have been constructed according to @xmath62 this choice is made according to previous experience on this series @xcite . in other words , @xmath11 , @xmath63 ( as value @xmath64 is omitted ) and @xmath65 .    in this simulation ,",
    "kohonen strings of 1 up to 200 prototypes in each space have been used .",
    "all the 40 000 possible models have been tested on the validation set .",
    "the best model among them has 179 prototypes in the regressor space and 161 prototypes in the deformation space . after relearning this model on both the learning and validation sets , 1000 simulations were performed on a horizon of 100 .",
    "then the mean and confidence interval at 95% level were computed , giving information on the time series trends .",
    "figure [ pred_sfa_lt ] shows the mean of the 1000 simulations compared to the true values contained in the test set , together with the confidence interval at 95% level .",
    "figure [ pred_sfa_lt_zoom ] shows a zoom on the first 30 values . in figure",
    "[ 100_simu_sfa ] , we can see 100 simulations for the same 30 values .",
    "note the stability obtained through the replications . for a simpler model with @xmath66 and @xmath67 ( used for illustrations purposes ) ,",
    "figure [ code_vector_sfa ] shows the code vectors and regressors ( resp .",
    "deformations ) in each class ; table [ trans_matrix_sfa ] shows the corresponding transition matrix .    from figure [ pred_sfa_lt_zoom ] , it should be noted that the method gives roughly the first 25 values of the time series , a result that is not so far from those obtained with the best neural network models of the santa fe competition @xcite .    from figure [ pred_sfa_lt ] , we can infer that the series mean will neither increase nor decrease .",
    "in addition , the confidence interval does contain the whole evolution of the time series for the considered 100 future values .",
    "the trend for long term forecasting is thus that the series , though chaotic , will show some kind of stability in its evolution for the next 100 values .",
    "as all the 40 000 models have been generated and learned , the influence of varying the @xmath19 and @xmath23 values can be observed .",
    "this influence is illustrated in figure [ sse_sfa ] .",
    "it is clear from this figure that there is a large flat region around the optimal values ; in this region , all models generalize rather equivalently .",
    "this justifies , a posteriori , the choice of a simple resampling method to choose @xmath19 and @xmath23 .",
    ".example of transition matrix , here with @xmath66 and @xmath67 as in figure [ code_vector_sfa ] .",
    "note that in each row , the frequency values sum to one .",
    "[ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]      as second example , we use the polish electrical load time series @xcite .",
    "this series contains hourly values from 1989 to 1996 .",
    "the whole dataset contains about 72 000 hourly data and is plotted in figure [ series_polish ] . due to the daily periodicity of the time series",
    ", we are interested in daily predictions .",
    "this is thus an illustration of the case @xmath68 , since it seems natural to forecast the 24 next values in one step ( the next day ) , the time window becoming daily instead of hourly .",
    "having now at our disposal 3000 @xmath15 data of dimension 24 , we use 2000 of them for the learning , 800 for a simple validation and 200 for the test .",
    "since the optimal regressor is unknown , many different regressors were tried , using intuitive understanding of the process .",
    "the final regressor is : @xmath69 that is the 24 hourly values of today , of yesterday , of two , six and seven days ago .",
    "this regressor is maybe not the optimal one , but it is the one that makes the lowest error on the validation set in comparison with other tested ones . since the regressor contains @xmath70 data of dimension @xmath71 , we work in a 120-dimensional space .",
    "we then run the algorithm again on the learning set with values for @xmath19 and @xmath23 each varying from 5 to 200 prototypes by steps of 5 .",
    "the lowest error is made by a model with @xmath72 and @xmath73 respectively .",
    "another model is then learned with 160 and 140 parameter vectors in each space with the new learning set , now containing 2000 + 800 data .",
    "the forecasting obtained from this model is repeated 1000 times .",
    "figure [ pred_polish_lt ] presents the mean of the 1000 simulations obtained with 24-dimensional vectors and with horizon @xmath31 limited to 40 days ( a single plot of the whole 24 * 200 predicted values becomes unreadable ) . for convenience ,",
    "figure [ pred_polish_zoom ] shows a zoom and a comparison between the mean of those 1000 long - term predictions and the real values . a confidence interval at 95% level is also provided .",
    "from figure [ pred_polish_zoom ] , it is clear that the mean of the prediction at long term will show the same periodicity as the true time series and that the values will be contained in a rather narrow confidence interval .",
    "this fact denotes a probable low variation of the series at long term .",
    "figure [ 100_simu_polish ] shows 100 predictions obtained by the monte - carlo procedure picked up at random before taking the mean .",
    "see that different simulations have about the same shape ; this is a main argument for determining long - term trends .    finally , as in the previous example , the influence of @xmath19 and @xmath23 can be observed . in figure",
    "[ sse_polish ] , a very large flat region is also present around the best model .",
    "sub - optimal selection of the @xmath19 and @xmath23 values will thus not penalize too heavily the model generalization abilities .",
    "in this paper , we have presented a time series forecasting method based on a double classification of the regressors and of their deformations using the som algorithm .",
    "the use of soms makes it possible to apply the method both on scalar and vector time series , as discussed in section [ descr ] and illustrated in section [ expe ] . a proof of the method stability is given in appendix .",
    "the proposed method is not designed to obtain an accurate forecast of the next values of a series , but rather aims to determine long - term trends .",
    "indeed , its stochastic nature allows repeating simulations by a monte - carlo procedure , allowing to compute statistics ( variance , confidence intervals , etc . ) on the predictions .",
    "such a method could also be used for example in the financial context , for the estimation of volatilities .",
    "we would like to thank professor osowsky from warsaw technical university for providing us the polish electrical consumption data used in our example .",
    "14 t. kohonen , self - organising maps , springer series in information sciences , vol .",
    "30 , springer , berlin , 1995 .",
    "e. de bodt , m. cottrell , p. letremy , m. verleysen , on the use of self - organizing maps to accelerate vector quantization , neurocomputing , elsevier , vol .",
    "56 ( january 2004 ) , pp .",
    "187 - 203 .",
    "m. cottrell , j .- c .",
    "fort , g. pags , theoretical aspects of the som algorithm , neurocomputing , 21 , p119 - 138 , 1998 . m. cottrell , e. de bodt , m. verleysen , kohonen maps versus vector quantization for data analysis , european symp .",
    "on artificial neural networks , april 1997 , bruges ( belgium ) , d - facto pub .",
    "( brussels ) , pp . 187 - 193 .",
    "m. cottrell , e. de bodt , ph .",
    "grgoire , simulating interest rate structure evolution on a long term horizon : a kohonen map application , proceedings of neural networks in the capital markets , californian institute of technology , world scientific ed . ,",
    "pasadena , 1996 .",
    "m. cottrell , b. girard , p. rousset , forecasting of curves using a kohonen classification , journal of forecasting , vol .",
    "429 - 439 , 1998 .",
    "j. walter , h. ritter , k. schulten , non - linear prediction with self - organising maps , proc . of ijcnn ,",
    "san diego , ca , 589 - 594 , july 1990 . j. vesanto , using the som and local models in time - series prediction , in proceedings of workshop on self - organizing maps ( wsom97 ) , espoo , finland , pp . 209 - 214 , 1997 .",
    "t. koskela , m. varsta , j. heikkonen , and k. kaski , recurrent som with local linear models in time series prediction , european symp . on artificial neural networks ,",
    "april 11 1998 , bruges ( belgium ) , d - facto pub .",
    "( brussels ) , pp . 167 - 172 .",
    "a. lendasse , m. verleysen , e. de bodt , m. cottrell , ph .",
    "grgoire , forecasting time - series by kohonen classification , european symp . on artificial neural networks ,",
    "april 1998 , bruges ( belgium ) , d - facto pub .",
    "( brussels ) , pp . 221 - 226 .",
    "m. verleysen , e. de bodt , a. lendasse , forecasting financial time series through intrinsic dimension estimation and non - linear data projection , in proc .",
    "of international workconference on artificial and natural neural networks ( iwann99 ) , springer - verlag lecture notes in computer science , n 1607 , pp .",
    "ii596-ii605 , june 1999 .",
    "a. s. weigend , n.a .",
    "gershenfeld , times series prediction : forecasting the future and understanding the past , addison - wesley publishing company , 1994 .",
    "e. de bodt , m. cottrell , m. verleysen , statistical tools to assess the reliability of selforganizing maps , neural networks , elsevier , vol . 15 , nos .",
    "8 - 9 ( october - november 2002 ) , pp .",
    "967 - 978 .",
    "g. fayolle , v. a.malyshev , m. v. menshikov , topics in constructive theory of countable markov chains , cambridge university press , 1995 .",
    "intuitively , the stability property of the method is not surprising .",
    "indeed , the model is designed such that it will mostly produce predictions that are in the range of the observed data . by construction",
    ", deformations are chosen randomly according to an empirical probability law and the obtained predictions should stay in the same range .",
    "if , for some reason , the prediction is about to exceed this range during one of the simulations , the next deformations will then tend to drive it back inside this range , at least with high probability .",
    "furthermore , as simulations are repeated with the monte - carlo procedure , the influence of such unexpected cases will be reduced when the mean is taken to obtain the final predictions .",
    "the following of this section is intended to prove this intuitive result .",
    "the proof consists in two steps : it is first shown that the series generated by the model is a markov chain ; secondly , it is demonstrated that this particular type of markov chain is stable . in order to improve the readability of the proof",
    ", lighter notations will be used . for a fixed @xmath3 and a fixed @xmath5 ,",
    "notation @xmath74 will represent the vector @xmath15 .",
    "the last known regressor will be denoted @xmath75 .",
    "the prototype of a cluster @xmath76 of deformations will be noted @xmath77 .",
    "finally , hats will be omitted for simplicity as all regressors @xmath74 are estimations , except for @xmath78 .    to prove that the series is a markov chain",
    ", we consider the starting vector of the simulation at time 0 .",
    "the corresponding initial regressor of the series is denoted @xmath75 , and @xmath79 is the corresponding som cluster in the regressor space .",
    "the deformation that is applied to @xmath75 at this stage is @xmath80 .",
    "then the next values of the series are given by @xmath81 , @xmath82 ,  , with @xmath80 , @xmath83 ,  drawn randomly from the transition matrix for clusters @xmath79 , @xmath84 ,  respectively .",
    "the series @xmath74 is therefore a markov chain , homogeneous in time ( the transition distribution are not time dependant ) , irreducible and defined over a numerable set ( the initial @xmath74 are in finite number , and so are the deformations ) .    to show the stability of this markov chain and thus the existence of a stationary distribution , foster s criterion @xcite is applied .",
    "note that this criterion is a stronger result which proves the ergodicity of the chain , which in turns implies the stability .",
    "foster s criterion is the following :          before going in further details , let us remark that for a som with at least 3 classes in general position , class @xmath79 covers less than a half plane .",
    "furthermore , we have to distinguish two cases for each cluster .",
    "first , the cluster may be included in a finite compact from @xmath91 .",
    "the second case is the case of an infinite cluster i.e. of a cluster which may does have any neighbour in some direction ; this happens to ckusters on the border of the map .",
    "the first case is easely proved . since @xmath92 , where @xmath93 can be any constant",
    ", then we have by triangular inequality : @xmath94 as the deformations @xmath77 are in finite number , the maximum of their norm is finite .",
    "this proves the first inequality of ( [ foster ] ) in an obvious way for the first case ( i.e. bounded cluster case ) .      looking at figure [ prop_1 ] , we see that each unbounded cluster is included in a cone with vertex @xmath87 and delimited by the normalized vectors @xmath96 and @xmath97",
    ". there are two possibilities : either @xmath96 and @xmath97 form an acute angle , either an obtuse one , as shown in figure [ prop_1a ] and figure [ prop_1b ] respectively .        denoting @xmath98 we have @xmath99 and @xmath100 both positive in the acute angle case , while either @xmath99 or @xmath100 is positive for an obtuse angle .",
    "indeed , using the origin @xmath101 , we define : @xmath102        we define @xmath107 such that the angle @xmath108 is @xmath109 .",
    "similarly @xmath110 is defined such that the angle @xmath111 is also @xmath109 .",
    "then , for both the acute and obtuse angle cases , we have : @xmath112 where @xmath113 is the considered cone which has border vectors @xmath96 and @xmath97 .",
    "assume that : @xmath116 where @xmath117 is the empirical distribution corresponding to class @xmath79 in the transition matrix .",
    "denoting @xmath118 with @xmath119 , then we have : @xmath120 for either @xmath121 or @xmath122 in case of an acute angle ( figure [ prop_3a ] ) or for both of @xmath121 and @xmath122 for the obtuse case ( figure [ prop_3b ] ) .",
    "the second term between the brackets can be bounded by a strictly positive constant @xmath125 . indeed , as @xmath126 is finite , @xmath127 is also finite .",
    "therefore , for @xmath128 and @xmath129 , we have @xmath130                equation ( [ dvlp_foster ] ) can now be simplified in : @xmath143 \\\\ \\hspace{0.5 cm } < 2 \\|x\\| \\left[-\\alpha_0 + \\frac{1}{2 } \\alpha_0 \\right ] \\\\",
    "\\hspace{0.5 cm } = -2 \\|x\\| \\frac{\\alpha_0}{2 } , \\end{array}\\ ] ] where @xmath144 and @xmath125 in ( [ dvlp_2nd_term ] ) is chosen such that @xmath145 .",
    "this development has been done for cluster @xmath79 .",
    "all values @xmath125 , @xmath146 , @xmath147 , @xmath148 depends on this cluster @xmath79 .",
    "now considering all unbounded clusters @xmath149 and taking @xmath150 and @xmath151 , we have : @xmath152      to conclude , we define the set @xmath155 used in foster s criterion according to @xmath156 where @xmath157 denotes the set of bounded cluster indexes as discussed in the introduction to the proof . with this definition ,",
    "the above developments prove foster s criterion ( [ foster ] ) .",
    "thus the markov chain defined by the @xmath158 for @xmath159 is ergodic , and admits a unique stationary distribution ."
  ],
  "abstract_text": [
    "<S> kohonen self - organisation maps are a well know classification tool , commonly used in a wide variety of problems , but with limited applications in time series forecasting context . in this paper </S>",
    "<S> , we propose a forecasting method specifically designed for multi - dimensional long - term trends prediction , with a double application of the kohonen algorithm . </S>",
    "<S> practical applications of the method are also presented . </S>"
  ]
}