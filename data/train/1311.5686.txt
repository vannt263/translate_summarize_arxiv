{
  "article_text": [
    "in the domain of large - scale computational analysis of risk , large amounts of data need to be rapidly processed and millions of simulations need to be quickly performed ( for example , @xcite ) .",
    "this can be achieved only if data is efficiently managed and parallelism is exploited within algorithms employed in the simulations .",
    "the domain , therefore , inherently opens avenues to exploit the synergy that can be achieved by bringing together state - of - the - art techniques in data processing and management and high - performance computing .",
    "research on aggregate analysis of risks @xcite using high - performance computing is sparse at best .",
    "the research reported in this paper is motivated towards exploring techniques for employing high - performance computing not only to speed up the simulation but also to process and manage data efficiently for the aggregate analysis of risk . in this context",
    "the mapreduce model @xcite is used for achieving high - performance aggregate analysis of risks .",
    "the aggregate analysis of risk is a monte carlo simulation performed on a portfolio of risks that an insurer or reinsurer holds .",
    "a portfolio can cover risks related to catastrophic events such as earthquakes , floods or hurricanes , and may comprise tens of thousands of contracts .",
    "the contracts generally follow an ` excess of loss ' ( xl ) @xcite structure providing coverage for single event occurrences or multiple event occurrences , or a combination of both single and multiple event occurrences .",
    "each trial in the aggregate analysis simulation represents a view of the occurrence of catastrophic events and the order in which they occur within a contractual year .",
    "the trial also provides information on how the occurrence of an event in a contractual year will interact with complex treaty terms to produce an aggregated loss . a pre - simulated year event table ( yet ) containing between several thousands and millions of alternative views of a single contractual year is input for the aggregate analysis .",
    "the output of aggregate analysis is a year loss table ( ylt ) . from a ylt , an insurer or",
    "a reinsurer can derive important portfolio risk metrics such as the probable maximum loss ( pml ) @xcite and the tail value - at - risk ( tvar ) @xcite which are used for both internal risk management and reporting to regulators and rating agencies .    in this paper , the analysis of portfolios of catastrophic risk is proposed and implemented using a mapreduce model on the hadoop @xcite platform .",
    "the algorithm rapidly consumes large amounts of data in the form of the yet and event loss tables ( elt ) .",
    "therefore , the challenges of organising input data and processing it efficiently , and applying parallelism within the algorithm are considered .",
    "the mapreduce model lends itself well towards solving embarrassingly parallel problems such as the aggregate analysis of risk , and is hence chosen to implement the algorithm .",
    "the algorithm employs two mapreduce rounds to perform both the numerical computations as well as to manage and process data efficiently .",
    "the algorithm is implemented on the apache hadoop platform .",
    "the hadoop distributed file system ( hdfs ) and the distributed cache ( dc ) are key components offered by the hadoop platform in addressing the data challenges .",
    "the preliminary results obtained from the experiments of the analysis indicate that the mapreduce model can be used to scale the analysis over multiple nodes of a cluster ; parallelism can be exploited in the analysis for achieving faster numerical computations and data management .",
    "the remainder of this paper is organised as follows .",
    "section [ ara ] considers the sequential and mapreduce algorithm for the analysis of aggregate risk .",
    "section [ hadoop ] presents the implementation of the mapreduce algorithm on the apache hadoop platform and the preliminary results obtained from the experimental studies .",
    "section [ conclusion ] concludes this paper by considering future work .",
    "the sequential and mapreduce algorithm of the analysis of aggregate risk is presented in this section .",
    "there are three inputs to the algorithm for the analysis of aggregate risk , namely the @xmath0 , @xmath1 , and a pool of @xmath2 .",
    "the @xmath0 is the year event table which is the representation of a pre - simulated occurrence of events @xmath3 in the form of trials @xmath4 .",
    "each trial captures the sequence of the occurrences of events for a year using time - stamps in the form of event time - stamp pairs .",
    "the @xmath1 is a portfolio that represents a group of programs , @xmath5 , which in turn represents a set of layers , @xmath6 that covers a set of @xmath2 using financial terms .",
    "the @xmath7 is the event loss table which represents the losses that correspond to an event based on an exposure ( one event can appear over different elts with different losses ) .",
    "the intermediary output of the algorithm are the layer loss table @xmath8 consisting trial - loss pairs .",
    "the final output of the algorithm is @xmath9 , which is the year loss table that contains the losses covered by a portfolio .",
    "algorithm [ algorithm1 ] shows the sequential analysis of aggregate risk .",
    "the algorithm scans through the hierarchy of the portfolio , @xmath1 ; firstly through the programs , @xmath5 , followed by the layers , @xmath6 , then the event loss tables , @xmath2 .",
    "5 - 8 shows how the loss associated with an event in the @xmath7 is computed . for this , the loss , @xmath10 associated with an event , @xmath3 is retrieved , after which contractual financial terms to the benefit of the layer are applied to the losses and are summed up as @xmath11",
    ".    aggregate losses of trial - loss pairs in @xmath12 populate @xmath9    in line nos .",
    "9 and 10 , two occurrence financial terms , namely the occurrence retention and the occurrence limit are applied to the loss , @xmath11 and summed up as @xmath13 .",
    "the @xmath13 losses correspond to the total loss in one trial .",
    "occurrence retention refers to the retention or deductible of the insured for an individual occurrence loss , where as occurrence limit refers to the limit or coverage the insurer will pay for occurrence losses in excess of the retention .",
    "the occurrence financial terms capture specific contractual properties of excess of loss treaties as they apply to individual event occurrences only .    in line nos .",
    "12 and 13 , two aggregate financial terms , namely the aggregate retention and the aggregate limit are applied to the loss , @xmath13 to produce aggregated loss for a trial .",
    "aggregate retention refers to the retention or deductible of the insured for an annual cumulative loss , where as aggregate limit refers to the limit or coverage the insurer will pay for annual cumulative losses in excess of the aggregate retention .",
    "the aggregate financial terms captures contractual properties as they apply to multiple event occurrences .",
    "the trial - loss pairs are then used to populate layer loss tables @xmath14 ; each layer is represented using a layer loss table consisting of trial - loss pairs .    in line nos . 16 and 17",
    ", the trial losses are aggregated from the layer level to the program level .",
    "the losses are represented again as a trial - loss pair and are used to populate program loss tables @xmath15 ; each program is represented using a program loss table .    in line nos .",
    "19 and 20 , the trial losses are aggregated from the program level to the portfolio level . the trial - loss pairs are populated in the year loss table @xmath9 which represents the output of the analysis of aggregate risk .",
    "financial functions or filters are then applied on the aggregate loss values .",
    "@xmath16(@xmath14 )    mapreduce is a programming model developed by google for processing large amount of data on large clusters .",
    "a map and a reduce function are adopted in this model to execute a problem that can be decomposed into sub - problems with no dependencies ; therefore the model is most attractive for embarrassingly parallel problems .",
    "this model is scalable across large number of computing resources .",
    "in addition to the computations , the fault tolerance of the execution , for example , handling machine failures are taken care by the mapreduce model . an open - source software framework that supports the mapreduce model , apache hadoop @xcite ,",
    "is used in the research reported in this paper .",
    "the mapreduce model lends itself well towards solving embarrassingly parallel problems , and therefore , the analysis of aggregate risk is explored on mapreduce . in the analysis of aggregate risks ,",
    "the programs contained in the portfolio are independent of each other , the layers contained in a program are independent of each other and further the trials in the year event table are independent of each other .",
    "this indicates that the problem of analysing aggregate risks requires a large number of computations which can be performed as independent parallel problems .",
    "another reason of choice for the mapreduce model is that it can handle large data processing for the analysis of aggregate risks .",
    "for example , consider a year event table comprising one million simulations , which is approximately 30 gb . so for a portfolio comprising 2 programs , each with 10 layers , the approximate volume of data that needs to be processed is 600 gb .    further mapreduce implementations such as hadoop provide dynamic job scheduling based on the availability of cluster resources and distributed file system fault tolerance .",
    "algorithm [ algorithm2 ] shows the mapreduce analysis of aggregate risk .",
    "the aim of this algorithm is similar to the sequential algorithm in which the algorithm scans through the portfolio , @xmath1 ; firstly through the programs , @xmath5 , and then through the layers , @xmath6 .",
    "the first round of mapreduce jobs , denoted as @xmath17 are launched for all the layers . the map function ( refer algorithm [ algorithm3 ] ) scans through all the event loss tables @xmath2 covered by the layers @xmath6 to compute the losses @xmath11 in parallel for every event in the elt .",
    "the computations of loss @xmath13 at the layer level are performed in parallel by the reduce function ( refer algorithm [ algorithm4 ] ) .",
    "the output of @xmath17 is a layer loss table @xmath8 .",
    "the second round of mapreduce jobs , denoted as + @xmath18 are launched for aggregating all the @xmath14 in each program to a @xmath9 .",
    "emit(@xmath4 , @xmath11 )    apply aggregate financial terms to @xmath13 emit(@xmath4 , @xmath13 )    the master node of the cluster solving a problem partitions the input data to intermediate files effectively splitting the problem into sub - problems .",
    "the sub - problems are distributed to the worker nodes by the master node , often referred to as the ` map ' step performed by the mapper . the map function executed by the mapper receives as input a @xmath19 pair to generate a set of @xmath20 @xmath21 @xmath22 pairs .",
    "the results of the decomposed sub - problems are then combined by the reducer referred to as the ` reduce ' step .",
    "the reduce function executed by each reducer merges the @xmath20 @xmath21 @xmath22 pairs to generate a final output .",
    "the reduce function receives all the values corresponding to the same intermediate key .",
    "algorithm [ algorithm3 ] and algorithm [ algorithm4 ] show how parallelism is achieved by using the map and reduce functions in a first round at the layer level .",
    "algorithm [ algorithm3 ] shows the map function whose inputs are a set of @xmath23 from the @xmath0 , and the output is a trial - loss pair @xmath24 which corresponds to an event . to estimate the loss , it is necessary to scan through every event loss table @xmath7 covered by a layer @xmath6 ( line nos . 1 - 5 ) .",
    "the loss , @xmath10 associated with an event , @xmath3 in the @xmath7 is fetched from memory in line no .",
    "contractual financial terms to the benefit of the layer are applied to the losses ( line no .",
    "3 ) to aggregate the losses as @xmath11 ( line no .",
    "the loss for every event in a trial is emitted as @xmath24 .",
    "algorithm [ algorithm4 ] shows the reduce function in the first mapreduce round .",
    "the inputs are the trial @xmath4 and the set of losses ( @xmath11 ) corresponding to that trial , represented as @xmath25 , and the output is a trial - loss pair @xmath26 . for every loss value @xmath11 in the set of losses @xmath25 , the occurence financial terms , namely occurrence retention and the occurrence limit ,",
    "are applied to @xmath11 ( line no .",
    "2 ) and summed up as @xmath13 ( line no .",
    "the aggregate financial terms , namely aggregate retention and aggregate limit are applied to @xmath13 ( line no .",
    "the aggregated loss for a trial , @xmath13 is emitted as @xmath27 to populate the layer loss table .",
    "algorithm [ algorithm5 ] and algorithm [ algorithm6 ] show how parallelism is achieved by using the map and reduce functions in a second round for aggregating all layer loss tables to produce the @xmath9 .",
    "algorithm [ algorithm5 ] shows the map function whose inputs are a set of layer loss tables @xmath14 , and the output is a trial - loss pair @xmath26 which corresponds to the layer - wise loss for trial @xmath4 .",
    "emit(@xmath28 )    algorithm [ algorithm6 ] shows the reduce function whose inputs are a set of losses corresponding to a trial in all layers @xmath29 , and the output is a trial - loss pair @xmath30 which is an entry to populate the final output , the year loss table @xmath9 .",
    "the function sums up trial losses @xmath13 across all layers to produce a portfolio - wise aggregate loss @xmath31 .",
    "the experimental platform for implementing the mapreduce algorithm is a heterogeneous cluster comprising ( a ) a master node which is an ibm blade of two xeon 2.67 ghz processors comprising six cores , memory of 20 gb per processor and a hard drive of 500 gb with an additional 7 tb raid array , and ( b ) six worker nodes each with an opteron dual core 2216 2.4 ghz processor comprising four cores , memory of 4 gb ram and a hard drive of 150 gb .",
    "the nodes are interconnected via infiniband .",
    "apache hadoop , an open - source software framework is used for implementing the mapreduce analysis of aggregate risk .",
    "other available frameworks @xcite require the use of additional interfaces , commercial or web - based , for deploying an application and were therefore not chosen .",
    "the hadoop framework works in the following way for a mapreduce round .",
    "first of all the data files from the hadoop distributed file system ( hdfs ) is loaded using the ` inputformat ` interface .",
    "hdfs provides a functionality called distributed cache for distributing small data files which are shared by the nodes of the cluster .",
    "the distributed cache provides local access to shared data .",
    "the ` inputformat ` interface specifies the input the mapper and splits the input data as required by the mapper .",
    "the ` mapper ` interface receives the partitioned data and emits intermediate key - value pairs .",
    "the ` partitioner ` interface receives the intermediate key - value pairs and controls the partitioning of these keys for the ` reducer ` interface .",
    "then the ` reducer ` interface receives the partitioned intermediate key - value pairs and generates the final output of this mapreduce round .",
    "the output is received by the ` outputformat ` interface and provides it back to hdfs .    the input data for mapreduce ara which are the year event table @xmath0 , the pool of event loss table @xmath7 and the portfolio @xmath1 specification are stored on hdfs .",
    "the master node executes algorithm [ algorithm2 ] to generate the year loss table @xmath9 which is again stored on the hdfs .",
    "the two mapreduce rounds are illustrated in figure [ figure1 ] .",
    "+    in the first mapreduce round the ` inputformat ` interface splits the @xmath0 based on the number of mappers specified for the mapreduce round .",
    "the mappers are configured such that they also receive the @xmath2 covered by one layer which are contained in the distributed cache .",
    "the mapper applies financial terms to the losses . in this implementation combining the @xmath2 is considered for achieving fast lookup .",
    "a typical @xmath7 would contain entries in the form of an event i d and related loss information .",
    "when the @xmath2 are combined they contain an event i d and the loss information related to all the individual @xmath2 .",
    "this reduces the number of lookups for retrieving loss information related to an event when the events in a trial contained in the @xmath0 are scanned through by the mapper .",
    "the mapper emits a trial - event loss pair which is collected by the partitioner .",
    "the partitioner delivers the trial - event loss pairs to the reducers ; one reducer receives all the trial - event loss pairs related to a specific trial .",
    "the reducer applies the occurrence financial and aggregate financial terms to the losses emitted to it by the mapper .",
    "then the ` outputformat ` writes the output of the first mapreduce round as layer loss tables @xmath8 to the hdfs .    in the second mapreduce",
    "round the ` inputformat ` receives all the @xmath14 from hdfs .",
    "the ` inputformat ` interface splits the set of @xmath14 and distributes them to the mappers .",
    "the ` mapper ` interface emits layer - wise trial - loss pairs .",
    "the ` partitioner ` receives all the trial - loss pairs and partitions them based on the trial for each reducer .",
    "the ` reducer ` interface uses the partitioned trial - loss pairs and combines them to portfolio - wise trial - loss pairs . then the ` outputformat ` writes the output of the second mapreduce round as a year loss table @xmath9 to the hdfs .",
    "experiments were performed for one portfolio comprising one program and one layer and sixteen event loss tables .",
    "the year event table has 100,000 trials , with each trial comprising 1000 events .",
    "the experiments are performed for up to 12 workers as there are 12 cores available on the cluster employed for the experiments .",
    "figure [ figure2 ] shows two bar graphs for the total time taken in seconds for the mapreduce rounds when the workers are varied between 1 and 12 ; figure [ fig : s21 ] for the first mapreduce round and figure [ fig : s22 ] for the second mapreduce round . in the first mapreduce",
    "round the best timing performance is achieved on 12 mappers and 12 reducers taking a total of 370 seconds , with 280 seconds for the mapper and 90 seconds for the reducer . over 85% efficiency",
    "is achieved in each case using multiple worker nodes compared to 1 worker .",
    "this round is most efficient on 3 workers achieving an efficiency of 97% and the performance deteriorates beyond the use of four workers on the cluster employed . in the second mapreduce round the best timing performance is achieved again on 12 mapper and 12 reducers taking a total of 13.9 seconds , with 7.2 seconds for the mapper and 6.7 seconds for the reducer . using 2 workers",
    "has the best efficiency of 74% ; the efficiency deteriorates beyond this .",
    "the second mapreduce round has performed poorly compared to the first round as there are large i / o and initialisation overheads on the workers .",
    "[ t ] +   +    figure [ figure3 ] shows two bar graphs in which the time for the first mapreduce round is profiled . for the mapper",
    "the time taken into account is for ( a ) applying financial terms , ( b ) local i / o operations , and ( c ) data delivery from the hdfs to the inputformat , from the inputformat to the mapper , and from the mapper to the partitioner .",
    "for the reducer the time taken into account is for ( a ) applying occurrence and aggregate financial terms , ( b ) local i / o operations , and ( c ) data delivery from the partitioner to the reducer , from the reducer to the outputformat and from the ouputformat to hdfs . for both the mappers and the reducers it is observed that over half the total time is taken for local i / o operations . in the case of the mapper",
    "the mathematical computations only take @xmath32 the total time , and the total time taken for data delivery from the hdfs to the inputformat , and from the inputformat to the mapper and from the mapper to the partitioner is only @xmath32 the total time . in the case of the reducer",
    "the mathematical computations take @xmath33 the total time , whereas the total time taken for data delivery from the partitioner to the reducer , from the reducer to the outputformat , and from the outputformat to hdfs is nearly @xmath34 the total time .",
    "this indicates that the local i / o operations on the cluster employed is expensive though the performance of hadoop is exploited for both the numerical computations and for large data processing and delivery .",
    "[ t ] +    figure [ figure4 ] shows a bar graph for the time taken in seconds for the second mapreduce round on 12 workers when the number of layers are varied from 1 to 5000 . there is a steady increase in the time taken for data processing and data delivery by the mapper and the reducer .",
    "gradually the time step decreases resulting in the flattening of the trend curve .",
    "figure [ figure5 ] shows the relative speed up achieved using mapreduce for aggregate risk analysis .",
    "there is close to linear speed up achieved until seven worker nodes are employed , and beyond seven nodes the gap between linear and relative speed up increases .",
    "this is reflected in the efficiency of the simulation for different number of workers shown in figure [ figure6 ] . over 90% efficiency",
    "is achieved up to seven worker nodes . beyond seven workers",
    "efficiency drops .",
    "for all the workers over 50% of the time is required for local i / o operations , and around 22% of the time is required for applying financial terms . between 15%-22% of the time is required for data delivery , with a slight increase in time for each additional worker employed .",
    "this is possibly due to the overhead involved in using a centralised raid data storage , which can be minimised if distributed file replication techniques are employed .",
    "the results indicate that there is scope for achieving high efficiency and speedup for numerical computations and large data processing and delivery within the hadoop system .",
    "however , it is apparent that large overheads for local i / o operations on the workers and for data transfer onto a centralised raid system are restraining performance .",
    "this large overhead is a resultant of the bottleneck in the connectivity between the worker nodes and the latency in processing data from local drives and the redundant data transfer to the centralised raid storage .",
    "therefore , efforts need to be made towards reducing the i / o overhead and seeking alternative distributed strategies to incorporate data replication for exploiting the full benefit of the hadoop mapreduce model .",
    "simulations for the analysis of portfolios of catastrophic risk need to manage and process large volumes of data in the form of a year event table and event loss tables . to be able to employ the simulations in real - time the algorithms need to rapidly process the data which poses both computational and data management challenges . in this paper ,",
    "how the mapreduce model using the hadoop framework can meet the requirements of rapidly consuming large volumes of data for the analysis of portfolios of catastrophic risk to address the challenges has been presented .",
    "an embarrassingly parallel algorithm for aggregate risk analysis is proposed and implemented using the map and reduce functions on the apache hadoop framework .",
    "the data challenges can be surmounted by employing the hadoop distributed file system and the distributed cache both offered by apache hadoop .",
    "a simulation of aggregate risk employing 100,000 trials with 1000 catastrophic events per trial performed on multiple worker nodes using two mapreduce rounds takes less than 6 minutes .",
    "the experimental results show the feasibility of employing mapreduce for parallel numerical computations and data management of aggregate risk analysis in real - time .",
    "future work will be directed towards optimising the implementation for reducing the local i / o overheads to achieve better speedup .",
    "efforts will be made towards incorporating additional financial filters , such as secondary uncertainty for fine - grain analysis of aggregate risk .",
    "a. k. bahl , o. baltzer , a. rau - chaplin and b. varghese , `` parallel simulations for analysing portfolios of catastrophic event risk , '' workshop of the international conference for high performance computing , networking , storage and analysis ( sc ) , 2012 .",
    "t. condie , n. conway , p. alvaro , j. m. hellerstein , k. elmeleegy and r. sears , `` mapreduce online , '' eecs department , university of california , berkeley ,",
    "usa , oct 2009 , technical report no .",
    "ucb / eecs-2009 - 136 .",
    "d. cummins , c. lewis and r. phillips , `` pricing excess - of - loss reinsurance contracts against catastrophic loss , '' the financing of catastrophe risk , editors : k. a. froot , university of chicago press , 1999 , pp ."
  ],
  "abstract_text": [
    "<S> monte carlo simulations employed for the analysis of portfolios of catastrophic risk process large volumes of data . often times these simulations are not performed in real - time scenarios as they are slow and consume large data . </S>",
    "<S> such simulations can benefit from a framework that exploits parallelism for addressing the computational challenge and facilitates a distributed file system for addressing the data challenge . to this end </S>",
    "<S> , the apache hadoop framework is chosen for the simulation reported in this paper so that the computational challenge can be tackled using the mapreduce model and the data challenge can be addressed using the hadoop distributed file system . a parallel algorithm for the analysis of aggregate risk </S>",
    "<S> is proposed and implemented using the mapreduce model in this paper . </S>",
    "<S> an evaluation of the performance of the algorithm indicates that the hadoop mapreduce model offers a framework for processing large data in aggregate risk analysis . </S>",
    "<S> a simulation of aggregate risk employing 100,000 trials with 1000 catastrophic events per trial on a typical exposure set and contract structure is performed on multiple worker nodes in less than 6 minutes . </S>",
    "<S> the result indicates the scope and feasibility of mapreduce for tackling the computational and data challenge in the analysis of aggregate risk for real - time use . </S>"
  ]
}