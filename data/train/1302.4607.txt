{
  "article_text": [
    "in recent years there has seen a growing interest in applying flexible statistical models for analyzing longitudinal data or the more general clustered data .",
    "various semiparametric [ e.g. , @xcite ] and nonparametric [ e.g. , @xcite , @xcite , @xcite , @xcite ( @xcite ) , @xcite , @xcite ] models have been proposed and studied in the literature .",
    "all of these flexible , semiparametric or nonparametric methods require specification of tuning parameters , such as the bandwidth for the local polynomial kernel methods , the number of knots for regression splines and the penalty parameter for penalized splines and smoothing splines .",
    "the `` leave - subject - out cross - validation '' ( lsocv ) or more generally called `` leave - cluster - out cross - validation , '' introduced by @xcite , has been widely used as the method for selecting tuning parameters in analyzing longitudinal data and clustered data ; see , for example , @xcite .",
    "the lsocv is intuitively appealing since the within - subject dependence is preserved by leaving out all observations from the same subject together in the cross - validation . in spite of its broad acceptance in practice , the use of lsocv still lacks a theoretical justification to date .",
    "computationally , the existing literature has focused on the grid search method for finding the minimizer of the lsocv criterion ( lsocv score ) [ @xcite ] , which is rather inefficient and even prohibitive when there are multiple tuning parameters .",
    "the goal of this paper is twofold : first , we develop a theoretical justification of the lsocv by showing that the lsocv criterion is asymptotically equivalent to an appropriately defined loss function ; second , we develop a computationally efficient algorithm to optimize the lsocv criterion for selecting multiple penalty parameters for penalized splines .",
    "we shall focus our presentation on longitudinal data , but all discussions in this paper apply to clustered data analysis .",
    "suppose we have @xmath0 subjects and subject @xmath1 , @xmath2 , has observations @xmath3 , @xmath4 , with @xmath5 being the @xmath6th response and @xmath7 being the corresponding vector of covariates .",
    "denote @xmath8 and @xmath9 .",
    "the marginal non- and semi - parametric regression model [ @xcite ] assumes that the mean and covariance matrix of the responses are given by @xmath10 where @xmath11 is a vector of linear regression coefficients , @xmath12 , @xmath13 , are unknown smooth functions , and @xmath14 s are within - subject covariance matrices .",
    "denote @xmath15 . by using a basis expansion to approximate each @xmath12 ,",
    "@xmath16 can be approximated by @xmath17 for some design matrix @xmath18 and unknown parameter vector @xmath19 , which then can be estimated by minimizing the penalized weighted least squares @xmath20 where @xmath21 s are working correlation matrices that are possibly misspecified , @xmath22 is a semi - positive definite matrix such that @xmath23 serves as a roughness penalty for  @xmath12 , and @xmath24 is a vector of penalty parameters .",
    "methods for choosing basis functions , constructing the corresponding design matrices @xmath18 s and defining the roughness penalty matrices are well established in the statistics literature .",
    "for example , b - spline basis and basis obtained from reproducing kernel hilbert spaces are commonly used .",
    "roughness penalty matrices can be formed corresponding to the squared second - difference penalty , the squared second derivative penalty , the thin - plate splines penalty or using directly the reproducing kernels .",
    "we refer to the books by @xcite , @xcite and @xcite for thorough treatments of this subject .",
    "the idea of using working correlation for longitudinal data can be traced back to the generalized estimating equations ( gee ) of @xcite , where it is established that the mean function can be consistently estimated with the correct inference even when the correlation structure is misspecified .",
    "@xcite further demonstrated that using a possibly misspecified working correlation structure @xmath25 has the potential to improve the estimation efficiency over methods that completely ignore the within - subject correlation .",
    "similarly , results have been obtained in the nonparametric setting in @xcite and @xcite .",
    "commonly used working correlation structures include compound symmetry and autoregressive models ; see @xcite for a detailed discussion .    in the case of independent data , @xcite established the asymptotic optimality of the generalized cross - validation ( gcv ) [ @xcite ] for penalty parameter selection by showing that minimizing the gcv criterion is asymptotically equivalent to minimizing a suitably defined loss function . to understand the theoretical property of lsocv , we ask the following question in this paper : what loss function does the lsocv mimic or estimate and how good is this estimation ? we are able to show that the unweighted mean squared error is the loss function that lsocv is targeting .",
    "specifically , we obtain that , up to a quantity that does not depend on the penalty parameters , the lsocv score is asymptotically equivalent to the mean squared error loss .",
    "our result provides the needed theoretical justification of the wide use of lsocv in practice .    in two related papers , @xcite and",
    "@xcite developed modifications of the gcv for dependent data under assumptions on the correlation structure and established the optimality of the modified gcvs .",
    "although their modified gcvs work well when the correlation structure is correctly specified up to some unknown parameters , they need not be suitable when there is not enough prior knowledge to make such a specification or the within - subject correlation is too complicated to be modeled nicely with a simple structure .",
    "the main difference between lsocv and these modified gcvs is that lsocv utilizes working correlation matrices in the estimating equations and allows misspecification of the correlation structure .",
    "moreover , since the lsocv and the asymptotic equivalent squared error loss are not attached to any specific correlation structure , lsocv can be used to select not only the penalty parameters but also the correlation structure .",
    "another contribution of this paper is the development of a fast algorithm for optimizing the lsocv criterion . to avoid computation of a large number of matrix inversions",
    ", we first derive an asymptotically equivalent approximation of the lsocv criterion and then derive a newton  raphson type algorithm to optimize this approximated criterion .",
    "the algorithm is particularly useful when we need to select multiple penalty parameters .",
    "the rest of the paper is organized as follows .",
    "section  [ sec2 ] presents the main theoretical results .",
    "section  [ sec3 ] proposes a computationally efficient algorithm for optimizing the loscv criterion .",
    "results from some simulation studies and a real data analysis are given in sections  [ sec4 ] and  [ sec5 ] .",
    "all technical proofs and computational implementations are collected in the and in the supplementary materials [ @xcite ] .",
    "let @xmath26 denote the estimate of the mean function obtained by using basis expansion of unknown functions @xmath12 s ( @xmath13 ) and solving the minimization problem ( [ eqpenalized ] ) for @xmath27 .",
    "let @xmath28}(\\cdot)$ ] be the estimate of the mean function @xmath29 by the same method but using all the data except observations from subject @xmath1 , @xmath30 . the lsocv criterion is defined as @xmath31}({\\mathbf{x}}_i)\\bigr\\}^t \\bigl\\{{\\mathbf}y_i- \\hat{\\mu}^{[-i]}({\\mathbf{x}}_i)\\bigr\\}.\\ ] ] by leaving out all observations from the same subject , the within - subject correlation is preserved in lsocv . before giving the formal justification of lsocv",
    ", we review a heuristic justification in section  [ secheuristic ] .",
    "section  [ secloss ] defines the suitable loss function .",
    "section  [ secconditions ] lists the regularity conditions and section  [ example ] provides an example illustrating how the regularity conditions in section  [ secconditions ] can be verified using more primitive conditions .",
    "section  [ secoptimality ] presents the main theoretical result about the optimality of lsocv .",
    "the initial heuristic justification of lsocv by @xcite is that it mimics the mean squared prediction error ( mspe ) .",
    "consider some new observations @xmath32 , taken at the same design points as the observed data .",
    "for a given estimator of the mean function @xmath29 , denoted as @xmath33 , the mspe is defined as @xmath34 using the independence between @xmath35}(\\cdot)$ ] and @xmath36 , we obtain that @xmath37}({\\mathbf{x}}_i)\\bigr\\|^2,\\ ] ] where @xmath38 . when @xmath0 is large , @xmath39}(\\cdot)$ ] should be close to @xmath33 , the estimate that uses observations from all subjects . thus , we expect @xmath40 to be close to the mspe",
    ".      [ secloss ] we shall provide a formal justification of lsocv by showing that the lsocv is asymptotically equivalent to an appropriately defined loss function .",
    "denote @xmath41 , @xmath42 , and @xmath43 .",
    "then , for a given choice of @xmath44 and @xmath25 , the minimizer of  ( [ eqpenalized ] ) has a closed - form expression @xmath45 the fitted mean function evaluated at the design points is given by @xmath46 where @xmath47 is the hat matrix defined as @xmath48 from now on , we shall use @xmath49 for @xmath47 without causing any confusion .    for a given estimator @xmath26 of @xmath29 ,",
    "define the mean squared error ( mse ) loss as the true loss function @xmath50 using ( [ eqlinear - est ] ) , we obtain that , for the estimator obtained by minimizing ( [ eqpenalized ] ) , the true loss function  ( [ trueloss ] ) becomes @xmath51 \\\\[-8pt ] \\nonumber & = & \\frac{1}{n}{\\bolds}\\mu^t({\\mathbf{i}}-{\\mathbf{a}})^t({\\mathbf{i}}-{\\mathbf{a}}){\\bolds}\\mu + \\frac{1}{n}{\\bolds}\\varepsilon^t{\\mathbf{a}}^t{\\mathbf{a}}{\\bolds}\\varepsilon - \\frac{2}{n}{\\bolds}\\mu^t\\bigl({\\mathbf{i}}-{\\mathbf{a}}^t\\bigr){\\mathbf{a}}{\\bolds}\\varepsilon,\\end{aligned}\\ ] ] where @xmath52 , @xmath53 .",
    "since @xmath54 and @xmath55 , the risk function can be derived as @xmath56      [ secconditions ] this section states some regularity conditions needed for our theoretical results . noticing that unless @xmath57 , @xmath49 is not symmetric .",
    "we define a symmetric version of @xmath49 as @xmath58 .",
    "let @xmath59 be the diagonal block of @xmath60 corresponding to the @xmath1th subject",
    ". with some abuse of notation ( but clear from the context ) , denote by @xmath61 and @xmath62 the largest and the smallest eigenvalues of a matrix .",
    "the regularity conditions involve the quantity @xmath63 , which takes the minimal value @xmath64 when @xmath57 or @xmath65 .",
    "let @xmath66 and @xmath67 be @xmath68 vectors such that @xmath69 , @xmath2 .    1 .   for some @xmath70 , @xmath71 , @xmath72 .",
    "2 .   a.   @xmath73 ; b.   @xmath74 .",
    "3 .   @xmath75 .",
    "4 .   @xmath76 .",
    "5 .   @xmath77",
    ".    condition 1 is a mild moment condition that requires that each component of the standardized residual @xmath78 has a uniformly bounded fourth moment .",
    "in particular , when @xmath79 s are from the gaussian distribution , the condition holds with @xmath80 .    condition 2 extends the usual condition on controlling leverage , used in theoretical analysis of linear regression models .",
    "note that @xmath81 can be interpreted as the leverage of subject @xmath1 , measuring the contribution to the fit from data of subject @xmath1 and @xmath82 is the average of the leverages .",
    "this condition says that the maximum leverage can not be arbitrarily larger than the average leverage or , in other words , there should not be any dominant or extremely influential subjects .",
    "in the special case that all subjects have the same design matrices , the condition automatically satisfies since @xmath83 for all @xmath2 .",
    "condition  2 is likely to be violated if the @xmath84 s are very unbalanced .",
    "for example , if @xmath85 of subjects have @xmath86 observations and the rest of the subjects only have @xmath87 or @xmath88 observations each , then @xmath89 can be very large .    when @xmath84 s are bounded , any reasonable choice of @xmath25 would generally yield a bounded value of the quantity @xmath90 , and condition 3",
    "reduces to @xmath91 , which simply says that the parametric rate of convergence of risk @xmath92 is not achievable .",
    "this is a mild condition since we are considering nonparametric estimation .",
    "when @xmath84 s are not bounded , condition  3 s verification should be done on a case - by - case basis . as a special case ,",
    "recent results for the longitudinal function estimation by @xcite indicate that condition 3 would be satisfied in this particular setting if @xmath93 and @xmath94 or @xmath95 and @xmath96 for some @xmath97 , where @xmath98 is the harmonic mean of @xmath99 .",
    "this conclusion holds for both fixed common designs and independent random designs .",
    "condition 4 essentially says that @xmath100 .",
    "it is straightforward to show that the left - hand side is bounded from above by @xmath101 , where @xmath102 is the condition number of a matrix @xmath103 .",
    "if @xmath84 s are bounded , for choices of @xmath25 such that @xmath104 and @xmath25 are not singular , to ensure condition 4 holds it suffices to have that @xmath105 . for regression splines ( @xmath106 ) , this condition holds if @xmath107 where @xmath108 is the number of basis functions used , since @xmath109 . for penalized splines and smoothing splines",
    ", we provide a more detailed discussion in section  [ example ] .",
    "if the working correlation matrix @xmath25 is chosen to be well - conditioned such that its condition number @xmath110 is bounded , condition  5 reduces to @xmath111 , which can be verified as condition  4 .",
    "conditions 35 all indicate that a bad choice of the working correlation matrix @xmath25 may deteriorate the performance of using the lsocv .",
    "for example , conditions  35 may be violated when @xmath112 or @xmath25 is nearly singular .",
    "thus , in practice , it is wise to avoid using a working correlation @xmath25 that is nearly singular .",
    "we do not make the assumption that @xmath84 s are bounded .",
    "however , @xmath84 obviously can not grow too fast relative to the number of subjects @xmath0",
    ". in particular , if @xmath84 s are too large , @xmath113 can be fairly large unless @xmath114 , and @xmath115 can be fairly large due to increase of dimensions of the working correlation matrices for individual subjects .",
    "thus , conditions 35 implicitly impose a limit to the growth rate of @xmath84 .",
    "in this section , we provide an example where conditions 35 can be discussed in a more specific manner .",
    "consider model  ( [ mean ] ) with only one nonparametric covariate @xmath116 and thus there is only one penalty parameter @xmath117 .",
    "we further assume that all eigeinvalues of matrices @xmath25 and @xmath104 are bounded from below and above , that is , there exist positive constants @xmath118 and @xmath119 such that @xmath120 and @xmath121 .",
    "under this assumption , it is straightforward to show that conditions 35 reduce to the following conditions .    @xmath122 as @xmath123 .",
    "@xmath124 .",
    "@xmath125 .",
    "using lemmas 4.1 and 4.2 from @xcite and similar arguments , we have the following three inequalities : @xmath126 and @xmath127 \\\\[-8pt ] \\nonumber & & \\qquad \\leq\\bigl\\{{\\mathbf{i}}-{\\mathbf{a}}(\\lambda,{\\mathbf{w}})\\bigr\\}^t\\bigl\\{{\\mathbf{i}}-{\\mathbf{a}}(\\lambda , { \\mathbf{w}})\\bigr\\ } \\leq c_2c_3\\bigl\\{{\\mathbf{i}}-\\tilde { { \\mathbf{a}}}(c_2\\lambda,{\\mathbf{i}})\\bigr\\},\\end{aligned}\\ ] ] where @xmath128 . these inequalities and the definition of the risk function @xmath129",
    "imply that we need only to check conditions  @xmath130@xmath131 for the case that @xmath57 .",
    "in particular , ( [ ine1])([ine3 ] ) imply that @xmath132 and , therefore , to show condition @xmath130 , it suffices to show @xmath133 as @xmath134 .",
    "we now use existing results from the literature to show how to verify conditions @xmath130@xmath131 .",
    "note that the notation used in the literature of penalized splines and smoothing splines is not always consistent . to fix notation",
    ", we denote for the rest of this section that @xmath135 and @xmath136 , where @xmath137 is the total number of observations from all subjects .",
    "let @xmath138 denote the order of the b - splines and consider a sequence of knots defined on the interval @xmath139 $ ] , @xmath140 .",
    "define b - spline basis functions recursively as @xmath141 for @xmath142 . when this b - spline basis is used for basis expansion , the @xmath6th row of @xmath18 is @xmath143 , for @xmath144 and @xmath2 .",
    "when the penalty is the integrated squared @xmath145th derivative of the spline function with @xmath146 , that is , @xmath147 , the penalty term can be written in terms of the spline coefficient vector @xmath27 as @xmath148 , where @xmath149 is a @xmath150 matrix with @xmath151 and @xmath152 is a matrix of weighted @xmath145th order difference operator [ @xcite ] .",
    "we make the following assumptions : ( a ) @xmath153 is of the order @xmath154 and @xmath155 for some constant @xmath156 ; ( b )  @xmath157}|q_n(x)-q(x)|=o(k_n^{-1})$ ] , where @xmath158 and @xmath159 are the empirical and true distribution function of all design points @xmath160 ; ( c )  @xmath161 .",
    "define quantity @xmath162 with some constant @xmath163 depending on @xmath145 and the design density .",
    "@xcite showed that , under above assumptions , if @xmath164 , @xmath165 and @xmath166 are both of the order @xmath167 and @xmath168 ; if @xmath169 , @xmath165 and @xmath166 are of order @xmath170 and @xmath171 .",
    "using these results and the results following inequalities ( [ ine1])([ine3 ] ) , it is straightforward to show that if @xmath172 ( for regression splines ) , letting @xmath173 and @xmath174 is sufficient to guarantee conditions @xmath130@xmath131 , and if @xmath175 ( for penalized splines ) , further assuming @xmath176 and @xmath177 ensures the validity of conditions @xmath130@xmath131 .    when @xmath169 , the asymptotic property of the penalized spline estimator is close to that of smoothing splines , where the number of internal knots @xmath178",
    "in fact , as discussed in @xcite , for smoothing splines , it typically holds that @xmath165 and @xmath166 are of order @xmath179 and @xmath180 for some @xmath181 as @xmath182 and @xmath176 ; see also @xcite , @xcite and @xcite .",
    "therefore , if one has @xmath176 and @xmath183 , conditions @xmath130@xmath131 can be verified for smoothing splines .      in this subsection , we provide a theoretical justification of using the minimizer of @xmath184 to select the optimal value of the penalty parameters @xmath44 .",
    "we say that the working correlation matrix @xmath25 is predetermined if it is determined by observation times and/or some other covariates .",
    "one way to obtain such @xmath25 is to use some correlation function plugged in with estimated parameters .",
    "naturally , it is reasonable to consider the value of @xmath44 that minimizes the true loss function @xmath185 as the optimal value of the penalty parameters for a predetermined @xmath25 .",
    "however , @xmath185 can not be evaluated using data alone since the true mean function in the definition of @xmath185 is unknown .",
    "one idea is to use an unbiased estimate of the risk function @xmath186 as a proxy of @xmath185 .",
    "define @xmath187 it is easy to show that @xmath188 which has expectation zero .",
    "thus , if @xmath189 is known , @xmath190 is an unbiased estimate of the risk @xmath186 .",
    "actually , the estimator is consistent , as stated in the following theorem .",
    "[ thmu ] under conditions @xmath191@xmath192 , for a predetermined @xmath25 and a nonrandom @xmath44 , as @xmath134 , @xmath193 and @xmath194    this theorem shows that the function @xmath195 , the loss function @xmath185 and the risk function @xmath186 are asymptotically equivalent .",
    "thus , if @xmath189 is known , @xmath190 is a consistent estimator of the risk function and , moreover , @xmath196 can be used as a reasonable surrogate of @xmath185 for selecting the penalty parameters , since the @xmath197 term does not depend on @xmath44 .",
    "however , @xmath196 depends on knowledge of the true covariance matrix @xmath189 , which is usually not available .",
    "the following result states that the lsocv score provides a good approximation of @xmath196 , without using the knowledge of @xmath189 .",
    "[ thmlsocv ] under conditions @xmath191@xmath198 , for a predetermined @xmath25 and a nonrandom @xmath44 , as @xmath134 , @xmath199 and , therefore , @xmath200    this theorem shows that minimizing @xmath201 with respect to @xmath44 is asymptotically equivalent to minimizing @xmath196 and is also equivalent to minimizing the true loss function @xmath185 . unlike @xmath196 , @xmath202",
    "can be evaluated using the data .",
    "the theorem provides the justification of using lsocv , as a consistent estimator of the loss or risk function , for selecting the penalty parameters .",
    "[ rem1 ] although the above results are presented for selection of the penalty parameter @xmath44 for penalized splines , the results also hold for selection of knot numbers ( or number of basis functions ) @xmath203 for regression splines when @xmath106 and @xmath203 is the tuning parameter to be selected .",
    "[ rem2 ] since the definition of the true loss function ( [ trueloss ] ) does not depend on the working correlation structure @xmath25 , we can use this loss function to compare performances of different choices of @xmath25 , for example , compound symmetry or autoregressive , and then choose the best one among several candidates .",
    "thus , the result in theorem  [ thmlsocv ] also provides a justification for using the lsocv to select the working correlation matrix .",
    "this theoretical implication is also confirmed in a simulation study in section  [ seccov - struct ] . when using the lsocv to select the working correlation matrix , we recommend to use regression splines , that is , setting @xmath106 , because this choice simplifies computation and provides more stable finite sample performance .",
    "in this section , we develop a computationally efficient newton  raphson - type algorithm to minimize the lsocv score .      the definition of lsocv would indicate that it is necessary to solve @xmath0 separate minimization problems in order to find the lsocv score .",
    "however , a computational shortcut is available that requires solving only one minimization problem that involves all data .",
    "recall that @xmath49 is the hat matrix .",
    "let @xmath204 denote the diagonal block of @xmath49 corresponding to the observations of subject @xmath1 .",
    "[ lemlsocvformula ] the lsocv score satisfies @xmath205 where @xmath206 is a @xmath207 identity matrix , and @xmath208 .    this result",
    ", whose proof is given in the supplementary material [ @xcite ] , extends a similar result for independent data [ e.g. , @xcite , page 31 ] .",
    "indeed , if each subject has only one observation , then ( [ eqlsocv ] ) reduces to @xmath209 , which is exactly the shortcut formula for the ordinary cross - validation score .",
    "a close inspection of the short - cut formula of @xmath210 given in ( [ eqlsocv ] ) suggests that the evaluation of @xmath202 can still be computationally expensive because of the requirement of matrix inversion and the formulation of the hat matrix @xmath49 . to further reduce the computational cost , using taylor s expansion @xmath211",
    ", we obtain the following approximation of @xmath212 : @xmath213 where @xmath214 is the part of @xmath215 corresponding to subject @xmath1 .",
    "the next theorem shows that this approximation is a good one in the sense that its minimization is asymptotically equivalent to the minimization of the true loss function .",
    "[ thmlsocv^ * ] under conditions 15 , for a predetermined @xmath25 and a nonrandom @xmath44 , as @xmath134 , we have @xmath216    this result and theorem  [ thmlsocv ] together imply that @xmath217 and @xmath210 are asymptotically equivalent , that is , for a predetermined @xmath25 and a nonrandom @xmath44 , @xmath218 .",
    "the proof of theorem  [ thmlsocv^ * ] is given in the .",
    "we developed an efficient algorithm to minimizing @xmath219 with respect to @xmath44 for a pre - given @xmath25 based on the works of @xcite and @xcite .",
    "the idea is to optimize the log transform of @xmath44 using the newton ",
    "raphson method .",
    "the detailed algorithm is described in the supplementary material [ @xcite ] and it can be shown that , for @xmath217 , the overall computational cost for each newton  raphson iteration is @xmath220 , which is much smaller than the cost of directly minimizing @xmath202 ( @xmath221 ) when the total number of used basis functions @xmath108 is large .",
    "[ secfun - est ] in this section , we illustrate the finite - sample performance of @xmath222 in selecting the penalty parameters . in each simulation run , we set @xmath223 and @xmath224 , @xmath225 . a random sample is generated from the model @xmath226 where @xmath227 is a subject level covariate and @xmath228 is an observational level covariate , both of which are drawn from @xmath229 .",
    "functions used here are from @xcite : @xmath230 where @xmath231 . the error term @xmath232 s are generated from a gaussian distribution with zero mean , variance @xmath233 and the compound symmetry within - subject correlation , that is , @xmath234 @xmath235 , @xmath236 . in this subsection",
    ", we take @xmath237 and @xmath238 . a  cubic spline with",
    "10 equally spaced interior knots in @xmath239 $ ] was used for estimating each function .",
    "functions were estimated by minimizing  ( [ eqpenalized ] ) with two working correlations : the working independence ( denoted as ) and the compound symmetry with @xmath238 ( denoted as @xmath240 ) .",
    "penalty parameters were selected by minimizing defined in ( [ eqlsocv^ * ] ) .",
    "the top two panels of figure  [ fig-1 ] show that the biases using @xmath241 and @xmath240 are almost the same , which is consistent with the conclusion in  @xcite that the bias of function estimation using regression splines does not depend on the choice of the working correlation .",
    "the bottom two panels indicate that using the true correlation structure @xmath240 yields more efficient function estimation , and the message is more clear in the estimation of @xmath242 .     equally spaced grid points in @xmath239 $ ] .",
    "top panels : estimated functions : solid  true functions ; dashed  average of estimates using  @xmath241 ; dotted  average of estimates using @xmath240 ( not distinguishable with dashed ) .",
    "bottom panels : variance of estimated functions : solid  estimates using  @xmath241 ; dashed  estimates using @xmath240 . ]      assuming that the structure of @xmath25 is known up to a parameter @xmath243 and the true covariance matrix @xmath189 is attained at @xmath244 , @xcite proposed to simultaneously select @xmath243 and @xmath44 by minimizing the following criterion : @xmath245 where @xmath137 is the total number of observations .",
    "they proved that v * is asymptotically optimal in selecting both the penalty parameter @xmath44 and the correlation parameter @xmath243 , provided that the within subject correlation structure is correctly specified . in this section",
    ", we compare the finite sample performance of @xmath222 and v * in selecting the penalty parameter when the working correlation matrix @xmath25 is given and fixed .",
    "we generated data using ( [ sim1 ] ) and ( [ corr1 ] ) as in the previous subsection and considered different parameters for the correlation matrix .",
    "in particular , we fixed @xmath238 and varied the noise standard deviation @xmath246 from @xmath247 to @xmath191 ; we also fixed @xmath237 and varied @xmath248 from @xmath249 to @xmath250 .",
    "a cubic spline with 10 equally spaced interior knots was used for each unknown regression function . for each simulation run , to compare the effectiveness of two selection criteria for a given working correlation matrix  @xmath25 , we calculated the ratio of true losses at different choices of penalty parameters : @xmath251 and @xmath252 , where @xmath253 and @xmath254 are penalty parameters selected by using v * and lsocv * , respectively , and @xmath255 is obtained by minimizing the true loss function defined in  ( [ trueloss ] ) assuming the mean function @xmath29 is known .    in the first experiment ,",
    "the true correlation matrix was used as the working correlation matrix , denoted as @xmath241 .",
    "this is the case that v * is expected to work well according to @xcite .",
    "results in figure  [ fig-31 ] indicate that performances of lsocv * and v * are comparable for this case regardless of values of @xmath246 or @xmath248 . in the second experiment ,",
    "the working correlation structure was chosen to be different from the true correlation structure .",
    "specifically , the working correlation matrix , denoted as @xmath240 , is a truncated version of ( [ corr1 ] ) where the correlation coefficient between @xmath256 and @xmath257 is set to @xmath248 if @xmath258 and @xmath259 if @xmath260 .",
    "results in figure  [ fig-3 ] show that lsocv * becomes more effective than v * in terms of minimizing the true loss of estimating the true mean function @xmath26 as @xmath246 or @xmath248 increases .",
    "these results are understandable since v * is applied to a situation that it is not designed for and its asymptotic optimality does not hold .",
    "moreover , from the right two panels of figures  [ fig-31 ] and  [ fig-3 ] , we see that the minimum value of lsocv * is reasonably close to the true loss function assuming the knowledge of the true function , as indicated by the conclusion of theorem  [ thmlsocv^ * ] .",
    "we conducted a simulation study to evaluate the performance of lsocv * in selecting the working correlation matrix  @xmath25 .",
    "the data was generated using the model ( [ sim1 ] ) with @xmath237 , @xmath224 for all @xmath2 . in this experiment , both @xmath227 and @xmath228",
    "are set to be observational level covariates drawn from @xmath229 .",
    "four types of within - subject correlation structures were considered : independence ( ind ) , compound symmetry with correlation coefficient @xmath248 ( cs ) , ar(1 ) with lag - one correlation @xmath248 ( ar ) , and unstructured correlation matrix with @xmath261 , @xmath262 and @xmath259 otherwise ( un ) .",
    "data were generated using one of these correlation structures and then the lsocv * was used to select the best working correlation from the four possible candidates .",
    "a cubic spline with @xmath263 equally spaced interior knots in @xmath239 $ ] was used to model each unknown function and we set the penalty parameter vector @xmath106 .",
    "table  [ tab-1 ] summarizes the results based on 200 simulation runs for each setup .",
    "we observe that lsocv * works well : the true correlation structure is selected in the majority of times .",
    "@lccd3.1d2.1d2.1d2.1@ & & & + & & & + @xmath264&@xmath265&*true structure * & & & & + 50&0.3&ind&97.0&2.0&1.0&0 + & & cs&8.5&78.0&13.5&0 + & & ar&13.5&10.0&76.5&0 + & & un&1.5&1.5&21.5&75.5 + & 0.5&ind&96.5 & 2.5 & 1.0 & 0 + & & cs & 3.0 & 78.5 & 18.5 & 0 + & & ar&4.0 & 9.5 & 86.5 & 0 + & & un & 3.5 & 4.0 & 11.5 & 81.0 + & 0.8&ind&98.5 & 1.0 & 0.5 & 0 + & & cs & 3.5 & 74.0 & 22.0 & 0.5 + & & ar&5.5 & 21.0 & 71.0 & 2.5 + & & un&5.5 & 1.0 & 8.5 & 85.0 + 100&0.3&ind&95.0 & 3.0 & 2.0 & 0 + & & cs&2.0 & 84.5 & 13.5 & 0 + & & ar&3.5 & 8.5 & 88.0 & 0 + & & un&0 & 1.0 & 13.5 & 85.5 + & 0.5&ind&99.5 & 0.5 & 0 & 0 + & & cs & 2.5 & 81.0 & 16.5 & 0 + & & ar&1.0 & 6.0 & 93.0 & 0 + & & un & 2.0 & 0.5 & 10.0 & 87.5 + & 0.8&ind&99.0 & 1.0 & 0 & 0 + & & cs & 2.5 & 73.5 & 24.0 & 0 + & & ar&2.0 & 20.0 & 76.5 & 1.5 + & & un&5.5 & 2.0 & 9.0 & 83.5 + 150&0.3&ind & 98.5 & 1.0 & 0.5 & 0 + & & cs&2.0 & 85.0 & 13.0 & 0 + & & ar&2.5 & 5.5 & 92.0 & 0 + & & un&0 & 0 & 16.5 & 83.5 + & 0.5&ind&100 & 0 & 0 & 0 + & & cs & 1.0 & 81.5 & 17.5 & 0 + & & ar&2.5 & 8.5 & 89.0 & 0 + & & un & 0.5 & 0 & 12.0 & 87.5 + & 0.8&ind&99.5 & 0.5 & 0 & 0 + & & cs & 1.0 & 78.0 & 20.0 & 1.0 + & & ar & 0.5 & 18.5 & 77.5 & 3.5 + & & un&1.0 & 2.0 & 6.5 & 90.5 +",
    "as a subset from the multi - center aids cohort study , the data set includes the repeated measurements of cd4 cell counts and percentages on 283 homosexual men who became hiv - positive between 1984 and 1991 .",
    "all subjects were scheduled to take their measurements at semi - annual visits . however , since many subjects missed some of their scheduled visits , there are unequal numbers of repeated measurements and different measurement times per subject .",
    "further details of the study can be found in @xcite .",
    "our goal is a statistical analysis of the trend of mean cd4 percentage depletion over time .",
    "denote by @xmath266 the time in years of the @xmath6th measurement of the @xmath1th individual after hiv infection , by @xmath5 the @xmath1th individual s cd4 percentage at time @xmath266 and by @xmath267 the @xmath1th individual s smoking status with values @xmath191 or @xmath259 for the @xmath1th individual ever or never smoked cigarettes , respectively , after the hiv infection . to obtain a clear biological interpretation",
    ", we define @xmath268 to be the @xmath1th individual s centered age at hiv infection , which is obtained by the @xmath1th individual s age at infection subtract the sample average age at infection .",
    "similarly , the @xmath1th individual s centered pre - infection cd4 percentage , denoted by @xmath269 , is computed by subtracting the average pre - infection cd4 percentage of the sample from the @xmath1th individual s actual pre - infection cd4 percentage .",
    "these covariates , except the time , are time - invariant .",
    "consider the varying - coefficient model @xmath270 where @xmath271 represents the trend of mean cd4 percentage changing over time after the infection for a nonsmoker with average pre - infection cd4 percentage and average age at hiv infection , and @xmath272 , @xmath273 and @xmath274 describe the time - varying effects on the post - infection cd4 percentage of cigarette smoking , age at hiv infection and pre - infection cd4 percentage , respectively .",
    "since the number of observations is very uneven among subjects , we only used subjects with at least 4 observations .",
    "a cubic spline with @xmath275 equally spaced knots was used for modeling each function .",
    "we first used the working independence @xmath276 to fit the data and then used the residuals from this model to estimate parameters in the correlation function @xmath277 where @xmath278 is the lag in time and @xmath279 , @xmath280 .",
    "this correlation function was considered previously in @xcite .",
    "the estimated parameter values are @xmath281 .",
    "the second working correlation matrix @xmath240 considered was formed using @xmath282 .",
    "we computed that @xmath283 and @xmath284 , which implies that using @xmath240 is preferable .",
    "this conclusion remains unchanged when the number of knots varies . to visualize the gain in estimation efficiency by using @xmath240 instead of @xmath241",
    ", we calculated the width of the @xmath285 pointwise bootstrap confidence intervals based on 1000 bootstrap samples , which is displayed in figure  [ fig-4 ] .",
    "we can observe that the bootstrap intervals using @xmath240 are almost uniformly narrower than those using @xmath241 , indicating higher estimation efficiency .",
    "the fitted coefficient functions ( not shown to save space ) using @xmath240 with @xmath44 selected by minimizing @xmath286 are similar to those published in previous studies conducted on the same data set [ @xcite ] .",
    "pointwise bootstrap confidence intervals based on 1000 bootstrap samples , using the working independence @xmath241 ( solid line ) and the working correlation matrix @xmath240 ( dashed line ) . ]    [ app ]",
    "this section is organized as follows .",
    "we first give three technical lemmas ( lemmas  [ lemeigen][lemvariance ] ) needed for the proof of theorem  [ thmu ] . after proving theorem  [ thmu ] , we give another lemma ( lemma  [ lemlastone ] ) that facilitates proofs of theorems  [ thmlsocv ] and  [ thmlsocv^ * ] .",
    "we prove theorem  [ thmlsocv^ * ] first and then proceed to the proof of theorem  [ thmlsocv ] .                  denote @xmath300 , where @xmath301 s are independent random vectors with length @xmath84 , @xmath302 and @xmath303 for @xmath2 . for each @xmath1 , define @xmath304 where @xmath305 if @xmath306 and 0 otherwise , @xmath307 .",
    "[ lemvariance ] if there exists a constant @xmath308 such that @xmath309 holds for all @xmath144 , @xmath2 , then @xmath310 where @xmath311 is any @xmath312 matrix ( not necessarily symmetric ) , @xmath313 is the @xmath1th @xmath314 diagonal block of @xmath311 and @xmath315 is an `` envelop '' matrix such that @xmath316 are positive semi - definite .",
    "we first prove ( [ eq321 ] ) . by ( [ eqtrueloss ] ) , we have @xmath318 define @xmath319 . then @xmath320 .",
    "since @xmath311 is positive semi - definite , by applying lemma  [ lemvariance ] with @xmath321 , @xmath322 and @xmath323 , we obtain @xmath324 for some @xmath70 as defined in lemma  [ lemvariance ] . by lemmas [ lemtrace ] and  [ lemeigen1 ] , under condition 3",
    ", we have @xmath325 \\\\[-8pt ] \\nonumber & \\leq & \\frac{2\\xi({\\bolds{\\sigma}},{\\mathbf{w}})}{n}\\frac{1}{n}\\operatorname{tr}\\bigl({\\mathbf{a}}^t{\\mathbf{a}}{\\bolds{\\sigma}}\\bigr)=o\\bigl(r^2({\\mathbf{w}},{\\bolds}\\lambda)\\bigr).\\end{aligned}\\ ] ] recall that @xmath59 is the @xmath1th diagonal block of @xmath60 .",
    "then , under condition 2(ii ) , @xmath326 .",
    "thus , @xmath327 since @xmath328 , under condition 3 , @xmath329 \\\\[-8pt ] \\nonumber & = & o(1)\\frac{k\\xi({\\bolds{\\sigma}},{\\mathbf{w}})}{n}\\frac{1}{n}\\operatorname{tr}\\bigl({\\mathbf{a}}^t{\\mathbf{a}}{\\bolds{\\sigma}}\\bigr)=o\\bigl(r^2({\\mathbf{w}},{\\bolds}\\lambda)\\bigr).\\end{aligned}\\ ] ] combining ( [ 1])([3 ] ) , we obtain @xmath330 since @xmath331 by lemma  [ lemeigen1 ] , under condition 3 , @xmath332 \\\\[-8pt ] \\nonumber & \\leq & \\frac{\\xi({\\bolds{\\sigma}},{\\mathbf{w}})}{n}\\frac{1}{n}{\\bolds}\\mu^t({\\mathbf{i}}-{\\mathbf{a}})^t({\\mathbf{i}}-{\\mathbf{a}}){\\bolds}\\mu \\\\ & = & o\\bigl(r^2({\\mathbf{w}},{\\bolds}\\lambda)\\bigr).\\nonumber\\end{aligned}\\ ] ] combining ( [ eql1])([eql3 ] ) and using the cauchy ",
    "schwarz inequality , we obtain @xmath333 , which proves ( [ eq321 ] ) .      to show ( [ eq323 ] ) , applying lemma  [ lemvariance ] with @xmath321 , @xmath335 .",
    "for each @xmath336 , noticing that @xmath337 is positive semi - definite , we can define an `` envelop '' matrix as @xmath338 for any @xmath339 . then by lemma  [ lemvariance ] , we obtain @xmath340 \\\\[-8pt ] \\nonumber & \\leq&\\frac{2}{n^2}\\operatorname{tr}\\bigl({\\mathbf{b}}{\\mathbf{b}}^t\\bigr ) + \\frac{k}{n^2 } \\sum_{i=1}^n\\bigl\\{\\operatorname{tr}\\bigl ( { \\mathbf{b}}_{ii}^*\\bigr)\\bigr\\}^2,\\end{aligned}\\ ] ] where @xmath308 is as in lemma  [ lemvariance ] . by lemma [ lemtrace ] , under condition 3",
    ", we have @xmath341 by using lemma  [ lemeigen ] repeatedly and taking @xmath342 , we have @xmath343 under conditions 2(i ) , 3 and 4 , we have @xmath344 therefore , combining ( [ eqerror - sq1])([eqerror - sq2 ] ) and noticing conditions 14 , we have @xmath345 which leads to ( [ eq323 ] ) .",
    "[ lemlastone ] let @xmath346 be a diagonal block matrix and @xmath347 be a positive semi - definite matrix such that @xmath348 are positive semi - definite .",
    "in addition , @xmath349",
    "s and@xmath350 s meet the following conditions : @xmath351 ; @xmath352 . then",
    ", under conditions 15 , we have @xmath353            condition 2 says that @xmath364 .",
    "using conditions 2 and 5 , we have @xmath365 which implies that all eigenvalues of @xmath366 are of order @xmath367 , and , hence , @xmath368 under condition 4 , the third term in ( [ bias1 ] ) can be bounded as @xmath369 for the last term in equation  ( [ bias1 ] ) , observe that @xmath370 is positive semi - definite for any @xmath371 .",
    "taking @xmath342 , we have @xmath372 where @xmath373 . equation  ( [ bias1 ] ) and thus ( [ bias ] ) have been proved .    to prove  ( [ variance ] ) , define @xmath374 and the corresponding `` envelop '' matrix @xmath347 , where the diagonal blocks are defined as @xmath375 with @xmath342 , then since @xmath376 we have that @xmath377 and that @xmath378 by condition 2 . under conditions  34 , ( [ variance ] ) follows from lemma  [ lemlastone ] .      for each @xmath2 , consider the eigen - decomposition @xmath382 , where @xmath383 is a @xmath207 orthogonal matrix and @xmath384 , @xmath385 . using this decomposition",
    ", we have @xmath386 where @xmath387 is a diagonal matrix with diagonal elements @xmath388 , @xmath144 .",
    "since under condition 2 @xmath389 , we have @xmath390 , which leads to @xmath391 define @xmath392 , where @xmath393 @xmath2 , @xmath394 it follows that , for each @xmath1 , @xmath395 since condition 2(i ) gives @xmath396 , we obtain that @xmath397 some algebra yields @xmath398 where @xmath399 and @xmath400 .",
    "to show  ( [ bias3 ] ) , note that @xmath401 using lemmas  [ lemeigen ] and  [ lemtrace ] repeatedly and condition 5 , we have @xmath402 thus , the first terms ( [ bias2 ] ) can be bounded as @xmath403 using lemma  [ lemeigen1 ] , under condition 4 and ( [ 11 ] ) , the second term of ( [ bias2 ] ) can be bounded as @xmath404 now consider the third term in ( [ bias2 ] ) . under condition 5 and (",
    "[ 11 ] ) , @xmath405 \\\\[-8pt ] \\nonumber & \\leq&2\\operatorname{tr}\\bigl(\\tilde{{\\mathbf{d}}}_{ii}^{(2)2}\\bigr)+2 \\lambda_{\\mathrm{max}}\\bigl({\\mathbf{w}}_i^{-1}\\bigr ) \\lambda_{\\mathrm{max}}({\\mathbf{w}}_i)\\operatorname{tr}\\bigl(\\tilde{{\\mathbf{d}}}_{ii}^{(2)2 } \\bigr ) \\\\ & = & o\\bigl(n^{-2}\\operatorname{tr}({\\mathbf{a}})^2\\bigr),\\nonumber\\end{aligned}\\ ] ] which implies that all eigenvalues of @xmath406 are of the order @xmath407 , and thus @xmath367 .",
    "then , under conditions 15 , we have @xmath408 to study the the fourth term in ( [ bias2 ] ) , we have @xmath409 & & \\qquad=\\frac{1}{n}\\sum _ { i=1}^n\\operatorname{tr }",
    "\\bigl\\{({\\mathbf{i}}_{ii}- { \\mathbf{a}}_{ii})^t{\\mathbf{d}}_{ii}^{(2 ) } ( { \\mathbf{i}}_{ii}-{\\mathbf{a}}_{ii}){\\bolds{\\sigma}}_i \\bigr\\ } \\\\[-3pt ] & & \\qquad\\quad { } -\\frac{1}{n}\\sum_{i=1}^n\\operatorname{tr}\\bigl ( { \\mathbf{a}}_{ii}^t{\\mathbf{d}}_{ii}^{(2 ) } { \\mathbf{a}}_{ii}{\\bolds{\\sigma}}_i\\bigr)+\\frac{1}{n}\\operatorname{tr}\\bigl ( { \\mathbf{a}}^t{\\mathbf{d}}^{(2)}{\\mathbf{a}}{\\bolds{\\sigma}}\\bigr).\\nonumber\\end{aligned}\\ ] ] to bound the first term in ( [ 12 ] ) , we note that @xmath410 & & \\qquad=\\frac{1}{2}\\operatorname{tr } \\bigl\\{({\\mathbf{i}}_{ii}-{\\mathbf{a}}_{ii})^t \\bigl({\\mathbf{w}}_i^{1/2}\\tilde { { \\mathbf{d}}}_{ii}^{(2 ) } { \\mathbf{w}}_i^{-1/2}+{\\mathbf{w}}_i^{-1/2}\\tilde { { \\mathbf{d}}}^{(2)}_{ii}{\\mathbf{w}}_i^{1/2}\\bigr ) ( { \\mathbf{i}}_{ii}-{\\mathbf{a}}_{ii}){\\bolds{\\sigma}}_i \\bigr\\},\\end{aligned}\\ ] ] which is bounded by @xmath411 & & \\qquad\\leq\\frac{1}{2}\\xi({\\bolds{\\sigma}}_i,{\\mathbf{w}}_i)\\operatorname{tr}\\bigl ( \\tilde{{\\mathbf{d}}}_{ii}^{(2)}\\bigr)+\\frac{\\alpha_i}{2}\\operatorname{tr } \\bigl\\ { \\bigl(\\tilde{{\\mathbf{d}}}_{ii}^{(2)}-2\\tilde{{\\mathbf{d}}}_{ii}^{(3 ) } \\bigr){\\mathbf{w}}_{i}^{-1/2}{\\bolds{\\sigma}}_{i } { \\mathbf{w}}_{i}^{-1/2 } \\bigr\\ } \\\\[-3pt ] & & \\qquad\\quad { } + \\frac{\\alpha_i}{2}\\operatorname{tr}\\bigl\\{\\tilde{{\\mathbf{d}}}_{ii}^{(2)}\\tilde { { \\mathbf{a}}}_{ii}{\\mathbf{w}}_i^{-1/2}{\\bolds{\\sigma}}_i { \\mathbf{w}}_i^{-1/2}\\tilde{{\\mathbf{a}}}_{ii}\\bigr\\ } \\\\[-3pt ] & & \\qquad\\leq\\frac{1}{2}\\xi({\\bolds{\\sigma}},{\\mathbf{w}})\\bigl\\{2+\\lambda_{\\mathrm{max}}\\bigl ( \\tilde{{\\mathbf{a}}}_{ii}^2\\bigr)\\bigr\\}\\operatorname{tr}\\bigl(\\tilde { { \\mathbf{d}}}_{ii}^{(2)}\\bigr ) \\\\[-3pt ] & & \\qquad = o\\bigl(r({\\mathbf{w}},{\\bolds}\\lambda)\\bigr),\\end{aligned}\\ ] ] where we take @xmath342 .",
    "the last equation follows from ( [ 11 ] ) and condition  4 .",
    "similarly , we can show that the second part of ( [ 12 ] ) is @xmath363 .",
    "next , we proceed to prove ( [ variance2 ] ) .",
    "define envelop matrices @xmath414 and @xmath415 , where @xmath416 with @xmath342 .",
    "it is easy to check that @xmath417 and @xmath418 are valid envelops of @xmath419 and @xmath420 , respectively .",
    "since under condition 5 , we have @xmath421 & & \\qquad\\leq\\lambda_{\\mathrm{max}}({\\mathbf{w } } ) \\lambda_{\\mathrm{max}}({\\mathbf{w}})\\lambda_{\\mathrm{max}}\\bigl({\\mathbf{w}}^{-1}\\bigr ) \\lambda_{\\mathrm{max}}^2\\bigl(\\tilde { { \\mathbf{d}}}_{ii}^{(1 ) } \\bigr)\\operatorname{tr}\\bigl(\\tilde{{\\mathbf{d}}}_{ii}^{(1)2}\\bigr ) \\\\[-2pt ] & & \\qquad = \\bigl\\ { \\lambda_{\\mathrm{max}}({\\mathbf{w}})\\lambda_{\\mathrm{max}}\\bigl ( { \\mathbf{w}}^{-1}\\bigr)o\\bigl(n^{-2}\\operatorname{tr}({\\mathbf{a}})^2\\bigr ) \\bigr\\ } \\lambda_{\\mathrm{max}}({\\mathbf{w}})o\\bigl(n^{-2}\\operatorname{tr}({\\mathbf{a}})^2\\bigr ) \\\\[-2pt ] & & \\qquad = \\lambda_{\\mathrm{max}}({\\mathbf{w}})o\\bigl(n^{-2}\\operatorname{tr}({\\mathbf{a}})^2\\bigr ) , \\\\[-2pt ] & & \\operatorname{tr}\\bigl({\\mathbf{d}}_{ii}^{(1)*}{\\mathbf{w}}_i\\bigr)\\leq \\lambda_{\\mathrm{max}}({\\mathbf{w}}_i)\\operatorname{tr}\\bigl(\\tilde{{\\mathbf{d}}}^{(1)2}_{ii } \\bigr)=\\lambda_{\\mathrm{max}}({\\mathbf{w}})o\\bigl(n^{-2}\\operatorname{tr}({\\mathbf{a}})^2 \\bigr)\\end{aligned}\\ ] ] and @xmath422 by applying lemma  [ lemlastone ] , we have @xmath423 and ( [ variance2 ] ) follows by the cauchy ",
    "schwarz inequality ."
  ],
  "abstract_text": [
    "<S> although the leave - subject - out cross - validation ( cv ) has been widely used in practice for tuning parameter selection for various nonparametric and semiparametric models of longitudinal data , its theoretical property is unknown and solving the associated optimization problem is computationally expensive , especially when there are multiple tuning parameters . in this paper , by focusing on the penalized spline method , we show that the leave - subject - out cv is optimal in the sense that it is asymptotically equivalent to the empirical squared error loss function minimization . </S>",
    "<S> an efficient newton - type algorithm is developed to compute the penalty parameters that optimize the cv criterion . </S>",
    "<S> simulated and real data are used to demonstrate the effectiveness of the leave - subject - out cv in selecting both the penalty parameters and the working correlation matrix . </S>"
  ]
}