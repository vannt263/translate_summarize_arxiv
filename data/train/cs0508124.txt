{
  "article_text": [
    "networked systems arise in various contexts such as the public internet peer - to - peer networks , ad - hoc wireless networks , and sensor networks .",
    "such systems are becoming central to our everyday life .",
    "the networked systems today employ traditional coding schemes for end - to - end connections and are generally not tailored to the network environment .",
    "for example , for reasons of design simplicity , intermediate nodes at a network are only allowed to forward and not to process incoming information flows . however , as the size of communication networks grows , it becomes less clear if the benefits of the simple end - to - end approach outweigh those of coding schemes that employ intermediate node processing .    from a theoretical point of view",
    "it is well - known that if intermediate nodes are allowed to decode and re - encode the information sent by the source , with no constraints on complexity and/or delay, then the information capacity between a sender and a receiver is upper bounded by the _ min - cut capacity _ of the network , as described in  @xcite . a crucial point in making schemes that employ intermediate node - processing practical and attractive ,",
    "is in realizing benefits without incurring excessive complexity and delay .    in this paper",
    "we propose coding schemes that employ intermediate node processing and discuss their performance .",
    "these schemes are based on _ fountain codes _ , a set of rate - less codes recently proposed @xcite that have a number of desirable properties for networked environments .",
    "we compare different coding schemes based on their _ complexity , delay , memory requirement , achievable rate , _ and _ adaptability _ ; we will define these metrics precisely in section [ model ] .",
    "for example , if we use an lt - code @xcite to encode @xmath0 information bits at the source and simply forward any received bit at the intermediate nodes , we would need @xmath1 xor operations at the transmitter , and @xmath2 xor operations at the receiver , where @xmath3 is the end - to - end capacity of the overall channel measured in bits per channel use .",
    "intermediate nodes would have no processing or memory requirements , and would not introduce delay .",
    "this scheme would further adapt to unknown channel parameters .",
    "however , the achievable rate can only approach the end - to - end capacity of the overall channel , which is in general less than the min - cut capacity of the network .    in @xcite",
    "the authors examined the benefits of intermediate node processing from an information theoretic point of view .",
    "our work can be viewed as approaching the same problem from a coding theory point of view .    in @xcite",
    "a scheme was proposed that takes advantage of intermediate node processing to approach the min - cut capacity , and puts emphasis on the queuing theory aspects of the problem .",
    "the authors show that if we allow intermediate nodes to transmit random linear combinations of the incoming packets over a finite field @xmath4 , the transmission rate approaches the min - cut capacity as @xmath5 goes to infinity . in this paper",
    "we will present alternative optimal coding schemes that approach the min - cut capacity using a constant field size , and in particular a binary field .",
    "the paper is organized as follows . in section  [ model ]",
    "we present our model and performance metrics in more detail . in section  [ coding ]",
    "we describe our proposed coding schemes . in section  [ general ]",
    "we discuss generalization to other networks ; in section  [ compare ] we compare our results with some related work in more details , and finally we conclude the paper in section  [ discuss ] .",
    "we consider a linear network that models a path between a source and a destination . the corresponding graph is comprised of a source node , a destination node and a series of @xmath6 intermediate nodes .",
    "the @xmath7 edges between the nodes correspond to independent memoryless erasure channels , and the information units sent over the @xmath8th link are erased with probability @xmath9 .",
    "we assume a discrete time model , where each node can transmit one unit of information at each time slot . for coding purposes , we will treat each information unit as a symbol , but in general we can have a packet of symbols , and apply to each symbol of the packet the same encoding / decoding operation ; in the following , we will refer to information units as packets or symbols interchangeably .",
    "intermediate nodes have the capability to process the packets they receive , and use them to generate new packets .",
    "we ignore the transmission delay along channels ( as it is beyond our control ) , i.e. , we assume that a packet transmitted at time @xmath10 , if not erased , is received immediately at the next node in the chain .    throughout this paper we will use as illustrating example the simple configuration depicted in fig .  [",
    "fig : line ] with @xmath11 links ; we will also discuss the generalization of our results to longer chains .",
    "the source node @xmath12 encodes @xmath0 symbols to create @xmath13 coded outputs using a code @xmath14 and sends them over the channel @xmath15 .",
    "node @xmath16 will receive on average @xmath17 coded symbols over @xmath13 time slots .",
    "node @xmath16 will send @xmath18 packets , using a code ( more generally , processing ) @xmath19 .",
    "if node @xmath16 finishes transmitting at time @xmath10 , where @xmath20 , then node @xmath3 will receive on average @xmath21 packets after @xmath10 time slots .",
    "for each coding scheme of this type , we define the following metrics :    1",
    ".   _ complexity _ for encoding / processing / decoding at nodes @xmath12 , @xmath16 and @xmath3 : the number of operations required as a function of @xmath0 , @xmath13 and @xmath18 .",
    "delay _ incurred at the intermediate node @xmath16 : this is the time @xmath22 , where @xmath23 is the min - cut capacity .",
    "we will remark more on this notion of delay in section  [ s : delay ] below .",
    "memory requirement _ : the number of memory elements needed at node @xmath16 .",
    "section  [ s : delay ] will also comment on the minimal memory requirements of any coding scheme over the line network .",
    "4 .   _ achievable rate _ : the rate at which information is transmitted from @xmath12 to @xmath3 .",
    "we say that a coding scheme is optimal in rate , if each individual link is used at a rate equal to its capacity .",
    "thus it can achieve the min - cut capacity between the source and the destination .",
    "adaptability _ : whether the coding scheme needs to be designed for specific erasure probabilities @xmath24 and @xmath25 or not .",
    "fountain codes , for example , are adaptable in this sense .",
    "we observe that , although it is possible to design a code over a single link that is both adaptable and is optimized for achievable rate and delay , the overall coding scheme can not be adaptable if we want to jointly optimize for achievable rate and delay . indeed , assume that @xmath26 .",
    "then the scheme that jointly optimizes the delay and the achievable rate requires node @xmath16 to transmit ( forward ) only when it receives a new packet .",
    "however , if @xmath24 and @xmath25 are equal and large , then a large fraction of the packets will get erased . in order to optimize for delay , node @xmath16 should transmit about @xmath27 packets for each packet it receives , without waiting to receive the next packet from node @xmath12 .",
    "therefore a single scheme can not be rate - optimal for both cases .",
    "depending on the application , different emphases might be placed on these performance metrics .",
    "for example , consider a real - time application , where information is collected into blocks of @xmath0 packets that are encoded and sent over the channel .",
    "in other words , we want to transmit the real - time information from a source , as it is produced .",
    "assume that we have @xmath28 such blocks .",
    "then the delay overhead at intermediate nodes can be considered to be a `` set - up '' delay for the connection , experienced only once , and hence insignificant if @xmath28 is large . on the other hand ,",
    "the memory requirements at intermediate nodes may be restrictive .",
    "indeed , there might exist a large number of connections ( paths ) that share an intermediate node that performs processing .",
    "thus , the memory available for each individual connection might need to be scaled down accordingly .",
    "recall that our notion of delay is linked with the optimal time of communication over a _",
    "single _ channel with equivalent min - cut capacity .",
    "note however that with this definition , it is impossible to achieve a ` zero delay ' scheme even for the simple network of fig .",
    "[ fig : line ] .",
    "in fact , even if both links @xmath15 and @xmath29 provide perfect feedback , there is an inherent delay to be suffered due to the existence of sequential links .",
    "as we will see , even in this perfect setting , there is also a need for memory storage , in amounts that grow with @xmath0 . in this section",
    "we will calculate the memory requirements , as well as the minimal delay which is incurred when perfect feedback exists ; certainly no coding scheme that does not rely on feedback can transmit in less time .",
    "the obvious optimal scheme in the presence of feedback is one where each node repeats transmission of each packet until it is successfully received at the destination .",
    "node @xmath12 then completes transmitting in time @xmath30 .",
    "the operations at node @xmath16 can be described using a markov chain with states @xmath31 , indicating the number of received packets still to be sent at each time ; therefore at each time @xmath8 , @xmath32 packets need to be stored in memory . at each time",
    "( when @xmath33 ) , with a probability @xmath34 the state is unchanged , and with a probability @xmath35 , the state is increased or decreased by @xmath36 , with equal probability . therefore , after @xmath37 time slots , the dynamics of this system resembles that of a random walk with a reflecting boundary at @xmath38 , over @xmath39 steps ; ( there is slight correction , due to ` longer stays ' at state @xmath38 , but for large @xmath37 , the probability of being at that state is insignificant . )",
    "thus the expected value of @xmath40 is the expected value of the absolute value of a random walk after @xmath41 steps .",
    "therefore @xmath42=o(\\sqrt{n'})=o(\\sqrt{2\\epsilon k})$ ] , where we have used that @xmath30 .",
    "node @xmath16 then completes transmitting the remaining @xmath40 packets in a time @xmath43 .",
    "therefore , the ` delay ' of this scheme is @xmath44 , while the expected memory requirement is @xmath45 .",
    "this argument can be extended to show that in linear network with @xmath7 similar links , where @xmath7 is a fixed finite number , each intermediate node incurs a delay of @xmath44 and requires @xmath45 units of memory .",
    "in this section we describe and compare a number of coding schemes for a line network with @xmath7 links . in the next section",
    "we will discuss how these schemes can be extended to more general settings .",
    "we will use the configuration in fig .",
    "[ fig : line ] , with @xmath11 , as the illustrating example , and assume for simplicity that @xmath46 , in which case @xmath47 . in all schemes below we will use as code @xmath14 over the link @xmath15 , a fountain code , such as an lt - code or a raptor code ; as demonstrated in @xcite and @xcite , these codes are low complexity , rate optimal , adaptable codes over erasure channels",
    ". then for each different coding scheme , we will specify the code @xmath19 over the link @xmath29 .",
    "a summary of the properties of all these schemes will be provided in table  [ tabl1 ] .",
    "an obvious scheme is to use a separate code for each of the @xmath7 links of the line network , and have each intermediate node completely decode and re - encode the incoming data .",
    "then it is obvious that we can achieve the min - cut capacity by using optimal codes ( e.g. lt - codes ) over each link .",
    "however , the system suffers a delay of about @xmath48 time - slots due to each intermediate node .",
    "indeed , at node @xmath16 , we can directly forward the @xmath49 received coded bits without delay , and then , after decoding , create and send an additional @xmath50 bits over the second channel .",
    "this straightforward scheme imposes low complexity requirements .",
    "we only need @xmath2 binary operations at each intermediate node to decode and re - encode an lt - code , and the complete decoding and re - encoding scheme has memory requirements of the order @xmath51 .",
    "moreover , lt - codes adapt to unknown channels in the sense defined previously .",
    "the complete decoding and re - encoding scheme of the previous section is adaptable , rate optimal and has low complexity .",
    "however it requires each intermediate node to store in memory the entire @xmath0 packets of information in order to re - encode .",
    "we propose a class of coding schemes , which we call _ systematic schemes _ , which minimize the memory requirement at the intermediate nodes , but require the knowledge of the erasure probabilities of the links .",
    "once again we consider the network in fig .",
    "[ fig : line ] and assume that we use a fountain code @xmath14 for link @xmath15 . in a systematic scheme , the intermediate node @xmath16 first forwards each coded bit ( packet ) from @xmath14 as they are received ; these are the systematic bits ( packets ) .",
    "meanwhile , @xmath16 forms ( about ) @xmath52 linear combinations of the systematic bits , which are transmitted in the @xmath53 time slots following the transmission of the systematic bits .",
    "thus all systematic codes will incur an average delay of @xmath50 , and will require @xmath50 memory elements .",
    "the savings in memory , as compared to the complete decoding and re - encoding , is significant when the erasure probability @xmath54 is small .    in a linear network with @xmath7 links",
    ", the same scheme can be repeated at each intermediate node .",
    "since the operation at each intermediate node is rate - optimal , it follows that for each fixed @xmath7 , the overall end - to - end transmission is also rate - optimal for large enough block length @xmath0 , while each intermediate node requires about @xmath50 memory elements and contributes a delay of @xmath55 .    below",
    "we will discuss a few possible methods to design systematic codes .",
    "here we use a fixed systematic code , consisting of @xmath0 systematic bits ( packets ) and @xmath56 parity coded bits , to transmit the information over link @xmath29 . a systematic lt - code @xcite , or a tornado code @xcite , for example ,",
    "can be used to generate the parity bits , and in fact any fixed systematic code can be used for this purpose . although not adaptable to unknown channel parameters , these codes have very low encoding and decoding complexities .",
    "tornado codes for example can be encoded and decoded with @xmath57 operations , where @xmath58 is a constant expressing the ( fixed ) rate penalty .      in this scheme ,",
    "the non - systematic packets are formed as random ( sparse ) linear combinations of the systematic ones .",
    "more precisely , whenever a new packet is received at @xmath16 , it is added to the storage space allocated to each of the non - systematic packets independently and with a ( small ) probability @xmath59 .",
    "[ t : sysiid ] with @xmath60 for , the described systematic random code asymptotically achieves the capacity over the channel @xmath29 .",
    "suppose @xmath61 systematic symbols are received at @xmath3 , and let @xmath62 .",
    "we will then wait for a further @xmath63 non - systematic symbols to be also received at @xmath3 , where @xmath64 is a constant . after eliminating the received systematic symbols , these linear combinations can be described by a random @xmath65 binary matrix , with i.i.d .",
    "entries which are nonzero with probability @xmath60 . the results of @xcite can be extended to show that , if , the probability that such a matrix is not full - rank approaches zero polynomially fast with @xmath66 . using this and the law of large of numbers then , with high probability @xmath3",
    "can retrieve all the @xmath0 symbols received at @xmath16 , e.g . by applying gaussian elimination to this sparse matrix, which can then be used to decode the fountain code @xmath14 .",
    "this code can decode the @xmath0 information symbols from an average of @xmath67 received symbols at @xmath3 , and hence this scheme rate optimal for large @xmath0 .",
    "the complexity of decoding this code is that of inverting the sparse @xmath68 matrix , which is @xmath69 .",
    "in fact , it can be shown that @xmath70 is the smallest possible value for the probability @xmath59 , and equivalently the density of the non - systematic part of the code , if the code is to be decodable with negligible overhead . in that sense",
    ", the scheme provided here offers the lowest decoding complexity for any such random code where the parity bits are chosen as linear combinations of the systematic bits with i.i.d .",
    "distribution .    [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]      in this scheme , at each time slot the intermediate node @xmath16 transmits random linear combinations ( over @xmath71 ) of all the packets it has received thus far .",
    "the main advantages of this random scheme are its adaptability and optimality in terms of delay .",
    "the drawbacks are large memory requirement , and high decoding complexity , which is @xmath72 xor operations on packets .",
    "we will need the following proposition to analyze the optimality of greedy random codes .",
    "[ p : rtriang ] given a constant @xmath64 , let @xmath12 be a ` random lower - triangular ' @xmath73 binary matrix , where the entries @xmath74 are zero for @xmath75 , and all the other entries are i.i.d .",
    "@xmath76 random variables .",
    "then @xmath77\\leq \\frac{1}{2 k^{c-1}}.\\ ] ]    let @xmath78 denote the right kernel of @xmath12 , i.e. , @xmath79 we will find the expected size of @xmath78 .",
    "let @xmath80 that is , @xmath81 is the set of vectors which have their first @xmath36-components at position @xmath8 ; then are @xmath82 such vectors .",
    "let @xmath83 denote the @xmath84th row of @xmath12 .",
    "then it is easy to verify that , for any @xmath85 , the probability that @xmath86 is one for @xmath87 , and is @xmath88 for @xmath89 .",
    "therefore the expected size of the intersection of @xmath81 and @xmath78 is @xmath90 the sets @xmath81 for @xmath91 partition @xmath92 , thus the expected size of @xmath78 is @xmath93=1+\\sum_{i=1}^k \\frac{1}{2 k^c}=1+\\frac{1}{2 k^{c-1}}.\\ ] ] now the expected size of the kernel can be used to bound the probability that @xmath12 is not full - rank : @xmath94&=\\sum_{i=0}^k \\pr[{{\\mathrm { rank}}}(a)=k - i ] 2^i\\\\      & \\geq \\pr[{{\\mathrm { rank}}}(a)=k]+2\\pr[{{\\mathrm { rank}}}(a)<k]\\end{aligned}\\ ] ] it follows that @xmath95\\leq { { \\mathbf e}}[|k|]-1 = \\frac{1}{2 k^{c-1}}.$ ]    an immediate consequence of proposition  [ p : rtriang ] is that , if the channels were noiseless , i.e. , @xmath96 , then the greedy random coding scheme described above is rate optimal ; this is because , with high probability , node @xmath3 can perform gaussian elimination on the generator matrix of the code @xmath19 , which is a random lower - triangular matrix of the type discussed in proposition  [ p : rtriang ] .",
    "a closer examination of the proof of proposition  [ p : rtriang ] reveals that , in order to make @xmath97 - 1 $ ] converge to zero , it is sufficient that , for each column @xmath8 , the matrix @xmath12 contains at least @xmath98 rows with @xmath99 random variables at the @xmath8th position ; this will then guarantee that the size of @xmath100 is no more than @xmath101 , for some @xmath64 , and we use ( [ e : ek ] ) to obtain the desired result .",
    "the interpretation of this statement in the context of our coding scheme is that , in order to be able to decode with high probability at @xmath3 , it is sufficient that for each @xmath102 , at least @xmath98 packets are successfully transmitted over @xmath29 after @xmath16 has received the @xmath8th coded packet from @xmath12 .",
    "let @xmath103 and @xmath104 denote the number of packets successfully transmitted over links @xmath15 and @xmath29 respectively .",
    "suppose now that we end transmission at a time @xmath37 when @xmath3 has received @xmath105 packets , i.e. , @xmath106 , where @xmath107 will be appropriately chosen .",
    "then the number of packets that will be received by @xmath3 after a time @xmath10 is equal to @xmath108 ; this , we would like to be at least @xmath109 .",
    "in other words , the sufficient conditions above require that at each time @xmath110 , the quantity @xmath111 be greater than @xmath112 .",
    "but @xmath113 behaves similar to a symmetric one - dimensional random walk : in fact , in @xmath34 fraction of the time slots , @xmath114 remains unchanged , while in the other @xmath35 fraction , it increases or decreases by @xmath36 with probability @xmath88 .",
    "therefore , in @xmath115 time it takes to complete transmission as described above , @xmath114 s movements are identical to @xmath41 steps of a random walk @xmath116 , where @xmath117 .",
    "straightforward calculation then shows that with @xmath118 , the probability that @xmath116 at any time @xmath119 goes below @xmath120 is polynomially small in @xmath0 .",
    "this proves that , with high probability , the @xmath0 packets of information can be retrieved at @xmath3 from @xmath121 received packets .",
    "the overhead goes to zero as @xmath0 becomes large , and hence this coding scheme is asymptotically rate optimal .",
    "in this section , we will represent a communication network of binary erasure channels as a directed acyclic graph .",
    "assume for simplicity that all edges of the graph have the same capacity @xmath122 .",
    "consider a unicast connection ; then the min - cut capacity between the source and the destination is @xmath123 for some integer @xmath124 .",
    "it is straightforward to see that if we are employing a capacity - achieving coding scheme , it is sufficient to route the information along @xmath124 parallel paths @xmath125 , where each path @xmath126 consists of @xmath127 links .",
    "we can then directly apply the coding schemes previously described to each path separately .    in practice , since coding schemes will employ codewords of finite block lengths , there might exist benefits in combining independent information streams @xcite .",
    "moreover , not all edges might be used at the same rate , for example because of cost considerations .    consider a routing scheme that observes the flow conservation principle and utilizes each edge at rate smaller or equal to its capacity .",
    "since all the component codes are linear , the received symbols along a link @xmath66 in the network can be described using an @xmath128 matrix , where @xmath129 is the number of information symbols sent along the link , and @xmath130 is the number of received symbols .",
    "the point we make in this section is that , as long as all such matrices corresponding to the intermediate links have full column rank , the end - to - end matrix that the receiver will have to decode in order to retrieve the information bits , will also be full rank and hence decodable .",
    "indeed , given the matrices associated with all individual links , to create the end - to - end matrix , we will have to perform the following types of matrix operations :    * partitioning a matrix into parts , to create the equivalent matrix corresponding to splitting an input stream to multiple outgoing streams , such as node @xmath12 in fig .",
    "[ fig : parallel ] .",
    "* multiplication of matrices , in order to create the equivalent matrix corresponding to serially concatenated channels , such as nodes @xmath16 and @xmath3 in fig .",
    "[ fig : parallel ] . * finding the direct sum of matrices , to create the equivalent matrix corresponding to merging multiple input streams of a node into a single outgoing stream , such as node @xmath131 in fig .",
    "[ fig : parallel ] .    all these operations preserve the full - rank property .",
    "thus , all coding schemes described in section  [ coding ] can be directly applied over a more general network . however , for this general case , a thorough study of the delay and memory requirements for each scheme is not provided here .",
    "in @xcite a scheme was proposed that takes advantage of intermediate node processing to approach the min - cut capacity .",
    "the authors model the departures and arrivals at nodes as poisson processes and work out the queuing - theory aspects of the problem .",
    "the employed coding scheme allows intermediate nodes to transmit random linear combinations of the incoming packets over a finite field @xmath4 .",
    "the transmission rate approaches the min - cut capacity as @xmath5 goes to infinity .",
    "this scheme , as described in @xcite , requires @xmath132 operations to encode @xmath0 symbols at the transmitter , @xmath133 operations for decoding at the receiver , and @xmath132 operations at each intermediate node .",
    "moreover , the operations are over @xmath4 that are more complex than binary operations .",
    "intermediate nodes require storage capabilities for @xmath0 packets over @xmath4 .",
    "the main benefit of the scheme in @xcite is in terms of delay as we do not decode at each intermediate node . indeed ,",
    "complete decoding and re - encoding requires a delay of @xmath50 time - slots .",
    "however , note that the scheme in @xcite achieves the min - cut rate for large @xmath5 , i.e. , assuming that we are able to send @xmath134 bits per time - slot instead of one bit per time - slot as we assume .",
    "thus in this sense it is not clear that the comparison is fair .",
    "in fact , the coding scheme employed in @xcite can be thought as employing the greedy random codes in section  [ s : greedy ] , where the linear combinations are performed over @xmath4 instead of the binary field , and where the encoding matrix is not sparse .",
    "thus our results can be viewed as an improvement over the coding scheme proposed in @xcite .",
    "in this paper we have examined the problem of communication over a line network , where processing of information at the intermediate nodes is required in order to achieve the min - cut capacity , and we have included guidelines to extend our results to general networks .",
    "we have proposed coding schemes based on fountain codes . each scheme has been analyzed and evaluated in terms of complexity , delay , memory requirement , achievable rate , and adaptability ( see table  [ tabl1 ] ) . in general , there is a trade - off between these desirable properties , and an absolute best scheme is not claimed .    8 j. blmer , r. karp and e. welzl , `` the rank of sparse random matrices over finite fields , '' _ random structures algorithms 10 _ , 1997 , pp .",
    "407 - 419 .",
    "t. cover and j. thomas , `` elements of information theory , '' _ wiley - interscience _ , new york , 1991 ."
  ],
  "abstract_text": [
    "<S> we consider a simple network , where a source and destination node are connected with a line of erasure channels . </S>",
    "<S> it is well known that in order to achieve the min - cut capacity , the intermediate nodes are required to process the information . </S>",
    "<S> we propose coding schemes for this setting , and discuss each scheme in terms of complexity , delay , achievable rate , memory requirement , and adaptability to unknown channel parameters . </S>",
    "<S> we also briefly discuss how these schemes can be extended to more general networks . </S>"
  ]
}