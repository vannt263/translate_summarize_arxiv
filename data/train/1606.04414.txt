{
  "article_text": [
    "in bayesian optimization @xcite ( bo ) , we wish to optimize a derivative - free expensive - to - evaluate function @xmath0 with feasible domain @xmath1 , @xmath2 with as few function evaluations as possible . in this paper , we assume that membership in the domain @xmath3 is easy to evaluate and we can evaluate @xmath0 only at points in @xmath3 .",
    "we assume that evaluations of @xmath0 are either noise - free , or have additive independent normally distributed noise .",
    "we consider the parallel setting , in which we perform more than one simultaneous evaluation of @xmath0 .",
    "bo typically puts a gaussian process prior distribution on the function @xmath0 , updating this prior distribution with each new observation of @xmath0 , and choosing the next point or points to evaluate by maximizing an acquisition function that quantifies the benefit of evaluating the objective as a function of where it is evaluated . in comparison with other global optimization algorithms ,",
    "bo often finds `` near optimal '' function values with fewer evaluations @xcite . as a consequence ,",
    "bo is useful when function evaluation is time - consuming , such as when training and testing complex machine learning algorithms ( e.g. deep neural networks ) or tuning algorithms on large - scale dataset ( e.g. imagenet ) @xcite .",
    "recently , bo has become popular in machine learning as it is highly effective in tuning hyperparameters of machine learning algorithms @xcite .",
    "most previous work in bo assumes that we evaluate the objective function sequentially @xcite , though a few recent papers have considered parallel evaluations @xcite .",
    "while in practice , we can often evaluate several different choices in parallel , such as multiple machines can simultaneously train the machine learning algorithm with different sets of hyperparameters . in this paper , we assume that we can access @xmath4 evaluations simultaneously at each iteration .",
    "then we develop a new parallel acquisition function to guide where to evaluate next based on the decision - theoretical analysis .    * our contributions . * we propose a novel batch bo method which measures the information gain of evaluating @xmath5 points via a new acquisition function , the parallel knowledge gradient ( @xmath6 ) .",
    "this method is derived using a decision - theoretic analysis that chooses the set of points to evaluate next that is optimal in the average - case with respect to the posterior when there is only one batch of points remaining . naively maximizing @xmath6",
    "would be extremely computationally intensive , especially when @xmath5 is large , and so , in this paper , we develop a method based on infinitesimal perturbation analysis ( ipa ) @xcite to evaluate @xmath6 s gradient efficiently , allowing its efficient optimization . in our experiments on both synthetic functions and",
    "tuning practical machine learning algorithms , @xmath6 consistently finds better function values than other parallel bo algorithms , such as parallel ei @xcite , batch ucb @xcite and parallel ucb with exploration @xcite .",
    "@xmath6 provides especially large value when function evaluations are noisy .",
    "the code in this paper is available at https://github.com/wujian16/qkg .",
    "the rest of the paper is organized as follows .",
    "section  [ sect : related ] reviews related work .",
    "section  [ sect : gaussian ] gives background on gaussian processes and defines notation used later .",
    "section  [ sect : qkg ] proposes our new acquisition function @xmath6 for batch bo .",
    "section  [ sect : practice ] provides our computationally efficient approach to maximizing @xmath6 .",
    "section  [ sect : numerical ] presents the empirical performance of @xmath6 and several benchmarks on synthetic functions and real problems .",
    "finally , section  [ sect : conclusion ] concludes the paper .",
    "[ sect : related ] within the past several years , the machine learning community has revisited bo @xcite due to its huge success in tuning hyperparameters of complex machine learning algorithms .",
    "bo algorithms consist of two components : a statistical model describing the function and an acquisition function guiding evaluations . in practice ,",
    "gaussian process ( gp ) @xcite is the mostly widely used statistical model due to its flexibility and tractability .",
    "much of the literature in bo focuses on designing good acquisition functions that reach optima with as few evaluations as possible . maximizing this acquisition function",
    "usually provides a single point to evaluate next , with common acquisition functions for sequential bayesian optimization including probability of improvement ( pi)@xcite , expected improvement ( ei ) @xcite , upper confidence bound ( ucb ) @xcite , entropy search ( es ) @xcite , and knowledge gradient ( kg ) @xcite .",
    "recently , a few papers have extended bo to the parallel setting , aiming to choose a batch of points to evaluate next in each iteration , rather than just a single point .",
    "@xcite suggests parallelizing ei by iteratively constructing a batch , in each iteration adding the point with maximal single - evaluation ei averaged over the posterior distribution of previously selected points .",
    "@xcite also proposes an algorithm called  constant liar \" , which iteratively constructs a batch of points to sample by maximizing single - evaluation while pretending that points previously added to the batch have already returned values .",
    "there are also work extending ucb to the parallel setting .",
    "@xcite proposes the gp - bucb policy , which selects points sequentially by a ucb criterion until filling the batch .",
    "each time one point is selected , the algorithm updates the kernel function while keeping the mean function fixed .",
    "@xcite proposes an algorithm combining ucb with pure exploration , called gp - ucb - pe . in this algorithm , the first point is selected according to a ucb criterion",
    "; then the remaining points are selected to encourage the diversity of the batch .",
    "these two algorithms extending ucb do not require monte carlo sampling , making them fast and scalable .",
    "however , ucb criteria are usually designed to minimize cumulative regret rather than immediate regret , causing these methods to underperform in bo , where we wish to minimize simple regret .",
    "the parallel methods above construct the batch of points in an iterative greedy fashion , optimizing some single - evaluation acquisition function while holding the other points in the batch fixed .",
    "the acquisition function we propose considers the batch of points collectively , and we choose the batch to jointly optimize this acquisition function . other recent papers that value points collectively include @xcite which optimizes the parallel ei by a closed - form formula , @xcite , in which gradient - based methods are proposed to jointly optimize a parallel ei criterion , and @xcite , which proposes a parallel version of the es algorithm and uses monte carlo sampling to optimize the parallel es acquisition function .",
    "we compare against methods from a number of these previous papers in our numerical experiments , and demonstrate that we provide an improvement , especially in problems with noisy evaluations .",
    "our method is also closely related to the knowledge gradient ( kg ) method @xcite for the non - batch ( sequential ) setting , which chooses the bayes - optimal point to evaluate if only one iteration is left @xcite , and the final solution that we choose is not restricted to be one of the points we evaluate .",
    "( expected improvement is bayes - optimal if the solution is restricted to be one of the points we evaluate . )",
    "we go beyond this previous work in two aspects .",
    "first , we generalize to the parallel setting .",
    "second , while the sequential setting allows evaluating the kg acquisition function exactly , evaluation requires monte carlo in the parallel setting , and so we develop more sophisticated computational techniques to optimize our acquisition function .",
    "recently , @xcite studies a nested batch knowledge gradient policy . however , they optimize over a finite discrete feasible set , where the gradient of kg does not exist . as a result ,",
    "their computation of kg is much less efficient than ours .",
    "moreover , they focus on a nesting structure from materials science not present in our setting .",
    "in this section , we state our prior on @xmath0 , briefly discuss well known results about gaussian processes ( gp ) , and introduce notation used later .",
    "we put a gaussian process prior over the function @xmath7 , which is specified by its mean function @xmath8 and kernel function @xmath9 .",
    "we assume either exact or independent normally distributed measurement errors , i.e. the evaluation @xmath10 at point @xmath11 satisfies @xmath12 where @xmath13 is a known function describing the variance of the measurement errors .",
    "if @xmath14 is not known , we can also estimate it as we do in section  [ sect : numerical ] .    supposing we have measured @xmath0 at @xmath15 points @xmath16 and obtained corresponding measurements @xmath17 , we can then combine these observed function values with our prior to obtain a posterior distribution on @xmath0 .",
    "this posterior distribution is still a gaussian process with the mean function @xmath18 and the kernel function @xmath19 as follows @xmath20",
    "in this section , we propose a novel parallel bayesian optimization algorithm by generalizing the concept of the knowledge gradient from @xcite to the parallel setting .",
    "the knowledge gradient policy in @xcite for discrete @xmath3 chooses the next sampling decision by maximizing the expected incremental value of a measurement , without assuming ( as expected improvement does ) that the point returned as the optimum must be a previously sampled point .",
    "we now show how to compute this expected incremental value of an additional iteration in the parallel setting .",
    "suppose that we have observed @xmath15 function values .",
    "if we were to stop measuring now , @xmath21 would be the minimum of the predictor of the gp .",
    "if instead we took one more batch of samples , @xmath22 would be the minimum of the predictor of the gp .",
    "the difference between these quantities , @xmath23 , is the increment in expected solution quality ( given the posterior after @xmath24 samples ) that results from the additional batch of samples .",
    "this increment in solution quality is random given the posterior after @xmath15 samples , because @xmath22 is itself a random vector due to its dependence on the outcome of the samples .",
    "we can compute the probability distribution of this difference ( with more details given below ) , and the @xmath6 algorithm values the sampling decision @xmath25 according to its expected value , which we call the parallel knowledge gradient factor , and indicate it using the notation @xmath6 .",
    "formally , we define the @xmath6 factor for a set of candidate points to sample @xmath26 as @xmath27 , \\label{eq : multikg}\\ ] ] where @xmath28 : = \\mathbb{e } \\left[\\cdot |{\\bm{x}^{(1:n ) } } , { y^{(1:n ) } } \\right]$ ] is the expectation taken with respect to the posterior distribution after @xmath15 evaluations .",
    "then we choose to evaluate the next batch of @xmath5 points that maximizes the parallel knowledge gradient , @xmath29    by construction , the parallel knowledge gradient policy is bayes - optimal for minimizing the minimum of the predictor of the gp if only one decision is remaining .",
    "the @xmath6 algorithm will reduce to the parallel ei algorithm if function evaluations are noise - free and the final recommendation is restricted to the previous sampling decisions . because under the two conditions above , the increment in expected solution quality will become @xmath30 which is exactly the parallel ei acquisition function .",
    "however , computing @xmath6 and its gradient is very expensive .",
    "we will address the computational issues in section  [ sect : practice ] .",
    "the full description of the @xmath6 algorithm is summarized as follows .",
    "the number of initial stage samples @xmath31 , and the number of main stage sampling iterations @xmath32 .",
    "initial stage : draw @xmath31 initial samples from a latin hypercube design in @xmath3 , @xmath33 for @xmath34 .",
    "main stange : solve ( [ eqn : innerkg ] ) , i.e. get @xmath35 sample these points @xmath36 , re - train the hyperparameters of the gp by mle , and update the posterior distribution of @xmath0 .",
    "in this section , we provide the strategy to maximize @xmath6 by a gradient - based optimizer .",
    "in section  [ sect : value ] and section  [ sect : gradient ] , we describe how to compute @xmath6 and its gradient when @xmath3 is finite in ( [ eq : multikg ] ) .",
    "section  [ sect : discrete ] describes an effective way to discretize @xmath3 in ( [ eq : multikg ] ) .",
    "the readers should note that there are two @xmath3s here , one is in ( [ eq : multikg ] ) which is used to compute the @xmath6 factor given a sampling decision @xmath26 .",
    "the other is the feasible domain in ( [ eqn : innerkg ] ) ( @xmath38 ) that we optimize over .",
    "we are discretizing the first @xmath3 .",
    "following @xcite , we express @xmath39 as @xmath40 because @xmath41 is normally distributed with zero mean and covariance matrix @xmath42 with respect to the posterior after @xmath15 observations , we can rewrite @xmath39 as @xmath43 where @xmath44 is a standard @xmath5-dimensional normal random vector , and @xmath45 where @xmath46 is the cholesky factor of the covariance matrix @xmath42 .",
    "now we can compute the @xmath6 factor using monte carlo sampling when @xmath3 is finite : we can sample @xmath44 , compute ( [ eqn : alternative ] ) , then plug in ( [ eq : multikg ] ) , repeat many times and take average .      in this section",
    ", we propose an unbiased estimator of the gradient of @xmath6 using ipa when @xmath3 is finite .",
    "accessing a stochastic gradient makes optimization much easier . by ( [ eqn : alternative ] )",
    ", we express @xmath6 as @xmath47 where @xmath48 . under the condition that @xmath49 and @xmath50 are continuously differentiable",
    ", one can show that ( please see the details in the supplementary materials ) @xmath51 where @xmath52 is the @xmath53th dimension of the @xmath54th point in @xmath26 . by the formula of @xmath55 , @xmath56 where @xmath57 , @xmath58 , and @xmath59 now we can sample many times and take average to estimate the gradient of @xmath6 via ( [ eqn : qkg_grad ] ) .",
    "this technique is called infinitesimal perturbation analysis ( ipa ) in gradient estimation @xcite . since we can estimate the gradient of @xmath6 efficiently when @xmath3 is finite",
    ", we will apply some standard gradient - based optimization algorithms , such as multi - start stochastic gradient ascent to maximize @xmath6 .",
    "we have specified how to maximize @xmath6 when @xmath3 is finite in ( [ eq : multikg ] ) , but usually @xmath3 is infinite . in this case , we will discretize @xmath3 to approximate @xmath6 , and then maximize over the approximate @xmath6 .",
    "the discretization itself is an interesting research topic @xcite .    in this paper , the discrete set  @xmath60",
    "is not chosen statically , but evolves over time : specifically , we suggest drawing  @xmath61 samples from the global optima of the posterior distribution of the gaussian process ( please refer to  @xcite for a description of this technique ) .",
    "this sample set , denoted by  @xmath62 , is then extended by the locations of previously sampled points  @xmath63 and the set of candidate points  @xmath26 .",
    "then ( [ eq : multikg ] ) can be restated as @xmath64,\\end{aligned}\\ ] ] where @xmath65 . for the experimental evaluation",
    "we recompute @xmath62 in every iteration after updating the posterior of the gaussian process .",
    "we conduct experiments in two different settings : the noise - free setting and the noisy setting . in both settings ,",
    "we test the algorithms on well - known synthetic functions chosen from @xcite and practical problems . following previous literature @xcite",
    ", we use a constant mean prior and the ard mat@xmath66rn @xmath67 kernel . in the noisy setting , we assume that @xmath68 is constant across the domain @xmath3 , and we estimate it together with other hyperparameters in the gp using maximum likelihood estimation ( mle ) .",
    "we set @xmath69 to discretize the domain following the strategy in section  [ sect : discrete ] . in general",
    ", the @xmath6 algorithm performs as well or better than state - of - art benchmark algorithms on both synthetic and real problems .",
    "it performs especially well in the noisy setting .    before describing the details of the empirical results , we highlight the implementation details of our method and the open - source implementations of the benchmark methods .",
    "our implementation inherits the open - source implementation of parallel ei from the ` metrics optimization engine `  @xcite , which is fully implemented in ` c++ ` with a python interface .",
    "we reuse their gp regression and gp hyperparameter fitting methods and implement the @xmath6 method in ` c++ ` . besides comparing to parallel ei in @xcite",
    ", we also compare our method to a well - known heuristic parallel ei implemented in ` spearmint `   @xcite , the parallel ucb algorithm ( gp - bucb ) and parallel ucb with pure exploration ( gp - ucb - pe ) both implemented in ` gpoptimization `  @xcite .      in this section ,",
    "we focus our attention on the noise - free setting , in which we can evaluate the objective exactly .",
    "we show that parallel knowledge gradient outperforms or is competitive with state - of - art benchmarks on several well - known test functions and tuning practical machine learning algorithms .",
    "first , we test our algorithm along with the benchmarks on 4 well - known synthetic test functions : branin2 on the domain @xmath70 ^ 2 $ ] , rosenbrock3 on the domain @xmath71 ^ 3 $ ] , ackley5 on the domain @xmath71 ^ 5 $ ] , and hartmann6 on the domain @xmath72 ^ 6 $ ] .",
    "we initiate our algorithms by randomly sampling @xmath73 points from a latin hypercube design , where @xmath74 is the dimension of the problem .",
    "figure  [ fig : syn ] reports the mean and the standard deviation of the base 10 logarithm of the immediate regret by running @xmath75 random initializations with batch size @xmath76 .",
    "the results show that @xmath6 is significantly better on rosenbrock3 , ackley5 and hartmann6 , and is slightly worse than the best of the other benchmarks on branin2 . especially on rosenbrock3 and ackley5",
    ", @xmath6 makes dramatic progress in early iterations .",
    "[ fig : syn ]      in this section , we test the algorithms on two practical problems : tuning logistic regression on the mnist dataset and tuning cnn on the cifar10 dataset .",
    "we set the batch size to @xmath76 .",
    "first , we tune logistic regression on the mnist dataset .",
    "this task is to classify handwritten digits from images , and is a @xmath77-class classification problem .",
    "we train logistic regression on a training set with @xmath78 instances with a given set of hyperparameters and test it on a test set with @xmath79 instances .",
    "we tune @xmath80 hyperparameters : mini batch size from @xmath77 to @xmath81 , training iterations from @xmath75 to @xmath79 , the @xmath82 regularization parameter from @xmath83 to @xmath84 , and learning rate from @xmath83 to @xmath84 .",
    "we report the mean and standard deviation of the test error for @xmath85 independent runs . from the results",
    ", one can see that both algorithms are making progress at the initial stage while @xmath6 can maintain this progress for longer and results in a better algorithm configuration in general .",
    ", width=604,height=226 ]    [ fig : uncertain ]    in the second experiment , we tune a cnn on cifar10 dataset .",
    "this is also a @xmath77-class classification problem .",
    "we train the cnn on the @xmath86 training data with certain hyperparameters and test it on the test set with @xmath79 instances . for the network architecture , we choose the one in ` tensorflow ` tutorial .",
    "it consists of @xmath87 convolutional layers , @xmath87 fully connected layers , and on top of them is a softmax layer for final classification .",
    "we tune totally @xmath88 hyperparameters : the mini batch size from @xmath77 to @xmath89 , training epoch from @xmath84 to @xmath77 , the @xmath82 regularization parameter from @xmath83 to @xmath84 , learning rate from @xmath83 to @xmath84 , the kernel size from @xmath87 to @xmath77 , the number of channels in convolutional layers from @xmath77 to @xmath89 , the number of hidden units in fully connected layers from @xmath75 to @xmath89 , and the dropout rate from @xmath83 to @xmath84 .",
    "we report the mean and standard deviation of the test error for @xmath90 independent runs . in this example , the @xmath6 is making better ( more aggressive ) progress than parallel ei even in the initial stage and maintain this advantage to the end .",
    "this architecture has been carefully tuned by the human expert , and achieve a test error around @xmath91 , and our automatic algorithm improves it to around @xmath92 .      in this section , we study problems with noisy function evaluations .",
    "our results show that the performance gains over benchmark algorithms from @xmath6 evident in the noise - free setting are even larger in the noisy setting .",
    "we test on the same @xmath80 synthetic functions from the noise - free setting , and add independent gaussian noise with standard deviation @xmath93 to the function evaluation . the algorithms are not given this standard deviation , and must learn it from data .",
    "[ fig : syn ]    the results in figure  [ fig : uncertain ] show that @xmath6 is consistently better than or at least competitive with all competing methods .",
    "also observe that the performance advantage of @xmath6 is larger than for noise - free problems .",
    "testing on a large test set such as imagenet is slow , especially when we must test many times for different hyperparameters . to speed up hyperparameter tuning , we may instead test the algorithm on a subset of the testing data to approximate the test error on the full set .",
    "we study the performance of our algorithm and benchmarks in this scenario , focusing on tuning logistic regression on mnist .",
    "we train logistic regression on the full training set of @xmath94 , but we test the algorithm by testing on @xmath95 randomly selected samples from the test set , which provides a noisy approximation of the test error on the full test set .    ,",
    "width=264,height=302 ]    [ fig : uncertain ]    we report the mean and standard deviation of the test error on the full set using the hyperparameters recommended by each parallel bo algorithm for @xmath85 independent runs .",
    "the result shows that @xmath6 is better than both versions of parallel ei , and its final test error is close to the noise - free test error ( which is substantially more expensive to obtain ) .",
    "as we saw with synthetic test functions , @xmath6 s performance advantage in the noisy setting is wider than in the noise - free setting .",
    "the authors were partially supported by nsf career cmmi-1254298 , nsf cmmi-1536895 , nsf iis-1247696 , afosr fa9550 - 12 - 1 - 0200 , afosr fa9550 - 15 - 1 - 0038 , and afosr fa9550 - 16 - 1 - 0046 .",
    "in this paper , we introduce a novel batch bayesian optimization method @xmath6 , derived from a decision - theoretical perspective , and develop a computational method to implement it efficiently .",
    "we show that @xmath6 outperforms or is competitive with the state - of - art benchmark algorithms on several synthetic functions and in tuning practical machine learning algorithms .    * supplementary material *",
    "the ( [ eqn : innerkg ] ) corresponds to the synchronous @xmath6 optimization , in which we wait for all @xmath5 points from our previous batch to finish before searching for a new batch of @xmath5 points .",
    "however , in some applications , we may wish to generate a new batch of points to evaluate next while @xmath96 points are still being evaluated , before we have their values .",
    "this is common in training machine learning algorithms , where different machine learning models do not necessarily finish at the same time .",
    "@xmath29    we can generalize ( [ eqn : innerkg ] ) to the asynchronous @xmath6 optimization .",
    "given that @xmath97 points are still under evaluation , now we would like to recommend a batch of @xmath5 points to evaluate . as we did for the synchronous @xmath6 optimization above",
    ", now we estimate the @xmath6 of the combined @xmath98 points only with respect to the @xmath5 points that we need to recommend",
    ". then we proceed the same way via gradient - based algorithms .",
    "next , we compare @xmath6 at different levels of parallelism against the fully sequential kg algorithm .",
    "we test the algorithms with different batch sizes on two noisy synthetic functions branin2 and hartmann6 , whose standard deviation of the noise is @xmath99 . from the results ,",
    "our parallel knowledge gradient method does provide a speed - up as @xmath5 goes up .",
    "[ fig : syn ]",
    "recall that in section 5 of the main document , we have expressed the @xmath6 factor as follows , @xmath100 where the expectation is taken over @xmath101 and @xmath102 the main purpose of this section is to prove the following proposition .",
    "when @xmath3 is finite , under the condition that @xmath49 and @xmath50 are continuous differentiable , @xmath103 where @xmath104 , @xmath105 , @xmath52 is the @xmath53th dimension of the @xmath54th point in @xmath26 and @xmath106 .    without loss of generality",
    ", we assume that ( 1 ) @xmath54 and @xmath53 are fixed in advance and ( 2 ) @xmath107^d$ ] , we would like to prove that ( [ eqn : qkg_grad_interchange ] ) is correct . before proceeding ,",
    "we define one more notation @xmath108 where @xmath26 equals to @xmath109 component - wise except for @xmath52 . to prove it",
    ", we cite theorem 1 in @xcite , which requires three conditions to make ( [ eqn : qkg_grad_interchange ] ) valid : there exists an open neighborhood @xmath110 of @xmath111 where @xmath111 is the @xmath53th dimension of @xmath54th point in @xmath109 such that ( i ) @xmath112 is continuous in @xmath113 for any fixed @xmath3 and @xmath44 , ( ii ) @xmath112 is differentiable except on a denumerable set in @xmath113 for any given @xmath3 and @xmath44 , ( iii ) the derivative of @xmath112 ( when it exists ) is uniformly bounded by @xmath114 for all @xmath115 , and the expectation of @xmath114 is finite .      under the condition that the mean function @xmath49 and the kernel function @xmath50 are continuous differentiable",
    ", we see that for any given @xmath116 , @xmath117 is continuous differentiable in @xmath26 by the result that the multiplication , the inverse ( when the inverse exists ) and the cholesky operators @xcite preserve continuous differentiability .",
    "when @xmath118 is finite , we see that @xmath119 is continuous in @xmath26 .",
    "then @xmath112 is also continuous in @xmath52 by the definition of the function @xmath112 .      by the expression that @xmath120 , if both @xmath121 and @xmath122 are unique , then @xmath112 is differentiable at @xmath52 .",
    "we define @xmath123 to be the set that @xmath112 is not differentiable , then we see that @xmath124 where @xmath125 .",
    "@xmath126 depend on @xmath52 if @xmath127 ( @xmath128 ) where @xmath129 is the @xmath54th point of @xmath26 . as @xmath3 is finite ,",
    "we only need to show that @xmath130 and @xmath131 is denumerable .    defining @xmath132 on @xmath113",
    ", one can see that @xmath133 is continuous differentiable on @xmath113 .",
    "we would like to show that @xmath134 is denumerable . to prove it",
    ", we will show that @xmath135 contains only isolated points .",
    "then one can use a theorem in real analysis : any set of isolated points in @xmath136 is denumerable ( see the proof of statement 4.2.25 on page 165 in @xcite ) . to prove that @xmath135 only contains isolated points",
    ", we use the definition of an isolated point : @xmath137 is an isolated point of @xmath135 if and only if @xmath138 is not a limit point of @xmath135 .",
    "we will prove by contradiction , suppose that @xmath137 is a limit point of @xmath135 , then it means that there exists a sequence of points @xmath139 all belong to @xmath135 such that @xmath140 .",
    "however , by the definition of derivative and @xmath141 , @xmath142 , a contradiction .",
    "so we conclude that @xmath135 only contains isolated points , so is denumerable .",
    "defining @xmath143 on @xmath113 , @xmath144 is also continuous differentiable on @xmath113 , then one can similarly prove that @xmath145 is denumerable .      recall that from section 5 of the main document , @xmath146 where @xmath57 , @xmath147 , and @xmath148 we can calculate the @xmath149 as follows @xmath150 using the fact that @xmath49 is continuously differentiable and @xmath3 is compact , then @xmath149 is bounded by some @xmath151 . by the result that @xmath152 is continous ,",
    "it is bounded by a vector @xmath153 as @xmath3 is compact .",
    "then @xmath154 where @xmath155 . and @xmath156 .",
    "in this section , we will prove that sga converges to a stationary point .",
    "we follow the same idea of proving the theorem 2 in @xcite .",
    "first , it requires the step size @xmath157 satisfying @xmath158 as @xmath159 , @xmath160 and @xmath161 .",
    "second , it requires the second moment of the gradient estimator is finite . in the above section 1.3 ,",
    "we have show that @xmath162 , then @xmath163 ."
  ],
  "abstract_text": [
    "<S> in many applications of black - box optimization , one can evaluate multiple points simultaneously , e.g. when evaluating the performances of several different neural networks in a parallel computing environment . in this paper </S>",
    "<S> , we develop a novel batch bayesian optimization algorithm  the parallel knowledge gradient method . by construction </S>",
    "<S> , this method provides the one - step bayes optimal batch of points to sample . </S>",
    "<S> we provide an efficient strategy for computing this bayes - optimal batch of points , and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms , especially when function evaluations are noisy . </S>"
  ]
}