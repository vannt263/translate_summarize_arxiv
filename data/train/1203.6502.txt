{
  "article_text": [
    "inferring causal relations is among the most important scientific goals since causality , as opposed to mere statistical dependencies , provides the basis for reasonable human decisions . during the past decade ,",
    "it has become popular to phrase causal relations in directed acyclic graphs ( dags ) @xcite with random variables ( formalizing statistical quantities after repeated observations ) as nodes and causal influences as arrows .",
    "we briefly explain this formal setting . here and throughout the paper , we assume causal sufficiency , that is , there are no hidden variables that influence more than one of the @xmath0 observed variables .",
    "let @xmath3 be a causal dag with nodes @xmath4 where @xmath5 means that @xmath6 influences @xmath7 `` directly '' in the sense that intervening on @xmath6 changes the distribution of @xmath7 even if all other variables are held constant ( also by interventions ) . to simplify notation",
    ", we will mostly assume the @xmath7 to be discrete .",
    "@xmath8 denotes the probability mass function of the joint distribution @xmath9 .",
    "according to the causal markov condition @xcite , which we take for granted in this paper , every node @xmath7 is conditionally independent of its nondescendants , given its parents with respect to the causal dag @xmath3 .",
    "if @xmath10 denotes the set of parent variables of @xmath7 ( i.e. , its direct causes ) in @xmath3 , the joint probability thus factorizes @xcite into @xmath11 where @xmath12 denotes the values of @xmath10 . by slightly abusing the notion of conditional probabilities , we assume that @xmath13 is also defined for those @xmath12 with @xmath14 . in other words , we know how the causal mechanisms act on potential combinations of values of the parents that never occur",
    ". note that this assumption has implications because such causal conditionals can not be learned from observational data even if the causal dag is known .",
    "given this formalism , why define causal strength ?",
    "after all , the dag together with the causal conditionals contain the complete causal information : one can easily compute how the joint distribution changes when an external intervention sets some of the variables to specific values @xcite .",
    "however , describing causal relations in nature with a dag always requires first deciding how detailed the description should be .",
    "depending on the desired precision , one may want to account for some weak causal links or not .",
    "thus , an objective measure distinguishing weak arrows from strong ones is required .",
    "we discuss some definitions of causal strength that are either known or just come up as straightforward ideas .",
    "_ average causal effect _ : following @xcite , @xmath15 denotes the distribution of @xmath16 when @xmath17 is set to the value @xmath18 [ it will be introduced more formally in equation  ( [ do ] ) ] .",
    "note that it only coincides with the usual conditional distribution @xmath19 if the statistical dependence between @xmath17 and @xmath16 is due to a direct influence of @xmath17 on @xmath16 , with no confounding common cause .",
    "if all @xmath6 are binary variables , causal strength can then be quantified by the average causal effect @xcite @xmath20 if a real - valued variable @xmath7 is affected by a binary variable @xmath6 , one considers the shift of the mean of @xmath7 that is caused by switching @xmath6 from @xmath21 to @xmath22 .",
    "formally , one considers the difference @xcite @xmath23 this measure only accounts for the linear aspect of an interaction since it does not reflect whether @xmath6 changes higher order moments of the distribution of @xmath7 .    _ analysis of variance anova _ : let @xmath6 be caused by @xmath24 .",
    "the variance of @xmath6 can formally be split into the average of the variances of @xmath6 , given @xmath25 with @xmath26 , and the variance of the expectations of @xmath6 , given  @xmath25 : @xmath27 in the common scenario of drug testing experiments , for instance , the first term in equation ( [ eq2 ] ) is given by the variability of @xmath6 within a group of equal treatments ( i.e. , fixed @xmath28 ) , while the second one describes how much the means of @xmath6 vary between different treatments .",
    "it is tempting to say that the latter describes the part of the total variation of @xmath6 that is _ caused by _ the variation of @xmath25 , but this is conceptually wrong for nonlinear influences and if there are statistical dependencies between @xmath25 and the other parents of @xmath6 @xcite .    for linear structural equations , @xmath29 and additionally assuming @xmath25 to be independent of the other parents of @xmath6 , the second term is given by @xmath30 , which indeed describes the amount by which the variance of @xmath6 decreases when @xmath25 is set to a fixed value by intervention .",
    "in this sense , @xmath31 is indeed the fraction of the variance of @xmath6 that is _ caused by _ @xmath25 . by rescaling all @xmath7 such that @xmath32 , we have @xmath33",
    ". then , the square of the structure coefficients itself can be seen as a simple measure for causal strength .",
    "_ conditional mutual information _ : the information of @xmath17 on @xmath16 or vice versa is given by @xcite @xmath34 the information of @xmath17 on @xmath16 or vice versa if @xmath35 is given is defined by @xcite @xmath36 there are situations where these expressions ( with @xmath35 describing some background condition ) can indeed be interpreted as measuring the strength of the arrow . an essential part of this paper describes the conditions where this makes sense and how to replace the expressions with other information - theoretic ones when it does not .    _ granger causality / transfer entropy / directed information _ :",
    "quantifyingcausal influence between time series [ e.g. between @xmath37 and @xmath38 is special because one is interested in quantifying the effect of all @xmath39 on all @xmath40 .",
    "if we represent the causal relations by a dag where every time instant defines a separate pair of variables , then we ask for the strength of a _ set of arrows_. if @xmath41 and @xmath42 are considered as instances of the variables @xmath43 , we leave the regime of i.i.d . sampling .",
    "measuring the reduction of uncertainty in one variable after knowing another is also a key idea in several related methods for quantifying causal strength in time series .",
    "granger causality in its original formulation uses reduction of variance @xcite .",
    "nonlinear information - theoretic extensions in the same spirit are transfer entropy  @xcite and directed information @xcite . both are essentially based on conditional mutual information , where each variable @xmath44 in ( [ cmi ] ) is replaced with an appropriate set of variables .",
    "_ information flow _ : since the above measures quantify dependencies rather than causality , several authors have defined causal strength by replacing the observed probability distribution with distributions that arise after interventions ( computed via the causal dag ) . @xcite defined information flow via an operation , `` source exclusion '' , which removes the influence of a variable in a network .",
    "@xcite defined a different notion of information flow explicitly via pearl s @xmath45-calculus .",
    "both measures are close to ours in spirit and in fact the version in @xcite coincides with ours when quantifying the strength of a single arrow .",
    "however , both do not satisfy our postulates .",
    "_ mediation analysis _ : @xcite explore how to separate the influence of @xmath17 on @xmath16 into parts that can be attributed to specific paths by `` blocking '' other paths .",
    "consider , for instance , the case where @xmath17 influences @xmath16 directly and indirectly via @xmath46 . to test its direct influence , one changes @xmath17 from some `` reference '' value @xmath47 to an `` active '' value @xmath18 while keeping the distribution of @xmath35 that either corresponds to the reference value @xmath47 or to the natural distribution @xmath48",
    ". a natural distinction between a reference state and an active state occurs , for instance , in drug testing scenario where taking the drug means switching from reference to active .",
    "in contrast , our goal is not to study the impact of one specific switching from @xmath47 to @xmath18 .",
    "instead , we want to construct a measure that quantifies the direct effect of the variable @xmath17 on @xmath16 , while treating all possible values of @xmath17 in the same way .",
    "nevertheless , there are interesting relation between these approaches and ours that we briefly discuss at the end of section  [ subsecse ] .",
    "let us first discuss the properties we expect a measure of causal strength to have .",
    "the key idea is that causal strength is supposed to measure the impact of an intervention that removes the respective arrows .",
    "we present five properties that we consider reasonable .",
    "let @xmath49 denote the strength of the arrows in set @xmath50 . by slightly overloading notation , we write @xmath51 instead of @xmath52 .",
    "_ causal markov condition _ : if @xmath53 , then the joint distribution satisfies the markov condition with respect to the dag @xmath54 obtained by removing the arrows in @xmath50 .    _ mutual information _ : if the true causal dag reads @xmath55 , then @xmath56    _ locality _ : the strength of @xmath57 only depends on ( 1 ) how @xmath16 depends on @xmath17 and its other parents , and ( 2 ) the joint distribution of all parents of @xmath16 .",
    "formally , knowing @xmath58 and @xmath59 is sufficient to compute @xmath51 . for strictly positive densities ,",
    "this is equivalent to knowing @xmath60 .    _",
    "quantitative causal markov condition _ : if there is an arrow from @xmath17 to @xmath16 , then the causal influence of @xmath17 on @xmath16 is greater than or equal to the conditional mutual information between @xmath16 and @xmath17 given all the other parents of @xmath16 .",
    "formally @xmath61",
    "_ heredity _ : if the causal influence of a set of arrows is zero , then the causal influence of all its subsets ( in particular , individual arrows ) is also zero .",
    "@xmath62    note that we do not claim that _ every _ reasonable measure of causal strength should satisfy these postulates , but we now explain why we consider them natural and show that the postulates make sense for simple dags .",
    "p0 : if the purpose of our measure of causal strength is to quantify relevance of arrows , then removing a set of arrows with zero strength must make no difference . if , for instance , @xmath63 , removing @xmath55 should not yield a dag that is ruled out by the causal markov condition .",
    "we should emphasize that @xmath49 can be nonzero even if @xmath50 consists of arrows each individually having zero strength .",
    "p1 : the mutual information actually measures the strength of statistical dependencies . since all these dependencies are generated by the influence of @xmath17 on @xmath16 ( and not by a common cause or @xmath16 influencing @xmath17 ) , it makes sense to measure causal strength by strength of dependencies .",
    "note that mutual information @xmath64 also quantifies the variability in @xmath16 that is due to the variability in @xmath17 , see also section  [ scontrol ] .",
    "_ mutual information versus channel capacity .",
    "_ given the premise that causal strength should be an information - like quantity , a natural alternative to mutual information is the capacity of the information channel @xmath65 , that is , the maximum over all values of mutual information @xmath66 for all input distributions @xmath67 of @xmath17 when keeping the conditional @xmath68 .",
    "while mutual information @xmath69 quantifies the observable dependencies , channel capacity quantifies the strength of the strongest dependencies that can be generated using the information channel @xmath68 . in this sense",
    ", @xmath69 quantifies the _ factual _ causal influence , while channel capacity measures the _ potential _ influence .",
    "channel capacity also accounts for the impact of setting @xmath18 to values that rarely or never occur in the observations .",
    "however , this sensitivity regarding effects of rare inputs can certainly be a problem for estimating the effect from sparse data .",
    "we therefore prefer mutual information @xmath69 as it better assesses the extent to which _ frequently observed changes _ in @xmath17 influence @xmath16 .",
    "p2 : locality implies that we can ignore causes of @xmath17 when computing @xmath51 , unless they are at the same time direct causes of @xmath16 .",
    "likewise , other effects of @xmath16 are irrelevant . moreover",
    ", it does not matter _ how _ the dependencies between the parents are generated ( which parent influences which one or whether they are effects of a common cause ) , we only need to know their joint distribution with @xmath17 .",
    "violations of locality have paradoxical implications .",
    "assume , for example , variable @xmath35 would be relevant in dag [ dagsobv](a ) .",
    "then , @xmath51 would depend on the mechanism that generates the distribution of @xmath17 , while we are actually concerned with the information flowing from @xmath17 to @xmath16 instead of that flowing _ to _",
    "@xmath17 from other nodes .",
    "likewise , [ see dags [ dagsobv](b ) and [ dagsobv](c ) ] it is irrelevant whether @xmath17 and @xmath16 have further effects .    .",
    "for we will obtain @xmath70 .",
    "the nodes @xmath17 and @xmath16 are shaded because they are source and target of the arrow @xmath55 , respectively . ]",
    "p3 : to justify the name of this postulate , observe that the restriction of p0 to the single arrow case @xmath71 is equivalent to @xmath72 to see this , we use the ordered markov condition @xcite , theorem  1.2.6 , which is known to be equivalent to the markov condition mentioned in the .",
    "it states that every node is conditionally independent of its predecessors ( according to some ordering consistent with the dag ) , given its parents . if @xmath73 denotes the predecessors of @xmath16 for some ordering that is consistent with @xmath3 and @xmath54 , the ordered markov condition for @xmath54 holds iff @xmath74 since the conditions for all other nodes remain the same as in @xmath3 . due to the semi - graphoid axioms ( weak union and contraction rule @xcite ) , ( [ omc ] ) is equivalent to @xmath75 since the condition on the left",
    "is guaranteed by the markov condition on @xmath3 , the markov condition on @xmath54 is equivalent to @xmath76 .    in words ,",
    "the arrow @xmath55 is the only reason for the conditional dependence @xmath77 to be nonzero , hence it is natural to postulate that its strength can not be smaller than the dependence that it generates .",
    "section  [ secproperties ] explains why we should not postulate equality .",
    "p4 : the postulate provides a compatibility condition :",
    "if a set of arrows has zero causal influence , and so can be eliminated without affecting the causal dag , then the same should hold for all subsets of that set .",
    "we refer to this as the heredity property by analogy with matroid theory , where heredity implies that every subset of an independent set is independent .",
    "our definition of causal strength is presented in section  [ secdef ] .",
    "this section discusses problems with alternate measures of causal strength .",
    "the first two measures are ruled out by p0 .",
    "consider a relation between three binary variables @xmath44 , where @xmath78 with @xmath17 and @xmath35 being unbiased and independent . then changing @xmath17 has no influence on the statistics of @xmath16 . likewise , knowing @xmath17 does not reduce the variance of @xmath16 . to satisfy p0 , we need modifications that account for the fact that we do observe an influence of @xmath17 on @xmath16 for each fixed value @xmath79 although this influence becomes invisible after marginalizing over @xmath35 .",
    "it suffices to consider a few simple dags to illustrate why mutual information and conditional mutual information are _ not _ suitable measures of causal strength in general . _",
    "mutual information is not suitable in figure  _ [ dags](a ) .",
    "it is clear that @xmath69 is inappropriate because we can obtain @xmath80 even when the arrow @xmath55 is missing , due to the common cause @xmath35 .",
    "is challenging . ]    _ conditional mutual information is not suitable for figure _  [ dags](a ) .",
    "consider the limiting case where the direct influence @xmath81 gets weaker until it almost disappears ( @xmath82 ) .",
    "then the behavior of the system ( observationally and interventionally ) is approximately described by the dag  [ dagsobv](a ) .",
    "using @xmath83 makes no sense in this scenario since , for example , @xmath17 may be obtained from @xmath35 by a simple copy operation , in which case @xmath84 necessarily , even when @xmath17 influences @xmath16 strongly .",
    "transfer entropy @xcite is intended to measure the influence of one time - series on another one .",
    "let @xmath85 be a bivariate stochastic process where @xmath41 influence some @xmath86 with @xmath87 , see figure  [ figten ] , left .",
    "then transfer entropy is defined as the following conditional mutual information : @xmath88 } \\rightarrow y_{t } |y_{(-\\infty , t-1]}):= i(x_{(-\\infty , t-1 ] } ; y_{t } |y_{(-\\infty , t-1]}).\\ ] ] it measures the amount of information the past of @xmath17 provides about the present of @xmath16 given the past of @xmath16 .",
    "to quantify causal influence by conditional information relevance is also in the spirit of granger causality , where information is usually understood in the sense of the amount of reduction of the linear prediction error .",
    "_ transfer entropy is an unsatisfactory measure of causal strength_. @xcite pointed out that transfer entropy fails to quantify causal influence for the following toy model : assume the information from @xmath41 is perfectly copied to @xmath89 and the information from @xmath42 to @xmath90 ( see figure  [ figten ] , right ) .",
    "then the past of @xmath16 is already sufficient to perfectly predict the present value of @xmath16 and the past of @xmath17 does not provide any further information",
    ". therefore , transfer entropy vanishes although both variables heavily influence one another .",
    "if the copy operation is noisy , transfer entropy is nonzero and thus seems more reasonable , but the quantitative behavior is still wrong ( as we will argue in example  [ experturbed ] ) .",
    "_ transfer entropy violates the postulates .",
    "_ transfer entropy yields @xmath21 bits of causal influence in a situation where common sense and p1 together with p2 require that causal strength is @xmath22 bit ( p2 reduces the dag to one in which p1 applies ) . since our postulates refer to the strength of a _ single _ arrow while transfer entropy is supposed to measure the strength of all arrows from @xmath17 to @xmath16 , we reduce the dag such that there is only one arrow from @xmath17 to @xmath16 ; see figure  [ figtred ] . then , @xmath91 } \\rightarrow y_{t } |y_{(-\\infty , t-1]})&= & i(x_{(-\\infty , t-1 ] } ; y_{t } |y_{(-\\infty , t-1]})\\\\ & = & i(x_{t-1 } ; y_{t } |y_{t-2}).\\end{aligned}\\ ] ] the causal structure coincides with dag [ dagsobv](a ) by setting @xmath92 , @xmath93 , and @xmath94 . with these replacements",
    ", transfer entropy yields @xmath95 bits instead of @xmath96 bit , as required by p1 and p2 .",
    "note that the same problem occurs if causal strength between time series is quantified by directed information @xcite because this measure also conditions on the entire past of @xmath16 .",
    "note that @xcite and @xcite introduce two different quantities , both called `` information flow . ''",
    "we consider them in turn .    after arguing that transfer entropy does not properly capture the strength of the impact of interventions",
    ", @xcite proposes to define causal strength using pearl s @xmath45 calculus @xcite . given a causal directed acyclic graph @xmath3",
    ", pearl computes the joint distribution obtained if variable @xmath7 is forcibly set to the value @xmath97 as @xmath98 intuitively , the intervention on @xmath7 removes the dependence of @xmath7 on its parents and therefore replaces @xmath99 with the kronecker symbol .",
    "likewise , one can define interventions on several nodes by replacing all conditionals with kronecker symbols",
    ".    given three sets of nodes @xmath100 , @xmath101 and @xmath102 in a directed acyclic graph @xmath3 , information flow is defined by latexmath:[\\ ] ] where we have used that the sources in @xmath250 are jointly independent and independent of the other parents of @xmath35 . by definition of @xmath438 ,",
    "the second summand reads @xmath439,\\ ] ] which proves ( [ orth ] ) . by lemma  [ tlocaladd ] ,",
    "it is only necessary to prove part ( d ) in the case where both @xmath50 and @xmath440 consist of arrows targeting a single node . to keep the exposition simple , we consider the particular case of a dag containing three nodes @xmath44 where @xmath441 and @xmath442 .",
    "the more general case follows similarly .",
    "observe that @xmath443 if and only if @xmath444 for all @xmath445 such that @xmath446 . multiplying both sides with @xmath152 and summing over all @xmath47 yields @xmath447 because the right - hand side does not depend on @xmath18 . using ( [ econd ] ) again , we obtain @xmath448 for all @xmath445 with @xmath449 .",
    "hence @xmath271 , and thus @xmath450 .",
    "causal influence is intimately related to control .",
    "suppose an experimenter wishes to understand interactions between components of a complex system .",
    "for the causal dag in figure  [ dagsobv](d ) , she is able to observe nodes @xmath16 and @xmath35 , and manipulate node @xmath17 . to",
    "what extent can she control node @xmath16 ?",
    "the notion of control has been formalized information - theoretically in @xcite :    node @xmath16 is _ perfectly controllable _ by node @xmath17 at @xmath451 if , given @xmath79 ,    states of @xmath16 are a deterministic function of states of @xmath17 ; and    manipulating @xmath17 gives rise to all states of @xmath16 .",
    "perfect control can be elegantly characterized :    [ tcontrol]a node @xmath16 with inputs @xmath17 and @xmath35 is perfectly controllable by @xmath17 alone for @xmath451 iff there exists a markov transition matrix @xmath452 such that    @xmath453 here , @xmath454 denotes the conditional shannon entropy of @xmath16 , given that @xmath451 has been observed and @xmath17 has been set to @xmath18 .",
    "the theorem restates the criteria in the definition . for a proof ,",
    "see  @xcite .",
    "it is instructive to compare theorem  [ tcontrol ] to our measure of causal influence .",
    "the theorem highlights two fundamental properties of perfect control .",
    "first , ( [ ec1 ] ) , perfect control requires there is no variation in @xmath16 s behavior  aside from that due to the manipulation via @xmath17given that @xmath79 is observed .",
    "second , ( [ ec2 ] ) , perfect control requires that all potential outputs of @xmath16 can be induced by manipulating node @xmath17 .",
    "this suggests a measure of the _ degree _ of control should reflect ( i ) the variability in @xmath16 s behavior that can not be eliminated by imposing @xmath17 values and ( ii ) the size of the repertoire of behaviors that can be induced on the target by manipulating a source .    for the dag under consideration ,",
    "theorem  [ thmdeco ] states that @xmath455 the first term , @xmath456 , quantifies size of the repertoire of outputs of @xmath16 averaged over manipulations of @xmath17 .",
    "it corresponds to requirement ( [ ec2 ] ) in the characterization of perfect control : that @xmath457 for all @xmath79 .",
    "specifically , the causal influence , interpreted as a measure of the degree of controllability , increases with the size of the ( weighted ) repertoire of outputs that can be induced by manipulations .",
    "the second term , @xmath458 [ which coincides with @xmath459 here ] , quantifies the variability in @xmath16 s behavior that can not be eliminated by controlling @xmath17 .",
    "it corresponds to requirement ( [ ec1 ] ) in the characterization of perfect control : that remaining variability should be zero .",
    "causal influence increases as the variability @xmath460 tends toward zero provided that the first term remains constant .",
    "we are grateful to gbor lugosi for a helpful hint for the proof of lemma  1 in supplement s.1 and to philipp geiger for several corrections ."
  ],
  "abstract_text": [
    "<S> many methods for causal inference generate directed acyclic graphs ( dags ) that formalize causal relations between @xmath0 variables . given the joint distribution on all these variables , the dag contains all information about how intervening on one variable changes the distribution of the other @xmath1 variables . however , _ quantifying _ </S>",
    "<S> the causal influence of one variable on another one remains a nontrivial question .    </S>",
    "<S> here we propose a set of natural , intuitive postulates that a measure of causal strength should satisfy . </S>",
    "<S> we then introduce a communication scenario , where edges in a dag play the role of channels that can be locally corrupted by interventions . </S>",
    "<S> causal strength is then the relative entropy distance between the old and the new distribution .    </S>",
    "<S> many other measures of causal strength have been proposed , including average causal effect , transfer entropy , directed information , and information flow . </S>",
    "<S> we explain how they fail to satisfy the postulates on simple dags of @xmath2 nodes . </S>",
    "<S> finally , we investigate the behavior of our measure on time - series , supporting our claims with experiments on simulated data .    ,    , </S>"
  ]
}