{
  "article_text": [
    "simultaneous testing of multiple hypotheses is an integral part of analyzing high - dimensional data from modern scientific investigations like those in genomics , brain imaging , astronomy , and many others , making multiple testing an area of current importance and intense statistical research . a variety of multiple testing methods",
    "have been put forward in the literature from both frequentist and bayesian perspectives . however ,",
    "the theories behind the developments of these methods are mostly driven by the overreaching goal of controlling an overall measure of type i errors or false discoveries , with other fundamentally important statistical issues often being ignored .",
    "for instance , in many of the aforementioned experiments there is a cost associated with the error of making a false discovery or missing a true discovery , and this cost increases with increasing severity of that error .",
    "this is an important issue not often taken into account when developing multiple testing procedures .",
    "a bayesian decision theoretic approach can yield a powerful multiple testing method not only incorporating costs of false and missed discoveries but also simultaneously addressing dependency , optimality , and multiplicity ( @xcite ) .",
    "this motivates us to take a similar approach , but in a more general framework that conforms more to the present problem , that is , to address the aforementioned issue related to severity of errors .",
    "before explaining this generalization , let us first briefly outline the approach taken in @xcite",
    ".    given a set of observations @xmath0 , where @xmath1 , consider the problem of deciding between @xmath2 and @xmath3 simultaneously for @xmath4 , assuming that @xmath5 , for some given densities @xmath6 and @xmath7 , and @xmath8 .",
    "@xcite started with the following uniformly weighted 0 - 1 loss function : @xmath9 for a decision rule @xmath10 , where @xmath11 is the relative cost of making a false discovery ( type i error ) to that of missing a true discovery ( type ii error ) and assumed to be constant over all the hypotheses .",
    "they considered the bayes rule associated with this loss function and showed that it is also optimal from a multiple testing point of view .",
    "specifically , given any @xmath12 , there exists a @xmath13 for which it controls the marginal false discovery rate , @xmath14 } { e   \\left [ \\sum_{i=1}^m\\delta_i ( \\mathbf{x } ) \\right ] } ,   \\nonumber \\end{aligned}\\ ] ] at @xmath15 , and minimizes the marginal false non - discovery rate , @xmath16 } { e   \\left [ \\sum_{i=1}^m \\left \\{1 - \\delta_i(\\mathbf{x } ) \\right \\ } \\right ] } , \\nonumber \\end{aligned}\\ ] ] among all decision rules defined in terms of statistics satisfying a monotone likelihood ratio condition ( mlr ) and controlling the mfdr at @xmath15 .",
    "they expressed this optimal procedure in an alternative form using hypothesis specific test statistics defined in terms of the local fdr measure [ lfdr , @xcite ] , and called it the oracle procedure .",
    "they provided numerical evidence showing that their oracle procedure can outperform its competitors , such as those in @xcite and @xcite .",
    "clearly , the loss function used in the above formulation is somewhat simplistic .",
    "it gives equal importance to all type i errors as well as to all type ii errors . while it might be reasonable to treat the type i errors equally in terms of severity and attach a fixed cost to all of them , it is often unrealistic to do so for type ii errors .",
    "for instance , in a microarray experiment , there might be a fixed cost of doing a targeted experiment to verify that each gene is active and the loss due to making a false discovery might be that cost ( which is being wasted in case the gene is found to be inactive )",
    ". however , it would be unrealistic to assume that the loss in identifying a truly active gene as inactive does not depend on how strong is the expected signal that has been missed .",
    "in fact , it might reasonably be proportional to the difference @xcite or even to the squared difference between the expected values of the missed and no signals .",
    "in other words , the above formulation needs to be generalized conforming it more to the reality in modern high - dimensional multiple testing . with that in mind , we consider testing @xmath17 against its one or two - sided alternative , for some specified values @xmath18 , simultaneously for @xmath19 , under the following model : @xmath20 given a density @xmath21 , and under the following more general loss function : @xmath22 we do not impose any dependence restriction on @xmath23 , @xmath24 or @xmath25 .",
    "it is assumed that there is only a baseline cost @xmath26 for each type i error ( which , as argued above , is reasonable for a point null hypothesis ) . for each type ii error , however , we assume that the cost is @xmath27 , the baseline cost , times @xmath28 , a function @xmath29 of @xmath30 such that @xmath31 and is non - decreasing as @xmath32 moves away from @xmath18 .",
    "we call @xmath33 the _ severity function _ for type ii errors . through this function ,",
    "a penalty is being imposed on making a type ii error for each @xmath34 ; the larger the value of @xmath35 is , the more severe this penalty is .",
    "the @xmath11 equals @xmath36 , the relative baseline cost of a type i error to a type ii error . in other words",
    ", @xmath37 is the relative cost of a type i error to a type ii error .",
    "the specific choice of @xmath33 will depend on how fast we want the cost of the type ii error to increase as @xmath32 moves away from @xmath18 .    our proposed loss function ( [ loss ] ) is a non - uniformly weighted 0 - 1 loss function giving less and less weight to the type i error relative to the type ii error as the type ii error gets more and more severe as measured by the severity function . in this paper , we focus on deriving the theoretical form of an optimal multiple testing procedure from the bayes rule under this general loss function .",
    "given a severity function @xmath29 , this bayes rule provides an optimal multiple testing procedure in the sense of minimizing a measure of non - discoveries subject to controlling a measure of false discoveries at a specified level for a suitably chosen @xmath11 .",
    "these measures of false discoveries and false non - discoveries are of course different from the mfdr and mfnr , respectively , since we now need to account for the weights or penalties attached to the type ii errors through the severity function that is not necessarily equal to one .",
    "we define these newer error rates as weighted mfdr and weighted mfnr and establish the aforementioned optimality result through these rates .",
    "we study the performance of this oracle optimal procedure with its relevant competitors through two numerical studies .",
    "the remainder of the paper is organized as follows .",
    "the development of the bayes rule under the loss function ( [ loss ] ) , its characterization as an optimal multiple testing procedure in the framework of weighted false discovery and false non - discovery rates , and our oracle multiple testing procedure are given in the next section . in section [ sec :",
    "comp : oracle ] , we present the results of two numerical studies providing evidence of this oracle procedure s superior performance over its relevant competitors .",
    "we end the paper with some concluding remarks in section [ sec : concluding ] .",
    "assuming that our problem is that of testing @xmath38 simultaneously for @xmath19 under the model ( [ model ] ) and the loss function @xmath39 in ( [ loss ] ) , we do the following in this section : ( i ) determine the bayes rule ; ( ii ) show that the bayes rule with an appropriately chosen @xmath11 provides an optimal multiple testing procedure in the sense of minimizing a measure of false non - discoveries among all rules that control a measure of false discoveries at a specified level ; and ( iii ) express this optimal multiple testing procedure in terms of some test statistics to define the oracle procedure in this paper .",
    "let us first define @xmath40,\\end{aligned}\\ ] ] the average severity of type ii errors conditional on the data @xmath41 and @xmath42 .",
    "then , we have the following :    [ thm1 ] consider testing @xmath38 simultaneously for @xmath19 under the model ( [ model ] ) and the loss function ( [ loss ] ) .",
    "then , the decision rule @xmath43 , where @xmath44 is the bayes rule .    * proof .",
    "* for any rule @xmath45 , we have @xmath46\\nonumber \\\\ & = & \\dfrac{1}{m}\\sum_{i=1}^{m}\\left\\{\\lambda \\delta_{i}(\\mathbf{x } ) p(\\theta_{i}=0\\mid \\mathbf{x})+[1-\\delta_{i}(\\mathbf{x})]e \\left [ s(\\mu_{i } ) i(\\theta_{i}=1 ) \\mid   \\mathbf{x } \\right ] \\right\\ } \\nonumber\\\\ & = & \\dfrac{1}{m}\\sum_{i=1}^{m}\\left\\{\\lambda \\delta_{i}(\\mathbf{x } ) p(\\theta_{i}=0\\mid \\mathbf{x})+[1-\\delta_{i}(\\mathbf{x})]e \\left [ s(\\mu_{i } ) \\mid \\theta_{i}=1 ,   \\mathbf{x}\\right ]   p(\\theta_{i}=1 \\mid \\mathbf{x } ) \\right\\ } \\nonumber\\\\ & = & \\dfrac{1}{m}\\sum_{i=1}^{m}\\left\\{w_i(\\mathbf{x } )   p(\\theta_{i}=1 \\mid \\mathbf{x } ) + \\delta_{i}(\\mathbf{x})\\left [ \\lambda p(\\theta_{i}=0",
    "\\mid \\mathbf{x } ) - w_i(\\mathbf{x } )   p(\\theta_{i}=1 \\mid \\mathbf{x } ) \\right ] \\right \\}.   \\nonumber\\end{aligned}\\ ] ] since the first term is constant with respect to , given @xmath41 , it is clear that @xmath47 in ( [ bayesrule ] ) is the rule for which this conditional expectation is the minimum among all @xmath48 , and hence is bayes .",
    "here we show that the aforementioned bayes rule with an appropriately chosen @xmath11 provides an optimal multiple testing procedure in the sense of minimizing a measure of false non - discoveries among all rules that control a measure of false discoveries at a specified level .",
    "these measures of false discoveries and false non - discoveries are defined for any multiple testing rule as @xmath49 } { e   \\left [ \\sum_{i=1}^m\\delta_i ( \\mathbf{x } ) w^ * ( \\theta_i , \\mu_i ) \\right ] } , \\end{aligned}\\ ] ] and @xmath50 } { e   \\left [ \\sum_{i=1}^m \\left \\{1 - \\delta_i(\\mathbf{x } ) \\right \\ } w^*(\\theta_i , \\mu_i ) \\right ] } , \\end{aligned}\\ ] ] respectively , where @xmath51    with @xmath52 representing a weight associated with the @xmath53th hypothesis , these measures of false discoveries and false non - discoveries can be referred to as weighted mfdr and weighted mfnr , respectively .",
    "when @xmath54 , they reduce to the corresponding mfdr or mfnr .",
    "[ oracle_thm1 ] consider the model in ( [ model ] ) .",
    "suppose there exists a testing procedure @xmath55 such that @xmath56 is defined as in ( [ bayesrule ] ) and @xmath57 .",
    "let @xmath58 be any other rule such that @xmath59",
    ". then @xmath60 .    *",
    "first note that @xmath61 \\leq 0,\\end{aligned}\\ ] ] according to ( [ bayesrule ] ) , and @xmath62 \\geq 0,\\end{aligned}\\ ] ] from the assumption , @xmath63 . from ( [ cond1 ] ) and ( [ cond2 ] )",
    ", we get @xmath64 \\geq 0,\\end{aligned}\\ ] ] which implies that @xmath65 \\geq \\sum_{i=1}^m e \\left [ \\delta_{i}(\\mathbf{x})w_{i}(\\mathbf{x } ) p(\\theta_{i}=1 \\mid \\mathbf{x } ) \\right ] , \\end{aligned}\\ ] ] since @xmath66 } { \\sum_{i=1}^m e \\left [ \\delta_{i0}(\\mathbf{x } ) w_{i}(\\mathbf{x})p(\\theta_{i}=1 \\mid \\mathbf{x } ) \\right ] } \\leq \\dfrac{1}{\\lambda}.\\ ] ] thus , we have from ( [ cond3 ] ) @xmath67 } \\right .",
    "\\right . - \\\\ & \\qquad \\left .",
    "\\dfrac{1-\\delta_{i}(\\mathbf{x})}{\\sum_{i=1}^m e \\left [ \\left \\{1-\\delta_{i}(\\mathbf{x } ) \\right \\}w_{i}(\\mathbf{x } ) p(\\theta_{i}=1 \\mid\\mathbf{x } ) \\right ] } \\right \\ } \\left \\ { p(\\theta_{i}=0 \\mid \\mathbf{x})- \\right .",
    "\\\\ & \\qquad \\qquad \\qquad \\left .",
    "\\dfrac{w_{i}(\\mathbf{x})}{\\lambda}p(\\theta_{i}=1 \\mid\\mathbf{x } ) \\right \\ } \\right ] \\geq 0.\\end{aligned}\\ ] ] this implies that @xmath68 that is , @xmath69 , as desired .",
    "[ remark:2.1 ] theorem [ oracle_thm1 ] improves the work of @xcite in the following sense : 1 ) it accommodates situations where penalties or weights associated with type ii errors can be assessed through a severity function and incorporated into the development of a multiple testing procedure ; 2 ) it provides a rule that is optimal among all procedures controlling the @xmath70fdr * at level @xmath15 without any distributional restriction on the corresponding test statistics .",
    "next , we will prove the existence of such a procedure @xmath71 .",
    "we can express the optimal procedure @xmath71 in theorem [ oracle_thm1 ] in terms of the following test statistics : @xmath72 the statistic @xmath73 will be referred to as generalized local fdr ( glfdr ) .",
    "it reduces to the usual definition of the local fdr ( lfdr ) of @xcite under independence and to the test statistic defined in sun and cai ( 2009 ) under arbitrary dependence when @xmath74 .",
    "we consider decision rules of the form @xmath75 , where @xmath76 with @xmath77 being such that @xmath78 .",
    "this will be our proposed oracle procedure .",
    "before we state this oracle procedure more explicitly in terms of the distributions of @xmath73 s , we give the following proposition asserting the existence of such a @xmath77 . in this paper",
    "we assume that @xmath41 is continuous and hence @xmath79 is continuous in @xmath77 .",
    "[ proposition:2.1 ] for the decision rule in ( [ rule ] ) with @xmath73 defined in ( [ glfdr ] ) , @xmath79 is non - decreasing in @xmath77 .",
    "we will prove this proposition by making use of the following two lemmas .",
    "[ lemma:1 ] consider the ratio of expectations @xmath80/e_{h_0 } \\left [ \\delta(t , c ) \\right]$ ] , for any random variable @xmath81 having distribution @xmath82 in the numerator and distribution @xmath83 in the denominator .",
    "it is non - decreasing ( non - increasing ) in @xmath84 , if @xmath85 is non - decreasing ( non - increasing ) in @xmath86 .",
    "* proof . *",
    "the ratio can be expressed as the expectation , @xmath87 , of the non - decreasing function @xmath88 , where @xmath89 is such that @xmath90.\\ ] ] since @xmath91 is totally positive of order two ( tp@xmath92 ) in @xmath93 , that is , it satisfies the inequality @xmath94 the lemma follows from the following result @xcite : the expectation of a non - decreasing ( non - increasing ) function of a random variable @xmath95 , with @xmath96 being tp@xmath92 in @xmath97 , is non - decreasing ( non - increasing ) in @xmath98 .",
    "[ remark:2.2 ] @xcite derived the above result for the collection of decisions based on the test statistics satisfying the mlr condition .",
    "note that our proof , which is different , does not rely on any such condition .",
    "[ lemma:2 ] given two distributions @xmath99 and @xmath100 of a random vector @xmath101 , define @xmath102 , for any constants @xmath103 .",
    "let @xmath104 , for @xmath105 .",
    "then , @xmath106 .",
    "* proof . * since @xmath107 \\left [ i(t({\\mathbf{x } } ) \\le   t ) - i(t ( { \\mathbf{x } } ) \\le t \\pm \\epsilon ) \\right ] \\le 0 , \\forall 0 < t < 1 , \\epsilon > 0,\\ ] ] by taking expectations of both sides in this inequality with respect to @xmath108 we have @xmath109 \\le   bt \\left [ h_{1 } ( t ) -h _ { 1 } ( t \\pm \\epsilon ) \\right ] , \\forall   0",
    "< t < 1 , \\epsilon > 0.\\ ] ]",
    "the desired result then follows by letting @xmath110 .",
    "* proof of proposition [ proposition:2.1 ] .",
    "* let @xmath111 denote the conditional distribution of @xmath112 given @xmath113 and @xmath24 , for @xmath114 then , from ( [ def : mfdrstar ] ) , we note that @xmath115 where @xmath116 with @xmath117 and @xmath118 representing the joint distribution of @xmath24 conditionally given @xmath119 and @xmath42 , respectively . @xmath120 } { e \\left [ \\sum_{i=1}^m \\delta ( t_i , c ) ( 1 - \\theta_i ) \\omega^*(\\theta_i , \\mu_i ) \\right ] } \\nonumber \\\\ & = & \\frac { e \\left [ \\sum_{i=1}^m \\delta ( t_i , c ) s(\\mu_i ) i(\\theta_i = 1 ) \\right ] } { e \\left [ \\sum_{i=1}^m \\delta ( t_i , c ) i(\\theta_i = 0 ) \\right ] }   \\nonumber \\\\ & = & \\frac{1-\\pi_0}{\\pi_0 } \\left ( \\frac{1}{m } \\sum_{i=1}^m \\beta_i \\right ) \\frac { e_{g_1 } \\left [ \\delta(t , c ) \\right]}{e_{g_0 } \\left [ \\delta(t , c ) \\right ] } , \\end{aligned}\\ ] ] where @xmath121 , @xmath122 , @xmath123 , and @xmath124 , with @xmath125 .",
    "the proposition will be proved from lemma [ lemma:1 ] if we can show that @xmath126 is a non - increasing function of @xmath86 , since the left hand side of proposition ( [ proposition ] ) is a decreasing function of @xmath79 .    since @xmath127 , and @xmath128 and @xmath129 are the cdf s of @xmath130 under the distributions @xmath131 and @xmath132 respectively , we see from lemma [ lemma:2 ] that @xmath133 , for any @xmath134 .",
    "thus , @xmath135 implying that @xmath126 is non - increasing in @xmath136 , as desired .",
    "thus , the proposition is proved .",
    "given proposition [ proposition:2.1 ] , we are now ready to define our oracle procedure in the following :    [ oracle ] consider the multiple testing procedure @xmath137 , where @xmath138    this is a generalized version of the oracle procedure of @xcite .",
    "it is developed not only under any dependence structure among ( @xmath41 , @xmath24 ) but also it allows the alternatives to vary across tests and each type ii error to be weighted by a measure of severity .",
    "moreover , for its optimality , any specific property , like the monotone likelihood ratio property that @xcite assumed , for the underlying test statistics is not required .",
    "[ remark:2.3 ] let @xmath139 and @xmath140 .",
    "then , it is to be noted that the @xmath141 can be expressed as follows : @xmath142}{\\sum_{i=1}^m e \\left [ i(t_i({\\bf x } ) < t)fdr_i({\\bf x } ) + i(t_i({\\bf x } ) < t)(1-fdr_i({\\bf x}))w_i({\\bf x } ) \\right ] } \\nonumber \\\\ & = & \\dfrac { \\sum_{i=1}^m e \\left [ i(t_i({\\bf x } ) < t ) t_i({\\bf x } ) d_i({\\bf x } ) \\right ] } { \\sum_{i=1}^m e \\left [ i(t_i({\\bf x } ) < t ) d_i({\\bf x } ) \\right ] } . \\end{aligned}\\ ] ]",
    "we carried out two numerical studies to see how our procedure in its oracle form compares with its relevant competitors for the problem of testing @xmath143 against @xmath144 , @xmath19 , with @xmath145 , under the following model .",
    "let @xmath146 , be such that @xmath147 often a multiple testing procedure can be seen as first ranking the hypotheses according to a measure of significance , based on some test statistic , @xmath148-value , or local fdr , before choosing a cut - off point for the significance measure to determine which hypotheses are to be declared significant subject to control over a certain error rate , such as fdr or mfdr , at a specified level .",
    "such ranking plays an important role in a procedure s performance , and can itself be used as a basis to compare with another procedure controlling a different error rate .",
    "more specifically , between two procedures providing the same number of discoveries , the one with better ranking should provide more true discoveries .",
    "the first numerical study was designed to make such ranking comparison between the @xcite and our oracle procedures that control two different measures of false discoveries , even though one is a generalized version of the other .    towards understanding what significance measure is being used to rank the hypotheses in our procedure , we note that under the independence model ( 3.1 ) , the @xmath70fdr@xmath149 given in remark 2.3 reduces to the following : @xmath150 with @xmath151 and @xmath152 . the numerator and denominator expectations in the above ratio",
    "can be approximated ( for large @xmath70 ) by @xmath153 and @xmath154 , respectively , resulting in a measure of @xmath155 at @xmath86 as follows : @xmath156 let @xmath157 be the ordered versions of @xmath158 , and @xmath159 and @xmath160 be respectively the null hypothesis and the @xmath161-value corresponding to @xmath162 .",
    "then , our oracle procedure can be described approximately as follows :    find @xmath163 and reject @xmath159 for all @xmath164 .",
    "in other words , our procedure can be seen as ranking the hypotheses according to the increasing values of @xmath165 , the glfdr scores corresponding to the @xmath34 s , before determining the cut - off point @xmath166 to control the mfdr * ; whereas , the sun - cai oracle procedure does the same in terms of the lfdr scores .",
    "the second numerical study was conducted to see how well our oracle procedure with the cut - off point chosen subject to controlling the mfdr * compares with sun - cai s oracle procedure and the @xmath148-value based oracle procedure in @xcite in terms of the acceptance region , the mfdr * , and the mfnr*.      we considered using a measure of non - discoveries to compare the rankings provided by the sun - cai and our oracle procedures .",
    "more specifically , we wanted to see how these procedures compare in terms of not discovering the _ most important _ signals ( i.e. , the signals that are truly and highly significant ) , given the same number of discoveries made by each of them .",
    "the measure of non - discoveries is defined with weights assigned to the signals according to their magnitudes using our chosen severity function @xmath167 to capture these _ most important _ signals with greater certainty .    with that in mind",
    ", we generated @xmath168 observations according to the model ( [ sim : model ] ) .",
    "here we chose @xmath169 and @xmath170 with @xmath171 , @xmath172 , @xmath173 , and @xmath174 .",
    "we then calculated the values of glfdr given in ( [ glfdr ] ) , which can be written for this model as @xmath175 with @xmath176 \\\\ &",
    "+ & \\pi_{12}\\left [ \\frac{1}{\\sqrt{1+\\tau^2}}\\phi\\left(\\frac{x_i-\\mu_+}{\\sqrt{1+\\tau^2}}\\right)\\frac{\\tau^2}{1+\\tau^2 } + \\frac{(\\tau^2x_i+\\mu_-)^2}{(1+\\tau^2 ) } \\right].\\end{aligned}\\ ] ] we ordered these values of glfdr increasingly as @xmath177 .",
    "let @xmath159 be the null hypothesis corresponding to @xmath178 , for @xmath19 .",
    "for each given @xmath179 , we marked the first @xmath180 null hypothesis to be rejected and the rest to be accepted . with",
    "@xmath181 or @xmath182 indicating whether the null hypothesis @xmath159 is true or false ( with @xmath183 being the true signal ) , respectively , we then calculated the weighted type ii errors @xmath184 .",
    "we replicated these steps 2,000 times and averaged the 2,000 values of the weighted type ii errors before obtaining the simulated value of @xmath185 , the expected weighted type ii errors ( or non - discoveries ) given @xmath180 rejections ( or discoveries ) .",
    "the red curve in figure [ fig : ranking ] shows the plot of @xmath185 against @xmath180 .",
    "the similar plot was obtained for the @xmath186 score and is shown using the green curve in this figure . as seen from this figure , between the sun - cai and our oracle procedures , ours can potentially be more powerful in the sense of producing a smaller amount of weighted type ii errors associated with missing the _ most important _ signals .",
    "we chose @xmath187 , @xmath188 with @xmath189 , @xmath190 , and let @xmath191 vary in @xmath192 .",
    "this model was also considered in example 1 , section 3.2 , of @xcite and was chosen here to make the comparison with the @xcite procedure meaningful .",
    "the rejection region for our oracle procedure is @xmath193 for each @xmath34 , with the cut - offs @xmath194 and @xmath195 being determined following the steps for their calculations as below :    * for a given @xmath196 , solve the following equation for @xmath197 to obtain @xmath198 and @xmath199 : @xmath200-\\pi_{0}(1-t)=0   \\end{aligned}\\ ] ] * calculate @xmath201 where @xmath202 , and @xmath203 is the cdf of @xmath204 . * repeat the above two steps until we find @xmath205 such that the @xmath70fdr * converges to @xmath15 . * @xmath194 and @xmath195 are then determined as @xmath206 and @xmath207 .    once @xmath194 and @xmath195 are determined , the mfnr@xmath208 of the oracle procedure is calculated as follows : @xmath209+\\pi_{12}\\mu_{2}^2 [ 1 - \\psi(c_{l}-\\mu_{2 } , c_{u}-\\mu_{2}]\\}}{\\pi_{0}[1 - \\psi(c_{l } , c_{u } ) ] + \\pi_{1}\\ { \\pi_{11}\\mu_{1}^2[1 - \\psi(c_{l}-\\mu_{1 } , c_{u}-\\mu_{1})]+\\pi_{12}\\mu_{2}^2[1 -\\psi(c_{l}-\\mu_{2 } , c_{u}-\\mu_{2})]\\}}. \\nonumber \\\\\\end{aligned}\\ ] ]    for the @xmath148-value based procedure , the rejection region for @xmath34 is @xmath210 where @xmath77 is determined according to @xcite .",
    "the oracle method of @xcite is the special case of ours with @xmath211 .",
    "the results of this numerical study are shown in figure [ fig0 ] .",
    "as seen from figure [ fig : accept ] , the rejection regions corresponding to our oracle procedure are much wider than those corresponding to both of the other two oracle procedures . from figures [ fig : mfnr ] and [ fig :",
    "mfnrstar ] , we see that while the sun - cai oracle procedure has smaller @xmath70fnr and @xmath70fnr@xmath208 than those of the @xmath148-value based oracle procedure for almost all values of @xmath191 , ours has the smallest @xmath70fnr and @xmath70fnr@xmath208 among all three for each value of @xmath191 . for instance",
    ", the ratio of the mfnr * of our procedure to that of the sun - cai oracle procedure can be as small as 0.15 .",
    "it is thus demonstrated that our proposed approach can potentially be more powerful than the other two approaches .",
    "the decision theoretic approach to a multiple testing problem is not new .",
    "other relevant work includes @xcite and @xcite .",
    "nevertheless , the idea of incorporating the severity of type ii errors has not been fully explored previously in the literature .",
    "we have developed the theory behind our idea from a compound decision theoretic point of view considering a loss function that incorporates the type ii error severity .",
    "the consideration of type ii error severity into the loss function allows us to re - formulate the work of @xcite in a more general framework involving newer , generalized forms of marginal false discovery and marginal false non - discovery rates .",
    "newer theoretical results generalizing and often improving the existing ones are given in this process .",
    "we now have the theory for developing a much wider class of multiple testing procedures constructed from a decision theoretic point of view .",
    "some of the newer methods in this class , those corresponding to non - constant type ii error severity , are seen to have better performance in their oracle forms , as shown in our numerical studies , than those with constant type ii error severity ( i.e. , those in @xcite and some standard @xmath148-value based procedures ) .",
    "the idea of weighting hypotheses or @xmath148-values while developing multiple testing methods in an fdr but non - decision theoretic framework has been proposed before .",
    "@xcite considered weighting the hypotheses in the original definition of the fdr to define the weighted fdr and proposed a weighted version of their 1995 fdr controlling method , the so - called bh method , that controls this weighted fdr .",
    "@xcite , on the other hand , weighted each @xmath148-value and developed a bh type method controlling the usual fdr based on these weighted @xmath148-values .",
    "our concern in this paper has been to define weighted versions of not only the marginal fdr but also the marginal fnr from their original definitions before providing a theoretical framework for the development of our procedure .",
    "our approach to defining weighted mfdr and weighted mfnr is similar to @xcite .",
    "we attach weights to the hypotheses , although they are chosen to effectively act only on the false nulls .",
    "more specifically , we have @xmath212 } { e   \\left [ \\sum_{i=1}^m",
    "i ( t_i < c , \\theta_i=0 ) + \\sum_{i=1}^m i(t_i",
    "< c , \\theta_i=1 ) s(\\mu_i ) \\right ] } , \\end{aligned}\\ ] ] and @xmath213 } { e   \\left [ \\sum_{i=1}^m i(t_i > c , \\theta_i=1 ) s(\\mu_i ) + \\sum_{i=1}^m i(t_i > c , \\theta_i = 0 ) \\right ] } .\\end{aligned}\\ ] ] the weight is assigned to a false null hypothesis according to its signal strength .",
    "it does not depend on whether acceptance or rejection of the false null contributes to a measure of false non - discoveries or false discoveries in the form of a penalty or boon .",
    "it is important to point out that our weights for all the hypotheses do nt add up to @xmath70 , contrary to what one might conclude from @xcite .",
    "in fact , a careful study of @xcite would reveal that such a restriction on the weights is not necessary in their paper , even though they have assumed it .",
    "derivation of an optimal multiple testing procedure incorporating type ii error severity in its oracle form has been our primary focus in this paper .",
    "now that we have this oracle procedure , a data - driven version of it with similar optimal property can potentially be constructed .",
    "however , construction of such an optimal data - driven procedure depends heavily on the underlying model and the chosen severity function , requiring newer efforts and techniques .",
    "we therefore leave this for a future communication .",
    "also , a more comprehensive study of the procedure in terms of its sensitivity under varying choice of the severity function is also on our agenda for future research .",
    "the research of li he is supported by merck research fellowship .",
    "sanat k. sarkar s research is supported by nsf grants dms-1006344 and dms-1208735 .",
    "zhigen zhao s research is supported by nsf grant dms-1208735 ."
  ],
  "abstract_text": [
    "<S> the severity of type ii errors is frequently ignored when deriving a multiple testing procedure , even though utilizing it properly can greatly help in making correct decisions . </S>",
    "<S> this paper puts forward a theory behind developing a multiple testing procedure that can incorporate the type ii error severity and is optimal in the sense of minimizing a measure of false non - discoveries among all procedures controlling a measure of false discoveries . </S>",
    "<S> the theory is developed under a general model allowing arbitrary dependence by taking a compound decision theoretic approach to multiple testing with a loss function incorporating the type ii error severity . </S>",
    "<S> we present this optimal procedure in its oracle form and offer numerical evidence of its superior performance over relevant competitors .    </S>",
    "<S> bayes rule , compound decision theory , oracle procedure , multiple testing , weighted marginal false discovery rate , weighted marginal false non - discovery rate </S>"
  ]
}