{
  "article_text": [
    "there are a number of applications in signal processing and machine learning that give rise to highly structured spectral optimization problems .",
    "we are particularly interested in the class of problems characterized by having solutions that are very low rank , and by involving linear operators that are best treated by matrix - free approaches .",
    "this class of problems is sufficiently restrictive that it allows us to design specialized algorithms that scale well and lend themselves to practical applications , but it is still sufficiently rich to include interesting problems .",
    "two examples include the nuclear - norm minimization for problems such as blind deconvolution @xcite , and the phaselift formulation of the celebrated phase retrieval problem @xcite .",
    "the problems can be cast generically in the semi - definite programming ( sdp ) framework , for which a variety of algorithms are available .",
    "however , typical applications can give rise to enormous optimization problems that challenge the very best workhorse algorithms .",
    "denote the set of complex - valued @xmath0 hermitian matrices by @xmath1 . the algorithm that we propose",
    "is designed to solve the problems    [ eq : primal - probs ] @xmath2    where the parameter @xmath3 controls the admissible deviations between the linear model @xmath4 and the vector of observations @xmath5 .",
    "( the particular properties of the vectors @xmath5 and of the linear operators @xmath6 are detailed in section  [ sec : formulations ] . )",
    "our approach for both problems is based on first solving a related hermitian eigenvalue optimization problem over a very simple constraint , and then using that solution to recover a solution of the original problem .",
    "this eigenvalue problem is highly structured , and because the constraint is easily handled , we are free to apply a projected first - order method with inexpensive per - iteration costs that scales well to very large problems .",
    "the key to the approach is to recognize that the problems   are members of the family of gauge optimization problems , which admit a duality concept different from the lagrange duality prevalent in convex optimization .",
    "gauges are nonnegative , positively homogeneous convex functions that vanish at the origin .",
    "they significantly generalize the familiar notion of a norm , which is a symmetric gauge function .",
    "the class of gauge optimization problems , as defined by @xcite s seminal 1987 work , can be stated simply : find the element of a convex set that is minimal with respect to a gauge .",
    "these conceptually simple problems appear in a remarkable array of applications , and include an important cross - section of convex optimization .",
    "for example , all of conic optimization can be phrased within the class of gauge optimization ; see ( * ? ? ?",
    "* example 1.3 ) , and section  [ sec : gauge - duality ] below .",
    "problem   is not explicitly stated as a gauge problem because the objective is not nonnegative everywhere on its domain , as required in order for it to be a gauge function .",
    "it is , however , nonnegative on the feasible set , and the problem can easily be cast in the gauge framework simply by changing the objective function to @xmath7 this substitution yields an equivalent problem , and the resulting convex function is nonnegative and positively homogeneous  and therefore a gauge function .",
    "more generally , it is evident that any function of the form @xmath8 is a gauge , in which @xmath9 is a gauge and @xmath10 is the indicator of a convex cone @xmath11 .",
    "the method that we develop applies to the much broader class of semidefinite optimization problems with nonnegative objective values , as described in section  [ sec : extensions ] .",
    "we pay special attention to the low - rank spectral problems just mentioned because they have a special structure that can be exploited both theoretically and computationally .",
    "to emphasize the role of the vector of singular values @xmath12 we adopt the schatten @xmath13-norm notation for the matrix - norms referenced in this paper , i.e. , @xmath14 .",
    "thus , the nuclear , frobenius , and spectral norms of a matrix @xmath15 are denoted by @xmath16 @xmath17 , and @xmath18 respectively .",
    "the notation for complex - valued quantities , particularly in the sdp context , is not entirely standard .",
    "here we define some objects we use frequently .",
    "define the complex inner product @xmath19 , where @xmath20 is the conjugate transpose of a complex matrix @xmath21 , i.e. , @xmath22 .",
    "the set of @xmath0 hermitian matrices is denoted by @xmath23 , and @xmath24 ( resp .",
    ", @xmath25 indicates that the matrix @xmath15 is both hermitian and positive semidefinite ( resp . , definite ) .",
    "let @xmath26 be the vector of ordered eigenvalues of @xmath27 , i.e. , @xmath28 ( an analogous ordering is assumed for the vector of singular values . ) for @xmath29 let @xmath30 denote the vector of generalized eigenvalues of the pencil @xmath31 , i.e. , @xmath32 let @xmath33 and @xmath34 denote the real and imaginary parts of their arguments .",
    "the norm dual to @xmath35 is defined by @xmath36 the positive part of a scalar is denoted by @xmath37_{+}=\\max\\{0,\\cdot\\}.$ ]    when we make reference to _",
    "one- _ and _ two - dimensional _ signals , our intent is to differentiate between problems that involve discretized functions of _ one _ and _ two _ variables , respectively , rather than to describe the dimension of the ambient space .",
    "hence the terms _ two - dimensional signals _ and _ two - dimensional images _ are used interchangeably .",
    "generally we assume that the problems are feasible , although we highlight in section  [ sec : gauge - duality ] how to detect infeasible problems .",
    "we also assume that @xmath38 , which ensures that the origin is not a trivial solution .",
    "in practice , the choice of the norm that defines the feasible set will greatly influence the computational difficulty of the problem .",
    "our implementation is based on the 2-norm , which often appears in many practical applications .",
    "our theoretical developments , however , allow for any norm .",
    "below we describe two applications , in phase retrieval and blind deconvolution , that motivate our work .",
    "there are other relevant examples , such as matrix completion @xcite , but these two applications require optimization problems that exemplify properties we exploit in our approach .",
    "the phase retrieval problem is concerned with recovery of the phase information of a signal  e.g .",
    ", an image  from magnitude - only measurements . one important application is x - ray crystallography , which generates images of the molecular structure of crystals @xcite .",
    "other applications are described by @xcite and @xcite .",
    "they describe the following recovery approach , based on convex optimization .",
    "magnitude - only measurements of the signal @xmath39 can be described as quadratic measurements of the form @xmath40 for some vectors @xmath41 that encode the waveforms used to illuminate the signal .",
    "these quadratic measurements of @xmath42 can be understood as linear measurements @xmath43 of the lifted signal @xmath44 , where @xmath45 is the @xmath46th lifted rank-1 measurement matrix .    in the matrix space ,",
    "the trace of the unknown lifted signal @xmath15 acts as a surrogate for the rank function .",
    "this is analogous to the 1-norm , which stands as a convex surrogate for counting the number of nonzeros in a vector .",
    "this leads us to an optimization problem of the form  , where @xmath47 is defined by @xmath48 .",
    "the parameter @xmath3 anticipates noise in the measurements . @xcite",
    "call this the _ phaselift _ formulation . in section  [ ssec : plexp ]",
    "we give numerical examples for recovering one- and two - dimensional signals with and without noise .",
    "the biconvex compressed sensing problem @xcite aims to recover two signals from a number of sesquilinear measurements of the form @xmath49 where @xmath50 and @xmath51 . in the context of blind deconvolution , @xmath52 and @xmath53",
    "correspond to coefficients of the signals in some bases .",
    "the lifting approached used in the phase retrieval formulation can again be used , and the measurements of @xmath52 and @xmath53 can be understood as coming from linear measurements @xmath54 of the lifted signal @xmath55 , where @xmath56 is the lifted asymmetric rank-1 measurement matrix .",
    "@xcite study conditions on the structure and the number of measurements that guarantee that the original vectors ( up to a phase ) may be recovered by minimizing the sum of singular values of @xmath15 subject to the linear measurements .",
    "this leads to an optimization problem of the form  , where @xmath57 is defined by @xmath58    in section  [ ssec : bcsexp ] , we describe a two - dimensional blind deconvolution application from motion deblurring , and there we provide further details on the structure of the measurement operators @xmath59 , and report on numerical experiments .",
    "it is convenient , for both our theoretical and algorithmic development , to embed the nuclear - norm minimization problem   within the symmetric sdp  .",
    "the resulting theory is no less general , and it permits us to solve both problems with what is essentially a single software implementation .",
    "the reduction to the hermitian trace - minimization problem   takes the form @xmath60",
    "\\frac{1}{2}\\left\\langle             \\begin{pmatrix}0&a_k\\\\\\phantom{-}a_k^*&0\\end{pmatrix } ,             \\begin{pmatrix}u&x\\\\x^*&v\\end{pmatrix }           \\right\\rangle           + r_{1k }           & = { \\mathfrak{r}}b_k ,         \\\\\\frac{i}{2}\\left\\langle           \\begin{pmatrix}0&a_k\\\\-a_k^*&0\\end{pmatrix } ,           \\begin{pmatrix}u&x\\\\x^*&v\\end{pmatrix }         \\right\\rangle         + r_{2k }         & = { \\mathfrak{i}}b_k ,        \\\\\\norm{r_{1}+i r_{2}}\\le\\epsilon ,        \\quad        \\begin{pmatrix}u&x\\\\x^*&v\\end{pmatrix}&\\succeq0,\\ \\ k=1,\\ldots , m .",
    "\\end{aligned } \\end{aligned}\\ ] ]    the residual variables @xmath61 merely serve to allow the compact presentation above , as they can be eliminated using the equality constraints .",
    "the additional variables @xmath62 and @xmath63 at the minimizer , correspond to @xmath64 and @xmath65 respectively ; the variable @xmath15 retains its original meaning .",
    "this reduction is based on a well - known reformulation of the nuclear norm as the optimal value of an sdp ; see  ( * ? ? ?",
    "* lemma 2 ) or  ( * ? ? ?",
    "* proof of proposition 2.1 ) .",
    "although this reduction is convenient for our presentation , it is not strictly necessary , as will be clear from the results in section  [ sec : extensions ] .",
    "in fact , the resulting increase in problem size might affect a solver s performance , as would likely be noticeable if dense solvers are employed .",
    "our focus , however , is on large problems that require matrix - free operators and exhibit low - rank solutions . throughout this paper ,",
    "we focus entirely on the sdp formulation   without loss of generality .",
    "our strategy for these low - rank spectral optimization problems is based on solving the constrained eigenvalue optimization problem @xmath66 that results from applying gauge duality @xcite to a suitable reformulation of  .",
    "this is outlined in section  [ sec : gauge - duality ] .",
    "the dimension of the variable @xmath67 in the eigenvalue optimization problem corresponds to the number of measurements . in the context of phase retrieval and blind deconvolution , @xcite and @xcite",
    "show that the number of measurements needed to recover with high probability the underlying signals is within a logarithmic factor of the signal length .",
    "the crucial implication is that the dimension of the dual problem grows slowly as compared to the dimension of the primal problem , which grows as the square of the signal length .    in our implementation",
    ", we apply a simple first - order projected subgradient method to solve the eigenvalue problem .",
    "the dominant cost at each iteration of our algorithm is the computation of rightmost eigenpairs of the @xmath0 hermitian linear operator @xmath68 , which are used to construct descent directions for  .",
    "the structure of the measurement operators allows us to use krylov - based eigensolvers , such as arpack @xcite , for obtaining these leading eigenpairs .",
    "primal solution estimates @xmath15 are recovered via a relatively small constrained least - squares problem , described in section  [ sec : implementation ] .",
    "an analogous approach based on the classical lagrangian duality also leads to a dual optimization problem in the same space as our dual eigenvalue problem : @xmath69 note that the lagrange dual possesses a rather simple objective and a difficult linear matrix inequality of order @xmath70 as a constraint .",
    "precisely the reverse situation holds for the gauge dual  , which has a relatively simple constraint .",
    "it is well known that sdps with a constant - trace property  i.e .",
    ", @xmath71 implies @xmath72 is constant  have lagrange dual problems that can be formulated as unconstrained eigenvalue problems .",
    "this approach is used by @xcite to develop a spectral bundle method .",
    "the applications that we consider , however , do not necessarily have this property",
    ".      the data files and  scripts used to generate the numerical results presented in section  [ sec : experiments ] can be obtained at the following url :    http://www.cs.ubc.ca/~mpf/low-rank-opt      other researchers have recognized the need for algorithms with low per - iteration costs that scale well for large - scale , low - rank spectral optimization problems . notable efforts include @xcite , @xcite , and @xcite , who advocate variations of the frank - wolfe ( fw ) method to solve some version of the problem @xmath73 where @xmath74 is a differentiable function .",
    "for example , the choice @xmath75 yields a problem related to  . the asymmetric version of the problem with @xmath76 is easily accommodated by simply replacing the above constraints . for simplicity , here we focus on the symmetric case , though our approach applies equally to the asymmetric case .",
    "the main benefit of using fw for this problem is that each iteration requires only a rightmost eigenvalue of the gradient @xmath77 , and therefore has the same per - iteration cost of the method that we consider , which requires a rightmost eigenvalue of the same - sized matrix @xmath78 .",
    "the same krylov - based eigensolvers apply in both cases .",
    "there are at least two issues that need to be addressed when comparing the fw algorithm to the approach we take here .",
    "first , as @xcite make clear , even in cases where low - rank solutions are expected , it is not possible to anticipate the rank of early iterates @xmath79 generated by the fw method .",
    "in particular , they observe that the rank of @xmath79 quickly increases during early iterations , and only slowly starts to decrease as the solution is approached .",
    "this motivates their development of algorithmic devices that attenuate rank growth in intermediate iterates .",
    "any implementation , however , must be prepared to increase storage for the factors of @xmath79 during intermediate iterates .",
    "in contrast , a subgradient method applied to the gauge dual problem can be implemented with constant storage .",
    "second , although in principle there exists a parameter @xmath80 that causes the optimization problems   and   to share the same solution , this parameter is not generally known in advance .",
    "one way around this is to solve a sequence of problems   for varying parameters @xmath81 using , for example , a level - set procedure described by @xcite .    as an alternative to applying the fw algorithm to",
    ", we might instead consider applying a variation of the fw method directly to the gauge dual problem  . because the gauge dual objective is not differentiable and the feasible set is not compact",
    "if @xmath82 , some modification to the standard fw method is required .",
    "@xcite , @xcite , and @xcite propose variations of fw that involve smoothing the objective .",
    "these smoothing approaches are typically based on infimal convolution with a smooth kernel , which may lead to a function whose gradient is expensive to compute .",
    "for example , the `` soft - max '' smooth approximation of @xmath83 is the function @xmath84 .",
    "forming the gradient of this smooth function requires computing all eigenvalues of an @xmath70-by-@xmath70 hermitian matrix .",
    "@xcite proposes a hybrid algorithm that interleaves a nonconvex subproblem within the fw iterations .",
    "if the local minimum of the nonconvex subproblem improves the objective value , it is used to replace the current fw iterate .",
    "this approach is similar in spirit to the primal - dual refinement that we describe in section  [ sec : pr - du - refinement ] , but because laue s method is entirely primal , it has the benefit of not requiring a procedure to feed the improved primal sequence back to the dual sequence .",
    "the derivation of the eigenvalue optimization problem   as a dual to   follows from a more general theory of duality for gauge optimization .",
    "here we provide some minimal background for our derivations related to spectral optimization ; see @xcite and @xcite for fuller descriptions .",
    "we begin with a general description of the problem class .",
    "let @xmath85 and @xmath86 be gauge functions , where @xmath87 is a linear operator that maps between the finite - dimensional real inner - product spaces @xmath88 and @xmath89 .",
    "the polar @xmath90 of a gauge @xmath74 plays a key role in the duality of gauge problems .",
    "the problems    [ eq:12 ] @xmath91    are dual to each other in the following sense : all primal - dual feasible pairs @xmath92 satisfy the weak - duality relationship @xmath93 moreover , a primal - dual feasible pair is optimal if this holds with equality .",
    "this strong - duality relationship provides a certificate of optimality .",
    "the sdp problem can be cast into the mold of the canonical gauge formulation   by using the redefined objective   and making the identifications @xmath94 we use the polar calculus described by ( * ? ? ?",
    "* section 7.2.1 ) together with the definition of the dual norm to obtain the correponding polar functions : @xmath95_+    \\textt{and }    \\rho^{\\circ}(y)=\\norm{y}_{*}.\\ ] ] it then follows from   that the following are a dual gauge pair :    [ eq:13 ] @xmath96_{+ }    & \\quad&\\st&\\quad    \\ip b y - \\epsilon\\norm{y}_*&\\ge1.\\end{aligned}\\ ] ]    the derivation of gauge dual problems relies on the polarity operation applied to gauges . when applied to a norm , for example , the polar is simply the dual norm .",
    "in contrast , lagrange duality is intimately tied to conjugacy , which is what gives rise to the dual problem .",
    "of course , the two operations are closely related . for any guage function @xmath97 , for example , @xmath98 .",
    "these relationships are described in detail by ( * ? ? ?",
    "* section  15 ) and ( * ? ? ?",
    "* section 2.3 ) .",
    "we can simplify the dual objective and safely eliminate the positive - part operator : because @xmath99 is necessarily strictly positive for all nonzero @xmath15 , and is additionally finite over the feasible set of the original problem  , it follows from   that @xmath100 is positive for all dual feasible points . in other words ,",
    "@xmath101_{+ } = \\lambda_{1}({\\mathcal{a}}^{*}y)\\ ] ] for all dual feasible points @xmath67 .",
    "hence we obtain the equivalent dual problem  .    in practice , we need to be prepared to detect whether the primal problem   is infeasible . the failure of condition   in fact furnishes a certificate of infeasibility for  : if @xmath102 for some dual - feasible vector @xmath67 , it follows from   that @xmath99 is necessarily infinite over the feasible set of  i.e .",
    ", @xmath103 for all @xmath15 feasible for  .",
    "thus ,   is infeasible .",
    "there are two key theoretical pieces needed for our approach .",
    "the first is the derivation of the eigenvalue optimization problem  , as shown in section  [ sec : gauge - duality ] .",
    "the second piece is the derivation of a subproblem that allows recovery of a primal solution @xmath15 from a solution of the eigenvalue problem  .      our derivation of a subproblem for primal recovery proceeds in two stages .",
    "the first stage develops necessary and sufficient optimality conditions for the primal - dual gauge pair   and  .",
    "the second stage uses these to derive a subproblem that can be used to recover a primal solution from a dual solution .",
    "the weak duality condition   holds for all primal - feasible pairs @xmath104 .",
    "the following result asserts that if the pair is optimal , then that inequality must necessarily hold tightly .",
    "[ prop : strong - duality ] if   is feasible and @xmath105 then @xmath106      \\cdot      \\left [        \\inf_{\\substack{y\\in{\\mathbb{r}}^m\\\\\\ip b y -            \\epsilon\\norm{y}_*\\geq1}}[\\lambda_1({\\mathcal{a}}^*y)]_+      \\right ]      = 1.\\ ] ]    we proceed by reasoning about the lagrangian - dual pair   and  .",
    "we then translate these results to the corresponding gauge - dual pair   and  .",
    "the primal problem   is feasible by assumption .",
    "because its lagrange dual problem   admits strictly feasible points ( e.g. , @xmath107 ) , it follows from ( * ? ? ?",
    "* theorems  28.2 and  28.4 ) that the primal problem attains its positive minimum value and that there is zero duality gap between the lagrange - dual pair .    moreover ,",
    "because the primal problem attains its positive minimum value for some @xmath108 , and there is zero duality gap , there exists a sequence @xmath109 such that @xmath110_+\\leq1 $ ] and @xmath111 because @xmath112 , we can take a subsequence @xmath113 for which @xmath114 is uniformly bounded above zero .",
    "define the sequence @xmath115 by @xmath116 .",
    "then @xmath117 for all @xmath46 , which is a feasible sequence for the gauge dual problem  .",
    "weak gauge duality   and the definition of @xmath118 then implies that @xmath119_+\\leq(\\ip{y_{j_k}}{b}-\\epsilon\\norm{y_{j_k}}_*)^{-1}\\searrow(\\operatorname{trace}\\xhat)^{-1}.\\ ] ] multiply the series of inequalities by @xmath120 to obtain  .",
    "note the lack of symmetry in the statement of proposition  [ prop : strong - duality ] : the primal problem is stated with a `` min '' , but the dual problem is stated with an `` inf '' .",
    "this is because the dual slater condition  i.e .",
    ", strict feasibility of the corresponding lagrange - dual problem  allows us to assert that a primal optimal solution necessarily exists .",
    "however , we can not assert in general that a dual optimal solution exists because the corresponding primal feasible set does not necessarily satisfy the slater condition .",
    "although in this work we do not attempt to delineate conditions under which dual attainment holds , a practical case in which it always does is when the primal objective is a norm and the measurement operator is surjective . in that case , the dual gauge objective @xmath121 defines a norm in @xmath122 , which has compact level sets .",
    "hence a dual solution always exists .",
    "we comment further on this theoretical question in section  [ sec : conclusions ] .",
    "the following result characterizes gauge primal - dual optimal pairs .",
    "it relies on von neumann s trace inequality : for hermitian matrices @xmath123 and @xmath124 , @xmath125 and equality holds if and only if @xmath123 and @xmath124 admit a simultaneous ordered eigendecomposition , i.e. , @xmath126u^{*}$ ] and @xmath127u^{*}$ ] for some unitary matrix  @xmath62 ; see  @xcite .",
    "[ prop : optimality ] if   is feasible and @xmath128 then @xmath129 is primal - dual optimal for the gauge dual pair and if and only if the following conditions hold :    1 .",
    "[ prop : sd : pf ] @xmath24 and @xmath130 2 .",
    "[ prop : sd : df ] @xmath131 3 .",
    "[ prop : sd : cs ] @xmath132 4 .",
    "[ prop : sd : cp ] @xmath133 , @xmath134 5 .",
    "[ prop : sd : vn ] @xmath15 and @xmath135 admit a simultaneous ordered eigendecomposition .    by strong duality ( proposition  [ prop : strong - duality ] ) , the pair @xmath136 is primal - dual optimal if and only if they are primal - dual feasible and the product of their corresponding objective values is equal to one . in this case , @xmath137         \\cdot[\\lambda_1({\\mathcal{a}}^*y)]_+\\tag{strong duality}\\\\       & = \\ip{e}{\\lambda(x)}\\cdot\\lambda_1({\\mathcal{a}}^*y)\\\\       & = \\ip{\\lambda_1({\\mathcal{a}}^*y)\\cdot e}{\\lambda(x)}\\\\       & \\ge\\ip{\\lambda({\\mathcal{a}}^*y)}{\\lambda(x)}\\tag{$\\lambda_1({\\mathcal{a}}^*y)\\geq\\lambda_i({\\mathcal{a}}^*y)$ and $ x\\succeq0$}\\\\       & \\ge\\ip{{\\mathcal{a}}^*y}{x}\\tag{von neumann 's trace inequality}\\\\       & = \\ip{y}{{\\mathcal{a}}x}\\\\       & = \\ip{y}{b}-\\ip{y}{b-{\\mathcal{a}}x}\\\\       & \\ge\\ip{y}{b}-\\norm{y}_*\\norm{b-{\\mathcal{a}}x }         \\tag{cauchy - schwartz inequality}\\\\       & \\ge\\ip{y}{b}-\\epsilon\\norm{y } _ *         \\tag{primal feasibility}\\\\       & \\ge1.\\tag{dual feasibility }    \\end{aligned}\\ ] ] thus all of the above inequalities hold with equality .",
    "this proves conditions 14 .",
    "condition 5 follows from again invoking von neumann s trace inequality and noting its implication that @xmath15 and @xmath68 share a simultaneous ordered eigenvalue decomposition .",
    "sufficiency of those conditions can be verified by simply following the reverse chain of reasoning and again noticing that the inequalities can be replaced by equalities .",
    "the optimality conditions stated in proposition  [ prop : optimality ] furnish the means for deriving a subproblem that can be used to recover a primal solution from a dual solution .",
    "the next result establishes an explicit relationship between primal solutions @xmath15 and @xmath68 for an arbitrary optimal dual solution @xmath67 .",
    "[ corl : pfd ] suppose that the conditions of proposition  [ prop : optimality ] hold .",
    "let @xmath138 be an arbitrary optimal solution for the dual gauge program  , @xmath139 be the multiplicity of @xmath140 , and @xmath141 be the matrix formed by the first @xmath142 eigenvectors of @xmath143 then a matrix @xmath144 is a solution for the primal problem   if and only if there exists an @xmath145 matrix @xmath146 such that @xmath147    the assumptions imply that the optimal dual value is positive . if @xmath138 is an optimal solution to  , the positive - homogeneity of its objective and constraint , and the positivity of the optimal value , allow us to deduce that the dual constraint must be active , i.e. , @xmath148 thus condition  [ prop : sd : df ] of proposition  [ prop : optimality ] holds .    the construction of @xmath15 in   guarantees that it shares a simultaneous ordered eigendecomposition with @xmath68 , and that it has rank of @xmath149 at most",
    "thus , conditions  [ prop : sd : cp ] and  [ prop : sd : vn ] of proposition  [ prop : optimality ] hold .",
    "we now show that conditions  1 and  3 of the proposition hold .",
    "the subdifferential @xmath150 corresponds to the set of maximizers of the linear function that defines the dual ball ; see  . then because @xmath151 it holds that @xmath152 and @xmath153 implying that @xmath154 and @xmath155 this way , condition  [ prop : sd : pf ] and  [ prop : sd : cs ] of proposition  [ prop : optimality ] are also satisfied .",
    "hence , all the conditions of the proposition are satisfied , and the pair @xmath129 is optimal .",
    "suppose now that @xmath144 is optimal for  .",
    "we can invoke proposition  [ prop : optimality ] on the pair @xmath156 condition  [ prop : sd : cp ] implies that any eigenvector of @xmath135 associated to an eigenvalue @xmath157 with @xmath158 is in the nullspace of @xmath159 therefore there is an @xmath145 matrix @xmath146 such that @xmath160 conditions  [ prop : sd : pf ] and  [ prop : sd : cs ] imply that @xmath152 and @xmath161 thus verifying that @xmath162 , as required .",
    "corollary  [ corl : pfd ] thus provides us with a way to recover a solution to our model problem   after computing a solution to the gauge dual problem  .",
    "when the residual in   is measured in the 2-norm , condition   simplifies , and implies that the matrix @xmath163 that defines @xmath164 can be obtained by solving @xmath165 when the multiplicity @xmath142 of the eigenvalue @xmath140 is much smaller than @xmath70 , this optimization problem is relatively inexpensive . in particular , if @xmath166which may be expected in some applications such as phaselift  the optimization problem is over a scalar @xmath167 that can be obtained immediately as @xmath168_{+}/\\norm{{\\mathcal{a}}(u_{1}u_{1}^*)}^{2}\\ ] ] where @xmath169 is the rightmost eigenvalue of @xmath68 .",
    "this approach exploits the complementarity relation on eigenvalues in condition  [ prop : sd : cp ] of proposition  [ prop : optimality ] to reduce the dimensionality of the primal solution recovery .",
    "its computational difficulty effectively depends on finding a dual solution @xmath67 at which the rightmost eigenvalue has low multiplicity @xmath142 .",
    "the success of our approach hinges on efficiently solving the constrained eigenvalue optimization problem   in order to generate solution estimates @xmath67 and rightmost eigenvector estimates @xmath170 of @xmath68 that we can feed to  .",
    "the two main properties of this problem that drive our approach are that it has a nonsmooth objective and that projections on the feasible set are inexpensive .",
    "our implementation is based on a basic projected - subgradient descent method , although certainly other choices are available .",
    "for example , @xcite and @xcite propose specialized algorithms for minimizing positively homogeneous functions with affine constraints ; some modification of this approach could possibly apply to  .",
    "another possible choice is helmberg and rendl s ( @xcite ) spectral bundle method . for simplicity , and because it has proven sufficient for our needs , we use a standard projected subgradient method , described below .",
    "the generic subgradient method is based on the iteration @xmath171 where @xmath172 is a subgradient of the objective at the current iterate @xmath67 , @xmath173 is a positive steplength , and the operator @xmath174 gives the euclidean projection onto the feasible set . for the objective function @xmath175 of  , the subdifferential has the form @xmath176 where @xmath170 is the @xmath177 matrix of rightmost eigenvectors of @xmath68 ( * ? ? ?",
    "* theorem  3 ) .",
    "a krylov - based eigenvalue solver can be used to evaluate @xmath178 and a subgradient @xmath179 .",
    "such methods require products of the form @xmath180 for arbitrary vectors  @xmath181 . in many cases ,",
    "these products can be computed without explicitly forming the matrix @xmath68 . in particular , for the applications described in section  [ sec : formulations ] , these products can be computed entirely using fast operators such as the fft .",
    "similar efficiencies can be used to compute a subgradient @xmath172 from the forward map @xmath182 .    for large problems ,",
    "further efficiencies can be obtained simply by computing a single eigenvector @xmath169 , i.e. , any unit - norm vector in the range of @xmath170 . in our implementation",
    ", we typically request at least _ two _ rightmost eigenpairs : this gives us an opportunity to detect if the leading eigenpair is isolated . if it is , then the subdifferential contains only a single element , which implies that @xmath74 is differentiable at that point .",
    "any sequence of step lengths @xmath183 that satisfies the generic conditions @xmath184 is sufficient to guarantee that the value of the objective at @xmath185 converges to the optimal value ( * ? ? ?",
    "* proposition 3.2.6 ) .",
    "a typical choice is @xmath186 .",
    "our implementation defaults to a barzilai - borwein steplength @xcite with a nonmonotonic linesearch @xcite if it is detected that a sequence of iterates is differentiable ( by observing separation of the leading eigenpair ) ; and otherwise it falls back to a decreasing step size .",
    "the projection operator @xmath187 onto the dual - feasible set   is inexpensive when the residual is measured in the 2-norm . in particular , if @xmath82 , the dual - feasible set is a halfspace , and the projection can be accomplished in linear time .",
    "when @xmath3 is positive , the projection requires computing the roots of a 1-dimensional degree-4 polynomial , which in practice requires little additional time .      at each iteration of the descent method   for the eigenvalue optimization problem  ,",
    "we compute a corresponding primal estimate @xmath188 maintained in factored form .",
    "the matrix @xmath170 has already been computed in the evaluation of the objective and its subgradient ; see  .",
    "the positive semidefinite matrix @xmath189 is the solution of the primal - recovery problem  .",
    "a byproduct of the primal - recovery problem is that it provides a suitable stopping criterion for the overall algorithm .",
    "because the iterations @xmath185 are dual feasible , it follows from corollary  [ corl : pfd ] that if   has a zero residual , then the dual iterate @xmath185 and the corresponding primal iterate @xmath190 are optimal .",
    "thus , we use the size of the residual to determine a stopping test for approximate optimality .      the primal - recovery procedure outlined in section  [ sec : primal - recovery ]",
    "is used only as a stopping criterion , and does not directly affect the sequence of dual iterates from  . in our numerical experiments , we find that significant gains can be had by refining the primal estimate   and feeding it back into the dual sequence .",
    "we use the following procedure , which involves two auxiliary subproblems that add relatively little to the overall cost .",
    "the first step is to refine the primal estimate obtained via   by using its solution to determine the starting point @xmath191 for the smooth unconstrained non - convex problem @xmath192 in effect , we continue to minimize  , where additionally @xmath170 is allowed to vary .",
    "several options are available for solving this smooth unconstrained problem .",
    "our implementation has the option of using a steepest - descent iteration with a spectral steplength and non - monotone linesearch @xcite , or a limited - memory bfgs method ( * ? ? ?",
    "* section 7.2 ) .",
    "the main cost at each iteration is the evaluation of the gradient @xmath193 we thus obtain a candidate improved primal estimate @xmath194 , where @xmath195 is a solution of  .",
    "when @xmath82 , this non - convex problem coincides with the problem used by @xcite .",
    "they use the initialization @xmath196 , where @xmath169 is a leading eigenvector of @xmath197 , and @xmath198 .",
    "our initialization , on the other hand , is based on a solution of the primal - recovery problem  .",
    "the second step of the refinement procedure is to construct a candidate dual estimate @xmath199 from a solution of the constrained linear - least - squares problem @xmath200 where @xmath201 is the reciprocal of the primal objective value associated with @xmath108 .",
    "this constrained linear - least - squares problem attempts to construct a vector @xmath199 such that the columns of @xmath195 correspond to eigenvectors of @xmath202 associated with @xmath203 .",
    "if @xmath204 , then @xmath199 improves on the current dual iterate @xmath205 obtained by the descent method  , and we are free to use @xmath199 in its place .",
    "this improved estimate , which is exogenous to the dual descent method , can be considered a `` spacer '' iterate , as described by ( * ? ? ?",
    "* proposition  1.2.6 ) .",
    "importantly , it does not interfere with the convergence of the underlying descent method .",
    "the projected - descent method used to solve the dual sequence can also be applied to  , though in this case the objective is guaranteed to be differentiable .",
    "the following steps summarize one iteration of the dual - descent algorithm : @xmath67 is the current dual iterate , and @xmath206 is the updated iterate",
    ". the primal iterate @xmath15 is maintained in factored form .",
    "steps 5 - 7 implement the primal - dual refinement strategy described in section  [ sec : pr - du - refinement ] , and constitute a heuristic that may improve the performance of the dual descent algorithm without sacrificing convergence guarantees .",
    "@xmath207    @xmath208    @xmath209    @xmath210 solution of    @xmath210 solution of initialized with @xmath211    @xmath210 solution of initialized with @xmath195    in step  1 , the rightmost eigenpair @xmath212 of @xmath78 is computed .",
    "the eigenvectors in the matrix @xmath213 are used in step  2 to compute a subgradient @xmath172 for the dual objective .",
    "any psd matrix @xmath214 that has trace equal to  1 can be used in step  2 .",
    "for example , the case where only a single rightmost eigenvector @xmath215 can be afforded corresponds to setting @xmath214 so that @xmath216 .",
    "step  3 is a projected subgradient iteration with steplength @xmath173 .",
    "step  4 solves the primal - recovery problem to determine the matrix @xmath211 used to define a primal estimate @xmath217 ; cf .  .",
    "we use the factorization @xmath218 to initialize the algorithm in the next step .",
    "step  5 applies an algorithm to the nonlinear least - squares problem   to obtain a stationary point @xmath195 used to define the dual - refinement problem used in the next step .",
    "step  6 computes a candidate dual solution @xmath199 that  if it improves the dual objective  is used to replace the latest dual estimate @xmath219 .",
    "this section reports on a set of numerical experiments for solving instances of the phase retrieval and blind deconvolution problems described in section  [ sec : formulations ] .",
    "the various algorithmic pieces described in section  [ sec : implementation ] have been implemented as a  software package .",
    "the implementation uses s ` eigs ` routine for the eigenvalue computations described in section  [ sec : dual - descent ] .",
    "we implemented a projected gradient - descent method , which is used for solving  , , and  .",
    "we conduct three experiments for phase retrieval via the phaselift formulation .",
    "the first experiment is for a large collection of small one - dimensional random signals , and is meant to contrast the approach against a general - purpose convex optimization algorithm and a specialized non - convex approach .",
    "the second experiment tests problems where the vector of observations @xmath5 is contaminated by noise , hence testing the case where @xmath220 .",
    "the third experiment tests the scalability of the approach on a large two - dimensional natural image .",
    "our phase retrieval experiments follow the approach outlined in  @xcite .",
    "the diagonal matrices @xmath221 encode diffraction patterns that correspond to the @xmath46th `` mask '' ( @xmath222 ) through which a signal @xmath223 is measured .",
    "the measurements are given by @xmath224,\\ ] ] where @xmath225 is the unitary discrete fourier transform ( dft ) .",
    "the adjoint of the associated linear map @xmath6 is then @xmath226 where @xmath227 and @xmath228 is the diagonal matrix formed from the vector @xmath185 .",
    "the main cost in the evaluation of the forward map @xmath229 involves @xmath230 applications of the dft for each column of @xmath231 .",
    "each evaluation of the adjoint map applied to a vector @xmath181i.e .",
    ", @xmath180requires @xmath230 applications of both the dft and its inverse . in the experimental results reported below",
    ", the columns labeled `` ndft '' indicate the total number of dft evaluations used over the course of a run .",
    "the costs of these dft evaluations are invariant across the different algorithms , and dominate the overall computation .      in this section",
    "we consider a set of experiments for different numbers of masks .",
    "for each value of @xmath232 , we generate a fixed set of  100 random complex gaussian vectors @xmath233 of length @xmath234 , and a set of  @xmath230 random complex gaussian masks @xmath235 .",
    "table  [ tab : phaselift - random ] summarizes the results of applying four different solvers to each set of 100 problems .",
    "the solver ` gauge ` is our implementation of the approach summarized in section  [ sec : algo - summary ] ; ` tfocs ` @xcite is a first - order conic solver applied to the primal problem  . the version used here",
    "was modified to avoid explicitly forming the matrix @xmath68 @xcite .",
    "the algorithm ` wflow ` @xcite is a non - convex approach that attempts to recover the original signal directly from the feasibility problem  , with @xmath82 . to make sensible performance comparisons to `",
    "wflow ` , we add to its implementation a stopping test based on the norm of the gradient  ; the default algorithm otherwise uses a fixed number of iterations .",
    "rcr<@l < cr<@l < cr<@l < rcr<@l < r & & & & + ( lr)2 -4 ( lr ) 5- 7 ( lr)8 - 11 ( l ) 12 - 15@xmath230&ndft&&ndft&&ndft&&%&ndft&&% + & & @xmath236&@xmath237&&@xmath238&@xmath237&&@xmath239&@xmath240&&&@xmath241&@xmath242 & + & & @xmath243&@xmath237&&@xmath244&@xmath237&&@xmath245&@xmath240&&&@xmath236&@xmath242 & + & & @xmath244&@xmath237&&@xmath236&@xmath237&&@xmath246&@xmath240&&&@xmath247&@xmath242 & + & & @xmath236&@xmath237&&@xmath244&@xmath237&&@xmath248&@xmath240&&&@xmath249&@xmath242 & + & & @xmath247&@xmath237&&@xmath250&@xmath237&&@xmath241&@xmath251&&&@xmath252&@xmath242 & + & & @xmath253&@xmath237&&@xmath254&@xmath237&&@xmath255&@xmath251&&&@xmath256&@xmath242 & + & & @xmath257&@xmath237&&@xmath249&@xmath237&&@xmath258&@xmath251&&&@xmath259&@xmath242 & +    rcr<@l < cr<@l < & & + ( lr)2 - 4 ( lr)5 - 7@xmath230&ndft&&ndft & + & & @xmath236&@xmath237&&@xmath236&@xmath237 + & & @xmath243&@xmath237&&@xmath236&@xmath237 + & & @xmath244&@xmath237&&@xmath254&@xmath237 + & & @xmath236&@xmath237&&@xmath250&@xmath237 + & & @xmath247&@xmath237&&@xmath250&@xmath237 + & & @xmath253&@xmath237&&@xmath260&@xmath237 + & & @xmath257&@xmath237&&@xmath261&@xmath237 +    we also show the results of applying the ` gauge ` code in a `` feasibility '' mode that exits as soon as the primal - refinment subproblem ( see step  7 of the algorithm summary in section  [ sec : algo - summary ] ) obtains a solution with a small residual . this resulting solver is labeled ` gauge - feas ` .",
    "this variant of ` gauge ` is in some respects akin to ` wflow ` , with the main difference that ` gauge - feas ` uses starting points generated by the dual - descent estimates , and generates search directions and step - lengths for the feasibility problem from a spectral gradient algorithm .",
    "the columns labeled `` xerr '' report the median relative error @xmath262 of the 100 runs , where @xmath263 is the solution returned by the corresponding solver .",
    "the columns labeled `` % '' give the percentage of problems solved to within a relative error of @xmath264 .",
    "at least on this set of artificial experiments , the ` gauge ` solver ( and its feasibility variant ` gauge - feas ` ) appear to be most efficient .",
    "table  [ tab : phaselift - random - nodfp ] provides an additional comparison of ` gauge ` with the variation ` gauge - nodfp ` , which ignores the `` spacer '' iterate computed by  .",
    "there seems to be significant practical benefit in using the refined primal estimate to improve the dual sequence .",
    "the columns labeled `` % '' are excluded for all versions of ` gauge ` because these solvers obtained the prescribed accuracy for all problems in each test set .",
    "note that the relative accuracy `` xerr '' is often slightly better for ` gauge - feas ` than for ` gauge ` .",
    "these small small discrepancies are explained by the different stopping criteria between the two versions of the solver . in particular , ` gauge ` will continue iterating past the point at which ` gauge - feas ` normally terminates because it is searching for a dual certificate that corresponds to the recovered primal estimate .",
    "this slightly changes the computed subspaces @xmath213 , which influence subsequent primal estimates .",
    "similar behaviour is exhbited in the noisy cases that we consider in the next section .",
    "as the number of measurements ( @xmath230 ) decreases , we expect the problem to be more difficult .",
    "indeed , we can observe that the total amount of work , as measured by the number of operator evaluations ( i.e. , the ratio between ` ndft ` and @xmath230 ) , increases monotonically for all variations of ` gauge ` .      in this set of experiments",
    ", we assess the effectiveness of the sdp solver to problems with @xmath220 , which could be useful in recovering signals with noise . for this purpose , it is convenient to generate problems instances with noise and known primal - dual solutions , which we can do by using corollary  [ corl : pfd ] .",
    "each instance is generated by first sampling octanary masks @xmath265as described by @xcite  and real gaussian vectors @xmath266 a solution @xmath267 is then chosen as a unit - norm rightmost eigenvector of @xmath135 , and the measurements are computed as @xmath268 where @xmath3 is chosen as @xmath269 for a given noise - level parameter @xmath270    .phase retrieval comparisons for problems with noise , i.e. , @xmath220 .",
    "numbers of the form @xmath271 are a shorthand for @xmath272 . [ cols= \" > , > , > , > , > , > , > , > , > , > \" , ]",
    "the problems   that we have considered so far are stated in their simplest form .",
    "general semidefinite optimization problems with nonnegative optimal value , and reweighted formulations for rank minimization , as introduced by @xcite and @xcite , are also useful and can be accommodated by our approach .    in the context of rank minimization over the psd cone",
    ", an approximate minimum - rank solution @xmath108 ( e.g. , computed via trace minimization ) might be used to obtain an even better approximation by using the weighted objective @xmath273 , where @xmath274 and @xmath275 is a small positive parameter .",
    "we might reasonably expect that the objective value at a minimizer of this objective more closely matches the rank function .",
    "@xcite show that such an iteratively reweighted sequence of trace minimization problems can improve the range of signals recoverable using phaselift .",
    "each problem in that sequence uses the previous solution @xmath108 to derive a weighting matrix @xmath276 for the next problem .",
    "the inverse of the matrix @xmath277 is a low - rank update @xmath278 to a small regularizing multiple @xmath275 of the identity matrix .",
    "this idea generalizes that of reweighting the @xmath279-norm for cardinality minimization problems in compressed sensing , where the number of nonzero entries of a vector @xmath42 is approximated by @xmath280 for small @xmath275 and an available approximation @xmath263 ( e.g. , computed via @xmath279-norm minimization ) .    in the next sections we derive the corresponding gauge duals for the weighted formulations of both trace minimization in the psd cone and nuclear - norm minimization .",
    "consider the semidefinite optimization problem @xmath281 where @xmath282 .",
    "define the maps @xmath283 it is evident that @xmath24 if and only if @xmath284 , and so the sdp problem can be stated equivalently as @xmath285 because @xmath286 is a bijection , we can optimize over @xmath287 instead of @xmath15 : @xmath288 this clearly falls within the structure of  , and has the corresponding gauge dual @xmath289_+    \\quad\\st\\quad      \\ip b y-\\epsilon\\norm{y}_*\\ge1.\\ ] ] observe that @xmath290 then @xmath291_+    \\quad\\st\\quad      \\ip b y - \\epsilon\\norm{y}_*\\ge1.\\ ] ]    this shows that the introduction of a weighting matrix @xmath277 that is not a simple multiple of the identity leads to a dual gauge problem involving the minimization of the rightmost _",
    "generalized _ eigenvalue of @xmath135 with respect to that weight . now that we have a formulation for the gauge dual problem , we focus on how a primal solution to the original weighted trace minimization can be computed given a dual minimizer .",
    "this extends corollary  [ corl : pfd ] .",
    "[ corl : wtmpfd ] suppose that problem   is feasible and @xmath292 let @xmath138 be an arbitrary optimal solution for the dual gauge  , @xmath139 be the multiplicity of @xmath293 , and @xmath141 be the matrix formed by the first @xmath142 generalized eigenvectors of @xmath135 with respect to @xmath294 then @xmath144 is a solution for the primal problem   if and only if there exists @xmath146 such that @xmath295    a solution for   is clearly a solution for  .",
    "we may thus invoke corollary  [ corl : pfd ] and assert that @xmath296 is a solution for   if and only if there is @xmath146 such that @xmath297 and @xmath298 where @xmath299 is a matrix formed by the first @xmath142 eigenvectors of @xmath300 from the structure of @xmath301 we have that @xmath15 is a solution to   if and only if @xmath302 thus , @xmath303 where @xmath304 corresponds to the first @xmath142 generalized eigenvectors of @xmath135 with respect to @xmath294    once again , this provides us with a way to recover a solution to the weighted trace minimization problem by computing a solution to the gauge dual problem ( now involving the rightmost generalized eigenvalue ) and then solving a problem of potentially much reduced dimensionality .",
    "we can similarly extend the reweighted extension to the asymmetric case  .",
    "let @xmath305 and @xmath306 be invertible .",
    "the weighted nuclear - norm minimization problem becomes @xmath307 define the weighted quantities @xmath308 the weighted problem can then be stated equivalently as @xmath309 which , following the approach introduced in  @xcite , can be embedded in a symmetric problem : @xmath310 define the measurement operator from @xmath311 by the map @xmath312 and identify @xmath313 with @xmath314 as a real inner - product space .",
    "the adjoint of the measurement operator is then given by @xmath315 where @xmath316 .",
    "we can now state the gauge dual problem : @xmath317_+    \\quad\\st\\quad      \\mathfrak{r}\\ip b y - \\epsilon\\norm{y}_*\\ge1.\\end{aligned}\\ ] ] observe the identity @xmath318_+",
    "\\\\&=\\left[\\norm{c_1^{-1}({\\mathcal{a}}^*y)c_2^{-*}}_\\infty\\right]_+       = \\norm{c_1^{-1}({\\mathcal{a}}^*y)c_2^{-*}}_\\infty.\\end{aligned}\\ ] ] we can now deduce the simplified form for the gauge dual problem : @xmath319    this weighted gauge dual problem can be derived from first principles using the tools from section  [ sec : gauge - duality ] by observing that the primal problem is already in standard gauge form .",
    "we chose this approach , however , to make explicit the close connection between the ( weighted ) nuclear - norm minimization problem and the ( weighted ) trace - minimization problem described in section  [ ssec : wtm ] .",
    "the following result provides a way to characterize solutions of the nuclear norm minimization problem when a solution to the dual gauge problem is available .",
    "[ corl : nnmpfd ] suppose that problem   is feasible and @xmath38 .",
    "let @xmath320 be an arbitrary optimal solution for the dual gauge problem  , @xmath139 be the multiplicity of @xmath321 @xmath322 and @xmath323 be the matrices formed by the first @xmath142 left and right singular - vectors of @xmath324 respectively .",
    "then @xmath325 is a solution for the primal problem   if and only if there exists @xmath146 such that @xmath326 and @xmath327    a solution for   is clearly a solution for  ; this way we invoke corollary  [ corl : pfd ] and have that @xmath328 induce a solution for   if and only if there is @xmath146 such that @xmath329 and @xmath298 where @xmath330 and @xmath331 are matrices formed by the first @xmath142 left and right singular - vectors of @xmath332 from the structure of @xmath301 we have that @xmath15 is a solution to   if and only if @xmath302 this way , @xmath333",
    "the phase retrieval and blind deconvolution applications are examples of convex relaxations of non - convex problems that give rise to large spectral optimization problems with strong statistical guarantees for correctly reconstructing certain signals",
    ". one of the criticisms that have been leveled at these relaxation approaches is that they lead to problems that are too difficult to be useful in practice .",
    "this has led to work on non - convex recovery algorithms that may not have as - strong statistical recovery guarantees , but are nonetheless effective in practice ; @xcite .",
    "our motivation is to determine whether it is possible to develop convex optimization algorithms that are as efficient as non - convex approaches .",
    "the numerical experiments on these problems suggest that the gauge - dual approach may prove effective .",
    "indeed , other convex optimization algorithms may be possible , and clearly the key to their success will be to leverage the special structure of these problems .",
    "a theoretical question we have not addressed is to delineate conditions under which dual attainment will hold . in particular , the conclusion   of theorem  [ prop : strong - duality ] is asymmetric : we can assert that a primal solution exists that attains the primal optimal value ( because the lagrange dual is strictly feasible ) , but we can not assert that a dual solution exists that attains the dual optimal value . a related theoretical question is to understand the relationship between the quality of suboptimal dual solutions , and the quality of the primal estimate obtained by the primal recovery procedure .",
    "in our experiments , we have observed that the rightmost eigenvalue of @xmath68 remains fairly well separated from the others across iterations .",
    "this seems to contribute to the overall effectiveness of the dual - descent method .",
    "is there a special property of these problems or of the algorithm that encourages this separation property ?",
    "it seems likely that there are solutions @xmath67 at which the objective is not differentiable , and in that case , we wonder if there are algorithmic devices that could be used to avoid such points .    the dual - descent method that we use to solve the dual subproblem ( cf .",
    "section  [ sec : dual - descent ] ) is only one possible algorithm among many .",
    "other more specialized methods , such as the spectral bundle method of @xcite , its second - order variant @xcite , or the stochastic - gradient method of @xcite , may prove effective alternatives .",
    "we have found it convenient to embed the nuclear - norm minimization problem   in the sdp formulation because it allows us to use the same solver for both problems .",
    "further efficiencies , however , may be gained by implementing a solver that applied directly to the corresponding gauge dual @xmath334 this would require an iterative solver for evaluating leading singular values and singular vectors of the asymmetric operator @xmath68 , such as propack @xcite .",
    "we extend sincere thanks to our colleague nathan krislock , who was involved in an earlier incarnation of this project , and to our colleague ting kei pong , who was our partner in establishing the crucial gauge duality theory in @xcite .",
    "we are also grateful to xiaodong li and mahdi soltanolkotabi for help with using their ` wflow ` code . finally , we wish to thank two anonymous referees who provided a careful list of comments and suggestions that helped to clarify our presentation ."
  ],
  "abstract_text": [
    "<S> various applications in signal processing and machine learning give rise to highly structured spectral optimization problems characterized by low - rank solutions . </S>",
    "<S> two important examples that motivate this work are optimization problems from phase retrieval and from blind deconvolution , which are designed to yield rank-1 solutions . an algorithm is described that is based on solving a certain constrained eigenvalue optimization problem that corresponds to the gauge dual which , unlike the more typical lagrange dual , has an especially simple constraint . </S>",
    "<S> the dominant cost at each iteration is the computation of rightmost eigenpairs of a hermitian operator . </S>",
    "<S> a range of numerical examples illustrate the scalability of the approach .    </S>",
    "<S> convex optimization , gauge duality , semidefinite optimization , sparse optimization , low - rank solutions , phase retrieval    90c15 , 90c25 </S>"
  ]
}