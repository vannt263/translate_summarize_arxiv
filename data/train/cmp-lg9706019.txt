{
  "article_text": [
    "recent advances in spoken language processing , text - to - speech , dialog modeling , and computational power have led to the development of spoken dialog agents for many types of information services .",
    "much work to date has focused on the development and evaluation of the component technologies and on the non - trivial task of making them work together .",
    "this has led to a number of fully integrated systems that can be experimentally evaluated with real users carrying out representative tasks .",
    "however , continued progress in understanding what design choices make the most effective systems requires a general spoken dialog evaluation framework that supports the quantitative comparison of different agent designs in the context of actual use . for example",
    "consider the two alternative dialog strategies for a voice email agent , exemplified by the dialogs in d and d ( a is the agent and u is the user ) :    both of these agents support the same task functionality for voice access to email , and are implemented in a system called elvis ( email voice interactive system ) . the elvis agent in d1",
    "is based on a system - initiative dialog strategy , whereas the elvis agent in d2 is based on a mixed - initiative dialog strategy .",
    "the system - initiative strategy acquires information in small increments and constrains the user to single word utterances whose content is explicitly prompted for .",
    "the mixed - initiative strategy lets the user control the dialog , does nt provide information unless the user asks for it , and allows the user random access to all the application functionality with utterances that combine a set of information requirements .",
    "it may seem obvious that the mixed - initiative strategy in d2 is preferable to the system - initiative strategy in d1 .",
    "previous work has emphasized the utility of mixed - initiative dialog strategies in advice - giving and diagnostic dialog domains @xcite .",
    "however , other work suggests that the performance of the system - initiative agent may be superior @xcite .",
    "one reason for this is the less than perfect performance of current speech recognizers .",
    "the mixed - initiative strategy requires more complex grammars , possibly resulting in higher automatic speech recognition ( asr ) error rates .",
    "this in turn may lead to a higher overall task error rate , or extremely long repair subdialogs .",
    "a second potential problem is that the mixed - initiative strategy may require users to learn what the system can understand , since the system does not explicitly prompt them with valid vocabulary .",
    "this suggests that the mixed - initiative strategy may be more suitable for experienced users .",
    "however , spoken dialog agents have rarely been evaluated in the context of repeated use by a single user @xcite , as would be expected in the case of an email agent .",
    "it is likely that ( 1 ) users become more expert over time ; and ( 2 ) in the future systems will adapt and learn .",
    "thus it is important to evaluate changes in performance over repeated user sessions .",
    "we hypothesize that the more experience a user has with the system , the better the mixed - initiative strategy will perform .",
    "this paper describes the implementation of these two dialog strategies in an agent for accessing email by phone .",
    "we present the results of an experiment in which users perform a series of tasks by interacting with an email agent using one of the dialog strategies .",
    "we also describe how our experimental results can be framed in the paradise framework for evaluating dialog agents .",
    "in order to determine the basic application requirements for email access by telephone , we conducted a wizard of oz study .",
    "the wizard simulated an email agent interacting with six users who were instructed to access their email over the phone at least twice over a four - hour period . in order to acquire a basic task model for email access over the phone ,",
    "the wizard was not restricted in any way , and users were free to use any strategy to access their mail .",
    "the study resulted in 15 dialogs , consisting of approximately 1200 utterances , which were transcribed and analyzed for key email access functions .",
    "we categorized email access functions into general categories based on the underlying application , as well as language - based requirements , such as the ability to use referring expressions to refer to messages in context ( as _ them , it , that _ ) , or by their properties such as the sender or the subject of the message . table 1 summarizes the functions used most frequently in our wizard of oz study ; these frequencies were used to prioritize the design of the email application module .",
    ".frequency of email functions from wizard - of - oz study over all dialogs [ cols=\"<,^\",options=\"header \" , ]     [ key ]    the task scenarios that the subjects were given were as follows , where scenarios 1.1 and 1.2 were done in the same conversation with elvis , similarly for 2.1 and 2.2 , and 3.1 . and 3.2 .",
    "* task 1.1 : you are working at home in the morning and plan to go directly to a meeting when you go into work .",
    "kim said she would send you a message telling you where and when the meeting is .",
    "find out * the meeting time * and * the meeting place*. * task 1.2 : the second task involves finding information in a different message .",
    "yesterday evening , you had told lee you might want to call him this morning .",
    "lee said he would send you a message telling you where to reach him .",
    "find out * lee s phone number*. * task 2.1 : when you got into work , you went directly to a meeting .",
    "since some people were late , you ve decided to call elvis to check your mail to see what other meetings may have been scheduled .",
    "find out the day , place and time of any scheduled meetings . *",
    "task 2.2 : the second task involves finding information in a different message . find out if you need to call anyone .",
    "if so , find out the number to call . *",
    "task 3.1 : you are expecting a message telling you when the discourse discussion group can meet . find out the * place * and * time * of the meeting .",
    "* task 3.2 : the second task involves finding information in a different message .",
    "your secretary has taken a phone call for you and left you a message .",
    "find out * who called * and * where * you can reach them .",
    "successful completion of a scenario requires that all attribute - values must be exchanged .",
    "the sender and subject attributes that are usable as selection criteria are known by the user at the beginning of the dialog , while the attributes to be extracted from the body of the email message are acquired from the agent in the course of the interaction .",
    "the avm representation for all six subtasks is similar to table 2 .",
    "note that the task s information - exchange requirement represented in the avm is independent of the agent strategy used to accomplish the task .",
    "experimental results were collected by three means , and a series of variables were extracted .",
    "first , all of the dialogs were recorded .",
    "this allows utterance transcription and checking aspects of the timing of the interaction , such as whether there were long delays for agent responses , and whether users barged in on agent utterances * barge in*. in addition , it was used to calculate the total time of the interaction ( the variable named * elapsed time * ) .",
    "second , the system logged the agent s dialog behavior on the basis of entering and exiting each state in the state transition table for the dialog . for each state , the system logged the number of timeout prompts ( * timeout prompts * ) , * asr rejections * , and the times the user said _ help _ ( * help requests * ) . the number of * system turns * and the number of * user turns * were calculated on the basis of this data . in addition , the asr result for the user s utterance was logged .",
    "the recordings were used in combination with the logged asr result to calculate a concept accuracy measure for each utterance .",
    "mean concept accuracy was then calculated over the whole dialog and used as a * mean recognition score * for the dialog .",
    "third , users were required to fill out the web page forms after each task specifying whether they had completed the task and the information they had acquired from the agent ( * task success * ) , e.g. the values for email.att1 and email.att2 in table 2 .",
    "in addition , users responded to a survey on their subjective evaluation of their satisfaction with the agent s performance with the following questions :    * was elvis easy to understand in this conversation ? ( * tts performance * ) * in this conversation , did elvis understand what you said ?",
    "( * asr performance * ) * in this conversation , was it easy to find the message you wanted ?",
    "( * task ease * ) * was the pace of interaction with elvis appropriate in this conversation ?",
    "( * interaction pace * ) * in this conversation , did you know what you could say at each point of the dialog ?",
    "( * user expertise * ) * how often was elvis sluggish and slow to reply to you in this conversation ? ( * system response * )",
    "* did elvis work the way you expected him to in this conversation ?",
    "( * expected behavior * ) * in this conversation , how did elvis s voice interface compare to the touch - tone interface to voice mail ? ( * comparable interface * ) * from your current experience with using elvis to get your email , do you think you d use elvis regularly to access your mail when you are away from your desk ? ( * future use * )    most question responses ranged over values such as ( _ almost never , rarely , sometimes , often , almost always _ ) , or an equivalent range .",
    "each of these responses was mapped to an integer in 1 @xmath0 5 .",
    "some questions had ( _ yes , no , maybe _ ) responses .",
    "each question emphasized the user s experience with the system in the current conversation , with the hope that satisfaction measures would indicate perceptions specific to each conversation , rather than reflecting an overall evaluation of the system over the three tasks .",
    "we calculated a * cumulative satisfaction * score for each dialog by summing the scores for each question .",
    "our goal was to compare performance differences between the mixed - initiative strategy and the system - initiative strategy , when the task is held constant , over a sequence of three equivalent tasks in which the users might be expected to learn and adapt to the system .",
    "we hypothesized that the mixed - initiative strategy might result in lower asr performance , which could potentially reduce the benefits of user initiative .",
    "in addition , we hypothesized that users might have more trouble knowing what they could say to the mixed - initiative agent , but that they would improve their knowledge over the sequence of tasks .",
    "thus we hypothesized that the system - initiative agent might be superior for the first task , but that the mixed initiative agent would have better performance by the third task .",
    "our experimental design consisted of three factors : strategy , task and subject .",
    "each of our result measures were analyzed using a three - way anova for these three factors . for each result",
    "we report f and p values indicating the statistical significance of the results .",
    "effects that are significant as a function of strategy indicate differences between the two strategies .",
    "effects that are significant as a function of task are potential indicators of learning .",
    "effects that are significant by subject may indicate problems individual subjects may have with the system , or may reflect differences in subjects attitude to the use of spoken dialog interfaces .",
    "we discuss each of these factors in turn .",
    "we first calculated task success using the @xmath1 ( kappa ) statistic @xcite .",
    "@xmath2    p(a ) is the proportion of times that the avms for the actual set of dialogs agree with the avms for the scenario keys , and p(e ) is the proportion of times that we would expect the avms for the dialogs and the keys to agree by chance . over all subjects , tasks and strategies , p(e )",
    "= .50 , p(a ) is .95 , and @xmath1 is .9 .",
    "thus users completed the assigned task in almost all cases .",
    "results of anova by strategy , task and subject showed that there were no significant differences for any factors for task success ( kappa ) , cumulative satisfaction , or elapsed time to complete the task .",
    "however there are differences in the individual satisfaction measures , which we discuss below .",
    "we believe the lack of an effect for elapsed time reflects the fact that the dominant time factor for the system is the email access application module , which was constant across strategy .",
    "* strategy effects * : as we hypothesized , the mean recognition score for the system - initiative strategy ( si ) was better than the mean recognition score for the mixed - initiative strategy ( mi ) ( df = 1 , f = 28.1 , p @xmath3 .001 ) .",
    "mean recognition score for si was .90 , while the mean recognition score for mi was .72 .",
    "furthermore , the performance ranges were different , with a minimum score for mi of .43 , as compared to a minimum score for si of .61 .",
    "the number of asr rejections was also significantly greater for the mi strategy , with the system rejecting an average of 1.33 utterances per dialog , as compared with only .44 utterances per dialog for si ( df = 1 , f = 6.35 , p@xmath3 .02 ) .",
    "however , despite the poorer asr performance that we predicted , the average number of user turns was significantly * less * for mi ( df = 1 , f= 6.25 , p @xmath3 .02 ) .",
    "the average number of user turns for si was 21.9 utterances , as compared with a mean of 15.33 for mi .",
    "we had hoped that users of the mi strategy would avail themselves of the context - specific help to learn the agent s valid vocabulary .",
    "while use of help requests was not significant by strategy , more timeout prompts were played for mi ( df = 1 , f = 62.4 , p @xmath3 .001 ) .",
    "the mean number of timeouts per dialog for si was .94 , while the mean for mi was 4.2 .",
    "timeout prompts suggest to the user what to say , and are triggered by occasions in which the user says nothing after a system utterance , perhaps because they do nt know what they * can * say . for example , the most commonly played prompt for mi was _ you can access messages using values from the sender or the subject field .",
    "if you need to know a list of senders or subjects , say ` list senders ' , or ` list subjects ' .",
    "if you want to exit the current folder , say ` i m done here ' .",
    "_    in terms of user satisfaction measures , there were no differences in the task ease measure as a function of strategy ; users did not think it was easier to find relevant messages using the si agent than the mi agent , even on the first day .",
    "users perceptions of whether elvis is sluggish to respond ( system response ) also did not vary as a function of strategy , probably because the response delays were due to the application module , which is identical for both strategies .",
    "however users did perceive differences in the interaction pace of the system .",
    "users were more likely to perceive the pace of mi as being _ too slow _",
    "( df = 1 , f = 14.01 , p @xmath4.001 ) . one possible source of this perception is that the si strategy kept users busy more of the time , specifying small incremental pieces of information .",
    "this would be consistent with claims about graphical user interfaces @xcite .",
    "thus the average pace of interaction in the si strategy would be faster , except for those interactions that finally accessed the specified email folder or message .",
    "in contrast , every mi interaction could result in accessing the email application module , so on average each interaction was slower paced , despite the fact that average task completion times were lower .",
    "there was a difference between the mi and the si agent in users perceptions of whether the agent understood them ( asr performance ) ( df = 1 , f= 14.54 , p@xmath3 .001 ) .",
    "this is consistent with the fact that the mean recognition score was much lower for mi .",
    "users also perceived the mi agent as more difficult for them to understand ( tts performance ) ( df = 1 , f = 4.30 , p @xmath3 .05 ) , possibly because the help and timeout messages had to be longer for the mi agent , in order to describe what type of input it could understand .",
    "there is a trend towards users having more difficulty knowing what to say to the mi agent ( user expertise ) ( df = 1 , f= 3.41 , p@xmath3 .07 ) .    * task effects * : several factors were also significant as a function of task . as mentioned above , factors that are significant as a function of task are potential indicators of learning effects .",
    "mean recognition score was significant as a function of task ( df = 2 , f = 4.2 , p @xmath3 .03 ) .",
    "the mean recognition score for mi for task 1 was .69 , for task 2 was .68 , and for task 3 was .80 , showing a potential learning effect of adapting to the system s language limitations over successive task .",
    "mean recognition score for si did not improve over task , in fact showing evidence that task 2 was more difficult , with task 1 mean recognition at .95 , task 2 at .82 and task 3 at .94 .",
    "the average number of asr rejections per dialog was also significant as a function of task ( df = 2 , f = 3.3 , p@xmath3 .05 ) .",
    "asr rejections averaged 1.33 for mi for task 1 , 2.0 for task 2 , and .67 for task 3 .",
    "for si , asr rejections averaged .16 for task 1 , 1.0 for task 2 , and .16 for task 3 .",
    "the number of help requests per dialog was significant as a function of task ( df = 2 , f = 4.8 , p@xmath3 .03 ) .",
    "users usually asked for help on the first task but not afterwards .",
    "users perceptions of knowing what they could say ( user expertise ) also improved over successive tasks for both versions of the system ( df = 2 , f = 4.67 , p @xmath3 .02 ) , showing the largest improvement for mi , as we hypothesized .",
    "for this question , 1 was mapped to _ almost never _ while 5 was mapped to _",
    "almost always_. the mean for si was 3.67 for task 1 , 2.83 for task 2 and 4.0 for task 3 .",
    "the mean for mi was 2.33 for task 1 , 3.00 for task 2 , and 3.67 for task 3 .",
    "thus at the beginning of the experiment , most mi subjects thought that they _",
    "rarely _ knew what to say , and by the end of the experiment , felt that they _",
    "often _ knew what to say .    * subject effects",
    "* : several factors were also significant as a function of subject .",
    "some subjects may have had an easier time using the system .",
    "there were significant differences in mean recognition score ( df = 10 , f = 2.7 , p @xmath3 .02 ) , and the frequency with which the system played timeout prompts as a function of subject ( df = 10 , f= 3.07 , p @xmath3 .01 ) .",
    "we had thought that the use of barge in might reflect learning , on the basis that as users acquired more expertise they would interrupt the system with responses to queries before the query was completed . however , there was no increase in the number of barge ins over task .",
    "there was a significant difference in the use of barge in across subjects .",
    "apparently some subjects felt more confident about interrupting elvis .",
    "it is also clear that subjects perceptions of the system varied .",
    "the user s perception that they knew what they could say ( user expertise ) differed ( df = 10 , f = 3.00 , p @xmath3 .01 ) , as well as whether elvis was easy to understand ( tts performance ) ( df = 10 , f = 3.71 , p@xmath3 .005 ) .",
    "perceptions of whether elvis was slow or sluggish to respond ( system response ) differed ( df = 10 , f = 2.96 , p @xmath3 .02 ) , as well as feelings about whether interaction pace was appropriate ( df = 10 , f = 4.84 , p @xmath3 .001 ) . finally , comparisons of elvis to the touch tone interface to voice mail ( comparable interface ) varied across users ( df = 8 , f= 3.74 , p @xmath3 .01 ) .",
    "given this experimental data , we draw on the paradise framework to estimate which factors are most significant in predicting cumulative satisfaction , and thus which factors might form the basis of a predictive performance function @xcite .",
    "the overall structure of objectives in paradise that provides the basis for estimating a performance function is shown in figure 1 .",
    "cumulative satisfaction is the user satisfaction measure in the objectives structure in figure 1 .",
    "the efficiency measures for this experiment are user turns , system turns , and elapsed time .",
    "the qualitative measures are barge ins , mean recognition score , timeout prompts , asr rejections and help requests .",
    "these qualitative measures reflect the style or the feel of the interaction .",
    "multivariate linear regression using all the objective performance measures shows that the only significant contributors to cumulative satisfaction are user turns and mean recognition score .",
    "the results of a second regression with only these factors included shows that user turns is significant at p @xmath3 .03 , and that mean recognition score is significant at p@xmath3 .0001 , and that the combination of these two variables accounts for 42% of the variance in cumulative satisfaction , the external validation criterion .",
    "kappa , as a measure of task success is not a significant variable because subjects completed the task in 33 out of 36 cases , leaving very little variance in the data .",
    "the predicted performance function is :    @xmath5    where @xmath6 is a normalization function that guarantees that the magnitude of the coefficients is independent of the scales of the factors . applying this performance function to our experimental data , independent of task , suggests that the si strategy overall performs better .",
    "the mean performance over all subjects for si is .214 , while mean performance for mi is -0.213 .",
    "however , as with the other measures , the performance of the mi strategy improves over each successive task , with performance at -0.27 for task 1 , rising to 0.125 by task 3 . continuing the trend that we observe over the three trials , it seems likely that the performance of mi would outpace that of si as users acquire more expertise .",
    "we discussed the results of an experiment comparing a mixed - initiative dialog agent with a system - initiative dialog agent , in the context of a personal agent for accessing email messages by phone .",
    "our initial hypotheses were that the system - initiative strategy would be better for inexperienced users , but that as users gained experience with the system over successive tasks , the mixed - initiative strategy would be preferred .",
    "our results demonstrated that user s satisfaction and ease of use with the mi strategy did improve over successive tasks .",
    "however , the overall performance function derived from the experimental data showed that the mi strategy did not surpass the si strategy over the three tasks .",
    "future experiments will give users more experience to test this hypothesis further .",
    "bruce buntschuh , candace kamm and russ ritenour provided useful help on questions about using the spoken dialog platform , and to our subjects for participating in the experiment .      m. danieli and e. gerbino .",
    "metrics for evaluating dialogue strategies in a spoken language system . in _ proceedings of the 1995 aaai spring symposium on empirical methods in discourse interpretation and generation _ , pages 3439 , 1995 .        c. kamm , s. narayanan , d. dutton , and r. ritenour . evaluating spoken dialog systems for telecommunication services . in _",
    "5th european conference on speech technology and communication , eurospeech 97 _ , 1997 .",
    "s.  m. marcus , d.  w. brown , r.  g. goldberg , m.  s. schoeffler , w.  r. wetzel , and r.  r. rosinski .",
    "prompt constrained natural language- evolving the next generation of telephony services . in _ proceedings of the international conference on spoken language processing icslp _ , pages 85760 , 1996 .",
    "j.  potjer , a.  russel , l.  boves , and e.  den os .",
    "subjective and objective evaluation of two types of dialogues in a call assistance service . in _",
    "1996 ieee third workshop : interactive voice technology for telecommunications applications , ivtta _ , pages 8992 .",
    "ieee , 1996 .",
    "e. shriberg , e. wade , and p. price .",
    "human - machine problem solving using spoken language systems ( sls ) : factors affecting performance and user satisfaction . in",
    "_ proceedings of the darpa speech and nl workshop _ , pages 4954 , 1992 .",
    "m.  a. walker , d. litman , c. kamm , and a. abella .",
    "paradise : a general framework for evaluating spoken dialogue agents . in _ proceedings of the 35th annual meeting of the association of computational linguistics ,",
    "acl / eacl 97 _ ,"
  ],
  "abstract_text": [
    "<S> this paper reports experimental results comparing a mixed - initiative to a system - initiative dialog strategy in the context of a personal voice email agent . to independently test the effects of dialog strategy and user expertise , </S>",
    "<S> users interact with either the system - initiative or the mixed - initiative agent to perform three successive tasks which are identical for both agents . </S>",
    "<S> we report performance comparisons across agent strategies as well as over tasks . </S>",
    "<S> this evaluation utilizes and tests the paradise evaluation framework , and discusses the performance function derivable from the experimental data . </S>"
  ]
}