{
  "article_text": [
    "david  w.  hogg + _ center  for  cosmology  and  particle  physics , department  of  physics , new york university _",
    "+ _ max - planck - institut fr astronomie , heidelberg _",
    "+ jo  bovy + _ center  for  cosmology  and  particle  physics , department  of  physics , new york university _ + dustin  lang + _ department of computer science , university of toronto _ + _ princeton university observatory _",
    "it is conventional to begin any scientific document with an introduction that explains why the subject matter is important .",
    "let us break with tradition and observe that in almost all cases in which scientists fit a straight line to their data , they are doing something that is simultaneously _",
    "wrong _ and _ unnecessary_. it is wrong because circumstances in which a set of two dimensional measurements  outputs from an observation , experiment , or calculation  are truly drawn from a narrow , linear relationship is exceedingly rare .",
    "indeed , almost any transformation of coordinates renders a truly linear relationship non - linear .",
    "furthermore , even if a relationship _ looks _ linear , unless there is a confidently held theoretical reason to believe that the data are generated from a linear relationship , it probably is nt linear in detail ; in these cases fitting with a linear model can introduce substantial systematic error , or generate apparent inconsistencies among experiments that are intrinsically consistent .",
    "even if the investigator does nt care that the fit is wrong , it is likely to be unnecessary .",
    "because it is rare that , given a complicated observation , experiment , or calculation , the important _ result _ of that work to be communicated forward in the literature and seminars is the _ slope and intercept _ of a best - fit line ! usually the full distribution of data is much more rich , informative , and important than any simple metrics made by fitting an overly simple model .",
    "that said , it must be admitted that one of the most effective ways to communicate scientific results is with catchy punchlines and compact , approximate representations , even when those are unjustified and unnecessary .",
    "it can also sometimes be useful to fit a simple model to predict new data , given existing data , even in the absence of a physical justification for the fit .",
    "for these reasons  and the reason that in rare cases the fit _ is _ justifiable and essential ",
    "the problem of fitting a line to data comes up very frequently in the life of a scientist .",
    "it is a miracle with which we hope everyone reading this is familiar that _ if _ you have a set of two - dimensional points @xmath0 that depart from a perfect , narrow , straight line @xmath1 only by the addition of gaussian - distributed noise of known amplitudes in the @xmath2 direction only , _ then _ the maximum - likelihood or best - fit line for the points has a slope @xmath3 and intercept @xmath4 that can be obtained justifiably by a perfectly linear matrix - algebra operation known as `` weighted linear least - square fitting '' .",
    "this miracle deserves contemplation .",
    "once any of the input assumptions is violated ( and note there are many input assumptions ) , all bets are off , and there are no consensus methods used by scientists across disciplines .",
    "indeed , there is a large literature that implies that the violation of the assumptions opens up to the investigator a wide range of possible _ options _ with little but aesthetics to distinguish them !",
    "most of these methods of `` linear regression '' ( a term we avoid ) performed in science are either unjustifiable or else unjustified in almost all situations in which they are used . perhaps because of this plethora of options , or perhaps because there is no agreed - upon bible or cookbook to turn to , or perhaps because most investigators would rather make some stuff up that works `` good enough '' under deadline , or perhaps because many realize , deep down , that much fitting is really unnecessary anyway , there are some egregious procedures and associated errors and absurdities in the literature .",
    "when an investigator cites , relies upon , or transmits a best - fit slope or intercept reported in the literature , he or she may be propagating more noise than signal .",
    "it is one of the objectives of this document  to promote an understanding that if the data can be _ modeled _ statistically there is little arbitrariness .",
    "it is another objective to promote consensus ; when the choice of method is not arbitrary , consensus will emerge automatically .",
    "we present below simple , straightforward , comprehensible , and  above all_justifiable _ methods for fitting a straight line to data with general , non - trivial , and uncertain properties .",
    "this document  is part polemic ( though we have tried to keep most of the polemic in the notes ) , part attempt at establishing standards , and part crib sheet for our own future re - use .",
    "we apologize in advance for some astrophysics bias and failure to review basic ideas of linear algebra , function optimization , and practical statistics .",
    "very good reviews of the latter exist in abundance .",
    "we also apologize for not reviewing the literature ; this is a cheat - sheet not a survey .",
    "our focus is on the specific problems of linear fitting that we so often face ; the general ideas appear in the context of these concrete and relatively realistic example problems .",
    "the reader is encouraged to do the exercises ; many of these ask you to produce plots which can be compared with the s.",
    "you have a set of @xmath5 points @xmath6 , with known gaussian uncertainties @xmath7 in the @xmath2 direction , and no uncertainty at all ( that is , perfect knowledge ) in the @xmath8 direction .",
    "you want to find the function @xmath9 of the form @xmath10 where @xmath3 is the slope and @xmath4 is the intercept , that _",
    "best fits _ the points .",
    "what is meant by the term `` best fit '' is , of course , very important ; we are making a choice here to which we will return . for now , we describe standard practice :    construct the matrices @xmath11 \\quad , \\\\ { { \\boldsymbol{a}}}&= \\left[\\begin{array}{cc } 1 & x_1 \\\\ 1 & x_2 \\\\ \\multicolumn{2}{c}{\\cdots } \\\\ 1 & x_n \\end{array}\\right ] \\quad , \\\\ { { \\boldsymbol{c } } } & = \\left[\\begin{array}{cccc } \\sigma_{y1}^{2 } & 0 & \\cdots & 0 \\\\ 0 & \\sigma_{y2}^{2 } & \\cdots & 0 \\\\ \\multicolumn{4}{c}{\\cdots } \\\\ 0 & 0 & \\cdots & \\sigma_{yn}^{2 } \\end{array}\\right ] \\quad , \\label{eq : covar}\\end{aligned}\\ ] ] where one might call @xmath12 a `` vector '' , and the covariance matrix @xmath13 could be generalized to the non - diagonal case in which there are covariances among the uncertainties of the different points .",
    "the best - fit values for the parameters @xmath3 and @xmath4 are just the components of a column vector @xmath14 found by @xmath15   = { { \\boldsymbol{x}}}= { { \\left[{{{{{\\boldsymbol{a}}}}^{\\scriptscriptstyle \\top}}}\\,{{{{{\\boldsymbol{c}}}}^{-1}}}\\,{{\\boldsymbol{a}}}\\right]}^{-1 } }    \\,\\left[{{{{{\\boldsymbol{a}}}}^{\\scriptscriptstyle \\top}}}\\,{{{{{\\boldsymbol{c}}}}^{-1}}}\\,{{\\boldsymbol{y}}}\\right ] \\quad .\\ ] ] this is actually the simplest thing that can be written down that is linear , obeys matrix multiplication rules , and has the right relative sensitivity to data of different statistical significance .",
    "it is not just the simplest thing ; it is the correct thing , when the assumptions hold .",
    "it can be justified in one of several ways ; the linear algebra justification starts by noting that you want to solve the equation @xmath16 but you ca nt because that equation is over - constrained .",
    "so you weight everything with the inverse of the covariance matrix ( as you would if you were doing , say , a weighted average ) , and then left - multiply everything by @xmath17 to reduce the dimensionality , and then equation  ( [ eq : lsf ] ) is the solution of that reduced - dimensionality equation .",
    "this procedure is not arbitrary ; it minimizes an objective function @xmath18 ( `` chi - squared '' ) , which is the total squared error , scaled by the uncertainties , or @xmath19",
    "^ 2}{\\sigma_{yi}^2 }   \\equiv { { \\left[{{\\boldsymbol{y}}}-{{\\boldsymbol{a}}}\\,{{\\boldsymbol{x}}}\\right]}^{\\scriptscriptstyle \\top } }   \\,{{{{{\\boldsymbol{c}}}}^{-1}}}\\,\\left[{{\\boldsymbol{y}}}-{{\\boldsymbol{a}}}\\,{{\\boldsymbol{x}}}\\right ]   \\quad , \\ ] ] that is , equation  ( [ eq : lsf ] ) yields the values for @xmath3 and @xmath4 that minimize @xmath18 .",
    "conceptually , @xmath18 is like a metric distance in the data space .",
    "this , of course , is only one possible meaning of the phrase `` best fit '' ; that issue is addressed more below .    when the uncertainties are gaussian and their variances @xmath7 are correctly estimated , the matrix @xmath20}^{-1}}$ ] that appears in equation  ( [ eq : lsf ] ) is just the covariance matrix ( gaussian uncertainty ",
    "uncertainty not error  variances on the diagonal , covariances off the diagonal ) for the parameters in @xmath14 .",
    "we will have more to say about this in section  [ sec : uncertainty ] .",
    "rrrrrr@.l 1 & 201 & 592 & 61 & 9 & -0 & 84 + 2 & 244 & 401 & 25 & 4 & 0 & 31 + 3 & 47 & 583 & 38 & 11 & 0 & 64 + 4 & 287 & 402 & 15 & 7 & -0 & 27 + 5 & 203 & 495 & 21 & 5 & -0 & 33 + 6 & 58 & 173 & 15 & 9 & 0 & 67 + 7 & 210 & 479 & 27 & 4 & -0 & 02 + 8 & 202 & 504 & 14 & 4 & -0 & 05 + 9 & 198 & 510 & 30 & 11 & -0 & 84 + 10 & 158 & 416 & 16 & 7 & -0 & 69 + 11 & 165 & 393 & 14 & 5 & 0 & 30 + 12 & 201 & 442 & 25 & 5 & -0 & 46 + 13 & 157 & 317 & 52 & 5 & -0 & 03 + 14 & 131 & 311 & 16 & 6 & 0 & 50 + 15 & 166 & 400 & 34 & 6 & 0 & 73 + 16 & 160 & 337 & 31 & 5 & -0 & 52 + 17 & 186 & 423 & 42 & 9 & 0 & 90 + 18 & 125 & 334 & 26 & 8 & 0 & 40 + 19 & 218 & 533 & 16 & 6 & -0 & 78 + 20 & 146 & 344 & 22 & 5 & -0 & 56 + [ table : data_allerr ]    [ prob : easy ] using the standard linear algebra method of this section , fit the straight line @xmath1 to the @xmath8 , @xmath2 , and @xmath21 values for data points 5 through 20 in  [ table : data_allerr ] on page  .",
    "that is , ignore the first four data points , and also ignore the columns for @xmath22 and @xmath23 .",
    "make a plot showing the points , their uncertainties , and the best - fit line .",
    "your plot should end up looking like  [ fig : easy ] .",
    "what is the standard uncertainty variance @xmath24 on the slope of the line ?    : the standard weighted least - square fit.,scaledwidth=50.0% ]    [ prob : standard ] repeat exercise  [ prob : easy ] but for all the data points in  [ table : data_allerr ] on page  .",
    "your plot should end up looking like  [ fig : standard ] .",
    "what is the standard uncertainty variance @xmath24 on the slope of the line ?",
    "is there anything you do nt like about the result ?",
    "is there anything different about the new points you have included beyond those used in exercise  [ prob : easy ] ?    : the standard weighted least - square fit but now on a larger data set.,scaledwidth=50.0% ]    [ prob : quadratic ] generalize the method of this section  to fit a general quadratic ( second order ) relationship .",
    "add another column to matrix @xmath25 containing the values @xmath26 , and another element to vector @xmath14 ( call it @xmath27 )",
    ". then re - do exercise  [ prob : easy ] but fitting for and plotting the best quadratic relationship @xmath28 your plot should end up looking like  [ fig : quadratic ] .",
    ": the standard weighted least - square fit but now with a second - order ( quadratic ) model.,scaledwidth=50.0% ]",
    "a scientist s justification of equation  ( [ eq : lsf ] ) can not appeal purely to abstract ideas of linear algebra , but must originate from the scientific question at hand . here and in what follows , we will advocate that the only reliable procedure is to use all one s knowledge about the problem to construct a ( preferably ) justified , ( necessarily ) scalar ( or , at a minimum , one - dimensional ) , _ objective function _ that represents monotonically the quality of the fit . in this framework ,",
    "fitting anything to anything involves a scientific question about the objective function representing `` goodness of fit '' and then a separate and subsequent engineering question about how to _ find the optimum _ and , possibly , the posterior probability distribution function around that optimum .",
    "note that in the previous section  we did _ not _ proceed according to these rules ; indeed the procedure was introduced prior to the objective function , and the objective function was not justified .    in principle , there are many choices for objective function .",
    "but the only procedure that is truly justified  in the sense that it leads to interpretable probabilistic inference , is to make a _ generative model _ for the data .",
    "a generative model is a parameterized , quantitative description of a statistical procedure that could reasonably have generated the data set you have . in the case of the straight line",
    "fit in the presence of known , gaussian uncertainties in one dimension , one can create this generative model as follows : imagine that the data _ really do _",
    "come from a line of the form @xmath29 , and that the only reason that any data point deviates from this perfect , narrow , straight line is that to each of the true @xmath2 values a small @xmath2-direction offset has been added , where that offset was drawn from a gaussian distribution of zero mean and known variance @xmath30 . in this model ,",
    "given an independent position @xmath31 , an uncertainty @xmath7 , a slope @xmath3 , and an intercept @xmath4 , the frequency distribution @xmath32 for @xmath33 is @xmath34 ^ 2}{2\\,\\sigma_{yi}^2}\\right ) \\quad , \\ ] ] where this gives the expected frequency ( in a hypothetical set of repeated experiments ) of getting a value in the infinitesimal range @xmath35 $ ] per unit @xmath36 .",
    "the generative model provides us with a natural , justified , scalar objective : we seek the line ( parameters @xmath3 and @xmath4 ) that maximize the probability of the observed data given the model or ( in standard parlance ) the _ likelihood of the parameters_. in our generative model the data points are independently drawn ( implicitly ) , so the likelihood @xmath37 is the product of conditional probabilities @xmath38 taking the logarithm , @xmath39 ^ 2}{2\\,\\sigma_{yi}^2 } \\nonumber\\\\   & = & k - \\frac{1}{2}\\,\\chi^2 \\quad , \\end{aligned}\\ ] ] where @xmath40 is some constant .",
    "this shows that likelihood maximization is identical to @xmath18 minimization and we have justified , scientifically , the procedure of the previous section .    the bayesian generalization of this is to say that @xmath41 where @xmath3 and @xmath4 are the model parameters , @xmath42 is a short - hand for all the data @xmath33 , @xmath43 is a short - hand for all the prior knowledge of the @xmath31 and the @xmath7 and everything else about the problem , @xmath44 is the _ posterior _ probability distribution for the parameters given the data and the prior knowledge , @xmath45 is the likelihood @xmath37 just computed in equation  ( [ eq : like ] ) ( which is a frequency distribution for the data ) , @xmath46 is the _ prior _ probability distribution for the parameters that represents all knowledge _ except _ the data @xmath42 , and the denominator can  for our purposes  be thought of as a normalization constant , obtained by marginalizing the numerator over all parameters . unless the prior @xmath46 has a lot of variation in the region of interest , the posterior distribution function @xmath44 here is going to look very similar to the likelihood function in equation  ( [ eq : like ] ) above .",
    "that is , with an uninformative prior , the straight line that maximizes the likelihood will come extremely close to maximizing the posterior probability distribution function ; we will return to this later .",
    "we have succeeded in justifying the standard method as optimizing a justified , scalar objective function ; the likelihood of a generative model for the data .",
    "it is just the great good luck of gaussian distributions that the objective can be written as a quadratic function of the data .",
    "this makes the optimum a linear function of the data , and therefore trivial to obtain .",
    "this is a miracle .",
    "imagine a set of @xmath47 measurements @xmath48 , with uncertainty variances @xmath49 , all of the same ( unknown ) quantity @xmath50 .",
    "assuming the generative model that each @xmath48 differs from @xmath50 by a gaussian - distributed offset , taken from a gaussian with zero mean and variance @xmath49 , write down an expression for the log likelihood @xmath51 for the data given the model parameter @xmath50 .",
    "take a derivative and show that the maximum likelihood value for @xmath50 is the usual weighted mean .",
    "take the matrix formulation for @xmath18 given in equation  ( [ eq : chisquared ] ) and take derivatives to show that the minimum is at the matrix location given in equation  ( [ eq : lsf ] ) .",
    "the standard linear fitting method is very sensitive to _ outliers _ , that is , points that are substantially farther from the linear relation than expected  or not on the relation at all  because of unmodeled experimental uncertainty or unmodeled but rare sources of noise ( or because the model has a restricted scope and does nt apply to all the data ) .",
    "there are two general approaches to mitigating this sensitivity , which are not necessarily different .",
    "the first is to find ways to objectively remove , reject , or become insensitive to `` bad '' points ; this is the subject of this section .",
    "the second is to better model your data - point uncertainties , permitting larger deviations than the gaussian noise estimates ; this is the subject of section  [ sec : non - gaussian ] . both of these are strongly preferable to sorting through the data and rejecting points by hand , for reasons of subjectivity and irreproducibility that we need not state .",
    "they are also preferable to the standard ( in astrophysics anyway ) procedure known as `` sigma clipping '' , which is a procedure and not the outcome of justifiable modeling .",
    "if we want to explicitly and objectively reject bad data points , we must add to the problem a set of @xmath47 binary integers @xmath52 , one per data point , each of which is unity if the @xmath53th data point is good , and zero if the @xmath53th data point is bad .",
    "in addition , to construct an objective function , one needs a parameter @xmath54 , the _ prior _ probability that any individual data point is bad , and parameters @xmath55 , the mean and variance of the distribution of bad points ( in @xmath2 ) .",
    "we need these latter parameters because , as we have emphasized above , our inference comes from evaluating a probabilistic generative model for the data , and that model must generate the bad points as well as the good points !    all these @xmath56 extra parameters @xmath57 may seem like crazy baggage , but their values can be _ inferred _ and _ marginalized out _ so in the end , we will be left with an inference just of the line parameters @xmath58 . in general",
    ", there is no reason to be concerned just because you have more parameters than data .",
    "however , the marginalization will require that we have a _ measure _ on our parameters ( integrals require measures ) and that measure is provided by a prior .",
    "that is , the marginalization will require , technically , that we become bayesian ; more on this below .    in this case , the likelihood is @xmath59^{q_i}\\ ,   \\left[{p_{\\mathrm{bg}}}({{\\{{y_i}\\}_{{i=1}}^{{n}}}}|{y_{\\mathrm{b}}},{v_{\\mathrm{b}}},i)\\right]^{[1-q_i ] }   \\nonumber\\\\ { \\mathscr{l}}&= & \\prod_{i=1}^n \\left[\\frac{1}{\\sqrt{2\\,\\pi\\,\\sigma_{yi}^2 } }   \\,\\exp\\left(-\\frac{[y_i - m\\,x_i - b]^2}{2\\,\\sigma_{yi}^2}\\right)\\right]^{q_i }   \\nonumber \\\\ & & \\quad\\times   \\left[\\frac{1}{\\sqrt{2\\,\\pi\\,[{v_{\\mathrm{b}}}+\\sigma_{yi}^2 ] } }   \\,\\exp\\left(-\\frac{[y_i-{y_{\\mathrm{b}}}]^2}{2\\,[{v_{\\mathrm{b}}}+\\sigma_{yi}^2]}\\right)\\right]^{[1-q_i ] }   \\quad , \\end{aligned}\\ ] ] where @xmath60 and @xmath61 are the generative models for the foreground ( good , or straight - line ) and background ( bad , or outlier ) points . because we are permitting data rejection",
    ", there is an important prior probability on the @xmath62 that penalizes each rejection : @xmath63^{q_i}\\,{p_{\\mathrm{b}}}^{[1-q_i ] }   \\quad , \\end{aligned}\\ ] ] that is , the binomial probability of the particular sequence @xmath62 .",
    "the priors on the other parameters @xmath64 should either be set according to an analysis of your prior knowledge or else be uninformative ( as in flat in @xmath54 in the range @xmath65 $ ] , flat in @xmath66 in some reasonable range , and flat in the logarithm of @xmath67 in some reasonable range ) .",
    "if your data are good , it wo nt matter whether you choose uninformative priors or priors that really represent your prior knowledge ; in the end good data dominate either .",
    "we have made a somewhat restrictive assumption that all data points are equally likely _ a  priori _",
    "to be bad , but you rarely know enough about the badness of your data to assume anything better .",
    "the fact that the prior probabilities @xmath54 are all set equal , however , does not mean that the _ posterior _ probabilities that individual points are bad will be at all similar .",
    "indeed , this model permits an objective ranking ( and hence public embarassment ) of different contributions to any data set .    we have used in this likelihood formulation a gaussian model for the bad points ; is that permitted ?",
    "well , we do nt know much about the bad points ( that is one of the things that makes them bad ) , so this gaussian model must be wrong in detail .",
    "but the power of this and the methods to follow comes not from making an _ accurate _ model of the outliers , it comes simply from _ modeling _ them .",
    "if you have an accurate or substantiated model for your bad data , use it by all means .",
    "if you do nt , because the gaussian is the maximum - entropy distribution described by a mean and a variance , it is  in some sense  the _ least _ restrictive assumption , given that we have allowed that mean and variance to vary in the fitting ( and , indeed , we will marginalize over both in the end ) .",
    "the posterior probability distribution function , in general , is the likelihood times the prior , properly normalized . in this case",
    ", the posterior we ( tend to ) care about is that for the line parameters @xmath58 .",
    "this is the full posterior probability distribution marginalized over the bad - data parameters @xmath57 .",
    "defining @xmath68 for brevity , the full posterior is @xmath69 where the denominator can  for our purposes here  be thought of as a normalization integral that makes the posterior distribution function integrate to unity .",
    "the marginalization looks like @xmath70 where by the integral over @xmath71 we really mean a sum over all of the @xmath72 possible settings of the @xmath52 , and the integrals over the other parameters are over their entire prior support .",
    "effectively , then , this marginalization involves evaluating @xmath72 different likelihoods and marginalizing each one of them over @xmath64 ! of course ,",
    "because very few points are true candidates for rejection , there are many ways to `` trim the tree '' and do this more quickly , but thought is not necessary : marginalization is in fact analytic .",
    "marginalization of this ",
    "what we will call `` exponential''model leaves a much more tractable  what we will call `` mixture''model .",
    "imagine marginalizing over an individual @xmath52 .",
    "after this marginalization , the @xmath53th point can be thought of as being drawn from a _ mixture _ ( sum with amplitudes that sum to unity ) of the straight - line and outlier distributions .",
    "that is , the generative model for the data @xmath42 integrates ( or sums , really ) to @xmath73   \\nonumber\\\\ { \\mathscr{l}}&\\propto &   \\prod_{i=1}^n \\left[\\frac{1-{p_{\\mathrm{b}}}}{\\sqrt{2\\,\\pi\\,\\sigma_{yi}^2 } }   \\,\\exp\\left(-\\frac{[y_i - m\\,x_i - b]^2}{2\\,\\sigma_{yi}^2}\\right)\\right .",
    "\\nonumber \\\\ & & \\quad   \\left.+ \\frac{{p_{\\mathrm{b}}}}{\\sqrt{2\\,\\pi\\,[{v_{\\mathrm{b}}}+\\sigma_{yi}^2 ] } }   \\,\\exp\\left(-\\frac{[y_i-{y_{\\mathrm{b}}}]^2}{2\\,[{v_{\\mathrm{b}}}+\\sigma_{yi}^2]}\\right)\\right ]   \\quad , \\end{aligned}\\ ] ] where @xmath60 and @xmath61 are the generative models for the foreground and background points , @xmath54 is the probability that a data point is bad ( or , more properly , the amplitude of the bad - data distribution function in the mixture ) , and @xmath55 are parameters of the bad - data distribution .",
    "marginalization of the posterior probability distribution function generated by the product of the mixture likelihood in equation  ( [ eq : mixture ] ) times a prior on @xmath74 does _ not _ involve exploring an exponential number of alternatives .",
    "an integral over only three parameters produces the marginalized posterior probability distribution function for the straight - line parameters @xmath58 .",
    "this scale of problem is very well - suited to simple multi - dimensional integrators , in particular it works very well with a simple markov - chain monte carlo , which we advocate below .",
    "we have lots to discuss .",
    "we have gone from a procedure ( section  [ sec : standard ] ) to an objective function ( section  [ sec : objective ] ) to a general expression for a non - trivial posterior probability distribution function involving priors and multi - dimensional integration .",
    "we will treat each of these issues in turn : we must set the prior , we must find a way to integrate to marginalize the distribution function , and then we have to decide what to do with the marginalized posterior : optimize it or sample from it ?    it is unfortunate that prior probabilities must be specified .",
    "conventional frequentists take this problem to be so severe that for some it invalidates bayesian approaches entirely ! but a prior is a _ necessary condition _ for marginalization , and marginalization is necessary whenever extra parameters are introduced beyond the two @xmath58 that we care about for our straight - line fit .",
    "furthermore , in any situation of straight - line fitting where the data are numerous and generally good , these priors do not matter very much to the final answer .",
    "the way we have written our likelihoods , the peak in the posterior distribution function becomes very directly analogous to what you might call the maximum - likelihood answer when the ( improper , non - normalizable ) prior `` flat in @xmath3 , flat in @xmath4 '' is adopted , and something sensible has been chosen for @xmath64 .",
    "usually the investigator _ does _ have some basis for setting some more informative priors , but a deep and sensible discussion of priors is beyond the scope of this document .",
    "suffice it to say , in this situation  pruning bad data  the setting of priors is the _",
    "least _ of one s problems .",
    "the second unfortunate thing about our problem is that we must marginalize .",
    "it is also a _ fortunate _ thing that we _ can _ marginalize .",
    "as we have noted , marginalization is necessary if you want to get estimates with uncertainties or confidence intervals in the straight - line parameters @xmath58 that properly account for their covariances with the nuisance parameters @xmath64 .",
    "this marginalization can be performed by direct numerical integration ( think gridding up the nuisance parameters , evaluating the posterior probability at every grid point , and summing ) , or it can be performed with some kind of sampler , such as a monte - carlo markov chain .",
    "we strongly recommend the latter because in situations where the integral is of a _ probability distribution _",
    ", not only does the mcmc scheme integrate it , it also produces a _ sampling _ from the distribution as a side - product for free .    the standard metropolis ",
    "hastings mcmc procedure works extremely well in this problem for both the optimization and the integration and sampling .",
    "a discussion of this algorithm is beyond the scope of this document , but briefly the algorithm is this simple : starting at some position in parameter space , where you know the posterior probability , iterate the following : _ ( a ) _",
    "step randomly in parameter space . _",
    "( b ) _  compute the new posterior probability at the new position in parameter space . _",
    "( c ) _  draw a random number @xmath75 in the range @xmath76 .",
    "_ ( d ) _  if @xmath75 is less than the ratio of the new posterior probability to the old ( pre - step ) probability , accept the step , add the new parameters to the chain ; if it is greater , reject the step , re - add the old parameters to the chain .",
    "the devil is in the details , but good descriptions abound and that with a small amount of experimentation ( try gaussians for the proposal distribution and scale them until the acceptance ratio is about half ) rapid convergence and sampling is possible in this problem . a large class of mcmc algorithms exists , many of which outperform the simple metropolis algorithm with little extra effort .",
    "once you have run an mcmc or equivalent and obtained a chain of samples from the posterior probability distribution for the full parameter set @xmath74 , you still must decide what to report about this sampling .",
    "a histogram of the samples in any one dimension is an approximation to the fully marginalized posterior probability distribution for that parameter , and in any two is that for the pair .",
    "you can find the `` maximum _ a  posteriori _ '' ( or map ) value , by taking the peak of the marginalized posterior probability distribution for the line parameters @xmath58 .",
    "this , in general , will be different from the @xmath58 value at the peak of the unmarginalized probability distribution for _ all _ the parameters @xmath74 , because the posterior distribution can be very complex in the five - dimensional space . because , for our purposes , the three parameters @xmath64 are nuisance parameters , it is more correct to take the `` best - fit '' @xmath58 from the marginalized distribution .",
    "of course finding the map value from a sampling is not trivial . at best",
    ", you can take the highest - probability sample .",
    "however , this works only for the unmarginalized distribution .",
    "the map value from the marginalized posterior probability distribution function for the two line parameters @xmath58 will not be the value of @xmath58 for the highest - probability sample . to find the map value in the marginalized distribution",
    ", it will be necessary to make a histogram or perform density estimation in the sample ; this brings up an enormous host of issues way beyond the scope of this document . for this reason",
    ", we usually advise just taking the mean or median or some other simple statistic on the sampling .",
    "it is also possible to establish confidence intervals ( or credible intervals ) using the distribution of the samples .",
    "while the map answer  or any other simple `` best answer''is interesting , it is worth keeping in mind the third unfortunate thing about any bayesian method : it does not return an `` answer '' but rather it returns a _ posterior probability distribution_. strictly , this posterior distribution function _ is _ your answer .",
    "however , what scientists are usually doing is not inference but rather _ decision - making_. that is , the investigator wants a specific answer , not a distribution .",
    "there are ( at least ) two reactions to this .",
    "one is to ignore the fundamental bayesianism at the end , and choose simply the map answer .",
    "this is the bayesian s analog of maximum likelihood ; the standard uncertainty on the map answer would be based on the peak curvature or variance of the marginalized posterior distribution .",
    "the other reaction is to suck it up and sample the posterior probability distribution and carry forward not one answer to the problem but @xmath77 answers , each of which is drawn fairly and independently from the posterior distribution function .",
    "the latter is to be preferred because _ ( a ) _  it shows the uncertainties very clearly , and _",
    "( b ) _  the sampling can be carried forward to future inferences as an approximation to the posterior distribution function , useful for propagating uncertainty , or standing in as a _",
    "prior _ for some subsequent inference .",
    "it is also returned , trivially ( as we noted above ) by the mcmc integrator we advised for performing the marginalization .",
    "[ prob : mixture ] using the mixture model proposed above  that treats the distribution as a mixture of a thin line containing a fraction @xmath78 $ ] of the points and a broader gaussian containing a fraction @xmath54 of the points  find the best - fit ( the maximum _ a  posteriori _ ) straight line @xmath1 for the @xmath8 , @xmath2 , and @xmath21 for the data in  [ table : data_allerr ] on page  .",
    "before choosing the map line , marginalize over parameters @xmath64 .",
    "that is , if you take a sampling approach , this means sampling the full five - dimensional parameter space but then choosing the peak value in the histogram of samples in the two - dimensional parameter space @xmath58 .",
    "make one plot showing this two - dimensional histogram , and another showing the points , their uncertainties , and the map line .",
    "how does this compare to the standard result you obtained in exercise  [ prob : standard ] ?",
    "do you like the map line better or worse ? for extra credit , plot a sampling of 10 lines drawn from the marginalized posterior distribution for @xmath58 ( marginalized over @xmath79 ) and plot the samples as a set of light grey or transparent lines .",
    "your plot should look like  [ fig : mixture ] .",
    ": on the left , a sampling approximation to the marginalized posterior probability distribution ( left ) for the outlier ( mixture ) model . on the right , the marginalized map line ( dark grey line ) and a draw from the sampling ( light grey lines).,title=\"fig:\",scaledwidth=50.0% ] : on the left , a sampling approximation to the marginalized posterior probability distribution ( left ) for the outlier ( mixture ) model . on the right , the marginalized map line ( dark grey line ) and a draw from the sampling ( light grey lines).,title=\"fig:\",scaledwidth=50.0% ]    [ prob : badfraction ] solve exercise  [ prob : mixture ] but now plot the fully marginalized ( over @xmath80 ) posterior distribution function for parameter @xmath54 . is this distribution peaked about where you would expect , given the data ?",
    "now repeat the problem , but dividing all the data uncertainty variances @xmath81 by 4 ( or dividing the uncertainties @xmath7 by 2 ) . again plot the fully marginalized posterior distribution function for parameter @xmath54 .",
    "your plots should look something like those in  [ fig : badfraction ] . discuss .    :",
    "the marginalized posterior probability distribution function for the @xmath54 parameter ( prior probability that a point is an outlier ) ; this distribution gets much worse when the data uncertainties are underestimated.,title=\"fig:\",scaledwidth=50.0% ] : the marginalized posterior probability distribution function for the @xmath54 parameter ( prior probability that a point is an outlier ) ; this distribution gets much worse when the data uncertainties are underestimated.,title=\"fig:\",scaledwidth=50.0% ]",
    "in the standard linear - algebra method of @xmath18 minimization given in section  [ sec : standard ] , the uncertainties in the best - fit parameters @xmath58 are given by the two - dimensional output covariance matrix @xmath82 = { { \\left[{{{{{\\boldsymbol{a}}}}^{\\scriptscriptstyle \\top}}}\\,{{{{{\\boldsymbol{c}}}}^{-1}}}\\,{{\\boldsymbol{a}}}\\right]}^{-1 } } \\quad , \\ ] ] where the ordering is defined by the ordering in matrix @xmath25 .",
    "these uncertainties for the model parameters _ only _ strictly hold under three extremely strict conditions , none of which is met in most real situations : _ ( a )",
    "_  the uncertainties in the data points must have variances correctly described by the @xmath81 ; _ ( b ) _",
    "there must be no rejection of any data or any departure from the exact , standard definition of @xmath18 given in equation  ( [ eq : chisquared ] ) ; and _ ( c ) _  the generative model of the data implied by the method  that is , that the data are truly drawn from a negligible - scatter linear relationship and subsequently had noise added , where the noise offsets were generated by a gaussian process  must be an accurate description of the data .",
    "these conditions are rarely met in practice .",
    "often the noise estimates are rough ( or missing entirely ! )",
    ", the investigator has applied data rejection or equivalent conditioning , and the relationship has intrinsic scatter and curvature . for these generic reasons ,",
    "we much prefer empirical estimates of the uncertainty in the best - fit parameters .    in the bayesian outlier - modeling schemes of section  [ sec :",
    "outliers ] , the output is a posterior distribution for the parameters @xmath58 .",
    "this distribution function is closer than the standard estimate @xmath20}^{-1}}$ ] to being an empirical measurement of the uncertainties of the parameters .",
    "the uncertainty variances @xmath83 and the covariance @xmath84 can be computed as second moments of this posterior distribution function .",
    "computing the variances this way does involve assumptions , but it is not extremely sensitive to the assumption that the model is a good fit to the data ; that is , as the model becomes a bad fit to the data ( for example when the data points are not consistent with being drawn from a narrow , straight line ) , these uncertainties change in accordance .",
    "that is in strong contrast to the elements of the matrix @xmath20}^{-1}}$ ] , which do nt depend in any way on the quality of fit .    either way ,",
    "unless the output of the fit is being used _ only _ to estimate the slope , and nothing else , it is a mistake to ignore the off - diagonal terms of the uncertainty variance tensor .",
    "that is , you generally know some linear combinations of @xmath3 and @xmath4 much , much better than you know either individually ( see , for example , s  [ fig : mixture ] and [ fig : reduceerror ] ) . when the data are good , this covariance is related to the location of the data relative to the origin , but any non - trivial propagation of the best - fit results requires use of either the full @xmath85 uncertainty tensor or else a two - dimensional sampling of the two - dimensional ( marginalized , perhaps ) posterior distribution .    in any non - bayesian scheme , or when",
    "the full posterior has been discarded in favor of only the map value , or when data - point uncertainties are not trusted , there are still empirical methods for determining the uncertainties in the best - fit parameters .",
    "the two most common are _ bootstrap _ and _ jackknife_. the first attempts to empirically create new data sets that are similar to your actual data set .",
    "the second measures your differential sensitivity to each individual data point .    in bootstrap , you do the unlikely - sounding thing of drawing @xmath47 data points randomly from the @xmath47 data points you have _ with replacement_. that is , some data points get dropped , and some get doubled ( or even tripled ) , but the important thing is that you select each of the @xmath47 that you are going to use in each trial independently from the whole set of @xmath47 you have .",
    "you do this selection of @xmath47 points once for each of @xmath77 bootstrap trials @xmath86 . for each of the @xmath77 trials @xmath86",
    ", you get an estimate  by whatever method you are using ( linear fitting , fitting with rejection , optimizing some custom objective function)of the parameters @xmath87 , where @xmath86 goes from @xmath88 to @xmath77 .",
    "an estimate of your uncertainty variance on @xmath3 is @xmath89 ^ 2 \\quad , \\ ] ] where @xmath3 stands for the best - fit @xmath3 using all the data .",
    "the uncertainty variance on @xmath4 is the same but with @xmath90 ^ 2 $ ] in the sum , and the covariance @xmath84 is the same but with @xmath91\\,[b_j - b]$ ] .",
    "bootstrap creates a new parameter , the number @xmath77 of trials .",
    "there is a huge literature on this , so we wo nt say anything too specific , but one intuition is that once you have @xmath77 comparable to @xmath47 , there probably is nt much else you can learn , unless you got terribly unlucky with your random number generator .    in jackknife , you make your measurement @xmath47 times , each time _ leaving out _ data point @xmath53 .",
    "again , it does nt matter what method you are using , for each leave - one - out trial @xmath53 you get an estimate @xmath92 found by fitting with all the data _ except _ point @xmath53 .",
    "then you calculate @xmath93 and the uncertainty variance becomes @xmath94 ^ 2 \\quad , \\ ] ] with the obvious modifications to make @xmath95 and @xmath84 .",
    "the factor @xmath96/n$ ] accounts , magically , for the fact that the samples are not independent in any sense ; this factor can only be justified in the limit that everything is gaussian and all the points are identical in their noise properties .",
    "jackknife and bootstrap are both extremely useful when you do nt know or do nt trust your uncertainties @xmath97 .",
    "they _ do _ make assumptions about the problem .",
    "for example , they can only be justified when data represent a reasonable sampling from some stationary frequency distribution of _ possible data _ from a set of hypothetical , similar experiments .",
    "this assumption is , in some ways , very strong , and often violated .",
    "for another example , these methods will not give correct answers if the model does not fit the data reasonably well .",
    "that said , if you _ do _",
    "believe your uncertainty estimates , any differences between the jackknife , bootstrap , and traditional uncertainty estimates for the best - fit line parameters undermine confidence in the validity of the model .",
    "if you believe the model , then any differences between the jackknife , bootstrap , and traditional uncertainty estimates undermine confidence in the data - point uncertainty variances @xmath97 .",
    "compute the standard uncertainty @xmath24 obtained for the slope of the line found by the standard fit you did in exercise  [ prob : standard ] .",
    "now make jackknife ( 20 trials ) and bootstrap estimates for the uncertainty @xmath24 . how do the uncertainties compare and which seems most reasonable ,",
    "given the data and uncertainties on the data ?",
    "[ prob : reduceerror ] re - do exercise  [ prob : mixture]the mixture - based outlier model  but just with the `` inlier '' points 5 through 20 from  [ table : data_allerr ] on page  .",
    "then do the same again , but with all measurement uncertainties reduced by a factor of 2 ( uncertainty variances reduced by a factor of 4 ) .",
    "plot the marginalized posterior probability distributions for line parameters @xmath58 in both cases .",
    "your plots should look like those in  [ fig : reduceerror ] .",
    "did these posterior distributions get smaller or larger with the reduction in the data - point uncertainties ?",
    "compare this with the dependence of the standard uncertainty estimate @xmath20}^{-1}}$ ] .    :",
    "the posterior probability distribution can become multi - modal ( right panel ) when you underestimate your observational uncertainties.,title=\"fig:\",scaledwidth=50.0% ] : the posterior probability distribution can become multi - modal ( right panel ) when you underestimate your observational uncertainties.,title=\"fig:\",scaledwidth=50.0% ]",
    "the standard method of section  [ sec : standard ] is only justified when the measurement uncertainties are gaussian with known variances .",
    "many noise processes in the real world are _ not _ gaussian ; what to do ?",
    "there are three general kinds of methodologies for dealing with non - gaussian uncertainties . before we describe them ,",
    "permit an aside on uncertainty and error .    in note",
    "[ note : error ] , we point out that an _ uncertainty _ is a measure of what you do nt know , while an _ error _ is a mistake or incorrect value .",
    "the fact that a measurement @xmath33 differs from some kind of `` true '' value @xmath98 that it would have in some kind of `` perfect experiment '' can be for two reasons ( which are not unrelated ) : it could be that there is some source of _ noise _ in the experiment that generates offsets for each data point @xmath53 away from its true value . or it could be that the `` measurement '' @xmath33 is actually the outcome of some probabilistic inference _ itself _ and therefore comes with a posterior probability distribution . in either case , there is a finite uncertainty , and in both cases it comes somehow from imperfection in the experiment , but the way you understand the distribution of the uncertainty is different . in the first case , you have to make a model for the noise in the experiment . in the second",
    ", you have to analyze some posteriors . in either case , clearly , the uncertainty can be non - gaussian .    all that said , and even when you _",
    "suspect _ that the sources of noise are not gaussian , it is still sometimes okay to treat the",
    "_ uncertainties _ as gaussian : the gaussian distribution is the maximum - entropy distribution for a fixed _ variance_.",
    "that means that if you have some ( perhaps magical ) way to estimate the variance of your uncertainty , and you are nt sure of anything else but the variance , then the gaussian is  contrary to popular opinion  the _ most conservative _ thing you can assume .",
    "of course it is very rare that you do know the variance of your uncertainty or noise ; most simple uncertainty estimators are measurements of the central part of the distribution function ( not the tails ) and are therefore _ not _ good estimators of the total variance .    back to business .",
    "the first approach to non - gaussian noise or uncertainty is ( somehow ) to estimate the true or total variance of your uncertainties and then do the most conservative thing , which is to once again assume that the noise is gaussian !",
    "this is the simplest approach , but rarely possible , because when there are non - gaussian uncertainties , the observed variance of a finite sample of points can be very much smaller than the true variance of the generating distribution function . the second approach is to make no attempt to understand the non - gaussianity , but to use the data - rejection methods of section  [ sec : outliers ] .",
    "we generally recommend data - rejection methods , but there is a third approach in which the likelihood is artificially or heuristically softened to reduce the influence of the outliers .",
    "this approach is not usually a good idea when it makes the objective function far from anything justifiable .",
    "the fourth approach  the only approach that is really justified  is to fully understand and model the non - gaussianity .",
    "for example , if data points are generated by a process the noise from which looks like a sum or mixture of @xmath99 gaussians with different variances , and the distribution can be understood well enough to be modeled , the investigator can replace the gaussian objective function with something more realistic .",
    "indeed , _ any _ ( reasonable ) frequency distribution can be described as a mixture of gaussians , so this approach is extremely general ( in principle ; it may not be easy ) .",
    "for example , the generative model in section  [ sec : objective ] for the standard case was built out of individual data - point likelihoods given in equation  ( [ eq : objectivei ] ) .",
    "if the frequency distribution for the noise contribution to data point @xmath33 has been expressed as the sum of @xmath99 gaussians , these likelihoods become @xmath100 ^ 2}{2\\,\\sigma_{yij}^2}\\right )   \\quad , \\ ] ] where the @xmath99 gaussians for measurement @xmath53 have variances @xmath101 , offsets ( means  there is no need for the gaussians to be concentric ) @xmath102 , and amplitudes @xmath103 that sum to unity @xmath104 in this case , fitting becomes much more challenging ( from an optimization standpoint ) , but this can become a _ completely general _ objective function for arbitrarily complicated non - gaussian uncertainties",
    ". it can even be generalized to situations of arbitrarily complicated joint distributions for _ all _ the measurement uncertainties .",
    "one very common non - gaussian situation is one in which the data points include upper or lower limits , or the investigator has a value with an uncertainty estimate but knows that the true value ca nt enter into some region ( for example , there are many situations in which one knows that all the true @xmath98 must be greater than zero ) . in all these situations  of upper limits or lower limits or otherwise limited uncertainties ",
    "the best thing to do is to model the uncertainty distribution as well as possible , construct the proper justified scalar objective function , and carry on .",
    "optimization might become challenging , but that is an engineering problem that must be tackled for scientific correctness .",
    "how can you decide if your fit is a good one , or that your assumptions are justified ? and how can you infer or assess the individual data - point uncertainties if you do nt know them , or do nt trust them ?",
    "these seem like different questions , but they are coupled : in the standard ( frequentist ) paradigm , you ca nt test your assumptions unless you are very confident about your uncertainty estimates , and you ca nt test your uncertainty estimates unless you are very confident about your assumptions .",
    "imagine that the generative model in section  [ sec : objective ] is a valid description of the data .",
    "in this case , the noise contribution for each data point @xmath53 has been drawn from a gaussian with variance @xmath81 .",
    "the expectation is that data point @xmath53 will provide a mean squared error comparable to @xmath81 , and a contribution of order unity to @xmath18 when the parameters @xmath58 are set close to their true values . because in detail there are two fit parameters @xmath58 which you have been permitted to optimize , the expectation for @xmath18 is smaller than @xmath47 .",
    "the model is linear , so the distribution of @xmath18 is known analytically and is given ( unsurprisingly ) by a _",
    "chi - square _ distribution : in the limit of large @xmath47 the rule of thumb is that  when the model is a good fit , the uncertainties are gaussian with known variances , and there are two linear parameters ( @xmath3 and @xmath4 in this case ) , @xmath105 \\pm \\sqrt{2\\,[n-2 ] } \\quad , \\ ] ] where the @xmath106 symbol is used loosely to indicate something close to a standard uncertainty ( something close to a 68-percent confidence interval ) . if you find @xmath18 in this ballpark , it is conceivable that the model is good .",
    "model rejection on the basis of too - large @xmath18 is a frequentist s option .",
    "a bayesian ca nt interpret the data without a model , so there is no meaning to the question `` is this model good ? '' .",
    "bayesians only answer questions of the form `` is this model better than that one ? '' .",
    "because we are only considering the straight - line model in this document , further discussion of this ( controversial , it turns out ) point goes beyond our scope .",
    "it is easy to get a @xmath18 much _ higher _ than @xmath107 $ ] ; getting a _ lower _ value seems impossible ; yet it happens very frequently .",
    "this is one of the many reasons that rejection or acceptance of a model on the @xmath18 basis is dangerous ; exactly the same kinds of problems that can make @xmath18 unreasonably low can make it unreasonably high ; worse yet , it can make @xmath18 reasonable when the model is bad .",
    "reasons the @xmath18 value can be lower include that uncertainty estimates can easily be overestimated .",
    "the opposite of this problem can make @xmath18 high when the model is good .",
    "even worse is the fact that uncertainty estimates are often correlated , which can result in a @xmath18 value significantly different from the expected value .",
    "correlated measurements are not uncommon ; the process by which the observations were taken often involve shared information or assumptions . if the individual data - points @xmath33 have been estimated by some means that effectively relies on that shared information , then there will be large covariances among the data points .",
    "these covariances bring off - diagonal elements into the covariance matrix @xmath13 , which was trivially constructed in equation  ( [ eq : covar ] ) under the assumption that all covariances ( off - diagonal elements ) are precisely zero . once the off - diagonal elements are non - zero , @xmath18 must be computed by the matrix expression in equation  ( [ eq : chisquared ] ) ; this is equivalent to replacing the sum over @xmath53 to two sums over @xmath53 and @xmath86 and considering all cross terms @xmath108}^{\\scriptscriptstyle \\top}}\\,{{{{{\\boldsymbol{c}}}}^{-1}}}\\,\\left[{{\\boldsymbol{y}}}-{{\\boldsymbol{a}}}\\,{{\\boldsymbol{x}}}\\right ]   = \\sum_{i=1}^n \\sum_{j=1}^n   w_{ij}\\,\\left[y_i - f(x_i)\\right]\\,\\left[y_j - f(x_j)\\right ]   \\quad,\\ ] ] where the @xmath109 are the elements of the inverse covariance matrix @xmath110 .    in principle , data - point uncertainty variance underestimation or mean point - to - point covariance can be estimated by adjusting them until @xmath18 is reasonable , in a model you know ( for independent reasons ) to be good .",
    "this is rarely a good idea , both because you rarely know that the model is a good one , and because you are much better served understanding your variances and covariances directly .",
    "if you do nt have or trust your uncertainty estimates and do nt care about them at all , your best bet is to go bayesian , infer them , and marginalize them out .",
    "imagine that you do nt know anything about your individual data - point uncertainty variances @xmath97 at all .",
    "imagine , further , that you do nt even know anything about their _ relative _ magnitudes ; that is , you ca nt even assume that they have similar magnitudes . in this case",
    "a procedure is the following : move the uncertainties into the model parameters to get the large parameter list @xmath111 .",
    "pick a prior on the uncertainties ( on which you must have _ some _ prior information , given , for example , the range of your data and the like ) .",
    "apply bayes s rule to obtain a posterior distribution function for all the parameters @xmath111 .",
    "marginalize over the uncertainties to obtain the properly marginalized posterior distribution function for @xmath58 .",
    "no sane person would imagine that the procedure described here can lead to any informative inference .",
    "however , if the prior on the @xmath97 is relatively flat in the relevant region , the _ lack of specificity _ of the model when the uncertainties are large pushes the system to smaller uncertainties , and the _ inaccuracy _ of the model when the uncertainties are small pushes the system to larger uncertainties .",
    "if the model is reasonable , the inferred uncertainties will be reasonable too .",
    "assess the @xmath18 value for the fit performed in exercise  [ prob : easy ] ( do that problem first if you havent already ) .",
    "is the fit good ?",
    "what about for the fit performed in exercise  [ prob : standard ] ?",
    "[ prob : chi2 ] re - do the fit of exercise  [ prob : easy ] but setting all @xmath112 , that is , ignoring the uncertainties and replacing them all with the same value @xmath113 .",
    "what uncertainty variance @xmath113 would make @xmath114 ?",
    "relevant plots are shown in  [ fig : chi2 ] .",
    "how does it compare to the mean and median of the uncertainty variances @xmath97 ?    : the dependence of the fitting scalar @xmath18 on what is assumed about the data uncertainties.,title=\"fig:\",scaledwidth=50.0% ] : the dependence of the fitting scalar @xmath18 on what is assumed about the data uncertainties.,title=\"fig:\",scaledwidth=50.0% ]    [ prob : bayeschi2 ] flesh out and write all equations for the bayesian uncertainty estimation and marginalization procedure described in this section .",
    "note that the inference and marginalization would be very expensive without excellent sampling tools ! make the additional ( unjustified ) assumption that all the uncertainties have the same variance @xmath112 to make the problem tractable .",
    "apply the method to the @xmath8 and @xmath2 values for points 5 through 20 in  [ table : data_allerr ] on page  .",
    "make a plot showing the points , the maximum _ a  posteriori _",
    "value of the uncertainty variance as error bars , and the maximum _ a  posteriori",
    "_  straight line .",
    "for extra credit , plot two straight lines , one that is maximum _ a  posteriori _  for the full posterior and one that is the same but for the posterior after the uncertainty variance @xmath113 has been marginalized out .",
    "your result should look like  [ fig : bayeschi2 ] .",
    "also plot two sets of error bars , one that shows the maximum for the full posterior and one for the posterior after the line parameters @xmath58 have been marginalized out .    : the results of inferring the observational uncertainties simultaneously with the fit parameters.,title=\"fig:\",scaledwidth=50.0% ] : the results of inferring the observational uncertainties simultaneously with the fit parameters.,title=\"fig:\",scaledwidth=50.0% ]",
    "of course most real two - dimensional data @xmath115 come with uncertainties in _ both _ directions ( in both @xmath8 and @xmath2 ) .",
    "you might not know the amplitudes of these uncertainties , but it is unlikely that the @xmath8 values are known to sufficiently high accuracy that any of the straight - line fitting methods given so far is valid . recall that everything so far has assumed that the @xmath8-direction uncertainties were negligible .",
    "this might be true , for example , when the @xmath31 are the times at which a set of stellar measurements are made , and the @xmath33 are the declinations of the star at those times .",
    "it is _ not _ going to be true when the @xmath31 are the right ascensions of the star .",
    "in general , when one makes a two - dimensional measurement @xmath6 , that measurement comes with uncertainties @xmath116 in both directions , and some covariance @xmath117 between them .",
    "these can be put together into a covariance tensor @xmath118 @xmath119 \\quad .\\ ] ] if the uncertainties are gaussian , or if all that is known about the uncertainties is their variances , then the covariance tensor can be used in a two - dimensional gaussian representation of the probability of getting measurement @xmath6 when the `` true value '' ( the value you would have for this data point if it had been observed with negligible noise ) is @xmath0 : @xmath120}^{\\scriptscriptstyle \\top } }    \\,{{{{\\boldsymbol{s}}}_i}^{-1}}\\,\\left[{{\\boldsymbol{z}}}_i - { { \\boldsymbol{z}}}\\right]\\right ) \\quad , \\ ] ] where we have implicitly made column vectors @xmath121 \\quad ; \\quad { { \\boldsymbol{z}}}_i = \\left[\\begin{array}{c } x_i \\\\ y_i \\end{array}\\right ] \\quad .\\ ] ]    now in the face of these general ( though gaussian ) two - dimensional uncertainties , how do we fit a line ? justified objective functions will have something to do with the probability of the observations @xmath115 given the uncertainties @xmath122 , as a function of properties @xmath58 of the line . as in section",
    "[ sec : objective ] , the probability of the observations given model parameters @xmath58 is proportional to the _ likelihood _ of the parameters given the observations . we will now construct this likelihood and maximize it , or else multiply it by a prior on the parameters and report the posterior on the parameters .",
    "schematically , the construction of the likelihood involves specifying the line ( parameterized by @xmath123 ) , finding the probability of each observed data point given any true point on that line , and marginalizing over all possible true points on the line .",
    "this is a model in which each point really does have a true location on the line , but that location is not directly measured ; it is , in some sense , a missing datum for each point .",
    "one approach is to think of the straight line as a two - dimensional gaussian with an infinite eigenvalue corresponding to the direction of the line and a zero eigenvalue for the direction orthogonal to this . in the end",
    ", this line of argument leads to _ projections _ of the two - dimensional uncertainty gaussians along the line ( or onto the subspace that is orthogonal to the line ) , and evaluation of those at the _",
    "projected displacements_. projection is a standard linear algebra technique , so we will use linear algebra ( matrix ) notation .",
    "a slope @xmath3 can be described by a unit vector @xmath124 _ orthogonal _ to the line or linear relation : @xmath125   = \\left[\\begin{array}{c}-\\sin\\theta\\\\\\cos\\theta\\end{array}\\right ] \\quad , \\ ] ] where we have defined the angle @xmath126 made between the line and the @xmath8 axis .",
    "the orthogonal displacement @xmath127 of each data point @xmath6 from the line is given by @xmath128 where @xmath129 is the column vector made from @xmath6 in equation  ( [ eq : mz ] ) .",
    "similarly , each data point s covariance matrix @xmath118 projects down to an orthogonal variance @xmath130 given by @xmath131 and then the log likelihood for @xmath58 or @xmath132 can be written as @xmath133 where @xmath40 is some constant .",
    "_ this _ likelihood can be maximized , and the resulting values of @xmath58 are justifiably the best - fit values .",
    "the only modification we would suggest is performing the fit or likelihood maximization not in terms of @xmath58 but rather @xmath134 , where @xmath135 $ ] is the perpendicular distance of the line from the origin . this removes the paradox that the standard `` prior '' assumption of standard straight - line fitting treats all _ slopes _",
    "@xmath3 equally , putting way too much attention on angles near @xmath136 .",
    "the bayesian must set a prior .",
    "again , there are many choices , but the most natural is _ something _ like flat in @xmath137 and flat in @xmath138 ( the latter not proper ) .",
    "the implicit generative model here is that there are @xmath47 points with true values that lie precisely on a narrow , linear relation in the @xmath8@xmath2 plane . to each of these true points a gaussian offset",
    "has been added to make each observed point @xmath6 , where the offset was drawn from the two - dimensional gaussian with covariance tensor @xmath118 .",
    "as usual , if this generative model is a good approximation to the properties of the data set , the method works very well .",
    "of course there are many situations in which this is _ not _ a good approximation . in section",
    "[ sec : scatter ] , we consider the ( very common ) case that the relationship is near - linear but not _ narrow _ , so there is an intrinsic width or scatter in the true relationship .",
    "another case is that there are outliers ; this can be taken care of by methods very precisely analogous to the methods in section  [ sec : outliers ] . we ask for this in the exercises  below .",
    "it is true that standard least - squares fitting is easy and simple ; presumably this explains why it is used so often when it is inappropriate .",
    "we hope to have convinced some readers that doing something justifiable and sensible when there are uncertainties in both dimensions  when standard linear fitting is inappropriate  is neither difficult nor complex .",
    "that said , there is something fundamental wrong with the generative model of this section , and it is that the model generates _ only _ the displacements of the points _ orthogonal _ to the linear relationship .",
    "the model is completely unspecified for the distribution _ along _ the relationship .    in the astrophysics literature ( see , for example , the tully  fisher literature ) ,",
    "there is a tradition , when there are uncertainties in both directions , of fitting the `` forward '' and `` reverse '' relations  that is , fitting @xmath2 as a function of @xmath8 and then @xmath8 as a function of @xmath2and then splitting the difference between the two slopes so obtained , or treating the difference between the slopes as a systematic uncertainty .",
    "this is unjustified .",
    "another common method for finding the linear relationship in data when there are uncertainties in both directions is _ principal components analysis_. the manifold reasons _ not _ to use pca are beyond the scope of this document .",
    "[ prob : twod ] using the method of this section , fit the straight line @xmath1 to the @xmath8 , @xmath2 , @xmath139 , @xmath140 , and @xmath30 values of points 5 through 20 taken from  [ table : data_allerr ] on page  .",
    "make a plot showing the points , their two - dimensional uncertainties ( show them as one - sigma ellipses ) , and the best - fit line .",
    "your plot should look like  [ fig : twod ] .    :",
    "the best fit after properly accounting for covariant noise in both dimensions.,scaledwidth=50.0% ]    [ prob : twodoutlier ] repeat exercise  [ prob : twod ] , but using all of the data in  [ table : data_allerr ] on page  .",
    "some of the points are now outliers , so your fit may look worse .",
    "follow the fit by a robust procedure analogous to the bayesian mixture model with bad - data probability @xmath54 described in section  [ sec : outliers ] . use something sensible for the prior probability distribution for @xmath58 .",
    "plot the two results with the data and uncertainties .",
    "for extra credit , plot a sampling of 10 lines drawn from the marginalized posterior distribution for @xmath58 and plot the samples as a set of light grey or transparent lines . for extra extra credit , mark each data point on your plot with the fully marginalized probability that the point is bad ( that is , rejected , or has @xmath141 ) .",
    "your result should look like  [ fig : twodoutlier ] .",
    ": on the left , the same as  [ fig : twod ] but including the outlier points . on the right , the same as in  [ fig : mixture ] but applying the outlier ( mixture ) model to the case of two - dimensional uncertainties.,title=\"fig:\",scaledwidth=50.0% ] : on the left , the same as  [ fig : twod ] but including the outlier points . on the right , the same as in  [ fig : mixture ] but applying the outlier ( mixture ) model to the case of two - dimensional uncertainties.,title=\"fig:\",scaledwidth=50.0% ]    [ prob : forwardreverse ] perform the abominable forward  reverse fitting procedure on points 5 through 20 from  [ table : data_allerr ] on page  .",
    "that is , fit a straight line to the @xmath2 values of the points , using the @xmath2-direction uncertainties @xmath30 only , by the standard method described in section  [ sec : standard ] . now transpose the problem and fit the same data but fitting the @xmath8 values using the @xmath8-direction uncertainties @xmath139 only .",
    "make a plot showing the data points , the @xmath8-direction and @xmath2-direction uncertainties , and the two best - fit lines .",
    "your plot should look like  [ fig : forwardreverse ] . comment .    : results of `` forward and reverse '' fitting .",
    "do nt ever do this.,scaledwidth=50.0% ]    [ prob : pca ] perform principal components analysis on points 5 through 20 from  [ table : data_allerr ] on page  .",
    "that is , diagonalize the @xmath142 matrix @xmath143 given by @xmath144    \\,{{\\left[{{\\boldsymbol{z}}}_i-{{\\left<{{{\\boldsymbol{z}}}}\\right>}}\\right]}^{\\scriptscriptstyle \\top } } \\quad , \\ ] ] @xmath145 find the eigenvector of @xmath143 with the largest eigenvalue .",
    "now make a plot showing the data points , and the line that goes through the mean @xmath146 of the data with the slope corresponding to the direction of the principal eigenvector .",
    "your plot should look like  [ fig : pca ] .",
    "comment .    : the dominant component from a principal components analysis.,scaledwidth=50.0% ]",
    "so far , everything we have done has implicitly assumed that there truly is a narrow linear relationship between @xmath8 and @xmath2 ( or there would be if they were both measured with negligible uncertainties ) .",
    "the words `` narrow '' and `` linear '' are both problematic , since there are very few problems in science , especially in astrophysics , in which a relationship between two observables is expected to be either . the reasons for intrinsic scatter abound ; but generically the quantities being measured are produced or affected by a large number of additional , unmeasured or unmeasurable quantities ; the relation between @xmath8 and @xmath2 is rarely exact , even in when observed by theoretically perfect , noise - free observations .",
    "proper treatment of these problems gets into the complicated area of estimating density given finite and noisy samples ; again this is a huge subject so we will only consider one simple solution to the one simple problem of a relationship that is linear but not narrow .",
    "we will not consider anything that looks like subtraction of variances in quadrature ; that is pure procedure and almost never justifiable . instead  as usual  we introduce a model that generates the data and infer the parameters of that model .",
    "introduce an intrinsic gaussian variance @xmath147 , orthogonal to the line . in this case , the parameters of the relationship become @xmath148 . in this case , each data point can be treated as being drawn from a projected distribution function that is a _ convolution _ of the projected uncertainty gaussian , of variance @xmath130 defined in equation  ( [ eq : sigma ] ) , with the intrinsic scatter gaussian of variance @xmath147 .",
    "convolution of gaussians is trivial and the likelihood in this case becomes @xmath149 } \\quad , \\ ] ] where again @xmath40 is a constant , everything else is defined as it is in equation  ( [ eq : twodlike ] ) , and an additional term has appeared to penalize very broad variances ( which , because they are less specific than small variances , have lower likelihoods ) . actually , that term existed in equation  ( [ eq : twodlike ] ) as well , but because there was no @xmath147 parameter , it did not enter into any optimization so we absorbed it into @xmath40 .    as we mentioned in note  [ note : orthogonal ] , there are limitations to this procedure because it models only the distribution _",
    "orthogonal _ to the relationship .",
    "probably all good methods fully model the intrinsic two - dimensional density function , the function that , when convolved with each data point s intrinsic uncertainty gaussian , creates a distribution function from which that data point is a single sample .",
    "such methods fall into the intersection of density estimation and deconvolution , and completely general methods exist .",
    "[ prob : intrinsic ] re - do exercise  [ prob : twod ] , but now allowing for an orthogonal intrinsic gaussian variance @xmath147 and only excluding data point 3 .",
    "re - make the plot , showing not the best - fit line but rather the @xmath150 lines for the maximum - likelihood _ intrinsic _ relation .",
    "your plot should look like  [ fig : intrinsic ] .",
    "[ prob : bayesintrinsic ] re - do exercise  [ prob : intrinsic ] but as a bayesian , with sensible bayesian priors on @xmath148 . find and marginalize the posterior distribution over @xmath134 to generate a marginalized posterior probability for the intrinsic variance parameter @xmath147 .",
    "plot this posterior with the 95 and 99  percent _ upper limits _ on @xmath147 marked .",
    "your plot should look like  [ fig : bayesintrinsic ] .",
    "why did we ask only for upper limits ?",
    "bovy ,  j. , hogg ,  d.  w. , & roweis , s.  t. , 2009 , extreme deconvolution : inferring complete distribution functions from noisy , heterogeneous , and incomplete observations , arxiv:0905.2979 [ stat.me ] jaynes ,  e.  t. , 2003 , _ probability theory : the logic of science _ ( cambridge university press ) gilks ,  w.  r. , richardson ,  s. , & spiegelhalter ,  d. , 1995 , _ markov chain monte carlo in practice : interdisciplinary statistics _",
    "( chapman & hall / crc ) hampel ,  f.  r. , ronchetti ,  e.  m. , rousseeuw ,  p.  j. , & stahel ,  w.  a. , 1986 , _ robust statistics : the approach based on influence functions _",
    "( new york : wiley ) isobe ,  t. , feigelson , e.  d. , akritas ,  m.  g. , & babu ,  g.  j. , 1990 , linear regression in astronomy , _ astrophysical journal _ * 364 * 104 kelly ,  b.  c. , 2007 , some aspects of measurement error in linear regression of astronomical data , _ astrophysical journal _ * 665 * 1489 mackay ,  d.  j.  c. , 2003 , _ information theory , inference , and learning algorithms _ ( cambridge university press ) neal . ,  r.  m. , 2003 , slice sampling , _ annals of statistics _ , * 31*(3 ) , 705 press ,  w.  h. , 1997 , understanding data better with bayesian and global statistical methods , in _ unsolved problems in astrophysics , _ eds .",
    "bahcall ,  j.  n. & ostriker ,  j.  p. ( princeton university press ) 4960 press ,  w.  h. , teukolsky ,  s.  a. , vetterling ,  w.  t. , & flannery ,  b.  p. , 2007 , _ numerical recipes : the art of scientific computing _",
    "( cambridge university press ) sivia ,  d.  s. & skilling ,  j. , 2006 , _ data analysis : a bayesian tutorial _ ( oxford university press )"
  ],
  "abstract_text": [
    "<S> we go through the many considerations involved in fitting a model to data , using as an example the fit of a straight line to a set of points in a two - dimensional plane . </S>",
    "<S> standard weighted least - squares fitting is only appropriate when there is a dimension along which the data points have negligible uncertainties , and another along which all the uncertainties can be described by gaussians of known variance ; these conditions are rarely met in practice . </S>",
    "<S> we consider cases of general , heterogeneous , and arbitrarily covariant two - dimensional uncertainties , and situations in which there are bad data ( large outliers ) , unknown uncertainties , and unknown but expected intrinsic scatter in the linear relationship being fit . </S>",
    "<S> above all we emphasize the importance of having a `` generative model '' for the data , even an approximate one . once there is a generative model , the subsequent fitting is non - arbitrary because the model permits direct computation of the likelihood of the parameters or the posterior probability distribution . </S>",
    "<S> construction of a posterior probability distribution is indispensible if there are `` nuisance parameters '' to marginalize away . </S>"
  ]
}