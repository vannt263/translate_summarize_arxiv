{
  "article_text": [
    "in high energy physics ( hep ) experiments , people usually need to select some events with specific interest , so called signal events , out of numerous background events for study . in order to increase the ratio of signal to background",
    ", one needs to suppress background events while keeping high signal efficiency . to this end",
    ", some advanced techniques , such as adaboost@xcite , @xmath0-boost@xcite , @xmath0-logitboost@xcite , @xmath0-hingeboost , random forests @xcite etc . , from statistics and computer sciences were introduced for signal and background event separation in the miniboone experiment@xcite at fermilab .",
    "the miniboone experiment is designed to confirm or refute the evidence for @xmath1 oscillations at @xmath2 found by the lsnd experiment@xcite .",
    "it is a crucial experiment which will imply new physics beyond the standard model if the lsnd signal is confirmed .",
    "these techniques are tuned with one sample of monte carlo ( mc ) events , the training sample , and then tested with an independent mc sample , the testing sample .",
    "initial comparisons of these techniques with artificial neural networks ( ann ) using the miniboone mc samples were described previously@xcite .",
    "this work indicated that the method of boosted decision trees is superior to the anns for particle identification ( pid ) using the miniboone mc samples .",
    "further studies show that the boosted decision tree method has not only better event separation , but is also more stable and robust than anns when using mc samples with varying input parameters .",
    "the boosting algorithm is one of the most powerful learning techniques introduced in the past decade@xcite .",
    "the motivation for the boosting algorithm is to design a procedure that combines many `` weak '' classifiers to achieve a final powerful classifier . in the present work numerous trials are made to tune",
    "the boosted decision trees , and comparisons are made for various algorithms . for a large number of discriminant variables ,",
    "several techniques are described to select a set of powerful input variables in order to obtain optimal event separation using boosted decision trees .",
    "furthermore , post - fitting of weights for the trained boosting trees is also investigated to attempt further possible improvement .",
    "this paper is focussed on the boosting tuning .",
    "all results appearing in this paper are relative numbers .",
    "they do not represent the miniboone pid performance ; that performance is continually improving with further algorithm and pid study .",
    "the description of the miniboone reconstruction packages@xcite , the reconstructed variables , the overall and absolute performance of the boosting pid@xcite , the validation of the input variables and the boosting pid variables by comparing various mc and real data samples@xcite will be described in future articles .",
    "boosting algorithms can be applied to any classifier , here they are applied to decision trees .",
    "a schematic of a simple decision tree is shown in figure 1 , s means signal , b means background , terminal nodes called leaves are shown in boxes .",
    "the key issue is to define a criterion that describes the goodness of separation between signal and background in the tree split .",
    "assume the events are weighted with each event having weight @xmath3 .",
    "define the purity of the sample in a node by @xmath4 where @xmath5 is the sum over signal events and @xmath6 is the sum over background events . note that @xmath7 is 0 if the sample is pure signal or pure background . for a given node let @xmath8 where @xmath9 is the number of events on that node .",
    "the criterion chosen is to minimize @xmath10    to determine the increase in quality when a node is split into two nodes , one maximizes @xmath11    at the end , if a leaf has purity greater than 1/2 ( or whatever is set ) , then it is called a signal leaf , otherwise , a background leaf .",
    "events are classified signal ( have score of 1 ) if they land on a signal leaf and background ( have score of -1 ) if they land on a background leaf . the resulting tree is a _",
    "decision tree_.    decision trees have been available for some time@xcite .",
    "they are known to be powerful but unstable , i.e. , a small change in the training sample can produce a large change in the tree and the results . combining many decision trees to make a `` majority vote '' , as in the random forests method , can improve the stability somewhat .",
    "however , as will be discussed in section 6 , the performance of the random forests method is significantly worse than the performance of the boosted decision tree method in which the weights of misclassified events are boosted for succeeding trees .",
    "if there are @xmath12 total events in the sample , the weight of each event is initially taken as @xmath13 .",
    "suppose that there are @xmath14 trees and @xmath15 is the index of an individual tree .",
    "let    * @xmath16 the set of pid variables for the @xmath17th event .",
    "* @xmath18 if the @xmath17th event is a signal event and @xmath19 if the event is a background event . * @xmath20 the weight of the @xmath17th event . *",
    "@xmath21 if the set of variables for the @xmath17th event lands that event on a signal leaf and @xmath22 if the set of variables for that event lands it on a background leaf .",
    "* @xmath23 if @xmath24 and 0 if @xmath25 .",
    "there are several commonly used algorithms for boosting the weights of the misclassified events in the training sample .",
    "the boosting performance is quite different using various ways to update the event weights .",
    "the first boosting method is called `` adaboost''@xcite or sometimes discrete adaboost .",
    "define for the @xmath15th tree : @xmath26 calculate : @xmath27 @xmath28 is the value used in the standard adaboost method . change the weight of each event @xmath17 , @xmath29 .",
    "@xmath30 renormalize the weights . @xmath31",
    "the score for a given event is @xmath32 which is just the weighted sum of the scores of the individual trees .",
    "a second boosting method is called `` @xmath0-boost '' @xcite , or sometimes `` shrinkage '' . after the @xmath15th tree ,",
    "change the weight of each event @xmath17 , @xmath29 .",
    "@xmath33 where @xmath0 is a constant of the order of 0.01 .",
    "renormalize the weights . @xmath31",
    "the score for a given event is @xmath34 which is the renormalized , but unweighted , sum of the scores over individual trees .",
    "a third boosting method is called `` @xmath0-logitboost '' .",
    "this method is quite similar to @xmath0-boost , but the weights are updated according to : @xmath35 where @xmath36 for the m@xmath37 tree iteration .",
    "a fourth boosting method is called `` @xmath0-hingeboost '' . again",
    "this method is quite similar to @xmath0-boost , but here the weights are updated according to : @xmath38 where @xmath36 for the m@xmath37 tree iterations .",
    "a fifth boosting method is called `` logitboost''@xcite .",
    "let @xmath39 for signal events and @xmath40 for background events .",
    "initial probability estimates are set to @xmath41 for event @xmath17 , where @xmath42 is the set of pid variables .",
    "let : @xmath43 @xmath44 where @xmath45 is the weight of event @xmath17 .",
    "let @xmath46 be the weighted average of @xmath47 over some set of events . instead of the gini criterion , the splitting variable and point to divide the events at a node into two nodes @xmath48 and @xmath49",
    "is determined to minimize @xmath50 the output for tree @xmath15 is @xmath51 for the node onto which event @xmath17 falls .",
    "the total output score is : @xmath52 the probability is updated according to : @xmath53      a sixth boosting method is called `` gentle adaboost''@xcite .",
    "it uses same criterion as described for the logitboost . here",
    "@xmath54 is same as @xmath55 .",
    "the weights are updated according to : @xmath56 for signal events and @xmath57 for background events , where @xmath58 is the weighted purity of the leaf on which event @xmath17 falls .",
    "a seventh boosting method is called `` real adaboost''@xcite .",
    "it is similar to the discrete version of adaboost described in section 3.1 , but the weights and event scores are calculated in different ways .",
    "the event score for event @xmath17 in tree @xmath15 is given by : @xmath59 where @xmath58 is the weighted purity of the leaf on which event @xmath17 falls .",
    "the event weights are updated according to : @xmath60 and then renormalized so that the total weight is one .",
    "the total output score including all of the trees is given by @xmath61",
    "miniboone mc samples from the february 2004 baseline mc were used to tune some parameters of the boosted decision trees .",
    "there are 88233 intrinsic @xmath62 signal events and 162657 @xmath63 background events .",
    "20000 signal events and 30000 background events were selected randomly for the training sample and the rest of the events were the test sample .",
    "the number of input variables for boosting training is 52 .",
    "the relative ratio is defined as the background efficiency divided by the corresponding signal efficiency and rescaled by a constant value .",
    "the left plot of figure 2 shows the relative ratio versus the signal efficiency for adaboost with 45 leaves per decision tree and various @xmath64 values for 1000 tree iterations .",
    "the boosting performances slightly depend on the @xmath64 values .",
    "adaboost with @xmath28 works slightly better in the high signal efficiency region ( eff @xmath65 65% ) but worse in the low signal efficiency region ( eff @xmath66 60% ) than adaboost with smaller @xmath64 values , 0.8 , 0.5 or 0.3 . to balance the overall performance , @xmath67 is selected to replace the standard value 1 for the adaboost training .",
    "the right plot of figure 2 shows the relative ratio versus the signal efficiency for adaboost with @xmath67 and 1000 tree iterations for various decision tree sizes ranging from 8 leaves to 100 leaves .",
    "adaboost with a large tree size worked significantly better than adaboost with a small tree size , 8 leaves ; the latter number has been recommended in some statistics literature@xcite . typically , it takes more tree iterations for the smaller tree size to reach optimal performance . for this application ,",
    "even with more tree iterations ( 10000 trees ) , results from boosting with small tree size ( 8 leaves ) are still significantly worse ( @xmath6810%-20% ) than results obtained with large tree size ( 45 leaves ) . here , 45 leaves per decision tree is selected ( this number is quite close to the number of input variables , 52 , for the boosting training . )    how many decision trees are sufficient ?",
    "it depends on the mc samples for boosting training and testing . for the given set of boosting parameters selected above",
    ", we ran boosting with 1000 tree iterations .",
    "the left plot of figure 3 shows the relative ratio versus the signal efficiency for adaboost with tree iterations of 100 , 200 , 500 , 800 and 1000 , respectively . the boosting performance becomes better with more tree iterations",
    ". the right plot of figure 3 shows the relative ratio versus the number of decision trees for signal efficiencies of 50% , 60% and 70% which cover the regions of greatest interest for the miniboone experiment .",
    "typically , the boosting performance for low signal efficiencies converges after few hundred tree iterations and is then stable . for high signal efficiency , boosting performance continues to improve as the number of decision trees is increased .",
    "for these particular mc samples , the boosting performance is close to optimal after 1000 tree iterations . for the sake of comparison , the adaboost performance of the boosting training mc samples is also shown in the right plot of figure 3 .",
    "the relative ratios drop quickly down to zero ( zero means no background events left after selection for a given signal efficiency ) within 100 tree iterations for 50%-70% signal efficiencies .",
    "the adaboost outputs for the training mc sample and for the testing mc sample for @xmath69 1 , 100 , 500 and 1000 are shown in figure 4 . the signal and background separation for the training sample becomes better as the number of tree iterations increases .",
    "the signal and background events are completely distinguished after about 500 tree iterations . for the testing samples , however , the signal and background separations are quite stable after a few hundred tree iterations .",
    "the corresponding relative ratios are stable for given signal efficiencies as shown in right plot of figure 3 .",
    "the tuning parameter for @xmath0-boost is @xmath0 .",
    "the left plot of figure 5 shows the relative ratio versus the signal efficiency for @xmath0-boost with @xmath0 values of 0.005 , 0.01 , 0.02 , 0.04 , respectively .",
    "@xmath0-boost with fairly large @xmath0 values for 45 leaves per decision tree and 1000 tree iterations has better performance for the high signal efficiency region ( eff @xmath65 50% ) .",
    "the results from adaboost with @xmath64=0.5 are comparable to those from @xmath0-boost .",
    "@xmath0-boost with @xmath70 0.01 works slightly better because @xmath0-boost converges more quickly with larger @xmath0 values .",
    "however , with more tree iterations , the final performances for different @xmath0 values are very comparable . here",
    "@xmath0 = 0.01 is chosen for further comparisons .",
    "the right plot of figure 5 shows the relative ratio versus the signal efficiency for adaboost and @xmath0-boost using two different ways to split tree nodes .",
    "one way is to maximize the criterion based on the gini index to select the next tree split , the other way is to split the left tree node first . for adaboost , the performance for the `` left node first '' method gets worse for signal efficiency less than about 65% . at about the same signal efficiency , the performance for the two @xmath0-boosts are quite comparable and are comparable with adaboost based on the gini index . however , the @xmath0-boost method based on the gini index becomes worse than the others for high signal efficiency .",
    "larger @xmath0 makes @xmath0-boost converge more quickly , but increasing the the size of decision trees also makes @xmath0-boost converge more quickly .",
    "the performance comparison of adaboost with different tree sizes shown in the right plot of figure 2 is for the same number of tree iterations ( 1000 ) . to make a fair comparison for the boosting performance with different tree sizes , it is better to let them have a similar number of total tree leaves .",
    "the top left plot of the figure 6 shows the relative ratio versus the signal efficiency for adaboost and @xmath0-boost with similar numbers of the total tree leaves , 1800 tree iterations for 45 leaves per tree and 10000 tree iterations for 8 leaves per tree . for a small decision tree size of 8 leaves , the performance of the @xmath0-boost is better than that of adaboost for 10000 trees . for a large decision tree size of 45 leaves , @xmath0-boost has slightly better performance than adaboost at low @xmath62 signal efficiency ( @xmath6665% ) , but worse at high @xmath62 signal efficiency ( @xmath6570% ) .",
    "the comparison between small tree size ( 8 leaves ) and large tree size ( 45 leaves ) with comparable overall decision tree leaves indicates that large tree size with 45 leaves yields @xmath6810%-20% better performance for the miniboone monte carlo samples .",
    "the other five plots in figure 6 show the relative ratio versus the number of tree iterations for adaboost and @xmath0-boost with 45 leaves and 8 leaves assuming signal efficiencies of 40% , 50% , 60% , 70% , 80% , respectively .",
    "the maximum number of tree iterations is 5000 for the large tree size of 45 leaves and 10000 for the small tree size of 8 leaves .",
    "usually , the performance of the boosting method becomes better with more tree iterations in the beginning ; then at some point , it may reach an optimal value and gradually get worse with increasing number of trees , especially in the low signal efficiency region .",
    "the turning point of the boosting performance depends on the signal efficiency and mc samples used for training and test .",
    "generally , if the number of weighted signal events is larger than the number of weighted background events in a given leaf , it is called a signal leaf , otherwise , a background leaf . here ,",
    "the threshold value for signal purity is 50% for the leaf to be called a signal leaf .",
    "this threshold value can be modified , say , to 30% , 40% , 45% , 60% or 70% .",
    "it is seen in figure 7 that the performance of boosted decision trees with adaboost degrades for threshold values away from the central value of 50% . especially for threshold values away from 50% , the @xmath71 of m@xmath37 tree often converges to 0.5 within about 100 tree iterations ; after that the weights of the misclassified events do not successfully update because @xmath72 if @xmath73 . then @xmath74 remains the same as for the previous tree .",
    "typically , the @xmath71 value increases for the first 100 - 200 tree iterations and then remains stable for further tree iterations , causing the weight of m@xmath37 tree , @xmath75 , to decrease for the first 100 - 200 tree iterations and then remain stable . for practical use of the adaboost algorithm , a lower limit , say , 0.01 , on @xmath75 will avoid the impotence of the succeeding boosted decision trees .",
    "this problem is unlikely to happen for @xmath0-boost because the weights of misclassified events are always updated by the same factor , @xmath76 . if differing purity threshold values are applied to boosted decision trees with @xmath0-boost , the performance peaks around 50% and slightly worsens , typically within 5% , for other values ranging from 30% to 70% .    the unweighted misclassified event rate , weighted misclassified event rate @xmath71 and @xmath75 for the boosted decision trees with the adaboost algorithm versus the number of tree iterations",
    "are shown in figure 8 , for a signal purity threshold value of 50% . from this plot ,",
    "it is clear that , after a few hundred tree iterations , an individual boosted decision tree has a very weak discriminant power ( i.e. , is a `` weak '' classifier ) .",
    "the @xmath71 is about 0.4 - 0.45 , corresponding to @xmath75 of around 0.2 - 0.1 .",
    "the unweighted event discrimination of an individual tree is even worse , as is also seen in figure 8 .",
    "boosted decision trees focus on the misclassified events which usually have high weights after hundreds of tree iterations .",
    "the advantage of the boosted decision trees is that the method combines all decision trees , `` weak '' classifiers , to make a powerful classifier as stated in the introduction section .    when the weights of misclassified events are increased ( boosted )",
    ", some events which are very difficult correctly classify obtain large event weights . in principle , some outliers which have large event weights may degrade the boosting performance . to avoid this effect ,",
    "it might be useful to set an upper limit for the event weights to trim some outliers .",
    "it is found that setting a weight limit does nt improve the boosting performance , and , in fact , may degrade the boosting performance slightly .",
    "however , the effect was observed to be within one standard deviation for the statistical error .",
    "one might also trim events with very low weights which can be correctly classified easily to provide a better chance for difficult events .",
    "no apparent improvement or degradation was observed considering the statistical error .",
    "these results may indicate that the boosted decision trees have the ability to deal with outliers quite well and to focus on the events located around the boundary regions where it is difficult to correctly distinguish signal and background events .",
    "besides adaboost and @xmath0-boost , there are other algorithms such as @xmath0-logitboost , and @xmath0-hingeboost which use different ways of updating the event weights for the misclassified events .",
    "the four plots of figure 9 show the relative ratio versus the signal efficiency for various boostings with different tree sizes .",
    "the top left , top right , bottom left , and bottom right plots are for 500 , 1000 , 2000 , and 3000 tree iterations , respectively . boosting with a large tree size of 45 leaves",
    "is seen to work better than boosting with a small tree size of 8 leaves as noted above .",
    "adaboost and @xmath0-boost have comparable performance , slightly better than that of @xmath0-logitboost .",
    "@xmath0-hingeboost is the worst among these four boosting algorithms , especially for the low signal efficiency region .",
    "the top left , top right , bottom left and bottom right plots of figure 10 show the relative ratio versus the signal efficiency with 45 leaves of @xmath0-boost , adaboost , @xmath0-logitboost and @xmath0-hingeboost for varying numbers of tree iterations . generally , boosting performance continuously improves with an increase in the number of tree iterations until an optimum point is reached . from the two top plots , it is apparent that @xmath0-boost converges more slowly than does adaboost",
    "; however , with about 1000 tree iterations , their performances are very comparable .",
    "there is only marginal improvement beyond 1000 tree iterations for high signal efficiency , and the performance may get worse for the low signal efficiency region if the boosting is over - trained ( goes beyond the optimal performance range ) .",
    "similar plots for the four boosting algorithms with 8 leaves per decision tree are shown in the figure 11 .",
    "results for @xmath0-hingeboost with 30 and 8 tree leaves are shown in the bottom right plots of figures 10 and 11 .",
    "the performance for 200 tree iterations seems worse than that for 100 tree iterations .",
    "this may indicate that its performance is unstable in the first few hundred tree iterations , but works well after about 500 tree iterations .",
    "however , the overall performance of @xmath0-hinge boost is the worst among the four boosting algorithms described above .    for some purposes , logitboost has been found to be superior to other algorithms@xcite .",
    "for the miniboone data , it was found to have about 10%-20% worse background contamination for a fixed signal efficiency than the regular adaboost .",
    "logitboost converged very rapidly after less than 200 trees and the contamination ratio got worse past that point .",
    "a modification of logitboost was tried in which the convergence was slowed by taking @xmath77 , the extra factor of @xmath78 slowing the weighting update rate .",
    "this indeed improved the performance considerably , but the results were still slightly worse than obtained with adaboost or @xmath0-boost for a tree size of 45 leaves .",
    "the convergence to an optimum point still took fewer than 300 trees , which was less than the number needed with adaboost or @xmath0-boost .",
    "gentle adaboost and real adaboost were also tried ; both of them were found slightly worse than the discrete adaboost .",
    "relative error ratio versus signal efficiency for various boosting algorithms are listed in table.[table : ratio ] .",
    ".[table : ratio ] relative error ratio versus signal efficiency for various boosting algorithms for miniboone data .",
    "differences up to about 0.03 are largely statistical .",
    "b=0.5 means smooth scoring function described in section 9 . [",
    "cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]",
    "the random forests is another algorithm which uses a `` majority vote '' to improve the stability of the decision trees .",
    "the training events are selected randomly with or without replacement .",
    "typically , one half or one third of the training events are selected for each decision tree training .",
    "the input variables can also be selected randomly for determining the tree splitters .",
    "there is no event weight update for the misclassified events . for the adaboost algorithm ,",
    "each tree is built using the results of the previous tree ; for the random forests algorithm , each tree is independent of the other trees .",
    "figure 12 shows a comparison between random forests of different tree sizes and adaboost , both with 1000 tree iterations .",
    "large tree size is preferred for the random forests ( the original random forests method lets each tree develop fully until all tree leaves are pure signal or background ) . in this study",
    "a fixed number of tree leaves were used .",
    "the performance of the random forests algorithm with 200 or 400 tree leaves is about equal . compared with adaboost ,",
    "the performance of the random forests method is significantly worse .",
    "the main reason for the inefficiency is that there is no event weight update for the misclassified events .",
    "one of main advantages for the boosting algorithm is that the weights of misclassified events are boosted which makes it possible for them to be correctly classified in succeeding tree iterations .",
    "considering this advantage , an event weight update algorithm ( adaboost ) was used to boost the random forests .",
    "the performances of the boosted random forests algorithm are then significantly better than those of the original random forests as can be seen in figure 12 .",
    "the performance of the adaboost with 100 leaves per decision tree is slightly better than that of the boosted random forests .",
    "other tests were made using one half training events selected randomly for each tree together with 30% , 50% , 80% or 100% of the input variables selected randomly for each tree split .",
    "the performances of the boosted random forests method using the adaboost algorithm are very stable .",
    "the boosted random forests only uses one half or one third of the training events selected randomly for each tree and also only a fraction of the input variables for each tree split , selected randomly ; this method has the advantage that it can run faster than regular adaboost while providing similar performance .",
    "in addition , it may also help to avoid over - training since the training events are selected partly and randomly for each decision tree .",
    "some recent papers @xcite indicate that post - fitting of the trained boosted decision trees may help to make further improvement .",
    "one possibility is that a selected ensemble of many decision trees could be better than the ensemble of all trees . here",
    "post - fitting of the weights of decision trees was tried .",
    "the basic idea is to optimize the boosting performance by retuning the weights of the decision trees or even removing some of them by setting them to have 0 weight .",
    "a genetic algorithm @xcite is used to optimize the weights of all trained decision trees .",
    "a new mc sample is used for this purpose .",
    "the mc sample is split into three subsamples , mc1 , mc2 and mc3 , each subsample having about 26700 signal events and 21000 background events .",
    "mc1 is used to train adaboost with 1000 decision trees . the background efficiency for mc1 , mc2 and mc3 for a signal efficiency of 60%",
    "are 0.12% , 5.15% and 4.94% , respectively .",
    "if mc1 is used for post - fitting , then the corresponding background efficiency can be driven down to 0.05% , but the background efficiency for test sample mc3 is about 5.5% .",
    "it has become worse after post - fitting .",
    "it seems that it is not good to use same sample for the boosting training and post - fitting .",
    "if mc2 is used for post - fitting , then the background efficiency goes down to 4.21% for the mc2 , and 4.76% for the testing sample mc3 .",
    "the relative improvement is about 3.6% and the statistical error for the background events is about 3.2% .",
    "suppose the mc samples for post - fitting and testing are exchanged , mc3 is used for post - fitting while mc2 is used for testing .",
    "the background efficiency is 4.38% for training sample mc3 and 5.06% for the testing sample mc2 .",
    "the relative improvement is about 1.5% .",
    "a second post - fitting program was tried , the pathseeker program of j.h .",
    "friedman and b.e .",
    "popescu@xcite , a robust regularized linear regression and classification method .",
    "this program produced no overall improvement , with perhaps a marginal 4% improvement for 50% signal efficiency .",
    "it seems that post - fitting makes only a marginal improvement based on our studies .",
    "one of the major advantages of the boosted decision tree algorithm is that it can handle large numbers of input variables as was pointed out previously@xcite . generally speaking ,",
    "more input variables cover more information which may help to improve signal and background event separation .",
    "often one can reconstruct several hundreds or even thousands of variables which have some discriminant power to separate signal and background events .",
    "some of them are superior to others , and some variables may have correlations with others .",
    "too many variables , some of which are `` noise '' variables , wo nt improve but may degrade the boosting performance .",
    "it is useful to select the most useful variables for boosting training to maximize the performance .",
    "new mc samples were generated with 182 reconstructed variables . in order to select the most powerful variables ,",
    "all 182 variables were used as input to boosted decision trees running 150 tree iterations .",
    "then the effectiveness of the input variables was rated based on how many times each variable was used as a tree splitter .",
    "the first variable in the sorted list was regarded as the most useful variable for boosting training .",
    "the first 100 sorted input variables were selected to train adaboost with @xmath67 , 45 leaves per decision tree and 1000 tree iterations .",
    "the dependence of the number of times a variable is used as a tree splitter versus the number of tree iterations is shown for some selected input variables ( variables number 1 , 5 , 10 , 20 , 50 , 80 and 100 ) in the top left plot with linear scale and in the top right plot with log scale .    in this way ,",
    "the first 30 , 40 , 60 , 80 , 100 , 120 and 140 input variables were selected from the sorted list to train boosted decision trees with 1000 tree iterations .",
    "a comparison of their performance is shown in the left plot of figure 14 .",
    "the boosting performance steadily improves with more input variables until about 100 to 120 . adding further input variables",
    "does nt improve and may degrade the boosting performance .",
    "the main reason for the degradation is that there is no further useful information in the additional input variables and these variables can be treated as `` noise '' variables for the boosting training .",
    "however , if the additional variables include some new information which is not included in the other variables , they should help to improve the boosting performance .    so far only one way to sort the input variables has been described .",
    "some other ways can also be used and work reasonably well as shown in the right plot of figure 14 .",
    "list1 means the input variables are sorted based on the how many times they were used as tree splitters for 150 tree iterations , list2 means the input variables are sorted based on their gini index contributions for 150 tree iterations , and list3 means the variables are sorted according to which variables are used earlier than others as tree splitters for 150 tree iterations .",
    "list1001 , list1002 and list1003 are similar to list1 , list2 and list3 , but use 1000 tree iterations .",
    "the first 100 input variables from the sorted lists are used for boosting training with 1000 tree iterations .",
    "the performances are comparable for 100 input variables sorted in different ways .",
    "however , the boosting performances for list1 and list3 are slightly better than the others .",
    "if an equal number of input variables of 100 are selected from each list , the number of variables which overlap typically varies from about 70 to 90 for the different lists . in other words ,",
    "about 10 to 30 input variables are different among the various lists . in spite of these differences ,",
    "the boosting performances are still comparable and stable . further studies with mc samples generated using varied mc input parameters corresponding to systematic errors show that the boosting outputs are very stable even though some input variables vary quite a lot .",
    "if these same varied mc samples are applied to the anns , it turns out that boosted decision trees work significantly better than the anns for both event separation performance and for stability .",
    "in the standard boost , the score for an event from an individual tree is a simple square wave depending on the purity of the leaf on which the event lands .",
    "if the purity is greater than 0.5 , the score is 1 and otherwise it is @xmath79 .",
    "one can ask whether a smoother function of the purity might be more appropriate .",
    "if the purity of a leaf is 0.51 , should the score be the same as if the purity were 0.99 ?",
    "two possible alternative scores were tested .",
    "let @xmath80purity@xmath79 .",
    "@xmath81 @xmath82 where @xmath83 and @xmath84 are parameters .",
    "tests were run for various parameter values for scores a and b and compared with the standard step function .",
    "performance comparisons of adaboost for various parameters @xmath83 ( left ) and @xmath84 ( right ) values are shown in figure 15 .    for a smooth function with @xmath85",
    ", boosting performance converges faster than the original adaboost algorithm for the first few hundred decision trees , as shown in figure 16 .",
    "however , no evidence was found that the optimum was reached any sooner by the smooth function .",
    "the reason is that the smooth function of the purity describes the probability of a given event to be signal or background in more detail than the step function used in the original adaboost algorithm . with an increase in the number of tree iterations",
    ", however , the `` majority vote '' plays the most important role for the event separation .",
    "the ultimate performance of the smooth function with @xmath85 is comparable to the performance of the standard adaboost .",
    "in miniboone , one is trying to improve the signal to background ratio by more than a factor of 100 .",
    "one might expect that one should start by giving the background a greater total weight than the signal .",
    "in fact , giving the background two to five times the weight of the signal slightly degraded the performance . giving the background 0.5 to 0.2 of the weight of the signal gave the same performance as equal initial weights .",
    "for one set of monte carlo runs the pid variables were carefully modified to be flat as functions of the energy of the event and the event location within the detector .",
    "this decreased the correlations between the pid variables .",
    "the performance of these corrected variables was compared with the performance of the uncorrected variables . as expected ,",
    "the convergence was much faster at first for the corrected variable boost .",
    "however , as the number of trees increased , the performance of the uncorrected variable boost caught up with the other . for 1000 trees ,",
    "the performance of the two boost tests was about the same . over the long run ,",
    "boost is able to compensate for correlations and dependencies , but the number of trees for convergence can be considerably shortened by making the pid variables independent .    the number of mc events used to train the boosting effectively is also an important issue we have investigated .",
    "generally , more training events are preferred , but it is impractical to generate unlimited mc events for training .",
    "the performance of adaboost with 1000 tree iterations , 45 tree leaves per tree using various number of background events ranging from 10000 to 60000 for training are shown in figure 17 , where the number of signal events is fixed 20000 .",
    "for the miniboone data , the use of 30000 or more background events works fairly well ; fewer background events for training degrades the performance .",
    "pid input variables obtained using the event reconstruction programs for the miniboone experiment were used to train boosted decision trees for signal and background event separation .",
    "numerous trials were made to tune the boosted decision trees .",
    "based on the performance comparison of various algorithms , decision trees with the adaboost or the @xmath0-boost algorithms are superior to the others .",
    "the major advantages of boosted decision trees include their stability , their ability to handle large number of input variables , and their use of boosted weights for misclassified events to give these events a better chance to be correctly classified in succeeding trees .",
    "boosting is a rugged classification method .",
    "if one provides sufficient training variables and sufficient leaves for the tree , it appears that it will , eventually , converge to close to an optimum value .",
    "this assumes that @xmath0 for @xmath0-boost or @xmath64 for adaboost are not set too large .",
    "there are modifications of the basic boosting procedure which can speed up the convergence .",
    "use of a smooth scoring function improves initial convergence . in the last section",
    ", it was seen that removing correlations of the input pid variables improved convergence speed .",
    "for some applications , the use of a boosted natural forests technique may also speed the convergence .    for a large set of discriminant variables ,",
    "several techniques can be used to select a set of powerful input variables to use for training boosted decision trees .",
    "post - fitting of the boosted decision trees makes only a marginal improvement in the tests presented here .",
    "we wish to express our gratitude to the miniboone collaboration for the excellent work on the monte carlo simulation and the software package for physics analysis .",
    "this work is supported by the department of energy and the national science foundation of the united states .",
    "j. friedman , _ greedy function approximation : a gradient boosting machine _ , annals of statistics , 29(5 ) , 1189 - 1232(2001 ) ; j. friedman , t. hastie , r. tibshirani , _ additive logistic regression : a statistical view of boosting _ , annals of statistics , 28(2 ) , 337 - 407(2000 )          b.p .",
    "roe , h.j .",
    "yang , j. zhu , y. liu , i. stancu , g. mcgregor , _ boosted decision trees as an alternative to artificial neural network for particle identification _ , nuclear instruments and methods in physics research a , 543 ( 2005 ) 577 - 584 , physics/0408124 .",
    "l. breiman , j.h .",
    "friedman , r.a .",
    "olshen , and c.j .",
    "stone , _ classification and regression trees _",
    ", wadsworth international group , belmont , california ( 1984 ) .",
    "schapire , _ the boosting approach to machine learning : an overview _ , msri workshop on nonlinear estimation and classification , ( 2002 ) .    y. freund and r.e .",
    "schapire , _ a short introduction to boosting _ ,",
    "journal of japanese society for artificial intelligence , 14(5 ) , 771 - 780 , ( september , 1999 ) .",
    "( appearing in japanese , translation by naoki abe . )",
    "t. hastie , r. tibshirani , j. friedman , _ elements of statistical learning , data mining , inference and prediction _ , chapter 10 , section 11 , page 324 , springer , ( 2001 ) .",
    "m. dettling , p. buhlmann , _ boosting for tumor classification with gene expression data _ , bioinformatics , vol.19 , no.9 , pp1061 - 1069 , ( 2003 )    z.h .",
    "zhou , j.x .",
    "wu , w. tang , _ ensembling neural networks : many could be better than all _ , artificial intelligence , 137(1 - 2):239 - 263,(2002 ) ; z.h .",
    "zhou , w. tang , _ selective ensemble of decision trees _",
    ", nanjing university , ( 2003 ) ."
  ],
  "abstract_text": [
    "<S> boosted decision trees are applied to particle identification in the miniboone experiment operated at fermi national accelerator laboratory ( fermilab ) for neutrino oscillations . </S>",
    "<S> numerous attempts are made to tune the boosted decision trees , to compare performance of various boosting algorithms , and to select input variables for optimal performance .    </S>",
    "<S> * *    studies of boosted decision trees    * *    for miniboone particle identification </S>"
  ]
}