{
  "article_text": [
    "in classical shannon theory , data processing inequalities ( in various forms ) are frequently used to prove converses to coding theorems and to establish fundamental properties of information measures , like the entropy , the mutual information , and the kullback  leibler divergence @xcite .",
    "a very well  known example is the converse to the joint source  channel coding theorem , which sets the stage for the separation theorem of information theory : when a source with rate  distortion function @xmath0 is encoded and transmitted across a channel with capacity @xmath1 , the distortion of the reconstruction at the decoder must obey the inequality @xmath2 , or equivalently , @xmath3 .",
    "this lower bound is achievable ( e , g . , by separate source coding and channel coding ) in the limit of large block length .",
    "ziv and zakai @xcite ( see also csiszr @xcite , @xcite , @xcite for related work ) have observed that in order to obtain a wider class of data processing inequalities , the ( negative ) logarithm function , that plays a role in the classical mutual information , can be replaced by an arbitrary convex function @xmath4 , provided that it obeys certain regularity conditions .",
    "this generalized mutual information , @xmath5 , was further generalized in @xcite to be based on multivariate convex functions , as opposed to the univariate convex functions in @xcite . in analogy to the classical converse to the joint source  channel coding theorem , one can then define a generalized rate  distortion function @xmath6 ( as the minimum of the generalized mutual information between the source and the reproduction , s.t .  some distortion constraint ) and a generalized channel capacity @xmath7 ( as the maximum generalized mutual information between the channel input and output ) and establish another lower bound on the distortion via the inequality @xmath8 that stems from the data processing inequality of @xmath9 .",
    "while this lower bound obviously can not be tighter than its classical counterpart in the limit of long blocks ( which is asymptotically achievable ) , ziv and zakai have demonstrated that for short block codes ( e.g. , codes of block length @xmath10 ) , sharper lower bounds can certainly be obtained ( see also @xcite for more recent developments ) .",
    "gurantz , in his m.sc .",
    "work @xcite ( supervised by ziv and zakai ) , continued the work in @xcite at a specific direction : he constructed a special class of generalized information functionals defined by iteratively alternating between applications of convex functions and multiplications by likelihood ratios ( or more generally , radon  nykodim derivatives ) . after proving that this functional obeys a data processing inequality ,",
    "gurantz demonstrated how it can be used to improve on the arimoto bound for coding above capacity @xcite and on the gallager upper bound of random coding @xcite by a pre - factor of @xmath11 .",
    "motivated by the belief that the interesting nested structure of gurantz information functional can be further exploited , we continue , in this work , to investigate this information measure and we further study its properties and potential .",
    "we begin by putting the gurantz functional in the broader perspective of the other information measures due to ziv and zakai @xcite , @xcite ( section 2 ) .",
    "specifically , we first discuss two possible methods to define a generalized mutual information from the gurantz functional , each one with its advantages and disadvantages .",
    "we then show that both of these generalized mutual informations can be viewed as special cases of the generalized mutual information of @xcite , which is based on multivariate convex functions .",
    "the proof of this fact then naturally suggests a way to broaden the scope and define a family of information measures with a tree structure of convex functions and likelihood ratios .",
    "we then focus on a concrete choice of the convex functions ( section 3 ) in the gurantz information measure ( in particular , power functions ) , which turn out to yield an information measure that extends the notion of the bhattacharyya distance ( or the chernoff divergence ) : while the ordinary bhattacharyya distance is based on the ( weighted ) geometric mean of two replicas of the channel s conditional distribution ( see , e.g. , ( * ? ? ?",
    "* eq.(2.3.15 ) ) ) , the more general information measure considered here , allows an arbitrary number of such replicas .",
    "this generalized bhattacharyya distance is also intimately related to the gallager function @xmath12 @xcite , @xcite , which is indeed another information measure obeying a data processing inequality ( * ? ? ?",
    "* proposition 2 ) , since it is yet another special case of the information measures in @xcite .",
    "finally , we apply the data processing inequality , induced by the above described generalized bhattacharyya distance , to a detailed study of lower bounds on parameter estimation under additive white gaussian noise ( awgn ) and show that in certain cases , tighter bounds can be obtained by using more than two replicas ( section 4 ) . in this particular case , it turns out that three is the optimum number of replicas in the high snr regime . while the resulting lower bound",
    "may still not compete favorably with the best available bounds for the ordinary awgn channel , the advantage of the new lower bound , relative to the other bounds , becomes apparent in the presence of channel uncertainty , like in the case of an awgn channel with unknown fading .",
    "this different behavior , in the presence of channel uncertainty , is explained by the convexity property of the information measure .",
    "in @xcite , a generalized information functional was defined in the following manner : let @xmath13 and @xmath14 be random variables taking on values in alphabets @xmath15 and @xmath16 , respectively , where here and throughout the sequel , all alphabets may either be finite , countably infinite , or uncountably infinite , like intervals or the entire real line .",
    "let @xmath17 be a given list of symbols ( possibly with repetitions ) from @xmath15 .",
    "let @xmath18 be a collection of univariate functions , defined on the positive reals , with the following properties , holding for all @xmath19 :    1 .",
    "2 .   @xmath21 .",
    "either the function @xmath22 is monotonically non - decreasing and @xmath23 is convex , or @xmath24 is monotonically non  increasing and @xmath23 is concave ( here , the notation @xmath25 means function composition ) .",
    "now , define the _",
    "gurantz functional _ as @xmath26 where here and throughout , it is understood that integrals and probability density functions should be replaced , in the countable alphabet case , by summations and probability mass functions , respectively .",
    "the data processing inequality associated with the gurantz functional is the following : let @xmath27 be a markov chain and let @xmath28 be a convex function which , together with @xmath29 , complies with rules 13 above .",
    "then , @xmath30 the direct proof of this inequality is fairly straightforward @xcite : first , observe that @xmath31 due to the markov property .",
    "then , one can easily obtain a sequence of lower bounds on the right  hand ",
    "side ( r.h.s . ) of eq.([gyz ] ) by successive applications of jensen s inequality , where at each stage , the expectation with respect to ( w.r.t . )",
    "@xmath32 propagates into the next convex function and then partially cancels out with the factor @xmath33 at the denominator of the likelihood ratio .    note that according to the definition of @xmath34 , @xmath35 is the random variable that controls the distribution of @xmath14 ( as the averaging is w.r.t .",
    "@xmath36 ) , whereas @xmath37 can be viewed as ` dummy ' variables .",
    "one way to define a generalized mutual information based on @xmath38 , which is a functional of @xmath39 , is by assigning a certain probability distribution to @xmath40 .",
    "let @xmath41 , where @xmath42 is the actual distribution of the random variable @xmath13 and @xmath43 is an arbitrary conditional distribution of @xmath44 given @xmath45 , for example , @xmath46 or @xmath47 for some deterministic functions @xmath48 .",
    "now , for a given choice of @xmath49 , the _ gurantz mutual information _",
    "@xmath50 can be defined as @xmath51 where the expectation is w.r.t .",
    "the above defined joint distribution of the random variables @xmath13 , @xmath52 , ... , @xmath53 .",
    "this generalized mutual information is now a well  defined functional of @xmath54 . in principle , one may apply the generalized data processing inequality @xmath55 for any given choice of @xmath49 ( consider these as parameters ) and then optimize the resulting distortion bound w.r.t .  the choice of these parameters .",
    "our first observation is that @xmath50 is a special case of the zakai ",
    "ziv generalized mutual information @xcite , defined as @xmath56 where @xmath4 is a multivariate convex function of @xmath57 variables and @xmath58 , @xmath59 , are arbitrary measures on @xmath60 .    to see why this is true ,",
    "consider the following : for each convex ( resp .",
    ", concave ) function @xmath61 , define the bivariate _ perspective function _ @xmath62 , where @xmath63 , which is a convex ( resp .",
    ", concave ) function as well , and jointly in both variables ( * ? ? ?",
    "* subsection 3.2.6 ) .",
    "thus , @xmath64 now , under the assumed properties of the functions @xmath65 , it is easy to see that @xmath66 is jointly convex in @xmath67 . thus , upon taking the expectation of the last line of ( [ specialcase ] ) w.r.t .",
    "@xmath68 , we have ( after multiplying the numerator and the denominator of each likelihood ratio by @xmath68 ) that @xmath69 is an instance of @xmath70 for every given @xmath71 , with the assignments @xmath72 , @xmath59",
    ".    we can represent the general structure of information functionals , such as @xmath73 and @xmath74 , as well as the forms in the different lines of eq.([specialcase ] ) , graphically , in terms of _ factor trees _",
    "( i.e. , factor graphs which are trees ) that obey the following rules",
    "there are two types of nodes , variable nodes and function nodes , and each edge of the tree connects a variable node and a function node .",
    "2 .   the root of the tree is a function node whereas the leaves are variable nodes .",
    "each function node is represented by a convex function @xmath75 and each variable node is represented by a likelihood ratio @xmath76 , whose shorthand notation here will be @xmath77 .",
    "4 .   there is a directed edge from function node @xmath75 to variable node @xmath78 ( denoted @xmath79 ) if the information measure includes a product of the form @xmath80 .",
    "there is a directed edge from variable node @xmath81 to function node @xmath82 ( denoted @xmath83 ) if @xmath81 multiplies an argument of @xmath82 .",
    "6 .   for every path @xmath84",
    ", @xmath85 must be equal to @xmath86 ( namely , @xmath87 ) .",
    "7 .   for all direct offsprings of the root , @xmath88",
    ", the second subscript @xmath85 is the same .",
    "now observe that @xmath73 and @xmath74 correspond to two extreme cases : while @xmath74 corresponds to a factor tree where all @xmath57 leaves are connected directly to the root , @xmath73 corresponds to a simple chain ( i.e. , every node has one offspring and there is only one leaf ) , which alternates between variable nodes and function nodes .",
    "the form that appears in the last line of ( [ specialcase ] ) corresponds to a binary tree with a comb structure , i.e. , every node that is not a leaf has two offsprings , one of which is a leaf . more generally , every factor graph with a tree structure , that complies with the above rules , corresponds to a valid information measure that satisfies a data processing inequality .",
    "for example , the factor graph of fig .",
    "[ factree ] corresponds to the information measure @xmath89    in view of the observation that @xmath69 a special case of the @xmath70 , there is another way to use it to obtain data processing inequalities for communication systems . according to ( * ? ? ?",
    "* theorems 3.1 and 5.1 ) , the following is true : let @xmath90 be a markov chain and let @xmath91 where @xmath92 is a deterministic function .",
    "let @xmath93 , @xmath59 , be arbitrary measures and define @xmath94 , @xmath95 , @xmath96 .",
    "then , @xmath97 as described informally in the introduction , the maximum of the left  hand side ( l.h.s . ) over @xmath98 and the minimum of the r.h.s .  over @xmath99 ( subject to some distortion constraint ) can be thought of as generalized channel capacity and generalized rate  distortion function , respectively , as in @xcite .",
    "now , consider the special case where @xmath74 is based on a multivariate convex function @xmath100 as defined in ( [ hatq ] ) , where each bivariate convex function @xmath101 is the perspective of a certain univariate convex function , i.e. , @xmath102 .",
    "then by a similar argument as above ( going the other direction ) , we get another information measure in the spirit of gurantz : @xmath103 since it is a special case of @xmath70 , then it obviously satisfies a strong and @xmath91 , we have @xmath104 . ] data processing inequality @xmath105 . assuming , in addition , that the encoder is given by a deterministic function @xmath106 , we can choose @xmath72 , where @xmath107 is a specific member in @xmath15 and then @xmath108 .",
    "we then obtain @xmath109 multiplying both sides by @xmath110 and integrating over @xmath111 , we get @xmath112 where the expectation on the l.h.s .  is w.r.t.@xmath113 , and the expectation on the r.h.s .  is w.r.t .",
    "this is different from the data processing theorem in @xcite , because it allows ` moving ' in both directions of the markov chain and not only to the right .    to summarize ,",
    "we have seen two approaches to derive data processing inequalities from the inequality @xmath115 for a markov chain @xmath116 ( where have slightly changed the notation relative to eq .",
    "( [ gurantz1 ] ) ) : according to the first approach , one allows an arbitrary distribution @xmath117 and averages both sides w.r.t .",
    "this defines the @xmath119 and @xmath120 as functionals of @xmath121 and @xmath122 , respectively , where @xmath117 serve as free parameters that can be optimized , to get the tightest distortion bound .",
    "the advantage of this approach is the free choice of @xmath117 , which gives many degrees of freedom .",
    "the disadvantage is that @xmath119 depends on the source and the encoder and there is no apparent way to prove a strong data processing theorem , in general , i.e. , to prove that @xmath119 can be further upper bounded by @xmath50 ( whatever its definition may be ) and thereby define a channel capacity , that is independent of the source ( in addition to a generalized rate distortion function , which is @xmath123 s.t .  some distortion constraint ) .",
    "the inequality @xmath124 is relevant to situations where there is no encoder to be optimized , namely , when the channel from @xmath125 to @xmath14 is given and can not be shaped by encoding .",
    "this happens , for example , in parameter estimation problems .",
    "according to the second approach , one limits @xmath126 to be @xmath127 .",
    "this leaves no degrees of freedom , but it admits a strong data processing theorem , and hence allows to define both a generalized rate ",
    "distortion function and a generalized channel capacity , whose calculations are completely decoupled of each other .",
    "it is also much simpler to use .",
    "this type of data processing inequality is more suitable for coded communication systems , where there is also an encoder to optimize .    from this point",
    "onward , we essentially confine ourselves to the second option , mainly for reasons of simplicity .",
    "an interesting and convenient choice of the functions @xmath65 is the following : @xmath128 , and @xmath129 for @xmath130 , where @xmath131 , @xmath96 . in this case , @xmath132 is monotonically decreasing and @xmath23 is concave , so this choice complies with the rules . in this case",
    ", we have : @xmath133 where @xmath134 are given by : @xmath135 note that the coefficients @xmath136 are all non  negative and their sum is equal to @xmath10 .",
    "conversely , for every set of coefficients @xmath134 with these properties , one can find @xmath137 , all in @xmath138 $ ] , using the following inverse transformation : @xmath139 this allows us parametrize the information measure directly in terms of an arbitrary set of non  negative numbers @xmath134 summing to unity , without worrying about @xmath140 .",
    "the resulting information measure can then be viewed as an extension of the chernoff divergence between two conditional densities , @xmath141 and @xmath142 , to a general number of densities , where the powers of @xmath143 always sum up to unity .",
    "specializing this to the case @xmath144 for all @xmath145 , eq.([specg ] ) extends the bhattacharyya distance . following the discussion of the second option at the end of section 2 ,",
    "if , in addition , we assign @xmath146 , then @xmath147 , where @xmath148 is the gallager function @xcite @xmath149^{1+\\rho}\\right\\}.\\ ] ] thus , @xmath50 extends , not only the chernoff divergence , but also the gallager function , albeit only at integer values of the parameter @xmath150 .",
    "indeed , it was shown in ( * ? ? ?",
    "* proposition 2 ) that the gallager function ( for every real @xmath151 ) satisfies a data processing inequality , because it is also a special case of @xmath70 .",
    "in other words , the generalized chernoff divergence can be obtained as a special case of @xmath70 in two different ways : one is via @xmath73 and the other is via the gallager function .",
    "the advantage of working with gallager s function for integer values of @xmath150 , is that an integral raised to an integer power @xmath152 can be expressed in terms of @xmath152dimensional integration over the @xmath152 replicas , @xmath153,@xmath154, ...",
    ",@xmath155 , that in turn can be commuted with the additional out  most integration over @xmath16 . in some situations , this enables explicit calculations more conveniently .",
    "in this section , we apply the data processing inequality associated with the generalized bhattacharyya distance to obtain a bayesian lower bound on the estimation error of parameter estimators of a parameter @xmath156 modulated in a signal @xmath157 that is in turn corrupted by gaussian white noise . as mentioned earlier",
    ", we essentially adopt the second approach discussed at the end of section 2 : although we use the data processing inequality @xmath158 , in some of our derivations , we eventually further upper bound @xmath119 by a universal bound , that is independent of the modulation scheme @xmath159 , so in a way , it conveys the notion of generalized capacity .",
    "the model we focus on is the following .",
    "the source symbol @xmath125 , which is uniformly distributed in @xmath160 $ ] , plays the role of a random parameter to be estimated . for reasons of convenience ,",
    "we define the distortion measure between a realization @xmath156 of the source and an estimate @xmath161 ( both in @xmath162 ) as @xmath163 ^ 2.\\ ] ] where @xmath164 @xmath165 being the fractional part of @xmath166 , that is , @xmath167 .",
    "note that in the high  resolution limit ( corresponding to the high signal  to  noise ( snr ) limit ) , the modulo 1 operation has a negligible effect , and hence @xmath168 becomes essentially equivalent to the ordinary quadratic distortion .",
    "indeed , most of our results in the sequel , refer to the high snr regime . at any rate , under the modulo 1 quadratic distortion measure , it is convenient to visualize @xmath125 as being evenly distributed across the circumference of a circle of radius @xmath169 ( or as a phase parameter ) and then @xmath168 is the squared length of the shorter arc ( or the smaller angel ) between the two corresponding points on the circle .",
    "the channel is assumed to be an awgn channel , namely , the channel output is given by @xmath170 where @xmath157 is an arbitrary waveform of unlimited bandwidth , parametrized by @xmath156 and @xmath171 is awgn with two  sided spectral density @xmath172 .",
    "the energy @xmath173 is assumed to be independent of @xmath156 ( for reasons of simplicity ) .",
    "the estimator @xmath161 is assumed to be a functional of the channel output waveform @xmath174 .    before deriving lower bounds on the estimation error , @xmath175",
    ", we first need to derive the generalized rate  distortion function and the generalized channel capacity pertaining to the generalized bhattacharyya distance .",
    "this will be done in the next two subsections .",
    "the `` rate  distortion function '' @xmath0 w.r.t .",
    "the information measure under discussion is given by the minimum of @xmath176^{k+1}\\ ] ] subject to the constraints @xmath177 and @xmath178 .",
    "as explained in @xcite , it is enough to consider channels of the form @xmath179 . defining @xmath180 ,",
    "the problem is then equivalent to @xmath181 this problem is easily solved using calculus of variations @xcite .",
    "suppose that @xmath182 is the optimum density and let @xmath183 , where @xmath92 satisfies @xmath184 defining the lagrangian @xmath185 the condition for @xmath182 being an extremum is @xmath186 for all @xmath92 .",
    "now , @xmath187=0.\\ ] ] for this integral to vanish for every @xmath92 , one must have @xmath188 this means that @xmath182 is of the form @xmath189 where @xmath190^{-1},\\ ] ] and the parameter @xmath191 is determined such that @xmath192 define also @xmath193 let us denote then @xmath194 .",
    "then , @xmath195^{1/(k+1)}\\right]^{k+1}\\nonumber\\\\ & = & c(s)\\left[\\int_{-1/2}^{+1/2}\\frac{\\mbox{d}w}{(1+sw^2)^{1/k}}\\right]^{k+1}\\nonumber\\\\ & = & c(s)[g(s)]^{k+1},\\end{aligned}\\ ] ] where we have defined @xmath196 to summarize , we have obtained a parametric representation of @xmath0 via the variable @xmath191 : @xmath197^{k+1},\\end{aligned}\\ ] ] for later use , we point out that the functions @xmath198 , @xmath199 , and @xmath200 are intimately related .",
    "first , observe that @xmath201 also , using integration by parts , @xmath202 thus , @xmath203 which gives a direct relationship between @xmath198 and @xmath199 whenever @xmath204 . for @xmath205 , the terms pertaining to @xmath199 cancel out , but we then have an explicit formula for @xmath198 .    while in general , @xmath0 is given only a parametric form and not directly , in the limits of very low and very high distortion , one can approximate @xmath0 directly as an explicit function of @xmath206 .",
    "in particular , it is shown in appendix a that in the low resolution regime , @xmath207 where it should be kept in mind that for this information measure , @xmath208 takes on values in the interval @xmath209 $ ] .",
    "here and throughout the sequel , the notation @xmath210 means that @xmath211 tends to unity as a certain parameter ( in this case , @xmath208 ) tends to a certain limit ( in this case , @xmath212 ) , which will always be clear from the context . here , the term @xmath213 is the variance of @xmath125 , which is uniform over @xmath214 $ ] , as no useful information is available except the prior .    in the high  resolution regime ( @xmath215 )",
    ", the behavior depends on whether @xmath216 , @xmath205 , or @xmath217 . in appendix b ,",
    "derivations are provided for all three cases . for @xmath216 , the rate ",
    "distortion function is approximated as @xmath218 or equivalently , the distortion ",
    "rate function is @xmath219 where @xmath220 for @xmath217 , we have @xmath221 the case @xmath205 lacks an explicit closed ",
    "form direct relation between @xmath208 and @xmath206 , but it shows that @xmath222,\\ ] ] which means that the relation between @xmath208 and @xmath206 is essentially linear , like in the case @xmath223 , but in a slightly weaker sense .",
    "it is also easy to extend all the derivations to higher  order moments modulo 1 ( see appendix c for the high resolution analysis ) .",
    "as mentioned earlier , the channel is assumed to be an awgn channel with unlimited bandwidth .",
    "the probability law of the channel from @xmath125 to @xmath14 is given by @xmath224 ^ 2dt\\right\\},\\ ] ] where @xmath225 in the l.h.s .",
    "designates the entire channel output waveform @xmath226 , and @xmath227 means that the constant of proportionality does not depend on @xmath156 .",
    "let us denote @xmath228 consider the integral @xmath229^{1/(k+1)}\\nonumber\\\\ & = & { \\mbox{\\boldmath $ e$}}\\left\\{\\frac{\\prod_{i=1}^k[p_{y|u}(y|u_i)]^{1/(k+1)}}{p_{y|u}(y|u_0)^{k/(k+1)}}\\bigg| u = u_0\\right\\ } \\nonumber\\\\ & = & { \\mbox{\\boldmath $ e$}}\\left\\{\\exp\\left[\\frac{k}{(k+1)n_0}\\int_0^t[y(t)-x(t , u_0)]^2dt- \\frac{1}{(k+1)n_0}\\sum_{i=1}^k\\int_0^t[y(t)-x(t , u_k)]^2dt \\right]\\bigg|u = u_0\\right\\}\\nonumber\\\\ & = & { \\mbox{\\boldmath $ e$}}\\exp\\left\\{\\frac{2}{(k+1)n_0}\\int_0^t[x(t , u_0)+n(t)]\\left[\\sum_{i=1}^kx(t , u_i)- kx(t , u_0)\\right]dt\\right\\}\\nonumber\\\\ & = & \\exp\\left\\{-\\frac{e}{n_0}\\left[1- \\frac{1}{(k+1)^2}\\sum_{i=0}^k\\sum_{j=0}^k\\rho(u_i , u_j)\\right]\\right\\ } , \\label{1stexp}\\end{aligned}\\ ] ] where the last passage is associated with the calculation of the moment  generating function of the gaussian random variable @xmath230\\mbox{d}t\\ ] ] which has zero mean and variance @xmath231 ^ 2\\mbox{d}t$ ] .",
    "the next step , in principle , is take another expectation over the last line of ( [ ksim ] ) w.r.t .  the randomness of @xmath232 .",
    "this can be done explicitly for some specific classes of signals ( e.g. , when @xmath125 is a phase parameter of a sinusoid ) , but in general , it is not a trivial task . as in @xcite and @xcite",
    ", we then resort to a lower bound ( hence an upper bound on @xmath119 ) based on jensen s inequality , by raising the expectation operator to the exponent . denoting @xmath233",
    "it is easily observed that since @xmath232 are independent , then for all @xmath234 : @xmath235 ^ 2\\mbox{d}t{\\stackrel{\\delta } { = } } \\varrho.\\ ] ] note that the parameter @xmath236 is always between @xmath237 and @xmath10 and it depends only on the parametric family of signals . is a rectangular pulse of duration @xmath238 then @xmath239 .",
    "] specifically , continuing from the last line of ( [ 1stexp ] ) , we have @xmath240\\right\\}\\nonumber\\\\ & = & \\exp\\left\\{-\\frac{e}{n_0}\\left[1-\\frac{1}{k+1}\\right]\\right\\}\\cdot{\\mbox{\\boldmath $ e$}}\\exp \\left\\{\\frac{e}{n_0(k+1)^2}\\sum_{i\\ne j}\\rho(u_i , u_j)\\right\\}\\nonumber\\\\ & \\ge&\\exp\\left\\{-\\frac{e}{n_0}\\cdot\\frac{k}{k+1}\\right\\}\\cdot\\exp \\left\\{\\frac{e}{n_0(k+1)^2}\\sum_{i\\ne j}{\\mbox{\\boldmath $ e$}}\\rho(u_i , u_j)\\right\\}\\nonumber\\\\ & = & \\exp\\left\\{-\\frac{e}{n_0}\\cdot\\frac{k}{(k+1)}(1-\\varrho)\\right\\}. \\label{capawgn}\\end{aligned}\\ ] ] note that the expression @xmath241 , that appears in the exponent , is equal to @xmath242 , which is a measure of the variability , or the sensitivity of the @xmath157 to the parameter @xmath156 ( in analogy the cramr  rao bound that depends on the energy of the derivative of the signal w.r.t .  @xmath156 , as another measure of sensitivity ) .",
    "accordingly , classes of signals with smaller values of @xmath236 ( or equivalently , higher values of the integrated variance of @xmath243 ) are expected to yield higher value of @xmath119 , and hence smaller estimation error , at least as far as our bounds predict , and since @xmath236 can not be negative , the best classes of signals , in this sense , are those for which @xmath244 .",
    "note also that for jensen s inequality to be reasonably tight , the random variables @xmath245 should be all close to their expectation @xmath236 with very high probability , and if this expectation vanishes , as suggested , then @xmath245 should all be nearly zero with very high probability .",
    "we will get back to classes of signals with this desirable rapidly vanishing correlation property later on .",
    "we now equate @xmath0 to @xmath119 in order to obtain estimation error bounds in the high snr regime , where the high  resolution expressions of @xmath0 are relevant . as discussed above , in this regime , we will neglect the effect of the modulo 1 operation in the definition of the distortion measure , and will refer to it hereafter as the ordinary quadratic distortion measure . the choice @xmath216 yields @xmath246 ( see also @xcite ) , and following eq .",
    "( [ drk=1 ] ) , this yields @xmath247 and so , the exponential decay of the lower bound is according to @xmath248 . for @xmath205 , according to eq .",
    "( [ drk=2 ] ) , we have @xmath249 , which means an exponential decay according to @xmath250 , which is better . for @xmath251 ,",
    "we use ( [ drk>2 ] ) and the resulting bound decays according to @xmath252\\}$ ] , which is better than the result of @xmath216 , but not as good as the one of @xmath205 .",
    "thus , the best choice of @xmath57 for the high snr regime is @xmath205 , namely , a generalized bhattacharyya distance with @xmath253 replicas , rather the two replicas of the ordinary bhattacharyya distance .    note that since @xmath254 , as mentioned earlier , then for any family of signals , the exponential function @xmath255 is a _",
    "universal _ lower bound ( at high snr ) in the sense that it applies , not only to every estimator of @xmath125 , but also to every parametric family of signals @xmath256 , i.e. , to every modulation scheme without being dependent on this modulation scheme ( see also @xcite ) .",
    "this is in contrast to most of the estimation error bounds in the literature . in other words",
    ", it sets a fundamental limit on the entire communication system and not only on the receiver end for a given transmitter . indeed , for some classes of signals , an mse with exponential decay in @xmath257",
    "is attainable at least in the high snr regime , although there might be gaps in the actual exponential rates compared to the above mentioned bound . for example , in @xcite , it is discussed that in the case of time delay estimation ( @xmath258 ) , it is possible to achieve an mse of the exponential order of @xmath259 by allowing the pulse @xmath260 to have bandwidth that grows exponentially with @xmath261 .",
    "( for some given @xmath262 ) , as well as chaotic signals parametrized by their initial condition ",
    "see @xcite , @xcite and references therein .",
    "] thus , by improving the lower bound @xmath263 ( a special case of the above with @xmath216 ) to @xmath264 $ ] , we are halving the gap between the exponential rates of the upper bound and the lower bound , from @xmath265 to @xmath266 .",
    "our asymptotic lower bound should be compared to other lower bounds available in the literature .",
    "one natural candidate would be the weiss ",
    "weinstein bound ( wwb ) @xcite , @xcite , @xcite , which for the model under discussion at high snr , reads @xcite : @xmath267e/(2n_0)\\}}{2(1-\\exp\\{-[1-r(2h)]e/(2n_0)\\})},\\ ] ] where @xmath268 is assumed to depend only on @xmath269 and not on @xmath156 . while this is an excellent bound for a given modulation scheme @xmath270 , it does not seem to lend itself easily to the derivation of universal lower bounds , as discussed above . to this end , in principle , the wwb should be minimized over all feasible correlation functions @xmath271 , which is not a trivial task .",
    "a reasonable compromise is to first minimize the wwb over @xmath271 for a given @xmath269 , and then to maximize the resulting expression over @xmath269 ( i.e. , max ",
    "min instead of min  max ) .",
    "since the expression of the bound is a monotonically increasing function of both @xmath272 and @xmath273 , and since both @xmath272 and @xmath273 can not be smaller than @xmath212 , we end up with @xmath274 as a modulation  independent bound .",
    "this is a faster exponential decay rate ( and hence weaker asymptotically ) than that of our proposed bound for @xmath205 .",
    "it is possible , however , to obtain a universal lower bound stronger than both bounds by a simple channel  coding argument , which is in the spirit of the ziv  zakai bound @xcite .",
    "this bound is given by ( see appendix d for the derivation ) : @xmath275 where @xmath276 and where @xmath277 is a free parameter , an even integer not smaller than @xmath278 , which is subjected to optimization . throughout the sequel , we refer to this bound as the _ channel  coding bound_. in the high snr regime , the exponential order of the channel  coding bound ( for fixed @xmath277 ) is @xmath279 which for large enough @xmath277 becomes arbitrarily close to @xmath280 , and hence better than the data  processing bound of @xmath255 . note that the ziv  zakai bound @xcite would be weaker in this context of universal lower bounds , since it is based on binary hypothesis testing ( @xmath281 ) , yielding an exponent of @xmath282 .    in view of this comparison , it is natural to ask then what is benefit of our data processing lower bound .",
    "the answer is that the potential of the data  processing bound is much better exploited in situations of channel uncertainty , like in channels with fading .",
    "this is the subject of the next subsection .",
    "it turns out that the feature that makes the data  processing  theorem approach to error lower bounds more powerful , relatively to other approaches , is the convexity property of the generalized mutual information ( in this case , @xmath119 ) w.r.t .",
    "the channel @xmath283 .",
    "suppose that the channel actually depends on an additional random parameter @xmath284 ( independent of @xmath125 ) , that is known to neither the transmitter nor the receiver , namely , @xmath285 where @xmath286 is the density of @xmath284 .",
    "if we think of @xmath119 as a functional of @xmath283 , denoted @xmath287 , then it is a convex functional , namely , @xmath288 this is a desirable property because the r.h.s .",
    "reflects a situation where @xmath284 is known to both parties , whereas the l.h.s",
    ".  pertains to the situation where @xmath284 is unknown , so the lower bound associated with the case where @xmath284 is unknown is always tighter than the expectation of the lower bound pertaining to a known @xmath284 .",
    "the wwb , on the other hand , does not have this convexity property , as we shall see .",
    "consider now the case where @xmath284 is a fading parameter , drawn only once and kept fixed throughout the entire observation time @xmath261 .",
    "more precisely , our model is the same as before except that now the signal is subjected to fading according to @xmath289 where @xmath290 and @xmath156 are realizations of the random variables @xmath284 and @xmath125 , respectively . for the sake of convenience in the analysis",
    ", we assume that @xmath284 is a zero  mean gaussian random variable with variance @xmath291 ( other densities are , of course , possible too ) .",
    "the channel  coding bound is based on a universal lower bound on the probability of error , which holds for every signal set .",
    "the problem is that under fading , we are not aware of such a universal lower bound .",
    "the only remaining alternative then is to use a lower bound corresponding to the case where @xmath284 is known to the receiver , and then to take the expectation w.r.t.@xmath284 , although one might argue that this comparison is not quite fair .",
    "nonetheless , the derivation of this appears in appendix e and the result is @xmath334 thus , the data processing bound is better by a factor of 22.4 ( 13.5db ) .",
    "yet another comparison , perhaps more fair , can be made with a related bound , which based on binary hypothesis testing , but has the advantage of avoiding the use of the chebychev inequality , that was used in the channel  coding bound .",
    "this is the chazan ",
    "ziv bound ( czzb ) , an improved version of the ziv  zakai bound @xcite . according to the czzb , applied to our problem ( see appendix f for the derivation ) , @xmath335 which is again significantly smaller than our bound .",
    "thus , we observe that while the wwb and the czzb are excellent bounds for ordinary channels without fading , when it comes to channels with fading , the proposed data  processing bound has an advantage .",
    "in this work , we have explored a certain class of information measures @xcite , which although being a special case of the zakai  ziv information measures @xcite , it has an interesting structure that calls for attention .",
    "we first put this class of information measures in the broader perspective , relating it to other information measures , like those of @xcite , and then , by a specific choice of the convex functions , we defined a generalized notion of the chernoff divergence that is based on an arbitrary number of replicas of the channel .",
    "relations have be drawn between the generalized chernoff divergence and the gallager function , the ordinary chernoff divergence , and even more specifically , the bhattacharyya distance . we have also suggested a somewhat more general structured class based on factor trees .",
    "we then applied the data processing inequality , based on the generalized chernoff divergence , and demonstrated that sometimes bounds can be improved by using more than @xmath336 replicas .",
    "in particular , for the awgn three replicas is the optimum number in the awgn model , thus improving on @xcite , where only two replicas were used ( the ordinary bhattacharyya distance ) .",
    "while this bound still falls short compared to other bounds available from estimation theory , the data processing bound seems to be more powerful than others when it comes to channels with uncertainty , like fading channels . in this case , the limit of @xmath337 gives the best result .",
    "interesting discussions with shlomo shamai are acknowledged with thanks .",
    "low resolution analysis corresponds to very small values of @xmath191 , which can be handled by a first order taylor series expansion of the functions @xmath199 , @xmath198 and @xmath200 .",
    "specifically , @xmath338 thus , @xmath339 and @xmath340^{k+1}\\approx 1-\\left(\\frac{k+1}{12k}\\right)^2\\cdot s^2\\ ] ] or @xmath341 and so @xmath342",
    "* high resolution analysis *    high resolution corresponds to @xmath343 . in this case",
    ", we have @xmath344 now , according to the relations between the functions @xmath1 , @xmath345 and @xmath38 , derived in subsection 4.1 , we have : @xmath346 and also @xmath347 comparing the two expressions of @xmath200 , we get @xmath348 which leads to the equation @xmath349 at this stage , we have to handle separately the cases @xmath216 , @xmath205 and @xmath350 .",
    "let us consider the case @xmath216 first .",
    "in this case , the last equation reads @xmath351 and so , @xmath352 thus , from the distortion equation , @xmath353 or equivalently , @xmath354 .",
    "now , @xmath355 from the rate equation , we have @xmath356 ^ 2\\nonumber\\\\ & \\approx & \\frac{\\sqrt{s}}{c_1}\\cdot \\frac{4c_1 ^ 2}{s}\\nonumber\\\\ & = & \\frac{4c_1}{\\sqrt{s}}=4c_1\\sqrt{d_s},\\end{aligned}\\ ] ] which means @xmath357 or equivalently , the distortion  rate function is @xmath358 where it should be kept in mind that @xmath208 takes on values in the range @xmath209 $ ] in this case .    the case @xmath205 is handled as follows : @xmath359 and so , for @xmath191 large @xmath360 . by comparing the two expressions for @xmath200 , we find that @xmath361 .",
    "consequently , @xmath362 thus , @xmath363 and @xmath364 in the high  resolution limit , the logarithmic terms are relatively negligible and so , we can deduce that @xmath365}= \\lim_{d\\to 0}\\frac{\\log d}{\\log [ -r(d)]}=1.\\ ] ] finally , we examine the case @xmath217 . returning to eq .",
    "( [ feq ] ) , now we have : @xmath366 and so @xmath367 and @xmath368 the distortion equation then gives @xmath369 and the rate equation yields @xmath356^{k+1}\\nonumber\\\\ & \\approx&\\frac{\\sqrt{s}}{c_k}\\cdot\\left [ \\frac{k4^{1/k}}{(k-2)s^{1/k}}\\right]^{k+1}\\nonumber\\\\ & = & \\frac{4^{1 + 1/k}}{c_ks^{1/2 + 1/k}}\\cdot\\left(\\frac{k}{k-2}\\right)^{k+1}\\nonumber\\\\ & = & 4\\left(\\frac{k}{k-2}\\right)^k\\cdot d_s,\\end{aligned}\\ ] ] thus , the rate  distortion function and the distortion  rate function are approximated as @xmath370",
    "* higher order moments *    the high  resolution analysis can easily be extended to handle general moments of the estimation error , @xmath371 , @xmath372 ( @xmath373 should not necessarily be integer ) .",
    "this gives for large @xmath191 , @xmath374^{1 + 1/k}}\\ ] ] and @xmath375 here , we have to handle separately the cases @xmath376 and @xmath377 ( and the case @xmath378 will not be covered here , but since @xmath373 is allowed to be non ",
    "integer , it can be approached by either @xmath379 or @xmath380 ) . in the case",
    "@xmath376 , we have @xmath381 and so @xmath382 thus , @xmath383 now , @xmath384 and so @xmath340^{k+1}\\approx c^k\\left(\\frac{p}{p - k}\\right)^{k+1}\\cdot\\frac{1}{s^{k / p}}.\\ ] ] thus , @xmath385^{p / k}.\\ ] ] where @xmath386 note that in terms of the asymptotic behavior for small values of @xmath387 , the best choice of @xmath57 is the largest integer strictly less than @xmath373 . for @xmath373 integer ,",
    "this means @xmath388 . as for the case @xmath377",
    ", we get : @xmath389 or @xmath390 so @xmath391 here , @xmath392 then , @xmath340^{k+1}\\approx 2^p\\left(\\frac{k}{k - p}\\right)^kd_s,\\ ] ] and we get @xmath393 where @xmath394",
    "* derivation of the channel  coding bound *    for a given positive integer @xmath277 , consider the following chain of inequalities : @xmath395 now , note that the integrand of the last expression has a simple interpretation : consider the codebook of signals @xmath396 , @xmath397 , @xmath398 where @xmath399 , and consider the ( suboptimum ) decoder that first estimates @xmath125 by an arbitrary estimator @xmath400 and then decodes the message according to the @xmath401 that is nearest to @xmath400 .",
    "the integrand in the last line above is simply the probability of error of that decoder .",
    "this probability of error is lower bounded ( * ? ? ?",
    "* , eqs .",
    "( 3.73 ) and ( 3.75 ) ) according to @xmath402 where now @xmath403 should be an integer at least as large as @xmath404 , namely , @xmath405 , thus , @xmath406",
    "* channel  coding bound for the awgn fading channel *    for a given value of the fading parameter @xmath407 , the earlier derivation of the channel  coding bound implies @xmath408 averaging over @xmath284 and using craig s formula ( see , e.g. , @xcite ) , we have @xmath409}}\\nonumber\\\\ & = & \\frac{1}{8\\pi m^2}\\int_0^{\\pi}\\frac{\\mbox{d}\\theta\\sin\\theta}{\\sqrt{\\sin^2\\theta+e\\sigma^2m/[n_0(m-2)]}}\\nonumber\\\\ & > & \\frac{1}{8\\pi m^2}\\int_0^{\\pi}\\frac{\\mbox{d}\\theta\\sin\\theta}{\\sqrt{1+\\sigma^2em/[n_0(m-2)]}}\\nonumber\\\\ & = & \\frac{1}{8\\pi m^2\\sqrt{1+\\sigma^2em/[n_0(m-2)]}}.\\end{aligned}\\ ] ] for @xmath257 large , this is approximately , @xmath410 which is maximized ( for even @xmath411 ) by @xmath412 to yield @xmath334",
    "* derivation of the chazan  zakai ",
    "ziv bound *    the czzb @xcite asserts that @xmath413 where @xmath414 is the probability of error associated with optimum hypothesis testing between the hypotheses @xmath415 and @xmath416 , assuming equal priors .",
    "let us denote the probabilities of error of the two kinds by @xmath417 and @xmath418 . then , according to the shannon ",
    "gallager  berlekamp theorem ( * ? ? ?",
    "* , theorem 3.5.1 ) , for every @xmath419 $ ] , at least one of the two following inequalities must hold : @xmath420{\\stackrel{\\delta } { = } } a(s)\\\\ p_e(u+h\\to u)&>&\\frac{1}{4}\\exp[\\mu(s , h)+(1-s)\\mu'(s , h)-(1-s)\\sqrt{2\\mu''(s , h)}]{\\stackrel{\\delta } { = } } b(s)\\end{aligned}\\ ] ] where @xmath421 and @xmath422 denote the first two partial derivatives of @xmath423 w.r.t  @xmath191 , and where for rapidly  vanishing  correlation signals , @xmath423 is given by the ( first line of ) eq.([musf ] ) . since @xmath424 , @xmath425 and @xmath426 at the high snr limit , this implies that @xmath427 and so , @xmath428      d.  andelman , _ bounds according to a generalized data processing theorem _ , m.sc .",
    "final paper ( in hebrew ) , department of electrical engineering , technion ",
    "israel institute of technology , haifa , israel , october 1974 .",
    "i.  gurantz , _ application of a generalized data processing theorem _ , m.sc .",
    "final paper ( in hebrew ) , department of electrical engineering , technion ",
    "israel institute of technology , haifa , israel , august 1974 .",
    "i.  hen , _ the threshold effect in the estimation of chaotic sequences _ , m.sc .",
    "dissertation , department of electrical engineering , technion ",
    "israel institute of technology , haifa , israel , february 2002 .",
    "g.  kaplan and s.  shamai ( shitz ) , `` information rates and error exponents of compound channels with application to antipodal signaling in a fading environment , '' _ ae _ , vol .   47 , no .  4 , pp .  228239 , 1993 ."
  ],
  "abstract_text": [
    "<S> we study data processing inequalities that are derived from a certain class of generalized information measures , where a series of convex functions and multiplicative likelihood ratios are nested alternately . while these information measures can be viewed as a special case of the most general zakai  </S>",
    "<S> ziv generalized information measure , this special nested structure calls for attention and motivates our study . </S>",
    "<S> specifically , a certain choice of the convex functions leads to an information measure that extends the notion of the bhattacharyya distance ( or the chernoff divergence ) : while the ordinary bhattacharyya distance is based on the ( weighted ) geometric mean of two replicas of the channel s conditional distribution , the more general information measure allows an arbitrary number of such replicas . </S>",
    "<S> we apply the data processing inequality induced by this information measure to a detailed study of lower bounds of parameter estimation under additive white gaussian noise ( awgn ) and show that in certain cases , tighter bounds can be obtained by using more than two replicas . while the resulting lower bound may not compete favorably with the best bounds available for the ordinary awgn channel , the advantage of the new lower bound , relative to the other bounds , becomes significant in the presence of channel uncertainty , like unknown fading . this different behavior in the presence of channel uncertainty </S>",
    "<S> is explained by the convexity property of the information measure . </S>",
    "<S> +    * index terms : * data processing inequality , chernoff divergence , bhattacharyya distance , gallager function , parameter estimation , fading .    </S>",
    "<S> department of electrical engineering + technion - israel institute of technology + haifa 32000 , israel + merhav@ee.technion.ac.il + </S>"
  ]
}