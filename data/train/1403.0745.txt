{
  "article_text": [
    "data sets are becoming increasingly large .",
    "machine learning practitioners are confronted with problems where the main computational constraint is the amount of time available .",
    "problems become particularly challenging when the training sets no longer fit into memory . accurately solving the dual problem for svm training with nonlinear kernels",
    "requires a run time which is at least quadratic in the size of the training set @xmath0 , thus training complexity is @xmath1 @xcite .    `",
    "ensemblesvm `  employs a divide - and - conquer strategy by aggregating many svm models , trained on small subsamples of the training set . through subdivision ,",
    "total training time decreases significantly , even though more models need to be trained .",
    "for example , training @xmath2 classifiers on subsamples of size @xmath3 , results in an approximate complexity of @xmath4 .",
    "this reduction in complexity helps in dealing with large data sets and nonlinear kernels .",
    "ensembles of svm models have been used in various applications @xcite .",
    "@xcite use ensembles for large scale learning and employ a neural network to aggregate base models .",
    "@xcite provide an implementation which allows base models to use different kernels . for efficiency reasons , we require base models to share a single kernel function .    while other implementations mainly focus on improving predictive performance , our framework primarily aims to ( i ) make nonlinear large - scale learning feasible through complexity reductions and ( ii ) enable fast prototyping of novel ensemble algorithms .",
    "the ` ensemblesvm `  software is freely available online under a lgpl license . `",
    "ensemblesvm `  provides ensembles of instance - weighted svms , as defined in equation  .",
    "the default approach we offer is bagging , which is commonly used to improve the performance of unstable classifiers @xcite . in bagging ,",
    "base models are trained on bootstrap subsamples of the training set and their predictions are aggregated through majority voting .",
    "base model flexibility is maximized by using instance - weighted binary support vector machine classifiers , as defined in equation  .",
    "this formulation lets users define misclassification penalties per training instance @xmath5 and encompasses popular approaches such as c - svc and class - weighted svm @xcite .",
    "@xmath6    when aggregating svm models , the base models often share support vectors ( svs ) .",
    "the ` ensemblesvm `  software intelligently caches distinct svs to ensure that they are only stored and used for kernel evaluations once . as a result , ` ensemblesvm `  models are smaller and faster in prediction than ensemble implementations based on wrappers .      `",
    "ensemblesvm `  has been implemented in ` c++ ` and makes heavy use of the standard library .",
    "the main implementation focus is training speed .",
    "we use facilities provided by the ` c++11 ` standard and thus require a moderately recent compiler , such as ` gcc ` @xmath7 or ` clang ` @xmath8 .",
    "a portable makefile system based on gnu autotools is used to build ` ensemblesvm ` .    `",
    "ensemblesvm `  interfaces with ` libsvm `  to train base models @xcite .",
    "our code must be linked to ` libsvm `  but does not depend on a specific version .",
    "this allows users to choose the desired version of the ` libsvm `  software in the back - end .",
    "the ` ensemblesvm `  programming framework is designed to facilitate prototyping of ensemble algorithms using svm base models .",
    "we particularly provide extensive support to define novel aggregation schemes , should the available options be insufficient .",
    "key components are extensively documented and on a broad overview is provided on our wiki .",
    "the ` ensemblesvm `  library was built with extensibility and user contributions in mind .",
    "major api functions are well documented to lower the threshold for external development .",
    "the executable tools provided with ` ensemblesvm `  are essentially wrappers for the library itself .",
    "the tools can be considered as use cases of the main api functions to help developers .",
    "the main tools in this package are esvm - train and esvm - predict , used to train and predict with ensemble models .",
    "both of these are pthread - parallelized .",
    "additionally , the merge - models tool can be used to merge standard ` libsvm `  models into ensembles .",
    "finally , esvm - edit provides facilities to modify the aggregation scheme used by an ensemble .    `",
    "ensemblesvm `  includes a variety of extra tools to facilitate basic operations such as stratified bootstrap sampling , cross - validation , replacing categorical features by dummy variables , splitting data sets and sparsifying standard data sets .",
    "we recommend retaining the original ratio of positives and negatives in the training set when subsampling .",
    "to illustrate the potential of our software , ` ensemblesvm `  @xmath9 has been benchmarked with respect to ` libsvm`@xmath10 .",
    "to keep the experiments simple , we use majority voting to aggregate predictions , even though more sophisticated methods are offered . for reference , we also list the best obtained accuracy with a linear model , trained using ` liblinear ` @xcite .",
    "linear methods are common in large - scale learning due to their speed , but may result in significantly decreased accuracy .",
    "this is why scalable nonlinear methods are desirable .",
    "we used two binary classification problems , namely the covtype and ijcnn1 data sets .",
    "both data sets are balanced .",
    "features were always scaled to @xmath11 $ ] .",
    "we have used c - svc as svm and base models ( @xmath12 ) .",
    "reported numbers are averages of @xmath13 test runs to ensure reproducibility .",
    "we used the rbf kernel , defined by the kernel function @xmath14 .",
    "optimal parameter selection was done through cross - validation .",
    "the ` covtype ` data set is a common classification benchmark featuring @xmath15 dimensions @xcite .",
    "we randomly sampled balanced training and test sets of @xmath16 and @xmath17 instances respectively and classified class @xmath18 versus all others .",
    "the ` ijcnn1 ` data set was used in a machine learning challenge during ijcnn 2001 @xcite .",
    "it contains @xmath19 training instances in @xmath20 dimensions .",
    ".summary of benchmark results per data set : test set accuracy , number of support vectors and training time .",
    "accuracies are listed for a single ` libsvm `  model , ` liblinear ` model and an ensemble model . [",
    "cols=\"<,^,^,^,^,^,^,^,^ \" , ]     results in table  [ resultstable ] show several interesting trends .",
    "training ` ensemblesvm `  models is orders of magnitude faster , because training svms on small subsets significantly reduces complexity .",
    "subsampling induces smaller kernels per base model resulting in lower overall memory use . due to our parallelized implementation ,",
    "ensemble models were faster in prediction than ` libsvm ` models in both experiments despite having twice as many svs .",
    "the ensembles in these experiments are competitive with a traditional svm even though we used simple majority voting . for ` covtype ` , ensemble accuracy is @xmath21 lower than a single svm and for ` ijcnn1 ` the ensemble is marginally better ( @xmath22 ) .",
    "linear svm falls far short in terms of accuracy for both experiments , but is trained much faster ( @xmath23 seconds ) .",
    "we obtained good results with very basic aggregation .",
    "@xcite illustrated that more sophisticated aggregation methods can improve the predictive performance of ensembles .",
    "others have reported performance improvements over standard svm for ensembles using majority voting @xcite .",
    "` ensemblesvm `  provides users with efficient tools to experiment with ensembles of svms .",
    "experimental results show that training ensemble models is significantly faster than training standard ` libsvm `  models while maintaining competitive predictive accuracy .",
    "linear methods are frequently applied in large - scale learning , mainly due to their low training complexity .",
    "linear methods are known to have competitive accuracy for high dimensional problems .",
    "as our benchmarks showed , the difference in accuracy may be large for low dimensional problems .",
    "as such , fast nonlinear methods remain desirable in large - scale learning , particularly for low dimensional tasks with many training instances .",
    "our benchmarks illustrate the potential of the ensemble approaches offered by ` ensemblesvm ` .",
    "ensemble performance may be improved by using more complex aggregation schemes . `",
    "ensemblesvm `  currently offers various aggregation schemes , both linear and nonlinear .",
    "additionally , it facilitates fast prototyping of novel methods . `",
    "ensemblesvm `  strives to provide high - quality , user - friendly tools and an intuitive programming framework for ensemble learning with svm base models .",
    "the software will be kept up to date by incorporating promising new methods and ideas when they are presented in the literature .",
    "user requests and suggestions are welcome and appreciated .            chih - chung chang and chih - jen lin . : a library for support vector machines . _",
    "acm transactions on intelligent systems and technology _ , 2:0 27:127:27 , 2011 .",
    "software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm .",
    "giorgio valentini and thomas  g. dietterich .",
    "low bias bagged support vector machines . in _ in accepted for publication , international conference on machine learning , icml-2003 _ , pages 752759 .",
    "morgan kaufmann , 2003 .",
    "wang , avin mathew , yan chen , li - feng xi , lin ma , and jay lee .",
    "empirical analysis of support vector machine ensemble classifiers .",
    "_ expert systems with applications _ , 360 ( 3 , part 2):0 6466  6476 , 2009"
  ],
  "abstract_text": [
    "<S> ` ensemblesvm `  is a free software package containing efficient routines to perform ensemble learning with support vector machine ( svm ) base models . </S>",
    "<S> it currently offers ensemble methods based on binary svm models . </S>",
    "<S> our implementation avoids duplicate storage and evaluation of support vectors which are shared between constituent models . </S>",
    "<S> experimental results show that using ensemble approaches can drastically reduce training complexity while maintaining high predictive accuracy . </S>",
    "<S> the ` ensemblesvm `  software package is freely available online at ` http://esat.kuleuven.be/stadius/ensemblesvm ` .    </S>",
    "<S> classification , ensemble  learning , support  vector  machine , bagging </S>"
  ]
}