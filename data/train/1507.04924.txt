{
  "article_text": [
    "side information channel has been actively studied since its initiation by shannon @xcite .",
    "coding for computer memories with defective cells was studied by kusnetsov - tsybakov @xcite .",
    "gelfand - pinsker ( gp ) @xcite determined the capacity of channels with channel side information ( csi ) known non - causally at the transmitter .",
    "heegard - el gamal @xcite obtained the capacity when the csi is known only at the receiver .",
    "cover - chiang @xcite extended these results to a general case where correlated two - sided state information are available at the transmitter and at the receiver .",
    "costa @xcite obtained an interesting result by carefully investigating the gp theorem for the gaussian channel , i.e. , he proved that the capacity of the gaussian channel with an interference known at the transmitter is the same as the capacity of interference free channels .",
    "there are many other important researches in the literature , e.g.@xcite .",
    "the results for the single user channel have been generalized possibly to multi user channels , at least in special cases @xcite . +      in this paper , we focus on the gaussian channel in presence of side information for two major aims : first , analyzing the problem of capacity of the gaussian channel in presence of two sided state information -the gaussian version of cover - chiang theorem @xcite , mathematically and information - theoretically .",
    "second we try to present an information - theoretical description of the concept of  cognition \" of the transmitter and or receiver in an improved manner .",
    "+      in this paper , we try to analyze the gaussian version of the cover - chaing unifying theorem @xcite .",
    "the problem of the effect of side information at the transmitter in a gaussian channel , in a special case , first , has been studied in costa s `` writing on dirty paper '' @xcite .",
    "let us consider a gaussian channel with side information known non - causally at the transmitter as depicted in fig .",
    "[ figure1 ] .",
    "we denote the side information at the transmitter , the channel input , the channel output , the channel noise and the auxiliary random variable at the transmitter by @xmath0 , @xmath1 , @xmath2 , @xmath3 and @xmath4 , respectively .",
    "moreover , it is assumed that @xmath0 and @xmath3 are gaussian random variables with powers @xmath5 and @xmath6 respectively and @xmath7 has the power constraint @xmath8 .",
    "costa @xcite shows that the capacity of this channel is surprisingly the same as the capacity of the channel without side information .",
    "an important assumption in costa theorem is that in the definition of the channel , there is no restriction for the correlation between @xmath7 and @xmath0 .",
    "however , costa shows that the maximum rate is obtained when @xmath7 and @xmath0 are independent and @xmath9 is a linear function of @xmath7 and @xmath0 .",
    "hence , his theorem is only applicable to cases where @xmath7 and @xmath0 have the chance to be uncorrelated .",
    "therefore a theorem which can handle the capacity of gaussian channels when there exists a specific correlation between @xmath7 and @xmath0 is theoretically and practically important .",
    "one example for correlated input and side information is cognitive interference channels in which the transmitted sequence of one transmitter is a known interference for the other transmitter and these two sequences may be dependent to each other .",
    "another example is a measurement system where the measuring signal may affect the system under measurement .",
    "this is equivalent to an interfering signal which is dependent on the original measuring signal .",
    "another related question is about the side information @xmath10 known non - causally at the receiver ( if exists as in fig .",
    "[ figure2 ] ) .",
    "the question now arises is that : how does the receiver knowledge @xmath10 , correlated to @xmath11 affect the channel capacity ? and how much does the receiver information about @xmath7 and @xmath0 , available through @xmath10 , change the channel capacity ?    some communication scenarios in which the channel input and the side information may be correlated and the related investigations can be found in @xcite and @xcite . in @xcite",
    "the problem of optimum transmission rate under the requirement of minimum mutual information @xmath12 is investigated .",
    "moreover both @xcite and @xcite study costa s  writing on dirty paper \" problem where the side information is correlated to the input of the channel ( our motivation ) , when only side information known at the transmitter exists .",
    "we , in another work , have considered and solved the problem of the capacity of gaussian channel with two - sided state information in a limited case @xcite .    moreover , examining the gaussian channel with two - sided state information with dependency on the channel noise and channel input , we try to solve the gaussian version of cover - chiang theorem @xcite as an open problem . +          one of the most known and important applications of the channels with side information is information theoretically describing the concept of  cognition \" of the transmitter in communication scenarios .",
    "side information in this description , for example , may be the interference which transmitter exactly knows all about it .",
    "two questions arise about this description :    \\1 ) it is usually expected the knowledge about or cognition on something to be  quantitative \" . for example the cognition that the transmitter can acquire about the interference may be incomplete or partial .",
    "so one question is : how can we describe the  quantity \" or  amount \" of the transmitter cognition ?",
    "the investigations of the channels with partial csi try to answer this question , for example @xcite .",
    "\\2 ) it is possible in a communication scenario that the transmitter has knowledge about more than one variable in the channel .",
    "for example in a cognitive interference channel the transmitter may have knowledge about the interference originated by the other transmitter and at the same time about the channel noise .",
    "hence , the other question is : how can we describe the  cognition that the transmitter has on some variables \" .    in this paper , we propose describing the concept of the  transmitter and or receiver cognition on some variables \" by side information available at the transmitter and or receiver _ probabilistically dependent _ on those variables .",
    "hence , the side information known at the transmitter correlated to the variable @xmath13 , describes the transmitter cognition on @xmath13 and the amount of this cognition increases as the correlation between the side information and the variable @xmath13 increases . distinguishing between this meaning of  cognition \" from the usual meaning widely used in the literature , it may be proper to use the word  * re - cognition * ( of the transmitter or receiver on something ) \" for it .    hence in a gaussian channel in presence of two - sided state information depicted in fig .",
    "[ figure2 ] , @xmath0 which is the side information known at the transmitter can be interpreted as the transmitter re - cognition on the channel noise , if @xmath0 is correlated with @xmath3 .",
    "it is seen that our first motivation , not only can be seen as an effort to solve an important open problem , but also , if solved , it can exemplify this new description . +      to provide the above motivations , we define a gaussian channel in presence of two - sided state information where the channel input @xmath7 , side information @xmath14 and the channel noise @xmath3 are arbitrarily correlated . using the extended version of cover - chiang unifying theorem @xcite to continuous alphabets , we prove a general achievable rate for the channel ( lemma 1 ) .",
    "then , we obtain a general upper bound for the channel in the case that the channel input @xmath7 , the side information @xmath15 and the channel noise @xmath3 , form the markov chain @xmath16 ( lemma 2 ) and we show the coincidence of the lower and upper bounds under this circumstance and therefore establish our capacity theorem for the channel . using our probabilistic description of  re - cognition \" of the transmitter , this circumstance can be explained as follows : if the whole  re - cognition \" that the transmitter has got on the channel noise , is gained from the side information @xmath17 -that is a meaningful and practically acceptable circumstance in our communication scenario- then the markov chain @xmath16 must be satisfied .",
    "the obtained channel capacity can be expressed as an increasing function of the mutual information between the side information @xmath18 and the channel noise @xmath3 ( i.e. @xmath19 ) and this shows that our new description of  re - cognition \" of the transmitter and the receiver can be exemplified by our channel and its capacity .",
    "this paper is organized as follows : in section ii , we briefly review the cover - chiang and the gelfand - pinsker theorems and then introduce a scrutiny of the costa theorem . in section iii , we define our gaussian channel thoroughly and prove a general lower bound for the defined channel and then obtain a general upper bound for the channel in mentioned case , which coincides with the lower bound and hence is the capacity of the channel . in section iv ,",
    "we examine the proved capacity in special cases and interpret them .",
    "specifically , we explain that how this capacity theorem can exemplify the new description of the  re - cognition \" of transmitter and or receiver on something .",
    "section vi contains the conclusion .",
    "the proofs of lower and upper bounds of the capacity of channel and two lemmas used in our proofs are given in the appendix .",
    "to clarify our approach in subsequent sections , in this section we first briefly review the cover - chiang capacity theorem for channels with side information available at the transmitter and at the receiver .",
    "we then review the gelfand - pinsker ( gp ) theorem which is a special case of cover - chiang theorem when side information is known only at the transmitter .",
    "finally costa theorem ( writing on dirty paper \" theorem ) , which is the gaussian version of the gp theorem , is deeply investigated .",
    "[ figure3 ] shows a channel with side information known at the transmitter and at the receiver where @xmath20 and @xmath21 are the transmitted and the received sequences respectively .",
    "the sequences @xmath22 and @xmath23 are the side information known non - causally at the transmitter and at the receiver respectively .",
    "the transition probability of the channel , @xmath24 , depends on the input @xmath1 , the side information @xmath25 and @xmath26 .",
    "it can be shown that if the channel is memoryless and the sequences @xmath27 is independent and identically distributed ( i.i.d . )",
    "random variables under @xmath28 , then the capacity of the channel is @xcite : @xmath29 \\label{eq.1}\\ ] ] where the maximum is over all distributions : @xmath30 and @xmath4 is an auxiliary random variable .",
    "it is important to note that the markov chains : @xmath31 @xmath32 are satisfied for all distributions in ( [ eq.2 ] ) .",
    "this theorem is special case of cover - chiang theorem when @xmath33 . according to gp theorem @xcite :    a memoryless channel with transition probability @xmath34 and side information sequence @xmath22 i.i.d . with @xmath35 known non - causally at the transmitter depicted in fig .",
    "[ figure4 ] has the capacity @xmath36 \\label{eq.4}\\ ] ] for all distributions : @xmath37 where @xmath4 is an auxiliary random variable .",
    "costa @xcite examined the gaussian version of the channel with side information known at the transmitter ( fig .",
    "[ figure1 ] ) .",
    "as can be seen , the side information is considered as an additive interference at the receiver .",
    "costa showed that the channel , surprisingly , has the capacity @xmath38 , which is the the same for channels with no interference @xmath25 .",
    "costa derived this capacity by using the results of gelfand - pinsker theorem extended to random variables with continuous alphabets . in this subsection , we first introduce the costa assumptions and then present a proof for this theorem in such a way that it enables us to introduce our channel and develop our theorem in subsequent sections .",
    "+ the channel is specified with properties c.1-c.3 below :    [ [ c.1 ] ] * c.1 * + + + + +    @xmath22 is a sequence of gaussian i.i.d .",
    "random variables with distribution @xmath39 .",
    "[ [ c.2 ] ] * c.2 * + + + + +    the transmitted sequence @xmath20 is assumed to have the power constraint @xmath8 .",
    "[ [ c.3 ] ] * c.3 * + + + + +    the output is given by @xmath40 , where @xmath41 is the sequence of white gaussian noise with zero mean and power @xmath42 i.e. @xmath43 and independent of @xmath44 .",
    "the sequence @xmath45 is non - causally known at the transmitter .",
    "it is readily seen that the distributions @xmath46 having the above three properties are in the form of ( [ eq.5 ] ) .",
    "we denote the set of all these @xmath46 s with @xmath47 .",
    "although for the costa channel described above , no restriction has been imposed on the correlation between @xmath7 and @xmath0 , in costa theorem , the maximum rate corresponds to independent @xmath7 and @xmath0 , and @xmath9 in form of linear combination of @xmath7 and @xmath0 .",
    "we define @xmath48 as a subset of @xmath47 with elements @xmath49 having the following properties as well as properties c.1-c.3 mentioned before :    [ [ c.4 ] ] c.4 + + +    @xmath7 is a zero mean gaussian random variable with the maximum average power @xmath50 and _ independent _ of @xmath51 .",
    "[ [ c.5 ] ] c.5 + + +    the auxiliary random variable @xmath9 takes the linear form @xmath52 .",
    "+ it is clear that the set @xmath48 ( described in c.1-c.5 ) and their marginal and conditional distributions are subsets of corresponding @xmath47 s ( described in c.1-c.3 ) .",
    "+ _ achievable rate for costa channel _ : from ( [ eq.4 ] ) , when extended to memoryless channels with discrete time and continuous alphabets , we can obtain an achievable rate for the channel .",
    "the capacity of costa channel can be written as : @xmath53\\label{eq.6}\\ ] ] where the maximum is over all @xmath54 s in @xmath47 . since @xmath55 we have : @xmath56\\label{eq.7}\\\\ & { } = { } & \\max_{p'\\left ( u \\mid x , s_{1}\\right ) p'\\left ( x \\mid s_{1}\\right )   } \\left[i\\left(u;y \\right ) -i\\left(u;s_{1 } \\right )   \\right]\\label{eq.8}\\\\ & { } = { } & \\max_\\alpha\\left[i\\left(u;y \\right ) -i\\left(u;s_{1 }",
    "\\right )   \\right]\\label{eq.9}\\end{aligned}\\ ] ] the expression in the last bracket is calculated for distributions @xmath57 in @xmath48 described in c.1-c.5 .",
    "thus , defining @xmath58 , @xmath59 is an achievable rate for the channel . @xmath60 and",
    "@xmath61 is calculated as : @xmath62 and @xmath63 where @xmath64",
    "both @xmath65 and @xmath66 are independent of @xmath67 and then of @xmath68",
    ".    _ converse part of costa theorem : _ from ( [ eq.4 ] ) we can also obtain an upper bound for the channel capacity .",
    "we have : @xmath69 where inequality ( [ eq.14 ] ) follows from the fact that conditioning reduces the entropy and ( [ eq.16 ] ) follows from markov chain @xmath70 which is correct for all distributions @xmath46 in the form of ( [ eq.5 ] ) , including the distributions in the set @xmath47 .",
    "hence we can write : @xmath71\\label{eq.17}\\\\ & { } \\leq{}&\\max_{p\\left ( x \\mid s_{1}\\right)}\\left[i\\left ( x;y\\mid s_{1 } \\right )   \\right]\\label{eq.18}\\\\   & { } = { } & \\max_{p\\left(x\\mid s_{1}\\right)}\\left[h\\left(y\\mid s_{1}\\right ) -h\\left(y\\mid x , s_{1 } \\right )   \\right]\\label{eq.19}\\\\ & { } = { } & \\max_{p\\left(x\\mid s_{1}\\right)}\\left[h\\left(x+z\\mid s_{1}\\right ) -h\\left(z \\mid x , s_{1}\\right )   \\right]\\label{eq.20}\\\\ & { } \\leq{}&\\max_{p\\left(x\\mid s_{1}\\right)}\\left[h\\left(x+z\\right ) -h\\left(z \\right )   \\right]\\label{eq.21}\\\\ & { } = { } & \\frac{1}{2}\\log\\left ( 1+\\frac{p}{n}\\right ) , \\label{eq.22}\\end{aligned}\\ ] ] where the inequality ( [ eq.21 ] ) is due to the fact that conditioning reduces the entropy .",
    "the maximum in ( [ eq.21 ] ) is obtained when @xmath7 and @xmath3 are jointly gaussian with @xmath72 because when the variance is limited , gaussian distribution maximizes the entropy . from ( [ eq.11 ] ) and ( [ eq.22 ] ) it is seen that the lower and the upper bounds of the capacity coincide , and therefore the channel capacity is equal to @xmath73 .",
    "it is also concluded that for the channel described in c.1-c.3 , the optimum condition which leads to the capacity is when @xmath74 and independent of @xmath0 .",
    "we can explain the costa theorem more , as follows : let consider @xmath75 with independent gaussian interference @xmath0 with power @xmath5 , @xmath76 with power @xmath77 and @xmath3 with power @xmath6 .",
    "if the transmitter knows nothing about this interference , then we take @xmath78 and @xmath79 .",
    "if @xmath0 is known at the transmitter , then we take @xmath80 and we have @xmath81 and if @xmath0 and @xmath76 are both known at the transmitter , then @xmath82 and @xmath83 .",
    "in this section we introduce a gaussian channel in the presence of two - sided state information correlated to the channel input and noise .",
    "then we present our capacity theorem for this gaussian channel .",
    "the theorem obtains the capacity of channel in the case the channel input @xmath7 , the side information @xmath14 and the channel noise @xmath3 , form the markov chain @xmath16 . with our new description of the re - cognition \" of the transmitter on the channel noise , the probabilistic dependency between the side information @xmath14 and the channel noise @xmath3 , determines the cognition on the channel noise that the side information carries to the transmitter .",
    "therefore , this markov chain states that the transmitter acquires all its knowledge on the channel noise just from the side information @xmath14 , which is practically meaningful and acceptable in our scenario . to prove the theorem",
    ", we obtain a general achievable rate for the channel capacity ( lemma 1 ) and then a general upper bound for the channel capacity in mentioned case ( lemma 2 ) and show the coincidence of these lower and upper bounds .",
    "+      as mentioned before , in a gaussian channel with side information known at the transmitter defined by the set @xmath47 with properties c.1-c.3 ( costa channel ) , no restriction is imposed upon the correlation between the channel input @xmath7 and the side information @xmath0 .",
    "as mentioned in section i , the capacity @xmath84 is only valid for channels in which @xmath7 and @xmath0 has the chance to be independent .",
    "specifically the maximum rate is achieved when @xmath7 and @xmath0 are independent .",
    "let @xmath47 is partitioned into subsets @xmath85 including the distributions @xmath46 for which the correlation coefficient between @xmath7 and @xmath0 is equal to @xmath86 as depicted in fig .",
    "[ figure7 ] .",
    "it is obvious that @xmath48 ( the set of distributions with properties c.1-c.5 ) is a subset of @xmath87 and therefore the optimum distribution leading to the capacity of the costa channel does not belong to other partitions .",
    "we can therefore claim that the costa theorem is not valid for channels defined with random variables @xmath88 in partition @xmath85 with @xmath89 .",
    "+     into @xmath85 s .",
    "@xmath90 is the optimum distribution for the costa channel.,width=192 ]    consider the gaussian channel depicted in fig .",
    "[ figure2 ] .",
    "the side information at the transmitter @xmath0 and at the receiver @xmath10 is considered as additive interference at the receiver . from the above discussion , providing our mentioned motivations in section i , our channel has three differences with costa s one as follows :    \\1 ) in our channel , a specified correlation coefficient @xmath86 between @xmath7 and @xmath0 , exists .",
    "\\2 ) to investigate the effect of the side information known at the receiver , we suppose that in our channel there exists a gaussian side information @xmath10 known non - causally at the receiver which is correlated to both @xmath7 and @xmath0 .",
    "\\3 ) we allow the channel input @xmath7 and the side information @xmath0 and @xmath10 to be correlated to the channel noise @xmath3 . + * * remark:**it is important to note that , as we prove in lemma 3 in the appendix c , assuming the input random variable @xmath7 correlated to @xmath0 and @xmath10 with specified correlation coefficients , does not impose any restriction on @xmath7 s own distribution and the distribution of @xmath7 is still free to choose .",
    "+ considering the above differences , our channel is defined by the following properties gc.1-gc.4 ( gc for general version of costa ) below :    [ [ gc.1 ] ] * gc.1 * + + + + + +    @xmath91 are i.i.d .",
    "sequences with zero mean and jointly gaussian distributions with power @xmath92 and @xmath93 respectively ( so we have @xmath39 and @xmath94 ) .    [ [ gc.2 ] ] * gc.2 * + + + + + +    the output sequence @xmath95 , where @xmath96 is the sequence of white gaussian noise with zero mean and power @xmath42 @xmath97",
    ". the sequences @xmath45 and @xmath98 are non - causally known at the transmitter and at the receiver respectively .    [ [ gc.3 ] ] * gc.3 * + + + + + +    random variables @xmath99 have the covariance matrix @xmath100 : @xmath101 and therefore , in our channel , the gaussian noise @xmath3 is not necessarily independent of the additive interference @xmath0 and @xmath10 and the input @xmath7 .",
    "moreover @xmath20 is assumed to have the constraint @xmath102 . except @xmath103 ,",
    "all other parameters in @xmath100 have fixed values specified for the channel and must be considered as _",
    "the definition of the channel_.    [ [ gc.4 ] ] * gc.4 * + + + + + +    @xmath104 form the markov chain @xmath105 . as mentioned earlier , this markov chain is satisfied by all distributions @xmath106 in the form of ( [ eq.2 ] ) in cover - chiang capacity theorem and is physically reasonable . since this",
    "markov chain results in the weaker markov chain @xmath107 , as proved in lemma 4 in the appendix d , this property implies that in the covariance matrix @xmath100 in ( [ eq.24 ] ) we have : @xmath108    it is readily seen that all distributions @xmath106 having the properties gc.1-gc.4 are in the form of ( [ eq.2 ] ) .",
    "therefore we can apply the extended version of cover - chiang theorem for random variables with continuous alphabets to our channel .",
    "we denote the set of all these distributions @xmath106 with @xmath85 ( again ) .",
    "+ * * remark:**in the absence of @xmath10 and when @xmath3 is independent of @xmath11 , we can compare the capacity of our channel with the costa channel and write : @xmath109 + where @xmath110 denotes the capacity of our channel when @xmath3 is independent of @xmath111 .",
    "note that in this case and when @xmath112 , we have @xmath113 and therefore , looking for the maximum rate in @xmath47 leads to the maximum rate among @xmath85 s .",
    "+ we will show that the optimum distribution resulting in maximum transmission rate , is obtained when @xmath114 are jointly gaussian and the auxiliary random variable @xmath9 is a linear combination of @xmath7 and @xmath0 .",
    "we denote the set of distributions @xmath115 having properties gc.5 and gc.6 below as well as properties gc.1-gc.4 , with @xmath116 :    [ [ gc.5 ] ] gc.5 + + + +    the random variables @xmath117 are jointly gaussian distributed and @xmath7 has zero mean and the maximum power @xmath118 i.e. @xmath119 . +    [ [ gc.6 ] ] gc.6 + + + +    as in the costa theorem : @xmath120 where @xmath7 and @xmath0 are now correlated .",
    "it is clear that the set @xmath116 ( described in gc.1-gc.6 ) and their marginal and conditional distributions are subsets of corresponding @xmath85 s ( described in gc.1-gc.4 ) .",
    "+ as the final part of this subsection we introduce some definitions required for our capacity theorem :    suppose @xmath121 is the covariance matrix for random variables @xmath122 having all properties gc.1-gc.6 ; defining : @xmath123 we can write @xmath121 , its determinant @xmath124 and its minors as : @xmath125 + @xmath126    @xmath127        the gaussian channel defined by properties gc.1-gc.4 , when the channel input @xmath7 , the side information @xmath14 and the channel noise @xmath3 form the markov chain @xmath16 , has the capacity : + @xmath128 where @xmath129    [ [ proof - of - theorem ] ] proof of theorem + + + + + + + + + + + + + + + +    to prove the theorem , first , we prove a general achievable rate for the channel in lemma 1 . then in lemma 2",
    ", we obtain an upper bound for the channel in the case the transmitter acquires all its knowledge on the channel noise @xmath3 from the side information @xmath130 , i.e , we have the markov chain @xmath16 .",
    "then we show the coincidence of this upper bound with the lower bound of the capacity .",
    "we note that the markov chain @xmath16 and the markov chain @xmath131 from gc4 , imply the weaker markov chain @xmath132 . and",
    "since @xmath0 and @xmath3 are gaussian , as we prove in lemma 4 in the appendix d , the recent markov chain implies that @xmath133      the capacity of the gaussian channel defined with properties gc.1-gc.4 has the lower bound : @xmath134^{2}\\left ( 1-\\rho_{s_{1}s_{2}}^{2}\\right ) } { \\sigma_{z}^{2}\\left ( \\left ( 1-\\rho_{xs_{1}}^{2}\\right ) d_{p}^{\\mathcal{n}}-\\left ( \\rho_{xs_{1}}\\rho_{s_{1}z}-\\rho_{xz}\\right ) ^{2}\\left ( 1-\\rho_{s_{1}s_{2}}^{2}\\right ) \\right ) } \\right ) \\label{eq.45}\\ ] ] where @xmath135 is defined in ( [ eq.96xx ] ) .",
    "[ [ proof ] ] proof + + + + +    appendix a contains the proof .",
    "+      the capacity of the gaussian channel defined by properties gc.1-gc.4 , when the channel input @xmath7 , the side information @xmath14 and the channel noise @xmath3 form the markov chain @xmath16 , has the upper bound @xmath136 in ( [ eq.95 + 1 ] ) .    [ [ proof-1 ] ] proof + + + + +    appendix b contains the proof .",
    "+ for completing the proof of the theorem , it is enough to compute the lower bound of the channel ( [ eq.45 ] ) , when we have the markov chain @xmath16 . applying the equation ( [ eq.300x ] ) to equation ( [ eq.45 ] ) , shows the coincidence of the upper and the lower bounds of the capacity of the channel in this case and the proof is completed .",
    "+ * remark 1 : * it can be shown that for variables @xmath0 , and @xmath10 and @xmath3 with properties gc.1 and gc.4 : @xmath137 and so the channel capacity ( [ eq.95 + 1 ] ) can be written as : @xmath138 that is an increasing function of @xmath139 . + * remark 2 : * the transmission rate @xmath136 in ( [ eq.95 + 1 ] ) can be reached by encoding and decoding schema represented in @xcite modified for continuous gaussian distributions .",
    "in previous section , the capacity of the gaussian channel with two - sided information correlated to the channel input and noise , has been obtained .",
    "the capacity theorem is general except that the markov chain @xmath16 must be satisfied . in this section",
    "we present some corollaries of the capacity theorem .",
    "first , we examine the effect of the correlation between the side information and the channel input on the channel capacity .",
    "second , we try to exemplify our new description of the concept of  cognition \" of a communicating object ( here , transmitter and or receiver ) on some features of channel ( here , channel noise ) , by our capacity theorem .",
    "if we assume that the channel noise @xmath3 is independent of @xmath140 , from ( [ eq.95 + 1 ] ) , the capacity of the channel is : @xmath141    _ corollary 1 : _ from ( [ eq.34 + 1 ] ) , @xmath110 is reduced to the costa capacity by maximizing it with @xmath142 .",
    "_ corollary 2 : _ it is seen that in the case the side information @xmath10 is independent of the channel noise @xmath3 , the capacity of the channel is equal to the capacity when there is no interference @xmath10 . in other words , in this case , the receiver can subtract the known @xmath98 from the received @xmath143 without losing any worthy information .",
    "_ corollary 3 : _ the correlation between @xmath7 and @xmath0 decreases the capacity of the channel .",
    "it can be explained as follows : by looking at @xmath144 in our dirty paper like coding , mitigating the input - dependent interference effect , also mitigates the input power impact on the channel capacity as this fact is seen in ( [ eq.150 ] ) as @xmath145 .    as an extreme and interesting case , when @xmath146 ( then @xmath147 ) , according to the usual gaussian coding , the capacity seems to be @xmath148 , which is the capacity when @xmath149 is transmitted and @xmath150 is received .",
    "but as our theorem shows , the capacity paradoxically is zero .",
    "because the receiver based on his information ought to decode according to the dirty paper like coding . in dp like coding , with given known sequence @xmath151 , we must find an auxiliary sequence @xmath152 like @xmath153 jointly typical with @xmath151 @xcite .",
    "jointly typicality of @xmath154 is equivalent to : @xmath155 where @xmath156 denotes the transpose operation and @xmath157 is computed according to ( [ eq.48 ] ) .",
    "if @xmath158 , there exists no such @xmath153 : since @xmath159 , we have @xmath160 where @xmath161 is the norm of the given known sequence @xmath151 and therefore ( [ eq.151 ] ) can not be true .",
    "in other words , in this case , encoding error occurs .",
    "[ capacity1 ] shows the variation of the capacity @xmath110 with respect to @xmath86 when @xmath162 .",
    "it is seen that when the correlation between the channel input and the side information known at the transmitter increases , the channel capacity decreases .",
    "the maximum capacity is gained when @xmath142 , that is costa s capacity .",
    "[ capacity2 ] shows the capacity @xmath110 with respect to @xmath163 for five values of @xmath86 .",
    "when @xmath112 and @xmath164.,width=336 ]     when @xmath112.,width=336 ]         cognition `` is an indispensable concept in communication . the assumption that an intelligent communicating object ( transmitter , receiver , relay and so on ) has got _ some _ side knowledge about _ some _ features of the communication channel , is a true and acceptable assumption .",
    "this exceeded information owned , for example , by the transmitter is described by ' ' side information \" known at the transmitter . in usual description ,",
    "the side information is considered as the subject of cognition itself , for example , the interference of another transmitter in a cognitive radio channel @xcite . on the other hand ,",
    "the assumption that the knowledge may be incomplete or imperfect , is necessary in most communication scenarios . describing this incomplete cognition and corresponding information - theoretic concept , i.e.",
    ", partial side information are found in the literature ; for example in @xcite the imperfect known interference is partitioned to one perfect known and one unknown parts ; and in @xcite partial side information is considered as a disturbed version of the subject variable by noise .",
    "we try here to present an alternative description for the concept of  cognition \" in communication by the concept of side information .",
    "the essential property of this description is the _ separation _ of the _ subject _ of knowledge @xmath165 ( for example interference , channel noise , fading coefficients and so on ) from the side information @xmath166 that _ carries _ the knowledge for the intelligent agent ( for example transmitter , receiver , relay and so on ) and known by it .",
    "this point of view is compatible with what happens in reality : we always acquire our knowledge on something indirectly by knowing other things .",
    "what make it possible to extract the knowledge about @xmath165 from @xmath166 is dependency between @xmath166 and @xmath165 .",
    "each method of extraction of knowledge about @xmath165 from @xmath166 ( estimation and so on ) , originally relies on this dependency .",
    "if @xmath166 is independent from @xmath165 then @xmath166 is non - informative about @xmath165 . and",
    "it is expected that increasing the dependency between @xmath166 and @xmath165 , increases the possible knowledge of @xmath166 about @xmath165 .",
    "avoiding confusion between this new with the usual descriptions of the cognition , we use the word *  re - cognition \" * for it and define it as follows :    _ a communicating agent ( transmitter , receiver , relay and so on ) has  re - cognition \" on some variable @xmath165 if the side information @xmath166 known by it , has probabilistic dependency on @xmath165 .",
    "_      in the gaussian channel defined and analyzed in the previous section , the side information @xmath167 is dependent to the channel noise and therefore the transmitter and the receiver have got re - cognition on the channel noise by @xmath0 and @xmath10 respectively .",
    "the capacity is proved with markovity constraint @xmath16 .",
    "considering the new description of re - cognition , this markov chain simply means that the transmitter acquires all its re - cognition on the channel noise via the side information @xmath14 , which is meaningful and acceptable .",
    "_ corollary 4 : _ if @xmath112 , the transmitter have re - cognition on the channel noise @xmath3 obtained by @xmath0 correlated to the noise .",
    "if there is no constraint on correlation between @xmath7 and @xmath0 , @xmath142 maximizes the transmission rate , as mentioned in ( [ eq.34 + 1 ] ) .",
    "therefore , from ( [ eq.95 + 1 ] ) and ( [ eq.95+c ] ) , the capacity in this case is : @xmath168 it is seen that more correlation between @xmath0 and @xmath3 results in more re - cognition of the transmitter on the channel noise and more capacity .",
    "the capacity reaches to infinite when @xmath169 and therefore the transmitter has perfect re - cognition about the channel noise .",
    "[ capacity3 ] illustrates the capacity of the channel with respect to @xmath170 , the correlation coefficient between the side information @xmath0 and the channel noise when @xmath162 .",
    "it is seen that when the correlation increases ( that it means that @xmath0 carries more re - cognition on the channel noise to the transmitter ) , the capacity increases .",
    "[ capacity4 ] shows the capacity of the channel with respect to @xmath163 for five values of @xmath170 .",
    "[ capacity6 ] illustrates the capacity of the channel with respects to mutual information @xmath171 for five values of @xmath163 .     when @xmath162.,width=336 ]     for five values of @xmath170.,width=336 ]     for five values of @xmath163.,width=336 ]    _ corollary 5 : _",
    "if @xmath172 , the receiver have re - cognition on the channel noise @xmath3 obtained by @xmath10 correlated to the noise .",
    "the capacity in this case is : @xmath173 it is seen that more correlation between @xmath10 and @xmath3 results in more re - cognition of the receiver on the channel noise and more capacity .",
    "perfect re - cognition takes place with @xmath174 and results in infinite capacity .",
    "_ corollary 6 : _ if @xmath175 , if there is no constraint on correlation between @xmath7 and @xmath0 , @xmath142 maximizes the transmission rate , as mentioned in ( [ eq.34 + 1 ] ) .",
    "therefore the capacity of the channel is : @xmath176 it is seen that when @xmath177 , the capacity reaches to infinite , even if neither the transmitter nor the receiver has perfect knowledge about the channel noise . in this case",
    "the transmitter and the receiver have their shares in re - cognition on the channel noise which leads to totally mitigating the channel noise .",
    "by fully detailed investigating the gaussian channel in presence of two - sided input and noise dependent state information , we obtained a general achievable rate for the channel and established the capacity theorem .",
    "this capacity theorem , first demonstrate the impact of the transmitter and receiver cognition , with a new introduced interpretation on the capacity and second show the effect of the correlation between the channel input and side information available at the transmitter and at the receiver on the channel capacity .",
    "whereas , as expected , the cognition of the transmitter and receiver increases the capacity , the correlation between the channel input and the side information known at the transmitter decreases it .",
    "using the extension of cover - chiang capacity theorem given in ( [ eq.1 ] ) for random variables with continuous alphabets , the capacity of our channel can be written as : @xmath178\\label{eq.49}\\ ] ] where the maximum is over all distributions @xmath179 in @xmath85 having properties gc.1-gc.4 . since @xmath180 we have : @xmath181\\label{eq.50}\\\\ & = & \\max_{p^{\\ast}\\left ( u\\mid x , s_{1}\\right ) p^{\\ast}\\left ( x\\mid s_{1}\\right ) } \\left [ i\\left ( u;y , s_{2}\\right ) -i\\left ( u;s_{1}\\right ) \\right ] \\label{eq.51}\\\\ & = & \\max_{\\alpha}\\left [ i\\left ( u;y , s_{2}\\right ) -i\\left ( u;s_{1}\\right ) \\right ] \\label{eq.52}\\end{aligned}\\ ] ] where the expression @xmath182 in ( [ eq.52 ] ) is calculated for the distributions in @xmath116 having properties gc.1-gc.6 .",
    "thus , defining @xmath183 , we have : @xmath184 therefore @xmath185 is a lower bound for the channel capacity . to compute @xmath185",
    ", we write : @xmath186 and @xmath187 for @xmath188 we have : @xmath189 where @xmath190_{2\\times 2}\\label{eq.57}\\ ] ] and @xmath191 where the @xmath50 , @xmath192 s , @xmath6 , @xmath193 s , @xmath194 s and @xmath195 are defined in previous section",
    ". therefore @xmath196 where the terms are defined in ( [ eq.300 ] ) .",
    "+ for @xmath197 we have : @xmath198 where @xmath199_{3\\times 3}\\label{eq.62}\\ ] ] and @xmath200 after some manipulations we have : @xmath201 for @xmath202 and @xmath203 we have : @xmath204 @xmath205 where @xmath206 and the determinant of this matrix is : @xmath207 substituting ( [ eq.56 ] ) , ( [ eq.61 ] ) , ( [ eq.69 ] ) and ( [ eq.70 ] ) in ( [ eq.54 ] ) and ( [ eq.55 ] ) , we obtain : @xmath208}{q_{1}\\bigg [    \\left ( \\alpha -1\\right ) ^{2}d_{n}+\\alpha ^{2}d_{p}+2\\alpha \\left ( \\alpha -1\\right ) d_{l_{0}}+2\\alpha d_{a_{1}}+2\\left ( \\alpha -1\\right ) d_{l_{1}}+d_{q_{1 } } \\bigg ] }        \\right ) .\\label{eq.47}\\end{aligned}\\ ] ] the optimum value of @xmath209 corresponding to maximum of @xmath210 is easily obtained as : @xmath211 substituting @xmath157 from ( [ eq.48 ] ) into ( [ eq.47 ] ) and using the equations ( [ eq.300 ] ) , ( [ eq.34 + 2])-([eq.34 + 5 ] ) and ( [ eq.43 + 1 ] ) we finally conclude that @xmath212 equals @xmath213 in ( [ eq.45 ] )",
    ". therefore @xmath213 in ( [ eq.45 ] ) is a lower bound for the capacity of the channel defined by properties gc.1-gc.4 in [ subsec.definition ] ( details of computations are omitted for the brevity ) .        for all distributions @xmath106 in @xmath85 defined by properties gc.1-gc.4 , we have : @xmath214 where ( [ eq.74 + 1 ] ) follows from the fact that conditioning reduces entropy and ( [ eq.75 ] ) follows from markov chain @xmath105 and ( [ eq.77 ] ) from markov chain @xmath215 which are satisfied for any distribution in the form of ( [ eq.2 ] ) , including the distributions in the set @xmath85 . from ( [ eq.1 ] ) and ( [ eq.77 ] ) we can write : @xmath216\\label{eq.78}\\\\ & \\leq & \\max_{p\\left ( x\\mid s_{1}\\right ) } \\left [ i\\left ( x;y\\mid s_{1},s_{2}\\right ) \\right ] .\\label{eq.79}\\end{aligned}\\ ] ] from ( [ eq.79 ] ) it is seen that the capacity of the channel can not be greater than the capacity when both @xmath25 and @xmath26 are available at both the transmitter and the receiver , which is physically predictable . to compute ( [ eq.79 ] ) we write : @xmath217 where ( [ eq.81 + 3 ] ) follows from the markov chain @xmath16 .",
    "hence , the maximum value in ( [ eq.79 ] ) occurs when @xmath218 is maximum . since @xmath25 , @xmath26 and @xmath219",
    "are gaussian , the maximum in ( [ eq.79 ] ) is achieved when @xmath220 are jointly gaussian and @xmath1 has its maximum power @xmath118 , in other words , @xmath221 must be computed for distribution @xmath222 having the properties gc.1-gc.6 .",
    "let @xmath223 be the maximum value in ( [ eq.79 ] ) .",
    "we have : @xmath224 to compute @xmath223 , we first compute @xmath218 for distribution @xmath222 defined by properties gc.1-gc.6 : @xmath225 where @xmath226 and the determinant : @xmath227 and the other term in ( [ eq.83 ] ) : @xmath228 where the terms are defined in ( [ eq.300 ] ) . + substituting ( [ eq.87 ] ) in ( [ eq.85 ] ) , and from ( [ eq.100 ] ) we have : @xmath229 rewriting ( [ eq.89 ] ) in terms of @xmath230 , @xmath231 , @xmath232 , @xmath233 , @xmath170,@xmath234 , and @xmath235 using ( [ eq.34 + 2])-([eq.34 + 5 ] ) and ( [ eq.300 ] ) and taking into account two makovity results ( [ eq.43 + 1 ] ) and ( [ eq.300x ] ) , we finally conclude that ( details of manipulations are omitted for the brevity ) : @xmath236 hence , @xmath136 in ( [ eq.95 + 1 ] ) is an upper bound for the capacity of the channel when we have the markov chain @xmath16 .",
    "suppose @xmath240 and @xmath241 are the distribution functions of @xmath242 and @xmath243 respectively .",
    "if @xmath7 and @xmath166 are jointly distributed with a joint density function @xmath244 given below , we prove that the correlation coefficient is @xmath239 : @xmath245 \\label{eq.200}\\ ] ] in which @xmath246 with @xmath247 and @xmath248 first we note that ( [ eq.200 ] ) is a joint density function with marginal densities @xmath242 and @xmath243 @xcite .",
    "then we need to prove that @xmath249 . from ( [ eq.200 ] )",
    "we have : @xmath250dxds\\\\ & = & e\\left\\lbrace x\\right\\rbrace e\\left\\lbrace s\\right\\rbrace + \\rho a_{x}a_{s}\\end{aligned}\\ ] ] to complete the proof , we need to show that @xmath251 and @xmath252 in ( [ eq.201 ] ) and ( [ eq.202 ] ) exist and have nonzero values",
    ". we can show that : @xmath253 _ { -\\infty}^{+\\infty}.\\label{eq.203 + 1}\\ ] ] the second expression in the right hand side of ( [ eq.203 + 1 ] ) is equal to zero because @xmath254 _ is exactly equal _ to zero by definition .",
    "the integrand at the left hand side of ( [ eq.203 + 1 ] ) is a positive and continuous function of @xmath255 and therefore the integral exists and has nonzero positive value .",
    "so @xmath251 exists and is nonzero .",
    "the same argument is valid for @xmath252 .",
    "+        consider three zero mean random variables @xmath256 with covariance matrix @xmath100 as : @xmath257 suppose @xmath258 are _ jointly gaussian _ random variables .",
    "then , if @xmath259 form markov chain @xmath107 , _ ( even if x is not gaussian ) _ we have : @xmath260 or equivalently : @xmath261      we can write : @xmath262 where ( [ eq.228 ] ) follows from the markov chain @xmath107 and ( [ eq.229 ] ) follows from gaussianness of @xmath263 and the fact that @xmath264 and ( [ eq.230 ] ) follows from the general rule that for random variables @xmath265 and @xmath266 we have @xmath267 @xcite .              t.  m. cover and m.  chiang , `` duality between channel capacity and rate distortion with two - sided state information , '' _ information theory , ieee transactions on _ , vol .",
    "48 , no .  6 , pp . 1629 1638 , jun 2002",
    ".            y.  steinberg , `` coding for the degraded broadcast channel with random parameters , with causal and noncausal side information , '' _ information theory , ieee transactions on _ , vol .",
    "51 , no .  8 , pp .",
    "2867 2877 , aug . 2005 .",
    "s.  sigurjonsson and y .- h .",
    "kim , `` on multiple user channels with state information at the transmitters , '' in _ information theory , 2005 .",
    "isit 2005 . proceedings .",
    "international symposium on _ , sept .",
    "2005 , pp .",
    "72 76 .",
    "r.  khosravi - farsani and f.  marvasti , `` multiple access channels with cooperative encoders and channel state information , '' _ submitted to european transactions on telecommunications _ , sep .",
    "2010 , available at : http://arxiv.org/abs/1009.6008 .",
    "y.  steinberg and s.  shamai , `` achievable rates for the broadcast channel with states known at the transmitter , '' in _ information theory , 2005 .",
    "isit 2005 . proceedings .",
    "international symposium on _ , sept .",
    "2005 , pp .",
    "2184 2188 .",
    "n.  s. anzabi - nezhad , g.  a. hodtani , and m.  molavi  kakhki , `` information theoretic exemplification of the receiver re - cognition and a more general version for the costa theorem , '' _ ieee communication letters _",
    "17 , no .  1 ,",
    "pp . 107110 , 2013 .",
    "a.  rosenzweig , y.  steinberg , and s.  shamai , `` on channels with partial channel state information at the transmitter , '' _ information theory , ieee transactions on _ , vol .",
    "51 , no .  5 , pp .",
    "1817  1830 , may 2005 .",
    "a.  zaidi and p.  duhamel , `` on channel sensitivity to partially known two - sided state information , '' in _ communications , 2006 .",
    "ieee international conference on _ , vol .  4 , june 2006 , pp .",
    "1520 1525 .",
    "e.  bahmani and g.  a. hodtani , `` achievable rate regions for the dirty multiple access channel with partial side information at the transmitters , '' in _ information theory , ieee international symposium on ( isit 2012 ) _ , 2012 ."
  ],
  "abstract_text": [
    "<S> in this paper , a new and general version of gaussian channel in presence of two - sided state information correlated to the channel input and noise is considered . determining a general achievable rate for the channel and obtaining the capacity in a non - limiting case </S>",
    "<S> , we try to analyze and solve the gaussian version of the cover - chiang theorem -as an open problem- mathematically and information - theoretically . our capacity theorem , while including all previous theorems as its special cases , explains situations that can not be analyzed by them ; for example , the effect of the correlation between the side information and the channel input on the capacity of the channel that can not be analyzed with costa s  writing on dirty paper \" theorem . </S>",
    "<S> meanwhile , we try to introduce our new idea , i.e. , describing the concept of  cognition \" of a communicating object ( transmitter , receiver , relay and so on ) on some variable ( channel noise , interference and so on ) with the information - theoretic concept of  side information \" correlated to that variable and known by the object . according to our theorem </S>",
    "<S> , the channel capacity is an increasing function of the mutual information of the side information and the channel noise . </S>",
    "<S> therefore our channel and its capacity theorem exemplify the  cognition \" of the transmitter and receiver on the channel noise based on the new description . </S>",
    "<S> our capacity theorem has interesting interpretations originated from this new idea .    </S>",
    "<S> gaussian channel capacity , correlated side information , two sided state information , transmitter cognition , receiver cognition . </S>"
  ]
}