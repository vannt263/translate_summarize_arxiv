{
  "article_text": [
    "the ground - based gravitational waves detectors ligo , virgo , and geo 600 ( @xcite , @xcite , @xcite ) are rapidly improving in sensitivity . by 2015",
    ", advanced versions of these detectors should be taking data with a design sensitivity approximately 10 times greater than the previous generation , and the probed volume will grow by a factor of about a thousand .",
    "such improvements in detector sensitivity mean that the first gravitational - wave signature of a compact - binary coalescence ( cbc ) could be detected in the next few years ( see for example @xcite ) . among the expected signals ,",
    "a special role is covered by inspiralling compact binaries .",
    "this follows from the ability to model the phase and amplitude of the signals quite accurately and consequently to maximize the signal - to - noise ratio ( snr ) by using matched filtering techniques .",
    "matched filters also provide a maxim likelihood estimation of the waveform parameters such as component masses or time of coalescence .",
    "the choice of the mles as reference estimators is also motivated by the fact that if an unbiased estimator that attain the crlb exists , is the mle@xcite . +   + the existing gw frequentist literature ( @xcite .. @xcite ) evaluates the mle accuracy in two ways : ( a ) analytically by calculating the so called fisher information matrix ( fim ) or equivalently the cramer rao lower bound ( crlb ) which is the square root of the diagonal elements of the inverse fim , and ( b ) numerically by performing monte carlo simulations .",
    "+ the fim was derived analytically in ( @xcite,@xcite,@xcite ) using newtonian waveforms , extended to second - order post - newtonian ( @xcite,@xcite ) and recently revisited up to 3.5pn ( @xcite,@xcite ) . in ( @xcite),(@xcite )",
    "the authors calculate the crlb for the three _ standard _ binary systems ( nns , nbh , bbh ) , and show how the errors change when the successive different pn orders are taken into account .",
    "they consider initial ligo , advanced ligo and virgo noises .",
    "they also considers pn corrections to the _",
    "amplitude_. + monte carlo simulations were performed , for example in ( @xcite , @xcite ) , for the lowest pn orders , where it is also suggested that the inclusion of the higher pn orders would be computationally expensive .",
    "more recent monte carlo simulations with 3.5 pn waveforms are described in ( @xcite ) .",
    "+ we did not try to compare the uncertainties derived here to other existing papers ( especially from the 90 ) since different parameter sets , noise spectra ( even for the same antenna ) and pn terms were used .",
    "for example in ( @xcite ) a comparison between the crlb and other bounds is done for a waveform at the 0th pn order .",
    "this work also uses different conventions on the waveform spectrum than more recent literature . in ( @xcite ) phasing",
    "is extended to the 1.5pn order .",
    "the spin parameters are taken in account but the noise spectrum for ligo is different than the currently used design noise curves . in ( @xcite ) , ( @xcite ) , ( @xcite ) the 2pn wave was used . in the work ( @xcite )",
    "interesting observations are made about the fluctuation of the parameters variance with the pn orders , analyzing both the case of spin and spinless systems .",
    "the fluctuations of the variance in the spinless case is also stressed in ( @xcite ) . +   + the crlb is a convenient tool to approximate the accuracies in large snrs and to obtain error bounds for unbiased estimators .",
    "unfortunately , for low snrs ( below 20 ) where the first detections might emerge , the crlb can grossly underestimate the errors @xcite,@xcite,@xcite,@xcite,@xcite .",
    "the reason is that with non linear data models and ( or ) with non gaussian noises the crlb depends only on the curvature of the likelihood function around the true value of the parameter .",
    "+   + in this paper we apply a recently derived analytical tool to better predict an mle accuracy and to establish necessary conditions on the signal - to - noise ratio ( snr ) for the mle error to attain the crlb .",
    "explicitly , within the frequentist framework , for arbitrary probability distribution of the noise , expansions of the bias and the covariance of a mle in inverse powers of the snr are discussed .",
    "the first order of the expansion of the variance is the inverse of the fim . by requiring that the second order covariance is smaller , or much smaller , than the first order this approach predicts a necessary snr to approximate the error with the crlb .",
    "the higher order of the expansions are determinant in the low snr regime where the inverse fim underestimates the error .",
    "we compared our the errors computed using the first two orders of the expansions to the monte carlo simulations in ( @xcite ) .",
    "we observed the first two orders of the expansions get error predictions closer than the crlb to what is observed in the numerical simulations . in ( @xcite ) the simulations are related to the fim to estabish ranges of snr where the crlb describe the error .",
    "our expansions predict the same snr range of validity for the crlb .",
    "+ the expansions are sensitive to the side lobes of the likelihood function because they make use of higher order derivatives than the second one ( which is only sensitive to the curvature of the main lobe ) .",
    "the methodology also provides new insight on the relationship between waveform properties , snr , dimension of the parameter space and estimation errors .",
    "for example the timing match filtering accuracy achieves the crlb only if the snr is larger than the kurtosis of the gravitational wave spectrum and the necessary snr is much larger if the other physical parameters are unknown .",
    "more specifically the mle of the arrival time for ns - ns binary signals might require an snr equal to 2 with the time as the only parameter or 15 when all the other parameters are unknown .",
    "these results are important to understand the domain of validity of recent papers like @xcite that defines @xmath0 confidence regions in direction reconstruction with time triangulation .",
    "the regions discussed in @xcite for snr smaller than 10 are based on timing mles , with the arrival time being the only unknown parameter , and the time uncertainties quantified by the crlb .",
    "+ we also note that @xcite , using a formalism introduced in @xcite,@xcite , describes a consistency criterion , different from the condition derived in this paper , for the validity of the crlb that , if applied to a 4pp compact binary signal computed with a 2pn expansion and @xmath1 , requires an snr of at least 10 . at the time of the writing of this paper we established with m.vallisneri that the equation ( [ varmatrixsimplified ] ) of this paper becomes , in the one parameter case and colored gaussian noise , equivalent to equation ( 60 ) in ( @xcite ) or ( a35 ) in ( @xcite ) . a comparison for the gaussian noise and multi parameter case is object of current work , while a comparison for arbitrary noise is not possible because ( @xcite ) and ( @xcite ) use from the beginning of their derivations gaussian noises .",
    "the explicit calculations shown here for different gws are also not performed in ( @xcite ) and ( @xcite ) . +   + in section [ expansions ]",
    "we present the explicit expressions of the expansions of the bias and the covariance matrix for arbitrary noise and size of the parameter space . in section [ match ]",
    "we explain how the expansion can be evaluated for signals in additive colored gaussian noise . in section [ waveformsection ]",
    "we describe the post - newtonian inspiral waveform used for the examples , describe the parameter space and the initial and advanced ligo noises . in section [ onedim ]",
    "we study the one - dimensional parameter space results when only one parameter at a time is considered unknown . in section [ fullparspace ]",
    "we present the results for full parameter space with the initial and advanced ligo noises .",
    "we also compare our results with published results from monte carlo simulations . in section [ conclusions ]",
    "we present some conclusions and in the appendix we describe the derivation of the expansions as well as the relationship of this method with the statistics literature .",
    "in this section we present the first two orders of the expansions in inverse powers of the snr for the bias and the covariance matrix .",
    "the details of the derivation are provided in appendix a. given an experimental data vector @xmath2 , where @xmath3 is the dimension , we assume that the data are described by a probability density @xmath4 that depends on a d - dimensional parameter vector @xmath5 . according to ( @xcite ) , we suppose that the mle @xmath6 of @xmath7 is given by a stationary point of the likelihood function @xmath8 with respect to the components of @xmath7 @xmath9 if we introduce the notations @xmath10 @xmath11\\ ] ]    where @xmath12 is the fisher information matrix @xmath13=e[l_{a}l_{b}]$ ] ( @xmath14 $ ] is the expected value ) , the first two orders of the bias for the mle of the @xmath15 component of the parameter vector @xmath7 are given by @xmath16    @xmath17 + \\frac{i^{ma}i^{bd}i^{ce}}{8}[v_{abcde } + 4v_{ac , bde } + 8v_{de , abc } + 4v_{abce , d } + 4v_{abc , d , e}\\nonumber\\\\ & + & 8v_{ab , cd , e}]+ \\frac{i^{ma}i^{bc}i^{df}i^{eg}}{4}\\bigg[(2v_{afed}v_{gb , c } + 2v_{bedf}v_{ac , g } + 4v_{abed}v_{gf , c } ) + ( v_{afed}v_{gcb } + \\nonumber\\\\ & + & 2v_{abed}v_{gcf } + 2v_{dbeg}v_{acf } ) + ( 2v_{aed}v_{gb , fc } + 4v_{acf}v_{dg , eb } + 4v_{bed}v_{ac , gf } + 2v_{fcb}v_{ag , ed } ) + \\nonumber\\\\ & + & ( 4v_{afe , g}v_{db , c } + 4v_{afe , c}v_{db , g } + 4v_{dbe , g}v_{af , c } ) + ( 2v_{abe , g}v_{cdf } + 4v_{dbe , g}v_{acf } + 4v_{abe , f}v_{cfg } + \\nonumber\\\\ & + & 2v_{dge , b}v_{acf } ) + ( 4v_{ag , fc}v_{ed , b } + 4v_{ed , fc}v_{ag , b } + 4v_{ag , ed}v_{fc , b } ) \\nonumber \\\\ & + & \\left.(4v_{acg}v_{ef , b , d } + 2v_{cde}v_{ab , f , g } ) + \\frac{2}{3}v_{abde}v_{c , f , g}\\right ] \\nonumber \\\\ & + & \\frac{i^{ma}i^{bc}i^{de}i^{fg}i^{ti}}{8}[v_{adf}(v_{ebc}v_{gti } + 2v_{etc}v_{gbi } + 4v_{gbe}v_{tci } + 8v_{gbt}v_{eci } + 2v_{ebc}v_{gt , i } \\nonumber \\\\ & + & 4v_{etc}v_{gb , i } + 2v_{gti}v_{eb , c } + 4v_{gtc}v_{eb ,",
    "i } + 8v_{gbt}v_{ce , i } + 8v_{gbt}v_{ci , e } + 8v_{gbe}v_{ct , i } + 8v_{cte}v_{gb , i } \\nonumber \\\\ & + & 4v_{cti}v_{gb , e } + 4v_{gt , i}v_{eb , c } + 4v_{eb , i}v_{gt , c } + 8v_{gt , b}v_{ic , e } + 8v_{gt , e}v_{ic , b } + 4v_{bet}v_{g , c , i } ) \\nonumber \\\\ & + & v_{dci}(8v_{bgt}v_{ae , f } + 4v_{bgf}v_{ae , t } + 8v_{ae , t}v_{bg , f } + 8v_{ae , f}v_{bg , t } + 8v_{af , b}v_{ge , t})]\\end{aligned}\\ ] ]    were we assumed the einstein convention to sum over repeated indices .",
    "for the covariance matrix the first order is the inverse of the fisher information matrix while the second order is given in by ( for simplicity we provide the diagonal terms ) :    @xmath18\\end{aligned}\\ ] ]",
    "for this analysis we assume that the output of an optimally oriented gw interferometer has the form :    @xmath19    where @xmath20 is the signal , which depends on the parameters vector @xmath21 , and @xmath22 a stationary gaussian noise with zero mean .",
    "the probability distribution can be written :    @xmath23\\,\\omega(t - t_1)\\,[x(t_1)-h(t_1,\\underline{\\theta})]\\,{\\mathrm{d}}t { \\mathrm{d}}t_1}\\right\\}\\ ] ]    the first and second derivative of the log - likelihood give :    @xmath24\\,{\\mathrm{d}}t { \\mathrm{d}}t_1}\\\\ l_{ab } & \\equiv & \\frac{\\partial l_a}{\\partial \\theta_b } =   \\int{\\left[h_{ab}(t,\\underline{\\theta})\\,\\omega(t - t_1)\\,[x(t_1)-h(t_1,\\underline{\\theta } ) ] - h_a(t,\\underline{\\theta})\\,\\omega(t - t_1)\\,h_b(t_1,\\underline{\\theta } ) \\right]{\\mathrm{d}}t { \\mathrm{d}}t_1}\\end{aligned}\\ ] ]    @xmath25 = -e[l_{ab}]&= & \\int{h_a(t,\\underline{\\theta})\\,\\omega(t - t_1)\\,h_b(t_1,\\underline{\\theta})\\,{\\mathrm{d}}t { \\mathrm{d}}t_1 } = \\\\ & = & \\int{{\\mathrm{d}}f { \\mathrm{d}}f ' \\,h_a(f ) \\,h_b(f ' ) \\,\\omega(-f ,- f')}\\end{aligned}\\ ] ]    it s easy to verify that @xmath26 where we have introduced , @xmath27,the _ one sided power spectral density _ defined as the fourier transform of the noise auto - correlation : @xmath28.\\\\ s_h(f)&\\equiv & \\int{{\\mathrm{d}}t\\ , e^{-2\\pi i f t } r(t)}.\\label{onesidednoise}\\end{aligned}\\ ] ] notice that the sign convention in ( [ sign ] ) follows from the implicit assumption that r(t ) is the fourier transform of @xmath29 $ ] . in the literature",
    "another convention with the minus sign is also found corresponding to defining r(t ) as the fourier transform of @xmath30 $ ] . using the relation @xmath31 ,",
    "we can finally write the fim : @xmath32 = \\langle h_a(f)\\,,\\,h_b(f)\\rangle\\ ] ] where @xmath33 are the derivatives of the fourier transform of the signal with respect to the a - th parameter . we have introduced a mean in the frequency space : @xmath34\\ ] ] where the range of integration depends on the antenna properties and the theoretical model for the binary system .",
    "the snr corresponds to the optimal filter : @xmath35    we can express in the same way all the quantities we need in order to calculate the second order variance , like scalar products of @xmath36 derivatives .",
    "@xmath37 @xmath38 @xmath39 if one uses these functions , the form for the second order variance , eq ( [ varmatrix ] ) can be further simplified :    @xmath40",
    "we apply the general results of the theory to the case of a binary system in the inspiral phase . starting from the the 3.5pn phasing formula ( @xcite ) we write the fourier transform of the chirp signal : @xmath41\\ ] ] where @xmath42 is the implicit solution of the 3.5pn phasing formula , using the _ stationary phase approximation _",
    "( spa ) ( @xcite , @xcite ) .",
    "the final result is :    @xmath43    the phase is given by :    @xmath44    where @xmath45 and @xmath46 are the arrival time and the arrival phase .",
    "the function @xmath47 can be defined either in terms of the total mass of the binary system , @xmath48 , or in terms of the chirp mass , @xmath49 :    @xmath50    where @xmath51 is the symmetrized mass ratio @xmath52 the amplitude is a function of the chirp mass , the effective distance , and the orientation of the source : @xmath53 the coefficients @xmath54 s with @xmath55 ( the meaning of each terms being the @xmath56 pn contribution ) are given by : @xmath57\\right)\\end{aligned}\\ ] ] @xmath58 where @xmath59 , @xmath60 .",
    "@xmath61 is the euler constant , and @xmath62 , with @xmath63",
    "the _ last stable orbit _ frequency for a test mass in a scharzschild space - time of mass m : @xmath64 which will be also used as upper cutoff for the integrals ( [ meanfreq ] ) .. ( [ vabcd ] ) .",
    "given the waveform ( [ waveform ] ) one can easily calculate the fisher information matrix , and its inverse , the crlb .",
    "( [ waveform ] ) contains five unknown parameters , @xmath65 ( the total mass @xmath66 could be used instead of the chirp mass ) , so that one should calculate a five dimensional square matrix .",
    "it was already observed by ( eg @xcite ) that the errors in the distance , and consequently the amplitude @xmath67 , are uncorrelated with the errors in other parameters , ie that the fisher information is block diagonal .",
    "we observed that this is also the case for the waveform ( [ waveform ] ) we use here for both the fim and the second order covariance matrix .",
    "we can therefore work in a four dimensional parameter space @xmath68 .",
    "however it is worth observing that this is not an obvious point since in general the amplitude estimation can be coupled with other other parameters if they enter in the signal in specific ways ( see ch.3 of @xcite ) .",
    "+ it is also worthwhile noticing that the snr and the amplitude @xmath67 are related like follows :    @xmath69    we perform the calculations using analytical forms of the design initial and advanced ligo noise spectrum ( [ iniligo]).the initial one sided power spectral density of ligo can be written for @xmath70 ( @xmath71 ) : @xmath72 , \\ ] ] where the lower frequency cutoff value is @xmath73 , @xmath74 , @xmath75 , and @xmath76 .",
    "the advanced ligo one sided psd has the following expression , for @xmath70 ( @xmath77 ) : @xmath78 , \\ ] ] where the lower frequency cutoff value is @xmath79 , @xmath74 , @xmath80 , and @xmath81 .",
    "they are plotted in fig .",
    "[ ligonoise ]    [ cols=\"^,^ \" , ]",
    "in this paper we applied a recently derived statistical methodology to gravitational waves generated by the inspiral phase of binary mergers and for noise spectral densities of gravitational wave interferometers .",
    "explicitly we computed the first two orders of mle expansions of bias and covariance matrix to evaluate mle uncertainties .",
    "we also compared the improved error estimate with existing numerical estimates .",
    "the value of the second order of the variance expansions allows to get error predictions closer to what is observed in numerical simulations than the inverse of the fim .",
    "the condition where the second order covariance is negligible with respect to the first order predicts correctly the necessary snr to approximate the error with the crlb and provides new insight on the relationship between waveform properties snrs and estimation errors .",
    "future applications include imr waveforms , network of detectors and source location estimation .",
    "mz would like to thank the national science foundation for the support through the awards nsf0855567 and nsf0919034 , while sv would like to thank the universite pierre - et - marie - curie and embry riddle aeronautical university for logistic and financial support .",
    "analytic expressions for the moments of a ( mle ) are often difficult to obtain given a non - linear data model . however , it is known from ( @xcite ) that likelihood expansions can be used to obtain approximate expressions for the moments of a mle in terms of series expansions in inverse powers of the sample size ( @xmath82 ) .",
    "these expansions are valid in small samples or snr where the mle may have significant bias and may not attain minimum variance .",
    "an expressions for the second covariance equivalent to the one used here , was first given in a longer form ( about a factor 2 ) , and with the prescription of isolating the different asymptotic orders inside the tensors , in ( @xcite ) .",
    "the expression presented here were first derived in an unpublished mit technical report ( @xcite ) by two of the authors of this paper , where ( a ) the bartlett identities ( @xcite,@xcite ) were used to simplify the expression of the second order variance , and derive the second order bias .",
    "( b ) the prescription on the tensors no longer needed to be implemented .",
    "the final form of the second order covariance has already been published without proof , in two papers ( @xcite , @xcite ) , were the first and third author of this paper are coauthors , involving mle of source and environment parameters that use acoustic propagation within shallow water ocean wave guide .    in this section ,",
    "we derive the second order bias for a multivariate mle and we introduce a chain rule that allows the derivation of the of the second order covariance matrix from the second order bias .",
    "the explanation follows closely  @xcite .",
    "the derivation of the bias is performed in two steps : first we derive the expansions in the non physical scenario of n statistically independent identical measurements and then set n=1 for the case of interest of this paper .",
    "+ the derivation follows the approach of ( @xcite ) for likelihood expansions , ( @xcite ) for the large sample approximations and ( @xcite , @xcite ) for the notation of the observed likelihood expansions . for the small sample case of interest , @xmath83 orders generated from the likelihood expansion may contain different powers of @xmath84 , and the contributions to a single power of @xmath84 may have to be collected from different @xmath83 orders .",
    "( @xcite ) extending the work of ( @xcite ) , confronted the equivalent problem of separating the powers in @xmath82 within the expectations of products of linear forms and arrived at expressions for the second order bias of the mle by embedding the derivation in a discrete probability scheme .",
    "some applications of their results are given in ( @xcite ) , ( @xcite ) , ( @xcite),(@xcite),(@xcite ) .",
    "we present here an independent derivation for the second order bias that is valid for general discrete or continuous random variables .",
    "+ let us consider a set of @xmath82 independent and identically distributed experimental data vectors @xmath85 , where @xmath3 is the dimension of every vector .",
    "we assume that these data are described by a probability density @xmath86 that depends on a d - dimensional parameter vector @xmath5 . according to ( @xcite ) , we suppose that the mle @xmath6 of @xmath7 is given by a stationary point of the likelihood function @xmath87 with respect to the components of @xmath7 @xmath88 the first step in deriving the likelihood expansion , if @xmath89 can be expanded as a taylor series in the components of the observed error @xmath90 , consists of writing @xmath89 as      where @xmath92 for @xmath93 are the components of the observed error . we will use the notation @xmath94   \\nonumber \\end{aligned}\\ ] ] where @xmath95 and @xmath12 is the information matrix @xmath13=e[l_{a}l_{b}]$ ] . introducing @xmath96 as the inverse of the matrix whose elements are given by @xmath97 , equation ( [ cippo ] ) can be rearranged to solve for @xmath98      finally we can iterate equation ( [ 2 ] ) with respect to the components of the observed error and expand @xmath96 in terms of the information matrix inverse @xmath100 ( @xcite , page 149 ) @xmath101_{ab}&=&j^{ab}=\\label{barnone}[(1-i^{-1}(i - j))^{-1}i^{-1}]^{ab}=\\nonumber \\\\   & = & { i^{ab}}+{i^{at}i^{bu}h_{tu}}+{i^{at}i^{bu}i^{vw}h_{tv}h_{uw}}+\\nonumber\\\\ & + & { i^{at}i^{bu}i^{vw}i^{cd}h_{tv}h_{wc}h_{du}}+ ... \\,\\,\\,\\,\\,\\,\\,\\,\\,\\end{aligned}\\ ] ] from ( [ 2 ] ) and ( [ barnone ] ) the terms that contribute to each @xmath83 order of the observed error can be obtained . however , in order to isolate the @xmath83 orders necessary to compute the second order bias we have chosen only a finite number of terms within equations ( [ 2 ] ) and ( [ barnone ] ) .",
    "this choice can be made by @xmath102 observing that @xmath103 is proportional to @xmath82 @xmath104 = n \\ ,",
    "e[\\frac{\\partial^s \\,ln ( p(\\underline{x}_i,\\underline{\\vartheta } ) ) } { \\partial\\vartheta_{a_1}\\partial\\vartheta_{a_2} .. \\partial\\vartheta_{a_s } } ] \\end{aligned}\\ ] ] where the value of @xmath105 is irrelevant because all the vector data have the same distribution , and @xmath106 using the large sample approximation expressed by ( @xcite , page 221 ) @xmath107\\sim n^{\\frac{1}{2}},\\end{aligned}\\ ] ] and proved for discrete and continuous random variables in the next paragraph .",
    "equation ( [ 5 ] ) indicates that the expectation of a product of @xmath108 generic factors @xmath109 is a polynomial of integer powers of @xmath82 , where the highest power of @xmath82 is the largest integer less than or equal to @xmath110 ( we use the notation @xmath111 ) .",
    "for example , @xmath112 is proportional to @xmath82 .",
    "+ the proof of the large sample approximation ( [ 5 ] ) is an extension to continuous random variables of the analysis performed in ( @xcite , page 36 ) , for discrete random variables .",
    "+ to prove equation ( [ 5 ] ) we show that the quantities @xmath113\\end{aligned}\\ ] ] obtained as the expectation of products of @xmath114 functions @xmath115 are polynomials in integer powers of @xmath82 of order less than or equal to @xmath116 .",
    "+ here , @xmath117 . the subscripts @xmath118 that appear in @xmath119 represent collections of indexes as introduced in equation ( [ 5 ] ) .",
    "+ the factors @xmath120 appearing in the expectation can be seen as the first @xmath114 terms of a succession @xmath119 , where we choose arbitrary @xmath119 for @xmath121 .",
    "using such a succession we can define the quantity @xmath122\\end{aligned}\\ ] ] where we assume that all the expectations exist .        where the quantity @xmath124 can be equal to 0 or to 1 to indicate that the quantity @xmath125 is present or not in the product inside the expectation .",
    "the summation over all @xmath126 is intended to account for all possible configurations of the indexes ( choosing some equal to zero and the others equal to one ) with the constraint that the number of factors within the expectation is equal to @xmath127 .",
    "we can now group together terms @xmath128 for different @xmath105 and define @xmath129 , where @xmath130 . in this manner we can also factorize the expectations in ( [ 100 ] ) as expectations for different data vectors . by making use of the statistical independence of the different data vectors , and defining @xmath131",
    "$ ] , ( [ 100 ] ) can be rewritten as follows :      we observe that , when the expectations contain only one factor , for @xmath133 , we have + @xmath134=e[h_{1,\\underline{\\alpha}_{j}}]=\\lambda(a_{1,\\underline{a}_j})=0 $ ] for any @xmath135 . as a consequence , the taylor expansion of @xmath136 in @xmath137 can be written as @xmath138 . + eventually , it is the absence of @xmath139 in this expansion that allows us to explain the properties of the polynomials in equation ( [ esempio ] ) and finally the large sample approximation ( [ 5 ] ) . to accomplish this last step",
    "we explain how the polynomials @xmath140 are related to the coefficients @xmath141 .",
    "let us consider the contribution to @xmath140 that originates from the product of @xmath142 factors @xmath143 with the constraints @xmath144 and @xmath145 .",
    "the number of ways these products can be formed for an assigned set of @xmath146 is proportional to @xmath147 .",
    "moreover , the contributions to @xmath140 are formed by an integer number of factors less than or equal to @xmath116 because @xmath148 .",
    "this property limits the highest power in @xmath82 contained in @xmath140 with the largest integer number smaller than or equal to @xmath116 .",
    "equation ( [ esempio ] ) is then proved as is the large sample approximation ( [ 5 ] ) .",
    "+ each @xmath83 order @xmath114 of the @xmath149 component of the observed error is denoted by @xmath150 $ ] where the index is given by @xmath151 and @xmath127 is a natural number .",
    "for example , the @xmath83 order for @xmath152 is given by @xmath153=\\frac{1}{2}\\upsilon^{rst}l_{s}l_{t}+i^{rs}i^{tu}h_{st}l_{u }   \\end{aligned}\\ ] ] where we have adopted the lifted indexes notation @xmath154 .",
    "the @xmath83 orders of the bias are then given as the expectation of the @xmath83 orders of the observed error @xmath155=e [ ( \\widehat{\\vartheta } -\\vartheta ) ^r[m]].\\ ] ] the @xmath83 order @xmath156 $ ] contains different powers of @xmath84 as we discuss in this paragraph .",
    "+ it follows from equations ( [ 2 ] ) to ( [ 5 ] ) that @xmath156 $ ] is the sum of terms having the structure + @xmath157 $ ] where @xmath158,@xmath159 , and @xmath108 are the numbers of factors in the three products satisfying @xmath160 .",
    "different powers of @xmath84 can be generated because @xmath161 $ ] can contain all the integer powers of @xmath82 equal to or less than @xmath162 .",
    "however from the fact that no power higher than @xmath162 can be generated follows that an asymptotic order @xmath114 will never generate powers in the sample size @xmath163 with @xmath164 smaller than @xmath114 .",
    "it still remains to prove which is the largest power in @xmath84 contained in the asymptotic order @xmath114 .",
    "we show that in the following . since the largest range of powers of @xmath84 is allowed when @xmath165 , we study these terms to evaluate the range of powers of @xmath84 contained in @xmath156 $ ] .",
    "the structure of equation ( [ 2 ] ) implies that its iteration with respect to the observed error components generates an equal number of @xmath96 and @xmath166 ( we recall that @xmath167 ) .",
    "similarly , the number of @xmath168 generated from the expansion of @xmath96 given in equation ( [ barnone ] ) is equal to the number of @xmath169 plus 1 .",
    "these two observations imply that the terms where @xmath165 also verify @xmath170 . as a consequence ,",
    "the powers of @xmath84 that @xmath156 $ ] contains can range from @xmath171 , up to @xmath172 .",
    "+ the analysis above implies that in order to compute the contributions to @xmath84 and to @xmath173 in the bias expansion that we denote with @xmath174 $ ] and @xmath175 $ ] , it is necessary to obtain the first three @xmath83 orders of the bias , @xmath176 $ ] , @xmath177 $ ] , and @xmath178 $ ] . in the explicit expressions below , to condense the notation , we introduce the quantities @xmath179 , so that    @xmath180&=&\\label{11}i^{ri , kl}\\upsilon _ { ik , l}+\\frac{1}{2}i^{rj , lp}\\upsilon _ { jlp}\\\\ { \\tilde{b } } [   \\widehat{\\vartheta } ^r , \\frac{3}{2}]&=&\\frac{1}{6}(i^{r\\alpha , s\\beta , t\\gamma , u\\delta } \\upsilon _ { \\alpha\\beta\\gamma\\delta}+\\label{12}3i^{r\\alpha , s\\beta , \\gamma\\delta , tg , uv } \\upsilon _ { \\alpha\\beta\\gamma}\\upsilon _ { \\delta , g , v }   \\upsilon _ { s , t , u}+ i^{r\\alpha , s\\beta , u\\gamma , tv } \\upsilon _ { \\alpha\\beta\\gamma}\\upsilon _ { uv , s , t}+\\\\ & + & \\frac{1}{2}i^{rs , t\\alpha , u\\beta , v\\gamma}\\upsilon _ { \\alpha\\beta\\gamma}\\upsilon _ { st , u , v } + + \\frac{1}{2}i^{rs , tu , vw}\\upsilon _ { stv , u , w } + i^{rs , tu , vw}\\upsilon _ { st , vu , w } + 2i^{rs , tw}\\upsilon _ { st , w } \\nonumber\\\\ & + & i^{r\\alpha,\\beta\\gamma}\\upsilon _",
    "{ \\alpha\\beta\\gamma}\\nonumber\\\\ { \\tilde{b } } [   \\widehat{\\vartheta } ^r , 2]&=&\\label{13 } ( i^{ra , bq , cd , tp}+   \\frac{1}{2}i^{rd , ta , bq , cp } )    \\upsilon_{\\{abc\\}\\{dt\\}\\{q\\}\\{p\\}}+\\frac{1}{2 } ( i^{ra , bs}\\upsilon ^{cpq }   + i^{bp , cq}\\upsilon ^{rsa }   ) \\upsilon_{\\{abc\\ }   \\{s\\}\\{p\\}\\{q\\ } } \\nonumber \\\\ & + & ( \\frac{1}{2}i^{ap , tq}\\upsilon ^{rbg}+i^{ra , tq}\\upsilon ^{bpg}+   \\frac{1}{2}i^{ra , bg}\\upsilon ^{tpq}+i^{bg , tq}\\upsilon ^{rpa})\\upsilon_{\\{ab\\}\\{gt\\}\\{p\\}\\{q\\}}+\\frac{1}{2 } [   \\upsilon ^{rsz}\\upsilon ^{pqt } \\nonumber\\\\ & + & \\upsilon ^{rzqs}i^{tp}+\\frac{1}{3}\\upsilon ^{pzqt}i^{rs}+(i^{dz , eq , pt}\\upsilon ^{rcs }    + i^{rp , dz , et}\\upsilon ^{sqc}+2i^{dq , es , pt}\\upsilon ^{rzc})\\upsilon_{cde } ] \\upsilon_{\\{sp\\}\\{z\\}\\{q\\}\\{t\\ } }   \\nonumber \\\\ & + & i^{ra , st , bc , de}\\upsilon_{\\{ab\\}\\{cd\\}\\{et\\}\\{s\\}}+\\frac{1}{6}i^{rj , kl , mp , qz}\\upsilon_{\\{jlpz\\}\\{k\\}\\{m\\}\\{q\\}}+[\\frac{1}{24}\\upsilon^{r \\alpha",
    "\\beta \\gamma \\delta}+\\nonumber\\\\ & + & \\frac{1}{6}\\upsilon^{r \\alpha a } i^{\\beta b , g\\delta,\\gamma d } \\upsilon _ { abgd}+\\frac{1}{4}\\upsilon^{r a \\gamma \\delta } i^{\\beta c,,b \\alpha } \\upsilon_{a b c}+    \\frac{1}{8 } \\upsilon^{r v a } i^{w \\alpha , z \\beta , b \\gamma , g \\delta }          \\upsilon_{v w z}\\upsilon_{abg}+\\nonumber\\\\ & + & \\frac{1}{2}\\upsilon^{r \\alpha v } i^{w\\beta }          \\upsilon_{v w z}\\upsilon^{z \\gamma \\delta}]\\upsilon_{\\{\\alpha\\}\\{\\beta\\}\\{\\gamma\\}\\{\\delta\\}}. \\label{biastworough}\\end{aligned}\\ ] ]    the first order of the bias @xmath174 $ ] can than be obtained by substituting the explicit expressions for the tensors in @xmath176 $ ] . the second order @xmath175 $ ] takes contributions from @xmath177 $ ] and @xmath178 $ ] .",
    "however , while @xmath181 $ ] generates only @xmath182 contributions , @xmath183 $ ] generates @xmath182 and @xmath184 contributions .",
    "consequently , to collect all and only the contributions to @xmath182 , it is necessary to extract the @xmath182 component from @xmath178 $ ] and add it to @xmath177 $ ] .",
    "the extraction can be done by introducing into @xmath178 $ ] only the highest powers of @xmath82 of the tensors .",
    "starting from ( [ biastwo ] ) the form for the second order covariance matrix can be obtained using the following theorem : + theorem : _ let @xmath185 be an invertible and differentiable function and let @xmath186 be a set of parameters dependent on @xmath7 .",
    "let @xmath187 and @xmath188 be the biases relative to the estimation of the components of @xmath7 and @xmath189 which can be computed as power series of @xmath84 , as explained in section 2 .",
    "the terms of the expansion in powers of @xmath84 for @xmath188 can be obtained from the terms of the expansion for @xmath187 by replacing the derivatives with respect to the components of @xmath7 , with derivatives with respect to the components of @xmath189 .",
    "the explicit dependence on @xmath189 can be removed at the end of a derivation by means of the jacobian matrix j defined by @xmath190 .",
    "_   + to prove the chain - rule theorem given in section [ expansions ] , we analyze how the derivation of the expansion in powers of @xmath84 for @xmath191 , which is described there , changes if we are interested in the expansion for @xmath192 where @xmath193 is a vector of parameters in an invertible relationship with @xmath194 : @xmath195 .",
    "the starting point for the derivation of the expansion of @xmath192 is the stationarity condition @xmath196 that can be obtained from equation ( [ stazionarieta ] ) by replacing only the derivatives @xmath197 with @xmath198 , because @xmath199 implies @xmath200 .",
    "the subsequent steps can then be obtained from equations ( [ cippo ] ) up to ( [ biastwo ] ) by replacing the derivatives in the same way . since @xmath201 , where the components of the jacobian matrix @xmath202 behave like constants in the expectations , the substitution of the derivatives can also be done only in the final result ( for example , in the orders given in equations ( [ biastworough ] ) and ( [ biastwo ] ) ) .",
    "the expectations contained in the expansion of @xmath203 in powers of @xmath84 can also be computed before the substitution of the derivatives if the likelihood function dependence on the parameters is expressed in terms of auxiliary functions .",
    "examples of auxiliary functions are @xmath204 for a general parametric dependence and @xmath205 @xmath206 for a scalar gaussian distribution . by means of these auxiliary functions ,",
    "the derivatives @xmath207 and @xmath208 become @xmath209 and @xmath210 . as a consequence",
    "the orders of the expansion for @xmath211 can be found from the orders for the expansion of @xmath203 implementing , in the result of the expectations , the substitutions @xmath212 the converse of the chain - rule , in which higher moments of the observed error are used to compute lower moments , is not possible .",
    "we can observe , for example , that the expansion of a general moment of an mle does not always contain all the powers of @xmath84 .",
    "the lowest order present in the expansion of the m order moment is given by the largest integer number smaller than or equal to @xmath116 ( we use the notation @xmath213 ) : @xmath214= \\frac{1}{n^{int(\\frac{m}{2})}}+higher\\,powers\\ , o\\!f\\ , \\frac{1}{n}.\\end{aligned}\\ ] ] the consequence of this observation is that only the bias and the error correlation matrix may contain first order terms .",
    "for this reason an inverse chain - rule would have to generate non - zero orders of lower moments expansions from the corresponding orders of the higher moments expansions that are zero for powers of @xmath84 lower than @xmath215 .",
    "let us consider how the chain - rule makes it possible to compute the expansion of the error correlation matrix @xmath216 defined by @xmath217 $ ] and the covariance matrix .",
    "using the invariance property of the mle , @xmath218 where @xmath219 is the transpose of a vector and @xmath220 is a matrix whose components are the bias of the product of two components of @xmath7 .",
    "once @xmath221 and the bias are known , then the covariance matrix @xmath222 can also be computed by means of @xmath223 to compute the right hand side of equation ( [ 74 ] ) we express it in terms of the components , obtaining @xmath224 it is important to realize that knowledge of @xmath225 is sufficient because the expansion of @xmath226 can be derived from it by means of the theorem given above .",
    "in fact , if we choose @xmath227 as a new set of parameters , @xmath226 becomes @xmath228 .",
    "however , is necessary to insure that the relationship between @xmath7 and @xmath189 is invertible .",
    "this condition holds if both @xmath229 where @xmath230 and the sign of the components of @xmath7 is known , for example , by means of constraints on the data model .",
    "+ in a scalar estimation scenario , equation ( [ 74 ] ) becomes simply @xmath231=b(\\label{61}\\widehat{{\\vartheta } ^2})-2\\vartheta b(\\widehat{{\\vartheta } } ) \\end{aligned}\\ ] ] and the variance @xmath232 . in this case",
    "@xmath233 can be derived from @xmath234 if we choose @xmath235 as the new parameter and the jacobian matrix becomes @xmath236 because @xmath237 . + a useful simplification of the algebra of the chain - rule in the derivation of second order moments",
    "is described in the following two paragraphs .",
    "the chain - rule and the subsequent conversion of the derivatives require the substitutions    @xmath238 } { \\partial \\vartheta_{i_1 } .. \\partial \\vartheta_{i_m }   } \\rightarrow \\frac { \\partial^m [ . ] } { \\partial \\xi_{i_1 } .. \\partial \\xi_{i_m }   }   \\rightarrow j_{i_1 j_1}(\\underline{\\vartheta } ) \\frac { \\partial } { \\partial \\label{convder } \\vartheta_{j_1 } } j_{i_2 j_2}(\\underline{\\vartheta } ) \\frac { \\partial } { \\partial \\vartheta_{j_2 } } .. j_{i_m j_m}(\\underline{\\vartheta } ) \\frac { \\partial } { \\partial \\vartheta_{j_m } } [ .]\\end{aligned}\\ ] ]    from the right hand side of ( [ convder ] ) it is clear that the derivatives will also be applied to the jacobian matrix , thereby generating @xmath239 terms for every derivative of order @xmath114 .",
    "however , it can be observed that the terms contributing to the final result are only a small subset .",
    "in fact , among all the terms generated from the conversion of the derivatives in the bias expansion , only those where a first derivative of the jacobian matrix appears must be considered .",
    "for example , we can consider the term @xmath240 that comes from equation ( [ biastwo ] ) , which must be used to derive the second order @xmath216 . in this case",
    ", we need to consider only the 3 terms in which one of the derivatives represented by @xmath241 operates on a jacobian matrix and @xmath242 operate on the likelihood function , plus the 3 terms where the role of @xmath243 and @xmath244 are inverted . in general , the total number of relevant terms among those generated in every derivative is equal to or less than the order of the derivative @xmath114 .",
    "+ the detailed analysis of equation ( [ 21 ] ) reveals that the terms generated in @xmath245 can be divided into three groups : ( a ) the terms where no derivative of the jacobian matrix appears are equal to @xmath246 ( we show in example 1 that @xmath246 cancels with @xmath247 after its introduction in equation ( [ 21 ] ) ) ; ( b ) the terms where only one derivative of the jacobian matrix appears give the error correlation matrix ; and ( c ) the terms that contain higher derivatives of the jacobian matrix or more than one first derivative of the jacobian matrix in the same term , summed together , give zero .",
    "+   + to clarify the use of the chain - rule and the algebraic issues discussed above , we present two examples . in example 1",
    "we use the first order term of the bias in a general vector estimate to derive the first order term of @xmath216 .",
    "it is useful to recall that the expansion of @xmath216 and the expansion of @xmath222 can be different only from the second order on , so this example also describes a derivation of the first order of the covariance matrix @xmath222 .",
    "following the same approach , the second order term of the error correlation matrix expansion can be derived from equation ( [ biastwo ] ) and the second order covariance matrix can also be derived if we also use ( [ biasone ] ) and ( [ conversion ] ) .",
    "+ in example 2 we illustrate the way the chain - rule can still be used , if the available expression of the bias expansion is explicitly computed for a particular data model . in particular , we derive the second order mean square error and the second order variance from the second order bias in two scalar gaussian models . in example 2",
    "we also illustrate the simplification introduced above for the algebra involved in the conversion of the derivatives .",
    "+   + example 1 + using the bartlett identities ( @xcite,@xcite ) , we rewrite the first order bias , given by equation ( @xmath248 ) , as @xmath249=-\\frac{1}{2}i^{rj}i^{lp}\\label{trichecone}(\\upsilon _ { j , l , p}+\\upsilon _ { j , lp})\\end{aligned}\\ ] ] from equation ( [ trichecone ] ) , @xmath250 can be computed by means of the chain - rule by replacing the derivatives with respect to the components of @xmath7 with derivatives with respect to the components of @xmath189 , where @xmath189 is given in equation ( [ variab ] ) and using the corresponding jacobian matrix .",
    "the tensors in equation ( [ trichecone ] ) become @xmath251=j_{lr}i_{rs}(\\vartheta)(j^t)_{sp}=\\\\ & = & j_{lr}e [ \\frac{\\partial^2 l } { \\partial \\vartheta_i \\partial \\vartheta_j}]\\label{tricheco2}(j^t)_{sp}\\\\    { i^{lp}}(\\xi)&=&{((j^{-1})^t)_{lr}}{i^{rs}}(\\vartheta)(j^{-1})_{sp}\\\\ \\upsilon _ { j , l , p}\\label{tricheco3}(\\xi)&=&j_{j\\alpha}j_{l\\beta}j_{p\\gamma}\\upsilon _ { \\alpha,\\beta,\\gamma}(\\vartheta)\\\\ \\upsilon _ { j , lp}(\\xi)&=&\\label{tricheco4}j_{j\\alpha}j_{l\\beta}(j_{p\\gamma}\\upsilon _ { \\alpha,\\beta,\\gamma}(\\vartheta)+ \\frac{\\partial j_{p \\gamma}}{\\partial \\beta } \\upsilon _ { \\alpha,\\gamma}(\\vartheta))\\end{aligned}\\ ] ] where we have specified in the bracket beside the tensors the dependence on the parameter sets . inserting these expressions in equations ( [ trichecone ] ) and observing that equation ( [ 21 ] ) can be expressed in the form @xmath252 the first order term of the error correlation matrix can then be obtained as @xmath253=\\frac{1}{2 } i^{\\alpha \\beta}\\frac{\\partial   ( j^{-1})_{\\alpha i}}{\\partial \\vartheta_{\\beta}}=\\frac{1}{2 } i^{\\alpha \\beta}\\gamma_{\\alpha \\beta}=i^{ij}\\end{aligned}\\ ] ] where we have introduced the tensor @xmath254 + example 2 + in this example we determine the second order variance and mean square error from the second order bias for two cases of parameter dependence for the scalar gaussian density @xmath255 as a direct application of equation ( [ simplechain ] ) . + in the case where c does not depend on the parameters ( @xmath256 ) the second order bias can be derived using the scalar version of equation ( [ biastwo ] ) and of the tensors @xmath257 defined in equation ( [ upsilondef ] ) . for this parameter dependence , the @xmath83 order for @xmath258 ( equation ( [ 12 ] ) ) is zero , and the second order bias can be directly obtained also from equation ( [ 13 ] ) the result is @xmath259=\\frac{c^2}{(\\dot{\\mu})^8}[\\frac{5}{4}\\dddot{\\mu}\\ddot{\\mu}(\\dot{\\mu})^2-\\frac{1}{8}\\ddddot{\\mu}(\\dot{\\mu})^3-\\frac{15}{8}\\dot{\\mu}(\\ddot{\\mu})^3]$ ] . applying the chain - rule , the second order mean square error for the estimation of @xmath260 becomes @xmath261=\\frac{15{\\ddot{\\mu}}^2 } { 4{\\dot{\\mu}}^6 } -\\frac{\\dddot{\\mu}}{{\\dot{\\mu}}^5 } $ ] , where the full conversions of the derivatives are given by @xmath262 . by means of these conversions and the expression of the second order bias , it can be observed that among the 18 terms that are in principle generated by the chain - rule , only 6 contribute to the second order mean square error .",
    "+   + in the following we show that the expansion in the inverse of the sample size is equivalent to an expansion in @xmath263",
    ". the derivation of the asymptotic orders in @xmath84 would be the same for an expansion in any quantity if ( [ upsilondef ] ) and ( [ 5 ] ) can be derived for a certain quantity @xmath61 instead of the sample size n. in this section we illustrate indeed that this is the case where the signal to noise ratio for a set of scalar data distributed according to a deterministic gaussian pdf takes the role of the sample size .",
    "the probability distribution and parameter dependent part of the likelihood function are given by : @xmath264 we also define the signal to noise ratio for this example as @xmath265 following standard practice for scalar deterministic gaussian data .",
    "we can obtain :        from ( [ gaussian_expect_la ] ) it becomes obvious that @xmath268 is proportional to not only the sample size , @xmath82 , but also to the @xmath61 .",
    "the term inside the square brackets is simply a sum over normalized derivatives of the mean and contains information about the shape of the signal . in the above equations we use :        @xmath271 & = & 2^p \\mu_{\\alpha_1 } \\ldots \\mu_{\\alpha_p } \\frac{1}{\\sigma^{2p } } e[\\sum_{i = 1}^{n}(x_i - \\mu(\\vartheta))^p ] \\nonumber \\\\ & = & 2^p ( \\frac{\\mu_{\\alpha_1}}{\\mu } ) \\ldots ( \\frac{\\mu_{\\alpha_p}}{\\mu } ) \\frac{\\mu^p}{\\sigma^{2p } } \\sigma^p \\frac{(2\\frac{p}{2})!}{2^{\\frac{p}{2}}(\\frac{p}{2})!}n^{\\frac{p}{2 } } \\nonumber \\\\ & = & ( \\frac{\\mu^2}{\\sigma^2})^{\\frac{p}{2 } } \\ldots ( \\frac{\\mu_{\\alpha_1}}{\\mu } ) \\ldots ( \\frac{\\mu_{\\alpha_p}}{\\mu})n^{\\frac{p}{2 } } \\nonumber \\end{aligned}\\ ] ]        note that for deterministic gaussian data , the non - integer asymptotic orders of the bias are zero and the integer orders are equal to @xmath274 etc .",
    "this is sufficient to prove that the orders in @xmath275 of the bias expansion are also orders in @xmath276 .",
    "this also holds for the variance expansion . a similar ,",
    "although longer proof can be written for the snr definition provided in [ snrtoa ] .",
    "abbott et al . , rep .",
    "phys * 72 * , 076901 ( 2009 ) h. grote , class .",
    "quantum grav . * 25 * , 114043 ( 2008 ) f. acernese et al . , class",
    ". quantum grav . * 23 * , 563 ( 2006 ) v. kalogera , k. belczynski , c. kim richard w. oshaughnessy b. willems , phys . rept . *",
    "442 * , 75 - 108 ( 2006 ) abramowitz and stegun _ handbook of mathematical functions _",
    "dover ( 1965 ) o.e .",
    "barndoff - nielsen and d.r .",
    "cox , int .",
    ", * 52 * , 309 - 326 ( 1984 ) o.e .",
    "barndoff - nielsen and d.r .",
    "asymptotic techniques for use in statistics _",
    ", chapman - hall , london ( 1989 ) o.e .",
    "barndoff - nielsen and d.r .",
    "cox_inference and asymptotic _ , chapman - hall , london , ( 1994 ) o.e .",
    "barndoff - nielsen , d.r.cox and n. reid , _ the role of differential geometry in statistics _ ( 1986 ) o.e .",
    "barndoff - nielsen and p.a .",
    "mc cullagh , biometrika , 80 .",
    "321 - 8 ( 1993 ) o.e .",
    "barndoff - nielsen , w.s .",
    "kendall , m.n.m .",
    "lieshout , _ stocastic geometry : likelihood and computation _ boca raton , fla . :",
    "chapman and hall / crc , c ( 1999 ) m.s .",
    "bartlett , biometrika , 40 , 12 - 19.(1953 ) m.s .",
    "bartlett , biometrika , 40 , 306 - 317.(1953 ) k.o.bowman and l.r.shenton , commun .",
    "theory meth .",
    "27 ( 11 ) 2743 - 2760 ( 1998 ) k.o.bowman , l.r.shenton , ann.inst.statist .",
    "44 , no 781 - 798 ( 1992 ) k.o .",
    "bowman , l.r .",
    "shenton , commun stat - simul c : ( 6 ) 697 - 710 ( 1983 ) haldane and smith , biometrika , 43 , 96 - 103 ( 1956 ) p.j .",
    "hanlon , r. stanley , j.r .",
    "stembridge , contemporary mathematics , vol 138 , ( 1992 ) d.n lawley , biometrika , 43 , 295 - 303 , ( 1956 ) p. mccullagh , biometrika , 71 , 461 - 76.(1984 ) p. mccullagh , _ tensor methods in statistics _ ,",
    "chapman and hall ( 1987 ) p. mccullagh , d.r .",
    "cox , ann stat 14 ,",
    "1419 - 1430 ( 1986 ) pierce d.a . and peters d. , biometrika , 81 , 1 , pages . 1 - 10 ( 1994 )",
    "rao c. , _ linear statistical inference and its application _ , wiley , new york , ( 1966 ) riordan j. , _ combinatorial identities _ , wiley ( 1968 ) shenton l.r . ,",
    "k.o.bowman , _ higher moments of a maximum - likelihood estimate _ j.roy.stat.soc .",
    "( 1963 ) shenton l.r . ,",
    "k.o.bowman , _ maximum likelihood estimation in small samples _ ( 1977 ) shenton l.r . , k.o.bowman,_the dark side of asymptotics and moment series _ , sankhya , ( 1986 ) shenton l.r . , k.o.bowman,_the approximate distribution of four moment statistics from type iii distributions_,commun .",
    "theory meth.1511 - 1579 ( 1990 ) shenton l.r . , k.o.bowman,_asymptotic kurtotis for maximum likelihood estimators _ , commun .",
    "theory meth .",
    "28 ( 11 ) 2641 - 2754 ( 1999 ) shenton l.r .",
    ", bowman k.o . and",
    "sheehan , _ sampling moments associated with univariate distributions _",
    "stanley r. , _ enumerative combinatorics _ , cambridge press,(1993 ) m. vallisneri , arxiv : gr - qc/0703086 ( 2007 ) m. vallisneri , phys .",
    "d 77 , 042001 ( 2008 ) c. cutler , and e. flanagan phys .",
    "d 49,6 2658 ( 1994 ) l.s.finn phys .",
    "d 49,6 2658 ( 1992 ) a.thode , m. zanolin , e. naftali , i. ingram , p. ratilal and n.c .",
    "makris , j. acoust .",
    "* 112 * ( 5 ) ( 2001 ) m. zanolin , i. ingram , a.thode and n.c .",
    "makris , j. acoust .",
    "* 116 * ( 4 ) ( 2004 ) m. zanonlin , n. makris , mit tech .",
    "report ( 2001 ) e. naftali and n.c .",
    "makris , j. acoust .",
    "* 110 * ( 4 ) ( 2001 ) waterman r.p .",
    ", lindsay b.g . _ a simple and accurate method for approximate conditional inference applied to exponential family models _ j.r.statist .",
    "soc.b 58,no 1 , pages 177 - 188 ( 1996 ) wlodzinierz bryc,_the normal distribution : characterization with applications_,springer verlag ( 1995 ) .",
    "l. blanchet , g. faye , b.r .",
    "iyer and b. joguet , phys .",
    "d * 65 * , 061501(r ) ( 2002 ) t. damour , b.r .",
    "iyer , and b.s .",
    "sathyaprakash , phys .",
    "d * 63 * , 044023 ( 2001 ) t. damour , b.r .",
    "iyer , and b.s .",
    "sathyaprakash , phys .",
    "d * 66 * , 027502 ( 2002 ) k.g arun , b. r. iyer , b.s sathyaprakash and p.a .",
    "sundararajan , phys .",
    "d * 71 * , 084008 - 1 ( 2005 ) k.g arun , b. r. iyer , b.s sathyaprakash and p.a .",
    "sundararajan , phys .",
    "d * 72 * , 069903 ( e ) ( 2005 ) r. balasubramanian , b.s .",
    "sathyaprakash and s.v .",
    "dhurandhar , phys .",
    "d * 53 * , 3033 ( 1996 ) r. balasubramanian and s.v .",
    "dhurandhar , phys .",
    "d * 57 * , 3408 ( 1998 ) l.s . finn and d.f .",
    "chernoff , phys .",
    "d * 47*,2198 ( 1993 ) p. jaranowski and a. krolak , phys .",
    "d * 47 * 1723 ( 1994 ) d. nicholson , a. vecchio , phys .",
    "d * 57 * , 4588 ( 1998 ) c. cutler , e. e. flanagan , phys .",
    "d * 49 * , 2658 ( 1994 ) a.willsky , j.shapiro ee6432 mit technical note , sept .",
    "1999 e. poisson , c.m .",
    "will , phys .",
    "d * 52 * , 848 ( 1995 ) a. krlak , k.d .",
    "kokkotas and g. schfer , s.fairhrst new j.phys . 11 123006 ( 2009 )",
    "d * 52 * , 2089 ( 1995 ) p. ajith and s. bose , arxiv 0901.4936v4 ( 2009 ) t. cokelaer , class .",
    "quantum grav . * 25 * , 184007 ( 2008 )"
  ],
  "abstract_text": [
    "<S> in this paper we apply to gravitational waves ( gw ) from the merger phase of binary systems a recently derived frequentist methodology to calculate analytically the error for a maximum likelihood estimate ( mle ) of physical parameters . </S>",
    "<S> we use expansions of the covariance and the bias of a mle estimate in terms of inverse powers of snrs where the square root of the first order in the covariance expansion is the cramer rao lower bound ( crlb ) . </S>",
    "<S> we evaluate the expansions , for the first time , for gw signals in noises of gw interferometers . </S>",
    "<S> the examples are limited to a single , optimally oriented , interferometer . </S>",
    "<S> we also compare the error estimates using the first two orders of the expansions with existing numerical monte carlo simulations . </S>",
    "<S> the first two orders of the covariance allows to get error predictions closer to what is observed in numerical simulations than the crlb . </S>",
    "<S> the methodology also predicts a necessary snr to approximate the error with the crlb and provides new insight on the relationship between waveform properties , snr , dimension of the parameter space and estimation errors . </S>",
    "<S> for example the timing match filtering can achieve the crlb only if the snr is larger than the kurtosis of the gravitational wave spectrum and the necessary snr is much larger if other physical parameters are also unknown . </S>"
  ]
}