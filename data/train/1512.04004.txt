{
  "article_text": [
    "usually , many real - life systems exhibit sparse representation i.e. , their system impulse response is characterized by small number of non zero taps in the presence of large number of inactive taps .",
    "sparse systems are encountered in many important practical applications such as network and acoustic echo cancelers @xmath1-@xmath2 , hdtv channels @xmath3 , wireless multipath channels @xmath4 , underwater acoustic communications @xmath5 .",
    "the conventional system identification algorithms such as lms and nlms are sparsity agnostic i.e. , they are unaware of underlying sparsity of the system impulse response .",
    "recent studies have shown that the _ a priori _ knowledge about the system sparsity , if utilized properly by the identification algorithm , can result in substantial improvement in its estimation performance .",
    "this resulted in a flurry of research activities in the last decade or so towards developing sparsity aware adaptive filter algorithms , notable amongst them being the proportionate normalized lms ( pnlms ) algorithm @xmath6 and its variants @xmath7-@xmath8 . unlike the nlms , the weighted euclidean norm of the input vector presented in proportionate - type nlms ( pt - nlms ) can not be computed recursively due to the presence of gain matrix @xmath9 , which varies at each time instance @xmath10 .",
    "computation of this weighted euclidean norm of the input vector requires requires @xmath11 multiplications and @xmath12 additions in each iteration that limits the throughput for real - time applications . in this paper , we present the performance analysis of proportionate - type lms ( pt - lms ) algorithm that is more suitable for real time vlsi applications .",
    "we consider the problem of identifying an unknown system ( supposed to be sparse ) , modeled by the @xmath13 tap coefficient vector @xmath14 which takes a signal @xmath15 with variance @xmath16 as the input and produces the observable output @xmath17 , where @xmath18^{t}$ ] is the input data vector at time @xmath10 , and @xmath19 is the observation noise with zero mean and variance @xmath20 which is assumed to be white and independent of @xmath21 for all @xmath22 .",
    "the pt - lms algorithm iteratively updates the filter coefficient vector @xmath23^{t}$ ] as , @xmath24 where @xmath25 is the step size , @xmath9 is a diagonal gain matrix that distributes the adaptation energy unevenly over the filter taps by modifying the step size of each tap , and @xmath26 is the filter output error .    the gain matrix @xmath9 is evaluated as , @xmath27 where , @xmath28 with , @xmath29\\ ] ] @xmath30, ... ,\\mathrm{f}[|w_{l-1}(n)|]\\ ] ] where @xmath31 is a very small , positive constant which , together with @xmath32 , ensures that @xmath33 and thus @xmath34 do not turn out to be zero for the inactive taps and thus the corresponding updation does not stall .",
    "the parameter @xmath35 is again a small positive constant employed to avoid stalling of the weight updation at the start of the iterations when the tap weight iterates are initialized to zero .",
    "the function @xmath36 $ ] is chosen differently for different pt - lms algorithms , as described in the table below . from ( 1 ) , it is easily seen that @xmath37 provides the effective step size for the @xmath38-th tap which , through the function @xmath36 $ ] , is monotonically related to @xmath39 .",
    "| l | p4.5 cm | + algorithm & @xmath36 $ ] or @xmath34 + [ 2ex ] + 1 .",
    "standard lms & @xmath36 = 1 $ ] + [ 2ex ] + 2 .",
    "plms & @xmath36 = |w_{l}(n)|$ ] + [ 2ex ] + 3 .",
    "iplms & @xmath40 @xmath41 $ ] + [ 2ex ] + 3 .",
    "@xmath25-law plms & @xmath36=ln(\\frac{1 + ( \\epsilon |w_{l}(n)|)}{1+\\epsilon})$ ] ; @xmath42 is a positive constant +",
    "in this section , we examine the convergence behavior of the proposed proportionate - type least mean square algorithm .      by denoting @xmath43 , from the recursion for the weight error vector of the pt - lms algorithm",
    "can be written as follows : @xmath44 \\widetilde{\\textbf{w}}(n )   -\\mu \\hspace{0.2em } \\textbf{g}(n ) \\hspace{0.2em } \\textbf{u}(n ) \\hspace{0.2em } v(n ) \\end{split}\\ ] ] the equation forms the basis for the performance analysis of the pt - lms algorithm . using the statistical independence between @xmath45 and @xmath46 ( i.e. ,  independence assumption \" ) , and recalling that @xmath19 is zero - mean i. i. d random variable which is independent of @xmath46 and thus of @xmath47",
    ", one can write @xmath48&=\\big[\\textbf{i}_{l}-\\mu \\hspace{0.2em } e\\big[\\textbf{g}(n ) \\hspace{0.2em } \\textbf{u}(n ) \\hspace{0.2em } \\textbf{u}^{t}(n ) \\big]\\big ] e[\\widetilde{\\textbf{w}}(n ) ] \\end{split}\\ ] ] when compared to @xmath45 as @xmath9 changes slowly with time ( nearly convergence ) , we can assume @xmath9 is independent of @xmath46 .",
    "therefore , the above equation can be rewritten as , @xmath49&=\\big[\\textbf{i}_{l}-\\mu \\hspace{0.2em } \\overline{\\textbf{g } } \\hspace{0.2em } \\textbf{r}\\big ] e[\\widetilde{\\textbf{w}}(n ) ] \\end{split}\\ ] ] where @xmath50 . from the above result",
    ", the convergence of pt - lms family is guaranteed only if and only if @xmath51 therefore , a sufficient condition for @xmath52 to hold is @xmath53 from matrix norm inequalities , finally the condition on @xmath25 is @xmath54 for white regressor data for which @xmath55 , from @xcite we have @xmath56 .",
    "therefore , for white input signal case , a sufficient condition for @xmath52 to hold is @xmath57 .      using the statistical independence between @xmath45 and @xmath46 ( i.e. ,  independence assumption \" ) , and recalling that @xmath19 is of zero - mean and also independent of @xmath46 and thus of @xmath47 , from @xmath58 , using energy conservation approach @xcite , the mean square of the weight error vector @xmath47 , weighted by any positive semi - definite matrix @xmath59 that we are free to choose , satisfies the following relation : @xmath60 \\hspace{0.2em } e\\big[\\textbf{u}^{t}(n ) \\textbf{g}(n ) \\sigma \\textbf{g}(n ) \\textbf{u}(n ) \\big ] \\end{split}\\ ] ] where @xmath61 the relations presented in @xmath62 and @xmath63 are useful to derive the condition for mean square stability and expressions for mse and msd .",
    "to extract the matrix @xmath59 from the expectation terms , a weighted variance relation is introduced by using @xmath64 column vectors : @xmath65 where @xmath66 denotes the vector operator . in addition , @xmath66 is also used to recover the original matrix @xmath67 from @xmath68 .",
    "one property of the @xmath66 operator when working with the kronecker product @xcite is used in this work , namely , @xmath69 where @xmath70 denotes the kronecker product of two matrices .",
    "using @xmath71 to @xmath63 after vectorization , a linear relation between the corresponding vectors @xmath72 is formulated as follows : @xmath73 where the coefficient matrix @xmath74 is @xmath75 and defined as @xmath76 with @xmath77 $ ] .",
    "the term @xmath78 \\hspace{0.2em } e\\big[\\textbf{u}^{t}(n ) \\textbf{g}(n ) \\sigma \\textbf{g}(n ) \\textbf{u}(n ) \\big]$ ] can be written as @xmath79 \\hspace{0.2em } e\\big[\\textbf{u}^{t}(n ) \\textbf{g}(n ) \\boldsymbol{\\sigma } \\textbf{g}(n ) \\textbf{u}(n ) \\big]&=\\sigma^{2}_{v } \\hspace{0.2em } tr\\bigg(e\\big [ \\textbf{g}(n ) \\hspace{0.2em } \\textbf{u}(n)\\textbf{u}^{t}(n ) \\hspace{0.2em } \\textbf{g}(n ) \\big ] \\boldsymbol{\\sigma }   \\bigg)\\\\ & = \\sigma^{2}_{v } \\hspace{0.2em } \\boldsymbol{\\gamma}^{t } \\hspace{0.2em } \\boldsymbol{\\sigma } \\end{split}\\ ] ] where @xmath80\\big\\}\\\\ & = e \\big(\\textbf{g } \\otimes \\textbf{g } \\big ) \\hspace{0.2em}\\boldsymbol{\\gamma}_{r } \\end{split}\\ ] ] with @xmath81 . using these results",
    "the recursion presented in @xmath62 can be rewritten as @xmath82 the pt - lms algorithms are mean square stable if , and only if , the matrix @xmath74 is stable .",
    "iterating the above recursion starting from @xmath83 , we get @xmath84 therefore , by selecting @xmath85 , we can relate @xmath86 and @xmath87 as follows : @xmath88\\textbf{f}^{n } \\hspace{0.2em } \\boldsymbol{\\sigma } } + \\mu^{2 } \\hspace{0.2em } \\sigma^{2}_{v } \\hspace{0.2em } \\boldsymbol{\\gamma}^{t } \\hspace{0.1em }   \\textbf{f}^{n}\\boldsymbol{\\sigma } \\end{split}\\ ] ] the weighted variance relation is useful to characterize the transient behavior of the pt - lms family .",
    "it is also useful to examine the steady - state msd , which is given as follows : @xmath89 by selecting @xmath90 , the steady - state msd is given as @xmath91 let @xmath92 + \\big [ \\big ( \\textbf{r } \\otimes \\textbf{i } \\big ) \\hspace{0.2em } \\big ( \\overline{\\textbf{g } } \\otimes \\textbf{i } \\big ) \\big]$ ] and @xmath93 so that @xmath94 .    from @xcite , the convergence in the mean square sense of pt - lms family",
    "is guaranteed for any @xmath25 in the range @xmath95 where @xmath96 $ ] .",
    "here the simulation results are presented for system identification example .",
    "first , the proposed algorithm has been simulated for identifying the system ( @xmath14 ) of length @xmath97 having @xmath98 active taps with the remaining coefficients being inactive as shown in fig . 1 .        simulations were performed using zero mean , gaussian white noise with unit variance ( @xmath99 ) . the observation noise @xmath19 was taken to be zero - mean gaussian white noise with variance @xmath100 .",
    "the performance of the proposed pt - lms algorithm was compared with the existing pnlms and lms algorithms by plotting the respective learning curves ( i.e. , normalized msd in db vs no . of iterations ) which are shown in fig .",
    "2 . the simulation results shown in fig .",
    "@xmath101 are obtained by plotting the normalized msd against the iteration index @xmath10 , by averaging over @xmath102 experiments .",
    "secondly , for theoretical performance comparison purpose , we considered a sparse system of length @xmath103 having @xmath101 active taps with the remaining coefficients being inactive .",
    "the input was taken to be zero mean , gaussian white noise with unit variance ( @xmath99 ) .",
    "the observation noise @xmath19 was taken to be zero - mean gaussian white noise with variance @xmath100 .",
    "theoretical and simulation results were compared by plotting the steady - state normalized msd in db vs step size value ( @xmath25 ) which are shown in fig .",
    "3 . from fig .",
    "3 , we can see the analytical results are coinciding with simulation results .",
    "we presented the performance analysis of proportionate - type lms ( pt - lms ) algorithm that is more suitable for real time vlsi applications .",
    "the convergence analysis of pt - lms algorithm is studied in mean and mean - square sense .",
    "v. v. krishna , j. rayala and b. slade , `` algorithmic and implementation apsects of echo cancellation in packet voice networks '' , in _ proc .",
    "36th asilomar conf .",
    "signals , syst .",
    "comput . , _ vol .",
    "2 , pp . 1252 - 1257 , 2002 .",
    "r. l. das and m. chakraborty , `` on convergence of proportionate - type normalized least mean square algorithms , '' in _ ieee trans . on circuits and systems",
    "ii : express briefs , _ vol .",
    "491 - 495 , may , 2015 ."
  ],
  "abstract_text": [
    "<S> in this paper , we present the convergence analysis of proportionate - type least mean square ( pt - lms ) algorithm that identifies the sparse system effectively and more suitable for real time vlsi applications . </S>",
    "<S> both first and second order convergence analysis of pt - lms algorithm is studied . </S>",
    "<S> optimum convergence behavior of pt - lms algorithm is studied from the second order convergence analysis provided in this paper . </S>",
    "<S> simulation results were conducted to verify the analytical results .    * index terms*-sparse systems , @xmath0 norm , compressive sensing , excess mean square error . </S>"
  ]
}