{
  "article_text": [
    "we address the simple problem of displaying an empirical probability density ( pd ) @xmath0 from data for a _ continuous _ variable @xmath1 .",
    "commonly this is done using histograms .",
    "this is appropriate when @xmath1 is discrete , because there is then a natural scale .",
    "but in case of a continuous variable @xmath1 , one is faced with choosing binsizes .",
    "this is a frustrated problem : one would like to keep the binsize small for a high resolution , but big to suppress statistical fluctuations . here",
    "we present a method @xcite to by - pass the problem .",
    "it is based on the cumulative distribution function ( cdf ) @xmath2 given a time series of @xmath3 real numbers ( data ) , a parameter free empirical estimate ( ecdf ) , is well - known : the step function @xmath4 defined by increasing by @xmath5 at each data point .",
    "this does not help directly in getting an estimate of the probability density , because the derivative is a sum of dirac delta functions .",
    "one needs some kind of interpolation of the cdf .",
    "this is no fun , as one has to decide whether the interpolation of 2 , 3 , 4 , or @xmath6 points will work best .",
    "in contrast , plotting a histogram is simple and robust , but not a smooth function .",
    "our way out relies on _ fourier expansion _ of the ecdf @xmath7 this leads to the desired smooth approximation as long as the expansion is sufficiently short , but will imitate every wiggle of the data , when carried too far .",
    "therefore , one needs a cut - off criterion .",
    "we base this on the _ kolmogorov test _ , which tells us whether the difference between the ecdf and an analytical approximation of the cdf is explained by chance .",
    "_ fortran code _ for our procedure @xcite is available from the cpc library .",
    "assume we generate @xmath3 random numbers @xmath8 , @xmath9 , @xmath10 we re - arrange the @xmath11 in increasing order ( @xmath12 a permutation of @xmath13 ) : @xmath14 an estimator for the distribution function @xmath15 is the ecdf @xmath16 and by definition @xmath17 , @xmath18 .",
    "[ fig_gcdf100 ] shows an ecdf from 100 gaussian distributed random numbers generated for the probability density @xmath19 together with the exact cdf .",
    "ecdf from 100 gaussian distributed random numbers together with the exact cdf . ]",
    "the cdf is in this case determined by the error function : @xmath20 the probability density of events is encoded in the slope of the ecdf .",
    "this makes it often difficult to read off high probability regions and , in particular , the median .",
    "this can be improved by switching to the peaked cdf @xcite : @xmath21 by construction the maximum of the peaked cdf is at the median @xmath22  and @xmath23 .",
    "therefore , @xmath24 has two advantages : the median is clearly exhibited and the accuracy of the ordinate is doubled .",
    "it looks a bit like a pd , but is in essence still the integrated  pd .",
    "an example from 10,000 gaussian random numbers is shown in fig .",
    "[ fig_gpcdf10000 ]",
    ".     peaked ecdf from the 10,000 gaussian random numbers versus exact gaussian peaked cdf .",
    "the arrows indicate 70% and 95% confidence intervals . ]",
    "do empirical and exact cdfs of our two figures agree ?",
    "the kolmogorov test answers this question ( for a review see @xcite ) .",
    "it returns the _ probability @xmath25 , that the difference between the analytical cdf and an ecdf from statistically independent data is due to chance .",
    "_ if the analytical cdf is known and the data are sampled from this distribution , @xmath25 is a uniformly distributed random variable in the range @xmath26 .",
    "turned around , if one is not sure about the exact cdf , or the data , or both , and @xmath25 is small ( say , @xmath27 ) one concludes that the difference between the proposed cdf and the data is presumably not due to chance .",
    "kolmogorov s ingenious test relies just on the maximum difference between the ecdf and the cdf : @xmath28 the test yields , respectively , @xmath29 and @xmath30 for the samples used in fig .",
    "[ fig_gcdf100 ] and  [ fig_gpcdf10000 ]",
    ". both values signal consistency between cdf and data .",
    "our method @xcite to construct an empirical probability density ( epd ) from an ecdf consists of two steps :    1 .   define as an initial approximation to @xmath4 a differentiable , monotonically increasing function @xmath31 .",
    "fourier expand the remainder until the kolmogorov test yields @xmath32 ( there may be some flexibility in lowering @xmath33 ) .    for @xmath31",
    "we require @xmath34 where @xmath35 $ ] has to lie within the range of the data .",
    "for pds with support on a compact interval , or with fast fall - off like for a gaussian distribution , the natural choice is @xmath36 and @xmath37 . in case of slow fall - off , like for a cauchy distribution , or other distributions with outliers , one has to restrict the analysis to @xmath35 $ ] regions , which are well populated by data .",
    "we denote the ecdf of the range @xmath35 $ ] by @xmath38 . as for @xmath31 , by construction @xmath39 for @xmath40 and 1 for @xmath41 .",
    "our aim is to construct a pd estimator @xmath42 from @xmath38 . in the following",
    "we restrict our choice of @xmath31 to the straight line , @xmath43 which keeps the approach simple .",
    "more elaborate definitions will likely give improvements in a number of situations , but may discourage applications .",
    "once @xmath31 is defined , the remainder of the ecdf is given by @xmath44 we expand @xmath45 into the fourier series @xmath46 the cosine terms are not present due to the boundary conditions @xmath47 .",
    "the fourier coefficients follow from @xmath48 in our case @xmath45 is the difference of a step function and a linear function .",
    "the integrals over the flat regions of the step function are easily calculated , and the @xmath49 obtained by adding them up .",
    "the fourier expansion is useless for too large values of @xmath50 , because it will then reproduce all statistical fluctuations of the data . to get around this problem",
    ", we perform the kolmogorov test first between @xmath38 and @xmath31 ( @xmath51 ) , and then each time @xmath50 is incremented from @xmath52 .",
    "once @xmath53 is reached , we know that the information left in the data is statistical noise and the expansion is terminated .",
    "the thus obtained smooth estimate of the cdf , @xmath54 yields @xmath42 by differentiation .",
    "we attach _ error bars _ to the estimate of the pd by dividing the ( unsorted ) original data into _ jackknife _ blocks and repeat the analysis for each block . comparing the thus obtained function values ,",
    "error bars follow in the usual jackknife way .",
    "an example for the gaussian distribution follows .",
    "see @xcite for more examples : the cauchy distribution and autocorrelated data from u(1 ) lattice gauge theory .",
    "histogram of 51 bins for 2@xmath55000 random numbers generated according to the gaussian distribution . ]    the histogram for the gaussian distribution is shown in fig .",
    "[ fig_ghist ] ( the error bars follow from the variance @xmath56 of the bimodal distribution with @xmath57 ) .",
    "estimate @xmath58 for the data of the previous figure . ]",
    "[ fig_gj ] gives our estimate @xmath58 of the pd obtained from the same data with the described method .",
    "we used @xmath36 and @xmath37 .",
    "@xmath59 was reached with @xmath60 ( @xmath61 with @xmath62 ) .",
    "twenty jackknife blocks were used to calculate the error bars .",
    "based on fourier expansion and kolmogorov tests , we introduced a method for constructing continuous probability density functions from data .",
    "we did not develop a statistically rigorous approach .",
    "we address physicists and others , who do not hesitate to use whatever works .",
    "our results were obtained with a straight line as initial approximation for the cdf .",
    "there is certainly space for improvement at the price of giving up some of the simplicity . with our @xmath63 rule",
    ", we are slightly overexpanding the fourier expansion . in the average @xmath25 should be 1/2 , but all our values are @xmath64 .",
    "that gives some flexibility to lower @xmath33 when the @xmath50 of the fourier expansion appears to be too large .",
    "there are many open questions .",
    "given the initial approximation , we construct a smooth fourier expansion of the remainder , that is consistent with the data , using the ordering in which the long wave lengths modes come first . obviously , the result of this procedure is not the only analytical function , which is consistent with the data .",
    "which ordering of the serious expansion or other complete function system gives the smoothest approximation ( smallest number of terms ) consistent with the data ?",
    "do systems of monotonically increasing functions exist , which are complete for the expansion of monotonically increasing functions ?    _ kernel density estimates _",
    "@xcite are in spirit similar ( but by no means identical ) to our method .",
    "a comparison remains to be carried out .",
    "this work was in part supported by the us department of energy under contract de - fg02 - 97er41022 .",
    "more references can be found in wikipedia under _ kernel density estimation_. the _ earth mover s distance _ might be of interest as a replacement of the kolmogorov test , if one likes to attempt a generalization of the method to more than one dimension ."
  ],
  "abstract_text": [
    "<S> based on cumulative distribution functions , fourier series expansion and kolmogorov tests , we present a simple method to display probability densities for data drawn from a continuous distribution . </S>",
    "<S> it is often more efficient than using histograms .    </S>",
    "<S> display of data , histograms , probability densities </S>"
  ]
}