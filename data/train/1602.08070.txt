{
  "article_text": [
    "multifactor risk models are a popular risk management tool , e.g. , in portfolio optimization . for stock portfolios , in their most popular incarnations , multifactor risk models are usually constructed based on industry and style risk factors . however , in some cases such constructions are unavailable , e.g. , because any industry classification ( or similar ) is lacking , any relevant style factors are impossible to define , etc .",
    "in fact , this is generally the case when the underlying returns are not for equities but some other  instruments \" , e.g. , quantitative trading alphas ( expected returns ) .    in such cases one usually resorts to statistical risk models",
    ". often times these are thought of in the context of principal components of a sample covariance ( or correlation ) matrix of returns .",
    "more generally , one can think of statistical risk models as constructed solely based on the time series of the underlying returns and no additional information .",
    "the purpose of these notes is to provide a simple and pedagogical discussion of statistical risk models oriented toward practical applications .    in section [ sec.2 ]",
    "we set up our discussion by discussing the sample covariance matrix , generalities of factor models , the requirement that factor models reproduce in - sample variances , and how a @xmath4-factor statistical risk model can be simply constructed by starting from the sample covariance ( or correlation ) matrix , writing down its spectral representation via principal components , truncating the sum by keeping only the first @xmath4 principal components , and compensating for the deficit in the variances ( i.e. , on the diagonal of the resultant matrix ) by adding specific ( idiosyncratic ) risk .",
    "this ( generally ) results in a positive - definite ( and thus invertible ) risk model covariance matrix so long as @xmath5 , where @xmath6 is the number of observations in the time series .",
    "this holds even if @xmath7 , in which case the sample covariance matrix is singular . in fact , one of the main motivations for considering factor models in the first instance is that in most practical applications @xmath8 ( and often @xmath9 ) , and even if @xmath10 , in which case the sample covariance matrix is nonsingular , it is still out - of - sample unstable unless @xmath11 , which is seldom ( if ever ) the case in practice .",
    "factor models are intended to reduce this instability to a degree .",
    "the beauty of the statistical risk model construction is its simplicity .",
    "however , one must fix the number of risk factors @xmath4 .",
    "we discuss two simple methods for fixing @xmath4 in section [ sub.fix.k ] ( with variations ) .",
    "one is that of @xcite .",
    "another , very different looking method , is based on our adaptation of erank ( effective rank ) of @xcite and yields results similar to ( and further validates ) that of @xcite .",
    "we use intraday alphas of @xcite and backtest these methods out - of - sample .",
    "the method of @xcite backtests better .",
    "we give r source code for computing a @xmath4-factor statistical risk model with @xmath4 fixed via the aforementioned two methods ( with variations ) in appendix [ app.a ] .",
    ", appendix [ app.b ] and appendix [ app.c ] is not written to be ",
    "fancy \" or optimized for speed or in any other way .",
    "its sole purpose is to illustrate the algorithms described in the main text in a simple - to - understand fashion .",
    "see appendix [ app.d ] for some legalese . ]    in section [ sub.pc ] we discuss how to compute principal components based on the returns .",
    "nave \" method is the power iterations method , which is applicable to more general matrices . however , it requires iterations and is computationally costly .. ] because here we are dealing with sample covariance matrices , there is a simpler and faster way of computing principal components when @xmath12 that does not require any costly iterations and involves only @xmath13 operations . we discuss this method in detail in section [ sub.pc ] and give r source code for it in appendix [ app.c ] .",
    "the main purpose of this exercise is to set up our further discussion in section [ sub.pc ] , where we explain that statistical risk models are simply certain deformations of the sample covariance matrix .",
    "we then also discuss  nontraditional \" statistical risk models such as shrinkage @xcite , which are also deformations of the sample covariance matrix , but involve @xmath14 principal components as opposed to @xmath5 principal components . generally , ",
    "nontraditional \" models underperform .",
    "we then take this a step further and explain that optimization using a statistical risk model is well - approximated by a weighted regression , where the regression is over the factor loadings matrix ( i.e. , the @xmath4 principal components ) , and the weights are inverse specific variances .",
    "more precisely , this holds when the number of underlying returns @xmath15 , which is the case in most applications .",
    "in fact , optimization reduces to a weighted regression for @xmath15 in a wider class of risk models that lack any  clustering \" structure ( we clarify the meaning of this statement in section [ sub.pc ] ) .",
    "we briefly conclude in section [ sec.conc ] , where we discuss additional backtests , etc .",
    "so , we have @xmath16 instruments ( e.g. , stocks ) with the time series of returns .",
    "each time series contains @xmath6 observations corresponding to times @xmath17 , and we will denote our returns as @xmath18 , where @xmath19 and @xmath20 ( @xmath21 is the most recent observation ) .",
    "the sample covariance matrix ( scm ) is given by in the denominator vs. the maximum likelihood estimate with @xmath6 in the denominator is immaterial ; in most applications @xmath22 . ]",
    "@xmath23 where @xmath24 are serially demeaned returns ; @xmath25 .",
    "we are interested in cases where @xmath7 , in fact , @xmath12 . when @xmath7 , @xmath26 is singular : we have @xmath27 , so only @xmath14 columns of the matrix @xmath28 are linearly independent .",
    "let us eliminate the last column : @xmath29 .",
    "then we can express @xmath26 via the first @xmath14 columns : @xmath30 here @xmath31 is a nonsingular @xmath32 matrix ( @xmath33 ) ; @xmath34 is a unit @xmath14-vector .",
    "note that @xmath35 is a 1-factor model ( see below ) .",
    "so , when @xmath36 , the sample covariance matrix @xmath26 is singular with @xmath14 nonzero eigenvalues . in this case we can not invert @xmath26 , which is required in , e.g. , optimization ( mean - variance optimization @xcite , sharpe ratio maximization @xcite , etc . ) . furthermore ,",
    "unless @xmath11 , which is almost never ( if ever ) the case in practical applications , the off - diagonal elements of @xmath26 ( covariances ) generally are not expected to be stable out - of - sample .",
    "in contrast , the diagonal elements ( variances ) typically are much more stable out - of - sample and can be relatively reliably computed even for @xmath12 ( which , in fact , is often the case in practical applications ) .",
    "so , we need to replace the sample covariance matrix @xmath26 by another",
    "_ constructed _ matrix  call it @xmath37  that is much more stable out - of - sample and invertible ( positive - definite ) .",
    "that is , we must build a risk model .",
    "a popular method  at least in the case of equities  for constructing a nonsingular replacement @xmath37 for @xmath26 is via a factor model : @xmath38 here : @xmath39 is the specific ( a.k.a .",
    "idiosyncratic ) risk for each return ; @xmath40 is an @xmath41 factor loadings matrix ; and @xmath42 is a @xmath43 factor covariance matrix ( fcm ) , @xmath44 . the number of factors @xmath45 to have fcm more stable than scm .",
    "and @xmath37 is positive - definite ( and invertible ) if fcm is positive - definite and all @xmath46 .",
    "the main objective of a risk model is to predict the covariance matrix out - of - sample as precisely as possible , including the out - of - sample variances .",
    "however , even though this requirement is often overlooked in practical applications , a well - built factor model had better reproduce the in - sample variances .",
    "that is , we require that the risk model variances @xmath47 be equal the in - sample variances @xmath48 : @xmath49 furthermore , as mentioned above , the @xmath16 variances @xmath48 are relatively stable out - of - sample .",
    "it is therefore the @xmath50 off - diagonal covariances , which are generally unstable out - of - sample , we must actually model .",
    "put differently , we must model the _ correlations _ @xmath51 , @xmath52 , where @xmath53 is the sample correlation matrix , whose diagonal elements @xmath54 .",
    "so , we need to replace the sample correlation matrix by another constructed matrix  let us call it @xmath55  that is much more stable out - of - sample and invertible ( positive - definite ) subject to the conditions @xmath56 once we build @xmath57 , the risk model covariance matrix is given by @xmath58 .",
    "the advantage of modeling the correlation matrix @xmath51 via @xmath57 as opposed to modeling the covariance matrix @xmath26 by @xmath37 is that the sample variances @xmath59 have a highly skewed ( quasi log - normal ) distribution , while @xmath60 are uniform . in the following , in the main text",
    "we give the source code which has an option to compute the factor model directly for @xmath26 as opposed to via @xmath51 .",
    "] we will always focus on modeling @xmath51 and for the sake of notational simplicity we will omit the twiddle on @xmath57 , i.e. , we will model @xmath61 via ( [ fac.mod ] ) subject to @xmath62 . here",
    "@xmath63 .",
    "looking at ( [ cormat ] ) , it resembles a factor model , except that it has no specific risk ( so it is singular ) .",
    "we can not simply add some specific risk ad hoc to ( [ cormat ] ) as this would violate the requirement that @xmath62 ( as the resulting @xmath47 would be greater than 1 ) .",
    "therefore , to add some specific risk , we must simultaneously reduce the diagonal contribution from the factor risk .",
    "all values of @xmath33 in the sum in ( [ cormat ] ) enter on the equal footing , in fact , we have a full @xmath64 permutational symmetry under which @xmath65 , @xmath66 , and @xmath67 is identified with @xmath68 ( @xmath69 . when reducing factor risk we must either preserve this symmetry or somehow break it .",
    "in fact , we can choose a different  but equivalent  basis , where this symmetry is not explicit and it is more evident how to  trim \" the factor risk .",
    "let @xmath70 , @xmath71 , be the principal components of @xmath51 forming an orthonormal basis @xmath72 such that the eigenvalues @xmath73 are ordered decreasingly : @xmath74 .",
    "more precisely , some eigenvalues may be degenerate .",
    "for simplicity  and this is not critical here  we will assume that all positive eigenvalues are non - degenerate . however , we can have multiple null eigenvalues .",
    "typically , the number of nonvanishing eigenvalues is @xmath14 , where , as above , @xmath6 is the number of observations in the return time series .",
    "so , we have @xmath75 this again resembles a factor model ( with a diagonal factor covariance matrix ) . however , the @xmath64 symmetry is gone and we can readily  trim \" the factor risk .",
    "this is simply done by keeping only @xmath5 first principal components in the sum in ( [ cm.pc ] ) and replacing the diagonal contribution of the dropped @xmath76 principal components via the specific risk : @xmath77 this corresponds to taking the factor loadings matrix and factor covariance matrix of the form @xmath78 this construction is nicely simple .",
    "however , what should @xmath4 be ?",
    "when @xmath79 we have @xmath80 , which is singular . therefore , we must have @xmath81 .",
    "so , what is @xmath82 ? and what is @xmath83 ( other than the evident @xmath84 )",
    "? it might be tempting to do complicated and convoluted things .",
    "we will not do this here .",
    "instead , we will follow a pragmatic approach .",
    "one simple (  minimization \" based ) algorithm was set forth in @xcite .",
    "we review it below and then give yet another simple algorithm based on erank ( effective rank ) .",
    "the idea is simple @xcite .",
    "it is based on the observation that , as @xmath4 approaches @xmath14 , @xmath85 goes to 0 ( i.e. , less and less of the total risk is attributed to the specific risk , and more and more of it is attributed to the factor risk ) , while as @xmath4 approaches 0 , @xmath86 goes to 1 ( i.e. , less and less of the total risk is attributed to the factor risk , and more and more of it is attributed to the specific risk ) .",
    "so , we can define @xmath4 as follows : @xmath87 this simple algorithm works pretty well in practical applications .",
    "is skewed ; typically , @xmath88 has a tail at higher values , while @xmath89 has a tail at lower values , and the distribution is only roughly log - normal .",
    "so @xmath4 is not ( the floor / cap of ) @xmath90 , but somewhat higher , albeit close to it .",
    "see @xcite for an illustrative example .",
    "[ fn.xi.distrib ] ]      another simple method is to set ( here @xmath91 can be replaced by @xmath92 ) @xmath93 here @xmath94 is the effective rank @xcite of a symmetric semi - positive - definite ( which suffices for our purposes here ) matrix @xmath95 .",
    "it is defined as @xmath96 where @xmath73 are the @xmath97 _ positive _ eigenvalues of @xmath95 , and @xmath98 has the meaning of the ( shannon a.k.a .",
    "spectral ) entropy @xcite , @xcite .",
    "the meaning of @xmath94 is that it is a measure of the effective dimensionality of the matrix @xmath95 , which is not necessarily the same as the number @xmath97 of its positive eigenvalues , but often is lower .",
    "this is due to the fact that many returns can be highly correlated ( which manifests itself by a large gap in the eigenvalues ) thereby further reducing the effective dimensionality of the correlation matrix .",
    "when the average correlation . since @xmath15 ,",
    "the difference is immaterial . ]",
    "@xmath99 is high , then both the  minimization \" and erank based algorithms can produce low values of @xmath4 ( including 1 ) .",
    "this is because in this case @xmath100 and there is a large gap in the eigenvalues . to circumvent this",
    ", we can define @xmath101 , where @xmath102 is defined as above via the  minimization \" or erank based algorithms for the matrix @xmath103 i.e. , we simply drop the first eigenvalue , determine the corresponding value of @xmath102 , and add 1 to it .",
    "appendix [ app.a ] provides r source code for both the  minimization \" and erank based algorithms with and without utilizing the @xmath102 based definition .",
    "let us backtest the above algorithms for fixing @xmath4 via utilizing the same backtesting procedure as in @xcite .",
    "the remainder of this subsection very closely follows most parts of section 6 of @xcite .",
    "let @xmath104 be the time series of stock prices , where @xmath19 labels the stocks , and @xmath105 labels the trading dates , with @xmath106 corresponding to the most recent date in the time series .",
    "the superscripts @xmath107 and @xmath108 ( unadjusted open and close prices ) and @xmath109 and @xmath110 ( open and close prices fully adjusted for splits and dividends ) will distinguish the corresponding prices , so , e.g. , @xmath111 is the unadjusted close price .",
    "@xmath112 is the unadjusted daily volume ( in shares ) .",
    "also , for each date @xmath113 we define the overnight return as the previous - close - to - open return : @xmath114 this return will be used in the definition of the expected return in our mean - reversion alpha .",
    "we will also need the close - to - close return @xmath115 an out - of - sample ( see below ) time series of these returns will be used in constructing the risk models .",
    "all prices in the definitions of @xmath116 and @xmath18 are fully adjusted .",
    "we assume that : i ) the portfolio is established at the open ( or adjusted @xmath117 ) , is used in computing the expected return ( via @xmath116 ) and as the establishing fill price . ]",
    "with fills at the open prices @xmath118 ; ii ) it is liquidated at the close on the same day ",
    "so this is a purely intraday alpha  with fills at the close prices @xmath111 ; and iii ) there are no transaction costs or slippage  our aim here is not to build a realistic trading strategy , but to test relative performance of various risk models and see what adds value to the alpha and what does not .",
    "the p&l for each stock @xmath119\\ ] ] where @xmath120 are the _ dollar _ holdings .",
    "the shares bought plus sold ( establishing plus liquidating trades ) for each stock on each day are computed via @xmath121 .      for the sake of simplicity , we select our universe based on the average daily dollar volume ( addv ) defined via ( note that @xmath122 is out - of - sample for each date @xmath113 ) : @xmath123 we take @xmath124 ( i.e. , one month ) , and then take our universe to be the top 2000 tickers by addv . to ensure that we do not inadvertently introduce a universe selection bias",
    ", we rebalance monthly ( every 21 trading days , to be precise ) .",
    "i.e. , we break our 5-year backtest period ( see below ) into 21-day intervals , we compute the universe using addv ( which , in turn , is computed based on the 21-day period immediately preceding such interval ) , and use this universe during the entire such interval",
    ". we do have the survivorship bias as we take the data for the universe of tickers as of 9/6/2014 that have historical pricing data on http://finance.yahoo.com ( accessed on 9/6/2014 ) for the period 8/1/2008 through 9/5/2014 .",
    "we restrict this universe to include only u.s .",
    "listed common stocks and class shares ( no otcs , preferred shares , etc . ) with bics ( bloomberg industry classification system ) sector assignments as of 9/6/2014 . however , as discussed in detail in section 7 of @xcite , the survivorship bias is not a leading effect in such backtests .",
    "we run our simulations over a period of 5 years ( more precisely , 1260 trading days going back from 9/5/2014 , inclusive ) .",
    "the annualized return - on - capital ( roc ) is computed as the average daily p&l divided by the intraday investment level @xmath125 ( with no leverage ) and multiplied by 252 .",
    "the annualized sharpe ratio ( sr ) is computed as the daily sharpe ratio multiplied by @xmath126 .",
    "cents - per - share ( cps ) is computed as the total p&l divided by the total shares traded .",
    "the optimized alphas are based on the expected returns @xmath116 optimized via sharpe ratio maximization using the risk models we are testing , i.e. , the covariance matrix @xmath127 with @xmath37 given by ( [ pc ] ) , which we compute every 21 trading days ( same as for the universe ) . for each date",
    "( we omit the index @xmath113 ) we maximize the sharpe ratio subject to the dollar neutrality constraint : @xmath128 the solution is given by @xmath129\\ ] ] where @xmath130 is the inverse of @xmath131 , and @xmath132 ( mean - reversion alpha ) is fixed via ( we set the investment level @xmath125 to $ 20 m in our backtests ) @xmath133 note that ( [ h.opt ] ) satisfies the dollar neutrality constraint ( [ d.n.opt ] ) .",
    "the simulation results are given in table [ table1 ] for @xmath4 obtained via the  minimization \" and erank based algorithms with and without utilizing the @xmath102 based definition ( see subsection [ sub.k.prime ] ) with @xmath91 and @xmath134 in ( [ eq.erank ] ) .",
    "the  minimization \" and erank methods not based on @xmath102 produce similar results , which further validates the  minimization \" method of @xcite .",
    "the slight improvement in cps in the erank method is immaterial and disappears when we impose position bounds ( which in this case are the same as trading bounds as the strategy is purely intraday ) @xmath135 where @xmath122 is addv defined in ( [ addv ] ) .",
    "these backtests use the r code in appendix c of @xcite for optimization with bounds .",
    "table [ table2 ] gives the simulation results with these bounds for all of the above cases except for the erank method with the @xmath102 based definition .",
    "in the latter case , because the typical value of @xmath4 is close to or the same as the maximum allowed @xmath136 ( see rows 5 and 6 in table [ table3 ] ) , some of the desired holdings come out to be large compared with the ( reasonable ) bounds ( [ liq ] ) ( see row 7 in table [ table3 ] ) .",
    "the  average correlation \" @xmath137 is not very high ( nor is it very low  see row 8 in table [ table3 ] ) , so it is just as well that the value of @xmath4 is not low and the @xmath102 based method is overkill , which is why it underperforms .",
    "to construct the statistical risk models , we need to compute the first @xmath14 principal components of @xmath51 .",
    "one way is to successively use the power iterations method @xcite , which usually costs more than @xmath13 operations . to compute the first principal component costs @xmath138 , where @xmath139 is the number of iterations : @xmath140_{r+1 } = { { \\widetilde v_i } \\over \\sqrt{\\sum_{j=1}^n { \\widetilde v}_i^2}}\\\\   & & { \\widetilde v_i } = \\sum_{j=1}^n \\psi_{ij}~\\left[v^{(1)}_j\\right]_{r } = { 1\\over m } \\sum_{j=1}^n \\sum_{s=1}^{m+1 } y_{is}~y_{js}~\\left[v^{(1)}_j\\right]_{r }   \\label{power.it}\\end{aligned}\\ ] ] where @xmath141 labels the iterations .",
    "rums from 1 to @xmath6 in ( [ power.it ] ) . ]",
    "each iteration costs @xmath142 operations .",
    "however , we need to compute @xmath14 principal components . this can be done as follows .",
    "let @xmath143 note that @xmath144 .",
    "the first principal component of @xmath145  which we can compute using the power iterations method  is the same as the @xmath146-th principal component @xmath147 of @xmath51 .",
    "each such computation costs @xmath148 operations .",
    "so , computing the first @xmath14 principal components costs @xmath149 operations , where @xmath150 , and typically @xmath151 .",
    "source code for this procedure is given in appendix [ app.b ] .",
    "table [ table.prin.comp ] gives an analysis for a time series with @xmath152 for @xmath153 stock returns ( close - to - close ) .",
    "the results show that @xmath151 for a reasonable computational precision , so the cost of computing the @xmath14 principal components is substantially greater than @xmath13 operations .",
    "the power iterations method is not cheap ...    the above procedure simply amounts to successively removing the already - computed principal components from @xmath51 .",
    "indeed , @xmath154 .",
    "the reason to express @xmath145 via @xmath155 in ( [ psi.y ] ) is so we have a factorized form , which leads to a much smaller number of operations required to multiply this matrix by the iteration @xmath156_{r}$ ] .",
    "note that @xmath157 , so irrespective of @xmath158_{init}$ ] , we get @xmath159 via ( [ pow.it.1 ] ) and ( [ power.it ] ) ( with @xmath160 , @xmath51 and @xmath161 replaced by @xmath159 , @xmath162 and @xmath163 ) right at the first iteration ( hence only 2 iterations in the last row in table [ table.prin.comp ] ) .",
    "however , when @xmath9 , we can compute the @xmath14 principal components of the sample correlation matrix without any costly iterations ( involving @xmath16-vectors ) and the cost is @xmath13 operations .",
    "we start with ( [ cormat ] ) .",
    "let ( in matrix notation ) @xmath164 , where @xmath165 is the cholesky decomposition of @xmath166 .",
    "( @xmath167 ; @xmath168 for @xmath169 ; @xmath170 for @xmath171 . )",
    "let @xmath172 .",
    "then we have @xmath173 the columns of @xmath174 are not orthonormal .",
    "let @xmath175 we can readily find its eigenpairs .",
    "( this costs only @xmath176 operations",
    ".- vectors ( @xmath12 ) . ] ) let the eigenvalues be @xmath177 and the principal components be @xmath178 , @xmath179 .",
    "then the first @xmath14 principal components of @xmath51 are given by ( this costs @xmath13 operations ) : @xmath180 indeed , we have ( in matrix notation ) @xmath181 , where @xmath182 , and @xmath183 .",
    "the r source code for this method is given in appendix [ app.c ] .",
    "the purpose of the last subsection is not only to discuss an efficient method for computing eigenpairs , but also to rewrite the statistical risk model given by ( [ pc ] ) and ( [ pc.xi ] ) directly in terms of the ( normalized demeaned ) returns @xmath161 .",
    "thus , using ( [ v.y ] ) we have , not just @xmath12 . ]",
    "@xmath184 where @xmath185 when @xmath79 , we have @xmath186 and @xmath187 , so @xmath188 and @xmath80 .",
    "so , the statistical risk model ( [ pc ] ) , as it can be rewritten via ( [ pc.def ] ) , is nothing but a _ deformation _ ( or regularization ) of the sample correlation matrix @xmath51 given by ( [ cormat ] ) : we deform @xmath189 thereby reducing the factor risk contribution into the total risk and replace the deficit by the specific risk . in this regard , note that we can consider more general deformations of the sample correlation matrix of the form @xmath190 subject to the requirements that @xmath191 be positive - definite and @xmath192 .",
    "however , in practice , sticking to our basic premise that there is no information available beyond the returns ( i.e. , no style or industry factors can be constructed ) , there is no choice but to take diagonal @xmath193 , which is then completely fixed .",
    "so , we are left with the choice of deforming @xmath189 .",
    "and ( [ phi.def ] ) is just _",
    "one _ of myriad such deformations .",
    "in fact , ( [ phi.def ] ) is not even the simplest such deformation .",
    "it is the one that arises in _ traditional _ statistical risk models based on principal components .",
    "however , a choice to work with the principal components is by large simply a matter of taste ( or even habit ) .",
    "it is just one basis , which a priori is not necessarily better or worse than any other basis .",
    "the _ intuitive _ justification behind a statistical risk model of the form ( [ pc ] ) is clear : we keep the first @xmath4 principal components with the largest contributions to the sample correlation matrix  based on the fact that the eigenvalues @xmath194  and replace the deficit ( on the diagonal ) by the specific risk . on the surface it all appears to make sense .",
    "however , the principal components beyond the first one are not stable out - of - sample , and this instability is inherited from that of the off - diagonal elements of the sample correlation matrix ( i.e. , pair - wise correlations ) .",
    "the first principal component also depends on the latter ; however , for large @xmath16 , in the leading approximation we have @xmath195 ( the so - called  market mode \"  see , e.g. , @xcite and references therein ) , which is by definition stable , albeit subleading corrections are not . in any event",
    ", there is no reason to limit ourselves to the deformations of the form ( [ phi.def ] ) .",
    "while a priori we can consider an arbitrary deformation @xmath196 ( subject to the requirement that @xmath46 ) , in practice it ought to be reasonable in the sense that it has to work out - of - sample . in this regard , keeping the first @xmath4 principal component of @xmath51 can be argued to be reasonable in the sense that , while the principal components themselves are not stable out - of - sample ( except for the quasi - stable first principal component  see above ) , tossing the higher principal components makes sense as their contributions are suppressed by the corresponding eigenvalues .",
    "this yields the deformation ( [ phi.def ] ) , which involves the matrix @xmath197 constructed from the returns @xmath161 .",
    "so , can we deform @xmath35 directly , without any reference to the returns @xmath161 ?",
    "the simplest such deformation is @xmath198 this is nothing but shrinkage @xcite .",
    "here we are shrinking the sample correlation matrix ( as opposed to the sample covariance matrix , which is what is usually done ) ; @xmath199 is the  shrinkage constant \" ; the  shrinkage target \" is the diagonal @xmath200 unit matrix .",
    "so , in this case we have @xmath79 , the factor loadings matrix is given by ( [ flm.pc ] ) , but instead of ( [ cormat.pc ] ) we have the factor covariance matrix @xmath201 .",
    "so , shrinkage is a factor model @xcite .    unlike in the traditional statistical risk models , where we need to fix the number of factors @xmath4 , in shrinkage the number of factors",
    "is fixed ( @xmath202 for the diagonal shrinkage target ; @xmath203  see below ) , but we must fix the shrinkage constant @xmath204 instead . is discussed in @xcite .",
    "] however , contrary to an apparent common misconception , the value of @xmath204 makes little difference if the number of returns @xmath15 .",
    "this is because in this case optimization using statistical models is well - approximated by a weighted regression .",
    "optimization involves the inverse @xmath206 of the model covariance matrix @xmath127 , where @xmath37 is given by ( [ fac.mod ] ) . for our purposes here it is convenient to rewrite @xmath37 via @xmath207 , where @xmath208 and @xmath209 . here",
    "( in matrix notation ) @xmath210 , and @xmath211 is the cholesky decomposition of @xmath212 , so @xmath213 .",
    "so , we have @xmath214 , where",
    "@xmath215 it then follows that , if all @xmath216 , then @xmath217 and @xmath218 i.e. , in this case @xmath219 is ( approximately ) independent of the factor covariance matrix .",
    "furthermore , for an arbitrary vector @xmath220 , we have @xmath221 = \\omega_i~\\varepsilon_i\\ ] ] here : @xmath222 ; @xmath223 ; and @xmath224 are the residuals of the cross - sectional weighted regression ( without the intercept ) of @xmath220 over @xmath225 with the weights @xmath226 .",
    "so , in this regime , the optimization based on shrinkage reduces to a weighted regression .",
    "the question is why  or , more precisely , when  all @xmath227 .",
    "this is the case when : i ) @xmath16 is large , and ii ) there is no  clustering \" in the vectors @xmath228 .",
    "that is , we do not have vanishing or small values of @xmath229 for most values of the index @xmath230 with only a small subset thereof having @xmath231 . without  clustering \" , to have @xmath232 , we would have to have @xmath233 , i.e. , @xmath234 and consequently @xmath37 would be almost diagonal . and",
    "such  clustering \" is certainly absent if @xmath40 are the @xmath14 principal components of the sample correlation matrix , so the above approximation holds in the case of shrinkage .",
    "the matrix @xmath235 , @xmath236 , and is independent of the shrinkage constant @xmath204 .",
    "the regression weights @xmath237 do depend on @xmath204 , but this dependence does not affect the desired holdings @xmath238 in ( [ h.opt ] ) as @xmath204 simply rescales the overall normalization coefficient @xmath239 in ( [ h.opt ] ) .",
    "that is , for large @xmath16 , the desired holdings based on shrinkage are approximately independent of the shrinkage constant @xmath204 .",
    "table [ table.shrinkage ] gives the simulation results )  not including the bounds does not change the qualitative picture . ] for various values of @xmath204 .",
    "for @xmath240 we have @xmath80 , which is singular , so in this case the computation is done via the weighted regression  see ( [ sh.reg ] ) . )",
    "can be obtained in two steps : first we regress the expected returns @xmath241 over @xmath225 ( without the intercept ) and weights @xmath242 , and then we demean the residuals . with the bounds",
    "these two steps can not be separated .",
    "so , we have two options .",
    "we can simply set @xmath204 to a small number ( e.g. , @xmath243 ) and use the r code in appendix c of @xcite .",
    "or we can modify said code by ( straightforwardly ) replacing the optimization procedure therein via a weighted regression thereby arriving at  bounded regression with linear constraints \" .",
    "the result is slightly better if we simply regress @xmath241 over @xmath225 _ with _ the intercept and weights @xmath242 , in which case we have roc 40.74% , sr 13.86 , cps 1.81.[fn.add.int ] ] not only is the value of the shrinkage constant immaterial , but the shrinkage based models sizably underperform the traditional statistical risk model with @xmath4 fixed via the  minimization \" algorithm of @xcite .    to be clear ,",
    "let us note that our observation that for large @xmath16 the optimization reduces to the weighted regression also applies to the traditional statistical risk models .",
    "the difference between the latter and shrinkage is that i ) fewer than @xmath14 principal components are used as the columns of the @xmath41 factor loadings matrix @xmath40 ( i.e. , @xmath5 ) , and ii ) the specific variances @xmath244 are no longer uniform ( cf .",
    "( [ xi.sh ] ) ) .",
    "let us mention that the shrinkage deformation ( [ def.sh ] ) can be straightforwardly generalized via permutational symmetry under which @xmath245 , @xmath246 , and @xmath247 is identified with @xmath248 ( @xmath249 ) . ]",
    "@xmath250 ( @xmath251 ) .",
    "in this case we still have the same @xmath14 risk factors , principal components of @xmath51  the deformation @xmath252 simply rotates the basis so long as @xmath253 is nonsingular . ]",
    "but we no longer have uniform @xmath254 ^ 2 $ ] . for @xmath255",
    "this gives the standard shrinkage .",
    "generally , we can have @xmath256 .",
    "finally , we can actually increase the number of risk factors beyond @xmath14 .",
    "this can be done by considering @xmath193 in ( [ psi.twiddle ] ) that itself is a factor model .",
    "as before , let us continue assuming that we can not construct any nontrivial style factors and there is no industry classification either .",
    "we can still construct a 1-factor model for @xmath193 with the intercept as the factor .",
    "if we choose the deformation @xmath257 , then we can set @xmath258 ( @xmath259 , @xmath260 ) , so the factor model covariance matrix now reads , then @xmath244 are nonuniform for @xmath261 .",
    "] @xmath262 where @xmath263 .",
    "so , we have @xmath6 factors , the @xmath14 principal components plus the intercept . for large @xmath16 , as above",
    ", the optimization reduces to a weighted regression , except that now it is with the intercept .",
    "the result is essentially independent of the values of @xmath204 and @xmath264 ( see table [ table.shrink.rho ] ) and expectedly somewhat better than in table [ table.shrinkage ] .",
    "to begin with , let us tie a loose end .",
    "we discussed the algorithms for fixing the number of statistical factors @xmath4 ( the  minimization \" and erank based algorithms and their variations ) . here",
    "we can ask : how do we know that , say , the  minimization \" based algorithm works better than picking some fixed value of @xmath4 ? this is tricky .    indeed , how do we pick such  optimal \" fixed @xmath4 ? we can do this by simply running @xmath265 backtests with fixed @xmath266 for a given alpha and picking the value of @xmath4 that performs best .",
    "however , this  optimal \" value would be in - sample .",
    "there is no guarantee that it will work out - of - sample .",
    "furthermore , generally it will vary from alpha to alpha .",
    "the aforementioned  minimization \" and erank based algorithms by construction are oblivious to a choice of a sample , and even if they do not necessarily produce the  optimal \" value of @xmath4 for any given sample , they work for any sample .",
    "so , here we can ask whether they produce reasonable results for a given sample .",
    "tables [ table.fixed.pc ] and [ table.fixed.pc.bounds ] give the simulation results for various fixed values of @xmath4 without and with the bounds ( [ liq ] ) , respectively .",
    "looking at these results ( especially those with the bounds , as any outperformance without the bounds should be taken with a grain of salt ) it is evident that the fixed @xmath4 performance peaks around @xmath267 , while the  minimization \" based algorithm compares closer to @xmath4 between 12 and 13 .",
    "this is consistent with the first row of table [ table3 ] .",
    "the important thing is that the  minimization \" based algorithm produces results that are close to the in - sample  optimal \" results for fixed @xmath268 .    in this regard",
    "it is instructive to run the following two series of backtests : i ) taking the maximum @xmath269 risk factors fixed but ad hoc basing the specific risks on the @xmath4-factor model with varying @xmath4 ( see table [ table.20.pc.xi.bounds ] and figure 1 ) ; and ii ) varying the number @xmath4 of the risk factors but ad hoc setting the specific risk equal the in - sample risk , i.e. , @xmath270 ( see table [ table.pc.tv.bounds ] and figure 2 ) .",
    "the simulation results indicate that both the specific risk and the number of risk factors make a difference .",
    "the performance in table [ table.20.pc.xi.bounds ] peaks around @xmath271 ( also see figure 1 ) , which is the ( approximate ) number of risk factors fixed via the  minimization \" based algorithm of @xcite . not surprisingly , the sharpe ratio in table [ table.pc.tv.bounds ] ( also see figure 2 ) improves as @xmath4 increases .",
    "however , the improvement rate slows down at higher @xmath4 , so the specific risk effect is dominant .",
    "the performance in both table [ table.20.pc.xi.bounds ] and table [ table.pc.tv.bounds ] is worse than for the  minimization \" based algorithm ( see table [ table1 ] ) .",
    "we already mentioned above ( see footnote [ fn.xi.distrib ] ) that the  minimization \" based algorithm produces @xmath4 somewhat higher than @xmath90 due to a typically skewed @xmath244 distribution for a typical universe of stocks we dealt with in our backtests .",
    "based on the above discussion , it might be tempting to simply set @xmath4 to the floor or cap of ( or just rounded ) @xmath90 .",
    "again , such a heuristic might work in - sample for some alphas , but not generally .",
    "thus , if the underlying returns are highly correlated , a typical value of @xmath4 produced by the  minimization \" and erank algorithms can be substantially lower than @xmath90 ( including @xmath272 ) .",
    "in such cases we can use the variation described in subsection [ sub.k.prime ] , and then a priori there is no reason to expect @xmath273 .",
    "a safer path would appear to be to use different methods , see if they produce consistent results out - of - sample , and pick one based thereon .",
    "let us also mention that in the r code in both functions in appendix [ app.a ] we use the built - in r function to compute eigenpairs . for large @xmath16 it is more efficient to replace it by the function given in appendix [ app.c ] .    from our discussion above and backtests it is evident that  nontraditional \" statistical risk models such as shrinkage underperform the traditional statistical risk models .",
    "the reason why is that in  nontraditional \" models the rank of @xmath274 equals @xmath14 , while in traditional models it is reduced , which yields nonuniform specific risks .",
    "furthermore , in the case of equity portfolios for which well - built and granular enough industry classifications are available , statistical risk models simply have no chance against risk models utilizing an industry classification such as heterotic risk models @xcite or heterotic capm @xcite .",
    "the reason for this is twofold : i ) industry factors are much more ubiquitous ( thereby covering much more of the relevant risk space ) ; and ii ) principal components beyond the first one are unstable out - of - sample .",
    "in contrast , heterotic risk models use much more stable first principal components within each  cluster \" ( e.g. , bics sub - industry ) , while heterotic capm uses a style factor .",
    "in this appendix we give the r ( r package for statistical computing , http://www.r-project.org ) source code for building a purely statistical risk model ( principal components ) based on the algorithm we discuss in sections [ sec.2 ] and [ sub.fix.k ] , including the  minimization \" and erank based algorithms for fixing the number of factors @xmath4 in section [ sub.fix.k ] .",
    "the two functions below are essentially self - explanatory and straightforward .",
    "the function corresponds to the  minimization \" based method for fixing @xmath4 .",
    "the input is : i ) , an @xmath275 matrix of returns ( e.g. , daily close - to - close returns ) , where @xmath16 is the number of returns , @xmath276 is the number of observations in the time series ( e.g. , the number of trading days ) , and the ordering of the dates is immaterial ; ii ) , where for ( default ) the risk factors are computed based on the principal components of the sample correlation matrix @xmath51 , whereas for they are computed based on the sample covariance matrix @xmath26 ; , where for the @xmath102 based method of subsection [ sub.k.prime ] is used .",
    "the output is a list : is the specific risk @xmath39 ( not the specific variance @xmath244 ) for , and @xmath277 ( where @xmath278 ) for ( recall that in this case @xmath39 is the specific risk for the factor model for @xmath51 , not @xmath26 ) ; is the factor loadings matrix @xmath40 for , and @xmath279 for ( recall that in this case @xmath40 is the factor loadings matrix for the factor model for @xmath51 , not @xmath26 ) ; is the factor covariance matrix @xmath42 ( with the normalization ( [ flm.pc ] ) for the factor loadings matrix , @xmath280 ) ; is the factor model covariance matrix @xmath37 for , and @xmath281 for ; is the matrix inverse to ; are the first @xmath4 principal components of a ) @xmath26 for , and b ) @xmath51 for .    the second function is and corresponds to the erank based method for fixing @xmath4 for the default parameter . the input is the same as in the function except for the additional parameters and . for a positive integer the code simply takes its value as the number of factors @xmath4 . for ( default )",
    "the code uses the erank method : if ( default ) , then @xmath282 , while if , then @xmath283 . ( the argument of @xmath284 is the matrix @xmath26 if , and the matrix @xmath51 if ) .",
    "the output is the same as in the function .",
    "in this appendix we give the r source code for calculating the first @xmath14 eigenpairs of the sample correlation matrix @xmath51 based on the successive application of the power iterations method as in section [ sub.pc ] .",
    "the code below is essentially self - explanatory and straightforward as it simply follows the formulas in section [ sub.pc ] .",
    "it consists of a single function ; is an @xmath285 matrix of returns ; @xmath16 is the number of the underlying returns ( e.g. , alphas ) ; @xmath6 is the number of data points in the time series ( e.g. , days ) ; is the number of the desired first @xmath286 eigenpairs ( in the decreasing order of the eigenvalues ) to be computed ( if @xmath287 , only the first @xmath14 eigenpairs are computed ) ; is the convergence precision and is not the same as how close @xmath288 are to 1 or the approximate eigenvectors @xmath147 are to the true eigenvectors . that precision is lower owing to cumulative effects ( due to sums , etc . ) .",
    "the output is a list : is a @xmath286-vector whose elements are the numbers of iterations @xmath289 , @xmath290 , for each eigenpair ; is a @xmath286-vector of the @xmath286 eigenvalues ; and is an @xmath291 matrix whose columns are the @xmath286 eigenvectors .",
    "the two lines and are optional and used here for the purpose of generating randomness in the runs in table [ table.prin.comp ] ; more economical ways of setting the initial iterations for the eigenpairs with @xmath292 can be used .",
    "in this appendix we give the r source code for calculating the first @xmath14 eigenpairs of the sample correlation matrix @xmath51 based on the no - iterations method discussed in subsection [ sub.no.iter ] .",
    "the code below is essentially self - explanatory and straightforward as it simply follows the formulas in subsection [ sub.no.iter ] .",
    "it consists of a single function ; is an @xmath285 matrix of returns ; @xmath16 is the number of the underlying returns ; @xmath6 is the number of data points in the time series ( e.g. , days ) ; for ( default ) , the code computes the eigenpairs for the covariance matrix ; for the code computes the eigenpairs for the correlation matrix .",
    "the method works only if @xmath293 .",
    "the output is a list : is an @xmath14-vector of the @xmath14 eigenvalues ; and is an @xmath294 matrix whose columns are the @xmath14 eigenvectors . +   +",
    "wherever the context so requires , the masculine gender includes the feminine and/or neuter , and the singular form includes the plural and _ vice versa_. the author of this paper (  author \" ) and his affiliates including without limitation quantigic@xmath3 solutions llc (  author s affiliates \" or  his affiliates \" ) make no implied or express warranties or any other representations whatsoever , including without limitation implied warranties of merchantability and fitness for a particular purpose , in connection with or with regard to the content of this paper including without limitation any code or algorithms contained herein (  content \" ) .",
    "the reader may use the content solely at his / her / its own risk and the reader shall have no claims whatsoever against the author or his affiliates and the author and his affiliates shall have no liability whatsoever to the reader or any third party whatsoever for any loss , expense , opportunity cost , damages or any other adverse effects whatsoever relating to or arising from the use of the content by the reader including without any limitation whatsoever : any direct , indirect , incidental , special , consequential or any other damages incurred by the reader , however caused and under any theory of liability ; any loss of profit ( whether incurred directly or indirectly ) , any loss of goodwill or reputation , any loss of data suffered , cost of procurement of substitute goods or services , or any other tangible or intangible loss ; any reliance placed by the reader on the completeness , accuracy or existence of the content or any other effect of using the content ; and any and all other adversities or negative effects the reader might encounter in using the content irrespective of whether the author or his affiliates is or are or should have been aware of such adversities or negative effects .    the r code included in appendix [ app.a ] , appendix [ app.b ] and appendix [ app.c ] hereof is part of the copyrighted r code of quantigic@xmath3 solutions llc and is provided herein with the express permission of quantigic@xmath3 solutions llc .",
    "the copyright owner retains all rights , title and interest in and to its copyrighted source code included in appendix [ app.a ] , appendix [ app.b ] and appendix [ app.c ] hereof and any and all copyrights therefor .",
    "black , f. , jensen , m. and scholes , m. ( 1972 ) the capital asset pricing model : some empirical tests . in : jensen , m. ( ed . )",
    "_ studies in the theory of capital markets_. new york , ny : praeger publishers , pp .",
    "79 - 121 .",
    "bouchaud , j .-",
    "p . and potters , m. ( 2011 ) financial applications of random matrix theory : a short review . in : akemann , g. , baik , j. and di francesco , p. ( eds . ) _ the oxford handbook of random matrix theory .",
    "_ oxford , united kingdom : oxford university press .",
    "connor , g. and korajczyk , r. ( 2010 ) factor models in portfolio and asset pricing theory . in : guerard jr , j.b .",
    "( ed . ) _ handbook of portfolio construction : contemporary applications of markowitz techniques_. new york , ny : springer , pp .",
    "401 - 418 .",
    "forni , m. , hallin , m. , lippi , m. and reichlin , l. ( 2005 ) the generalized dynamic factor model : one - sided estimation and forecasting . _ journal of the american statistical association _",
    "100(471 ) : 830 - 840 .",
    "mukherjee , d. and mishra , a.k .",
    "( 2005 ) multifactor capital asset pricing model under alternative distributional specification .",
    "ssrn working papers series , http://ssrn.com/abstract=871398 ( december 29 , 2005 ) .",
    "treynor , j.l .",
    "( 1999 ) towards a theory of market value of risky assets . in : korajczyk , r. ( ed . ) _ asset pricing and portfolio performance : models , strategy , and performance metrics .",
    "_ london : risk publications .",
    ".simulation results for the optimized alphas without bounds using the statistical risk models .",
    "see subsection [ sub.backtests ] for details .",
    "the result in the first row is the same as in @xcite .",
    "we label the risk models m1-m6 for notational convenience in table [ table3 ] .",
    "[ cols= \" < , < , < , < \" , ]"
  ],
  "abstract_text": [
    "<S> we give complete algorithms and source code for constructing statistical risk models , including methods for fixing the number of risk factors . </S>",
    "<S> one such method is based on erank ( effective rank ) and yields results similar to ( and further validates ) the method of @xcite . </S>",
    "<S> we also give a complete algorithm and source code for computing eigenvectors and eigenvalues of a sample covariance matrix which requires i ) no costly iterations and ii ) the number of operations linear in the number of returns . </S>",
    "<S> the presentation is intended to be pedagogical and oriented toward practical applications .    * statistical risk models *    zura kakushadze@xmath0@xmath1 solutions llc , and a full professor at free university of tbilisi . </S>",
    "<S> email : zura@quantigic.com ] and willie yu@xmath2    _ </S>",
    "<S> @xmath0 quantigic@xmath3 solutions llc _    _ 1127 high ridge road # 135 , stamford , ct 06905 solutions llc , the website or any of their other affiliates . ] _    _ @xmath1 free university of tbilisi , business school & school of physics _    _ 240 , david agmashenebeli alley , tbilisi , 0159 , georgia _    _ @xmath2 centre for computational biology , duke - nus medical school _    _ 8 college road , singapore 169857 _    ( february 14 , 2016 ) </S>"
  ]
}