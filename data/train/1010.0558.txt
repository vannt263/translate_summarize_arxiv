{
  "article_text": [
    "paper presents a new way to analyze gossip protocols based on random linear network coding that substantially simplifies , extends , and strengthens the results of previous work@xcite .",
    "gossip is a powerful tool to efficiently disseminate information .",
    "its randomized nature is especially well - suited to work in unstructured networks with unknown , unstable or changing topologies .",
    "because of this , gossip protocols have found a wide range of applications @xcite and have been extensively studied over the past several decades @xcite .",
    "recently , gossip protocols based on random linear network coding ( rlnc )  @xcite have been suggested  @xcite to cope with the additional complexities that arise when multiple messages are to be distributed in parallel .",
    "rlnc gossip has been adopted in many practical implementations @xcite and has performed extremely well in practice .",
    "these successes stand in contrast to how little rlnc gossip is understood theoretically .",
    "since its initial analysis on the complete graph @xcite , several papers  @xcite have tried to give good upper bounds on the stopping time of rlnc gossip in more general topologies . however , none of them address the case of unstable or changing topologies , and , even with the restriction to static networks , the guarantees are far from being general or tight on most graphs .",
    "in addition , all existing proofs are quite involved and do not seem to generalize easily .    [",
    "[ our - results ] ] our results + + + + + + + + + + +    this paper has two main contributions .",
    "the first is a new analysis technique that is both simpler and more powerful than previous approaches .",
    "our technique relates the stopping time for @xmath1 messages to the much easier to analyze time @xmath2 needed to disseminate a single message . for the first time , and in practically all settings , this technique shows that rlnc gossip achieves perfect pipelining , i.e. , it disseminates @xmath1 messages in order optimal @xmath4 time .",
    "our results match , and in most cases improve , all previously known bounds and apply to much more general models . to formalize this ,",
    "we give a general framework for network and communication models that encompasses and unifies the models suggested in the literature so far .",
    "we give concrete results for several instantiations of this framework and give more detailed comparisons with previous results in each section separately .    as a second major contribution",
    ", our framework extends all models to ( highly ) dynamic networks in which the topology is allowed to completely change at any time .",
    "all of our results hold in these networks even if the network dynamics are controlled by a fully adaptive adversary that decides the topology at each time based on the complete network state as well as all previously used randomness . virtually nothing , besides simple sequential flooding protocols  @xcite , was previously known in such truly pessimistic network dynamics .",
    "having optimal `` perfectly pipelined '' stopping times in worst - case adaptive dynamic networks is among the strongest stability guarantees for rlnc gossip that one might hope for . to this end ,",
    "our results are the first that formally explain rlnc gossip performance in the dynamic environments it is used in and was designed for . while the algorithm works in this wide variety of settings , our analysis remains mostly the same and extremely simple , in contrast with complex proofs that were previously put forward for the static setting .",
    "gossip is the process of spreading information via a randomized flooding procedure to all nodes in an unstructured network .",
    "it stands in contrast to structured multi - cast in which information is distributed via an explicitly built and maintained structure ( e.g. spanning tree ) .",
    "while structured multi - cast can often guarantee optimal use of the limited communication resources it relies heavily on having a know and stable network topology and fails in distributed or uncoordinated settings .",
    "gossip protocols were designed to overcome this problem . by flooding information in a randomized fashion",
    "they guarantee to deliver messages with high probability to all nodes with little communication overhead .",
    "this stability and distributed nature of gossip makes it an important tool for collaborative content distribution , peer - to - peer networks , sensor networks , ad - hoc networks and wireless networks and literature applying gossip in many areas and for many purposes is vast ( e.g. @xcite ) .    the gossip spreading of both a single message and multiple messages @xcite has been intensely studied .",
    "the spreading of one message often follows a comparatively simple epidemic random process in which the message is flooded to a randomly chosen subset of neighbors .",
    "spreading multiple messages in parallel is significantly more complicated because nodes need to select which information to forward .",
    "the main problem in this context is that widely spread messages get forwarded more often and quickly outnumber rarer messages . in many cases",
    "the slow spread of the rare messages dominates the time needed until all nodes know every message .    a powerful and elegant way to avoid",
    "this and similar problems is the use of network coding techniques .",
    "network coding as introduced by the seminal work of ahlswede , cai , li and yeung @xcite breaks with the traditional concept that information is transported by the network as an unchanged entity .",
    "ahlswede at al .",
    "show that in many multi - cast scenarios the optimal communication bandwidth can be achieved if and only if intermediate nodes in the network code information together .",
    "li , yeung and cai @xcite showed that for multi - cast it is enough if intermediate nodes use linear coding , i.e. computing linear combinations of messages . following this ho , koetter , mdard",
    ", karger and effros @xcite showed that the coefficients for these linear combinations need not be carefully chosen with regard to the network topology but that for any fixed network the use of random linear combinations works with high probability .",
    "the strong performance guarantees and the independence of the coding procedure from any global information about the network makes random linear network coding ( rlnc ) the perfect tool for spreading multiple messages .",
    "this was first observed and made formal by deb and mdard @xcite .",
    "they show that using randomized gossip and rlnc in a complete network in which each of the nodes starts with one message all information can be spread to all nodes in linear time , beating all non - coding approaches . after the introduction of this protocol in @xcite and its follow - up @xcite",
    "it was used in many applications @xcite , most notably the microsoft secure content distribution ( mscd ) or avalanche system @xcite .",
    "there has also been more theoretical work @xcite investigating the convergence time of the rlnc - algorithm on general static network topologies .",
    "we give a detailed description and comparison to these works in section [ sec : applications ] .",
    "[ [ gossip - in - dynamic - networks - models ] ] gossip in dynamic networks models + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    while previous work on rlnc gossip focused on static networks our analysis shows that it works equally well in a wide range of dynamic network topologies .",
    "this contributes to ongoing work on modeling dynamic networks and exploring ways to efficiently communicate over them . with more and more modern networks being highly dynamic",
    "this task has recently gained importance .",
    "the model for studying these networks is still in flux .",
    "substantial work has been devoted to random connectivity models in which a particular graph suffers different random edge faults in each round @xcite , or in which each node is connected to other random nodes in each round .",
    "other work , e.g. on population protocols ( see @xcite for a recent survey ) has been invested in studying networks that eventually stabilize .",
    "other models @xcite allows for worst - case changes in network connectivity to happen , but only at a slow pace with plenty of time for self - stabilization to adapt to the changes .",
    "gossip @xcite and broadcasting @xcite are among the most frequently considered primitives in these settings .",
    "recently , kuhn , lynch , and oshman@xcite proposed a truly pessimal model of network connectivity : that an adaptive adversary chooses the network structure in each round , subject only to the requirement that the network be connected in each round , and that nodes _ anonymously broadcast _ some chosen message without knowing who their current neighbors are .",
    "the strength of this model means that any algorithms that work in it will be broadly applicable to dynamic networks .",
    "kuhn et al .",
    "give simple algorithms based on sequentially flooding messages through the network as a proof that computation is at least possible though with strong performance losses compared to static networks ( even a simple consensus takes @xmath5 rounds in which all @xmath6 nodes communicate ) .    our network model framework adopts the pessimal dynamics of kuhn et al .",
    "@xcite and can be seen as extending the model to also include network topologies with different connectivities , asynchronous communication or non - broadcasting behavior .",
    "more importantly is that this paper shows that rlnc gossip remains highly efficient in these dynamic networks giving the first improvements over the simple flooding algorithms in @xcite .",
    "[ [ organization ] ] organization + + + + + + + + + + + +    section [ sec : algorithm ] reviews the rlnc algorithm and section [ sec : technique ] gives our new analysis technique . in section [ sec : model ]",
    "we introduce the network model framework .",
    "section [ sec : applications ] shows how to apply our technique in various instantiations of this framework .",
    "section [ sec : extensions ] finally discusses several ways in which the intentionally simple proofs from section [ sec : applications ] can be extended or sharpened .",
    "in this section , we give a brief description of the rlnc algorithm .",
    "the algorithm is simple and completely independent of the network structure or communication protocol .",
    "alternative descriptions of the same algorithm can be found in @xcite or @xcite .",
    "the rlnc algorithm sends out packets in the form of vectors over a finite field @xmath7 , where @xmath8 is an arbitrary prime or prime power .",
    "we assume that there are @xmath1 messages , @xmath9 , that are vectors from @xmath10 of length @xmath11 .",
    "every packet that is sent around during the execution of the algorithm has the form @xmath12 , where @xmath13 is a linear combination of the messages , and @xmath14 is the vector of the coefficients . if enough packets of this form are known to a node , i.e. , the span of the coefficient vectors is the full space @xmath15 , gaussian elimination can be used to reconstruct all messages . for this , only @xmath1 packets with linearly independent coefficient vectors are needed .",
    "linearity furthermore guarantees that any `` new packet '' that is created by taking a linear combination of old packets has the same valid format . with this",
    ", it is easy to see that a node can produce any packet whose coefficient vector is spanned by the coefficient vectors of the packets it knows .",
    "the algorithm is now easily described :    each node @xmath16 maintains a subspace @xmath17 that is the span of all packets known to it at the beginning and received so far . if @xmath16 does not know any messages at the beginning , then @xmath17 is initialized to contain only the zero vector .",
    "if @xmath16 knows some message(s ) @xmath18 at the beginning , @xmath17 is initialized to contain the packet @xmath19 in which @xmath20 is the @xmath21 standard basis vector .",
    "@xmath17 furthermore contains all linear combinations that complete the span of these packet(s ) . whenever node @xmath16 sends out a packet , it chooses a uniformly random packet from @xmath17 . at the end of each round",
    ", all received packets are added to @xmath17 and again the span is taken . if the subspace spanned by the coefficient vectors is the full space , a node decodes all messages .    throughout the rest of the paper we will solely concentrate on",
    "the `` spreading '' of the coefficient vectors ; the linear combination of the messages implied by a coefficient vector @xmath20 is always sent along with it .",
    "we therefore define @xmath22 to be only the coefficient part of @xmath23 , i.e. , the projection onto the first @xmath1 components .",
    "* remark : * the parameter @xmath8 is used to trade of a faster running time versus bandwidth . while a larger @xmath8 can lead to faster convergence it increases communication overhead by increasing the size of the @xmath1 @xmath24-size rlnc - coefficients . in contrast to some of the related papers all results in this paper hold for arbitrary choices of @xmath8 . for simplicity we will often restrict ourself to @xmath25 .",
    "note that this is the hardest case for running time considerations and it can be safely assumed that convergence times for larger @xmath8 will only be better .",
    "the case @xmath25 is furthermore interesting because it leads to the minimal rlnc - coefficients overhead and allows the use of simple xors as a basic arithmetic operation .",
    "the crux of multi - message gossip , especially in dynamically changing networks , is that it is not known to a sender who will receive a packet when it is transmitted .",
    "this renders exchanging information difficult and typically makes information broadcast protocols work well in the beginning but deteriorate when nodes begin knowing a lot of ( mostly the same ) information .",
    "for example , if @xmath1 messages need to be spread , and one node knows all of them and is allowed to transmit to another node that is only missing one message , the chance that the right message is picked is at most @xmath26 . here , network coding can drastically improve the performance .",
    "mixing packets over a large enough field makes it highly likely that the node will transmit some new information .    in the above example",
    ", the first node could transmit a specified random xor ( or random linear combination over @xmath7 ) of messages . whenever the message not known to the second node",
    "is mixed with a non - zero coefficient , the node can reconstruct the missing message .",
    "if the field size @xmath8 is taken to be large enough , the probability @xmath27 that this happens can be made arbitrarily high .",
    "when analyzing the rlnc algorithm presented in section [ sec : algorithm ] , sub and mdard @xcite were the first to use the notion of dimensionality of the subspaces @xmath28 as a measure of progress .",
    "they made the observation that a node @xmath29 can , and most likely will , transmit new information to a node @xmath16 , and thus increase the dimension of @xmath28 , whenever the subspace @xmath22 is not already contained in @xmath28 .",
    "for this reason .",
    "they call such a node @xmath29 _ helpful _ for @xmath16 .",
    "it is easy to see that the vectors that do not extend the dimensionality of @xmath16 , namely those in @xmath30 , form a lower dimensional subspace in @xmath22 .",
    "this results in a success probability of at least @xmath27 if a random vector from @xmath22 is chosen as a transmission .",
    "this fact and the notion of helpfulness is used as a crucial tool in all further rlnc proofs @xcite .",
    "unfortunately , it becomes hard and complicated to keep track of helpfulness especially because dimensionality does not accurately capture the progress of a system towards a mixed state well enough .",
    "take for example a network in which two cliques or  @xmath31 nodes are connected to each other . in this system",
    "there are two extreme states in which every node has dimension @xmath32 and every node is helpful to all others . in the first one the knowledge of a message",
    "is restricted to one clique , i.e. , @xmath31 messages are known by the first clique and @xmath31 messages are known by the second clique .",
    "a message is known to all nodes in its clique but to one .",
    "this makes all nodes helpful to each other .",
    "it is clear that this state is highly concentrated and not well mixed while the truly mixed state would be the one in which a `` randomly chosen '' half of the nodes knows about each message .",
    "we argue that the right way to look at the spreading of information is to look at the orthogonal ( dual ) complement offers additional information on orthogonal complements . ]",
    "@xmath33 of the coefficient subspaces @xmath22 . while the coefficient subspaces grow monotonically to the full space their orthogonal complement decreases monotonically to the empty span . to see",
    "how quickly this happens we first concentrate on one fixed ( dual ) vector @xmath20 , determine the time that is needed until it disappears from all subspaces @xmath33 with high probability and than take a union bound over all those dual vectors .",
    "to formalize this we introduce the following crucial notion of knowing :    a node @xmath34 knows about @xmath35 if its coefficient subspace @xmath36 is not orthogonal to @xmath20 , i.e. , if there is a vector @xmath37 with @xmath38 .",
    "note that a node @xmath34 knowing a vector @xmath20 does not imply @xmath39 or anything about @xmath34 being able to decode a message associated with the coefficients @xmath20 .",
    "knowing @xmath20 only indicates that the node is not completely ignorant about the set of packets that have a coefficient vector orthogonal to @xmath20 .",
    "counterintuitively , because we are not working over a positive - definite inner - product space , it can even be that @xmath39 but @xmath34 does not know @xmath20 .",
    "for example , over @xmath40 , if @xmath36 is just ( the span of ) the vector @xmath41 , then since @xmath41 over @xmath40 ( has dot product 0 with itself mod 2 ) , @xmath34 does not know @xmath41 , even though @xmath42 .",
    "the next lemma proves the two facts that make this notion of knowledge so useful :    [ lem : knowledge - spreads ] if a node @xmath34 knows about a vector @xmath20 and transmits a packet to node @xmath43 then @xmath43 knows about @xmath20 afterwards with probability at least @xmath27 .",
    "furthermore if a node knows about all vectors in @xmath15 then it is able to decode all @xmath1 messages .",
    "knowledge about a @xmath20 essentially spreads with probability @xmath27 because the vectors in @xmath22 that are perpendicular to @xmath20 form a hyperplane in @xmath22 .",
    "for a complete and more elementary proof see appendix [ app : proofs ] .    with this , the spreading of knowledge for a vector @xmath20 is a monotone increasing set growing process .",
    "it is usually relatively easy to understand this process and to determine its expected cover time @xmath2 .",
    "because the spreading process can be seen as a monotone markov process , it is easy to prove that the cover time always has an exponentially decaying tail . in most cases this tail kicks in close to the expectation .",
    "this allows to pick a @xmath44 ( usually @xmath45 ) such that after @xmath44 time any vector in @xmath15 has spread with probability @xmath46 and then take a union bound over all @xmath47 vectors to complete the proof that with high probability everything has spread .",
    "the following theorem summarizes this idea :    [ thm : reduction ] fix a prime ( power ) @xmath48 , a probability @xmath49 and an arbitrary network and communication model .",
    "+ suppose a single message is initiated at a node @xmath16 and then flooded through the network by the following faulty broadcast : in every round every node that knows the message and is supposed to communicate according to the communication model does forward the message with probability @xmath50 and remains silent otherwise .",
    "if for every node @xmath16 the probability that the message reaches all nodes after @xmath44 rounds is at least @xmath51 then @xmath1 messages can be spread in the same model in time @xmath44 with probability @xmath52 using the rlnc gossip protocol with field size @xmath8 .",
    "this follows directly from the discussion above and lemma [ lem : knowledge - spreads ] .",
    "initially every non - zero vector @xmath53 is known to at least one node namely the one that knows about the @xmath54th message where @xmath54 is a non - zero component of @xmath20 .",
    "whenever the network and communication model dictates that a node @xmath34 that knows @xmath20 sends a message to a node @xmath43 lemma [ lem : knowledge - spreads ] shows that with probability @xmath27 the node @xmath43 afterwards knows @xmath20 .",
    "the spreading of each vector @xmath20 therefore behaves like a faulty flooding process that floods @xmath20 in every transmission with probability @xmath27 . by assumption",
    "we have that after @xmath44 time steps every vector from @xmath15 fails to spread to all nodes with probability at most @xmath55 .",
    "taking a union bound over all @xmath56 vectors gives the guarantee that the probability that after @xmath44 rounds all nodes know about all vectors is at least @xmath52 . according to lemma [ lem : knowledge - spreads ]",
    "all nodes can decode in this case and have learned the @xmath1 messages .",
    "next we give a typical and easy way to apply theorem [ thm : reduction ] .",
    "we show that the cover time for one vector @xmath20 is often dominated by a negative binomial distribution @xmath57 , where @xmath2 is the expected coverage - time , and @xmath58 is a constant probability .",
    "such a distribution has a strong enough tail to prove optimal @xmath4 stopping times . in what follows",
    "we give a simple template to establish this :    what is needed for this template is a definition of a `` successful round '' such that at most @xmath2 such rounds are needed to spread a single vector @xmath20 and such that a round is not a success with ( say for now constant ) probability at most @xmath58 .",
    "the appropriate definition of success depends on the network model and is usually centered around its expansion , cuts , or diameter which determine how many additional nodes come to know about the vector in a `` good round '' . since nodes do not forget any information this spreading process is monotone and no progress gets lost in a bad round .",
    "thus if the knowledge about @xmath20 has not spread after @xmath59 steps , then there were at least @xmath60 failures , whereas one would only expect @xmath61 .",
    "if we choose the constant @xmath62 large enough , a chernoff bound or even simpler methods can now show that the probability for this to happen is at most @xmath63 .",
    "this is small enough that , after a union bound over all @xmath47 vectors ( e.g. for @xmath25 ) , the probability that all @xmath1 messages have not spread is at most @xmath64 .",
    "this simple template often applies directly and leads to simple proofs of expected and high probability converges times of @xmath0 that are often already order optimal .",
    "even when not stated explicitly , all of our results hold furthermore with high probability . in particular as shown here , an optimal additive @xmath65 additional rounds typically suffice to obtain a @xmath52 success probability for any @xmath49 .",
    "in this section , we elaborate on our network model framework that encompasses and extends the models suggested in the literature so far .",
    "the models and the results are very stable and can easily be extended further .",
    "we chose the following description as a trade - off between simplicity and generality .",
    "[ [ the - network ] ] the network + + + + + + + + + + +    we consider networks that consist of @xmath6 nodes .",
    "a network is specified by a ( directed ) graph @xmath66 on these nodes for every time @xmath44 .",
    "edges in @xmath66 are links and present potential communication connections between two nodes in round @xmath44 .",
    "we will usually assume that the network has , at all times , certain connectivity properties and will express the stopping time in terms of these parameters .",
    "( see also section [ sec : modelextensions ] . )",
    "[ [ adversarial - dynamics ] ] ( adversarial ) dynamics + + + + + + + + + + + + + + + + + + + + + +    in all previous papers that analyzed the rlnc algorithm , the network topology was assumed to be _ static _ , i.e. , @xmath67 .",
    "as discussed in the introduction , we allow the network topology to change completely from round to round and allow a fully adaptive adversary to choose the network .",
    "because we are dealing with randomized protocols , we have to specify precisely what the adversary is allowed to adapt to . in our models ( similar to @xcite )",
    "an _ adaptive adversary _ gets to know the complete network state and all previously used randomness when choosing the topology .",
    "after that , independent randomness is used to determine the communication behavior and the messages of the nodes on this topology .",
    "this means that the adversary can not adapt to who is sending to whom , or which messages are chosen for this round .",
    "the first assumption is necessary in many models to not render communication impossible .",
    "the second assumption can be weakened to get _ strongly adaptive _ or even _ omniscient _ adversaries who know in advance all future randomness that is used to create messages .",
    "the companion paper @xcite shows a trade - off between the adaptiveness of the adversary and the field size @xmath8 for these models . in this paper",
    "we restrict our attention to the adaptive adversary .",
    "[ [ the - goal - gossip ] ] the goal : gossip + + + + + + + + + + + + + + + +    distributed over the network are @xmath1 messages numbered @xmath68 each known to at least one node . throughout this paper , we assume a worst - case starting configuration for all messages including the case in which all messages are exclusively known to only one node ( see also section [ sec : mixed - initial - state ] ) .",
    "the goal of gossip protocols is to make all messages known to all nodes in the network using as little time as possible ( in expectation and with high probability )    [ [ communication ] ] communication + + + + + + + + + + + + +    nodes communicate along links with each other during transactions that are atomic in time . in each round , one packet",
    "is transmitted over a link if this link is activated in this round . from the view of a node , there are four commonly considered types of connections .",
    "either a node sends to all its neighbors , which is usually referred to as broadcast , or it establishes a connection to one ( e.g. uniformly random ) neighbor and sends ( push ) or receives ( pull ) a message or both ( exchange ) . in all cases , the packet is chosen without the sender knowing which node(s ) will receive it .    [",
    "[ message - and - packet - size ] ] message and packet size + + + + + + + + + + + + + + + + + + + + + + +    as described in section [ sec : algorithm ] we assume that all messages and packets have the same size , and that a packet exactly contains one encoded message and its rlnc - coefficients . note that the restriction on the message size is without loss of generality , since one can always cut a big message into multiple messages that fit into a packet .",
    "we also assume that the message size is large enough that the size of the rlnc - coefficients that are sent along is negligible .",
    "this assumption was made by all previous work and is justified by simulations and implementations in which the overhead is only a small fraction ( e.g. @xmath69 @xcite ) of the packet size .",
    "[ [ synchronous - versus - asynchronous - communication ] ] synchronous versus asynchronous communication + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we consider two types of timing models . in the synchronous case ,",
    "all nodes get activated at the same time and choose their messages independently , and messages get delivered according to the current network @xmath66 and who sends and receives from whom .",
    "note that this model is inherently discrete , and we assume that @xmath70 are the times when nodes communicate .",
    "we discuss this model in section  [ sec : randomphonecall ] . for the asynchronous case , we assume that every node communication is triggered independently by a poisson clock .",
    "this means that ( with probability one ) at any time only one node sends its message .",
    "this model can be directly translated into a discrete time model that defines round @xmath54 as the @xmath21 time such a communication takes place .",
    "the model considered in the literature so far assumes that every node is activated uniformly at random to communicate and then chooses a uniformly random neighbor for a push , pull or exchange .",
    "they also scale the time in the asynchronous model by a factor of @xmath71 so that each node gets activated once per time unit in expectation .",
    "we do not assume uniformity in either of the two distributions , and we present results for this more general model in section [ sec : asynchsingle ] .",
    "in this section we take the models from section [ sec : model ] and describe the results that can be obtained for them using our analysis technique .",
    "there is a section for each different kind of communication model .",
    "we start with the random phone call model @xcite that introduced rlnc - gossip .",
    "we than cover the extensions to arbitrary underlying network topologies as considered by @xcite .",
    "section [ sec : asynchsingle ] proves stopping times for a communication model that encompasses all former asynchronous communication protocols ( push , pull , exchange ,  ) . for this model",
    "we answer a question of @xcite and show that a simple min - cut quantity exactly captures the behavior of gossip of @xmath6 messages .",
    "lastly in section [ sec : broadcast ] we give the first bounds for the performance of synchronous and asynchronous broadcast in general networks . in this section",
    "we concentrate on showing only simple proofs that solely use the template from section [ sec : simple - template ] . in section [ sec : extensions ] , we revisit the models covered here and show some proof extensions .",
    "in this section , we consider the work of deb and mdard  @xcite and its follow - up  @xcite and show how to simplify and improve the analysis .",
    "the papers use a fairly simple model from our framework , namely the synchronous push or pull model on the complete graph , i.e. , @xmath72 .",
    "this means in each round each node picks a random other node to exchange information with .",
    "this model is also known as the random phone call model and was introduced by @xcite .",
    "it is shown  @xcite that it is possible in this model to spread @xmath73 messages in @xmath74 time if @xmath75 .",
    "this beats the @xmath76 time of @xmath6 sequential @xmath77-phases of flooding just one message .",
    "the follow - up papers@xcite generalize this result to smaller number of messages @xmath1 and allow @xmath8 to be as small as @xmath1 .",
    "they show that the running time of the algorithm is @xmath78 , i.e. , order optimal as long as @xmath79 . in order to prove this result",
    ", they have to assume that each node knows initially only one message and that initially the messages are equally spread . even with these assumptions",
    "the analysis is long and complicated and the authors state themselves in their abstract that `` while the asymptotic results might sound believable , owing to the distributed nature of the system , a rigorous derivation poses quite a few technical challenges and requires careful modeling and analysis of an appropriate time - varying bernoulli process . ''",
    "our next lemma shows that rlnc gossip actually always finishes with high probability in order optimal stopping time @xmath80 .",
    "our analysis is much simpler and has many further advantages : it holds for all choices of @xmath1 and allows @xmath8 to be as small as @xmath81 .",
    "our proof does also not rely on any assumptions on the initial message distribution .",
    "we show in section  [ sec : exact - dependence - k ] that the well - mixed initial state assumed in @xcite actually provably speeds up the convergence compared to the worst - cast distribution for which our result holds .",
    "our proof furthermore gives a success probability of @xmath82 if the algorithm runs for @xmath83 time . in the setting of @xcite with @xmath84 ,",
    "this is @xmath85 instead of the @xmath86 stated there .",
    "lastly it is interesting to note that previous general approaches  @xcite are unable to prove any running time that beats the simple non - coding non - gossiping @xmath76 sequential flooding approach when applied to the complete graph / network .    [",
    "lem : randomphonecall ] the rlnc gossip in the random phone call model with @xmath25 spreads @xmath1 messages with high probability in exactly @xmath87 time .",
    "this holds independently from the initial distribution of the messages and of the communication model ( e.g. push , pull , exchange ) .",
    "after the helpfulness of rlnc gossip was established for the complete graph by @xcite , the papers @xcite,@xcite and @xcite generalized it to general static topologies and consider asynchronous and synchronous push , pull and exchange gossip .",
    "in this section we first review the previous results and than show how to improve over them giving an exact characterization of the stopping time or rlnc gossip for @xmath84 messages using the template of section [ sec : simple - template ] .",
    "the paper `` information dissemination via network coding''@xcite by mosk - aoyama and shah was the first to consider general topologies .",
    "they consider a similarly general version of the synchronous and asynchronous gossip as presented here and analyze the stopping times for @xmath84 in dependence on the conductance .",
    "their analysis implies that with high probability @xmath76 phases of @xmath6 asynchronous rounds suffice for the complete graph and constant degree expanders and @xmath5 such phases for the ring - graph .",
    "while the analysis is very interesting , these results do not beat the simple ( non - coding ) sequential flooding protocol and the stopping time of the ring - graph and many other graphs is even off by a factor of @xmath6 .",
    "their running times for the synchronous model are similar but lose another @xmath88-factor .",
    "their dependence is on the success probability @xmath52 is furthermore multiplicative in @xmath89 because it stems from a standard probability amplification argument .",
    "two recent papers  @xcite analyzed rlnc gossip using two completely different approaches . the second  @xcite points out that the analysis of the first  @xcite is flawed and prove that the asynchronous rlnc gossip on a network with maximum degree @xmath90 takes with high probability @xmath91 time .",
    "their proof uses an interesting reduction to networks of queues and applies jackson s theorem .",
    "they also give a tight analysis and lower bounds for a few special graphs with interesting behavior ( see below ) .",
    "while their analysis is exact for few selected graphs the analysis is far from tight and in most graphs the maximum degree has nothing to do with the stopping time of rlnc gossip .",
    "the major question asked in @xcite is to find a characterizing property of the graph that determines the stopping time .",
    "we give exactly such a characterization for the asynchronous case with @xmath84 assuming a worst - cast message initialization .",
    "the model we use is a generalization of the classical push , pull and exchange model : we allow the topology in every round to be specified by a graph with directed and/or undirected edges and a probability weight @xmath92 on every edge @xmath93 , such that the sum over all edges is at most 1 . in every round each edge gets exclusively selected with probability @xmath92 , i.e. , in each round at most one edge gets selected .",
    "if the edge is undirected an exchange is performed and if a directed edge gets activated a packet is delivered in the direction of the edge .",
    "note that this model is a generalization of the `` classical '' communication models . to obtain the probability graph from the undirected network with push or",
    "pull one just has to replace every undirected edge @xmath94 by two directed edges with probability weight @xmath95 and @xmath96 where @xmath97 and @xmath98 are the degrees of @xmath29 and @xmath16 respectively . to obtain the exchange protocol",
    "each undirected edge @xmath94 simply has the probability weight @xmath99 .",
    "given such a network graph @xmath100 with probability weights @xmath92 we define the min - cut @xmath101 as : @xmath102 where @xmath103 are all edges leaving a non - empty vertex - subset @xmath104 in @xmath100 .",
    "the next two lemmas show that this quantity exactly captures how long rlnc gossip for @xmath6 messages takes .",
    "[ lem : cut - asynch - single ] if for every time @xmath44 the min - cut of @xmath66 is at least @xmath105 then the asynchronous single transfer algorithm with @xmath25 spreads @xmath6 messages with probability at least @xmath106 in @xmath107 time .",
    "the next lemma proves that @xmath107 is optimal .",
    "[ lem : lowerbound - asynch - single ] with high probability , the asynchronous single transfer algorithm takes at least @xmath108 rounds to spread @xmath1 messages if it is used on fixed graph @xmath100 with ( min-)cut @xmath105 on which at least @xmath109 messages are initialized inside this cut .    applying lemma [ lem : cut - asynch - single ] to the standard push / pull model gives a @xmath110 stopping time for any dynamic graph whose maximum degree is bounded by @xmath90 , which is the main result of @xcite .",
    "it also gives @xmath5 for the complete graph ( instead of the worst case @xmath111 of @xcite ) and nicely explains the behavior of the barbel graph and the extended barbel - graph that were considered by @xcite .",
    "the proof of lemma [ lem : cut - asynch - single ] can furthermore easily be extended to show that the dependency on the success probability is only logarithmic and additive in contrast to the previous work @xcite .      in this section",
    "we give convergence results for synchronous and asynchronous broadcast gossip in arbitrary dynamic networks .",
    "these are to our knowledge the first results for the rlnc algorithm in such a setting .",
    "we think the results in this section are of particular interest for highly dynamic networks .",
    "the reason for this is that many of the highly unstable or dynamic networks that occur in practice like ad - hoc- , vehicular- or sensor - networks are wireless and thus have inherent broadcasting behavior .    to fix a model we first consider the simple synchronous broadcast model .",
    "we assume without loss of generality that the network graph @xmath100 is directed because any undirected edge can be replaced by its two anti - parallel directed edges .",
    "having wireless networks in mind we also assume that in each round each nodes computes only one packet that is then send out to all neighbors .",
    "our results also hold for the less realistic model where a node sends out a different packet to each neighbor .",
    "the parameter that governs the time to spread one message in a static setting is ( not surprisingly ) the diameter @xmath112 and it is easy to prove @xmath113 stopping times for @xmath1 messages using our technique . in a dynamic setting this is not true . even for just one message , an adaptive adversary can , for example , always connect both the set of nodes that know about it and the set of nodes that do not know about it to a clique and connect the two cliques by one edge .",
    "even though the graph @xmath66 has diameter @xmath81 at all times , it clearly takes at least @xmath6 rounds to spread one message . in order to prove stopping times in the adaptive adversaries model we switch to a parameter that indirectly gives a good upper - bound on the diameter for many graphs",
    "the parameter we use is the isoperimetric number @xmath114 , which is defined as follows : @xmath115 where @xmath103 are the nodes in @xmath100 outside of the subset @xmath116 that are in the directed neighborhood of @xmath116 .    to give a few example values : for disconnected graphs @xmath114 is zero and for connected graphs it ranges between @xmath117 and @xmath118 ; for a @xmath1-vertex - connected graph @xmath100 we have @xmath119 and @xmath120 holds if and only if @xmath100 is a vertex - expander ( or a complete graph ) .",
    "we are going to show that the expected time for one message to be broadcasted is at most @xmath121 .",
    "this is @xmath74 for a line and @xmath77 for any vertex - expander .",
    "our bound is tight in the sense that for any value @xmath122 with @xmath123 there is a static graph @xmath100 that has diameter at least @xmath124 and isoperimetric number @xmath125 . having an upper bound on the time @xmath2 it takes to spread one message we again prove an perfectly pipelined time of @xmath4 for @xmath1 messages :    [ lem : synchbroadcast ] the synchronous broadcast gossip protocol takes with high probability at most @xmath126 rounds to spread @xmath1 messages as long as the isoperimetric number of the graph @xmath66 is at least @xmath122 at every time @xmath44 .",
    "a similar result to lemma [ lem : synchbroadcast ] can be proven for the asynchronous broadcast model in which at every round each node gets selected uniformly independently at random ( i.e. with probability @xmath127 ) to broadcast its packet to its neighbors :    [ lem : asynchbroadcast ] the asynchronous broadcast gossip protocol takes with high probability at most @xmath128 rounds to spread @xmath1 messages as long as the isoperimetric number of the graph @xmath66 is at least @xmath122 at any time @xmath44 .",
    "in this section we discuss how the simple proofs from section [ sec : applications ] that use only the template from section [ sec : simple - template ] can be extended to give more detailed or sharper bounds .",
    "as stated in section [ sec : model ] we assume throughout the paper that @xmath1 messages are to be spread that are initially distributed in a worst - case fashion .",
    "all earlier papers restricted themselves to the easier special case that @xmath84 and that each node initially holds exactly one message @xcite , or that @xmath1 is arbitrary but the network starts in a similarly well - mixed state in which each message is known by a different node and all messages are equally spread over the network @xcite .",
    "in many cases the worst - case and any well - mixed initialization take equally long to converge because the running time is lower bounded and bottlenecked by the flooding time @xmath2 for a single message or the time it takes for a node to receive at least @xmath1 packets .",
    "nevertheless there are cases where a well - mixed initialization can drastically improve performance .",
    "our proof technique explains this and we give a simple way to exploit assumptions about well - mixed initializations to prove stronger performance guarantees : if , e.g. , each node initially holds exactly one of @xmath84 messages then most vectors @xmath20 are already known to most nodes initially .",
    "more precisely exactly the @xmath129 vectors with @xmath54 non - zero components are initially known to exactly @xmath54 nodes . with many vectors",
    "already widely spread initially the union bound over the failure probabilities for all vectors to spread after @xmath44 rounds can decrease significantly .",
    "taking the different quantities and probabilities for nodes that are initially known to a certain number of nodes in account one can prove in theses cases that a smaller @xmath44 suffices .",
    "one example for a mixed initialization being advantageous is discussed in the next section [ sec : exact - dependence - k ] and another one is the convergence time of the asynchronous push and pull protocol on the star - graph : for both push and pull the network induced by the star - graph has a min - cut of @xmath130 which leads according to lemma [ lem : cut - asynch - single ] and [ lem : lowerbound - asynch - single ] to a stopping time of @xmath131 under a worst - case initialization . to lower bound",
    "the convergence time lemma [ lem : lowerbound - asynch - single ] , which relates the convergence time to the min - cut of the network graph , has to assume that at least a constant fraction of the messages are initialized inside a bad cut .",
    "for the `` classical '' initialization in which each node starts with exactly one message this is true for the push model but not in the pull model in which every bad cut only contains few messages . indeed assuming a well - mixed initialization the push protocol takes still @xmath131 time to converge while a much lower @xmath132 stopping time for the pull model can be easily derived using our techniques .      in most ( highly connected ) networks",
    "the spreading time @xmath2 for one message is short and @xmath133 becomes the dominant term in the order optimal @xmath0-type upper bounds presented in this paper .",
    "so is , for example , @xmath134 for most expanding networks .",
    "while it is clear that at least @xmath1 packets need to be received at each node it becomes an interesting question how large the constant factor hidden by the @xmath135-notation is .",
    "differently stated , we ask how large the fraction of helpful or innovative packets received by a node is over the execution of the protocol .    determining and even more optimizing proofs to obtain such constants is usually a big hassle or even infeasible due to involved proofs .",
    "simulation is therefore often used in practice to get a good estimation of the constants ( e.g. @xcite ) . our template from section [ sec : simple - template ]",
    "reduces the question for the stopping time of rlnc gossip to a simple standard question about tail bounds for negative binomial random variables .",
    "this makes it often possible to determine and prove ( optimal ) constants ( and lower order terms ) .",
    "all that is needed is to replace the chernoff bound in the template from section [ sec : simple - template ] by an argument that gives the correct base in the exponential tail - bound . in section [ sec : tighter - tail ] we give such a bound .",
    "we than exemplify then how to apply this bound by two examples : in section [ sec : exact - broadcast ] the synchronous broadcast gossip from section [ sec : broadcast ] and in section [ sec : exact - rumormongering ] the rumor mongering from section [ sec : randomphonecall ] . in both cases",
    "we can show that the constant in the dependency on @xmath1 is arbitrarily close to the absolutely optimal constant @xmath117 , i.e. we can obtain a perfectly pipelined @xmath136 stopping time .",
    "the following simple lemma gives a stronger guarantee on the tail of a negative binomial random variable than the chernoff bound used in the template from section [ sec : simple - template ] .",
    "the lemma proves that a constant factor away from the expectation the probability drops by a factor of @xmath58 with every additional trial instead of a constant factor drop that would be obtained by a standard chernoff bound :    [ lem : tail ] the probability that after @xmath136 independent trials there are less than @xmath2 successes is at most @xmath137 where @xmath58 is the failure probability ( with @xmath138 ) .",
    "if we apply this stronger tail bound in the template from section [ sec : simple - template ] we obtain the following corollary :    [ cor : tail ] let @xmath139 and @xmath140 .",
    "if in order to spread any fixed coefficient vector @xmath141 only @xmath2 successful rounds are needed and if a round fails with probability at most @xmath58 then @xmath1 messages spread in @xmath142 rounds with probability at least @xmath143 . for @xmath144",
    "this means a running time of @xmath136 in expectation and with high probability .      in this section",
    "we use the tighter tail bounds from the last section [ sec : tighter - tail ] to sharpen the bounds on the convergence time of the synchronous broadcast from section [ sec : broadcast ] :    [ lem : exact - synchbroadcast ] the synchronous broadcast gossip protocol takes with high probability at most @xmath145 rounds to spread @xmath1 messages where @xmath146 if the isoperimetric number of the graph @xmath66 is at least @xmath122 at any time @xmath44 . ( and @xmath147 ) )      another interesting case in which the exact dependence on the number of messages @xmath1 was considered is the rumor mongering process from section [ sec : randomphonecall ] .",
    "the authors of @xcite give a theoretical analysis in the regime @xmath148 where the @xmath133 term clearly dominates and prove an upper bound of @xmath149 for the push protocol and @xmath150 for the pull model .",
    "they also simulated the protocol and estimated the stopping time to be @xmath151 .",
    "both their analytic bounds and the simulation assume that messages start out in separate nodes and are equally spread over the network ( see also section [ sec : mixed - initial - state ] ) .",
    "in this section we improve over these findings and show that the pull model in this setting actually converges in @xmath152 time for @xmath153 .",
    "interestingly we also show that with a worst - cast initialization ( see also section [ sec : mixed - initial - state ] ) the pull model does not achieve this convergence time but has a leading constant between @xmath154 and @xmath155 :    determining the correct constants for random communication protocols like the random phone call model is much more delicate than proving order optimal convergence times .",
    "the reason for this is that the union of random exchanges over many rounds almost surely form an expander while the graph in a single round is usually not even connected .",
    "this is the case for all of the presented random phone call models .",
    "while all these models are very stable order optimal one must be much more careful to achieve and even more prove optimal @xmath156-type bounds for large @xmath1 .",
    "we exemplify this by describing these concerns in detail for the pull protocol :    the worst - case initialization for the pull protocol is when all messages are initially known to only one node . in this case",
    "this node is not pulled at all in one round with probability @xmath157 . in order to get pulled at least @xmath1 times it takes therefore in expectation at least @xmath158 rounds .",
    "thus for the case that only one node initially knows about all messages and if this node prepares a message in each round which it sends out to the nodes requesting it this is an information - theoretic lower bound on the number of rounds .",
    "a direct analysis of the protocol using corollary [ cor : tail ] for this case gives a constant of @xmath159 which is @xmath160 for @xmath25 .",
    "this can be improved if the start state is a bit more mixed , e.g. , if each message is known to @xmath54 nodes initially . in this case the information - theoretical lower bound becomes @xmath161 and our upper bound becomes @xmath162 this means that for @xmath163 our proof gives the optimal stopping time @xmath164 .",
    "lemma [ lem : better - initialization ] also shows a @xmath165 stopping time for the case where all messages are initiated at different nodes .",
    "this contrasts the upper bound of @xmath166 and the estimate of @xmath167 of @xcite for this setting .",
    "more extensive simulation results than the ones in @xcite confirm that the constant for the dependency on @xmath1 should indeed be smaller than the projected @xmath168 .",
    "[ lem : better - initialization ] the rlnc algorithm in the random phone call pull model even with @xmath25 spreads @xmath169 messages with high probability in @xmath165 time if all messages are initially known to different nodes .",
    "section [ sec : asynchsingle ] proves convergence times for spreading @xmath84 messages using the asynchronous single transfer protocols .",
    "these bounds are tight and directly extend to a @xmath170 bound for @xmath171 messages . in what follows",
    "we want to generalize this to smaller number of messages and discuss the bounds that can be obtained using the technique from section [ sec : technique ] .",
    "for small number of messages , e.g. @xmath172 , the convergence time of rlnc single transfer gossip can be much faster than @xmath107 but still be @xmath173 .",
    "this shows that the min - cut @xmath105 is not the right quantity to look at in this scenario .",
    "again , as in section [ sec : broadcast ] , conductance quantities capture much better how fast a small number of messages spreads .",
    "the quantity we consider is : @xmath174    the next lemma shows that it takes at most @xmath175 time for one message to spread if the conductance is bounded by @xmath176 .",
    "[ thm : one - message - asynch - single ] in the asynchronous single transfer model ( with any @xmath8 ) it takes in expectation at most @xmath175 time for one message to spread .",
    "the probability that a set of nodes that know about the message grows from size @xmath177 to @xmath178 is at least @xmath179 .",
    "it thus takes at least @xmath180 rounds in expectation for the first success , @xmath181 rounds for the second success and in general @xmath182 rounds in expectation for one message to spread .",
    "this is a tight bound for many regular graphs and gives e.g. a flooding time of @xmath183 for the complete graph or any other regular expanders .",
    "it is clear that rlnc - gossip for any @xmath1 needs to take at least so much time .",
    "the other lower bound that kicks in for large enough @xmath1 is the @xmath108 lower bound from lemma [ lem : lowerbound - asynch - single ] .",
    "similar to the results for the other models we want show that the total running time is essentially ( up to at most a @xmath88 factor ) either dominated by the @xmath184 rounds to spread one message or for larger number of messages @xmath1 the @xmath185 rounds coming from the communication lower bound that the @xmath1 messages have to cross the worst case cut .",
    "[ lem : exact - asynch - single ] disseminating @xmath1 messages in the asynchronous single transfer model with @xmath25 takes with high probability at most @xmath186 rounds if the graph @xmath100 as a min - cut of at most @xmath105 and a conductance of at least @xmath176 at all times @xmath44 .",
    "the idea behind proving performances in the rather strong adaptive adversary model introduced in this paper is that the guarantees directly extend to the widest possible range of dynamic networks including random models .",
    "most of our proofs like the ones of lemma [ lem : cut - asynch - single ] , [ lem : synchbroadcast ] or [ lem : asynchbroadcast ] demand that the network graph @xmath66 has a certain connectivity requirement at any time @xmath44 .",
    "these requirements might be too strong especially for random network models .",
    "we discuss in the following how these requirements can be easily weakened in many ways :    the simple fact that no progress in the spreading of knowledge gets lost makes it easy to deal with the case that the connectivity fluctuates ( e.g. , randomly ) .",
    "increasing the stopping time by a constant factor easily accounts for models in which the desired connectivity occurs only occasionally or with constant probability .",
    "looking at the average connectivity is another possibility .",
    "it is furthermore not necessary to require the entire graph to be expanding on average but it suffices to demand that each subset expands with constant probability according to its size .",
    "this way convergence can be proven even for always disconnected graphs . especially for random models it can also be helpful to consider the union of the network graphs of consecutive rounds , i.e. @xmath187 .",
    "this gives for example directly valid upper bounds for the synchronous or asynchronous broadcast model .    as a simple example for the usefulness of these approaches we discuss an alternative way to prove lemma [ lem : randomphonecall ] about the stopping time of the rumor mongering process : instead of analyzing the rumor mongering as a synchronous protocol on the complete graph in which each node performs a pull , push or exchange one can alternatively see it as a synchronous broadcast ( see section [ sec : broadcast ] ) on a random network .",
    "the network graph @xmath66 in this case is simply formed by a random directed in - edge , directed out - edge or undirected edge at each node depending on whether on looks at the push , pull or exchange model .",
    "the results from lemma [ lem : synchbroadcast ] or [ lem : synchbroadcast ] will not directly give any bounds simply because the network graph @xmath66 is with high probability disconnected .",
    "using either of the two more advanced extensions solves this problem : with constant probability every set has a constant expansion ; alternatively one can use that the union of a constant number of rounds , as described above , forms with an expander with high probability .",
    "we have given a new technique to analyze the stopping times of rlnc - gossip that drastically simplifies , strengthens and extends previous results .",
    "most notably all our results hold in highly dynamic networks that are controlled by a fully adaptive adversary .",
    "theorem [ thm : reduction ] gives a direct way to transfer results for the single - message flooding / gossip process to the multi - message rlnc - gossip if strong enough tail bounds are provided .",
    "one candidate for which this could work is , e.g. , @xcite which can be interpreted as giving bounds on a synchronous single transfer gossip for one message .",
    "this paper also gives evidence that in most network models rlnc - gossip achieves perfect pipelining , i.e. the bounds for disseminating @xmath1 messages have the form @xmath0 where @xmath2 is the expected time to ( faultily ) flood one message .",
    "it is a very intriguing question under which general conditions on the network model one can prove this behavior .",
    "it is easy to see that the monotone set - growing process induced by the faulty flooding process of one message always exhibits a strong exponential tail as needed to apply lemma [ thm : reduction ] .",
    "this already implies asymptotic convergence times of the form @xmath188 ( see also lemma [ lem : exact - asynch - single ] ) where @xmath105 is the min - cut in the induced markov - chain , i.e. the minimal probability over all sets to inform another node within one round .",
    "the main question remaining is therefore to guarantee that this tail kicks in after @xmath124 rounds .",
    "in this section we provide a few background facts in linear algebra on vector spaces without ( positive - definite ) inner product , especially the notions involved in orthogonality .",
    "even so the section [ sec : technique ] is fully self - containing this section might be helpful in understanding the proofs .    for a vector space @xmath189 the _ dual space _",
    "@xmath190 consists of all linear forms on @xmath189 . for any subset @xmath191 the orthogonal ( dual ) complement @xmath192 is defined as all elements from @xmath190 that disappear on @xmath116 .",
    "it is easy to see that the orthogonal complement is a subspace in @xmath190 and has co - dimension equal to the dimension of the span of @xmath116 in @xmath189 .",
    "the dual space @xmath190 is isomorphic to @xmath189 and in the case of @xmath15 the dot - product @xmath193 is an isomorphism . using this identification the orthogonal complement",
    "can also be defined as the space of all vectors that are perpendicular ( i.e. having a zero dot - product ) to all vectors in @xmath116 .",
    "this is the standard definition of orthogonality and for inner - product spaces like @xmath194 it matches the geometrical notion of orthogonality .",
    "this is not true for @xmath15 in which the dot - product is not positive definite .",
    "this leads to counter - intuitive situations , e.g. the vector @xmath195 $ ] is orthogonal to itself in @xmath40 .",
    "but the fact remains that every subspace @xmath196 can be assigned a orthogonal complement subspace @xmath197 with @xmath198 remains true and is the important notion used in section [ sec : technique ] .",
    "we give a more basic proof here : for this we define two vectors @xmath199 as equivalent if @xmath200 .",
    "this splits @xmath36 in exactly @xmath8 equivalence classes of equal size . to see this note that , because @xmath36 is a subspace , scalar - multiplication is a bijection between any two equivalence classes that correspond to a non - zero dot - product . by assumption @xmath36 furthermore contains a vector @xmath201 that has a non - zero dot - product with @xmath20",
    "this gives that @xmath20-translation is a bijection between the zero dot - product equivalent class and another equivalence class .",
    "thus with probability exactly @xmath27 a packet with coefficient vector from a non - zero equivalence class is chosen for transmission . in this case",
    "this coefficient vector gets added to @xmath202 and the node @xmath43 now knows @xmath20 .    for the second claim",
    "we prove that any node @xmath34 that is not able to decode does not know about at least one vector @xmath20 : if @xmath34 can not decode than @xmath36 is not the full space . because @xmath36 is a subspace it is lower - dimensional and we can use gram - schmidt to construct a orthogonal basis of @xmath36 and a vector @xmath20 that is orthogonal to @xmath36 .",
    "this vector @xmath20 is then by definition not known to @xmath34 , a contradiction .    for the lower",
    "bound we note that each node receives in expectation ( and with high probability ) only @xmath203 packets per round .",
    "thus if in the beginning at least one node did not already know about a constant fraction of the messages , then the algorithm has to run for at least @xmath204 rounds .",
    "it is also clear that even one message takes in expectation @xmath205 time to spread to all nodes .",
    "this completes the lower bound .    to prove the upper bound",
    ", we use the template from [ sec : simple - template ] : for this we fix a coefficient vector @xmath20 and define a round as successful if the number of nodes that know about it increases by at least a constant factor @xmath206 or if the number of nodes that do not know about @xmath20 decreases by a factor of @xmath176 .",
    "there are at most @xmath77 successful rounds needed until at least @xmath31 nodes know about @xmath20 and at most another @xmath77 successful rounds until all nodes know about @xmath20 .",
    "it remains to be shown that each round succeeds with constant probability .",
    "we first consider the pull model . at first",
    "we have @xmath207 nodes that know about @xmath20 and at least @xmath31 nodes pulling for it .",
    "each of those nodes has a probability of @xmath208 to hit a knowing node .",
    "we expect a @xmath208 fraction of the ignorant nodes , i.e. , at least @xmath209 nodes , to receive a message from a node that knows about @xmath20 .",
    "the independence of these successes and lemma [ lem : knowledge - spreads ] prove that with constant probability at least @xmath210 nodes learn about @xmath20 .",
    "once there are at least @xmath31 nodes that know @xmath20 , each of the ignorant nodes pulls a packet from a knowing node with probability at least @xmath211 .",
    "the proof for the push model is similar .",
    "if there are @xmath207 nodes that know about @xmath20 and push out a message , then there are at least @xmath31 ignorant nodes that each receive at least one message from one of the @xmath54 nodes with probability @xmath212 .",
    "it is not hard to see that , in total , @xmath210 ignorant nodes receive a message from a node that knows @xmath20 with constant probability . lemma [ lem : knowledge - spreads ] now guarantees that , with constant probability , the number of ignorant nodes that learn @xmath20 is only a small factor smaller .",
    "once there are @xmath31 nodes knowing about @xmath20 and each of these pushes out , each node that does not know @xmath20 has a chance of @xmath213 per round to receive a message from a node that knows @xmath20 . applying lemma [ lem : knowledge - spreads ] again finishes the proof .",
    "our proof proceeds along the lines of the simple template from section [ sec : simple - template ] and concentrates on the spreading of one coefficient vector .",
    "we define a round as a success if and only if one more node learns about it .",
    "it is clear that exactly @xmath6 successes are needed . from the definition of @xmath105 and lemma [",
    "lem : knowledge - spreads ] follows that each round is successful with probability at least @xmath214 .",
    "thus if we run the protocol for @xmath215 rounds we expect at least @xmath216 successes and by chernoff bound the probability that we get less than @xmath6 is at most @xmath217 .",
    "if we choose @xmath62 appropriately this is small enough to end up with @xmath218 after taking the union bound over the @xmath219 vectors .    in each round , at most one packet can cross the cut . for this to happen , an edge going out of the cut has to be selected and the probability for this is by definition exactly @xmath105 . in order to be able to decode the @xmath1 messages at least @xmath109 packets have to cross the cut each taking in expectation at least @xmath220 rounds .",
    "it takes with high probability at least @xmath108 rounds until @xmath109 packets have crossed the cut .",
    "we use the simple template from section [ sec : simple - template ] and concentrate on the spreading of one coefficient vector @xmath20 .",
    "we define a round to be a success if and only if the number of nodes that know about @xmath20 grows at least by a @xmath221 fraction or the number of nodes that do not know about @xmath20 shrinks at least by the same factor .",
    "+ we want to argue that at most @xmath222 successes are needed to spread @xmath20 completely .",
    "note that this is slightly better than the straight forward @xmath223 bound that would lead to @xmath224 .",
    "the improvement comes from exploiting the fact that the number of nodes that learn is an integral quantity : in the first @xmath225 successful rounds at least one node learns about @xmath20 .",
    "the next @xmath226 successful rounds at least @xmath81 nodes learn about @xmath20 and the following @xmath227 successful rounds it is @xmath228 new nodes and so on .",
    "there are @xmath229 such phases until at least @xmath31 nodes know about @xmath20 . the downward progression than follows by symmetry .",
    "the total number of successes sums up to : @xmath230 to finish the proof we show that every round has a constant success probability .",
    "this follows from lemma [ lem : knowledge - spreads ] if for a success only one node is supposed to learn about @xmath20 .",
    "if at least @xmath231 nodes are supposed to learn then by the definition of a success and of @xmath232 there are @xmath233 nodes on the knowledge cut , i.e. , at least @xmath1 nodes that do not know about @xmath20 are connected to a node that knows about @xmath20 .",
    "we invoke lemma [ lem : knowledge - spreads ] again to see that each of these nodes fails to learn about @xmath20 with probability at most @xmath234 .",
    "finally markov s inequality gives that the probability that more than @xmath235 fail to learn is at most @xmath236 .",
    "a round is therefore successful with probability at least @xmath237 .",
    "the proof is nearly identical to the one of lemma [ lem : synchbroadcast ] but instead of defining a round as a success we define successes for phases of @xmath6 consecutive rounds .",
    "using the same definition of success and following the same reasoning as before it is clear that at most @xmath238 successful phases are needed .",
    "to finish the proof we have to show that every phase has a constant success probability .",
    "for this we note again that at least @xmath239 nodes are on the knowledge - cut of @xmath20 if @xmath240 nodes need to learn about @xmath20 . for each of these @xmath241 nodes",
    "the probability that no neighboring node that knows @xmath20 is activated during @xmath6 rounds is at most @xmath242 . according to lemma [ lem : knowledge - spreads ]",
    "the probability for each of the @xmath1 nodes to fail to learn about @xmath20 is thus at most @xmath243 .",
    "markov s inequality again implies that the probability for a failed round in which more than @xmath244 fail is at most @xmath245 .",
    "we pick @xmath246 and have now that @xmath247 which is exactly the probability for having at least @xmath248 failures in @xmath44 rounds .",
    "follows directly by applying theorem [ thm : reduction ] according to the template in section [ sec : simple - template ] and the use of lemma [ lem : tail ] to get the right bound on the tail probability .",
    "[ lem : weighted - sum - bernoulli ] let @xmath249 be i.i.d .",
    "bernoulli variables with probability @xmath250 .",
    "the probability that a positively weighted sum of the variables is at most @xmath251 its expectation is at most @xmath58 : @xmath252",
    "we first scale the weights such that @xmath253 and than use the second moment method :    @xmath254    now the left - hand side is the variance of a weighted sum of i.i.d .",
    "bernoulli variables with probability @xmath255 , and as such its expectation is exactly @xmath256 . using markov s inequality on this expectation , we get that the probability we want to bound is at most : @xmath257 the last transformation holds because @xmath258 and because we can assume that all weights are at most @xmath259 .",
    "this is true because if there is a @xmath260 then already @xmath261 leads to an outcome of at least @xmath259 the expectation and the probability for this to happen is @xmath58 .",
    "we modify the proof of lemma [ lem : synchbroadcast ] only in the way that we use the stronger tail bound from corollary [ cor : tail ] instead of the simpler template from section [ sec : simple - template ] .",
    "we keep the same definition of success but prove that the success probability of a round is at least @xmath262 instead of @xmath259 as in lemma [ lem : synchbroadcast ] :    if only one node is supposed to learn for a success this is again clear by lemma [ lem : knowledge - spreads ] .",
    "if at least @xmath240 nodes nodes are needed to a success we know also by the definition of a success that at least @xmath241 nodes that do not know about @xmath20 are connected to a node that knows about it .",
    "we assign each ignorant node to exactly one node that knows about @xmath20 breaking ties arbitrarily . now according to lemma [ lem : knowledge - spreads ] with probability @xmath50 each such node independently sends out a message that is not perpendicular to @xmath20 and all ignorant nodes that are connected to it learn @xmath20 .",
    "we can now directly apply lemma [ lem : weighted - sum - bernoulli ] and obtain that we indeed have a success probability of at least @xmath262 per round .",
    "this finishes the proof .",
    "we assume each message is initially known to exactly one node and all messages are known to different nodes .",
    "this implies that exactly the @xmath263 vectors that have @xmath54 non - zero components are initially known to exactly @xmath54 nodes .",
    "we will prove that the running time @xmath264 suffices to spread all messages with probability at least @xmath265 .",
    "for this we pick a threshold @xmath266 and first look at the @xmath267 vectors that are known to at most @xmath268 nodes initially . from the proof of lemma [ lem : randomphonecall ] we know that after @xmath44 rounds each of these vectors has a probability of at most @xmath269 to not have spread completely .",
    "choosing @xmath270 therefore suffices easily to make the contribution of these vectors to the union bound at most @xmath271 .",
    "most of the @xmath47 vectors start initially known to at least @xmath268 nodes . for these vectors",
    "@xmath20 we choose the same definition of success as in the proof of lemma [ lem : randomphonecall ] : a round is successful if the number of nodes that know about @xmath20 increases by at least a constant factor @xmath206 or if the number of nodes that do not know about @xmath20 decreases by a factor of @xmath176 . we will show that if we choose @xmath176 small enough these vectors have a probability of @xmath272 to spread successfully in one round .    while with our initial analysis the start phase was the critical bottleneck we can show that the success probability for this phase can now even be pushed below @xmath262 by choosing @xmath176 small enough .",
    "in the first phase we have @xmath273 nodes that know @xmath20 and at least @xmath31 nodes that are pulling for it . each of those nodes has an independent probability of @xmath274 to hit a knowing node . because @xmath275 we have that the probability that none of these nodes pulls from a node knowing about @xmath20 is @xmath276 .",
    "lemma [ lem : knowledge - spreads ] shows than that each node that does pull from a node that knows about @xmath20 has a probability of @xmath277 to learn @xmath20 .",
    "this means more generally we have at least @xmath31 nodes that have an independent chance of @xmath278 to learn @xmath20 . for a small enough @xmath176 it is clear that the probability that at least @xmath279 nodes learn about @xmath20 can be made an arbitrarily small constant .    in the second phase",
    "there are at least @xmath31 nodes that know about @xmath20 and we want that of the remaining @xmath280 nodes at least a @xmath176-fraction learns @xmath20 .",
    "each of these nodes has a probability of at least @xmath281 to pull from a knowing node and learn @xmath20 ( see lemma [ lem : knowledge - spreads ] ) . choosing @xmath282 suffices to guarantee that the probability that at least a @xmath176-fraction learns @xmath20 is at least @xmath211 .",
    "the only reason that this probability can not be reduced is because if only one node remains to learn to learn about @xmath20 a round is successful with probability exactly @xmath211 .    using the proof from lemma [ lem : tail ] it is easy to verify that choosing @xmath44 such that @xmath264 suffices to also make a union bound over these vectors at most @xmath271 . combining this to a union bound over all vectors finished the proof by showing that the probability that after @xmath44 rounds not all vectors have spread is at most @xmath271 .",
    "we want to show that running the protocol for @xmath283 rounds , where @xmath284 suffices to spread @xmath1 messages .",
    "note that we always have @xmath285 and can also safely assume that @xmath286 .",
    "as a first step we define @xmath287 to be a lower bound for the probability that if @xmath54 nodes know about @xmath20 in the next round one more node learns about @xmath20 .",
    "note that by assumption and lemma [ lem : knowledge - spreads ] @xmath287 is lower bounded by @xmath288 and @xmath289 .",
    "we now look at @xmath6 phases in which we allow @xmath290 tries for @xmath54 nodes informing the next node about @xmath20 .",
    "the number of rounds spend in successful phases sums up to at most @xmath291 .",
    "lets now look at the probability that @xmath20 has not spread after @xmath292 steps . in this case",
    "we have at least @xmath293 failures that can occur after any of the @xmath6 phases .",
    "the probability that at least @xmath294 errors occur after phase @xmath54 is at most @xmath295 .",
    "we thus get a @xmath296 factor for every phase that does not finish `` in time '' .",
    "we also get a total factor of @xmath297 from all @xmath248 failures occurring after any round .",
    "let @xmath298 be the number of phases that finish not `` in time '' .",
    "there are exactly @xmath299 ways of distributing the @xmath248 failures to these @xmath298 phases .",
    "putting all this together we get the following upper bound on the probability that the algorithm did not converge after @xmath292 steps : @xmath300 choosing @xmath283 makes this smaller than @xmath301 .",
    "applying theorem [ thm : reduction ] now finishes the proof .",
    "the author wants to thank jon kelner for his incredible help while finishing this write - up .",
    "he also wants to thank an anonymous reviewer of a related paper , david karger and muriel mdard .",
    "a.  demers , d.  greene , c.  hauser , w.  irish , j.  larson , s.  shenker , h.  sturgis , d.  swinehart , and d.  terry , `` epidemic algorithms for replicated database maintenance , '' in _ proceedings of the 6th symposium on principles of distributed computing ( podc ) _",
    ", 1987 , pp .",
    "d.  agrawal , a.  el  abbadi , and r.  c. steinke , `` epidemic algorithms in replicated databases ( extended abstract ) , '' in _ proceedings of the 16th symposium on principles of database systems ( pods ) _ , 1997 , pp .",
    "161172 .",
    "j.  aspnes and e.  ruppert , `` an introduction to population protocols , '' in _",
    "middleware for network eccentric and mobile applications _ , b.  garbinato , h.  miranda , and l.  rodrigues , eds.1em plus 0.5em minus 0.4emspringer - verlag , 2009 , pp .",
    "97120 .",
    "d.  kempe and j.  kleinberg , `` protocols and impossibility results for gossip - based communication mechanisms , '' in _ proceedings of 43rd symposium on foundations of computer science ( focs ) _ , 2002 , pp .",
    "471480 .",
    "f.  chierichetti , s.  lattanzi , and a.  panconesi , `` almost tight bounds for rumour spreading with conductance , '' in _ proceedings of the 42nd acm symposium on theory of computing ( stoc ) _ , 2010 , pp .",
    "399408 .        t.  ho , r.  koetter , m.  medard , d.  karger , and m.  effros , `` the benefits of coding over routing in a randomized setting , '' in _ proceedings of the ieee international symposium on information theory ( isit _ , 2003 , pp . 442442 .",
    "s.  katti , d.  katabi , w.  hu , h.  rahul , and m.  medard , `` the importance of being opportunistic : practical network coding for wireless environments , '' in _",
    "proceedings 43rd allerton conference on communication , control , and computing _ , 2005 .",
    "s.  katti , h.  rahul , w.  hu , d.  katabi , m.  mdard , and j.  crowcroft , `` xors in the air : practical wireless network coding , '' _ ieee / acm transactions on networking ( ton ) _ , vol .",
    "16 , no .  3 , pp .",
    "497510 , 2008 .    c.  fragouli , j.  widmer , and j.  boudec , `` a network coding approach to energy efficient broadcasting : from theory to practice , '' in _ proceedings of the 25th international conference on computer communications ( infocom ) _ , 2006 .                  r.  bar - yehuda , o.  goldreich , and a.  itai , `` on the time complexity of broadcast in radio networks : an exponential gap between determinism and randomization , '' _ journal of computer and system sciences ( jcss ) _ , vol .  45 , no .  1 , pp .",
    "104126 , 1992 .",
    "a.  e.  g. clementi , a.  monti , and r.  silvestri , `` distributed multi - broadcast in unknown radio networks , '' in _",
    "proceedings  of 20th symposium  on principles of distributed computing ( podc ) _ , 2001 , pp",
    ". 255263 .",
    "bernhard haeupler received the b.sc . and m.sc .",
    "degree in mathematics from the technical university munich , germany , and the m.sc .",
    "degree in computer science and electrical engineering from the massachusetts institute of technology in 2007 , 2008 and 2010 respectively .",
    "he is currently a ph.d .",
    "candidate with the computer science department at mit . in 2007 - 2008",
    "he was a visiting graduate student at the computer science department of princeton university working with robert tarjan . for his graduate studies",
    "he has received an akamai / mit presidential fellowship ."
  ],
  "abstract_text": [
    "<S> we give a new technique to analyze the stopping time of gossip protocols that are based on random linear network coding ( rlnc ) . </S>",
    "<S> our analysis drastically simplifies , extends and strengthens previous results . </S>",
    "<S> we analyze rlnc gossip in a general framework for network and communication models that encompasses and unifies the models used previously in this context . </S>",
    "<S> we show , in most settings for the first time , that it converges with high probability in the information - theoretically optimal time . </S>",
    "<S> most stopping times are of the form @xmath0 where @xmath1 is the number of messages to be distributed and @xmath2 is the time it takes to disseminate one message . </S>",
    "<S> this means rlnc gossip achieves `` perfect pipelining '' .    </S>",
    "<S> our analysis directly extends to highly dynamic networks in which the topology can change completely at any time . </S>",
    "<S> this remains true even if the network dynamics are controlled by a fully adaptive adversary that knows the complete network state . </S>",
    "<S> virtually nothing besides simple @xmath3 sequential flooding protocols was previously known for such a setting .    </S>",
    "<S> while rlnc gossip works in this wide variety of networks its analysis remains the same and extremely simple . </S>",
    "<S> this contrasts with more complex proofs that were put forward to give less strong results for various special cases .    </S>",
    "<S> haeupler : analyzing network coding gossip made easy </S>"
  ]
}