{
  "article_text": [
    "we begin by determining the upper bound on @xmath19 , the probability of any pair of neurons being simultaneously active , given @xmath18 , the probability of any one neuron being active , in the large @xmath0 regime , where @xmath0 is the total number of neurons .",
    "time is discretized and we assume any neuron can spike no more than once in a time bin .",
    "we have @xmath56 because @xmath19 is the probability of a pair of neurons firing together and thus each neuron in that pair must have at least a firing probability of @xmath19 .",
    "furthermore , it is easy to see that the case @xmath18 = @xmath19 is feasible when there are only two states with non - zero probabilities : all neurons silent ( @xmath57 ) or all neurons active ( @xmath58 ) . in this case , @xmath59 .",
    "we use the term  active \" to refer to neurons that are spiking , and thus equal to one , in a given time bin , and we also refer to  active \" states in a distribution , which are those with non - zero probabilities .",
    "we now proceed to show that the lower bound on @xmath19 for large @xmath0 is @xmath60 , the value of @xmath19 consistent with statistical independence among all @xmath0 neurons .",
    "we can find the lower bound by viewing this as a linear programming problem  @xcite , where the goal is to maximize @xmath61 given the normalization constraint and the constraints on @xmath18 .",
    "it will be useful to introduce the notion of an _ exchangeable distribution _",
    "@xcite , for which any permutation of the neurons in the binary words labeling the states leaves the probability of each state unaffected . for example",
    "if @xmath62 , an exchangeable solution satisfies @xmath63 in other words , the probability of any given word depends only on the number of ones it contains , not their particular locations , for an exchangeable distribution .    in order to find the allowed values of @xmath18 and @xmath19 ,",
    "we need only consider exchangeable distributions .",
    "if there exists a probability distribution that satisfies our constraints , we can always construct an exchangeable one that also does given that the constraints themselves are symmetric ( eqs .",
    "( 1 ) and ( 2 ) ) .",
    "let us do this explicitly : suppose we have a probability distribution @xmath6 over binary words @xmath5 that satisfies our constraints but is not exchangeable we construct an exchangeable distribution @xmath64 with the same constraints as follows : @xmath65 where @xmath66 is an element of the permutation group @xmath67 on @xmath0 elements .",
    "this distribution is exchangeable by construction , and it is easy to verify that it satisfies the same symmetric constraints as does the original distribution , @xmath6 .    therefore , if we wish to find the maximum @xmath61 for a given value of @xmath18 , it is sufficient to consider exchangeable distributions .",
    "from now on in this section we will drop the @xmath68 subscript on our earlier notation , define @xmath69 to be exchangeable , and let @xmath70 be the probability of a state with @xmath1 spikes .",
    "the normalization constraint is @xmath71 here the binomial coefficient @xmath72 counts the number of states with @xmath1 active neurons .",
    "the firing rate constraint is similar , only now we must consider summing only those probabilities that have a particular neuron active .",
    "how many states are there with only a pair of active neurons given that a particular neuron must be active in all of the states ?",
    "we have the freedom to place the remaining active neuron in any of the @xmath73 remaining sites , which gives us @xmath74 states with probability @xmath75 . in general",
    "if we consider states with @xmath1 active neurons , we will have the freedom to place @xmath76 of them in @xmath73 sites , yielding : @xmath77    finally , for the pairwise firing rate , we must add up states containing a specific pair of active neurons , but the remaining @xmath78 active neurons can be anywhere else : @xmath79    now our task can be formalized as finding the maximum value of @xmath80 subject to @xmath81    this gives us the following dual problem : minimize @xmath82 given the following @xmath83 constraints ( each labeled by @xmath1 ) @xmath84 where @xmath85 is taken to be zero for @xmath86 .",
    "the principle of strong duality  @xcite ensures that the value of the objective function at the solution is equal to the extremal value of the original objective function @xmath61 .     and",
    "@xmath87 for the dual problem ( @xmath88).,width=336 ]    the set of constraints defines a convex region in the @xmath87 , @xmath89 plane as seen in figure .",
    "the minimum of our dual objective generically occurs at a vertex of the boundary of the allowed region .",
    "it can be shown that this occurs where eq .",
    "is an equality for two adjacent values of @xmath1 . calling the first of these two values @xmath90 , we then have the following two equations that allow us to determine the optimal values of @xmath89 and @xmath87 ( @xmath91 and @xmath92 , respectively ) as a function of @xmath90 @xmath93 solving for @xmath91 and @xmath92 , we find @xmath94 plugging this into eq .",
    "we find the optimal value @xmath95 is @xmath96    now all that is left it to express @xmath90 as a function of @xmath18 and take the limit as @xmath0 becomes large .",
    "this expression can be found by noting from eq .   and fig .",
    "[ fig : dual ] that at the solution , @xmath90 satisfies @xmath97 where @xmath98 is the slope , @xmath99 , of constraint @xmath1 .",
    "the expression for @xmath98 is determined from eq .  ,",
    "@xmath100 substituting eq . into eq . , we find @xmath101 this allows us to write @xmath102 where @xmath103 is between 0 and 1 for all @xmath0 .",
    "solving this for @xmath90 , we obtain @xmath104    substituting eq .   into eq .  , we find @xmath105 taking the large @xmath0 limit we find that @xmath106 and by the principle of strong duality  @xcite the maximum value of @xmath61 is @xmath107 .",
    "therefore we have shown that for large @xmath0 , the region of satisfiable constraints is simply @xmath108 as illustrated in fig .",
    "[ fig : allowed ] .     and",
    "@xmath19 that can be satisfied for at least one probability distribution in the @xmath109 limit .",
    "the purple line along the diagonal where @xmath110 is the distribution for which only the all active and all inactive states have non - zero probability .",
    "it represents the global entropy minimum for a given value of @xmath18 .",
    "the red parabola , @xmath111 , at the bottom border of the allowed region corresponds to a wide range of probability distributions , including the global maximum entropy solution for given @xmath18 in which each neuron fires independently .",
    "we find that low entropy solutions reside at this low @xmath19 boundary as well.,width=336 ]",
    "we begin by stating the general form for the solution for known mean firing rate and pairwise constraints and impose the symmetry that all statistics are equal across neurons and pairs of neurons .",
    "we will then demonstrate that for arbitrary fixed values for @xmath18 and @xmath19 , the maximum entropy must scale linearly with @xmath0 as @xmath109 .",
    "in general , the constraints can be written @xmath112 where the sums run over all @xmath8 states of the system . in order to enforce the constraints ,",
    "we can add terms involving lagrange multipliers @xmath113 and @xmath114 to the entropy in the usual fashion to arrive at a function to be maximized @xmath115 maximizing this function gives us the boltzmann distribution for an ising model @xmath116 where @xmath117 is the normalization factor or partition function .",
    "the values of @xmath113 and @xmath114 are left to be determined by ensuring this distribution is consistent with our constraints @xmath18 and @xmath19 .",
    "it can be shown that for symmetric constraints the lagrange multipliers are uniform .",
    "in other words , @xmath118 this allows us to write the following expression for the maximum entropy distribution : @xmath119    if there are @xmath120 neurons active , this becomes @xmath121 note that there are @xmath122 states with probability @xmath123 .",
    "using expression , we find the maximum entropy by using the ` fsolve ` function from the ` scipy ` package of python subject to constraints and .",
    ", as shown here for various values of @xmath18 and @xmath19 .",
    "note that this linear scaling holds even for large correlations.,width=336 ]    as fig .",
    "[ fig : max ] shows , the entropy scales linearly as a function of @xmath0 , even in cases where the correlations between all pairs of neurons ( @xmath19 ) are quite large .",
    "while this is perhaps a surprising result , we can see that this must be the case for independent neurons , the maximum entropy solution with @xmath111 .",
    "because each neuron is independent , the entropy of this system must certainly scale linearly with @xmath0 .",
    "moreover , we can construct a distribution that has entropy with linear scaling for any allowed values of @xmath18 and @xmath19 using this solution . recall that the vector @xmath124 represents the full distribution over all @xmath8 states .",
    "consider the following probability distribution @xmath125 , which we will call the `` population spike '' model .",
    "this model contains only two states with non - zero probability : the state with all neurons active ( @xmath58 ) and the state with no active neurons ( @xmath57 ) .",
    "they are weighted so that the firing rate of this model matches that of the independence model : @xmath126 as mentioned above , in this model @xmath19 is equal to its maximum allowed value , @xmath18 . because the independent model has the smallest allowed value of @xmath19 ( in the large @xmath0 limit ) , we can combine these two models to create a one - parameter family of distributions that have fixed @xmath18 value and cover all allowed values for @xmath19 .",
    "the independent part of this distribution will guarantee that the entire family has an entropy that scales linearly with @xmath0 ; thus , the true maximum must grow _ at least _ linearly with @xmath0 as well .",
    "our new distribution @xmath127 is simply @xmath128 @xmath127 has firing rate @xmath18 ( just like both @xmath129 and @xmath125 ) and @xmath130 .",
    "because entropy is a concave function  @xcite , by jensen s inequality the entropy of @xmath127 is bounded below by @xmath131 \\geq ( 1-x ) s[{{\\mathbf}p}_{ind } ] + x s[{{\\mathbf}p}_{pop } ] .\\ ] ] for fixed @xmath18 and @xmath19 the second term is a constant in @xmath0 , whereas the first term grows linearly with @xmath0 .",
    "this implies that the true maximum entropy must grow at least linearly with @xmath0 for any fixed values of @xmath18 and @xmath19 .",
    "we note that there is a simple upper bound on the entropy that also scales linearly with @xmath0 .",
    "the maximum possible entropy for fixed @xmath0 is obtained by setting all probabilities equal to one another yielding an entropy of exactly @xmath0 ( in fact , this is the entropy of the independence model with @xmath25 ) . considering that both the upper bound and lower bound for the maximum entropy for fixed @xmath18 and @xmath19 scale linearly , the maximum entropy itself must also scale linearly for large @xmath0 , consistent with our computations ( fig .",
    "[ fig : max ] ) .",
    "our goal is to minimize the entropy function @xmath132 where @xmath133 is the number of states , the @xmath134 satisfy a set of @xmath135 independent linear constraints , and @xmath136 for all @xmath1 . for the main problem we consider , @xmath137 and normalization , mean firing rates and pairwise firing rates give @xmath138 . for the exchangeable case with symmetric constraints , @xmath139 and @xmath140 .",
    "our task is therefore to minimize a globally concave function over a @xmath141 dimensional linear ( affine ) space @xmath142 contained in the ( compact ) simplex @xmath143 .",
    "it is well known that the minima of such a problem occur at the vertices of the boundary of the space  @xcite , which necessarily have some @xmath134 equal to zero , unless @xmath142 intersects the simplex in a single point . moreover ,",
    "if a distribution satisfying the constraints exists , then there is one with at most @xmath135 non - zero @xmath134 ( e.g. , from arguments as in @xcite ) .",
    "together , these two facts imply that there are minimum entropy distributions with a maximum of @xmath135 non - zero @xmath134 ( and can occasionally have fewer ) .",
    "this means that even though the state space may grow exponentially with @xmath0 , the support of the minimum entropy solution for fixed means and pairwise correlations will only scale quadratically with @xmath0 .",
    "in fact , we know that for certain values of @xmath18 and @xmath19 solutions can have a far smaller support because the construction shown in appendix  [ sec : comm_construction ] has a support size that scales only linearly with @xmath0 .",
    "although the values of the firing rate ( @xmath18 ) and pairwise correlations ( @xmath19 ) may be identical for each neuron and pair of neurons , the probability distribution that gives rise to these statistics need not be exchangeable as we have already shown .",
    "indeed , as we explain below , it is possible to construct non - exchangeable probability distributions that have dramatically lower entropy then both the maximum and the minimum entropy for exchangeable distributions .",
    "that said , exchangeable solutions are interesting in their own right because they have large @xmath0 scaling behavior that is distinct from the global entropy minimum , and they provide a symmetry that can be used to lower bound the information transmission rate close to the maximum possible across all distributions .",
    "restricting ourselves to exchangeable solutions represents a significant simplification . in the general case ,",
    "there are @xmath8 probabilities to consider for a system of @xmath0 neurons .",
    "there are @xmath0 constraints on the firing rates ( one for each neuron ) and @xmath144 pairwise constraints ( one for each pair of neurons ) .",
    "this gives us a total number of constrains ( @xmath135 ) that grows quadratically with @xmath0 : @xmath145 however in the exchangeable case , all states with the same number of spikes have the same probability so there are only @xmath146 free parameters .",
    "moreover , the number of constraints becomes 3 as there is only one constraint each for normalization , firing rate , and pairwise firing rate ( as expressed in eqs .",
    ", , and , respectively ) .    in general , the minimum entropy solution for exchangeable distributions should have the minimum support consistent with these three constraints .",
    "therefore , the minimum entropy solution should have at most three non - zero probabilities .    for the symmetrical case with @xmath25 and @xmath26",
    ", we can construct the exchangeable distribution with minimum entropy for all even @xmath0 .",
    "this distribution consists of the all ones state , the all zeroes state , and all states with @xmath147 ones .",
    "the constraint @xmath25 implies that @xmath148 , and the condition @xmath26 implies @xmath149 which corresponds to an entropy of @xmath150 .",
    "\\label{eq : exch__sym_min_approx}\\end{aligned}\\ ] ]    for arbitrary values of @xmath18 , @xmath19 and @xmath0 , it is difficult to determine from first principles which three probabilities are non - zero for the minimum entropy solution , but fortunately the number of possibilities @xmath151 is now small enough that we can exhaustively search by computer to find the set of non - zero probabilities corresponding to the lowest entropy .     for various values of @xmath18 and @xmath19 .",
    "note that , like the maximum entropy , the exchangeable minimum entropy scales linearly with @xmath0 as @xmath109 , albeit with a smaller slope for @xmath152 .",
    "we can calculate the entropy exactly for @xmath18 = 0.5 and @xmath19 = 0.25 as @xmath109 , and we find that the leading term is indeed linear : @xmath153$].,width=336 ]    using this technique , we find that the scaling behavior of the exchangeable minimum entropy is linear with @xmath0 as shown in fig .  [",
    "fig : exch_min ] .",
    "we find that the asymptotic slope is positive , but less than that of the maximum entropy curve , for all @xmath152 .",
    "for the symmetrical case , @xmath111 , our exact expression eq .   for the exchangeable distribution consisting of the all ones state , the all zeros state , and all states with @xmath147 ones agrees with the minimum entropy exchangeable solution found by exhaustive search , and in this special case the asymptotic slope is identical to that of the maximum entropy curve .",
    "we can construct a probability distribution with roughly @xmath154 states with nonzero probability out of the full @xmath8 possible states of the system such that @xmath155 where @xmath0 is the number of neurons in the network and @xmath13 is the number of neurons that are active in every state . using this solution as a basis , we can include the states with all neurons active and all neurons inactive to create a low entropy solution for all allowed values for @xmath18 and @xmath19 ( see appendix  [ sec : validity ] ) .",
    "we refer to the entropy of this low entropy construction @xmath37 to distinguish it from the entropy ( @xmath36 ) of another low entropy solution described in the next section .",
    "our construction essentially goes back to joffe  @xcite as explained by luby in @xcite .",
    "we derive our construction by first assuming that @xmath0 is a prime number , but this is not actually a limitation as we will be able to extend the result to all values of @xmath0 .",
    "specifically , non - prime system sizes are handled by taking a solution for a larger prime number and removing the appropriate number of neurons .",
    "it should be noted that occasionally the solution derived using the next largest prime number does not necessarily have the lowest entropy and occasionally we must use even larger primes to find the minimum entropy possible using this technique ; all plots in the main text were obtained by searching for the lowest entropy solution using the 10 smallest primes that are each at least as great as the system size @xmath0 .",
    "we begin by illustrating our algorithm with a concrete example ; following this illustrative case we will prove that each step does what we expect in general .",
    "consider @xmath88 , and @xmath156 .",
    "the algorithm is as follows :    1 .",
    "begin with the state with @xmath156 active neurons in a row : + [ cols=\"^ \" , ]    4 .",
    "note that each state is represented twice in this collection , removing duplicates we are left with @xmath157 total states . by inspection",
    "we can verify that each neuron is active in @xmath158 states and each pair of neurons is represented in @xmath159 states .",
    "wighting each state with equal probability gives us the values for @xmath18 and @xmath19 stated in eq .",
    "now we will prove that this construction works in general for @xmath0 prime and any value of @xmath13 by establishing ( 1 ) that step 2 of the above algorithm produces a set of states with @xmath13 spikes , ( 2 ) that this method produces a set of states that when weighted with equal probability yield neurons that all have the same firing rates and pairwise statistics , and ( 3 ) that this method produces at least double redundancy in the states generated as stated in step 4 ( although in general there may be a greater redundancy ) .",
    "in discussing ( 1 ) and ( 2 ) we will neglect the issue of redundancy and consider the states produced through step 3 as distinct .",
    "first we prove that step 2 always produces states with @xmath13 neurons , which is to say that no two spikes are mapped to the same location as we shift them around .",
    "we will refer to the identity of the spikes by their location in the original starting state ; this is important as the operations in step 2 and 3 will change the relative ordering of the original spikes in their new states . with this in mind",
    ", the location of the @xmath1th spike with a spacing of @xmath160 between them will result in the new location @xmath161 ( here the original state with all spikes in a row is @xmath162 ) : @xmath163 where @xmath164 . in this form",
    ", our statement of the problem reduces to demonstrating that for given values of @xmath160 and @xmath0 , no two values of @xmath1 will result in the same @xmath161 .",
    "this is easy to show by contradiction .",
    "if this were the case , @xmath165 for this to be true , either @xmath160 or @xmath166 must contain a factor of @xmath0 , but each are smaller than @xmath0 so we have a contradiction .",
    "this also demonstrates why @xmath0 must be prime  if it were not , it would be possible to satisfy this equation in cases where @xmath160 and @xmath166 contain between them all the factors of @xmath0 .",
    "it is worth noting that this also shows that there is a one - to - one mapping between @xmath160 and @xmath161 given @xmath1 . in other words ,",
    "each spike is taken to every possible neuron in step 2 .",
    "for example , if @xmath88 , and we fix @xmath167 : @xmath168    if we now perform the operation in step 3 , then the location @xmath161 of spike @xmath1 becomes @xmath169 where @xmath170 is the amount by which the state has been rotated ( the first column in step 3 is @xmath171 , the second is @xmath172 , etc . ) .",
    "it should be noted that step 3 trivially preserves the number of spikes in our states so we have established that steps 2 and 3 produce only states with @xmath13 spikes .",
    "we now show that each neuron is active , and each pair of neurons is simultaneously active , in the same number of states .",
    "this way when each of these states is weighted with equal probability , we find symmetric statistics for these two quantities .",
    "beginning with the firing rate , we ask how many states contain a spike at location @xmath161 . in other words , how many combinations of @xmath160 , @xmath1 , and @xmath170 can we take such that eq .",
    "is satisfied for a given @xmath161 . for each choice of @xmath160 and @xmath1",
    "there is a unique value of @xmath170 that satisfies the equation .",
    "@xmath160 can take values between @xmath173 and @xmath73 , and @xmath1 takes values from @xmath174 to @xmath175 , which gives us @xmath176 states that include a spike at location @xmath161 . dividing by the total number of states @xmath177 we obtain an average firing rate of @xmath178    consider neurons at @xmath179 and @xmath180 ; we wish to know how many values of @xmath160 , @xmath170 , @xmath181 and @xmath182 we can pick so that @xmath183 taking the difference between these two equations , we find @xmath184 from our discussion above , we know that this equation uniquely specifies @xmath160 for any choice of @xmath181 and @xmath182 .",
    "furthermore , we must pick @xmath170 such that eqs .   and are satisfied .",
    "this means that for each choice of @xmath181 and @xmath182 there is a unique choice of @xmath160 and @xmath170 , which results in a state that includes active neurons at locations @xmath179 and @xmath180 .",
    "swapping @xmath181 and @xmath182 will result in a different @xmath160 and @xmath170 .",
    "therefore , we have @xmath185 states that include any given pair - one for each choice of @xmath181 and @xmath182 . dividing this number by the total number of states",
    ", we find a correlation @xmath19 equal to @xmath186 where @xmath0 is prime .",
    "finally we return to the question of redundancy among states generated by steps 1 through 3 of the algorithm .",
    "although in general there may be a high level of redundancy for choices of @xmath13 that are small or close to @xmath0 , we can show that in general there is at least a twofold degeneracy .",
    "although this does not impact our calculation of @xmath18 and @xmath19 above , it does alter the number of states , which will affect the entropy of system .",
    "the source of the twofold symmetry can be seen immediately by noting that the third and fourth rows of our example contain the same set of states as the second and first respectively .",
    "the reason for this is that each state in the @xmath187 case involves spikes that are one leftward step away from each other just as @xmath162 involves spikes that are one rightward shift away from each other .",
    "the labels we have been using to refer to the spikes have reversed order but the set of states are identical .",
    "similarly the @xmath188 case contains all states with spikes separated by two leftward shifts just as the @xmath189 case .",
    "therefore , the set of states with @xmath190 is equivalent to the set of states with @xmath191 . taking this degeneracy into account ,",
    "there are at most @xmath157 unique states ; each neuron spikes in @xmath158 of these states and any given pair spikes together in @xmath159 states .    because these states each have equal probability the entropy of this system",
    "is bounded from above by @xmath192 where @xmath0 is prime . as mentioned above ,",
    "we write this as an inequality because further degeneracies among states beyond the factor of two that always occurs are possible for some prime numbers .",
    "in fact , in order to avoid non - monotonic behavior , the curves for @xmath193 shown in figs .",
    "1,2 of the main text were generated using the lowest entropy found for the 10 smallest primes greater than @xmath0 for each value of @xmath0 .",
    "we can extend this result to arbitrary values for @xmath0 including non - primes by invoking the bertrand - chebyshev theorem , which states that there always exists at least one prime number @xmath69 with @xmath194 for any integer @xmath195 : @xmath196 where @xmath0 is any integer .",
    "unlike the maximum entropy and the entropy of the exchangeable solution , which we have shown to both be extensive quantities , this scales only logarithmically with the system size @xmath0 .",
    "we have found another low entropy construction in the regime most relevant for communications systems ( @xmath25 , @xmath26 ) that allows us to satisfy our constraints for a system of @xmath0 neurons with only @xmath197 active states .",
    "the algorithm to determine the states needed is recursive in that the states needed for @xmath198 are built from the states needed for @xmath199 , where @xmath200 is any integer greater than 2 .",
    "we begin with @xmath201 . here",
    "we can easily write down a set of states that when weighted equally lead to the desired statistics .",
    "listing these states as rows of zeros and ones , we see that they include all possible two - neuron states : @xmath202    in order to find the states needed for @xmath203 we replace each @xmath173 in the above by @xmath204 and each @xmath174 by @xmath205 to arrive at a new array for twice as many neurons and twice as many states with nonzero probability : @xmath206    by inspection , we can verify that each new neuron is spiking in half of the above states and each pair is spiking in a quarter of the above states .",
    "this procedure preserves @xmath207 , @xmath208 , and @xmath52 for all neurons ; thus providing a distribution that mimics the statistics of independent binary variables up to third order ( although it does not for higher orders ) .",
    "let us consider the the proof that @xmath25 is preserved by this transformation .",
    "in the process of doubling the number of states from @xmath209 to @xmath210 , each neuron with firing rate @xmath211 `` produces '' two new neurons with firing rates @xmath212 and @xmath213 .",
    "it is clear from eqs .   and that we obtain the following two relations , @xmath214 it is clear from these equations that if we begin with @xmath215 that this will be preserved by this transformation . by similar , but more tedious , methods",
    "one can show that @xmath208 , and @xmath52 .",
    "therefore , we are able to build up arbitrarily large groups of neurons that satisfy our statistics using only @xmath197 states by repeating the procedure that took us from @xmath216 to @xmath217 .",
    "since these states are weighted with equal probability we have an entropy that grows only logarithmically with @xmath0 @xmath218    we mention briefly a geometrical interpretation of this probability distribution . the active states in this distribution",
    "can be thought of as a subset of @xmath197 corners on an @xmath0 dimensional hypercube with the property that the separation of almost every pair is the same .",
    "specifically , for each active state , all but one of the other active states has a hamming distance of exactly @xmath147 from the original state ; the remaining state is on the opposite side of the cube , and thus has a hamming distance of @xmath0 . in other words , for any pair of polar opposite active states , there are @xmath219 active states around the  equator . \"    we can extend eq .",
    "( [ eq : con_for_power_two ] ) to arbitrary numbers of neurons that are not multiples of 2 by taking the least multiple of 2 at least as great as @xmath0 , so that in general : @xmath220 by adding two other states we can extend this probability distribution so that it covers most of the allowed region for @xmath18 and @xmath19 while remaining a low entropy solution , as we now describe .",
    "we remark that the authors of  @xcite provide a lower bound of @xmath221 for the sample size possible for a pairwise independent binary distribution , making the sample size of our novel construction essentially optimal .",
    "we now show that each of these low entropy probability distributions can be generalized to cover much of the allowed region depicted in fig .  [",
    "fig : allowed ] ; in fact , the distribution derived in appendix  [ sec : prime_construction ] can be extended to include all possible combinations of the constraints @xmath18 and @xmath19 .",
    "this can be accomplished by including two additional states : the state where all neurons are silent and the state where all neurons are active .",
    "if we weight these states by probabilities @xmath57 and @xmath58 respectively and allow the @xmath157 original states to carry probability @xmath222 in total , normalization requires @xmath223 we can express the value of the new constraints ( @xmath224 and @xmath225 ) in terms of the original constraint values ( @xmath18 and @xmath19 ) as follows : @xmath226 these values span a triangular region in the @xmath18-@xmath19 plane that covers the majority of satisfiable constraints .",
    "[ fig : constructed ] illustrates the situation for @xmath25 . note that by starting with other values of @xmath18 , we can construct a low entropy solution for any possible constraints @xmath224 and @xmath225 .    with the addition of these two states",
    ", the entropy of the expanded system @xmath227 is bounded from above by @xmath228 for given values of @xmath224 and @xmath225 , the @xmath134 are fixed and only the first term depends on @xmath0 .",
    "this means that , like the original distribution , the entropy of this distribution scales logarithmically with @xmath0 . therefore , by picking our original distribution properly , we can find low entropy distributions for any @xmath18 and @xmath19 for which the number of active states grows as a polynomial in @xmath0 ( see fig .  [",
    "fig : constructed ] ) .",
    "similarly , we can extend the range of validity for the construction described in appendix  [ comm_construction ] to the triangular region shown in fig .",
    "[ fig : allowed ] by assigning probabilities @xmath57 , @xmath58 , and @xmath229 to the all silent state , all active state , and the total probability assigned to the remaining @xmath230 states of the original model , respectively",
    ". the entropy of this extended distribution must be no greater than the entropy of the original distribution ( eq .  ) , since the same number of states are active , but now they are not weighted equally , so this remains a low entropy distribution .     and",
    "@xmath19 for all possible probability distributions , replotted from fig .",
    "[ fig : allowed ] .",
    "the triangular blue shaded region includes all possible values for the constraints beginning with either of our constructed solutions with @xmath25 and @xmath26 .",
    "choosing other values of @xmath18 and @xmath19 for the construction described in appendix  [ sec : prime_construction ] would move the vertex to any desired location on the @xmath111 boundary .",
    "note that even with this solution alone , we can cover most of the allowed region.,width=336 ]",
    "using the concavity of the entropy function , we can derive a _",
    "lower _ bound on the minimum entropy .",
    "our lower bound asymptotes to a constant except for the special case @xmath25 and @xmath26 , which is especially relevant for communication systems since it matches the low order statistics of the maximum entropy solution .",
    "we begin by re - expressing the entropy as follows : @xmath231 where @xmath124 represents the full vector of all @xmath8 state probabilities .",
    "note that @xmath232 can be thought of as a probability distribution over @xmath233 since its elements are nonnegative and they sum to one . in this form",
    ", we can take advantage of the convexity of @xmath234 by using jensen s inequality to obtain a _",
    "lower _ bound on the entropy : @xmath235 in the final step we use the fact that @xmath236 is normalized .",
    "now we seek an upper bound on @xmath237 .",
    "this can be obtained by starting with the matrix representation @xmath238 of the constraints ( for now , we consider each state of the system , @xmath239 , as binary column vectors , where @xmath1 labels the state and each of the @xmath0 components is either 1 or 0 ) : @xmath240 where @xmath238 is an @xmath241 matrix . in this form ,",
    "the diagonal entries of @xmath238 , @xmath242 , are equal to @xmath243 and the off diagonal entries , @xmath244 , are equal to @xmath245 .",
    "of course , in the symmetric problem we consider here , all diagonal entries are the same , and all off - diagonal entries are the same .",
    "we will take this to be the case from this point on .    for the calculation that follows , it is expedient to represent words of the system as @xmath246 rather than @xmath247 ( _ i.e. _",
    ", -1 represents a silent neuron instead of 0 ) .",
    "the relationship between the two can be written @xmath248 where @xmath249 is the vector of all ones . using this expression",
    ", we can relate @xmath250 to @xmath238 : @xmath251 for our symmetric case , this reduces to @xmath252    returning to eq .   to find an upper bound on @xmath237 , we take the square of the frobenius norm of @xmath250 : @xmath253 the final line is where our new representation pays off : in this representation , @xmath254 .",
    "this gives us the desired upper bound for @xmath237 : @xmath255"
  ],
  "abstract_text": [
    "<S> maximum entropy models are increasingly being used to describe the collective activity of neural populations with measured mean neural activities and pairwise correlations , but the full space of probability distributions consistent with these constraints has not been explored . </S>",
    "<S> we provide lower and upper bounds on the entropy for both the minimum and maximum entropy distributions over binary units with fixed mean and pairwise correlation , and we construct distributions for several relevant cases . </S>",
    "<S> surprisingly , the minimum entropy solution has entropy scaling logarithmically with system size , unlike the linear behavior of the maximum entropy solution , resolving an open question in neuroscience . </S>",
    "<S> our results show how only small amounts of randomness are needed to mimic low - order statistical properties of highly entropic distributions , and we discuss some applications for engineered and biological information transmission systems .    </S>",
    "<S> maximum entropy models are central to the study of physical systems in thermal equilibrium  @xcite , and they have recently been found to model protein folding  @xcite , antibody diversity  @xcite , neural population activity  @xcite , and even flock behavior  @xcite quite well ( _ cf . </S>",
    "<S> _ , @xcite ) . </S>",
    "<S> this is perhaps surprising since the usual physical arguments involving ergodicity or equality among energetically accessible states are not obviously applicable for such systems , though such models have been justified in terms of imposing no structure beyond what is explicitly measured  @xcite . </S>",
    "<S> conversely , it is not clear to what extent this good agreement was inevitable . </S>",
    "<S> if the space of distributions were sufficiently constrained by observations , then agreement is an unavoidable consequence of the constraints rather than a consequence of the unique suitability of the maximum entropy model . in neuroscience </S>",
    "<S> , there is also controversy  @xcite over the notion that small pairwise correlations can conspire to constrain the behavior of large neural ensembles , and it has been shown  @xcite that pairwise models do not always allow accurate extrapolation from small populations to large ensembles .    </S>",
    "<S> previous authors have studied these issues with maximum entropy models expanded to second-  @xcite , third-  @xcite , and fourth - order  @xcite . </S>",
    "<S> here we use non - perturbative methods to derive rigorous upper and lower bounds on the entropy of the _ minimum _ entropy distribution for fixed means and pairwise correlations , and we construct explicit low and high entropy models for the full range of possible uniform first- and second - order constraints ( eqs . - ; figs . </S>",
    "<S> [ fig : svn ] , [ fig : svnu ] ) . </S>",
    "<S> interestingly , we find that entropy differences between models with the same first- and second - order statistics can be nearly as large as is possible between any two arbitrary distributions . </S>",
    "<S> thus , entropy is only weakly constrained by these statistics , and the success of maximum entropy models in biology  @xcite , when it occurs for large enough systems  @xcite , represents a real triumph of the maximum entropy approach .    </S>",
    "<S> our results demonstrate that empirically measured first- , second- , and third - order statistics are essentially inconsequential for testing coding optimality in a broad class of engineered information transmission systems , whereas the existence of other statistical properties , such as finite exchangeability  @xcite , do guarantee information transmission near channel capacity  @xcite , the maximum possible information rate given the properties of the information channel </S>",
    "<S> . a better understanding of minimum entropy distributions subject to constraints is also important for minimal state space realization @xcite  a form of optimal model selection based on an interpretation of occam s razor complementary to that of jaynes  @xcite . </S>",
    "<S> our results also have implications for computer science as algorithms for generating binary random variables with low entropy have found many applications ( e.g. ,  @xcite ) .    </S>",
    "<S> consider an abstract description of a neural ensemble consisting of @xmath0 spiking neurons . in any given time bin , </S>",
    "<S> each neuron @xmath1 has binary state @xmath2 denoting whether it is currently firing an action potential ( @xmath3 ) or not ( @xmath4 ) . </S>",
    "<S> the state of the full network is represented by @xmath5 . </S>",
    "<S> let @xmath6 be the probability of state @xmath7 so that the distribution over all @xmath8 states of the system is represented by @xmath9^{2^n}$ ] , @xmath10 .    in neural studies using maximum entropy models </S>",
    "<S> , electrophysiologists typically measure the time - averaged firing rates @xmath11 and pairwise event rates @xmath12 and fit the maximum entropy model consistent with these constraints , yielding a boltzmann distribution for an ising spin glass  @xcite . this </S>",
    "<S>  inverse \" problem of inferring the interaction and magnetic field terms in an ising spin glass hamiltonian that produce the measured means and correlations is nontrivial , but there has been progress  @xcite . </S>",
    "<S> the maximum entropy distribution is not the only one consistent with these observed statistics , however . </S>",
    "<S> in fact , there are many others , and we will refer to the complete set of these as the `` solution space '' for a given set of constraints . </S>",
    "<S> little is known about the minimum entropy permitted for a particular solution space .    </S>",
    "<S> our question is , given a set of observed mean firing rates and pairwise correlations between neurons , what are the possible entropies for the system ? </S>",
    "<S> we will denote the maximum ( minimum ) entropy compatible with a given set of imposed correlations up to order @xmath13 by @xmath14 ( @xmath15 ) . </S>",
    "<S> the maximum entropy framework  @xcite provides a hierarchical representation of neural activity : as increasingly higher order correlations are measured , the corresponding model entropy @xmath14 is reduced until , at least in principle , it reaches a lower limit . </S>",
    "<S> here we introduce a complementary , minimum entropy framework : as higher order correlations are specified , the corresponding model entropy @xmath15 is increased until all correlations are known . </S>",
    "<S> the range of possible entropies for any given set of constraints is the gap ( @xmath16 ) between these two model entropies , and our primary concern is whether this gap is greatly reduced for any observed first- or second - order statistics for any system size @xmath0 . </S>",
    "<S> we find that the gap grows linearly with @xmath0 , up to a logarithmic correction .    </S>",
    "<S> we will restrict ourselves here to symmetric constraints ; that is , values of _ mean firing rates _ and _ pairwise correlations _ are uniform : @xmath17 given symmetric constraints , we find the following bounds on the maximum and minimum entropies for fixed values of @xmath18 and @xmath19 . </S>",
    "<S> for the _ maximum entropy _ </S>",
    "<S> : @xmath20 where @xmath21 and @xmath22 is a constant that only depends on @xmath18 and @xmath19 . for the _ minimum entropy _ : @xmath23 where @xmath24 . in most cases , </S>",
    "<S> the lower bound in eq .   </S>",
    "<S> asymptotes to a constant for large @xmath0 , but in the special case where @xmath18 and @xmath19 have values consistent with _ independent neurons _ ( @xmath25 and @xmath26 ) , we can give the tighter bound : @xmath27    an important class of probability distributions are the _ exchangeable distributions _  @xcite , which have the property that the probability of a sequence of ones and zeros is only a function of the number of ones in the binary string . we have constructed a family of exchangeable distributions that we conjecture is a minimum entropy exchangeable solution with entropy @xmath28 that scales linearly with @xmath0 : @xmath29 where @xmath30 and @xmath31 are constants that only depend on @xmath18 and @xmath19 . we have empirically confirmed that this is indeed a minimum entropy exchangeable solution for @xmath32 . </S>",
    "<S> we obtained these bounds by constructing families of low entropy distributions and exploiting the geometry of the entropy function . </S>",
    "<S> entropy is a strictly concave function of the probabilities and therefore has a unique maximum that can be identified using standard methods @xcite , at least for sufficiently small or symmetric systems . </S>",
    "<S> indeed , it is easy to show ( see appendix  [ sec : max_entropy ] ) that the maximum entropy @xmath33 for any system with specified mean and pairwise correlation scales linearly with @xmath0 ( eq .  , fig .  </S>",
    "<S> [ fig : svn ] ) .    </S>",
    "<S> by contrast , the minimum entropy distribution exists at a vertex of the allowed space , where most states have probability zero ( @xcite ; see appendix  [ sec : support ] ) . </S>",
    "<S> our challenge then is to determine in which vertex ( or vertices ) a minimum resides . </S>",
    "<S> the entropy function is nonlinear , precluding approaches from linear programming , and the dimensionality of the probability space grows exponentially with @xmath0 , making exhaustive search and gradient descent techniques intractable for @xmath34 . </S>",
    "<S> fortunately , we can compute a lower bound @xmath35 on the entropy of the minimum entropy solution for all @xmath0 ( fig .  </S>",
    "<S> [ fig : svn ] ) , and we have constructed two families of explicit solutions with low entropies ( @xmath36 and @xmath37 ; figs .  [ </S>",
    "<S> fig : svn],[fig : svnu ] ) for a broad parameter regime covering all allowed values for @xmath18 and @xmath19 . using the concavity of the entropy function together with jensen s inequality </S>",
    "<S> , one can derive an upper bound on the entropy  @xcite , but similar methods also allow us to obtain a lower bound @xmath35 as in eq . on the entropy ( see appendix  [ sec : lower_bound ] ) : @xmath38 where @xmath24 , and @xmath39 is the minimum entropy given a network of size @xmath0 with constraints @xmath18 and @xmath19 . </S>",
    "<S> this lower bound asymptotes to the constant value @xmath40 as @xmath0 becomes large except for the special case : @xmath41 where @xmath42 vanishes . in the large @xmath0 limit </S>",
    "<S> , we have the inequality @xmath43 ( see appendix  [ sec : range_of_mu_nu ] ) , so the only values of @xmath18 and @xmath19 satisfying eq .  </S>",
    "<S> ( [ alpha_zero ] ) are @xmath44 in this particular case , the lower bound eq .  </S>",
    "<S> ( [ eq : s2_low_bound ] ) scales logarithmically with @xmath0 , rather than as a constant , but for large systems this difference is insignificant compared with the linear dependence @xmath45 of the maximum entropy solution ( _ i.e. _ , @xmath0 fair i.i.d . </S>",
    "<S> bernoulli variables ) .    </S>",
    "<S> in addition to this lower bound , we can also construct probability distributions that provide upper bounds on the entropy of a minimum entropy solution . </S>",
    "<S> each of these solutions has an entropy that grows logarithmically with @xmath0 ( see appendices  [ sec : prime_construction ] ,  [ comm_construction ] , eqs . </S>",
    "<S> - ) : @xmath46 where @xmath47 is the ceiling function and @xmath48 represents the smallest prime at least as large as its argument . </S>",
    "<S> thus , there is always a solution whose entropy grows no faster than logarithmically with the size of the system , for any observed levels of mean activity and pairwise correlation .    as illustrated in fig .  </S>",
    "<S> [ fig : svn]a , for large binary systems with first- and second - order statistics matched to those of many neural populations , which have low firing rates and correlations slightly above chance  ( @xcite ; @xmath49 , @xmath50 ) , the range of possible entropies grows almost linearly with @xmath0 , despite the highly symmetric constraints imposed ( eqs .  </S>",
    "<S> ( [ eq : sym_mu ] ) and ( [ eq : sym_nu ] ) ) . </S>",
    "<S> consider the special case of first- and second - order constraints ( eq .  [ eq : munu_max_s ] ) that correspond to the unconstrained global maximum entropy distribution . </S>",
    "<S> for these highly symmetric constraints , both our upper and lower bounds on the minimum entropy grow logarithmically with @xmath0 , rather than just the upper bound as we found for the neural regime ( fig .  </S>",
    "<S> [ fig : svn]a ) . </S>",
    "<S> in fact , we have constructed an explicit solution ( eq . </S>",
    "<S> ( [ h_for_con ] ) ; figs .  </S>",
    "<S> [ fig : svn]b,[fig : svnu]a , d , e , h ; appendix  [ sec : comm_construction ] ) , whose entropy @xmath36 is never more than two bits above our lower bound ( eq .  ( [ eq : s2_low_bound ] ) ) for all @xmath0 . </S>",
    "<S> clearly then , these constraints alone do not guarantee a level of independence of the neural activities commensurate with the maximum entropy distribution . by varying the relative probabilities of states in this explicit construction </S>",
    "<S> we can make it satisfy a much wider range of @xmath18 and @xmath19 values covering most of the allowed region ( see appendix  [ sec : validity ] ) while still remaining a distribution whose entropy grows only logarithmically with @xmath0 .    </S>",
    "<S> the large gap between @xmath28 and @xmath51 demonstrates that a distribution can dramatically reduce its entropy if it is allowed to violate the symmetries present in the constraints . </S>",
    "<S> this is reminiscent of other examples of symmetry - breaking in physics for which a system finds an equilibrium that breaks symmetries present in the physical laws . </S>",
    "<S> however , here the situation is in a sense reversed : observed statistics obeying a symmetry ( the observations about the system ) are produced by an underlying model that does not .    </S>",
    "<S> we now examine consequences for engineered communication systems . </S>",
    "<S> specifically , consider a device such as a digital camera that exploits compressed sensing  @xcite to reduce the dimensionality of its image representations . </S>",
    "<S> a compressed sensing scheme involves taking inner products between the vector of raw pixel values and a set of random vectors , followed by a digitizing step to output @xmath0-bit strings . </S>",
    "<S> theorems exist for expected information rates of compressed sensing systems , but we are unaware of any that do not depend on some knowledge about the input signal , such as its sparse structure  @xcite . without such knowledge </S>",
    "<S> , it would be desirable to know which empirically measured output statistics could tell us whether such a camera is utilizing as much of the @xmath0 bits of channel capacity as possible for each photograph .    </S>",
    "<S> as we have shown , even if the mean of each bit is @xmath25 , and the second- and third - order correlations are at chance level ( @xmath26 ; @xmath52 , for distinct @xmath53 ) , consistent with the maximum entropy distribution , it is possible that the shannon mutual information shared by the original pixel values and the compressed signal is only on the order of @xmath54 bits , well below the channel capacity ( @xmath0 bits ) of this ( noiseless ) output stream . </S>",
    "<S> we emphasize that , in such a system , the transmitted information is limited not by corruption due to noise , which can be neglected for many applications involving digital electronic devices , but instead by the nature of the second- and higher - order correlations in the output .    thus , measuring pairwise or even triplet - wise correlations between all bit pairs and triplets is insufficient to provide a useful floor on the information rate , no matter what values are empirically observed . </S>",
    "<S> however , measuring the extent to which other statistical properties are obeyed can yield strong guarantees of system performance . </S>",
    "<S> in particular , exchangeability is one such constraint . </S>",
    "<S> fig .  </S>",
    "<S> [ fig : svn ] illustrates the near linear behavior of the lower bound on information ( @xmath55 ) for distributions obeying exchangeability , in both the neural regime ( cyan curve , panel ( a ) ) and the regime relevant for our engineering example ( cyan curve , panel ( b ) ) . </S>",
    "<S> we experimentally find that any exchangeable distribution has as much entropy as the maximum entropy solution , up to terms of order @xmath54 ( see appendix  [ sec : exch_entropy ] ) . in computer science , </S>",
    "<S> it is sometimes possible to construct efficient deterministic algorithms from randomized ones by utilizing low entropy distributions . </S>",
    "<S> one common technique is to replace the independent binary random variables used in a randomized algorithm with those satisfying only pairwise independence @xcite . in many cases </S>",
    "<S> , such a randomized algorithm can be shown to succeed even if the original independent random bits are replaced by pairwise independent ones having significantly less entropy . </S>",
    "<S> in particular , efficient derandomization can be accomplished in these instances by finding pairwise independent distributions with small sample spaces . </S>",
    "<S> several such designs are known and use tools from finite fields and linear codes @xcite , combinatorial block designs @xcite , hadamard matrix theory @xcite , and linear programming @xcite , among others . </S>",
    "<S> our construction here of a pairwise independent distribution with entropy @xmath36 adds to this literature and is completely elementary .    </S>",
    "<S> maximum entropy models are powerful tools for understanding physical systems and they are proving to be useful for describing biology as well , but a deeper understanding of the full solution space is needed as we explore systems less amenable to arguments involving ergodicity or equally accessible states . in some settings , minimum entropy models can also provide a floor on information transmission , complementary to channel capacity , which provides a ceiling on system performance .      the authors would like to thank tony bell , michael berry , bill bialek , amir khosrowshahi , peter latham , lionel levine , fritz sommer , and all members of the redwood center for many useful discussions . </S>",
    "<S> mrd is grateful for support from the hellman foundation , the mcdonnell foundation , the mcknight foundation , and the mary elizabeth rennie endowment for epilepsy research . </S>",
    "<S> ch was supported under an nsf all - institutes postdoctoral fellowship administered by the mathematical sciences research institute through its core grant dms-0441170 . </S>"
  ]
}