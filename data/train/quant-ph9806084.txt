{
  "article_text": [
    "the algorithms presented here are useful to factor very large numbers , that is , thousands to millions of digits .",
    "quantum computers which could do that will at best be available in several decades .",
    "still i think it is worth it to investigate already now what such machines could do .",
    "also it shows that messages encrypted even with very large rsa keys could possibly be decrypted in several decades .",
    "we do nt know the architecture and power of future ( sizable ) quantum computers .",
    "it may therefore seem premature to optimize a quantum algorithm and i admit that my results should be considered as a rough guide rather than precise predictions .",
    "i make plausible assumptions on the architecture of future quantum computers which i will try to justify below .",
    "my main assumptions are that qc s will anyways be parallel , that qubits will be expensive , that qc s will have a high `` connectivity '' between its qubits , that qcs may be slow and that fault tolerant techniques ( quantum error correcting codes ) will be used .",
    "that means that i m looking for highly parallelizable algorithms which nevertheless do nt use much more space ( qubits ) than simpler algorithms .",
    "if there are plenty of qubits , additional parallelization schemes could be used with a relatively bad space - time tradeoff of the form @xmath0 .",
    "because present propositions for fault tolerant techniques @xcite are especially slow for toffoli gates , i will only count those for my performance analyses .",
    "first i give an improved version of the standard algorithm @xcite which uses only @xmath3 qubits and some @xmath4 toffoli gates , where @xmath5 is the number of bits of the number to be factored .",
    "the ideas that lead to these improvements are due to several people .",
    "my contribution is the observation that it is enough to compute the modular exponentiation correctly for most but not all input values , as it should still be possible to extract the period of this function is a few runs of the quantum computer .",
    "this allows substantial simplifications of the algorithm .",
    "the idea is used throughout this paper .",
    "the standard way to compute the modular exponentiation @xcite is to decompose it into many modular multiplications which again are decomposed into additions .",
    "usually the addition has to be done bit after bit .",
    "i propose a way to parallelize it such that the execution time essentially becomes a constant for large numbers .",
    "the space requirements of the algorithm increase from @xmath1 to @xmath6 qubits .      here",
    "we directly `` attack '' the modular multiplication by using the fast fourier transform ( fft ) -based multiplication technique which led to the famous schnhage - strassen algorithm .",
    "note that the fft here has nothing to do with the quantum fourier transform .",
    "the latter fourier - transforms the amplitudes of a quantum register .",
    "the former is a classical operation , even though it is applied to a superposition and thus is computed in `` quantum parallelism '' .",
    "applied to a `` classical '' basis state it computes the fourier transform of the values which are represented in binary in several registers .",
    "fft - based fast integer multiplication is rather complicated , it consists of several `` subroutines '' which i have figured out how to do reversibly .",
    "also the fourier transform employed is not the usual one over the complex numbers , but over the finite ring of integers modulo some fixed integer .",
    "fft - multiply reduces the multiplication of two big numbers to many smaller multiplications .",
    "i will use the same technique again to compute these smaller multiplications , thus i propose a 2-level fft - multiply .",
    "i have investigated numerous other versions , like 1-level fft - multiply with parallelized addition or the @xmath7 karatsuba - ofman algorithm or fft - multiply using a modulus which is not of the form @xmath8 as is usually used .",
    "these algorithms seemed not to perform well enough either on space ( karatsuba - ofman ) or on time and i m not discussing all these possibilities .",
    "as mentioned above , my main assumptions are that quantum computers will be parallel , that qubits will be expensive and that communication across the qc will be fast ( contrary e.g. to a cellular automaton ) .",
    "the parallelism assumption comes from the following observations : qubits have to be controlled by exterior field which are again controlled by a classical computer .",
    "probably every qubit or few qubits will have its own independent ( classical ) control unit .",
    "this also explains why i think that qubits will be expensive , thus we want to use as few of them as possible .",
    "also if decoherence is strong we will have a high rate of memory errors acting on resting qubits .",
    "this will make necessary periodic error recovery operations ( quantum error correction ) also on resting qubits , thus the qc must anyways be capable of a high degree of parallelism .",
    "note that in the case where memory errors dominate over gate inaccuracy errors we actually loose nothing by running the computation in parallel , as it will not increase the error rate by much .",
    "here i have of course assumed that large quantum computations will need fault tolerant techniques ( see e.g. shor @xcite ) .",
    "thus all `` computational '' qubits will be encoded in several physical qubits and operations ( gates ) will be done on the encoded qubits , thus without decoding . presently proposed schemes ( shor @xcite , gottesman @xcite ) are much slower on toffoli gates than on the other used gates , which is why i propose to only count the toffoli gates when assessing the execution time .",
    "i assume that better fault tolerant schemes will be found , e.g. ones using less space than the very space - consuming concatenation technique ( see knill et al . @xcite or aharonov et al @xcite ) .",
    "also i think that faster ways of implementing the toffoli gate fault tolerantly will be found .",
    "actually there is an argument ( manny knill , private communication ) which shows that it is not possible to implement all gates of a set of universal gates transversally , that is bitwise between the individual qubits of the encoded computational qubits .",
    "that is because otherwise errors that do nt lead out of the code space could happen , which could nt be detected .",
    "present schemes allow the transversal implementation of cnot , not and hadamard transform . therefore in these schemes",
    "it is not possible to implement the toffoli gate transversally .",
    "i now propose to look for a scheme where all the `` classical '' gates toffoli , cnot and not can be implemented transversally , but not non - classical gates like the hadamard .",
    "this would help a lot for shor s algorithm ( and probably also for applications of grover s algorithm ) as there except for a few gates at the eginning and at the end , we use only such `` classical '' gates .",
    "why do i think that qc s may be much slower than todays conventional computers ?",
    "i think that 2-bit gates ( or multi - bit gates ) will be slow , as it is not easy to keep qubits from interacting with each other sometimes and then ( with exterior fields ) to make them interact strongly .",
    "actually this is essentially true for all present quantum computer hardware proposals .",
    "the assumption that fast quantum communication will be possible between ( relatively ) distant parts of a qc is not well founded , rather i think it will simply be a necessity for quantum computations",
    ". it should be possible for qc s which either use photons to represent qubits or where a coupling to photons is possible .",
    "then connections could simply be optical , possibly simply with optical fibers . in the ion trap scheme it is clear that a large qc could hardly consist of a single ion trap .",
    "cirac and zoller have thus extended their proposal to couple ion - trap qubits to photons .",
    "the degree of connectivity may still be less than desired .",
    "when interpreting my results for computation time , one must keep in mind that things might actually be worse for a realistic quantum computer .",
    "a general assumption that i make is that measurements of qubits will not be too hard , as otherwise fault tolerance would be much harder to achieve .",
    "i also assume that classical computation will be much cheaper ( and probably also faster ) than quantum computation .",
    "therefore where possible i ( pre- ) compute things classically .",
    "( of course this makes sense only up to a certain point . )",
    "the most part of shor s quantum factoring algorithm consists of computing the modular exponentiation .",
    "this can be seen as a classical computation , as it transforms computational basis states into computational basis states .",
    "of course this transformation is applied `` in quantum parallelism '' to a large superposition of such states .",
    "still we can think of it as being applied to just one basis state .",
    "then it differs from conventional computation only in that it must be reversible .",
    "so for shor s algorithm we have to compute :    @xmath9    any conventional algorithm can also be run on a reversible machine , except that then a lot of `` garbage '' bits will be produced .",
    "if we are content with leaving around the input ( here @xmath10 ) , there is a general procedure to get rid of the garbage .",
    "say we want to compute @xmath11 , but necessarily also produce garbage @xmath12",
    ". then we can do the following to get rid of it :    @xmath13    in the @xmath14 step we copy @xmath11 to an auxiliary register , initialized to 0 .",
    "this can simply be done by bitwise cnot .",
    "the last step is the time reverse of the first , thus it `` uncomputes '' @xmath12 and the first copy of @xmath11 . in general the work space",
    "will have a size about the number of operations of the computation .",
    "fortunately we can do much better for the modular exponentiation , as we will see later .",
    "how can one compute efficiently a modular exponentiation with a large exponent ?",
    "the method is :    @xmath15    where the @xmath16 are the bits of the binary expansion of @xmath10 .",
    "the numbers @xmath17 can be calculated by repeated squaring .",
    "the modular exponentiation is then computed by modular multiplication of a subset of these numbers .",
    "we will have a `` running product '' @xmath18 which we will modularly multiply with the next candidate factor @xmath17 if @xmath16 is 1 .",
    "reversibly we will do :    @xmath19    the @xmath14 step is possible because modular multiplication with @xmath20 is a 1 to 1 function and furthermore because we know how to efficiently compute its inverse .",
    "the general scheme in reversible computation for such a situation is :    @xmath21    for",
    "the @xmath14 step imagine that we applied the inverse of @xmath22 to @xmath23 , thus obtaining @xmath24 .",
    "so the @xmath14 step above is essentially the time reverse of this . for modular multiplication",
    "the inverse function is simply the modular multiplication with the inverse of @xmath20 modulo n. @xmath20 has such an inverse because it is relatively prim with @xmath25 .",
    "this is because we assume that the constant @xmath26 is relatively prime with @xmath25",
    ". then @xmath27 can easily be precomputed classically using euclid s algorithm for finding the least common divisor of 2 numbers .",
    "let s now concentrate on how to compute    @xmath28    we can make a further simplification :    @xmath29    again the numbers @xmath30 can be precomputed classically .",
    "so now we have reduced modular multiplication to the addition of a set of numbers of the same length as @xmath25 .",
    "for this modular addition there are 2 possibilities : either we first add all numbers not modularly and then compute the modulus , or we have a `` running sum '' @xmath31 and every time we add a new number to it we compute the modulus . at least for conventional computation",
    "the @xmath32 possibility is preferable : say @xmath5 is the number of bits of @xmath25 , and thus also the length and the number of the summands .",
    "the total sum will then be some @xmath33 bits longer than @xmath5 . to compute the modulus of this number",
    "will take some @xmath33 steps , whereas we have to do some @xmath5 steps if we compute the modulus at each addition .",
    "for reversible computation the problem is that computing the modulus of the total sum is not 1 to 1 whereas modular addition ( of a fixed `` classical '' number ) is .",
    "fist i will describe the modular addition technique and later i will show that the more efficient method is also possible in reversible computation .",
    "let s first see how we can make a ( non - modular ) addition of a fixed ( `` classical '' ) number to a quantum register .",
    "actually this can be done directly without leaving the input around .",
    "so we want to do    @xmath34    addition of course is done binary place by binary place , starting with the least significant bit . for every binary place we will also have to compute a carry bit . in reversible computation",
    "we will need a whole auxiliary register to temporarily hold these carry bits before they get `` uncomputed '' .",
    "thus we will do    @xmath35    say @xmath36 is the carry bit that has been calculated from the place number @xmath37 , thus we will want to add it to the bit @xmath38 . then for every place we have to do the following operations :    @xmath39    here the parenthesis means a logical expression and @xmath40 means xor which is the same as addition modulo 2 . to show how to uncompute the carries later ,",
    "it is preferable to first compute the new @xmath38 and then compute the carry @xmath41 :    @xmath42    the and and or can be realized by using a toffoli gate ( ccnot ) .",
    "thus for either value of @xmath43 we need a toffoli gate per binary place and later another toffoli gate to uncompute the carry .",
    "actually we want to add only if some conditional quantum bit is 1 .",
    "we could now make every gate conditional on this bit .",
    "thus a not would become a cnot , a cnot a ccnot and so on .",
    "this would increase the cost of the computation a lot , as much more toffoli gates would be used , e.g. a cccnot needs 3 toffoli gates .",
    "therefore it is preferable to do as little as possible conditional on the `` enable - qubit '' ( which decides whether we should add or not ) .",
    "for addition i propose the following : when uncomputing the carries , we also uncompute the @xmath38 s conditional on the negation of the `` enable - qubit '' .",
    "the computation of the @xmath38 s usually needs only cnot s and no toffolis , so by making this operation conditional on the enable qubit we will avoid costly things like cccnot .",
    "the following quantum network shows these operations for 1 binary place :    ( 300,120 )    ( 50,100)@xmath44 ( 20,0)(0,25)4(1,0)120 ( 0,75)@xmath45 ( 0,50)@xmath38 ( 0,25)@xmath36 ( 0,0)@xmath41    ( 50,50)(0,5)5(0,1)2 ( 50,25)(0,1)25 ( 50,25 ) ( 50,75 ) ( 50,50 )    ( 10,10 ) ( 0,0 ) ( 0,-5)(0,1)10 ( -5,0)(1,0)10    ( 110,0)(0,1)50 ( 110,0 )    ( 10,10 ) ( 0,0 ) ( 0,-5)(0,1)10 ( -5,0)(1,0)10    ( 110,50 ) ( 110,25 ) ( 95,50 )    ( 10,10 ) ( 0,0 ) ( 0,-5)(0,1)10 ( -5,0)(1,0)10    ( 125,50 )    ( 10,10 ) ( 0,0 ) ( 0,-5)(0,1)10 ( -5,0)(1,0)10    ( 250,100)@xmath46 ( 220,0)(0,25)4(1,0)120 ( 200,75)@xmath45 ( 200,50)@xmath38 ( 200,25)@xmath36 ( 200,0)@xmath41    ( 240,50)(0,5)5(0,1)3 ( 240,75 ) ( 240,50 )    ( 10,10 ) ( 0,0 ) ( 0,-5)(0,1)10 ( -5,0)(1,0)10    ( 260,50)(0,5)5(0,1)3 ( 260,25)(0,1)25 ( 260,25 ) ( 260,75 ) ( 260,50 )    ( 10,10 ) ( 0,0 ) ( 0,-5)(0,1)10 ( -5,0)(1,0)10    ( 295,25 )    ( 10,10 ) ( 0,0 ) ( 0,-5)(0,1)10 ( -5,0)(1,0)10    ( 310,0)(0,1)50 ( 310,0 )    ( 10,10 ) ( 0,0 ) ( 0,-5)(0,1)10 ( -5,0)(1,0)10    ( 310,50 ) ( 310,25 ) ( 325,0 )    ( 10,10 ) ( 0,0 ) ( 0,-5)(0,1)10 ( -5,0)(1,0)10     + here the dashed lines mean that only when we run this algorithm backwards , should these gates also depend on the negated enable bit @xmath45 .",
    "now let s look at modular addition    @xmath47    note that @xmath48 if @xmath49 and @xmath50 if @xmath51 .",
    "to do this , we first compute the condition bit @xmath52 , and then , depending on it , we add @xmath53 , resp . @xmath54 .",
    "how can we uncompute the condition bit ? to do this we have to be able to compute it from @xmath55 .",
    "it is easy to see that it is simply @xmath56 .",
    "formally :    @xmath57    of course @xmath54 is negative , so we have to add the complement @xmath58 , where @xmath59 is the smallest power of 2 which is larger or equal to @xmath25 .",
    "after this addition we then simply have to flip the bit with place value @xmath59 in the result .",
    "let s look at the second step in the above equation where the addition is done .",
    "thus at the @xmath60 binary place the classical bit we want to add is either @xmath43 or @xmath61 .",
    "if these two bits are equal we can use the simple addition network described above . for the other 2 cases where @xmath62 the network is more complicated , even though i have found a way to do it with the same number of toffoli gates .",
    "i describe below only the case @xmath44 and @xmath63 as the remaining case is very similar .",
    "so :    ( 300,145 )    ( 50,125)@xmath64 ( 70,0)(0,25)5(1,0)250 ( 0,100)@xmath45 ( 0,75)@xmath52 ( 0,50)@xmath38 ( 0,25)@xmath36 ( 0,0)@xmath41    ( 90,0 )    ( 0,25)(0,1)50 ( 0,75 ) ( 0,25 )    ( 10,10 ) ( 0,0 ) ( 0,-5)(0,1)10 ( -5,0)(1,0)10    ( 120,0 )    ( 0,50)(0,5)10(0,1)3 ( 0,25)(0,1)25 ( 0,100 ) ( 0,25 ) ( 0,50 )    ( 10,10 ) ( 0,0 ) ( 0,-5)(0,1)10 ( -5,0)(1,0)10    ( 180,0 )    ( 0,50)(0,1)25 ( 0,75 ) ( 0,50 )    ( 10,10 ) ( 0,0 ) ( 0,-5)(0,1)10 ( -5,0)(1,0)10    ( 200,50 )    ( 10,10 ) ( 0,0 ) ( 0,-5)(0,1)10 ( -5,0)(1,0)10    ( 220,0 )    ( 0,1)50 ( 0,50 ) ( 0,25 ) ( 0,0 )    ( 10,10 ) ( 0,0 ) ( 0,-5)(0,1)10 ( -5,0)(1,0)10    ( 240,50 )    ( 10,10 ) ( 0,0 ) ( 0,-5)(0,1)10 ( -5,0)(1,0)10    ( 260,0 )    ( 0,50)(0,1)25 ( 0,75 ) ( 0,50 )    ( 10,10 ) ( 0,0 ) ( 0,-5)(0,1)10 ( -5,0)(1,0)10    ( 290,0 )    ( 0,0)(0,1)75 ( 0,75 ) ( 0,0 )    ( 10,10 ) ( 0,0 ) ( 0,-5)(0,1)10 ( -5,0)(1,0)10     + the dashed line should be left away when we first compute the sum but should be solid ( thus here forming a toffoli gate ) when we uncompute the carries ( and possibly also the sum ) . to see that this network does what it should , consider separately the cases @xmath65 and @xmath66 . in the first case",
    "the carry @xmath41 should only be set if @xmath67 and in the other case it should always be set except when @xmath68 .",
    "it is easy to check that this works out as it should .",
    "let s now look at the computation of the comparison ( qu)bits . a comparison seems to cost about as much as an addition .",
    "say we start the comparison from the most significant bits downwards .",
    "for 2 random numbers we will usually see after a few bits which number is larger , only if the two numbers are equal or differ only in the least significant bit , have we to go through all bits .",
    "of course in quantum parallelism we ca nt do that , as there will usually always be some fraction of the superposition ( of computational basis states ) which are still equal for all bits we have looked at .",
    "but , i argue , that once this fraction is small enough , we do nt really have to care about it .    more generally i say that if we compute the modular exponentiation wrongly for some small fraction of input values @xmath10 , it does nt matter much .",
    "say    @xmath69    say the fraction of the input values @xmath10 in the superposition for which @xmath70 is equal to @xmath71 .",
    "the scalar product of this state and the intended state then is @xmath72 .",
    "it is clear that there will be little change in the distribution of output values observed at the end of the quantum computation .",
    "anyways it is clear that a quantum computer will work imperfectly and the error level we expect will not be small .",
    "this may well still be true with error correction techniques , as these techniques are very expensive ( especially in space ) so that we will only use them as much as necessary .",
    "therefore the classical post - processing will anyways have to take such errors into account .",
    "so i propose to simplify the modular exponentiation computation by allowing `` algorithmic '' ( or `` deterministic '' ) errors .",
    "in particular here i propose to only compare some of the most significant bits of two numbers to be compared . if these bits are equal for the two numbers , we e.g. say that the 1 .",
    "number is the larger .",
    "i make here the plausible assumption that for estimating the error rate i can think of the numbers as uniformly distributed random numbers . mathematically ( and therefore very cautiously ) inclined people have questioned the validity of this assumption . here",
    "i simply assume that it is true , but note that one could heuristically test it by running the simplified modular exponentiation algorithm on a conventional computer for many inputs and check the error rate .",
    "what error rate per modular addition can we tolerate ?",
    "the modular exponentiation consists of @xmath73 modular multiplications each of which consists of @xmath5 modular additions .",
    "so for an overall error rate @xmath74 we are allowed an error rate of about @xmath75 . with the random number assumption",
    "this says that for comparison we should look at the @xmath76 most significant bits of the two numbers .",
    "for definiteness i will choose @xmath77 .",
    "so e.g. for @xmath78 bit numbers we only have to compare some @xmath79 bits , thus only a small fraction of @xmath5 . for estimates of the cost ( time and space ) of the algorithm i will leave this small contribution away ( note also that the contribution is not in leading order of @xmath5 ) .    how many toffoli gates do we use ?",
    "first we need 1 toffoli gate per binary place and then 2 toffolis for uncomputing the carries .",
    "so conditional modular addition cost us @xmath3 toffoli gates .",
    "now let s look at the algorithm where we first add up all summands and only then compute the modulus ( mod @xmath25 ) .",
    "the problem is that computing the modulus of the total sum is not a 1 to 1 function .",
    "i propose the following algorithm : along with the total sum @xmath31 we compute an `` approximate total sum '' @xmath55 which carries only some of the most significant bits . to compute @xmath55",
    "of course we also only add up the most significant bits of the summands , thus this does nt cost us much .",
    "now we can determine from @xmath55 how often we have to subtract @xmath25 from @xmath31 to get the modulus .",
    "finally we run part of this algorithm backwards and in particular uncompute @xmath55 . in more detail :    @xmath80    here @xmath81 means the integral part of @xmath82 .",
    "thus we compute @xmath83 as @xmath84 .",
    "how many of the most significant bits do we have to use for @xmath55 ?",
    "we want the probability of a wrong modular multiplication to be smaller than @xmath85 .",
    "thus @xmath55 should have some @xmath86 correct bits below the most significant bit of @xmath25 . to get these bits correct we have to use some @xmath87 bits in each addition when computing @xmath55 .",
    "being the sum of @xmath5 numbers of about the size of @xmath25 , @xmath55 will be some @xmath88 bits longer than @xmath25 .",
    "so all in all @xmath55 will have length @xmath89 and each addition will also use as many bits .",
    "i wo nt describe detailed quantum circuits for all this , because at any rate the cost associated with @xmath55 is relatively low .",
    "( exercise for the ambitious reader : why do i go through the trouble of computing @xmath55 separately and not just copy some of the most significant bits of @xmath31 ? )    let s look at the total number of toffoli gates for such a modular exponentiation .",
    "we now use the simple ( non - modular ) conditional addition circuit described above . to compute the sum and uncompute the carries it needs 3 toffoli gates per binary place .",
    "then per modular multiplication we need @xmath5 such additions , each of length @xmath5 .",
    "we need another modular multiplication to uncompute the old value of the running product by using @xmath27 . then to compute the modular exponentiation we need @xmath90 such steps .",
    "this gives a total of @xmath91 toffoli gates .",
    "so far it seems that this algorithm uses some @xmath92 qubits ( in leading order ) . to see this",
    "let s look at a modular multiplication step :    @xmath93    now @xmath10 is a @xmath90 bit number , whereas @xmath18 and @xmath94 are @xmath5 bit numbers .",
    "another @xmath5 qubits of workspace is needed to temporarily store the carry bits for each addition before they are uncomputed .",
    "thus we get a total of @xmath92 qubits .",
    "now manny knill ( private communication ) and possibly others have observed that one can reduce this to @xmath3 qubits .",
    "first let s look at the quantum fft ( that is fourier transform of the amplitudes of a register ) . in shor s algorithm",
    "this qfft is the last operation before readout . in this case",
    "the qfft can be simplified ( see appendix ) by interleaving unitary operations and measurements of qubits .",
    "this qfft procedure has the following structure : hadamard - transform the most significant qubit of the register and measure it .",
    "then if the observed value is 1 , apply certain phase shifts to all the other qubits of the register .",
    "then hadamard transform the second most significant qubit and measure it .",
    "again , depending on the measured values of the two most significant qubits , we have to apply certain phase shifts on the remaining unobserved qubits .",
    "so every step consists of hadamard transforming the most significant still unobserved qubit , then measuring it and then applying certain phase shifts on the remaining unobserved qubits .",
    "the values of these phase shifts depend on the so far measured qubit - values .",
    "the measured values give us the binary representation of the `` output number '' .",
    "note that it is in bit - reversed order , thus we get the least significant bits first .",
    "this bit - inversion is a feature of the fft algorithm .",
    "the register we apply this procedure to , is the input register @xmath10 .",
    "but most action happens in the other registers .",
    "all that happens to the input register is the following : it starts out initialized to the @xmath95 state , then we make the uniform amplitude superposition of all possible input values @xmath10 by hadamard transforming each qubit",
    ". then each of these qubits controls a modular multiplication , thus it decides whether the multiplication is done or not ( in each of the quantum - parallel computations ) . and finally the @xmath10 register undergoes the qfft procedure described above .",
    "so after having controlled `` its '' modular multiplication a @xmath10-qubit does nothing until the final qfft .",
    "usually we would imagine that we would go from least significant modular multiplications to most significant ones ( significance = significance of associated @xmath10-control bit ) .",
    "but of course the order in which we multiply does nt matter , so we can e.g. turn around the order .",
    "then the most significant @xmath10-qubits will first be ready for the qfft .",
    "so we can interleave controlled modular multiplication steps and qfft steps , thus after each controlled modular multiplication we will get another bit of the ( classical ) final output .",
    "after having been hadamard transformed right at the beginning , a @xmath10-qubit does nt do anything until it controls `` its '' modular multiplication .",
    "moreover the @xmath10-qubits in the uniform - amplitude superposition are not entangled .",
    "thus we do nt really have to prepare them all at the beginning of the algorithm , but we prepare the @xmath10-qubit only just before `` its '' modular multiplication .",
    "therefore we have eliminated the @xmath90 qubit long @xmath10-register and in leading order need only @xmath3 qubits .",
    "let s first think of a classical computation of the modular exponentiation .",
    "clearly there are several possibilities to parallelize the algorithm , at least if we are ready to use much more space ( bits ) .",
    "the modular exponentiation consists of many modular multiplications and each such multiplication consists of many additions .",
    "let s e.g. look at the addition of @xmath5 number , each of length @xmath5 . instead of having just 1",
    "running sum , adding one summand after the other to it , we could in parallel sum up equal subsets of the summands and then add together these partial sums . in the most extreme case we would group the summands in pairs , then add each pair , then add pairwise these sums etc .",
    "thus we could make the whole sum in @xmath88 steps instead of @xmath5 . of course this would require on the order of @xmath5 additional @xmath5-bit registers . in reversible computation",
    "the partial sums would also have to be uncomputed , possibly roughly doubling the cost .",
    "note that the same ideas can also be applied to the parallelization of the @xmath5 modular multiplications .",
    "rough considerations show that with this kind of parallelization one reduces the time of the computation about in the same proportion as one increases the space ( qubits ) :    @xmath96    in this paper i consider such a space - time tradeoff as too costly , assuming that qubits will be very expensive , but just in case , it may be good to keep in mind this possibility for parallelization .",
    "i propose a technique for parallelizing the individual addition steps with a much better space - time tradeoff .",
    "the technique is described in detail in the appendix , here i just give a rough outline of the basic ideas :    usually we have to do addition binary place by binary place , as any lower significance bit can change the value of a higher significance bit .",
    "now a first observation which we exploit , is that the probability of such a dependence goes down exponentially with the distance of the two bits in the binary representation , thus we can use the observation that some `` algorithmic errors '' are tolerable .",
    "the other idea is to chop the two numbers to be added into blocks of some fixed length . to add tw corresponding blocks",
    "we really should know the value of the carry bit coming from the preceding block .",
    "the idea is to compute the sum of the blocks for both possible values of this unknown carry bit . in a second step",
    "we then go through all blocks from low to high significance and for each block determine what the correct carry bit should have been .",
    "the first step takes time proportional to the length of a block and the second step proportional to the number of blocks .",
    "thus , by choosing block - length and the number of blocks about equal , we get a square root speed up .",
    "the above `` standard algorithm '' can directly handle a modular multiplication .",
    "if we want to use fast integer multiplication techniques , it seems that we have to compose the modular multiplication out of several regular multiplications ,",
    "at least i havent found a better way to do it .",
    "thus we use :    @xmath97    where @xmath98 means integral part . note that of course we precompute @xmath99 classically , and that we only need it to some @xmath5 significant digits .",
    "thus we have to compute some three @xmath5 times @xmath5 bit multiplications .",
    "i will show later how to do all this reversibly .",
    "2there is a way to speed up the multiplication of large integers by using the fast fourier transform technique , where the fourier transform is performed over the ring of integer modulo something . by iterating this technique one obtains the famous schnhage - strassen algorithm @xcite with complexity of order @xmath100 to multiply two @xmath101 bit integers .",
    "although for very large numbers this is the fastest known algorithm , it is seldomly used because up to quite large numbers other ways of speeding up multiplication are faster . in particular the @xmath7 karatsuba - ofman algorithm or its variations are used .",
    "i wo nt use it because it seems to use too much space , as i show in the appendix . on the other hand",
    "the fft based multiplication technique is naturally parallel without using much more space .",
    "here i give a rough outline of how one can speed up multiplication using fft .",
    "things are described in more details in the appendices .",
    "say we want to multiply two @xmath5-bit numbers .",
    "first we split each of them into @xmath102 blocks of size @xmath103 .",
    "then the multiplication consists essentially of convolutions :    @xmath104    where the range of the summation indices has to be figured out carefully .",
    "the expression in parenthesis is known as a convolution .",
    "the fourier transformation comes in because the fourier transform of a convolution of two functions is the pointwise product of the fourier transforms of the functions . for the discrete fourier",
    "transform this is true for the discrete convolutions appearing in the above equation .",
    "the discrete fourier transform of the numbers @xmath105 is given by :    @xmath106    using the fast fourier transform algorithm ( fft ) one can now compute the convolutions according to the scheme :    @xmath107 \\cdot fft[b]]\\ ] ]    where the multiplication on the right hand side means pointwise multiplication .",
    "pointwise multiplication is of course much easier to compute than the convolution and furthermore it can trivially be done in parallel .",
    "this procedure can help save time because of the great efficiency of the fft .",
    "a problem is that we really want to make exact integer arithmetic , but with the usual fft we use real ( actually complex ) numbers and can compute only to some accuracy .",
    "still this technique is sometimes used @xcite and the accuracy is chosen high enough so that rounding at the end always gives the correct integers .    for higher efficiency one generalizes",
    "the fourier transform to the ring of integers modulo some fixed modulus .",
    "apart from the @xmath108 term , the discrete fourier transform can be defined over any ring with any @xmath109 .",
    "the point is that we still want to be able to use the fft algorithm and of course still want the convolution theorem to be true .    for the fast fourier transform algorithm to work ,",
    "we need @xmath110 . for the convolution theorem still to be true",
    "we furthermore need the condition :    @xmath111    this is e.g. true in the ring mod 13 for @xmath112 and @xmath113 , thus in particular @xmath114 . because we also have to compute the inverse fourier transform",
    ", we must furthermore demand that @xmath109 has an inverse in the ring , but this is usually no problem .",
    "we will compute the fft over a modulo ring .",
    "the convolution theorem is then modified in that we will get the modulus of the intended results .",
    "usually we will therefore simply choose the modulus larger than any possible result , so that taking the modulus does nt change anything .",
    "sometimes we will also use the chinese remainder theorem .",
    "thus we will compute the convolution with respect to 2 relatively prime moduli which are individually too small , but which allow us to recover the correct result .",
    "the fft algorithm is most efficient for @xmath25 a power of 2 .",
    "furthermore for their algorithm schnhage - strassen chose @xmath115 or some small power of 2 , and the modulus a power of 2 plus 1 .",
    "this makes the operations in the fft very easy because multiplication with @xmath116 is just a shift in binary and also the modulus is easy to compute . more precisely , schnhage - strassen chose the modulus @xmath117 for which the above conditions are always fulfilled .",
    "i will also adopt this choice .    to get the cost ( space and time ) of the algorithm we need to know the `` parameters '' @xmath102 and @xmath118 .",
    "@xmath102 is the number of blocks and @xmath118 is essentially the block length rounded to the next power of 2 .",
    "the ring over which we compute the fft will be the ring of integer modulo @xmath119 , thus we will handle numbers with @xmath120 bits . both numbers @xmath102 and @xmath118 are of the same order of magnitude , namely either @xmath121 or @xmath122 .",
    "the product @xmath123 is usually @xmath90 rounded to the next power of 2 .",
    "thus    @xmath124    depending on @xmath5 .",
    "again , things are described in detail in the appendix .      as mentioned above the schnhage - strassen algorithm",
    "iterates the fft multiplication technique .",
    "thus the component - wise multiplication of @xmath125 $ ] and @xmath126 $ ] is again done with fft - multiply and so on .",
    "i propose to use fft - multiply on two levels .",
    "i have investigated various algorithms , like 1-level fft - multiply with parallelized addition for the component - wise multiplications . for simplicity i will only present the algorithms which have turned out to perform well .",
    "this is in particular 2-level fft - multiply with the individual operations in the fft parallelized , as described in the appendix .",
    "an important point is that the @xmath14 level fft - multiply is much more efficient than the @xmath32 level ( for the same size of numbers ) .",
    "remember that the component - wise multiplications are actually done modulo some modulus of the form @xmath127 .",
    "it turns out that this can be done directly with some minor modification of the fft - multiply .",
    "this modification consists of some multiplication with a square root of @xmath109 . to make this simple we have to choose @xmath109 an even power of 2",
    "as for the first level fft we have two parameters that characterize the second level fft , namely @xmath128 and @xmath129 .",
    "where @xmath128 is the number of blocks and @xmath129 is the number of bits per block .",
    "the modulus relative to which we calculate the fft is @xmath130 .",
    "first let s see how to compute the modular multiplication reversibly .",
    "i propose the straight forward scheme :    @xmath131    in every one of these 4 steps we have to compute a multiplication and then uncompute the garbage .",
    "this is of the form :    @xmath132    thus for a modular multiplication we need a total of 8 simple multiplications of the form @xmath133 . for a 1-level fft - multiply",
    "this would look as follows :    @xmath134    where the tilde stands for fourier transform and @xmath135 is a component - wise product .",
    "for a 2-level fft - multiply we have :    @xmath136    where @xmath137 represents the lower level garbage which is produced when @xmath138 and @xmath139 are multiplied component - wise using the second level fft - multiply .",
    "so one multiplication consists of 2 first level fft s and @xmath102 lower level multiplications .",
    "so for the modular multiplication ( mm ) we can write :    @xmath140    where @xmath141 stands for 1 lower level multiplication .",
    "schematically the step @xmath142 is done as follows :    @xmath143    where the carries are used as work space during the lowest level multiplication which is done as in the standard algorithm .",
    "one lower level multiplication consists of 2 lower level fft s and @xmath128 lowest level multiplications ( denoted by @xmath144 ) .",
    "thus now :    @xmath145      we have to look where in the algorithm most qubits are used . because fft - multiply is relatively space - intensive , this is during the lower level fft - multiply and in particular when during the lowest level multiplication the carries are used for addition ( eq . [ mspace ] ) . in this step @xmath146 , @xmath147 and the carries each use space @xmath148 ( as the modulus is @xmath130 ) . in the modular multiplication scheme",
    "we see that apart from this , there is also sometimes an @xmath5-bit number around , but for simplicity i neglect this relatively small contribution . thus all in all",
    "we get :    @xmath149    where i have used that @xmath150 and @xmath151 , depending on the size of @xmath5 relative to the next power of 2 . on average",
    "( averaging over @xmath152 ) the space requirements are about @xmath153 , thus closer to @xmath154 than to @xmath155 .",
    "for this we first have to know the cost of the fft which consists of steps of the form :    @xmath156    where @xmath157 are non - negative and smaller than @xmath158 . how to do this and what it costs is investigated in the appendix .",
    "there we get for the number of toffoli gates for such an operation on two @xmath101-bit numbers :    @xmath159    where @xmath160 can be thought of as standing for either `` time '' or `` toffoli '' .    now without further measures the first level fft would dominate the time of the parallelized 2-level algorithm .",
    "so in the appendix i also show how to parallelize the individual fft - operations .",
    "the number of toffoli gates per individual fft - operation then rises to :    @xmath161    but because of the parallelization , the execution time becomes approximately a constant ( for our range of input numbers ) :    @xmath162    where @xmath163 means the execution time of the parallelized algorithm , measured in units of the time needed for one toffoli gate .    plugging all this into equation ( [ mm ] ) we get the number of toffoli gates for 1 modular multiplication .",
    "multiplying this with @xmath73 gives the number of toffoli gates for the whole modular exponentiation :    @xmath164    remember that for the first level fft we use the parallel algorithm whereas for the second level algorithm we use the standard algorithm . in an fft the basic operation is carried out @xmath165 times . for the standard modular multiplication which is used at the lowest level , there is a 3 for the cost of a conditional addition .",
    "let s now obtain the execution time of the parallelized algorithm .",
    "for this we have to drop the factors @xmath166 , @xmath167 and @xmath128 , and plug in the execution time for the parallelized basic fft - operation :    @xmath168",
    "the dominant part of shor s quantum factoring algorithm is modular exponentiation .",
    "i have found 2 ( reversible ) algorithms for modular exponentiation which perform well for factoring very large numbers .",
    "in particular i have introduced much parallelism while still using @xmath169 space ( qubits ) .",
    "i have looked at numbers of up to several million digits and some of the approximate results below are only valid for this range .    also i have given an improved version of the standard version of shor s algorithm ( which uses @xmath170 gates , where @xmath5 is the number of bits of the number to be factored ) .",
    "the first of the 2 fast algorithms is a version of this standard algorithm with a parallelized addition .",
    "it s performance is summarized in the following 3 quantities : @xmath171    where @xmath172 is the number of qubits used in the algorithm , @xmath160 is the total number of toffoli gates and @xmath163 is the execution time of the parallelized algorithm measured in execution times of a single toffoli gate .",
    "note that the result for @xmath163 is approximate and only valid for the range of @xmath5 we are looking at .",
    "for very large numbers an fft - based multiplication algorithm is used to compute the modular multiplications in the modular exponentiation .",
    "the fft - multiply technique is iterated once , thus it is a 2-level fft - multiply .",
    "the main virtue of this algorithm is that it is naturally parallel without using much more space ( qubits ) .",
    "the performance of this algorithm depends on the value of @xmath5 relative to the next power of 2 and it is not easy to give closed form expressions for @xmath172 , @xmath160 and @xmath163 .",
    "the expressions for @xmath160 and @xmath163 come from rough fits to the graph below .",
    "@xmath173    @xmath160 and @xmath163 are plotted as the thick solid lines in the following graph .",
    "the 3 thin solid lines refer to the standard algorithm with parallelized addition .",
    "the two straight lines are @xmath160 and @xmath163 for this algorithm . for a fair comparison with the fft - based algorithm we have to give this algorithm the same amount of space , even though the space - time tradeoff of the additional parallelization achievable with this additional space is not good .",
    "this is what the zig - zagged thin solid line represents .",
    "i have assumed that the extra space can be used for parallelization with space - time tradeoff of the form @xmath174 .",
    "the zig - zag is because the amount of space used by the fft - based algorithm has such a non continuous behavior .",
    "the straight dotted line is the standard algorithm with @xmath1 and @xmath2 .",
    "note that the graph is logarithmic in the number @xmath5 of bits of the number to be factored and also in @xmath175 .",
    "+      the thin zig - zag line and the lower thick line are of most interest as they give the execution times of the two algorithms ( given the same amount of space for both ) .",
    "we see that they cross around @xmath176 .",
    "thus to factor numbers of more than 8000 bits the fft - based algorithm is preferable .",
    "( this is provided that we have the necessary @xmath177 qubits available and not e.g. only @xmath1 . )",
    "given a quantum computer with space and speed comparable to todays conventional pc s but fully parallel and well `` connected '' , the fft - based algorithm could be used to factor numbers with maybe up to millions of binary digits .",
    "consider e.g. a number with @xmath178 binary digits .",
    "we would need something around @xmath179 million qubits ( @xmath180 6 million qubytes ) . assuming a toffoli execution - time of @xmath181 the computation would take around 1 month .",
    "the standard algorithm would take some @xmath182 times more time .",
    "note that the computational resources of the universe are only enough to factor numbers with several thousand decimal digits when using the best known classical factoring algorithms .",
    "the possibility that very large numbers could be factored once sizable quantum computers can be built , may be of interest already now , as it shows that very large rsa - keys would have to be used to ensure that a message remains unreadable for several decades to come .",
    "the following is a known technique in classical computation : to add two @xmath5 bit numbers , decompose them each into @xmath102 blocks each of length @xmath183 , thus @xmath184 .",
    "we ca nt simply add the corresponding blocks in parallel to get the sum because we do nt know the carry bit coming from the preceding block .",
    "the idea now is to compute for each block ( really each pair of blocks ) both possibilities , once assuming that the carry bit coming from the preceding block is 0 and once that it is 1 .",
    "this takes time @xmath185 .",
    "then starting with the least significant block , we go through all blocks , each time determining what the correct carry for the next block is and from that which of the two trial - additions was correct and what the correct carry is for the next block .",
    "this takes time @xmath186 .",
    "this method can also be iterated , thus e.g. a scheme with @xmath187 will take time @xmath188 .    because we are allowed to make `` algorithmic '' errors for a small fraction of the input values , we can speed up things even further .",
    "the probability that flipping a block carry bit will change such a carry bit much further up ( in the number ) is small for generic summands , namely 2 to the minus the number of bits between them .",
    "thus the @xmath14 step of the above algorithm can be somewhat parallelized .",
    "say we group the blocks into @xmath128 superblocks each containing @xmath189 blocks , thus @xmath190 .",
    "now we first assume that the input carry bit to each superblock is 0 and compute the sequence of correct block carry bits within each superblock .",
    "the probability that the outgoing carry bit of the superblock is wrong because we used 0 as an incoming carry bit is small . of course",
    "this is done in parallel for all superblocks .",
    "so now that we have the ( most probably correct ) superblock input carry bits we compute a second sequence of block - carry bits for each superblock .",
    "this may need some explanation .",
    "@xmath26 is the ( `` quantum- '' ) number to which we want to add the fixed number @xmath20 .",
    "actually the above sequence of operations does nt manage to get rid of the input @xmath26 directly , so it will have to be called twice to uncompute @xmath26 is a @xmath14 step . @xmath192 and @xmath193 are the 2 numbers we get by once guessing 0 as input carry for all blocks and once 1 .",
    "these 2 numbers also include the output carries of all blocks .",
    "@xmath192 can be computed from @xmath26 without leaving the input @xmath26 around .",
    "then we compute @xmath193 from @xmath192 .",
    "because we leave the input around , such an addition can actually be done without carry bits and therefore also without the need to uncompute them .",
    "@xmath18 denotes the provisional block carry bits which are determined by assuming that the input carry to each superblock is 0 .",
    "then @xmath22 is the final such carry series .",
    "thus when we want to assemble the final sum , @xmath22 tells us for each block whether we should take it from @xmath192 or @xmath193 .",
    "remember that we want to do the addition depending on a conditional qubit @xmath194 ( @xmath194 for `` enable '' ) .",
    "for this purpose i use @xmath195 and @xmath196 .",
    "@xmath195 is the bitwise and of @xmath22 with @xmath194 and @xmath196 is the bitwise and of @xmath197 and @xmath194 .",
    "we have a quantum register initialized to @xmath95 ready for the final sum @xmath198 .",
    "we now copy @xmath192 into it if for the block the corresponding bit of @xmath195 is 1 and then we copy @xmath193 into it if the corresponding bit of @xmath196 is 1 . by `` copy '' i really mean xor . because this xor depends on @xmath199",
    ", we actually need a toffoli gate for `` copying '' each binary place .        in a second run of the sequence we bitwise",
    "xor the first register with the second one minus @xmath20 .",
    "again we do this only if @xmath201 .",
    "thus depending on the enable bit , we are left with either @xmath202 ( when @xmath201 ) or @xmath203 ( when @xmath204 ) . in order to have the result for both cases in the same register",
    ", we have to swap ( exchange ) the bits in the two registers depending on @xmath194 . to do this i propose to first xor the @xmath32 register into the @xmath14 which gives @xmath202 resp .",
    "@xmath205 . then , depending on @xmath206 we xor the @xmath14 into the @xmath32 , which costs us a toffoli gate per binary place .",
    "now let s look how this algorithm performs .",
    "how much space do we need ? to get @xmath192 and @xmath193 we need @xmath207 qubits .",
    "this is when we store the carry bits for getting @xmath192 in the space for @xmath193 , which forces us to compute @xmath192 and @xmath193 one after the other .",
    "then we need another @xmath208 qubits for the 4 versions of block carry bits and finally another @xmath5 for the final sum . thus @xmath209 .",
    "below we will see that for the range of @xmath5 in which we are interested , @xmath210 and thus @xmath211 .",
    "the standard addition costs @xmath90 , thus parallelization here costs about @xmath90 additional qubits .",
    "let s count the total number of toffoli gates : computing @xmath192 costs @xmath90 toffolis as it is an unconditional addition .",
    "computing @xmath193 costs only @xmath5 toffolis as here we do nt even have to uncompute carries .",
    "the bits of @xmath18 are computed by copying either one of two bits into it , which one depending on yet another bit .",
    "this can certainly be done with 2 toffolis .",
    "thus @xmath18 and @xmath22 cost us each @xmath212 toffolis . @xmath195 and @xmath196",
    "cost us each @xmath102 toffolis and the copying of @xmath192 resp .",
    "@xmath193 to get the final sum costs another @xmath90 toffolis .",
    "this totals to @xmath213 . but to uncompute @xmath26 we have to do all this twice , plus at the end we need @xmath5 toffolis to swap the two registers depending on @xmath194 . thus a total addition step costs @xmath214 toffolis .",
    "this compares to only @xmath3 toffolis for the standard addition , but we hope that we still get a substantial speed up due to parallelization .",
    "so how much time does the algorithm take ?",
    "i again measure this in ( sequential ) toffoli gates .",
    "the result may depend more than other quantities on the unknown details of a quantum computer architecture , so the following is essentially an educated guess . @xmath192 and @xmath193 are computed one after the other , but in parallel for all blocks .",
    "this costs some @xmath215 toffoli time steps .",
    "the computation of @xmath18 takes some @xmath189 steps , each involving 2 toffolis .",
    "the same is true for @xmath22 .",
    "it looks like @xmath195 and @xmath196 could be computed in full parallelism . in practise",
    "it may not be possible to use @xmath194 simultaneously as control bit for many toffoli gates , thus we may want to make some copies of @xmath194 ( and maybe also of @xmath206 ) .",
    "somewhat arbitrarily i set the time cost to @xmath216 for computing both @xmath199 s .",
    "a similar situation occurs at the end when we obtain the final sum by copying from @xmath192 and @xmath193 . with the conservative assumption that we do this sequentially for each block , we get @xmath217 time steps .",
    "the total is @xmath218 .",
    "again this has to be doubled to account for the uncomputation of @xmath26 and we also have to take into account the conditional swapping of the registers at the end .",
    "for this conditional swapping i assume that there are enough copys of @xmath194 to do this in @xmath183 time steps . thus a total addition step costs @xmath219 toffoli time steps .",
    "( here the index @xmath18 stands for `` parallel '' . )",
    "let s now consider how we make the decomposition @xmath220 into superblocks and blocks .",
    "the superblocks have to be large enough to make it very improbable that a carry `` runs all the way through them '' .",
    "the error probability per superblock has to be smaller than the error probability per full addition .",
    "let s first conservatively set it to @xmath221 .",
    "thus with the usual uniform random number assumption we get that a superblock should be at least some @xmath89 bits long .",
    "let s not overdo formal generality , and let s assume that @xmath5 is in the following range : @xmath222 .",
    "then @xmath223 of the length of a superblock is about 5 to 6 and we get that the overall error is still smaller than @xmath74 when we reduce the length of a superblock to about @xmath224 .    to chose a reasonable block size within a superblock , note that the number of blocks per superblock @xmath189 and the length of a block @xmath183 contribute about equally to the computation time @xmath163 .",
    "so to minimize @xmath163 we should chose them about equal .",
    "so we get the following approximate formula for the computation time per addition : @xmath225 . for the range @xmath222 we get @xmath226 .",
    "as i will propose to use this algorithm for not too large values of @xmath5 , i simply set @xmath227 .",
    "in particular there is the karatsuba - ofman algorithm with running time @xmath230 .",
    "actually there is a whole series of algorithms with ever smaller exponents , which are actually approaching 1 , but here",
    "i will only consider the basic case .    in conventional computation",
    "it seems that one of these algorithms usually beats schnhage - strassen for the size of numbers of interest .",
    "of course for practical applications the computer architecture , word size etc .",
    "also play a big role .",
    "it seems that for reversible computation and assumptions i make about the possible architecture of a quantum computer , things look differently and a variant of the schnhage - strassen algorithm may actual find an application .",
    "the reason for this is that i assume that qubits will be very expensive .",
    "also i look for algorithms which can be massively parallelized , but again without increasing the work space too much .",
    "as i will show below , schnhage - strassen does this nicely , massive parallelization will not increase the space demand dramatically .",
    "karatsuba - ofman can also naturally be parallelized , but , as i show , it seems that it will use a lot of space for this .",
    "the algorithm is simple , to multiply to @xmath101-bit numbers we split each of the numbers in two @xmath231-bit pieces , assuming that @xmath101 is even .",
    "thus :      so we could now compute the product by computing 4 products of @xmath231-bit numbers , which takes about the same time .",
    "but there is the following simple trick which allows to do it in only 3 such multiplications :      note that we anyways have to compute @xmath234 and @xmath235 . by iterating this technique , thus applying it again to the smaller multiplications ,",
    "we get the asymptotic @xmath7 performance .",
    "clearly the smaller multiplications can be done in parallel .",
    "now let s look at the space requirements of such a parallelization .",
    "at the first level we have to store the six @xmath231-bit numbers @xmath236 and @xmath237 .",
    "for each lower level the space needed is 3/2 times the space needed at the uper level . by summing up this geometric series and noting that there are some @xmath238 levels",
    ", we get the result that we need total space @xmath7 .",
    "this is too much if , as i expect , qubits will be very expensive .",
    "if @xmath244 and @xmath25 can be decomposed into small prime factors , the fast fourier transform ( fft ) -algorithm can be used , which is widely used on computers .",
    "usually @xmath25 is a power of 2 : @xmath245 , as then the fft is most efficient and best suited for binary digital computers .",
    "i demonstrate the fft for this case and will also include the normalization @xmath108 , but it can easily be left away for rings where there is no @xmath246 .    in the first step of the fft",
    "we reduce the original task to 2 fourier transforms over @xmath247 numbers each .",
    "this is then iterated @xmath248 times until we are left with ( trivial ) fourier transforms of single numbers .",
    "note that the fourier transform over the complex numbers is a unitary transformation of @xmath25 complex numbers .",
    "it is therefore in principle possible to apply it physically to the @xmath260 amplitudes of an @xmath183-qubit quantum register :      where the @xmath262 are the `` computational '' basis states given by the binary representation of @xmath101 .",
    "note that such a transformation is very different from what is usually done on conventional computers where the values represented by binary words are transformed .",
    "it is therefore misleading to say that quantum computers are much faster than conventional computers at the fft .",
    "the qfft can be done very efficiently .",
    "the fft is done level by level ( @xmath183 levels ) as the operations on every level are individually unitary . in the classical fft on every level @xmath247 basic operations of the form ( eq . [ bo2 ] )",
    "are carried out . in the qfft",
    "all these @xmath247 operations are done in parallel .        where the parenthesis mean the value in binary given by the bits .",
    "this operation can be done in 2 steps , first without the phase factor @xmath265 and then multiplying the basis states with @xmath266 with appropriate phase factors :          so these are @xmath268 gates , each one a phase shift on some qubit , conditional on qubit number @xmath268 .",
    "note that in the end we should also swap bit number @xmath270 with bit number @xmath271 , bit number @xmath272 with bit number @xmath273 and so on to get the originally intended result , but usually it is not necessary to do this explicitly .",
    "let s summarize in words how the qfft is carried out : we begin by hadamard transforming the most significant qubit .",
    "then we apply appropriate phase shifts to all less significant qubits , conditional on the most significant one being 1",
    ". then we hadamard transform the second most significant qubit and conditional on it we apply appropriate phase shifts to all less significant qubits and so on .",
    "after hadamard transforming a qubit we do nt apply gates to it any more . thus if , as in shor s algorithm , the register is to be observed right after the qfft , we can always measure qubits after they were hadamard transformed , thus interleaving unitary gates and measurements .",
    "also we can wait with applying phase shifts to a qubit until just before it is hadamard transformed ( and then measured ) .",
    "the size of the phase shift will then depend on the already measured values of the higher significance qubits .",
    "so now what we do it to apply some phase shift to a qubit , hadamard transform and measure it .",
    "then we move on to the next lower significance qubit .    in fault",
    "tolerant quantum computing the phase shift gates will have to be composed of several gates from a fixed `` set of universal gates '' .",
    "still the qfft is a negligible part of the factoring algorithm .",
    "this can efficiently be computed using the fft .",
    "the statement is essentially that the component - wise product of the fft s of the @xmath26 s and the @xmath102 s is the fft of the convolution .",
    "so we try :            also besides the convolution we have gotten a second unwanted term .",
    "it can be made to vanish by setting the `` upper half '' of the sets @xmath105 and @xmath275 equal to zero .",
    "so to compute the convolution of two @xmath25-number sets we then have to use fft s with @xmath282 numbers .",
    "i assume that the reader has looked at the outline of fft - multiply in the main body of the paper .",
    "we want to do the fft over the ring of integers modulo some fixed element @xmath109 .",
    "first note that things will work out for the particular class of moduli @xmath283 , where @xmath25 is a power of 2 .",
    "it is easy to see that , as required , @xmath284 , where we now do all operations modulo @xmath285 .",
    "the other property ( eq . [ cond2 ] ) is demonstrated in the book by hopcroft et .",
    "we will choose @xmath109 a small power of 2 which makes the operations in the fft very efficient on a binary digital computer .",
    "as mentioned , the numbers to be multiplied are first decomposed into blocks of bits .",
    "the question now is what size of blocks we should choose .",
    "if there were an adequate modulus , we would like to choose the block size much smaller than the number of blocks . unfortunately the modulus proposed above is quite large and so there is no point in choosing small blocks as in the course of the fft the numbers we operate with will quickly become the size of the modulus .",
    "if we insist on a small modulus we must be ready to give up the ease with which we can operate with the above @xmath109 and @xmath285 .",
    "also it is not trivial to find such moduli where there are primitive roots with @xmath286 and the condition of eq .",
    "[ cond2 ] .",
    "it may still pay off , but i have given up this path as it is rather complicated .",
    "so back to the above choice of @xmath109 and @xmath285 .",
    "so how do we choose the number @xmath102 of blocks and their size @xmath183 ?",
    "note that @xmath102 is the number of numbers we have to fourier transform , thus it must be a power of 2 .",
    "then the modulus will be @xmath287 .",
    "for the result not to get truncated the block size has to be somewhat less than @xmath288 , which means that @xmath102 and @xmath183 are both going to be of the order of magnitude of @xmath289 , where @xmath5 is the number of bits of the numbers we want to multiply .",
    "the modulus is @xmath293 .",
    "thus for @xmath294 even we have @xmath115 and for @xmath294 odd @xmath295 .",
    "the components of the convolution are the sum of some @xmath102 products of numbers with @xmath183 bits . for the convolution",
    "not to be truncated by the modulus we thus must require :      if this condition is not true i simply revise my choice of @xmath102 , effectively going to the next larger size of the algorithm .",
    "thus in practise i increase @xmath294 by 1 and get the new @xmath102 and @xmath118 from it",
    ". there would be more efficient methods , but as this occurs seldomly for large @xmath5 , i choose the easier way",
    ".      one can iterate the fft - multiply and apply it to the component product of the fourier transformed factors .",
    "actually fft - multiply is much more efficient on the second level ( or still lower levels ) . on the second level we have to multiply two @xmath120 bit numbers",
    "modulo @xmath297 which can be done quite efficiently by modifying the way we computed the convolution .",
    "this time we do nt have to pad the factors with zeros , so the decomposition into blocks is @xmath298 , where the prime refers to the second level .",
    "note that now the block size @xmath129 is automatically a power of 2 .",
    "so a factor @xmath26 is decomposed into its blocks @xmath105 as follows :                          thus by doing @xmath308 and @xmath309 we get @xmath310 .",
    "we will choose @xmath306 a small even power of 2 so that @xmath305 is a small power of 2 .",
    "these operations ( which are always done modulo @xmath311 ) are rather easy . how to do them reversibly",
    "is described at the end of the appendix about how to compute reversibly the fft .",
    "from there it should be clear that their cost can be neglected .          for @xmath317",
    "even the modulus is not large enough .",
    "we can fix this problem by also making an fft - multiply with the modulus @xmath319 and using the chinese remainder theorem to recover the correct result .",
    "the computational cost for this should be negligible . here",
    "i just give a short outline without derivations .",
    "so say we are looking for a non - negative number @xmath10 which is smaller than @xmath320 and we are given @xmath321 and @xmath322 . then :                here i show how to carry out the basic operations of the fft . in principle",
    "every one of these steps is reversible and it thus seems that the fft - algorithm is particularly well suited for reversible computation . here",
    "we consider the fft over the ring of integers modulo a modulus of the form @xmath8 and with the primitive root @xmath109 a small power of 2 .",
    "the basic fft - operations acting on 2 registers are then of the form :      where @xmath157 are non - negative and smaller than @xmath158 .",
    "it is easy to see how this operation could be reversed , as 2 has a multiplicative inverse modulo any odd modulus .",
    "thus we could use the standard tricks ( eqs .",
    "[ rev1 ] , [ rev2 ] ) of reversible computation to get rid of garbage and input , but usually one can find more efficient `` shortcuts '' which is what we will try here .",
    "actually i havent found an efficient way of doing it without leaving anything but the result ( rhs of eq .",
    "[ fftop ] ) ) around , so i propose a scheme which leaves 2 or 3 qubits of garbage around .",
    "this is no problem as it does nt take up much space , but i would have expected to find a more elegant solution .",
    "so everybody interested is invited to try to do better .",
    "note that any fft we perform will soon afterwards be undone , so then the garbage qubits will also go away .",
    "step consists of additions of 2 `` quantum '' numbers , which is no big problem .",
    "step consists of conditionally subtracting @xmath285 from @xmath327 and conditionally adding @xmath285 to @xmath328 .",
    "the problem here is how to uncompute the conditional bits .",
    "step looks rather simple as this operation is easy in conventional computation , but here i havent managed to find an efficient scheme without leaving garbage around .",
    "step above consists of 2 such operations .",
    "a subtraction is essentially the same using the complement technique .",
    "one would expect that an addition of a `` quantum '' number to another `` quantum '' number would use more toffoli gates than the previously described addition of a fixed `` classical '' number to a quantum number , but this is not so .",
    "note that one of the `` quantum '' numbers has to stay around as otherwise the operation would not be reversible .",
    "so lets do @xmath329 .",
    "thus we add @xmath26 to the @xmath102-register .",
    "again we temporarily need carry qubits ( which are initially set to 0 ) .",
    "i denote by @xmath36 the carry which comes from the @xmath330 binary place but has the same place value as @xmath331 and @xmath332 . for every binary place",
    "there are 2 operations , one to compute the next carry @xmath36 from @xmath333 and @xmath36 and one to compute the sum bit :                                                                       + to check the correctness of this sequence and of the above formulas i recommend considering separately the case @xmath65 and @xmath66 . the first 2 gates ( from the left )",
    "compute the sum and the rest is for the carry . after having computed the sum we must run everything backwards to uncompute the carries .",
    "but because we do nt want to uncompute the sum , we then leave away the 2 leftmost gates .",
    "actually we begin by subtracting @xmath285 from @xmath327 and add it again if we have obtained a negative number , but of course @xmath337 will remain around . as we use complement notation for negative numbers , @xmath338 is trivial to obtain",
    ". then we make a conditional addition to get @xmath340 .",
    "so now we have the result and would like to get rid of the conditional bits . to do this is equivalent to computing those bits from the result @xmath341 . as @xmath285 is odd",
    "it is easy to obtain the xor of the two bits .",
    "this can be seen as follows :      thus if this sum is even we know that @xmath343 and vice versa .",
    "this we read off simply from the lowest significance bits of @xmath344 and @xmath345 .",
    "so now we can reduce the garbage to 1 qubit .",
    "to get rid of this remaining bit seems to be more costly and i propose not to do it .",
    "nevertheless i show how it could be done .",
    "we have to consider two cases :              for our @xmath350 we have only 2 terms in the sum",
    ". the potentially non - zero bits in these two blocks overlap in exactly 1 bit , as @xmath350 has @xmath353 potential non - zero bits .",
    "when we compute the sum @xmath354 we will have to leave around one of these 2 bits .",
    "one of those bits is the @xmath355 bit of @xmath10 and this is the one i propose to leave around .",
    "after this we still may have to add @xmath158 if the sum is negative to get the correct remainder .",
    "i also propose to leave around the associated control qubit as i havent found an easy way to uncompute it .",
    "the first 2 lines are ( unconditional ) `` q+q '' additions .",
    "uncomputing the carries doubles the number of toffoli gates from @xmath101 to @xmath357 .",
    "the next 2 lines are essentially conditional addition of the `` c+q '' type , the same we use in the standard algorithm .",
    "the cost of the last line comes from the conditional addition of @xmath285 .",
    "so the total number of toffoli gates per elementary fft - operation is :          for large @xmath101 we want to parallelize the additions in the above scheme , furthermore for large @xmath101 we can make substantial simplifications which will only lead to a small `` algorithmic '' error rate . without such improvements",
    "the first level fft in my proposed 2-level fft - multiply scheme would dominate the overall execution time .",
    "i have estimated that for the range of @xmath5 we consider , an error rate per elementary fft - operation of about @xmath361 still leads to a tolerable overall error rate .",
    "as on the first level fft @xmath101 will be larger than 40 we can make simplifications based on the special form of the modulus @xmath8 .",
    "first we can assume that all numbers modulo @xmath158 are actually smaller than @xmath59 .",
    "note that this will also reduce the garbage from 3 to 2 bits , as we can assume that the other one is zero ( it s the @xmath355 bit of @xmath10 ) .    in the above list of costs of the individual operations",
    "the three lines with cost @xmath362 can be simplified as they essentially involve an addition or subtraction of @xmath8 .",
    "note that the condition bit @xmath363 is replaced by @xmath364 which is trivial to compute . also adding or subtracting @xmath59 is simple . adding or subtracting 1 costs a bit more . to keep the error rate low enough ,",
    "we need to extend the addition to the 40 least significant bits . because it is a conditional addition this costs @xmath365 toffoli gates .",
    "also the second and third lines can actually be done simultaneously .",
    "what remains are the first two lines which are `` q+q '' type additions .",
    "it turns out that my scheme for parallelizing `` c+q '' type additions works just as well for `` q+q '' additions .",
    "from there we get @xmath366 and @xmath367 per addition , where @xmath160 is the total number of toffoli gates and @xmath163 is the number of sequential toffoli gates , thus essentially the time measured in units of toffoli execution times .",
    "p. shor , + _ polynominal - time algorithms for prime factorization and discrete logarithms on a quantum computer .",
    "_ + in _ proc .",
    "35th annual symposium on foundations of computer science . _ ieee press , pp 124 - 134 , nov .",
    "1994 , quant - ph/9508027 + also siam j. computing 26 ( 1997 ) 1484                c. miquel , j.p .",
    "paz and r. perazzo , phys .",
    "a * 54 * , 2605 ( 1996 ) ; v. vedral , a. barenco and a. ekert , phys . rev .",
    "a * 54 * , 139 ( 1996 ) ; d. beckman , a. n. chari , s. devabhaktumi , j. preskill , phys . rev .",
    "a * 54 * , 1034 ; e. knill , private communication ."
  ],
  "abstract_text": [
    "<S> we present fast and highly parallelized versions of shor s algorithm . with a sizable quantum computer it would then be possible to factor numbers with millions of digits . </S>",
    "<S> the main algorithm presented here uses fft - based fast integer multiplication . </S>",
    "<S> the quick reader can just read the introduction and the `` results '' section . </S>"
  ]
}