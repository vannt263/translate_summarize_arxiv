{
  "article_text": [
    "probabilistic kernel machines based on gaussian processes ( gps ) @xcite are popular in a number of applied domains as they offer the possibility to flexibly model complex data and , depending on the choice of covariance function , to gain some understanding on the underlying behavior of the system under study .",
    "when quantification of uncertainty is of primary interest , it is necessary to accurately characterize the posterior distribution over covariance parameters .",
    "this has been argued in a number of papers where this is done by means of markov chain monte carlo ( mcmc ) methods @xcite .",
    "the limitation of mcmc approaches to draw samples from the posterior distribution over covariance parameters is that they need to compute the marginal likelihood at every iteration . in gp regression , a standard way to compute the marginal likelihood involves storing and factorizing an @xmath0 matrix , leading to @xmath1 time and @xmath2 space complexities , where @xmath3 is the size of the data set . for large data sets",
    "this becomes unfeasible , so a large number of contributions can be found in the literature on how to make these calculations tractable .",
    "for example , when the gp covariance matrix has some particular properties , e.g. , it has sparse inverse @xcite , it is computed on regularly spaced inputs @xcite , or it is computed on univariate inputs @xcite , it is possible to considerably reduce the complexity in computing the marginal likelihood .",
    "when these properties do not hold , which is common in several machine learning applications , approximations are usually employed .",
    "some examples involve the use subsets of the data @xcite , the determination of a small number of surrogate input vectors that represent the full set of inputs @xcite , and the application of gps to subsets of the data obtained by partitioning the input space @xcite , to name a few .",
    "unfortunately , it is difficult to assess to what extent approximations affect the quantification of uncertainty in predictions , although some interesting results in this direction are reported in @xcite .",
    "the focus of this paper are applications of gp regression where the structure of the covariance matrix is not necessarily special and quantification of uncertainty is of primary interest , so that approximations should be avoided .",
    "this paper proposes an adaptation of the stochastic gradient langevin dynamics ( sgld ) algorithm @xcite to draw samples from the posterior distribution over gp covariance parameters .",
    "sgld does not require the computation of the marginal likelihood and yields samples from the posterior distribution of interest with negligible bias .",
    "this has the enormous advantage that stochastic gradients can be computed by solving linear systems only @xcite .",
    "linear systems can be solved by means of iterative methods , such as the conjugate gradient ( cg ) algorithm , that are based on parallelizable covariance matrix - vector products @xcite .",
    "similar ideas were previously put forward to optimize gp covariance parameters @xcite . despite the @xmath2 in time and @xmath4 in space complexities of these methods compare well with the @xmath1 in time and @xmath2 in space complexities of traditional mcmc - based inference , solving dense linear systems at each iteration makes the whole inference framework too slow to be of practical use .",
    "we compare a number of standard ways to speed up the solution of dense linear systems , such as fast covariance matrix - vector products @xcite and preconditioning @xcite , and in line with what reported in @xcite , we observe that they yield little gain in computational speed compared to the standard cg algorithm . in order to enable practical inference for gps applied to large data sets",
    ", we therefore develop an unbiased linear systems solver ( ulisse ) that essentially allows the cg algorithm to stop early while retaining unbiasedness of the solution .",
    "we highlight here that ( i ) in @xcite , an unbiased estimate of the gradient is computed by considering small batches of data .",
    "recent alternative contributions on scaling bayesian inference by analyzing small batches of data can be found in @xcite .",
    "gps do not lend themselves to this treatment , due to the covariance structure making all data dependent on one another .",
    "( ii ) ulisse is complementary to recent approaches in the area of probabilistic numerics that aim at infering , rather than computing , solutions to linear systems @xcite .",
    "( iii ) the proposed inference method is based on `` noisy '' gradients and is complementary to recent inference approaches based on noisy likelihoods @xcite . in gp regression , iterative methods akin to the cg algorithm @xcite can be employed to obtain an unbiased estimate of the log - determinant of the covariance matrix , but this remains an extremely onerous calculation needed to get an unbiased estimate of the log - marginal likelihood .",
    "a further and perhaps more challenging issue is transforming the unbiased estimate of the log - marginal likelihood in an unbiased estimate of the marginal likelihood @xcite .",
    "this paper demonstrates that employing ulisse within sgld makes it possible to accurately carry out inference of covariance parameters in gps and effectively scale these computations to large data sets .",
    "we report results on a data set with about @xmath5 thousand input vectors where we can draw ten thousand samples per day from the posterior distribution over covariance parameters on a desktop machine with standard hardware . to the best of our knowledge",
    ", this paper reports the first real attempt to enable full quantification of uncertainty of covariance parameters of gps without reducing the number of input vectors and without imposing sparsity on the gp covariance or its inverse .",
    "the paper is organized as follows .",
    "section  [ sec : inference ] briefly reviews gps and motivates the adoption of sgld to infer gp covariance parameters .",
    "section  [ sec : linear : system ] describes and evaluates the cg algorithm to solve linear systems and some variants based on fast covariance matrix - vector product and preconditioning .",
    "section  [ sec : ulisse ] presents ulisse and its use to obtain an unbiased estimate of the gradient of the log - marginal likelihood in gps .",
    "section  [ sec : experiments ] demonstrates the ability of the proposed methodology to accurately infer covariance parameters in gps and its scalability properties to a large data set where the marginal likelihood can not be computed exactly .",
    "finally , section  [ sec : conclusions ] draws the conclusions .",
    "in gp regression , a set of continuous labels @xmath6 is associated with a set of input vectors @xmath7 . throughout the paper , we will employ zero mean gps with the following covariance function : @xmath8 with @xmath9 if @xmath10 and zero otherwise .",
    "the parameter @xmath11 determines the rate of decay of the covariance function , whereas @xmath12 represents the marginal variance of each gaussian random variable comprising the gp .",
    "the parameter @xmath13 is the variance of the ( gaussian ) noise on the labels .",
    "let @xmath14 be the covariance matrix with @xmath15 and denote by @xmath16 the vector comprising all parameters of the covariance matrix @xmath14 , namely @xmath17 . in a bayesian sense",
    ", we would like to carry any uncertainty in parameters estimates forward to predictions over the label @xmath18 for a new input vector @xmath19 .",
    "in particular , this requires solving the following integral : @xmath20 such an expectation , like any other expectation under the posterior over @xmath16 , is analytically intractable , so it is necessary to resort to some approximations . a standard way to tackle",
    "this intractability is to draw samples from @xmath21 using mcmc methods , and approximate the expectation with the monte carlo estimate @xmath22 where @xmath23 denotes the @xmath24th of a set of samples from @xmath21 .",
    "drawing samples from the posterior distribution can be done using several mcmc algorithms that essentially are based on a proposal mechanism and on an accept / reject step that requires the evaluation of the log - marginal likelihood : @xmath25 = -\\frac{1}{2 } \\log\\left(|k|\\right ) - \\frac{1}{2 } { \\mathbf{y}}^{{\\mathrm{t } } } k^{-1 } { \\mathbf{y}}+ \\mathrm{const.}\\ ] ] a standard way to proceed , is to factorize the covariance matrix @xmath26 using the cholesky algorithm @xcite .",
    "the factorization costs @xmath1 operations and requires the storage of @xmath2 entries of the covariance matrix , but after that computing the log - determinant and the inverse of @xmath14 multiplied by @xmath27 can be done using @xmath2 operations .",
    "the computational complexities above pose a constraint on the scalability of gps to large data sets .",
    "iterative methods based on covariance matrix - vector products ( cmvps ) have been proposed to obtain an unbiased estimate of the log - marginal likelihood .",
    "even though these methods scale with @xmath2 in time and @xmath4 in space , they are of little practical use in the task of sampling from @xmath21 , as the number of iterations needed to estimate the log - determinant term can be prohibitively large ( see , e.g. , @xcite ) .",
    "we now illustrate our proposal to obtain samples from @xmath21 with negligible bias and without having to estimate log - determinants and marginal likelihoods .",
    "we briefly describe how to adapt sgld @xcite to obtain samples from @xmath21 in gps .",
    "the idea behind sgld is to modify the standard stochastic gradient optimization algorithm @xcite by injecting gaussian noise in a way that ensures transition into a langevin dynamics phase yielding samples from the posterior distribution of interest . in particular",
    ", the proposal of a new set of parameters is @xmath28 \\right\\ } + { \\boldsymbol{\\eta}}_t\\ ] ] with @xmath29 and @xmath30 an unbiased estimate of the gradient of @xmath31 $ ] .",
    "we have also introduced a preconditioning matrix @xmath32 that can be chosen to improve convergence of sgld .",
    "the update equation , except for @xmath33 , is the standard update used in stochastic gradient optimization .",
    "the parameters @xmath34 are chosen to satisfy @xmath35 as these conditions , along with some other technical assumptions , guarantee convergence to a local maximum .",
    "the injected noise @xmath33 is gaussian with covariance @xmath36 ensuring that the algorithm transitions into a discretized version of a langevin dynamics with target distribution given by the posterior over @xmath16 . in principle",
    ", it would be necessary to accept or reject the proposals , which would require evaluating the marginal likelihood .",
    "the key result in @xcite is that when sgld reaches the langevin dynamics phase , the step - size @xmath34 is small enough to make the acceptance rate close to one .",
    "therefore , in this phase it is possible to accept all proposals , avoiding having to evaluate the marginal likelihood , at the cost of introducing a negligible amount of bias .",
    "following @xcite , we can estimate when the algorithm reaches the langevin dynamics phase by monitoring the ratio between the variance of the stochastic gradients and the variance of the injected noise . defining @xmath37 to be the sampling covariance of the stochastic gradients and @xmath38 to be the largest eigenvalue of a matrix @xmath39 ,",
    "we can write such a ratio as @xmath40 when this ratio is small enough the algorithm is in its langevin dynamics phase and produces samples from the posterior distribution over @xmath16 .",
    "further theoretical analyses on the convergence properties of sgld can be found in @xcite .",
    "the motivation for employing sgld for inference of gp covariance parameters comes from inspecting the gradient of the log - marginal likelihood that has components @xmath41 computing the @xmath42 s requires again @xmath1 operations due to the trace term and the linear system @xmath43 .",
    "however , we can introduce @xmath44 vectors @xmath45 with components drawn from @xmath46 with probability @xmath47 and unbiasedly estimate the trace term @xcite , obtaining : @xmath48 given that @xmath49 , we can readily verify that @xmath50 =   { \\mathrm{tr}}\\left[k^{-1 } \\frac{\\partial k}{\\partial \\theta_i } { \\mathrm{e } } ( { \\mathbf{r}}^{(i ) } { { \\mathbf{r}}^{(i)}}^{{\\mathrm{t } } } ) \\right ] $ ] , which yields the trace term in eq .",
    "[ eq : gradient : exact ] .",
    "hence , in order to compute an unbiased version of the gradient of the log - marginal likelihood we need to solve one linear system for @xmath27 and one for each of the @xmath44 vectors @xmath45 used to estimate the trace term .",
    "this consideration forms the basis of the proposed methodology .",
    "computing an unbiased version of the gradient involves solving linear systems only , which is much easier and cheaper than estimating log - determinants .",
    "we have discussed that sgld to infer covariance parameters in gps requires solving linear systems . here",
    "we briefly review the conjugate gradient ( cg ) algorithm that is a popular method to iteratively solve linear systems based on covariance matrix vector product ( cmvp ) operations .",
    "cmvps can be carried out without having to store @xmath14 , so their time and space complexities are in @xmath2 and @xmath4 , respectively .",
    "we also discuss and evaluate a few variants to speed up computations / convergence , such as preconditioning and fast cmvps . throughout this section",
    "we will evaluate the effectiveness of these alternatives on a gp regression task applied to the concrete data set from the uci repository @xcite . this data set contains data about the compressive strength of @xmath51 samples of concrete described by @xmath52 features .",
    "data @xmath53 , vector @xmath54 , convergence threshold @xmath55 , initial vector @xmath56 , maximum number of iterations @xmath57 @xmath58 ; @xmath59 ; @xmath60 ; @xmath61 ; @xmath62 ; @xmath63 ; @xmath64 ;    given a linear system of the form @xmath65 with @xmath14 and @xmath54 given , the cg algorithm @xcite yields the solution @xmath66 without having to invert or factorize the matrix @xmath14 .",
    "the idea is to calculate the solution @xmath66 as the minimizer of @xmath67 which can be obtained by employing gradient - based optimization .",
    "the cg algorithm is initialized from an initial guess @xmath56 . after that",
    ", the iterations refine the solution @xmath66 by updates in directions @xmath68 .",
    "the cg algorithm , in comparison with the standard gradient descent , is characterized by the fact that @xmath14-orthogonality ( or conjugacy with respect to @xmath14 ) of the search directions is imposed , namely @xmath69 when @xmath70 .",
    "this condition yields a sequence of residuals @xmath71 that are mutually orthogonal , and guarantees convergence in at most @xmath3 iterations .",
    "remarkably , the cg algorithm can be implemented in a way that requires a single cmvp ( @xmath72 ) at each iteration ( see algorithm  [ alg : cg : algorithm ] ) .    the trade - off between accuracy and speed",
    "is governed by the threshold @xmath55 , which in this paper is set to @xmath73 .",
    "theoretically , the cg algorithm is guaranteed to converge in at most @xmath3 iterations , but in practice , due to the representation in finite numerical precision , orthogonality of the directions can be lost , especially in badly conditioned systems , and the cg algorithm can take more than @xmath3 iterations to converge . the condition number of a matrix is defined as the ratio between its largest and smallest eigenvalues : @xmath74 fig .",
    "[ fig : condition : number : prior ] shows the distribution of the condition number when each covariance parameter @xmath75 is sampled form a gamma distribution with shape and rate parameters @xmath76 and @xmath77 .",
    "the distributions are reasonably vague and give a rough idea of the typical condition numbers encountered during the inference of gp covariance parameters for the concrete data set .     of the covariance matrix for different choices of shape and rate parameters of a gamma prior on each covariance parameter @xmath78 . ]",
    "-0.2 in    we can expect slower convergence speed when the condition number is large due to numerical instabilities ; we are interested in quantifying to what extent this applies to gps and what is the impact of cheap cmvps and preconditioning on convergence speed . in the remainder of this section",
    ", we will consider the problem of solving the linear system @xmath79 that is needed in the calculation of part of the gradient in eq .",
    "[ eq : gradient : unbiased ] .",
    "the results pertaining to the solution of the linear systems @xmath80 are quite similar , so for the sake of brevity we will omit them .",
    "we consider here the use of two fast cmvps based on efficient representation of input data that we will call `` kdtree '' @xcite and `` anchors '' @xcite .",
    "these methods yield fast cmvps at the price of a lower accuracy .    in the top row of fig .",
    "[ fig : compare : matrix : vector : preconditioning ] we show the number of iterations required by the cg algorithm to reach convergence versus the condition number and the error in the solution versus the condition number .",
    "the error is defined as the norm of the difference between the solution obtained by the cg algorithm and the one obtained by factorizing @xmath14 using the cholesky algorithm and carrying out forward and back substitutions with @xmath27 .",
    "we compare a baseline cg algorithm with cmvps performed in double precision with cg algorithms implemented with ( i ) single precision ( `` float '' ) cmvps , ( ii ) `` kdtree '' cmvps and ( iii ) `` anchors '' cmvps .",
    "the convergence threshold of the cg algorithm was set to @xmath81 , so in order to be able to satisfy this criterion when employing `` kdtree '' and `` anchors '' cmvps , we selected the relative and absolute tolerance parameters to be @xmath82 .",
    "the results indicate that double precision calculations lead to the lowest number of iterations compared to the other methods , especially when @xmath83 is large .",
    "double precision calculations also offer the lowest error .",
    "single precision calculations lead to a very poor error compared to the other methods .",
    "the cg algorithm with `` kdtree '' cmvps seems to take longer to converge than the one with `` anchor '' cmvps , but it achieves a lower error .",
    "drawing definitive conclusions on whether fast cmvps yield any gain in computing time is far from trivial , as this very much depends on implementation details and hardware where the code is run .",
    "what we can say , however , is that gaining orders of magnitude speed - ups would require reducing the accuracy of fast cmvps , but this would require loosening up the convergence criterion in order for the cg algorithm to converge . as a result",
    ", we would be able to obtain solutions of linear systems faster but at the cost of a reduced accuracy in the solution , which in turn would bias the estimation of gradients .",
    "-0.2 in      the preconditioned cg ( pcg ) is a variant of the cg algorithm that aims at mitigating the issues associated with the rate of convergence of the cg algorithm when the condition number @xmath83 is large .",
    "a ( right ) preconditioning matrix @xmath84 operates on the linear system yielding @xmath85 the success of pcg is based on the possibility to construct @xmath84 so that @xmath86 is well conditioned .",
    "this can be achieved when @xmath87 well approximates @xmath88 , and a complication immediately arises on how to do so for general kernel matrices without carrying out expensive operations ( in @xmath1 ) .",
    "in @xcite it was proposed to define @xmath89 with @xmath90 .",
    "compared to the standard cg algorithm , the pcg algorithm introduces the solution of an `` inner '' linear system of the form @xmath91 at each iteration , that can be solved again using the cg algorithm .",
    "a large value of @xmath92 makes @xmath93 well conditioned and makes convergence speed of the inner cg algorithm faster , whereas it makes @xmath87 and @xmath88 considerably different leading to the necessity to run the outer cg algorithm for several iterations . for small values of @xmath92 the situation is reversed , so @xmath92 needs to be tuned to find an optimal compromise .    in the bottom row of fig .",
    "[ fig : compare : matrix : vector : preconditioning ] , we compare the standard cg algorithm with two versions of the pcg algorithm on number of iterations and accuracy of the solution . in the first version of the pcg algorithm we used double precision calculations when solving the inner linear systems , whereas in the second version we used single precision calculations . in both versions of the pcg algorithm we set @xmath92 to yield the lowest number of iterations in order to show whether it is possible to reduce the number of computations .",
    "the results show that the standard cg algorithm takes less iterations to converge than the pcg algorithm ( counting both inner and outer iterations ) . even in the case of single precision calculations in the inner cg algorithm ,",
    "we did not experience any gain in computing time due to the increased number of iterations . for other data and in different experimental conditions",
    "there might be a computational advantage in using a preconditioner , as shown in @xcite , but the gain is generally modest .",
    "from the analysis in the previous sections it is evident that none of the standard ways to speedup calculations and convergence of the cg algorithm offer substantial gains in computing time . as a result , employing iterative methods as an alternative to traditional factorization techniques seems beyond practicality as pointed out , e.g. , in @xcite .",
    "one of the novel contributions of this paper is to accelerate the cg algorithm at the expenses of obtaining an ( unbiased ) estimate of the solution .",
    "the idea is to stop the cg algorithm before the convergence criterion is satisfied and apply some corrections to ensure unbiasedness of the solution .",
    "we note here that our proposal can be applied to any of the variants of the cg algorithm presented earlier and to dense as well as sparse linear systems .",
    "we can rewrite the final solution of a linear system obtained by the cg algorithm as a sum of incremental updates @xmath94 assuming that it takes @xmath57 iterations to satisfy the convergence threshold @xmath55 .",
    "we can define an `` early stop '' threshold @xmath95 that will be reached after @xmath96 iterations , and rewrite the final solution by introducing a series of coefficients as follows @xmath97 we will focus on coefficients defined as @xmath98 , but this choice is not restrictive . we can now obtain an unbiased estimate of the solution of the linear system by adding these instructions to the standard cg algorithm : set @xmath99 and iterate for @xmath100 the following two steps ( i ) draw @xmath101 $ ] ( ii ) if @xmath102 then @xmath103 , else return @xmath104 and stop the cg algorithm .",
    "the expectation of @xmath104 is clearly @xmath66 and the rate of decay @xmath105 in the expression of @xmath106 determines the average number of steps that are carried out after the convergence threshold @xmath107 is reached .",
    "calculated with @xmath108 and @xmath109 on number of iterations and standard deviation of the solution .",
    "the top row corresponds to @xmath110 in the calculation of the weights @xmath106 , whereas the bottom row corresponds to @xmath111.,title=\"fig : \" ]   calculated with @xmath108 and @xmath109 on number of iterations and standard deviation of the solution .",
    "the top row corresponds to @xmath110 in the calculation of the weights @xmath106 , whereas the bottom row corresponds to @xmath111.,title=\"fig : \" ]     calculated with @xmath108 and @xmath109 on number of iterations and standard deviation of the solution .",
    "the top row corresponds to @xmath110 in the calculation of the weights @xmath106 , whereas the bottom row corresponds to @xmath111.,title=\"fig : \" ]   calculated with @xmath108 and @xmath109 on number of iterations and standard deviation of the solution .",
    "the top row corresponds to @xmath110 in the calculation of the weights @xmath106 , whereas the bottom row corresponds to @xmath111.,title=\"fig : \" ]    -0.2 in    for simplicity , we set the early stop threshold to @xmath112 as @xmath113 gives a rough indication of the average error that we are expecting in each element of the solution . in fig .",
    "[ fig : compare : early : stop ] we report number of iterations and average standard deviation across the elements of the solution .",
    "ulisse with two different values of @xmath105 and @xmath113 is compared with the baseline cg algorithm without early stop ( `` cg '' ) .",
    "we stress again that the error is such that the solution is unbiased .",
    "we conclude this section by showing the impact of ulisse in the calculation of stochastic gradients in gps . applying the proposed unbiased solver to the first term of @xmath114 in eq .",
    "[ eq : gradient : unbiased ] is straightforward and it requires solving @xmath44 linear systems , one for each of the @xmath45 vectors . for the quadratic term in @xmath27 , instead",
    ", we need to obtain two independent unbiased estimates of @xmath43 in order for the expectation of the whole term to be unbiased .",
    "this can be implemented by running a single instance of the cg algorithm and keeping track of two solutions obtained by independent draws of the uniform variables @xmath115 used to early stop the cg algorithm .",
    "we remark that the unbiased estimation of gradients involves now two sources of stochasticity : one due to the stochastic estimate of the trace term in eq .",
    "[ eq : gradient : exact ] , and one due to the proposed way to unbiasedly solve all linear systems in eq .",
    "[ eq : gradient : unbiased ] .     and @xmath116 to estimate the gradient of the log - marginal likelihood in eq .",
    "[ eq : gradient : unbiased ] . in ulisse ,",
    "the weights @xmath106 are calculated with @xmath117.,title=\"fig : \" ]   and @xmath116 to estimate the gradient of the log - marginal likelihood in eq .",
    "[ eq : gradient : unbiased ] . in ulisse ,",
    "the weights @xmath106 are calculated with @xmath117.,title=\"fig : \" ]    -0.2 in    fig .",
    "[ fig : comparison : gradient ] reports the average , taken with respect to @xmath118 repetition of the @xmath119 of the relative square norm of the error : @xmath120 as a function of the condition number @xmath83 .",
    "we used one vector @xmath121 to estimate the gradient in eq .",
    "[ eq : gradient : unbiased ] .",
    "the figure shows that the estimate in eq .",
    "[ eq : gradient : unbiased ] ( `` cg '' in the figure ) is quite accurate , as the relative error is small in a wide range of values of @xmath83 . also , at the expenses of a larger variance in the estimate of the gradient , ulisse yields orders of magnitude improvements in the number of iterations .",
    "in this section , we infer covariance parameters of gp regression models using sgld with ulisse .",
    "we start by considering the concrete data set where it is possible to compare our proposal with the metropolis - hastings ( mh ) algorithm .",
    "we then demonstrate the scalability of the proposed methodology by considering a data set with @xmath122 and @xmath123 .",
    "we ran the mh algorithm for fifty - thousand iterations to the gp regression model with covariance in eq .",
    "[ eq : covariance : rbf ] applied to the concrete data set .",
    "we allowed for an initial adaptive phase to reach an average acceptance rate between @xmath124 and @xmath125 , and we discarded the first ten - thousand samples .",
    "[ fig : compare : mcmc : concrete ] shows the running mean and the interval corresponding to plus / minus twice the running standard deviation of the posterior over the three parameters ( solid red lines ) computed over the remaining forty - thousand samples .    ) - left panel : comparison of mcmc ( red ) and sgld with ulisse ( black ) on running mean and plus / minus two standard deviations .",
    "the trace of one chain of sgld is shown in gray .",
    "right panel : convergence analysis of sgld with ulisse with psrf computed over ten chains.,title=\"fig : \" ] ) - left panel : comparison of mcmc ( red ) and sgld with ulisse ( black ) on running mean and plus / minus two standard deviations .",
    "the trace of one chain of sgld is shown in gray .",
    "right panel : convergence analysis of sgld with ulisse with psrf computed over ten chains.,title=\"fig : \" ]    ) - left panel : comparison of mcmc ( red ) and sgld with ulisse ( black ) on running mean and plus / minus two standard deviations .",
    "the trace of one chain of sgld is shown in gray .",
    "right panel : convergence analysis of sgld with ulisse with psrf computed over ten chains.,title=\"fig : \" ] ) - left panel : comparison of mcmc ( red ) and sgld with ulisse ( black ) on running mean and plus / minus two standard deviations .",
    "the trace of one chain of sgld is shown in gray .",
    "right panel : convergence analysis of sgld with ulisse with psrf computed over ten chains.,title=\"fig : \" ]    ) - left panel : comparison of mcmc ( red ) and sgld with ulisse ( black ) on running mean and plus / minus two standard deviations .",
    "the trace of one chain of sgld is shown in gray .",
    "right panel : convergence analysis of sgld with ulisse with psrf computed over ten chains.,title=\"fig : \" ] ) - left panel : comparison of mcmc ( red ) and sgld with ulisse ( black ) on running mean and plus / minus two standard deviations .",
    "the trace of one chain of sgld is shown in gray .",
    "right panel : convergence analysis of sgld with ulisse with psrf computed over ten chains.,title=\"fig : \" ]    -0.2 in    we compare the run from the mh algorithm with sgld , where we made the following design choices .",
    "we employed ulisse within the cg algorithm with double precision cmvps .",
    "we set the early stop threshold @xmath107 to @xmath126 and the parameter @xmath105 in the computation of the weights @xmath106 to @xmath127 .",
    "stochastic gradients were computed using @xmath128 vectors @xmath45 .",
    "we ran sgld for forty - thousand iterations ; the step - size was set to @xmath129 , with @xmath130 , and it was chosen to start from @xmath131 and reduce to @xmath132 on the last iteration . during the execution of sgld we monitored the quantity @xmath133 as discussed in section  [ sec : ulisse ] , and we froze the value of @xmath34 when it was less than @xmath134 ; the covariance of the gradients @xmath37 was estimated on batches of one - hundred iterations . in order to speed up computations",
    ", we decided to redraw the vectors @xmath45 every twenty iterations and to keep them fixed in between .",
    "the advantage of this is that the solutions of the linear systems @xmath135 can be used to initialize the same systems when proposing new @xmath16 s thus speeding up convergence .",
    "finally , we set the preconditioning matrix @xmath32 in sgld as the inverse of the negative hessian of the log of the posterior density at its mode computed on a subset of five hundred input vectors , as this is cheap way to obtain a rough idea of the covariance structure of the posterior distribution for the full data set .",
    "sgld yields an effective sample size of about @xmath136 and it draws one independent sample every @xmath137  sec . in fig .",
    "[ fig : compare : mcmc : concrete ] we report the running statistics for the three parameters ( solid black lines ) , and the trace - plot of one run of sgld ( solid gray lines ) , where we discarded all iterations prior to the freezing of the step - size @xmath34 .",
    "the figure shows a striking match between the results obtained by a standard mcmc approach and sgld with ulisse .",
    "this demonstrates that our proposal is a valid alternative to other mcmc approaches to reliably quantify uncertainty in gps .    in order to check convergence speed of sgld , we ran ten parallel chains and computed the potential scale reduction factor ( psrf ) @xcite .",
    "the chains were initialized by drawing from a gaussian with mean on the map solution over a subset of five hundred input vectors and covariance @xmath32 , so as to ensure enough dispersion to reliably report the psrf .",
    "[ fig : compare : mcmc : concrete ] shows the median and the @xmath138th percentile of the psrf across the ten chains .",
    "the analysis of these plots reveals that sgld achieves convergence after few thousand iterations .      )",
    "- left panel : running mean and plus / minus two standard deviations ( black ) and trace of one chain of sgld ( gray ) .",
    "right panel : convergence analysis of sgld with ulisse reporting the psrf computed over five chains.,title=\"fig : \" ] ) - left panel : running mean and plus / minus two standard deviations ( black ) and trace of one chain of sgld ( gray ) .",
    "right panel : convergence analysis of sgld with ulisse reporting the psrf computed over five chains.,title=\"fig : \" ]    ) - left panel : running mean and plus / minus two standard deviations ( black ) and trace of one chain of sgld ( gray ) .",
    "right panel : convergence analysis of sgld with ulisse reporting the psrf computed over five chains.,title=\"fig : \" ] ) - left panel : running mean and plus / minus two standard deviations ( black ) and trace of one chain of sgld ( gray ) .",
    "right panel : convergence analysis of sgld with ulisse reporting the psrf computed over five chains.,title=\"fig : \" ]    ) - left panel : running mean and plus / minus two standard deviations ( black ) and trace of one chain of sgld ( gray ) .",
    "right panel : convergence analysis of sgld with ulisse reporting the psrf computed over five chains.,title=\"fig : \" ] ) - left panel : running mean and plus / minus two standard deviations ( black ) and trace of one chain of sgld ( gray ) .",
    "right panel : convergence analysis of sgld with ulisse reporting the psrf computed over five chains.,title=\"fig : \" ]    -0.2 in    we now present the application of sgld with ulisse to a data set where it is not possible to run any mcmc algorithm with exact computation of the marginal likelihood on a conventional desktop machine .",
    "this data set contains data collected as part of the 1990 us census . in this study , we used the 8l data set where the regression task associates the median house price in a given region with demographic composition and housing market features ( @xmath139 and @xmath123 ) .",
    "we kept the same experimental conditions as in the case of the concrete data , except that @xmath34 was chosen to decrease from @xmath140 to @xmath141 to cope with the larger gradients obtained for this data set , and the preconditioner @xmath32 was estimated based on the map on one - thousand data points .",
    "the running statistics for the three parameters for one chain are reported in fig .",
    "[ fig : final : census ] , along with the psrf computed across five chains , which shows that convergence was reached after few thousand iterations .",
    "sgld with ulisse was run on a desktop machine with an eight core ( i7 - 2600 cpu at 3.40ghz ) processor , and an nvidia geforce gtx 590 graphics card ( released in 2011 ) . the two gpus in the graphics card are used to carry out cmvps . with this arrangement",
    ", we were able to draw roughly ten thousand samples per day from the posterior distribution over covariance parameters .",
    "sgld yields an effective sample size of roughly @xmath136 , and it can draw one independent sample every @xmath142  hours .",
    "this paper presented a novel way to accurately infer covariance parameters in gps .",
    "the novelty stems from the combination of stochastic gradient - based inference and a fast unbiased solver of linear systems .",
    "the results demonstrate that it is possible to carry out inference of gp covariance parameters over a data set comprising about @xmath5 thousand input vectors in a day on a desktop machine with standard hardware .",
    "the proposed methodology can exploit parallelism in computing covariance matrix - vector products , so there is an opportunity to scale `` exact '' inference ( in a monte carlo sense ) to even larger data sets .",
    "we are not aware of any method that is capable of carrying out full quantification of uncertainty of gp covariance parameters on such large data sets without imposing special structures on the covariance or reducing the number of input vectors .",
    "these results are important not only in machine learning , but also in areas where quantification of uncertainty is of primary interest and gps are routinely employed , such as calibration of computer models @xcite and optimization @xcite .",
    "the results reported in this paper , although promising , indicate some directions for improvements .",
    "sgld requires the tuning of a preconditioning matrix @xmath32 .",
    "choosing @xmath32 to be similar to the covariance of the posterior speeds up convergence of sgld when it reaches the langevin dynamics phase .",
    "however , @xmath32 also affects the scaling of the gradient in the proposal . during the first phase of sgld this might not be optimal , and ideally",
    ", gradients should be scaled in a way similar to adagrad @xcite . in @xcite , it was possible to establish a connection between the covariance of the gradients , the fisher information , and @xmath32 due to the fact that stochastic gradients are computed on subsets of the data .",
    "we were unable to do so for gps due to the different way stochasticity is introduced in the computation of the gradients . despite this complication",
    ", we demonstrated that it is still possible to obtain convergence to the posterior distribution over covariance parameters in a reasonable number of iterations , which is of ultimate importance in any inference task .",
    "we are currently investigating the application of sgld to automatic relevance determination covariances and the possibility to extend our proposal to scale inference for other gp models , e.g. , gp classification and gps for spatio - temporal data .",
    "other interesting aspects to explore would be the introduction of mixed precision calculations within the cg algorithm to improve convergence and computation speed as presented , e.g. , in @xcite .",
    "mf gratefully acknowledges support from epsrc grant ep / l020319/1 .",
    "cevahir , a. , nukada , a. , and matsuoka , s. . in allen , g. , nabrzyski , j. , seidel , e. , van albada , g. , dongarra , j. , and sloot , p. ( eds . ) ,",
    "_ computational science ",
    "iccs 2009 _ , volume 5544 of _ lecture notes in computer science _ , pp .",
    "springer berlin heidelberg , 2009 .",
    "filippone , m. bayesian inference for gaussian process classifiers with annealing and pseudo - marginal mcmc . in _ 22nd international conference on pattern recognition , icpr 2014 , stockholm , sweden , august 24 - 28 , 2014 _ , pp .",
    "ieee , 2014 .",
    "liu , k .- f . .",
    "in frommer , a. , lippert , t. , medeke , b. , and schilling , k. ( eds . ) , _ numerical challenges in lattice quantum chromodynamics _ , volume  15 of _ lecture notes in computational science and engineering _ , pp . 142152 .",
    "springer berlin heidelberg , 2000 .",
    "murray , i. aussian processes and fast matrix - vector multiplies , 2009 .",
    "presented at the numerical mathematics in machine learning workshop at the 26th international conference on machine learning ( icml 2009 ) , montreal , canada ."
  ],
  "abstract_text": [
    "<S> in applications of gaussian processes where quantification of uncertainty is of primary interest , it is necessary to accurately characterize the posterior distribution over covariance parameters . </S>",
    "<S> this paper proposes an adaptation of the stochastic gradient langevin dynamics algorithm to draw samples from the posterior distribution over covariance parameters with negligible bias and without the need to compute the marginal likelihood . in gaussian process regression </S>",
    "<S> , this has the enormous advantage that stochastic gradients can be computed by solving linear systems only . </S>",
    "<S> a novel unbiased linear systems solver based on parallelizable covariance matrix - vector products is developed to accelerate the unbiased estimation of gradients . </S>",
    "<S> the results demonstrate the possibility to enable scalable and exact ( in a monte carlo sense ) quantification of uncertainty in gaussian processes without imposing any special structure on the covariance or reducing the number of input vectors . </S>"
  ]
}