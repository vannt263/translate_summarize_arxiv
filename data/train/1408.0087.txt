{
  "article_text": [
    "experts probability assessments are often evaluated on _ calibration _ , which measures how closely the frequency of event occurrence agrees with the assigned probabilities . for instance , consider all events that an expert believes to occur with a 60% probability .",
    "if the expert is well calibrated , 60% of these events will actually end up occurring . even though several experiments have shown that experts are often poorly calibrated [ see , e.g. , @xcite ] , these are noteworthy exceptions . in particular , @xcite argue that higher self - reported expertise can be associated with better calibration .",
    "calibration by itself , however , is not sufficient for useful probability estimation . consider a relatively stationary process , such as rain on different days in a given geographic region , where the observed frequency of occurrence in the last 10 years is 45% . in this setting an expert",
    "could always assign a constant probability of 0.45 and be well - calibrated .",
    "this assessment , however , can be made without any subject - matter expertise .",
    "for this reason the long - term frequency is often considered the baseline probability  a naive assessment that provides the decision - maker very little extra information .",
    "experts should make probability assessments that are as far from the baseline as possible .",
    "the extent to which their probabilities differ from the baseline is measured by _ sharpness _ [ @xcite ] .",
    "if the experts are both sharp and well calibrated , they can forecast the behavior of the process with high certainty and accuracy .",
    "therefore , useful probability estimation should maximize sharpness subject to calibration [ see , e.g. , @xcite ]",
    ".    there is strong empirical evidence that bringing together the strengths of different experts by combining their probability forecasts into a single consensus , known as the _ crowd belief _ , improves predictive performance .",
    "prompted by the many applications of probability forecasts , including medical diagnosis [ @xcite ] , political and socio - economic foresight [ @xcite ] , and meteorology [ @xcite ] , researchers have proposed many approaches to combining probability forecasts [ see , e.g. , @xcite for some recent studies , and @xcite for a comprehensive overview ] .",
    "the general focus , however , has been on developing one - time aggregation procedures that consult the experts advice only once before the event resolves .",
    "consequently , many areas of probability aggregation still remain rather unexplored .",
    "for instance , consider investors aiming to assess whether a stock index will finish trading above a threshold on a given date . to maximize their overall predictive accuracy",
    ", they may consult a group of experts repeatedly over a period of time and adjust their estimate of the aggregate probability accordingly .",
    "given that the experts are allowed to update their probability assessments , the aggregation should be performed by taking into account the temporal correlation in their advice .",
    "this paper adds another layer of complexity by assuming a heterogeneous set of experts , most of whom only make one or two probability assessments over the hundred or so days before the event resolves .",
    "this means that the decision - maker faces a different group of experts every day , with only a few experts returning later on for a second round of advice .",
    "the problem at hand is therefore strikingly different from many time - series estimation problems , where one has an observation at every time point  or almost every time point . as a result ,",
    "standard time - series procedures like arima [ see , e.g. , @xcite ] are not directly applicable .",
    "this paper introduces a time - series model that incorporates self - reported expertise and captures a sharp and well - calibrated estimate of the crowd belief .",
    "the model is highly interpretable and can be used for the following :    * analyzing under and overconfidence in different groups of experts , * obtaining accurate probability forecasts , and * gaining question - specific quantities with easy interpretations , such as expert disagreement and problem difficulty .",
    "this paper begins by describing our geopolitical database .",
    "it then introduces a dynamic hierarchical model for capturing the crowd belief .",
    "the model is estimated in a two - step procedure : first , a sampling step produces constrained parameter estimates via gibbs sampling [ see , e.g. , @xcite ] ; second , a calibration step transforms these estimates to their unconstrained equivalents via a one - dimensional optimization procedure .",
    "the model introduction is followed by the first evaluation section that uses synthetic data to study how accurately the two - step procedure can estimate the crowd belief .",
    "the second evaluation section applies the model to our real - world geopolitical forecasting database .",
    "the paper concludes with a discussion of future research directions and model limitations .",
    "forecasters were recruited from professional societies , research centers , alumni associations , science bloggers and word of mouth ( @xmath0 ) .",
    "requirements included at least a bachelor s degree and completion of psychological and political tests that took roughly two hours .",
    "these measures assessed cognitive styles , cognitive abilities , personality traits , political attitudes and real - world knowledge .",
    "the experts were asked to give probability forecasts ( to the second decimal point ) and to self - assess their level of expertise ( on a 1-to-5 scale with 1@xmath1not at all expert and 5@xmath1extremely expert ) on a number of 166 geopolitical binary events taking place between september 29 , 2011 and may 8 , 2013 .",
    "each question was active for a period during which the participating experts could update their forecasts as frequently as they liked without penalty .",
    "the experts knew that their probability estimates would be assessed for accuracy using brier scores .",
    "this incentivized them to report their true beliefs instead of attempting to game the system [ @xcite ] .",
    "in addition to receiving $ 150 for meeting minimum participation requirements that did not depend on prediction accuracy , the experts received status rewards for their performance via leader - boards displaying brier scores for the top 20 experts .",
    "given that a typical expert participated only in a small subset of the 166 questions , the experts are considered indistinguishable conditional on the level of self - reported expertise .",
    "the average number of forecasts made by a single expert in one day was around 0.017 , and the average group - level response rate was around 13.5 forecasts per day . given that the group of experts is large and diverse , the resulting data set is very sparse .",
    "tables  [ datastats ] and [ expertisetable ] provide relevant summary statistics on the data .",
    "notice that the distribution of the self - reported expertise is skewed to the right and that some questions remained active longer than others . for more details on the data set and its collection",
    "see @xcite .",
    "@ld3.0d3.1d3.1d3.1 d3.2d4.0@ & & & & & & + # of days a question is active & 4 & 35.6 & 72.0 & 106.3 & 145.20 & 418 + # of experts per question & 212 & 543.2 & 693.5 & 783.7 & 983.2 & 1690 + # forecasts given by each expert on a question & 1 & 1.0 & 1.0 & 1.8 & 2.0 & 131 + # questions participated by an expert & 1 & 14.0 & 36.0 & 55.0 & 90.0 & 166 +    @ld2.1d2.1d2.1d1.1d1.1@ expertise level & 1 & 2 & 3 & 4 & 5 + frequency ( % ) & 25.3 & 30.7 & 33.6 & 8.2 & 2.1 +        to illustrate the data with some concrete examples , figure  [ exampleplotsfinal](a ) and [ exampleplotsfinal](b ) show scatterplots of the probability forecasts given for ( a ) _ will the expansion of the european bailout fund be ratified by all 17 eurozone nations before 1 november 2011 ?",
    "_ and ( b ) _ will the nikkei 225 index finish trading at or above 9500 on 30 september 2011 ? _ the points have been shaded according to the level of self - reported expertise and jittered slightly to make overlaps visible .",
    "the solid line gives the posterior mean of the calibrated crowd belief as estimated by our model .",
    "the surrounding dashed lines connect the point - wise 95% posterior intervals .",
    "given that the european bailout fund was ratified before november 1 , 2011 and that the nikkei 225 index finished trading at around 8700 on september 30 , 2011 , the general trend of the probability forecasts tends to converge toward the correct answers .",
    "the individual experts , however , sometimes disagree strongly , with the disagreement persisting even near the closing dates of the questions .",
    "let @xmath2 be the probability forecast given by the @xmath3th expert at time @xmath4 for the @xmath5th question , where @xmath6 , @xmath7 , and @xmath8 .",
    "denote the logit probabilities with @xmath9 and collect the logit probabilities for question @xmath5 at time @xmath4 into a vector @xmath10^t$ ] .",
    "partition the experts into @xmath11 groups based on some individual feature , such as self - reported expertise , with each group sharing a common multiplicative bias term @xmath12 for @xmath13 .",
    "collect these bias terms into a bias vector @xmath14^t$ ] .",
    "let @xmath15 be a @xmath16 matrix denoting the group memberships of the experts in question @xmath5 ; that is , if the @xmath3th expert participating in the @xmath5th question belongs to the @xmath17th group , then the @xmath3th row of @xmath18 is the @xmath17th standard basis vector @xmath19 .",
    "the bias vector @xmath20 is assumed to be identical across all @xmath21 questions . under this notation",
    ", the model for the @xmath5th question can be expressed as @xmath22 where ( [ observedpr ] ) denotes the observed process , ( [ hiddenpr ] ) shows the hidden process that is driven by the constant @xmath23 , and @xmath24 are hyperparameters fixed a priori to 0 and 1 , respectively .",
    "the error terms follow : @xmath25 therefore , the parameters of the model are @xmath20 , @xmath26 , @xmath27 and @xmath28 for @xmath8 .",
    "their prior distributions are chosen to be noninformative , @xmath29 and @xmath30 .",
    "the hidden state @xmath31 represents the aggregate logit probability for the @xmath5th event given all the information available up to and including time @xmath4 . to make this more specific , let @xmath32 indicate whether the event associated with the @xmath5th question happened @xmath33 or did not happen @xmath34 . if @xmath35 is a filtration representing the information available up to and including a given time point , then according to our model @xmath36 = { \\mathbb{p}}(z_k = 1| \\mathcal{f}_{t , k } ) = { \\operatorname{logit}}^{-1}(x_{t , k})$ ] .",
    "ideally this probability maximizes sharpness subject to calibration [ for technical definitions of calibration and sharpness see @xcite ] . even though a single expert is unlikely to have access to all the available information",
    ", a large and diverse group of experts may share a considerable portion of the available information .",
    "the collective wisdom of the group therefore provides an attractive proxy for @xmath37 .",
    "given that the experts may believe in false information , hide their true beliefs or be biased for many other reasons , their probability assessments should be aggregated via a model that can detect potential bias , separate signal from noise and use the collective opinion to estimate @xmath31 . in our model",
    "the experts are assumed to be , on average , a multiplicative constant @xmath20 away from @xmath31 .",
    "therefore , an individual element of @xmath20 can be interpreted as a group - specific _ systematic bias _ that labels the group either as overconfident [ @xmath38 or as underconfident [ @xmath39 .",
    "see section  [ model ] for a brief discussion on different bias structures .",
    "any other deviation from @xmath31 is considered _",
    "random noise_. this noise is measured in terms of @xmath40 and can be assumed to be caused by momentary over - optimism ( or pessimism ) , false beliefs or other misconceptions .",
    "the _ random fluctuations _ in the hidden process are measured by @xmath28 and are assumed to represent changes or shocks to the underlying circumstances that ultimately decide the outcome of the event .",
    "the _ systematic component _",
    "@xmath27 allows the model to incorporate a constant signal stream that drifts the hidden process .",
    "if the uncertainty in the question diminishes [ @xmath41 , the hidden process drifts to positive or negative infinity .",
    "alternatively , the hidden process can drift to zero , in which case any available information does not improve predictive accuracy [ @xmath42 .",
    "given that all the questions in our data set were resolved within a prespecified timeframe , we expect @xmath43 for all @xmath8 .    as",
    "for any future time @xmath44 , @xmath45 the model can be used for time - forward prediction as well .",
    "the prediction for the aggregate logit probability at time @xmath46 is given by an estimate of @xmath47 .",
    "naturally the uncertainty in this prediction grows in @xmath48 . to make such time - forward predictions , it is necessary to assume that the past population of experts is representative of the future population .",
    "this is a reasonable assumption because even though the future population may consist of entirely different individuals , on average the population is likely to look very similar to the past population . in practice , however , social scientists are generally more interested in an estimate of the current probability than the probability under unknown conditions in the future .",
    "for this reason , our analysis focuses on probability aggregation only up to the current time @xmath4 .    for the sake of model identifiability , it is sufficient to share only one of the elements of @xmath20 among the @xmath21 questions .",
    "in this paper , however , all the elements of @xmath20 are assumed to be identical across the questions because some of the questions in our real - world data set involve very few experts with the highest level of self - reported expertise .",
    "the model can be extended rather easily to estimate bias at a more general level .",
    "for instance , by assuming a hierarchical structure @xmath49 , where @xmath50 denotes the self - reported expertise of the @xmath3th expert in question @xmath5 , the bias can be estimated at an individual - level .",
    "these estimates can then be compared across questions .",
    "individual - level analysis was not performed in our analysis for two reasons .",
    "first , most experts gave only a single prediction per problem , which makes accurate bias estimation at the individual - level very difficult .",
    "second , it is unclear how the individually estimated bias terms can be validated .",
    "if the future event can take upon @xmath51 possible outcomes , the hidden state @xmath31 is extended to a vector of size @xmath52 and one of the outcomes , for example , the @xmath53th one , is chosen as the base case to ensure that the probabilities will sum to one at any given time point .",
    "each of the remaining @xmath52 possible outcomes is represented by an observed process similar to ( [ observedpr ] ) . given that this multinomial extension is equivalent to having @xmath52 independent binary - outcome models , the estimation and properties of the model are easily extended to the multi - outcome case .",
    "this paper focuses on binary outcomes because it is the most commonly encountered setting in practice .",
    "this section introduces a two - step procedure , called _ sample - and - calibrate _",
    "( sac ) , that captures a well - calibrated estimate of the hidden process without sacrificing the interpretability of our model .      given that @xmath54 for any yield the same likelihood for @xmath55 , the model as described by ( [ observedpr ] ) and ( [ hiddenpr ] ) is not identifiable .",
    "a well - known solution is to choose one of the elements of @xmath20 , say , @xmath56 , as the reference point and fix @xmath57 . in section  [ syntheticdata ] we provide a guideline for choosing the reference point .",
    "denote the constrained version of the model by @xmath58 where the trailing input notation , ( a ) , signifies the value under the constraint @xmath59 .",
    "given that this version is identifiable , estimates of the model parameters can be obtained .",
    "denote the estimates by placing a hat on the parameter symbol .",
    "for instance , @xmath60 and @xmath61 represent the estimates of @xmath62 and @xmath63 , respectively .",
    "these estimates are obtained by first computing a posterior sample via gibbs sampling and then taking the average of the posterior sample .",
    "the first step of our gibbs sampler is to sample the hidden states via the _ forward - filtering - backward - sampling _ ( ffbs ) algorithm .",
    "ffbs first predicts the hidden states using a kalman filter and then performs a backward sampling procedure that treats these predicted states as additional observations [ see , e.g. , @xcite for details on ffbs ] .",
    "given that the kalman filter can handle varying numbers or even no forecasts at different time points , it plays a very crucial role in our probability aggregation under sparse data . our implementation of the sampling step is written in c@xmath64 and runs quite quickly . to obtain 1000 posterior samples for 50 questions each with 100 time points and",
    "50 experts takes about 215 seconds on a 1.7 ghz intel core i5 computer .",
    "see the supplemental article for the technical details of the sampling steps [ @xcite ] and , for example , @xcite for a discussion on the general principles of gibbs sampling .      given that the model parameters can be estimated by fixing @xmath56 to any constant , the next step is to search for the constant that gives an optimally sharp and calibrated estimate of the hidden process .",
    "this section introduces an efficient procedure that finds the optimal constant without requiring any additional runs of the sampling step .",
    "first , assume that parameter estimates @xmath65 and @xmath61 have already been obtained via the sampling step described in section  [ sampling_step ] .",
    "given that for any @xmath66 , @xmath67 we have that @xmath68 and @xmath69 .",
    "recall that the hidden process @xmath70 is assumed to be sharp and well calibrated .",
    "therefore , @xmath56 can be estimated with the value of @xmath71 that simultaneously maximizes the sharpness and calibration of @xmath72 . a natural criterion for this maximization",
    "is given by the class of _ proper scoring rules _ that combine sharpness and calibration [ @xcite ] .",
    "due to the possibility of _ complete separation _ in any one question [ see , e.g. , @xcite ] , the maximization must be performed over multiple questions .",
    "therefore , @xmath73 where @xmath32 is the event indicator for question @xmath5 .",
    "the function @xmath74 is a strictly proper scoring rule such as the negative brier score [ @xcite ] @xmath75 or the logarithmic score [ @xcite ] @xmath76 the estimates of the unconstrained model parameters are then given by @xmath77 notice that estimates of @xmath40 and @xmath27 are not affected by the constraint .",
    "this section uses synthetic data to evaluate how accurately the sac - procedure captures the hidden states and bias vector .",
    "the hidden process is generated from standard brownian motion .",
    "more specifically , if @xmath78 denotes the value of a path at time @xmath4 , then @xmath79\\end{aligned}\\ ] ] gives a sequence of @xmath80 calibrated logit probabilities for the event @xmath81 .",
    "a hidden process is generated for @xmath21 questions with a time horizon of @xmath82 .",
    "the questions involve 50 experts allocated evenly among five expertise groups .",
    "each expert gives one probability forecast per day with the exception of time @xmath83 when the event resolves .",
    "the forecasts are generated by applying bias and noise to the hidden process as described by ( [ observedpr ] ) .",
    "our simulation study considers a three - dimensional grid of parameter values : @xmath84 where @xmath71 varies the bias vector by @xmath85^t \\beta$ ] .",
    "forty synthetic data sets are generated for each combination of @xmath86 , @xmath71 and @xmath21 values .",
    "the sac - procedure runs for 200 iterations of which the first 100 are used for burn - in .",
    "sac under the brier ( @xmath87 ) and logarithm score ( @xmath88 ) are compared with the _ exponentially weighted moving average _ ( ewma ) .",
    "ewma , which serves as a baseline , can be understood by first denoting the ( expertise - weighted ) average forecast at time @xmath4 for the @xmath5th question with @xmath89 where @xmath90 refers to an index set of all experts in the @xmath17th expertise group and @xmath91 denotes the weight associated with the @xmath17th expertise group .",
    "the ewma forecasts for the @xmath5th problem are then constructed recursively from @xmath92 where @xmath93 and @xmath94 are learned from the training set by @xmath95 } \\sum _ { k = 1}^k \\sum_{t=1}^{t_k } \\bigl(z_k - \\hat { p}_{t , k}(\\alpha , \\bolds{\\omega } ) \\bigr)^2   \\qquad\\mbox{s.t . }",
    "\\sum _ { j=1}^j \\omega_j = 1.\\ ] ]    if @xmath96 and @xmath97 is the corresponding probability estimated by the model , the model s accuracy to estimate the hidden process is measured with the quadratic loss , @xmath98 , and the absolute loss , @xmath99 .",
    "table  [ lossessynth ] reports these losses averaged over all conditions , simulations and time points .",
    "the three competing methods , @xmath87 , @xmath100 and ewma , estimate the hidden process with great accuracy . based on other performance measures that are not shown for",
    "the sake of brevity , all three methods suffer from an increasing level of noise in the expert logit probabilities but can make efficient use of extra data .",
    "250pt@lcc@ * model * & * quadratic loss * & * absolute loss * +   + @xmath87 & 0.00226 & 0.0334 + @xmath88 & 0.00200 & 0.0313 + ewma & 0.00225 & 0.0339 +   + @xmath87 & 0.147 & 0.217 + @xmath88 & 0.077 & 0.171 +     on the average quadratic loss . ]",
    "some interesting differences emerge from figure  [ synthetic ] which shows the marginal effect of @xmath71 on the average quadratic loss .",
    "as can be expected , ewma performs well when the experts are , on average , close to unbiased .",
    "interestingly , sac estimates the hidden process more accurately when the experts are overconfident ( large @xmath71 ) compared to underconfident ( small @xmath71 ) . to understand this result ,",
    "assume that the experts in the third group are highly underconfident .",
    "their logit probabilities are then expected to be closer to zero than the corresponding hidden states .",
    "after adding white noise to these expected logit probabilities , they are likely to cross to the other side of zero . if the sampling step fixes @xmath101 , as it does in our case , the third group is treated as unbiased and some of the constrained estimates of the hidden states are likely to be on the other side of zero as well .",
    "unfortunately , this discrepancy can not be corrected by the calibration step that is restricted to shifting the constrained estimates either closer or further away from zero but not across it . to maximize the likelihood of having all the constrained estimates on the right side of zero and hence avoiding the discrepancy , the reference point in the sampling step",
    "should be chosen with care .",
    "a helpful guideline is to fix the element of @xmath20 that is a priori believed to be the largest .",
    "the accuracy of the estimated bias vector is measured with the quadratic loss , @xmath102 , and the absolute loss , @xmath103 .",
    "table  [ lossessynth ] reports these losses averaged over all conditions , simulations and elements of the bias vector .",
    "unfortunately , ewma does not produce an estimate of the bias vector .",
    "therefore , it can not be used as a baseline for the estimation accuracy in this case .",
    "given that the losses for @xmath87 and @xmath88 are quite small , they estimate the bias vector accurately .",
    "this section presents results for the real - world data described in section  [ data ] .",
    "the goal is to provide application specific insight by discussing the specific research objectives itemized in section  [ intro ] .",
    "first , however , we discuss two practical matters that must be taken into account when aggregating real - world probability forecasts .",
    "the first matter regards human experts making probability forecasts of 0.0 or 1.0 even if they are not completely sure of the outcome of the event .",
    "for instance , all 166 questions in our data set contain both a zero and a one .",
    "transforming such forecasts into the logit space yields infinities that can cause problems in model estimation . to avoid this ,",
    "@xcite suggest changing @xmath104 and @xmath105 to @xmath106 and @xmath107 , respectively .",
    "this is similar to _ winsorising _ that sets the extreme probabilities to a specified percentile of the data [ see , e.g. , @xcite for more details on winsorising ] .",
    "@xcite , on the other hand , consider only probabilities that fall within a constrained interval , say , @xmath108 $ ] , and discard the rest . given that this implies ignoring a portion of the data , we adopt a censoring approach similar to @xcite by changing @xmath104 and @xmath105 to @xmath109 and @xmath110 , respectively .",
    "our results remain insensitive to the exact choice of censoring as long as this is done in a reasonable manner to keep the extreme probabilities from becoming highly influential in the logit space .",
    "the second matter is related to the distribution of the class labels in the data . if the set of occurrences is much larger than the set of nonoccurrences ( or vice versa )",
    ", the data set is called _",
    "imbalanced_. on such data the modeling procedure can end up over - focusing on the larger class and , as a result , give very accurate forecast performance over the larger class at the cost of performing poorly over the smaller class [ see , e.g. , @xcite ] . fortunately , it is often possible to use a well - balanced version of the data .",
    "the first step is to find a partition @xmath111 and @xmath112 of the question indices @xmath113 such that the equality @xmath114 is as closely approximated as possible .",
    "this is equivalent to an np - hard problem known in computer science as the _ partition problem _ : determine whether a given set of positive integers can be partitioned into two sets such that the sums of the two sets are equal to each other [ see , e.g. , @xcite ] .",
    "a simple solution is to use a greedy algorithm that iterates through the values of @xmath80 in descending order , assigning each @xmath80 to the subset that currently has the smaller sum [ see , e.g. , @xcite for more details on the _ partition problem _ ] . after finding a well - balanced partition ,",
    "the next step is to assign the class labels such that the labels for the questions in @xmath115 are equal to @xmath116 for @xmath117 or @xmath118 .",
    "recall from section  [ calibration_step ] that @xmath119 represents the event indicator for the @xmath5th question . to define a balanced set of indicators @xmath120 for all @xmath121 , let @xmath122 where @xmath123 , and @xmath7 .",
    "the resulting set @xmath124 is a balanced version of the data .",
    "this procedure was used to balance our real - world data set both in terms of events and time points .",
    "the final output splits the events exactly in half ( @xmath125 ) such that the number of time points in the first and second halves are 8737 and 8738 , respectively .",
    "the goal of this section is to evaluate the accuracy of the aggregate probabilities made by sac and several other procedures .",
    "the models are allowed to utilize a training set before making aggregations on an independent testing set . to clarify some of the upcoming notation , let @xmath126 and @xmath127 be index sets that partition the data into training and testing sets of sizes @xmath128 and @xmath129 , respectively .",
    "this means that the @xmath5th question is in the training set if and only if @xmath130 . before introducing the competing models , note that all choices of thinning and burn - in made in this section are conservative and have been made based on pilot runs of the models .",
    "this was done to ensure a posterior sample that has low autocorrelation and arises from a converged chain .",
    "the competing models are as follows :    1 .",
    "_ simple dynamic linear model _ ( sdlm ) .",
    "this is equivalent to the dynamic model from section  [ model ] but with @xmath131 and @xmath132 .",
    "thus , @xmath133 where @xmath31 is the aggregate logit probability . given that this model does not share any parameters across questions , estimates of the hidden process can be obtained directly for the questions in the testing set without fitting the model first on the training set .",
    "the gibbs sampler is run for 500 iterations of which the first 200 are used for burn - in .",
    "the remaining 300 iterations are thinned by discarding every other observation , leaving a final posterior sample of 150 observations .",
    "the average of this sample gives the final estimates .",
    "2 .   _ the sample - and - calibrate procedure both under the brier _ ( @xmath87 ) _ and the logarithmic score _ ( @xmath100 ) .",
    "the model is first fit on the training set by running the sampling step for 3000 iterations of which the first 500 iterations are used for burn - in .",
    "the remaining 2500 observations are thinned by keeping every fifth observation .",
    "the calibration step is performed for the final 500 observations .",
    "the out - of - sample aggregation is done by running the sampling step for 500 iterations with each consecutive iteration reading in and conditioning on the next value of @xmath71 and @xmath20 found during the training period .",
    "the first 200 iterations are used for burn - in .",
    "the remaining 300 iterations are thinned by discarding every other observation , leaving a final posterior sample of 150 observations .",
    "the average of this sample gives the final estimates .",
    "_ a fully bayesian version of _ @xmath134 ( @xmath135 ) .",
    "denote the calibrated logit probabilities and event indicators across all @xmath21 questions with @xmath136 and @xmath137 , respectively .",
    "the posterior distribution of @xmath71 conditional on @xmath136 is given by @xmath138 .",
    "the likelihood is @xmath139 \\\\[-8pt ] \\nonumber & & \\qquad \\propto\\prod _ { k=1}^k \\prod_{t=1}^{t_k } { \\operatorname{logit}}^{-1 } \\bigl(x_{t , k}(1)/\\beta \\bigr)^{z_k } \\bigl ( 1- { \\operatorname{logit}}^{-1 } \\bigl ( x_{t , k}(1)/\\beta \\bigr ) \\bigr)^{1-z_k}.\\end{aligned}\\ ] ] as in @xcite , the prior for @xmath71 is chosen to be locally uniform , @xmath140 . given that this model estimates @xmath141 and @xmath71 simultaneously , it is a little more flexible than @xmath142 .",
    "posterior estimates of @xmath71 can be sampled from ( [ ose2 ] ) using generic sampling algorithms such as the metropolis algorithm [ @xcite ] or slice sampling [ @xcite ] .",
    "given that the sampling procedure conditions on the event indicators , the full conditional distribution of the hidden states is not in a standard form .",
    "therefore , the metropolis algorithm is also used for sampling the hidden states .",
    "estimation is made with the same choices of thinning and burn - in as described under _ sample - and - calibrate_. 4 .",
    "due to the lack of previous literature on dynamic aggregation of expert probability forecasts , the main competitors are exponentially weighted versions of procedures that have been proposed for static probability aggregation : a.   _ exponentially weighted moving average _ ( ewma ) as described in section  [ syntheticdata ] .",
    "b.   _ exponentially weighted moving logit aggregator _ ( ewmla ) .",
    "this is a moving version of the aggregator @xmath143 that was introduced in @xcite .",
    "the ewmla aggregate probabilities are found recursively from @xmath144 where the vector @xmath145 collects the bias terms of the expertise groups , and @xmath146 the parameters @xmath93 and @xmath20 are learned from the training set by @xmath147 } \\sum _ { k \\in s_{\\mathrm{train } } } \\sum_{t=1}^{t_k } \\bigl(z_k - \\hat{p}_{t , k}(\\alpha , \\mathbf { b } ) \\bigr)^2.\\ ] ] c.   _ exponentially weighted moving beta - transformed aggregator(ewmba)_. the static version of the beta - transformed aggregator was introduced in @xcite .",
    "a dynamic version can be obtained by replacing @xmath148 in the ewmla description with @xmath149 , where @xmath150 is the cumulative distribution function of the beta distribution and @xmath151 is given by ( [ weighted mean ] ) .",
    "the parameters @xmath152 and @xmath94 are learned from the training set by @xmath153 } \\sum_{k \\in s_{\\mathrm{train } } } \\sum _ { t=1}^{t_k } \\bigl(z_k - \\hat{p}_{t , k}(\\alpha , \\nu , \\tau , \\bolds{\\omega } ) \\bigr)^2 \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\eqntext{\\displaystyle\\mbox{s.t . }",
    "\\sum_{j=1}^j \\omega_j = 1.}\\end{aligned}\\ ] ]    the competing models are evaluated via a 10-fold cross - validation that first partitions the 166 questions into 10 sets such that each set has approximately the same number of questions ( 16 or 17 questions in our case ) and the same number of time points ( between 1760 and 1764 time points in our case ) .",
    "the evaluation then iterates 10 times , each time using one of the 10 sets as the testing set and the remaining 9 sets as the training set .",
    "therefore , each question is used nine times for training and exactly once for testing",
    ". the testing proceeds sequentially one testing question at a time as follows : first , for a question with a time horizon of @xmath80 , give an aggregate probability at time @xmath154 based on the first two days .",
    "compute the brier score for this probability .",
    "next give an aggregate probability at time @xmath155 based on the first three days and compute the brier score for this probability .",
    "repeat this process for all of the @xmath156 days .",
    "this leads to @xmath156 brier scores per testing question and a total of 17,475 brier scores across the entire data set .    @lcccc@",
    "* model * & & & & +   + sdlm & 0.100 ( 0.156 ) & 0.066 ( 0.116 ) & 0.098 ( 0.154 ) & 0.102 ( 0.157 ) + @xmath135 & 0.097 ( 0.213 ) & * 0.053 * ( 0.147 ) & 0.100 ( 0.215 ) & 0.098 ( 0.215 ) +",
    "@xmath87 & 0.096 ( 0.190 ) & 0.056 ( 0.134 ) & 0.097 ( 0.190 ) & 0.098 ( 0.192 ) + @xmath88 & * 0.096 * ( 0.191 ) & 0.056 ( 0.134 ) & * 0.096 * ( 0.189 ) & * 0.098 * ( 0.193 ) + ewmba & 0.104 ( 0.204 ) & 0.057 ( 0.120 ) & 0.113 ( 0.205 ) & 0.105 ( 0.206 ) + ewmla & 0.102 ( 0.199 ) & 0.061 ( 0.130 ) & 0.111 ( 0.214 ) & 0.103 ( 0.200 ) + ewma & 0.111 ( 0.146 ) & 0.080 ( 0.101 ) & 0.116 ( 0.152 ) & 0.112 ( 0.146 ) +   + sdlm & 0.089 ( 0.116 ) & 0.064 ( 0.085 ) &",
    "0.106 ( 0.141 ) & 0.092 ( 0.117 ) + @xmath135 & 0.083 ( 0.160 ) & * 0.052 * ( 0.103 ) & 0.110 ( 0.198 ) & 0.085 ( 0.162 ) + @xmath87 & 0.083 ( 0.142 ) & 0.055 ( 0.096 ) & 0.106 ( 0.174 ) & 0.085 ( 0.144 ) + @xmath88 & * 0.082 * ( 0.142 ) & 0.055 ( 0.096 ) & * 0.105 * ( 0.174 ) & * 0.085 * ( 0.144 ) + ewmba & 0.091 ( 0.157 ) & 0.057 ( 0.095 ) & 0.121 ( 0.187 ) & 0.093 ( 0.164 ) + ewmla & 0.090 ( 0.159 ) & 0.064 ( 0.109 ) & 0.120 ( 0.200 ) & 0.090 ( 0.159 ) + ewma & 0.102 ( 0.108 ) & 0.080 ( 0.075 ) & 0.123 ( 0.130 ) & 0.103 ( 0.110 ) +    table  [ prediction ] summarizes these scores in different ways .",
    "the first option , denoted by _",
    "scores by day _ , weighs each question by the number of days the question remained open .",
    "this is performed by computing the average of the 17,475 scores .",
    "the second option , denoted by _ scores by problem _ , gives each question an equal weight regardless of how long the question remained open .",
    "this is done by first averaging the scores within a question and then averaging the average scores across all the questions .",
    "both scores can be further broken down into subcategories by considering the length of the questions .",
    "the final three columns of table  [ prediction ] divide the questions into _ short _ questions ( 30 days or fewer ) , _ medium _ questions ( between 31 and 59 days ) and _ long _ problems ( 60 days or more ) .",
    "the number of questions in these subcategories were 36 , 32 and 98 , respectively .",
    "the bolded scores indicate the best score in each column .",
    "the values in the parenthesis quantify the variability in the scores : under _ scores by day _ the values give the standard errors of all the scores . under _",
    "scores by problem _ , on the other hand , the values represent the standard errors of the average scores of the different questions .",
    "as can be seen in table  [ prediction ] , @xmath134 achieves the lowest score across all columns except _ short _ where it is outperformed by @xmath135 .",
    "it turns out that @xmath135 is overconfident ( see section  [ calibration ] ) .",
    "this means that @xmath157 underestimates the uncertainty in the events and outputs aggregate probabilities that are typically too near 0.0 or 1.0 .",
    "this results into highly variable performance .",
    "the short questions generally involved very little uncertainty . on such easy questions",
    ", overconfidence can pay off frequently enough to compensate for a few large losses arising from the overconfident and drastically incorrect forecasts .",
    "sdlm , on the other hand , lacks sharpness and is highly underconfident ( see section  [ calibration ] ) .",
    "this behavior is expected , as the experts are underconfident at the group level ( see section  [ expertbias ] ) and sdlm does not use the training set to explicitly calibrate its aggregate probabilities .",
    "instead , it merely smooths the forecasts given by the experts .",
    "the resulting aggregate probabilities are therefore necessarily conservative , resulting into high average scores with low variability .    similar behavior is exhibited by ewma that performs the worst of all the competing models .",
    "the other two exponentially weighted aggregators , ewmla and ewmba , make efficient use of the training set and present moderate forecasting performance in most columns of table  [ prediction ] .",
    "neither approach , however , appears to dominate the other .",
    "the high variability and average of their performance scores indicate that their performance suffers from overconfidence .",
    "a calibration plot is a simple tool for visually assessing the sharpness and calibration of a model .",
    "the idea is to plot the aggregate probabilities against the observed empirical frequencies .",
    "therefore , any deviation from the diagonal line suggests poor calibration",
    ". a model is considered underconfident ( or overconfident ) if the points follow an s - shaped ( or -shaped ) trend . to assess sharpness of the model ,",
    "it is common practice to place a histogram of the given forecasts in the corner of the plot . given that the data were balanced , any deviation from the the baseline probability of 0.5 suggests improved sharpness .",
    "the top and bottom rows of figure  [ calibration - out ] present calibration plots for sdlm , @xmath88 , @xmath158 and @xmath135 under in- and out - of - sample probability aggregation , respectively .",
    "each setting is of interest in its own right : good in - sample calibration is crucial for model interpretability . in particular ,",
    "if the estimated crowd belief is well calibrated , then the elements of the bias vector @xmath20 can be used to study the amount of under or overconfidence in the different expertise groups .",
    "good out - of - sample calibration and sharpness , on the other hand , are necessary properties in decision making . to guide our assessment , the dashed bands around the diagonal connect the point - wise , bonferroni - corrected [ @xcite ] 95% lower and upper critical values under the null hypothesis of calibration .",
    "these have been computed by running the bootstrap technique described in @xcite for 10,000 iterations .",
    "the in - sample predictions were obtained by running the models for 10,200 iterations , leading to a final posterior sample of 1000 observations after thinning and using the first 200 iterations for burn - in .",
    "the out - of - sample predictions were given by the 10-fold cross - validation discussed in section  [ forecasting ] .",
    "overall , sac is sharp and well calibrated both in- and out - of - sample with only a few points barely falling outside the _ point - wise _ critical values . given that the calibration does not change drastically from the top to the bottom row , sac can be considered robust against overfitting .",
    "this , however , is not the case with @xmath157 that is well calibrated in - sample but presents overconfidence out - of - sample . figure  [ calibration - out](a ) and ( e ) serve as baselines by showing the calibration plots for sdlm .",
    "given that this model does not perform any explicit calibration , it is not surprising to see most points outside the critical values .",
    "the pattern in the deviations suggests strong underconfidence .",
    "furthermore , the inset histogram reveals drastic lack of sharpness .",
    "therefore , sac can be viewed as a well - performing compromise between sdlm and @xmath135 that avoids overconfidence without being too conservative .",
    "this section explores the bias among the five expertise groups in our data set .",
    "figure  [ biases ] compares the posterior distributions of the individual elements of @xmath20 with side - by - side boxplots .",
    "given that the distributions fall completely below the _ no - bias _ reference line at 1.0 , all the expertise groups are deemed underconfident . even though the exact level of underconfidence is affected slightly by the extent to which the extreme probabilities are censored ( see section  [ practicalmatters ] ) , the qualitative results in this section remain insensitive to different levels of censoring .     for @xmath159 . ]",
    "figure  [ biases ] shows that underconfidence decreases as expertise increases . the posterior probability that the most expert group is the least underconfident is approximately equal to @xmath160 , and the posterior probability of a strictly decreasing level of underconfidence is approximately 0.87 .",
    "the latter probability is driven down by the inseparability of the two groups with the lowest levels of self - reported expertise .",
    "this inseparability suggests that the experts are poor at assessing how little they know about a topic that is strange to them .",
    "if these groups are combined into a single group , the posterior probability of a strictly decreasing level of underconfidence is approximately  1.0.=1    the decreasing trend in underconfidence can be viewed as a process of bayesian updating .",
    "a completely ignorant expert aiming to minimize a reasonable loss function , such as the brier score , has no reason to give anything but 0.5 as his probability forecast .",
    "however , as soon as the expert gains some knowledge about the event , he produces an updated forecast that is a compromise between his initial forecast and the new information acquired .",
    "the updated forecast is therefore conservative and too close to 0.5 as long as the expert remains only partially informed about the event .",
    "if most experts fall somewhere on this spectrum between ignorance and full information , their average forecast tends to fall strictly between 0.5 and the most informed probability forecast [ see @xcite for more details ] .",
    "given that expertise is to a large extent determined by subject matter knowledge , the level of underconfidence can be expected to decrease as a function of the group s level of self - reported expertise .",
    "finding underconfidence in all the groups may seem like a surprising result given that many previous studies have shown that experts are often overconfident [ see , e.g. , @xcite for a summary of numerous calibration studies ] .",
    "it is , however , worth emphasizing three points : first , our result is a statement about groups of experts and hence does not invalidate the possibility of the individual experts being overconfident . to make conclusions at the individual level based on the group level bias terms",
    "would be considered an _ ecological inference fallacy _ [ see , e.g. , @xcite ] .",
    "second , the experts involved in our data set are overall very well calibrated [ @xcite ] .",
    "a group of well - calibrated experts , however , can produce an aggregate forecast that is underconfident .",
    "in fact , if the aggregate is linear , the group is necessarily underconfident [ see theorem  1 of @xcite ] .",
    "third , according to @xcite , the level of confidence depends on the way the data were analyzed .",
    "they explain that experts probability forecasts suggest underconfidence when the forecasts are averaged or presented as a function of independently defined objective probabilities , that is , the probabilities given by @xmath161 in our case .",
    "this is similar to our context and opposite to many empirical studies on confidence calibration .",
    "one advantage of our model arises from its ability to produce estimates of interpretable question - specific parameters @xmath27 , @xmath26 and @xmath28 .",
    "these quantities can be combined in many interesting ways to answer questions about different groups of experts or the questions themselves .",
    "for instance , being able to assess the difficulty of a question could lead to more principled ways of aggregating performance measures across questions or to novel insight on the kinds of questions that are found difficult by experts [ see , e.g. , a discussion on the _ hard - easy effect _ in @xcite ] . to illustrate , recall that higher values of @xmath40 suggest greater disagreement among the participating experts . given that experts are more likely to disagree over a difficult question than an easy one , it is reasonable to assume that @xmath40 has a positive relationship with question difficulty .",
    "an alternative measure is given by @xmath162 that quantifies the volatility of the underlying circumstances that ultimately decide the outcome of the event .",
    "therefore , a high value of @xmath162 can cause the outcome of the event to appear unstable and difficult to predict .    as a final illustration of our model",
    ", we return to the two example questions introduced in figure  [ exampleplotsfinal ] . given that @xmath163 and @xmath164 for the questions depicted in figure  [ exampleplotsfinal](a ) and [ exampleplotsfinal](b ) , respectively , the first question provokes more disagreement among the experts than the second one .",
    "intuitively this makes sense because the target event in figure  [ exampleplotsfinal](a ) is determined by several conditions that may change radically from one day to the next while the target event in figure  [ exampleplotsfinal](b ) is determined by a relatively steady stock market index .",
    "therefore , it is not surprising to find that in figure  [ exampleplotsfinal](a ) @xmath165 , which is much higher than @xmath166 in figure  [ exampleplotsfinal](b ) .",
    "we may conclude that the first question is inherently more difficult than the second one .",
    "this paper began by introducing a rather unorthodox but nonetheless realistic time - series setting where probability forecasts are made very infrequently by a heterogeneous group of experts .",
    "the resulting data is too sparse to be modeled well with standard time - series methods . in response to this lack of appropriate modeling procedures",
    ", we propose an interpretable time - series model that incorporates self - reported expertise to capture a sharp and well - calibrated estimate of the crowd belief .",
    "this procedure extends the forecasting literature into an under - explored area of probability aggregation .",
    "our model preserves parsimony while addressing the main challenges in modeling sparse probability forecasting data .",
    "therefore , it can be viewed as a basis for many future extensions . to give some ideas , recall that most of the model parameters were assumed constant over time .",
    "it is intuitively reasonable , however , that these parameters behave differently during different time intervals of the question .",
    "for instance , the level of disagreement ( represented by @xmath40 in our model ) among the experts can be expected to decrease toward the final time point when the question resolves .",
    "this hypothesis could be explored by letting @xmath167 evolve dynamically as a function of the previous term @xmath168 and random noise .",
    "this paper modeled the bias separately within each expertise group .",
    "this is by no means restricted to the study of bias or its relation to self - reported expertise .",
    "different parameter dependencies could be constructed based on many other expert characteristics , such as gender , education or specialty , to produce a range of novel insights on the forecasting behavior of experts",
    ". it would also be useful to know how expert characteristics interact with question types , such as economic , domestic or international .",
    "the results would be of interest to the decision - maker who could use the information as a basis for hiring only a high - performing subset of the available experts .",
    "other future directions could remove some of the obvious limitations of our model .",
    "for instance , recall that the random components are assumed to follow a normal distribution .",
    "this is a strong assumption that may not always be justified .",
    "logit probabilities , however , have been modeled with the normal distribution before [ see , e.g. , @xcite ] .",
    "furthermore , the normal distribution is a rather standard assumption in psychological models [ see , e.g. , signal - detection theory in @xcite ] .",
    "a second limitation resides in the assumption that both the observed and hidden processes are expected to grow linearly .",
    "this assumption could be relaxed , for instance , by adding higher order terms to the model .",
    "a more complex model , however , is likely to sacrifice interpretability . given that our model can detect very intricate patterns in the crowd belief ( see figure  [ exampleplotsfinal ] ) ,",
    "compromising interpretability for the sake of facilitating nonlinear growth is hardly necessary .",
    "a third limitation appears in an online setting where new forecasts are received at a fast rate . given that our model is fit in a retrospective fashion",
    ", it is necessary to refit the model every time a new forecast becomes available .",
    "therefore , our model can be applied only to offline aggregation and online problems that tolerate some delay .",
    "a more scalable and efficient alternative would be to develop an aggregator that operates recursively on streams of forecasts .",
    "such a _ filtering _ perspective would offer an aggregator that estimates the current crowd belief accurately without having to refit the entire model each time a new forecast arrives . unfortunately , this typically implies being less accurate in estimating the model parameters such as the bias term .",
    "however , as estimation of the model parameters was addressed in this paper , designing a filter for probability forecasts seems like the next natural development in time - series probability aggregation .",
    "the u.s . government is authorized to reproduce and distribute reprints for government purposes notwithstanding any copyright annotation thereon . disclaimer : the views and conclusions expressed herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements , either expressed or implied , of iarpa , doi / nbc or the u.s . government ."
  ],
  "abstract_text": [
    "<S> most subjective probability aggregation procedures use a single probability judgment from each expert , even though it is common for experts studying real problems to update their probability estimates over time . </S>",
    "<S> this paper advances into unexplored areas of probability aggregation by considering a dynamic context in which experts can update their beliefs at random intervals . </S>",
    "<S> the updates occur very infrequently , resulting in a sparse data set that can not be modeled by standard time - series procedures . in response to the lack of appropriate methodology </S>",
    "<S> , this paper presents a hierarchical model that takes into account the expert s level of self - reported expertise and produces aggregate probabilities that are sharp and well calibrated both in- and out - of - sample . </S>",
    "<S> the model is demonstrated on a real - world data set that includes over 2300 experts making multiple probability forecasts over two years on different subsets of 166 international political events .    ,    ,    , </S>"
  ]
}