{
  "article_text": [
    "let @xmath0 be the cone of positive semidefinite matrices in the space of @xmath1 symmetric matrices @xmath2 endowed with the standard trace inner product @xmath3 and the frobenius norm @xmath4 .",
    "consider the convex quadratic semidefinite programming ( qsdp ) problem in the following standard form : @xmath5 where @xmath6 is a self - adjoint positive semidefinite linear operator , @xmath7 is a linear map whose adjoint is denoted as @xmath8 , @xmath9 , @xmath10 are given data , @xmath11 is a simple nonempty closed convex polyhedral set in @xmath12 , e.g. , @xmath13 with @xmath14 being given matrices .",
    "the main objective of this paper is to design and analyse efficient algorithms for solving ( * p * ) and its dual .",
    "we are particularly interested in the case where the dimensions @xmath15 and/or @xmath16 are large , and the matrix representation of @xmath17 may not be explicitly stored or computed .",
    "for example , if @xmath18 is the kronecker product of a dense symmetric positive semidefinite matrix @xmath19 with itself , then it would be extremely expensive to store the matrix representation of @xmath17 explicitly when @xmath15 is larger than , say , 500 .",
    "as far as we are aware of , the best solvers currently available for solving ( * p * ) are based on inexact primal - dual interior - point methods @xcite . however , they are highly inefficient for solving large scale problems as interior - point methods have severe inherent ill - conditioning limitations which would make the convergence of a krylov subspace iterative solver employed to compute the search directions to be extremely slow . on the other hand , an interior - point method which employs a direct solver to compute the search directions is prohibitively expensive for solving ( * p * ) since the cost is at least @xmath20 arithmetic operations per iteration .",
    "thus , there is currently no efficient solver for solving the general qsdp problem ( * p * ) and this provides the main motivation for writing this paper .",
    "the algorithms which we will design later are based on the augmented lagrangian function for the dual of ( * p * ) , which takes the form of @xmath21 where @xmath22 is any subspace of @xmath12 containing the range space of @xmath17 , @xmath23 , @xmath24 is the support function of @xmath11 . in this paper",
    ", we fix @xmath25 , and we will see in the subsequent analysis that this choice in fact plays an important role in making our designed algorithms efficient . let @xmath26 be a given positive number . the augmented lagrangian function for ( * d * ) is defined by @xmath27 & + \\frac{\\sigma}{2}\\norm{z-\\cq w+s + \\ca^*y - c + \\sig^{-1}x } ^2 -\\frac{1}{2\\sig } \\norm{x}^2 \\end{aligned}\\ ] ] for @xmath28 .",
    "due to its possible wide applications and mathematical elegance @xcite , qsdp has been studied quite actively both from the theoretical and numerical aspects .",
    "for the recent theoretical developments , one may refer to @xcite and references therein . here",
    "we focus on the numerical aspect and we will next briefly review some of the methods available for solving qsdp problems .",
    "toh et al @xcite and toh @xcite proposed inexact primal - dual path - following interior - point methods to solve the special class of convex qsdp without the constraint in @xmath11 . in theory , these methods can be used to solve qsdp problems with inequality constraints and constraint in @xmath11 by reformulating the problems in the required standard form . however , as already mentioned , in practice interior - point methods are not efficient for solving qsdp problems beyond moderate scale either due to the extremely high computational cost per iteration or the inherent ill - conditioning of the linear systems governing the search directions . in @xcite , zhao designed a semismooth newton - cg augmented lagrangian ( nal ) method and analyzed its convergence for solving the primal qsdp problem ( * p * ) .",
    "however , the nal algorithm often encounters numerical difficulty ( due to singular or nearly singular generalized hessian ) when the polyhedral set constraint @xmath29 is present .",
    "subsequently , jiang et al @xcite proposed an inexact accelerated proximal gradient method for least squares semidefinite programming with neither inequality constraints nor polyhedral set constraint , and the objective function in ( * p * ) is expressed explicitly in the form of @xmath30 for some given linear map @xmath31 .",
    "more recently , inspired by the successes achieved in @xcite for solving the linear sdp problems with nonnegative constraints , li , sun and toh @xcite proposed a first - order algorithm , known as the schur complement based semi - proximal alternating direction method of multipliers ( scb - spadmm ) , for solving the dual form ( * d * ) of qsdp . as far as we aware of",
    ", @xcite is the first paper to advocate using the dual approach for solving qsdp problems . by leveraging on the inexact block symmetric gauss - seidel ( sgs ) decomposition technique to be presented later in section [ sec : sgs ] , chen , sun and",
    "toh @xcite also employed the dual approach by proposing an efficient inexact admm - type first - oder method ( which we name as sgs - ispadmm ) for solving problem ( * d * ) .",
    "although the dual approach has demonstrated to be highly efficient in obtaining an approximate solution with lower to medium accuracy @xcite , the design of algorithms aiming to solve ( * d * ) to high accuracy by incorporating second - order information is still a challenging task .    in this paper , which is motivated by the recent progress made in @xcite",
    ", we aim to provide a thorough study on the design of an algorithm which employs second - order information to solve qsdp problems to high accuracy .",
    "we achieve our goal by proposing a two - phase augmented lagrangian based algorithm with phase i to generate a reasonably good initial point to warm - start the phase ii algorithm so as to obtain accurate solutions efficiently .",
    "we call this new method qsdpnal since it extends the ideas of sdpnal @xcite and sdpnal+ @xcite for linear sdp problems to qsdp problems .",
    "the algorithm which we will adopt in qsdpnal - phase i is a variant of the sgs - ispadmm algorithm developed in @xcite .",
    "the latter algorithm designed for the dual qsdp problem ( * d * ) , and the inexact accelerated block coordinate descent ( abcd ) method @xcite designed for least squares semidefinite programming problems , are both based on the powerful and elegant inexact sgs decomposition technique to be presented in section [ sec : sgs ] .",
    "both algorithms have demonstrated to be highly efficient for computing a low to medium accuracy solution for the underlying qsdp problem .",
    "we should emphasize that besides our contributions in designing highly efficient algorithms for solving ( * d * ) , our another piece of important contribution in this paper is the powerful inexact sgs decomposition technique developed in section [ sec : sgs ] , which is designed to provide an elegant reformulation of the decomposition technique developed in @xcite .    in qsdpnal - phase ii , we design a proximal augmented lagrangian method ( alm ) for solving ( * d * ) where the inner subproblem in each iteration is solved via the abcd method together with the intelligent incorporation of the semismooth newton - cg algorithm .",
    "given @xmath32 , @xmath33 , the @xmath34th iteration of the proximal alm consists of the following steps : @xmath35    & + \\frac{1}{2\\sigma}\\big(\\norm{z - z^k}^2   + \\norm{w - w^k}^2_{\\cq } + \\norm{s - s^k}^2 \\\\[5pt ]    & + \\norm{y - y^k}^2\\big )   \\,\\mid\\ , ( z , w , s , y)\\in\\sn\\times\\range(\\cq)\\times\\sn\\times\\re^m    \\end{array }    \\right\\ } , \\\\[5pt ]    & x^{k+1 } = x^k + \\sigma_k(z^{k+1 } - \\cq w^{k+1 } + s^{k+1 } + \\mathcal{a}^*y^{k+1 } - c ) ,    \\end{aligned}\\ ] ] where @xmath36 . by restricting @xmath37 ,",
    "we are able to design checkable stopping criterion for solving the inner subproblem inexactly while establishing the global convergence of the above proximal augmented lagrangian method , as well as analyzing its convergence rate . meanwhile , under this restriction , the difficulties in analyzing the convergence of the abcd algorithm in @xcite and the suplinear ( quadratic ) convergence of the newton - cg algorithm are also circumvented . however , at first glance , the restriction that @xmath37 appears to introduce severe numerical difficulties when we solve linear systems under this restriction .",
    "fortunately , by carefully examining our algorithm and performing  smart \" numerical linear algebra , we are able to provide techniques in section [ sec : numerical - issues ] to overcome these difficulties .",
    "our preliminary evaluation of has demonstrated that our algorithm is capable of solving large scale general qsdp problems of the form ( * p * ) to high accuracy very efficiently .",
    "for example , we are able to solve an elementwise weighted nearest correlation matrix estimation problem with matrix dimension @xmath38 and having 50 millions lower bound constraints in about 65 hours to the relative accuracy of less than @xmath39 in the kkt residual .",
    "the remaining parts of this paper are organized as follows . in the next section ,",
    "we introduce our inexact block symmetric gauss - seidel decomposition technique .",
    "we should emphasize here that the sgs technique plays a pivotal role in the design of both two phases of our algorithm .",
    "in section 3 , we propose an inexact proximal augmented lagrangian ( ipalm ) algorithm which will be used as a prototype for our algorithmic design and study its convergence results . in section 4 , we propose to solve the inner minimization subproblems of the ipalm method by wisely combining the sgs decomposition technique and the semismooth newton - cg algorithm with the recent developed abcd algorithm in @xcite .",
    "section 5 is devoted to our main algorithm qsdpnal , which is a two - phase augmented lagrangian algorithm whose phase i is used to generate a reasonably good initial point to warm - start the phase ii algorithm so as to obtain accurate solutions efficiently . in section 6 , we discuss issues concerning the efficient implementation of qsdpnal . in section 7 , we discuss the special case of applying for solving semidefinite matrix least squares problems . in section 8 ,",
    "we conduct numerical experiments to evaluate the performance of in solving various convex quadratic semidefinite programming problems and their extensions .",
    "we conclude our paper in the final section .",
    "for a given closed proper convex function @xmath40 $ ] , where @xmath41 is a finite - dimensional real inner product space , the proximal mapping @xmath42 for @xmath43 at a point @xmath44 is defined by @xmath45 throughout this paper , we will often make use of the following identity : @xmath46 where @xmath47 is a given parameter , and @xmath48 $ ] is the conjugate function of @xmath43 .",
    "here we introduce the powerful and elegant inexact symmetric gauss - seidel ( sgs ) decomposition technique for solving a convex minimization problem whose objective is the sum of a multi - block quadratic function and a non - smooth function involving only the first block . this decomposition technique will be the foundation for us to design our algorithm qsdpnal .",
    "let @xmath49 be a given integer and @xmath50 , where @xmath51 are real finite dimensional euclidean spaces .",
    "for any @xmath52 , we write @xmath53 with @xmath54 , @xmath55 .",
    "let @xmath56 be a given self - adjoint positive semidefinite linear operator .",
    "consider the following block decomposition of @xmath57 @xmath58 where @xmath59 are self - adjoint positive semidefinite linear operators , @xmath60 are linear maps and each @xmath61 denotes the adjoint of @xmath62 . here",
    ", we further assume that @xmath63 define linear operators @xmath64 by @xmath65 note that @xmath66 and @xmath67 . for later purpose",
    ", we define @xmath68 with the convention that @xmath69 .",
    "we also define the self - adjoint positive semidefinite linear operator @xmath70 by @xmath71    given @xmath72 , define the convex quadratic function @xmath73 by @xmath74 let @xmath75 $ ] be a given closed proper convex function .",
    "suppose @xmath76 are given error tolerance vectors with @xmath77 .",
    "denote @xmath78 given @xmath79 , we consider solving the following problem @xmath80 the following theorem shows that @xmath81 can be computed in a block symmetric gauss - seidel fashion , and it is the key ingredient for our subsequent algorithmic developments .",
    "[ thm : nbsgs ] assume that the self - adjoint linear operators @xmath82 are positive definite for all @xmath83 .",
    "then , it holds that @xmath84 for @xmath85 suppose we have computed @xmath86 defined as follows : @xmath87       & = & \\ch_{ii}^{-1}\\big(r_i + \\delta_i ' - \\mbox{$\\sum_{j=1}^{i-1}$ } \\ch_{ji}^*\\bx_j - \\mbox{$\\sum_{j = i+1}^s$ } \\ch_{ij}x^\\prime_j \\big ) .",
    "\\end{array }    \\label{xpimrei}\\end{aligned}\\ ] ] then the optimal solution @xmath81 defined by can be obtained exactly via @xmath88   x_i^+ & = & \\operatorname*{argmin}_{x_i\\in\\cx_i } \\ ;",
    "\\phi(x_1^+ ) + h(x^+_{\\le i-1},x_i , x^\\prime_{\\ge i+1 } ) - \\inprod{\\delta_i}{x_i}\\\\[5pt ]   & = & \\ch_{ii}^{-1}\\big(r_i+\\delta_i -\\mbox{$\\sum_{j=1}^{i-1}$ } \\ch_{ji}^*x_j^+ - \\mbox{$\\sum_{j = i+1}^s$ } \\ch_{ij}x^\\prime_j\\big),\\quad i=2,\\ldots , s .   \\end{array } \\right.\\end{aligned}\\ ] ]    since @xmath89 , we know that @xmath90 , @xmath91 and @xmath92 are all nonsingular .",
    "the result in can easily be obtained from the following observation @xmath93    next we show the equivalence between ( [ prox - t ] ) and ( [ prox - nt ] ) . by noting that @xmath94 and @xmath95",
    ", we can define @xmath96 as follows : @xmath97 the optimality conditions corresponding to @xmath96 and @xmath98 in can be written as    _",
    "11x_1^= r_1 -_1 + _ 1 ^ - _ ijx^_j , [ optx1p ] + _ 11x_1^+ = r_1 -_1 + _ 1 - _ ijx^_j , [ optx1 + ]    where @xmath99 .",
    "simple calculations show that together with can equivalently be rewritten as @xmath100 where @xmath101 , while can equivalently be recast as @xmath102 by substituting @xmath103 into the above equation , we have @xmath104    = { } & \\cd(\\cd + \\cu)^{-1}(r - \\gamma ) + \\cu(\\cd + \\cu)^{-1}\\cu^*\\bx + \\delta - \\cu(\\cd + \\cu)^{-1}\\delta^\\prime ,   \\nn\\end{aligned}\\ ] ] which , together with , and the definition of @xmath105 in , implies that @xmath106 by noting that is in fact the optimality condition for and @xmath107 , we have thus obtained the equivalence between ( [ prox - t ] ) and ( [ prox - nt ] ) .",
    "this completes the proof of the theorem .",
    "[ rmk : sgs ] in and , @xmath108 and @xmath109 should be regarded as inexact solutions to the corresponding minimization problems without the linear error terms @xmath110 and @xmath111 .",
    "when the approximate solutions @xmath108 and @xmath109 have been computed , the corresponding error vectors @xmath112 and @xmath113 are then given by : @xmath114 \\delta_1 & \\in & \\partial \\phi(x_1^+ ) + \\ch_{11 } x_1^+ - \\big(r_1 - \\sum_{j=2}^s \\ch_{1j } x_j^\\prime\\big ) \\\\[5pt ] \\delta_i & = & \\ch_{ii}x_i^+ - \\big(r_i -\\sum_{j=1}^{i-1 } \\ch_{ji}^ * x^+_j -\\sum_{j = i+1}^s \\ch_{ij } x_j^\\prime \\big ) , \\quad i=2,\\ldots , s .",
    "\\end{array}\\end{aligned}\\ ] ] with the above known error vectors , we have that @xmath108 and @xmath109 are the exact solutions to the minimization problems in and , respectively .",
    "the following proposition is useful in estimating the error term .",
    "suppose that @xmath115 is positive definite .",
    "let @xmath116 .",
    "it holds that @xmath117    recall that @xmath118 .",
    "thus , we have @xmath119 which , together with the definition of @xmath120 in , implies that @xmath121 the desired result then follows .",
    "in this section , we propose an inexact proximal augmented lagrangian method which serves as a prototype for our algorithm qsdpnal - phase ii .",
    "consider the following convex optimization problem @xmath122 where @xmath123 $ ] is a closed proper convex function , @xmath124 is a given linear map , @xmath125 is a real finite dimensional euclidean space .",
    "we denote by @xmath126 the lagrangian function for : @xmath127 the optimality conditions ( kkt conditions ) for are given by : @xmath128 given @xmath129 , the augmented lagrangian function associated with ( [ aug - problem ] ) is defined as follows : @xmath130 in this section , we study the following inexact proximal augmented lagrangian method , which is a modification of the classical method proposed by rockafellar in @xcite .",
    "note that in the inner subproblem of algorithm ipalm , a more general positive definite proximal term @xmath131 is added while in the classic proximal method of multipliers in @xcite , @xmath132 is required to be the identity operator . in @xcite ,",
    "the positive definite proximal term is introduced to guarantee the existence of the optimal solution to the inner subproblem , and to ensure that the stopping criteria proposed for the inexact minimization procedure in will be attainable .",
    "our modification , however , emphasizes more on the aspect of making the subproblem easier to solve .",
    "moreover , it will be necessary in our phase ii algorithm when the subspace constraint @xmath37 is present .    to establish the convergence of algorithm ipalm",
    ", we will adapt the theory developed by rockafellar in @xcite . to this end , for @xmath133 , and any given @xmath134 , we first define the function : @xmath135 corresponding to the closed proper convex - concave function @xmath126 , we can define the maximal monotone operator @xmath136 ( * ? ? ?",
    "* corollary 37.5.2 ) , by @xmath137 whose corresponding inverse operator is given by @xmath138 let @xmath139 and @xmath140 be the smallest and largest eigenvalues of @xmath141 , respectively . define the function @xmath142 which is also a closed proper convex - concave function .",
    "thus , associated with @xmath143 , we can define the maximal monotone operator @xmath144 as @xmath145 we know by simple calculations that @xmath146 and @xmath147 .",
    "the optimal solution of problem , i.e. , @xmath148 , can be obtained via the following lemma .",
    "[ lemma : ppa ] for all @xmath149 , it holds that @xmath150    now , we analyze the convergence properties of algorithm ipalm by applying the general theory of proximal point algorithm ( ppa ) for maximal monotone operators @xcite to it .",
    "the key to this application is the following proposition , which is in fact a generalization of ( * ? ? ?",
    "* proposition 8) .",
    "[ prop : vx - ippa ] let @xmath151 , @xmath152 and @xmath141 be defined in , and , respectively .",
    "let @xmath153 be generated by .",
    "it holds that @xmath154    from the definition of @xmath155 in , we know that @xmath156 therefore , for any @xmath157 , we have @xmath158 which , by ( * ? ? ? * proposition 7 ) , implies @xmath159 thus , @xmath160 and @xmath161 or equivalently , @xmath162 then , by lemma [ lemma : ppa ] and the nonexpansive property of the proximal mapping associated with the maximal monotone operator @xmath144 , we know that @xmath163 = { } & \\norm{(\\ci + \\sigma_k\\ct_{\\wtl})^{-1}(\\sigma_k\\lambda^{-1/2}(\\xi,0 ) + \\lambda^{1/2}(v^k , x^k ) ) - ( \\ci + \\sigma_k \\ct_{\\wtl})^{-1}(\\lambda^{\\frac{1}{2}}(v^k , x^k))}\\\\[5pt ] \\le { } & \\norm{\\sigma_k\\lambda^{-1/2}(\\xi,0 ) } \\le { } \\frac{\\sigma_k}{\\sqrt{\\lambda_{\\min}}}\\norm{\\xi}. \\end{aligned}\\ ] ] since holds for all @xmath164 , we obtain the desired result .    next , we shall discuss the stopping criteria for the subproblem in algorithm ipalm .",
    "following @xcite , we propose the following stopping criteria to terminate the subproblem in algorithm ipalm : @xmath165 ( { \\rm b})\\displaystyle\\quad\\quad \\text{dist}(0,\\partial\\phi_k(v^{k+1}))\\leq \\frac{\\delta_k\\lambda_{\\min}}{\\sigma_k}\\norm{(v^{k+1},x^{k+1 } ) - ( v^k , x^k ) } , \\quad \\sum_{k=0}^\\infty\\delta_k < + \\infty .",
    "\\end{array}\\end{aligned}\\ ] ] after all these preparations , we can state the global convergence of algorithm ipalm from rockafellar @xcite without much difficulty .",
    "[ thm : golbal - converge - ppa ] suppose that the solution set to the kkt system is nonempty",
    ". then the sequence @xmath166 generated by ipalm with stopping criterion @xmath167 is bounded .",
    "in addition , @xmath168 converges to an optimal solution of ( [ aug - problem ] ) , @xmath169 converges to an optimal solution of the corresponding dual problem .    to study the local convergence rate of our proposed algorithm ipalm , we need the following error bound assumption used in @xcite .    [",
    "assumption : err - bound ] for a maximal monotone operator @xmath170 with @xmath171 nonempty , there exist @xmath172 and @xmath173 such that @xmath174    [ rmk : error - bound - qp ] the error bound assumption holds automatically when @xmath175 is a polyhedral multifunction @xcite .",
    "specifically , for a convex piecewise quadratic programming problem @xcite , assumption [ assumption : err - bound ] holds for the corresponding @xmath136 .",
    "[ prop : error - bound ] assume that @xmath176 is nonempty and that there exist @xmath172 and @xmath173 such that @xmath177 then , we have that @xmath178 is nonempty , and @xmath179 i.e. , the error bound assumption also holds for @xmath144 .    for any given @xmath180 and @xmath181 , let @xmath182 we have that @xmath183 and @xmath184 .",
    "thus , @xmath185 .",
    "since @xmath186 is a closed nonempty set , there exists @xmath187 such that @xmath188 . for @xmath189",
    ", we therefore have @xmath190 thus , @xmath191 this completes the proof of the proposition .",
    "we are now ready to present the local linear convergence of algorithm ipalm .",
    "[ thm : local - linear ] suppose assumption [ assumption : err - bound ] holds for @xmath192 , i.e. , @xmath193 is nonempty and there exist @xmath172 and @xmath173 such that @xmath177 let @xmath194 be the sequence generated by ipalm with stopping criterion @xmath195 .",
    "let @xmath196 and @xmath197 .",
    "then , for all @xmath198 sufficiently large , @xmath199 where @xmath200 as @xmath201 .",
    "note that stopping criterion @xmath195 implies @xmath202 therefore , by combining proposition [ prop : error - bound ] and ( * ? ? ?",
    "* theorem 2.1 ) , we obtain the desired results .",
    "in this section , we will adapt the recently developed inexact accelerated block coordinate descent ( abcd ) algorithm @xcite to solve the inner subproblems in the proximal augmented lagrangian method , where each subproblem takes the form of : @xmath203 & \\,\\mid\\ , ( z , w , s , y)\\in\\sn\\times\\range(\\cq)\\times\\sn\\times\\re^{m } \\end{aligned } \\right\\}\\ ] ] for a given @xmath204 , and @xmath205 with @xmath206 denoting the identity operator defined on the appropriate space .",
    "it is important to emphasize that due to our choice of @xmath25 , @xmath207 is in fact a strongly convex function on @xmath208 .",
    "thus , for any @xmath209 , the level set @xmath210 is a closed and bounded convex set",
    ". meanwhile , problem admits a unique optimal solution denoted as @xmath211 .",
    "we will present two variants of the abcd algorithm to be used here . in the first variant",
    ", @xmath212 is decomposed into two groups , namely @xmath213 and @xmath214 . in this case",
    ", @xmath214 is regarded as a single block and the corresponding subproblem in the abcd algorithm can only be solved by an iterative method inexactly .",
    "we will design a semismooth newton - cg ( sncg ) method to solve the subproblem later in subsection [ subsec - sncg ] . in the second variant ,",
    "we write @xmath212 as @xmath215 , i.e. , @xmath216 and @xmath217 are handled separately .",
    "the block @xmath217 again will be computed via the semismooth newton - cg method for solving the corresponding subproblem .",
    "the detail steps of the first variant of the inexact abcd algorithm are given as follows .",
    "as mentioned above , we will design a semismooth newton - cg algorithm to solve the subproblem later in subsection [ subsec - sncg ] .",
    "let @xmath218 from the optimality conditions for and the definition of @xmath207 , we can prove the following results for each @xmath126 : @xmath219    next we describe the second variant of the inexact abcd algorithm for solving , whose steps are given as follows .",
    "note that the inner subproblem involving the block @xmath217 in will again be solved by a semismooth netwon - cg algorithm similar to the one to be described in subsection [ subsec - sncg ] .",
    "let @xmath220 from the optimality conditions in , we can prove that @xmath221    the convergence results for the above algorithm abcd1 and abcd2 are stated in the next theorem , whose proof follows from the strong convexity of @xmath207 and ( * ? ? ?",
    "* theorem 3.1 ) .    [ abcd-1 ]",
    "let @xmath222 be the sequence generated by algorithm abcd1 ( algorithm abcd2 ) .",
    "then @xmath223 and @xmath222 converges to the unique optimal solution @xmath211 of problem .",
    "moreover , we have @xmath224 with @xmath225 given in ( @xmath226 given in ) .    by ( * ? ? ?",
    "* theorem 3.1 ) , we know that @xmath227 the uniqueness of the optimal solution @xmath211 follows from the strong convexity of @xmath207 on @xmath228 . meanwhile , by using the strong convexity of @xmath229 on @xmath230 , we have @xmath231 and thus @xmath232 if algorithm abcd1 is used . similarly , @xmath233 if algorithm abcd2 is used .",
    "note that @xmath234 with @xmath235 .",
    "the fact @xmath236 and the lipschitz continuity of @xmath237 then yield @xmath238      next we shall discuss how the inner problems involved in algorithm abcd1 can be solved efficiently . given @xmath239 and @xmath240 , for all @xmath241 , denote @xmath242 r(w , y ) & : = & \\ca^*y - \\cq w - \\widetilde c + \\widehat s   + \\sigma \\pi_{\\sn_+}(s(w , y ) ) , \\end{array}\\end{aligned}\\ ] ] where @xmath243 . observe that if @xmath244 then @xmath245 can be computed in the following manner @xmath246    + \\frac{1}{2\\hat\\sigma } \\norm { \\cq w - \\ca^*y + \\widetilde c - \\widehat s}^2    + \\frac{1}{2\\sigma}(\\norm{w - \\widehat w}_{\\cq}^2 + \\norm{y - \\hat y}^2 ) \\\\[5pt ]    \\end{array }    \\right\\ } , \\\\[5pt ]    & s^ * = \\hat\\sigma^{-1}\\pi_{\\sn_+}(-s(w^*,y^ * ) ) , \\end{aligned } \\right.\\ ] ] where @xmath247 .",
    "hence , to obtain @xmath248 , we need to solve the following optimization problem @xmath249 note that @xmath250 is a continuously differentiable function on @xmath251 with @xmath252      -b + \\hat\\sigma^{-1}\\ca\\big(r(w , y)\\big )   + \\sigma^{-1}(y-\\hat y )    \\end{array } \\right).\\ ] ] note that solving is equivalent to solving the following nonsmooth equation : @xmath253 since @xmath254 is strongly semismooth @xcite , we can design a semismooth newton - cg ( sncg ) method to solve and could expect to get a fast superlinear or even quadratic convergence . for any @xmath241 , define @xmath255 + \\hat\\sigma^{-1 } \\left [ \\begin{array}{c }         \\cq   \\\\",
    "-\\ca     \\end{array } \\right]\\big(\\ci + \\sigma^2",
    "\\partial\\pi_{\\sn_+}(s(w , y))\\big)[\\cq \\ ; -\\ca^*],\\ ] ] where @xmath256 is the clarke subdifferential @xcite of @xmath257 at @xmath258 . note that from @xcite , we know that @xmath259 where @xmath260 denotes the generalized hessian of @xmath261 at @xmath262 , i.e. , the clarke subdifferential of @xmath263 at @xmath262 .    given @xmath264 , consider the following eigenvalue decomposition : @xmath265 where @xmath266 is an orthogonal matrix whose columns are eigenvectors , and @xmath267 is the corresponding diagonal matrix of eigenvalues , arranged in a nonincreasing order : @xmath268 . define the following index sets @xmath269 we define the operator @xmath270 by @xmath271 where @xmath272 denotes the hadamard product of two matrices , @xmath273 \\nu^{{\\mbox{\\textrm{\\tiny{t}}}}}_{\\alpha \\bar{\\alpha } } & 0 \\end{array } \\right ] , \\quad \\nu_{ij } : = \\frac{\\lambda_i}{\\lambda_i-\\lambda_j } , \\,\\",
    ", i \\in \\alpha , j \\in \\bar{\\alpha},\\end{aligned}\\ ] ] and @xmath274 is the matrix of ones . in (",
    "* lemma 11 ) , it is proved that @xmath275 define @xmath276 + \\hat\\sigma^{-1 } \\left [ \\begin{array}{c }         \\cq   \\\\         -\\ca     \\end{array }",
    "\\right](\\ci + \\sigma^2 u^0)[\\cq \\ , -\\ca^*].\\ ] ] then , we have @xmath277 .",
    "note that @xmath278 is positive definite on @xmath279 .",
    "after all the above preparations , we can design the following semismooth newton - cg method as in @xcite to solve .",
    "10 true pt    the convergence results for the above sncg algorithm are stated in the next theorem .",
    "[ convergence - zwy - newton ] let the sequence @xmath280 be generated by algorithm sncg .",
    "suppose at each step @xmath281 , when the cg algorithm terminates , the stopping condition is achieved , i.e. , @xmath282 then the sequence @xmath280 converges to the unique optimal solution @xmath283 of the problem in and @xmath284    first we note that @xmath257 is strongly semismooth .",
    "since @xmath285 is a strongly convex function on @xmath286 , problem has a unique solution @xmath287 and the level set @xmath288 is compact .",
    "therefore , the sequence generated by sncg is bounded as @xmath289 is a descent direction ( * ? ? ?",
    "* propsition 3.3 ) .",
    "note that for all @xmath290 , every @xmath291 is self - adjoint and positive definite on @xmath292 , the desired results can then be obtained from ( * ? ? ?",
    "* theorem 3.4 and 3.5 ) .",
    "it is clear that the positive definiteness of @xmath293 in the semismooth newton system is guaranteed by adding the positive proximal terms corresponding to @xmath214 . under the constraint nondegenerate condition for ( * p * )",
    ", we shall show in the next theorem that one can still ensure the positive definiteness of the coefficient matrix in the semismooth newton system at the solution point even if the proximal terms are absent .",
    "[ prop : psd - rangeq ] let @xmath294 be a self - adjoint positive semidefinite linear operator and @xmath26 .",
    "then , it holds that @xmath295 is positive definite if and only if @xmath296}{\\left(\\left [ \\begin{array}{cc }         \\cq &    \\\\             & 0     \\end{array } \\right]+\\sigma \\left [ \\begin{array}{c }         \\cq",
    "\\\\         -\\ca     \\end{array } \\right ] \\cu [ \\cq \\ , -\\ca^*]\\right)\\left[\\begin{array}{c }    w\\\\    y    \\end{array}\\right ] } > 0\\ ] ] for all @xmath297    since the  if \" statement obviously holds true , we only need to prove the  only if \" statement .",
    "note that @xmath298 now suppose that @xmath295 is positive definite , and hence nonsingular . by the schur complement condition for ensuring the positive definiteness of a linear operator , we know that holds if and only if @xmath299 but for any @xmath37 , we have @xmath300 & = &   \\inprod { \\cu^{\\frac{1}{2 } } \\cq w}{(\\ci -\\cu^{\\frac{1}{2}}\\ca^*(\\ca \\cu\\ca^*)^{-1}\\ca \\cu^{\\frac{1}{2}})\\cu^{\\frac{1}{2}}\\cq w } \\;\\geq \\ ; 0.\\end{aligned}\\ ] ] hence , holds automatically .",
    "this completes the proof of the proposition .",
    "when the proximal terms corresponding to @xmath214 are absent , problem now takes the following form @xmath301    \\end{aligned}\\right\\}.\\ ] ]    [ sncg - no - prox ] let @xmath302 be the optimal solution for problem .",
    "let @xmath303 .",
    "the following conditions are equivalent :    1 .",
    "the constraint nondegenerate condition , @xmath304 holds at @xmath305 , where @xmath306 denotes the lineality space of the tangent cone of @xmath307 at @xmath308 .",
    "2 .   every element in @xmath309+\\sigma \\left [ \\begin{array}{c }         \\cq   \\\\",
    "-\\ca     \\end{array } \\right ] \\partial\\pi_{\\sn_+}(\\widehat x + \\sigma(\\widehat z + \\ca^*\\bar y -\\cq \\overline w   - c ) ) [ \\cq \\ , -\\ca^*]\\ ] ] is self - adjoint and positive definite on @xmath310    in the same fashion as in ( * ? ? ?",
    "* proposition 3.2 ) , we can prove that @xmath311 is positive definite for all @xmath312 if only if ( i ) holds .",
    "then , by proposition [ prop : psd - rangeq ] , we readily obtain the desired results .",
    "in this section , we shall present our two - phase algorithm for solving the convex quadratic semidefinite programming problem ( * d * ) . for the convergence of algorithm qsdpnal , we need the following slater condition for ( * p * ) .    [",
    "assumption : slater ] problem ( * p * ) satisfies the following slater condition : @xmath313        & \\exists\\ , x_0\\in\\sn_+\\cap\\ck\\ \\textup{such that}\\        \\ca(x_0 ) = b , \\ ,",
    "x_0\\succ 0 .      \\end{aligned }      \\right.\\ ] ]      in phase i , we propose a new variant of the sgs based inexact semi - proximal admm ( sgs - ispadmm ) developed in @xcite to solve ( * d * ) .",
    "recall the augmented lagrangian function associated with problem ( * d * ) defined in .",
    "the detail steps of our phase i algorithm for solving ( * d * ) are given as follows .",
    "the convergence of the above algorithm follows from ( * ? ? ?",
    "* theorem 1 ) without much difficulty .",
    "[ thm : sgs - qsdp ] suppose that the solution set of ( * p * ) is nonempty and assumption [ assumption : slater ] holds .",
    "let @xmath314 be the sequence generated by algorithm qsdpnal - phase i. if @xmath315 , then the sequence @xmath316 converges to an optimal solution of ( * d * ) and @xmath317 converges to an optimal solution of ( * p * ) .",
    "next , we discuss our phase ii algorithm for solving the convex qsdp ( * d * ) .",
    "the purpose of this phase is to obtain high accuracy solutions efficiently after warm - started by our phase i algorithm .",
    "the prototype algorithm ipalm described in section [ sec : ipalm ] , when applied to the problem ( * d * ) , has the following template .    at @xmath198th iteration in the above algorithm , by ( or ) , we know that @xmath318 with @xmath319 defined in .",
    "let @xmath141 be the self - adjoint positive definite linear operator defined by @xmath320 on @xmath321 .",
    "let @xmath322 be its smallest eigenvalue .",
    "we shall investigate the procedure under the following stopping criteria : @xmath323 & ( { \\rm b'})\\quad \\norm{d^{k+1}}\\leq \\displaystyle \\frac{\\delta_k\\lambda_{\\min}}{\\sigma_k}\\norm{(z^{k+1},w^{k+1 } ,   s^{k+1},y^{k+1},x^{k+1 } ) - ( z^{k},w^{k},s^{k},y^{k},x^{k } ) } ,   \\quad\\sum_{k=0}^\\infty\\delta_k < + \\infty .",
    "\\end{array}\\ ] ]    [ thm : qsdpnal-2-global ] suppose that algorithm is executed with stopping criterion @xmath324 and assumption [ assumption : slater ] holds",
    ". then the sequence @xmath314 generated by the algorithm is bounded , and @xmath317 converges to an optimal solution to ( * p * ) and @xmath316 converges to an optimal solution to ( * d * ) .    by noting the fact that @xmath325 , we obtain the results directly from theorem [ thm : golbal - converge - ppa ] .",
    "suppose that @xmath326 is a solution to the following kkt system : @xmath327     & 0\\in x + \\partial\\delta_{\\sn_+}(s),\\       \\ca x - b = 0,\\       0\\in x + \\partial\\delta^*_{\\ck}(-z ) .    \\end{aligned }    \\right.\\ ] ] to establish the local linear convergence of algorithm qsdpnal - phase ii , we need the following assumptions .    [",
    "assumption : srcq ] the strict robinson constraint qualifications ( srcq ) hold at @xmath328 for ( * p * ) and ( * d * ) , i.e. , @xmath329         { } & t_{\\sn_+}(\\,\\overline s\\ , )         \\cap \\overline x^{\\,\\bot }         + \\gamma_{\\overline x , \\overline z }         + \\ca^*\\re^m - \\cq\\,\\range(\\cq ) = \\sn ,      \\end{aligned }      \\right.\\ ] ] where @xmath330 and @xmath331 is a nonempty closed convex cone defined by @xmath332    [ rmk : error - bound ] from ( * ? ? ?",
    "* therorem 5.1 ) , it is easy to see that under assumption [ assumption : srcq ] , the maximal monotone operator @xmath333 corresponding to ( * p * ) and ( * d * ) satisfies assumption [ assumption : err - bound ] .",
    "next we state the local linear convergence of algorithm qsdpnal - phase ii .",
    "[ thm : qsdpnal-2-local ] suppose that @xmath326 is a solution to the kkt system .",
    "let algorithm be executed with stopping criterion @xmath334 .",
    "suppose that assumption [ assumption : srcq ] holds at @xmath328 , then the sequence @xmath314 converges to @xmath328 , i.e. , the unique solution to the kkt system , and for all @xmath198 sufficiently large , @xmath335 \\leq { } & \\theta_{\\infty}\\,\\norm{(z^k , w^k , s^k , y^k , x^k)-(\\overline z , \\overline w , \\overline s , \\bar y , \\overline x)}_{\\lambda } , \\end{aligned}\\ ] ] for some @xmath336 with the property that @xmath337 if @xmath338 for any sufficiently large @xmath339 .",
    "the conclusions follow from the results in ( * ? ? ?",
    "* theorem 5.1 ) and theorem [ thm : local - linear ] .",
    "in algorithm qsdpnal - phase i , in order to obtain @xmath340 and @xmath341 at the @xmath198th iteration , we need to solve the following linear system of equations @xmath342 with the residual @xmath343 where @xmath344 and @xmath172 are given .",
    "note that the exact solution to is unique since @xmath345 is positive definite on @xmath346 .",
    "but the linear system is typically very large even for a moderate @xmath15 , say @xmath347 . under the high dimensional",
    "setting which we are particularly interested in , the matrix representation of @xmath17 is generally not available or too expensive to be stored explicitly .",
    "thus can only be solved inexactly by an iterative method .",
    "however when @xmath17 is singular ( and hence @xmath348 ) , due to the presence of the subspace constraint @xmath37 , it is extremely difficult to apply preconditioning to while ensuring that the approximate solution is contained in @xmath346 . fortunately ,",
    "as shown in the next proposition , instead of solving directly , we can solve a simpler and yet better conditioned linear system to overcome this difficulty .",
    "[ prop : p1-com ] let @xmath349 be an approximate solution to the following linear system : @xmath350 with the residual satisfying @xmath351 then , @xmath352 solves with the residual satisfying .",
    "moreover , @xmath353 and @xmath354 .",
    "first we note that the results @xmath353 and @xmath354 follow from the decomposition @xmath355 .",
    "next , by observing that @xmath356 one can easily obtain the desired results .    by proposition [ prop : p1-com ] , in order to obtain @xmath357 , we can first apply an iterative method such as the preconditioned conjugate gradient ( pcg ) method to solve to obtain @xmath358 and then perform the projection step .",
    "however , by carefully analysing the steps in qsdpnal - phase i , we are delighted to observe that instead of explicitly computing @xmath357 , we can update the iterations in the algorithm by using only @xmath359 .",
    "thus , we only need to compute @xmath360 and the projection step to compute @xmath361 is not necessary .",
    "it is important for us to emphasize the computational advantage of solving the linear system over .",
    "first , the former only requires one evaluation of @xmath362 whereas the latter requires two such evaluations in each pcg iteration .",
    "second , the coefficient matrix in the former system is typically much more well - conditioned than the coefficient matrix in the latter system . more precisely , when @xmath17 is positive definite , then @xmath363 is clearly better conditioned than @xmath364 . when @xmath17 is singular , with its smallest positive eigenvalue denoted as @xmath365 , then @xmath363 is better conditioned when @xmath366 .",
    "the previous inequality would obviously hold when @xmath367 .    in algorithm qsdpal - phase ii",
    ", the subspace constraint @xmath37 also appears when we solve the semismooth newton linear system in algorithm sncg .",
    "specifically , we need to find @xmath368 to solve the following linear system @xmath369 with the residual satisfying the following condition @xmath370 where @xmath371 + \\hat\\sigma^{-1 } \\left [ \\begin{array}{c }         \\cq",
    "\\\\         -\\ca     \\end{array } \\right](\\ci + \\sigma^2 \\cu)[\\cq \\ , -\\ca^*]\\ ] ] and @xmath372 is a given self - adjoint positive semidefinite linear operator on @xmath12 .",
    "again , instead of solving directly , we can solve a simpler linear system to compute @xmath373 approximately , as shown in the next proposition .",
    "[ prop : p2-com ] let @xmath374 + \\hat\\sigma^{-1 } \\left [ \\begin{array}{c }         \\ci   \\\\",
    "-\\ca     \\end{array } \\right](\\ci + \\sigma^2 \\cu)[\\cq \\ ; -\\ca^*].\\ ] ] suppose @xmath375 is an approximate solution to the following system : @xmath376 with the residual satisfying @xmath377 let @xmath378 then @xmath379 solves with the residual satisfying .",
    "moreover , @xmath380 and @xmath381 .",
    "the proof that @xmath380 and @xmath381 is the same as in the previous proposition . observe that @xmath382 .",
    "then , by using the fact that @xmath383 - [ \\cq(r_1 ) ; r_2 ] }    = \\norm{v\\,(\\widehat{dw},\\widehat{dy } ) - ( \\cq(r_1 ) , r_2 ) } \\\\[5pt ]    & \\le&\\norm{\\diag(\\cq,\\ci)}\\,\\norm{\\widehat v\\ , ( \\widehat{dw } , \\widehat{dy } ) - ( r_1 , r_2 ) } \\leq \\norm{\\diag(\\cq,\\ci ) } \\frac{\\eta}{\\norm{\\diag(\\cq,\\ci ) } }   = \\eta,\\end{aligned}\\ ] ] one can obtain the desired results readily .",
    "in this section , we show that for semidefinite matrix least squares problems , can be used in a smart way to avoid the difficulty of handling the subspace constraint @xmath37 .",
    "consider the following semidefinite matrix least squares problem @xmath384 where @xmath385 and @xmath386 are two linear maps , @xmath9 , @xmath10 and @xmath387 are given data , @xmath11 is a simple nonempty closed convex polyhedral set in @xmath12 .",
    "it is easy to see that can be rewritten as follows @xmath388 the dual of takes the following form @xmath389     \\mbox{s.t . }         & z + \\cb^*\\xi + s + \\ca^*y = c,\\quad s\\in\\sn_+ . \\end{array }   \\label{eq - smlsd}\\end{aligned}\\ ] ]    when qsdpnal - phase i is applied to solve , instead of solving , the linear system corresponding to the quadratic term is given by @xmath390 where @xmath391 and @xmath239 are given data .",
    "meanwhile , in qsdpnal - phase ii , the linear system in the sncg method corresponding to problem is given by @xmath392 + \\sigma\\left [                            \\begin{array}{c }                              \\cb \\\\",
    "\\ca \\\\",
    "\\end{array }                          \\right]\\cu\\left [                                \\begin{array}{cc }                                  \\cb^ * & \\ca^*\\\\                                \\end{array }                              \\right ] \\right)\\left [           \\begin{array}{c }             d\\xi \\\\",
    "dy \\\\",
    "\\end{array }         \\right ] \\approx \\left [           \\begin{array}{c }              r_1\\\\              r_2   \\\\           \\end{array }         \\right],\\ ] ] where @xmath393 and @xmath394 are given data , @xmath372 is a given self - adjoint positive semidefinite linear operator on @xmath12 .",
    "note that here we omit the positive definite proximal terms added in qsdpnal - phase ii for simplicity .",
    "it is clear that just like , one can solve efficiently via the pcg method . for",
    ", one can also solve it by the pcg method , which is more appealing compared to using a nonsymmetric iterative solver such as the preconditioned bicgstab to solve the nonsymmetric linear system .",
    "[ rmk : partial - ppa ] when the polyhedral constraint @xmath29 in is absent , i.e. , the polyhedral convex set @xmath395 , jiang , sun and toh in @xcite proposed the partial proximal point algorithm for solving the semidefinite matrix least squares problem .",
    "hence , algorithm qsdpnal can be viewed as a generalization of their algorithm for solving more general convex composite qsdp problems .",
    "consider the following general qsdp problem : @xmath396 where @xmath397 and @xmath398 are two linear maps . by adding slack variable @xmath44 , we can equivalently rewrite into the following standard form : @xmath399     \\mbox{s.t . }         & \\ca_e x    =   b_e ,         \\quad \\ca_i x + \\cd x = b_i , \\quad         x \\in \\sn_+\\cap \\ck ,   \\quad   \\cd x \\ge 0 , \\end{array }   \\label{eq - qsdp - standard}\\end{aligned}\\ ] ] where @xmath400 is a positive definite diagonal matrix which is introduced for the purpose of scaling the variable @xmath44 .",
    "the dual of is given by @xmath401     \\mbox{s.t .",
    "} & z    - \\cq w + s + \\ca_e^ * y_e + \\ca_i^*y_i   = c ,   \\\\[5pt ]     & \\cd^*(s + y_i ) = 0 , \\quad s\\in\\sn_+,\\quad s\\ge 0 , \\quad w\\in\\range(\\cq ) .",
    "\\end{array }     \\label{eq - d - qsdp - standard}\\ ] ] we can express in a form which is similar to ( * d * ) as follows : @xmath402     \\mbox{s.t . } & \\left ( \\begin{array}{c }   \\ci \\\\[3pt ] 0 \\end{array } \\right ) z - \\left ( \\begin{array}{c }   \\cq \\\\[3pt ] 0 \\end{array } \\right ) w + \\left ( \\begin{array}{cc }    \\ci & 0   \\\\[3pt ] 0 & \\cd^ * \\end{array } \\right ) \\left ( \\begin{array}{c }   s \\\\[3pt ] s \\end{array } \\right ) + \\left ( \\begin{array}{cc }   \\ca^*_e & \\ca_i^ * \\\\[3pt ] 0 & \\cd^ * \\end{array } \\right )   \\left ( \\begin{array}{c }    y_e\\\\[3pt ] y_i \\end{array } \\right )   = \\left ( \\begin{array}{c }   c \\\\[3pt ] 0 \\end{array } \\right ) , \\\\[12pt ]   & ( s , s)\\in\\sn_+\\times \\re^{m_i}_+ , \\quad w\\in\\range(\\cq ) .",
    "\\end{array }     \\label{eq - d - qsdp - d}\\end{aligned}\\ ] ] we can readily extend to solve the above more general form of , and our implementation of indeed can be used to solve .    in this section ,",
    "we evaluate the performance our algorithm for solving large - scale qsdp problems .",
    "since contains two phases , we also report the numerical results obtained by running qsdpnal - phase i ( a first - order algorithm ) alone for the purpose of demonstrating the power and importance of our two - phase framework for solving difficult qsdp problems . in our implementation of qsdpnal",
    ", we employ a restarting strategy similar to the one in @xcite , i.e. , when the progress of qsdpnal - phase ii is not satisfactory , we will restart qsdpnal - phase i by using the most recently computed @xmath403 as the initial point .",
    "besides , we also adopt a dynamic tuning strategy to adjust the penalty parameter @xmath404 properly according to the progress of the algorithm . in fact , the aforementioned restarting strategy can also be viewed as a special realization of our dynamic tuning strategy as the penalty parameter @xmath404 is tuned more refinely in qsdpnal - phase i.    in our numerical experiments , we measure the accuracy of an approximate optimal solution @xmath405 for qsdp and its dual by using the following relative kkt residual : @xmath406 where @xmath407 & & \\eta_{s_1 } = \\frac{|\\inprod{s}{x}|}{1+\\norm{s}+\\norm{x } } , \\quad \\eta_{s_2 } = \\frac{\\norm{x - \\pi_{\\sn_+}(x)}}{1+\\norm{x}},\\quad \\eta_{i_1 } = \\frac{\\norm{\\min(b_i - \\ca_i x,0)}}{1+\\norm{b_i}},\\quad \\eta_{i_2 } = \\frac{\\norm{\\max(y_i,0)}}{1+\\norm{y_i}},\\\\[5pt ] & & \\eta_{i_3 } = \\frac{|\\inprod{b_i-\\ca_i x}{y_i}|}{1+\\norm{y_i}+\\norm{b_i - \\ca_i x}},\\quad \\eta_{w } = \\frac{\\norm{\\cq w - \\cq x}}{1+\\norm{\\cq}}.\\end{aligned}\\ ] ] additionally , we compute the relative gap by @xmath408 where @xmath409 and @xmath410 . we terminate both and qsdpnal - phase i when @xmath411 with the maximum number of iterations set at 50,000 .    all our computational results are obtained from a workstation running on 64-bit windows operating system having 16 cores with 32 intel xeon e5 - 2650 processors at 2.60ghz and 64 gb memory . we have implemented in matlab version 7.13 .",
    "our first test example is the problem of finding the nearest correlation matrix ( ncm ) to a given matrix @xmath412 : @xmath413 where @xmath414 is a nonnegative weight matrix , @xmath415 is the vector of all ones , and @xmath416 with @xmath14 being given matrices .",
    "in our numerical experiments , we first take a matrix @xmath417 , which is a correlation matrix generated from gene expression data from @xcite . for testing purpose , we then perturb @xmath417 to @xmath418 where @xmath419 is a given parameter and @xmath420 is a randomly generated symmetric matrix with entries uniformly distributed in @xmath421 $ ] except for its diagonal elements which are all set to @xmath422 .",
    "the weight matrix @xmath423 is generated from a weight matrix @xmath424 used by a hedge fund company .",
    "the matrix @xmath424 is a @xmath425 symmetric matrix with all positive entries .",
    "it has about @xmath426 of the entries equal to @xmath427 and the rest are distributed in the interval @xmath428.$ ] it has @xmath429 eigenvalues in the interval @xmath430 $ ] , @xmath431 eigenvalues in the interval @xmath432 $ ] , and the rest of @xmath433 eigenvalues in the interval @xmath434 $ ] .",
    "the matlab code for generating the matrix @xmath423 is given by    ....",
    "tmp = kron(ones(110,110),h0 ) ; h = tmp(1:n,1:n ) ; h = ( h'+h)/2 .",
    "....    the reason for using such a weight matrix is because the resulting problems generated are more challenging to solve as opposed to a randomly generated weight matrix .",
    "we also test four more instances , namely pdidx2000 , pdidx3000 , pdidx5000 and pdidx10000 , where the raw correlation matrix @xmath435 is generated from the probability of default ( pd ) data obtained from the rmi credit research initiative , national university of singapore .",
    "we consider two choices of @xmath11 , i.e. , case ( i ) : @xmath395 and case ( ii ) : @xmath436 .",
    "+ & & & & & + & & & & & & & + & 587 & 0.10 & 12 @xmath437 52 & 280 & 9.2 - 7 @xmath437 9.9 - 7 & 8.3 - 7 @xmath437 -4.7 - 7 & 13 @xmath437 24 + & 587 & 0.05 & 11 @xmath437 38 & 205 & 9.5 - 7 @xmath437 9.9 - 7 & 7.5 - 7 @xmath437 -4.1 - 7 & 10 @xmath437 17 + & 692 & 0.10 & 12 @xmath437 54 & 250 & 9.8 - 7 @xmath437 9.9 - 7 & 5.4 - 7 @xmath437 -4.8 - 7 & 17 @xmath437 29 + & 692 & 0.05 & 12 @xmath437 43 & 218 & 7.3 - 7 @xmath437 9.7 - 7 & 2.5 - 7 @xmath437 -4.4 - 7 & 14 @xmath437 25 + & 834 & 0.10 & 12 @xmath437 56 & 285 & 8.5 - 7 @xmath437 9.9 - 7 & 2.9 - 7 @xmath437 -5.3 - 7 & 28 @xmath437 49 + & 834 & 0.05 & 12 @xmath437 44 & 230 & 8.0 - 7 @xmath437 9.5 - 7 & -6.8 - 8 @xmath437",
    "-4.5 - 7 & 24 @xmath437 40 + & 1255 & 0.10 & 12 @xmath437 62 & 340 & 8.4 - 7 @xmath437 9.9 - 7 & 3.1 - 7 @xmath437 -5.4 - 7 & 1:11 @xmath437 2:24 + & 1255 & 0.05 & 12 @xmath437 49 & 248 & 7.6 - 7 @xmath437 8.7 - 7 & -1.3 - 7 @xmath437 -4.5 - 7 & 1:03 @xmath437 1:51 + & 1869 & 0.10 & 13 @xmath437 76 & 393 & 6.4 - 7 @xmath437 9.9 - 7 & -2.2 - 7 @xmath437 -9.8 - 7 & 3:32 @xmath437 6:44 + & 1869 & 0.05 & 13 @xmath437 60 & 311 & 5.6 - 7 @xmath437 9.9 - 7 & -4.7 - 7 @xmath437    green -1.0 - 6    & 3:15 @xmath437 5:24 + & 2000 & 0.10 & 10 @xmath437 131 & 590 & 7.1 - 7 @xmath437 9.9 - 7 &    blue 5.7 - 6    @xmath437 -8.5 - 7 & 9:50 @xmath437 11:43 + & 2000 & 0.05 & 10 @xmath437 139 & 626 & 8.1 - 7 @xmath437 9.9 - 7 &    green 3.3 - 6    @xmath437 -9.5 - 7 & 10:37 @xmath437 12:41 + & 3000 & 0.10 & 10 @xmath437 144 & 1201 & 9.3 - 7 @xmath437 9.9 - 7 & 8.0 - 7 @xmath437    green 2.1 - 6    & 33:37 @xmath437 1:15:01 + & 3000 & 0.05 & 10 @xmath437 136 & 1263 & 7.7 - 7 @xmath437 9.7 - 7 & 2.5 - 7 @xmath437    green 2.0 - 6    & 38:03 @xmath437 1:19:27 + & 5000 & 0.10 & 11 @xmath437 189 & 1031 & 7.7 - 7 @xmath437 9.9 - 7 &    green 2.3 - 6    @xmath437    green 1.8 - 6    & 2:50:55 @xmath437 4:17:10 + & 5000 & 0.05 & 10 @xmath437 164 & 1699 & 9.7 - 7 @xmath437 9.9 - 7 &    green 1.6 - 6    @xmath437 -1.3 - 7 & 2:34:15 @xmath437 6:18:29 + & 10000 & 0.10 & 13 @xmath437 200 & 2572 & 5.5 - 7 @xmath437 9.9 - 7 &    green 2.0 - 6    @xmath437 -1.5 - 7 & 27:35:12 @xmath437 60:07:08 + & 10000 & 0.05 & 14 @xmath437 200 & 2532 & 6.9 - 7 @xmath437 9.9 - 7 &    green 1.9 - 6    @xmath437 1.4 - 7 & 29:02:46 @xmath437 59:34:13 +   +   + & 587 & 0.10 & 6 @xmath437 132 & 264 & 9.9 - 7 @xmath437 9.9 - 7 & -1.3 - 7 @xmath437 -4.7 - 7 & 16 @xmath437 28 + & 587 & 0.05 & 7 @xmath437 119 & 257 & 9.8 - 7 @xmath437 9.9 - 7 & -3.4 - 7 @xmath437",
    "-4.2 - 7 & 14 @xmath437 25 + & 692 & 0.10 & 7 @xmath437 144 & 266 & 9.9 - 7 @xmath437 9.9 - 7 & -2.7 - 7 @xmath437 -5.1 - 7 & 23 @xmath437 36 + & 692 & 0.05 & 7 @xmath437 108 & 217 & 8.2 - 7 @xmath437 9.9 - 7 & -2.1 - 7 @xmath437 -4.4 - 7 & 18 @xmath437 29 + & 834 & 0.10 & 9 @xmath437 295 & 472 & 9.8 - 7 @xmath437 9.9 - 7 & -5.9 - 7 @xmath437 -6.0 - 7 & 1:09 @xmath437 1:35 + & 834 & 0.05 & 7 @xmath437 262 & 442 & 8.6 - 7 @xmath437 9.9 - 7 & -4.6 - 7 @xmath437 -5.6 - 7 & 1:04 @xmath437 1:37 + & 1255 & 0.10 & 9 @xmath437 171 & 333 & 9.7 - 7 @xmath437 9.9 - 7 & -3.4 - 7 @xmath437 -5.5 - 7 & 2:01 @xmath437 3:02 + & 1255 & 0.05 & 7 @xmath437 188 & 253 & 8.9 - 7 @xmath437 9.9 - 7 & -4.6 - 7 @xmath437 -5.3 - 7 & 2:02 @xmath437 2:20 + & 1869 & 0.10 & 16 @xmath437 384 & 577 & 9.2 - 7 @xmath437 9.9 - 7 & -7.8 - 7 @xmath437 -8.9 - 7 & 10:07 @xmath437 12:33 + & 1869 & 0.05 & 12 @xmath437 379 & 472 & 9.9 - 7 @xmath437 9.9 - 7 & -8.5 - 7 @xmath437 -8.6 - 7 & 9:36 @xmath437 10:21 + & 2000 & 0.10 & 14 @xmath437 691 & 718 & 9.9 - 7 @xmath437 9.8 - 7 & -6.8 - 7 @xmath437 -8.0 - 7 & 20:57 @xmath437 18:07 + & 2000 & 0.05 & 22 @xmath437 756 & 1333 & 9.6 - 7 @xmath437 5.8 - 7 & -6.3 - 7 @xmath437 -4.0 - 7 & 23:22 @xmath437 40:11 + & 3000 & 0.10 & 34 @xmath437 661 & 1648 & 9.9 - 7 @xmath437 9.9 - 7 & -6.9 - 7 @xmath437 -9.2 - 7 & 1:23:43 @xmath437 2:11:47 + & 3000 & 0.05 & 41 @xmath437 728 & 1538 & 9.9 - 7 @xmath437 9.9 - 7 & -6.2 - 7 @xmath437    green -1.2 - 6    & 1:26:52 @xmath437 2:01:41 + & 5000 & 0.10 & 27 @xmath437 794 & 1154 & 9.3 - 7 @xmath437 9.9 - 7 & -6.8 - 7 @xmath437    green 1.2 - 6    & 4:22:07 @xmath437 4:53:35 + & 5000 & 0.05 & 33 @xmath437 1081 & 1722 & 9.9 - 7 @xmath437 9.9 - 7 & -6.4 - 7 @xmath437 -1.5 - 7 & 6:06:20 @xmath437 6:47:43 + & 10000 & 0.10 & 42 @xmath437 1289 & 2190 & 9.9 - 7 @xmath437 9.9 - 7 & -7.1 - 7 @xmath437 2.6 - 7 & 58:44:49 @xmath437 64:17:14 + & 10000 & 0.05 & 40 @xmath437 1519 & 3320 & 9.9 - 7 @xmath437 4.5 - 7 & -6.6 - 7 @xmath437 -1.7 - 8 & 65:13:19 @xmath437 94:53:10 +    in table [ table : ncm_f ] , we report the numerical results obtained by and qsdpnal - phase i in solving various instances of the h - weighted ncm problem . in the table , `` it '' denotes the number of outer iterations of qsdpnal - phase ii and `` itsgs '' stands for the total number iterations used in qsdpnal - phase i. we can see from table [ table : ncm_f ] that is more efficient than the purely first - order algorithm qsdpnal - phase i , although the latter algorithm has already been demonstrated to be highly efficient for the h - weighted ncm problem @xcite . in particular , for the instance pdidx10000 where the matrix dimension @xmath38 and @xmath11 contains about 50 millions lower bound constraints , we are able to solve the problem in about 65 hours .      based on the sdp relaxation of a binary integer quadratic ( biq ) problem considered in @xcite , we construct our second qsdp test example as following : @xmath438     \\mbox{s.t . }         & \\textup{diag}(y ) - x = 0 , \\quad \\alpha = 1 , \\quad   x = \\left (                 \\begin{array}{cc }                   y & x \\\\                   x^t & \\alpha \\\\                 \\end{array }               \\right )         \\in \\sn_+ , \\quad x\\in \\ck , \\\\[5pt ]         & -y_{ij}+x_i\\ge 0 , \\ , -y_{ij}+x_j\\ge0,\\ , y_{ij}-x_i - x_j\\ge -1,\\ , \\forall \\ ,",
    "i < j , \\ , j=2,\\ldots , n-1 ,         \\end{array }    \\label{qsdp - biq}\\ ] ] where the convex set @xmath439 .",
    "here @xmath440 is a self - adjoint positive semidefinite linear operator defined by @xmath441 with @xmath442 being matrices truncated from two different large correlation matrices ( generated from russell 1000 and russell 2000 index , respectively ) fetched from yahoo finance by matlab . in our numerical experiments ,",
    "the test data for @xmath443 and @xmath444 are taken from biq mac library maintained by wiegele , which is available at http://biqmac.uni-klu.ac.at/biqmaclib.html .",
    "table [ table : biqi ] reports the numerical results for and qsdpnal - phase i in solving some large scale qsdp - biq problems .",
    "note that from the numerical experiments conducted in @xcite , one can clearly conclude that qsdpnal - phase i ( a variant of sgs - ispadmm ) is the most efficient first - order algorithm for solving qsdp - biq problems with a large number of inequality constraints .",
    "even so , it can be observed from table [ table : biqi ] that is still faster than qsdpnal - phase i on most of the problems tested .",
    "+ & & & & & + & & & & & + & & & & & & +      next we test the following qsdp problem motivated from the sdp relaxation of a quadratic assignment problem ( qap ) considered in @xcite . the sdp relaxation we used is adopted from @xcite but we add a convex quadratic term in the objective to modify it into a qsdp problem . specifically , given the data matrices @xmath445 of a qap problem , the problem we test is given by : @xmath446    { \\rm s.t . } &   \\sum_{i=1}^l x^{ii } = i , \\",
    "\\inprod{i}{x^{ij } } = \\delta_{ij } \\quad \\forall \\ , 1\\leq i \\leq j\\leq l , \\\\[5pt ]    &    \\inprod{e}{x^{ij } } = 1\\quad \\forall\\ ,",
    "1\\leq i \\leq j\\leq l ,    \\quad x \\in \\cs^{n}_+,\\ ; x \\in \\ck ,    \\end{array }    \\label{p2:qsdp - qap}\\end{aligned}\\ ] ] where @xmath447 , and @xmath448 denotes the @xmath449-th block of @xmath450 when it is partitioned uniformly into an @xmath451 block matrix with each block having dimension @xmath451 .",
    "the convex set @xmath439 , @xmath420 is the matrix of ones , and @xmath452 if @xmath453 , and @xmath454 otherwise .",
    "note that here we use the same self - adjoint positive semidefinite linear operator @xmath440 constructed in . in our numerical experiments , the test instances @xmath455 are taken from the qap library @xcite .    in table",
    "[ table : qsdpnal ] , we present the detail numerical results for and qsdpnal - phase i in solving some large scale qsdp - qap problems . it is interesting to note that can solve all the @xmath456 difficult qsdp - qap problems to an accuracy of @xmath39 efficiently , while the purely first - order algorithm qsdpnal - phase i can only solve @xmath457 of the problems to required accuracy .",
    "the superior numerical performance of over qsdpnal - phase i clearly demonstrates the importance and necessity of our proposed two - phase algorithm for which second - order information is incorporated in the inexact proximal augmented lagrangian algorithm in phase ii .",
    "+ & & & & & + & & & & & + & & & & & & +      we also test the qsdp problems arising from the following sensor network localization problems with @xmath16 anchors and @xmath126 sensors : @xmath458 where the location of each anchor @xmath459 , @xmath460 is known , and the location of each sensor @xmath461 , @xmath462 , is to be determined .",
    "the distance measures @xmath463 and @xmath464 are known pair - wise distances between sensor - sensor pairs and sensor - anchor pairs , respectively .",
    "note that our model is a variant of the model studied in @xcite .",
    "let @xmath465\\in\\re^{d\\times l}$ ] be the position matrix that needs to be determined .",
    "we know that @xmath466^t[u",
    "\\ i_d]a_{ik},\\ ] ] where @xmath467 and @xmath468 $ ] . here",
    ", @xmath469 is the @xmath470th unit vector in @xmath471 , and @xmath472 is the @xmath473 identity matrix . let @xmath474 for @xmath475 , @xmath476 $ ] for @xmath477 , and @xmath478\\in \\cs^{(d+l)\\times(d+l)}.\\ ] ] following the same approach in @xcite , we can obtain the following qsdp relaxation model with regularization term for @xmath479    { \\rm s.t . } &   g_{ik}^t x g_{ik } = d_{ik}^2,\\ , ( i , k)\\in\\cm , \\quad x\\succeq 0 ,    \\end{array}\\ ] ] where @xmath480 is a given positive regularzation parameter and @xmath481 $ ] with @xmath482 and @xmath483 . here",
    "@xmath415 is the vector of all ones .",
    "the test examples are generated in the following way .",
    "we first randomly generate @xmath126 points @xmath484 in @xmath485^d$ ] .",
    "then , the edge set @xmath486 is generated by considering only pairs of points that have distances less than a given positive number @xmath487 , i.e. , @xmath488 given @xmath16 anchors @xmath489 , the edge set @xmath490 is similarly given by @xmath491 we also assume that the distances @xmath492 are perturbed by random noises @xmath493 as follows : @xmath494 where @xmath495 is the true distance between point @xmath470 and @xmath496 , @xmath493 are assumed to be independent standard normal random variables , @xmath497 is the noise parameter . for the numerical experiments , we generate @xmath498 instances where the number of sensors @xmath126 ranges from 250 to 1500 and the dimension @xmath499 is set to be 2 or 3 .",
    "w set the noise factor @xmath500 .",
    "the 4 anchors for the two dimensional case ( @xmath501 ) are placed at @xmath502 and the positions of the anchors for the three dimensional case ( @xmath503 ) are given by @xmath504         + & & & & & + & & & & & + & & & & & & +    let @xmath505 be the set of neighbors of the @xmath470th sensor . to further test our algorithm qsdpnal , we also generate the following valid inequalities and add them to problem @xmath506 where @xmath507 then , we obtain the following qsdp relaxation model @xmath508    { \\rm s.t . } &   g_{ik}^t x g_{ik } = d_{ik}^2,\\ , ( i , k)\\in\\cm , \\\\[5pt ]      & g_{ij}^t x g_{ij } \\ge r^2,\\ , ( i , j)\\in\\widehat\\cn,\\quad x\\succeq 0 .    \\end{array}\\ ] ]    in table [ table : snl ] and [ table : snli ] , we present the detail numerical results for and qsdpnal - phase i in solving some instances of problem and , respectively . clearly , outperforms the purely first - order algorithm qsdpnal - phase i by a significant margin",
    ". this superior numerical performance of over qsdpnal - phase i again demonstrates the importance and necessity of our proposed two - phase framework .",
    "+ & & & & & + & & & & & + & & & & & & +",
    "in this paper , we have introduced an inexact sgs decomposition technique , based on which we design a two - phase proximal augmented lagrangian method for solving large scale convex quadratic semidefinite programming problems .",
    "the global convergence and local convergent rate analyses of our algorithm are based on the classic results of proximal point algorithms @xcite , together with the recent advances in second order variational analysis of the convex composite quadratic semidefinite programming @xcite . by devising `` smart '' numerical linear algebra ,",
    "we have overcome various challenging numerical difficulties encountered in the implementation of qsdpnal .",
    "numerical experiments on various large scale qsdps have demonstrated the efficiency and robustness of our proposed two - phase framework in obtaining accurate solutions . specifically , for problems of moderate difficulty , our qsdpnal - phase i is already powerful enough and it is not necessary to execute qsdpnal - phase ii . on the other hand , for more difficult problems",
    ", the purely first - order qsdpnal - phase i algorithm may stagnant because of slow local convergence .",
    "in contrast , for qsdpnal ( which has second order information wisely incorporated ) , it can still obtain highly accurate solutions efficiently ."
  ],
  "abstract_text": [
    "<S> in this paper , we present a two - phase proximal augmented lagrangian method , called qsdpnal , for solving convex quadratic semidefinite programming ( qsdp ) problems with constraints consisting of a large number of linear equality , inequality constraints , a simple convex polyhedral set constraint , and a positive semidefinite cone constraint . as the cornerstone of qsdpnal , we first introduce the powerful and elegant inexact symmetric gauss - seidel ( sgs ) decomposition technique for solving a convex minimization problem whose objective is the sum of a multi - block quadratic function and a non - smooth function involving only the first block . </S>",
    "<S> then , a first order algorithm which relies on our inexact sgs decomposition technique is developed in qsdpnal - phase i with the aim of solving a qsdp problem to moderate accuracy or using it to generate a reasonably good initial point . in qsdpnal - phase ii </S>",
    "<S> , we design a proximal augmented lagrangian method ( alm ) where the inner subproblem in each iteration is solved via the inexact accelerated block coordinate descent ( abcd ) method [ arxiv preprint arxiv:1505.04278 , ( 2015 ) ] , which again relies on our inexact sgs decomposition technique , together with the novel incorporation of the semismooth newton - cg algorithm . moreover , under suitable conditions , we are able to analyze the rate of convergence of the proposed algorithm . in the implementation of qsdpnal , we also develop special techniques for solving large scale linear systems of equations under certain subspace constraints . </S>",
    "<S> more specifically , simpler and yet better conditioned linear systems are carefully designed to replace the original linear systems and innovative shadow sequences are constructed to alleviate the numerical difficulties brought about by the crucial subspace constraints . </S>",
    "<S> extensive numerical results for various large scale qsdps show that our two - phase framework is not only fast but also robust in obtaining accurate solutions .    </S>",
    "<S> * keywords : * convex quadratic semidefinite programming , symmetric gauss - seidel , augmented lagrangian , semismooth newton - cg method    * ams subject classifications : * 90c06 , 90c20 , 90c22 , 90c25 , 65f10 </S>"
  ]
}