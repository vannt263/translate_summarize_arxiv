{
  "article_text": [
    "the use of interferometric techniques in radio astronomy allow astronomers to achieve extraordinary angular resolution without having to build large monolithic antennas .",
    "interferometric radio telescope arrays create a virtual antenna of large diameter by placing many relatively small antennas over a large area ( anywhere from 100s of meters to 1000s of kilometers ) .",
    "interferometric radio telescopes work by correlating the signals between antennas in the array to form complex - valued correlation products , or ` visibilities ' , which are accumulated over a short time then stored for subsequent calibration and image processing .",
    "the visibilities are discrete samples of the fourier transform of the sky intensity as seen by each antenna in the array .",
    "an image of the sky can be made by placing the calibrated visibilities onto a grid , fourier transforming the grid then deconvolving the resulting image .",
    "more details of the fundamentals of radio interferometry can be found in one of the many texts on the subject ( e.g. * ? ? ?",
    "the murchison widefield array ( mwa ) is a new low - frequency radio telescope that is currently under construction at the murchison radio observatory in western australia .",
    "the full array will consist of 512 antennas spread over approximately 1 km .",
    "the array has some ambitious science goals including measurement of the power spectrum from the epoch of reionization ( eor ) in the early universe , measurement of properties of the heliosphere and discovery and monitoring of transient and variable radio sources @xcite .    during 2008 ,",
    "a 32 antenna prototype system was deployed in western australia .",
    "the full mwa will have a dedicated hardware correlator , however to enable interferometry for the prototype system in real - time , a gpu - based software correlator was developed .    in this paper , we describe the gpu - based software correlation system that was deployed with the 32 antenna prototype of the mwa . in  [ sec : corr ] we provide a brief overview of the requirements for radio telescope array correlators .",
    "we describe the prototype system and the data capture hardware in  [",
    "sec : mwa32 t ] . in ",
    "[ sec : gpu_corr ] , we review the basic design requirements and issues for an fx correlator system on a gpu . in  [ sec : perf ] , we list the performance of the gpu correlator for the prototype system and compare to a conventional cpu - based correlator . in  [ sec : discussion ] we discuss the performance of the correlator designs in terms of the theoretical peak performance and their utilisation of gpu resources .",
    "we also discuss some potential applications for similar signal processing tasks in radio astronomy .",
    "radio telescope correlators form visibilities from nyquist - sampled band - limited signals received from each antenna .",
    "signals from all antenna pair combinations are correlated and integrated over some time , each forming a complex - valued correlation coefficient , or visibility .    for a correlator with @xmath0 input signals",
    "there are @xmath1 cross - correlation products ( between different input signals ) and @xmath0 auto - correlation products , which is the correlation of a signal with itself .",
    "the total number of correlation products for @xmath0 inputs , including auto - correlations , is @xmath2 .",
    "all modern correlators used in radio astronomy are digital .",
    "once a band - limited signal is digitized , correlation becomes a signal processing problem and there is some flexibility in the choice of hardware and software that performs the task .",
    "historically , large radio telescope facilities have custom - built hardware correlators based on asics or fpgas .",
    "more recently , software based correlation systems built on clusters of commodity computers have been used for modest bandwidth applications , e.g. at the gmrt @xcite , or for processing of vlbi data @xcite .",
    "such systems offer the benefits of using commodity computing hardware , networking and high - level programming languages .",
    "astronomers also often implement a spectrometer within the correlator to study the details of the spectrum contained within the sampled band . in the mwa application ,",
    "fine frequency resolution is required for the implementation of delay and phase calibration of the array , as discussed below .",
    "the correlator is therefore required to divide the input bandwidth into consecutive sub - bands , or frequency ` channels ' , and form the correlation between inputs for each channel .",
    "software correlation systems are well suited to the so - called ` fx ' correlator design .",
    "conceptually , an fx correlator takes batches of samples from two antennas , fourier transforms ( ` f ' ) the signals into a frequency spectrum then then does a channel - by - channel multiplication ( ` x ' ) of the first spectrum with the complex conjugate of the second to form the correlation product , the complex - valued cross - power spectrum .",
    "the results of the cross - multiplications are accumulated for each baseline .",
    "the samples input to the correlator may be real or complex depending on the data capture system and any pre - processing .",
    "for real - valued samples , @xmath3 samples are transformed into @xmath4 spectral channels , including the dc term .",
    "for complex valued samples , @xmath5 samples are transformed into @xmath5 spectral channels . as noted later , the mwa receiving system produces complex - valued samples , which will be assumed henceforth .",
    "the cross - multiply is performed for all antenna - pair combinations , hence the number of operations in an fx correlator is dominated by the ` x ' stage when the number of antennas is large .",
    "specifically , the number of operations to perform the fft is @xmath6 _ per sample _ , which is effectively constant , whereas the number of operations to perform the cross - multiply is @xmath7 per sample . since the number of samples to process per second scales as @xmath8 where @xmath9 is the bandwidth of the data",
    ", an fx correlator must perform @xmath10 operations per second .",
    "many correlators are also required to apply a delay to the inputs from the antennas to compensate for the geometric delay between antennas .",
    "the delay depends on the direction on the sky that the telescope is pointing , which can add significant additional complexity at the input to the correlator . for the mwa , the delay correction is applied post correlation as a phase correction without loss of sensitivity .",
    "this can be done because the mwa s baselines are short enough and the correlated spectral resolution is fine enough that the phase gradient within a channel due to geometric delay is negligible .",
    "this simplifies the requirements of the correlator .",
    "the mwa prototype consists of 32 antennas placed in an approximate reuleaux triangle configuration with maximum separation between antennas approximately 350 meters .",
    "each antenna comprises 16 dual linear polarization dipoles operating as a phased array .",
    "the signals from the dipoles are combined in an analog beamformer , that electronically steers the antennas to the desired pointing direction by applying the appropriate delay to each dipole .",
    "each produces one analog signal for each polarization .",
    "signals from groups of eight antennas are fed into a receiver node .",
    "the receiver performs several tasks , including band - limiting and adjusting the power levels of the signals @xcite .",
    "the receivers internally use a polyphase filterbank to create 256 output channels , covering the frequency spectrum between 1.28 mhz and 327.68 mhz , each having 1.28 mhz bandwidth .",
    "complex valued samples from each of these channels are produced at 1.28 msamples / second , which is the nyquist rate for a 1.28 mhz band . in the prototype system ,",
    "one channel is input to the correlator .    for the prototype ,",
    "there are four ` receiver ' units , each servicing eight antennas .",
    "each receiver feeds data to a pc - based data capture and recording system @xcite .",
    "the receivers are locked to a common 655.36 mhz clock so that samples recorded on each data capture pc are from the same instant , preserving interferometric phase across the array .",
    "the recorded samples are represented by 5 bits each for the real and imaginary part of the number in twos - complement format . for convenience in the prototyping effort ,",
    "these 10-bit samples were stored as 16-bit words .",
    "the format of the recorded data is in groups of 17 2-byte words , consisting of a marker / counter word followed by 16 data words , one for each input to the receiver ( 8 antennas @xmath11 2 polarizations ) .",
    "the net data rate per capture pc is 43.52 mb / s .",
    "although the data recording systems are running on independent pcs , the captured samples are explicitly synchronized by the common clock in the receivers .",
    "a relatively simple data aggregation system is used to stream the data to a common pc with a gpu for correlation .",
    "this aggregation system is not synchronized ; it merely needs to preserve the number and ordering of samples , which is achieved using standard tcp / ip networking .",
    "the aggregation system removes the marker words and interleaves the four streams so that the 64 samples for the same time instant are grouped together for input to the correlator .",
    "we implemented the correlator using nvidia gpu hardware and cuda programming environment .",
    "there are three basic steps in the correlation process : + 1- unpacking a block of data , which involves reordering samples as discussed below , + 2- fourier transforming into frequency space , and + 3- performing the cross - multiplication of spectra and adding the results into accumulators .",
    "the correlator is configured to receive input streams from @xmath12 inputs ( where here @xmath13 polarizations @xmath11 32 antennas ) , and form correlation products with @xmath14 spectral channels .",
    "the basic data flow is shown in fig .",
    "[ fig : fx ] .",
    "code in cuda is written in c , with some language - specific extensions .",
    "optimal use of a gpu requires some understanding of how the gpu operates at a relatively low level and for the software design to exploit multi - threaded simd - like hardware .",
    "we briefly review the most important concepts here .",
    "code on an nvidia gpu is executed by one of many parallel multiprocessors , each of which contains eight compute cores .",
    "a multiprocessor runs code in blocks of threads , each thread ideally executes the same instructions on different data .",
    "gpus have global memory and registers , similar to conventional computers .",
    "registers are fast to access but are only available within a thread .",
    "global memory can be accessed by all threads but has high latency and there is no cache for general global memory .",
    "some data may also be shared between the threads through the use of _ shared memory_. gpu shared memory does not have an analog on conventional cpus .",
    "it is like a register that can be accessed by a small group of threads .",
    "accessing shared memory is much faster than global memory .",
    "the gpu also has a small amount of read - only _ constant memory _ which is cached and fast to access .",
    "there are four main design considerations for efficient use of the gpu : + 1- coalesced global memory access . if consecutive threads in a thread block access consecutive locations in global memory , the memory operation will be coalesced into a single operation by the gpu memory management hardware .",
    "using non - coalesced operations can have a significant performance penalty .",
    "+ 2- using shared memory to reduce global memory operations .",
    "data that can be shared between threads need only be read once from global memory then is accessible to a group of threads through shared memory .",
    "+ 3- register and shared memory use .",
    "the total number of registers and shared memory required by a thread dictates how many blocks of threads can be run concurrently on the gpu .",
    "the fraction of the gpu compute resources actually used is the ` occupancy ' .",
    "+ 4- thread serialization .",
    "designs that use shared memory to reduce global memory access may require forced synchronisation points in the code to guarantee all required data have been loaded before continuing . this synchronisation can force groups of threads to wait , which reduces the efficiency of parallel processing .    unpacking the data requires taking a block of @xmath15 complex samples which are ordered with input varying most quickly , converting to floating - point format and putting the samples into @xmath12 arrays of length @xmath14 , with channel number varying most quickly ( fig [ fig : fx ] , left two columns ) . with the exception of converting to floating - point ,",
    "this is equivalent to a matrix transpose for the block of samples .",
    "this will force non - coalesced memory reads , although as we note later , the unpacking operation is a small fraction of the total processing time for the mwa prototype application even when using non - coalesced operations .",
    "the second step is performed by the built - in fft library in cuda . for the complex valued samples of the mwa ,",
    "the fft is @xmath14 channels in input and output ( second and third columns of fig .",
    "[ fig : fx ] ) .",
    "this results in coalesced memory operations for power - of - two transforms .",
    "the correlator also supports real - valued input samples . for real valued samples , @xmath16 samples",
    "are transformed into @xmath17 channels . to avoid having odd - sized array lengths for real - valued data and the associated memory coalescence problems",
    ", we also use complex - to - complex transform of size @xmath16 for real - valued samples and ignore the redundant half of the result .",
    "the third step performs the cross - multiplication and accumulation ( cmac ) .",
    "the correlation products for a given frequency channel can be represented in a matrix of size @xmath18 formed by the outer product of a vector with its complex conjugate as shown in fig [ fig : corr_matrix ] .",
    "the vector contains the samples ( in frequency space ) for all inputs in a channel for a single time instant .",
    "@xcite ( hereafter hhs08 ) examined various design options for optimizing gpu performance for the correlation task . here",
    ", we evaluate three design options on two generations of hardware . two of our designs are very similar to the ` pair parallel ' and ` group parallel ' approaches in hhs08 , and we reuse the names . a third design , not considered in hhs08",
    ", we call the ` hybrid group ' because its design has elements of the first two .    the _ pair parallel _ ( pp ) approach is conceptually the simplest . @xmath2 blocks of threads are created , with @xmath14 threads in each block .",
    "each thread is independent of all others and computes a single correlation product for a single channel and adds it into an accumulator .",
    "hence a block of threads computes all channels for one correlation product . for a large number of frequency channels , it is straightforward to divide @xmath14 into chunks that optimise the gpu occupancy ( e.g. 128 channels ) by creating @xmath19 thread blocks .",
    "this approach does not use shared memory .",
    "each correlation product requires two reads from global memory per channel and requires one accumulator per thread .",
    "our pair parallel design differs from hhs08 only in the number of thread blocks created .",
    "instead of creating @xmath20 threads per channel and using a condition to test for the redundant combinations , our design creates @xmath21 thread blocks .",
    "this design requires the threads to decode which input pair they correspond to . instead of using expensive modulo operations to decode the input pair , we pre - compute a lookup table for this task and load it into the gpu constant memory , which is cached and fast to access in our application because all threads in a thread block are reading the same data from the constant memory",
    "the _ group parallel _ ( gp ) approach aims to minimise the reads from global memory by sharing common data between a small group of threads , where @xmath22 is the size of the group .",
    "each block of threads runs @xmath23 ( where @xmath22 = 2 or 4 ) channels in the frequency direction and @xmath22 inputs in the @xmath24 direction .",
    "a total of @xmath25 thread blocks are created , with redundant input pairs doing no work .",
    "each thread loads one value from the @xmath24 axis , which is shared , then loads one value from the @xmath26 axis which is not shared .",
    "the thread then calculates the @xmath22 products along the @xmath24 axis and adds them to accumulators .",
    "this approach uses one complex number ( 8 bytes ) of shared memory per thread and @xmath22 accumulators per thread .",
    "the design makes @xmath27 reads from global memory to calculate @xmath28 products . because data are shared between threads , synchronisation is required .    the _ hybrid group _ ( hg ) approach aims to use shared memory to reduce the total number of global memory reads while maximising occupancy and avoiding thread synchronisation .",
    "in this approach , @xmath29 ( g=2 or 4 ) thread blocks are created , including a redundant half of the correlation matrix .",
    "thread blocks for the redundant half do no work and exit .",
    "each thread loads the value from the @xmath24 axis into shared memory , then loads @xmath22 successive values from the @xmath26 axis to calculate @xmath22 correlation products .",
    "@xmath30 reads are required to calculate @xmath22 products . because data are not shared between threads ,",
    "there is no need for thread synchronisation .",
    "this design simply uses shared memory as additional register space .    in all designs , consecutive threads process consecutive channels so all memory operations are coalesced .",
    "additionally , all designs transfer and process large batches of data per call to the gpu .",
    "this has the benefit that the correlation products can be accumulated in registers , thereby reducing the total number of fetch - add - store global memory accesses , and by reducing the overhead of making the gpu calls .    for the designs that use shared memory ,",
    "the choice of @xmath31 or @xmath32 is pragmatic .",
    "@xmath22 must divide evenly into @xmath0 hence @xmath22 must be even for our system . for @xmath33 , the designs require an increased number of registers that significantly reduces the gpu occupancy .",
    "the performance of the designs was tested on three different nvidia gpus , representing two generations of hardware with different capabilities , as shown in table [ tab : gpu_list ] .",
    ".gpu models used for performance testing .",
    "[ cols=\"<,<,<,<,<\",options=\"header \" , ]     [ tab : theoretical_perf ]    table [ tab : theoretical_perf ] shows the actual and theoretical bandwidth for the cmac operation that can be processed by the different gpus for a 64 input system , assuming the computation is memory bandwidth limited , for the different correlator designs .",
    "the actual performance is calculated using the execution times from table [ tab : perf ] , and therefore includes all the overhead of making the call to the gpu from the host system and within the gpu itself , but does not include the cost of transferring , unpacking or ffting the data .",
    "the designs that access global memory the least have the greatest theoretical bandwidth .",
    "the numbers in parentheses are the ratio of actual / theoretical signal bandwidth , expressed as a percentage , and represent how much time each design spends in global memory access operations .",
    "a small number indicates that the gpu is spending time _ not _ waiting for global memory , which means that it is doing arithmetic and hence is being efficiently utilized .",
    "the execution time of the pp design in table [ tab : theoretical_perf ] is close to the theoretical performance .",
    "this indicates that the basic pp design is memory bandwidth limited , with 2 global memory fetches per complex multiply and add , despite having the highest occupancy . by reducing the total load on global memory ,",
    "the hg and gp designs run significantly faster than the pp design .",
    "the hg @xmath34 and gp @xmath31 designs have a similar load on global memory with @xmath35 and @xmath36 fetches per cmac respectively .",
    "the small fractional improvement in execution time and lower performance relative to peak of the gp @xmath31 , between the two designs on the 8800 models , does not match the naive estimate of potential improvement from memory bandwidth alone .",
    "this indicates that thread synchronisation is forcing the multiprocessors to wait on the 8800 models , offsetting the reduced memory usage and increased multiprocessor occupancy of the gp design . on the c1060",
    ", however , the bottleneck appears to be eliminated with the hg @xmath34 and gp @xmath31 designs running at almost the same fraction of theoretical peak .",
    "the low memory throughput compared to theoretical for the gp @xmath34 design indicates that the cmac operation is no longer limited by memory bandwidth and is therefore spending much of the time performing the arithmetic .",
    "we conclude that this design is using the gpu processing resources optimally .      as a signal processing device ,",
    "gpus compare favorably to custom - built hardware both from the processing power and development time perspective .",
    "the development is more like traditional software engineering , with a c - like programming language , while the performance is more like custom hardware .",
    "there are limitations to the scalability of the correlator , however .",
    "the amount of work the correlator must perform per time interval scales as @xmath37 .",
    "the @xmath38 scaling with inputs is the limiting factor for radio arrays with large numbers of antennas . for a given bandwidth ,",
    "all of the cmac work could in principle be divided over several gpus .",
    "such a setup would require all of the input data to be streamed to all gpus and for all gpus to do all ffts .",
    "then the cmac would be divided between gpus . given the speed of modern pci - express buses relative to gigabit ethernet , this scheme",
    "would probably be limited by the number of gpus that can be attached to a single host .",
    "a potential improvement to the gpu correlator would be to use a polyphase filterbank ( pfb ) instead of an fft when forming the spectra from the time - series data .",
    "ffts have poor performance for narrowband signals that do not fall exactly in the center of a spectral channel , the resulting signal power is spread over several adjacent channels .",
    "this spectral leakage is undesirable for high signal - to - noise or high - precision experiments .",
    "using a pfb can greatly reduce spectral leakage , with the extra cost coming in the unpack stage of the correlator .",
    "we have evaluated several designs for a gpu - based radio astronomy correlator system using nvidia s hardware and cuda programming environment .",
    "the designs were compared for the mwa s 32 antenna prototype system which must correlate 64 data streams of 1.28mhz bandwidth into 128 spectral channels in real - time .",
    "we found the group parallel design , which uses the gpu shared memory to minimise the amount of global memory reads , was the optimal solution on three different models of gpu hardware .",
    "running on the current generation c1060 hardware , the gpu correlator runs approximately 68 times faster than a single - threaded cpu equivalent and leaves the host system resources free to perform the necessary streaming of data      we thank the hard work and dedication of the november 2008 site visit team : david basden , roger cappallo , mark derome , david herne , colin lonsdale , merv lynch , daniel mitchell , miguel morales , ed morgan , anish roshi , steven tingay , mark waterson , alan whitney , andrew williams and jamie stevens for remote support .",
    "we thank the team at raman research institute led by anish roshi for enabling the data capture system .",
    "the team includes srivani k s , prabu t. , deepak kumar , gopala krishna , kamini p. a. and madhavi s."
  ],
  "abstract_text": [
    "<S> modern graphics processing units ( gpus ) are inexpensive commodity hardware that offer tflop / s theoretical computing capacity . </S>",
    "<S> gpus are well suited to many compute - intensive tasks including digital signal processing .    </S>",
    "<S> we describe the implementation and performance of a gpu - based digital correlator for radio astronomy . </S>",
    "<S> the correlator is implemented using the nvidia cuda development environment . </S>",
    "<S> we evaluate three design options on two generations of nvidia hardware . </S>",
    "<S> the different designs utilize the internal registers , shared memory and multiprocessors in different ways . </S>",
    "<S> we find that optimal performance is achieved with the design that minimizes global memory reads on recent generations of hardware .    </S>",
    "<S> the gpu - based correlator outperforms a single - threaded cpu equivalent by a factor of 60 for a 32 antenna array , and runs on commodity pc hardware . the extra compute capability provided by </S>",
    "<S> the gpu maximises the correlation capability of a pc while retaining the fast development time associated with using standard hardware , networking and programming languages . in this way , a gpu - based correlation system represents a middle ground in design space between high performance , custom built hardware and pure cpu - based software correlation .    </S>",
    "<S> the correlator was deployed at the murchison widefield array 32 antenna prototype system where it ran in real - time for extended periods . </S>",
    "<S> we briefly describe the data capture , streaming and correlation system for the prototype array . </S>"
  ]
}