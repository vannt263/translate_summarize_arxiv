{
  "article_text": [
    "there are numerous new programming languages , libraries , and techniques that have been developed over the past few years to either simplify the process of developing parallel programs or provide additional functionality that traditional parallel programming techniques ( such as mpi@xcite or openmp@xcite ) do not provide .",
    "these include programming extensions such as co - array fortran@xcite ( caf ) , upc@xcite , and new languages such as chapel@xcite , openmpd@xcite or xmp@xcite    however , these approaches often have not has a focus of optimising the parallelisation overheads ( the cost of the communications ) associated with distributed memory parallelisation , and have limited appeal for the many scientific applications which are already parallelised with an existing parallelisation approach ( primarily mpi ) and have very large code bases which would be prohibitively expensive to re - implement in a new parallel programming language .",
    "the challenge of optimising parallel communications is becoming increasingly important as we approach exascale - type high performance computers ( hpc ) , where it is looking increasingly likely the that ratio between the computational power of a node of the computer and the relative performance of the network is going to make communications increasingly expensive when compared to the cost of calculations .",
    "furthermore , the rise of multi - core and many - core computing on the desktop , and the related drop in single core performance , means that many more developers are going to need to exploit parallel programming to utilise the computational resources they have access to where they could have relied on increases in serial performance of the hardware they were using to maintain program performance in the past .",
    "therefore , we have devised a new parallel programming approach , called managed data message passing ( mdmp)@xcite , which is based on the mpi library but provides a new method for parallelising programs .",
    "mdmp follows the directives based approach , favoured by openmp and other parallel programming techniques , which are translated into mdmp library function calls or code snippets , which in turn utilise communication library calls ( such as mpi ) to provide the actual parallel communication functionality . using a directives based approach enables us to reduce the complexity , and therefore development code , of writing parallel programs , especially for the novice hpc programmer . however , the novel aspect of mdmp is that it allows users to specify the communication patterns required in the program but devolves the responsibility for scheduling and carrying out the communications to the mdmp functionality .",
    "mdmp instruments data accesses for data being communicated to optimise when communications happen and therefore better overlap communication and computation than is easily possible with traditional mpi programming .",
    "furthermore , by taking the directive approach mdmp can be incrementally added to a program that is already parallelised with mpi , replacing or extending parts of the existing mpi parallelisation without requiring any changes to the rest of the code .",
    "users can start by replacing one part of the current communication in the code , evaluate the performance impacts , and replace further communications as required .    in this paper",
    "we outline work others have undertaken in creating new parallel programming techniques , and optimisation communications .",
    "we describe the basic issues we are looking to tackle with mdmp , and go on to describe the basic feature and functionality of mdmp , outlining the performance benefits and costs of such an approach , and highlighting the scenarios where mdmp can provide reduced communication costs for the types of communication patterns seen in some scientific simulation codes using our prototype implementation of mdmp ( which implements mdmp as library calls rather than directives ) .",
    "recent evaluation of the common programming languages used in large scale parallel simulation code has found the majority are still implemented using mpi , with a minority also including a hybrid parallelisation through the addition of openmp ( or in a small number of cases shmem ) alongside the mpi functionality@xcite .",
    "this highlights to us the key requirement for any new parallel programming language or technique of being easily integrated with existing mpi - based parallel codes .",
    "there are a wide range of studies evaluating the performance of a range of different parallel languages , including partitioned global address space ( pgas ) languages , on different application domains and hardware@xcite .",
    "these show that there are many approaches that can provide performance improvements for parallel programs , compared to standard parallelisation techniques on a given architecture or set of architectures .",
    "existing parallel programming languages or models for distributed memory system provide various features to describe parallel programs and to execute them efficiently .",
    "for instance , xmp provides features similar to both caf and hpf@xcite , allowing users to use either global or local view programming models , and providing easy to program functionality through the use of compiler directives for parallel functionality .",
    "likewise , openmpd provided easy to program directives based parallelisation for message passing functionality , extending an openmp like approach to a distributed memory supercomputer",
    ".    however , both of these approaches generally require the re - writing of existing codes , or parts of existing codes , into a new languages , which we argue is prohibitively expensive for most existing computational simulation applications and therefore has limited the take - up of these different parallel programming languages or techniques by end user applications . furthermore ,",
    "both only target parts of the problem we are aiming to tackle , namely improving programmability and optimising performance .",
    "xmp and openmpd both aim to make parallel programming simpler , but have not direct features for optimising communications in the program ( although they can enable users to implement different communication methods and therefore choose the most efficient method for themselves ) . pgas languages , and other new languages , may provide lower cost communications or new models of communications to enable different algorithms to be used for a given problem , or may provide simpler programming model , but none seems to offer both as a solution for parallel programming . also , crucially , they do not expect to work with existing mpi programs , negating the proposed benefits for the largest part of current hpc usage",
    ".    there has also been significant work undertaken looking at optimising communications in mpi programs .",
    "a number of authors have looked at compiler based optimisations to provide automatic overlapping of communications and computation in existing parallel programs@xcite .",
    "these approaches have shown that performance improvements can be obtained , generally evaluated against kernel benchmarks such as the nas parallel benchmarks , by transforming user specified blocking communication code to non - blocking communication functionality , and using static compiler analysis to determine where the communications can be started and finished . furthermore , other authors have looked at communication patterns or models in mpi based parallel programs and suggested code transformations that could be undertaken to improve communication and computation overlap@xcite .",
    "however , these approaches are what we would class as _ coarse - grained _ communication optimisation .",
    "they use only static compiler analysis to identify the communication patterns , and identify the outer bounds of where communications can occur to try and start and finish bulk non - blocking operations in the optimal places .",
    "they do not address the fundamental separation of communication and computation into different phases that such codes generally employ .",
    "our work , outlined in this paper , is looking at _ fine - grained _ communication optimisations , where individual communication calls are _ intermingled _ with computation to truly mix communication and computation .",
    "the has also been work on both offline and runtime identification and optimisation of mpi communications , primarily for collective communication@xcite , or other auto - tuning techniques such optimising mpi library variables@xcite or individual library routines@xcite .",
    "all these approaches have informed the way we have constructed mdmp .",
    "we belief that the work we have undertaken is unique as it brings together attempts to provide simple message passing programming which fine - grained communication optimisation along with the potential for runtime auto - tuning of communication patterns into a single parallel programming tool .",
    "whilst there is a very wide range of communication and computational patterns in parallel programs , a large proportion of common parallel applications use regular domain decomposition techniques coupled with _ halo _",
    "communications to exploit parallel resources . as shown in figure[fig : commpattern ] , which is a representation of a jacobi - style stencil based simulation method , many simulations undertake a set of calculations that iterate over a n - dimensional array , with a set of communications to neighbouring processes every iteration of the simulation .",
    "[ fig : commpattern ]    the core computational kernel of a simple jacobi style simulation , as illustrated in the previous paragraph , can be implemented as shown in figure [ code : original ] ( undertaking a 2d simulation ) .    .... for ( iter=1;iter<=maxiter ; iter++ ) {           mpi_irecv(&old[0][1 ] , np , mpi_float , prev , 1 ,         mpi_comm_world , & requests[0 ] ) ;     mpi_irecv(&old[mp+1][1 ] , np , mpi_float , next , 2 ,         mpi_comm_world , & requests[1 ] ) ;           mpi_isend(&old[mp][1 ] , np , mpi_float , next , 1 ,         mpi_comm_world , & requests[2 ] ) ;     mpi_isend(&old[1][1 ] , np , mpi_float , prev , 2 ,         mpi_comm_world , & requests[3 ] ) ;           mpi_waitall(4 , requests , statuses ) ;           for ( i=1;i < mp+1;i++ ) {        for ( j=1;j < np+1;j++ ) {           new[i][j]=0.25*(old[i-1][j]+old[i+1][j]+          old[i][j-1]+old[i][j+1 ] - edge[i][j ] ) ;        }     }     for ( i=1;i < mp+1;i++ ) {        for ( j=1;j < np+1;j++ ) {           old[i][j]=new[i][j ] ;        }     } } ....    [ code : original ]    it is evident from the above code that , whilst it has been optimised to use non - blocking communications , the communication and computation parts of the simulation are performed separately , with no opportunity to overlap communications and computations . in practice",
    "this means that the application will only be using the communication network to send and receive data in short bursts , leaving it idle whilst computation is being performed .",
    "many large scale hpc resources are used by a large number of running applications at any one time , which may help to ensure that the overall usage of the interconnect is high , even though individual applications often utilise it in a _ bursty",
    "however , that still will not be true of the part of the network dedicated to the individual application , only to the load of the network overall .",
    "furthermore , when considering the very largest hpc resources in the world , and including the proposed exascale resources , there are often only a handful of applications utilising the resource at any one time .",
    "therefore , enabling applications to effectively utilise the network , especially the _ spare _ resources that the current separated communication and computation patterns engender , is likely to be beneficial to overall application performance and resource utilisation ( provided that the cost of doing this is not significant ) .",
    "it is possible to split the sends and receives in the previous example and place them around the computation rather than just before the computation , using the non - blocking functionality , to further ensure that more optimal communications are occurring .",
    "however , this is still does not allow for overlapping communication and computation because the computational is still occurring in a single block , with communications outside this block .",
    "for developers to ensure that communications and computations are truly mixed would require further code modifications , as shown in the code example in figure [ code : optmpi ] ( which implements a strategy of sending data as soon as it has been computed ) .    .... for ( iter=1;iter<=maxiter ; iter++ ) {      requestnum = 0 ;      for ( j=0;j < np;j++ ) {          mpi_irecv(&tempprev[j ] , 1 , mpi_float , prev , 1 ,              mpi_comm_world , & requests[requestnum ] ) ;          requestnum++ ;          mpi_irecv(&tempnext[j ] , 1 , mpi_float , next , 2 ,              mpi_comm_world , & requests[requestnum ] ) ;          requestnum++ ;      }      for ( i=1;i <",
    "mp+1;i++ ) {          for ( j=1;j <",
    "np+1;j++ ) {              new[i][j]=0.25*(old[i-1][j]+old[i+1][j]+                 old[i][j-1]+old[i][j+1 ] - edge[i][j ] ) ;              if(i = = mp ) {                 mpi_isend(&new[i][j ] , 1 , mpi_float , next , 1 ,                    mpi_comm_world , & requests[requestnum ] ) ;                 requestnum++ ;              } else if(i = = 1 ) {                  mpi_isend(&new[i][j ] , 1 , mpi_float , prev , 2 ,                     mpi_comm_world , & requests[requestnum ] ) ;                  requestnum++ ;              }          }      }      for ( i=1;i <",
    "mp+1;i++ ) {          for ( j=1;j < np+1;j++ ) {              old[i][j]=new[i][j ] ;          }      }      mpi_waitall(requestnum , requests , statuses ) ;      if(prev ! = mpi_proc_null ) {         for ( j=1;j < np+1;j++ ) {             old[0][j ] = tempprev[j-1 ] ;             old[mp+1][j ] = tempnext[j-1 ] ;                  }      }      if(next !",
    "= mpi_proc_null ) {         for ( j=1;j < np+1;j++ ) {             old[mp+1][j ] = tempnext[j-1 ] ;                  }      } } ....    [ code : optmpi ]    whilst the code implemented in figure [ code : optmpi ] will enable the mixing of communication and computation , ensuring that data is sent as soon as it is ready to be communicated and potentially ensuring better utilisation of the communication network , it has come at the cost of considerable code _ mutilation _",
    ", requiring developers to undertake significant code optimisations . as well as the damage to the readability and maintainability of the code that this causes , it also means that a code has been significantly changed for a potentially architecturally dependent optimisation , i.e. an optimisation that may be beneficial on one or more current hpc systems but may not be beneficial on other or future hpc systems .",
    "we are proposing mdmp as a mechanism for implementing such optimisations without the requirement to significantly change users codes , or the need to tailor codes to a specific platform , as the mdmp functionality can implement communications in the most optimal form for the application and hardware currently being used .",
    "to re - iterate the challenges for mdmp that we have previously discussed , mdmp is designed to address the following issues :    * work with existing mpi based codes * provide framework for optimisation communications * simplify parallel development    mdmp uses a directives based approach , relying on the compiler to implement the actual message passing functionality based on the users instructions .",
    "compiler directives are used , primarily , to address the third point above , namely ease of use .",
    "we provide functionality that can be easily enabled and disabled in an application , hides some of the complexities of current mpi programming ( such as providing message tags , error variables , communicators , etc ... ) that often complicate development for new users of mpi , and also provides some flexibility to the user over the type and level of message optimisation used .",
    "the mdmp directives are translated into code snippets and library calls by the mdmp - enabled compiler , either directly in the equivalent non - blocking mpi calls ( which simply mimics the communication that would have been implemented directly by the user ) or to further optimised mpi communications or other another communication library as appropriate on the particular hardware being used .",
    "this enables mdmp to target different communication libraries transparently to the developer for a given hpc system .",
    "also , crucially the ability to target mpi communications means that mdmp functionality can be added to existing mpi - parallelised programs , either as additional functionality or to replace existing mpi functionality , without requiring the program to be completely changed into a new programming language or utilise a new message - passing ( or other ) communication library .    however , simply using directives for programming message passing will not optimise the communication that are undertaken by a program .",
    "therefore , mdmp provides not only directives to specify the communications to be undertaken in the program but also directives to specify _",
    "communication regions_. communication regions define the areas of code where the data that is to be sent and received is worked on , and where communications occur , so that mdmp can , at runtime , examine the data access patterns and undertake communications at the optimal time to intermingle communications and computations and therefore better utilise the communication network .",
    "the optimisation of communications is based on runtime functionality that monitors the reads and writes of data that has been specified as communication data ( data that will be sent or received ) . as any data monitoring entails some runtime overheads the communication region specifies the scope of the data monitoring to ensure it",
    "is only performed where required ( i.e. where communications are occurring ) .",
    "any data that is specified by the users and being involved in send or receives is tracked so each read and writing in a communication region is recorded and the number of reads and writes that have occurred when the send or receive happens is evaluated .",
    "this data , the number of reads and writes that have occurred for a particular piece of data when it comes to be sent or written over by a receive , can then be used in any subsequent iterations of the computation to launch the communication of that data once it is ready to be communicated .",
    "communications are triggered for any given piece of data as follows :    * last write occurs ( sends ) * last read and/or write occurs ( receives )    using this functionality we can implement a communication pattern that intermingles communication and computation for the example code shown in figure [ code : original ] , as shown in figure [ code : mdmp ] .    .... # pragma commregion for ( iter=1;iter<=maxiter ; iter++ ) {        # pragma recv(old[0][0 ] , np , prev ) # pragma recv(old[mp+1][1 ] , np , next ) # pragma send(old[mp][1 ] , np , next ) # pragma send(old[1][1 ] , np , prev )     for ( i=1;i < mp+1;i++ ) {         for ( j=1;j < np+1;j++ ) {             new[i][j]=0.25*(old[i-1][j]+old[i+1][j]+                old[i][j-1]+old[i][j+1 ] - edge[i][j ] ) ;         }     }     for ( i=1;i < mp+1;i++ ) {         for ( j=1;j < np+1;j++ ) {             old[i][j]=new[i][j ] ;         }     } } # pragma commregionfinished    ....    [ code : mdmp ]    when compiled with an mdmp - enabled compiled , the code in figure [ code : mdmp ] will be processed by the compiler and non - blocking sends and receives inserted where the ` send ` and ` recv ` directives are placed .",
    "the compile then looks through the code associated with the communicating region ( between ` commregion ` and ` commregionfinished ` ) and replaces any variable reads or writes linked to those sends and receives by mdmp code which will perform the reads and writes and also record those reads and writes occurring",
    ".    compiler based code analysis for data accesses will be straightforward for many applications , however we recognise that there will be a number of scenarios , such as when pointers are heavily used in c or fortran , or possibly where pre - processing or function pointers or conditional function calls are used , where it will not be possible for the compiler to access where the data accesses for a particular ` send ` or ` recv ` occur . in that situation mdmp will revert to simply inserting the basic mpi function calls required to undertake the specified communication and not perform the optimise message functionality . whilst this negates the possibility of optimising the communications , it will not add any overheads to the program compared to the standard mpi performance a developer would experience , and it does still leave scope for the mdmp functionality to target communication libraries other than mpi to enable optimisation for users would requiring them to modify their code , if such functionality is available .",
    "furthermore , whilst we are not investigating such functionality at the moment , the design of mdmp means that it can also undertake auto - tuning or other runtime activities to optimise communication performance for users beyond the intermingling communication optimisations we have already discussed . for instance , mdmp could implement additional helper threads that enable progression of communications whilst the main program is undertaking calculations , albeit at the cost of utilising a computational core for that purpose .",
    "it could also evaluate different communication optimisations at runtime to auto - tune the performance of the program whilst it is running .    a difference between the mpi functionality that a developer would add to a code like the one we have been considering and the functionality that mdmp implements is that where intermingling of communications is undertaken mdmp will be sending lots of single element ( or small numbers of elements ) messages between processes rather than a single message with all the data in it . in general",
    ", mpi performs best when small numbers of large messages are used , rather than large numbers of small messages .",
    "this is because in the case that large numbers of small message are sent the communication costs are dominated by the latency costs of each message , whereas using small numbers of large messages reduces the overall number of message latencies that are incurred .",
    "we recognise the fact the the mdmp functionality may not be optimal in terms of the overall message costs associated with communications but we are assuming that this penalty will be negated by the benefits associated with more consistent use of the network and less concentrated nature of the communication and computation patterns .",
    "however , this is something that is investigated in our performance analysis of mdmp , and as with other previously discussed potential problems with mdmp if it is impacting performance the optimised message passing can be disabled at compile time .",
    "we also recognise that mdmp functionality does not come without a cost to the performance of the program .",
    "mdmp is adding additional computational requirements above those specified in the user program , and also requires additional memory to store data associated with the communications ( such as the counters that record the reads and writes to variables ) .",
    "the premise behind the optimised message passing functionality we are aiming for is that communications are much more expensive than computations for an application on a modern hpc machine , and this relationship is likely to get worse for future hpc machines .",
    "if this is the case then adding additional computational requirements can be acceptable provided the communication costs are reduced through this addition of extra computation .",
    "we have evaluated the performance impact of mdmp , and the communication verses computation trade - off on current hpc architectures where mdmp becomes beneficial , through benchmarking of our software , described in the next section .",
    "however , we are still working on minimising the memory requirements for mdmp , as this will be important to ensure mdmp is usage on current and future hpc systems .",
    "furthermore , we should remember that mdmp can be used as a simpler programming alternative to mpi with the optimised message passing functionality turned of at compile time , thereby removing all of these overheads if they are not beneficial for a given application of hpc platform .",
    "we evaluated the performance of the mdmp compared to standard c and mpi codes .",
    "we undertook our evaluation using a range of common large scale hpc platforms and a set of simple , _ kernel _ style , benchmarks .",
    "we have only evaluated the functionality using 2 nodes on each system , primarily testing the communications between a pair of communicating processors , one on each node .",
    "we used three different large scale hpc machines to benchmark performance .",
    "the first was a * cray xe 6 * , hector , is the uk national supercomputing service consists of 2816 nodes , each containing two 16-core 2.3 ghz _",
    "interlagos _ amd opteron processors per node , giving a total of 32 cores per node , with 1 gb of memory per core .",
    "this configuration provides a machine with 90,112 cores in total , 90 tb of main memory , and a peak performance of over 800 tflop / s .",
    "we used the pgi fortran compile on hector , compiled with the ` -fastsse ` optimisation flag .",
    "the seconds was a * bullx b510 * , helios , which is based on intel xeon processors .",
    "a node contains 2 intel xeon e5 - 2680 2.7 ghz processors giving 16-cores and 64 gb memory .",
    "helios is composed of 4410 nodes , providing a total of 70,560 cores and a peak performance of over 1.2 pflop / s .",
    "the network is built using infiniband qdr non - blocking technology and is arranged using a fat - tree topology .",
    "we used the intel fortran compiler on helios , compiling with the ` -o2 ` optimisation flag .",
    "the final resource was a * bluegene / q * , juqueen at forschungszentrum juelich .",
    "juqueen is a ibm bluegene / q system based on the ibm power architecture .",
    "there are 28 racks composed of 28,672 nodes giving a total of 458,752 compute cores and a peak performance of 5.9 pflop / s .",
    "each node has an ibm powerpc a2 processor running at 1.6 ghz and containing 16 smt cores , each capable of running 4 threads , and 16 gb of sdram - ddr3 memory .",
    "ibm s fortran compile , xlf90 , was used on juqueen , compiling using the ` -o2 ` optimisation flag .",
    "we have been evaluating mdmp functionality using a number of different benchmarks .",
    "initially we tested the performance impact of instrumenting data reads and writes on a non - communicating code , the streams @xcite benchmark , with the results presented in the first subsection below .",
    "after this we evaluated the communications performance of mdmp verses communication implemented directly with mpi , the results of these evaluations are in the second subsection below .",
    "each of the streams benchmark tests were repeated 10 times and an average runtime calculated . for the communications benchmarks each operation was run 100 times , and",
    "each benchmark as repeated 3 times with the average time taken .",
    "whilst we have , in previous sections in this paper , outlined the principles of mdmp and how it designed to work , we do not yet have a full compiler based implementation of this functionality .",
    "we have designed and implemented the runtime functionality that any compiler would added to a code when encountering mdmp pragmas , but have not yet implemented the compiler functionality .",
    "therefore , for this performance evaluation we are using benchmarks where the mdmp functionality has be implemented directly in the benchmark .",
    "we have implemented two versions of mdmp ; the first version implements all the required functionality within function calls to the mdmp library .",
    "this includes data stores and lookups for all the data marked as being communicated within an mdmp communicating region .",
    "the second version implements exactly the same functionality but uses pre - processor macros to insert the required code directly into the source code , thereby removing the need for function calls at every point mdmp is used . in the benchmark results this",
    "is named _ optimised mdmp _",
    "work is currently ongoing to implement a compiler based solution , utilising the llvm@xcite compiler infrastructure , to enable us to target all the main hpc computer languages with a single , full reference implementation .",
    "streams is often used to evaluate the memory bandwidth of computer hardware , and therefore was chosen as it will highlight any impact on the memory access and update efficiencies of computations when mdmp is added to a code .",
    "the performance of the streams benchmark was evaluated on all three of the hardware platforms we had available to us , although we are only presenting the results in the following tables from the cray xe6 because , although the performance of the benchmark varies between machines , the relative performance difference between the original implementation of streams and our mdmp implementations does not change significantly between these platforms .",
    "we are reporting the results from a single process running on an otherwise empty node on the cray xe6 .",
    "table [ tab : mdmpstreambenchcommregion ] ( where int stands for integer and db stands for double ) outlines the performance of the two versions of mdmp verses the original streams code when the fully communicating functionality of mdmp is enabled and we have forced the mdmp library to treat each variable in the arrays being processed as if they were being communicated ( i.e. we are fully tracking all the reads and writes to these array entries even though no communications are occurring ) .    [",
    "tab : mdmpstreambenchcommregion ]    we can see from table [ tab : mdmpstreambenchcommregion ] that the mdmp functionality does have a significant impact on the overall runtime of all the benchmarks , adding around an order of magnitude increase to the runtime of the benchmark .",
    "we would expect any benchmark like this , where no communications are involved and therefore there is no optimisation for mdmp to perform , to be detrimentally impacted by the additional functionality added in the mdmp implementation .",
    "however , we can see that the optimised implementation of mdmp does not have as significant an impact as the original mdmp implementation . as",
    "this is the first optimisation we have done to the mdmp functionality we are hopeful there is further scope for optimising the performance of mdmp and reducing the computational impact of the mdmp functionality .",
    "furthermore , it is worth re - iterating that in this benchmark we are forcing mdmp to mark and track all the data in the streams benchmark as if it is to be communicated .",
    "mdmp is not designed to be beneficial for scenarios such as this , it is primarily designed to be useful in scenarios where a small amount of data ( compared to the overall amount computed upon ) is sent each iteration . in a scenario such as the one this benchmark mimics ( where all the data used in the computation is also communicated ) mdmp should simply compiled down to the basic mpi calls as they would be more efficient in this scenario .",
    "mdmp is designed to enable users to try different communication strategies , such as simply using the plain mpi calls , or trying to very the amount communication and computation intermingling , which enables users to experiment and evaluate which will give them the best performance for their application and use case .",
    "indeed , such functionality could also be built into the runtime of mdmp , enabling auto - tuning of the choice of communication optimisation on the fly .",
    "we also ran the same benchmark with the code marked as outside a communicating region . in this scenario , whilst the mdmp functionality has be enabled for all the variables in the calculation , the absence of a communicating region disables , at runtime , any data tracking associated with the variables .",
    "table [ tab : mdmpstreambenchnocommregion ] presents the results for this benchmark .",
    "we can see that the cost of the mdmp functionality has been substantially reduce , and indeed if we used the optimised functionality where the mdmp function calls have been removed and replaced with pre - processed code the mdmp performance is extremely close to the plain benchmark codes performance .",
    "this confirms that the mdmp functionality can be constructed in such a way as not to have a significant adverse impact on the key computational kernels of a code outside the places that communications are occurring .",
    "[ tab : mdmpstreambenchnocommregion ]    however , we can see from the results that if communications are present there is a significant performance impact on the data that is tracked by the mdmp functionality .",
    "our assumption is that the computational cost associated by mdmp can be more than offset by the reduction in communication costs for a program , but clearly this is dependent on the ratio between communications and computations for a given kernel , and the ratio of relative costs ( in terms of overall runtime ) of a communication verses a computation .",
    "we evaluate the performance impact verses the communication cost savings in the next subsection , where we analyse some communication benchmarks .      we have constructed four simple benchmarks to evaluate mdmp against mpi .",
    "the first is a * pingpong * benchmark where a process sends a message to another process who copies the received data from the receive buffer into it s send buffer and sends it back to the first process , who performs the same copying process and sends it back again .",
    "this pattern is repeated many times and the time for the communications are recorded .",
    "the benchmark can send a range of message sizes . for the reference",
    "mpi benchmark only a single message is sent each iteration of the benchmark containing the fully amount of data to be sent . for the mdmp version",
    "the ` send ` and ` recv ` functionality specifies the single message to be sent and received , and performs the send and receive on the first iteration of the benchmark but on subsequent iterations of the benchmark the mdmp functionality identifies when each element of the message data is ready to be sent ( through tracking the data copying process between the send and receive buffers ) and sends individual elements when they are ready to go .",
    "this will mean that for a run of the benchmark using a message of 1000 elements in size the mpi version will send one message between processes whereas the mdmp version will send 1000 message ( apart from on the first iteration where it will only send one message ) .",
    "the second benchmark , called * selectivepingpong * alters the basic pingpong benchmark we have already described by performing the same functionality but only sending a portion of the overall data owned by a process in the messages .",
    "it is possible to vary both the overall size of data each process has , and the amount of that data that is sent , for instance you could have each process having an array that is 100 elements long but only the first 10 and last 10 elements are sent in the pingpong messages .",
    "this benchmark is designed to investigate the performance impact of varying the overall data in a computation and the amount that is being communicated via mdmp .",
    "the third benchmark , called * delaypingpong * , also alters the basic pingpong benchmark by adding a delay in the loop that copies the data from the receive buffer to the send buffer .",
    "this delay is variable and is designed to simulate some level of computational work being undertaken during what would be the main computational loop for a computational kernel using mdmp .",
    "the delay is performed by a routine which iterates through a loop adding an integer to a double a specified number of times ( delay elements ) .",
    "the final benchmark , * selectivedelaypingpong * , combines the second and third benchmarks meaning the pingpong process can contain both user defined delay in the data copy loop and a selective amount of data to be transferred .",
    "figure [ fig : combinedpingpong ] demonstrates the cost of mdmp compared to plain mpi where there is no scope for communication and computational overlaps .",
    "the runtime for mdmp increases more or less linearly as the size of the data to be transferred increases , whereas the runtime for mpi stays relatively constant",
    ".    however , if we examine figure [ fig : combinedpingpongdelay ] we can see that mdmp begins to see some benefits over mpi when the delay added to the data copy routine is increased .",
    "the juqueen and hector mdmp is faster than mpi when the delay elements are around 1000 and 800 elements respectively , although for helios mpi is always faster than mdmp ( albeit with a smaller gap in performance between the two methods ) .",
    "if not all the data that is copied between buffers is sent , as in the case of the selectivepingpong benchmark shown in figure [ fig : combinedpingpongselective1024 ] , then in comparison to the normal pingpong benchmark the overall difference in performance is reduced between mpi and mdmp although mdmp is still more costly than mpi .    finally , the combined benchmark , results shown in [ fig : combinedpingpongselective1024 ] where 1024 overall data elements are processed and either 1 or 32 elements are sent with variable amounts of delays , highlight where mdmp can improve performance . when only one element is being sent then all it requires",
    "is 16 floating point adds between communications ( 16 delay elements ) delay elements as there is a delay per array element ] to enable mdmp to optimise communications .",
    "if 32 elements are being sent then around 32 floating point adds are required to enabling the communication hiding that mdmp enables to provide a performance benefit .",
    "whilst these benchmarks are beneficial in enabling us to evaluate mdmp performance we recognise that a more realistic benchmark that evaluates mdmp performance against real kernel computations would also be useful as it would enable us to evaluate the overall impact of mdmp on cache , memory , and processor usage for real applications .",
    "we are in the process of undertaking such benchmarks at the moment but unfortunately do not have these results in time for this paper submission .",
    "we have outlined a novel approach of message passing programming on distributed memory hpc architectures and demonstrated that , given a reasonable level of computations to the communications to be performed , mdmp can reduce the overall cost of communications and improve application performance .",
    "we are aware the mdmp presents performance risks for parallel programs , including impacting cache and memory usage , and consuming additional memory .",
    "however , we belief the ability to enable and disable mdmp optimisations , and the potential benefits to ease of use and programmability from mdmp , make this approach a sensible one to investigate future message passing programming .",
    "we are currently working on a full compiler implementation of mdmp , including a formal mdmp language definition , and more involved benchmarks to evaluate mdmp in much more detail .",
    "part of this work was supported by an e - science grant from chalmers university .",
    "f.  blagojevi , p.  hargrove , c.  iancu , and k.  yelick .",
    "hybrid pgas runtime support for multicore nodes . in _ proceedings of the fourth conference on partitioned global address space programming model _ , pgas 10 , pages 3:13:10 , new york , ny , usa , 2010 .",
    "acm .",
    "a.  faraj , x.  yuan , and d.  lowenthal .",
    "star - mpi : self tuned adaptive routines for mpi collective operations . in _ proceedings of the 20th annual international conference on supercomputing _ , ics 06 , pages 199208 , new york , ny , usa , 2006 .",
    "l.  fishgold , a.  danalis , l.  pollock , and m.  swany .",
    "an automated approach to improve communication - computation overlap in clusters . in _ proceedings of the 20th international conference on parallel and",
    "distributed processing _ , ipdps06 , pages 290290 , washington , dc , usa , 2006 .",
    "ieee computer society .",
    "t.  hoefler and t.  schneider . runtime detection and optimization of collective communication patterns . in _ proceedings of the 21st international conference on parallel architectures and compilation techniques _ , pact 12 , pages 263272 , new york , ny , usa , 2012 .",
    "acm .    c.  hu , y.  shao , j.  wang , and j.  li .",
    "automatic transformation for overlapping communication and computation .",
    "in j.  cao , m.  li , m .- y .",
    "wu , and j.  chen , editors , _ network and parallel computing _ ,",
    "volume 5245 of _ lecture notes in computer science _ , pages 210220 .",
    "springer berlin heidelberg , 2008 .    c.  iancu , w.  chen , and k.  yelick .",
    "performance portable optimizations for loops containing communication operations . in _ proceedings of the 22nd annual international conference on supercomputing",
    "_ , ics 08 , pages 266276 , new york , ny , usa , 2008 .",
    "h.  jin , r.  hood , and p.  mehrotra . a practical study of upc using the nas parallel benchmarks . in _ proceedings of the third conference on partitioned global address space programing models",
    ", pgas 09 , pages 8:18:7 , new york , ny , usa , 2009 .",
    "a.  knapfer , d.  kranzlmaller , and w.  nagel .",
    "detection of collective mpi operation patterns . in d.",
    "kranzlmaller , p.  kacsuk , and j.  dongarra , editors , _ recent advances in parallel virtual machine and message passing interface _ ,",
    "volume 3241 of _ lecture notes in computer science _ , pages 259267 .",
    "springer berlin heidelberg , 2004 .    c.  lattner and v.  adve .",
    "llvm : a compilation framework for lifelong program analysis & transformation . in _ proceedings of the international symposium on code generation and optimization : feedback - directed and runtime optimization _ , cgo 04 , pages 75 , washington , dc , usa , 2004 .",
    "ieee computer society .",
    "j.  lee , m.  sato , and t.  boku .",
    "openmpd : a directive - based data parallel language extension for distributed memory systems . in _ parallel processing - workshops , 2008 . icpp - w 08 .",
    "international conference on _ , pages 121128 , 2008 .",
    "y.  li , j.  dongarra , and s.  tomov . a note on auto - tuning gemm for gpus . in _ proceedings of the 9th international conference on computational science",
    ": part i _ , iccs 09 , pages 884892 , berlin , heidelberg , 2009 .",
    "springer - verlag .",
    "a.  mller and r.  rhl .",
    "extending high performance fortran for the support of unstructured computations . in _ proceedings of the 9th international conference on supercomputing",
    "_ , ics 95 , pages 127136 , new york , ny , usa , 1995 .",
    "acm .",
    "s.  pellegrini , t.  fahringer , h.  jordan , and h.  moritsch .",
    "automatic tuning of mpi runtime parameter settings by using machine learning . in _ proceedings of the 7th acm international conference on computing frontiers _",
    ", cf 10 , pages 115116 , new york , ny , usa , 2010 .",
    "h.  shan , f.  blagojevi , s .- j .",
    "min , p.  hargrove , h.  jin , k.  fuerlinger , a.  koniges , and n.  j. wright . a programming model performance study using the nas parallel benchmarks .",
    ", 18(3 - 4):153167 , aug . 2010 .",
    "t.  wen , j.  su , p.  colella , k.  yelick , and n.  keen .",
    "an adaptive mesh refinement benchmark for modern parallel programming languages . in _ proceedings of the 2007 acm / ieee conference on supercomputing",
    "_ , sc 07 , pages 40:140:12 , new york , ny , usa , 2007 ."
  ],
  "abstract_text": [
    "<S> mdmp is a new parallel programming approach that aims to provide users with an easy way to add parallelism to programs , optimise the message passing costs of traditional scientific simulation algorithms , and enable existing mpi - based parallel programs to be optimised and extended without requiring the whole code to be re - written from scratch . </S>",
    "<S> mdmp utilises a directives based approach to enable users to specify what communications should take place in the code , and then implements those communications for the user in an optimal manner using both the information provided by the user and data collected from instrumenting the code and gathering information on the data to be communicated . in this paper </S>",
    "<S> we present the basic concepts and functionality of mdmp and discuss the performance that can be achieved using our prototype implementation of mdmp on some simple benchmark cases . </S>"
  ]
}