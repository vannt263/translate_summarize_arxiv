{
  "article_text": [
    "models for populations of dividing cells possibly differentiated by covariates such as size have made the subject of an abundant literature in recent years ( starting from athreya and ney @xcite , harris @xcite , jagers @xcite ... ) covariates termed as ` size ' are variables that grow deterministically with time ( such as volume , length , level of certain proteins , dna content , _",
    "etc_. ) such models of structured populations provide descriptions for the evolution of the size distribution , which can be interesting for applications .",
    "for instance , in the spirit of stewart _ et al . _",
    ", we can imagine that each cell contains some toxicities whose quantity plays the role of the size .",
    "the asymmetric divisions of the cells , where one daughter contains more toxicity than the other , can lead under some conditions to the purge of the toxicity in the population by concentrating it into few lineages .",
    "these results are linked with the concept of aging for cell lineage .",
    "this concept has been tackled in many papers ( e.g. ackermann _ et al . _",
    "@xcite , aguilaniu _ et al . _",
    "@xcite , c - y .",
    "@xcite , evans and steinsaltz @xcite , moseley @xcite ... ) .",
    "here we consider a stochastic individual - based model of size - structured population in continuous time , where individuals are cells undergoing asymmetric binary divisions and whose size is the quantity of toxicity they contain .",
    "a cell containing a toxicity @xmath3 divides at a rate @xmath4 .",
    "the toxicity grows inside the cell with rate @xmath5 .",
    "when a cell divides , a random fraction @xmath6 $ ] of the toxicity goes in the first daughter cell and @xmath7 in the second one . if @xmath8 , the daughters are the same with toxicity @xmath9 .",
    "we assume that @xmath10 has a symmetric distribution on @xmath11 $ ] with a density @xmath1 with respect to lebesgue measure such that @xmath12 .",
    "if @xmath1 is piked at @xmath13 ( _ i.e. _ @xmath14 ) , then both daughters contain the same toxicity , _",
    "i.e. _ the half of their mother s toxicity .",
    "the more @xmath1 puts weight in the neighbourhood of @xmath15 and @xmath16 , the more asymmetric the divisions are , with one daughter having little toxicity and the other a toxicity close to its mother s one . if we consider that having a lot of toxicity is a kind of senescence , then , the kurtosis of @xmath1 provides indication on aging phenomena ( see @xcite ) .",
    "modifications of this model to account for more complex phenomena have been considered in other papers .",
    "bansaye and tran @xcite , cloez @xcite or tran @xcite consider non - constant division and growth rates .",
    "_ @xcite studies whether divisions can occur only when a size threshold is reached .",
    "our purpose here is to estimate the density @xmath1 ruling the divisions , and we stick to constant rates @xmath17 and @xmath18 for the sake of simplicity . notice that several similar models for binary cell division in discrete time also exist in the literature and have motivated statistical question as here , see for instance bansaye et al .",
    "@xcite , bercu _ et al . _",
    "@xcite , bitseki penda @xcite , delmas and marsalle @xcite or guyon @xcite .",
    "individual - based models provide a natural framework for statistical estimation .",
    "estimation of the division rate is , for instance , the subject of doumic et al .",
    "@xcite and hoffmann and olivier @xcite . here",
    ", the density @xmath1 is the kernel division that we want to estimate . assuming that we observe the divisions of cells in continuous time on the interval @xmath0 $ ] , with @xmath19",
    ", we propose an adaptive kernel estimator @xmath20 of @xmath1 for which we obtain an oracle inequality in theorem [ th1 ] .",
    "the construction of @xmath20 is detailed in the sequel . from oracle inequality",
    "we can infer adaptive exponential rates of convergence with respect to @xmath21 depending on @xmath22 the smoothness of the density .",
    "most of the time , nonparametric rates are of the form @xmath23 ( see for instance tsybakov @xcite ) and exponential rates are not often encountered in the literature .",
    "the exponential rates are due to binary splitting , the number of cells _",
    "i.e _ the sample size increases exponentially in @xmath24 ( see section 2.3 ) . by comparison ,",
    "in @xcite hoffmann and olivier obtain a similar rate of convergence @xmath25 of the kernel estimator of their division rate @xmath26 , where @xmath27 is the malthus parameter and @xmath28 is the smoothness of @xmath26 . however , their estimator @xmath29 of @xmath30 is not adaptive since the choice of their optimal bandwidth still depends on @xmath31 .",
    "our estimator is adaptive with an  optimal \" bandwidth chosen from a data - driven method .",
    "we derive upper bounds and lower bounds for asymptotic minimax risks on hlder classes and show that they coincide .",
    "hence , the rate of convergence of our estimator @xmath32 proves to be optimal in the minimax sense on the hlder classes .",
    "this paper is organized as follows . in section 2",
    ", we introduce a stochastic differential equation driven by a poisson point measure to describe the population of cells .",
    "then , we construct the estimator of @xmath1 and obtain upper and lower bounds for the mise ( mean integrated squared error ) .",
    "our main results are stated in theorems [ th : upper - bound ] and [ th : lower - bound ] .",
    "numerical results and discussions about aging effect are presented in section 3 .",
    "the main proofs are shown in section 4 .",
    "* notation * we introduce some notations used in the sequel .",
    "hereafter , @xmath33 and @xmath34 denote the @xmath35 and @xmath36 norms on @xmath37 with respect to lebesgue measure : @xmath38 the @xmath39 norm is defined by @xmath40 finally , @xmath41 denotes the convolution of two functions @xmath42 and @xmath43 defined by @xmath44",
    "we recall the ulam - harris - neveu notation used to describe the genealogical tree . the first cell is labelled by @xmath45 and when the cell @xmath46 divides , the two descendants are labelled by @xmath47 and @xmath48 .",
    "the set of labels is @xmath49 we denote @xmath50 the set of cells alive at time @xmath51 , and @xmath52 .",
    "let @xmath53 be the space of finite measures on @xmath54 embedded with the topology of weak convergence and @xmath55 be the quantity of toxicity in the cell @xmath46 at time @xmath51 , we describe the population of cells at time @xmath51 by a random point measure in @xmath53 : @xmath56 is the number of individuals living at time @xmath51 . for a measure @xmath57 and a positive function @xmath42",
    ", we use the notation @xmath58 .    along branches of the genealogical tree , the toxicity @xmath59 satisfies @xmath60 with @xmath61 .",
    "when the cells divide , the toxicity is shared between the daughter cells .",
    "this is described by the following stochastic differential equation ( sde ) .",
    "let @xmath62 be an initial condition such that @xmath63 and let @xmath64 be a poisson point measure on @xmath65 $ ] with intensity @xmath66 .",
    "@xmath67 is the counting measure on @xmath68 and @xmath69 is lebesgue measure on @xmath70 .",
    "we denote @xmath71 the canonical filtration associated with the poisson point measure and the initial condition .",
    "the stochastic process @xmath72 can be described by a sde as follows .",
    "[ def : sde - model ] for every test function @xmath73 ( bounded of class @xmath74 in @xmath51 and @xmath75 with bounded derivatives ) , the population of cells is described by : @xmath76q(ds , di , d\\gamma ) .",
    "\\label{eq : sde - model}\\end{aligned}\\ ] ]    the second term in the right hand side of ( [ eq : sde - model ] ) corresponds to the growth of toxicities in the cells and the third term gives a description of cell divisions where the sharing of toxicity into two daughter cells depends on the random fraction @xmath10 .",
    "we now state some properties of @xmath77 that are useful in the sequel .    [",
    "prop : n_t]let @xmath19 , and assume the initial condition @xmath78 , the number of mother cells at time @xmath79 , is deterministic , for the sake of simplicity .",
    "we have    * let @xmath80 be the @xmath81 jump time . then : @xmath82 * @xmath83 is distributed according to a negative binomial distribution , denoted as @xmath84 .",
    "its probability mass function is then @xmath85 for @xmath86 .",
    "when @xmath87 , @xmath83 has a geometric distribution @xmath88 consequently , we have @xmath89 = n_0e^{rt}.\\ ] ] * when @xmath87 : @xmath90 = \\frac{rte^{-rt}}{1-e^{-rt}}.\\ ] ] when @xmath91 , we have : @xmath92 & = \\left(\\frac{e^{-rt}}{1-e^{-rt } } \\right)^{n_0}(-1)^{n_0 - 1}\\left(\\sum_{k=1}^{n_0 - 1}\\binom{n_0 - 1}{k } \\frac{(-1)^ke^{krt}}{k } + rt \\right ) .",
    "\\label{eq : inverse - n_t - nb } \\end{aligned}\\ ] ] * furthermore , when @xmath93 , we have @xmath94 \\le \\frac{e^{-rt}}{n_0 - 1}.\\ ] ]    the proof of proposition [ prop : n_t ] is presented in section 4 .      in this section ,",
    "we study the aging effect via the mean age which is defined as follows .",
    "[ def : mean - age ] the mean age of the cell population up to time @xmath95 is defined by : @xmath96 where @xmath97 .    following the work of bansaye et al .",
    "@xcite , we note that the long time behavior of the mean age is related to the law of an auxiliary process @xmath98 started at @xmath99 with infinitesimal generator characterized for all @xmath100 by @xmath101    the empirical distribution @xmath102 gives the law of the path of a particle chosen at random at time @xmath51 .",
    "heuristically , the distribution of @xmath98 restricted to @xmath103 $ ] approximates this distribution .",
    "hence , this explains the coefficient @xmath104 which is a size - biased phenomenon , _",
    "i.e. _ when one chooses a cell in the population at time @xmath51 , a cell belonging to a branch with more descendants is more likely to be chosen .",
    "let @xmath98 be the auxiliary process with infinitesimal generator , for @xmath95 , @xmath105 where @xmath106 is a square - integrable martingale .",
    "consequently , we have @xmath107 = \\left(y_0 - \\frac{\\alpha}{r}\\right)e^{-rt } + \\frac{\\alpha}{r},\\ ] ] and @xmath108 = \\frac{\\alpha}{r}.\\ ] ]    we will show that the auxiliary process @xmath98 satisfies ergodic properties ( see section [ proof : th - mean - age ] ) which entails the following theorem .    [",
    "th : mean - age ] assume that there exists @xmath109 such that for all @xmath110 , @xmath111 .",
    "then @xmath112    theorem [ th : mean - age ] is a consequence of the ergodic properties of @xmath98 , of theorem 4.2 in bansaye et al . @xcite and of lemma [ lem1 ] .",
    "it shows that the average of the mean age tends to the constant @xmath113 when the time @xmath51 is large .",
    "simulations in section 3 illustrate the results .",
    "the proofs of lemma [ lem1 ] and theorem [ th : mean - age ] are presented in section [ proof : lem - mean - age ] and section [ proof : th - mean - age ] .",
    "when the population is large , we are interested in studying the asymptotic behavior of the random point measure . as in doumic et al .",
    "@xcite , we can show that our stochastic model is approximated by a growth - fragmentation partial differential equation .",
    "this problem is a work in progress .      *",
    "data and construction of the estimator * + suppose that we observe the evolution of the cell population in a given time interval @xmath0 $ ] . at the @xmath114 division time @xmath115 ,",
    "let us denote @xmath116 the individual who splits into two daughters @xmath117 and @xmath118 and define @xmath119 the random fractions that go into the daughter cells , with the convention @xmath120 .",
    "@xmath121 and @xmath122 are exchangeable with @xmath123 , @xmath121 and @xmath122 are thus not independent but the couples @xmath124 are independent and identically distributed with distribution @xmath125 where @xmath126 and @xmath127 .",
    "since @xmath1 is a density function , it is natural to use a kernel method .",
    "we define an estimator @xmath128 of @xmath1 based on the data @xmath124 as follows .",
    "[ def : estimator - h ] let @xmath129 is an integrable function such that @xmath130 let @xmath131 be the random number of divisions in the time interval @xmath0 $ ] and assume that @xmath132 . for all @xmath133 , define @xmath134 where @xmath135",
    ", @xmath136 is the bandwidth to be chosen .    since @xmath137 ,",
    "the number of random divisions @xmath131 is not equal to the number of individuals living at time @xmath21 .",
    "indeed , we have @xmath138 .    in , @xmath128 depends also on @xmath21 . however , we omit @xmath21 for the sake of notation",
    ". the estimator @xmath139 will satisfy the following properties .",
    "[ prop : mean - var - h ]    * the conditional expectation and conditional variance given @xmath131 of @xmath140 and variance @xmath140 are : @xmath141 & = k_\\ell\\star h(\\gamma ) \\,\\text { and } \\ , { \\mathbb{e}}\\big[\\hat{h}_{\\ell}(\\gamma ) \\big ] = k_\\ell\\star h(\\gamma ) \\label{eq : mean - h},\\\\   { \\mathbb{v}ar}\\big[\\hat{h}_{\\ell}(\\gamma)\\big|m_t ] & = \\frac{1}{m_t}{\\mathbb{v}ar}\\left[k_\\ell(\\gamma - \\gamma^1_1)\\right ] , \\label{eq : var - cond - h } \\\\    { \\mathbb{v}ar}\\big[\\hat{h}_{\\ell}(\\gamma)\\big ] & = { \\mathbb{e}}\\big[\\frac{1}{m_t}\\big]{\\mathbb{v}ar}\\left[k_\\ell(\\gamma - \\gamma^1_1)\\right ] \\label{eq : var - h}.\\end{aligned}\\ ] ] consequently , we have @xmath142   = { \\mathbb{e}}\\big[\\hat{h}_{\\ell}(\\gamma ) \\big]$ ] . * for all @xmath133 , @xmath143    * adaptive estimation of @xmath1 by goldenshluger and lepski s ( gl ) method * + let @xmath139 be the kernel estimator of @xmath1 as in definition [ def : estimator - h ] .",
    "we measure the performance of @xmath139 via its @xmath36-loss _ i.e _ the average @xmath36 distance between @xmath144 and @xmath1 .",
    "the objective is to find a bandwidth which minimizes this @xmath36-loss . since @xmath131 is random , we first study the @xmath36-loss conditionally to @xmath131 .",
    "[ prop : bias - variance - decomposition ] the @xmath36-loss of @xmath144 given @xmath131 satisfies : @xmath145 \\le { \\|h - k_\\ell\\star h\\|_2 } + \\frac{{\\|k\\|_2}}{\\sqrt{m_t\\ell}}.\\ ] ]    in the right hand side of the risk decomposition the first term is a bias term . hence it decreases when @xmath146 whereas the second term which is a variance term increases when @xmath146 .",
    "the best choice of @xmath147 should minimize this bias - variance trade - off .",
    "thus , from a finite family of bandwidths @xmath148 , the best bandwidth @xmath149 would be @xmath150    the bandwidth @xmath151 is called `` the oracle bandwidth '' since it depends on @xmath1 which is unknown and then it can not be used in practice .",
    "since the oracle bandwidth minimizes a bias variance trade - off , we need to find an estimation for the bias - variance decomposition of @xmath144 .",
    "goldenshluger and lepski @xcite developed a fully data - driven bandwidth selection method ( gl method ) .",
    "the main idea of this method is based on an estimate of the bias term by looking at several estimators . in a similar fashion , doumic et al .",
    "@xcite and reynaud - bouret et al .",
    "@xcite have used this method . to apply the gl method ,",
    "we set for any @xmath147 , @xmath152 : @xmath153 finally , the adaptive bandwidth and the estimator of @xmath1 are selected as follows :    [ def : gl - bw ] given @xmath154 and setting @xmath155 , we define @xmath156 where , for any @xmath157 , @xmath158 then , the estimator @xmath20 is given by @xmath159    an inspection of the proof of theorem 2 shows that the term @xmath160 provides a control for the bias @xmath161 up to the term @xmath162 ( see ( [ aell ] ) and ( [ proof - th1-el ] ) in the proof of theorem [ th1 ] , section 4 ) . since @xmath163 depends only on @xmath164 and @xmath165 , the estimator @xmath20 can be computed in practice .",
    "we shall now state an oracle inequality which highlights the bias - variance decomposition of the mise of @xmath166 .",
    "we recall that the mise of @xmath166 is the quantity @xmath167 $ ] .",
    "[ th1 ] let @xmath19 and assume that observations are taken on @xmath0 $ ] .",
    "let @xmath78 be the number of mother cells at the beginning of divisions and @xmath131 is the random number of divisions in @xmath0 $ ] .",
    "consider @xmath148 a countable subset of @xmath168 in which we choose the bandwidths and @xmath169 for some @xmath170 .",
    "assume @xmath171)$ ] and let @xmath20 be a kernel estimator defined with the kernel @xmath172 where @xmath173 is chosen by the gl method .",
    "define @xmath174 for large @xmath21 , the main term in @xmath175 is @xmath176 in any case .",
    "it is exactly the order of @xmath175 for @xmath91 .",
    "then , given @xmath177 @xmath178 \\le   c_1\\underset{\\ell \\in h}{\\inf}\\left\\{{\\|k_\\ell\\star h - h\\|_2}^2 + \\frac{{\\|k\\|_2}^2}{\\ell}\\varrho(t)^{-1}\\right\\ } + c_2\\varrho(t)^{-1},\\ ] ] where @xmath179 is a constant depending on @xmath78 , @xmath180 and @xmath181 and @xmath182 is a constant depending on @xmath78 , @xmath183 , @xmath181 , @xmath180 , @xmath184 and @xmath185 .",
    "the term @xmath186 is an approximation term , @xmath187 is a variance term and the last term @xmath188 is asymptotically negligible .",
    "hence the right hand side of the oracle inequality corresponds to a bias variance trade - off .",
    "we now establish upper and lower bounds for the mise .",
    "the lower bound is obtained by perturbation methods ( theorem [ th : lower - bound ] ) and is valid for any estimator @xmath189 of @xmath1 , thus indicating the optimal convergence rate .",
    "the upper bound is obtained in theorem [ th : upper - bound ] thanks to the key oracle inequality of theorem [ th1 ] .    for the rate of convergence ,",
    "it is necessary to assume that the density @xmath1 and the kernel function @xmath2 satisfy some regularity conditions introduced in the following definitions .",
    "[ def : holder - class ] let @xmath190 and @xmath191 .",
    "the hlder class of smoothness @xmath22 and radius @xmath192 is defined by @xmath193    [ def : regularity - k ] let @xmath194 .",
    "an integrable function @xmath195 is a kernel of order @xmath196 if    * @xmath197 , * @xmath198 , * for @xmath199 , @xmath200 , @xmath201 .",
    "then , the following theorem gives the rate of convergence of the adaptive estimator @xmath166 .",
    "[ th : upper - bound ] let @xmath194 and k be a kernel of order @xmath196 .",
    "let @xmath202 .",
    "let @xmath173 be the adaptive bandwidth defined in .",
    "then , for any @xmath19 , the kernel estimator @xmath166 satisfies @xmath203 where @xmath204 is defined in and @xmath205 is a constant depending on @xmath78 , @xmath183 , @xmath181 , @xmath180 , @xmath184 , @xmath185 , @xmath22 and @xmath192 .",
    "we now establish a lower bound in theorem [ th : lower - bound ] .",
    "[ th : lower - bound ] for any @xmath19 , @xmath206 and @xmath191 .",
    "assume that @xmath207 , then there exists a constant @xmath208 such that for any estimator @xmath209 of @xmath1 @xmath210    contrary to the classical cases of nonparametric estimation ( _ e.g. _ tsybakov @xcite ,  ) , the number of observations @xmath131 is a random variable that converges to @xmath211 when @xmath212 which is one of the main difficulty here . from theorem",
    "[ th : upper - bound ] , when @xmath213 the upper bound is in @xmath214 which is the same rate as the lower bound .",
    "the rate of convergence @xmath166 is thus optimal .",
    "when @xmath215 , the upper bound is in @xmath216 that differs with a logarithmic from the rate in the lower bound .",
    "the rate of convergence is thus slightly slower than in the case @xmath91 and our estimator is optimal up to a logarithmic factor .",
    "furthermore , theorem 3 illustrates adaptive properties of our procedure : it achieves the rate @xmath217 over the hlder class @xmath218 as soon as @xmath22 is smaller than @xmath196 .",
    "so , it automatically adapts to the unknown smoothness of the signal to estimate .",
    "we use the * r * software to implement simulations with two original distributions of division kernel @xmath1 and compare with their estimators . on the interval",
    "@xmath11 $ ] , the first distribution to test is @xmath219 .",
    "@xmath220 distributions on @xmath11 $ ] are characterized by their densities @xmath221 where @xmath222 is the renormalization constant .    since @xmath1 is symmetric",
    ", we only consider the distributions with @xmath223 . generally , asymmetric divisions correspond to @xmath224 and symmetric divisions with kernels concentrated around @xmath225 correspond to @xmath226",
    ". the smaller the parameter @xmath227 , the more asymmetric the divisions . for the second density ,",
    "we choose a beta mixture distribution as @xmath228 this choice gives us a bimodal density corresponding to very asymmetric divisions .",
    ".47    .47    we estimate @xmath20 by using and we take the classical gaussian kernel @xmath229 . for the choice of bandwidth ,",
    "we apply the gl method with the family @xmath230 for some @xmath170 small enough when @xmath131 is large to reduce the time of numerical simulation .",
    "we have @xmath231 , @xmath232 and @xmath233 , hence it is not difficult to calculate in practice @xmath164 as well as @xmath165 . finally , the value of @xmath181 in @xmath234 is chosen to find an optimal value of the mise . to do this ,",
    "we implement a preliminary simulation to calibrate @xmath181 in which we choose @xmath235 to ensure that @xmath236 .",
    "we compute the mise and @xmath237 as functions of @xmath181 where @xmath238 $ ] and @xmath1 is the density of @xmath219 . in figure",
    "[ fig5b : sub1 ] , simulation results show that the risk has minimum value at @xmath239 .",
    "this value is not justified from a theoretical point of view .",
    "the theoretical choice @xmath240 ( see theorem 2 ) does not give bad results but this choice is too conservative for non - asymptotic practical purposes as often met in the literature ( see bertin et al .",
    "@xcite for more details about the gl methodology ) .",
    "moreover , following the discussion in lacour and massart @xcite we investigate ( see figure 2b ) the difference @xmath237 and observe some explosions close to @xmath241 .",
    "consequently , we choose @xmath239 for all following simulations .",
    "figure [ fig1 ] illustrates a reconstruction for the density of @xmath219 and beta mixture @xmath242 when @xmath243 .",
    "we choose here the division rate and the growth rate @xmath244 and @xmath245 respectively .",
    "we compare the estimated densities when using the gl bandwidth with those estimated with the oracle bandwidth .",
    "the oracle bandwidth is found by assuming that we know the true density .",
    "moreover , the gl estimators are compared with estimators using the cross - validation ( cv ) method and the rule of thumb ( rot ) .",
    "the cv bandwidth is defined as follows : @xmath246 where @xmath247 .",
    "the rot bandwidth can be calculated simply by using the formula @xmath248 where @xmath249 is the standard deviation of the sample @xmath250 .",
    "more details about these methods can be found in section 3.4 of silverman @xcite or tsybakov @xcite .",
    ".46    .46    to estimate the mise , we implement monte - carlo simulations with respect to @xmath251 and @xmath252 .",
    "the number of repetitions for each simulation is @xmath253 .",
    "then , we compute the mean of relative error @xmath254 and the standard deviation @xmath255 where @xmath256 and @xmath257 denotes the estimator of @xmath1 corresponding to @xmath114 repetition .    ._mean of relative error and its standard deviation for the reconstruction of beta mixture @xmath258 .",
    "[ tab2 ] _ [ cols=\"<,<,<,<,<,<,<\",options=\"header \" , ]      for @xmath259 , recall the mean age defined in . to study the influence of the distribution on the mean age ,",
    "we simulate @xmath260 trees with respect to @xmath261 with @xmath262 .",
    "for each sample ( @xmath263 ,  , @xmath264 ) , we compute the average mean , the @xmath265 ( @xmath266 ) quartile and @xmath267 ( @xmath268 ) quartile .",
    "figure [ fig3]a and [ fig3]b show the simulation results corresponding to the density of @xmath219 with @xmath269 and @xmath270 .",
    "one can see that the average of mean age and the mean age converge to @xmath271 for larger @xmath51 .",
    "this agrees with the theoretical result proved in section [ sec : mean - age ] .",
    "moreover , @xmath266 and @xmath268 vary when the parameter @xmath227 changes . in figure",
    "[ fig3]c , we draw a fitted curve of the average of @xmath272 when @xmath227 varies from @xmath15 to @xmath104 . as we mentioned in the introduction , if divisions are more asymmetric corresponding to small values of @xmath227 , the toxicities concentrate on few cells , _",
    "i.e. _ we have more older cells after the divisions .",
    "this explains the decreasing trend in the average of @xmath272 .",
    "finally , figure [ fig3]d displays the average of mean ages with respect to @xmath227 .",
    "one can note that it does not change when we replace the kernel distribution , _",
    "@xmath273 instead of @xmath219 .",
    "_ ii ) _ the proof of ii ) can be found easily in literature .",
    "here we refer to @xcite , section 5.3 for this proof . + _",
    "i ) _ let us prove that @xmath274 .",
    "since our model has only births and no death , @xmath275}$ ] is a non - decreasing process : @xmath276 .",
    "all the @xmath80 s are finite and @xmath277 a.s . from _",
    "ii ) _ , we have @xmath278 = n_0 e^{rt}$ ] .",
    "hence , we deduce from the estimate @xmath279}{\\sup}\\,}{\\mathbb{e}}[n_t]<+{\\infty}$ ] for all @xmath19 that @xmath280 a.s .",
    "then we also have @xmath281 a.s .",
    "+ _ iii ) _ let @xmath282 .",
    "when @xmath87 , @xmath283 .",
    "then we have @xmath284 & = \\sum_{n=1}^{\\infty } \\frac{1}{n}{\\mathbb{p}}\\big(n_t = n\\big ) = \\sum_{n=1}^{\\infty } \\frac{1}{n } p(1-p)^{n-1 } \\\\ & = \\frac{p}{1-p}\\sum_{n=1}^{+{\\infty } } \\frac{(1-p)^n}{n } = -\\frac{p}{1-p}\\log(p).\\end{aligned}\\ ] ] replace @xmath285 with @xmath176 , we obtain .",
    "+ when @xmath93 , @xmath286 .",
    "hence , we have @xmath284 = & \\sum_{n = n_0}^{\\infty } \\frac{1}{n}\\binom{n-1}{n - n_0 } p^{n_0 } ( 1- p)^{n - n_0}\\notag\\\\ = & \\left(\\frac{p}{1-p}\\right)^{n_0}\\sum_{n = n_0}^{\\infty } \\frac{1}{n}\\binom{n-1}{n - n_0 } ( 1-p)^n \\notag\\\\ : = & \\left(\\frac{p}{1-p}\\right)^{n_0}f(1-p ) , \\label{eq : temp4}\\end{aligned}\\ ] ] where @xmath287",
    ". we can differentiate @xmath288 by taking derivative under the sum . then : @xmath289since the sum is 1 ( we recognize the negative binomial ) .",
    "+ hence , @xmath290 \\notag \\\\ = & ( -1)^{n_0}\\left[\\sum_{k=1}^{n_0 - 1}\\binom{n_0 - 1}{k } \\frac{(-1)^k}{p^{k+1 } } + \\frac 1p \\right].\\label{eq : temp-1}\\end{aligned}\\ ] ] integrating equation and notice that @xmath291 , we get @xmath292 \\notag\\\\ = & ( -1)^{n_0 - 1}\\left[\\sum_{k=1}^{n_0 - 1 } \\binom { n_0 - 1}{k } \\frac{(-1)^k}{k}\\frac{1}{p^k } + \\log\\left(\\frac 1p \\right ) \\right ] .",
    "\\label{eq : temp5}\\end{aligned}\\ ] ] combine , and replace @xmath285 with @xmath176 , we get . _",
    "iv ) _ we first prove the lower bound of .",
    "from , taking @xmath293 , we have @xmath294 applying it formula for jump processes ( see @xcite , theorem 5.1 on p.67 ) to , we obtain @xmath295 hence , @xmath296 = \\frac{1}{n_0 } - { \\mathbb{e}}\\left[\\int_0^t\\frac{1}{n_{s}\\left(n_{s } + 1 \\right)}rn_s ds \\right ] = \\frac{1}{n_0 } - r\\int_0^t{\\mathbb{e}}\\left[\\frac{1}{n_s + 1 } \\right]ds.\\ ] ] since @xmath297 , we have @xmath298 therefore , ( [ eq : temp0 ] ) implies that @xmath299 \\ge \\frac{1}{n_0 } - r\\int_0^t{\\mathbb{e}}\\left[\\frac{1}{n_s } \\right]ds   .\\ ] ] by comparison of @xmath300 $ ] with the solutions of the ode @xmath301 with @xmath302 , we finally obtain @xmath303 \\ge \\frac{1}{n_0}e^{-rt}.\\ ] ]    for the upper bound , notice that @xmath304 \\le { \\mathbb{e}}\\left[\\frac{1}{n_t - 1 } \\right]$ ] for @xmath213",
    ". then we have @xmath305= & \\sum_{n = n_0}^{+\\infty } \\frac{1}{n-1 } \\binom{n-1}{n - n_0 } p^{n_0 } ( 1-p)^{n - n_0}\\\\   = & \\sum_{n = n_0}^{+\\infty } \\frac{(n-2)!}{(n - n_0)!(n_0 - 1 ) ! } p^{n_0 } ( 1-p)^{n - n_0}\\\\   = & \\frac{p}{n_0 - 1 } \\sum_{n = n_0}^{+\\infty } \\frac{(n-2)!}{(n - n_0)!(n_0 - 2 ) ! } p^{n_0 - 1 } ( 1-p)^{n - n_0}\\\\   = & \\frac{p}{n_0 - 1 } \\sum_{m = n_0 - 1}^{+\\infty } \\frac{(m-1)!}{(m-(n_0 - 1))!((n_0 - 1)-1 ) ! } p^{n_0 - 1 } ( 1-p)^{m-(n_0 - 1)}\\\\   = & \\frac{p}{n_0 - 1 } = \\frac{e^{-rt}}{n_0 - 1},\\end{aligned}\\ ] ] by changing the index in the sum ( @xmath306 ) and by recognizing the negative binomial with parameter @xmath307 .",
    "hence , we conclude that for @xmath91 @xmath308 \\le \\frac{e^{-rt}}{n_0 - 1}.\\ ] ] this ends the proof of proposition [ prop : n_t ] .      by symmetry of @xmath1 with respect to @xmath13",
    ", we have : @xmath309 where @xmath106 is a square - integrable martingale .",
    "+ let @xmath310 , @xmath311 . by it formula , we get @xmath312 replacing @xmath313 by @xmath314 , we obtain @xmath315 we end the proof by taking the expectation and the limit as @xmath316 of @xmath317 to obtain and .          from",
    "we note that @xmath322 < + { \\infty}$ ] for all @xmath259 . to prove the second point , from we have @xmath323 & = { \\mathbb{e}}\\left[y_0 ^ 2 + \\int_0^t\\left(2\\alpha y_s + 2r\\int_0 ^ 1\\left(\\gamma^2y_s^2 - y_s^2 \\right)h(\\gamma)d\\gamma \\right)ds \\right]\\notag\\\\ & = y_0 ^ 2 + 2\\alpha\\int_0^t{\\mathbb{e}}[y_s]ds - 2\\theta r\\int_0^t{\\mathbb{e}}[y_s^2]ds,\\label{eq : mean - age - temp3}\\end{aligned}\\ ] ] with @xmath324 and @xmath325 .    substituting @xmath322 = ( y_0 - \\alpha / r)e^{-rt } + \\alpha / r$ ] into , we see that @xmath326 solves the following equation : @xmath327}{dt } = -2\\theta r{\\mathbb{e}}[y_t^2 ] + \\left(2\\alpha y_0 - \\frac{2\\alpha^2}{r}\\right)e^{-rt } + \\frac{2\\alpha^2}{r}.\\ ] ] the solution of the equation is : @xmath328 = e^{-2\\theta rt}\\left[y_0 ^ 2 + \\int_0^t e^{2\\theta rs}\\left(\\big(2\\alpha y_0 - \\frac{2\\alpha^2}{r}\\big)e^{-rs } + \\frac{2\\alpha^2}{r } \\right)ds \\right].\\ ] ] hence , if @xmath329 , we have @xmath323 & = y_0 ^ 2e^{-rt } + \\big(2\\alpha y_0 - \\frac{2\\alpha^2}{r } \\big)te^{-rt } + \\frac{2\\alpha^2}{r^2}\\left(1-e^{-rt}\\right ) \\\\ & \\le y_0 ^ 2e^{-rt } + \\big(2\\alpha y_0 - \\frac{2\\alpha^2}{r } \\big)e^{-(r-\\theta)t } + \\frac{2\\alpha^2}{r^2 } \\\\ & \\le \\left(y_0 ^ 2 + 2\\alpha y_0 + \\frac{2\\alpha^2}{r } + \\frac{2\\alpha^2}{r^2}\\right)e^{(0\\vee ( \\theta - r))t } = c_1 e^{\\varpi t},\\end{aligned}\\ ] ] with @xmath330 .",
    "+ if @xmath331 , @xmath323 & = e^{-2\\theta rt}\\left[y_0 ^ 2 +   \\big(2\\alpha y_0 - \\frac{2\\alpha^2}{r}\\big)\\int_0^t e^{(2\\theta-1)rs}ds +   \\frac{2\\alpha^2}{r}\\int_0^t e^{2\\theta rs}ds \\right ] \\notag \\\\ & = y_0 ^ 2e^{-2\\theta rt } + \\big(2\\alpha y_0 - \\frac{2\\alpha^2}{r}\\big)\\frac{1}{(2\\theta-1)r}\\left(e^{-rt}-e^{-2\\theta rt}\\right)\\notag\\\\ & \\phantom{aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa }   + \\frac{\\alpha^2}{\\theta r^2}\\left(1-e^{-2\\theta rt}\\right)\\notag \\\\ & \\le \\left(y_0 ^ 2 + \\big(2\\alpha y_0 + \\frac{2\\alpha^2}{r}\\big)\\frac{1}{|2\\theta-1|r } + \\frac{\\alpha^2}{\\theta r^2 } \\right ) = c_2 . \\notag\\end{aligned}\\ ] ] thus , if we set @xmath332 then @xmath321 < ce^{\\varpi t}$ ] for all @xmath259 .",
    "+ the infinitesimal generator @xmath333 of @xmath98 is defined for @xmath74 test functions as @xmath334 for @xmath335 and @xmath336 , we have @xmath337 hence , by theorem 5.3 of meyn and tweedie @xcite , there exists @xmath338 such that @xmath339 = \\langle \\pi , f \\rangle = \\frac{\\alpha}{r}$ ] . finally ,",
    "applying theorem 4.2 of @xcite , we obtain the result @xmath340      to prove , let us remark that the number of random divisions @xmath131 is independent of @xmath341 , because the division rate @xmath17 is constant and because of the construction of our stochastic process .",
    "therefore , we have @xmath342 & = { \\mathbb{e}}\\big[\\frac{1}{m_t}\\sum_{i=1}^{m_t } { k_\\ell(\\gamma - \\gamma^1_i)}\\big| m_t\\big]= \\frac{m_t{\\mathbb{e}}[{k_\\ell(\\gamma - \\gamma^1_1)}]}{m_t } \\\\ & = { \\mathbb{e}}\\big[{k_\\ell(\\gamma - \\gamma^1_1)}\\big ] = k_\\ell\\star h ( \\gamma),\\end{aligned}\\ ] ] and @xmath343",
    "= { \\mathbb{e}}\\left[{\\mathbb{e}}\\big[{\\hat{h}_\\ell}|m_t \\big ] \\right ] = k_\\ell\\star h ( \\gamma)$ ] . by similar calculations as , we obtain and .",
    "we have @xmath347 \\le { \\|h - k_\\ell\\star h\\|_2 } + { \\mathbb{e}}\\big[{\\|{\\hat{h}_{\\ell}}- { \\mathbb{e}}[{\\hat{h}_{\\ell}}]\\|_2}|m_t \\big].\\end{aligned}\\ ] ] for the variance term , using that @xmath348 = { \\mathbb{e}}\\big[\\hat{h}_{\\ell}(\\gamma)|m_t \\big]$ ] @xmath349\\|_2}^2 |m_t\\big ] & =   { \\mathbb{e}}\\big[\\int_{\\mathbb{r}}\\big|{\\hat{h}_{\\ell}}(\\gamma ) - { \\mathbb{e}}\\big[{\\hat{h}_{\\ell}}(\\gamma)\\big ]   \\big|^2d\\gamma\\big|m_t \\big ] \\notag \\\\ & = \\int_{\\mathbb{r}}{\\mathbb{v}ar}\\big[{\\hat{h}_{\\ell}}(\\gamma ) \\big| m_t \\big]d\\gamma \\notag \\\\ & = \\frac{1}{m_t}\\int_{\\mathbb{r}}{\\mathbb{v}ar}\\big[k_\\ell(\\gamma - \\gamma^1_1 ) \\big]d\\gamma \\notag\\\\ & \\le \\frac{1}{m_t}\\int_{\\mathbb{r}}{\\mathbb{e}}\\big[k_\\ell^2(\\gamma - \\gamma^1_1)\\big]d\\gamma \\notag\\end{aligned}\\ ] ] by fubini s theorem , we get @xmath350d\\gamma & = \\int_{\\mathbb{r}}\\int_{\\mathbb{r}}k^2_\\ell(\\gamma - u)h(u)du\\,d\\gamma \\\\ & = \\int_{\\mathbb{r}}h(u)\\left(\\int_{\\mathbb{r}}k^2_\\ell(\\gamma - u)d\\gamma\\right)du \\\\ & = { \\|k_\\ell\\|_2}^2\\int_{\\mathbb{r}}h(u)du = \\frac{{\\|k\\|_2}^2}{\\ell}.\\end{aligned}\\ ] ] then we have @xmath351\\|_2}^2 |m_t\\big ] \\le   \\frac{{\\|k\\|_2}^2}{m_t\\ell}. \\label{eq : upper - bound - var}\\ ] ] hence , applying cauchy - schwarz s inequality , we obtain .",
    "this ends the proof of proposition [ prop : bias - variance - decomposition ] .",
    "this proof is inspired by the proof of doumic et al .",
    "however , our problem here is that the number of observations @xmath131 is random . to overcome this difficulty",
    ", we work conditionally to @xmath131 to get concentration inequalities .",
    "+ hereafter , we refer @xmath352 to @xmath353 and since the support of @xmath1 is @xmath354 , we can write @xmath355 instead of @xmath356 . recall that @xmath357    then , for any @xmath358 , we have @xmath359 where @xmath360 by definition of @xmath173 , we have @xmath361 and @xmath362 \\big ) - \\big({\\hat{h}_{\\ell'}}- { \\mathbb{e}}[{\\hat{h}_{\\ell ' } } ] \\big)\\|_2 } \\right . \\notag\\\\ & \\phantom{aaaaaaaaaaaaaaaaa}\\left . + { \\|{\\mathbb{e}}[{\\hat{h}_{\\ell,\\ell ' } } ] - { \\mathbb{e}}[{\\hat{h}_{\\ell'}}]\\|_2 } - \\frac{\\chi{\\|k\\|_2}}{\\sqrt{m_t \\ell'}}\\right\\}_{+}\\notag\\\\ & \\le \\xi_t(\\ell ) + \\underset{\\ell'\\in h}{\\sup}\\left\\{{\\|{\\mathbb{e}}[{\\hat{h}_{\\ell,\\ell ' } } ] - { \\mathbb{e}}[{\\hat{h}_{\\ell'}}]\\|_2}\\right\\},\\end{aligned}\\ ] ] where @xmath363 \\big ) - \\big({\\hat{h}_{\\ell'}}- { \\mathbb{e}}[{\\hat{h}_{\\ell ' } } ] \\big)\\|_2 }      - \\frac{\\chi{\\|k\\|_2}}{\\sqrt{m_t\\ell ' } } \\right\\}_{+}.\\ ] ]    for the term @xmath364 - { \\mathbb{e}}[{\\hat{h}_{\\ell'}}]\\|_2}\\right\\}$ ] , we have @xmath365 & - { \\mathbb{e}}[{\\hat{h}_{\\ell ' } } ] = \\int \\big(k_\\ell\\star k_{\\ell'}\\big)(\\gamma - u)h(u)du       - \\int k_{\\ell'}(\\gamma - v)h(v)dv \\notag \\\\ & = \\int\\int k_\\ell(\\gamma - u - t)k_{\\ell'}(t)h(u)dtdu - \\int k_{\\ell'}(\\gamma - v)h(v)dv \\notag \\\\ & = \\int\\int k_\\ell(v - u)k_{\\ell'}(\\gamma - v)h(u)dudv - \\int k_{\\ell'}(\\gamma - v)h(v)dv \\notag \\\\ & = \\int k_{\\ell'}(\\gamma - v)\\left(\\int k_\\ell(v - u)h(u)du - h(v ) \\right)dv \\notag \\\\ & = \\int k_{\\ell'}(\\gamma - v)\\left(k_\\ell\\star h(v ) - h(v ) \\right)dv .",
    "\\notag\\end{aligned}\\ ] ] hence , we derive @xmath366 - { \\mathbb{e}}[{\\hat{h}_{\\ell'}}]\\|_2 } = { \\|k_{\\ell'}\\star ( k_\\ell\\star h - h)\\|_2 } \\le { \\|k\\|_{1}}{\\|k_\\ell\\star h - h\\|_2},\\ ] ] where the right hand side does not depend on @xmath367 allowing us to take @xmath368 in the left hand side .",
    "thus , , and give @xmath369 then , @xmath370   \\le 12{\\mathbb{e}}[\\xi_t^2({\\ell } ) ] + 12{\\|k\\|_{1}}^2{\\|k_\\ell\\star h - h\\|_2}^2 + 12\\frac{\\chi^2{\\|k\\|_2}^2}{\\ell } { \\mathbb{e}}\\left[\\frac{1}{m_t}\\right].\\ ] ]    for the term @xmath371 , we have from @xmath372 & = { \\|{\\mathbb{e}}[{\\hat{h}_\\ell } ] - h\\|_2}^2 + { \\mathbb{e}}\\left[{\\|{\\hat{h}_\\ell}- { \\mathbb{e}}[{\\hat{h}_\\ell}]\\|_2}^2\\right ] \\notag\\\\ & \\le { \\|k_\\ell\\star h - h\\|_2}^2 + \\frac{{\\|k\\|_2}^2}{\\ell}{\\mathbb{e}}\\left[\\frac{1}{m_t}\\right ] .",
    "\\notag\\end{aligned}\\ ] ]    finally , replacing @xmath373 by @xmath374 , we have for any @xmath375 @xmath376 \\le 2{\\mathbb{e}}\\left[(a_1 + a_2)^2\\right ] + 2{\\mathbb{e}}\\left[a_3 ^ 2\\right ] \\notag\\\\ & \\phantom{aaa } \\le 24{\\mathbb{e}}\\left[\\xi^2_t({\\ell})\\right ] + 2\\left(1 + 12{\\|k\\|_{1}}^2\\right){\\|k_\\ell\\star h - h\\|_2}^2 \\notag\\\\ & \\phantom{aaaaaaaa }",
    "+ 2\\big(1 + 12(1 + \\epsilon)^2(1 + { \\|k\\|_{1}})^2\\big)\\frac{{\\|k\\|_2}^2}{\\ell}{\\mathbb{e}}\\left[\\frac{1}{m_t}\\right ] \\notag\\\\ & \\phantom{aaa } \\le 24{\\mathbb{e}}\\left[\\xi^2_t(\\ell)\\right ] + c_1\\left ( { \\|k_\\ell\\star h - h\\|_2}^2 + \\frac{{\\|k\\|_2}^2}{\\ell}{\\mathbb{e}}\\left[\\frac{1}{m_t}\\right]\\right ) , \\label{eq : proof - th1-bound1}\\end{aligned}\\ ] ] with @xmath179 a constant depending on @xmath181 and @xmath180 .    it remains to deal with the term @xmath377 $ ] where @xmath378 is defined in , @xmath379\\|_2 } + { \\|{\\hat{h}_{\\ell'}}- { \\mathbb{e}}[{\\hat{h}_{\\ell'}}]\\|_2 }      - \\frac{\\chi{\\|k\\|_2}}{\\sqrt{m_t\\ell ' } } \\right\\}_{+ } \\\\ & \\le { \\,\\underset{\\ell'\\in h}{\\sup}\\,}\\left\\{{\\|{\\hat{h}_{\\ell'}}- { \\mathbb{e}}[{\\hat{h}_{\\ell'}}]\\|_2}{\\|k\\|_{1 } }   + { \\|{\\hat{h}_{\\ell'}}- { \\mathbb{e}}[{\\hat{h}_{\\ell'}}]\\|_2 }      - \\frac{\\chi{\\|k\\|_2}}{\\sqrt{m_t\\ell ' } } \\right\\}_{+ } \\\\ & \\le { \\,\\underset{\\ell'\\in h}{\\sup}\\,}\\left\\{\\big(1 + { \\|k\\|_{1}}\\big){\\|{\\hat{h}_{\\ell'}}- { \\mathbb{e}}[{\\hat{h}_{\\ell'}}]\\|_2 }      - \\frac{(1+\\epsilon)(1+{\\|k\\|_{1}}){\\|k\\|_2}}{\\sqrt{m_t\\ell ' } } \\right\\}_{+ } \\\\ & \\le ( 1+{\\|k\\|_{1}})s_t,\\end{aligned}\\ ] ] where @xmath380\\|_2 } - \\frac{(1+\\epsilon){\\|k\\|_2}}{\\sqrt{m_t\\ell}}\\right\\}_+.\\ ] ] hence , @xmath381 \\le ( 1 + { \\|k\\|_{1}})^2{\\mathbb{e}}\\big[{\\mathbb{e}}\\big[s_t^2|m_t\\big]\\big ] .",
    "\\label{eq : proof - th1-bound - xi}\\end{aligned}\\ ] ] if we show that @xmath382 \\le c_*\\frac{1}{n},\\ ] ] then @xmath383 \\le c_*(1 + { \\|k\\|_{1}})^2{\\mathbb{e}}\\left[\\frac{1}{m_t}\\right]\\ ] ] where @xmath384 is a constant .",
    "let us establish .",
    "when @xmath385 , @xmath386 , we set @xmath387 = { \\mathbb{e}}\\big[s_t^2|m_t = n\\big]\\ ] ] where @xmath388 with @xmath389 = \\frac{1}{n}\\sum_{i=1}^n { k_\\ell(\\gamma - \\gamma^1_i)}- { \\mathbb{e}}[{k_\\ell(\\gamma - \\gamma^1_i)}].\\ ] ] then , @xmath390 & = { \\mathbb{e}}\\left[{\\,\\underset{\\ell\\in h}{\\sup}\\,}\\left\\{{\\|z_\\ell\\|_2 } -   \\frac{(1+\\epsilon){\\|k\\|_2}}{\\sqrt{n\\ell}}\\right\\}_+^2 \\right ] \\\\ & \\le \\int_0^{+{\\infty } } { \\mathbb{p}}\\left[{\\,\\underset{\\ell\\in h}{\\sup}\\,}\\left\\{{\\|z_\\ell\\|_2 } -   \\frac{(1+\\epsilon){\\|k\\|_2}}{\\sqrt{n\\ell}}\\right\\}_+^2 \\ge x \\right]dx \\\\ & \\le \\sum_{\\ell\\in h}\\int_0^{+{\\infty } } { \\mathbb{p}}\\left[\\left\\{{\\|z_\\ell\\|_2 } -   \\frac{(1+\\epsilon){\\|k\\|_2}}{\\sqrt{n\\ell}}\\right\\}_+^2 \\ge x \\right]dx.\\end{aligned}\\ ] ] we bound this with talagrand s inequality .",
    "let @xmath391 be a countable dense subset of the unit ball of @xmath392)$ ] .",
    "we express the norm @xmath393 as @xmath394 \\right)d\\gamma .",
    "\\notag\\end{aligned}\\ ] ] let @xmath395 \\right)d\\gamma.\\ ] ] then @xmath396 , @xmath397 is a sequence of i.i.d random variables with zero mean .",
    "thus , we can apply talagrand s inequality ( see @xcite ) to @xmath398 . for all @xmath399 ,",
    "one has @xmath400 + \\sqrt{2\\nu x } + c(\\eta)bx \\right ) \\le e^{-x},\\ ] ] where @xmath401 , @xmath402 \\right)d\\gamma \\right)^2 \\right],\\ ] ] and , @xmath403 \\right)d\\gamma.\\ ] ]    next , we calculate the terms @xmath404 $ ] , @xmath405 and @xmath406 . applying cauchy - schwarz s inequality and using independence of variables , we get @xmath407 & \\le \\left({\\mathbb{e}}\\big[{\\|z_\\ell\\|_2}^2\\big]\\right)^{1/2 } \\\\ & \\le \\left({\\mathbb{e}}\\left[\\int \\left(\\frac 1n \\sum_{i=1}^n { k_\\ell(\\gamma - \\gamma^1_i)}- { \\mathbb{e}}[{k_\\ell(\\gamma - \\gamma^1_i ) } ] \\right)^2d\\gamma \\right ] \\right)^{1/2 } \\end{aligned}\\ ] ] @xmath408 } & = \\frac 1n\\left(\\int { \\mathbb{e}}\\left[\\left(\\sum_{i=1}^n { k_\\ell(\\gamma - \\gamma^1_i)}- { \\mathbb{e}}[{k_\\ell(\\gamma - \\gamma^1_i ) } ] \\right)^2\\right]d\\gamma \\right)^{1/2 } \\\\ & = \\frac 1n\\left(\\int \\sum_{i=1}^n { \\mathbb{e}}\\left[\\left({k_\\ell(\\gamma - \\gamma^1_i)}- { \\mathbb{e}}[{k_\\ell(\\gamma - \\gamma^1_i ) } ] \\right)^2\\right]d\\gamma \\right)^{1/2 } \\\\ & \\le \\frac 1n\\left(n\\int { \\mathbb{e}}\\left[k_\\ell(\\gamma - \\gamma_1 ^ 1)^2 \\right]d\\gamma \\right)^{1/2 } = \\frac{{\\|k\\|_2}}{\\sqrt{n\\ell}}.\\end{aligned}\\ ] ] for the term @xmath405 , we have @xmath409 \\\\ & \\le",
    "\\frac 1n { \\,\\underset{a\\in { \\mathcal{a}}}{\\sup}\\,}{\\mathbb{e}}\\left[\\int |k_\\ell(\\gamma - \\gamma_1 ^ 1 ) |d\\gamma \\times \\int a^2(\\gamma)|k_\\ell(\\gamma - \\gamma_1 ^ 1)|d\\gamma \\right ] \\\\ & \\le \\frac{{\\|k\\|_{1}}}{n}{\\,\\underset{a\\in { \\mathcal{a}}}{\\sup}\\,}{\\mathbb{e}}\\left[\\int a^2(\\gamma)|k_\\ell(\\gamma - \\gamma_1 ^ 1)|d\\gamma \\right ] \\\\ & \\le \\frac{{\\|k\\|_{1}}}{n}{\\,\\underset{a\\in { \\mathcal{a}}}{\\sup}\\,}\\int a^2(\\gamma){\\mathbb{e}}\\left[|k_\\ell(\\gamma - \\gamma_1 ^ 1)| \\right]d\\gamma \\\\ & \\le \\frac{{\\|k\\|_{1}}}{n}{\\,\\underset{a\\in { \\mathcal{a}}}{\\sup}\\,}\\int\\int a^2(\\gamma)|k_\\ell(\\gamma - u)|h(u)dud\\gamma \\\\ & \\le \\frac{{\\|h\\|_{{\\infty}}}{\\|k\\|_{1}}^2}{n}.\\end{aligned}\\ ] ] for the term @xmath406 , we have @xmath410\\|_2 } \\\\ & \\le \\frac 1n \\left({\\,\\underset{y\\in ( 0,1)}{\\sup}\\,}{\\|k_{\\ell}(\\cdot - y)\\|_2 } + \\left({\\mathbb{e}}\\big[\\int k_{\\ell}^2(\\gamma - \\gamma_1 ^ 1)d\\gamma\\big ] \\right)^{1/2 } \\right ) \\le \\frac{2{\\|k\\|_2}}{n\\sqrt{{\\ell}}}.\\end{aligned}\\ ] ] so , for all @xmath411 , we have @xmath412    let @xmath413 be some strictly positive weights , we apply the previous inequality to @xmath414 for @xmath415 .",
    "we have @xmath416 if we set @xmath417 then , @xmath418 let @xmath419 = \\int_0^{+{\\infty } } { \\mathbb{p}}\\left[{\\,\\underset{{\\ell}\\in h}{\\sup}\\,}\\left({\\|z_\\ell\\|_2 } - \\psi_{\\ell}\\right)^2_+ \\ge x \\right]dx .\\ ] ] an upper bound of @xmath420 is given by @xmath421dx.\\end{aligned}\\ ] ] let us take @xmath422 such that @xmath423 so , @xmath424 hence , @xmath425    we need to choose @xmath426 and @xmath427 such that @xmath428 = { \\mathbb{e}}\\left[{\\,\\underset{\\ell\\in h}{\\sup}\\,}\\left\\{{\\|z_\\ell\\|_2 } - \\frac{(1+\\epsilon){\\|k\\|_2}}{\\sqrt{n\\ell}}\\right\\}_+^2 \\right ] \\le \\lambda.\\ ] ]    let @xmath429 , we choose @xmath430 the we have @xmath431 obviously ,",
    "the series in is finite and for any @xmath358 , since @xmath432 , we have @xmath433 since @xmath434 , if we choose @xmath435 for some @xmath436 , then @xmath437 and we obtain @xmath438 it remains to choose @xmath439 and @xmath440 small enough such that @xmath441 then @xmath442 and we get @xmath443 \\le c_*\\times\\frac{1}{n},\\ ] ] where @xmath384 is a constant depending on @xmath183,@xmath181,@xmath185,@xmath180 and @xmath184 .",
    "hence , we get",
    ".      moreover , since @xmath445 , we have @xmath446 & = { \\mathbb{e}}\\left[\\frac{1}{n_t - n_0}\\right ] = { \\mathbb{e}}\\left[\\frac{n_t}{n_t - n_0}\\frac{1}{n_t } \\right ] = { \\mathbb{e}}\\left[\\frac{1}{1-\\frac{n_0}{n_t}}\\frac{1}{n_t } \\right ] \\notag\\\\ & \\le { \\mathbb{e}}\\left[\\frac{1}{1-\\frac{n_0}{n_0 + 1}}\\frac{1}{n_t } \\right ] \\notag \\\\   & \\le ( n_0 + 1){\\mathbb{e}}\\left[\\frac{1}{n_t}\\right ] .",
    "\\label{eq : inverse - mt}\\end{aligned}\\ ] ]    then , using , and , recall the definition of @xmath204 in , we obtain for any @xmath447 @xmath448\\le c_1\\left({\\|k_{\\ell}\\star h - h\\|_2}^2   + \\frac{{\\|k\\|_2}^2}{{\\ell}}\\varrho(t)^{-1 } \\right ) + c_2\\varrho(t)^{-1 } .\\ ] ] this ends the proof of theorem [ th1 ] .",
    "we begin with the bias term @xmath449 in the right hand side of the oracle inequality .",
    "for any @xmath375 and @xmath133 , let @xmath450 and @xmath451 , then we have @xmath452 since @xmath2 is a kernel of order @xmath196 and @xmath453 , we get @xmath454du.\\end{aligned}\\ ] ] setting @xmath455 for the sake of notation .",
    "since @xmath456 and applying twice the generalized minskowki s inequality , we obtain @xmath457\\|_2}^2 = \\int b^2(\\gamma)d\\gamma \\\\ & \\le \\int\\left(\\int e_{k,\\ell}(u)\\left[\\int_0 ^ 1 ( 1-\\theta)^{k-1}\\big|h^{(k)}(\\gamma + \\theta u\\ell ) -         h^{(k)}(\\gamma)\\big|d\\theta\\right]du \\right)^2d\\gamma \\\\ & \\le \\bigg(\\int e_{k,\\ell}(u)\\left[\\int\\left(\\int_0 ^ 1 ( 1-\\theta)^{k-1}\\big|h^{(k)}(\\gamma + \\theta u\\ell ) -         h^{(k)}(\\gamma)\\big|d\\theta \\right)^2d\\gamma \\right]^{1/2}du \\bigg)^2 \\\\ & \\le \\bigg(\\int e_{k,\\ell}(u ) \\left[\\int_0 ^ 1 ( 1-\\theta)^{k-1}\\left(\\int \\big|h^{(k)}(\\gamma + \\theta u\\ell ) -         h^{(k)}(\\gamma)\\big|^2 d\\gamma \\right)^{1/2}d\\theta \\right]du \\bigg)^2 \\\\ & \\le \\left(\\int e_{k,\\ell}(u ) \\left[\\int_0 ^ 1 ( 1-\\theta)^{k-1 } l(\\theta u\\ell)^{\\beta - k } d\\theta \\right]du \\right)^2 \\\\ & \\le \\left(\\int |k(u)|\\frac{|u\\ell|^k}{(k-1 ) ! } \\left[\\int_0 ^ 1 ( 1-\\theta)^{k-1}l(u\\ell)^{\\beta - k}d\\theta \\right]du \\right)^2 \\\\ & \\le   c_{k , l,\\beta}\\ell^{2\\beta},\\end{aligned}\\ ] ] where @xmath458 .",
    "finally , we have @xmath459\\le c_1\\underset{\\ell\\in h}{\\inf}\\left\\{c_{k , l,\\beta}\\ell^{2\\beta }   + \\frac{{\\|k\\|_2}^2}{{\\ell}}\\varrho(t)^{-1 } \\right\\ } + c_2\\varrho(t)^{-1 } .\\ ] ] taking the derivative of the expression inside the @xmath460 of with respect to @xmath461 , we obtain the minimizer @xmath462 since the optimal bandwidth @xmath463 is proportional to @xmath464 up to a multiplicative constant . therefore , by substituting @xmath147 by @xmath463 in the right hand side of , we obtain @xmath448\\le c_3\\varrho(t)^{-\\frac{2\\beta}{2\\beta+1}},\\ ] ] with @xmath205 a constant depending on @xmath78 , @xmath183 , @xmath181 , @xmath180 , @xmath184 , @xmath185 , @xmath22 and @xmath192 .",
    "this ends the proof of theorem [ th : upper - bound ] .      for @xmath19 ,",
    "let us denote by @xmath209 the estimator of @xmath1 . to prove the theorem [ th : lower - bound ]",
    ", we apply the general reduction scheme proposed by tsybakov @xcite ( section 2.2 , p.79 ) .",
    "we will show the existence of a family @xmath465 such that :    * @xmath466 , @xmath467 .",
    "* @xmath468 . * @xmath469 for @xmath470 . @xmath471 and @xmath472 are the distribution of observations when the division kernels are @xmath473 and @xmath474 , respectively .",
    "@xmath475 denotes the kullback - leibler divergence between two measures @xmath476 and @xmath477 : @xmath478    under the preceding conditions 1 , 2 , 3 , tsybakov @xcite ( theorem 2.5 , p.99 ) show that @xmath479 where the infimum is taken over all estimators @xmath209 and positive constant @xmath480 is independent of @xmath21 .",
    "this will be sufficient to obtain theorem [ th : lower - bound ] by ( * ? ? ? * theorem 2.7 ) .",
    "the proof ends with proposing a family @xmath481 and checking the assumptions 1 , 2 , 3 .",
    "+ _ * construction of the family * _ @xmath481 :      let @xmath484 be a real number , and let @xmath110 , @xmath485 where @xmath43 is a regular function having support @xmath354 and @xmath486 , @xmath487 , we define @xmath488 by definition , the functions @xmath489 s have disjoint support and one can check that the functions @xmath490 .",
    "we now check that @xmath493 is a density , since @xmath494 , it remains to verify that @xmath495 .",
    "we have @xmath496 by the choice of @xmath497 .",
    "thus the family of densities @xmath498 is well - defined .",
    "+ _ * 1 ) the condition @xmath499 * _ : + let us denote @xmath500 , then for all @xmath501 we have @xmath502 which is always satisfied with @xmath503 , thus @xmath504 . + _ * 2 ) the condition @xmath505 * _ : + for all @xmath506 , we have @xmath507^{1/2 } = \\left[\\int_0 ^ 1\\left(c_1\\sum_{k=1}^d ( \\delta_k - \\delta_k')f_k(\\gamma ) \\right)^2d\\gamma \\right]^{1/2 } \\\\ & = c_1\\left[\\int_0 ^ 1\\sum_{k=1}^d ( \\delta_k - \\delta_k')^2f^2_k(\\gamma ) d\\gamma \\right]^{1/2 } = c_1\\left[\\sum_{k=1}^d ( \\delta_k - \\delta_k')^2\\int_{\\frac{k-1}{d}}^{\\frac{k}{d } } f_k^2(\\gamma)d\\gamma\\right]^{1/2}\\\\ & = c_1\\left[\\sum_{k=1}^d ( \\delta_k - \\delta_k')^2\\int_{\\frac{k-1}{d}}^{\\frac{k}{d } } l^2 d^{-2\\beta } g^2\\left(d\\gamma - ( k-1)\\right)d\\gamma\\right]^{1/2 } \\\\ & = c_1l d^{-\\beta - 1/2}{\\|g\\|_2}\\left[\\sum_{k=1}^d ( \\delta_k - \\delta_k')^2 \\right]^{1/2 } = c_1 l d^{-\\beta - 1/2}{\\|g\\|_2}\\sqrt{d_h(\\delta , \\delta')},\\end{aligned}\\ ] ] where @xmath508 is the hamming distance between @xmath183 and @xmath509 .",
    "according to the lemma of varshamov - gilbert ( cf .",
    "tsybakov @xcite , p.104 ) , there exist a subset @xmath510 of @xmath511 with cardinal such that @xmath512 , @xmath513 and @xmath514 then , by setting @xmath515 , @xmath516 , we obtain @xmath517 whenever @xmath518 .",
    "suppose that @xmath519 where @xmath520 .",
    "then , @xmath518 and @xmath521 .",
    "this implies : @xmath522 but , @xmath523 hence , we obtain @xmath524 where @xmath525 _ * 3 ) the condition @xmath526 for @xmath527 * _ : + we need to show that for all @xmath528 , @xmath529 where @xmath530,\\ ] ] and where @xmath531}$ ] is defined in with the random measure @xmath477 having intensity @xmath532 .    here , the difficulty comes from the fact that @xmath83 is variable because the observations result from a stochastic process @xmath533 .",
    "the law of these observations is not a probability distribution on a fixed @xmath534 where @xmath535 would be the sample size , but rather a probability distribution on a path space .",
    "@xmath536 is the probability distribution when the poisson point measure @xmath477 has intensity @xmath537 .",
    "thus a natural tool is to use girsanov s theorem ( see @xcite , theorem 3.24 , p. 159 ) saying that @xmath536 is absolutely continuous with respect to @xmath472 on @xmath538 with @xmath539 where @xmath540}$ ] is the unique solution of the following sde ( see proposition 4.17 of @xcite for a similar sde ) : @xmath541 apply it formula for jump processes to , we get @xmath542q(ds , di , d\\gamma ) \\\\ & = \\int_0^t\\int_{{\\mathcal{e } } } { \\mathds{1}}_{\\{i\\le n_{s-}\\}}\\log\\frac{h_\\delta(\\gamma)}{h_0(\\gamma ) } q(ds , di , d\\gamma ) = \\sum_{i=1}^{n_t}\\log\\frac{h_\\delta(\\gamma^1_i)}{h_0(\\gamma^1_i)}\\end{aligned}\\ ] ] by definition of @xmath543 .",
    "+ then , @xmath544 = { \\mathbb{e}}_\\delta\\left[\\sum_{i=1}^{n_t}\\log\\frac{h_\\delta(\\gamma^1_i)}{h_0(\\gamma^1_i ) } \\right]\\\\ & = { \\mathbb{e}}\\left[n_t \\right]{\\mathbb{e}}_\\delta\\left[\\log\\frac{h_\\delta(\\gamma^1_1)}{h_0(\\gamma^1_1 ) } \\right ] = { \\mathbb{e}}\\left[n_t \\right]\\int_0 ^ 1 h_\\delta(\\gamma)\\log\\frac{h_\\delta(\\gamma)}{h_0(\\gamma)}d\\gamma.\\end{aligned}\\ ] ] here , @xmath545 $ ] does not depend on @xmath493 and we have @xmath278 = n_0e^{rt}$ ] .",
    "thus , recall the definition of @xmath546 and note that @xmath547 for @xmath548 , we get @xmath549 \\\\ & \\le n_0e^{rt}c_1 ^ 2l^2d^{-2\\beta}\\int_0 ^ 1 g^2(\\gamma)d\\gamma   \\\\ & \\le n_0 c_1 ^ 2l^2{\\|g\\|_2}^2 e^{rt } c_0^{-2\\beta } e^{-\\frac{2\\beta}{2\\beta+1}rt } \\\\ & \\le n_0 l^2{\\|g\\|_2}^2 c_0^{-2\\beta-1}d \\quad \\text { since } c_1 \\le 1.\\end{aligned}\\ ] ] from , we have @xmath550 then @xmath551 hence , if we set @xmath552 we obtain @xmath553 .",
    "this ends the proof of theorem [ th : lower - bound ] .",
    "* acknowledgements * + the author is deeply grateful to v.c .",
    "tran , v. rivoirard and t.m .",
    "pham ngoc for the guidance and useful suggestions .",
    "the author would like to express his sincere thanks to p. massart for constructive comments that improved the results .",
    "the author would like to thank centre de resources informatiques de lille 1 ( cri ) for the computational cluster to implement the numerical simulations .",
    "this work is supported by program 911 of vietnam ministry of education and training and is partly supported by the french agence nationale de la recherche ( anr 2011 bs01 010 01 projet calibration ) .",
    "lai , e. jaruga , c. borghouts and s. m. jazwinski1 a mutation in the atp2 gene abrogates the age asymmetry between mother and daughter cells of the yeast saccharomyces cerevisiae . , 162 , 73 - 87 , 2002 ."
  ],
  "abstract_text": [
    "<S> we consider a size - structured population describing the cell divisions . </S>",
    "<S> the cell population is described by an empirical measure and we observe the divisions in the continuous time interval @xmath0 $ ] . </S>",
    "<S> we address here the problem of estimating the division kernel @xmath1 ( or fragmentation kernel ) in case of complete data . </S>",
    "<S> an adaptive estimator of @xmath1 is constructed based on a kernel function @xmath2 with a fully data - driven bandwidth selection method . </S>",
    "<S> we obtain an oracle inequality and an exponential convergence rate , for which optimality is considered .    </S>",
    "<S> * keywords : * random size - structured population , division kernel , nonparametric estimation , goldenshluger - lepski s method , adaptive estimator , penalization , optimal rate . </S>"
  ]
}