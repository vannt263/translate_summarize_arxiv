{
  "article_text": [
    "quantile regression was introduced as a nonparametric method for modelling a variable of interest as a function of covariates @xcite . by estimating the conditional quantiles rather than the mean",
    ", it gives a more complete description of the conditional distribution of the response variable than least squares regression , and is especially relevant in certain types of applications .",
    "consider a random variable @xmath1 with cumulative distribution function @xmath2 .",
    "the @xmath3th quantile function of @xmath1 is defined as @xmath4 and can be obtained by minimising the expected loss @xmath5 $ ] with respect to @xmath6 , where @xmath7 .",
    "the @xmath3th sample quantile is obtained in a similar way by minimising @xmath8 .",
    "suppose that the @xmath3th conditional quantile function , @xmath9 , is a linear function of the predictors so that @xmath10 the parameter estimates @xmath11 are then obtained as @xmath12 a closed - form solution for this minimisation problem does not exist since the objective function is not differentiable at the origin , and it is solved using linear programming techniques @xcite .",
    "the problem with applying quantile regression to count data is that the cumulative distribution function of the response variable is not continuous , resulting in quantiles that are not continuous , and which thus can not be expressed as a continuous function of the covariates .",
    "one way to overcome this problem is by adding uniform random noise ( `` jittering '' ) to the counts @xcite .",
    "the general idea is to construct a continuous variable whose conditional quantiles have a one - to - one relationship with the conditional quantiles of the counts . defining the new continuous variable @xmath13 where @xmath1 is the count variable and",
    "@xmath14 is a uniform random variable in the interval @xmath15 , the conditional quantiles @xmath16    the variable @xmath17 is transformed in such a way that the new quantile function is linear in the parameters , i.e.@xmath18 where    @xmath19    with @xmath20 being a small positive number .",
    "the parameters @xmath21 are estimated by running a linear quantile regression of @xmath22 on @xmath23 .",
    "finally , the conditional quantiles of interest , @xmath24 can be obtained from the previous quantiles as @xmath25 where @xmath26 denotes the ceiling function which returns the smallest integer greater than , or equal to , @xmath3 .",
    "while the jittering approach eliminates the problem of a discrete response distribution , for small values of the response variable @xmath1 , the mean and the variance in the transformed variable @xmath17 will be mainly due to the added noise , resulting in poor estimates of the conditional quantiles @xmath24 .",
    "as an example , when @xmath27 the term @xmath28 could go from @xmath29 to @xmath30 , simply due to the added noise . in addition , quantile regression can suffer from the problem of crossing quantile curves , which is usually seen in sparse regions of the covariate space .",
    "this happens due to the fact that the conditional quantile curve for a given @xmath31 will not be a monotonically increasing function of @xmath3 .",
    "another approach would be to view the counts as ordinal variables with fixed thresholds and then model the new latent variable by an infinite mixture of normal densities @xcite . instead of using the aforementioned methods , we propose an adaptive dirichlet process mixture approach which estimates the conditional density of the data .",
    "the approach is based on an adaptive dirichlet process mixture ( dpm ) of com - poisson regression models .",
    "the com - poisson distribution @xcite is a two - parameter generalisation of the poisson distribution that allows for different levels of dispersion .",
    "the probability mass function of the com - poisson(@xmath32 , @xmath33 ) distribution is @xmath34 where @xmath35 and @xmath36 and @xmath37 , where the normalisation constant does not have a closed form and has to be approximated numerically .",
    "the extra parameter @xmath33 allows the distribution to model under- ( @xmath38 ) or over - dispersed ( @xmath39 ) data , having the poisson distribution as a special case ( @xmath40 ) . the above formulation of the com - poisson does not have a clear centering parameter",
    "since the parameter @xmath32 is close to the mean only when @xmath33 takes values close to @xmath41 , which makes it difficult to interpret for under- or over - dispersed data .",
    "substituting the parameter @xmath32 with @xmath42 , where @xmath43 is the mode of the distribution @xmath44\\approx \\mu , \\ \\ \\",
    "\\mathbb{v}[y]\\approx \\frac{\\mu}{\\nu } \\label{reform}\\ ] ] and the new probability mass function is @xmath45 where @xmath46 .",
    "the com - poisson is flexible enough to approximate distributions with any kind of dispersion in contrast to a poisson or a mixture of poisson distributions which can only deal with overdispersion .",
    "the two parameters of the com - poisson distribution allow it to have arbitrary ( positive ) mean and variance ; one can obtain a point mass by letting the variance parameter @xmath33 tend to infinity .",
    "thus one can show that mixtures of com - poisson distributions can provide an arbitrarily precise approximation to any discrete distribution with support @xmath47 , which is why com - poisson distributions are used by our method .",
    "all other generalisations of the poisson distribution we are aware of do not have this property .      a regression model can be defined based on , in which both the mean and the variance parameter are modelled as a function of covariates : @xmath48 where @xmath1 is the response variable being modelled , and @xmath49 are the regression coefficients for the centering link function and the shape link function respectively .",
    "the parameters in this formulation have a direct link to either the mean or the variance , providing insight into the behaviour of the response variable .",
    "notably , @xmath50\\approx \\exp({\\boldsymbol}{x}_i'{\\boldsymbol}{\\beta } ) , \\ \\ \\",
    "\\mathbb{v}[y]\\approx \\frac{\\exp({\\boldsymbol}{x}_i'{\\boldsymbol}{\\beta})}{\\exp({\\boldsymbol}{x}_i'{\\boldsymbol}{c } ) } = \\exp({\\boldsymbol}{x}_i'({\\boldsymbol}{\\beta}-{\\boldsymbol}{c})).\\ ] ] the calculation of the normalisation constant of the com - poisson distribution is the computationally most expensive part of the proposed regression model .",
    "it can be seen , in the next subsection , that this calculation is redundant .",
    "any probability density function @xmath51 can be written as @xmath52 where @xmath53 is the unnormalised density and the normalising constant @xmath54 is unknown . in this case",
    "the acceptance ratio of the metropolis - hastings algorithm is @xmath55 where @xmath56 is the prior distribution of @xmath57 . the acceptance ratio in involves computing unknown normalising constants .",
    "introducing auxiliary variables @xmath58 and sampling from an augmented distribution @xmath59 results in @xmath60 where the normalising constants cancel out and @xmath61 is a symmetric distribution @xcite . in order to be able to use this algorithm one has to be able to sample from from the unnormalised density which in the case of the com - poisson distribution can be done efficiently using rejection sampling .    updating the parameter @xmath62 of the com - poisson we have @xmath63 and @xmath64 where @xmath65 follows a normal distribution centered at @xmath62 and @xmath66 likewise for updating the parameter @xmath33 .",
    "density regression is similar to quantile regression in that it allows flexible modelling of the response variable @xmath1 given the covariates @xmath67 .",
    "features ( mean , quantiles , spread ) of the conditional distribution of the response variable , vary with @xmath68 , so , depending on the predictor values , features of the conditional distribution can change in a different way than the population mean . the difference between density regression and quantile regression is that density regression models the probability density function or probability mass function rather than directly modelling the quantiles .",
    "this paper focuses on the following mixture of regression models : @xmath69 where @xmath70 the conditional density of the response variable given the covariates is expressed as a mixture of com - poisson regression models with @xmath71 and @xmath72 is an unknown mixture distribution that changes according to the location of @xmath73 .",
    "let @xmath74 denote the @xmath75 distinct values of @xmath76 and let @xmath77 be a vector of indicators denoting the global configuration of subjects to distinct values @xmath78 , with @xmath79 indexing the location of the @xmath80th subject within the @xmath78 .",
    "in addition , let @xmath81 with @xmath82 denoting that @xmath83 is an atom from the basis distribution , @xmath84 .",
    "hence @xmath85 denotes that subject @xmath80 is drawn from the @xmath86th basis distribution .    excluding the @xmath80th subject , @xmath87 denotes the @xmath88 distinct values of @xmath89",
    ", @xmath90 denotes the configuration of subjects @xmath91 to these values and @xmath92 indexes the dp component numbers for the elements of @xmath93 .    grouping the subjects in the same cluster and updating the prior with the likelihood for the data @xmath94",
    ", we obtain the conditional posterior @xmath95 where @xmath96 is the posterior obtained by updating the prior @xmath97 and the likelihood @xmath98 : @xmath99    where @xmath100 are weights that depend on the distance between subjects predictor values , @xmath101 is a normalising constant and @xmath102 . since there is no closed form expression for the posterior distribution , approximation of the probability @xmath103",
    "is difficult .",
    "we overcome this problem by bridging : i ) an mcmc algorithm for sampling from the posterior distribution of a dirichlet process model , with a non - conjugate prior , found in @xcite ; ii ) the mcmc algorithm found in @xcite ; and iii ) a variation of the mcmc exchange algorithm .    the mcmc algorithm alternates between the following steps :    step 1 : : :    update @xmath104 for @xmath105 by    proposing , from the conditional prior , a move to a new cluster or an    already existing cluster with probabilities proportional to    @xmath106 and @xmath107 for    @xmath108    +    1 .",
    "if the proposed move is to go to a new cluster we draw parameters    ( @xmath109 ) for that cluster from @xmath110    and at the same time sample an observation @xmath111 from the    com - poisson(@xmath112 ) .",
    "the acceptance ratio of the    metropolis - hastings algorithm is    @xmath113    if the proposal is accepted , @xmath114 multinomial    @xmath115 .    2 .",
    "if the proposed move is to an already existing cluster    @xmath116 , we sample an observation @xmath111 from the    com - poisson(@xmath117 ) and accept with the same    probability as in . if the proposal is accepted    @xmath118 .",
    "step 2 : : :    update the parameters @xmath83 , for    @xmath119 by sampling from the conditional    posterior distribution    @xmath120    using the metropolis - hasting algorithm with acceptance probability as    in .",
    "step 3 : : :    update @xmath121 , for @xmath119 , by    sampling from the multinomial conditional with    @xmath122    and location weights @xmath123 for    @xmath124 , using an approach used in @xcite .",
    "we consider two simulated data sets to compare the proposed discrete bayesian density regression method to the `` jittering '' method .",
    "these are @xmath125 where @xmath126 .",
    "table   shows the absolute mean errors obtained using both methods .",
    "if @xmath127 is the true conditional quantile when @xmath128 and @xmath129 is the estimated conditional quantile , the mean absolute error is defined as @xmath130 $ ] . the discrete bayesian density regression ( bdr )",
    "estimates outperform the `` jittering '' method and in almost all cases the `` jittering '' method leads to crossing quantiles ( except when @xmath131 ) .",
    "= 0.05 cm    we apply the discrete density regression technique to data on housebreakings in greater glasgow ( scotland ) .",
    "the data consist of the number of housebreakings in each of the 127 intermediate geographies in greater glasgow in 2010 .",
    "we aim to relate the number of housebreakings to the deprivation score of the intermediate geography area , as measured by the scottish index of multiple deprivation ( simd ) .",
    "the deprivation score is standardised by considering the difference of each intermediate geography s deprivation from the average deprivation in greater glasgow e.g. low values relate to affluent areas , large values to deprived areas .",
    "the solid and dashed lines in figure  [ fig : glm2 ] show the quantiles ( for @xmath132 ) obtained for the standard poisson regression model and the com - poisson model respectively .",
    "the first model is not able to capture the overdispersion of the data , nor the skewness of the distribution .",
    "in this manuscript we have proposed a novel bayesian density regression technique for discrete data which is based on mixing com - poisson distributions .",
    "the new method takes advantage of the exchange algorithm and updates the cluster allocations by drawing a new allocation for an auxiliary observation and then accepting or rejecting it . as a result the mcmc samples from the target distribution without the need to estimate the normalisation constant of the likelihood .",
    "the method overcomes the two main drawbacks of the `` jittering '' method for discrete quantile regression , namely that it does not require the addition of artificial additional noise and that it does not suffer from the problem of crossing quantiles .",
    "we have illustrated the method in a real world application as well as simulated examples in which our method compared favourably to the `` jittering '' method .",
    "further research efforts will be devoted in improving the computational speed and efficiency of the mcmc algorithm to make it an even more attractive alternative to `` jittering '' .",
    "mller , j. and pettitt , a. n. and reeves , r. and berthelsen , k. k. ( 2006 ) _ an efficient markov chain monte carlo method for distributions with intractable normalising constants .",
    "_ biometrika , 93**2 * * , 451458 .",
    "shmueli , galit and minka , thomas p. and kadane , joseph b. and borle , sharad and boatwright , peter ( 2008 ) _ a useful distribution for fitting discrete data : revival of the conway - maxwell - poisson distribution .",
    "_ journal of the royal statistical society : series c , * 54 * , 127142 ."
  ],
  "abstract_text": [
    "<S> despite the increasing popularity of quantile regression models for continuous responses , models for count data have so far received little attention . </S>",
    "<S> the main quantile regression technique for count data involves adding uniform random noise or `` jittering '' , thus overcoming the problem that the conditional quantile function is not a continuous function of the parameters of interest . </S>",
    "<S> although jittering allows estimating the conditional quantiles , it has the drawback that , for small values of the response variable @xmath0 the added noise can have a large influence on the estimated quantiles . </S>",
    "<S> in addition , quantile regression can lead to `` crossing '' quantiles . </S>",
    "<S> we propose a bayesian dirichlet process ( dp)-based approach to quantile regression for count data . </S>",
    "<S> the approach is based on an adaptive dp mixture ( dpm ) of com - poisson regression models and determines the quantiles by estimating the density of the data , thus eliminating all the aforementioned problems . </S>",
    "<S> taking advantage of the exchange algorithm , the proposed mcmc algorithm can be applied to distributions on which the likelihood can only be computed up to a normalising constant . </S>"
  ]
}