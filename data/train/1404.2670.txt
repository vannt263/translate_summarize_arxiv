{
  "article_text": [
    "many recent survey articles on the challenges of achieving exascale computing identify three issues to be overcome : exploiting massive parallelism , reducing energy usage and , in particular , coping with run - time failures  @xcite . faults are an issue at peta / exa - scale due to the increasing number of components in such systems .",
    "traditional checkpoint - restart based solutions become unfeasible at this scale as the decreasing mean time between failures approaches the time required to checkpoint and restart an application .",
    "algorithm based fault tolerance has been studied as a promising solution to this issue for many problems  @xcite .",
    "sparse grids were introduced in the study of high dimensional problems as a way to reduce the _ curse of dimensionality_. they are based on the observation that when a solution on a regular grid is decomposed into its hierarchical bases the highest frequency components contribute the least to sufficiently smooth solutions .",
    "removing some of these high frequency components has a small impact on the accuracy of the solution whilst significantly reducing the computational complexity  @xcite .",
    "the combination technique was introduced to approximate sparse grid solutions without the complications of computing with a hierarchical basis . in recent years these approaches",
    "have been applied to a wide variety of applications from real time visualisation of complex datasets to solving high dimensional problems that were previously cumbersome  @xcite .",
    "previously  @xcite it has been described how the combination technique can be implemented within a _ map reduce _ framework .",
    "doing so allows one to exploit an extra layer of parallelism and fault tolerance can be achieved by recomputing failed map tasks as described in  @xcite .",
    "also proposed was an alternative approach to fault tolerance in which recomputation can be avoided for a small trade off in solution error . in  @xcite we demonstrated this approach for a simple two - dimensional problem showing that the average solution error after simulated faults was generally close to that without faults . in this paper",
    "we develop and discuss this approach in much greater detail .",
    "in particular we develop a general theory for computing new combination coefficients and discuss a three - dimensional implementation based on mpi and openmp which scales well for relatively small problems .",
    "as has been done in the previous literature  @xcite , we use the solution of the scalar advection pde for our numerical experiments .",
    "the remainder of the paper is organised as follows . in section",
    "[ sec : back ] we review the combination technique and provide some well - known results which are relevant to our analysis of the fault tolerant combination technique .",
    "we then develop the notion of a general combination technique",
    ".    in section  [ sec : faults ] we describe how the combination technique can be modified to be fault tolerant as an application of the general combination technique . using a simple model for faults on each node of a supercomputer we are able to model the failure of component grids in the combination technique and apply this to the simulation of faults in our code .",
    "we present bounds on the expected error and discuss in how faults affect the scalability of the algorithm as a whole .    in section  [ sec : implem ]",
    "we describe the details of our implementation .",
    "in particular we discuss the multi - layered approach and the way in which multiple components work together in order to harness the many levels of parallelism .",
    "we also discuss the scalability bottleneck caused by communications and several ways in which one may address this .    finally , in section  [ sec : numres ] we present numerical results obtained by running our implementation with simulated faults on a pde solver .",
    "we demonstrate that our approach scales well to a large number of faults and has a relatively small impact on the solution error .",
    "we introduce the combination technique and a classical result which will be used in our analysis of the fault tolerant algorithm . for a complete introduction of the combination technique one should refer to  @xcite .",
    "we then go on to extend this to a more general notion of a combination technique building on existing work on adaptive sparse grids  @xcite .",
    "let @xmath0 , then we define @xmath1 to be a discretisation of the unit interval .",
    "similarly for @xmath2 we define @xmath3 as a grid on the unit @xmath4-cube . throughout the rest of this paper",
    "we treat the variables @xmath5 as multi - indices in @xmath6 .",
    "we say @xmath7 if and only if @xmath8 for all @xmath9 , and similarly , @xmath10 if and only if @xmath7 and @xmath11 .",
    "now suppose we have a problem with solution @xmath12^{d})$ ] , then we use @xmath13 to denote the function space consisting of piecewise linear functions uniquely determined by their values on the grid @xmath14 .",
    "further , we denote an approximation of @xmath15 in the space @xmath16 by @xmath17 .",
    "the sparse grid space of level @xmath18 is defined to be @xmath19 .",
    "a sparse grid solution is a @xmath20 which closely approximates @xmath21 .",
    "the combination technique approximates a sparse grid solution by taking the sum of several solutions from different anisotropic grids .",
    "the classical combination technique is given by the equation @xmath22 fundamentally , this is an application of the inclusion / exclusion principle .",
    "this can be seen if the function spaces are viewed as a lattice  @xcite .",
    "for example , if one wishes to add the functions @xmath23 and @xmath24 then the result will have two contributions from the intersection space @xmath25 , with @xmath26 . to avoid this",
    "we simply take @xmath27 .",
    "this can be seen in figure  [ fig : ct2d ] , which shows a level 4 combination in 2 dimensions .",
    "iin 0, ... ,4 ; ( -0.06 + 1.5 * , 5.94 - 1.5 * )  ( -0.06 + 1.5*+1.4 , 5.94 - 1.5 * ) ; ( -0.06 + 1.5 * , 5.94 - 1.5 * ) ",
    "( -0.06 + 1.5 * , 5.94 - 1.5*+1.4 ) ; ( -0.06 + 1.5*+1.4 , 5.94 - 1.5 * )  ( -0.06 + 1.5*+1.4 , 5.94 - 1.5*+1.4 ) ; ( -0.06 + 1.5 * , 5.94 - 1.5*+1.4 )  ( -0.06 + 1.5*+1.4 , 5.94 - 1.5*+1.4 ) ; ( 0.64 + 1.5 * -0.25 , 6.0 + 0.64 - 1.5 * )  ( 0.64 + 1.5*+0.25 , 6.0 + 0.64 - 1.5 * ) ; ( 0.64 + 1.5 * , 6.0 + 0.64 - 1.5 * -0.25 )  ( 0.64 + 1.5 * , 6.0 + 0.64 - 1.5*+0.25 ) ; iin 0, ... ,3 ; ( -0.06 + 1.5 * , 4.44 - 1.5 * )  ( -0.06 + 1.5*+1.4 , 4.44 - 1.5 * ) ; ( -0.06 + 1.5 * , 4.44 - 1.5 * ) ",
    "( -0.06 + 1.5 * , 4.44 - 1.5*+1.4 ) ; ( -0.06 + 1.5*+1.4 , 4.44 - 1.5 * ) ",
    "( -0.06 + 1.5*+1.4 , 4.44 - 1.5*+1.4 ) ; ( -0.06 + 1.5 * , 4.44 - 1.5*+1.4 )  ( -0.06 + 1.5*+1.4 , 4.44 - 1.5*+1.4 ) ; ( 0.64 + 1.5 * -0.25 , 4.5 + 0.64 - 1.5 * )  ( 0.64 + 1.5*+0.25 , 4.5 + 0.64 - 1.5 * ) ; iin 0, ... ,33 ; ; at ( 0.0 + 1.28*,6.0 + 0.08 * ) ; at ( 6.0 + 0.08*,0.0 + 1.28 * ) ; iin 0, ... ,26 ; ; at ( 1.5 + 0.64*,4.5 + 0.16 * ) ; at ( 4.5 + 0.16*,1.5 + 0.64 * ) ; iin 0, ... ,24 ; ; at ( 3.0 + 0.32*,3.0 + 0.32 * ) ; iin 0, ... ,17 ; ; at ( 0.0 + 1.28*,4.5 + 0.16 * ) ; at ( 4.5 + 0.16*,0.0 + 1.28 * ) ; iin 0, ... ,14 ; ; at ( 1.5 + 0.64*,3.0 + 0.32 * ) ; at ( 3.0 + 0.32*,1.5 + 0.64 * ) ; iin 0, ... ,33 ; ; at ( 4.5 + 2.56*,4.5 + 0.16 * ) ; at ( 4.5 + 0.16*,4.5 + 2.56 * ) ; iin 0, ... ,8 ; ; at ( 4.5 + 1.28,4.5 + 0.32 * ) ; at ( 4.5 + 0.32*,4.5 + 1.28 ) ; iin 0, ... ,9 ; ; at ( 4.5 + 0.64 + 1.28*,4.5 + 0.64 * ) ;    an important concept in the development of sparse grids is that of the hierarchical space ( sometimes also referred to as a hierarchical surplus ) . a simple definition of the hierarchical space @xmath28 is the space of all functions @xmath29 such that @xmath30 is zero when sampled on all grid points in the set @xmath31 .",
    "equivalently we have @xmath32 . noting that @xmath33 , a hierarchical decomposition of @xmath23 is the computation of the unique components @xmath34 for @xmath35 such that @xmath36 .",
    "the sparse grid space can also be written in terms of hierarchical spaces as @xmath37 .",
    "let @xmath38 be the sobolev - space with dominating mixed derivatives with norm @xmath39 if @xmath40 then we have the estimate @xmath41 for each of the hierarchical spaces where @xmath42 is a semi - norm  @xcite . in the classical theory of the combination technique this estimate",
    "is used to prove the error bound @xmath43 for sparse grid interpolation .",
    "similar bounds can be shown for the @xmath44 and energy norms  @xcite .    in practice , strongly anisotropic grids ,",
    "i.e.  those with @xmath45 , can be problematic .",
    "not only are they difficult to compute in some circumstances but one also finds that they give poor approximations that do not cancel out in the combination as expected .",
    "therefore it is often beneficial to implement a _ truncated _ combination . in this paper",
    "we define a truncated combination as @xmath46 where @xmath47 is referred to as the truncation parameter .",
    "such combinations will be used for the numerical results presented in section  [ sec : numres ] .",
    "the combination technique can be generalised into arbitrary sums of solutions . given a ( finite ) set of multi - indices @xmath48 ,",
    "one can write @xmath49 where the @xmath50 are referred to as the combination coefficients .",
    "it is easy to see that @xmath51 where @xmath52 . not",
    "every choice of the @xmath50 will produce a reasonable approximation to @xmath15 .",
    "the question now is which combination coefficients produce the best approximation to @xmath15 ?",
    "one could attempt to solve the optimisation problem of minimising for example @xmath53 which is known as _",
    "opticom _  @xcite .",
    "however this is cumbersome to solve in a massively parallel implementation and also requires an approximation of the residual . in this paper",
    "we consider combinations which we know _ a priori _ will give a good approximation in some sense .",
    "we define a _",
    "sensible _ combination to be one in which each hierarchical space contributes either once to the solution , or not at all .",
    "in particular we would like the coefficients to follow an inclusion / exclusion as described in section  [ sec : cct ] .",
    "we observe that @xmath54 for all @xmath55 , therefore one can easily determine how many times a given @xmath28 contributes to the solution by summing all of the coefficients @xmath56 for which @xmath54 . in light of this",
    "we have the following definition .",
    "a set of combination coefficients @xmath57 is said to be * _ valid _ * if for each @xmath58 it satisfies the property @xmath59 we will also refer to a combination ( of solutions ) as valid if the corresponding set of combination coefficients are valid .",
    "another common assumption is that @xmath60 . this relates to the definition if @xmath61 in which case we could assert that @xmath62 .",
    "the motivation for the definition is that this property is satisfied for dimension adaptive sparse grids  @xcite",
    ". let @xmath63 be a lattice of projection operators associated with the tensor product space @xmath64 .",
    "as in @xmath65 satisfies @xmath66 ( where @xmath67 ) , @xmath68 and @xmath69 .",
    "defining @xmath70 it follows from  @xcite that for @xmath58 @xmath71 since @xmath72 it follows that @xmath73 .    for a given @xmath74",
    "there are a many sets of valid combination coefficients that one might take .",
    "to determine which of these _ a priori _ will give the best approximation of @xmath15 we use approximations which are based on sparse grid interpolation .",
    "it is reasonable to expect that such combinations will work well in a more general setting given the underlying inclusion / exclusion principle .",
    "the error estimate   for sparse grid interpolation is extended to the general combination technique by @xmath75 finding a set of valid coefficients which minimises this this bound is then equivalent to maximising @xmath76    therefore , the general problem of finding the best combination of solutions @xmath17 for @xmath58 can be formulated as an optimisation problem , in particular the maximisation of @xmath77 subject to the constraints @xmath78 for each @xmath58 .",
    "since the @xmath50 must be integers this would be a simple integer linear programming ( ilp ) problem if not for the non - trivial constraints .",
    "( to see why the @xmath50 must be integers , we note that the non - zero @xmath50 for which @xmath79 for all @xmath80 must be @xmath81 in order for the set of coefficients to be valid .",
    "the remaining coefficients are now obtained from the application of the inclusion / exclusion principle which can only result in integer coefficients . )",
    "fortunately we can simplify this by introducing the hierarchical coefficient .",
    "let @xmath74 be a set of multi - indices , then for @xmath82 we define the * _ hierarchical coefficient _ * @xmath83    suppose we expand our list of coefficients to the set @xmath84 with the assumption @xmath85 for @xmath86 .",
    "now let @xmath87 be vectors for the sets @xmath88 , respectively ( both having the same ordering with respect to @xmath89 ) . using",
    "we can write @xmath90 where @xmath91 is an @xmath92 matrix .",
    "further , we note that if the elements of @xmath87 are ordered according to ascending or descending values of @xmath93 then @xmath91 is an upper or lower triangular matrix respectively with @xmath81 s on the diagonal",
    ". therefore @xmath91 is invertible and we have @xmath94 .",
    "additionally , the restriction that @xmath85 for @xmath86 can be written as @xmath95 .    since @xmath96 for any set of valid coefficients we can formulate the _ general coefficient problem _ ( gcp ) as the binary integer programming ( bip ) problem of maximising @xmath97 with the equality constraints @xmath95 for @xmath86 .",
    "this is much more manageable in practice and can be solved using a variety of algorithms that are typically based on branch and bound , and/or cutting plane techniques .",
    "however , this formulation also reveals that the general coefficient problem is np - complete  @xcite .",
    "equivalently , one can also think of this as a weighted maximum satisfiability problem ( weighted max - sat ) .",
    "if @xmath74 is a downset then we can solve this rather quickly , but in general there exist cases which take an incredibly long time to solve .",
    "another problem one runs into is that there is often not a unique solution . in such circumstances",
    "we will simply pick any one of the solutions as they can not be further distinguished without additional information about @xmath15 .",
    "the only way to guarantee that a solution can be found quickly is to carefully choose the index set @xmath74 .",
    "one particular class of index sets of interest are those which are closed under the @xmath98 operator , that is if @xmath99 then @xmath100 . in the theory of partially ordered sets ,",
    "@xmath101 with this property is referred to as a lower semi - lattice . for such @xmath74",
    "there is a unique solution to the gcp , namely @xmath102 for all @xmath82 which clearly maximises  .",
    "computationally the coefficients can be found quickly by first finding @xmath103 , setting @xmath104 for @xmath105 , and then using the inclusion / exclusion principle to find the remaining coefficients in the order of descending @xmath93 .",
    "this can also be viewed as an application of the lattice theory of projections on function spaces presented by hegland  @xcite .    whilst the gcp presented is based upon @xmath106",
    "we anticipate that the resulting combinations will still yield reasonable results for larger function spaces .",
    "this is based on the observation that the classical combination technique has been successfully applied to a wide variety of problems for which @xmath107 .",
    "the restriction of @xmath108 to binary variables can be relaxed to reals if we change the quantity we intend to optimise .",
    "the important observation to make here is that one would expect having two contributions from a hierarchical space is comparable to having no contributions . further having a fractional contribution like @xmath109 would be better than having no contribution at all . in light of this",
    "we can try to solve the linear programming problem of minimising @xmath110 subject to the equality constraints @xmath95 for @xmath86 .",
    "if @xmath74 is closed under @xmath98 then a minimum of @xmath111 is achieved for the same hierarchical coefficients found in the binary formulation . in other cases",
    "the solution to this relaxed optimisation problem is no worse than the solution to the binary problem .",
    "this relaxation means the combination may no longer follow an inclusion / exclusion principle . in practice",
    "the results are highly dependant upon the true solution @xmath15 and the approximation properties of each of the @xmath17 .",
    "additionally , the non - differentiability of equation means that the problem is still non - trivial to solve in practice .",
    "an approach that can sometimes speed up the computation of a solution to this problem is to first find the solution to the quadratic programming problem of minimising @xmath112 subject to the same equality constraints .",
    "this is a linear problem which is easily solved using the method of lagrange multipliers for example . in most circumstances",
    "we would expect the solution of this problem to be close to the minimum of equation and therefore make a good initial guess .",
    "in  @xcite a fault tolerant combination technique was introduced . the most difficult aspect of generalising this work is the updating of coefficients .",
    "whilst some theory and a few simple cases have been investigated , no general algorithm has been presented .",
    "given the development of the general combination technique in section  [ sec : gct ] we are now able to consider a more complete theory of the ftct .",
    "suppose we have a set @xmath74 of multi - indices for which we intend to compute each of the solutions @xmath17 and combine as in .",
    "as each of the @xmath17 can be computed independently the computation of these is easily distributed across different nodes of a high performance computer .",
    "suppose that one or more of these nodes experiences a fault , hardware or software in nature . as a result , some of our @xmath17 may not have been computed correctly .",
    "we denote @xmath113 to be the set of indices for which the @xmath17 where not correctly computed . a lossless approach",
    "to fault tolerance would be to recompute @xmath17 for @xmath114 .",
    "however , since recomputation is often costly , we propose a lossy approach to fault tolerance in which the failed solutions are not recomputed . in this approach , rather than solving the generalised coefficient problem ( gcp ) for @xmath74 , we instead solve it for @xmath115 .",
    "as @xmath116 we expect this solution to have a larger error than that if no faults had occurred . however , if @xmath117 is relatively small we would also expect the loss of accuracy to be small because of the redundancy in the set of @xmath118 .    as discussed in section  [ sec : gct ] , the gcp is difficult to solve in its most general form . whilst it can be solved rather quickly",
    "if the poset @xmath101 is a lower semi - lattice , this is no longer any help in the ftct since the random nature of faults means we can not guarantee that @xmath119 is always a lower semi - lattice .",
    "the only way we could ensure this is to restrict which elements of @xmath74 can be in @xmath120 . a simple way to achieve",
    "this is to recompute missing @xmath17 if @xmath121 is not a lower semi - lattice . in particular",
    "this is achieved if all @xmath17 with @xmath122 are recomputed .",
    "since elements in @xmath123 correspond to the solutions on the largest of the grids , we are avoiding the recomputation of the solutions which take the longest to compute .",
    "additionally , this also means only the largest of the hierarchical spaces are ever omitted as a result of a failure .",
    "as these contribute the least to the solution we expect the resulting error to be relatively close to that of the solution if no faults had occurred .",
    "finally , since @xmath124 is then a lower semi - lattice , the resulting gcp for @xmath115 has a unique maximal solution which is easily computed .",
    "we now illustrate this approach as it is applied to the classical combination technique .",
    "we define @xmath125 .",
    "it was shown in  @xcite that the proportion of additional unknowns in computing the solutions @xmath17 for all @xmath126 compared to @xmath127 is at most @xmath128 .",
    "if no faults occur then the combination is exactly the classical combination technique with @xmath129 if @xmath130 and @xmath85 otherwise .",
    "if faults do occur then we recompute any @xmath17 with @xmath131 that was not successfully computed .",
    "if no faults occurred for any @xmath17 with @xmath132 then we can again proceed with the classical combination .",
    "if faults affect any @xmath17 with @xmath133 then we add such @xmath134 to the set @xmath120 and then solve the gcp for @xmath135 .",
    "the solution is trivially obtained with hierarchical coefficients @xmath102 for all @xmath136 . the largest solutions ( in terms of unknowns ) which may have to be recomputed are those with @xmath137 which would be expected to take at most half the time of those solutions with @xmath133 . since they take less time to compute they are also less likely to be lost due to failure .",
    "additionally , there are @xmath138 solutions with @xmath137 which is less than the @xmath139 with @xmath133 . as a result of these observations",
    ", we would expect to see far less disruptions caused by recomputations when using this approach compared to a lossless approach where all failed solutions are recomputed .",
    "the worst case scenario with this approach is that all @xmath17 with @xmath133 are not successfully computed due to faults . in this case",
    "the resulting combination is simply a classical combination of level @xmath140 .",
    "this only requires the solutions @xmath17 with @xmath141 .",
    "likewise , all solutions to the gcp in this approach result in zero coefficients for all @xmath50 with @xmath142 .",
    "we can therefore reduce the overhead of the ftct by only computing the solutions @xmath17 for @xmath143 .",
    "it is known that the proportion of additional unknowns compared to the classical combination technique in this case is at most @xmath144  @xcite .",
    "the solutions @xmath17 with @xmath137 are only half the size of the largest @xmath17 and hence recomputation of these may also be disruptive and undesirable",
    ". we could therefore consider recomputing only solutions with @xmath145 . by doing this",
    "the recomputations are even more manageable having at most one quarter the unknowns of the largest @xmath17 .",
    "the worst case here is that all solutions with @xmath146 fail and we end up with a classical combination of level @xmath147 .",
    "again it turns out one does not require the entire downset @xmath148 , in this case the ( modified ) ftct requires solutions @xmath17 with @xmath149 . using arguments similar to those in  @xcite it is easily shown that the overhead in this case is at most @xmath150 .",
    "the trade - off now is that the update of coefficients takes a little more work .",
    "we are back in the situation where we can not guarantee that @xmath151 is a lower semi - lattice .    to solve the gcp in this case we start with all @xmath108 equal to @xmath81 .",
    "if failures affected any @xmath17 with @xmath133 we set the corresponding constraints @xmath152 . for failures occurring on @xmath17 with @xmath137 we have the constraints @xmath153 ( with @xmath154 being the multi - index with @xmath155 ) .",
    "we note that ( since the @xmath108 are binary variables ) this can only be satisfied if at most one of the @xmath156 is equal to @xmath81 .",
    "further , if @xmath157 we must also have @xmath158 .",
    "this gives us a total of @xmath159 feasible solutions to check for each such constraint .",
    "given @xmath160 failures on solutions with @xmath137 we have at most @xmath161 feasible solutions to the gcp to check .",
    "this can be kept manageable if solutions are combined frequently enough that the number of failures @xmath160 that are likely occur in between is small .",
    "one solves the gcp by computing the objective function   for each of the feasible solutions identified and selecting one which maximises this . where some of the failures on the second layer are sufficiently far apart on the lattice , it is possible to significantly reduce the number of cases to check as constraints can be optimised independently",
    ". we could continue and describe an algorithm for only recomputing the fourth layer and below , however the coefficient updates here begin to become much more complex ( both to describe and to compute ) .",
    "our experience indicates that the recomputation of the third layer and below is a good trade - off between the need to recompute and the complexity of updating the coefficients . our numerical results in section  [ sec : numres ]",
    "are obtained using this approach .      to analyse the expected outcome of the fault tolerant combination technique described in section  [ sec : ftct ]",
    "we need to know the probability of each @xmath17 failing .",
    "in particular , the availability of @xmath17 will be modelled as a simple bernoulli process @xmath162 which is @xmath111 if @xmath17 was computed successfully and is @xmath81 otherwise .",
    "it is assumed that each @xmath17 is computed on a single computational node .",
    "therefore we are interested in the probability that a failure occurs on this node before the computation of @xmath17 is complete , that is @xmath163 .",
    "suppose @xmath164 is a random variable denoting the time to failure on a given node and the time required to compute @xmath17 is given by @xmath165 , then one has @xmath166 .",
    "one therefore needs to know something about the distribution of @xmath164 .",
    "schroeder and gibson analysed the occurrence of faults on @xmath167 high performance machines at lanl from 1996 - 2005  @xcite .",
    "they found that the distribution of time between failures for a typical node in the systems studied was best fit by the weibull distribution with a shape parameter of @xmath168 .",
    "based upon this study we will consider a model of faults on each node based upon the weibull renewal process , that is a renewal process where inter - arrival times are weibull distributed , with shape parameter @xmath169 .",
    "there are several reasons for considering a renewal process for modelling faults .",
    "first , renewal theory is commonly used in availability analysis and there are many extensions such as alternating renewal processes in which one can also consider repair times .",
    "second , we expect a fault tolerant implementation of mpi to enable the substitution of a failed node with another available node in which case computation can continue from some recovered state .",
    "this will be further discussed in section  [ sec : fs ] .",
    "we now derive the value of @xmath163 .",
    "let @xmath170 be random variables for the successive times between failures on a node .",
    "we assume that the @xmath171 are positive , independent and identically distributed with cumulative distribution @xmath172 for some @xmath173 and @xmath174 .",
    "let @xmath175 ( for @xmath176 ) be the waiting time to the @xmath177th failure .",
    "let @xmath178 count the number of failures that have occured up until ( and including ) time @xmath179 , that is @xmath180 ) . by the elementary renewal theorem one has @xmath181=\\frac{1}{\\mathbb{e}[x_{1}]}=\\frac{1}{\\lambda\\gamma(1+\\frac{1}{\\kappa } ) }",
    "\\,.\\ ] ] we thus get an expression for the ( long term ) average rate of faults .",
    "now , whilst we have a distribution for the time between failures , when a computation starts it is generally unknown how much time has elapsed since the last failure occurred .",
    "hence our random variable @xmath164 is what is referred to as the random incidence ( or residual lifetime ) . noting that one is more likely to intercept longer intervals of the renewal process than shorter ones and that the probability distribution of the starting time is uniform over the interval , then it is straightforward to show  @xcite that @xmath164 has density @xmath182}=\\frac{e^{-(t/\\lambda)^{\\kappa}}}{\\lambda\\gamma(1+\\frac{1}{\\kappa } ) } \\,.\\ ] ] it follows that the cumulative probability distribution @xmath183 the resulting distribution has similar properties to the original weibull distribution .",
    "in fact , when @xmath184 we note that @xmath171 and @xmath164 are identically distributed , they are exponential with mean @xmath185 .",
    "further , for @xmath169 we have the following bound :    [ lem : rib ] for @xmath169 one has @xmath186    we note that via a change of variables that @xmath187 and therefore @xmath188_{0}^{\\infty } -\\int_{0}^{\\infty}-\\frac{1}{\\lambda}e^{-(x/\\lambda)^{\\kappa}-(t/\\lambda)^{\\kappa } } dx \\\\ & = \\int_{0}^{\\infty}\\frac{1}{\\lambda}e^{-(x/\\lambda)^{\\kappa}-(t/\\lambda)^{\\kappa } } dx \\,.\\end{aligned}\\ ] ] since @xmath169 and @xmath189 one has @xmath190 and hence @xmath191 rearranging gives @xmath192 which is the desired inequality .    as a result of lemma  [ lem : rib ] one",
    "has that the probability of @xmath17 failing to compute successfully is bounded above by @xmath193    the result of lemma  [ lem : rib ] is essentially a consequence of the property @xmath194 where @xmath195 is weibull distributed with shape parameter @xmath169 and @xmath196 .",
    "this property can be extended to the fact that if @xmath197 then @xmath198 this has important implications on the order in which we compute successive solutions on a single node .",
    "solutions one is least concerned about not completing due to a fault should be computed first and solutions for which we would like to minimise the chance of failure should be computed later .",
    "we note that for @xmath199 the inequalities of equations   and   are reversed thus @xmath200 and @xmath201 for @xmath196 .",
    "we first describe the parallel ftct algorithm :    * given a ( finite ) set of multi - indices @xmath74 , distribute the computation of component solutions @xmath17 for @xmath58 amongst the available nodes . *",
    "each node begins to compute the @xmath17 which have been assigned to it . for time",
    "evolving pde s the solvers are evolved for some fixed simulation time @xmath202 . * on each node ,",
    "once a @xmath17 is computed a checkpoint of the result is saved .",
    "if the node experiences a fault during the computation of a @xmath17 , a fault tolerant implementation of mpi is used to replace this node with another ( or continue once the interrupted node is rebooted ) . on the new node ,",
    "checkpoints of previously computed @xmath17 are loaded and we then assess whether the interrupted computation should be recomputed or discarded . if it is to be recomputed this is done before computing any of the remaining @xmath17 allocated to the node .",
    "* once all nodes have completed their computations they communicate which @xmath17 have been successfully computed via a mpi_allreduce .",
    "all nodes now have a list of multi - indices @xmath203 and can solve the gcp to obtain combination coefficients . * all nodes",
    "compute a partial sum @xmath204 for the @xmath17 that it has computed and then the sum is completed globally via a mpi_allreduce such that all nodes now have a copy of @xmath205 . * in the case of a time evolving pde the @xmath17 can be sampled from @xmath205 and the computation further evolved by repeating from 2 .",
    "the optional step 6 for time evolution problems has many advantages .",
    "first , by combining component solutions several times throughout the computation one can improve the approximation to the true solution .",
    "second , each combination can act like a global checkpoint such that when a @xmath17 fails it can be easily restarted from the last combination ( rather than the very beginning ) .",
    "third , there are potential opportunities to reassess the load balancing after each combination and potentially re - distribute the @xmath17 to improve performance in the next iteration .    for the numerical results in section  [ sec : numres ] we do not currently use a fault tolerant implementation of mpi and instead simulate faults by modifying the following steps :    * before each node begins computing the @xmath17 assigned to it , a process on the node computes a ( simulated ) time of failure by sampling a distribution for time to failure and adding it to the current time . in our results we sample the weibull distribution for some mean @xmath206 and shape @xmath169 .",
    "* immediately after a @xmath17 has been computed on a node we check to see if the current time has exceeded the ( simulated ) time of failure",
    ". if this is the case the most recent computation is discarded .",
    "we then pretend that the faulty node has been instantly replaced and continue with step 3 as described .",
    "note from section  [ sec : pfc ] that sampling the weibull distribution produces faults at least as often as the random incidence @xmath207 .",
    "thus , by sampling the weibull distribution in our simulation , the results should be no worse than what might occur in reality allowing for some small discrepancy in fitting the weibull distribution to existing data of time between failures on nodes of a real machine .",
    "the assumption in step 3 of replacing a failed node with another is based upon what one might expect from a fault tolerant mpi .",
    "in fact both harness ft - mpi and the relatively new ulfm specification allow this , although it certainly does not occur in an instant as is assumed in our simulation . due to limited data at the current time we are unable to predict what recovery times one might expect .",
    "we also note that ( simulated ) failures are checked for at the completion of the computation of each @xmath17 . since a failure is most likely to occur some time before the computation completes then time is wasted in the simulation from the sampled time of failure to the completion of the affected computation . improving these aspects of the simulation and implementation with a fault",
    "tolerant mpi will be the subject of future work .      in this section",
    ", we bound the expected interpolation error for the ftct as applied to the classical combination technique as described in section  [ sec : ftct ] .",
    "in particular we look at the case where all solutions with @xmath131 are recomputed , and the case where all solutions with @xmath208 are recomputed as described in section  [ sec : ftct ] .",
    "given @xmath40 , let @xmath209 such that @xmath210 , see  . now given a ( finite ) set of multi - indices @xmath74 we denote @xmath211 to be a combination @xmath212 which is a solution to the gcp described in section  [ sec : gct ] .",
    "when faults prevent successful computation of some of the @xmath17 we must find @xmath205 for some @xmath213 . consider the bernoulli process @xmath214 for which each @xmath215 if @xmath17 is computed successfully and is @xmath81 otherwise as described in section  [ sec : pfc ] .",
    "additionally it is assumed that the computation of each @xmath17 is done within one node , that is many @xmath17 can be computed simultaneously on different nodes but each individual @xmath17 is computed within one hardware node .",
    "let @xmath165 be the time required to compute @xmath17 for each @xmath58 .",
    "we assume that the time between failures on each node is weibull distributed . as demonstrated in section  [ sec : pfc ] the probability of each @xmath17 being lost a the result of a fault is given by the random incidence distribution @xmath193 given that @xmath17 with the same @xmath93 have a similar number of unknowns we assume they will take roughly the same amount of time to compute .",
    "we therefore define @xmath216 , that is the maximal time to compute any @xmath17 with level @xmath177 .    as a result , for each @xmath58 the probability of each @xmath17 not completing due to failures is bounded by @xmath217",
    ". with this we can now give the main result .",
    "[ prop : err1 ] given @xmath218 and @xmath219 let @xmath17 be the interpolant of @xmath40 for @xmath126 .",
    "let each @xmath17 be computed on a different node of a parallel computer for which the time between failures on every node is independent and identically weibull distributed with mean @xmath206 and shape parameter @xmath169 .",
    "let @xmath165 be the ( wall ) time required to compute each @xmath17 and @xmath220 .",
    "suppose we recompute any @xmath17 with @xmath131 which is interrupted by a fault , let @xmath221 be the set of all possible @xmath222 for which @xmath17 was successfully computed ( eventually ) iff @xmath223 .",
    "let @xmath224 be the function - valued random variable corresponding to the result of the ftct ( i.e. @xmath205 for some random @xmath225 ) , then the expected error is bounded above by @xmath226 \\leq\\epsilon_{n}\\left(1 + 3\\left(1-e^{-(t_{n}/\\lambda)^{\\kappa}}\\right)\\right ) \\,.\\ ] ]    since @xmath17 with @xmath131 are recomputed we have that @xmath215 for all @xmath131 and therefore @xmath227 for @xmath228 .",
    "note that @xmath229 is a downset for @xmath230 , that is @xmath231 .",
    "since the @xmath134 with @xmath133 are covering elements for @xmath232 it follows that each of the @xmath233 for which @xmath234 are also downsets .",
    "it follows that there is a unique solution to the gcp for such @xmath233 , namely @xmath102 for all @xmath223 and @xmath158 otherwise . that is @xmath158 iff @xmath235 and hence @xmath236 .",
    "it follows that the error is bounded by @xmath237 from lemma  [ lem : rib ] , the probability of a fault occurring during the computation of any @xmath17 with @xmath133 is bounded by @xmath238 and therefore for @xmath133 one has @xmath239=0\\cdot\\pr(u_{i}=0)+1\\cdot\\pr(u_{i}=1)=\\pr(u_{i}=1)=g(t_{i})\\leq g(t_{n } ) \\,.\\end{aligned}\\ ] ] it follows that @xmath240 & \\leq\\mathbb{e}\\left[\\|u - u^{c}_{n}\\|_{2}+\\sum_{\\|i\\|_{1}=n}u_{i}\\|h_{i}\\|_{2}\\right ] \\nonumber \\\\ & = \\|u - u^{c}_{n}\\|_{2}+\\sum_{\\|i\\|_{1}=n}\\mathbb{e}[u_{i}]\\|h_{i}\\|_{2 } \\nonumber \\\\ & \\leq\\|u - u^{c}_{n}\\|_{2}+\\sum_{\\|i\\|_{1}=n}g(t_{i})\\|h_{i}\\|_{2 }   \\ , , \\label{eqn : pro1est}\\end{aligned}\\ ] ] and substituting the estimate @xmath241 yields @xmath240 & \\leq\\epsilon_{n}+\\sum_{\\|i\\|_{1}=n}g(t_{n } ) 3^{-d}2^{-2n}|u|_{h^{2}_{\\text{mix } } } \\\\ &",
    "\\leq\\epsilon_{n}+f(t_{n})\\sum_{\\|i\\|_{1}=n } 3^{-d}2^{-2n}|u|_{h^{2}_{\\text{mix } } } \\\\ & = \\epsilon_{n}+\\left(1-e^{-(t_{n}/\\lambda)^{\\kappa}}\\right)\\binom{n+d-1}{d-1 } 3^{-d}2^{-2n}|u|_{h^{2}_{\\text{mix } } } \\,.\\end{aligned}\\ ] ] now , noting that @xmath242 one has @xmath243 and therefore , @xmath226\\leq\\epsilon_{n}\\left(1 + 3\\left(1-e^{-(t_{n}/\\lambda)^{\\kappa}}\\right)\\right ) \\ , .",
    "\\label{eq : eeb1}\\ ] ] note that as @xmath244 we have @xmath245\\leq4\\epsilon_{n}$ ] .",
    "however , the worst case scenario is when @xmath246 which results in a classical combination of level @xmath140 which has the error bound @xmath247 this is consistent with the upper bound   which is the desired result .",
    "note the assumption that each @xmath17 be computed on a different node is not necessary as we have bounded the probability of a failure during the computation of @xmath17 to be independent of the starting time . as a result our bound is independent of the number of nodes that are used during the computation and how the @xmath17 are distributed among them as long as each individual @xmath17 is not distributed across multiple nodes .",
    "the nice thing about this result is that the bound on the expected error is simply a multiple of the error bound for @xmath248 , i.e. the result in the absence of faults .",
    "if the bound on @xmath249 was tight then one might expect @xmath226\\lessapprox \\|u - u^{c}_{n}\\|_{2}\\left(1 + 3\\left(1-e^{-(t_{n}/\\lambda)^{\\kappa}}\\right)\\right ) \\,.\\ ] ] also note that   can be expressed as @xmath226 \\leq\\|u - u^{c}_{n}\\|_{2}+\\pr(t\\leq t_{n})\\|u^{c}_{n-1}-u^{c}_{n}\\|_{2 } \\,.\\ ] ] if the combination technique converges for @xmath15 then @xmath250 as @xmath251 . since @xmath252 the error due to faults diminishes as @xmath251 .",
    "we now prove an analogous result for the case where only solutions with @xmath208 are recomputed .",
    "given @xmath218 and @xmath219 let @xmath17 , @xmath165 and @xmath253 be as described in proposition  [ prop : err1 ] with each @xmath17 computed on different nodes for which time between failures is iid having weibull distribution with @xmath206 and @xmath169 .",
    "additionally let @xmath254 .",
    "suppose we recompute any @xmath17 with @xmath208 which is interrupted by a fault , let @xmath221 be the set of all possible @xmath222 for which @xmath17 was successfully computed ( eventually ) iff @xmath223 .",
    "let @xmath224 be the function - valued random variable corresponding to the result of the ftct , then @xmath226\\leq\\epsilon_{n}\\cdot\\min\\left\\{16,1 + 3\\left(d+5-e^{-\\left(\\frac{t_{n}}{\\lambda}\\right)^{\\kappa}}-(d+4 ) e^{-\\left(\\frac{t_{n-1}}{\\lambda}\\right)^{\\kappa}}\\right)\\right\\ } \\,.\\ ] ]    this is much the same as the proof of proposition  [ prop : err1 ] .",
    "the solution to the gcp for @xmath148 satisfies the property that if @xmath158 for @xmath137 then @xmath17 was not computed successfully , that is @xmath235 .",
    "however the converse does not hold in general . regardless ,",
    "if @xmath235 for @xmath137 then the worst case is that @xmath158 and @xmath255 for the @xmath4 possible @xmath256 satisfying @xmath80 .",
    "we therefore note that the error generated by faults affecting @xmath17 with @xmath137 is bounded by @xmath257 therefore we have @xmath240 & \\leq\\|u - u^{c}_{n}\\|_{2}+\\sum_{\\|i\\|_{1}=n}g(t_{n})\\|h_{i}\\|_{2 } \\\\ & \\qquad+\\sum_{\\|i\\|_{1}=n-1}g(t_{n-1})\\sum_{j\\in i,\\,j\\geq i}\\|h_{j}\\|_{2 } \\\\ & \\leq\\epsilon_{n}\\left(1 + 3\\left(1-e^{-(t_{n}/\\lambda)^{\\kappa}}\\right)\\right ) \\\\ & \\qquad+\\binom{n-1+d-1}{d-1}\\left(1-e^{-(t_{n-1}/\\lambda)^{\\kappa}}\\right)(d+4)3^{-d}2^{-2n}|u|_{h^{2}_{\\text{mix } } } \\\\ & \\leq\\epsilon_{n}\\left(1 + 3\\left(1-e^{-(t_{n}/\\lambda)^{\\kappa}}\\right ) + 3(d+4)\\left(1-e^{-(t_{n-1}/\\lambda)^{\\kappa}}\\right)\\right ) \\,.\\end{aligned}\\ ] ] now the expected error should be no more than the worse case which is @xmath258 for which we have @xmath259 . taking the minimum of the two estimates yields the desired result",
    ".    to illustrate how this result may be used in practice , suppose we compute a level @xmath260 interpolation in @xmath261 dimensions on a machine whose mean time to failure can be modelled by the weibull distribution with a mean of @xmath262 seconds and shape parameter @xmath168 .",
    "further , suppose @xmath17 with @xmath263 are not recomputed if lost as a result of a fault and that @xmath264 is estimated to be @xmath265 seconds and @xmath266 is at most @xmath267 seconds .",
    "the expected error for our computation is bounded above by @xmath268 times the error bound if no faults were to occur .    whilst this provides some theoretical validation of our approach , in practice we can numerically compute an improved estimate by enumerating all possible outcomes and the probability of each occurring .",
    "the reason for this is that equation   is an overestimate in general , particularly for relatively small @xmath4 . in practice ,",
    "a fault on @xmath17 with @xmath137 will generally result in the loss of @xmath269 of the largest hierarchical spaces in which case equation   overestimates by a factor of @xmath270 .",
    "we now repeat the above analysis , this time focusing on the mean time required for recomputations .",
    "the first issue to consider is that a failure may occur during a recomputation which will trigger another recomputation .",
    "given a solution @xmath17 with @xmath271 , the probability of having to recompute @xmath272 times is bounded by @xmath273 .",
    "hence the expected number of recomputations for such a @xmath17 is bounded by @xmath274 let the time required to compute each @xmath17 be bounded by @xmath275 for some fixed @xmath276 and all @xmath277 . for a given @xmath278 ,",
    "suppose we intend to recompute all @xmath17 with @xmath279 upon failure , then the expected time required for recomputations is bounded by @xmath280 and by bounding components of the sum with the case @xmath281 one obtains @xmath282 the time required to compute all @xmath17 with @xmath277 once is similarly bounded by @xmath283 and hence @xmath284 estimates the expected proportion of extra time spent on recomputations .",
    "we would generally expect that @xmath285 ( we assume the time to compute level @xmath286 grids is much less than the mean time to failure ) and therefore this quantity is small . as an example , if we again consider a level @xmath260 computation in @xmath261 dimensions for which @xmath287 and the time to failure is weibull distributed with mean @xmath262 seconds with shape parameter @xmath168 , the expected proportion of time spent recomputing solutions level @xmath288 or smaller is @xmath289 . in comparison ,",
    "if any of the @xmath17 which fail were to be recomputed then a proportion of @xmath290 additional time for would be expected for recomputations , almost @xmath291 times more . whilst this is a somewhat crude estimate",
    "it clearly demonstrates the our approach will scale better than a traditional checkpoint restart when faults are relatively frequent .",
    "at the heart of our implementation is a very simple procedure : solve the problem on different grids , combine the solutions , and repeat .",
    "this section is broken up into different sub - sections based upon where different layers of parallelism can be implemented .",
    "we conclude by discussing some bottlenecks in the current implementation .",
    "the top layer is written in python .",
    "as in many other applications , we use python to glue together the different components of our implementation as well as providing some high level functions for performing the combination .",
    "it is based upon the development of numrf  @xcite : intended to be a clean interface where different computation codes can be easily interchanged or added to the application .",
    "this layer can be further broken down into 4 main parts .",
    "the first is the loading of all dependencies including various python and _ numpy _ modules as well as any shared libraries that will be used to solve the given problem . in particular , bottom",
    "layer components which have been compiled into shared libraries from various languages ( primarily c++ with c wrappers in our case ) are loaded into python using _",
    "ctypes_. the second part is the initialisation of data structures and construction / allocation of arrays which will hold the relevant data .",
    "this is achieved using pygraft  @xcite which is a general class of grids and fields that allows us to handle data from the various components in a generic way .",
    "also in this part of the code is the building of a sparse grid data structure .",
    "this is done with our own c++ implementation which was loaded into python in the first part of the code .",
    "the third part consists of solving the given problem .",
    "this is broken into several `` combination steps '' .",
    "a `` combination step '' consists of a series of time steps of the underlying solver for each component solution , followed by a combination of the component solutions into a sparse grid solution , and finally a sampling of the component solutions from the sparse grid solution before repeating the procedure .",
    "the fourth and final part of the code involves checking the error of the computed solutions , reporting of various log data and finalise / cleanup .",
    "the top layer is primarily responsible for the coarsest grain parallelism , that is distributing the computation of different component solutions across different processes .",
    "this is achieved through mpi using _",
    "mpi4py_. there are two main tasks the top layer must perform in order to effectively handle this .",
    "the first is to determine an appropriate load balancing of the different component solutions across a given number of processes . for our simple problem this",
    "can be done statically on startup before initialising any data structures . for more complex problems this",
    "can be done dynamically by evaluating the load balancing and redistributing if necessary at start of each combination step based upon timings performed in the last combination step .",
    "re - distribution of the grids may require reallocating many of the data structures .",
    "the second task the top layer is responsible for is the communication between mpi processes during the combination step .",
    "this is achieved using two all_reduce calls .",
    "the first call is to establish which solutions have been successfully computed .",
    "this is required so that all processes are able to compute the correct combination coefficients .",
    "each process then does a partial sum of the component solutions it has computed .",
    "the second all_reduce call then completes the combination of all component solutions distributing the result to all processes . following this the component solutions",
    "are then sampled from the complete sparse grid solution .",
    "the bottom layer is made up of several different components , many of which are specific to the problem that is intended to be solved .",
    "when solving our advection problem we have 2 main components , one is responsible for the sparse grid data structure and functions relating to the sparse grid ( e.g. interpolation and sampling of component solutions ) and the other component is the advection solver itself .",
    "both the sparse grid and advection solver components use openmp to achieve a fine grain level of parallelism .",
    "this is primarily achieved by distributing the work of large for loops within the code across different threads .",
    "the for loops have roughly constant time per iteration so the distribution of work amongst threads is done statically .",
    "the middle layer is currently being developed into the programming model .",
    "it is intended solely to handle various aspects relating to the computation of component solutions where domain decompositions are added as a third layer of parallelism .",
    "this will be achieved through an interface with a distributed array class of the numrf / pygraft framework at the top layer .",
    "this layer will need to interface with solver kernel from the bottom layer and then perform communication of data across domain boundaries .",
    "the combination of solutions onto the sparse grid in the top layer will also need to interface with this layer to handle the communication of different domains between mpi processes .",
    "it is intended that most of this will be transparent to the user .      since interpolation of the sparse grid and the solver ( and any other time consuming operations ) each benefit from load balancing with mpi and work sharing with openmp , any major hurdles to scalability will be caused by the all_reduce communication and any serial operations in the code ( e.g. initialisation routines ) .",
    "ignoring initialisation parts of the code it becomes clear we need to either reduce the size of the data which is communicated , or reduce the frequency at which it is communicated .",
    "the first can be done if we apply some compression to the data before communicating , i.e. we trade - off smaller communications for additional cpu cycles .",
    "another way is to recognise that it is possible to do the all_reduce on a sparse grid of level @xmath140 if a partial hierarchisation is done to the largest component grids .",
    "this does nt improve the rate in which the complexity grows but can at least reduce it by a constant . reducing the frequency of the all_reduce",
    "can be done by performing partial combinations in place of full combinations for some proportion of the steps .",
    "this trades off the time taken to combine with some accuracy of the approximation .",
    "a partial combination is where a grid combines only with its neighbouring grids .",
    "however , the only way to really address the bottleneck caused by communication is to perform a full hierarchisation of the component grids  @xcite . by doing this one can significantly reduce the communication volume at the expense of increasing the number of messages .",
    "one can then reduce the number of messages by identifying those which are communicated to the same mpi processes .",
    "we currently have a first implementation of this which we intend to improve as development continues .",
    "in this section , we present some numerical results which validate our approach .",
    "the problem used to test our algorithm is the scalar advection equation @xmath292 on the domain @xmath293^{3}\\subset\\mathbb{r}^{3}$ ] for constant @xmath294 . for the results presented in this section we use @xmath295 , periodic boundary conditions and the initial condition @xmath296 the pde",
    "is solved using a lax - wendroff finite difference scheme giving results which are second order in space and time .",
    "we compare numerical solutions against the exact solution @xmath297 to determine the solution error at the end of each computation .",
    "a truncated combination technique as in   is used for our experiments . in order to apply the ftct we need to compute some additional grids .",
    "we define @xmath298 which is the set of indices for which we are required to compute solutions @xmath17 if the top two levels are not to be recomputed in the event of a fault .",
    "note that as the grid sizes vary between the @xmath17 so does the maximum stable time step size as determined by the cfl condition .",
    "we choose the same time step size for all component solutions to avoid instability that may otherwise arise from the extrapolation of time stepping errors during the combination . as a result",
    "our timesteps must satisfy the cfl condition for all component grids . by choosing @xmath299 such that it satisfies the cfl condition for the numerical solution of @xmath300 it follows that the cfl condition is also satisfied for all @xmath17 with @xmath301 .",
    "all of our computations were performed on a fujitsu primergy cluster consisting of 36 nodes each with 2 intel xeon x5670 cpus ( 6 core , 2.934ghz ) with infiniband interconnect .",
    ".[tab : wr2 ] numerical results for @xmath302 runs for each @xmath303 using the weibull distribution with mean of @xmath304 seconds and shape parameter of @xmath168 for the fault simulation .",
    "the computation was performed on 2 nodes with 6 openmp threads on each . [ cols=\"^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     in table  [ tab : wr6 ] we repeat this experiment with @xmath305 runs on 6 nodes with 6 openmp threads on each . whilst running with additional nodes leads to a decrease in computation time we experience more faults on average because of the additional nodes .",
    "however , we can see that the effect of the increased average number of faults is quite small on both the average solution error and the average wall time .",
    "table  [ tab : er2 ] again shows results for @xmath302 runs of the ftct with fault simulation on 2 nodes with 6 openmp threads on each .",
    "however , for this experiment the faults are exponentially distributed with a mean of @xmath304 seconds .",
    "we see that for this distribution the faults are a little less frequent on average leading to a slightly smaller average error .",
    "similar is observed in table  [ tab : er6 ] where we repeat the experiment with @xmath305 runs on 6 nodes with 6 openmp threads on each .",
    "here the average number of faults is substantially less than the results of table  [ tab : wr6 ] and this is again reflected by a smaller average error in comparison .",
    "the large @xmath306 in the 2nd row is due to a single outlier , the next largest time being @xmath307 .",
    "no simulated faults occurred for this outlier so we suspect it was due to a system issue .      in figure",
    "[ fig : scala2 ] we demonstrate the scalability and efficiency of our implementation when the fault simulation is disabled . noting from table  [ tab : wr6 ] that faults have very little effect on the computation time we expect similar results with",
    "fault simulation turned on .",
    "the advection problem was solved using a @xmath308 truncated combination .",
    "the component solutions were combined only once at the end of the computation .",
    "the _ solver _",
    "time reported here is the timing of the core of the code , that is the repeated computation , combination and communication of the solution which is highly scalable .",
    "the _ total _",
    "time reported here includes the time for python to load modules and shared libraries , memory allocation and error checking .",
    "the error checking included in the _ total _ time is currently computed in serial and could benefit from openmp parallelism .",
    "coordinates ( 1 , 1.0 ) ( 2 , 1.999503001868713 ) ( 4 , 3.9778915936641024 ) ( 6 , 5.878718802969197 ) ( 8 , 7.836917562724014 ) ( 12 , 11.574774152713045 ) ( 16 , 15.145158861617226 ) ( 24 , 22.36579942183678 ) ( 36 , 32.12360268284893 ) ( 48 , 40.523368251410155 ) ( 72 , 60.516847172081825 ) ; coordinates ( 1 , 1.0 ) ( 2 , 1.9895584696232995 ) ( 4 , 3.91569199837609 ) ( 6 , 5.72765319684416 ) ( 8 , 7.554623102457946 ) ( 12 , 10.928991528624614 ) ( 16 , 14.031659161759613 ) ( 24 , 19.969141279700285 ) ( 36 , 27.32690232056125 ) ( 48 , 33.1609364767518 ) ( 72 , 45.26189944134078 ) ; coordinates ( 1 , 1.0 ) ( 2 , 2.0 ) ( 4 , 4.0 ) ( 6 , 6.0 ) ( 8 , 8.0 ) ( 12 , 12.0 ) ( 16 , 16.0 ) ( 24 , 24.0 ) ( 36 , 36.0 ) ( 48 , 48.0 ) ( 72 , 72.0 ) ;    in figure  [ fig : tvf ] we compare the computation time required for our approach to reach a solution compared to more traditional checkpointing approaches , in particular , with a local and global checkpointing approach . with global checkpointing",
    "we keep a copy of the last combined solution .",
    "if a failure affects any of the component grids it is assumed that the entire application is killed and computations must be restarted from the most recent combined solution .",
    "we emulate this by checking for faults at each combination step and restart from the last combination step if any faults have occurred . with local checkpointing",
    "each mpi process saves a copy of each component solution it computes . in this case",
    "when a faults affect component solutions we need only recompute the affected component solutions from their saved state . in both checkpointing methods",
    "the extra component solutions used in our approach are not required and are hence not computed . as a result",
    "these approaches are slightly faster when no faults occur .",
    "however , as the number of faults increases , it can be seen from figure  [ fig : tvf ] that the computation time for the local and global checkpointing methods begins to grow .",
    "a line of best fit has been added to the figure which makes it clear that the time for recovery with global checkpointing increases rapidly with the number of faults .",
    "local checkpointing is a significant improvement on this but still shows some growth . on the other hand",
    "our approach is barely affected by the number of faults and beats both the local and global checkpointing approaches after only a few faults . for much larger number of faults",
    "our approach is significantly better .",
    "table[x = f , y = t ] from ; table[x = f , y = t ] from ; table[x = f , y = t ] from ; table[x = f , y = t , forget plot ] from ; table[x = f , y = t ] from ; table[x = f , y = t , forget plot ] from ;",
    "a generalisation of the sparse grid combination technique has been presented . from this generalisation",
    "a fault tolerant combination technique has been proposed which significantly reduces recovery times at the expense of some upfront overhead and reduced solution accuracy .",
    "theoretical bounds on the expected error and numerical experiments show that the reduction in solution accuracy is very small .",
    "the numerical experiments also demonstrate that the upfront overheads become negligible compared to the costs of recovery using checkpoint - restart techniques if several faults occur .",
    "there are some challenges associated with load balancing and efficient communication with the implementation of the combination technique .",
    "studying these aspects and improving the overall scalability of the initial implementation will be the subject of future work . as the ulfm specification continues to develop ,",
    "the validation of the ftct on a system with real faults is also being investigated .                      , _ a robust combination technique _",
    ", in s.  mccue , t.  moroney , d.  mallet , and j.  bunder , editors , proceedings of the 16th biennial computational techniques and applications conference , anziam journal , 54 ( ctac2012 ) , pp .  c394c411 .    , _ robust solutions to pdes with multiple grids _ , sparse grids and applications - munich 2012 , j.  garcke , d.  pflger ( eds . ) , lecture notes in computational science and engineering 97 , springer , 2014 , to appear .",
    ", _ a parallel fault tolerant combination technique _ , m.  bader , a.  bode , h .- j .",
    "bungartz , m.  gerndt , g.r .",
    "joubert , f.  peters ( eds . ) , parallel computing : accelerating computational science and engineering ( cse ) , advances in parallel computing 25 , ios press , 2014 , pp .",
    "584592 .          , _ global communication schemes for the sparse grid combination technique _ , m.  bader , a.  bode , h .- j .",
    "bungartz , m.  gerndt , g.r .",
    "joubert , f.  peters ( eds . ) , parallel computing : accelerating computational science and engineering ( cse ) , advances in parallel computing 25 , ios press , 2014 , pp .  564573 .    , _",
    "reducibility among combinatorial problems _ , in complexity of computer computations : proc . of a symp . on the complexity of computer computations , r. e. miller and j. w. thatcher , eds . , the ibm research symposia series , new york , ny : plenum press , 1972 , pp .  85103 .    , _ fault - tolerant grid - based solvers : combining concepts from sparse grids and mapreduce _ , proceedings of 2013 international conference on computer science ( iccs ) , procedia computer science , elsevier , 2013 .    , _ managing complexity in the parallel sparse grid combination technique _ , m.  bader , a.  bode , h .- j .",
    "bungartz , m.  gerndt , g.r .",
    "joubert , f.  peters ( eds . ) , parallel computing : accelerating computational science and engineering ( cse ) , advances in parallel computing 25 , ios press , 2014 , pp ."
  ],
  "abstract_text": [
    "<S> this paper continues to develop a fault tolerant extension of the sparse grid combination technique recently proposed in  [ b.  harding and m.  hegland , _ </S>",
    "<S> anziam j. _ , 54  ( ctac2012 ) , pp .  </S>",
    "<S> c394c411 ] . </S>",
    "<S> the approach is novel for two reasons , first it provides several levels in which one can exploit parallelism leading towards massively parallel implementations , and second , it provides algorithm - based fault tolerance so that solutions can still be recovered if failures occur during computation . </S>",
    "<S> we present a generalisation of the combination technique from which the fault tolerant algorithm is a consequence . using a model for the time between faults on each node of a high performance computer we provide bounds on the expected error for interpolation with this algorithm . </S>",
    "<S> numerical experiments on the scalar advection pde demonstrate that the algorithm is resilient to faults on a real application . </S>",
    "<S> it is observed that the trade - off of recovery time to decreased accuracy of the solution is suitably small . </S>",
    "<S> a comparison with traditional checkpoint - restart methods applied to the combination technique show that our approach is highly scalable with respect to the number of faults .    exascale computing , algorithm - based fault tolerance , sparse grid combination technique , parallel algorithms    65y05 , 68w10 </S>"
  ]
}