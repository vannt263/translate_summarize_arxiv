{
  "article_text": [
    "determinantal point processes ( dpps ) are discrete probability models over the subsets of a ground set of @xmath0 items .",
    "they provide an elegant model to assign probabilities to an exponentially large sample , while permitting tractable ( polynomial time ) sampling and marginalization .",
    "they are often used to provide models that balance `` diversity '' and quality , characteristics valuable to numerous problems in machine learning and related areas  @xcite .",
    "the antecedents of dpps lie in statistical mechanics  @xcite , but since the seminal work of  @xcite they have made inroads into machine learning . by now they have been applied to a variety of problems such as document and video summarization  @xcite , sensor placement  @xcite , recommender systems  @xcite , and object retrieval  @xcite .",
    "more recently , they have been used to compress fully - connected layers in neural networks  @xcite and to provide optimal sampling procedures for the nystrm method  @xcite .",
    "the more general study of dpp properties has also garnered a significant amount of interest , see e.g. ,  @xcite .    however , despite their elegance and tractability , widespread adoption of dpps is impeded by the @xmath2 cost of basic tasks such as ( exact ) sampling  @xcite and learning  @xcite .",
    "this cost has motivated a string of recent works on approximate sampling methods such as mcmc samplers  @xcite or core - set based samplers  @xcite .",
    "the task of learning a dpp from data has received less attention ; the methods of  @xcite cost @xmath2 per iteration , which is clearly unacceptable for realistic settings .",
    "this burden is partially ameliorated in  @xcite , who restrict to learning low - rank dpps , though at the expense of being unable to sample subsets larger than the chosen rank .",
    "these considerations motivate us to introduce krondpp , a dpp model that uses kronecker ( tensor ) product kernels . as a result ,",
    "krondppenables us to learn large sized dpp kernels , while also permitting efficient ( exact and approximate ) sampling .",
    "the use of kronecker products to scale matrix models is a popular and effective idea in several machine - learning settings  @xcite .",
    "but as we will see , its efficient execution for dpps turns out to be surprisingly challenging .    to make our discussion more concrete , we recall some basic facts now .",
    "suppose we have a ground set of @xmath0 items @xmath3 .",
    "a discrete dpp over @xmath4 is a probability measure @xmath5 on @xmath6 parametrized by a positive definite matrix @xmath7 ( the _ marginal kernel _ ) such that @xmath8 , so that for any @xmath9 drawn from @xmath5 , the measure satisfies @xmath10 where @xmath11 is the submatrix of @xmath7 indexed by elements in @xmath12 ( i.e. , @xmath13_{i , j \\in a}$ ] ) .",
    "if a dpp with marginal kernel @xmath7 assigns nonzero probability to the empty set , the dpp can alternatively be parametrized by a positive definite matrix @xmath14 ( the _ dpp kernel _ ) so that @xmath15 a brief manipulation ( see e.g. ,  ( * ? ? ?",
    "* eq .  15 ) ) shows that when the inverse exists , @xmath16 .",
    "the determinants , such as in the normalization constant in  , make operations over dpps typically cost @xmath1 , which is a key impediment to their scalability .",
    "therefore , if we consider a class of dpp kernels whose structure makes it easy to compute determinants , we should be able to scale up dpps .",
    "an alternative approach towards scalability is to restrict the size of the subsets , as done in @xmath17-dpp  @xcite or when using rank-@xmath17 dpp kernels  @xcite ( where @xmath18 ) .",
    "both these approaches still require @xmath2 preprocessing for exact sampling ; another caveat is that they limit the dpp model by assigning zero probabilities to sets of cardinality greater than @xmath17 .",
    "in contrast , krondppuses a kernel matrix of the form @xmath19 , where each _ sub - kernel _",
    "@xmath20 is a smaller positive definite matrix .",
    "this decomposition has two key advantages : ( i ) it significantly lowers the number of parameters required to specify the dpp from @xmath21 to @xmath22 ( assuming the sub - kernels are roughly the same size ) ; and ( ii ) it enables fast sampling and learning .    for ease of exposition",
    ", we describe specific details of krondppfor @xmath23 ; as will become clear from the analysis , typically the special cases @xmath23 and @xmath24 should suffice to obtain low - complexity sampling and learning algorithms .",
    "[ [ contributions . ] ] contributions .",
    "+ + + + + + + + + + + + + +    our main contribution is the krondppmodel along with efficient algorithms for sampling from it and learning a kronecker factored kernel .",
    "specifically , inspired by the algorithm of  @xcite , we develop krk - picard(**kr**onecker-**k**ernel picard ) , a block - coordinate ascent procedure that generates a sequence of kronecker factored estimates of the dpp kernel while ensuring monotonic progress on its ( difficult , nonconvex ) objective function .",
    "more importantly , we show how to implement krk - picardto run in @xmath25 time when implemented as a batch method , and in @xmath26 time and @xmath27 space , when implemented as a stochastic method . as alluded to above , unlike many other uses of kronecker models , krondppdoes not admit trivial scaling up , largely due to extensive dependence of dpps on arbitrary submatrices of the dpp kernel . an interesting theoretical nugget that arises from our analysis",
    "is the combinatorial problem that we call _ subset clustering _ , a problem whose ( even approximate ) solution can lead to further speedups of our algorithms .",
    "we begin by recalling basic properties of kronecker products needed in our analysis ; we omit proofs of these well - known results for brevity .",
    "the kronecker ( tensor ) product of @xmath28 with @xmath29 two matrices is defined as the @xmath30 block matrix @xmath31_{i , j=1}^{p , q}$ ] .",
    "we denote the block @xmath32 in @xmath33 by @xmath34 for any valid pair @xmath35 , and extend the notation to non - kronecker product matrices to indicate the submatrix of size @xmath36 at position @xmath35 .",
    "[ prop : basic ] let @xmath37 be matrices of sizes so that @xmath38 and @xmath39 are well - defined . then",
    ",    a.   if @xmath40 , then , @xmath41 ; b.   if @xmath12 and @xmath42 are invertible then so is @xmath33 , with @xmath43 ; c.   @xmath44 = @xmath45 .    an important consequence of prop .  [ prop : basic]@xmath46 is the following corollary .    [ corr : eigendecompose ]",
    "let @xmath47 and @xmath48 be the eigenvector decompositions of @xmath12 and @xmath42 . then , @xmath33 diagonalizes as @xmath49 .    we will also need the notion of partial trace operators , which are perhaps less well - known :    let @xmath50 .",
    "the _ partial traces _ @xmath51 and @xmath52 are defined as follows : @xmath53_{1 \\leq i , j \\leq n_1 } \\in \\mathbb r^{n_1\\times n_1 } , \\qquad \\operatorname{tr}_2(a ) : = \\sum\\nolimits_{i=1}^{n_1 } a_{(ii ) } \\in \\mathbb r^{n_2 \\times n_2}.\\ ] ]    the action of partial traces is easy to visualize : indeed , @xmath54 and @xmath55 . for us ,",
    "the most important property of partial trace operators is their positivity .",
    "[ prop : posdef - operator ] @xmath56 and @xmath57 are positive operators , i.e. , for @xmath58 , @xmath59 and @xmath60",
    ".    please refer to  ( * ? ? ?",
    "in this section , we consider the key difficult task for krondpps : learning a kronecker product kernel matrix from @xmath61 observed subsets @xmath62 . using the definition  ( [ eq:2 ] ) of @xmath63 , maximum - likelihood learning of a dpp with kernel @xmath14 results in the optimization problem : @xmath64 this problem is nonconvex and conjectured to be np - hard  ( * ? ? ? * conjecture 4.1 ) .",
    "moreover the constraint @xmath65 is nontrivial to handle .",
    "writing @xmath66 as the indicator matrix for @xmath67 of size @xmath68 so that @xmath69 , the gradient of @xmath70 is easily seen to be @xmath71 in  @xcite , the authors derived an iterative method ( `` the picard iteration '' ) for computing an @xmath14 that solves @xmath72 by running the simple iteration @xmath73 moreover , iteration   is guaranteed to monotonically increase the log - likelihood @xmath70  @xcite .",
    "but these benefits accrue at a cost of @xmath2 per iteration , and furthermore a direct application of   can not guarantee the kronecker structure required by krondpp .      our aim is to obtain an efficient algorithm to ( locally ) optimize  . beyond its nonconvexity , the kronecker structure @xmath74 imposes another constraint . as in  @xcite",
    "we first rewrite @xmath70 as a function of @xmath75 , and re - arrange terms to write it as @xmath76 it is easy to see that @xmath77 is concave , while a short argument shows that @xmath78 is convex  @xcite .",
    "an appeal to the convex - concave procedure  @xcite then shows that updating @xmath79 by solving @xmath80 , which is what   does  ( * ? ? ?",
    "2.2 ) , is guaranteed to monotonically increase @xmath70 .",
    "but for krondppthis idea does not apply so easily : due the constraint @xmath81 the function @xmath82 fails to be convex , precluding an easy generalization . nevertheless , for fixed @xmath83 or @xmath84 the functions @xmath85 are once again concave or convex .",
    "indeed , the map @xmath86 is linear and @xmath77 is concave , and @xmath87 is also concave ; similarly , @xmath88 is seen to be concave and @xmath89 and @xmath90 are convex .",
    "hence , by generalizing the arguments of  ( * ? ? ?",
    "2 ) to our `` block - coordinate '' setting , updating via @xmath91 should increase the log - likelihood @xmath70 at each iteration .",
    "we prove below that this is indeed the case , and that updating as per   ensure positive definiteness of the iterates as well as monotonic ascent .      in order to show the positive definiteness of the solutions to",
    ", we first derive their closed form .",
    "[ prop : differenciation ] for @xmath92 , @xmath93 , the solutions to are given by the following expressions : @xmath94 moreover , these solutions are positive definite .",
    "the details are somewhat technical , and are hence given in appendix  [ app : cccp - psd ] . we know that @xmath95 , because @xmath96 . since the partial trace operators are positive ( prop .",
    "[ prop : posdef - operator ] ) , it follows that the solutions to   are also positive definite .",
    "we are now ready to establish that these updates ensure monotonic ascent in the log - likelihood .",
    "[ thm : cccp ] starting with @xmath97 , @xmath98 , updating according to   generates positive definite iterates @xmath99 and @xmath100 , and the sequence @xmath101 is non - decreasing .    updating according to   generates positive definite matrices @xmath102 , and hence positive definite subkernels @xmath103 .",
    "moreover , due to the convexity of @xmath89 and concavity of @xmath104 , for matrices @xmath105 @xmath106 hence , @xmath107 .",
    "thus , if @xmath108 verify , by setting @xmath109 and @xmath110 we have @xmath111 the same reasoning holds for @xmath112 , which proves the theorem .    as @xmath113 ( and similarly for @xmath112 ) , updating as in is equivalent to updating @xmath114    * genearlization .",
    "* we can generalize the updates to take an additional step - size parameter @xmath115 : @xmath116 experimentally , @xmath117 ( as long as the updates remain positive definite ) can provide faster convergence , although the monotonicity of the log - likelihood is no longer guaranteed .",
    "we found experimentally that the range of admissible @xmath115 is larger than for picard , but decreases as @xmath0 grows larger .",
    "the arguments above easily generalize to the multiblock case .",
    "thus , when learning @xmath118 , by writing @xmath119 the matrix with a 1 in position @xmath35 and zeros elsewhere , we update @xmath120 as @xmath121.\\ ] ]    from the above updates it is not transparent whether the kronecker product saves us any computation .",
    "in particular , it is not clear whether the updates can be implemented to run faster than @xmath2 .",
    "we show below in the next section how to implement these updates efficiently .      from theorem  [ thm : cccp ] , we obtain algorithm  [ algo : cccp ] ( which is different from the picard iteration in  @xcite , because it operates alternatingly on each subkernel ) .",
    "it is important to note that a further speedup to algorithm  [ algo : cccp ] can be obtained by performing stochastic updates , i.e. , instead of computing the full gradient of the log - likelihood , we perform our updates using only one ( or a small minibatch ) subset @xmath67 at each step instead of iterating over the entire training set ; this uses the stochastic gradient @xmath122 .",
    "matrices @xmath123 , training set @xmath124 , parameter @xmath115 . @xmath125 // or update stochastically @xmath126// or",
    "update stochastically    * return",
    "* @xmath127    the crucial strength of algorithm  [ algo : cccp ] lies in the following result :    [ thm : complexity ] for @xmath128 , the updates in algorithm  [ algo : cccp ] can be computed in @xmath129 time and @xmath25 space , where @xmath130 is the size of the largest training subset .",
    "furthermore , stochastic updates can be computed in @xmath131 time and @xmath132 space .",
    "indeed , by leveraging the properties of the kronecker product , the updates can be obtained without computing @xmath133 .",
    "this result is non - trivial : the components of @xmath134 , @xmath135 and @xmath136 , must be considered separately for computational efficiency . the proof is provided in app .",
    "[ app : cccp - complexity ]",
    ". however , it seems that considering more than 2 subkernels does not lead to further speed - ups .",
    "if @xmath128 , these complexities become :    * for non - stochastic updates : @xmath129 time , @xmath25 space , * for stochastic updates : @xmath137 time , @xmath138 space .",
    "this is a marked improvement over  @xcite , which runs in @xmath25 space and @xmath139 time ( non - stochastic ) or @xmath1 time ( stochastic ) ; algorithm  [ algo : cccp ] also provides faster stochastic updates than  @xcite .",
    "however , one may wonder if by learning the sub - kernels by alternating updates the log - likelihood converges to a sub - optimal limit .",
    "the next section discusses how to jointly update @xmath140 and @xmath112 .",
    "we also analyzed the possibility of updating @xmath140 and @xmath112 jointly : we update @xmath141 and then recover the kronecker structure of the kernel by defining the updates @xmath142 and @xmath143 such that : @xmath144 we show in appendix  [ app : joint - updates ] that such solutions exist and can be computed by from the first singular value and vectors of the matrix @xmath145_{i , j=1}^{n_1}$ ] .",
    "note however that in this case , there is no guaranteed increase in log - likelihood .",
    "the pseudocode for the related algorithm ( joint - picard ) is given in appendix  [ app : joint - pseudocode ] .",
    "an analysis similar to the proof of thm .",
    "[ thm : complexity ] shows that the updates can be obtained @xmath146 .",
    "although krondppshave tractable learning algorithms , the memory requirements remain high for non - stochastic updates , as the matrix @xmath147 needs to be stored , requiring @xmath25 memory . however ,",
    "if the training set can be subdivised such that @xmath148 @xmath149 can be decomposed as @xmath150 with @xmath151 .",
    "due to the bound in eq .",
    "[ eq:6 ] , each @xmath152 will be sparse , with only @xmath153 non - zero coefficients .",
    "we can then store each @xmath152 with minimal storage and update @xmath140 and @xmath112 in @xmath154 time and @xmath155 space .    determining the existence of such a partition of size",
    "@xmath156 is a variant of the np - hard subset - union knapsack problem ( sukp )  @xcite with @xmath156 knapsacks and where the value of each item ( i.e. each @xmath67 ) is equal to 1 : a solution to sukp of value @xmath61 with @xmath156 knapsacks is equivalent to a solution to  [ eq:6 ] .",
    "however , an approximate partition can also be simply constructed via a greedy algorithm .",
    "sampling exactly ( see alg .  [",
    "algo : sampling ] and  @xcite ) from a full dpp kernel costs @xmath157 where @xmath17 is the size of the sampled subset .",
    "the bulk of the computation lies in the initial eigendecomposition of @xmath14 ; the @xmath17 orthonormalizations cost @xmath158 .",
    "although the eigendecomposition need only happen once for many iterations of sampling , exact sampling is nonetheless intractable in practice for large @xmath0 .",
    "matrix @xmath14 .",
    "eigendecompose @xmath14 as @xmath159 .",
    "@xmath160 @xmath161 with probability @xmath162 .",
    "@xmath163 , @xmath164 sample @xmath165 from @xmath166 with probability @xmath167 @xmath168 , @xmath169 , where @xmath170 is an orthonormal basis of the subspace of @xmath171 orthonormal to @xmath172    * return * @xmath173    it follows from prop .",
    "[ corr : eigendecompose ] that for krondpps , the eigenvalues @xmath174 can be obtained in @xmath175 , and the @xmath17 eigenvectors in @xmath176 operations . for @xmath128 , exact sampling thus only costs @xmath177 . if @xmath178 , the same reasoning shows that exact sampling becomes linear in @xmath0 , only requiring @xmath158 operations .",
    "one can also resort to mcmc sampling ; for instance such a sampler was considered in  @xcite ( though with an incorrect mixing time analysis ) .",
    "the results of  @xcite hold only for @xmath17-dpps , but suggest their mcmc sampler may possibly take @xmath179 time for full dpps , which is impractical .",
    "nevertheless if one develops faster mcmc samplers , they should also be able to profit from the kronecker product structure offered by krondpp .",
    "in order to validate our learning algorithm , we compared krk - picardto joint - picardand to the picard iteration ( picard ) on multiple real and synthetic datasets .",
    "all three algorithms were used to learn from synthetic data drawn from a `` true '' kernel .",
    "the sub - kernels were initialized by @xmath180 , with @xmath181 s coefficients drawn uniformly from @xmath182 $ ] ; for picard , @xmath14 was initialized with @xmath183 .    ; the thin dotted lines indicated the standard deviation from the mean . ]    1.28 ; the thin dotted lines indicated the standard deviation from the mean.,title=\"fig : \" ]    1.15 ; the thin dotted lines indicated the standard deviation from the mean.,title=\"fig : \" ]    1.15 ; the thin dotted lines indicated the standard deviation from the mean.,title=\"fig : \" ]    for figures  [ fig : synth - small ] and  [ fig : synth - large ] , training data was generated by sampling 100 subsets from the true kernel with sizes uniformly distributed between 10 and 190 .    to evaluate krk - picardon matrices too large to fit in memory and with large @xmath130 ,",
    "we drew samples from a @xmath184 kernel of rank @xmath185 ( on average @xmath186 ) , and learned the kernel stochastically ( only krk - picardcould be run due to the memory requirements of other methods ) ; the likelihood drastically improves in only two steps ( fig.[fig : synth - very - large ] ) .    as shown in figures  [ fig : synth - small ] and  [ fig : synth - large ] , krk - picardconverges significantly faster than picard , especially for large values of @xmath0 .",
    "however , although joint - picardalso increases the log - likelihood at each iteration , it converges much slower and has a high standard deviation , whereas the standard deviations for picardand krk - picardare barely noticeable . for these reasons ,",
    "we drop the comparison to joint - picardin the subsequent experiments .",
    "we compared krk - picardto picardand em  @xcite on the baby registry dataset ( described in - depth in  @xcite ) , which has also been used to evaluate other dpp learning algorithms  @xcite .",
    "the dataset contains 17 categories of baby - related products obtained from amazon .",
    "we learned kernels for the 6 largest categories ( @xmath187 ) ; in this case , picardis sufficiently efficient to be prefered to krk - picard ; this comparison serves only to evaluate the quality of the final kernel estimates .",
    "the initial marginal kernel @xmath7 for em was sampled from a wishart distribution with @xmath0 degrees of freedom and an identity covariance matrix , then scaled by @xmath188 ; for picard , @xmath14 was set to @xmath189 ; for krk - picard , @xmath140 and @xmath112 were chosen ( as in joint - picard ) by minimizing @xmath190 .",
    "convergence was determined when the objective change dipped below a threshold @xmath191 .",
    "as one em iteration takes longer than one picard iteration but increases the likelihood more , we set @xmath192 and @xmath193 .",
    "the final log - likelihoods are shown in table  [ tab : babies ] ; we set the step - sizes to their largest possible values , i.e. @xmath194 and @xmath195 .",
    "table  [ tab : babies ] shows that krk - picardobtains comparable , albeit slightly worse log - likelihoods than picardand em , which confirms that for tractable @xmath0 , the better modeling capability of full kernels make them preferable to krondpps .",
    ".5    .test set [ cols=\"<,^,^,^\",options=\"header \" , ]     [ tab : runtimes ]    we construct a ground truth gaussian dpp kernel on the genes dataset and use it to obtain 100 training samples with sizes uniformly distributed between 50 and 200 items . similarly to the synthetic experiments , we initialized krk - picard s kernel by setting @xmath196 where @xmath197 is a random matrix of size @xmath198 ; for picard , we set the initial kernel @xmath199 .    , @xmath200 .",
    "]    1.3 , @xmath200.,title=\"fig : \" ]    1.3 , @xmath200.,title=\"fig : \" ]    figure  [ fig : genetic ] shows the performance of both algorithms . as with the synthetic experiments , krk - picardconverges much faster ;",
    "stochastic updates increase its performance even more , as shown in fig .",
    "[ fig : genetic - stochastic ] . average runtimes and speed - up are given in table  [ tab : runtimes ] : krk - picardruns almost an order of magnitude faster than picard , and stochastic updates are more than two orders of magnitude faster , while providing slightly larger initial increases to the log - likelihood .",
    "we introduced krondpps , a variant of dpps with kernels structured as the kronecker product of @xmath156 smaller matrices , and showed that typical operations over dpps such as sampling and learning the kernel from data can be made efficient for krondppson previously untractable ground set sizes .    by carefully leveraging the properties of the kronecker product",
    ", we derived for @xmath23 a low - complexity algorithm to learn the kernel from data which guarantees positive iterates and a monotonic increase of the log - likelihood , and runs in @xmath129 time .",
    "this algorithm provides even more significant speed - ups and memory gains in the stochastic case , requiring only @xmath201 time and @xmath132 space .",
    "experiments on synthetic and real data showed that krondppscan be learned efficiently on sets large enough that @xmath14 does not fit in memory .",
    "while discussing learning the kernel , we showed that @xmath140 and @xmath112 can not be updated simultaneously in a cccp - style iteration since @xmath78 is not convex over @xmath202 .",
    "however , it can be shown that @xmath78 is geodesically convex over the riemannian manifold of positive definite matrices , which suggests that deriving an iteration which would take advantage of the intrinsic geometry of the problem may be a viable line of future work .",
    "krondppsalso enable fast sampling , in @xmath203 operations when using two sub - kernels and in @xmath158 when using three sub - kernels ; this allows for exact sampling at comparable or even better costs than previous algorithms for approximate sampling .",
    "however , as we improve computational efficiency @xmath14 , the subset size @xmath17 becomes limiting , due to the @xmath158 cost of sampling and learning .",
    "a necessary line of future work to allow for truly scalable dpps is thus to overcome this computational bottleneck .    * appendix : kronecker determinantal point processes *",
    "we use ` @xmath204 ' to denote the operator that stacks columns of a matrix to form a vector ; conversely , ` @xmath205 ' takes a vector with @xmath206 coefficients and returns a @xmath207 matrix .",
    "let @xmath81 , @xmath208 and @xmath209 .",
    "we note @xmath119 the matrix with all zeros except for a 1 at position @xmath35 , its size being clear from context .",
    "we wish to solve @xmath210 it follows from the fact that @xmath211 that @xmath212 and @xmath213 .",
    "moreover , we know that @xmath214    the jacobian of @xmath215 is given by @xmath216 . hence , @xmath217 the last equivalence is simply the result of indices manipulation .",
    "thus , we have @xmath218    similarly , by setting @xmath219 , we have that @xmath220 hence , @xmath221 which proves prop .  [ prop : differenciation ] .",
    "the updates to @xmath140 and @xmath112 are obtained efficiently through different methods ; hence , the proof to thm .",
    "[ thm : complexity ] is split into two sections .",
    "we write @xmath222 so that @xmath223 .",
    "recall that @xmath224 .",
    "we wish to compute @xmath225 efficiently .",
    "we have @xmath226 \\\\           & = \\operatorname{tr}\\left[l_2^{-1 } ( l\\delta l)_{(ij)}\\right ] \\\\           & = \\operatorname{tr}\\left[l_2^{-1}\\sum\\nolimits_{k,\\ell=1}^{n_1 } l_{(ik ) } \\delta_{(k\\ell)}l_{(\\ell j)}\\right ] \\\\           & = \\sum\\nolimits_{k,\\ell=1}^{n_1 } { l_1}_{ik } { l_1}_{\\ell j } \\operatorname{tr}(l_2^{-1 } l_2 \\delta_{(k\\ell ) } l_2 ) \\\\           & = \\sum\\nolimits_{k,\\ell=1}^{n_1 } { l_1}_{ik } { l_1}_{\\ell j } \\underbrace{\\operatorname{tr}(\\theta_{(k\\ell ) } l_2)}_{a_{k\\ell } } - \\underbrace{\\operatorname{tr}((i+l)^{-1}_{(k\\ell ) } l_2)}_{b_{k\\ell } } \\\\           & = ( l_1 a l_1 - l_1 b l_1)_{ij}.   \\end{aligned}\\ ] ]    the @xmath198 matrix @xmath12 can be computed in @xmath227 simply by pre - computing @xmath149 in @xmath228 and then computing all @xmath229 traces in @xmath230 time .",
    "when doing stochastic updates for which @xmath149 is sparse with only @xmath231 non - zero coefficients , computing @xmath12 can be done in @xmath232 .    by diagonalizing @xmath233 and @xmath234 ,",
    "we have @xmath235 with @xmath236 and @xmath237 . @xmath238 and @xmath239 can all be obtained in @xmath240 as a consequence of prop .",
    "[ prop : basic ] . then @xmath241 let @xmath242 , which can be computed in @xmath243 .",
    "then @xmath244 is computable in @xmath175 .",
    "overall , the update to @xmath140 can be computed in @xmath245 , or in @xmath246 if the updates are stochastic . moreover ,",
    "if @xmath149 is sparse with only @xmath247 non - zero coefficients ( for stochastic updates @xmath248 ) , @xmath12 can be computed in @xmath249 space , leading to an overall @xmath250 memory cost .",
    "we wish to compute @xmath251 $ ] efficiently .",
    "@xmath252 @xmath12 can be computed in @xmath253 .",
    "as before , when doing stochastic updates @xmath12 can be computed in @xmath254 time and @xmath255 space due to the sparsity of @xmath149 .",
    "regarding @xmath42 , as all matrices commute , we can write @xmath256 where @xmath257 is diagonal and is obtained in @xmath240 . moreover , @xmath258 which allows us to compute @xmath42 in @xmath259 total .    overall , we can obtain @xmath181 in @xmath260 or in @xmath261 for stochastic updates , in which case only @xmath262 space is necessary .",
    "in order to minimize the number of matrix multiplications , we equivalently ( due to the properties of the frobenius norm ) minimize the equation @xmath263 and set @xmath264 .",
    "suppose that @xmath267 has an eigengap between its largest singular value and the next , and let @xmath268 be the first singular vectors and value of @xmath267 .",
    "let @xmath269 and @xmath270",
    ". then @xmath271 and @xmath171 are either both positive definite or negative definite .",
    "the proof is a consequence of  ( * ? ? ?",
    "this shows that if @xmath14 is initially positive definite , setting the sign of @xmath275 based on whether @xmath271 and @xmath171 are positive or negative definite , which will be positive if and only if @xmath276 .",
    "] , and updating @xmath277 maintains positive definite iterates . given that if @xmath278 and @xmath279 , @xmath280 , a simple induction then shows that by choosing an initial kernel estimate @xmath265 , subsequent values of @xmath14 will remain positive definite .",
    "matrices @xmath123 , training set @xmath124 , step - size @xmath282 .",
    "@xmath283 power_method@xmath284 to obtain the first singular value and vectors of matrix @xmath267 . @xmath285 @xmath286 @xmath287 * return * @xmath127"
  ],
  "abstract_text": [
    "<S> determinantal point processes ( dpps ) are probabilistic models over all subsets a ground set of @xmath0 items . </S>",
    "<S> they have recently gained prominence in several applications that rely on `` diverse '' subsets . however , their applicability to large problems is still limited due to the @xmath1 complexity of core tasks such as sampling and learning . </S>",
    "<S> we enable efficient sampling and learning for dpps by introducing krondpp , a dpp model whose kernel matrix decomposes as a tensor product of multiple smaller kernel matrices . </S>",
    "<S> this decomposition immediately enables fast _ exact _ sampling . </S>",
    "<S> but contrary to what one may expect , leveraging the kronecker product structure for speeding up dpp learning turns out to be more difficult . </S>",
    "<S> we overcome this challenge , and derive batch and stochastic optimization algorithms for efficiently learning the parameters of a krondpp . </S>"
  ]
}