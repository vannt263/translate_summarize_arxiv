{
  "article_text": [
    "artificial neural networks ( ann ) have been shown to solve many real world problems , such as , computer vision , natural language processing , recommendation systems and many other fields @xcite .",
    "convolutional neural network ( cnn ) architectures achieve human performance in many computer vision problems including image classification tasks @xcite . however , the number of parameters in these high - performance networks ranges from millions to billions which require computers capable of handling high computational complexity , high energy and memory size .",
    "consequently , the minimal computational environment for such a network is a desktop computer with a powerful cpu and a dedicated high - end gpu .",
    "recent developments in vlsi industry create powerful mobile devices which can be used in many practical recognition applications .",
    "anns are already being used in drones and unmanned aerial vehicles for flight control , path estimation @xcite , obstacle avoidance and human recognition like abilities @xcite ( dji phantom 4 ) .",
    "however , the current structure of the anns , especially , deep networks , prohibits us to implement these algorithms effectively on mobile devices due to high energy requirements .",
    "a typical neuron needs to perform three main tasks to produce an output : ( i ) an inner product operation involving multiplication of inputs by weights , ( ii ) addition , and ( iii ) pass the result of the inner product through an activation function . according to the @xcite , the multiplication operation is the most energy consuming operation . in this paper , we propose an @xmath2 norm based energy efficient neural network , called _ additive neural network _ , that replaces the multiplication operation with a new energy efficient operator , called _ ef - operator_. instead of multiplications , we use sign multiplications and addition operations in a typical neuron .",
    "the sign multiplication of two real numbers is a simple bit operation .",
    "an addition consumes relatively lower energy compared to a regular multiplication as shown in @xcite in most processors .",
    "our object recognition experiments on mnist and cifar datasets show that we are able to match the performance of the state of the art neural networks without performing any other changes on the ann structure . in section 2 ,",
    "we review the related work in energy efficient neural network design . in section 3",
    ", we define a new vector product and the corresponding operator , called ef - operator . in section 4",
    ", we introduce the additive neural network , based on the ef - operator . in section 5",
    ", we made a brief analysis for the existence and convergence problems of the proposed additive neural network .",
    "section 6 , provides the experimental results to compare the performance of the proposed additive neural network with multi - layer perceptron and convolutional neural networks .",
    "finally , section 7 concludes the paper .",
    "due to large size of the parameter space , artificial neural networks are generally computationally prohibitive and become inefficient in terms of energy consumption and memory allocation .",
    "several approaches from different perspectives have been proposed to design computationally efficient neural network structures to handle high computational complexity .",
    "we first introduced the @xmath2 norm based vector product for some image processing applications in 2009 @xcite .",
    "we also proposed the multiplication free neural network structure in 2015 @xcite .",
    "however , the recognition rate was below @xmath3of a regular neural network . in this article , we are able to match the performance of regular neural networks by introducing a scaling factor to the @xmath2 norm based vector product and new training methods .",
    "we are only @xmath4 below the recognition rate of a regular neural network in mnist dataset .",
    "other solutions to energy efficient neural networks include dedicated software for a specific hardware , i.e. neuromorphic devices @xcite .",
    "although such approaches reduces energy consumption and memory usage , they require special hardware . our neural network framework can be implemented in ordinary microprocessors and digital signal processors .",
    "sarwar et al . used the error resiliency property of neural networks and proposed an approximation to multiplication operation on artificial neurons for energy - efficient neural computing @xcite .",
    "they approximate the multiplication operation by using the alphabet set multiplier ( asm ) and computation sharing multiplication ( cshm ) methods . in asm ,",
    "the multiplication steps are replaced by shift and add operators which are performed by some alphabet defined by a pre - computer bank .",
    "this alphabet is basically a subset of the lower order multiplies of the input .",
    "the multiplies that are not exist in the computed subset are approximated by rounding them to nearest existing multiplies .",
    "this method reduces the energy consumption since addition and bit shifting operations are much efficient than the multiplication .",
    "therefore , the smaller sized alphabets result in a more efficient architecture . additionally , they define a special case called multipler - less artificial neuron ( man ) , in which there is only one alphabet for each layer .",
    "this method provides more energy efficiency with a minimum accuracy loss .",
    "it should be noted that this method is applied on test stages , therefore , the training step still uses the conventional method .",
    "han et al .",
    "proposed a model that reduces both computational cost and storage by feature learning @xcite .",
    "their approach consists of three steps . in the first step , they train the network to discriminate important features from redundant ones .",
    "then , they remove the redundant weights , and occasionally neurons , according to a threshold value to obtain a sparser network .",
    "this step reduces the test step s cost . at the final step",
    "they retrain the network to fine tune the remaining weights .",
    "they state that this step is much more efficient than using the fixed network architecture .",
    "they tested the proposed network architecture with imagenet and vgg-16 .",
    "the parameter size for these networks reduces between @xmath5 to @xmath6 without any accuracy loss .",
    "abdelsalam et al .",
    "approximate the tangent activation function using the discrete cosine transform interpolation filter ( dctif ) to run the neural networks on fpga boards efficiently @xcite .",
    "they state that dctif approximation reduces the computational complexity at the activation function calculation step by performing simple arithmetic operations on stored samples of the hyperbolic tangent activation function and input set .",
    "the proposed dctif architecture divides the activation function into three regions , namely , pass , process and saturation regions . in the pass region",
    "the activation function is approximated by y = x and in the saturation region the activation function is taken as y = 1 .",
    "the dctif takes place in the process region .",
    "parameters of the transformation should be selected carefully to find a balance between computational complexity and accuracy .",
    "they have shown that the proposed method achieve significant decrease on energy consumption while keeping the accuracy difference within @xmath7 with conventional method .",
    "rastegari et al . proposes two methods to provide efficiency on cnns .",
    "the first method , binary - weight - networks , approximates all the weight values to binary values @xcite . in this way",
    "the network needs less memory ( nearly @xmath8 ) .",
    "since the weight values are binary , convolutions can be estimated by only addition and subtraction , which eliminates the main power draining multiplication operation .",
    "therefore , this method both provides energy efficiency and faster computations .    the second method proposed by them",
    "is called xnor - networks where both weights and inputs to the convolutional and fully connected layers are approximated by binary values .",
    "this extends the earlier proposed method by replacing addition and subtraction operations with xnor and bitcounting operations .",
    "this method offers @xmath9 faster computation on cpu on average .",
    "while this method enables us to run cnns on mobile devices , it costs @xmath10 loss accuracy on average .",
    "let @xmath11 and @xmath12 be two vectors in @xmath13 .",
    "we define an new operator , called ef - operator , as the vector product of @xmath11 and @xmath12 as follows ;    @xmath14    which can also be represented as follows ;    @xmath15    where @xmath16^t,\\mathbf{y}=[y_1,\\,\\hdots,\\,y_d]^t\\in\\mathds{r}^d$ ] .",
    "the new vector product operation does not require any multiplications . the operation @xmath17 uses the sign of the ordinary multiplication but it computes the sum of absolute values of @xmath18 and @xmath19 .",
    "ef - operator , @xmath20 , can be implemented without any multiplications .",
    "it requires summation , unary minus operation and if statements which are all energy efficient operations .",
    "ordinary inner product of two vectors induces the @xmath21 norm .",
    "similarly , the new vector product induces a scaled version of the @xmath22 norm :    @xmath23    therefore , the ef - operator performs a new vector product , called @xmath22 product of two vectors , defined in eq .",
    "[ eq : operator_definition1 ] .",
    "we use following notation for a compact representation of ef - operation of a vector by a matrix .",
    "let @xmath24 and @xmath25 be two matrices , then the ef - operation between @xmath26 and @xmath27 is defined as follows ;    @xmath28    where @xmath29 is @xmath30th column of @xmath31 for @xmath32 .",
    "we propose a modification to the representation of a neuron in a classical neural network , by replacing the vector product of the input and weight with the @xmath2 product defined in ef - operation .",
    "this modification can be applied to a wide range of artificial neural networks , including multi - layer perceptrons ( mlp ) , recurrent neural networks ( rnn ) and convolutional neural networks ( cnn ) .",
    "a neuron in a classical neural network is represented by the following activation function ;    @xmath33    where @xmath34 , @xmath35 are weights and biases , respectively , and @xmath36 is the input vector .    a neuron in the proposed additive neural network is represented by the activation function , where we modify the affine transform by using the ef - operator , as follows ;    @xmath37    where @xmath38 is element - wise multiplication operator , @xmath34 , @xmath39 are weights , scaling coefficients and biases , respectively , and @xmath36 is the input vector .",
    "the neural network , where each neuron is represented by the activation function defined in eq .",
    "[ eq : proposed_layer ] , is called additive neural network .",
    "comparison of eq .",
    "[ eq : classic_layer ] and eq .",
    "[ eq : proposed_layer ] shows that the proposed additive neural networks are obtained by simply replacing the affine scoring function ( @xmath40 ) of a classical neural network by the scoring function function defined over the ef - operator , @xmath41 .",
    "therefore , most of the neural networks can easily be converted into the additive network by just representing the neurons with the activation functions defined over ef - operator , without modification of the topology and the general structure of the optimization algorithms of the network .",
    "standard back - propagation algorithm is applicable to the proposed additive neural network with small approximations",
    ". back - propagation algorithm computes derivatives with respect to current values of parameters of a differentiable function to update its parameters .",
    "derivatives are computed iteratively using previously computed derivatives from upper layers due to chain rule .",
    "activation function , @xmath42 , can be excluded during these computations for simplicity as its derivation depends on the specific activation function and choice of activation function does not affect the remaining computations .",
    "hence , the only difference in the additive neural network training is the computation of the derivatives of the argument , @xmath41 , of the activation function with respect to the parameters , @xmath43 , and input , @xmath11 , as given below :    @xmath44    @xmath45    @xmath46    @xmath47    where @xmath48 , and @xmath34 are the parameters of the hidden layer , @xmath36 is the input of the hidden layer , @xmath49 is the @xmath50th element of standard basis of @xmath51 , @xmath52 is the @xmath50th column of @xmath31 , @xmath53 for @xmath54 , @xmath55 is the dirac delta function .",
    "the above derivatives can be easily calculated using the following equation suggested by @xcite :    @xmath56    approximations to derive the above equation are based on the fact that @xmath57 , almost surely .      in this section , first , we show that the proposed additive neural network satisfies the universal approximation property of @xcite , over the space of lebesgue integrable functions . in other words .",
    "there exists solutions computed by the proposed additive network , which is equivalent to the solutions obtained by activation function with classical vector product .",
    "then , we make a brief analysis for the convergence properties of the back propagation algorithm when the vector product is replaced by the ef - operators .",
    "the universal approximation property of the suggested additive neural network is to be proved for each specific form of the activation function . in the following proposition , we suffice to provide the proofs of universal approximation theorem for linear and relu activation functions , only .",
    "the proof ( if it exits ) for a general activation function requires a substantial amount of effort , thus it is left to a future work .",
    "[ prop : approximation ] the additive neural network , defined by the neural activation function with identity    @xmath58    or an activation function with rectified linear unit ,    @xmath59    is dense in @xmath60 .    in order to prove the above proposition ,",
    "the following two lemmas are proved first :    [ lemma : sign ] if activation function @xmath42 is taken as identity ( as in eq .",
    "[ eq:12 ] ) , then there exist additive neural networks , defined over the ef - operator , which can compute @xmath61 , for any @xmath62 and @xmath63 .",
    "constructing an additive neural network , defined over ef - operator , is enough to prove the lemma .",
    "we can construct explicitly a sample network for any given @xmath62 and @xmath63 .",
    "one such network consists of four hidden layers for @xmath64 , this network can easily extended into higher dimensions .",
    "let @xmath11 be @xmath65^t$ ] and @xmath12 be @xmath66^t$ ] , then four hidden layers with following parameters can compute @xmath67 .",
    "* hidden layer 1 , + @xmath68^t$ ] , + @xmath69^t$ ] , + @xmath70 . * hidden layer 2 , + @xmath71 $ ] , + @xmath72 $ ] , + @xmath73 . * hidden layer 3 , + @xmath74 , + @xmath75 , + @xmath76 . * hidden layer 4 , + @xmath77 $ ] , + @xmath78 $ ] , + @xmath79 .",
    "the function computed by this network can be simplified using the fact that , @xmath80 and @xmath81 ,    @xmath82    then , the hidden layers @xmath83 and @xmath84 can be represented as follows ;    @xmath85    [ lemma : relu_equiv ] if the function @xmath86 can be computable with activation function    @xmath87    then there exist an additive neural network architectures with a rectified linear unit activation function ,    @xmath88    which can also compute @xmath86 .",
    "this lemma can be proven using the following simple observations ,    * observation 1 : if + @xmath89 + then , + @xmath90 + where @xmath91 , @xmath92 , and @xmath93 . *",
    "observation 2 : if + @xmath89 + then , + @xmath94 + where @xmath91 , @xmath92 , and @xmath95 .",
    "* observation 3 : if + @xmath89 + then , + @xmath96 + where @xmath97 , @xmath98 , and @xmath99 .",
    "lets assume that there exists an additive neural network , defined over the ef - operator , using identity as activation function which can compute the function @xmath86 .",
    "we can extend each layer using observation 1 , to compute both @xmath86 and @xmath100 .",
    "afterwards , we can replace zeros on the weights introduced during previous extension on each layer using observation 3 , to replace the activation function with relu .",
    "this works , because either @xmath101 or @xmath102 is 0 .",
    "the modified network is an additive neural network with relu activation function , which can compute the function @xmath86 .",
    "this can be shown by the universal approximation theorem for bounded measurable sigmoidal functions @xcite .",
    "this theorem states that finite sums of the form @xmath103    are dense in @xmath60 , where @xmath104 and @xmath105 for @xmath106 .",
    "it can be easily shown that @xmath107 function is a bounded sigmoidal function .",
    "* lemma * [ lemma : sign ] shows that , if the activation function is taken as identity , then there exist networks which compute @xmath108 for @xmath109 . *",
    "lemma * [ lemma : relu_equiv ] shows that there are equivalent networks using relu as the activation function which compute the same functions .",
    "these networks can be combined with concatenation of layers of the additive neural networks to a single network .",
    "also , proposed architecture contains fully connected linear layer at the output , and this layer can compute superposition of the computed @xmath107 functions yielding @xmath110 . since @xmath111 can be computable by the additive neural networks , and @xmath111 functions are dense in @xmath60 , then functions computed by the additive neural networks are also dense in @xmath60 .",
    "the proposed additive neural network contains more parameters then the classical neuron representation in mlp architectures .",
    "however , each hidden layer can be computed using considerably less number of multiplication operator .",
    "a classical neural network , represented by the activation function @xmath112 , containing @xmath113 neurons with @xmath114 dimensional input , requires @xmath115 many multiplication operator to compute @xmath116 . on the other hand , the additive neural network , represented by the activation function",
    ", @xmath117 with the same number of neurons and input space requires @xmath113 many multiplication operator to compute @xmath118 .",
    "this reduction on number of multiplications is especially important when input size is large or hidden layer contains large number of neurons .",
    "if activation function is taken as either identity or relu , then output of this layer can be computed without any complex operations , and efficiency of the network can be substantially increased .",
    "multiplications can be removed entirely , if scaling coefficients , @xmath119 are taken as 1 .",
    "however , these networks may not represent some functions , and consequently may perform poorly on some datasets .",
    "due to the sign operation performed in each neuron , the ef - operator creates a bunch of hyperoctants in the cost function at each layer of the additive neural network .",
    "therefore , the local minima computed at each layer , depends on the specific hyperoctant for a set of weights .",
    "the change in the signs results in a jump from a hyperoctant to another one .    for some datasets",
    ", some of the local minima may lie on the boundaries of the hyperoctants .",
    "since the hyperoctants are open sets , this may leave some hyperoctands with non - existing local minima . a gradient based search algorithm may update the weights such that the algorithm converges to the local minima on the boundary .",
    "if the step size and number of epochs are increased , then the updated weights leave the current hyperoctant without converging to a local minima on the boundary and new set of weights make the algorithm to converge to a local minima in another hyperoctant .",
    "however , the new hyperoctant may have the same problem .",
    "[ cols=\"<,^,^,^,^,^,^,^\",options=\"header \" , ]         multi - layer perceptron ( mlp ) @xcite is used to measure the ability of the proposed additive neural network , in machine learning problems .",
    "mlp consists of a single input and output layer and multiple hidden layers .",
    "the size and the number of hidden layers can vary a great deal , depending on the problem domain . in this research ,",
    "we use one , two and three hidden layers , respectively , in two different classification problems , namely xor problem and character recognition of mnist dataset .",
    "the input layer receives pattern sample @xmath120 to the network .    on the other hand , the hidden layer(s )",
    "contains biological inspired units called neurons which learns a new representations from the input patterns .",
    "each neuron consists of a scoring function and an activation function .",
    "as discussed in the section  [ sec : annwithef ] , the scoring function is an affine transform in the form of @xmath121 in the classic neural network where @xmath11 and @xmath122 are the parameters . in this study , we call the widely used classic scoring function @xmath121 as c - operator . as discussed in the section  [ sec : aneeo ] and [ sec : annwithef ] , the proposed score function , ef - oprerator , is an energy efficient alternative of the classical vector product .",
    "in addition to the score function , each neuron of a hidden layer also has an activation function that makes the network nonlinear .",
    "several activation functions such as sigmoid , hyperbolic tangent ( tanh ) and rectified linear unit ( relu ) functions have been used as the activation function .",
    "while some studies such as @xcite have shown that relu outperform the others in most of the cases , we also examined sigmoid and tanh in the following experiments .",
    "finally , the last layer of mlp , called output layer , maps the final hidden layer to the scores of the classes by using its own score function .",
    "we used both the classical c - operator and the new ef - operator at the output layer to make the final decision .",
    "the aim of mlp is to find the optimal values for parameters @xmath31 and @xmath122 using backpropagation @xcite and optimization algorithms such as stochastic gradient descent ( sgd ) . in order to implement the network , tensorflow @xcite , a python library for numeric computation , is used .    in the first experiment",
    ", we examine the ability of additive neural network to partition a simple nonlinear space , solving the xor problem .",
    "we compare the classical mlp with affine scoring function and additive neural network with ef - operator .",
    "since a single hidden layer mlp with c - operator can solve xor problem , we used one hidden layer in both classical and the proposed architectures . mean",
    "squared error is used as cost function to measure the amount of loss in training phase of the network , and we fixed the number of neurons in the hidden layer to 10",
    ".    the additive neural network with ef - operator could successfully solve the xor problem and reached to @xmath123 accuracy in this problem .",
    "we also investigate the rate of changes inloss changes at each epoch .",
    "it is also notable that some of the runs that are shown by colors , do not reach to minimum values in 1000 epochs .",
    "this shows that more epochs is needed in some runs .",
    "generally , the number of epochs depends on learning rate and initialization condition , and the final epoch can be determined by some stopping criteria .",
    "however , in this study , we are only interested to see the variations in the cost ; therefore , we fixed the number of epochs to 1000 .",
    "left and right sides of fig .",
    "[ costfig ] show the change of loss in the mlp using c - operator and ef - operator , respectively , with relu as the activation function .",
    "we rerun the network for 200 times in 1000 epochs , and used k - fold cross validation to specify the learning - rate parameter of sgd .",
    "each color of the plots shows the variations in loss or cost value ( x axis ) across the epochs ( y axis ) in one specific run of the network . as the figure shows , the cost value of the network with our proposed ef - operator decreases along the epochs and acts similar to classical affine operator , called c - operator .    in the second experiment , we classified the digits of mnist dataset of @xcite which consists of handwritten examples to examine our proposed additive neural network in multiclass classification problem .",
    "mnist dataset consists of 30,000 training samples and 5,000 test data .",
    "each example is an image of a digit from 0 to 9 .",
    "one - hot code is used to encode the class labels .",
    "each example is an image of size @xmath124 , and each image is concatenated in a single vector to input the network .",
    "therefore , the size of the input layer of the network is 784 .",
    "we used cross - entropy based cost function and sgd to train the network .",
    "we used 150 number of examples in each iteration of sgd .",
    "in other words , the batch size is equal to 150 .",
    "table  [ resulttable ] contains the classification accuracies of the mlp architecture using three activation functions : relu , tanh and sigmoid with four different learning rates .",
    "as the table shows , our additive neural network over ef - operator reaches to the performance of classic mlp with c - operator . in other words , with a slightly sacrificing the classification performance we can use the proposed ef - operator which much more energy - efficient . note that , we have not used any regularization methods such as drop out used by krizhevsky et al .",
    "@xcite , because we simply aim to show that our proposed ef - operator gives the learning ability to the deep mlp .",
    "also table .",
    "[ resulttable ] shows that maximum of the performances have been obtained using relu activation function .",
    "we are also interested to see the variations in the classification performances during the epochs and along the epochs .",
    "with addition to mlp , we have used the proposed ef - operator to learn the parameters of lenet-5 @xcite to classifying mnist dataset .",
    "table  [ resulttable ] contains the classification accuracy of lenet-5 architecture that contains two conventional and one fully connected layer .",
    "we trained the network with sgd and cross - entropy based cost functions as we did on mlp case .",
    "it should be noted that we have used the conventional c - operator in the output layer of both mlp and lenet-5 architectures . as shown in the table , the proposed ef - operator catches up the c - operator with a small amount of loss .",
    "figure  [ resutlscurve ] shows the results of the classification accuracies obtained from mlp based on our proposed ef - operator and traditionally used c - operator .",
    "the performances ( shown in the y axis of the sub figures ) obtained in successive epochs ( shown in the x axis of the sub figures ) . in each epoch , the network is trained with all of the training examples .",
    "the plots of the sub - figures are obtained using four different learning rates : 0.1 , 0.005 , 0.001 and 0.0005 .",
    "subplots ( a ) and ( b ) at the left of figure shows the results of c - operator in mlp with 2 and 3 hidden layers respectively , and subplots ( c ) and ( d ) shows the results of our proposed ef - operator . as figure  [ resutlscurve ] shows , our operator effectively increases the classification performance as the number of epochs increases and reaches nearly to the original linear function .",
    "in this study , we propose an energy efficient additive neural network architecture .",
    "the core of this architecture is the lasso norm based ef - operator that eliminates the energy - consumption multiplications in the conventional architecture .",
    "we have examined the universal approximation property of the proposed architecture over the space of lebesgue integrable functions and test it in real world problems .",
    "we showed that ef - operator can successfully solve the nonlinear xor problem .",
    "moreover , we have observed that with sacrificing @xmath125 and @xmath126 accuracy , our proposed network can be used in the multilayer perceptron ( mlp ) and conventional neural network respectively to classify mnist dataset . as a future work",
    ", we plan to test the proposed architecture in the state - of - the - art deep neural networks .",
    "a. enis cetin s work was funded in part by a grant from qualcomm .",
    "y. taigman , m. yang , m. ranzato , and l. wolf , `` deepface : closing the gap to human - level performance in face verification , '' in proceedings of the ieee conference on computer vision and pattern recognition,2014 , pp .",
    "17011708 .    c. szegedy , w. liu , y. jia , p. sermanet , s. reed , d. anguelov , d. erhan , v. vanhoucke , and a. rabinovich , `` going deeper with convolutions , '' in proceedings of the ieee conference on computer vision and pattern recognition , 2015 , pp",
    ". 19 .",
    "a. giusti , j. guzzi , d. c. cires an , f .-",
    "he , j. p. rodr guez , f. fontana , m. faessler , c. forster , j. schmidhuber , g. di caro et al .",
    ", `` a machine learning approach to visual perception of forest trails for mobile robots , '' ieee robotics and automation letters , vol . 1 , no",
    ". 2 , pp . 661667 , 2016 .",
    "a. suhre , f. keskin , t. ersahin , r. cetin - atalay , r. ansari , and a. e. cetin , `` a multiplication - free framework for signal processing and applications in biomedical image analysis , '' in 2013 ieee international conference on acoustics , speech and signal processing ( icassp ) .",
    "ieee , 2013 , pp .",
    "11231127 .        c. e. akbas  , a. bozkurt , a. e. cetin , r. cetin - atalay , and a. uner , `` multiplication - free neural networks , '' in 2015 23th signal processing and communications applications conference ( siu ) .",
    "ieee , 2015 , pp .",
    "24162418 .",
    "s. k. esser , p. a. merolla , j. v. arthur , a. s. cassidy , r. appuswamy , a. andreopoulos , d. j. berg , j. l. mckinstry , t. melano , d. r. barch et al . , `` convolutional networks for fast , energy - efficient neuromorphic computing , '' arxiv preprint arxiv:1603.08270 , 2016 .",
    "e. painkras , l. a. plana , j. garside , s. temple , f. galluppi , c. patterson , d. r. lester , a. d. brown , and s. b. furber , `` spinnaker : a 1-w 18-core system - on - chip for massively - parallel neural network simulation , '' ieee journal of solid - state circuits , vol .",
    "48 , no . 8 , pp . 19431953 , 2013 .",
    "t. pfeil , a. gr ubl , s. jeltsch , e. m uller , p. m uller , m. a. petrovici , m. schmuker , d. br uderle , j. schemmel , and k. meier , `` six net - works on a universal neuromorphic computing substrate , '' arxiv preprint arxiv:1210.7083 , 2012 .",
    "s. moradi and g. indiveri ,",
    "`` an event - based neural network architecture with an asynchronous programmable synaptic memory , '' ieee transactions on biomedical circuits and systems , vol . 8 , no .",
    "1 , pp . 98107 , 2014 .",
    "j. park , s. ha , t. yu , e. neftci , and g. cauwenberghs , `` a 65k - neuron 73-mevents / s 22-pj / event asynchronous micro - pipelined integrate - and- fire array transceiver , '' in 2014 ieee biomedical circuits and system conference ( biocas ) proceedings .",
    "ieee , 2014 , pp .",
    "675678 .",
    "s. s. sarwar , s. venkataramani , a. raghunathan , and k. roy , `` multiplier - less artificial neurons exploiting error resiliency for energy - efficient neural computing , '' in 2016 design , automation & test in europe conference & exhibition ( date )",
    ". ieee , 2016 , pp .",
    "145150 .",
    "m. abadi , a. agarwal , p. barham , e. brevdo , z. chen , c. citro , g. s. corrado , a. davis , j. dean , m. devin , s. ghemawat , i. goodfellow , a. harp , g. irving , m. isard , y. jia , r. jozefowicz , l. kaiser , m. kudlur , j. levenberg , d. man e , r. monga , s. moore , d. murray , c. olah , m. schuster , j. shlens , b. steiner , i. sutskever , k. talwar , p. tucker , v. vanhoucke , v. vasudevan , f. vi egas , o. vinyals , p. warden , m. wattenberg , m. wicke , y. yu , and x. zheng , `` tensorflow : large - scale machine learning on heterogeneous systems , '' 2015 , software available from tensorflow.org ."
  ],
  "abstract_text": [
    "<S> in recent years , machine learning techniques based on neural networks for mobile computing become increasingly popular . </S>",
    "<S> classical multi - layer neural networks require matrix multiplications at each stage . </S>",
    "<S> multiplication operation is not an energy efficient operation and consequently it drains the battery of the mobile device . in this paper </S>",
    "<S> , we propose a new energy efficient neural network with the universal approximation property over space of lebesgue integrable functions . </S>",
    "<S> this network , called , additive neural network , is very suitable for mobile computing . </S>",
    "<S> the neural structure is based on a novel vector product definition , called ef - operator , that permits a multiplier - free implementation . in ef - operation , the `` product '' of two real numbers is defined as the sum of their absolute values , with the sign determined by the sign of the product of the numbers . </S>",
    "<S> this `` product '' is used to construct a vector product in @xmath0 . </S>",
    "<S> the vector product induces the @xmath1 norm . </S>",
    "<S> the proposed additive neural network successfully solves the xor problem . </S>",
    "<S> the experiments on mnist dataset show that the classification performances of the proposed additive neural networks are very similar to the corresponding multi - layer perceptron and convolutional neural networks ( lenet ) . </S>"
  ]
}