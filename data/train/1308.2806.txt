{
  "article_text": [
    "the inevitable occurence of dead time when counting events from a random process needs to be taken into account during analysis .",
    "the two traditional limits , nonextending and extending deadtime , are mathematically convenient but do not seem to describe actual data , see @xcite and references therein for details .",
    "recently an analysis method for extracting halflives from decay data was suggested by horvat and hardy @xcite in which this problem is circumvented by imposing a software extendable dead time .",
    "the method was shown to give a significant reduction of the systematic error on the extracted halflife .",
    "the horvat and hardy paper focussed on the case where beta particles were measured so that all events enter in the analysis .",
    "we explore here extensions to other cases where more information is available for each event , the example we shall use is detection of @xmath0-rays where the energy is also recorded . in this case one can often select a subset of events that have a more favourable signal to noise ratio , e.g.  by gating on a prominent @xmath0 ray in the decay .",
    "this extension is needed if one wishes to use the method also for electron capture decays , one example being the decay of @xmath3be @xcite .",
    "the extension will also be useful in other cases where the total count rate is sufficient to make dead time a potential problem , but the fraction of interesting events is a smaller part of the total .",
    "we shall furthermore show how the method of horvat and hardy can be easily modified to provide a goodness - of - fit test as well and compare to a histogram analysis procedure .",
    "we first specify the notation .",
    "decays follow an exponential distribution , @xmath4 being the normalized function .",
    "the mean lifetime @xmath5 and the halflife @xmath6 are related to the decay constant @xmath7 by @xmath8 .",
    "the total rate of events , @xmath9 , when a background is present is @xmath10 where @xmath11 is the rate of background events and @xmath12 can be interpreted as the total number of decay events that could be recorded in an ideal experiment ( i.e.  without dead time and with the measurement continuing until all decays have happened ) .",
    "our notation differs from the one of horvat and hardy that include @xmath7 in the front factor ; our choice leads to fit parameters with smaller correlation which gives smaller final relative uncertainties on @xmath12 .",
    "the expression for @xmath9 can easily be generalized to the case where several activities with each their halflife is present in the sample .",
    "when a subset of interesting events can be selected , one divides all events into two types @xmath13 ( this can again be generalized ) .",
    "both groups of events may have both `` true decays '' and background events and will have different decay rates @xmath14 where @xmath15 and @xmath16 .",
    "the decay constant @xmath7 is of course the same for the two types .",
    "we let type 1 denote the events that have the best signal to background ratio , i.e.  @xmath17 .",
    "the analysis takes place on @xmath18 events measured from the same sample with decay times @xmath19 , @xmath20 .",
    "the limitation that the data must result from one time series can be lifted by analyzing simultaneously several sets of data with each their parameters @xmath21 , @xmath22 ( the decay constant @xmath7 still being the same and the index @xmath23 denoting the different sets ) .",
    "the method becomes impractical if the number of produced samples is too large .",
    "we refer to @xcite for a detailed description of the horvat and hardy procedure and will here give a brief alternative derivation of it .",
    "there are two steps in the procedure . in the first step",
    "the time series is pruned by imposing a fixed extendable dead time @xmath24 after each recorded event .",
    "all events falling within a dead time window are removed and the `` live time '' @xmath25 preceeding each surviving event ( the time that has passed since the end of the last preceeding time window until event number @xmath26 ) is calculated . in the second step , the probability of each surviving event is calculated and combined into a likelihood function .",
    "since the rate @xmath9 depends ( slowly ) on time the appropriate probability density is that of an nonhomogenous poission process ( see e.g.  equation ( 2.2.23 ) in @xcite ) @xmath27 the final likelihood is then the product @xmath28 ( horvat and hardy derive the expression for @xmath29 and include a factor in @xmath30 corresponding to the time between the last event and the end of the measurement .",
    "this last factor must be included to avoid a bias in the method , but the bias is of order @xmath31 which in many situations is negligible . )",
    "the log - likelihood is finally given by @xmath32 .",
    "note that the time series from a given sample need not be uninterrupted .",
    "the data taking may therefore include shorter or longer breaks , e.g.  for file change or when different samples are measured with the same set - up .",
    "it is straight - forward to extend this result by forming a likelihood ratio @xmath33 .",
    "this gives the possibility of performing also goodness - of - fit tests .",
    "to form @xmath34 we optimize the `` rate parameter '' independently for each event . in the approximation where the rate is taken as constant over the time interval @xmath25 this",
    "is easily shown to give @xmath35 .",
    "the final log - likelihood is therefore @xmath36 if @xmath37 one may increase the numerical precision by taylor expanding in the integral @xmath38    one reason for inserting the factor @xmath39 into @xmath40 is that the resulting quantity in many cases asymptotically will be @xmath41 distributed @xcite .",
    "however , in our case the result is different as shown in [ sec : etap ] : the value per degree of freedom @xmath42 ( @xmath43 being the number of fit parameters ) is @xmath44 with a standard deviation of @xmath45 .",
    "( a @xmath41 distribution would give @xmath46 and @xmath47 , respectively . )",
    "if dead time was absent and one could select a subset of events with better signal to background ratio , one would simply restrict the halflife analysis to this subset .",
    "however , taking the dead time into account implies one must work with the total recorded data set . rather than attempting to use the time intervals between type 1 events directly , a procedure that would be strongly entangled with the original analysis , we focus on including the independent information on the type of event .",
    "by far the simplest way to do this is to return to the expression for a poisson process and use the appropriate rate , @xmath48 or @xmath49 when the event is of type 1 or 2 , explicitly instead of @xmath9 as the first factor in equation ( [ eq : nonhp ] ) ( the integral in the exponential of course still contains the total rate @xmath9 ) .",
    "this corresponds to introducing a factor @xmath50 for each event of type 1 and a factor @xmath51 for each event of type 2 .",
    "the value obtained for @xmath40 will therefore increase by an amount that depends on the size of @xmath50 for each event . the details are given in [ sec : etaseq ] that also discusses a more involved procedure for analyzing the information of the type of event .",
    "if more than one activity is present in the sample , due to contaminants or daughter decays , this needs to be included in the rate @xmath9 . one may extend the above analysis to more than two types simply by employing the appropriate decay rates for each type .",
    "however , it is clearly possible even in the general case to work with just two types , the interesting ones and `` the rest '' , as long as @xmath9 gives a satisfactory account of the event rate .",
    "the need to include in the analysis all activities present in the sample may at first glance seem to be a drawback of the method by comparison to a histogram analysis with a selective gate .",
    "the latter will in favourable cases include only one activity and a background term .",
    "however , if dead time is of concern one must also worry about possible pile - up that could distort the histogram analysis since not all pile - up events can be identified and rejected .",
    "pile - up will remove events from the selected gate and lead to a distortion of the decay curve , a distortion that is much less pronounced if all events are included , see @xcite and references therein for a detailed treatment of pile - up at high countrates . furthermore",
    ", many @xmath0 ray detectors will have a peak - to - total ratio much less than one so that the counting statistics , and therefore the final precision , is increased significantly if events outside the peak region are included as well .",
    "note that pile - up also will affect our method in the previous subsection .",
    "in such cases one can use @xmath52 for all events in the start of the data set and switch to using @xmath48 and @xmath49 when total count rates have decreased to make pile - up negligible .",
    "the optimum analysis method will depend on the exact experimental circumstances and it may pay off to perform several analyses to check for systematic effects .",
    "the time interval analysis is much slower than a conventional histogram analysis and may become impractical for very large data samples .",
    "an alternative procedure is then to remove events from the recorded sequence that falls within the extendable dead time @xmath24 , as done above , but then project the resulting time sequence into a histogram with appropriate time bin width . the histogram coming out of this _ hybrid method _ can then be analyzed with standard methods , as given e.g.  in equation ( 5 ) in @xcite , since the dead time now is known exactly .",
    "this procedure is significantly faster and examples will be shown below .",
    "whatever analysis procedure is used one will in practice need to check for contaminants in the data .",
    "one way is to do a visual inspection of the ( scaled ) residuals between data and fit , as done in @xcite .",
    "our results allow also to use the goodness - of - fit value for a quantitative test .",
    "if need be , the goodness - of - fit value per data point can also be used in a differential manner as check of systematic deviations ( giving a worse fit ) for smaller sections of the total data set .",
    "if the goodness - of - fit value had been @xmath41-distributed one could also have made a quantitative analysis based on the residuals , since the sum of the square of the scaled residuals gives the @xmath41 .",
    "the methods developed above will first be tested on monte carlo data . for the original @xmath40 ( without goodness - of - fit )",
    "much of this was done in @xcite and we will only add a few results to what was obtained there .",
    "figure [ fig : mc100 ] gives results of simulations performed with similar parameters as in @xcite , namely a halflife of 6.3452 s , a background rate of 1/s and an initial activity of 100/s .",
    "a software extendable dead time of 2 @xmath53s is used and about 1000 decays are recorded per run . the analysis is seen to reproduce well the input parameters ( up to terms of order @xmath31 ) and the goodness - of - fit value is seen to be centered around @xmath54 , as expected .",
    "simulations have also been carried out for an initial activity of 1000/s and give similar good agreement .",
    "see [ sec : halflife ] for general comments on the uncertainty on the halflife .",
    "the hybrid method with projection into histograms with bin width 1s gave in this case the same results as the @xmath40 analysis .",
    "( here the poisson likelihood @xmath55 was used that , similar to @xmath40 , does not give a goodness - of - fit per channel of one for low count numbers , see @xcite for details . ) for high initial counting rates a difference between a time interval analysis and a histogram analysis was found in @xcite .",
    "the histograms were there projected out using a non - extendable dead time , whereas we use an extendable dead time . in our simulations",
    "we do not find as strong differences between the two methods as reported in @xcite , but great care must be taken when applying the corrections for an extendable dead time when the corrections become sizeable ( for values of the product of rate and dead time above 0.02 @xcite ) , so we also recommend that the @xmath40 analysis is used whenever the initial counting rates are high .",
    "if the uncertainty on the rate parameter is of interest ( e.g.  in cross - section determinations in activation experiments ) , it is important to use the parametrisation in equation ( [ eq : rho ] ) rather than including @xmath7 in the factor in front of the exponential . in the latter case the correlation coefficient with the halflife",
    "will always be large : the final derived uncertainty on the parameter @xmath12 is in the simulations found to be more than 40% larger .     and the right panel the extracted halflife values .",
    "both have their expected mean value and a spread consitent with the statistics . ]",
    "the method was employed on the nuclei @xmath1cu and @xmath2mn produced via neutron activitation of cu and mn foils .",
    "the literature values for the halflife of the two nuclei are 12.7004(20 ) h @xcite and 2.5789(1 ) h @xcite , respectively .",
    "the cu sample will contain the @xmath56cu 5.1 min activity , and the data from the first hour is therefore excluded from the spectrum shown in figure [ fig : cu64 ] .",
    "the activity coming from the decays , the 511 kev annihilation @xmath0 ray for @xmath1cu and several @xmath0 rays for @xmath2mn , were detected with a standard ge detector . in the analysis a software extendable time interval of 100 @xmath53s was used .",
    "cu per 200 seconds versus time .",
    "the red line through the points is the fit to the data .",
    "the bottom panel shows the corresponding residuals , ( data - fit)/uncertainty . ]    for both nuclei the final error on the halflife decreases by about a factor two when all data are used rather than the restricted gate .",
    "furthermore , analysis of histograms for gated events gives a too high halflife in both cases , most probably due to effects of pile - up .",
    "we therefore quote only results corresponding to analysis of all @xmath0-ray events and check the effect of delaying the starting point of the analysis to make sure that short - lived contaminants are not included .",
    "the final extracted halflives are 12.710(3 ) h for @xmath1cu and 2.5800(7 ) h for @xmath2mn , where the initial count rates were 585/s and 1310/s .",
    "the results of the time interval analysis and the hybrid method agree , with a tendency for the former to be more robust ( the histogram results for @xmath2mn vary more ) . in both cases our values are higher than the current literature values , although only a few ( of our ) standard deviations",
    ". it may be that some of the previous high - precision experiments with initial high counting rates were prone to the systematic effect pointed out by horvat and hardy @xcite .",
    "the residuals for both mn and cu ( bottom panel in figure [ fig : cu64 ] ) give no indications for remaining systematic effects .",
    "the goodness - of - fit value per degree of freedom is in both cases slightly above the theoretical one , @xmath57 for @xmath1cu and @xmath58 for @xmath2mn ( expected deviations from 1.1544 are 2 and 4 on the last digit , respectively ) .",
    "this was traced to a periodic structure ( period around 400 @xmath53s ) in the time interval spectra that must be due to the data acquisition system , but is not expected to influence the extracted halflife values .",
    "the @xmath41 values from the corresponding histogram analyses are acceptable ( 3354/3228 and 2543/2418 ) .    including information on the event type",
    "will always lead to a reduction of the uncertainty on the extracted halflife , but the simulations show that this reduction is only significant when the initial source and background activities are comparable . as an example , with the same parameters as above of halflife 6.3452 s and initial activity 100/s but background rate increased to 100/s one gets an uncertainty of 0.76 s for the extracted halflife ; if a type 1 event in this case has initial activity 25/s and background 1/s the uncertainty decreases to 0.53 s ( with an increase of gof / dof of 0.22 ) .",
    "even though only a quarter of the decay events are of type 1 ( therefore giving twice the uncertainty if no background had been present ) the signal to background ratio is sufficiently more favourable to give a significant reduction in final uncertainty . as a realistic example , we have repeated the analysis of the @xmath1cu spectrum including only data after 72 h where cu and background activities in the full spectrum are about equal .",
    "the time interval analysis of all data gives here a halflife of 12.674(49 ) h with a gof / dof of @xmath59 ( expected spread 8 on the last digit ) whereas analysis with a gate on the 511 kev line gives a halflife of 12.702(42 ) h and a gof value of 7031447 ( the increase per dof of 0.488 is consistent with the results given in [ sec : etaseq ] ) .",
    "the increase in precision is small , but significant .",
    "if the time interval method is employed for experiments with charged particle detection rather than @xmath0 ray detection , the `` good events '' can be much more concentrated in energy and it will always be advantageous to use the information on event type .",
    "the basis for all the analysis performed here is the assumption that effects of dead time will have a maximum extent @xmath24 in time ( @xmath24 can be varied to achieve this ) : for shorter time intervals @xmath60 between events there may be corrections to the usual exponential waiting time distribution , but for larger intervals we can safely assume the exponential distribution .",
    "only events with @xmath61 are selected . in the time interval analysis @xmath62",
    "is used for further analysis via equation ( [ eq : dp ] ) , whereas the data set are projected into a time histogram in the hybrid method .",
    "if more information is available for the event , e.g.  whether a @xmath0-ray energy falls within a `` good window '' or not , this may be used to refine the analysis . in both cases one needs to evaluate the effects of dead time from the full data set , so that e.g.  a correction found from the histogram based on all events can be used for the histogram with the gated spectrum .    the results of the simulations and analysis of real data presented in this paper can be summarized in the following recommendations . if there are only moderate dead time effects one may expect pile - up to be negligible . in this case",
    "it should be safe to do analysis dividing the data into several types ( a gate in energy etc ) , but if doable one should always check results with the full data set .",
    "if the count rates are high analysis should only be done on the full data set . concerning the analysis method @xmath40",
    "should be used unless the amount of data is very large and the product of the initial counting rate and the imposed dead time @xmath24 is less than 0.01 .",
    "our normalization of @xmath40 allows to use it for goodness - of - fit tests . it can be a very sensitive indicator for the presence of contaminants in the data .",
    "the small spread of the expected value of @xmath40 , caused by the large value of @xmath18 , is not kept when more information is included , but figure [ fig : etaseq ] still allows to test goodness - of - fit on about the same level as a @xmath41 test in the histogram analysis .",
    "the methods were employed to analyse decay data from two neutron - activated samples and gave halflives of 12.710(3 ) h for @xmath1cu and 2.5800(7 ) h for @xmath2mn .",
    "these values are free from pile - up and dead time effects and it is noteworthy that the precision obtained here from a single run is approaching the one reached in many of the previous halflife determinations for these two isotopes .",
    "the methods put forward here allow the time interval analysis to be applied to a larger range of problems than beta counting including halflife determinations in more exotic nuclear decays @xcite .",
    "we would like to thank jens ledet jensen for valuable advice on statistical issues .",
    "there is an extensive statistical litterature on possion processes , see e.g.  @xcite that gives a detailed overview of the many tests that have been employed . due to the dead time",
    "we do not record all decay events , the changes this gives for a standard histogram analysis are described e.g.  in @xcite .",
    "we here follow @xcite and analyse instead the distribution of times between individual recorded events that furthermore is `` truncated from the left '' by the imposed software dead time @xmath24 .",
    "since there is no memory in a decaying system , we still have an exponential distribution for each time interval , leading in the general case to the expression in equation ( [ eq : nonhp ] ) .",
    "if no background is present it is easy to show from maximum likelihood ( see e.g.  @xcite ) that the mean of the recorded decay times @xmath63 is the best fit value for @xmath5 and that the relative uncertainties become @xmath64 .",
    "the case where only events up to a time @xmath65 are recorded can also be treated analytically and yields a relative uncertainty of @xmath66^{-1/2 }   \\;.\\ ] ] this shows explicitly the well - known fact that decays should be followed for many halflives in order to extract a precise value .",
    "the presence of a background limits how far in time it is meaningful to follow a decay , but having the possibility to gate on a subset of events may extend this limit",
    ".    it may be illustrative to trace how the final uncertainty on the halflife emerges in our case .",
    "this can be done by using @xcite that the variance on the parameter @xmath7 can be estimated as the inverse of the expectation value of : @xmath67 where the simplest case of equation ( [ eq : rho ] ) was used . here",
    "the second term quantifies the effect of the dead time : if no dead time is present one has @xmath68 and the whole term gives zero .",
    "if no background is present , the first term gives @xmath69 and therefore the standard uncertainty of @xmath70 . with a background term the contributions to the sum decrease when @xmath11 becomes larger than @xmath71 which will increase the final error on @xmath7 .",
    "the effective cut off time for how long it makes sense to continue a measurement is therefore of order the time where the source activity and background activity are equal .",
    "continuing data taking will reduce the uncertainty on the background rate .",
    "the quantity @xmath39 times the log - likelihood ratio is sometimes called deviance in the statistical literature @xcite and we therefore use the name @xmath40 for our case where it is applied to a poisson process .",
    "we generalize the case from the main text slightly and assume that the decay rate can be written as a sum of terms where the amplitudes are independent ( there may be more variables @xmath72 , typically decay rates , in each term and they need not be independent ) @xmath73 so that @xmath74 . as an example , in the case considered in the main text at equation ( [ eq : rhoj ] ) one has @xmath75 , @xmath76 , @xmath77 and @xmath78 . then at the minimum of @xmath40 we have @xmath79 inserting these relations in @xmath40 gives @xmath80 since the middel term can be rewritten to @xmath81 .",
    "the result actually holds @xcite also for the more general case of a gamma distribution ( the exponential distribution being a special case ) .",
    "the hat added to @xmath9 is to remind that it is the decay rate with fit parameters inserted , the restrictions coming from minimization with respect to the parameters @xmath72 ( in our case the halflife ) also have to be added . for the corresponding case of a @xmath41-distribution the main effect of these restrictions",
    "is to reduce the number of degrees of freedom of the distribution ( by up to the number of parameters ) .",
    "we shall assume a similar behaviour here ; in general there may be correction terms of order @xmath82 ( or higher orders ) , see @xcite for general asymptotic results on the deviance for gamma distributions .",
    "if the decay rate describes the data we have an exponential distribution @xmath83 for all parameters @xmath84 . in the limit where the decay rate can be taken as constant over @xmath25 each term in the sum in equation ( [ eq : dp ] ) reduces to @xmath85",
    "the expectation value of this is @xmath86 , @xmath0 being euler s constant , and from @xmath87 one obtains the variance of @xmath88 .",
    "the way our likelihood ratio is constructed we have ( for @xmath34 ) as many parameters as data points , so we are not in the asymptotic limit where log - likelihood ratios become distributed as @xmath41 @xcite .",
    "however , the central limit theorem can be used and gives that the expected value of @xmath40 per degree of freedom becomes @xmath54 with a standard deviation of @xmath89 .      in the procedure in section [ sec : seq ] the likelihood @xmath30 was modified to @xmath90 where @xmath91 or 2 according to the event type , whereas @xmath34 was left unchanged . in the limit where all decay rates are slowly varying one can interpret @xmath92 as the ( local ) probability of obtaining an event of type 1 , @xmath93 then being the probability of obtaining type 2 . based on this one would expect @xmath94 , the extra contribution to @xmath40 per degree of freedom , to give @xmath95 , where the average has to be taken if @xmath96 varies substantially during the data set .",
    "the extra contribution is plotted in figure [ fig : etaseq ] and goes , as expected , to zero in the two limits of @xmath97 and @xmath98 where one type of events dominate .",
    "the same modification of @xmath30 can be derived in the following manner : let there be a total of @xmath99 events of type 1 and let @xmath100 be the number of events of type 2 between the ( i-1)th and ith event of type 1 . as an example , for the sequence @xmath101 the values of @xmath100 are 1 , 4 , 0 and 2 .",
    "we keep the original analysis of @xmath30 and include the information on the type of event with a separate likelihood function for the recorded sequence of types which becomes @xmath102 ( note that this is a geometric distribution @xcite in @xmath100 and not a binomial distribution since the type 1 event by definition is at the beginning of the ith subsequence . ) in doing this we disregard the possible events of type 2 that follow the last event of type 1 .",
    "it is easy to see that @xmath103 is the same as the modified @xmath104 above , but the alternative derivation allows to form a likelihood ratio also for @xmath105 by optimizing each @xmath106 individually which results in @xmath107 and gives @xmath108 where @xmath109 must be used .",
    "the contribution @xmath110 to the log - likelihood from this sequence - analysis therefore becomes @xmath111 \\;.\\ ] ] the corresponding number of degrees of freedom is @xmath112 since two extra parameters ( @xmath113 , @xmath114 ) were introduced in the analysis .",
    "we should consider whether the terms in @xmath115 changes the analysis in [ sec : etap ] and restrict , to simplify notation , ourselves to the case in equation ( [ eq : rhoj ] ) . in this approximation the extra terms that will be added to equation ( [ eq : deriv_al ] )",
    "can be written as @xmath116 explicit evaluation of the four different terms gives a combined contribution to @xmath117 of @xmath118 = 0 \\;,\\ ] ] so that the addition of @xmath115 will not change the expected value for the @xmath40 part ( assuming that the total decay rate @xmath119 is unchanged ) .     per degree of freedom from the event type analysis",
    "is shown as a function of @xmath120 .",
    "bottom panel : the contribution to @xmath115 per degree of freedom , given in equation ( [ eq : etaseq ] ) , is shown as a function of the probability @xmath106 . ]    turning then to the contribution from @xmath121 we first calculate the expectation value of the ith term @xmath122 that is @xmath123 which can be rewritten as @xmath124 that has to be evaluated numerically .",
    "the result is displayed in figure [ fig : etaseq ] .",
    "as can be seen from the expression the contribution goes to zero as @xmath106 approaches one ; this is the limit where all events are of type 1 and the extra analysis therefore does not add any information . in the opposite limit where @xmath106 goes to zero the contribution seems numerically to approach @xmath54 .",
    "note that since @xmath125 the two first terms in equation ( [ eq : etaseq ] ) gives the same overall contribution as the extra contribution to @xmath104 .",
    "mller , nucl .",
    "* 112 * ( 1973 ) 47 .",
    "mller , nucl .",
    "instr . meth .",
    "a * 301 * ( 1991 ) 543 .",
    "v. horvat and j.c .",
    "hardy , nucl .",
    "instr . meth .",
    "a * 713 * ( 2013 ) 19 . c. mazzocchi , z. janas , p. baczyk , h.o.u .",
    "fynbo , u. kster , acta phys.pol .",
    "b43 ( 2012 ) 279 .",
    "cox and p.a.w .",
    "lewis , the statistical analysis of series of events ( methuen , 1966 ) f. james , statistical methods in experimental physics , 2nd edition ( world scientific , 2006 ) .",
    "s. pomm , b. denecke and j.p .",
    "alzetta , nucl .",
    "instr . meth .",
    "a * 426 * ( 1999 ) 564 .",
    "s. pomm , nucl .",
    "instr . meth .",
    "a * 437 * ( 1999 ) 481 .",
    "u.c . bergmann and k. riisager ,",
    "phys . a * 701 * ( 2002 ) 213c .",
    "b et al . , applied radiation and isotopes * 70 * ( 2012 ) 1894 .",
    "h. junde , h. su and y. dong , nuclear data sheets * 112 * ( 2011 ) 1513 .",
    "m. pftzner , m. karny , l.v .",
    "grigorenko and k. riisager , rev .",
    "* 84 * ( 2012 ) 567 .",
    "barlow , statistics , a guide to the use of statistical methods in the physical sciences  ( wiley , 1989 ) .",
    "nelder and r.w.m .",
    "wedderburn , j. r. statist .",
    "a * 135 * ( 1972 ) 370 .",
    "p. mccullagh and j.a .",
    "nelder , generalized linear models ( chapman and hall , 1989 ) ."
  ],
  "abstract_text": [
    "<S> several extensions of the halflife analysis method recently suggested by horvat and hardy are put forward . goodness - of - fit testing is included , and the method is extended to cases where more information is available for each decay event which allows applications also for e.g.  @xmath0 decay data . </S>",
    "<S> the results are tested with monte carlo simulations and are applied to the decays of @xmath1cu and @xmath2mn .    </S>",
    "<S> time interval , dead time , statistical analysis , maxmimum likelihood </S>"
  ]
}