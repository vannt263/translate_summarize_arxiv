{
  "article_text": [
    "this paper applies the _ optimum information principle _",
    "recently axiomatized by @xcite by refining jaynes s axioms of plausible reasoning ( * ? ? ? * chapters 1 and 2 ) to nonparametric density estimation .",
    "the optimum information principle , which is more fundamental than bayesian inference @xcite , fisher s maximum likelihood principle @xcite , jaynes s maximum entropy principle @xcite , and kullback s principle of minimum discrimination information @xcite , prescribes to minimize the information gain ( measured by the kullback - leibler information @xcite ) of updating from a prior to posterior subject to all relevant information . in particular , for nonparametric density estimation , it means to maximize the shannon entropy @xcite of a density subject to sample moment constraints . such an information - theoretic approach to density estimation has been known ( see @xcite and the references therein ) but has not yet gained popularity in econometrics possibly due to the lack of a theoretical foundation as well as a simple algorithm of computation .",
    "this paper provides both . as a by - product",
    "i obtain a measure of goodness of fit of parametric models and a criterion for model selection , which is closely related to bic @xcite but distinct from other conventional methods such as aic @xcite and bic in that it is an _",
    "absolute _ criterion , not relative .",
    "aic and bic can only pick the best model among the competing ones , but it may be the case that all models are poor .",
    "our measure tells the poor fit if all models are indeed poor .",
    "suppose that @xmath0 are i.i.d .",
    "random variables taking values in @xmath1 , with an unknown density @xmath2 .",
    "let @xmath3 be the realization of @xmath0 .",
    "our task is to obtain a reasonable estimate @xmath4 of @xmath5 from the data .",
    "since the data @xmath6 is a finite set , it is compact and discrete . therefore there is no reason to believe that the true distribution @xmath5 has an unbounded support or that @xmath5 is discontinuous .",
    "hence throughout the paper let us assume that @xmath5 is continuous and compactly supported on @xmath7 , where @xmath8 denotes the closed ball with radius @xmath9 with center at the origin .",
    "[ [ sample - moments ] ] sample moments + + + + + + + + + + + + + +    sample moments of @xmath5 are computed by @xmath10 , @xmath11 , etc . in general",
    "let us introduce the multi - index @xcite of nonnegative integers @xmath12 and let the @xmath13-th sample moment denoted by @xmath14 we set @xmath15 , @xmath16 , etc .",
    "even more generally , if @xmath17 is a lebesgue measurable function , the sample moments of @xmath18 can be defined by @xmath19 .",
    "the function @xmath18 represents the moments that the econometrician considers relevant for inference . if there is no particular reason to choose otherwise , it is natural to set @xmath20 , where @xmath21 is typically an even integer from 2 to 10 .",
    "[ [ optimum - information - estimator - defined ] ] optimum information estimator defined + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the _ optimum information estimator _ ( oie ) of @xmath5 , denoted by @xmath4 , is defined by @xmath22 where @xmath23 denotes the lebesgue measure and @xmath24 is the kullback - leibler information of the density @xmath25 with respect to the uniform prior on @xmath26 . and",
    "@xmath27 if @xmath28 . ]",
    "hereafter all integrations are carried out on the compact set @xmath26 and therefore we drop the subscript @xmath26 from the integral sign .",
    "minimizing the information as in is optimal from an information theoretic as well as a bayesian point of view .",
    "see @xcite for a theoretical justification of this definition .",
    "[ [ computing - the - optimum - information - estimator ] ] computing the optimum information estimator + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the minimization problem is a special case of an entropy - like minimization problem , which can be solved by fenchel duality . since @xmath29 ,",
    "is equivalent to @xmath30{\\mathrm{d}}x\\notag\\\\ & \\text{subject to}~\\int t(x ) g(x){\\mathrm{d}}x=\\hat{t},~\\int g(x){\\mathrm{d}}x=1.\\tag{p}\\label{eq : primal}\\end{aligned}\\ ] ] by corollary 2.6 and example 5.6(ii ) of @xcite , the dual problem of is given by @xmath31.\\tag{d}\\label{eq : dual}\\ ] ] i assume that a regularity condition of the fenchel duality theorem holds and therefore the dual problem has a solution and the primal and dual value coincide .",
    "one such regularity condition is that @xmath32 belongs to the interior of @xmath33 , which is very weak .",
    "since the objective function in contains an integral over the compact set @xmath26 , it is always finite and @xmath34 in @xmath35 ( * ? ? ?",
    "* proposition b.5 ) .",
    "since @xmath35 are unconstrained , the maximum is obtained by differentiating and setting equal to zero . partially differentiating with respect to @xmath36 , we obtain @xmath37 substituting this into , and using the fenchel duality theorem ( * ? ? ?",
    "* corollary 2.6 ) , after some algebra we obtain :    [ thm : dual ] @xmath38.\\label{eq:2.2}\\end{aligned}\\ ] ]    the duality theorem [ thm : dual ] was first exploited by @xcite , although its full understanding had to wait for the development of modern convex analysis half a century later .",
    "the reduced dual problem has a unique solution @xmath39 if @xmath40 ( , @xmath41 is not contained in a lower dimensional affine space ) because in that case the function @xmath42 is strictly convex ( * ? ? ?",
    "* proposition b.4 ) . in most applications",
    "the maximization problem has no closed - form solutions .",
    "however , since the objective function is differentiable and concave ( * ? ? ?",
    "* proposition b.5 ) , a numerical solution @xmath39 can be easily obtained by the newton - raphson algorithm ( * ? ? ?",
    "* chapter 10 ) .",
    "differentiating the objective function in with respect to @xmath43 and setting equal to zero , @xmath39 satisfies @xmath44=\\int t(x)\\hat{f}(x){\\mathrm{d}}x$ ] , where @xmath45 the function @xmath4 is the optimum information estimator . to verify this , substitute @xmath4 defined by for @xmath25 in and",
    "we can see that the equality is satisfied .    the functional form in shows that the optimum information estimator @xmath4 belongs to an exponential family , and one might guess that the lagrange multiplier @xmath39 is the maximum likelihood estimator of that family .",
    "this conjecture is indeed true .",
    "[ prop : ml ] @xmath39 is a maximum likelihood estimator for the exponential family @xmath46 , where @xmath47 .",
    "the log likelihood of the model @xmath48 is @xmath49\\\\ & = n\\left[\\lambda'\\hat{t}-\\log\\left(\\int { \\mathrm{e}}^{\\lambda't(x)}{\\mathrm{d}}x\\right)\\right],\\end{aligned}\\ ] ] which is precisely the expression in multiplied by the sample size @xmath50 .",
    "we should not misinterpret this result such that the optimum information principle is a special case of maximum likelihood , however , for two reasons .",
    "first , i showed ( * ? ? ? * theorem 5 ) that maximum likelihood is ( approximately ) implied by the optimum information principle : we should therefore interpret the above result such that a particular optimum information estimator may coincide to the density corresponding to a particular maximum likelihood estimator .",
    "second , and more importantly , maximum likelihood is valid only if the model contains the truth ( , @xmath51 for some @xmath43 ) , but a model is never true , as @xcite puts  since all models are wrong the scientist can not obtain a ",
    "correct `` one by excessive elaboration '' .",
    "hence the maximum likelihood here is actually the quasi - maximum likelihood @xcite .",
    "on the other hand , from a bayesian point of view the optimum information principle makes no reference to the truth .",
    "let us summarize the above results in a theorem and an algorithm to compute the optimum information estimator :    [ thm : oie ] let @xmath52 , i.i.d",
    ".  with @xmath5 compactly supported on @xmath53 , @xmath6 their realizations , @xmath17 be lebesgue measurable , + @xmath19 , @xmath54 , and @xmath40",
    ". then    1 .",
    "there exists a unique optimum information estimator @xmath4 defined by , 2 .",
    "@xmath55 , where @xmath39 is the maximum likelihood estimator of the exponential family @xmath47 , 3 .",
    "@xmath39 can be computed by the newton - raphson algorithm .    1 .   [ item : step.1 ] choose the relevant support @xmath26 and moments @xmath56 to exploit .",
    "choose @xmath57 with @xmath9 and @xmath20 , where @xmath21 is typically 10 , unless there is a strong reason to do otherwise .",
    "[ item : step.2 ] for each @xmath58 , compute the optimal information estimator by the newton - raphson algorithm ( theorem [ thm : oie ] ) .",
    "[ item : step.3 ] since the optimum information estimator corresponds to the maximum likelihood distribution of an exponential family ( proposition [ prop : ml ] ) , it is natural to use bic @xcite to select the best estimate among @xmath58 .",
    "this is the final optimum information estimator .",
    "it is acceptable to simplify step [ item : step.2 ] by estimating only one density ( corresponding to @xmath21 ) and omitting step [ item : step.3 ] .",
    "( more discussion on bic is given in section [ sec : model ] . )    [ [ estimating - conditional - densities ] ] estimating conditional densities + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    economists are usually interested in the conditional density of a variable @xmath59 ( , income ) conditional on some other variables @xmath60 ( , sex , age , education , experience , etc . ) or the conditional expectation @xmath61 $ ] rather than the unconditional distribution .",
    "estimation of these quantities are straightforward . for instance , the conditional density of @xmath59 given @xmath60 is estimated by @xmath62 , and the conditional expectation is estimated by @xmath63:=\\int y \\hat{f}(y|x){\\mathrm{d}}y$ ] .    [",
    "[ asymptotic - properties - of - the - optimum - information - estimator ] ] asymptotic properties of the optimum information estimator + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    since the quasi maximum likelihood estimator is a special case of an @xmath64-estimator ( * ? ? ?",
    "* chapter 5 ) , as the sample size @xmath50 gets large , @xmath39 converges in probability to the @xmath43 that solves the population counterpart of , and so does the kullback - leibler information : @xmath65 the quantity @xmath66 is asymptotically distributed as noncentral @xmath67 with @xmath68 degrees of freedom and noncentrality parameter @xmath69 @xcite .",
    "now we show that the optimum information estimator @xmath4 asymptotically approximates the true distribution @xmath5 arbitrarily well , but we need a lemma first . to avoid unnecessary complication",
    "i assume that the true density @xmath5 is positive everywhere on its support .",
    "[ lem : approx ] let @xmath5 be a positive , continuous density on the compact set @xmath70 .",
    "then there exists @xmath71 such that the exponential family @xmath47 , where @xmath20 , contains a density @xmath72 that arbitrarily approximates @xmath5 uniformly over @xmath26 .",
    "since @xmath5 is positive and continuous on @xmath26 , it has a positive minimum and @xmath73 is continuous on @xmath26 . by the stone - weierstrass theorem @xcite",
    ", we can take @xmath74 and @xmath75 such that @xmath76 is arbitrarily small on @xmath26 .",
    "then we can make @xmath77 small . by normalizing the density ,",
    "we obtain @xmath78 for all @xmath79 for some @xmath72 in the exponential family .",
    "[ thm : approx ] let @xmath5 be a positive , continuous density on the compact set @xmath70 . for any @xmath80",
    ", there exists a number @xmath71 such that for any @xmath81 , the optimum information estimator @xmath4 corresponding to the moments @xmath20 satisfies @xmath82    let @xmath83 be exponential family in lemma [ lem : approx ] and @xmath4 be the optimum information estimator of @xmath5 using the moments @xmath84 . take @xmath85 that uniformly approximates @xmath5 .",
    "since @xmath86 , where @xmath87 minimizes the kullback - leibler information among the exponential family @xmath83 , we get @xmath88 .",
    "hence we only need to show that we can choose @xmath21 such that @xmath89 .",
    "since @xmath90 are positive on @xmath26 , we get @xmath91\\\\ & = \\int\\left[\\frac{1}{2f}(f_0-f)^2+o((f_0-f)^2)\\right],\\end{aligned}\\ ] ] which can be made arbitrarily small by lemma [ lem : approx ] .",
    "this section is an application of the optimum information estimator to evaluate the goodness of fit of parametric models and select the best fitting model .",
    "i consider two cases separately , models for the unconditional density and the conditional density .",
    "[ [ unconditional - models ] ] unconditional models + + + + + + + + + + + + + + + + + + + +    suppose that we have @xmath64 competing models denoted by @xmath92 , where model @xmath93 has a parametric density @xmath94 with parameter @xmath95 .",
    "given data , how should we choose between these models , and how should we evaluate the goodness of fit ?    ideally , by the optimum information principle the goodness of fit should be evaluated by the minimized kullback - leibler information , @xmath96 where @xmath97 is a particular parametric model ( * ? ? ?",
    "* theorem 5 ) .",
    "however , this approach is infeasible because we do not know the true @xmath5 . as an approximation ,",
    "in his seminal paper @xcite approximated the quantity @xmath98 , which appears in the expansion of and @xmath99 denotes the optimum parameter value , and derived his celebrated akaike information criterion ( aic ) .",
    "we take a different approach . since the optimum information principle implies bayesian inference (",
    "* theorem 4 ) , which is an exact implication as opposed to the approximate implication for maximum likelihood ( * ? ? ?",
    "* theorem 5 ) , the bayesian approach to model selection by @xcite can be fully justified from an information - theoretic point of view .",
    "now the optimum information estimator @xmath4 in , being a maximum likelihood distribution of a particular exponential family ( theorem [ thm : oie ] ) , is also a particular parametric model .",
    "therefore the goodness of fit and model selection of the competing models @xmath92 reduces to the model selection of @xmath100 , where model 0 corresponds to the exponential family in proposition [ prop : ml ] that generates the optimum information estimator .",
    "model 0 serves as our benchmark model .",
    "an approximation of the _ _ evidence _ _ ( the logarithm of the bayesian likelihood ) of model @xmath93 is given by @xcite ) , is true for any such density . ]",
    "@xmath101 where @xmath102 is the log likelihood of model @xmath93 , @xmath103 the maximum likelihood estimator , @xmath104 the number of unknown parameters ( the dimension of @xmath105 ) , and @xmath50 the sample size . of bic and hence there is no reason to introduce a new name . ] in particular , for the benchmark model 0 , we have @xmath106 .",
    "the larger the evidence @xmath107 is , the better the model fits .    by laplace s principle of indifference @xcite ( which is a particular axiom of plausible reasoning in @xcite ) ,",
    "let us assign prior probability @xmath108 to models @xmath109 .",
    "then , by schwarz s fundamental proposition @xcite and bayes s rule ( which is implied by the optimum information principle ( * ? ? ?",
    "* theorem 4 ) ) , the posterior probability of model @xmath93 given data @xmath110 is approximately given by @xmath111 @xmath112 is a measure of model fit . if @xmath112 is large for some @xmath113 , then it is a good model . if @xmath114 is large , then all models are poor .",
    "the fundamental difference between our evidence @xmath107 and the posterior model probability @xmath112 and other information criteria such as aic and bic is that while aic and bic are _ relative _ measures of model fit , evidence @xmath107 and @xmath112 are _ absolute _ measures .",
    "by using aic and bic we can select the best model among the competing models , but it might be the case that all models are poor . on the other hand , our approach tells us each model s absolute fit .    [",
    "[ conditional - models ] ] conditional models + + + + + + + + + + + + + + + + + +    suppose now that the models are conditional , meaning that they only describe a conditional density @xmath115 , which is more important in economics .",
    "the optimum information principle gives us a measure of goodness of fit and model selection in this case , too .",
    "the evidence @xmath116 in is changed to @xmath117 where @xmath118 denotes the maximized conditional log likelihood of model @xmath93 and @xmath119 denotes the log likelihood of data @xmath120 . since the true density of @xmath0 is unknown , @xmath119 is a nuisance parameter .",
    "but since it is common across all models , if we substitute @xmath116 in into the fundamental formula , @xmath121 in the numerator and denominator cancel out ( ! ) .",
    "therefore the posterior model probability @xmath112 can just be computed by using the maximized conditional log likelihood .",
    "let us summarize this in a theorem :    [ thm : model ] let @xmath122 be @xmath64 competing models with parametric conditional density @xmath123 , where model 0 is the benchmark model corresponding to the optimum information estimator @xmath62 .",
    "let @xmath103 be the maximum likelihood estimator of model @xmath93 and define the conditional evidence of model @xmath124 by @xmath125 where @xmath126 and @xmath50 is the sample size , and @xmath127 where @xmath128 denotes the maximized log likelihood of the exponential family corresponding to the optimum information estimator for the density @xmath129 ( @xmath130 or @xmath131 ) , and @xmath132 is its parameter dimension .",
    "then , the posterior probability of model @xmath93 is @xmath133    our information - theoretic approach to the goodness of fit of conditional distribution is much simpler than the frequentist approach such as @xcite , @xcite , and @xcite since it requires no bootstrapping , no kernel density estimation , or no complicated integration .",
    "all we need is maximum likelihood estimation .",
    "another advantage is that the frequentist approach can only test a null hypothesis , thereby accepting or rejecting a model , but our approach can evaluate an arbitrary number of models simultaneously .",
    "a related method to our approach of model comparison is the likelihood ratio test ( see @xcite for nested models and @xcite for non - nested models ) , but it only applies to _ two _ models and it provides no information for the goodness of fit .",
    "the information - theoretic approach is applicable to any number of non - nested models and gives an absolute measure of goodness of fit .",
    "[ [ aic - or - bic ] ] aic or bic ?",
    "+ + + + + + + + + + +    i used bic @xcite to define the evidence of a model in .",
    "however , there are other information criteria such as aic , aicc , bic , cic , dic , eic , qic , tic , etc .",
    "( see @xcite and @xcite ) , among which aic ( aicc ) and bic are by far the most applied .",
    "anderson @xcite dismisses bic as having  nothing linking it to information theory \" , which is incorrect , but anderson s book was written before my discovery @xcite .",
    "i believe that bic is the most fundamental concept because bayesian inference is an exact implication of the optimum information principle and @xcite uses only one approximation to derive bic ( approximating log likelihood ) , whereas the approach of @xcite is conceptually closer to the optimum information principle but it invokes _ two _ approximations ( approximating the kullback - leibler information and log likelihood ) . but let us not be dogmatic : it is equally acceptable to use aic or aicc @xcite , which are also derived by information theory .",
    "to the best of my knowledge , the only applications of the maximum entropy estimation in economics and finance are @xcite and the references therein . the estimated maximum entropy density superimposed on the histogram of 1999 u.s .",
    "family income in figure 1 of @xcite shows a good fit , as predicted by the theory ( theorem [ thm : approx ] ) .",
    "maximum entropy estimation ( information - theoretic estimation ) is much more popular outside economics , in particular physics .",
    "jaynes @xcite mentions that the bayesian analysis ( synonymous to information - theoretic analysis ) of nuclear magnetic resonance ( nmr ) data obtained by his student @xcite showed an orders of magnitude ( , at least 10 fold ) improvement of resolution over fourier transforms method which was conventional at the time , and because of this surprising improvement bretthorst s result was not believed for a long time .",
    "the value of the information - theoretic nonparametric estimation ( and model selection ) method proposed in this paper compared to conventional methods such as kernel density estimation should ultimately be judged by their relative performances in analyzing real data . however , there are a few reasons to believe that the optimum information estimator is superior :    1 .",
    "the optimum information principle fully exploits the available information as opposed to other _",
    "ad hoc _ methods .",
    "for instance , kernel density estimation is essentially a local linear regression and hence uses only _ local _ information .",
    "2 .   kernel density estimation has a lot of arbitrariness with regard to the choice of the kernel and the bandwidth , whereas the only arbitrariness in the information - theoretic density estimation is the number of moments to include as constraints .",
    "even this arbitrariness can be removed by selecting the optimal number of constraints by bic .",
    "since the optimum information estimation reduces to the maximum likelihood estimation of an exponential family ( in the present case ) , it is fully parametric , computationally straightforward , and free from the curse of dimensionality .",
    "statistics and econometrics are sciences of extracting information from data .",
    "hence an inference method is valuable if and only if it is useful in analyzing real data , and therefore an inference method requires no interpretation , and no justification except practical usefulness : we should refrain from being too dogmatic as exemplified by the heated frequentist / bayesian debate in the past .",
    "comparisons of the performance of the optimum information estimator and other methods using real data are welcome , although it is beyond the scope of the present paper .",
    "huber , p.  j. ( 1967 ) : `` the behavior of maximum likelihood estimates under nonstandard conditions , '' in _ proceedings of the fifth berkeley symposium on mathematical statistics and probability _ , ed . by j.  neyman , vol .  1 , 221233"
  ],
  "abstract_text": [
    "<S> this paper applies the recently axiomatized optimum information principle ( minimize the kullback - leibler information subject to all relevant information ) to nonparametric density estimation , which provides a theoretical foundation as well as a computational algorithm for maximum entropy density estimation . </S>",
    "<S> the estimator , called _ optimum information estimator _ , approximates the true density arbitrarily well . as a by - product </S>",
    "<S> i obtain a measure of goodness of fit of parametric models ( both conditional and unconditional ) and an _ absolute _ criterion for model selection , as opposed to other conventional methods such as aic and bic which are _ relative _ measures . </S>"
  ]
}