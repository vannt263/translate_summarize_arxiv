{
  "article_text": [
    "with gpu hardware and the corresponding software environments becoming mature , compute clusters with gpu - accelerated nodes establish as a new , powerful platform for high - performance computing ( hpc ) .",
    "mainly motivated by the expected boost for application performance ( i.e.  reducing `` time to solution '' ) and also by energy - efficiency considerations ( i.e.  reducing `` energy to solution '' ) , major research organizations and providers of hpc resources have already deployed an appreciable amount of gpu - accelerated resources worldwide @xcite .",
    "moreover , gpu - like architectures are expected to play a major role in the upcoming exascale era @xcite .",
    "it is well known , however , in the community , that the new hardware architecture together with the apparently disruptive programming models pose substantial challenges to scientific application developers ( e.g.  @xcite ) .",
    "while selected algorithms and applications have in fact been demonstrated to keep up with the shiny performance promises of gpus , in some cases even at the very large scale ( e.g.  @xcite ) , it remains to be seen whether a broader class of scientific applications can take advantage of gpu - accelerated systems with reasonable programming effort and in a sustainable way .",
    "often inappropriately termed `` legacy applications '' in this context , leading scientific hpc codes are typically being actively developed , comprise many tens or hundreds of thousands of lines of code achieved by a team effort of many dozens of person years , they provide state - of - the - art functionality , as well as high optimization , parallel scalability and portability .",
    "the codes gene and vertex , which have been developed in the max - planck - society with continuous support from its high - performance computing centre ( rzg ) may serve as prototypical examples in this respect .",
    "but as a matter of fact , such highly tuned codes are often reaching the limits of ( strong ) scalability .",
    "for example , due to increasing inter - node communication times , or , as in the case of vertex , due to the lack of conventionally exploitable parallelism in the code structure , the time to solution for a given setup can no more be efficiently reduced by utilizing more cpu resources .",
    "thus , a significantly increased node performance due to accelerators appears as a promising route towards further boosting application performances at scale .",
    "although both , gene and vertex are written in fortran ( with mpi and hybrid mpi / openmp parallelization , respectively ) we decided to adopt the c - based cuda programming model because it is the performance reference for nvidia gpus . while we found the commercial cuda - fortran language to deliver competitive performance on the gpu",
    ", the employed pgi compiler falls behind the intel compiler ( which is our reference for the cpu ) on the remaining cpu parts which marginalizes the overall application speedups of the heterogeneous code . for the same reason we did not yet make productive use of the openacc programming model @xcite .",
    "the performance baseline for all comparisons is defined by a highly optimized , parallel cpu implementation of the respective algorithms . rather than quoting single - core speedups ( which , in our opinion is hardly meaningful in most cases )",
    "our comparisons are always based on the same number of gpu cards and _ multicore _ cpus ( `` sockets '' ) .",
    "specifically , we compare the run time obtained on a certain number of nodes , each equipped with two intel xeon e5 - 2670 8-core cpus and two nvidia k20x gpus , with the run time measured with the original , parallel cpu code on the same number of nodes ( without gpus ) .",
    "gene @xcite is a massively parallel code for the simulation of plasma turbulence in fusion devices .",
    "the code solves the time - dependent , five - dimensional vlasov - maxwell system of equations on a fixed phase - space grid . depending on the physical problem",
    ", typical gene simulations take between a few days and many weeks , using thousands of cores on x86_64-based hpc systems .",
    "gene is open - source @xcite and has a world - wide user base .",
    "the gene algorithm employs coordinates aligned to the magnetic field lines in a fusion device like a tokamak . in this paper",
    "we use the so called _ x - global _ version , where all physical quantities are handled in a spectral representation with respect to the @xmath0 coordinate , which is the second of the three space dimensions @xmath1 ( radial ) , @xmath0 ( binormal ) and @xmath2 ( along the field line ) .",
    "the remaining phase - space coordinates are ( in this order ) the velocity along the field line @xmath3 and the magnetic moment @xmath4 ( see @xcite for details ) . although gene is able to handle any number of ion species and the electrons in the framework of gyrokinetics , we use for this paper only a single ion species , neutralized by electrons . for all performance comparisons a problem setup with a number of @xmath5 grid points",
    "is used .",
    "the starting point of this work was a profiling of the gene code ( svn revision 3440 ) , with the times given in table  [ tab : profiling_snb ] .",
    "[ cols= \" < , < \" , ]     table  [ vertex : tab1]a shows that the run time of vertex on the cpu is dominated by solving the radiative transfer equations ( item ` transport ` ) , and in particular for computing neutrino absorption and emission rates ( item ` rates ` ) .",
    "[ vertex : fig1]a identifies the positions of the individual routines in the execution flow .",
    "about 50 percent of the run time is spent in the computation of one particular interaction rate ( named ` rate kernel ` , ` c2 ` ) .",
    "the different interaction rates are often termed `` local physics '' , which expresses the fact that the computations are to a high degree independent of each other and provide a data parallelism on the grid level .",
    "different interaction processes ( ` rates ` ) can be computed independently of each other , which implies additional , coarse - grained parallelism on the function level ( see blowup in fig .",
    "[ vertex : fig1]a ) .      in the following the algorithm for offloading the ` rate kernel ` to the gpu is outlined . due to its dominance in the code ,",
    "high data parallelism and arithmetic intensity the suitability for the gpu shall become immediately apparent .    as input for the computations",
    "a few one - dimensional arrays are needed , which represent the local thermodynamic conditions for which the interaction kernel is evaluated .",
    "all operations are performed on a five - dimensional grid representing discretized phase space .",
    "the size of this grid varies with the resolution , in a typical setup the total number of grid points is about @xmath6 .",
    "for the major part of the kernel , computations on each grid zone can be done independently of the others , which leads to a high degree of data parallelism ( up to @xmath7 threads ) .",
    "only after all grid zones are processed , a reduction ( corresponding to a phase space integral ) to a three dimensional grid is performed .",
    "this can be still done in parallel , but with much less parallelism ( @xmath8 threads ) .",
    "all computations are done twice for subsets of different input data , accounting for two possible reaction channels .",
    "the actual implementation of the part ` c2 ` is straightforward : the data is copied asynchronously to the gpu and the five - fold nested loops of the cpu version are separated in kernel calls with about 100000 threads .",
    "the kernels are scheduled in streams , in order to allow the cuda run time to overlap kernel executions corresponding to the twofold computation of the processes .",
    "the problem is compute bound , as data transfer is negligible ( 0.9 ms ) compared to gpu computations ( 40 ms ) and at least 140 double - precision floating - point operations are executed per transferred byte .    for",
    "good performance results it turned out to be crucial to use shared memory for the input data and to use as much registers as possible on the device . after tuning our cuda code with the help of the nvidia profiler",
    ", we achieve an occupancy of 93% of the theoretical upper limit for the most important kernels .",
    "however , we still encounter about 10% of branch - divergence overhead and 25% of global memory replay overhead . work is still ongoing to improve on the latter performance metrics .    as mentioned above , the different sub - steps ` c1 ` to ` cn ` ( see fig .  [",
    "vertex : fig1]b ) are independent of each other and can be computed in any order within one openmp thread , or ray. in the original code , however , the order across different openmp threads is always the same , e.g. when a thread computes sub - step ` c1 ` , also the other threads work on the same sub - step .",
    "an overlap of computations on the cores and the gpu was thus achieved by : a ) individually shuffling the computations of the sub - steps ` c1 ` to ` cn ` on each ray , and b ) ensuring that the sub - step ` c2 ` from each ray to the gpu is offloaded in a queue ( see fig .",
    "[ vertex : fig1]b ) . in an ideal situation",
    "where all steps ` c1 ` to ` cn ` take the same amount of execution time , work on the cpus and the gpu would be perfectly overlapped . in reality , a balancing of about 80% could be reached .",
    "the rate kernel ` c2 ` requires 2.16  s on one cpu thread ( cf .",
    "tab  [ vertex : tab1 ] ) and scales almost perfectly with openmp .",
    "the same kernel can be computed on the gpu in 0.04  s. thus , with one gpu , speedups of 7 or 54 are achieved when comparing with one cpu socket or a single core , respectively .",
    "this demonstrates that a significant speedup was achieved with respect to a sandy bridge cpu . as the coarse grained openmp parallelization of vertex ( which is crucial for achieving its excellent weak scalability )",
    "does not allow to use the threaded rate kernel on the cpu , the acceleration factor of 54 applies for production applications which effectively eliminates the rate kernel from the computing time budget and in practice accounts for a twofold acceleration ( corresponding to the original 50% share of the rate kernel , cf .",
    "tab  [ vertex : tab1 ] ) of the entire application .",
    "with the specific cases of gene and vertex we have shown that complex hpc applications can successfully be ported to heterogeneous cpu - gpu clusters .",
    "besides writing fast gpu code , exploiting and balancing both the gpu _ and _ the cpu resources of the heterogeneous compute nodes turned out to be an essential prerequisite for achieving good overall `` speedups '' , which we define as the ratio of the run time obtained on a number of gpu - accelerated sockets and the run time measured with parallel code on the same number of cpu sockets .",
    "in the case of vertex we have demonstrated twofold speedups which hold for production applications on gpu - clusters with many hundreds of nodes . in particular , the excellent weak scalability of vertex @xcite is not affected by the additional acceleration due to gpus . threefold speedups appear in reach but would require at least additional porting of a linear solver for a block - tridiagonal system .",
    "limitations in the software environment ( lack of device - callable lapack functionality ) have so far impeded a successful port of this part of the algorithm .",
    "importantly , due to the specific code structure of vertex , such speedups would not have been possible with comparable programming effort by simply using more cpu cores .",
    "the performance of gene is currently limited by the data transfer between the host cpu and the gpu as we have shown by an elaborate performance - modeling analysis .",
    "after this bottleneck will have relaxed by upcoming hardware improvements ( pcie 3 ) further optimization efforts on the gpu code will increase the overall speedups on a heterogeneous cluster .",
    "the question whether the effort of several person - months , which we have invested for each code , and which we consider typical for such projects , is well justified can not be answered straightforwardly . for complex , and `` living '' scientific hpc codes , for which gene and vertex can serve as prototypical examples , achieving up to threefold speedups in overall application performance appears very competitive @xcite . also from the point of view of hardware investment",
    "( buying gpus instead of cpus ) and operational costs ( `` energy to solution '' ) the migration of applications from pure cpu machines to gpu - accelerated clusters can be considered cost - effective if speedups of at least about two are achieved . on the other hand , while very valuable for increasing simulation throughput , twofold or threefold application speedups usually do not enable qualitatively new science objectives .",
    "for this reason we sometimes observe reluctance in the scientific community to invest significant human resources for achieving gpu - performance improvements in this range .",
    "this is further exacerbated by legitimate concerns about sustainability , maintainability and portability of gpu - kernel code .",
    "these are no serious issues for gene and vertex , where the parts we have ported to the gpu are not under heavy algorithmic development and were also carefully encapsulated by us . in general , however , the _ need _ for kernel programming , which is considered as a pain by many , currently appears as the largest hurdle for a broader adoption of gpu programming in the scientific hpc community . moreover",
    ", it may turn out necessary to port significant parts of the application code to the gpu , e.g. in cases like gene where the data transfers become a limiting factor , or even to completely reimplement the application .",
    "these concerns could be mitigated by the establishment of a high - level , directive based programming model , based e.g.  on the openacc standard @xcite or a future revision of openmp @xcite , together with appropriate compiler support .",
    "also intel s xeon phi many - core coprocessor with its less disruptive programming model appears very prospective in this respect . despite serious efforts ,",
    "however , we were not yet successful with gene or vertex to achieve performances on this platform which are competitive with the gpu .",
    "we attribute this mostly to a comparably lower maturity of the xeon phi software stack and we expect improvements with upcoming versions of the compiler and the openmp run time",
    ".    most importantly , today s gpus ( and many - core coprocessors ) might provide a first glimpse on the architecture and the related programming challenges of future hpc architectures of the exascale era @xcite .",
    "applications need to be prepared _ in time _ for the massive simt and simd parallelism which is expected to become prevalent in such systems . even on contemporary multicore cpus with comparably moderate thread - counts and simd width , the experience we have gained with porting gene and vertex has already led to appreciable performance improvements of the cpu codes .",
    "we thank f.  jenko and h .- th .",
    "janka for encouraging the development of gpu versions for gene and vertex , respectively .",
    "nvidia corp .  and intel corp .",
    "are acknowledged for providing hardware samples and technical consulting .",
    "buras , r. , janka , h .- th . , rampp , m. , kifonidis , k. _ two - dimensional hydrodynamic core - collapse supernova simulations with spectral neutrino transport . i. numerical method and results for a 15 m@xmath9 star_. astronomy & astrophysics * 447",
    "* , 1049 ( 2006 ) .",
    "cardall , c. , endeve , e. , budiardja , r. d. , marronetti , p. , mezzacappa , a. _ towards the core - collapse supernova explosion mechanism_. advances in computational astrophysics : methods , tools , and outcome .",
    "asp conference proceedings , * 453 * , 81 ( 2012 ) .",
    "shimokawabe , t. , aoki , t. et al .",
    "_ peta - scale phase - field simulation for dendritic solidification on the tsubame 2.0 supercomputer_. proceedings of 2011 international conference for high performance computing , networking , storage and analysis , sc 11 , acm new york , ny , us ( 2011 ) 3:13:11"
  ],
  "abstract_text": [
    "<S> we have developed gpu versions for two major high - performance - computing ( hpc ) applications originating from two different scientific domains . </S>",
    "<S> gene @xcite is a plasma microturbulence code which is employed for simulations of nuclear fusion plasmas . </S>",
    "<S> vertex @xcite is a neutrino - radiation hydrodynamics code for `` first principles''-simulations of core - collapse supernova explosions @xcite . </S>",
    "<S> the codes are considered state of the art in their respective scientific domains , both concerning their scientific scope and functionality as well as the achievable compute performance , in particular parallel scalability on all relevant hpc platforms . </S>",
    "<S> gene and vertex were ported by us to hpc cluster architectures with two nvidia _ kepler _ gpus mounted in each node in addition to two intel xeon cpus of the _ sandy bridge _ family . </S>",
    "<S> on such platforms we achieve up to twofold gains in the overall application performance in the sense of a reduction of the time to solution for a given setup with respect to a pure cpu cluster . </S>",
    "<S> the paper describes our basic porting strategies and benchmarking methodology , and details the main algorithmic and technical challenges we faced on the new , heterogeneous architecture .    ,    and    gpu , hpc application , gene , vertex </S>"
  ]
}