{
  "article_text": [
    "futures  @xcite are an attractive way to structure many parallel programs because they are easy to reason about ( especially if the futures have no side - effects ) and they lend themselves well to sophisticated dynamic scheduling algorithms , such as work - stealing  @xcite and its variations , that ensure high processor utilization . at the same time , however , modern multicore architectures employ complex multi - level memory hierarchies , and technology trends are increasing the relative performance differences among the various levels of memory . as a result , processor utilization can no longer be the sole figure of merit for schedulers .",
    "instead , the _ cache locality _ of the parallel execution will become increasingly critical to overall performance . as a result ,",
    "cache locality will increasingly join processor utilization as a criterion for evaluating dynamic scheduling algorithms .",
    "several researchers  @xcite have shown , however , that introducing parallelism through the use of futures can sometimes substantially reduce cache locality . in the worst case ,",
    "if we add futures to a sequential program , a parallel execution managed by a work - stealing scheduler can incur @xmath0 deviations , which , as we show , implies @xmath1 more cache misses than the sequential execution . here , @xmath2 is the number of cache lines , @xmath3 is the number of processors , @xmath4 is the number of touches , and @xmath5 is the computation s _ span _ ( or _ critical path _ ) .",
    "as technology trends cause the cost of cache misses to increase , this additional cost is troubling .",
    "this paper makes the following three contributions .",
    "first , we show that if futures are used in a simple , disciplined way , then the situation with respect to cache locality is much better : if each future is touched only once , either by the thread that created it , or by a later descendant of that thread , then parallel executions with work stealing can incur at most @xmath6 additional cache misses , a substantial improvement over the unstructured case .",
    "this result provides a simple way to identify computations for which introducing futures will not incur a high cost in cache locality , as well as providing guidelines for the design of future parallel computations .",
    "( informally , we think these guidelines are natural , and correspond to structures programmers are likely to use anyway . ) our second contribution is to observe that when the scheduler has a choice between running the thread that created a future , and the thread that implements the future , running the future thread first provides better cache locality .",
    "finally , we show that certain variations of structured computation also have good cache locality .    the paper is organized as follows .",
    "section  [ section : model ] describes the model for future - parallel computations . in section  [ section : work - stealing and cache locality ] , we describe parsimonious work - stealing schedulers , and briefly discuss their cache performance measures . in section",
    "[ section_structured ] , we define some restricted forms of structured future - parallel computations . among them , we highlight structured single - touch computations , which , we believe , are likely to arise naturally in many programs . in section  [ section_futurethreadfirst ] , we prove that work - stealing schedulers on structured single - touch computations incur only @xmath6 additional cache misses , if a processor always chooses the future to execute first when it creates that future .",
    "we also prove this bound is tight within a factor of @xmath2 . in section  [",
    "section : parent thread first at each fork ] , we show that if a processor chooses the current thread over the future thread when it creates that future , then the cache locality of a structured single - touch computation can be much worse . in section  [ section : other kinds of structured computations ] , we show that some other kinds of structured future - parallel computations also achieve relatively good cache locality . finally , we present conclusions in section  [ section : conclusions ] .",
    "in _ fork - join parallelism _",
    "@xcite , a sequential program is split into a directed acyclic graph of _ tasks _ linked by directed dependency edges .",
    "these tasks are executed in an order consistent with their dependencies , and tasks unrelated by dependencies can be executed in parallel .",
    "fork - join parallelism is well - suited to dynamic load - balancing techniques such as _ work stealing",
    "_  @xcite .",
    "a popular and effective way to extend fork - join parallelism is to allow threads to create _",
    "futures _  @xcite .",
    "a future is a data object that represents a _ promise _ to deliver the result of an asynchronous computation when it is ready .",
    "that result becomes available to a thread when the thread _ touches _ that future , blocking if necessary until the result is ready .",
    "futures are attractive because they provide greater flexibility than fork - join programs , and they can also be implemented effectively using dynamic load - balancing techniques such as work stealing .",
    "fork - join parallelism can be viewed as a special case of future - parallelism , where the ` spawn ` operation is an implicit future creation , and the ` sync ` operation is an implicit touch of the untouched futures created by a thread .",
    "future - parallelism is more flexible than fork - join parallelism , because the programmer has finer - grained control over touches ( joins ) .",
    "a thread creates a future by marking an expression ( usually a method call ) as a _",
    "future_. this statement spawns a new thread to evaluate that expression in parallel with the thread that created the future .",
    "when a thread needs access to the results of the computation , it applies a _ touch",
    "_ operation to the future .",
    "if the result is ready , it is returned by the touch , and otherwise the touching thread blocks until the result becomes ready . without loss of generality , we will consider fork - join parallelism to be a special case of future - parallelism , where forking a thread creates a future , and joining one thread to another is a touch operation .",
    "our notation and terminology follow earlier work  @xcite .",
    "a future - parallel computation is modeled as a _ directed acyclic graph _ ( dag ) .",
    "each node in the dag represents a task ( one or more instructions ) , and an edge from node @xmath7 to node @xmath8 represents the dependency constraint that @xmath7 must be executed before @xmath8 .",
    "we follow the convention that each node in the dag has in - degree and out - degree either 1 or 2 , except for a distinguished _ root node _ with in - degree 0 , where the computation starts , and a distinguished _ final node _ with out - degree 0 , where the computation ends .",
    "there are three types of edges :    * _ continuation edges _ ,",
    "which point from one node to the next in the same thread , * _ future edges _",
    "( sometimes called _ spawn _ edges ) , which point from node @xmath7 to the first node of another thread spawned at @xmath7 by a future creation , * _ touch edges _",
    "( sometimes called _",
    "edges ) , directed from a node @xmath7 in one thread @xmath4 to a node @xmath8 in another thread , indicating that @xmath8 touches the future computed by @xmath4 .",
    "a _ thread _ is a maximal chain of nodes connected by continuation edges .",
    "there is a distinguished _ main thread _ that begins at the root node and ends at the final node , and every other thread @xmath4 begins at a node with an incoming future edge from a node of the thread that spawns @xmath4 .",
    "the last node of @xmath4 has only one outgoing edge which is a touch edge directed to another thread , while other nodes of @xmath4 may or may not have incoming and outgoing touch edges .",
    "a _ critical path _ of a dag is a longest directed path in the dag , and the dag s _ computation span _ is the length of a critical path .",
    "as illustrated in figure  [ figure : terminology ] , if a thread @xmath9 spawns a new thread @xmath10 at node @xmath8 in @xmath9 ( i.e. , @xmath8 has two out - going edges , a continuation edge and a future edge to the first node of @xmath10 ) , then we call @xmath9 the _ parent thread _ of @xmath10 , @xmath10 the _ future thread _ ( of @xmath9 ) at @xmath8 , and @xmath8 the _ fork _ of @xmath10 .",
    "a thread @xmath11 is a _ descendant thread _ of @xmath9 if @xmath11 is a future thread of @xmath9 or , by induction , @xmath11 s parent thread is a descendant thread of @xmath9 .",
    "if there is a touch edge directed from node @xmath12 in thread @xmath9 to node @xmath13 in thread @xmath10 ( i.e. , @xmath10 touches a future computed by @xmath9 ) , and a continuation edge directed from node @xmath14 in @xmath10 to @xmath13 , then we call node @xmath13 a _ touch of",
    "_ @xmath9 by @xmath10 , @xmath12 the _ future parent _ of @xmath13 , @xmath14 the _ local parent _ of @xmath13 , and @xmath9 the future thread of @xmath13 .",
    "( note that the touch @xmath13 is actually a node in thread @xmath10 . )",
    "we call the fork of @xmath9 the _ corresponding fork _ of @xmath13 .",
    "note that only touch nodes have in - degree 2 . to distinguish between the two types of nodes with out - degree 2 , forks and future parents of touches , we follow the convention of previous work that the children of a fork both have in - degree 1 and can not be touches . in this way",
    ", a fork node has two children with in - degree 1 , while a touch s future parent has a ( touch ) child with in - degree 2 .",
    "we follow the convention that when a fork appears in a dag , the future thread is shown on the left , and the future parent on the right .",
    "( note that this does not mean the future thread is chosen to execute first at a fork . ) similarly , the future parent of a touch is shown on the left , and the local parent on the right .",
    "we use the following ( standard ) notation . given a computation dag , @xmath3 is the number of processors executing the computation , @xmath4 is the number of touches in the dag , @xmath15 , the _ computation span _ ( or _ critical path _ ) , is the length of the longest directed path , and @xmath2 is the number of cache lines in each processor .",
    "in the paper , we focus on parsimonious work stealing algorithms @xcite , which have been extensively studied  @xcite and used in systems such as cilk  @xcite . in a parsimonious work stealing algorithm , each processor",
    "is assigned a double - ended queue ( deque ) .",
    "after a processor executes a node with out - degree 1 , it continues to execute the next node if the next node is ready to execute .",
    "after the processor executes a fork , it pushes one child of the fork onto the bottom of its deque and executes the other .",
    "when the processor runs out of nodes to execute , it pops the first node from the bottom of its deque if the deque is not empty .",
    "if , however , its deque is empty , it steals a node from the top of the deque of an arbitrary processor .    in our model ,",
    "a cache is fully associative and consists of multiple _ cache lines _ , each of which holds the data in a _",
    "memory block_. each instruction can access only one memory block . in our analysis",
    "we focus only on the widely - used _ least - recently used _ ( lru ) cache replacement policy , but our results should apply to all _ simple _ cache replacement policies  @xcite .    the _ cache locality _ of an execution is measured by the number of cache misses it incurs , which depends on the structure of the computation . to measure the effect on cache locality of parallelism ,",
    "it is common to compare cache misses encountered in a sequential execution to the cache misses encountered in various parallel executions , focusing on the number of _ additional _ cache misses introduced by parallelism .",
    "scheduling choices at forks affect the cache locality of executions with work stealing . after executing a fork",
    ", a processor picks one of the two child nodes to execute and pushes the other into its deque . for a sequential execution , whether a choice results in a better cache performance is a characteristic of the computation itself . for a parallel execution of a computation satisfying certain properties , however",
    ", we will show that choosing future threads ( the left children ) at forks to execute first guarantees a relatively good upper bound on the number of additional cache misses , compared to a sequential execution that also chooses future threads first .",
    "in contrast , choosing the parent threads ( the right children ) to execute first can result in a large number of additional cache misses , compared to a sequential execution that also chooses parent threads first .",
    "consider a sequential execution where node @xmath12 is executed immediately before node @xmath13 .",
    "a _ deviation _",
    "@xcite , also called a drifted node @xcite , occurs in a parallel execution if a processor @xmath3 executes @xmath13 , but not immediately after @xmath12 .",
    "for example , @xmath16 might execute @xmath12 after @xmath13 , it might execute other nodes between @xmath12 and @xmath13 , or @xmath12 and @xmath13 might be executed by distinct processors .",
    "spoonhower et al .",
    "@xcite showed that a parallel execution of a future - parallel computation with work stealing can incur @xmath17 deviations .",
    "this implies a parallel execution of a future - parallel computation with work stealing can incur @xmath17 additional cache misses . with minor modifications in that computation ( see figure  [ figure : worstcasedaginspoonhower09 ] ) , a parallel execution can even incur @xmath18 additional cache misses .    .",
    "figure 5 in @xcite shows a dag , as a building block of a worst - case computation , that can incur @xmath19 deviations because of one touch . we can replace it with the dag in figure  [ figure : worstcasedaginspoonhower09 ] , which can incur @xmath20 additional cache misses due to one touch @xmath8 ( if the processor at a fork always chooses the parent thread to execute first ) , so that the worst - case computation in @xcite can incur @xmath21 additional cache misses because of @xmath4 such touches",
    "this dag is similar to the dag in figure  [ fig_parentfirst_lowerbound1](a ) in this paper .",
    "the proof of theorem  [ parentthreadfirst ] shows how a parallel execution of this dag incurs @xmath20 additional cache misses.,width=151 ]    our contribution in this paper is based on the observation that such poor cache locality occurs primarily when futures in the dag are touched by threads created before the future threads computing these futures where created .",
    "as illustrated in figure  [ fig_structured](a ) , a parallel execution of such a computation can arrive at a scenario where a thread touches a future before the future thread computing that future has been spawned .",
    "( as a practical matter , an implementation must ensure that such a touch does not return a reference to a memory location that has not yet been allocated . )",
    "we will show that such scenarios are avoided by _ structured _ future - parallel computations that follow certain simple restrictions .",
    "a dag is a _ structured future - parallel computation _",
    "if , ( 1 ) for the future thread @xmath4 of any fork @xmath8 , the local parents of the touches of @xmath4 are descendants of @xmath8 , and ( 2 ) at least one touch of @xmath4 is a descendant of the right child of @xmath8 .",
    "there are two reasons we require that at least one touch of @xmath4 is a descendant of the right child of @xmath8 .",
    "first , it is natural that a computation spawns a future thread to compute a future because the computation itself later needs that value . at the fork @xmath8 , the parent thread ( the right child of @xmath8 ) represents the  main body \" of the computation .",
    "hence , the future will usually be touched either by the parent thread , or by threads spawned directly or indirectly by the parent thread .",
    "second , a computation usually needs a kind of `` barrier '' synchronization to deal with resource release at the end of the computation .",
    "some node in the future thread @xmath4 , usually the last node , should have an outgoing edge pointing to the  main body \" of the computation to tell the main body that the future thread has finished . without such synchronization , @xmath4 and its descendants",
    "will be isolated from the main body of the computation , and we can imagine a dangerous scenario where the main body of the computation finishes and releases its resources while @xmath4 or its descendant threads are still running .    in our dag model ,",
    "such a synchronization point is by definition a touch node , though it may not be a real touch .",
    "we follow the convention that the thread that spawns a future thread releases it , so the synchronization point is a vertex in the parent thread or one of its descendants .",
    "another possibility is to place the synchronization point at the last node of the entire computation , which is the typically case in languages such as java , where the main thread of a program is in charge of releasing resources for the entire computation .",
    "these two styles are essentially equivalent , and should have almost the same bounds on cache overheads .",
    "we will briefly discuss this issue in section  [ section_superfinalnode ] .",
    "we consider how the following constraint affects cache locality .",
    "a structured _ single - touch _ computation is a structured computation where each future thread spawned at a fork @xmath8 is touched only once , and the touch node is a descendant of @xmath8 s right child .    by the definition of threads , the future parent of the only touch of a future thread is the last node of that future thread ( the last node can also be a parent of a join node , but we do nt distinguish between a touch node and a join node ) . we will show that work - stealing parallel executions of structured single - touch computations achieve significantly less cache overheads than unstructured computations .    in principle , a future could be touched multiple times by different threads , so structured single - touch computations are more restrictive structured computations in general . nevertheless , the single - touch constraint is one that is likely to be observed by many programs .",
    "for example , as noted , the cilk  @xcite language supports fork - join parallelism , a strict subset of the future - parallelism model considered here . if we interpret the cilk  @xcite language s `",
    "spawn ` statement as creating a future , and its ` sync ` statement as touching all untouched futures previously created by that thread , then cilk programs ( like all fork - join programs ) are structured single - touch computations .",
    "structured single - touch computations encompass fork - join computations , but are strictly more flexible .",
    "figure  [ figure : examplesofsingletouch ] presents two examples that illustrate the differences .",
    "if a thread creates multiple futures first and touches them later , fork - join parallelism requires they be touched ( evaluated ) in the reverse order .",
    "methoda in figure  [ figure : examplesofsingletouch](a ) shows the only order in which a thread can first create two futures and then touch them in a fork - join computation .",
    "this rules out , for instance , a program where a thread creates a sequence of futures , stores them in a priority queue , and evaluates them in some priority order .",
    "in contrast , our structured computations permit such futures to be evaluated by their creating thread or its descendants in any order .    also , unlike fork - join parallelism , our notion of structured computation permits a thread to pass a future to a subroutine or descendant thread which touches that future , as illustrated in figure  [ figure : examplesofsingletouch](b ) .",
    "our restrictions are : ( 1 ) only one thread can touch a future , and ( 2 ) the descendant thread that touches the future has to be created after the future .",
    "in fact , methodc can even pass the future to a descendant of its own . in a fork - join computation , however , only the thread creating the future can touch it , which is much more restrictive .",
    "we believe these restrictions are easy to follow and should be compatible with how many people program in practice .",
    "void methoda \\",
    "{ + = future x = some computation ; + future y = some computation ; + a = y.touch ( ) ; + b = x.touch ( ) ; + }     + void methodb \\ { + = future x = some computation ; + fork methodc(x ) ; + } + void methodc(future f)\\ { + = a = f.touch ( ) ; + }    belloch and reid - miller  @xcite observe that if a future can be touched multiple times , then complex and potentially inefficient operations and data structures are needed to correctly resume the suspended threads that are waiting for the touch .",
    "by contrast , the run - time support for futures can be significantly simplified if each future is touched at most once .",
    "the single - touch constraint can be relaxed as follows .",
    "a structured _ local - touch _ computation is one where each future thread spawned at a fork @xmath8 is touched only at nodes in the parent thread of @xmath4 , and these touches are descendants of the right child of @xmath8 .",
    "informally , the local touch constraint implies that a thread that needs the value of a future should create the future itself .",
    "note that in a structured computation with local touch constraint , a future thread is now allowed to evaluate multiple futures and these futures can be touched at different times .",
    "though allowing a future thread to compute multiple futures is not very common , blelloch and reid - miller  @xcite point out that it can be useful for some future - parallel computations like pipeline parallelism  @xcite .",
    "we will show in section  [ section_localtouch ] that work - stealing parallel executions of computations satisfying the local touch constraint also have relatively low cache overheads .",
    "note that structured computations with both single touch and local touch constraints are still a superset of fork - join computations .",
    "we now analyze cache performance of work stealing on parallel executions of structured single - touch computations . we will show that work stealing has relatively low cache overhead if the processor at a fork always chooses the future thread to execute first , and puts the parent future into its deque .",
    "for brevity , all the arguments and results in this section assume that every execution chooses the future thread at a fork to execute first .",
    "[ lemmafutureparentfirst ] in the sequential execution of a structured single - touch computation , any touch @xmath22 s future parent is executed before @xmath22 s local parent , and the right child of @xmath22 s corresponding fork @xmath8 immediately follows @xmath22 s future parent .    by induction . given a dag ,",
    "initially let @xmath23 be an empty set and @xmath24 the set of all touches .",
    "note that @xmath25 consider any touch @xmath22 in @xmath24 , such that @xmath22 has no ancestors in @xmath24 .",
    "( that is , @xmath22 has no ancestor nodes that are also touches . ) let @xmath4 be the future thread of @xmath22 and @xmath8 the corresponding fork .",
    "note that @xmath22 s future parent is the last node of @xmath4 by definition .",
    "when the single processor executes @xmath8 , the processor pushes @xmath8 s right child into the deque and continues to execute thread @xmath4 . by hypothesis",
    ", there are no touches by @xmath4 , since any touch by @xmath4 must be an ancestor of @xmath22 .",
    "there may be some forks in @xmath4 .",
    "however , whenever the single processor executes a fork in @xmath4 , it pushes the right child of that fork , which is a node in @xmath4 , into the deque and hence @xmath4 ( i.e. , a node in @xmath4 ) is right below @xmath8 s right child in the deque .",
    "therefore , the processor will always resume thread @xmath4 before the right child of @xmath8 .",
    "since there is no touch by @xmath4 , all the nodes in @xmath4 are ready to execute one by one .",
    "thus , when the future parent of the touch @xmath22 is executed eventually , the right child of @xmath8 is right at the bottom of the deque . by the single touch constraint ,",
    "the local parent of @xmath22 is a descendant of the right child of @xmath8 , so the local parent of @xmath22 can not be executed yet .",
    "thus , the processor will pop the right child of @xmath8 from the bottom of the deque to execute . since this node is not a touch , it is ready to execute .",
    "therefore , @xmath22 satisfies the following two properties .",
    "its future parent is executed before its local parent .",
    "[ prop:1 ]    the right child of its corresponding fork immediately follows its future parent .",
    "[ prop:2 ]    now set @xmath26 and @xmath27 .",
    "thus , all touches in @xmath23 satisfy properties  [ prop:1 ] and  [ prop:2 ] .",
    "note that equation  [ eq : st ] still holds .",
    "now suppose that at some point all nodes in @xmath23 satisfy properties  [ prop:1 ] and  [ prop:2 ] , and that equation  [ eq : st ] holds .",
    "again , we now consider a touch @xmath22 in @xmath24 , such that no touches in @xmath24 are ancestors of @xmath22 , i.e. , all the touches that are ancestors of @xmath22 are in @xmath23 .",
    "since the computation graph is a dag , there must be such an @xmath22 as long as @xmath24 is not empty .",
    "let @xmath4 be the future thread of @xmath22 and @xmath8 the corresponding fork .",
    "if there are no touches by @xmath4 , then @xmath22 satisfies properties  [ prop:1 ] and  [ prop:2 ] , as shown above .",
    "now assume there are touches by @xmath4 .",
    "since those touches are ancestors of @xmath22 , they are all in @xmath23 and hence they all satisfy property  [ prop:1 ] .",
    "when the processor executes @xmath8 , it pushes @xmath8 s right child into the deque and starts executing @xmath4 .",
    "similar to what we showed above , when the processor gets to a fork in @xmath4 , it will always push @xmath4 into its deque , right below the right child of @xmath8 .",
    "thus , the processor will always resume @xmath4 before the right child of @xmath8 .",
    "when the processor gets to the local parent of a touch by @xmath4 , we know the future parent of the touch has already been executed since the touch satisfies property  [ prop:1 ] .",
    "thus , the processor can immediately execute that touch and continue to execute @xmath4 .",
    "therefore , the processor will eventually execute the future parent of @xmath22 while the right child of @xmath4 is still the next node to pop in the deque .",
    "again , since the local parent of @xmath22 is a descendant of the right child of @xmath8 , the local parent of @xmath22 as well as @xmath22 can not be executed yet .",
    "therefore , the processor will now pop the right child of @xmath8 to execute , and hence @xmath22 satisfies properties  [ prop:1 ] and  [ prop:2 ] .",
    "now we set @xmath26 and @xmath27 .",
    "therefore , all touches in @xmath23 satisfy properties  [ prop:1 ] and  [ prop:2 ] , and equation[eq : st ] ) also holds . by induction",
    ", we have @xmath28 and all touches satisfy properties  [ prop:1 ] and  [ prop:2 ] .",
    "acar _ et al . _",
    "@xcite have shown that the number of additional cache misses in a work - stealing parallel computation is bounded by the product of the number of deviations and the number of cache lines .",
    "it is easy to see that only two types of nodes in a dag can be deviations : the touches and the child nodes of forks that are not chosen to execute first .",
    "since we assume the future thread ( left child ) at a fork is always executed first , only the right children of forks can be deviations .",
    "next , we bound the number of deviations incurred by a work - stealing parallel execution to bound its cache overhead .",
    "[ deviationtouch ] let @xmath4 be the future thread at a fork @xmath8 in a structured single - touch computation .",
    "if @xmath4 s touch @xmath22 or @xmath8 s right child @xmath7 is a deviation , then either @xmath7 is stolen or there is a touch by @xmath4 which is a deviation .    by lemma  [ lemmafutureparentfirst ] ,",
    "a touch is a deviation if and only if its local parent is executed before its future parent .",
    "now suppose a processor @xmath16 executes @xmath8 and pushes @xmath7 into its deque .",
    "assume that @xmath7 is not stolen and there are no touches by @xmath4 that are deviations .",
    "thus , @xmath7 will stay in @xmath16 s deque until @xmath16 pops it out .",
    "the proof of this lemma is similar to that of lemma  [ lemmafutureparentfirst ] .",
    "after @xmath16 executes @xmath8 , it moves to execute thread @xmath4 .",
    "there are two possibilities that can make @xmath16 move from @xmath4 to another thread : when it executes a fork or the local parent of a touch .",
    "when it executes a fork , it will push @xmath4 ( the right child of the fork ) into its deque , right below @xmath7 . since a thief processor always steals from the top of a deque , and by hypothesis @xmath7",
    "is not stolen , @xmath4 can not be stolen .",
    "thus , @xmath16 will always resume @xmath4 before @xmath7 and then @xmath7 will become the next node in the deque to pop . when @xmath16 executes the local parent of a touch by @xmath4 , the future parent of that touch must have been executed , since we assume that touch is not a deviation .",
    "thus , @xmath16 can continue to execute that touch immediately and keep moving on in @xmath4 .",
    "therefore , @xmath16 will finally get to the local parent of @xmath22 and then pop @xmath7 out from its deque , since @xmath22 is a descendant of @xmath7 and @xmath22 can not be execute yet . hence",
    ", neither @xmath22 nor @xmath7 can be a deviation .",
    "[ futurefirstupperbound ] if , at each fork , the future thread is chosen to execute first , then a parallel execution with work stealing incurs @xmath29 deviations and @xmath30 additional cache misses in expectation on a structured single - touch computation , where ( as usual ) @xmath3 is the number of processors involved in this computation , @xmath15 is the computation span , and @xmath2 is the number of cache lines .",
    "arora _ et al .",
    "_ have shown that in a parallel execution with work stealing , there are in expectation @xmath31 steals  @xcite . now let us count how many deviations these steals can incur .",
    "a steal on the right child @xmath7 of a fork @xmath8 can make @xmath7 and @xmath8 s corresponding touch @xmath32 deviations .",
    "suppose @xmath32 is a touch by a thread @xmath10 , then the right child of the fork of @xmath10 and @xmath10 s touch @xmath33 can be deviations . if @xmath33 is a deviation and @xmath33 is a touch by another thread @xmath11 , then the right child of the fork of @xmath11 and @xmath11 s touch @xmath34 can be deviation too .",
    "note that @xmath33 is a descendant of @xmath32 and @xmath34 is a descendant of @xmath33 . by repeating this observation ,",
    "we can find a chain of touches @xmath35 , called a _ deviation chain _",
    ", such that each @xmath36 and the right child of the corresponding fork of @xmath36 can be deviations .",
    "since for each @xmath37 , @xmath36 is a descendant of @xmath33 , @xmath38 is in a directed path in the computation dag .",
    "since the length of any path is at most @xmath15 , we have @xmath39 .",
    "since each future thread has only one touch , there is only one deviation chain for a steal .",
    "since there are @xmath31 steals in expectation in a parallel execution  @xcite , we can find in expectation @xmath31 deviation chains and in total @xmath29 touches and right children of the corresponding forks involved , i.e. , @xmath29 deviations involved .",
    "next , we prove by contradiction that no other touches or right children of forks can be deviations .",
    "suppose there is touch @xmath40 , such that @xmath40 or the right child of the corresponding fork of @xmath40 is a deviation , and that @xmath40 is not in any deviation chain .",
    "the right child of the corresponding fork of @xmath40 can not be stolen , since by hypothesis @xmath40 is not the first touch in any of those chains .",
    "thus by lemma  [ deviationtouch ] , there is a touch @xmath41 by the future thread of @xmath40 and @xmath41 is a deviation .",
    "note that @xmath42 can not be in any deviation chain either .",
    "otherwise @xmath40 and the deviation chain @xmath41 is in will form a deviation chain too , a contradiction . therefore , by repeating such  tracing back \" , we will end up at a deviation touch that is not in any deviation chain and has no touches as its ancestors . therefore , there are no touches by the future thread of this touch , and the right child of the corresponding future fork of it is not stolen , contradicting lemma  [ deviationtouch ] .",
    "the upper bound on the expected number of additional cache misses follows from the result of acar _",
    "et al . _",
    "@xcite that the number of additional cache misses in a work - stealing parallel computation is bounded by the product of the number of deviations and the number of cache lines .",
    "the bound on the number of deviations in theorem  [ futurefirstupperbound ] is tight , and the bound on the number of additional cache misses is tight within a factor of @xmath2 , as shown below in theorem  [ futurefirstlowerbound ] .",
    "[ futurefirstlowerbound ] if , at each fork node , the future thread is chosen to execute first , then a parallel execution with work stealing can incur @xmath43 deviations and @xmath43 additional cache misses on a structured single - touch computation , while the sequential execution of this computation incurs @xmath44 cache misses .",
    "figure  [ fig_futurefirst_lowerbound](c ) shows a computation dag on which we can get the bounds we want to prove .",
    "the dag in figure  [ fig_futurefirst_lowerbound](c ) uses the dags in figures  [ fig_futurefirst_lowerbound](a ) and [ fig_futurefirst_lowerbound](b ) as building blocks .",
    "let s look at figures  [ fig_futurefirst_lowerbound](a ) first .",
    "suppose there are two processors @xmath45 and @xmath46 executing the dag in figure  [ fig_futurefirst_lowerbound](a ) .",
    "suppose @xmath46 executes @xmath8 , pushes @xmath47 into its deque , and then falls asleep before executing @xmath48 .",
    "now suppose @xmath45 steals @xmath47 .",
    "for each @xmath49 , @xmath50 or @xmath51 can not be executed since @xmath48 has not been executed yet .",
    "now @xmath45 takes a solo run , executing @xmath52 .",
    "after @xmath45 finishes , @xmath46 wakes up and executes the rest of the computation dag .",
    "note that the right ( local ) parent of @xmath50 is executed before the left ( future ) parent of the touch is executed .",
    "thus , by lemma  [ lemmafutureparentfirst ] , each @xmath50 is a deviation .",
    "hence , this parallel execution incurs @xmath53 deviations and the computation span of the computation is @xmath54 .",
    "now let us consider a parallel execution of the computation in  [ fig_futurefirst_lowerbound](b ) . for each @xmath55 ,",
    "the subgraph rooted at @xmath56 is identical to the computation dag in  [ fig_futurefirst_lowerbound](a ) ( except that the last node of the subgraph has an extra edge pointing to a node of the main thread ) .",
    "suppose there are three processors @xmath45 , @xmath46 , and @xmath57 working on the computation .",
    "assume @xmath46 executes @xmath58 and @xmath12 and then falls asleep when it is about to execute @xmath48 .",
    "@xmath57 now steals @xmath59 from @xmath46 and then falls asleep too .",
    "then @xmath45 steals @xmath47 from @xmath46 s deque .",
    "now @xmath45 and @xmath46 execute the subgraph rooted at @xmath12 in the same way they execute the dag in  [ fig_futurefirst_lowerbound](a ) .",
    "after @xmath45 and @xmath46 finish , @xmath57 wakes up , executes @xmath59 .",
    "now these three processors start working on the subgraph rooted at @xmath60 in the same way they executed the graph rooted at @xmath58 . by repeating this",
    ", the execution ends up incurring @xmath61 deviations when all the @xmath53 subgraphs are done . since the length of the path @xmath62 on the right - hand side is @xmath54 , the computation span of the dag is still @xmath54 .",
    "now we construct the final computation dag , as in figure  [ fig_futurefirst_lowerbound](c ) .",
    "top \" nodes of the dag are all forks , each spawning a future thread .",
    "thus , they form a binary tree and the number of threads increase exponentially",
    ". the dag stops creating new threads at level @xmath63 when it has @xmath64 threads rooted at @xmath65 , respectively . for each @xmath66 ,",
    "the subgraph rooted at @xmath67 is identical to the dag in  [ fig_futurefirst_lowerbound](b ) .",
    "suppose there are @xmath68 processors working on the computation .",
    "it is easy to see @xmath64 processors can eventually get to @xmath65 .",
    "suppose they all fall asleep immediately after executing the first two nodes of @xmath67(corresponding to @xmath58 and @xmath12 in figure  [ fig_futurefirst_lowerbound](b ) ) and then each two of the rest @xmath69 free processors join to work on the subgraph rooted at @xmath67 , in the same way @xmath45 , @xmath46 and @xmath57 did in figure  [ fig_futurefirst_lowerbound](b ) .",
    "therefore , this execution will finally incur @xmath70 deviations , while the computation span of the dag is @xmath71 .",
    "therefore , by setting @xmath72 , we get a parallel execution that incurs @xmath43 deviations , when @xmath73 .    to get the bound on the number of additional cache misses , we just need to modify the graph in  [ fig_futurefirst_lowerbound](a ) as follows .",
    "for each @xmath74 , @xmath75 consists of a chain of @xmath2 nodes @xmath76 , where @xmath2 is the number of cache lines .",
    "@xmath77 access memory blocks @xmath78 , respectively",
    ". similarly , each @xmath51 consists of a chain of @xmath2 nodes @xmath79 .",
    "@xmath80 access memory blocks @xmath81 , respectively . all @xmath50 access memory block @xmath82 .",
    "for all @xmath74 , @xmath83 and @xmath36 both access memory block @xmath84 .",
    "it does not matter which memory blocks the other nodes in the dag access . for simplicity ,",
    "assume the other nodes do not access memory . in the sequential execution ,",
    "the single processor has @xmath78 in its cache after executing @xmath85 and it has incurred @xmath86 cache misses so far .",
    "now it executes @xmath14 and @xmath33 , incurring one cache miss at node @xmath14 by replacing @xmath87 with @xmath84 in its cache , since @xmath87 is the least recently used block .",
    "when it executes @xmath88 and @xmath89 , it only incurs one cache miss by replacing @xmath84 with @xmath87 at the last node of @xmath88 , @xmath90 .",
    "likewise , it is easy to see that the sequential execution will only incur cache misses at nodes @xmath83 and at the last nodes of @xmath75 for all @xmath66 .",
    "hence , the sequential execution incurs only @xmath91 cache misses . when @xmath92 , the sequential execution incurs only @xmath93 cache misses .",
    "now consider the parallel execution by two processors @xmath45 and @xmath46 we described before .",
    "@xmath46 will incur only @xmath2 cache misses , since @xmath51 and @xmath50 only access @xmath94 different blocks @xmath95 and hence @xmath46 does nt need to swap any memory blocks out of its cache .",
    "however , @xmath45 will incur lots of cache misses . after executing each @xmath75",
    ", @xmath45 will execute @xmath96 .",
    "thus at @xmath96 , one cache miss is incurred and @xmath97 is replaced with @xmath84 , since @xmath97 is the least recently used block . then , when @xmath45 executes the first node @xmath98 in @xmath75 , , @xmath99 is not in its cache .",
    "since @xmath100 now becomes the least recently used memory block in @xmath45 s cache , @xmath101 is replaced by @xmath99 .",
    "thus , @xmath101 will not be in the cache when it is in need at @xmath102 .",
    "therefore , it is obvious that @xmath45 will incur a cache miss at each node in @xmath75 and hence incur @xmath103 cache misses in total in the entire execution .",
    "note that the computation span of this modified dag is @xmath104 , since each @xmath51 now has @xmath2 nodes .",
    "therefore , the sequential execution and the parallel execution actually incur @xmath105 and @xmath106 , respectively , when @xmath73 . therefore ,",
    "if we use this modified dag as the building blocks in  [ fig_futurefirst_lowerbound](c ) , we will get the bound on the number of additional cache misses stated in the theorem .      in this section",
    ", we show that if the parent thread is always executed first at a fork , a work - stealing parallel execution of a structured single - touch computation can incur @xmath107 deviations and @xmath21 additional cache misses , where @xmath4 is the number of touches in the computation , while the corresponding sequential execution incurs only a small number of cache misses .",
    "this bound matches the upper bound for general , unstructured future - parallel computations @xcite .",
    "this result , combined with the result in section  [ section_futurethreadfirst ] , shows that choosing the future threads at forks to execute first achieves better cache locality for work - stealing schedulers on structured single - touch computations .",
    "[ parentthreadfirst ] if , at each fork , the parent thread is chosen to execute first , then a parallel execution with work stealing can incur @xmath107 deviations and @xmath21 additional cache misses on a structured single - touch computation , while the sequential execution of this computation incurs only @xmath108 cache misses .",
    "the final dag we want to construct is in figure  [ fig_parentfirst_lowerbound2 ] .",
    "it uses the dags in figure  [ fig_parentfirst_lowerbound1 ] as building blocks .",
    "we first describe how a single deviation at a touch @xmath109 can incur @xmath19 deviations and @xmath20 additional cache misses in figure  [ fig_parentfirst_lowerbound1](a ) . in order to get the",
    "bound we want to prove , here we follow the convention in @xcite to distinguish between touches and join nodes in the dag . more specifically , @xmath110 is a join node , not a touch , for each @xmath111 . for each @xmath112 , node @xmath36 accesses memory block @xmath99 and @xmath110 accesses memory block @xmath84 .",
    "@xmath51 consists of a chain of @xmath2 nodes @xmath113 , accessing memory blocks @xmath114 respectively .",
    "all the other nodes do not access memory .",
    "assume in the sequential execution a single processor @xmath45 executes the entire dag in figure  [ fig_parentfirst_lowerbound1](a ) .",
    "suppose initially the left ( future ) parent of @xmath109 has already been executed .",
    "@xmath45 starts executing the dag at @xmath47 .",
    "since @xmath45 always stays on the parent thread at a fork , it first pushes @xmath115 into its deque , continues to execute @xmath116 , and then executes @xmath117 while pushing @xmath118 into its deque .",
    "since @xmath8 can not be executed due to @xmath115 , @xmath45 pops @xmath119 out of its deque and executes the nodes in @xmath120 .",
    "then @xmath45 executes all the nodes in @xmath121 , in this order .",
    "so far @xmath45 has only incurred @xmath2 cache misses , since all the nodes it has executed only access memory blocks @xmath122 and hence it did not need to swap any memory blocks out of its cache . now @xmath45 executes @xmath123 and then @xmath124 , incurring only one more cache miss by replacing @xmath99 with @xmath84 at @xmath125 .",
    "hence , this execution incurs @xmath126 cache misses in total .",
    "note that the left parent of @xmath110 is executed before the right parent @xmath110 for all @xmath66 .    now assume in another execution by @xmath45 , the left parent of @xmath109 is in @xmath45 s deque when @xmath45 starts executing @xmath47 .",
    "thus , @xmath109 is a deviation with respect to the previous execution .",
    "since @xmath109 is not ready to execute after @xmath45 executes @xmath14 , @xmath45 pops @xmath115 out of its deque to execute .",
    "since @xmath8 is not ready , @xmath45 now pops the left parent of @xmath109 to execute and then executes @xmath127 .",
    "now @xmath45 pops @xmath119 out and executes all the nodes @xmath120 . note that @xmath125 is now ready to execute and the memory blocks in @xmath45 s cache at the moment are @xmath128 .",
    "now @xmath45 executes @xmath125 , replacing the least recently used block @xmath99 with @xmath84 .",
    "@xmath45 then pops @xmath129 out and executes all the nodes @xmath130 in @xmath131 one by one .",
    "when @xmath45 executes @xmath129 , it replaces @xmath101 with @xmath99 , and when it executes @xmath132 , it replaces @xmath133 with @xmath101 , and so on .",
    "the same thing happens to all @xmath51 and @xmath110 .",
    "thus , @xmath45 will incur a cache miss at every node afterwards , ending up with @xmath134 cache misses in total .",
    "note that the computation span of this dag is @xmath135 .",
    "thus , this execution with a deviation at @xmath109 incurs @xmath20 cache misses when @xmath136 .",
    "moreover , all @xmath110 are deviations and hence this execution incurs @xmath19 deviations",
    ".    now let us see how a single steal at the beginning of a thread results in @xmath19 deviations and @xmath20 cache misses at the end of the thread .",
    "figure  [ fig_parentfirst_lowerbound1](b ) presents such a computation .",
    "first we consider the sequential execution by a processor @xmath45 .",
    "it is easy to check @xmath45 executes nodes in the order @xmath137 , @xmath138 , @xmath139 , @xmath140 .",
    "the key observation is that @xmath141 is executed before @xmath50 is executed for any odd numbered @xmath66 while @xmath141 is executed after @xmath50 is executed for any even numbered @xmath66 .",
    "this statement can be proved by induction .",
    "obviously , this holds for @xmath142 and @xmath143 , as we showed before .",
    "now suppose this fact holds for all @xmath144 , for some even numbered @xmath66 .",
    "now suppose @xmath45 executes @xmath145 .",
    "then @xmath45 pushes @xmath50 into its deque and executes @xmath146 .",
    "since we know @xmath146 should be executed before @xmath147 , @xmath147 has not been executed yet .",
    "moreover , @xmath147 must already be in the deque before @xmath148 was pushed into the deque , since @xmath147 s parent @xmath149 has been executed and @xmath147 is ready to execute .",
    "now @xmath45 pops @xmath50 to execute .",
    "since @xmath56 is not ready to execute , @xmath45 pops @xmath147 and then executes @xmath150 , and pushes @xmath151 into the deque .",
    "now @xmath45 continues to execute @xmath152 and pushes @xmath151 into its deque .",
    "then @xmath153 executes @xmath154 and pops @xmath155 , since @xmath156 is not ready due to @xmath151 .",
    "now we can see @xmath154 and @xmath155 have been executed , but @xmath151 and @xmath157 not yet .",
    "therefore , @xmath154 is executed before @xmath151 and @xmath157 is executed after @xmath155 .",
    "that is , the statement holds for @xmath158 and @xmath159 , and hence the proof .",
    "the subgraph rooted at @xmath160 is identical to the graph in figure  [ fig_parentfirst_lowerbound1](a ) , with @xmath161 corresponding to @xmath109 in figure  [ fig_parentfirst_lowerbound1](a ) .",
    "therefore , if @xmath53 is an even number , @xmath161 s left parent has been executed when @xmath162 is executed and hence the sequential execution will incur only @xmath126 cache misses on the subgraph rooted at @xmath160 .",
    "now consider a parallel execution of the dag in  [ fig_parentfirst_lowerbound1](b ) by two processors @xmath45 and @xmath46 .",
    "@xmath45 executes @xmath163 and pushes @xmath164 into its deque .",
    "@xmath46 immediately steals @xmath164 and executes it .",
    "then @xmath46 falls asleep , leaving @xmath45 executing the rest of the dag alone .",
    "it is easy to check @xmath45 will execute the nodes in the dag in the order @xmath165 it can be proved by induction that @xmath141 is executed after @xmath50 is executed for any odd numbered @xmath66 while @xmath141 is executed before @xmath50 is executed for any even numbered @xmath66 , which is opposite to the order in the sequential execution .",
    "the induction proof is similar to that of the previous observation in the sequential execution , so we omit the proof here . if @xmath53 is an even number , @xmath162 will be executed before the left parent of @xmath161 and hence this execution will incur @xmath19 deviations and @xmath20 cache misses when @xmath136 and @xmath166 .",
    "the final dag we want to construct is in figure  [ fig_parentfirst_lowerbound2 ] .",
    "this is actually a generalization of the dag in figure  [ fig_parentfirst_lowerbound1](b ) . instead of having one fork @xmath83 before each touch @xmath56",
    ", it has two forks @xmath83 and @xmath36 , for each @xmath66 .",
    "after each touch @xmath56 , the thread at @xmath110 splits into two identical branches , touching the futures spawned at @xmath83 and @xmath36 , respectively . in this figure , we only depict the right branch and omit the identical left branch .",
    "as we can see , the right branch later has a touch @xmath156 touching the future @xmath151 spawned at the fork @xmath36 .",
    "if we only look at the thread on the right - hand side , it is essentially the same as the dag in figure[fig_parentfirst_lowerbound1](b ) .",
    "the sequential execution of this dag by @xmath45 is similar to that in figure[fig_parentfirst_lowerbound1](b ) .",
    "the only difference is that @xmath45 at each @xmath110 will execute the right branch and then the left branch recursively .",
    "similarly , it can be proved by induction that @xmath141 is executed before @xmath50 is executed for any odd numbered @xmath66 while @xmath141 is executed after @xmath50 is executed for any even numbered @xmath66 .",
    "obviously this also holds for each left branch .",
    "now consider a parallel execution by two processors @xmath45 and @xmath46 .",
    "@xmath45 first executes @xmath163 .",
    "@xmath46 immediately steals @xmath164 and executes it and then sleeps forever .",
    "now @xmath45 makes a solo run to execute the rest of the dag .",
    "again , we can prove by induction that @xmath141 is executed after @xmath50 is executed for any odd numbered @xmath66 while @xmath141 is executed before @xmath50 is executed for any even numbered @xmath66 , which is opposite to the order in the sequential execution .",
    "the proofs of the two observations above are a little more complicated than those for the dag in figure[fig_parentfirst_lowerbound1](b ) , but the ideas are essentially the same . due to space limits , we again omit the two induction proofs .    by splitting each thread into two after each @xmath110 , the number of branches in the dag increases exponentially .",
    "suppose there are @xmath4 touches in the dag .",
    "thus , there are eventually @xmath167 branches and the height of this structure is @xmath168 . at the end of each branch",
    "is a subgraph identical to the dag in figure  [ fig_parentfirst_lowerbound1](a ) .",
    "therefore , the parallel execution with only one steal can end up incurring @xmath169 deviations and @xmath170 cache misses .",
    "the sequential execution incurs only @xmath171 cache misses , since the sequential execution will incur only 2 cache misses by swapping @xmath84 in and out at each branch , after it incurs @xmath2 cache misses to load @xmath172 at the first branch .",
    "hence , when @xmath173 and @xmath136 , we get the bound stated in the theorem .",
    "deviations and @xmath21 if it chooses parents threads to execute first at forks .",
    "this example uses the dags in figure  [ fig_parentfirst_lowerbound1 ] as building blocks.,width=264 ]",
    "it is natural to ask whether other kinds of structured computations can also achieve relatively good cache locality .",
    "we now consider two alternative kinds of restrictions .      in this section",
    ", we prove that work - stealing parallel executions of structured local - touch computations also have relatively good cache locality , if the future thread is chosen to execute first at each fork .",
    "this result , combined with theorems  [ futurefirstupperbound ] and  [ parentthreadfirst ] , implies that work - stealing schedulers for structured computations are likely better off choosing future threads to execute first at forks .",
    "[ lemmafutureparentfirstlocaltouch ] in the sequential execution of a structured local - touch computation where the future thread at a fork is always chosen to execute first , any touch @xmath22 s future parent is executed before @xmath22 s local parent , and the right child of any fork @xmath8 immediately follows the last node of the future thread spawned at @xmath8 , i.e. , the future parent of the last touch of the future thread .",
    "the proof is omitted because it is almost identical to the proof of lemma [ lemmafutureparentfirst ] .",
    "[ localtouchupperbound ] if the future thread at a fork is always chosen to execute first , then a parallel execution with work stealing incurs @xmath29 deviations and @xmath30 additional cache misses in expectation on a structured local - touch computation .",
    "let @xmath8 be a fork that spawns a future thread @xmath4 .",
    "now we consider a parallel execution .",
    "let @xmath16 be a processor that executes @xmath8 and pushes the right child of @xmath8 into its deque .",
    "suppose the right child of @xmath8 is not stolen .",
    "now consider the subgraph @xmath174 consisting of @xmath4 and its descendant threads .",
    "note that @xmath174 itself is a structured computation dag with local touch constraint .",
    "now @xmath16 starts executing @xmath174 .    according to local touch constraint ,",
    "the only nodes outside @xmath174 that connect to the nodes in @xmath174 are @xmath8 and the touches of @xmath4 , and @xmath175 is the only node outside @xmath174 that the nodes in @xmath174 depend on .",
    "now @xmath8 has been executed and the touches of @xmath4 are not ready to execute due to the right child of @xmath8 .",
    "hence , @xmath16 is able to make a sequential execution on @xmath174 without waiting for any node outside to be done or jumping to a node outside , as long as no one steals a node in @xmath174 from @xmath16 s deque .",
    "since we assume the right child of @xmath8 will not be stolen and any nodes in @xmath174 can only be pushed into @xmath16 s deque below @xmath8 , no nodes in @xmath174 can be stolen .",
    "hence , @xmath174 will be executed by a sequential execution by @xmath16 .",
    "therefore , there are no deviations in @xmath174 .",
    "after @xmath16 executed the last node in @xmath174 , which is the last node in @xmath4 , @xmath16 pops the right child of @xmath8 to execute .",
    "hence , the right child of @xmath8 can not be a deviation either , if it is not stolen .",
    "that is , those nodes can be deviations only if the right child of @xmath8 is stolen . since there are in expectation @xmath31 steals in an parallel execution and each future thread",
    "has at most @xmath15 touches , the expected number of deviations is bounded by @xmath29 and the expected number of additional touches is bounded by @xmath30 .",
    "as discussed in section  [ section_structured ] , in languages such as java , the program s main thread typically releases all resources at the end of an execution .",
    "to model this structure , we add an edge from the last node of each thread to the final node of the computation dag . thus , the final node becomes the only node with in - degree greater than 2 . since the final node is always the last to execute , simply adding those edges pointing to the final node into a dag will not change the execution order of the nodes in the dag .",
    "it is easy to see that having such a super node will not change the upper bound on the cache overheads of the work - stealing parallel executions of a structured computation .    for structured computations with super final nodes",
    ", it also makes sense to relax slightly the single - touch constraint .",
    "a structured single - touch computation with a _ super final node _ is one where each future thread @xmath4 at a fork @xmath8 has at least one and at most _ two _ touches , a descendant of @xmath8 s right child and the super final node .",
    "in such a computation , a future thread can have the super final node as its only touch .",
    "this structure corresponds to a program where one thread forks another thread to accomplish a side - effect instead of computing a value .",
    "the parent thread never touches the resulting future , but the computation as a whole can not terminate until the forked thread completes its work .",
    "now we prove that the parallel executions of structured single - touch computations with super final nodes also have relatively low cache overheads .    in the sequential execution of a structured single - touch computation with a super final node , where the future thread at a fork is always chosen to execute first ,",
    "any touch @xmath22 s future parent is executed before @xmath22 s local parent , and the right child of any fork @xmath8 immediately follows the last node of the future thread spawned at @xmath8 , i.e. , the future parent of the last touch of the future thread .",
    "let @xmath4 be the future thread at a fork @xmath8 in a structured single - touch computation with a super final node . if a touch of @xmath4 or @xmath8 s right child @xmath7 is a deviation , then either @xmath7 is stolen or there is a touch by @xmath4 which is a deviation .",
    "the proofs of these two lemmas are omitted because they are almost identical to the proofs of lemma [ lemmafutureparentfirst ] and lemma  [ deviationtouch ] , respectively .",
    "if , at each fork , the future thread is chosen to execute first , then a parallel execution with work stealing incurs @xmath29 deviations and @xmath30 additional cache misses in expectation on a structured single - touch computation with a super final node .",
    "the proof is similar to that of theorem  [ futurefirstupperbound ] .",
    "the only difference is that if a touch by a thread @xmath4 is a deviation , now the two touches of @xmath4 can both be deviations , which could be a trouble for constructing the deviation chains .",
    "fortunately , one of these two touches is the super final node , which is always the last node to execute and hence will not make the touches of other threads become deviations .",
    "therefore , we can still get a unique deviation chain starting from a steal and hence the proof of theorem  [ futurefirstupperbound ] still applies here .",
    "we have focused primarily on structured single - touch computations , in which futures are used in a restricted way .",
    "we saw that for such computations , a parallel execution by a work - stealing scheduler that runs future threads first can incur at most @xmath6 cache misses more than the corresponding sequential execution , a substantially better cache locality than the @xmath1 worst - case additional cache misses possible with unstructured use of futures .",
    "although we can not prove this claim formally , we think that these restrictions correspond to program structures that would occur naturally anyway in many ( but not all ) parallel programs that use futures . for example , cilk  @xcite programs are structured single - touch computations , and that belloch and reid - miller  @xcite observe that the single - touch requirement substantially simplifies implementations .",
    "we also considered some alternative restrictions on future use , such as structured local - touch computations , and structured computations with super final nodes , that also incur a relatively low cache - locality penalty . in terms of future work , we think it would be promising to investigate how far these restrictions can be weakened or modified while still avoiding a high cache - locality penalty .",
    "we would also like to understand how these observations can be exploited by future compilers and run - time systems",
    ".    umut  a. acar , guy  e. blelloch , and robert  d. blumofe .",
    "the data locality of work stealing . in",
    "_ proceedings of the twelfth annual acm symposium on parallel algorithms and architectures _ , spaa 00 , pages 112 , new york , ny , usa , 2000 .",
    "kunal agrawal , yuxiong he , and charles  e. leiserson . adaptive work stealing with parallelism feedback . in _ proceedings of the 12th acm sigplan symposium on principles and practice of parallel programming",
    "_ , ppopp 07 , pages 112120 , new york , ny , usa , 2007 .",
    "nimar  s. arora , robert  d. blumofe , and c.  greg plaxton .",
    "thread scheduling for multiprogrammed multiprocessors . in _ proceedings of the tenth annual acm symposium on parallel algorithms and architectures _ , spaa 98 , pages 119129 , new york , ny , usa , 1998 .",
    "acm .",
    "guy  e. blelloch , phillip  b. gibbons , and yossi matias .",
    "provably efficient scheduling for languages with fine - grained parallelism . in _ proceedings of the seventh annual acm symposium on parallel algorithms and architectures _ , spaa 95 , pages 112 , new york , ny , usa , 1995 .",
    "guy  e. blelloch and margaret reid - miller .",
    "pipelining with futures . in _ proceedings of the ninth annual acm symposium on parallel algorithms and architectures _ , spaa 97 , pages 249259 , new york , ny , usa , 1997 .",
    "robert  d. blumofe , matteo frigo , christopher  f. joerg , charles  e. leiserson , and keith  h. randall .",
    "an analysis of dag - consistent distributed shared - memory algorithms . in _ proceedings of the eighth annual acm symposium on parallel algorithms and architectures _ , spaa 96 , pages 297308 , new york , ny , usa , 1996 .",
    "robert  d. blumofe , christopher  f. joerg , bradley  c. kuszmaul , charles  e. leiserson , keith  h. randall , and yuli zhou .",
    "cilk : an efficient multithreaded runtime system . in _ proceedings of the fifth acm sigplan symposium on principles and practice of parallel programming",
    "_ , ppopp 95 , pages 207216 , new york , ny , usa , 1995 .",
    "acm .",
    "f.  warren burton and m.  ronan sleep . executing functional programs on a virtual tree of processors . in _ proceedings of the 1981 conference on functional programming languages and computer architecture _ , fpca 81 , pages 187194 , new york , ny , usa , 1981 .",
    "david chase and yossi lev .",
    "dynamic circular work - stealing deque . in _ proceedings of the seventeenth annual acm symposium on parallelism in algorithms and architectures _ , spaa 05 , pages 2128 , new york , ny , usa , 2005 . acm .",
    "matthew fluet , mike rainey , john reppy , and adam shaw .",
    "implicitly - threaded parallelism in manticore . in _ proceedings of the 13th acm",
    "sigplan international conference on functional programming _ , icfp 08 , pages 119130 , new york , ny , usa , 2008 .",
    "matteo frigo , charles  e. leiserson , and keith  h. randall .",
    "the implementation of the cilk-5 multithreaded language . in",
    "_ proceedings of the acm sigplan 1998 conference on programming language design and implementation _ , pldi 98 , pages 212223 , new york , ny , usa , 1998 .",
    "john giacomoni , tipp moseley , and manish vachharajani .",
    "fastforward for efficient pipeline parallelism : a cache - optimized concurrent lock - free queue . in _ proceedings of the 13th acm sigplan symposium on principles and practice of parallel programming",
    "_ , ppopp 08 , pages 4352 , new york , ny , usa , 2008 .",
    "michael  i. gordon , william thies , and saman amarasinghe .",
    "exploiting coarse - grained task , data , and pipeline parallelism in stream programs . in _ proceedings of the 12th international conference on architectural support for programming languages and operating systems _ , asplos xii ,",
    "pages 151162 , new york , ny , usa , 2006 .",
    "robert  h. halstead , jr .",
    "implementation of multilisp : lisp on a multiprocessor . in _ proceedings of the 1984 acm symposium on lisp and functional programming _ , lfp 84 , pages 917 , new york , ny , usa , 1984 .",
    "d.  a. kranz , r.  h. halstead , jr . , and e.  mohr .",
    "mul - t : a high - performance parallel lisp . in _ proceedings of the acm sigplan 1989 conference on programming language design and implementation",
    "_ , pldi 89 , pages 8190 , new york , ny , usa , 1989 .",
    "i - ting  angelina lee , charles  e. leiserson , tao  b. schardl , jim sukha , and zhunping zhang . on - the - fly pipeline parallelism . in _ proceedings of the 25th acm symposium on parallelism in algorithms and architectures _ , spaa 13 , pages 140151 , new york , ny , usa , 2013 .",
    "daniel spoonhower , guy  e. blelloch , phillip  b. gibbons , and robert harper . beyond nested parallelism :",
    "tight bounds on work - stealing overheads for parallel futures . in _ proceedings of the twenty - first annual symposium on parallelism in algorithms and architectures _ , spaa 09 , pages 91100 , new york , ny , usa , 2009 ."
  ],
  "abstract_text": [
    "<S> in _ fork - join parallelism _ , a sequential program is split into a directed acyclic graph of tasks linked by directed dependency edges , and the tasks are executed , possibly in parallel , in an order consistent with their dependencies . a popular and effective way to extend fork - join parallelism </S>",
    "<S> is to allow threads to create _ </S>",
    "<S> a thread creates a future to hold the results of a computation , which may or may not be executed in parallel . </S>",
    "<S> that result is returned when some thread _ touches _ that future , blocking if necessary until the result is ready .    </S>",
    "<S> recent research has shown that while futures can , of course , enhance parallelism in a structured way , they can have a deleterious effect on cache locality . in the worst case </S>",
    "<S> , futures can incur @xmath0 deviations , which implies @xmath1 additional cache misses , where @xmath2 is the number of cache lines , @xmath3 is the number of processors , @xmath4 is the number of touches , and @xmath5 is the _ </S>",
    "<S> computation span_. since cache locality has a large impact on software performance on modern multicores , this result is troubling .    in this paper , however , we show that if futures are used in a simple , disciplined way , then the situation is much better : if each future is touched only once , either by the thread that created it , or by a later descendant of the thread that created it , then parallel executions with work stealing can incur at most @xmath6 additional cache misses , a substantial improvement . </S>",
    "<S> this structured use of futures is characteristic of many ( but not all ) parallel applications .    </S>",
    "<S> = 10000 = 10000    scheduling ; work stealing ; futures ; parallel programming ; cache locality ; performance bounds </S>"
  ]
}