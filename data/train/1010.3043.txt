{
  "article_text": [
    "the candecomp / parafac ( cp ) tensor factorization can be considered a higher - order generalization of the matrix singular value decomposition @xcite and has many applications .",
    "the canonical fit function for the cp tensor factorization is based on the frobenius norm , meaning that it is a maximum likelihood estimate ( mle ) under the assumption of additive i.i.d .",
    "gaussian perturbations .",
    "it turns out , however , that this loss function can be very sensitive to violations in the gaussian assumption .",
    "however , many other types of noise are relevant for cp models . for example , in fmri neuroimaging studies , movement by the subject can lead to sparse high - intensity changes that are easily confused with brain activity @xcite .",
    "likewise , in foreground / background separation problems in video surveillance , a subject walking across the field of view represents another instance of a sparse high intensity change @xcite . in both examples , there is a relatively large perturbation in magnitude that affects only a relatively small fraction of data points ; we call this artifact noise .",
    "these scenarios are particularly challenging because the perturbed values are on the same scale as normal values ( i.e. , true brain activity signals and background pixel intensities ) .",
    "consequently , there is a need to explore factorization methods that are robust against violations in the gaussian assumption . in this paper , we consider a loss based on the 1-norm which is known to be robust or insensitive to gross non - gaussian perturbations @xcite .",
    "vorobyov et al .",
    "previously described two ways of solving the least 1-norm cp factorization problem based on a linear programming and weighted median filtering @xcite .",
    "our method differs in that we use a majorization - minimization ( mm ) strategy @xcite .",
    "like @xcite our method performs block minimization .",
    "an advantage of our approach is that each block minimization can be split up into many small and independent optimization problems which may scale more favorably with a tensor s size .    throughout",
    ", we use the following definitions and conventions .",
    "all vectors are column vectors .",
    "the transpose of the @xmath0 row of a matrix @xmath1 is denoted by @xmath2 .",
    "the _ order _ of a tensor is the number of dimensions , also known as ways or modes .",
    "_ fibers _ are the higher - order analogue of matrix rows and columns .",
    "a fiber is defined by fixing every index but one .",
    "a matrix column is a mode-@xmath3 fiber and a matrix row is a mode-@xmath4 fiber .",
    "the mode-@xmath5 matricization of a tensor @xmath6 is denoted by @xmath7 and arranges the mode-@xmath5 fibers to be the columns of the resulting matrix .",
    "the rest of this paper is organized as follows .",
    "the robust iterative algorithm is derived in section  [ sec : mm ] . in section  [ sec : simulation ]",
    "we compare cpal1 and the standard cp factorizations by alternating least squares ( cpals ) in the presence of non - gaussian perturbations on simulated data .",
    "concluding remarks are given in section  [ sec : conclusion ] .",
    "mm algorithms have been applied to factorization problems previously @xcite .",
    "the idea is to convert a hard optimization problem ( e.g. , non - convex , non - differentiable ) into a series of simpler ones ( e.g. , smooth convex ) , which are easier to minimize than the original .",
    "to do so , we use majorization functions , i.e. , @xmath8 majorizes @xmath9 at @xmath10 if @xmath11 for all @xmath12 and @xmath13 .    given a procedure for constructing a majorization , we can define the mm algorithm to find a minimizer of a function @xmath9 as follows .",
    "let @xmath14 denotes the @xmath15 iterate .",
    "find a majorization @xmath16 of @xmath9 at @xmath14 .",
    "set @xmath17 .",
    "repeat until convergence .",
    "this algorithm always takes non - increasing steps with respect to @xmath9 .",
    "moreover , sufficient conditions for the mm algorithm to converge to a stationary point are well known @xcite .",
    "specifically , the mm iterates will converge to a stationary point of @xmath9 if @xmath9 is continuously differentiable , coercive in the sense that all its level sets must be compact , and all its stationary points are isolated ; and the function @xmath18 is jointly twice continuously differentiable in @xmath19 and is strictly convex in @xmath20 with @xmath21 fixed .",
    "we now derive an appropriate majorization function for approximate @xmath22 regression ; this is subsequently used for our robust tensor factorization .",
    "the unregularized majorization can be found in @xcite .",
    "given a vector @xmath23 and a matrix @xmath24 , we search for a vector @xmath25 that minimizes the loss @xmath26 where @xmath27 .",
    "note that @xmath28 is not smooth and may not be strictly convex if @xmath29 is not full rank .",
    "therefore , we instead consider the following smoothed and regularized version to @xmath28 : @xmath30 where @xmath31 and @xmath32 are small positive numbers . in this case",
    ", @xmath33 at @xmath34 is majorized by @xmath35 both the loss @xmath36 and its majorization @xmath37 meet the sufficient conditions that guarantee convergence of the mm algorithm to a stationary point of @xmath36 . since @xmath38 is strictly convex and coercive , it has exactly one stationary point .",
    "thus , the mm algorithm converges to the global minimum of @xmath36 . after some simplification",
    ", the iterate mapping can be expressed as @xmath39 let @xmath40 be the diagonal matrix with @xmath41",
    ". then the minimization problem ( [ eq : least_el1_mmb ] ) is a regularized weighted least squares problem with a unique solution , i.e. , @xmath42      we now derive cpal1 for a 3-way tensor @xmath43 of size @xmath44 ( it is straightforward to generalize the algorithm to tensors of arbitrary size ) . to perform a rank-@xmath45 factorization",
    "we minimize @xmath46 which is the regularized approximate 1-norm of the difference between @xmath43 and its rank-@xmath45 approximation , where @xmath47 : @xmath48 in a round - robin fashion , we repeatedly update one factor matrix while holding the other two fixed .",
    "note that the mode-1 matricization of the rank-@xmath45 approximation is @xmath49 where @xmath50 denotes the khatri - rao product @xcite .",
    "then the subproblem of updating @xmath1 for a fixed @xmath51 and @xmath52 is @xmath53 this minimization problem is separable in the rows of @xmath1 , and the optimization problem for a given row is an @xmath22 regression problem .",
    "thus , we can apply the update rule ( [ eq : least_el1_mm_sol ] ) with @xmath21 equal to the @xmath0 row of @xmath54 and @xmath55 .",
    "the other two subproblems are solved analogously .",
    "we compare the results of cpal1 with cpals implemented in the tensor toolbox @xcite in the presence of gaussian and artifact noise .",
    "we created @xmath56-way tensors , @xmath57 of rank-@xmath58 as follows .",
    "we first generated random factor matrices @xmath59 where the matrix elements were the absolute values of i.i.d .",
    "draws from a standard gaussian .",
    "the @xmath60 entry of the noise free tensor @xmath43 was then set to be @xmath61 .",
    "then to each @xmath43 we added dense gaussian noise and artifact outliers .",
    "all random variables we describe were independently drawn .",
    "we generated an artifact tensor @xmath62 as follows .",
    "a fraction @xmath63 of the tensor entries was selected randomly .",
    "we then assigned to each of the selected entries a value drawn from a gamma distribution with shape parameter @xmath64 and scale parameter @xmath65 .",
    "all other entries were set to @xmath66 .",
    "for the dense gaussian noise tensor @xmath67 , the entries @xmath68 were i.i.d .",
    "draws from a standard gaussian .",
    "the tensor @xmath69 was obtained by adding the noise and artifact tensors to @xmath70 @xmath71 for @xmath72 and @xmath73 and @xmath74 . for all combinations of @xmath63 and @xmath75 the scaled values of @xmath68",
    "were less than the largest value of @xmath43 .",
    "for every pair @xmath76 we performed @xmath77 rank-@xmath58 factorizations under the two methods .",
    "for cpal1 computations we set @xmath78 and @xmath79 .",
    "initial points for all tests were generated using the @xmath5-mode singular vectors of the tensor ( i.e. , the ` vecs   comma`d in the tensor toolbox ) . to assess the goodness of a computed factorization we calculated the factor match score ( fms ) between the estimated and true factors @xcite .",
    "the fms ranges between 0 and 1 ; an fms of @xmath3 corresponds to a perfect recovery of the original factors .",
    "+    figure  [ fig : boxplots ] shows boxplots of the fms under both methods . the scores for cpals decreased as the contribution of non - gaussian noise increased .",
    "in contrast regardless of the noise distributions applied cpal1 tended to recover the true factorization with the exception of occasionally finding local minima ,    figure  [ fig : factors ] compares one column of one recovered factor matrix when @xmath80 and @xmath81 for the two methods . in this instance",
    "the cpals factorization has some trouble recovering the true factor column . in this example",
    "the fms was @xmath82 and @xmath83 for cpal@xmath3 and cpals respectively .",
    "the median cpals fms was about @xmath84 , so the example shown is somewhat typical .",
    "the factorization is not terrible qualitatively , but the errors in the factor 2 estimates do fail to capture details that cpal1 solution does .",
    "we derived a robust tensor factorization algorithm based on an approximate 1-norm loss . in comparisons with methods using an lp solver we found that our method performed slightly faster on tensors of similar size to those factored in the simulation experiments of this paper (",
    "not shown ) .",
    "we suspect the performance gap may widen depending on the size of the tensor .",
    "indeed , to factor an arbitrary tensor of size @xmath85 the lp update for the @xmath0 factor matrix would be an optimization problem over @xmath86 parameters .",
    "in contrast , the @xmath0 factor matrix update consists of @xmath87 independent @xmath22 minimizations over @xmath45 parameters .",
    "moreover , the independence of these minimizations present speed - up opportunities through parallelization .",
    "our simulations demonstrated that there are non - gaussian noise scenarios in which the quality of cpals solutions suffer while those of cpal1 tend to be insensitive to the presence of non - gaussian noise . in simulation studies",
    "not shown we have seen that not all non - gaussian perturbations cause noticeable degradation in the cpals factorization .",
    "conversely , there are situations when cpal1 struggles as much as cpals in the presence of artifact noise , e.g. when the data tensor is sparse as well .",
    "we conjecture that cpal1 is most suited to handle artifact noise when the data tensor is dense .",
    "finding an alternative to the 1-norm loss for sparse data with non - gaussian noise is a direction for future research ."
  ],
  "abstract_text": [
    "<S> tensors are multi - way arrays , and the candecomp / parafac ( cp ) tensor factorization has found application in many different domains . the cp model is typically fit using a least squares objective function , which is a maximum likelihood estimate under the assumption of i.i.d . </S>",
    "<S> gaussian noise . </S>",
    "<S> we demonstrate that this loss function can actually be highly sensitive to non - gaussian noise . </S>",
    "<S> therefore , we propose a loss function based on the 1-norm because it can accommodate both gaussian and grossly non - gaussian perturbations . </S>",
    "<S> we also present an alternating majorization - minimization algorithm for fitting a cp model using our proposed loss function . </S>"
  ]
}