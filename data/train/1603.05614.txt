{
  "article_text": [
    "as our society enters the big data era , the main problem that data scientists are facing is how to process the unprecedented large datasets . besides , data sources are heterogenous , comprising documents , images , sounds , and videos .",
    "such challenges require the data processing algorithms to be more computationally efficient .",
    "the concept of submodularity plays an important role in pursuing efficient solutions for combinatorial optimization , since it has rich theoretical and practical features .",
    "hence submodular optimization has been adopted to preprocess massive data in order to reduce the computational complexity .",
    "for example , in the kernel - based machine learning @xcite , the most representative subset of data is first selected in order to decrease the dimension of the feature space , by solving a submodular maximization problem under a cardinality constraint . besides , such submodular optimization models have been extended to address data summarization problems  @xcite .",
    "although maximizing a submodular function under a cardinality constraint is a typical np - hard problem , a simple greedy algorithm developed in @xcite achieves a @xmath2-approximation of the optimal solution with a much lower computation complexity .",
    "when the main memory can store the whole dataset , such a greedy algorithm can be easily applied for various applications . however , as it requires the full access to the whole dataset , large - scale problems prevent the greedy algorithm from being adequate due to practical computation resource and memory limitations . even in the case",
    "when the memory size is not an issue , it is possible that the number of data samples grows rapidly such that the main memory is not able to read all of them simultaneously .    under the scenarios discussed above , processing data in a streaming fashion",
    "becomes a necessity , where at any time point , the streaming algorithm needs to store just a small portion of data into the main memory , and produces the solution right at the end of data stream .",
    "a streaming algorithm does not require the full access to the whole dataset , thus only needs limited computation resource . in @xcite",
    ", the authors introduced a streaming algorithm to maximize a submodular function under a cardinality constraint , where the cardinality constraint is just a special case of a @xmath0-knapsack constraint @xcite with each weight being one .",
    "when each element has multiple weights or there are more than one knapsack constraints , the algorithm proposed in @xcite is no longer applicable .    in this paper , we develop a new streaming algorithm to maximize a monotone submodular function , subject to a general @xmath0-knapsack constraint .",
    "it requires only one single pass through the data , and produces a @xmath3-approximation of the optimal solution , for any @xmath4 .",
    "in addition , the algorithm only requires @xmath5 memory ( independent of the dataset size ) and @xmath6 computation per element with @xmath7 being the standardized @xmath0-knapsack capacity . to our knowledge , it is the first streaming algorithm that provides a constant - factor approximation guarantee with only monotone submodularity assumed . in our experiments , compared with the classical greedy algorithm developed in @xcite , the proposed streaming algorithm achieves over 10,000 times running time reduction with a similar performance .    the rest of this paper is organized as follows . in section",
    "ii we introduce the formulation and related existing results . in section",
    "iii we describe the proposed algorithms . in section",
    "iv we present two applications in news and scientific literature recommendations .",
    "we draw the conclusions in section v.",
    "let @xmath8 be the ground set and @xmath9 be a nonnegative set function on the subsets of @xmath10 . for any subset @xmath11 of @xmath10",
    ", we denote the characteristic vector of @xmath11 by @xmath12 , where for @xmath13 , @xmath14 , if @xmath15 ; @xmath16 , otherwise . for @xmath17 and @xmath18 , the marginal gain of @xmath19 with respect to @xmath11 and @xmath20",
    "is defined to be @xmath21 which quantifies the increase in the utility function @xmath22 when @xmath20 is added into subset @xmath11 .",
    "a function @xmath19 is submodular if it satisfies that for any @xmath23 and @xmath24 , the diminishing returns condition holds : @xmath25 also , @xmath19 is said to be a monotone function , if for any @xmath17 and @xmath18 , @xmath26 .",
    "for now , we adopt the common assumption that @xmath19 is given in terms of a black box that computes @xmath22 for any @xmath27 . in sections",
    "[ sectionspecial ] ,  [ sectiongeneral ] ,  [ sectiononline ] , we will discuss the case when the submodular function is independent @xcite of the ground set @xmath10 ( i.e. , for any @xmath17 , @xmath22 depends on only @xmath11 , not @xmath28 ) , and in section  [ sectiondependent ] , we will discuss the setting where the value of @xmath22 depends on not only the subset @xmath11 but also the ground set @xmath10 .",
    "next , we introduce the @xmath0-knapsack constraint .",
    "let @xmath29 be a @xmath0-dimensional budget vector , where for @xmath30 , @xmath31 is the budget corresponding to the @xmath32-th resource .",
    "let @xmath33 denote a @xmath34 matrix , whose @xmath35-th entry @xmath36 is the weight of the element @xmath37 with respect to the @xmath32-th knapsack resource constraint .",
    "then the @xmath0-knapsack constraint can be expressed by @xmath38 . the problem for maximizing a monotone submodular function @xmath39 subject to a @xmath0-knapsack constraint can be formulated as @xmath40 we aim to * *",
    "ma**ximize a monotone * * s**ubmodular set function subject to a @xmath0-**k**napsack constraint , which is called @xmath0-*mask * for short . without loss of generality , for @xmath41",
    ", we assume that @xmath42 .",
    "that is , no entry in @xmath43 has a larger weight than the corresponding knapsack budget , since otherwise the corresponding element is never selected into @xmath11 .    for the sake of simplicity , we here standardize problem  ( [ overallproblem ] ) .",
    "let @xmath44 for @xmath30 , @xmath13 , we replace each @xmath45 with @xmath46 and @xmath47 with @xmath48 we then create a new matrix @xmath49 by concatenating @xmath43 and @xmath50 over columns .",
    "that is , @xmath51 is a @xmath52 matrix , such that , for @xmath30 , @xmath53 if @xmath13 ; @xmath54 if @xmath55 .",
    "the standardized problem has the same optimal solution as problem  ( [ overallproblem ] ) . in the rest of the paper ,",
    "we only consider the standardized version of the @xmath0-mask problem .",
    "submodular optimization has been regarded as a powerful tool for combinatorial massive data mining and machine learning , for which a streaming algorithm processes the dataset piece by piece and then produces an approximate solution right at the end of the data stream .",
    "this makes it quite suitable to process a massive dataset in many applications .",
    "when @xmath56 and all entries of @xmath43 are ones , problem  ( [ overallproblem ] ) is equivalent to maximizing a monotone submodular function under a cardinality constraint .",
    "this optimization problem has been proved to be np - hard @xcite , and people have developed many approximation algorithms to solve this problem , among which the greedy algorithm @xcite is the most popular one .",
    "specifically , the greedy algorithm selects the element with the maximum marginal value at each step and produces a @xmath2-approximation guarantee with @xmath57 computation complexity , where @xmath58 is the maximum number of elements that the solution set can include , and @xmath59 is the number of elements in the ground set @xmath10 .",
    "recently , some accelerated algorithms were proposed in @xcite .",
    "unfortunately , neither of them can be applied to the case when the size of the dataset is over the capacity of the main memory .",
    "a streaming algorithm was developed in @xcite with a @xmath60-approximation of the optimal value , for any @xmath4 .",
    "this streaming algorithm does not require the full access to the dataset , and needs only one pass through the dataset .",
    "thus it provides a practical way to process a large dataset on the fly with a low memory requirement , but not applicable under a general @xmath0-knapsack constraint .",
    "further , the authors in @xcite dealt with the case when @xmath56 and each entry of @xmath43 can take any positive values . maximizing a monotone submodular function under a single knapsack constraint",
    "is also called a budgeted submodular maximization problem .",
    "this problem is also np - hard , and the authors in @xcite suggested a greedy algorithm , which produces a @xmath2-approximation of the optimal value with @xmath61 computation complexity .",
    "specifically , it first enumerates all the subsets of cardinalities at most three , then greedily adds the elements with maximum marginal values per weight to every subset starting with three elements , and finally outputs the suboptimal subset .",
    "although the solution has a @xmath2-approximation guarantee , the @xmath61 computation cost prevents this greedy algorithm from being widely used in practice .",
    "hence some modified versions of the greedy algorithm have been developed .",
    "the authors in @xcite applied it to document summarization with a @xmath62 performance guarantee . in @xcite , the so - called cost effective forward ( cef ) algorithm for outbreak detection",
    "was proposed , which produces a solution with a @xmath63-approximation guarantee and requires only @xmath64 computation complexity , where @xmath65 is the knapsack budget when @xmath56 .",
    "the considered @xmath0-mask problem is a generalization of the above problems to maximize a submodular function under more than one budgeted constraints .",
    "a framework was proposed in @xcite for maximizing a submodular function subject to a @xmath0-knapsack constraint , which yields a @xmath66-approximation for any @xmath4 . however , it is hard to implement this algorithm , since it involves some high - order terms with respect to the number of budgets , making it inappropriate for processing large datasets @xcite .",
    "later , an accelerated algorithm was developed in @xcite .",
    "it runs for @xmath67 rounds in mapreduce @xcite for a constant @xmath68 , and provides an @xmath69-approximation .",
    "however , this algorithm needs an @xmath70 blowup in communication complexity among various parts .",
    "as observed in @xcite , such a blowup decreases its applicability in practice .",
    "note that the authors in @xcite mentioned that the mapreduce method with an @xmath69-approximation can be extended to execute in a streaming fashion , but did not provide any concrete algorithms and the associated analysis .    [ cols=\"^,^,^,^,^ \" , ]     [ table1 ]    table  [ table1 ] shows the comparison among the approximation guarantees and computation costs of the aforementioned algorithms against our proposed algorithm .",
    "to our best knowledge , this paper is the first to propose an efficient streaming algorithm for maximizing a monotone submodular function under a @xmath0-knapsack constraint , with 1 ) a constant - factor approximation guarantee , 2 ) no assumption on full access to the dataset , 3 ) execution of a single pass , 4 ) @xmath71 memory requirement , 5 ) @xmath72 computation complexity per element , and 6 ) only assumption on monotonicity and submodularity of the objective function . in the following section ,",
    "we describe the proposed algorithm in details .",
    "we first consider a special case of the @xmath0-mask problem : maximizing a submodular function subject to one cardinality constraint : @xmath73 in @xcite , the authors proved this problem is np - hard and proposed a classical greedy algorithm . at each step of the algorithm ,",
    "as we explained earlier , the element with the largest marginal value is added to the solution set .",
    "this operation , in fact , reduces the `` gap '' to the optimal solution by a significant amount .",
    "formally , if element @xmath74 is added to the current solution set @xmath11 by the greedy algorithm , the marginal value @xmath75 of this picked element should be at least above certain threshold . in @xcite",
    ", the authors developed the so - called sieve - streaming algorithm , where the threshold for the marginal value is set to be @xmath76 , where @xmath11 is the current solution set , @xmath58 is the maximum allowed number of elements in @xmath11 , and opt is the optimal value of the optimization problem . in our paper , for this submodular maximization problem under a single cardinality constraint , we first introduce a simple streaming algorithm under the assumption that we have the knowledge of the optimal value of the problem .",
    "[ h ]    input : @xmath77 such that @xmath78 , for some @xmath79 $ ] .",
    "@xmath80 @xmath81 + @xmath82    [ th ] the simple streaming algorithm ( algorithm [ c ] ) produces a solution @xmath11 such that @xmath83    given @xmath84 $ ] , let us discuss the following two cases .    @xmath85 . for @xmath86 ,",
    "let @xmath87 be the element added to @xmath11 in the @xmath32-th iteration of the for - loop .",
    "then we obtain @xmath88.\\end{aligned}\\ ] ] by the condition in line 4 of algorithm 1 , for @xmath86 , we have @xmath89 and hence @xmath90    @xmath91 .",
    "let @xmath92 , where @xmath93 is the optimal solution to the problem ( [ specialcaseproblem ] ) .",
    "for each element @xmath94 , we have @xmath95 since @xmath19 is monotone submodular , we obtain @xmath96 < \\frac{v}{2k } \\cdot k \\leq \\frac{1}{2}f(s^*),\\end{aligned}\\ ] ] which implies that @xmath97    this simple streaming algorithm produces a solution by visiting every element in the ground set only once .",
    "but it requires the knowledge of the optimal value of the problem . besides , when the elements have non - uniform weights , this algorithm does not work . to deal with the problem with non - uniform weights and more than one constraint",
    ", we are going to modify the greedy rule and take the weight - dependent marginal values into account in a streaming fashion .      in order to get the desirable output , in this subsection",
    ", we first assume we have some knowledge of , and then remove this assumption by estimating based on the maximum value per weight of any single element . at the end",
    ", we will remove all assumptions to develop the final version of the streaming algorithm for the general case of a @xmath0-mask problem .",
    "suppose that we know a value @xmath77 such that @xmath78 for some @xmath98 .",
    "that is , we know an approximation of @xmath99 up to a constant factor @xmath100 .",
    "we then construct the following algorithm to choose a subset @xmath11 with the knowledge of the optimal value of the problem .",
    "[ h ]    input : @xmath77 such that @xmath78 , for some @xmath79.$ ] @xmath80 @xmath101 * return * @xmath11 .",
    "@xmath102 + @xmath11 .    at the beginning of the algorithm ,",
    "the solution set @xmath11 is set to be an empty set .",
    "the algorithm will terminate when either we find an element @xmath103 satisfying @xmath104,\\ ] ] or we finish one pass through the dataset . here",
    "we define that an element @xmath37 is a _ big element _ if it satisfies ( [ defbigelement ] ) .",
    "when the algorithm finds a big element @xmath105 , it simply outputs @xmath106 and terminates .",
    "the following lemma shows that @xmath106 is already a good enough solution .",
    "[ lemmabigelement]assume the input @xmath77 satisfies @xmath78 , and @xmath10 has at least one big element .",
    "the output @xmath11 of algo- satisfies @xmath107    let @xmath105 be the first big element that algorithm  2 finds .",
    "then @xmath106 is output and the algorithm terminates .",
    "therefore , by ( [ defbigelement ] ) , we have @xmath108    when @xmath10 does not contain any big elements , during the data streaming , an element @xmath74 is added to the solution set @xmath11 if 1 ) the marginal value per weight for each knapsack constraint @xmath109 is at least @xmath110 for @xmath30 , and 2 ) the overall @xmath0-knapsack constraint is still satisfied . in this paper",
    ", we set @xmath111 , which gives us the best approximation guarantee as shown in the proof of theorem  [ th1 ] .",
    "the following lemma shows the property of the output of algorithm 2 .",
    "[ 2p ] assume that @xmath10 has no big elements .",
    "the output @xmath11 of algorithm  [ d-1 ] has the following two properties :    1 .",
    "there exists an ordering @xmath112 of the elements in @xmath11 , such that for all @xmath113 and @xmath114 , we have @xmath115 where @xmath116 .",
    "2 .   assume that for @xmath114 , @xmath117 .",
    "then for each @xmath118 , there exists an index @xmath119 , with @xmath120 such that @xmath121    \\1 ) for @xmath122 , at the @xmath123-th step of the algorithm , assume that @xmath124 is the element added to the current solution set @xmath125 .",
    "then @xmath126 forms an ordering satisfying ( [ condition1 ] ) .",
    "\\2 ) by contradiction , assume that there exists @xmath103 such that for @xmath30 , we have @xmath127 since @xmath74 is not a big element and @xmath19 is submodular , we have @xmath128 , for @xmath30 . then @xmath74 can be added into @xmath11 , where a contradiction occurs .",
    "we then establish the following theorem to show that algorithm  [ d-1 ] produces an @xmath129-approximation of the optimal solution to problem ( [ overallproblem ] ) .",
    "[ th1 ] assuming that the input @xmath77 satisfies @xmath78 , algorithm [ d-1 ] has the following properties :    * it outputs @xmath11 that satisfies @xmath130 ; * it only goes one pass over the dataset , stores at most @xmath131 elements , and has @xmath132 computation complexity per element .",
    "if @xmath10 contains at least one big element , by lemma  [ lemmabigelement ] , we have @xmath133 otherwise , we discuss the following two cases :    @xmath134 , for some @xmath135 $ ] . by the submodularity of @xmath19 and property 1 ) in lemma  [ 2p ] , we have @xmath136    @xmath137 , for all @xmath135 $ ] .",
    "let @xmath138 be the set of elements @xmath139 such that @xmath140 , for @xmath114 .",
    "then we have @xmath141 with the help of the submodularity of @xmath19 and property 2 ) in lemma  [ 2p ] , we obtain @xmath142 for @xmath30",
    ". then we have @xmath143 < \\frac{2dv}{1 + 2d } , \\end{aligned}\\end{aligned}\\ ] ] and further , @xmath144    in both cases , we conclude @xmath145    since we have @xmath146 for all @xmath135 $ ] , @xmath147",
    "$ ] , we store at most @xmath131 elements during the algorithm . in the for - loop , we compare the values at most @xmath0 times .",
    "then the computation cost per element in the algorithm is @xmath132 .",
    "we can obtain an approximation of the optimal value opt by solving the @xmath0-mask problem via algorithm [ d-1 ] .",
    "but in certain scenarios , requiring the knowledge of an approximation to the optimization problem and utilizing the approximation in algorithm [ d-1 ] lead to a chicken and egg dilemma .",
    "that is , we have to first estimate @xmath99 and then use it to compute @xmath99 .",
    "fortunately , even in such scenarios , we still have the following lemma to estimate @xmath99 if we know @xmath148 , the maximum value per weight of any single element .    [ l3 ]",
    "let @xmath149^l \\right.|l \\in \\mathbb{z } , \\\\ & \\frac{m}{1+(1 + 2d)\\epsilon}\\leq [ 1+(1 + 2d)\\epsilon]^l \\leq bm\\big\\}\\end{aligned}\\ ] ] for some @xmath150 with @xmath151 .",
    "then there exists at least some @xmath152 such that @xmath153\\textrm{opt } \\leq v \\leq \\textrm{opt}$ ] .",
    "first , choose @xmath154,$ ] @xmath155 $ ] such that @xmath156 since @xmath157 , we have @xmath158 also , let @xmath159 be a subset of @xmath10 such that @xmath160 .",
    "then by the submodularity of @xmath19 , @xmath161\\\\ & \\le f(\\emptyset)+\\sum_{i=1}^t [ f(\\{j_i\\})-f(\\emptyset)]\\\\ & \\le \\sum_{i=1}^t f(\\{j_i\\})\\le m\\sum_{i=1}^tc_{1,j_i}\\le bm.\\end{aligned}\\ ] ] setting @xmath162^{\\left\\lfloor\\log_{1+(1 + 2d)\\epsilon}\\textrm{opt}\\right\\rfloor}$ ] , we then obtain @xmath163 and @xmath164\\textrm{opt}.\\ ] ]    based on lemma  [ l3 ] , we propose the following algorithm that gets around the chick and egg dilemma .",
    "input : @xmath165 .",
    "l \\in\\mathbb{z } , $ ] @xmath167^l \\leq b m\\}$ ] .",
    "@xmath168 @xmath101 * return * @xmath11 . @xmath169 @xmath170",
    ". + @xmath11",
    ".    then we establish the following theorem to show that the above algorithm achieves a @xmath1-approximation guarantee , and requires @xmath171 memory and @xmath6 computation complexity per element .",
    "[ th2 ] with @xmath165 known , algorithm [ d-2 ] has the following properties :    * it outputs @xmath11 that satisfies @xmath172 ; * it goes one pass over the dataset , stores at most @xmath5 elements , and has @xmath6 computation complexity per element .    by lemma  [ l3 ] , we choose @xmath173 such that + @xmath153\\textrm{opt}\\le v\\le \\textrm{opt}$ ] .",
    "then by theorem [ th1 ] , the output @xmath11 satisfies @xmath174    notice that there are at most @xmath175 ( of order @xmath176 ) elements in @xmath177 . at the end of the algorithm ,",
    "@xmath178 with the largest function value will be picked to be the output . since @xmath11 contains at most @xmath7 elements , algorithm  [ d-2 ] stores at most @xmath5 elements and has @xmath6 computation complexity per element .",
    "introducing the maximum marginal value per weight @xmath165 avoids the chicken and egg dilemma in algorithm [ d-1 ] . with",
    "@xmath165 known , algorithm [ d-2 ] needs only one pass over the dataset . however , we need an extra pass through the dataset to obtain the value of @xmath165 . in the following , we will develop our final one - pass streaming algorithm with @xmath165 unknown .",
    "@xmath179^l | l \\in\\mathbb{z}\\}$ ] .",
    "@xmath168 @xmath180 @xmath101 * return * @xmath82 @xmath181 .",
    "l \\in\\mathbb{z},$ ] @xmath182^l \\leq",
    "2bm \\}$ ] . @xmath169 @xmath170 .",
    "+ @xmath11 .",
    "we modify the estimation candidate set @xmath177 into @xmath183^l |",
    "l \\in\\mathbb{z } , \\frac{m}{1+(1 + 2d)\\epsilon } \\leq [ 1+(1 + 2d)\\epsilon]^l \\leq 2bm\\}$ ] , and maintain the variable @xmath165 that holds the current maximum marginal value per weight of all single element . during the data streaming ,",
    "if a big element @xmath105 is observed , the algorithm simply outputs @xmath106 and terminates .",
    "otherwise , the algorithm will update @xmath165 and the estimation candidate set @xmath177 .",
    "if the marginal value per weight for each knapsack constraint @xmath109 is at least for @xmath30 , and the overall @xmath0-knapsack constraint is still satisfied , then an element @xmath74 is added to the corresponding candidate set",
    ". then we establish the following theorem , which shows the property of the output of algorithm 4 .",
    "its proof follows the same lines as the proof of theorem  [ th2 ] .",
    "[ th4 ] algorithm [ d-3 ] has the following properties :    * it outputs @xmath11 that satisfies that @xmath172 ; * it goes one pass over the dataset , stores at most @xmath5 elements , and has @xmath6 computation complexity per element .      to evaluate the performance of our proposed algorithms",
    ", we need to compare the function values obtained by our streaming algorithm against opt , by calculating their relative difference .",
    "since opt is unknown , we could use an upper bound of opt to evaluate the performance of the proposed algorithms .    by theorem  [ th4 ]",
    ", we obtain @xmath184 then @xmath185 is an upper bound of the optimal value to the @xmath0-mask problem . in most of cases ,",
    "this bound is not tight enough . in the following ,",
    "we provide a much tighter bound derived by the submodularity of @xmath19 .",
    "[ theoremforoutbound ] consider a subset @xmath27 .",
    "for @xmath30 , let @xmath186 , and @xmath187 be the sequence such that @xmath188 let @xmath189 be the integer such that @xmath190 and @xmath191 . and let @xmath192 .",
    "then we have @xmath193.\\end{aligned}\\ ] ]    here we use a similar proof as the proof of theor in @xcite , where the author deals with the submodular maximization problem under one knapsack constraint .",
    "let @xmath93 be the optimal solution to problem  ( [ overallproblem ] ) .",
    "first we consider the @xmath194-mask problem , which has the same objective function as problem  ( [ overallproblem ] ) but only with the @xmath32-th knapsack constraint .",
    "assume @xmath195 is its optimal solution .",
    "since this @xmath194-mask problem has fewer constraints than problem  ( [ overallproblem ] ) , we have @xmath196 .",
    "hence , @xmath197 since @xmath19 is monotone submodular , for @xmath30 , @xmath198 we first assume that all weights @xmath45 and knapsack @xmath7 are rational numbers . for the @xmath32-th @xmath194-mask problem",
    ", we can multiply all @xmath45 and @xmath7 by the least common multiple of their denominators , making each weight and budget be an integer .",
    "we then replicate each element @xmath199 in @xmath10 into @xmath200 copies .",
    "let @xmath201 denote any one copy of @xmath199 , and let @xmath202 and @xmath203 be the sets of the copies of all elements in @xmath10 and @xmath204 , respectively . also , define @xmath205 . then @xmath206 to find the value of the right - hand side of ( [ bound3 ] )",
    ", we actually need to solve a unit - cost modular optimization problem as follows .",
    "we first sort all elements @xmath207 in @xmath202 such that the corresponding values @xmath208 form a non - increasing sequence . in this sequence ,",
    "the first @xmath7 elements are @xmath209 copies of @xmath210 for @xmath211 , and @xmath212 copies of @xmath213 .",
    "therefore , we obtain @xmath214 combining ( [ ex ] ) , ( [ bound2 ] ) , ( [ bound3 ] ) and ( [ bound4 ] ) , we obtain ( [ outboundtheorem ] ) .    for irrational weights and knapsacks , let @xmath215 and @xmath216 be two rational sequences with limits @xmath209 and @xmath7 , respectively . and further let @xmath217 be the integer such that @xmath218 and @xmath219 , and let @xmath220 then @xmath221 is a rational sequence with limit @xmath222 . according to the above argument",
    ", we obtain for each @xmath223 , @xmath224.\\end{aligned}\\ ] ] by letting @xmath223 go to infinity , we then finish the proof .",
    "a bound is called to be _ offline _ @xcite",
    "if it can be stated before we run the algorithm ; otherwise , it is an _ online _",
    "one @xcite . here",
    ", we obtain an offline bound ( [ theofflinebound ] ) and an online bound ( [ outboundtheorem ] ) , the latter of which can be calculated by the following algorithm .",
    "* input * : @xmath82 @xmath225 .",
    "@xmath226 @xmath227 .",
    "@xmath230 . + @xmath231 .      in the previous sections ,",
    "we have discussed the case when the submodular function @xmath19 is independent of the ground set @xmath10 . in the following",
    ", we will discuss the setting where @xmath19 is additively decomposable @xcite , and the value of @xmath22 depends on not only the subset @xmath11 but also the ground set @xmath10 . here",
    "a function @xmath19 is called to be _ additively decomposable _ @xcite over the ground set @xmath10 , if there exists a family of functions @xmath232 with @xmath233 independent of the ground set @xmath10 such that @xmath234 algorithm 4 is still useful for the case when @xmath19 is dependent on the ground set but additively decomposable . to reduce the computational complexity",
    ", we randomly choose a small subset @xmath235 of @xmath10 , and use @xmath236 instead of @xmath19 in algorithm @xmath237 .",
    "it can be proved that with a high probability , we can still obtain a good approximation to the optimal solution , when @xmath238 s are bounded .",
    "the accuracy of the approximation is quantified by the following theorem .    [ bx ] assume that for @xmath17 and @xmath239 , @xmath240 .",
    "we uniformly choose a subset @xmath235 from @xmath10 , with @xmath241 and use @xmath242 instead of @xmath19 in algorithm [ d-3 ] .",
    "then with probability of at least @xmath243 , the output @xmath11 of algorithm [ d-3 ] satisfies @xmath244    its proof follows the similar argument as the proof of theorem 6.2 in @xcite , where the authors deal with the submodular maximization problem under one cardinality constraint .",
    "now we adopt a two - pass streaming algorithm for the @xmath0-mask problem with ground - set dependent submodular objective functions : in the first pass , we utilize reservoir sampling @xcite to sample an evaluation set @xmath235 randomly ; in the second pass , we run algorithm [ d-3 ] with the objective function @xmath242 instead of @xmath19 .",
    "in this section , we discuss two real - world applications for algorithm [ d-3 ] : news recommendation and scientific literature recommendation .",
    "nowadays , people are facing many news articles on the daily basis , which highly stresses their limited reading time .",
    "a news recommendation system helps people quickly fetch the information they need .",
    "specifically , it provides the most relevant and diversified news to people by exploiting their behaviors , considering their reading preferences , and learning from their previous reading histories .",
    "however , the vast amount of news articles in the dataset are hard to be processed efficiently . in @xcite",
    ", the authors modeled the user behavior as a submodular maximization problem .",
    "based on the learning result , a classical greedy algorithm @xcite was implemented to provide a set of relevant articles to the users .",
    "however , the large amount of data in the dataset prevents the classical greedy algorithm from producing the solution in time due to its expensive computation cost . besides",
    ", the reading behavior of the users was oversimplified in @xcite , where it is assumed that each user reads a fixed number of articles per day .",
    "since the time spent on different news articles varies , it is more reasonable to use the number of words of the articles as the measure of the reading behaviour .",
    "hence , we can formulate this question into a @xmath194-mask problem as follows : @xmath245 where @xmath246 is the number of words in article @xmath74 .",
    "here @xmath247 , where @xmath165 is the number of features .",
    "we require the total number of words in the selected articles not to exceed a specified budget @xmath7 , due to the limitation of the user reading time .",
    "in addition , we assume that the non - negative parameter vector @xmath248 is learnt by a statistical learning algorithm , based on the historical user preference ( three such learning algorithms can be found in @xcite , @xcite , and @xcite , respectively ) . let @xmath249 be the characteristic vector of article @xmath0 , where for @xmath250 , @xmath251 if @xmath0 has feature @xmath74 , @xmath252 , otherwise .",
    "we then define @xmath253 ; here for @xmath250 , @xmath254 is the aggregation function of @xmath11 with respect to feature @xmath74 and defined by @xmath255 this choice of function @xmath256 guarantees both precision and coverage of the solution set .",
    "on one hand , the monotonicity of @xmath254 encourages feature @xmath74 to be selected if its corresponding weighting parameter @xmath257 ( the @xmath74-th coordinate of the vector @xmath248 ) is relatively large . on the other hand ,",
    "the diminishing return property of @xmath256 prevents too many items with feature @xmath74 from being selected .",
    "notice that function @xmath256 is a monotone submodular function . to see this ,",
    "let @xmath258 obviously , @xmath259 is a non - decreasing modular function . with the fact that @xmath260 is an increasing concave function , we can conclude that @xmath261 is a monotone submodular function . since both monotonicity and submodularity are closed under the non - negative linear combinations @xcite , @xmath19 is a monotone submodular function as well .",
    "the solution based on algorithm [ d-3 ] to this @xmath194-mask problem provides the user a quick news recommendation .    as an illustration",
    ", we analyze the dataset collected in @xcite , which contains over @xmath262 feedback entries from @xmath263 people with around @xmath264 news articles .",
    "we set @xmath265 and @xmath266 , with each entry of @xmath43 randomly chosen from a uniform distribution over @xmath267 .",
    "the learning algorithm proposed in @xcite is used to calculate @xmath248 .",
    "we then compare algorithm [ d-3 ] with the greedy algorithm in @xcite .        in fig .",
    "[ fig : bar1 ] , we set the objective function value obtained by the classical greedy algorithm and its computation time both to be @xmath194 , after using them to normalize the function value and computation time corresponding to our streaming algorithm , respectively .",
    "it has been shown that our streaming algorithm achieves @xmath268 utility of the greedy algorithm , but only requires a tiny fraction of the computation cost .",
    "thus the proposed algorithm works well in the news recommendation system and is practically useful over large datasets .",
    "next , we introduce an application in scientific literature recommendation . nowadays",
    ", the researchers have to face an enormous amount of articles and collect information that they are interested in , where they have to filter the massive existing scientific literatures and pick the most useful ones . a common approach to locate the targeted literatures",
    "is based on the so - called citation networks @xcite .",
    "the authors in @xcite mapped a citation network onto a rating matrix to filter research papers . in @xcite ,",
    "an algorithm utilizing the random - walker properties was proposed .",
    "it transforms a citation matrix into a probability transition matrix and outputs the entries with the highest biased pagerank scores .",
    "we here propose a new scientific literature recommendation system based on the citation networks and the newly proposed streaming algorithm ( algorithm [ d-3 ] ) .",
    "consider a directed acyclic graph @xmath269 with @xmath8 , where each vertex in @xmath10 represents a scientific article .",
    "let @xmath270 denote the number of references contained in article @xmath32 .",
    "the arcs between papers represent their citation relationship . for two vertices @xmath271 , arc @xmath272 if and only if paper @xmath32 cites paper @xmath74 . the information spreads over the reverse directions of the arcs . as an example , fig .",
    "[ fig : example ] presents a citation network , which contains six vertices and seven arcs .",
    "each of six papers cites a certain number of references .",
    "the information initiates from a set of vertices ( source papers ) , and then spreads across the network .",
    "let @xmath273 be the collection of the source papers .",
    "our target is to select a subset @xmath11 out of @xmath10 to quickly detect the information spreading of @xmath273 .",
    "for example , @xmath274 in fig .",
    "[ fig : example ] .",
    "if we choose @xmath275 , we can detect the source papers @xmath276 by paths @xmath277 , @xmath278 and @xmath279 , respectively .",
    "this problem can be formulated as a monotone submodular maximization under a @xmath280-knapsack constraint will be explained later in this section ; based on the different usages , the number of knapsack constraints and the corresponding budgets can be changed accordingly . ] : @xmath281 where @xmath33 is a @xmath282 matrix and @xmath283 .",
    "observe that the papers in @xmath273 transfer their influence through the citation network , but this influence becomes less as it spreads through more hops .",
    "let @xmath284 be the length of the shortest directed path from @xmath199 to @xmath105 .",
    "then the shortest path length from any vertex in @xmath11 to @xmath105 is defined as @xmath285 let @xmath286 be a pre - assigned weight to each vertex @xmath287 such that @xmath288 .",
    "then our goal is to minimize the expected penalty @xmath289 or maximize the expected penalty reduction @xmath290^+,\\ ] ] where @xmath291^+\\triangleq \\max\\{x,0\\}$ ] and @xmath292 is a given maximum penalty .",
    "note that @xmath293 is a monotone submodular function . to see this , for two subsets @xmath294",
    ", we have @xmath295 for any @xmath287 , such that @xmath296 ; @xmath297 is a submodular function with respect to @xmath11 since @xmath298^+ \\\\ & \\ge [ t(c , a)-t(v , a)]^+=t(c , a)-t(c\\cup \\{v\\},a),\\end{aligned}\\ ] ] with @xmath299 .",
    "then @xmath300 is also submodular , since it is a convex combination of @xmath297 for @xmath287 .",
    "we construct three constraints in ( [ litrec ] ) from the aspects of recency , biased pagerank score , and reference number respectively .",
    "the first aspect is from the fact that readers prefer to read the recently published papers .",
    "let @xmath301 be the time difference between the publishing date of paper @xmath74 and the current date , and @xmath302 be the corresponding limit .",
    "for the second aspect , the classical pagerank algorithm @xcite could be used to compute an important score for every vertex in the graph : a vertex will be assigned a higher score if it is connected to a more important vertex with a lower out - degree .",
    "the authors in @xcite introduced a so - called biased pagerank score .",
    "it is a measure of the significance of each paper , not only involving the propagation and attenuation properties of the network , but also taking the set of source vertices into account .",
    "let @xmath303 be the biased pagerank score of article @xmath74 .",
    "we further choose a function @xmath304 to map the pagerank score onto @xmath305 $ ] .",
    "then paper @xmath74 with the smaller value @xmath306 is more valuable for the researchers .",
    "also we set @xmath307 to be corresponding budget .",
    "thirdly , we assume that more references listed in the paper , more time the reader spends on picking the valuable information",
    ". then we set @xmath308 to be the number @xmath309 of references in paper @xmath74 and @xmath310 be the budget of the total number of references .",
    "to evaluate the performance of algorithm [ d-3 ] , for scientific literature recommendation , we utilize a dataset collected in @xcite .",
    "this dataset includes more than 20,000 papers in the association of computational linguistics ( acl ) .",
    "there are two methods to evaluate the performance of an algorithm for literature recommendation : online evaluation and offline evaluation . in the online evaluation ,",
    "some volunteers are invited to test the performance of the recommendation system and express their opinions .",
    "here we use the offline evaluation to compare the function values obtained by our proposed algorithm ( algorithm [ d-3 ] ) and the pagerank algorithm proposed in @xcite .",
    "we perform the sensitive analysis over different knapsack constraints . with the other two constraints fixed",
    ", we change the value of the budget corresponding to the recency , biased pagerank score or reference number , respectively . here",
    "we randomly select five nodes as the source papers .",
    "we set @xmath311 and @xmath312 for each source paper @xmath105 .",
    "the results for the optimal objective values are shown in fig .",
    "[ fig : b1 ] ( with fixed @xmath313 , @xmath314 ) , fig .",
    "[ fig : b2 ] ( with fixed @xmath315 , @xmath314 ) and fig .",
    "[ fig : b3 ] ( with fixed @xmath315 , @xmath313 ) , respectively .",
    "it can be observed that the relative difference is around 10% between the function values obtained by our streaming algorithm ( blue lines ) and the corresponding online bounds ( red lines ) .",
    "also , we find that our algorithm highly outperforms the biased pagerank algorithm as shown in figs .",
    "[ fig : b1 ] , [ fig : b2 ] and [ fig : b3 ] , respectively .",
    "although the biased pagerank algorithm suggests the papers with high biased pagerank scores , most of the suggested papers have very long distances from the set of source articles ( even disconnected from the source papers in some cases ) , which leads to a very low objective function value .",
    "in this paper , we proposed a streaming algorithm to maximize a monotone submodular function under a @xmath0-knapsack constraint .",
    "it leads to a @xmath316 approximation of the optimal value , and requires only a single pass through the dataset and a small memory size .",
    "it achieves a major fraction of the utility function value obtained by the greedy algorithm with a much lower computation cost , which makes it very practically implementable .",
    "our algorithm provides a more efficient way to solve the related combinatorial optimization problems , which could find many good applications , such as in news and scientific literature recommendations as shown in the paper .",
    "we thank dr .",
    "nao kakimura for comments that greatly improved the manuscript .",
    "99 h.  lin and j.  bilmes , `` how to select a good training - data subset for transcription : submodular active selection for sequences , '' _ proc .",
    "2009 annu .",
    "speech commun .",
    "_ , brighton , uk , sept . 2009 .",
    "y.  liu , k.  wei , k.  kirchhoff , y.  song , and j.  bilmes , `` submodular feature selection for high - dimensional acoustic score spaces , '' _ proc .",
    "2013 ieee int .",
    "acoust . speech signal process .",
    "_ , pp . 71847188 , may 2013 .",
    "a.  badanidiyuru , b.  mirzasoleiman , a.  karbasi , and a.  krause , `` streaming submodular maximization : massive data summarization on the fly '' , _ proc .",
    "20th acm sigkdd int .",
    "data mining _ , pp .",
    "671680 , new york , ny , aug . 2014 .",
    "a.  kulik , h.  shachnai , and t.  tamir , `` maximizing submodular set functions subject to multiple linear constraints , '' _ proc .",
    "20th annu .",
    "acm - siam symp",
    ". discrete algor .",
    "_ , pp . 545554 , new york , ny , jan . 2009 .",
    "b.  mirzasoleiman , a.  karbasi , r.  sarkar , and a.  krause , `` distributed submodular maximization : identifying representative elements in massive data , '' _ proc .",
    "neural inf . process .",
    "_ , pp . 20492057 , dec .",
    "2013 .",
    "j.  leskovec , a.  krause , c.  guestrin , c.  faloutsos , j.  vanbriesen , and n.  glance , `` cost - effective outbreak detection in networks , '' _ proc .",
    "13th acm sigkdd int .",
    "data mining _ , pp .",
    "420429 , san jose , ca , aug .",
    "2007 .",
    "r.  kumar , b.  moseley , s.  vassilvitskii , and a.  vattani , `` fast greedy algorithms in mapreduce and streaming , '' _ proc .",
    "25th annu .",
    "parallelism algor .",
    "_ , pp . 110 , montreal , qc , june 2013 .",
    "k.  raman , p.  shivaswamy , and t.  joachims , `` online learning to diversify from implicit feedback , '' _ proc .",
    "18th acm sigkdd int .",
    "data mining _ , pp .",
    "705713 , beijing , china , aug .",
    "l.  li , d.  wang , t.  li , d.  knox , and b.  padmanabhan , `` scene : a scalable two - stage personalized news recommendation system , '' _ proc .",
    "34th annu .",
    "acm sigir conf .",
    "_ , pp . 125134 , beijing , china , july 2011 .",
    "s.  m.  mcnee , i.  albert , d.  cosley , p.  gopalkrishnan , s.  k.  lam , a. rashid , j.  a.  konstan , and j.  riedl , `` on the recommending of citations for research papers , '' _ proc .",
    "2002 acm conf .",
    "comput . support . coop .",
    "work _ , pp .",
    "116125 , new orleans , la , nov . 2002 ."
  ],
  "abstract_text": [
    "<S> submodular maximization problems belong to the family of combinatorial optimization problems and enjoy wide applications . in this paper , we focus on the problem of maximizing a monotone submodular function subject to a @xmath0-knapsack constraint , for which we propose a streaming algorithm that achieves a @xmath1-approximation of the optimal value , while it only needs one single pass through the dataset without storing all the data in the memory . in our experiments , </S>",
    "<S> we extensively evaluate the effectiveness of our proposed algorithm via two applications : news recommendation and scientific literature recommendation . it is observed that the proposed streaming algorithm achieves both execution speedup and memory saving by several orders of magnitude , compared with existing approaches . </S>"
  ]
}