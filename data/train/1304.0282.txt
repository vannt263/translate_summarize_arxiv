{
  "article_text": [
    "we consider independent and identically distributed data vectors @xmath2 that obey the regression model @xmath3 where @xmath4 is the main regressor and coefficient @xmath5 is the main parameter of interest .",
    "the vector @xmath6 denotes other high - dimensional regressors or controls .",
    "the regression error @xmath7 is independent of @xmath8 and @xmath6 and has median zero , that is , @xmath9 .",
    "the distribution function of @xmath7 is denoted by @xmath10 and admits a density function @xmath11 such that @xmath12 .",
    "the assumption motivates the use of the least absolute deviation or median regression , suitably adjusted for use in high - dimensional settings .",
    "the framework ( [ eq : direct ] ) is of interest in program evaluation , where @xmath4 represents the treatment or policy variable known a priori and whose impact we would like to infer @xcite .",
    "we shall also discuss a generalization to the case where there are many parameters of interest , including the case where the identity of a regressor of interest is unknown a priori .",
    "the dimension @xmath13 of controls @xmath6 may be much larger than @xmath1 , which creates a challenge for inference on @xmath5 . although the unknown nuisance parameter @xmath14 lies in this large space , the key assumption that will make estimation possible is its sparsity , namely @xmath15 has @xmath16 elements , where the notation @xmath17 denotes the support of a vector @xmath18 . here",
    "@xmath19 can depend on @xmath1 , as we shall use array asymptotics .",
    "sparsity motivates the use of regularization or model selection methods .",
    "a non - robust approach to inference in this setting would be first to perform model selection via the @xmath20-penalized median regression estimator @xmath21 where @xmath22 is a penalty parameter and @xmath23 is a diagonal matrix with normalization weights , where the notation @xmath24 denotes the average @xmath25 over the index @xmath26 .",
    "then one would use the post - model selection estimator @xmath27 to perform inference for @xmath5 .",
    "this approach is justified if ( [ def : l1qr ] ) achieves perfect model selection with probability approaching unity , so that the estimator ( [ def : postl1qr ] ) has the oracle property .",
    "however conditions for perfect selection are very restrictive in this model , and , in particular , require strong separation of non - zero coefficients away from zero .",
    "if these conditions do not hold , the estimator @xmath28 does not converge to @xmath5 at the @xmath29 rate , uniformly with respect to the underlying model , and so the usual inference breaks down @xcite .",
    "we shall demonstrate the breakdown of such naive inference in monte carlo experiments where non - zero coefficients in @xmath14 are not significantly separated from zero .",
    "the breakdown of standard inference does not mean that the aforementioned procedures are not suitable for prediction .",
    "indeed , the estimators ( [ def : l1qr ] ) and ( [ def : postl1qr ] ) attain essentially optimal rates @xmath30 of convergence for estimating the entire median regression function @xcite .",
    "this property means that while these procedures will not deliver perfect model recovery , they will only make moderate selection mistakes , that is , they omit controls only if coefficients are local to zero .    in order to provide uniformly valid inference , we propose a method whose performance does not require perfect model selection , allowing potential moderate model selection mistakes .",
    "the latter feature is critical in achieving uniformity over a large class of data generating processes , similarly to the results for instrumental regression and mean regression studied in @xcite and @xcite .",
    "this allows us to overcome the impact of moderate model selection mistakes on inference , avoiding in part the criticisms in @xcite , who prove that the oracle property achieved by the naive estimators implies the failure of uniform validity of inference and their semiparametric inefficiency @xcite .    in order to achieve robustness with respect to moderate selection mistakes",
    ", we shall construct an orthogonal moment equation that identifies the target parameter . the following auxiliary equation,@xmath31 which describes the dependence of the regressor of interest @xmath8 on the other controls @xmath32 , plays a key role .",
    "we shall assume the sparsity of @xmath33 , that is , @xmath34 has at most @xmath16 elements , and estimate the relation ( [ eq : indirect ] ) via lasso or post - lasso least squares methods described below .",
    "we shall use @xmath35 as an instrument in the following moment equation for @xmath5 : @xmath36 where @xmath37 .",
    "we shall use the empirical analog of ( [ eq : est equation ] ) to form an instrumental median regression estimator of @xmath5 , using a plug - in estimator for @xmath38 .",
    "the moment equation ( [ eq : est equation ] ) has the orthogonality property @xmath39 so the estimator of @xmath5 will be unaffected by estimation of @xmath38 even if @xmath14 is estimated at a slower rate than @xmath29 , that is , the rate of @xmath40 would suffice .",
    "this slow rate of estimation of the nuisance function permits the use of non - regular estimators of @xmath14 , such as post - selection or regularized estimators that are not @xmath29 consistent uniformly over the underlying model .",
    "the orthogonalization ideas can be traced back to @xcite and also play an important role in doubly robust estimation @xcite .",
    "our estimation procedure has three steps : ( i ) estimation of the confounding function @xmath38 in ( [ eq : direct ] ) ; ( ii ) estimation of the instruments @xmath35 in ( [ eq : indirect ] ) ; and ( iii ) estimation of the target parameter @xmath5 via empirical analog of ( [ eq : est equation ] ) .",
    "each step is computationally tractable , involving solutions of convex problems and a one - dimensional search .",
    "step ( i ) estimates for the nuisance function @xmath38 via either the @xmath20-penalized median regression estimator ( [ def : l1qr ] ) or the associated post - model selection estimator ( [ def : postl1qr ] ) .    step ( ii ) provides estimates @xmath41 of @xmath42 in ( [ eq : indirect ] ) as @xmath43 or @xmath44 @xmath45 .",
    "the first is based on the heteroscedastic lasso estimator @xmath46 , a version of the lasso of @xcite , designed to address non - gaussian and heteroscedastic errors @xcite , @xmath47 where @xmath22 and @xmath48 are the penalty level and data - driven penalty loadings defined in the supplementary material .",
    "the second is based on the associated post - model selection estimator and @xmath49 , called the post - lasso estimator : @xmath50.\\ ] ]    step ( iii ) constructs an estimator @xmath51 of the coefficient @xmath5 via an instrumental median regression @xcite , using @xmath52 as instruments , defined by @xmath53 where @xmath54 is a possibly stochastic parameter space for @xmath5 .",
    "we suggest @xmath55 $ ] with @xmath56 , though we allow for other choices .",
    "our main result establishes that under homoscedasticity , provided that @xmath57 and other regularity conditions hold , despite possible model selection mistakes in steps ( i ) and ( ii ) , the estimator @xmath51 obeys @xmath58 in distribution , where @xmath59 with @xmath60 is the semi - parametric efficiency bound for regular estimators of @xmath5 . in the low - dimensional case , if @xmath61 , the asymptotic behavior of our estimator coincides with that of the standard median regression without selection or penalization , as derived in @xcite , which is also semi - parametrically efficient in this case .",
    "however , the behaviors of our estimator and the standard median regression differ dramatically , otherwise , with the standard estimator even failing to be consistent when @xmath62 .",
    "of course , this improvement in the performance comes at the cost of assuming sparsity .",
    "an alternative , more robust expression for @xmath63 is given by @xmath64 we estimate @xmath65 by the plug - in method and @xmath66 by powell s ( @xcite ) method .",
    "furthermore , we show that the neyman - type projected score statistic @xmath67 can be used for testing the null hypothesis @xmath68 , and converges in distribution to a @xmath69 variable under the null hypothesis , that is , @xmath70 in distribution .",
    "this allows us to construct a confidence region with asymptotic coverage @xmath71 based on inverting the score statistic @xmath67 : @xmath72 where @xmath73 is the @xmath74-quantile of the @xmath75-distribution .    the robustness with respect to moderate model selection mistakes , which is due to ( [ eq : explain : robustness ] ) , allows ( [ eq : result1 ] ) and ( [ eq : inferenceln ] ) to hold uniformly over a large class of data generating processes . throughout the paper ,",
    "we use array asymptotics , asymptotics where the model changes with @xmath1 , to better capture finite - sample phenomena such as small coefficients that are local to zero .",
    "this ensures the robustness of conclusions with respect to perturbations of the data - generating process along various model sequences .",
    "this robustness , in turn , translates into uniform validity of confidence regions over many data - generating processes .",
    "the second set of main results addresses a more general setting by allowing @xmath0-dimensional target parameters defined via huber s z - problems to be of interest , with dimension @xmath0 potentially much larger than the sample size @xmath1 , and also allowing for approximately sparse models instead of exactly sparse models .",
    "this framework covers a wide variety of semi - parametric models , including those with smooth and non - smooth score functions .",
    "we provide sufficient conditions to derive a uniform bahadur representation , and establish uniform asymptotic normality , using central limit theorems and bootstrap results of @xcite , for the entire @xmath0-dimensional vector .",
    "the latter result holds uniformly over high - dimensional rectangles of dimension @xmath76 and over an underlying approximately sparse model , thereby extending previous results from the setting with @xmath77 @xcite to that with @xmath76 .    in what follows ,",
    "the @xmath78 and @xmath79 norms are denoted by @xmath80 and @xmath81 , respectively , and the @xmath82-norm , @xmath83 , denotes the number of non - zero components of a vector .",
    "we use the notation @xmath84 and @xmath85 .",
    "denote by @xmath86 the distribution function of the standard normal distribution .",
    "we assume that the quantities such as @xmath87 , and hence @xmath88 and @xmath89 are all dependent on the sample size @xmath1 , and allow for the case where @xmath90 and @xmath91 as @xmath92",
    ". we shall omit the dependence of these quantities on @xmath1 when it does not cause confusion .",
    "for a class of measurable functions @xmath93 on a measurable space , let @xmath94 denote its @xmath95-covering number with respect to the @xmath96 seminorm @xmath97 , where @xmath98 is a finitely discrete measure on the space , and let @xmath99 denote the uniform entropy number where @xmath100 .",
    "each of the steps outlined in section [ sec : intro ] could be implemented by several estimators .",
    "two possible implementations are the following .",
    "the algorithm is based on post - model selection estimators .",
    "run post-@xmath20-penalized median regression ( [ def : postl1qr ] ) of @xmath101 on @xmath4 and @xmath6 ; keep fitted value @xmath102 .",
    "+ _ step _ ( ii ) .",
    "run the post - lasso estimator ( [ estpostlasso1 ] ) of @xmath103 on @xmath104 ; keep the residual @xmath105 .",
    "+ _ step _ ( iii ) .",
    "run instrumental median regression ( [ estiv ] ) of @xmath106 on @xmath4 using @xmath41 as the instrument .",
    "report @xmath107 and perform inference based upon ( [ eq : result1 ] ) or ( [ eq : result2 ] ) .",
    "the algorithm is based on regularized estimators .",
    "run @xmath20-penalized median regression ( [ def : postl1qr ] ) of @xmath101 on @xmath4 and @xmath6 ; keep fitted value @xmath102 .",
    "+ _ step _ ( ii ) .",
    "run the lasso estimator ( [ estlasso1 ] ) of @xmath103 on @xmath104 ; keep the residual @xmath105 .",
    "+ _ step _ ( iii ) .",
    "run instrumental median regression ( [ estiv ] ) of @xmath106 on @xmath4 using @xmath41 as the instrument .",
    "report @xmath107 and perform inference based upon ( [ eq : result1 ] ) or ( [ eq : result2 ] ) .",
    "in order to perform @xmath79-penalized median regression and lasso , one has to choose the penalty levels suitably .",
    "we record our penalty choices in the supplementary material .",
    "algorithm 1 relies on the post - selection estimators that refit the non - zero coefficients without the penalty term to reduce the bias , while algorithm 2 relies on the penalized estimators . in step ( ii ) , instead of the lasso or the post - lasso estimators , dantzig selector @xcite and gauss - dantzig estimators could be used .",
    "step ( iii ) of both algorithms relies on instrumental median regression ( [ estiv ] ) .    alternatively , in this step",
    ", we can use a one - step estimator @xmath51 defined by@xmath108^{-1 } { e_n}\\ { \\varphi ( y_{{{i}}}- d_{{{{i } } } } { \\widehat}\\alpha -x_{{{i}}}^ { { { \\mathrm{\\scriptscriptstyle t } } } } { \\widehat}\\beta){\\widehat}v_{{{i}}}\\},\\ ] ] where @xmath109 is the @xmath20-penalized median regression estimator ( [ def : l1qr ] ) .",
    "another possibility is to use the post - double selection median regression estimation , which is simply the median regression of @xmath101 on @xmath4 and the union of controls selected in both steps ( i ) and ( ii ) , as @xmath51 .",
    "the supplemental material shows that these alternative estimators also solve ( [ estiv ] ) approximately .",
    "we state regularity conditions sufficient for validity of the main estimation and inference results .",
    "the behavior of sparse eigenvalues of the population gram matrix @xmath110 with @xmath111 plays an important role in the analysis of @xmath79-penalized median regression and lasso .",
    "define the minimal and maximal @xmath112-sparse eigenvalues of the population gram matrix as @xmath113 where @xmath114 . assuming that @xmath115 requires that all population gram submatrices formed by any @xmath112 components of @xmath116 are positive definite .",
    "the main condition , condition [ condition i ] , imposes sparsity of the vectors @xmath117 and @xmath33 as well as other more technical assumptions .",
    "below let @xmath118 and @xmath119 be given positive constants , and let @xmath120 , and @xmath121 be given sequences of positive constants .",
    "suppose that [ condition i ] ( i ) @xmath122 is a sequence of independent and identically distributed random vectors generated according to models ( [ eq : direct ] ) and ( [ eq : indirect ] ) , where @xmath123 has distribution distribution function @xmath10 such that @xmath124 and is independent of the random vector @xmath125 ; ( ii ) @xmath126 and @xmath127 almost surely ; moreover , @xmath128 ; ( iii ) there exists @xmath129 such that @xmath130 and @xmath131 ; ( iv ) the error distribution @xmath10 is absolutely continuous with continuously differentiable density @xmath132 such that @xmath133 and @xmath134 for all @xmath135 ; ( v ) there exist constants @xmath136 and @xmath137 such that @xmath138 and @xmath139 almost surely , and they obey the growth condition @xmath140 ( vi ) @xmath141 .",
    "condition [ condition i ] ( i ) imposes the setting discussed in the previous section with the zero conditional median of the error distribution .",
    "condition [ condition i ] ( ii ) imposes moment conditions on the structural errors and regressors to ensure good model selection performance of lasso applied to equation ( [ eq : indirect ] ) .",
    "condition [ condition i ] ( iii ) imposes sparsity of the high - dimensional vectors @xmath14 and @xmath33 .",
    "condition [ condition i ] ( iv ) is a set of standard assumptions in median regression @xcite and in instrumental quantile regression .",
    "condition [ condition i ] ( v ) restricts the sparsity index , namely @xmath142 is required ; this is analogous to the restriction @xmath143 made in @xcite in the low - dimensional setting .",
    "the uniformly bounded regressors condition can be relaxed with minor modifications provided the bound holds with probability approaching unity .",
    "most importantly , no assumptions on the separation from zero of the non - zero coefficients of @xmath33 and @xmath14 are made .",
    "condition [ condition i ] ( vi ) is quite plausible for many designs of interest .",
    "conditions [ condition i ] ( iv ) and ( v ) imply the equivalence between the norms induced by the empirical and population gram matrices over @xmath19-sparse vectors by @xcite .",
    "the following result is derived as an application of a more general theorem [ theorem2 ] given in section 3 ; the proof is given in the supplementary material .    [ theorem : inferencealg1 ] let @xmath51 and @xmath144 be the estimator and statistic obtained by applying either algorithm 1 or 2 .",
    "suppose that condition [ condition i ] is satisfied for all @xmath145 .",
    "moreover , suppose that with probability at least @xmath146 , @xmath147 .",
    "then , as @xmath92 , @xmath148 and @xmath149 in distribution , where @xmath150",
    ".    theorem [ theorem : inferencealg1 ] shows that algorithms 1 and 2 produce estimators @xmath51 that perform equally well , to the first order , with asymptotic variance equal to the semi - parametric efficiency bound ; see the supplemental material for further discussion .",
    "both algorithms rely on sparsity of @xmath151 and @xmath46 .",
    "sparsity of the latter follows immediately under sharp penalty choices for optimal rates .",
    "the sparsity for the former potentially requires a higher penalty level , as shown in @xcite ; alternatively , sparsity for the estimator in step 1 can also be achieved by truncating the smallest components of @xmath151 .",
    "the supplemental material shows that suitable truncation leads to the required sparsity while preserving the rate of convergence .",
    "an important consequence of these results is the following corollary . here",
    "@xmath152 denotes a collection of distributions for @xmath153 and for @xmath154 the notation @xmath155 means that under @xmath155 , @xmath153 is distributed according to the law determined by @xmath156 .",
    "[ cor : uniformity ] let @xmath107 be the estimator of @xmath157 constructed according to either algorithm 1 or 2 , and for every @xmath145 , let @xmath152 be the collection of all distributions of @xmath153 for which condition [ condition i ] holds and @xmath147 with probability at least @xmath146 . then for @xmath158 defined in ( [ eq : result2 ] ) , @xmath159 \\right \\ }",
    "- ( 1- \\xi )   \\right | & \\to 0 , \\\\ \\sup_{p_{n } \\in   \\mathcal{p}_{n } } \\left | { { \\mathrm{pr}}}_{p_{n } } ( \\alpha_0 \\in { \\widehat}a_{\\xi } ) - ( 1-\\xi ) \\right | & \\to 0 , \\quad n \\to \\infty.\\end{aligned}\\ ] ]    corollary [ cor : uniformity ] establishes the second main result of the paper .",
    "it highlights the uniform validity of the results , which hold despite the possible imperfect model selection in steps ( i ) and ( ii ) .",
    "condition [ condition i ] explicitly characterizes regions of data - generating processes for which the uniformity result holds .",
    "simulations presented below provide additional evidence that these regions are substantial . here we rely on exactly sparse models , but these results extend to approximately sparse model in what follows .    both of the proposed algorithms exploit the homoscedasticity of the model ( [ eq : direct ] ) with respect to the error term @xmath7 .",
    "the generalization to the heteroscedastic case can be achieved but we need to consider the density - weighted version of the auxiliary equation ( [ eq : indirect ] ) in order to achieve the semiparametric efficiency bound . the analysis of the impact of estimation of weights is delicate and is developed in our working paper  robust inference in high - dimensional approximate sparse quantile regression models \" ( arxiv:1312.7186 ) .",
    "we consider the generalization to the previous model : @xmath160 where @xmath161 are regressors , and @xmath95 is the noise with distribution function @xmath10 that is independent of regressors and has median zero , that is , @xmath124 .",
    "the coefficients @xmath162 are now the high - dimensional parameter of interest .",
    "we can rewrite this model as @xmath0 models of the previous form : @xmath163 where @xmath164 is the target coefficient , @xmath165 and where @xmath166 .",
    "we would like to estimate and perform inference on each of the @xmath0 coefficients @xmath162 simultaneously .",
    "moreover , we would like to allow regression functions @xmath167 to be of infinite dimension , that is , they could be written only as infinite linear combinations of some dictionary with respect to @xmath168 .",
    "however , we assume that there are sparse estimators @xmath169 that can estimate @xmath167 at sufficiently fast @xmath40 rates in the mean square error sense , as stated precisely in section 3 . examples of functions @xmath170 that permit such estimation by sparse methods include the standard sobolev spaces as well as more general rearranged sobolev spaces @xcite .",
    "here sparsity of estimators @xmath171 and @xmath172 means that they are formed by @xmath173-sparse linear combinations chosen from @xmath13 technical regressors generated from @xmath174 , with coefficients estimated from the data .",
    "this framework is general ; in particular it contains as a special case the traditional linear sieve / series framework for estimation of @xmath170 , which uses a small number @xmath175 of predetermined series functions as a dictionary .",
    "given suitable estimators for @xmath176 , we can then identify and estimate each of the target parameters @xmath177 via the empirical version of the moment equations @xmath178 = 0   \\quad   ( j=1,\\dots , p_{1}),\\ ] ] where @xmath179 and @xmath180 .",
    "these equations have the orthogonality property : @xmath181\\big |_{t = h_{j}(z_{j } ) } = 0   \\quad   ( j=1,\\dots , p_{1}).\\ ] ] the resulting estimation problem is subsumed as a special case in the next section .",
    "in this section we generalize the previous example to a more general setting , where @xmath0 target parameters defined via huber s z - problems are of interest , with dimension @xmath0 potentially much larger than the sample size .",
    "this framework covers median regression , its generalization discussed above , and many other semi - parametric models .",
    "the interest lies in @xmath182 real - valued target parameters @xmath162 .",
    "we assume that each @xmath183 , where each @xmath184 is a non - stochastic bounded closed interval .",
    "the true parameter @xmath164 is identified as a unique solution of the moment condition : @xmath185 = 0.\\ ] ] here @xmath186 is a random vector taking values in @xmath187 , a borel subset of a euclidean space , which contains vectors @xmath188 as subvectors , and each @xmath174 takes values in @xmath189 ; here @xmath174 and @xmath190 with @xmath191 may overlap .",
    "the vector - valued function @xmath192 is a measurable map from @xmath193 to @xmath194 , where @xmath195 is fixed , and the function @xmath196 is a measurable map from an open neighborhood of @xmath197 to @xmath198 .",
    "the former map is a possibly infinite - dimensional nuisance parameter .",
    "suppose that the nuisance function @xmath199 admits a sparse estimator @xmath200 of the form @xmath201 where @xmath202 may be much larger than @xmath1 while @xmath203 , the sparsity level of @xmath204 , is small compared to @xmath1 , and @xmath205 are given approximating functions .",
    "the estimator @xmath206 of @xmath164 is then constructed as a z - estimator , which solves the sample analogue of the equation ( [ eq : ivequation ] ) : @xmath207 | \\leq \\inf_{\\alpha \\in { \\widehat}{{\\mathcal{a}}}_{j } } |{e_n } [ \\psi\\{w , \\alpha , { \\widehat}h_{j}(z_{j } ) \\ } ] | + \\epsilon_n , \\label{eq : analog}\\ ] ] where @xmath208 is the numerical tolerance parameter and @xmath209 ; @xmath210 is a possibly stochastic interval contained in @xmath184 with high probability . typically , @xmath211 or",
    "can be constructed by using a preliminary estimator of @xmath164 .    in order to achieve robust inference results , we shall need to rely on the condition of orthogonality , or immunity , of the scores with respect to small perturbations in the value of the nuisance parameters , which we can express in the following condition : @xmath212 where we use the symbol @xmath213 to abbreviate @xmath214 .",
    "it is important to construct the scores @xmath215 to have property ( [ eq : orthogonality ] ) or its generalization given in remark [ remark : general orthogonality ] below .",
    "generally , we can construct the scores @xmath215 that obey such properties by projecting some initial non - orthogonal scores onto the orthogonal complement of the tangent space for the nuisance parameter @xcite .",
    "sometimes the resulting construction generates additional nuisance parameters , for example , the auxiliary regression function in the case of the median regression problem in section 2 .    in conditions",
    "[ condition : sp ] and [ condition : as ] below , @xmath216 , and @xmath119 are given positive constants ; @xmath195 is a fixed positive integer ; @xmath217 and @xmath218 are given sequences of constants .",
    "let @xmath219 and @xmath209 .",
    "[ condition : sp ] for every @xmath145 , we observe independent and identically distributed copies @xmath220 of the random vector @xmath186 , whose law is determined by the probability measure @xmath221 .",
    "uniformly in @xmath222 , and @xmath223 , the following conditions are satisfied : ( i ) the true parameter @xmath164 obeys ( [ eq : ivequation ] ) ; @xmath210 is a possibly stochastic interval such that with probability @xmath224 , @xmath225 \\subset { \\widehat}{{\\mathcal{a}}}_{j } \\subset { \\mathcal{a}}_{j}$ ] ; ( ii ) for @xmath226-almost every @xmath174 , the map @xmath227 is twice continuously differentiable , and for every @xmath228 , @xmath229|^2 ) \\leq c_{1}$ ] ; moreover , there exist constants @xmath230 , and a cube @xmath231 in @xmath194 with center @xmath232 such that for every @xmath233 , @xmath234 , and for every @xmath235 , @xmath236 \\leq l_{2n } ( | \\alpha-\\alpha ' |^{\\varsigma } + \\| t - t ' \\|^{\\varsigma});$ ] ( iii ) the orthogonality condition ( [ eq : orthogonality ] ) or its generalization stated in ( [ eq : orthogonality general ] ) below holds ; ( iv ) the following global and local identifiability conditions hold : @xmath237| \\geq   |\\gamma_{j } ( \\alpha- \\alpha_{j})| \\wedge c_{1 } \\text {   for all } \\alpha \\in { \\mathcal{a}}_{j},$ ] where @xmath238 $ ] , and @xmath239 ; and ( v ) the second moments of scores are bounded away from zero : @xmath240 \\geq c_{1}$ ] .    condition [ condition : sp ] states rather mild assumptions for z - estimation problems , in particular , allowing for non - smooth scores @xmath215 such as those arising in median regression .",
    "they are analogous to assumptions imposed in the setting with @xmath241 , for example , in @xcite .",
    "the following condition uses a notion of pointwise measurable classes of functions @xcite .",
    "[ condition : as ] uniformly in @xmath222 , and @xmath223 , the following conditions are satisfied : ( i ) the nuisance function @xmath242 has an estimator @xmath243 with good sparsity and rate properties , namely , with probability @xmath244 , @xmath245 , where @xmath246 and each @xmath247 is the class of functions @xmath248 of the form @xmath249 such that @xmath250 , @xmath251 for all @xmath252 , and @xmath253 \\leq c_{1 } s ( \\log a_{n})/n$ ] , where @xmath254 is the sparsity level , obeying ( iv ) ahead ; ( ii ) the class of functions @xmath255 is pointwise measurable and obeys the entropy condition @xmath256 for all @xmath257 ; ( iii ) the class @xmath258 has measurable envelope @xmath259 , such that @xmath260 obeys @xmath261 for some @xmath262 ; and ( iv ) the dimensions @xmath263 , and @xmath264 obey the growth conditions : @xmath265    condition [ condition : as ] ( i ) requires reasonable behavior of sparse estimators @xmath266 . in the previous section ,",
    "this type of behavior occurred in the cases where @xmath267 consisted of a part of a median regression function and a conditional expectation function in an auxiliary equation .",
    "there are many conditions in the literature that imply these conditions from primitive assumptions . for the case with @xmath268 , condition [ condition : as ] ( vi )",
    "implies the following restrictions on the sparsity indices : @xmath269 for the case where @xmath270 , which typically happens when @xmath215 is smooth , and @xmath271 for the case where @xmath272 , which typically happens when @xmath215 is non - smooth .",
    "condition [ condition : as ] ( iii ) bounds the moments of the envelopes , and it can be relaxed to a bound that grows with @xmath1 , with an appropriate strengthening of the growth conditions stated in ( iv ) .",
    "condition [ condition : as ] ( ii ) implicitly requires @xmath215 not to increase entropy too much ; it holds , for example , when @xmath215 is a monotone transformation , as in the case of median regression , or a lipschitz transformation ; see @xcite .",
    "the entropy bound is formulated in terms of the upper bound @xmath19 on the sparsity of the estimators and @xmath13 the dimension of the overall approximating model appearing via @xmath273 . in principle",
    "our main result below applies to non - sparse estimators as well , as long as the entropy bound specified in condition [ condition : as ] ( ii ) holds , with index @xmath274 interpreted as measures of effective complexity of the relevant function classes .",
    "recall that @xmath238 $ ] ; see condition [ condition : sp ] ( iii ) .",
    "define @xmath275 ,   \\quad   \\phi_{j } ( w )   = - \\sigma^{-1}_{j } \\gamma_{j}^{-1 } \\psi_{j}\\{w , \\alpha_{j } , h_{j } ( z_{j})\\ } \\quad ( j = 1,\\ldots , p_1).\\ ] ] the following is the main theorem of this section ; its proof is found in appendix [ sec : proof of theorem 2 ] .",
    "[ theorem2 ] under conditions [ condition : sp ] and [ condition : as ] , uniformly in @xmath221 , with probability @xmath276 , @xmath277    an immediate implication is a corollary on the asymptotic normality uniform in @xmath221 and @xmath223 , which follows from lyapunov s central limit theorem for triangular arrays .    under the conditions of theorem [ theorem2 ]",
    ", @xmath278 this implies , provided @xmath279 uniformly in @xmath221 , that @xmath280 \\right \\ } - ( 1-\\xi ) \\right |= o(1 ) ,   \\quad n \\to \\infty.\\ ] ]    this result leads to marginal confidence intervals for @xmath164 , and shows that they are valid uniformly in @xmath281 and @xmath223 .",
    "another useful implication is the high - dimensional central limit theorem uniformly over rectangles in @xmath282 , provided that @xmath283 , which follows from corollary 2.1 in @xcite .",
    "let @xmath284 be a normal random vector in @xmath285 with mean zero and covariance matrix @xmath286_{j , j'=1}^{p_1}$ ] .",
    "let @xmath287 be a collection of rectangles @xmath288 in @xmath282 of the form @xmath289 for example , when @xmath290 , @xmath291 .",
    "[ cor : high dim clt ] under the conditions of theorem [ theorem2 ] , provided that @xmath283 , @xmath292 -   { { \\mathrm{pr}}}_p ( \\mathcal{n } \\in r )   \\right |   = o(1 ) , \\quad n \\to \\infty.\\ ] ] this implies , in particular , that for @xmath293-quantile of @xmath294 , @xmath295 , \\",
    "j = 1,\\ldots ,",
    "p_1   \\right ) - ( 1-\\xi )   \\right | = o(1 ) , \\quad n \\to \\infty.\\ ] ]    this result leads to simultaneous confidence bands for @xmath177 that are valid uniformly in @xmath221 .",
    "moreover , corollary [ cor : high dim clt ] is immediately useful for testing multiple hypotheses about @xmath177 via the step - down methods of @xcite which control the family - wise error rate ; see @xcite for further discussion of multiple testing with @xmath76 .    in practice",
    "the distribution of @xmath296 is unknown , since its covariance matrix is unknown , but it can be approximated by the gaussian multiplier bootstrap , which generates a vector @xmath297 where @xmath298 are independent standard normal random variables , independent of the data @xmath299 , and @xmath300 are any estimators of @xmath301 , such that @xmath302 uniformly in @xmath221 .",
    "let @xmath303 .",
    "theorem 3.2 in @xcite then implies the following result .    under the conditions of theorem [ theorem2 ] ,",
    "provided that @xmath283 , with probability @xmath276 uniformly in @xmath221 , @xmath304 this implies , in particular , that for @xmath305-conditional quantile of @xmath306 , @xmath307 ,   \\",
    "j = 1,\\ldots , p_1 \\right ) - ( 1-\\xi ) \\right | = o(1).\\ ] ]    [ remark : general orthogonality ] the proof of theorem [ theorem2 ] shows that the orthogonality condition ( [ eq : orthogonality ] ) can be replaced by a more general orthogonality condition : @xmath308 = 0 ,   \\quad ( { \\widetilde}h_j \\in \\mathcal{h}_j , \\ j=1 , \\ldots , p_1),\\ ] ] where @xmath309 , or even more general condition of approximate orthogonality : @xmath310 = o(n^{-1/2 } b_n^{-1})$ ] uniformly in @xmath311 and @xmath312 .",
    "the generalization ( [ eq : orthogonality general ] ) has a number of benefits , which could be well illustrated by the median regression model of section [ sec : intro ] , where the conditional moment restriction @xmath313 could be now replaced by the unconditional one @xmath314 , which allows for more general forms of data - generating processes .",
    "we consider the regression model @xmath315 where @xmath316 , @xmath317 , and @xmath318 otherwise , @xmath319 consists of an intercept and covariates @xmath320 , and the errors @xmath7 and @xmath35 are independently and identically distributed as @xmath321 .",
    "the dimension @xmath13 of the controls @xmath6 is @xmath322 , and the sample size @xmath1 is @xmath323 .",
    "the covariance matrix @xmath324 has entries @xmath325 with @xmath326 .",
    "the coefficients @xmath327 and @xmath328 determine the @xmath329 in the equations @xmath330 and @xmath331 .",
    "we vary the @xmath329 in the two equations , denoted by @xmath332 and @xmath333 respectively , in the set @xmath334 , which results in 100 different designs induced by the different pairs of @xmath335 ; we performed @xmath336 monte carlo repetitions for each .",
    "the first equation in ( [ mc ] ) is a sparse model . however , unless @xmath327 is very large , the decay of the components of @xmath337 rules out the typical assumption that the coefficients of important regressors are well separated from zero .",
    "thus we anticipate that the standard post - selection inference procedure , discussed around ( [ def : postl1qr ] ) , would work poorly in the simulations .",
    "in contrast , from the prior theoretical arguments , we anticipate that our instrumental median estimator would work well .",
    "the simulation study focuses on algorithm 1 , since algorithm 2 performs similarly .",
    "standard errors are computed using ( [ eq : robustse ] ) . as the main benchmark we consider the standard post - model selection estimator @xmath338 based on the post @xmath20-penalized median regression method ( [ def : postl1qr ] ) .",
    "level tests based on : ( a ) the standard post - model selection procedure based on @xmath338 , ( b ) the proposed post - model selection procedure based on @xmath51 , ( c ) the score statistic @xmath339 , and ( d ) an ideal procedure with the false rejection rate equal to the nominal size.,height=432 ]     ( panels ( a)-(c ) ) , and of the proposed post - model selection estimator @xmath51 ( panels ( d)-(f ) ) . , scaledwidth=100.0%,height=432 ]    in figure [ fig : simfirst ] , we display the empirical false rejection probability of tests of a true hypothesis @xmath340 , with nominal size @xmath341 .",
    "the false rejection probability of the standard post - model selection inference procedure based upon @xmath338 deviates sharply from the nominal size .",
    "this confirms the anticipated failure , or lack of uniform validity , of inference based upon the standard post - model selection procedure in designs where coefficients are not well separated from zero so that perfect model selection does not happen . in sharp contrast , both of our proposed procedures , based on estimator @xmath51 and the result ( [ eq : result1 ] ) and on the statistic @xmath339 and the result ( [ eq : result2 ] ) , closely track the nominal size .",
    "this is achieved uniformly over all the designs considered in the study , and confirms the theoretical results of corollary [ cor : uniformity ] .    in figure",
    "[ fig : simsecond ] , we compare the performance of the standard post - selection estimator @xmath338 and our proposed post - selection estimator @xmath51 .",
    "we use three different measures of performance of the two approaches : mean bias , standard deviation , and root mean square error .",
    "the significant bias for the standard post - selection procedure occurs when the main regressor @xmath4 is correlated with other controls @xmath6 .",
    "the proposed post - selection estimator @xmath51 performs well in all three measures .",
    "the root mean square errors of @xmath51 are typically much smaller than those of @xmath338 , fully consistent with our theoretical results and the semiparametric efficiency of @xmath51 .",
    "in the supplementary material we provide omitted proofs , technical lemmas , discuss extensions to the heteroscedastic case , and alternative implementations .",
    "we first state a maximal inequality used in the proof of theorem [ theorem2 ] .",
    "[ lemma : cck ] let @xmath342 be independent and identically distributed random variables taking values in a measurable space , and let @xmath93 be a pointwise measurable class of functions on that space .",
    "suppose that there is a measurable envelope @xmath343 such that @xmath344 for some @xmath345 .",
    "consider the empirical process indexed by @xmath93 : @xmath346 , f \\in { \\mathcal{f}}$ ] .",
    "let @xmath347 be any positive constant such that @xmath348 .",
    "moreover , suppose that there exist constants @xmath349 and @xmath350 such that @xmath351 for all @xmath257 . then @xmath352^{1/2}/\\sigma ) \\right \\}^{1/2 } \\\\ +   n^{-1/2 + 1/q } s [ { e}\\ { f^{q } ( w ) \\ } ] ^{1/q } \\log ( a [ { e}\\ { f^{2 } ( w ) \\ } ] ^{1/2}/\\sigma ) \\bigg ] , \\end{gathered}\\ ] ] where @xmath353 is a universal constant",
    ". moreover , for every @xmath354 , with probability not less than @xmath355 , @xmath356^{1/q } t \\right ) , \\ ] ] where @xmath357 is a constant that depends only on @xmath358 .",
    "the first and second inequalities follow from corollary 5.1 and theorem 5.1 in @xcite applied with @xmath359 , using that @xmath360^{1/2 } \\leq [ { e}\\ { \\max_{i=1,\\ldots , n } f^{q}(w_{i } ) \\ } ] ^{1/q } \\leq n^{1/q } [ { e}\\ { f^{q } ( w ) \\ } ] ^{1/q}$ ] .",
    "it suffices to prove the theorem under any sequence @xmath361 .",
    "we shall suppress the dependence of @xmath226 on @xmath1 in the proof . in this proof ,",
    "let @xmath362 denote a generic positive constant that may differ in each appearance , but that does not depend on the sequence @xmath221 , @xmath1 , or @xmath223 .",
    "recall that the sequence @xmath218 satisfies the growth conditions in condition [ condition : as ] ( iv ) .",
    "we divide the proof into three steps .",
    "below we use the following notation : for any given function @xmath363 , @xmath364 $ ] .    _ step _ 1",
    "let @xmath365 be any estimator such that with probability @xmath276 , @xmath366 .",
    "we wish to show that , with probability @xmath276 , @xmath367   = { e_n}[\\psi_{j } \\{w , \\alpha_{j } , h_{j}(z_{j})\\ } ] + \\gamma_{j } ( { \\widetilde}{\\alpha}_{j } - \\alpha_{j } ) + o ( n^{-1/2 } b_{n}^{-1}),\\ ] ] uniformly in @xmath223 .",
    "expand @xmath368 = { e_n}[\\psi_{j } \\{w , \\alpha_{j } , h_{j}(z_{j})\\ } ] + { e } [ \\psi_{j } \\{w , \\alpha , { \\widetilde}h(z_{j } ) \\ }   ] |_{\\alpha = { \\widetilde}{\\alpha}_{j},{\\widetilde}h={\\widehat}{h}_{j } } \\\\",
    "+ n^{-1/2 } { g_n}[\\psi_{j } \\{w , { \\widetilde}{\\alpha}_{j } , { \\widehat}{h}_{j}(z_{j } ) \\ } - \\psi_{j } \\{w , \\alpha_{j } , h_{j}(z_{j } ) \\ } ] = i_{j}+ii_{j } + iii_{j},\\end{gathered}\\ ] ] where we have used @xmath369 = 0 $ ] .",
    "we first bound @xmath370 .",
    "observe that , with probability @xmath276 , @xmath371 , where @xmath93 is the class of functions defined by @xmath372 which has @xmath373 as an envelope .",
    "we apply lemma [ lemma : cck ] to this class of functions . by condition",
    "[ condition : as ] ( ii ) and a simple covering number calculation , we have @xmath374 . by condition",
    "[ condition : sp ] ( ii ) , @xmath375 is bounded by @xmath376 ^ 2 \\mid z_{j } \\right ) \\right \\ } \\leq cl_{2n}\\rho_n^{\\varsigma},\\ ] ] where we have used the fact that @xmath377 \\leq   c \\rho_{n}^{2}$ ] for all @xmath378 whenever @xmath379 . hence applying lemma [ lemma : cck ] with @xmath380 ,",
    "we conclude that , with probability @xmath276 , @xmath381 where the last equality follows from condition [ condition : as ] ( iv ) .",
    "next , we expand @xmath382 . pick any @xmath383 with @xmath384 . then by taylor s theorem , for any @xmath385 and @xmath386 , there exists a vector @xmath387 on the line segment joining @xmath388 and @xmath389 such that @xmath390 $ ] can be written as @xmath391 + { e}(\\partial_{\\alpha } { e } [ \\psi_{j } \\{w , \\alpha_{j } , h_{j}(z_{j } ) \\ }   \\mid z_{j } ] ) ( \\alpha - \\alpha_{j } )   \\notag \\\\ & + { \\textstyle   \\sum}_{m=1}^{m }   { e}\\ { { e}\\left ( \\partial_{t_{m } }   { e}[\\psi_{j } \\ { w , \\alpha_{j } , h_{j}(z_{j } ) \\ }   \\mid z_{j } ] \\right )   \\ { { \\widetilde}h_{m}(z_{j } ) - h_{j m}(z_{j } ) \\ } \\ } \\notag \\\\ &   + 2^{-1 } { e}(\\partial_{\\alpha}^{2 } { e } [ \\psi_{j } \\ {",
    "w,\\bar{\\alpha}(z_j ) , \\bar{t}(z_j ) \\ }   \\mid z_{j } ] ) ( \\alpha - \\alpha_{j})^{2 }    \\notag \\\\ & + 2^{-1 } { \\textstyle   \\sum}_{m , m ' = 1}^{m } { e } ( \\partial_{t_{m } } \\partial_{t_{m ' } } { e } [ \\psi_{j } \\ { w,\\bar{\\alpha}(z_j ) , \\bar{t}(z_j)\\ }   \\mid z_{j } ] \\ { { \\widetilde}h_{m}(z_{j } ) - h_{j m}(z_{j } ) \\ } \\ { { \\widetilde}h_{m'}(z_{j } ) - h_{j m'}(z_{j } ) \\ } ) \\notag \\\\ & +   { \\textstyle   \\sum}_{m = 1}^{m } { e } ( \\partial_{\\alpha } \\partial_{t_{m } } { e } [ \\psi_{j } \\ { w,\\bar{\\alpha}(z_j ) , \\bar{t}(z_j)\\ }   \\mid z_{j } ] ( \\alpha - \\alpha_{j } ) \\ { { \\widetilde}h_{m}(z_{j } ) - h_{j m}(z_{j } ) \\ } ) .",
    "\\label{eq : expansion}\\end{aligned}\\ ] ] the third term is zero because of the orthogonality condition ( [ eq : orthogonality ] ) .",
    "condition [ condition : sp ] ( ii ) guarantees that the expectation and derivative can be interchanged for the second term , that is , @xmath392 \\right ) = \\partial_{\\alpha } { e } [ \\psi_{j } \\{w , \\alpha_{j } , h_{j}(z_{j } ) \\ } ] = \\gamma_{j}$ ] .",
    "moreover , by the same condition , each of the last three terms is bounded by @xmath393 , uniformly in @xmath223 .",
    "therefore , with probability @xmath276 , @xmath394 , uniformly in @xmath223 . combining the previous bound on @xmath370 with these bounds leads to the desired assertion .",
    "_ step _ 2 .",
    "we wish to show that with probability @xmath276 , @xmath395 |   = o(n^{-1/2}b_{n}^{-1})$ ] , uniformly in @xmath223 .",
    "define @xmath396 \\quad ( j = 1,\\ldots , p_1)$ ] .",
    "then we have @xmath397 |$ ] .",
    "consider the class of functions @xmath398 , which has @xmath399 as an envelope .",
    "since this class is finite with cardinality @xmath400 , we have @xmath401 . hence applying lemma [ lemma : cck ] to @xmath402 with @xmath403^{1/2 } \\leq c$ ] and @xmath380 , we conclude that with probability @xmath276 , @xmath404 | \\leq c n^{-1/2 } \\ {   ( \\log a_{n})^{1/2 } + n^{-1/2 + 1/q } \\log a_{n } \\ } \\leq c n^{-1/2 }   \\log a_{n}.\\ ] ] since @xmath405 $ ] with probability @xmath276 , @xmath406 with probability @xmath276 .    therefore , using step 1 with @xmath407 , we have , with probability @xmath276 , @xmath408 | \\leq |   { e_n}[\\psi_{j } \\{w , \\alpha_{j}^ { * } , { \\widehat}{h}_{j}(z_{j})\\ } ] | = o(n^{-1/2 } b_{n}^{-1}),\\ ] ] uniformly in @xmath223 , where we have used the fact that @xmath409 + \\gamma_{j } ( \\alpha_{j}^ { * } - \\alpha_{j } ) = 0 $ ] .    _ step _ 3 .",
    "we wish to show that with probability @xmath276 , @xmath410 . by step 2 and the definition of @xmath411 , with probability @xmath276",
    ", we have @xmath412 | = o(n^{-1/2 } b_{n}^{-1})$ ] .",
    "consider the class of functions @xmath413 . then with probability @xmath276 , @xmath414 | \\geq \\left | { e } [ \\psi_{j}\\ { w,\\alpha,{\\widetilde}h(z_{j } ) \\ } ] |_{\\alpha = { \\widehat}{\\alpha}_{j},{\\widetilde}h={\\widehat}{h}_{j } } \\right | - n^{-1/2 } \\sup_{f \\in { \\mathcal{f } } } | { g_n}(f ) |,\\ ] ] uniformly in @xmath223 .",
    "observe that @xmath415 has @xmath399 as an envelope and , by condition [ condition : as ] ( ii ) and a simple covering number calculation , @xmath416 . then applying lemma [ lemma : cck ] with @xmath417^{1/2 } \\leq c$ ] and @xmath418",
    ", we have , with probability @xmath276 , @xmath419 moreover , application of the expansion ( [ eq : expansion ] ) with @xmath420 together with the cauchy ",
    "schwarz inequality implies that @xmath421 - { e } [ \\psi_{j}\\ { w,\\alpha , h_{j}(z_{j } ) \\ } ] |$ ] is bounded by @xmath422 , so that with probability @xmath276 , @xmath423|_{\\alpha = { \\widehat}{\\alpha}_{j},{\\widetilde}h={\\widehat}{h}_{j } } \\right | \\geq",
    "\\left | { e } [ \\psi_{j}\\ { w,\\alpha , h_{j } ( z_{j } ) \\ } ] |_{\\alpha = { \\widehat}{\\alpha}_{j } } \\right | - o(\\rho_{n}),\\ ] ] uniformly in @xmath223 , where we have used condition [ condition : sp ] ( ii ) together with the fact that @xmath424 \\leq c \\rho^{2}_{n}$ ] for all @xmath425 whenever @xmath379 .",
    "by condition [ condition : sp ] ( iv ) , the first term on the right side is bounded from below by @xmath426 , which , combined with the fact that @xmath427 , implies that with probability @xmath276 , @xmath428 , uniformly in @xmath223 .",
    "_ step _ 4 . by steps 1 and 3 , with probability @xmath276 , @xmath429   = { e_n}[\\psi_{j } \\{w , \\alpha_{j } , h_{j}(z_{j})\\ } ] + \\gamma_{j } ( { \\widehat}{\\alpha}_{j } - \\alpha_{j } ) + o ( n^{-1/2 } b_{n}^{-1}),\\ ] ] uniformly in @xmath223 . moreover , by step 2 , with probability @xmath276 , the left side is @xmath430 uniformly in @xmath223 . solving this equation with respect to @xmath431 leads to the conclusion of the theorem .",
    "10    donald  wk andrews . empirical process methods in econometrics . , 4:22472294 , 1994 .",
    "a.  belloni , d.  chen , v.  chernozhukov , and c.  hansen .",
    "sparse models and methods for optimal instruments with an application to eminent domain .",
    "80(6):23692430 , november 2012 .",
    "a.  belloni and v.  chernozhukov .",
    "@xmath20-penalized quantile regression for high dimensional sparse models .",
    ", 39(1):82130 , 2011 .",
    "a.  belloni , v.  chernozhukov , and c.  hansen .",
    "inference for high - dimensional sparse econometric models .",
    ", 3:245295 , 2013 .",
    "a.  belloni , v.  chernozhukov , and c.  hansen .",
    "inference on treatment effects after selection amongst high - dimensional controls .",
    ", 81:608650 , 2014 .",
    "a.  belloni , v.  chernozhukov , and l.  wang .",
    "pivotal estimation via square - root lasso in nonparametric regression . , 42:757788 , 2014 .",
    "p.  j. bickel , y.  ritov , and a.  b. tsybakov .",
    "simultaneous analysis of lasso and dantzig selector .",
    ", 37(4):17051732 , 2009 .",
    "e.  candes and t.  tao .",
    "the dantzig selector : statistical estimation when @xmath13 is much larger than @xmath1 . , 35(6):23132351 , 2007 .",
    "v.  chernozhukov , d.  chetverikov , and k.  kato .",
    "gaussian approximations and multiplier bootstrap for maxima of sums of high - dimensional random vectors .",
    ", 41(6):27862819 , 2013 .",
    "v.  chernozhukov , d.  chetverikov , and k.  kato .",
    "gaussian approximation of suprema of empirical processes .",
    ", 42:15641597 , 2014 .",
    "victor chernozhukov and christian hansen .",
    "instrumental variable quantile regression : a robust inference approach .",
    ", 142:379398 , 2008 .    victor  h. de  la pea , tze  leung lai , and qi - man shao .",
    "springer , new york , 2009 .    xuming he and qi - man shao . on parameters of increasing dimensions .",
    ", 73(1):120135 , 2000 .    p.  j. huber .",
    "robust regression : asymptotics , conjectures and monte carlo .",
    ", 1:799821 , 1973 .",
    "guido  w. imbens .",
    "nonparametric estimation of average treatment effects under exogeneity : a review .",
    ", 86(1):429 , 2004 .",
    "roger koenker . .",
    "cambridge university press , cambridge , 2005 .",
    "michael  r. kosorok . .",
    "springer , new york , 2008 .",
    "sokbae lee .",
    "efficient semiparametric estimation of a partially linear quantile regression model . , 19:131 , 2003 .",
    "hannes leeb and benedikt  m. ptscher .",
    "model selection and inference : facts and fiction .",
    ", 21:2159 , 2005 .",
    "hannes leeb and benedikt  m. ptscher .",
    "sparse estimators and the oracle property , or the return of hodges estimator .",
    ", 142(1):201211 , 2008 .",
    "hua liang , suojin wang , james  m. robins , and raymond  j. carroll .",
    "estimation in partially linear models with missing covariates .",
    ", 99(466):357367 , 2004 .",
    "j.  neyman .",
    "optimal asymptotic tests of composite statistical hypotheses . in u.",
    "grenander , editor , _ probability and statistics , the harold cramer volume_. new york : john wiley and sons , inc . ,",
    "s.  portnoy .",
    "asymptotic behavior of m - estimators of @xmath13 regression parameters when @xmath432 is large",
    ". i. consistency .",
    ", 12:12981309 , 1984 .",
    "s.  portnoy .",
    "asymptotic behavior of m - estimators of @xmath13 regression parameters when @xmath432 is large .",
    "ii . normal approximation .",
    ", 13:12511638 , 1985 .",
    "j.  l. powell . censored regression quantiles .",
    ", 32:143155 , 1986 .",
    "james  m. robins and andrea rotnitzky .",
    "semiparametric efficiency in multivariate regression models with missing data .",
    ", 90(429):122129 , 1995 .    p.  m. robinson .",
    "root-@xmath1-consistent semiparametric regression .",
    ", 56(4):931954 , 1988 .",
    "joseph  p. romano and michael wolf .",
    "stepwise multiple testing as formalized data snooping . , 73(4):12371282 , july 2005 .",
    "m.  rudelson and s.  zhou .",
    "reconstruction from anisotropic random measurements .",
    ", 59:34343447 , 2013 .",
    "r.  j. tibshirani .",
    "regression shrinkage and selection via the lasso .",
    ", 58:267288 , 1996 .    a.  w. van  der vaart .",
    ". cambridge university press , cambridge , 1998 .",
    "a.  w. van  der vaart and j.  a. wellner . .",
    "springer - verlag , new york , 1996 .",
    "@xmath433 penalized lad estimator for high dimensional linear regression .",
    ", 120:135151 , 2013 .",
    "cun - hui zhang and stephanie  s. zhang .",
    "confidence intervals for low - dimensional parameters with high - dimensional data . , 76:217242 , 2014 .",
    "suplementary material + uniform post selection inference for least absolute deviation regression and other z - estimation problems    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ this supplementary material contains omitted proofs , technical lemmas , discussion of the extension to the heteroscedastic case , and alternative implementations of the estimator . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _",
    "in addition to the notation used in the main text , we will use the following notation .",
    "denote by @xmath434 the maximal absolute element of a vector . given a vector @xmath435 and a set of indices @xmath436",
    ", we denote by @xmath437 the vector such that @xmath438 if @xmath439 and @xmath440 if @xmath441",
    ". for a sequence @xmath442 of constants , we write @xmath443 . for example , for a vector @xmath18 and @xmath13-dimensional regressors",
    "@xmath444 , @xmath445^{1/2}$ ] denotes the empirical prediction norm of @xmath446 .",
    "denote by @xmath447 the population @xmath448-seminorm .",
    "we also use the notation @xmath449 to denote @xmath450 for some constant @xmath451 that does not depend on @xmath1 ; and @xmath452 to denote @xmath453 .",
    "we emphasize that both proposed algorithms exploit the homoscedasticity of the model ( [ eq : direct ] ) with respect to the error term @xmath7 .",
    "the generalization to the heteroscedastic case can be achieved as follows .",
    "recall the model @xmath454 where @xmath7 is now not necessarily independent of @xmath4 and @xmath6 but obeys the conditional median restriction @xmath455 . to achieve the semiparametric efficiency bound in this general case , we need to consider the weighted version of the auxiliary equation ( [ eq : indirect ] ) . specifically , we rely on the weighted decomposition : @xmath456 where the weights are the conditional densities of the error terms @xmath7 evaluated at their conditional medians of zero : @xmath457 which in general vary under heteroscedasticity . with",
    "that in mind it is straightforward to adapt the proposed algorithms when the weights @xmath458 are known .",
    "for example algorithm 1 becomes as follows .",
    "the algorithm is based on post - model selection estimators .",
    "run post-@xmath20-penalized median regression of @xmath101 on @xmath4 and @xmath6 ; keep fitted value @xmath459 .",
    "+ _ step _ ( ii ) .",
    "run the post - lasso estimator of @xmath460 on @xmath461 ; keep the residual @xmath462 .",
    "+ _ step _ ( iii ) .",
    "run instrumental median regression of @xmath106 on @xmath4 using @xmath463 as the instrument .",
    "report @xmath107 and/or perform inference .",
    "analogously , we obtain algorithm @xmath464 , as a generalization of algorithm 2 in the main text , based on regularized estimators , by removing the word `` post '' in algorithm 1@xmath465 .    under similar regularity conditions , uniformly over a large collection @xmath466 of distributions of @xmath467 ,",
    "the estimator @xmath51 above obeys @xmath468 in distribution .",
    "moreover , the criterion function at the true value @xmath5 in step ( iii ) also has a pivotal behavior , namely @xmath469 in distribution , which can also be used to construct a confidence region @xmath470 based on the @xmath339-statistic as in ( [ eq : result2 ] ) with coverage @xmath71 uniformly in a suitable collection of distributions .    in practice",
    "the density function values @xmath458 are unknown and need to be replaced by estimates @xmath471 .",
    "the analysis of the impact of such estimation is very delicate and is developed in the companion work ",
    "robust inference in high - dimensional approximately sparse quantile regression models \" ( arxiv:1312.7186 ) , which considers the more general problem of uniformly valid inference for quantile regression models in approximately sparse models .      the asymptotic variance , @xmath472 , of the estimator @xmath51 is the semiparametric efficiency bound for estimation of @xmath5 . to see this , given a law @xmath156 with @xmath473",
    ", we first consider a submodel @xmath474 such that @xmath475 , indexed by the parameter @xmath476 for the parametric components @xmath477 and described as : @xmath478 where the conditional density of @xmath7 varies . here",
    "we use @xmath479 to denote the overall model collecting all distributions for which a variant of conditions of theorem [ theorem : inferencealg1 ] permitting heteroscedasticity is satisfied . in this submodel",
    ", setting @xmath480 leads to the given parametric components @xmath477 at @xmath481 .",
    "then by using a similar argument to @xcite , section 5 , the efficient score for @xmath5 in this submodel is @xmath482 so that @xmath483 is the efficiency bound at @xmath481 for estimation of @xmath5 relative to the submodel , and hence relative to the entire model @xmath479 , as the bound is attainable by our estimator @xmath51 uniformly in @xmath481 in @xmath479 .",
    "this efficiency bound continues to apply in the homoscedastic model with @xmath484 for all @xmath485 .",
    "an alternative proposal for the method is reminiscent of the double selection method proposed in @xcite for partial linear models .",
    "this version replaces step ( iii ) with a median regression of @xmath486 on @xmath487 and all covariates selected in steps ( i ) and ( ii ) , that is , the union of the selected sets .",
    "the method is described as follows :    the algorithm is based on double selection .",
    "run @xmath20-penalized median regression of @xmath101 on @xmath4 and @xmath6 : @xmath488 _ step _ ( ii ) .",
    "run lasso of @xmath4 on @xmath6 : @xmath489 _ step _ ( iii ) .",
    "run median regression of @xmath101 on @xmath4 and the covariates selected in steps ( i ) and ( ii ) : @xmath490 report @xmath107 and/or perform inference .",
    "the double selection algorithm has three main steps : ( i ) select covariates based on the standard @xmath20-penalized median regression , ( ii ) select covariates based on heteroscedastic lasso of the treatment equation , and ( ii ) run a median regression with the treatment and all selected covariates .",
    "this approach can also be analyzed through theorem [ theorem2 ] since it creates instruments implicitly .",
    "to see that let @xmath491 denote the variables selected in steps ( i ) and ( ii ) : @xmath492 . by the first order conditions for @xmath493 we have @xmath494 which creates an orthogonal relation to any linear combination of @xmath495 .",
    "in particular , by taking the linear combination @xmath496 , which is the instrument in step ( ii ) of algorithm 1 , we have @xmath497 as soon as the right side is @xmath498 , the double selection estimator @xmath107 approximately minimizes @xmath499},\\ ] ] where @xmath500 is the instrument created by step ( ii ) of algorithm 1 . thus the double selection estimator can be seen as an iterated version of the method based on instruments where the step ( i ) estimate @xmath501 is updated with @xmath502 .",
    "in this section we state relevant theoretical results on the performance of the estimators : @xmath20-penalized median regression , post-@xmath20-penalized median regression , heteroscedastic lasso , and heteroscedastic post - lasso estimators .",
    "there results were developed in @xcite and @xcite .",
    "we keep the notation of sections 1 and 2 in the main text , and let @xmath503 . throughout the section ,",
    "let @xmath504 be a fixed constant chosen by users .",
    "in practice , we suggest to take @xmath505 but the analysis is not restricted to this choice",
    ". moreover , let @xmath506 .",
    "recall the definition of the minimal and maximal @xmath112-sparse eigenvalues of a matrix @xmath507 as @xmath508 where @xmath509 .",
    "also recall @xmath510 , and define @xmath511 .",
    "observe that @xmath512 .",
    "suppose that @xmath513 are independent and identically distributed random vectors satisfying the conditional median restriction @xmath514 we consider the estimation of @xmath515 via the @xmath20-penalized median regression estimate @xmath516 where @xmath517 is a diagonal matrix of penalty loadings . as established in @xcite and @xcite , under the event that @xmath518\\|_\\infty,\\ ] ] the estimator above achieves good theoretical guarantees under mild design conditions .",
    "although @xmath515 is unknown , we can set @xmath22 so that the event in ( [ eq : penaltyl1qr ] ) holds with high probability .",
    "in particular , the pivotal rule discussed in @xcite proposes to set @xmath519 with @xmath520 where @xmath521\\|_\\infty),\\ ] ] where @xmath522 denotes the @xmath523-quantile of a random variable @xmath524 . here",
    "@xmath525 are independent uniform random variables on @xmath526 independent of @xmath527 . this quantity can be easily approximated via simulations .",
    "the values of @xmath528 and @xmath529 are chosen by users , but we suggest to take @xmath530 and @xmath531 . below we summarize required technical conditions .",
    "[ conditionplad ] assume that @xmath532 , @xmath533 , @xmath534 for @xmath535 with probability @xmath276 , the conditional density of @xmath536 given @xmath537 , denoted by @xmath538 , and its derivative are bounded by @xmath539 and @xmath540 , respectively , and @xmath541 is bounded away from zero .",
    "condition [ conditionplad ] is implied by condition [ condition i ] after a normalizing the variables so that @xmath533 for @xmath535 .",
    "the assumption on the conditional density is standard in the quantile regression literature even with fixed @xmath13 or @xmath13 increasing slower than @xmath1 , see respectively @xcite and @xcite .",
    "we present bounds on the population prediction norm of the @xmath20-penalized median regression estimator .",
    "the bounds depend on the restricted eigenvalue proposed in @xcite , defined by @xmath542 where @xmath543 , @xmath544 and @xmath545 .",
    "the following lemma follows directly from the proof of theorem 2 in @xcite applied to a single quantile index .",
    "[ theorem : l1qrnp ] under condition [ conditionplad ] and using @xmath546^{1/2}$ ] , we have with probability at least @xmath547 , @xmath548^{1/2},\\ ] ] provided that @xmath549^{1/2 } } \\frac{\\bar f \\bar f'}{\\underline{f}}\\inf_{\\delta\\in\\delta_{c_{0 } } } \\frac{\\| x^ { { { \\mathrm{\\scriptscriptstyle t } } } } \\delta \\|_{p,2}^{3}}{{e}(|{\\widetilde}x_{{{i}}}^ { { { \\mathrm{\\scriptscriptstyle t } } } } \\delta|^3 ) } \\to \\infty.\\ ] ]    lemma [ theorem : l1qrnp ] establishes the rate of convergence in the population prediction norm for the @xmath20-penalized median regression estimator in a parametric setting .",
    "the extra growth condition required for identification is mild .",
    "for instance for many designs of interest we have @xmath550 bounded away from zero as shown in @xcite . for designs with bounded regressors",
    "we have @xmath551 where @xmath552 is a constant such that @xmath553 almost surely .",
    "this leads to the extra growth condition that @xmath554 .    in order to alleviate the bias introduced by the @xmath20-penalty",
    ", we can consider the associated post - model selection estimate associated with a selected support @xmath555 @xmath556 the following result characterizes the performance of the estimator in ( [ def : app : postl1qr ] ) ; see theorem 5 in @xcite for the proof .",
    "[ thm : maintwostep ] suppose that @xmath557 and let @xmath558 .",
    "then under the same conditions of lemma [ theorem : l1qrnp ] , @xmath559^{1/2},\\ ] ] provided that @xmath560^{1/2 } } \\frac{\\bar f \\bar f ' } { \\underline{f}}{\\displaystyle \\inf_{\\|\\delta\\|_0 \\leq { \\widehat}s + s } } \\frac{\\|{\\widetilde}x_{{{i}}}^ { { { \\mathrm{\\scriptscriptstyle t } } } } \\delta\\|_{p,2}^{3}}{{e}(|{\\widetilde}x_{{{i}}}^ { { { \\mathrm{\\scriptscriptstyle t } } } } \\delta|^3 ) } \\to_p \\infty.\\ ] ]    lemma [ thm : maintwostep ] provides the rate of convergence in the prediction norm for the post model selection estimator despite possible imperfect model selection .",
    "the rates rely on the overall quality of the selected model , which is at least as good as the model selected by @xmath20-penalized median regression , and the overall number of components @xmath561 .",
    "once again the extra growth condition required for identification is mild .",
    "[ comment : alpha ] in step ( i ) of algorithm 2 we use @xmath20-penalized median regression with @xmath562 , @xmath563 , and we are interested in rates for @xmath564 instead of @xmath565 .",
    "however , it follows that @xmath566 since @xmath350 , without loss of generality we can assume the component associated with the treatment @xmath4 belongs to @xmath567 , at the cost of increasing the cardinality of @xmath567 by one which will not affect the rate of convergence .",
    "therefore we have that @xmath568 provided that @xmath569 , which occurs with probability at least @xmath570 . in most applications of interest @xmath571 and @xmath572",
    "are bounded from above .",
    "similarly , in step ( i ) of algorithm 1 we have that the post-@xmath20-penalized median regression estimator satisfies @xmath573.\\ ] ]      in this section we consider the equation ( [ eq : indirect ] ) of the form @xmath574 where we observe @xmath575 that are independent and identically distributed random vectors",
    ". the unknown support of @xmath33 is denoted by @xmath576 and it satisfies @xmath577 . to estimate @xmath33 ,",
    "we compute @xmath578 where @xmath22 and @xmath48 are the associated penalty level and loadings which are potentially data - driven .",
    "we rely on the results of @xcite on the performance of lasso and post - lasso estimators that allow for heteroscedasticity and non - gaussianity . according to @xcite",
    ", we use an initial and a refined option for the penalty level and the loadings , respectively @xmath579^{1/2 } ,   &   \\lambda =   2cn^{1/2 } \\phi^{-1}\\ { 1- \\gamma/(2p ) \\ } , \\\\ &    { \\widehat}\\gamma_{j }   = \\{{e_n } ( x^2_{{{{i}}}j } \\widehat v^2_{{{i}}})\\}^{1/2 } ,   & \\lambda = 2cn^{1/2 } \\phi^{-1}\\ { 1- \\gamma/(2p ) \\ } , \\end{array}\\ ] ] for @xmath535 , where @xmath580 is a fixed constant , @xmath581 , @xmath582 and @xmath41 is an estimate of @xmath35 based on lasso with the initial option or iterations .",
    "we make the following high - level conditions . below @xmath583",
    "are given positive constants , and @xmath584 is a given sequence of constants .",
    "[ conditionhl ] suppose that ( i ) there exists @xmath585 such that @xmath131 .",
    "( ii ) @xmath586 , @xmath587 almost surely , and @xmath588 .",
    "( iii ) @xmath589 .",
    "( iv ) with probability @xmath276 , @xmath590 and @xmath591 . ( v ) with probability @xmath276 , @xmath592 .",
    "condition [ conditionhl ] ( i ) implies condition as in @xcite , while conditions [ conditionhl ] ( ii)-(iv ) imply condition rf in @xcite .",
    "lemma 3 in @xcite provides primitive sufficient conditions under which condition ( iv ) is satisfied .",
    "the condition on the sparse eigenvalues ensures that @xmath593 in theorem 1 of @xcite , applied to this setting , is bounded away from zero with probability @xmath276 ; see lemma 4.1 in @xcite .",
    "next we summarize results on the performance of the estimators generated by lasso .",
    "[ thm : rateestimatedlasso ] suppose that condition [ conditionhl ] is satisfied .",
    "setting @xmath594 for @xmath580 , and using the penalty loadings as in ( [ choice of loadings2 ] ) , we have with probability @xmath276 , @xmath595    associated with lasso we can define the post - lasso estimator as @xmath596 that is , the post - lasso estimator is simply the least squares estimator applied to the regressors selected by lasso in ( [ estlasso ] ) .",
    "sparsity properties of the lasso estimator @xmath46 under estimated weights follows similarly to the standard lasso analysis derived in @xcite . by combining such sparsity properties and the rates in the prediction norm",
    ", we can establish rates for the post - model selection estimator under estimated weights .",
    "the following result summarizes the properties of the post - lasso estimator .",
    "[ corollary3:postrate ] suppose that condition [ conditionhl ] is satisfied .",
    "consider the lasso estimator with penalty level and loadings specified as in lemma [ thm : rateestimatedlasso ] .",
    "then the data - dependent model @xmath597 selected by the lasso estimator @xmath46 satisfies with probability @xmath276 : @xmath598 moreover , the post - lasso estimator obeys @xmath599",
    "the proof of theorem [ theorem : inferencealg1 ] consists of verifying conditions 2 and 3 and application of theorem [ theorem2 ] .",
    "we will use the properties of the post-@xmath20-penalized median regression and the post - lasso estimator together with required regularity conditions stated in section [ sec : analysisaux ] of this supplementary material .",
    "moreover , we will use lemmas [ cor : loadingconvergence ] and [ lemma : equivnorm ] stated in section [ sec : auxiliary ] of this supplementary material . in this proof",
    "we focus on algorithm 1 .",
    "the proof for algorithm 2 is essentially the same as that for algorithm 1 and deferred to the next subsection .    in application of theorem [ theorem2 ] ,",
    "take @xmath600 , @xmath601 $ ] where @xmath602 will be specified later , and @xmath603 , we omit the subindex `` @xmath604 . '' in what follows , we will separately verify conditions [ condition : sp ] and [ condition : as ] .",
    "verification of condition [ condition : sp ] : part ( i ) .",
    "the first condition follows from the zero median condition , that is , @xmath605 .",
    "we will show in verification of condition [ condition : as ] that with probability @xmath276 , @xmath606 , so that for some sufficiently small @xmath607 , @xmath608 \\subset { \\widehat}{{\\mathcal{a } } } \\subset { \\mathcal{a}}$ ] , with probability @xmath276 .    part ( ii ) .",
    "the map @xmath609 ( d - t_2 ) \\mid x)\\ ] ] is twice continuously differentiable since @xmath610 is continuous . for every @xmath611",
    ", @xmath612 is @xmath613 $ ] or @xmath614 $ ] or @xmath615 $ ] .",
    "hence for every @xmath616 , @xmath617 | \\leq c_{1 } { e } ( |d v| \\mid x ) \\vee c_{1 } { e } ( |v| \\mid x ) \\vee 1.\\ ] ] the expectation of the square of the right side is bounded by a constant depending only on @xmath618 , as @xmath619 .",
    "moreover , let @xmath620 with any fixed constant @xmath621 .",
    "then for every @xmath622 , whenever @xmath623 , @xmath624.\\end{aligned}\\ ] ] since @xmath625 for @xmath626 , and @xmath627 , we have @xmath628 \\leq 2 { e } [ \\ { m^{2}(x ) + v^{2 } \\ } ( c_{3 } + |v| ) \\mid x ] \\\\ & \\quad \\leq 2 { e}\\ { ( m_{n}^{2}+v^{2})(c_{3 } + |v| ) \\mid x \\ }",
    "\\lesssim m_{n}^{2}.\\end{aligned}\\ ] ] similar computations lead to @xmath629 for some constant @xmath362 depending only on @xmath618 .",
    "we wish to verify the last condition in ( ii ) . for every @xmath630 , @xmath631 \\leq c_{1 } { e}\\ { | d ( d - t_{2 } ) | \\mid x \\ } | \\alpha - \\alpha ' | \\\\ & \\quad + c_{1 } { e}\\ { | ( d - t_{2 } ) | \\mid x \\ } | t_{1 } - t_{1 } ' | + ( t_{2}-t_{2}')^{2 } \\leq c'm_{n } ( |\\alpha-\\alpha'| + |t_{1}-t_{1}'| ) + ( t_{2}-t_{2}')^{2},\\end{aligned}\\ ] ] where @xmath632 is a constant depending only on @xmath618 . here as @xmath633 , the right side is bounded by @xmath634 .",
    "hence we can take @xmath635 and @xmath272 .",
    "part ( iii ) .",
    "recall that @xmath636 .",
    "then we have @xmath637    part ( iv ) .",
    "pick any @xmath616 .",
    "there exists @xmath638 between @xmath157 and @xmath639 such that @xmath640 = \\partial_{\\alpha } { e } [ \\psi\\{w,\\alpha_{0},h(x)\\ } ] ( \\alpha-\\alpha_{0 } ) + \\frac{1}{2 } \\partial_{\\alpha}^{2 } { e } [ \\psi\\{w,\\alpha',h(x)\\ } ] ( \\alpha-\\alpha_{0})^{2}\\end{aligned}\\ ] ] let @xmath641 = f_{\\epsilon}(0){e}(v^{2 } ) \\geq c_{1}^{2}$ ] .",
    "then since @xmath642 | \\leq c_{1 } { e } ( | d^{2 } v| ) \\leq c_{2}$ ] where @xmath643 can be taken depending only on @xmath119 , we have @xmath644 \\geq \\frac{1}{2 } \\gamma | \\alpha-\\alpha_{0 } |,\\ ] ] whenever @xmath645 .",
    "take @xmath646 in the definition of @xmath647 .",
    "then the above inequality holds for all @xmath616 .",
    "part ( v ) . observe that @xmath648=(1/4){e}(v^{2 } ) \\geq c_{1}/4 $ ] .",
    "verification of condition [ condition : as ] : note here that @xmath649 and @xmath650 .",
    "we first show that the estimators @xmath651 are sparse and have good rate properties .",
    "the estimator @xmath501 is based on post-@xmath20-penalized median regression with penalty parameters as suggested in section [ sec : step1 ] of this supplementary material . by assumption in theorem [ theorem : inferencealg1 ] , with probability @xmath146 we have @xmath652 .",
    "next we verify that condition [ conditionplad ] in section [ sec : step1 ] of this supplementary material is implied by condition [ condition i ] and invoke lemmas [ theorem : l1qrnp ] and [ thm : maintwostep ] .",
    "the assumptions on the error density @xmath653 in condition [ conditionplad ] ( i ) are assumed in condition [ condition i ] ( iv ) .",
    "because of conditions [ condition i ] ( v ) and ( vi ) , @xmath654 is bounded away from zero for @xmath1 sufficiently large , see lemma 4.1 in @xcite , and @xmath655 for every @xmath535 .",
    "moreover , under condition [ condition i ] , by lemma [ lemma : equivnorm ] , we have @xmath656 and @xmath657 with probability @xmath276 for some @xmath658 . the required side condition of lemma [ theorem : l1qrnp ] is satisfied by relations ( [ verificationlemma4a ] ) and ( [ verificationlemma4b ] ) ahead . by lemma [ thm : maintwostep ] in section [ sec : step1 ] of this supplementary material , we have @xmath659 since the required side condition holds . indeed , for @xmath562 and @xmath660 ,",
    "because @xmath661 with probability @xmath146 , @xmath662 , and @xmath663 , we have @xmath664 therefore , since @xmath665 , we have @xmath666 the argument above also shows that @xmath667 with probability @xmath276 as claimed in verification of condition [ condition : sp ] ( i ) .",
    "indeed by lemma [ theorem : l1qrnp ] and remark [ comment : alpha ] we have @xmath668 with probability @xmath276 as @xmath669 .",
    "the @xmath670 is a post - lasso estimator with penalty parameters as suggested in section [ sec : estlasso ] of this supplementary material .",
    "we verify that condition [ conditionhl ] in section [ sec : estlasso ] of this supplementary material is implied by condition [ condition i ] and invoke lemma [ corollary3:postrate ] .",
    "indeed , condition [ conditionhl ] ( ii ) is implied by conditions [ condition i ] ( ii ) and ( iv ) , where condition [ condition i](iv ) is used to ensure @xmath671 .",
    "next since @xmath672 , condition [ conditionhl ] ( iii ) is satisfied if @xmath673 , which is implied by condition [ condition i ] ( v ) .",
    "condition [ conditionhl ] ( iv ) follows from lemma [ cor : loadingconvergence ] applied twice with @xmath674 and @xmath675 as @xmath676 and @xmath677 .",
    "condition [ conditionhl ] ( v ) follows from lemma [ lemma : equivnorm ] . by lemma [ corollary3:postrate ] in section [ sec : estlasso ] of this supplementary material",
    ", we have @xmath678 and @xmath679 with probability @xmath276 .",
    "thus , by lemma [ lemma : equivnorm ] , we have @xmath680 . moreover , @xmath681 .",
    "combining these results , we have @xmath682 with probability @xmath276 , where @xmath683 \\leq \\ell_n ' s(\\log a_n)/n\\ } , \\\\",
    "\\mathcal{h}_2 & = \\ { { \\widetilde}h_2 : { \\widetilde}h_2(x)=x^ { { { \\mathrm{\\scriptscriptstyle t } } } } \\theta , \\| \\theta \\|_{0 } \\leq c_3s , { \\textstyle \\sup}_{\\| x \\|_{\\infty } \\leq k_{n } } | { \\widetilde}h_{2}(x ) - m(x ) | \\leq c_{3 } , \\\\ & \\qquad { e } [ \\{{\\widetilde}h_2(x)-m(x)\\}^2 ] \\leq \\ell_n ' s(\\log a_n)/n\\},\\end{aligned}\\ ] ] with @xmath684 a sufficiently large constant and @xmath685 sufficiently slowly .    to verify condition [ condition : as ] ( ii ) , observe that @xmath686 , where @xmath687 , and @xmath688 and @xmath689 are the classes of functions defined by @xmath690 the classes @xmath691 , and @xmath692 , as @xmath693 is monotone and by lemma 2.6.18 in @xcite , consist of unions of @xmath13 choose @xmath694 vc - subgraph classes with vc indices at most @xmath695 .",
    "the class @xmath692 is uniformly bounded by @xmath696 ; recalling @xmath697 , for @xmath698 , @xmath699 .",
    "hence by theorem 2.6.7 in @xcite , we have @xmath700 for all @xmath701 for some constant @xmath702 that depends only on @xmath684 ; see the proof of lemma 11 in @xcite for related arguments .",
    "it is now straightforward to verify that the class @xmath703 satisfies the stated entropy condition ; see the proof of theorem 3 in @xcite , relation ( a.7 ) .    to verify condition [ condition : as ] ( iii ) , observe that whenever @xmath698 , @xmath704 which has four bounded moments , so that condition [ condition : as ] ( iii ) is satisfied with @xmath705 .    to verify condition [ condition : as ] ( iv ) , take @xmath706 with @xmath685 sufficiently slowly and @xmath707 as @xmath708 , @xmath709 and @xmath710 , condition [ condition : as ] ( iv ) is satisfied provided that @xmath711 and @xmath712 , which are implied by condition [ condition i ] ( v ) with @xmath685 sufficiently slowly .",
    "therefore , for @xmath713 = { e}(v_{{{i}}}^2)/\\{4f_\\epsilon^2(0)\\}$ ] , by theorem [ theorem2 ] we obtain the first result : @xmath714 .",
    "next we prove the second result regarding @xmath715 .",
    "first consider the denominator of @xmath716 .",
    "we have @xmath717 where we have used @xmath718 and @xmath719 .",
    "second consider the numerator of @xmath716 .",
    "since @xmath720=0 $ ] we have @xmath721 & =   { e_n}[\\psi\\{w,\\alpha_0,h(x)\\ } ] + o_{p}(n^{-1/2 } ) , \\end{array}\\ ] ] using the expansion in the displayed equation of _ step _ 1 in the proof of theorem [ theorem2 ] evaluated at @xmath5 instead of @xmath722 .",
    "therefore , using the identity that @xmath723 with @xmath724 , \\",
    "b_n = { e_n}[\\psi\\{w,\\alpha_0,h(x)\\ } ] , \\ \\",
    "\\lesssim_p \\ { { e}(v_{{{i}}}^2 ) \\}^{1/2 } n^{-1/2},\\ ] ] we have @xmath725|^2}{{e_n}({\\widehat}v_{{{i}}}^2 ) } = \\frac{4n|{e_n}[\\psi\\{w,\\alpha_0 , h(x)\\}]|^2}{{e_n}[\\psi^2\\{w,\\alpha_0 , h(x)\\ } ] } + o_{p}(1)\\ ] ] since @xmath726 is bounded away from zero . by theorem 7.1 in @xcite , and the moment conditions @xmath727 and @xmath728 , the following holds for the self - normalized sum @xmath729}{({e_n}[\\psi^2\\{w,\\alpha_0 , h(x)\\}])^{1/2}}\\to \\mathcal{n}(0,1)\\ ] ] in distribution and the desired result follows since @xmath730 .",
    "an inspection of the proof leads to the following stochastic expansion : @xmath731 & = -\\ { f_\\epsilon { e } ( v_{{{i}}}^2 ) \\ } ( { \\widehat}\\alpha - \\alpha_0)+    { e_n}[\\psi \\ { w,\\alpha_0 , h(x)\\ } ] \\\\ & \\quad + o_{p } ( n^{-1/2 } + n^{-1/4 } | { \\widehat}\\alpha - \\alpha_{0}| ) + o_{p}(| { \\widehat}\\alpha - \\alpha_{0}|^{2}),\\end{aligned}\\ ] ] where @xmath109 is any consistent estimator of @xmath157 . hence provided that @xmath732 , the remainder term in the above expansion is @xmath498 , and the one - step estimator @xmath51 defined by @xmath733\\ ] ] has the following stochastic expansion : @xmath734 + o_{p}(n^{-1/2 } ) ] \\\\   & = \\alpha_{0 } + \\ { f_{\\epsilon } { e } ( v_{{{{i}}}}^{2 } ) \\ } ^{-1 }    { e_n } [ \\psi \\ { w,\\alpha_0,h(x ) \\ } ] + o_{p}(n^{-1/2}),\\end{aligned}\\ ] ] so that @xmath735 in distribution .",
    "the proof is essentially the same as the proof for algorithm 1 and just verifying the rates for the penalized estimators .",
    "the estimator @xmath151 is based on @xmath20-penalized median regression .",
    "condition [ conditionplad ] is implied by condition [ condition i ] , see the proof for algorithm 1 . by lemma [",
    "theorem : l1qrnp ] and remark [ comment : alpha ] we have with probability @xmath276 @xmath736 because @xmath737 and the required side condition holds .",
    "indeed , without loss of generality assume that @xmath567 contains @xmath487 so that for @xmath562 , @xmath660 , because @xmath654 is bounded away from zero , and the fact that @xmath738 , we have @xmath739 therefore , since @xmath665 , we have @xmath740    the estimator @xmath46 is based on lasso .",
    "condition [ conditionhl ] is implied by condition [ condition i ] and lemma [ cor : loadingconvergence ] applied twice with @xmath674 and @xmath675 as @xmath741 . by lemma [ thm : rateestimatedlasso ]",
    "we have @xmath742 .",
    "moreover , by lemma [ corollary3:postrate ] we have @xmath743 with probability @xmath276 .",
    "the required rate in the @xmath744 norm follows from lemma [ lemma : equivnorm ] .",
    "in this section we provide additional experiments to further examine the finite sample performance of the proposed estimators . the experiments",
    "investigate the performance of the method on approximately spare models and complement the experiments on exactly sparse models presented in the main text .",
    "specifically , we considered the following regression model : @xmath745 where @xmath316 , and now we have @xmath746 .",
    "the other features of the design are the same as the design presented in the main text .",
    "namely , the vector @xmath747 consists of an intercept and covariates @xmath748 , and the errors @xmath95 and @xmath749 are independently and identically distributed as @xmath750 .",
    "the dimension @xmath13 of the covariates @xmath751 is @xmath322 , and the sample size @xmath1 is @xmath323 . the regressors are correlated with @xmath325 and @xmath326 .",
    "we vary the @xmath329 in the two equations , denoted by @xmath332 and @xmath333 respectively , in the set @xmath752 , which results in 100 different designs induced by the different pairs of @xmath335 .",
    "we performed 500 monte carlo repetitions for each .    in this design , the vector @xmath337 has all @xmath13 components different from zero . because the coefficients decay it is conceivable that it can be well approximated by considering only a few components , typically the ones associated with the largest coefficients in absolute values",
    ". the coefficients omitted from that construction define the approximation error .",
    "however , the number of coefficients needed to achieve a good approximation will also depend on the scalings @xmath327 and @xmath328 since they multiply all coefficients . therefore ,",
    "if @xmath327 or @xmath328 is large the approximation might require a larger number of coefficients which can violate our sparsity requirements .",
    "this is the main distinction from the an exact sparse designs considered in the main text .",
    "the simulation study focuses on algorithm 1 since the algorithm based on double selection worked similarly .",
    "standard errors are computed using the formula ( [ eq : robustse ] ) . as the main benchmark we consider the standard post - model selection estimator @xmath338 based on the post-@xmath20-penalized median regression method , as defined in ( [ def : postl1qr ] ) .",
    "level tests of a true hypothesis based on : ( a ) the standard post - model selection procedure based on @xmath338 , and ( b ) the proposed post - model selection procedure based on @xmath51 .",
    "ideally we should observe a flat surface at the @xmath341 rejection rate ( of a true null).,title=\"fig:\",scaledwidth=49.0% ]   level tests of a true hypothesis based on : ( a ) the standard post - model selection procedure based on @xmath338 , and ( b ) the proposed post - model selection procedure based on @xmath51 .",
    "ideally we should observe a flat surface at the @xmath341 rejection rate ( of a true null).,title=\"fig:\",scaledwidth=49.0% ]     ( panels ( a)-(c ) ) , and of the proposed post - model selection estimator @xmath51 ( panels ( d)-(f)).,title=\"fig:\",scaledwidth=49.0% ]   ( panels ( a)-(c ) ) , and of the proposed post - model selection estimator @xmath51 ( panels ( d)-(f)).,title=\"fig:\",scaledwidth=49.0% ]   ( panels ( a)-(c ) ) , and of the proposed post - model selection estimator @xmath51 ( panels ( d)-(f)).,title=\"fig:\",scaledwidth=49.0% ]   ( panels ( a)-(c ) ) , and of the proposed post - model selection estimator @xmath51 ( panels ( d)-(f)).,title=\"fig:\",scaledwidth=49.0% ]   ( panels ( a)-(c ) ) , and of the proposed post - model selection estimator @xmath51 ( panels ( d)-(f)).,title=\"fig:\",scaledwidth=49.0% ]   ( panels ( a)-(c ) ) , and of the proposed post - model selection estimator @xmath51 ( panels ( d)-(f)).,title=\"fig:\",scaledwidth=49.0% ]    figure [ fig : simfirstsupp ] displays the empirical rejection probability of tests of a true hypothesis @xmath340 , with nominal size of tests equal to @xmath341 .",
    "the rejection frequency of the standard post - model selection inference procedure based upon @xmath338 is very fragile , see left plot . given the approximately sparse model considered here , there is no true model to be perfectly recovered and the rejection frequency deviates substantially from the ideal rejection frequency of @xmath341 .",
    "the right plot shows the corresponding empirical rejection probability for the proposed procedures based on estimator @xmath51 and the result ( [ eq : result1 ] ) .",
    "the performance is close to the ideal level of @xmath341 over 99 out of the 100 designs considered in the study which illustrate the uniformity property .",
    "the design for which the procedure does not perform well corresponds to @xmath753 and @xmath754 .",
    "figure [ fig : simsecondsupp ] compares the performance of the standard post - selection estimator @xmath338 , as defined in ( [ def : postl1qr ] ) , and our proposed post - selection estimator @xmath51 obtained via algorithm 1 .",
    "we display results in the same three metrics used in the main text : mean bias , standard deviation , and root mean square error of the two approaches . in those metrics , except for one design , the performance for approximately sparse models is very similar to the performance of exactly sparse models . the proposed post - selection estimator @xmath51 performs well in all three metrics while the standard post - model selection estimators @xmath338 exhibits a large bias in many of the dgps considered . for the design with @xmath753 and @xmath754 , both procedures breakdown .    except for the design with largest values of @xmath329 s , @xmath753 and @xmath755 , the results are very similar to the results presented in the main text for an exactly sparse model where the proposed procedure performs very well .",
    "the design with the largest values of @xmath329 s correspond to large values of @xmath327 and @xmath328 . in that case",
    "too many coefficients are needed to achieve a good approximation for the unknown functions @xmath756 and @xmath757 which translates into a ( too ) large value of @xmath19 in the approximate sparse model .",
    "such performance is fully consistent with the theoretical result derived in theorem [ theorem2 ] which covers approximately sparse models but do impose sparsity requirements .",
    "in this section we collect some auxiliary technical results .    [ cor : loadingconvergence ]",
    "let @xmath758 be independent random vectors where @xmath759 are scalar while @xmath760 are vectors in @xmath761 .",
    "suppose that @xmath762 for @xmath763 , and there exists a constant @xmath136 such that @xmath764 almost surely .",
    "then for every @xmath765 , with probability at least @xmath766 , @xmath767      [ lem : maxineq ] let @xmath768 be independent random vectors in @xmath761 . then for every @xmath769 and @xmath770 , with probability at least @xmath771 , @xmath772 \\\\ \\vee 2\\max_{j=1,\\ldots , p } q[1/2 , | n^{-1/2 } { \\textstyle \\sum}_{i=1}^{n } \\ { z_{ij } - { e}(z_{ij } ) \\}|],\\end{gathered}\\ ] ] where for a random variable @xmath524 we denote @xmath773 .",
    "going back to the proof of lemma [ cor : loadingconvergence ] , let @xmath774 . by markov s inequality , we have @xmath775 \\leq \\{2n^{-1 } { \\textstyle \\sum}_{i=1}^{n } { e } ( z_{ij}^{2})\\}^{1/2 } \\leq k_{n}^{2 } \\{(2/n){\\textstyle \\sum}_{i=1}^{n } { e}(\\zeta_{i}^{4})\\}^{1/2},\\ ] ] and @xmath776 hence the conclusion of lemma [ cor : loadingconvergence ] follows from application of lemma [ lem : maxineq ] with @xmath777 .",
    "[ lemma : bound2nnorm ] consider vectors @xmath151 and @xmath14 in @xmath761 where @xmath130 , and denote by @xmath780 the vector @xmath151 truncated to have only its @xmath781 largest components in absolute value . then @xmath782      by the triangle inequality we have @xmath786 for an integer @xmath787 , @xmath788 and @xmath789 .",
    "moreover , given the monotonicity of the components , @xmath790 then @xmath791 where the last inequality follows from the arguments used to show the first result ."
  ],
  "abstract_text": [
    "<S> we develop uniformly valid confidence regions for regression coefficients in a high - dimensional sparse median regression model with homoscedastic errors . </S>",
    "<S> our methods are based on a moment equation that is immunized against non - regular estimation of the nuisance part of the median regression function by using neyman s orthogonalization . </S>",
    "<S> we establish that the resulting instrumental median regression estimator of a target regression coefficient is asymptotically normally distributed uniformly with respect to the underlying sparse model and is semi - parametrically efficient . </S>",
    "<S> we also generalize our method to a general non - smooth z - estimation framework with the number of target parameters @xmath0 being possibly much larger than the sample size @xmath1 . </S>",
    "<S> we extend huber s results on asymptotic normality to this setting , demonstrating uniform asymptotic normality of the proposed estimators over @xmath0-dimensional rectangles , constructing simultaneous confidence bands on all of the @xmath0 target parameters , and establishing asymptotic validity of the bands uniformly over underlying approximately sparse models . </S>",
    "<S> + keywords : instrument ; post - selection inference ; sparsity ; neyman s orthogonal score test ; uniformly valid inference ; z - estimation . </S>",
    "<S> + publication : biometrika , 2014 doi:10.1093/biomet / asu056 </S>"
  ]
}