{
  "article_text": [
    "the large hadron collider ( lhc ) , currently under construction at cern , will collide protons on protons with an energy of @xmath6  tev . together with its high collision rate the high available centre - of - mass energy",
    "will make it possible to test new interactions at very short distances that might be revealed in the production cross - sections of standard model ( sm ) particles at very high transverse momentum ( @xmath7 ) as deviation from the sm theory .",
    "the sensitivity to new physics crucially depends on experimental uncertainties in the measurements and on theoretical uncertainties in the sm predictions .",
    "it is therefore important to work out a strategy to minimize both the experimental and theoretical uncertainties from lhc data .",
    "for instance , one could use single inclusive jet or drell - yan cross - sections at low @xmath7 to constrain the pdf uncertainties at high @xmath7 .",
    "typical residual renormalisation and factorisation scale uncertainties in next - to - leading order ( nlo ) calculations for single inclusive jet - cross - section are about @xmath8 and should hopefully be reduced as nnlo calculations become available",
    ". the impact of pdf uncertainties on the other hand can be substantially larger in some regions , especially at large @xmath7 , and for example at @xmath9  gev dominate the overall uncertainty of @xmath10 . if a suitable combination of data measured at the tevatron and lhc can be included in global nlo qcd analyses , the pdf uncertainties can be constrained .",
    "the aim of this contribution is to propose a method for consistently including final - state observables in global qcd analyses .    for inclusive data like",
    "the proton structure function @xmath11 in deep - inelastic scattering ( dis ) the perturbative coefficients are known analytically . during the fit the cross - section",
    "can therefore be quickly calculated from the strong coupling ( @xmath12 ) and the pdfs and can be compared to the measurements . however , final state observables , where detector acceptances or jet algorithms are involved in the definition of the perturbative coefficients ( called `` weights '' in the following ) , have to be calculated using nlo monte carlo programs .",
    "typically such programs need about one day of cpu time to calculate accurately the cross - section .",
    "it is therefore necessary to find a way to calculate the perturbative coefficients with high precision in a long run and to include @xmath12 and the pdfs `` a posteriori '' .    to solve this problem",
    "many methods have been proposed in the past @xcite . in principle",
    "the highest efficiencies can be obtained by taking moments with respect to bjorken-@xmath0 @xcite , because this converts convolutions into multiplications .",
    "this can have notable advantages with respect to memory consumption , especially in cases with two incoming hadrons . on the other hand , there are complications such as the need for pdfs in moment space and the associated inverse mellin transforms",
    ".    methods in @xmath0-space have traditionally been somewhat less efficient , both in terms of speed ( in the ` a posteriori ' steps  not a major issue here ) and in terms of memory consumption .",
    "they are , however , somewhat more transparent since they provide direct information on the @xmath0 values of relevance .",
    "furthermore they can be used with any pdf",
    ". the use of @xmath0-space methods can be further improved by using methods developed originally for pdf evolution @xcite .",
    "we make the assumption that pdfs can be accurately represented by storing their values on a two - dimensional grid of points and using @xmath13-order interpolations between those points . instead of using the parton momentum fraction @xmath0 and the factorisation scale @xmath14",
    ", we use a variable transformation that provides good coverage of the full @xmath0 and @xmath14 range with uniformly spaced grid points : grid is to use @xmath15 with @xmath16 a parameter that serves to increase the density of points in the large @xmath0 region . ]",
    "@xmath17 the parameter @xmath18 is to be chosen of the order of @xmath19 , but not necessarily identical .",
    "the pdf @xmath20 is then represented by its values @xmath21 at the 2-dimensional grid point @xmath22 , where @xmath23 and @xmath24 denote the grid spacings , and obtained elsewhere by interpolation : @xmath25 where @xmath26 , @xmath27 are the interpolation orders .",
    "the interpolation function @xmath28 is 1 for @xmath29 and otherwise is given by : @xmath30 defining @xmath31 to be the largest integer such that @xmath32 , @xmath33 and @xmath34 are defined as : @xmath35 given finite grids whose vertex indices range from @xmath36 for the @xmath37 grid and @xmath38 for the @xmath39 grid , one should additionally require that eq .",
    "( [ eq : interp ] ) only uses available grid points .",
    "this can be achieved by remapping @xmath40 and @xmath41 .",
    "suppose that we have an nlo monte carlo program that produces events @xmath42 .",
    "each event @xmath43 has an @xmath0 value , @xmath44 , a @xmath14 value , @xmath45 , as well as a weight , @xmath46 , and a corresponding order in @xmath12 , @xmath47 .",
    "normally one would obtain the final result @xmath48 of the monte carlo integration from : @xmath49    instead one introduces a weight grid @xmath50 and then for each event updates a portion of the grid with : + @xmath51 @xmath52 the final result for @xmath48 , for an arbitrary pdf , can then be obtained _ subsequent _ to the monte carlo run : @xmath53 where the sums index with @xmath54 and @xmath55 run over the number of grid points and we have have explicitly introduced @xmath56 and @xmath57 such that : @xmath58      if one has the weight matrix @xmath50 determined separately order by order in @xmath12 , it is straightforward to vary the renormalisation @xmath59 and factorisation @xmath60 scales a posteriori ( we assume that they were kept equal in the original calculation ) .",
    "it is helpful to introduce some notation relating to the dglap evolution equation : @xmath61 where the @xmath62 and @xmath63 are the lo and nlo matrices of dglap splitting functions that operate on vectors ( in flavour space ) @xmath64 of pdfs .",
    "let us now restrict our attention to the nlo case where we have just two values of @xmath65 , @xmath66 and @xmath67 .",
    "introducing @xmath68 and @xmath69 corresponding to the factors by which one varies @xmath59 and @xmath60 respectively , for arbitrary @xmath68 and @xmath69 we may then write : @xmath70 \\ , , \\nonumber\\end{aligned}\\ ] ] where @xmath71 and @xmath72 ( @xmath73 ) is the number of colours ( flavours ) . though this formula is given for @xmath0-space based approach ,",
    "a similar formula applies for moment - space approaches .",
    "furthermore it is straightforward to extend it to higher perturbative orders .      in",
    "hadron - hadron scattering one can use analogous procedures with one more dimension .",
    "besides @xmath14 , the weight grid depends on the momentum fraction of the first ( @xmath74 ) and second ( @xmath75 ) hadron .    in the case of jet production in proton - proton collisions",
    "the weights generated by the monte carlo program as well as the pdfs can be organised in seven possible initial state combinations of partons : @xmath76 where @xmath77 denotes gluons , @xmath64 quarks and @xmath78 quarks of different flavour @xmath79 and we have used the generalized pdfs defined as : @xmath80 where @xmath81 is the pdf of flavour @xmath82 for hadron @xmath83 and @xmath84 ( @xmath85 ) denotes the first or second hadron , quarks have values from @xmath86-@xmath87 and anti - quarks have the corresponding negative values . ] .",
    "the analogue of eq .",
    "[ eq : wfinalxq ] is then given by : @xmath88      it is again possible to choose arbitrary renormalisation and factorisation scales , specifically for nlo accuracy : @xmath89 \\ , , \\nonumber\\end{aligned}\\ ] ] where @xmath90 is calculated as @xmath91 , but with @xmath92 replaced wtih @xmath93 , and analogously for @xmath94 .",
    "to test the scheme discussed above we use the nlo monte carlo program nlojet++ @xcite and the cteq6 pdfs @xcite .",
    "the grid @xmath95 of eq .",
    "[ eq : wfinalxq_twohadrons ] is filled in a nlojet++ user module .",
    "this module has access to the event weight and parton momenta and it is here that one specifies and calculates the physical observables that are being studied ( e.g. jet algorithm ) .    having filled the grid we construct the cross - section in a small standalone program which reads the weights from the grid and multiplies them with an arbitrary @xmath12 and pdf according to eq .  [ eq : wfinalxq_twohadrons ] .",
    "this program runs very fast ( in the order of seconds ) and can be called in a pdf fit .",
    "the connection between these two programs is accomplished via a c++ class , which provides methods e.g. for creating and optimising the grid , filling weight events and saving it to disk .",
    "the classes are general enough to be extendable for the use with other nlo calculations .    the complete code for the nlojet++ module , the c++ class and the standalone job is available from the authors .",
    "it is still in a development , testing and tuning stage , but help and more ideas are welcome .",
    "the main data members of this class are the grids implemented as arrays of three - dimensional root histograms , with each grid point at the bin centers , @xmath75 and @xmath14 grid boundaries using the nlojet++ program before final filling . to avoid this residual overhead and to exploit certain symmetries in the grid , a special data class ( e.g. a sparse matrix )",
    "might be constructed in the future . ] : @xmath96[l][iobs](x_1,x_2,q^2)},\\ ] ] where the @xmath97 and @xmath65 are explained in eq .",
    "[ eq : wfinalxq_twohadrons ] and @xmath98 denotes the observable bin , e.g. a given @xmath7 range and the @xmath99 initial state parton configurations in one grid .",
    "in addition , the weights for some of the initial state parton configurations are symmetric in @xmath74 and @xmath75 .",
    "this could be exploited in future applications to further reduce the grid size . ] .",
    "the c++ class initialises , stores and fills the grid using the following main methods :    * _ default constructor : _ given the pre - defined kinematic regions of interest , it initializes the grid . *",
    "_ optimizing method : _ since in some bins the weights will be zero over a large kinematic region in @xmath100 , the optimising method implements an automated procedure to adapt the grid boundaries for each observable bin .",
    "these boundaries are calculated in a first ( short ) run . in the present implementation ,",
    "the optimised grid has a fixed number of grid points .",
    "other choices , like a fixed grid spacing , might be implemented in the future . *",
    "_ loading method : _ reads the saved weight grid from a root file * _ saving method : _ saves the complete grid to a root file , which will be automatically compressed .      the user module has to be adapted specifically to the exact definition of the cross - section calculation .",
    "if a grid file already exists in the directory where nlojet++ is started , the grid is not started with the default constructor , but with the optimizing method ( see [ sec : class ] ) . in this way",
    "the grid boundaries are optimised for each observable bin .",
    "this is necessary to get very fine grid spacings without exceeding the computer memory .",
    "the grid is filled at the same place where the standard nlojet++ histograms are filled . after a certain number of events , the grid is saved in a root - file and the calculation",
    "is continued .",
    "the standalone program calculates the cross - section in the following way :    1 .",
    "load the weight grid from the root file 2 .",
    "initialize the pdf interface , load @xmath20 on a helper pdf - grid ( to increase the performance ) 3 .",
    "for each observable bin , loop over @xmath101 and calculate @xmath102 from the appropriate pdfs @xmath20 , multiply @xmath12 and the weights from the grid and sum over the initial state parton configuration @xmath97 , according to eq .",
    "[ eq : wfinalxq_twohadrons ] .",
    "we calculate the single inclusive jet cross - section as a function of the jet transverse momentum ( @xmath7 ) for jets within a rapidity of @xmath3 . to define the jets we use the seedless cone jet algorithm as implemented in nlojet++ using the four - vector recombination scheme and the midpoint algorithm .",
    "the cone radius has been put to @xmath103 , the overlap fraction was set to @xmath104 .",
    "we set the renormalisation and factorization scale to @xmath105 , where @xmath106 is the @xmath7 of the highest @xmath7 jet in the required rapidity region will in general differ from the @xmath7 of the other jets , so when binning an inclusive jet cross section , the @xmath7 of a given jet may not correspond to the renormalisation scale chosen for the event as a whole .",
    "for this reason we shall need separate grid dimensions for the jet @xmath7 and for the renormalisation scale . only in certain moment - space approaches @xcite has this requirement so far been efficiently circumvented . ] .    in our test runs , to be independent from statistical fluctuations ( which can be large in particular in the nlo case ) , we fill in addition to the grid a reference histogram in the standard way according to eq .",
    "[ eq : normalint ] .",
    "the choice of the grid architecture depends on the required accuracy , on the exact cross - section definition and on the available computer resources . here",
    ", we will just sketch the influence of the grid architecture and the interpolation method on the final result .",
    "we will investigate an example where we calculate the inclusive jet cross - section in @xmath107 bins in the kinematic range @xmath108 . in future applications this can serve as guideline for a user to adapt the grid method to his / her specific problem .",
    "we believe that the code is transparent and flexible enough to adapt to many applications .    as reference for comparisons of different grid architectures and interpolation methods we use",
    "the following :    * _ grid spacing in @xmath109 : _ @xmath110 with @xmath111 * _ grid spacing in @xmath112 : _ @xmath113 with @xmath114 * _ order of interpolation : _",
    "@xmath115    the grid boundaries correspond to the user setting for the first run which determines the grid boundaries for each observable bin . in the following we call this grid architecture @xmath116x@xmath117x@xmath118",
    "such a grid takes about @xmath119  mbyte of computer memory .",
    "the root - file where the grid is stored has about @xmath120  mbyte .",
    "the result is shown in fig .",
    "[ fig:101obsbins]a ) .",
    "the reference cross - section is reproduced everywhere to within @xmath121 .",
    "the typical precision is about @xmath122 . at low and high @xmath7",
    "there is a positive bias of about @xmath123 . also shown in fig .",
    "[ fig:101obsbins]a ) are the results obtained with different grid architectures . for a finer @xmath0 grid ( @xmath124x@xmath117x@xmath118 ) the accuracy",
    "is further improved ( within @xmath125 ) and there is no bias .",
    "a finer ( @xmath116x@xmath126x@xmath118 ) as well as a coarser ( @xmath116x@xmath127x@xmath118 ) binning in @xmath14 does not improve the precision .",
    "[ fig:101obsbins]b ) and fig .",
    "[ fig:101obsbins]c ) show for the grid ( @xmath116x@xmath117x@xmath4 ) different interpolation methods . with an interpolation of order @xmath128",
    "the precision is @xmath122 and the bias at low and high @xmath7 observed for the @xmath129 interpolation disappears .",
    "the result is similar to the one obtained with finer @xmath0-points .",
    "thus by increasing the interpolation order the grid can be kept smaller .",
    "an order @xmath130 interpolation gives a systematic negative bias of about @xmath131 becoming even larger towards high @xmath7 .    depending on the available computer resources and the specific problem",
    ", the user will have to choose a proper grid architecture . in this context , it is interesting that a very small grid @xmath132x@xmath127x@xmath133 that takes only about @xmath127  mbyte computer memory reaches still a precision of @xmath134 , if an interpolation of order @xmath128 is used ( see fig .  [ fig:101obsbins]d ) ) .",
    "@xmath7 bins calculated with the grid technique and the reference cross - section calculated in the standard way . shown",
    "are the standard grid , grids with finer @xmath0 and @xmath14 sampling ( a ) with interpolation of order @xmath86 , @xmath135 and @xmath136 ( b ) ( and on a finer scale in c ) ) and a small grid ( d ) .",
    ", title=\"fig:\",scaledwidth=49.0% ]   @xmath7 bins calculated with the grid technique and the reference cross - section calculated in the standard way . shown",
    "are the standard grid , grids with finer @xmath0 and @xmath14 sampling ( a ) with interpolation of order @xmath86 , @xmath135 and @xmath136 ( b ) ( and on a finer scale in c ) ) and a small grid ( d ) .",
    ", title=\"fig:\",scaledwidth=49.0% ]   @xmath7 bins calculated with the grid technique and the reference cross - section calculated in the standard way . shown",
    "are the standard grid , grids with finer @xmath0 and @xmath14 sampling ( a ) with interpolation of order @xmath86 , @xmath135 and @xmath136 ( b ) ( and on a finer scale in c ) ) and a small grid ( d ) .",
    ", title=\"fig:\",scaledwidth=49.0% ]   @xmath7 bins calculated with the grid technique and the reference cross - section calculated in the standard way . shown",
    "are the standard grid , grids with finer @xmath0 and @xmath14 sampling ( a ) with interpolation of order @xmath86 , @xmath135 and @xmath136 ( b ) ( and on a finer scale in c ) ) and a small grid ( d ) .",
    ", title=\"fig:\",scaledwidth=49.0% ]    ( 0,0 ) ( -450,0)c ) ( -210,0)d ) ( -450,135)a ) ( -210,135)b )",
    "we have developed a technique to store the perturbative coefficients calculated by an nlo monte carlo program on a grid allowing for a - posteriori inclusion of an arbitrary parton density function ( pdf ) set .",
    "we extended a technique that was already successfully used to analyse hera data to the more demanding case of proton - proton collisions at lhc energies .",
    "the technique can be used to constrain pdf uncertainties , e.g. at high momentum transfers , from data that will be measured at lhc and allows the consistent inclusion of final state observables in global qcd analyses .",
    "this will help increase the sensitivity of lhc to find new physics as deviations from the standard model predictions .",
    "even for the large kinematic range for the parton momentum fractions @xmath74 and @xmath75 and of the squared momentum transfer @xmath14 accessible at lhc , grids of moderate size seem to be sufficient . the single inclusive jet cross - section in the central region @xmath3 can be calculated with a precision of @xmath122 in a realistic example with @xmath4 bins in the transverse jet energy range @xmath108 . in this example , the grid occupies about @xmath119  mbyte computer memory . with smaller grids of order @xmath127  mbyte",
    "the reachable accuracy is still @xmath134 .",
    "this is probably sufficient for all practical applications .",
    "we would like to thank z. nagy , m. h. seymour , t. schrner - sadenius , p. uwer and m. wobisch for useful discussions on the grid technique and a.  vogt for discussion on moment - space techniques .",
    "we thank z. nagy for help and support with nlojet++ .",
    "f.  siegert would like to thank cern for the summer student program ."
  ],
  "abstract_text": [
    "<S> any nlo calculation of a qcd final - state observable involves monte carlo integration over a large number of events . for dis and hadron colliders this </S>",
    "<S> must usually be repeated for each new pdf set , making it impractical to consider many ` error ' pdf sets , or carry out pdf fits . here </S>",
    "<S> we discuss `` a posteriori '' inclusion of pdfs , whereby the monte carlo run calculates a grid ( in @xmath0 and @xmath1 ) of cross section weights that can subsequently be combined with an arbitrary pdf . </S>",
    "<S> the procedure is numerically equivalent to using an interpolated form of the pdf . </S>",
    "<S> the main novelty relative to prior work is the use of higher - order interpolation , which substantially improves the tradeoff between accuracy and memory use . </S>",
    "<S> an accuracy of about @xmath2% has been reached for the single inclusive cross - section in the central rapidity region @xmath3 for jet transverse momenta from @xmath4 to @xmath5 . </S>",
    "<S> this method should facilitate the consistent inclusion of final - state data from hera , tevatron and lhc data in pdf fits , thus helping to increase the sensitivity of lhc to deviations from standard model predictions . </S>"
  ]
}