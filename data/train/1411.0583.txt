{
  "article_text": [
    "automatic differentiation ( short ad ) , also called algorithmic or computational differentiation , is a method to evaluate derivatives of functions which differs significantly from the classical ways of computer - based differentiation through either approximative , numerical methods , or through symbolic differentiation , using computer algebra systems . while approximative methods ( which are usually based on finite differences ) are inherently prone to truncation and rounding errors and suffer from numerical instability , symbolic differentiation may ( in certain cases ) lead to significant long computation times .",
    "automatic differentiation suffers from none of these problems and is , in particular , well - suited for the differentiation of functions implemented as computer code .",
    "furthermore , while automatic differentiation is also numerical differentiation , in the sense that it computes numerical values , it computes derivatives up to machine precision .",
    "that is , the only inaccuracies which occur are those which appear due to rounding errors in floating - point arithmetic or due to imprecise evaluations of elementary functions . for these reasons",
    ", ad has received significant interest from computer scientists and applied mathematicians , in the last decades .",
    "the very first article on this procedure is probably due to wengert @xcite and appeared already in 1964 .",
    "two further major publications regarding ad were published by rall in the 1980s @xcite , @xcite and , since then , there has been a growing community of researcher interested in this topic .",
    "so what is automatic differentiation ?",
    "the answer to this question may be sought in one of the many publications on this topic , which usually provide a short introduction to the general theory .",
    "furthermore , there are also excellent and comprehensive publications which describe the area as a whole ( see for example griewank @xcite and griewank and walther @xcite )",
    ". however , an unfamiliar reader may find it nevertheless difficult to grasp the essence of automatic differentiation .",
    "the problem lies in the diversity with which the ( actual simple ) ideas can be described . while in @xcite and ( * ? ? ?",
    "* section 2 ) the step - wise evaluation of a matrix - vector product is described as the basic procedure behind ad , in @xcite automatic differentiation is defined via a certain multiplication on pairs ( namely the multiplication which defines the algebra of _ dual numbers _ ) .",
    "similarly , in @xcite the lifting of a function to said dual numbers is presented as the core principle of ad , where in ( * ? ? ?",
    "* section 2 ) , the evaluation of the taylor series expansion of a function on dual numbers appears to be the main idea .",
    "finally , manzyuk @xcite bases his description on the push - forward operator known from differential geometry and , again , gives a connection to functions on dual numbers . while the latter descriptions at least appear to be similar ( although not identical ) , certainly the matrix - vector product based approach seems to differ from the remaining methods quite a lot .",
    "of course , all the publications mentioned contain plenty of cross - references and each different description of ad has its specific purpose .",
    "however , for somebody unfamiliar with the theory , it may still be difficult to see why the described techniques are essentially all equivalent .",
    "this article hopes to clarify the situation .",
    "we will in the following give short overviews of the distinct descriptions of ad mentioned above and show , why they all are just different expressions of the same principle .",
    "it is clear that the purpose of this article is mainly educational and there is little intrinsically new in our elaborations . indeed ,",
    "in particular with regards to @xcite , we only give a extremely shorted and simplified version of the work in the original publication .",
    "furthermore , there are actually at least two distinct versions , or modes , of ad . the so - called _ forward mode _ and the _ reverse mode _ ( along with variants such as _ checkpoint reverse mode _",
    "the different descriptions mentioned above all refer to the forward mode only .",
    "we are , therefore , mainly concerned with forward ad .",
    "we will discuss the standard reverse mode only in the preliminaries and in a section at the end of this paper .",
    "in addition , we will mainly restrict ourselves to ad in its simplest form .",
    "namely , automatic differentiation to compute ( directional ) first order derivatives , of a differentiable , multivariate function @xmath0 , on an open set @xmath1 .",
    "we only briefly discuss the computation of higher - order partial derivatives in section [ higher - order partial ] , referring mainly to the works of berz @xcite and karczmarczuk @xcite .",
    "there is also a rich literature on the computation of whole hessians ( see , for instance , @xcite or @xcite ) , however , we will not be concerned with this extension of ad in this article .",
    "the same holds for nested automatic differentiation , which involves a kind of recursive calling of ad ( see , for example , @xcite ) .",
    "again , we will not be concerned with this topic in this paper .",
    "as mention above , automatic differentiation is often ( and predominantly ) used to differentiate computer programs , that is , implementations of mathematical functions as code . in the case of first order forward ad , the mathematical principle used is usually the lifting of functions to dual numbers ( see figure [ figure haskell code dual ] for an implementation example with test case ) .",
    "more information on this topic can , for example , be found in @xcite , @xcite or ( in particular considering higher - order differentiation ) @xcite .",
    "the notation we are using is basically standard .",
    "as mentioned above , the function we want to differentiate will be denoted by @xmath2 and will be defined on an open set @xmath1 ( denoted by @xmath3 in section [ higher - order partial ] to avoid confusion ) . in particular , in this paper @xmath4 always denotes the number of variables of @xmath2 , while @xmath5 denotes the dimension of its co - domain . in sections [ section griewank ] and [ section reverse ] , the notation @xmath6 is reserved for variables of the function @xmath2 , while other variables are denoted by @xmath7 .",
    "the symbol @xmath8 always denotes a fixed value ( a constant ) . for real vectors ,",
    "we use boldface letters like @xmath9 or @xmath10 ( where the latter will be a constant vector ) .",
    "furthermore , @xmath11 and @xmath12 will be ( usually fixed ) directional vectors or @xmath13-row matrices , respectively .",
    "entries of @xmath11 or @xmath12 will be denoted by @xmath14 or @xmath15 , respectively for entries of @xmath11 is somewhat historical and based on the idea that , very often , @xmath14 may be considered as a derivative of either the identity function , or a constant function . for us , however , each @xmath16 is simply a chosen real number .",
    "the same holds for the notation @xmath15 .",
    "finally , we denote all multiplications ( of numbers , as well as matrix - vector multiplication ) mostly by a simple dot .",
    "the symbol @xmath17 will be used sometimes when we want to emphasize that multiplication of numbers is a differentiable function on @xmath18 .",
    "before we start with the theory , let us demonstrate the ideas of ad in a very easy case : let @xmath19 be differentiable functions with @xmath20 . let further @xmath21 be real numbers .",
    "assume we want to compute @xmath22 or @xmath23 , respectively .",
    "( of course , the distinction between multiplication from the left and from the right is motivated by the more general case of multivariate functions . )    by the chain rule , @xmath24 as one easily sees , the evaluation of @xmath22 can be achieved by computing successively the following pairs of real numbers :    . ]",
    "@xmath25 and taking the second entry of the final pair .",
    "as we see , the first element of each pair appears as an argument of the functions @xmath26 in the following pair , while the second element appears as a factor ( from the right ) to the second element in the following pair .    regarding the computation of @xmath23 , we have obviously @xmath27 the computation of this derivative can now be achieved by the computing the following two lists of real numbers : @xmath28 and taking the last entry of the second list . here ,",
    "each entry ( apart from @xmath29 ) in the second list consists of values of @xmath30 evaluated at an element of the first list ( note that the order is reversed ) and the previous entry as a factor ( from the left ) .    .",
    "]    in both examples , the computation of @xmath31 is actually unnecessary to obtain the sought derivative .",
    "this value is , however , computed in all models we will consider in this article .    if now the functions @xmath32 and their derivatives @xmath33 are implemented in the system , then the evaluation of values @xmath34 for some @xmath7 means simply calling these functions / derivatives with suitable inputs .",
    "the computation of @xmath22 or @xmath23 then becomes nothing else than obtaining values @xmath34 , performing a multiplication and passing the results on .",
    "that is , neither is some derivative evaluated symbolically , nor is some differential or difference quotient computed . in that sense ,",
    "the derivative of @xmath2 is computed ` automatically ' .      as mentioned above , ( first order ) automatic differentiation , in its simplest form ,",
    "is concerned with the computation of derivatives of a differentiable function @xmath0 , on an open set @xmath1 .",
    "the assumption made is that each @xmath35 in @xmath36 consists ( to be defined more precisely later ) of several sufficiently smooth so - called _ elementary _ ( or _ elemental _ ) _ functions _ @xmath37 , defined on open sets @xmath38 , with @xmath39 for some index set @xmath40 .",
    "the set of elementary functions @xmath41 has to be given and can , in principle , consist of arbitrary functions as long as these are sufficiently often differentiable . however , certain functions are essential for computational means , including addition and multiplication and @xmath42 . ] , constant , trigonometric , exponential functions etc .",
    "figure [ table essential functions ] shows a table of such a ( minimal ) list .",
    "a more comprehensive list can be found , for example , in ( * ? ? ?",
    "* table 2.3 ) .",
    "all elementary functions will be implemented in the system together with their gradients .",
    "automatic differentiation now does not compute the actual mapping + @xmath43 , which maps a vector @xmath44 to the jacobian @xmath45 of @xmath2 at @xmath9 . instead",
    ", directional derivatives of @xmath2 or left - hand products of row - vectors with its jacobian at a fixed vector @xmath46 are determined .",
    "that is , given @xmath46 and @xmath47 or @xmath48 , we determine either @xmath49 ( this is not a subtle difference , since , while @xmath43 is a matrix - valued function , @xmath50 and @xmath51 are vectors or one - row matrices , respectively , in euclidean space . )    the computation of directional derivatives of @xmath52 is referred to as the forward mode of ad , or forward ad , while the computation of @xmath53 is referred to as the reverse mode of ad , or reverse ad",
    ". we may give the following , informal descriptions :    let @xmath0 consist of ( not necessarily distinct ! ) elementary functions @xmath54 .",
    "then    * _ forward automatic differentiation is the computation of @xmath50 for fixed @xmath46 and @xmath47 through the successive computation of pairs of real numbers @xmath55 for suitable vectors @xmath56 , @xmath57 . _ * _ reverse automatic differentiation is the computation of @xmath53 for fixed @xmath46 and @xmath48 through the computation of the two lists of real numbers @xmath58   \\textrm{and } \\ \\ \\ & v_{1 } \\cdot   \\frac{\\partial \\varphi_1}{\\partial   v_k}(\\mathbf{c_{1 } } ) + v_{1,k}\\ , ... , \\ v_{\\mu } \\cdot   \\frac{\\partial \\varphi_\\mu}{\\partial   v_k}(\\mathbf{c_{\\mu } } ) + v_{\\mu , k } \\",
    "\\in { \\mathbb{r } } , \\ \\ k = 1, ... ,n_{i},\\end{aligned}\\ ] ] for suitable vectors @xmath59 and suitable numbers @xmath60 , @xmath57 . _    of course , the vectors and numbers @xmath61 are determined in a certain way ; as is the order of the @xmath54 .    as mentioned before",
    ", the function @xmath2 has to be constructed using elementary functions .",
    "loosely speaking , we may say that @xmath2 has to be a composition of elements of @xmath41 .",
    "however , this is not quite correct from a strictly mathematically point of view .",
    "since all elementary functions are real - valued , it is clear that a composition @xmath62 can not be defined , as soon as one of the @xmath63 is multivariate . as a composition of @xmath64 and @xmath65 .",
    "] admittedly , this is a rather technical and not really important issue , but , for completeness , we give the following inductive definition :    [ definition automatically differentiable ]    * we call a function @xmath66 on open @xmath67 _ automatically differentiable _ , if * * @xmath68 or * * there exist functions @xmath69 on open sets @xmath70 , @xmath71 , such that for all @xmath72 , there exist @xmath73 many @xmath74 and , for @xmath75 , @xmath76 , there exist @xmath77 many @xmath78 , with @xmath79 and @xmath80 and , for @xmath75 , each @xmath81 is automatically differentiable .",
    "* we call a function @xmath0 with @xmath82 for all @xmath83 _ automatically differentiable _ ,",
    "if each @xmath35 is automatically differentiable .",
    "[ example automatically differentiable ] the function @xmath84 given by @xmath85 is automatically differentiable .    from now on , without necessarily stating it explicitly , we will always assume that our function @xmath0 is automatically differentiable in the sense of definition [ definition automatically differentiable ] .",
    "one may , rightfully , ask why we use an inductive description of automatically differentiable functions , instead of just describing them as compositions of suitable multi - variable , mappings .",
    "however , from a computational point of view , one should note that an automatically differentiable @xmath0 will usually be given in the form of definition [ definition automatically differentiable ] , such that expressing @xmath2 as a composition requires additional work .",
    "nevertheless , expressing @xmath2 as a composition is indeed the basic step in an elementary description of automatic differentiation , which we describe in the next section .",
    "we will describe other ( equivalent ) approaches which work directly with functions of the form of definition [ definition automatically differentiable ] in later sections .",
    "in this approach , the function @xmath2 is described as a composition of multi - variate and multi - dimensional mappings . differentiating this composition to obtain @xmath50 , for given @xmath86 and @xmath87 ,",
    "leads , by the chain rule , to a product of matrices .",
    "this method has , for example , been described in ( * ? ? ?",
    "* section 2 ) and , comprehensively , in the works of griewank @xcite and griewank and walther @xcite .",
    "we follow mainly the notation of @xcite .",
    "the simple idea is to express @xmath2 as a composition of the form @xmath88 here , @xmath89 is the ( linear ) natural embedding of the domain @xmath1 into the so - called _ state space _ @xmath90 , where @xmath91 is the total number of ( not necessarily distinct ) elementary functions @xmath92 of which @xmath2 consists .",
    "each + @xmath93 , referred to as an _ elementary transition _ , corresponds to exactly one such elementary function .",
    "the mapping @xmath94 is some suitable linear projection of @xmath95 down into @xmath96 .",
    "determining now @xmath50 for fixed @xmath46 and fixed @xmath87 becomes , by the chain rule , the evaluation of the matrix - vector product @xmath97 where @xmath98 denotes the jacobian of @xmath99 at @xmath100 .",
    "the process is now performed in a particular ordered fashion , which we describe in the following .",
    "the evaluation of @xmath2 at some point @xmath101 can be described by a so - called _ evaluation trace _",
    "@xmath102}=\\mathbf{v}^{[0]}(\\mathbf{x}), ... ,\\mathbf{\\mathbf{v}}^{[\\mu]}=\\mathbf{v}^{[\\mu]}(\\mathbf{x})$ ] , where each @xmath103 } \\in h$ ] is a so - called _ state vector _ , representing the state of the evaluation after @xmath104 steps .",
    "more precisely , we set @xmath105}:= p_x(x_1, ... ,x_n ) = ( x_1, ... ,x_n,0, ... ,0 ) \\ \\ \\textrm{and } \\ \\ \\mathbf{\\mathbf{v}}^{[i ] } = \\phi_i(\\mathbf{\\mathbf{v}}^{[i-1 ] } ) , \\ \\",
    "i = 1, ... ,\\mu.\\ ] ] the elementary transitions @xmath99 are now given by imposing some suitable ordering on the @xmath91 elementary functions @xmath106 of which @xmath2 consists , such that @xmath92 is the @xmath104-th elementary function with respect to this order , and by setting @xmath107 and @xmath108 ( where @xmath38 is the open domain of @xmath92 ) .",
    "note that this is not a definition in the strict sense , since we neither specify the ordering of the @xmath92 , nor the arguments @xmath109 of each @xmath92 .",
    "these will depend on the actual functions @xmath2 and @xmath92 .",
    "( compare the example below . )",
    "therefore , we have @xmath110}(\\mathbf{x } ) = \\phi_i(\\mathbf{\\mathbf{v}}^{[i-1]}(\\mathbf{x } ) ) = \\left(\\begin{array}{c } \\mathbf{\\mathbf{v}}^{[i-1]}_1(\\mathbf{x } ) = x_1                                              \\\\ \\vdots                                               \\\\ \\mathbf{\\mathbf{v}}^{[i-1]}_n(\\mathbf{x } ) = x_n                                              \\\\",
    "\\vdots                                              \\\\ \\mathbf{\\mathbf{v}}^{[i-1]}_{n+i-1 } ( \\mathbf{x } )                                              \\\\ \\varphi_i(v_{i_1}(\\mathbf{x}), ... ,v_{i_{n_i}}(\\mathbf{x } ) )                                               \\\\ 0                                               \\\\",
    "\\vdots                                               \\\\ 0 \\end{array } \\right )   = \\left(\\begin{array}{c } \\mathbf{\\mathbf{v}}^{[i-1]}_1 = x_1                                              \\\\",
    "\\vdots                                               \\\\ \\mathbf{\\mathbf{v}}^{[i-1]}_n = x_n                                              \\\\",
    "\\vdots                                              \\\\ \\mathbf{\\mathbf{v}}^{[i-1]}_{n+i-1 }                                               \\\\",
    "\\varphi_i(v_{i_1}, ... ,v_{i_{n_i } } )                                               \\\\ 0                                               \\\\ \\vdots                                               \\\\ 0 \\end{array } \\right ) , \\end{aligned}\\ ] ] for @xmath111}_1, ... ,\\mathbf{\\mathbf{v}}^{[i-1]}_{n+i-1}\\ } \\cap u_i$ ] .",
    "it is clear that , for the above to make sense , the ordering imposed on the elementary functions @xmath106 must have the property that all arguments in @xmath112 have already been evaluated , before @xmath92 is applied .",
    "the definition of the projection @xmath113 depends on the ordering imposed on the @xmath106 . if this ordering is such that we have @xmath114}_{n+\\mu - m } \\ , ... , \\",
    "f_m(x_1, ... ,x_n ) = \\mathbf{\\mathbf{v}}^{[\\mu]}_{n+\\mu},\\ ] ] we can obviously choose @xmath115 .",
    "the following is a trivial modification of an example taken from @xcite .",
    "consider the function @xmath116 given by @xmath117    choose @xmath118 and @xmath119 with @xmath120 @xmath121 , @xmath122 , with @xmath123 and @xmath124    analogously to the evaluation of @xmath125 , the evaluation of the matrix - vector product for some @xmath126 and some @xmath127 can be expressed as an evaluation trace @xmath128}=\\mathbf{v'}^{[0]}(\\mathbf{c},\\mathbf{\\overset{\\rightharpoonup}{x}}), ... ,\\mathbf{v'}^{[\\mu]}=\\mathbf{v'}^{[\\mu]}(\\mathbf{c},\\mathbf{\\overset{\\rightharpoonup}{x}})$ ] , where @xmath129 } : = p_x \\cdot \\mathbf{\\overset{\\rightharpoonup}{x}}= ( x_1', ...",
    ",x_m',0, ... ,0 )   \\ \\ \\ \\textrm{and } \\ \\ \\ \\mathbf{\\mathbf{v'}}^{[i ] } : = \\phi'_{i,\\mathbf{c } } \\cdot \\mathbf{v'}^{[i-1 ] } , \\ \\",
    "i = 1, ... ,\\mu.\\ ] ] by the nature of the elementary transformations @xmath99 , each jacobian + @xmath130}(\\mathbf{c}))$ ] will be of the form @xmath131 where @xmath132 is interpreted as @xmath133 if @xmath92 does not depend on @xmath134 .",
    "thus , each @xmath135}$ ] will be of the form @xmath136 } = \\left(\\begin{array}{c } \\mathbf{\\mathbf{v'}}^{[i-1]}_1 = x'_1                                              \\\\",
    "\\vdots                                               \\\\",
    "\\mathbf{\\mathbf{v'}}^{[i-1]}_n = x'_n                                              \\\\",
    "\\vdots                                              \\\\ \\mathbf{\\mathbf{v'}}^{[i-1]}_{n+i-1 }                                               \\\\",
    "\\nabla \\varphi_i(v_{i_1}, ... ,v_{i_{n_i } } )                                               \\cdot \\left ( \\begin{array}{c } v'_{i_1 } \\\\ \\vdots \\\\",
    "v'_{i_{n_i } } \\end{array }                                               \\right )                                              \\\\ 0                                               \\\\",
    "\\vdots                                               \\\\ 0 \\end{array } \\right),\\ ] ] for @xmath137}_1, ... ,\\mathbf{v'}^{[i-1]}_{n+i-1}\\}$ ] , where the + @xmath138 correspond exactly to the @xmath139 .",
    "that is , if @xmath140}_l(\\mathbf{c})$ ] , then @xmath141}_l(\\mathbf{c},\\mathbf{\\overset{\\rightharpoonup}{x}})$ ] .",
    "the directional derivative of @xmath2 at @xmath10 in direction of @xmath11 is then simply @xmath142}.\\ ] ]    [ example griewank ] let @xmath143 .",
    "the computation of @xmath144 with @xmath116 given by @xmath145 has five _ evaluation trace pairs _",
    ",[\\mathbf{v}^{[5]},\\mathbf{v'}^{[5]}]$ ] , where @xmath147 } = ( c_1 , c_2 , 0 , 0 , 0 , 0 , 0)\\ \\ \\textrm{and } \\ \\ \\mathbf{v'}^{[0 ] } = ( x'_1 , x'_2 , 0 , 0 , 0 , 0 , 0)\\end{aligned}\\ ] ] and @xmath148}=   \\left(\\begin{array}{c } c_1 \\\\ c_2 \\\\ \\exp(c_1 ) \\\\ c_1 + c_2   \\\\",
    "\\sin(c_1+c_2 ) \\\\",
    "\\exp(c_1 ) * \\sin(c_1+c_2 ) \\\\ c_2 \\end{array } \\right),\\end{aligned}\\ ] ] @xmath149}=   \\left(\\begin{array}{c } x'_1 \\\\ x'_2 \\\\ \\exp(c_1)x'_1 \\\\ x'_1 + x'_2   \\\\",
    "\\cos(c_1+c_2)(x'_1+x'_2 ) \\\\   \\sin(c_1+c_2)\\exp(c_1)x'_1 + \\exp(c_1)\\cos(c_1+c_2)(x'_1+x'_2 ) \\\\",
    "x'_2\\end{array } \\right).\\end{aligned}\\ ] ] then @xmath150}$ ] , which is @xmath151    note that in the evaluation process , given the @xmath99 , each pair @xmath152},\\mathbf{v'}^{[i]}]$ ] depends only on the previous pair @xmath153},\\mathbf{v'}^{[i-1]}]$ ] and the given vectors @xmath154 .",
    "( since @xmath155 } = \\phi_i(\\mathbf{v}^{[i-1]})$ ] and @xmath156}=j_{\\phi_i}(\\mathbf{v}^{[i-1]}(\\mathbf{c } ) ) \\cdot \\mathbf{v'}^{[i-1]}$ ] . ) therefore , in an implementation , one can actually overwrite @xmath153},\\mathbf{v'}^{[i-1]}]$ ] by @xmath152},\\mathbf{v'}^{[i]}]$ ] in each step .",
    "note further that the @xmath157-th entry in each pair @xmath152},\\mathbf{v'}^{[i]}]$ ] is of the form @xmath158 i.e. consisting of a value of @xmath92 and a directional derivative of this elementary function . since the previous @xmath159 entries are identical to the first + @xmath159 entries of @xmath153},\\mathbf{v'}^{[i-1]}]$ ] , the computation of @xmath152},\\mathbf{v'}^{[i]}]$ ] is effectively the computation of .",
    "we summarize the discussion of this section :    by the above , given @xmath160 and @xmath87 , the evaluation of @xmath50 of an automatically differentiable function @xmath0 can be achieved by computing the evaluation trace pairs @xmath152},\\mathbf{v'}^{[i]}]$ ] .",
    "this process is equivalent to the computation of the pairs .",
    "the following section is concerned with a method which uses this last fact directly from the start .",
    "the approach about to be described also provides a better understanding on how an automatic differentiation system could actually be implemented . a question which may not be quite clear from the discussion so far .",
    "many descriptions and implementation of forward ad actually use a slightly different approach than the elementary one that we have just described . instead of expressing the function whose",
    "derivative one wants to compute as a composition , the main idea in this ` alternative ' approach as @xmath161 . ]",
    "is to lift this function ( and all elementary functions ) to ( a subset of ) the algebra of _ dual numbers _ @xmath162 .",
    "this method has , for example , been described in @xcite , @xcite and @xcite .",
    "dual numbers , introduced by clifford @xcite , are defined as @xmath163 , where addition is defined component - wise , as usual , and multiplication is defined as @xmath164 it is easy to verify that @xmath165 with these operations is an associative and commutative algebra over @xmath166 with multiplicative unit @xmath167 and that the element @xmath168 is nilpotent of order two . is sometimes referred to as an infinitesimal in the literature . the correct interpretation of this is probably that one can replace dual numbers by elements from non - standard analysis in the context of ad . however",
    ", this approach is actually unnecessary and , given the complexity of non - standard analysis , we will not consider it here . ]",
    "analogously to a complex number , we write a dual number @xmath169 as @xmath170 , where we identify each @xmath171 with @xmath172 .",
    "we will further use the notation @xmath173 instead of @xmath174 , i.e. we write @xmath175 .",
    "the @xmath176 in this representation will be referred to as the _ dual part _ of @xmath177 .",
    "we now define an extension of a differentiable , real - valued function + @xmath66 , defined on open @xmath1 , to a function @xmath178 defined on a subset of the dual numbers , by setting @xmath179 this definition easily extends to differentiable functions @xmath0 , where + @xmath180 is defined via @xmath181\\notag & \\",
    "= f(x_1, ... ,x_n ) + \\left ( j_f(x_1, ...",
    ",x_n ) \\cdot \\left ( \\begin{array}{c } x'_1 \\\\ \\vdots \\\\ x'_{n } \\end{array } \\right ) \\right)\\varepsilon.\\end{aligned}\\ ] ]    the following statement shows that definition makes sense .",
    "i.e. , that it is compatible with the natural extension of functions which are defined via usual arithmetic , i.e. polynomials , and analytic functions .",
    "that is :    [ prop .",
    "dual numbers def .",
    "makes sense ] definition is compatible with the `` natural '' extension of    * real - valued constant functions * projections of the form @xmath182 , * the arithmetic operations @xmath65 , @xmath17 @xmath183 and + @xmath184 , with @xmath185 * ( multivariate ) polynomials and rational functions * ( multivariate ) real analytic functions    to subsets of @xmath186 or @xmath187 , respectively .",
    "\\(i ) and ( ii ) follow easily from the definition .",
    "( iii ) : we have @xmath188 and , since @xmath189 , @xmath190 finally , considering division , it is easy to see that the multiplicative inverse of a dual numbers @xmath191 is defined if , and only if , @xmath192 and given by @xmath193 .    then , for @xmath194 , since @xmath189 , @xmath195 -\\frac{x_1}{x_2 ^ 2 } \\end{array } \\right)^t \\cdot   \\left ( \\begin{array}{c } x_1 ' \\\\ x_{2 } ' \\end{array } \\right ) \\right)\\varepsilon \\\\   & = \\frac{x_1}{x_2 } + \\left ( \\frac{x'_1}{x_2 } - \\frac{x_1\\cdot x'_2}{x_2 ^ 2 } \\right ) \\varepsilon\\\\ & = ( x_1 + x'_1 \\varepsilon ) \\cdot \\left ( \\frac{1}{x_2 } + \\left ( - \\frac{x'_2}{x_2 ^ 2 } \\right ) \\varepsilon \\right ) \\\\ & = \\frac{x_1 + x'_1 \\varepsilon}{x_2 + x'_2 \\varepsilon } \\ . \\end{aligned}\\ ] ]    ( iv ) : this will follow from proposition [ proposition dual numbers quasi - compositions ] in connection with ( iii ) .",
    "( v ) : we will recall the definition of multi - variate taylor series in section [ section dual and taylor ] . for the moment , let @xmath196 denote the @xmath197-th degree ( multi - variate ) taylor polynomial of @xmath66 about @xmath198 . since @xmath199 is real analytic",
    ", we have @xmath200 for all @xmath201 , where @xmath202 is an open neighbourhood of @xmath10 .",
    "it is well - known , that then @xmath203 on @xmath202 ( see for example ( * ? ? ?",
    "* chapter ii.1 ) ) . since addition and multiplication are continuous , then also @xmath204 on @xmath202 , for any fixed @xmath205 .",
    "consequently , @xmath206 for all @xmath207 .    to use definition for automatic differentiation ,",
    "we need to show that it behaves well for automatically differentiable functions as defined in definition [ definition automatically differentiable ] .",
    "basically , we need to show that is compatible with the chain rule .",
    "[ proposition dual numbers quasi - compositions ] let @xmath66 defined on open @xmath67 be automatically differentiable , @xmath208 .",
    "then @xmath209 for all @xmath210 , with + @xmath211 .",
    "we prove this statement by direct computation . as a composition and using the fact that the push - forward operator ( see section [ section diffgeom ] ) is a functor .",
    "] in the following , denote @xmath212 then the right hand - side of equation is equal to @xmath213 where @xmath214    the left hand side of is obviously equal to @xmath215 by assumption , @xmath216 .",
    "further , by the chain rule , @xmath217 & = \\nabla h_{\\ell } \\left(\\mathbf{x_0},h_1(\\mathbf{x_1}), ... ,h_{\\ell-1}(\\mathbf{x_{\\ell-1 } } ) \\right)\\\\ & \\ \\ \\ \\",
    "\\cdot \\frac{d\\left ( \\mathbf{x } \\mapsto ( \\mathbf{x_0},h_1(\\mathbf{x_1}), ... ,h_{\\ell-1}(\\mathbf{x_{\\ell-1}})\\right)}{d\\mathbf{x } } \\left(\\mathbf{x } \\right ) \\cdot \\mathbf{\\overset{\\rightharpoonup}{x}}.\\end{aligned}\\ ] ] now , @xmath218 equals @xmath219   \\frac{\\partial ( \\mathbf{x } \\mapsto h_1(\\mathbf{x_1}))}{\\partial x_1}(\\mathbf{x } ) & \\cdots & \\frac{\\partial ( \\mathbf{x } \\mapsto h_1(\\mathbf{x_1}))}{\\partial x_{n}}(\\mathbf{x } ) \\\\[1ex ]   \\vdots & \\vdots & \\vdots",
    "\\\\   \\frac{\\partial ( \\mathbf{x } \\mapsto h_{\\ell-1}(\\mathbf{x_{\\ell-1}}))}{\\partial x_1}(\\mathbf{x } ) & \\cdots & \\frac{\\partial ( \\mathbf{x } \\mapsto h_{\\ell-1}(\\mathbf{x_{\\ell-1}}))}{\\partial x_{n}}(\\mathbf{x } ) \\end{array } \\right ) \\end{aligned}\\ ] ] hence , @xmath220 thus , equals and we are done",
    ".    we can now automatically compute directional derivatives @xmath50 of an automatically differentiable function @xmath0 , on open @xmath1 , at fixed @xmath221 in direction of fixed @xmath222 by computing the directional derivatives @xmath223 in the following way :    [ thm .",
    "result process dual numbers ] assume that definition [ definition extension dual numbers ] is implemented for all elementary functions in the set @xmath224 .",
    "then the directional derivative @xmath223 of an automatically differentiable function @xmath35 can be computed ` automatically ' through extending @xmath225 to @xmath226 , and evaluating the dual part of @xmath227 .",
    "each @xmath225 is automatically differentiable . by assumption ,",
    "the case @xmath228 is clear : we simply obtain the pair @xmath227 by calling @xmath225 and all @xmath229 and computing the gradient - vector product .",
    "the sought directional derivative is the dual part ( second entry ) of that pair .",
    "so assume that there exists real - valued @xmath230 on open sets @xmath70 , such that for all @xmath83 , @xmath231 for suitable @xmath232 , with @xmath80 and @xmath233 automatically differentiable .",
    "we proceed by induction on the depth of @xmath225 .",
    "base case : assume that each @xmath234 . extending @xmath225 to @xmath226 leads to the extension of @xmath235 to sets @xmath236 . since definition [ definition extension dual numbers ]",
    "is implemented for all functions in @xmath224 , @xmath237 is defined for all @xmath81 and computed by calling @xmath81 and all @xmath238 with suitable inputs and computing the gradient - vector product . by proposition [ proposition dual numbers quasi - compositions ] , the computation of @xmath239 which is performed last , gives @xmath227 ,",
    "whose dual part is @xmath223 .",
    "induction step : assume that @xmath240 is not an elementary function .",
    "again , we extend @xmath225 to @xmath226 , which leads to the extension of @xmath235 to sets @xmath236 . since @xmath241 is still automatically differentiable @xmath242 is computed by induction assumption . then again",
    ", the computation of @xmath243 ( note that @xmath244 is elementary ) gives , by proposition [ proposition dual numbers quasi - compositions ] , the dual number @xmath227 .",
    "[ example dual ] consider the function @xmath245 given by @xmath246 let @xmath143 .",
    "we evaluate the value of @xmath247 at @xmath248 and @xmath249 . by definition ,",
    "proposition [ prop .",
    "dual numbers def .",
    "makes sense ] and proposition [ proposition dual numbers quasi - compositions ] , @xmath250 by theorem [ thm .",
    "result process dual numbers ] , the dual part of this expression is @xmath251 .",
    "that is , @xmath252     with primal parts in blue and dual parts in red . ]",
    "thus , a ( basic ) implementation of an automatic differentiation system can be realised by implementing for all elementary functions .",
    "usually , this is done by simply overloading elementary functions .",
    "constant functions will usually be identified with real numbers , which themselves will be lifted to dual number with zero dual part ( see figure [ figure haskell code dual ] ) .",
    "note again that , at no time during the described process , any symbolic differentiation takes place . instead ,",
    "since each ` top - level ' function @xmath244 of an automatically differentiable function is elementary , we are computing and passing on pairs @xmath253 which computes ` automatically ' the directional derivatives @xmath223 and , therefore , the directional derivative @xmath50 .",
    "note further that the pairs are as the ones in .",
    "this means that the processes described in this and in the previous section reduce to computations .    indeed ,",
    "if we store the pairs @xmath254 and in an array , we obtain the evaluation trace pairs @xmath152},\\mathbf{v'}^{[i]}]$ ] . in summary :    by the above , given @xmath160 and @xmath87 , the evaluation of @xmath50 of an automatically differentiable function @xmath0 can be achieved through the lifting of each @xmath225 to a function @xmath255 as defined in and by evaluating @xmath227 .",
    "this process is equivalent to the computation of the evaluation trace pairs @xmath152},\\mathbf{v'}^{[i]}]$ ] , as described in the previous section , and to the computation of the pairs in suitable order .",
    "before we move on to further descriptions of forward ad and to a brief description of the reverse mode , we take a look at some example cases to demonstrate how automatic differentiation solves some complexity issues which appear in systems which use symbolic differentiation .",
    "assume that we want to obtain the derivative of a composition @xmath2 of uni - variate elementary functions @xmath256 , that is , @xmath257 at a certain value @xmath258 .",
    "a symbolic differentiation system will first use the chain rule to determine the derivative function @xmath259 , which is given by @xmath260 for all @xmath171 , and then compute @xmath261 by substituting @xmath262 by @xmath8 .",
    "that is , each factor is computed and the results are multiplied .",
    "hence , the system computes the following values : @xmath263 as we see many expressions will be computed multiple times ( _ loss of sharing _ ) .",
    "if we ignore the time the system needs to determine @xmath259 , as well as the time for performing multiplications , and set the cost for the computation of each value of @xmath26 as @xmath13 , then the total cost of computing @xmath261 is @xmath264    in comparison , a forward automatic differentiation system will perform a computation of pairs starting with @xmath265 , where the @xmath266-st pair for @xmath267 looks like @xmath268 for some @xmath269 .",
    "see the computational graph in figure [ first graph ] in subsection [ subsection basic idea ] for the case @xmath270 ( set @xmath271 ) .",
    "if we again ignore costs for multiplications , the cost for evaluating each pair is @xmath272 .",
    "hence , the total costs of evaluating @xmath261 via automatic differentiation is @xmath273 .    in @xcite",
    "the example of a product of the form @xmath274 is given . here , the evaluation of the derivative of @xmath261 at some @xmath258 by a symbolic differentiation system will first use the product rule to compute @xmath259 given by @xmath275 for all @xmath171 , and then again substitute @xmath262 by @xmath8 . again , many function values will be computed multiple times .",
    "since we have @xmath4 functions in each summand and @xmath4 summands , ignoring cost for multiplications and addition , the computation of @xmath261 has a total cost of @xmath276 .",
    "in contrast , a forward automatic differentiation system will compute pairs starting with @xmath277 where the remaining pairs for @xmath278 look like @xmath279 for some @xmath269 . since the @xmath280rd , @xmath281th , @xmath282th etc .",
    "pairs are those which are created by lifting @xmath17 to the dual numbers , they contain only additions and multiplications of values which have already been computed . hence , for simplicity",
    ", we may discard these pairs with regards to the costs of the evaluations of @xmath261 .",
    "thus , the total cost of computing @xmath261 is the cost of computing the @xmath4 pairs @xmath283 which is @xmath273 .",
    "a further advantage of forward ad , at least in many implementation , is the efficient handling of common intermediate expressions feeding into several subsequent intermediates . consider for this the case of a sum of the @xmath4 elementary uni - variate functions @xmath92 each composed with another univariate elementary function @xmath284 : @xmath285 the derivative @xmath259 is in this case obviously given by @xmath286 for all @xmath171 . to determine @xmath261",
    ", a symbolic differentiation system will evaluate @xmath287 in each summand , that is @xmath4 times ( and possibly , if the expression @xmath288 is not factored out , @xmath289 as well @xmath4-times ) . hence , the computational cost of evaluating @xmath261 is at least @xmath290 ( if @xmath288 is factored out , @xmath291 otherwise ) , where we again ignore costs for additions and multiplications and for determining the derivative function @xmath259 . if we want to obtain the value @xmath292 , too , the cost increases to @xmath293 ( or @xmath294 , respectively ) .    in this particular case",
    ", some realisations of a forward automatic differentiation system might evaluate @xmath289 @xmath4-times as well .",
    "however , in many implementations the value @xmath287 will be assigned to a new variable @xmath177 , such that @xmath295 where @xmath296 .",
    "the forward ad system will then compute pairs starting with @xmath297 clearly , in this process the values ( numbers ) @xmath298 and @xmath299 are computed only once . if we again ignore costs for additions and multiplications ( including the cost for computing pairs created by lifting @xmath65 ) , the total cost of determining both @xmath292 and @xmath261 is the cost of computing the @xmath300 pairs @xmath301 , @xmath302 , @xmath303 , which is @xmath304",
    ", the symbolic evaluation appears to be slightly faster than fad .",
    "however , we have chosen this example mainly to demonstrate how the redundant computation of common sub - expressions can be avoided using automatic differentiation .",
    "note further that we have disregarded the cost for determining the derivative function @xmath259 in a symbolic differentiation in our considerations . ]     and @xmath305 in the case @xmath270 , where @xmath306 and @xmath296 . ]",
    "note that the substitution @xmath307 in a symbolic differentiation system would not have the same effect of avoiding redundant calculations .",
    "since @xmath308 is not a number , but an algebraic expression , to obtain @xmath261 the variable @xmath262 would still have to be substituted by @xmath8 in each instance of @xmath177 in @xmath309 .",
    "of course , the situation is more difficult when the function @xmath2 is more complicated or when multi - variate elementary functions other than @xmath65 or @xmath17 are involved .",
    "however , we hope to have demonstrated that , in general , forward automatic differentiation avoids redundant computations of common sub - expressions and does not suffer from the same complexity issues as symbolic computation .",
    "in the literature ( see , for example , ( * ? ? ?",
    "* section 2 ) ) , definition is sometimes described as being obtained by evaluating the taylor series expansion of @xmath310 about @xmath311 .    to understand this argument , recall that the taylor series of an infinitely many times differentiable multivariate function @xmath66 on an open set @xmath67 about some point @xmath312 is given by @xmath313 for all @xmath314 .",
    "let now @xmath315 be an extension of @xmath199 to the dual numbers ( that is @xmath316 ) .",
    "we define the taylor series of @xmath317 about some vector of dual numbers @xmath318 analogously to the real case .",
    "that is , @xmath319   = \\sum_{k_1 + \\cdots + k_{n } = 0}^{\\infty}\\bigg(&\\frac{(x_1 - c_1 + ( x'_1 - c'_1 ) \\varepsilon)^{k_1 }   \\cdots ( x_{n } -c_{n } + ( x'_{n } - c'_{n } ) \\varepsilon)^{k_{n}}}{k_1 ! \\cdots k_{n}!}\\\\ & \\cdot \\frac{\\partial^{k_1+\\cdots + k_{n}}\\tilde{h}}{\\partial x_1^{k_1 } \\cdots \\partial x_{n}^{k_{n}}}((\\mathbf{c},\\mathbf{\\overset{\\rightharpoonup}{c}}))\\bigg),\\end{aligned}\\ ] ] for all @xmath320 .",
    "trivially , this series converges for @xmath321 .",
    "further , due to @xmath322 , the taylor series about any @xmath323 converges for the arguments @xmath324 , for all @xmath325 .",
    "we have , identifying @xmath9 with @xmath326 , @xmath327   & = \\sum_{k_1 + \\cdots + k_{n } = 0}^{1}\\frac{(x'_1\\varepsilon)^{k_1 }   \\cdots ( x'_{n}\\varepsilon)^{k_{n}}}{k_1 ! \\cdots k_{n } ! }",
    "\\frac{\\partial^{k_1+\\cdots + k_{n}}}{\\partial x_1^{k_1 } \\cdots \\partial x_{n}^{k_{n}}}\\tilde{h}(\\mathbf{x } ) \\\\[1ex]\\notag & = \\tilde{h}(\\mathbf{x } ) + \\sum_{j=1}^{n}\\frac{\\partial}{\\partial x_j}\\tilde{h}(\\mathbf{x } ) \\cdot x'_j\\varepsilon\\\\[1ex ] \\notag & = \\tilde{h}(\\mathbf{x } ) + \\left(\\nabla \\tilde{h}(\\mathbf{x } ) \\cdot \\left(\\begin{array}{c } x'_1 \\\\ \\vdots \\\\ x'_{n } \\end{array } \\right ) \\right ) \\varepsilon = h(\\mathbf{x } ) + ( \\nabla h(\\mathbf{x } ) \\cdot \\mathbf{\\overset{\\rightharpoonup}{x } } ) \\varepsilon ,    \\end{aligned}\\ ] ] where @xmath328 .",
    "as we see , the right - hand side of the last equation is equal to + @xmath329 in definition .",
    "hence , if we choose @xmath317 as @xmath310 , we obtain @xmath330 that is :    the extension of an infinitely many times differentiable @xmath66 , on open @xmath67 , to a set @xmath226 as defined in , is the ( unique ) function @xmath310 , with the property that the images of any @xmath331 under @xmath310 and @xmath332 are equal .",
    "it is clear that the statement remains true if we replace ` infinitely many times differentiable ' by ` differentiable ' and @xmath332 by the first - degree taylor polynomial @xmath333 .",
    "would not be sufficient . ]",
    "since we identify @xmath334 with its natural embedding into @xmath335 , we can replace @xmath336 by @xmath337 in the right - hand side of .",
    "it is custom to do this in the left - hand side of as well .",
    "that is , one usually writes @xmath338 instead of @xmath339 or @xmath332 .    by , it is obvious that one can describe the process of determining the directional derivatives @xmath223 of each @xmath225 in terms of taylor series expansion , if @xmath225 is infinitely many times differentiable , or its first - degree taylor polynomial , otherwise .",
    "taylor series expansion or taylor polynomials can also be used to compute higher - order partial derivatives which we discuss briefly in section [ higher - order partial ] ( see also the work in ( * ? ? ?",
    "* chapter 13 ) ) .",
    "in recent literature ( see @xcite ) the extension of differentiable functions @xmath66 on open @xmath67 to a function @xmath178 is described in terms of the _ push - forward _ operator known from differential geometry .",
    "we shortly summarize the discussion provided in @xcite .",
    "let @xmath340 be differentiable manifolds , @xmath341 their tangent bundles and let @xmath342 be a differentiable function . the push - forward ( or _",
    "differential _ ) @xmath343 of @xmath199 can be defined via @xmath344 . ] as @xmath345 where @xmath346 is the _ push - forward ( or differential ) of @xmath199 at @xmath9 _ applied to @xmath11 .",
    "if now @xmath0 on open @xmath67 is a differentiable function , this reads @xmath347 considering @xmath348 as a subset of @xmath187 and identifying @xmath349 with @xmath350 , in light of , this means nothing else than @xmath351 furthermore , in the special case of a real - valued and infinitely many times differentiable function @xmath35 on open @xmath67 we also have , by equation , @xmath352 which justifies using the letter @xmath353 for both , the push - forward and the taylor - series of @xmath225 in this setting .",
    "it is well - known that @xmath354 and that @xmath355 for all @xmath356 and @xmath357 , for differentiable manifolds @xmath358 .",
    "that is , the mapping given by @xmath359 is a functor from the category of differentiable manifolds to the category of vector bundles ( see , for example , ( * ? ? ?",
    "* iii ,  2 ) ) . furthermore , since @xmath360 , one can even consider the algebra of dual number @xmath165 as the image of @xmath166 under @xmath353 , equipped with the push - forwards of addition and multiplication .",
    "i.e. , @xmath361 extending this to higher dimensions , the lifting of a differentiable function + @xmath362 on open @xmath67 to a function @xmath247 on a set @xmath363 may be considered as the application of the functor @xmath353 to @xmath334 , @xmath96 and @xmath2 . in other words , @xmath364 in summary , the forward mode of ad",
    "may also be studied from viewpoints of differential geometry and category theory .",
    "this fact may be used to generalise the concept of forward ad to functions operating on differentiable manifolds other than subsets of @xmath365 .",
    "the computation of higher - order partial derivatives of a sufficiently often and automatically differentiable function @xmath366 on open @xmath367 can , for example , be achieved through the extension of @xmath225 to a function defined on a truncated polynomial algebra . here to avoid confusion with indeterminates which we denote by @xmath334 or @xmath368 . ]",
    "this approach has , for instance , been described by berz in @xcite with further elaborations to be found in @xcite and the work in @xcite and @xcite extending the idea ( for the two latter , see the following subsection ) .",
    "indeed , the extension of @xmath225 to a function on dual numbers as given in definition can already be considered in the context of truncated polynomials , since @xmath369/(x^2)$ ] .",
    "let now @xmath370 and consider the algebra @xmath371/i_{n}$ ] , where @xmath372 is the ideal generated by all monomials of order @xmath373 .",
    "then @xmath374/i_{n } \\cong \\left\\{\\sum_{k_1 + \\cdots + k_{n } = 0}^{n}x_{(k_1, ...",
    ",k_n)}x_1^{k_1 } \\cdots x_n^{k_n } \\ | \\ x_{(k_1, ... ,k_n ) } \\in { \\mathbb{r}}\\right\\}\\ ] ] consists of all polynomials in @xmath375 of degree @xmath376 .",
    "denote @xmath377 and , for simplicity , identify @xmath378 .",
    "let further @xmath379 be open and define @xmath380/i_{n}\\right)^n_u\\\\ & : = \\left\\{({\\mathfrak{f}}_1, ...",
    ",{\\mathfrak{f}}_n ) \\in \\left({\\mathbb{r}}[x_1, ... ,x_n]/i_{n}\\right)^n \\ |\\ ( x_1, ... ,x_n ) \\in u \\right\\}.\\end{aligned}\\ ] ] that is , @xmath381/i_{n}\\right)^n_u$ ] consists of vectors of polynomials in + @xmath371/i_{n}$ ] with the property that the vector consisting of the trailing coefficients lies in @xmath3 .",
    "we now define an extension of an @xmath382-times differentiable , real - valued function @xmath383 to a function @xmath384/i_{n}\\right)^n_u \\to { \\mathbb{r}}[x_1, ... ,x_n]/i_{n}$ ] via @xmath385 in other words , @xmath386 is the unique function with the property that the images of any @xmath387 under @xmath386 and its @xmath382-th degree taylor polynomial + @xmath388 about @xmath389 are equal .",
    "the reason for this definition becomes apparent when we apply @xmath386 to a vector of polynomials of the form @xmath390 .",
    "then @xmath391 for @xmath101 .",
    "one can now compute partial derivatives of order @xmath382 of a sufficiently often and automatically differentiable function @xmath366 by implementing for all elementary functions @xmath92 .",
    "this leads to the extension of @xmath225 to @xmath392 and one obtains a partial derivative @xmath393 at a given @xmath394 as the ( @xmath395)-th multiple of the coefficient of @xmath396 in + @xmath397 .",
    "of course , to show that this method actually works , one needs to prove an analogue of proposition [ proposition dual numbers quasi - compositions ] .",
    "however , we will omit the proof here .",
    "obviously , this method requires the computation of the @xmath382-th taylor polynomial , or , equivalently , of the taylor coefficients up to degree @xmath382 , of each elementary function @xmath92 appearing in @xmath225 .",
    "the complexity of this problem is discussed in detail in @xcite : if no restrictions on an elementary function @xmath92 is given , even in the uni - variate case , order-@xmath398 arithmetic operations may be required . however , in practice all elementary functions @xmath92 are solutions of linear odes which reduces the computational costs of their taylor coefficients to @xmath399 for @xmath400 ( see ( * ? ? ? * ( 13.7 ) and proposition 13.1 ) ) .",
    "we further remark that berz in @xcite , instead of @xmath371/i_{n}$ ] , actually uses the algebra @xmath401 where multiplication is defined via @xmath402 for @xmath403 .",
    "this obviously makes no real difference to the theory , the main advantage is that after lifting a function @xmath199 to this algebra , one can extract partial derivatives directly , without the need to multiply with @xmath395 .",
    "differential algebra is an area which has originally been developed to provide algebraic tools for the study of differential equations ( see , for example , the original work by ritt @xcite or the introductory article @xcite ) . in the context of automatic differentiation",
    "it was utilized in @xcite , @xcite .",
    "a _ differential algebra _ is an algebra @xmath404 with a mapping @xmath405 called a _ derivation _ , which satisfies @xmath406 for all @xmath407 .",
    "if @xmath408 is a field , it is called a _",
    "differential field_. examples for differential fields or algebras are the set of ( real or complex ) rational functions with any partial differential operator or polynomial algebras @xmath371 $ ] with a formal partial derivative.truncated polynomial algebras as described in the previous section can be made into differential algebras as well , however , as garczynski in @xcite points out , a formal ( partial ) derivative is not a derivation in that case .",
    "( for instance , the mapping @xmath409/(x^2 ) \\to { \\mathbb{r}}[x]/(x^2)$ ] with @xmath410 is a derivation , but the formal derivative @xmath411 with @xmath412 is not . )",
    "karczmarczuk describes now in @xcite a system in which , through a",
    "_ lazy evaluation _",
    ", to each object @xmath413 in a differential field @xmath408 , the sequence @xmath414 is assigned . in the case of an infinitely many times differentiable univariate function @xmath415 , on open @xmath416 , and the differential operator as derivation ,",
    "this obviously gives @xmath417 .",
    "the set @xmath418 now forms a differential algebra itself , where addition is defined entry - wise , multiplication is given by @xmath419 \\notag   & = \\left(a \\cdot b , a \\cdot \\delta(b)+\\delta(a ) \\cdot b , a\\cdot \\delta^2(b ) + 2 ( \\delta(a ) \\cdot \\delta(b ) ) + \\delta^2(a ) \\cdot b,\\right.\\\\ \\notag & \\left . \\",
    "\\ \\ \\ \\ a \\cdot \\delta^3(b ) + 3 ( \\delta^2(a ) \\cdot \\delta(b ) ) + 3 ( \\delta(a ) \\cdot \\delta^2(b ) ) + \\delta^3(a ) \\cdot b, ...",
    "\\right ) \\\\[1ex ] & = \\left ( \\sum_{k_a+k_b = n}\\frac{n!}{k_a!k_b!}\\delta^{k_a}(a)\\cdot\\delta^{k_b}(b ) \\right)_{n \\in { \\mathbb{n}}}\\end{aligned}\\ ] ] and the derivation , denoted by @xmath420 , is the right - shift operator , given by @xmath421 for all @xmath407 .",
    "( these definition are given recursively in the original work ; for implementation details , see ( * ? ? ?",
    "* subsections 3.23.3 ) . )    to utilize these ideas for automatic differentiation , the assignment + @xmath422 is implemented for all uni - variate infinitely many times differentiable elementary functions @xmath423 , defined on open @xmath424 , and the elementary functions @xmath65 , @xmath17 and @xmath425 are replaced by addition , multiplication and division is defined here , which can be found in the original publication . ] on the @xmath426 .",
    "this then generates for any univariate automatically differentiable function @xmath427 , on open @xmath416 , which is constructable by the @xmath92 , @xmath65 , @xmath17 and @xmath425 , the sequence @xmath428 .",
    "the @xmath382-the derivative of @xmath2 is then the first entry ( which is distinguished in the implementation in @xcite ) in @xmath429 .",
    "however , since a differential algebra / field is an abstract concept , this approach can , in principle , be applied to other objects than differentiable functions    we only remark that karczmarczuk briefly describes in @xcite a generalisation to the multi - variate case .",
    "kalman in @xcite also constructs a system which appears to be similar .    comparing with shows that the multiplication on + @xmath418 is identical to the multiplication on the algebra used by berz .",
    "hence , one can express the described system , at least in the case of @xmath430 being a function , in terms of polynomial algebras .",
    "that is , consider the truncated algebra @xmath431 with multiplication as in and define an extension of an @xmath432 to a mapping @xmath433 via @xmath434 for all @xmath435 .",
    "then , for all @xmath436 , @xmath437 which corresponds to the tuple @xmath438 .",
    "the lazy evaluation technique in @xcite increases the degree of truncation @xmath382 successively _ ad infinitum _ to compute any entry of the sequence @xmath439 for any @xmath436 .",
    "similarly , pearlmutter and siskind describe in @xcite the lifting of a multi - variate function @xmath440 to a function on @xmath381/i_{n}\\right)^n_u$ ] as defined in , and then , through a lazy evaluation , increase the degree of truncation @xmath382 successively . instead of @xmath441 , which makes no real difference .",
    "] this computes any entry of the sequence + @xmath442 ( given in some order ) , for any vector @xmath443 .",
    "let , as before , @xmath362 on open @xmath1 be automatically differentiable .",
    "as already mentioned , the reverse mode of automatic differentiation evaluates products of the jacobian of @xmath2 with row vectors .",
    "that is , it computes @xmath444 to our knowledge , there exists currently no method to achieve this computation , which resembles forward ad using dual numbers . instead , an elementary approach , similar to the forward ad approach in section [ section griewank ] will have to suffice .",
    "the reverse mode is , for example , described in @xcite , @xcite and @xcite .",
    "we follow mainly the discussion in @xcite .",
    "express again @xmath2 as the composition @xmath161 , with @xmath445 , @xmath446 and the @xmath99 as in section [ section griewank ] .",
    "the computation of @xmath447 is , by the chain rule , the evaluation of the product @xmath448 \\leftrightarrow \\ \\ \\ \\",
    "j_f(\\mathbf{c})^t \\cdot \\mathbf{\\overset{\\leftharpoonup}{y}}^t & = p_x^t \\cdot \\phi'^t_{1,\\mathbf{c } }   \\ \\cdots \\ \\phi'^t_{\\mu,\\mathbf{c } } \\cdot p_y^t \\cdot \\mathbf{\\overset{\\leftharpoonup}{y}}^t,\\end{aligned}\\ ] ] where again @xmath98 denotes the jacobian of @xmath99 at @xmath100 .    obviously , the sequence of state vectors @xmath103 } \\in h={\\mathbb{r}}^{n+\\mu}$ ] is the same as in the forward mode case .",
    "( here , @xmath91 is , of course , again the total number of elementary functions which make up the function @xmath2 . )",
    "the difference lies in the computation of the evaluation trace of , which we denote by @xmath449 } = \\mathbf{\\mathbf{\\overline{v}}}^{[\\mu]}(\\mathbf{c},\\mathbf{\\overset{\\leftharpoonup}{y } } ) , ... , \\mathbf{\\mathbf{\\overline{v}}}^{[0]}=\\mathbf{\\mathbf{\\overline{v}}}^{[0]}(\\mathbf{c},\\mathbf{\\overset{\\leftharpoonup}{y}})$ ]",
    ".    for simplicity , assume @xmath115 , for all + @xmath450 and denote @xmath451 .",
    "we define the evaluation trace of as @xmath452}:= p_y^t \\cdot \\mathbf{\\overset{\\leftharpoonup}{y}}^t = ( 0, ... ,0,y_1', ... ,y_m ' ) \\ \\ \\textrm{and } \\",
    "\\ \\mathbf{\\mathbf{\\overline{v}}}^{[i-1]}:= \\phi'^t_{i,\\mathbf{c } } \\cdot \\mathbf{\\mathbf{\\overline{v}}}^{[i]}.\\ ] ] by , @xmath453 & \\phi'^t_{i , \\mathbf{c } } = \\left(\\begin{array}{ccccccc }   1 & \\cdots & 0 & \\frac{\\partial \\varphi_i}{\\partial   v_{1 } } ( \\cdots )   & 0 & \\cdots & 0 \\\\                                                    \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\ & \\vdots \\\\ 0 & \\cdots & 1 & \\vdots & 0 & \\cdots & 0 \\\\ 0 & \\cdots & 0 & \\vdots & 1 & \\cdots & 0 \\\\ \\vdots & \\ & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & 0 & \\ \\ \\ \\frac{\\partial \\varphi_i}{\\partial   v_{n+\\mu } } ( \\cdots )   & 0 & \\cdots & 1   \\end{array}\\right ) , \\end{aligned}\\ ] ] where @xmath132 is interpreted as @xmath133 if @xmath92 does not depend on @xmath134 , and the @xmath454}_1(\\mathbf{c}), ... ,\\mathbf{\\mathbf{v}}^{[i-1]}_{n+i-1}(\\mathbf{c})\\}$ ] .",
    "therefore , each @xmath455}$ ] is of the form @xmath456 in reverse } \\mathbf{\\mathbf{\\overline{v}}}^{[i-1 ] } & = \\left(\\begin{array}{c } \\mathbf{\\overline{v}}^{[i]}_{n+i } \\cdot \\frac{\\partial \\varphi_i}{\\partial   v_1}(\\cdots ) + \\mathbf{\\overline{v}}^{[i]}_{1 } \\\\ \\vdots \\\\   \\mathbf{\\overline{v}}^{[i]}_{n+i } \\cdot \\frac{\\partial \\varphi_i}{\\partial   v_{n+i-1}}(\\cdots ) + \\mathbf{\\overline{v}}^{[i]}_{n+i-1 } \\\\[2ex]\\mathbf{\\overline{v}}^{[i]}_{n+i } \\cdot \\frac{\\partial \\varphi_i}{\\partial   v_{n+i}}(\\cdots ) \\\\[2ex ]   \\mathbf{\\overline{v}}^{[i]}_{n+i } \\cdot \\frac{\\partial \\varphi_i}{\\partial   v_{n+i+1}}(\\cdots ) + \\mathbf{\\overline{v}}^{[i]}_{n+i+1}\\\\ \\vdots \\\\ \\mathbf{\\overline{v}}^{[i]}_{n+i } \\cdot   \\frac{\\partial \\varphi_i}{\\partial   v_{n+\\mu}}(\\cdots ) + \\mathbf{\\overline{v}}^{[i]}_{n+\\mu }   \\end{array } \\right)\\\\[1.5ex ] & =   \\mathbf{\\overline{v}}^{[i]}_{n+i}\\   \\cdot \\ \\left(\\begin{array}{c } \\frac{\\partial \\varphi_i}{\\partial   v_1}(\\cdots )   \\\\ \\vdots \\\\",
    "\\frac{\\partial \\varphi_i}{\\partial   v_{n+i-1}}(\\cdots ) \\\\[2ex]\\frac{\\partial \\varphi_i}{\\partial   v_{n+i}}(\\cdots ) \\\\[2ex ]   \\frac{\\partial \\varphi_i}{\\partial   v_{n+i+1}}(\\cdots ) \\\\ \\vdots \\\\",
    "\\frac{\\partial \\varphi_i}{\\partial   v_{n+\\mu}}(\\cdots )   \\end{array } \\right ) +    \\left(\\begin{array}{c } \\mathbf{\\overline{v}}^{[i]}_{1 } \\\\ \\vdots \\\\",
    "\\mathbf{\\overline{v}}^{[i]}_{n+i-1 } \\\\ 0 \\\\ \\mathbf{\\overline{v}}^{[i]}_{n+i+1}\\\\ \\vdots \\\\ \\mathbf{\\overline{v}}^{[i]}_{n+\\mu }   \\end{array } \\right)\\end{aligned}\\ ] ]      if we let @xmath458 be an extension of @xmath37 to @xmath95 with @xmath459 if @xmath92 does not depend on @xmath134 , and define @xmath460 } \\in h$ ] by @xmath460}_k = \\mathbf{\\mathbf{\\overline{v}}}^{[i]}_k$ ] , for @xmath461 , and @xmath460}_{n+i } = 0 $ ] , then we can rewrite as @xmath462}=\\left(\\mathbf{\\overline{v}}^{[i]}_{n+i } \\cdot \\nabla \\overline{\\varphi}_i(v_1, ... ,v_{n+\\mu})\\right)^t + \\mathbf{\\mathbf{\\overline{v}}}^{[i,*]}.\\ ] ] the expression on the right is the analogue of the term @xmath463 which appears in the process of forward ad , where the main difference is the appearance of the added vector @xmath460}$ ] .",
    "note that , in contrast to forward ad , the sequence of evaluation trace pairs @xmath464},\\mathbf{\\overline{v}}^{[i]}]$ ] appears in reverse order ( that is , @xmath465},\\mathbf{\\overline{v}}^{[\\mu]}], ...",
    ",[\\mathbf{\\mathbf{v}}^{[1]},\\mathbf{\\overline{v}}^{[1]}]$ ] ) . in particular , unlike to forward ad , it is not efficient to overwrite the previous pair in each computational step . indeed , since the state vector @xmath155}$ ] is needed to compute @xmath466}$ ] , the pairs @xmath464},\\mathbf{\\overline{v}}^{[i]}]$ ] are not computed ( as pairs ) at all .",
    "instead , one first evaluates the evaluation trace @xmath467}, ... ,\\mathbf{\\mathbf{v}}^{[\\mu]}$ ] , stores these values , and then uses them to compute the @xmath468}, ... ,\\mathbf{\\overline{v}}^{[1]}$ ] afterwards .",
    "set @xmath473 and @xmath474 with @xmath475 & \\phi_1 : { \\mathbb{r}}^5 \\to { \\mathbb{r}}^5 , \\ \\ \\textrm{with } \\ \\",
    "\\phi_1\\left(v_1,v_2,v_3,v_4,v_5\\right )   = ( v_1,\\exp(v_1),v_3,v_4 , ) , \\\\   & \\phi_2 : { \\mathbb{r}}^5 \\to { \\mathbb{r}}^5 , \\ \\",
    "\\textrm{with } \\ \\",
    "\\phi_2\\left(v_1,v_2,v_3,v_4,v_5\\right ) = ( v_1 , v_2 , \\sin(v_1),v_4,v_5 ) , \\\\[1ex ] & \\phi_3 : { \\mathbb{r}}^5 \\to { \\mathbb{r}}^5 , \\ \\ \\textrm{with } \\ \\",
    "\\phi_3\\left(v_1,v_2,v_3,v_4,v_5\\right ) = ( v_1 , v_2 , v_3 , v_1 , v_5),\\\\[1ex ] & \\phi_4 : { \\mathbb{r}}^5 \\to { \\mathbb{r}}^5 , \\ \\ \\textrm{with } \\ \\",
    "\\phi_4\\left(v_1,v_2,v_3,v_4,v_5\\right ) = ( v_1 , v_2 , v_3 , v_4 , v_2 * v_3),\\\\[1ex ] & p_y : { \\mathbb{r}}^5 \\to { \\mathbb{r } } , \\ \\ \\textrm{with } \\ \\",
    "p_y(v_1,v_2,v_3,v_4,v_5 ) = ( v_4,v_5).\\end{aligned}\\ ] ] clearly , we obtain the evaluation trace @xmath102}(c), ... ,\\mathbf{v}^{[4]}(c)$ ] with @xmath476}(c)=\\left ( \\begin{array}{c } c \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array } \\right ) , ... ,    \\mathbf{v}^{[4]}(c)=\\left ( \\begin{array}{c } c \\\\ \\exp(c ) \\\\",
    "\\sin(c ) \\\\ c \\\\",
    "\\exp(c)*\\sin(c ) \\end{array } \\right).\\ ] ] the reverse mode of automatic differentiation produces now the vectors + @xmath477}, ... ,\\mathbf{\\overline{v}}^{[0]}$ ] with + @xmath477}=\\left ( \\begin{array}{c } 0 \\\\ 0 \\\\ 0 \\\\ y'_1 \\\\",
    "y'_2 \\end{array }",
    "\\right ) , \\mathbf{\\overline{v}}^{[3]}=\\left ( \\begin{array}{c } 0 \\\\ y'_2\\cdot \\sin(c ) \\\\ y'_2\\cdot \\exp(c ) \\\\",
    "y'_1 \\\\ 0 \\end{array } \\right ) , \\mathbf{\\overline{v}}^{[2]}=\\left ( \\begin{array}{c } y'_1 \\\\",
    "y'_2\\sin(c ) \\\\",
    "y'_2\\exp(c ) \\\\ 0 \\\\ 0 \\end{array } \\right)$ ] , + @xmath478}=\\left ( \\begin{array}{c } y'_2\\exp(c ) \\cdot \\cos(c ) +",
    "y'_1\\\\ y'_2\\sin(c ) \\\\ 0 \\\\ 0 \\\\ 0 \\end{array } \\right)$ ] and finally + @xmath479}=\\left ( \\begin{array}{c }",
    "y'_2\\sin(c ) \\cdot \\exp(c ) + \\big ( y'_2\\exp(c)\\cos(c)+y'_1\\big ) \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array } \\right)$ ] + then @xmath480          by the above , given @xmath160 and @xmath48 , the evaluation of @xmath447 for an automatically differentiable function @xmath0 can be achieved by computing the vectors @xmath484}, ... ,\\mathbf{\\mathbf{v}}^{[\\mu]}$ ] and @xmath468}, ...",
    ",\\mathbf{\\overline{v}}^{[0]}$ ] , where the computation of each @xmath485}$ ] is effectively the computation of the real numbers @xmath486}_{n+i } \\cdot \\frac{\\partial \\varphi_i}{\\partial   v_k}(v_{i_1}(\\mathbf{c}), ...",
    ",v_{i_{n_i}}(\\mathbf{c } ) ) +    \\mathbf{\\overline{v}}^{[i]}_{k } , \\ \\ \\ k \\neq n+i,\\\\[1ex ]   \\textrm{and } \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ & \\mathbf{\\overline{v}}^{[i]}_{n+i } \\cdot \\frac{\\partial \\varphi_i}{\\partial   v_{n+i}}(v_{i_1}(\\mathbf{c}), ... ,v_{i_{n_i}}(\\mathbf{c } ) ) .\\end{aligned}\\ ] ]    comparing the complexity of reverse ad with the one of symbolic differentiation for the examples given in section [ section comparison fad with symbolic ] gives the same result as the comparison of forward ad with symbolic differentiation .",
    "( see , for example , figure [ second graph ] for the case of a composition of @xmath280 uni - variate functions . )",
    "* acknowledgements : * i like to thank felix filozov for his advice and his help with the code in figure [ figure haskell code dual ] , atilim gne baydin for his advice and his help with the code in figure [ figure fsharp reverse ] and barak pearlmutter for general valuable advice .",
    "i am further very thankful to the anonymous referees for valuable criticism and suggestions which led to a significant improvement of this paper . + * funding : * this work was supported by science foundation ireland grant + 09/in.1/i2637 .",
    "99 m. berz , differential algebraic description of beam dynamics to very high orders , _ particle accelerators _ 24 ( 1989 ) , 109124 . c. h. bischof . on the automatic differentiation of computer programs and an application to multibody systems , _",
    "iutam symposium on optimization of mechanical systems solid mechanics and its applications _ 43 ( 1996 ) , + 4148 .",
    "w. k. clifford , preliminary sketch of bi - quaternions , _ proceedings of the london mathematical society _ 4 ( 1873 ) , 381395 .",
    "l. dixon , automatic differentiation : calculation of the hessian , in : encyclopedia of optimization , second edition , springer science+business media , llc . , 2009 , 133137 .",
    "v. garczynski , remarks on differential algebraic approach to particle beam optics by m. berz , _ nuclear instruments and methods in physics research section a _ , 334 ( 2 - 3 ) ( 1993 ) , 294298 .",
    "r. m. gower , m. p. mello , a new framework for the computation of hessians , _ optimization methods and software _ 27 ( 2 ) ( 2012 ) , 251273 .",
    "a. griewank , achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation , _ optimization methods and software _ 1 ( 1992 ) , 3554 .",
    "a. griewank , a mathematical view of automatic differentiation , _ acta numerica _ 12 ( 2003 ) , 321398 .",
    "a. griewank and a. walther , evaluating derivatives : principles and techniques of algorithmic differentiation , second edition , siam , philadelphia , pa , 2008 .",
    "a. g. baydin , b. a. pearlmutter , diffsharp : automatic differentiation library , version of 17th june 2015 , http://diffsharp.github.io / diffsharp/. a. g. baydin , b. a. pearlmutter , a. a. radul , j. m. siskind , automatic differentiation and machine learning : a survey .",
    "arxiv preprint .",
    "arxiv:1502.05767 ( 2015 ) .",
    "j. h. hubbard and b. e. lundell , a first look at differential algebra , _ american mathematical monthly _ 118 ( 3 ) ( 2011 ) , 245261 .",
    "f. john , partial differential equations , second edition , springer - verlag , new york - heidelberg - berlin , 1975 .",
    "d. kalman , double recursive multivariate automatic differentiation , _ mathematics magazine _ 75 ( 3 ) ( 2002 ) , 187202 .",
    "j. karczmarczuk , functional coding of differential forms , _ proc first scottish workshop on functional programming , stirling , scotland , 1999_. j. karczmarczuk , functional differentiation of computer programs , _ proc of the iii acm sigplan international conference on functional programming , baltimore , md , 1998 _ , 195203",
    ". j. karczmarczuk , functional differentiation of computer programs , _ higher - order and symbolic computation _ 14 ( 1 ) ( 2001 ) , 3557 .",
    "s. lang , introduction to differentiable manifolds , second edition , springer - verlag , new york - heidelberg - berlin , 2002 .",
    "o. manzyuk , a simply typed @xmath487-calculus of forward automatic differentiation , _ electronic notes in theoretical computer science _ 286 ( 2012 ) , 257272 .",
    "b. a. pearlmutter and j. m. siskind , lazy multivariate higher - order forward - mode ad , _ proc of the 2007 symposium on principles of programming languages , nice , france , 2007 _ , 155160 .",
    "b. a. pearlmutter and j. m. siskind , reverse - mode ad in a functional framework : lambda the ultimate backpropagator , _ toplas _ 30 ( 2 ) ( 2008 ) , 136 .",
    "l. b. rall , differentiation and generation of taylor coefficients in pascal - sc , in : a new approach to scientific computation , academic press , new york , 1983 , 291309 .",
    "l. b. rall , the arithmetic of differentiation , _ mathematics magazine _ 59 , ( 1986 ) , 275282 .",
    "j. ritt , differential algebra , american mathematical society colloquium publications , vol .",
    "xxxiii , american mathematical society , new york , 1950 . j. m. siskind and b. a. pearlmutter , nesting forward - mode ad in a functional framework , _ higher - order and symbolic computation _ 21 ( 4 ) ( 2008),361376 .",
    "r. wengert , a simple automatic derivative evaluation program , _ communications of the acm _ 7 ( 8) ( 1964 ) , 463464 ."
  ],
  "abstract_text": [
    "<S> this article provides an overview of some of the mathematical principles of automatic differentiation ( ad ) . </S>",
    "<S> in particular , we summarise different descriptions of the forward mode of ad , like the matrix - vector product based approach , the idea of lifting functions to the algebra of dual numbers , the method of taylor series expansion on dual numbers and the application of the push - forward operator , and explain why they all reduce to the same actual chain of computations . </S>",
    "<S> we further give a short mathematical description of some methods of higher - order forward ad and , at the end of this paper , briefly describe the reverse mode of automatic differentiation .    </S>",
    "<S> _ preprint of an article to appear in : + numerical algorithms _    </S>",
    "<S> * keywords : * automatic differentiation , forward ad , reverse ad , dual numbers    ams subject classification ( 2010 ) : 65 - 02 , 65k99 </S>"
  ]
}