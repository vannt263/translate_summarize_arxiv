{
  "article_text": [
    "a statistical mixture model  @xcite @xmath1 with @xmath2 weighted components has underlying probability distribution :    @xmath3    with @xmath4 and @xmath5 denoting the mixture parameters : the @xmath6 s are positive weights summing up to one , and the @xmath7 s denote the individual component parameters . (",
    "appendix  [ sec : notations ] summarizes the notations used throughout the paper . )",
    "mixture models of @xmath8-dimensional gaussians are the most often used statistical mixtures  @xcite . in that case",
    ", each component distribution @xmath9 is parameterized by a mean vector @xmath10 and a covariance matrix @xmath11 that is symmetric and positive definite .",
    "that is , @xmath12 .",
    "the gaussian distribution has the following probability density defined on the support @xmath13 :    @xmath14    where @xmath15 denotes the squared mahalanobis distance  @xcite @xmath16 defined for a symmetric positive definite matrix @xmath17 ( @xmath18 , the precision matrix ) .    to draw a random variate from a gaussian mixture model ( gmm ) with @xmath0 components ,",
    "we first draw a multinomial variate @xmath19 , and then sample a gaussian variate from @xmath20 . a multivariate normal variate @xmath21",
    "is drawn from the chosen component @xmath22 as follows : first , we consider the cholesky decomposition of the covariance matrix : @xmath23 , and take a @xmath8-dimensional vector with coordinates being random standard normal variates : @xmath24^t$ ] with @xmath25 ( for @xmath26 and @xmath27 uniform random variates in @xmath28 ) .",
    "finally , we assemble the gaussian variate @xmath21 as @xmath29 .",
    "this drawing process emphasizes that sampling a statistical mixture is a _ doubly stochastic process _ by essence : first , we sample a multinomial law for choosing the component , and then we sample the variate from the selected component .",
    "figure  [ fig : gmm5d](b ) shows a gmm with @xmath30 components learned from a color image modeled as a 5d xyrgb point set ( figure  [ fig : gmm5d](a ) ) . since",
    "a gmm is a _ generative model _ , we can sample the gmm to create a `` sample image '' as shown in figure  [ fig : gmm5d](c ) .",
    "observe that low frequency information of the image is nicely modeled by gmms .",
    "figure  [ fig : gmmhighd](f ) shows a gmm with @xmath30 components learned from a color image modeled as a high - dimensional point set .",
    "each @xmath31 color image patch anchored at @xmath32 is modeled as a point in dimension @xmath33 .",
    "gmm representations of images and videos  @xcite provide a compact feature representation that can be used in many applications , like in information retrieval ( ir ) engines  @xcite .",
    "( a ) components is trained ( b ) .",
    "drawing many random variates from the generative gmm yields a sample image(c ) that keeps low - frequency visual information .",
    "[ fig : gmm5d],title=\"fig:\",scaledwidth=25.0% ] ( b ) components is trained ( b ) .",
    "drawing many random variates from the generative gmm yields a sample image(c ) that keeps low - frequency visual information .",
    "[ fig : gmm5d],title=\"fig:\",scaledwidth=25.0%,scaledwidth=25.0% ] ( c ) components is trained ( b ) .",
    "drawing many random variates from the generative gmm yields a sample image(c ) that keeps low - frequency visual information .",
    "[ fig : gmm5d],title=\"fig:\",scaledwidth=25.0% ]    in this paper , we consider the general case of mixtures of distributions belonging the same exponential family  @xcite , like gaussian mixture models  @xcite ( gmms ) , rayleigh mixture models  @xcite ( rmms ) , laplacian mixture models ( lmms)@xcite , bernoulli mixture models  @xcite ( bmms ) , multinomial mixture models  @xcite ( mmms ) , poisson mixture models ( pmms )  @xcite , weibull mixture models  @xcite ( weimms ) , wishart mixture models  @xcite ( wismm ) , etc .",
    "ccc -gmm modeling depicted by its covariance ellipses ,",
    "( c ) hard segmentation using the gmm , ( d ) sampling the 5d gmm , ( e ) mean colors ( @xmath34 patches ) for gmm with patch size @xmath35 , ( f ) patch mean @xmath36 for @xmath35 patch size width.[fig : gmmhighd],title=\"fig : \" ] & -gmm modeling depicted by its covariance ellipses , ( c ) hard segmentation using the gmm , ( d ) sampling the 5d gmm , ( e ) mean colors ( @xmath34 patches ) for gmm with patch size @xmath35 , ( f ) patch mean @xmath36 for @xmath35 patch size width.[fig : gmmhighd],title=\"fig : \" ] & -gmm modeling depicted by its covariance ellipses , ( c ) hard segmentation using the gmm , ( d ) sampling the 5d gmm , ( e ) mean colors ( @xmath34 patches ) for gmm with patch size @xmath35 , ( f ) patch mean @xmath36 for @xmath35 patch size width.[fig : gmmhighd],title=\"fig : \" ] + ( a ) & ( b ) & ( c ) + -gmm modeling depicted by its covariance ellipses , ( c ) hard segmentation using the gmm , ( d ) sampling the 5d gmm , ( e ) mean colors ( @xmath34 patches ) for gmm with patch size @xmath35 , ( f ) patch mean @xmath36 for @xmath35 patch size width.[fig : gmmhighd],title=\"fig : \" ] & -gmm modeling depicted by its covariance ellipses , ( c ) hard segmentation using the gmm , ( d ) sampling the 5d gmm , ( e ) mean colors ( @xmath34 patches ) for gmm with patch size @xmath35 , ( f ) patch mean @xmath36 for @xmath35 patch size width.[fig : gmmhighd],title=\"fig : \" ] & -gmm modeling depicted by its covariance ellipses , ( c ) hard segmentation using the gmm , ( d ) sampling the 5d gmm , ( e ) mean colors ( @xmath34 patches ) for gmm with patch size @xmath35 , ( f ) patch mean @xmath36 for @xmath35 patch size width.[fig : gmmhighd],title=\"fig : \" ] +   + ( d ) & ( e ) & ( f )      expectation - maximization  @xcite ( em ) is a traditional algorithm for learning finite mixtures  @xcite . banerjee et al .",
    "@xcite proved that em for mixture of exponential families amounts to perform equivalently a soft bregman clustering .",
    "furthermore , this em - bregman soft clustering equivalence was extended to total bregman soft clustering for curved exponential families  @xcite .",
    "although mathematically convenient , we should remember that mixture data should be hard clustered as each observation should emanate from exactly one component .",
    "it is well - known that @xmath0-means clustering technique can be interpreted as a limit case of em for isotropic gaussian mixtures  @xcite .",
    "kearns  et al .",
    "@xcite casted further light on the hard / soft relationship using an information - theoretic analysis of hard @xmath0-means and soft expectation - mazimization assignments in clustering .",
    "banerjee et al  @xcite proved a mathematical equivalence between the estimation of maximum likelihood of exponential family mixtures ( mlme , maximum likelihood mixture estimation ) and a rate distortion problem for bregman divergences",
    ". furthermore , banerjee et al .",
    "@xcite proposed the hardened expectation for the special case of von mises - fisher mixtures ( hard em , section 4.2 of  @xcite ) for computational efficiency .    in this paper",
    ", we build on the duality between bregman divergences and exponential families  @xcite to design @xmath0-mle that iteratively ( 1 ) assigns data to mixture components , ( 2 ) update mixture parameters  la @xmath0-means and repeat step ( 1 ) until local convergence , ( 3 ) update weights and reiterate from ( 1 ) until local convergence ( see algorithm  [ algo : kmle ] ) .",
    "we prove that @xmath0-mle maximizes monotonically the complete likelihood function .",
    "we also discuss several initialization strategies and describe a probabilistic initialization @xmath0-mle++ with guaranteed performance bounds .",
    "the paper is organized as follows : section  [ sec : preliminaries ] recall the basic notions of exponential families , legendre transform , bregman divergences , and demonstrate the duality between bregman divergences and exponential families to study the maximum likelihood estimator ( mle ) . section  [ sec : kmle - theta ] presents the framework of @xmath0-mle for mixtures with prescribed weights , based on the bregman - exponential family duality .",
    "the generic @xmath0-mle algorithm is described in section  [ sec : fullkmle ] , and section  [ sec : speedup ] discusses on proximity location data - structures to speed up the assignment step of the algorithm .",
    "section  [ sec : kmlepp ] presents @xmath0-mle++ , a probabilistic initialization of @xmath0-mle .",
    "finally , section  [ sec : concl ] concludes the paper and discusses on avenues for future research .",
    "an exponential family  @xcite @xmath37 is a set of parametric probability distributions @xmath38 whose probability density .",
    "we do not introduce the framework of probability measures nor radon - nikodym densities .",
    "] can be decomposed canonically as    @xmath39    where @xmath40 denotes the sufficient statistics , @xmath41 the natural parameter , @xmath42 the log - normalizer , and @xmath43 a term related to an optional auxiliary carrier measure .",
    "@xmath44 denotes the inner product ( i.e. , @xmath45 for vectors @xmath46 for matrices , etc . ) .",
    "let @xmath47 denotes the natural parameter space .",
    "the dimension @xmath48 of the natural parameter space is called the order of the family . for the @xmath8-variate gaussian distribution ,",
    "the order is @xmath49 .",
    "it can be proved using the cauchy - schwarz inequality  @xcite that the log - normalizer @xmath50 is a strictly convex and differentiable function on an open convex set @xmath51 .",
    "the log - density of an exponential family is    @xmath52    to build an exponential family , we need to choose a basic density measure on a support @xmath53 , a sufficient statistic @xmath40 , and an auxiliary carrier measure term @xmath43 . taking the log - laplace transform , we get    @xmath54    and define the natural parameter space as the @xmath41 values ensuring convergence of the integral .    in fact , many usual statistical distributions such as the gaussian , gamma , beta , dirichlet , poisson , multinomial , bernoulli , von mises - fisher , wishart , weibull are exponential families in disguise . in that case , we start from their probability density or mass function to retrieve the canonical decomposition of eq .",
    "[ eq : cexpfam ] .",
    "see  @xcite for usual canonical decomposition examples of some distributions that includes a bijective conversion function @xmath55 for going from the usual @xmath56-parameterization of the distribution to the @xmath41-parametrization .",
    "furthermore , exponential families can be parameterized canonically either using the natural coordinate system @xmath41 , or by using the dual moment parameterization @xmath57 ( also called mean value parameterization ) arising from the legendre transform ( see appendix  [ sec : mvn ] for the case of gaussians ) .      for a strictly convex and differentiable function @xmath58",
    ", we define its convex conjugate by    @xmath59    the maximum is obtained for @xmath60 and is unique since @xmath50 is convex @xmath61 :    @xmath62    thus strictly convex and differentiable functions come in pairs @xmath63 with gradients being functional inverses of each other @xmath64 and @xmath65 .",
    "legendre transform is an involution : @xmath66 for strictly convex and differentiable functions . in order to compute @xmath67",
    ", we only need to find the functional inverse @xmath68 of @xmath69 since    @xmath70    however , this inversion may require numerical solving when no analytical expression of @xmath71 is available . see for example the gradient of the log - normalizer of the gamma distribution  @xcite , the dirichlet or von mises - fisher distributions  @xcite .      a bregman divergence",
    "@xmath72 is defined for a strictly convex and differentiable generator @xmath50 as    @xmath73    the kullback - leibler divergence ( relative entropy ) between two members @xmath74 and @xmath75 of the same exponential family amounts to compute a bregman divergence on the corresponding swapped natural parameters :    @xmath76    the proof follows from the fact that @xmath77=\\int_{x\\in\\mathbb{x } } t(x)p_f(x;\\theta)\\dx=\\nabla f(\\theta)$ ]  @xcite . using legendre transform , we further have the following equivalences of the relative entropy :    @xmath78    where @xmath60 is the dual moment parameter ( and @xmath79 ) .",
    "information geometry  @xcite often considers the canonical divergence @xmath80 of eq .",
    "[ eq : cd ] that uses the mixed coordinate systems @xmath81 , while computational geometry  @xcite tends to consider dual bregman divergences , @xmath72 or @xmath82 , and visualize structures in one of those two canonical coordinate systems .",
    "those canonical coordinate systems are dually orthogonal since @xmath83 , the identity matrix .      for exponential family mixtures with a single component @xmath84 ( @xmath85 , @xmath86 )",
    ", we easily estimate the parameter @xmath87 . given @xmath88 independent and identically distributed observations @xmath89 , the maximum likelihood estimator ( mle ) is maximizing the likelihood function :    @xmath90    for exponential families , the mle reports a unique maximum since the hessian of @xmath50 is positive definite ( @xmath91\\succ 0 $ ] ) : @xmath92    the mle is consistent and efficient with asymptotic normal distribution :    @xmath93    where @xmath94 denotes the fisher information matrix :    @xmath95 = \\nabla ^2 f(\\theta ) = ( \\nabla^2 g(\\eta))^{-1}\\ ] ]    ( this proves the convexity of @xmath50 since the covariance matrix is necessarily positive definite . ) note that the mle may be biased ( for example , normal distributions ) .    by using the legendre transform ,",
    "the log - density of an exponential family can be interpreted as a bregman divergence  @xcite : @xmath96    table  [ tab : duality ] reports some illustrating examples of the bregman divergence @xmath97 exponential family duality .",
    ".some examples illustrating the duality between exponential families and bregman divergences.[tab : duality ] [ cols=\"^,^,^ \" , ]     * \\0 . *",
    "initialization * : * * calculate global mean @xmath98 and global covariance matrix @xmath99 : @xmath100 * * @xmath101 , initialize the @xmath102th seed as @xmath103",
    "* assignment * : + @xmath104 with @xmath105 the squared mahalanobis distance : @xmath106 .",
    "+ let @xmath107 be the cluster partition : @xmath108 .",
    "+ ( anisotropic voronoi diagram  @xcite ) * \\2 . *",
    "update the parameters * : + @xmath109 + * goto step  1 * unless local convergence of the complete likelihood is reached .",
    "update the mixture weights * : @xmath110 .",
    "+ * goto step  1 * unless local convergence of the complete likelihood is reached .",
    "the @xmath0-mle++ initialization for the gmm is reported in algorithm  [ algo : kmleppgmm ] .",
    "* choose first seed @xmath111 , for @xmath112 uniformly random in @xmath113 .",
    "* for @xmath114 to @xmath0 * * choose @xmath115 with probability + @xmath116 where @xmath117 .",
    "+ @xmath118 * * add selected seed to the initialization seed set : @xmath119 .",
    "we instantiate the soft bregman em , hard em , @xmath0-mle , and @xmath0-mle++ for the rayleigh distributions , a sub - family of weibull distributions .",
    "a rayleigh distribution has probability density @xmath120 where @xmath121 denotes the _ mode _ of the distribution , and @xmath122 the support .",
    "the rayleigh distributions form a @xmath123-order univariate exponential family ( @xmath124 ) .",
    "re - writing the density in the canonical form @xmath125 , we deduce that @xmath126 , @xmath127 , @xmath128 , and @xmath129 .",
    "thus @xmath130 and @xmath131 .",
    "the natural parameter space is @xmath132 and the moment parameter space is @xmath133 ( with @xmath134 ) .",
    "we check that conjugate gradients are reciprocal of each other since @xmath135 , and we have @xmath136 ( i.e , dually orthogonal coordinate system ) with @xmath137 and @xmath138 .",
    "rayleigh mixtures are often used in ultrasound imageries  @xcite .      following banerjee et al .",
    "@xcite , we instantiate the bregman soft clustering for the convex conjugate @xmath139 , @xmath126 and @xmath134 . the rayleigh density expressed in the @xmath57-parameterization yields @xmath140",
    ".    expectation .",
    ": :    soft membership for all observations @xmath89 :    +    @xmath141    +    ( we can use any of the equivalent @xmath142 ,    @xmath41 or @xmath57 parameterizations for    calculating the densities . ) maximization .",
    ": :    barycenter in the moment parameterization :    +    @xmath143      the associated bregman divergence for the convex conjugate generator of the rayleigh distribution log - normalizer is    @xmath144    this is the itakura - saito divergence is ( indeed , @xmath67 is equivalent modulo affine terms to @xmath145 , the burg entropy ) .    1 . hard assignment .",
    ": :    @xmath146    +    voronoi partition into clusters :    +    @xmath147 2 .",
    "@xmath57-parameter update .",
    ": :    @xmath148    +    @xmath149    +    go to 1 . until ( local ) convergence is met .",
    "weight update .",
    ": :    @xmath150    +    go to 1 . until ( local ) convergence is met .",
    "note that @xmath0-mle does also model selection as it may decrease the number of clusters in order to improve the complete log - likelihood .",
    "if initialization is performed using random point and uniform weighting , the first iteration ensures that all voronoi cells are non - empty .",
    "a good initialization for rayleigh mixture models is done as follows : compute the order statistics for the @xmath151-th elements ( in overall @xmath152-time ) .",
    "those pivot elements split the set @xmath53 into @xmath0 groups @xmath153 of size @xmath154 , on which we estimate the mles .",
    "the @xmath0-mle++ initialization is built from the itakura - saito divergence : @xmath155    k - mle++ :    * choose first seed @xmath111 , for @xmath112 uniformly random in @xmath113 . * for @xmath114 to @xmath0 * * choose @xmath156 with probability + @xmath157 * * add selected seed to the initialization seed set : @xmath119 .",
    "ll : + @xmath44 & inner product ( e.g. , @xmath158 for vectors , @xmath159 for matrices ) + @xmath160 & exponential distribution parameterized using the @xmath41-coordinate system + @xmath161 & support of the distribution family ( @xmath162 ) + @xmath8 & dimension of the support @xmath161 ( univariate versus multivariate ) + @xmath48 & dimension of the natural parameter space + & ( uniparameter versus multiparameter ) + @xmath40 & sufficient statistic ( @xmath163 ) + @xmath43 & auxiliary carrier term + @xmath50 & log - normalizer , log - laplace , cumulant function ( @xmath164 ) + @xmath69 & gradient of the log - normalizer ( for moment @xmath57-parameterization ) + @xmath165 & hessian of the log - normalizer + & ( fisher information matrix , spd : @xmath166 ) + @xmath67 & legendre convex conjugate + : + @xmath41 & canonical natural parameter + @xmath167 & natural parameter space + @xmath57 & canonical moment parameter + @xmath168 & moment parameter space + @xmath56 & usual parameter + @xmath169 & usual parameter space + @xmath170 & density or mass function using the usual @xmath56-parameterization + @xmath171 & density or mass function using the usual moment parameterization + : + @xmath172 & mixture model + @xmath173 & closed probability @xmath174-dimensional simplex + @xmath175 & shannon entropy @xmath176 ( with @xmath177 by convention ) + @xmath178 & shannon cross - entropy @xmath179 + @xmath6 & mixture weights ( positive such that @xmath180 ) + @xmath7 & mixture component natural parameters + @xmath181 & mixture component moment parameters + @xmath182 & estimated mixture + @xmath0 & number of mixture components + @xmath183 & mixture parameters + : + @xmath184 & sample ( observation ) set + @xmath185 & cardinality of sets : @xmath88 for the observations , @xmath0 for the cluster centers + @xmath186 & hidden component labels + @xmath187 & sample sufficient statistic set + @xmath188 & likelihood function + @xmath189 & maximum likelihood estimates + @xmath190 & soft weight for @xmath191 in cluster / component @xmath192 ( @xmath193 ) + @xmath102 & index on the sample set @xmath194 + @xmath195 & index on the mixture parameter set @xmath196 + @xmath197 & cluster partition + @xmath198 & cluster centers + @xmath199 & cluster proportion size + @xmath72 & bregman divergence with generator @xmath50 : + &       + @xmath201 & jensen diversity index : + & @xmath202 + : + @xmath203 & average incomplete log - likelihood : + & @xmath204 + @xmath205 & average complete log - likelihood + & @xmath206 + @xmath207 & geometric average incomplete likelihood : + & @xmath208 + @xmath209 & geometric average complete likelihood : + & @xmath210 + @xmath211 & average @xmath0-means loss function ( average divergence to the closest center ) + &                    cdric archambeau , john  aldo lee , and michel verleysen . on convergence problems of the em algorithm for finite gaussian mixtures . in _",
    "european symposium on artificial neural networks ( esann ) _ , pages 99106 , 2003 .",
    "arindam banerjee , inderjit dhillon , joydeep ghosh , and srujana merugu .",
    "an information theoretic analysis of maximum likelihood mixture estimation for exponential families . in _ proceedings of the twenty - first international conference on machine learning _ , icml , pages 5764 , new york , ny , usa , 2004 .",
    "acm .",
    "jason  v. davis and inderjit  s. dhillon .",
    "differential entropic clustering of multivariate gaussians . in bernhard scholkopf ,",
    "john platt , and thomas hoffman , editors , _ neural information processing systems ( nips ) _ , pages 337344 . mit press , 2006 .",
    "kittipat kampa , erion hasanbelliu , and jose principe .",
    "closed - form cauchy - schwarz pdf divergence for mixture of gaussians . in _ proceeding of the international joint conference on neural networks ( ijcnn ) _ , pages 2578",
    " 2585 , 2011 .",
    "michael kearns , yishay mansour , and andrew  y. ng .",
    "an information - theoretic analysis of hard and soft assignment methods for clustering . in",
    "_ proceedings of the thirteenth conference on uncertainty in artificial intelligence _ ,",
    "uai , pages 282293 , 1997 .",
    "franois labelle and jonathan  richard shewchuk .",
    "anisotropic voronoi diagrams and guaranteed - quality anisotropic mesh generation . in _ proceedings of the nineteenth annual symposium on computational geometry _ , scg 03 , pages 191200 , new york , ny , usa , 2003 .",
    "acm .",
    "james  b. macqueen .",
    "some methods of classification and analysis of multivariate observations . in l.",
    "m.  le cam and j.  neyman , editors , _ proceedings of the fifth berkeley symposium on mathematical statistics and probability_. university of california press , berkeley , ca , usa , 1967 .",
    "frank nielsen , paolo piro , and michel barlaud .",
    "bregman vantage point trees for efficient nearest neighbor queries . in _",
    "ieee international conference on multimedia and expo ( icme ) _ , pages 878881 , new york city , usa , june 2009 . ieee .",
    "richard nock , panu luosto , and jyrki kivinen .",
    "mixed bregman clustering with approximation guarantees . in _ proceedings of the european conference on machine learning and knowledge discovery in databases _ , pages 154169 , berlin , heidelberg , 2008 .",
    "springer - verlag .",
    "paolo piro , frank nielsen , and michel barlaud .",
    "tailored bregman ball trees for effective nearest neighbors . in _",
    "european workshop on computational geometry ( eurocg ) _ , loria , nancy , france , march 2009 ."
  ],
  "abstract_text": [
    "<S> we describe @xmath0-mle , a fast and efficient local search algorithm for learning finite statistical mixtures of exponential families such as gaussian mixture models . </S>",
    "<S> mixture models are traditionally learned using the expectation - maximization ( em ) soft clustering technique that monotonically increases the incomplete ( expected complete ) likelihood . given prescribed mixture weights , the hard clustering @xmath0-mle algorithm iteratively assigns data to the most likely weighted component and update the component models using maximum likelihood estimators ( mles ) . using the duality between exponential families and bregman divergences , </S>",
    "<S> we prove that the local convergence of the complete likelihood of @xmath0-mle follows directly from the convergence of a dual additively weighted bregman hard clustering . </S>",
    "<S> the inner loop of @xmath0-mle can be implemented using any @xmath0-means heuristic like the celebrated lloyd s batched or hartigan s greedy swap updates . </S>",
    "<S> we then show how to update the mixture weights by minimizing a cross - entropy criterion that implies to update weights by taking the relative proportion of cluster points , and reiterate the mixture parameter update and mixture weight update processes until convergence . </S>",
    "<S> hard em is interpreted as a special case of @xmath0-mle when both the component update and the weight update are performed successively in the inner loop . to initialize @xmath0-mle , </S>",
    "<S> we propose @xmath0-mle++ , a careful initialization of @xmath0-mle guaranteeing probabilistically a global bound on the best possible complete likelihood .     </S>",
    "<S> # 1#2#1,#2 # 1#2#1,#2 # 1doi:#1    exponential families , mixtures , bregman divergences , expectation - maximization ( em ) , @xmath0-means loss function , lloyd s @xmath0-means , hartigan and wong s @xmath0-means , hard em , sparse em . </S>"
  ]
}