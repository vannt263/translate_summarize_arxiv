{
  "article_text": [
    "we consider a greedy strategy based algorithm for preprocessing on vector database necessary for subspace clustering .",
    "the problem of subspace clustering consists in classification of the vector data belonging to a few linear or affine low - dimensional subspaces of the high dimensional ambient space when neither subspaces or even their dimensions are not known , i.e. , they have to be identified from the same database .",
    "the algorithm considered below does not require clean input data .",
    "vice versa , we assume that those data are corrupted with sparse errors , random noise distributed over all vector entries and quite significant part of data is missed .    the property of errors to constitute a sparse set means that some ( but not all ) vector entries are corrupted , i.e. , those values are randomly replaced .",
    "the locations ( indices ) of corrupted entries are unknown .    the noise is randomly introduced in each entry .",
    "its magnitude is usually much less than the data magnitude .    in information theory ,",
    "the missed samples are called erasures .",
    "they have two main features .",
    "first , the data values in erasures does not have practical importance .",
    "the most natural way is to think about erasures as about lost data .",
    "second , the coordinates of erasures are known .    using more formal definition",
    ", we have @xmath0 vectors @xmath1 in @xmath2 linear or affine subspaces @xmath3 with the dimensions @xmath4 of the @xmath5-dimensional euclidean space @xmath6 .",
    "we do not assume that those spaces do not have non - trivial intersections .",
    "however , we do assume that any one of those spaces is not a subspace of other one .",
    "at the same time , the situation when one subspace is a subspace of the direct sum of two other subspaces is allowed .",
    "such settings inspire the hope that when @xmath0 is large enough and the points are randomly and independently distributed on those planes some sophisticated algorithm can identify those planes and classify the belongingness of each point to the found subspaces . of course",
    ", some of points may belong to the intersection of two or more subspaces , then such point maybe assigned to one of those space or to all of them . with the probability 1 the points belong to only one of subspaces .",
    "then the problem consists in finding a permutation matrix @xmath7 such that @xmath8=y\\gamma,\\ ] ] where @xmath9 is an input matrix whose columns are the given points in an arbitrary random order , whereas in @xmath10 $ ] is rearrangement of the matrix @xmath11 in the accordance with the affiliation of the vector with the subspaces @xmath12 .",
    "the problem of finding clusters @xmath13 is usually by means of finding the clusters in the graph whose edges ( to be more precise the weights of edges ) characterize the interconnection between pairs of vertexes . in our case , the popular method of clustering consists in making the points to play role of the vertexes , while the weights are set from the coefficients of decomposition of the vectors through other vectors from the same space @xmath12",
    "this idea looks as vicious circle .",
    "we are trying to identify the space @xmath12 accommodating the vector @xmath14 , using its linear decomposition in the remaining vectors of @xmath12 .",
    "however , the situation is not hopeless at all . in @xcite ,",
    "the excellent suggestion was formulated .",
    "obviously , the decomposition problem formulated above is reduced to solving the non - convex problem @xmath15 where @xmath16 is the hamming weight of the vector @xmath17 , @xmath18 .",
    "the problem of finding the sparsest solutions to ( [ cs ] ) , so - called , _ compressed sensing _ or _ compressive sampling _ underwent thorough study originated in @xcite , @xcite , @xcite and continued in hundreds of theoretical and applied papers .    in the ideal world of perfect computational precision and unlimited computational power , with probability one , the solutions of ( [ cs ] ) would point out the elements of the appropriate @xmath19 by their non - zero decomposition coefficients . provided that no vectors of wrong subspaces participate in decomposition of each column of @xmath11 , the matrix @xmath20 whose columns are @xmath21 allows perfectly reconstruct the structure of the subspaces in polynomial time .",
    "there are two obstacles on that way .",
    "first , the precision of the input matrix @xmath11 is not perfect .",
    "so the decompositions may pick up wrong vectors even if we are able to solve problem ( [ cs ] ) . in this case , the problem of subspace clustering is considered for the similarity graph defined by the symmetric matrix @xmath22 . while , generally speaking , this problem has non - polynomial complexity , there exist practical algorithms allowing right clustering when the number of `` false '' interconnections of elements from different subspaces are not very dense and not very intensive .",
    "following @xcite , we use some modification of the spectral clustering algorithm from @xcite which is specified in @xcite as `` graph s random walk laplacian '' .",
    "the second obstacle consists in non - polynomial complexity of the problem ( [ cs ] ) . the elegant solution allowing to overcome this obstacle is replacement of non - convex problem ( [ cs ] ) with the convex problem @xmath23 or @xmath24 in the matrix form .",
    "it follows from the fundamental results from @xcite , @xcite , @xcite that for matrices @xmath11 with some reasonable restrictions on @xmath11 and for not very large hamming weight of the ideal sparse solution , it can be uniquely found by solving convex problem ( [ cs1 ] ) .",
    "there are other more efficient @xmath25-based methods for finding sparse solutions ( e.g. , see @xcite , @xcite , @xcite ) .",
    "it should be mentioned , that the matrices @xmath11 in practical problems may be very far from the requirements for the uniqueness of solutions . at the same time , the uniqueness of the solutions are not necessary in our settings .",
    "we just wish to have the maximum of separation between indices of the matrix @xmath26 corresponding to different subspaces .    in the case of successful clustering , the results for each @xmath12 may be used for further processing like data noise removal , error correction , and so on .",
    "such procedures become significantly more efficient when applied to low - rank submatrices of @xmath11 corresponding to one subspace .    thus , in applications , the problems involving subspace clustering can be split into 3 stages : 1 ) preprocessing ; 2 ) search for clusters in the graphs ; 3 ) processing on clusters . in this paper , we develop a first stage algorithm helping to perform the second stage much more efficiently then the state - of - the - art algorithms .",
    "we do not discuss any aspects of improvements of stage 2 .",
    "we just take one of such algorithms , specifically the spectral clustering , and use it for comparison of the influence of our and competing preprocessing algorithms on the efficiency of clustering .    as for stage 3 ,",
    "its content depends on an applied problem requesting subspace clustering .",
    "one of typical possible goal of the third stage can be data recovery from incomplete and corrupted measurements .",
    "sometimes this problem is called `` netflix problem '' .",
    "we will discuss below how the same problems of incompleteness and corruption can be solved within clustering preprocessing .",
    "however , for low - rank matrices it can be solved more efficiently . among",
    "many existing algorithms we mention the most recent papers @xcite , @xcite @xcite , @xcite , @xcite , @xcite providing the best results for input having both erasures and errors .    in section [ disc ] , we discuss the problem formal settings . in section [ alg ] , the existing ssc algorithm and our modification will be given .",
    "the results of numerical experiments showing the consistency of the proposed approach will be given in section [ exper ] .",
    "we use sparse subspace clustering algorithm ( ssc ) from @xcite as a basic algorithm for our modification based on a greedy approach .",
    "therefore , significant part of reasoning we give in this section can be found in @xcite and in the earlier paper @xcite .",
    "very similar ideas of subspace self - representation for subspace clustering were used also in @xcite .",
    "however , the error resilience mechanism in that paper was used under assumption that there are enough uncorrupted data vector , whereas , this assumption is not required in @xcite .",
    "optimization cs problems [ cs1 ] and [ csm ] assume that the data are clean , i.e. , they have no noise and errors . considering the problem within the standard cs framework , the problem [ cs ] can be reformulated as finding the sparsest vectors @xmath27 ( decomposition coefficients ) and @xmath28 satisfying the system of linear equations @xmath29 .",
    "it was mentioned in @xcite that the last system can be re - written as @xmath30\\left[\\begin{array}{c } \\bc \\\\ \\mathbf e\\end{array}\\right],\\ ] ] where @xmath31 is the identity matrix .",
    "therefore , the problem of sparse reconstruction and error correction can be solved simultaneously with cs methods . in @xcite",
    ", we designed an algorithm efficiently finding sparse solutions to ( [ errorcor ] ) .",
    "unfortunately , the subspace clustering can not use this strategy straightforwardly because not only `` measurements '' @xmath14 are corrupted in ( [ cs ] ) but `` measuring matrix '' @xmath32 also can be corrupted .",
    "it should be mentioned that if the error probability is so low that there exist uncorrupted columns of @xmath11 constituting bases for all subspaces @xmath33 , the method from @xcite can solve the problem of sparse representation with simultaneous error correction . in",
    "what follows , the considered algorithm will admit a significantly higher error rate .",
    "in particular , all columns of @xmath11 may be corrupted .",
    "following @xcite , we introduce two ( unknown for the algorithm ) @xmath34 matrices @xmath35 and @xmath36 .",
    "the matrix @xmath35 contains a sparse ( i.e. , @xmath37 ) set of errors with relatively large magnitudes .",
    "the matrix @xmath36 defines the noise having relatively low magnitude but distributed over all entries of @xmath36 .",
    "thus , the clean data are representable as @xmath38 .",
    "therefore , when the data are corrupted with sparse errors and noise , the equation @xmath39 has to be replaces by @xmath40 the authors of @xcite applied a reasonable simplification of the problem by replacing 2 last terms of ( [ corr ] ) with some ( unknown ) sparse matrix @xmath41 and the matrix with the deformed noise @xmath42 .",
    "provided that sparse @xmath20 exists , the matrix @xmath43 still has to be sparse .",
    "this transformation leads to some simplification of the optimization procedure .",
    "this is admissible simplification since , generally speaking , we do not need to correct and denoise the input data @xmath11 .",
    "our only goal is to find the sparse matrix @xmath20 .",
    "therefore , we do not need matrices @xmath35 and @xmath36 .",
    "while , as we mentioned above , the error correction procedure can be applied after subspace clustering , original setting ( [ corr ] ) with the genuine values of the errors and noise within subspace clustering still makes sense and deserves to become a topic for future research .",
    "taking into account modifications from last paragraph , the authors of @xcite formulate constrained optimization problem @xmath44 where @xmath45 is the frobenius norm of a matrix",
    ". if the clustering into affine subspaces is required , the additional constrain @xmath46 is added .    on the next step , using the representation @xmath47 and introducing an auxiliary matrix @xmath48 , constrained optimization problem ( [ constr1 ] ) is transformed into @xmath49 optimization problems ( [ constr1 ] ) and ( [ constr2 ] ) are equivalent . indeed , obviously , at the point of extremum of ( [ constr2 ] ) , diag@xmath50 .",
    "hence , @xmath51 .    at last ,",
    "the quadratic penalty functions with the weight @xmath52 corresponding to constrains are added to the functional in ( [ constr2 ] ) and the lagrangian functional is composed .",
    "the final lagrangian functional is as follows @xmath53 where the vector @xmath54 and the matrix @xmath55 are lagrangian coefficients .",
    "obviously , since the penalty functions are formed from the constrains , they do not change the point and value of the minimum .",
    "for minimization of functional ( [ lagr ] ) an alternating direction method of multipliers ( admm , @xcite ) is used . in @xcite ,",
    "this is a crucial part of the entire algorithm which is called the sparse subspace clustering algorithm .",
    "the parameters @xmath56 and @xmath57 in ( [ lagr ] ) are selected in advance .",
    "they define the compromise between good approximation of @xmath11 with @xmath58 and the high sparsity of @xmath20 .",
    "the general rule is to set the larger values of the parameters for the less level of the noise or errors . in @xcite , the selection of the parameters by formulas @xmath59 where",
    "@xmath60 and @xmath61 is recommended .",
    "the initial parameter @xmath62 is set in advance .",
    "it is updated as @xmath63 with iterations of ssc algorithm .",
    "we notice that , adding the penalty terms , we do not change the problem .",
    "it still has the same minimum .",
    "however , the appropriate selection of @xmath64 and @xmath65 accelerates the algorithm convergence significantly .",
    "each iteration of the algorithm is based on consecutive optimization with respect to each of the unknown values @xmath66 , @xmath20 , @xmath43 , @xmath67 , @xmath55 which are initialized by zeros before the algorithm starts .",
    "due to appropriate form of functional ( [ lagr ] ) , optimization of each value is simple and computationally efficient .",
    "the five formulas for updating the unknown values are discussed below .",
    "the matrix @xmath68 is a solution of the system of linear equations @xmath69    when the data are located on linear subspaces the terms @xmath70 and @xmath71 may be removed ( set to 0 ) from ( [ a ] ) .    while the system ( [ a ] ) has matrices of size @xmath72 , due to its special form , the complexity of the algorithm for the inverse matrix is @xmath73 that is much lower than @xmath74 , provided that @xmath75",
    "unfortunately , the matrix @xmath55 may have a full rank .",
    "therefore , the computational cost of its product with the inverse matrix is @xmath76 , i.e. , not so impressive as for the matrix inversion",
    ".    we will need the following notation @xmath77:=\\left\\{\\begin{array}{ll } x-\\epsilon , & x>\\epsilon,\\\\x+\\epsilon , & x<-\\epsilon,\\\\0 , & \\text{otherwise};\\end{array}\\right.\\ ] ] where @xmath78 can be either a number or a vector or a matrix .",
    "the operator @xmath79 $ ] is called the shrinkage operator .",
    "let @xmath80.\\ ] ] then the matrix @xmath81 is defined by the formula @xmath82 the remaining values @xmath83 , @xmath67 , and @xmath55 are computed as @xmath84.\\ ] ]    @xmath85    @xmath86    the algorithm goes to the next iteration if one of conditions @xmath87 @xmath88 fails , where @xmath89 is the given error tolerance .    in the form shown above ssc algorithm",
    "gives the state - of - the - art benchmarks for subspace clustering problems .",
    "our suggestion is to attract ideas of greedy algorithms to increase the capability of ssc algorithm in subspace clustering .",
    "greedy algorithms are very popular in non - linear approximation ( especially in redundant systems ) when the global optimization is replaced with iterative selection of the most probable candidates from the point of view their prospective contribution into approximation .",
    "the procedure is repeated with selection of new entries , considering the previously selected entries as reliable with guaranteed participation in approximation .",
    "the most typical case is orthogonal greedy algorithm consisting in selection of the approximating entries having the biggest inner products with the current approximation residual and follow - up orthogonal projection of the approximated object onto the span of selected entries .    in many cases",
    ", oga allows to find the sparsest representations if they exist . in @xcite and @xcite , we applied greedy idea in combination with the reweighted @xmath25-minimization to cs problem of finding the sparsest solutions of underdetermined system .",
    "we used the existing @xmath25-minimization scheme with the the opportunity to reweight entries .",
    "when the basic algorithm fails , the greedy blocks picks the biggest ( the most reliable ) entries in the decomposition whose magnitudes are higher than some threshold .",
    "those entries are considered as reliable",
    ". therefore they get the less weight in the @xmath25 norm while other entries are competing on next iterations for the right to be picked up .",
    "the similar idea was employed in our recent paper @xcite , where the greedy approach was applied to the algorithm for completion of low - rank matrices from incomplete highly corrupted samples from @xcite based on augmented lagrange multipliers method .",
    "the simple greedy modification of the matrix completion algorithm from @xcite gave the boost in the algorithm restoration capability .",
    "now we discuss details how the greedy approach can be incorporated in ( to be more precise _ over _ ) ssc algorithm .",
    "first of all we introduce a non - negative matrix @xmath90 whose entries reflect our knowledge about the entries of error matrix @xmath43 .",
    "let us think that the regular entries with no ( say , side ) information have values @xmath91 , whereas entries with coordinates of presumptive errors are set to small value or to 0 .",
    "let us consider the mechanism of the influence of the parameter @xmath56 on the output matrix @xmath20 .",
    "@xmath56 sets the balance between the higher level of the sparsity of @xmath20 with the more populated error matrix @xmath43 and less sparse @xmath20 but less populated matrix @xmath43 .",
    "setting too small @xmath56 makes too many `` errors '' and very sparse @xmath20 .",
    "however , probably , this is not what we want .",
    "this would mean that sake of @xmath20 sparsity we introduced too large distortion into the input data @xmath11 . at the same time , if we know for sure or almost for sure that some entry of @xmath11 with indices @xmath92 is corrupted , we loose nothing by assigning to this element an individual small weight in functional ( [ lagr ] )",
    ". this weight can be much less than @xmath56 or even equal to @xmath93 .",
    "thus , we have to replace the term @xmath94 with @xmath95 , where operation @xmath96 means entrywise product of two matrices . in practice",
    ", this means that in formula ( [ e ] ) we will apply different shrinkage threshold for different indices .",
    "generally speaking , it makes sense to use all range of non - negative real numbers to reflect our knowledge about @xmath43 .",
    "say , highly reliable entries have to be protected from distortion by the weight greater than 1 in @xmath97 .",
    "however , in this paper we restrict ourself with two - level entries : either 1 ( no knowlege ) or @xmath98 ( suspicious to be an error ) .    when no a priori knowledge is available , we set all entries of @xmath97 to 1 .",
    "however , in information theory there is a special form of corrupted information which is called _",
    "erasure_. the losses of network packets is the most typical reason of erasures .",
    "another example of erasure is given by occlusions in video models when moving object temporary overlap the background or each other .",
    "erasures represent rather missing than corrupted information . erased",
    "entries like entries with errors have to be restored or at least taken into account .",
    "the only difference of erasures with errors is a priory knowledge of their locations .",
    "this additional information allows to reconstruct the values of erasures in more efficient way than errors .",
    "so the entries with erasures have to be marked ( say , by setting the corresponding entries of @xmath97 to 0 ) before the algorithm starts .",
    "the entries which are suspicious to be errors dynamically extend `` the map of marked errors '' in @xmath97 after each iteration of the greedy algorithm .",
    "thus , in the greedy sparse subspace clustering ( gssc ) we organize an external loop over sparsification part of ssc algorithm with an additional input matrix @xmath97 which is initialized with ones at regular entries and with some small non - negative number @xmath99 at the places of erased entries .",
    "one iteration of our greedy algorithm consists in running the modified version of ssc and @xmath97 and @xmath11 updates .",
    "our modification of ssc consists of two parts .",
    "first , we take into account the matrix @xmath97 while computing @xmath83 by formula ( [ e ] ) .",
    "second , we update @xmath100 on each iteration of greedy algorithm .",
    "after the first iteration , the estimated matrix @xmath43 is used to set the threshold @xmath101 where @xmath102 .",
    "we use the median estimate to avoid too low threshold leading to large error map when there is no error in the data .    starting from the second greedy iteration ,",
    "we just update threshold @xmath103 , @xmath104 .",
    "the current value @xmath105 is used for the extension of the error map by formula @xmath106 where @xmath107 is the error matrix obtained on the previous iteration .",
    "the updated @xmath97 is used for next iteration .",
    "in addition , on each iteration of greedy algorithm , we update the input matrix of ssc algorithm by the formula @xmath108 for the pairs @xmath109 marked in the current @xmath110 as errors / erasures . while @xmath111 is not a genuine matrix of errors , this is not serious drawback for the original ssc algorithm .",
    "however , for gssc this may lead to unjustified and very undesirable extension of the set @xmath97 .",
    "more accurate estimate of the error set in future algorithms may bring significant benefits for gssc .    as we mentioned",
    ", erasures recovery is easier than error correction .",
    "putting new entries into the map of errors , we , in fact , announce them erasures .",
    "if we really have solid justification for this action , then the new iteration of gssc can be considered as a new problem which is easier for ssc than the previous iteration .",
    "we will come back to the discussion about interconnection between erasures and errors after presentation of numerical experiments .",
    "we will present the comparison of gssc and ssc on synthetic data .",
    "the input data was composed in accordance with the model given in @xcite .",
    "105 data vectors of dimension @xmath112 are equally split between three 4-dimensional linear spaces @xmath113 . to make the problem more complicated each of those 3 spaces belongs to sum of two others .",
    "the smallest angles between spaces @xmath114 and @xmath115 are defined by formulas @xmath116 we construct the data sets using vectors generated by decompositions with random coefficients in orthonormal bases @xmath117 of spaces @xmath115 .",
    "three vectors @xmath118 belongs to the same 2d - plane with angles @xmath119 and @xmath120 .",
    "the vectors @xmath121 are mutually orthogonal and orthogonal to @xmath122 ; @xmath123 , @xmath124 .",
    "the generator of standard normal distribution is used to generate data decomposition coefficients .",
    "after the generation , a random unitary matrix is applied to the result to avoid zeros in some regions of the matrix @xmath11 .",
    "we use the notation @xmath125 and @xmath126 for probabilities of erasures and errors correspondingly .",
    "when we generate erasures we set random entries of the matrix @xmath11 with probability @xmath125 to zero and set those elements of @xmath97 to @xmath127 .",
    "the coordinates of samples with errors are generated randomly with probability @xmath126 .",
    "we use the additive model of errors , adding values of errors to the correct entries of @xmath11 .",
    "the magnitudes of errors are taken from standard normal distribution .",
    "we run 20 trials of gssc and ssc algorithms for each combination of @xmath128 , @xmath129 @xmath130 @xmath131 and output average values of misclassification .",
    "we note that for the angle @xmath132 the spaces @xmath133 have a common line and @xmath134 .",
    "nevertheless , we will see that ssc and especially gssc shows high capability even for this hard settings .",
    "now we describe the algorithm parameters .",
    "we do not use any creative stop criterion for greedy iterations .",
    "we set just make 5 iterations in each of 20 trials for all combinations @xmath128 .",
    "the set @xmath97 is updated after each iteration as described above .",
    "the parameters for greedy envelop loop are : @xmath135 , @xmath136 , @xmath137 ,    the input parameters of the basic ssc block are as follows .",
    "we set @xmath138 , @xmath139 , @xmath140 , @xmath141 , @xmath142 .",
    "the results presented on fig .",
    "1 confirms that gssc has much higher error resilience than ssc .",
    "for all models of input data and for both algorithms `` the phase transition curve '' is in fact straight line with the slope about 0.4 . in particular",
    ", this means that the influence on clustering of one error is approximately corresponds to the influence of 2.5 erasures .",
    "we can see that gssc gives reliable clustering when @xmath143 for @xmath144 and @xmath145 for @xmath146 . for the case",
    "@xmath147 , clustering can not be absolutely perfect even for gssc .",
    "indeed , the clustering for @xmath148 and @xmath149 can not be better than for error free model . at the same time",
    ", gssc was designed for better error handling . in the error",
    "free case , gssc has no advantage over ssc algorithm .",
    "thus , the images on fig .  1 for @xmath132 have the gray background of the approximate level @xmath150 equal to the rate of misclassification of ssc .",
    "we believe that the reason of the misclassification lies in the method how we define the success . for @xmath147 , there is a  common line belonging to all spaces @xmath19 . for points close to that line , the considered algorithm has to make a hard decision , appointing only one cluster for each such point .",
    "( 00,110 ) ( 0,5 ) ( 120,5 )    ( 00,110 ) ( 0,5 ) ( 120,5 )    ( 00,115 ) ( 0,10 ) ( 120,10 ) ( 70 , 0 )    probably , for most of applied problems , the information about multiple point accommodation is more useful than unique space selection .",
    "we advocate for such multiple selection because the typical follow - up problem after clustering is correction of errors in each of clusters . for this problem , it is not important to which of clusters the vector belonged from the beginning .",
    "when the vector affiliation is really important , side information has to be attracted .    the second part of experiment deals with noisy data processing .",
    "we apply to the matrix @xmath11 independent gaussian noise of magnitude 10% of mean square value of the data matrix @xmath11 , i.e. , the noise level is -20db . on fig .",
    "2 , we present the results of processing of the noisy input analogous to results on fig .",
    "1 . evidently , that this quite strong noise has minor influence on the clustering efficiency .",
    "if we increase the noise up to -15 db , the algorithms still resist .",
    "for -10 db ( see fig .",
    "3 ) gssc looses a lot but still outperforms ssc .",
    "those losses are obviously caused by the increase of the noise fraction in the mixture errors - erasures - noise , while the greedy idea efficiently works for highly localized corruption like errors and erasures .",
    "we emphasize that all results on figs .",
    "13 were obtained with the same algorithm parameters .",
    "( 00,110 ) ( 0,5 ) ( 120,5 )    ( 00,110 ) ( 0,5 ) ( 120,5 )    ( 0,115 ) ( 0,10 ) ( 120,10 ) ( 50 , 0 )    ( 00,115 ) ( 0,10 ) ( 120,10 ) ( 50 , 0 )    in conclusion , we demonstrate the role and dynamic of the greedy iterations . in tab .",
    "[ tab ] , we give the dependence of the rate of misclassification for @xmath151 , @xmath152 , snr=20 db from the number of gssc iterations .",
    "the value iter=0 corresponds to the pure ssc algorithm .",
    "the result was obtained as the average value of 100 trials .",
    ".misclassification rate and number of iterations [ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]     [ tab ]",
    "we consider a modification of sparse subspace clustering algorithm based on greedy approach giving significant improvement of the algorithm resilience to ( simultaneous ) entry corruption , incompleteness , and noise .",
    "while the basic ssc algorithm has some internal resilience to corruption , it reduces error and erasure influence on clustering quality , strictly speaking , it does not have error correction capabilities .      in the described version ,",
    "gssc has 5 - 6 iterations of the algorithm ssc , having proportional increase of computing time .",
    "we believe that this computing time increase can be significantly eliminated with preserving the algorithm efficiency if updates of @xmath97 are incorporated into internal iterations of ssc algorithm .",
    "this option was implemented by us in very analogous situation for acceleration of the @xmath25-greedy algorithm in @xcite .",
    "finding an appropriate stopping criterion also could reduce computing time and improve clustering .",
    "one more reserve for algorithm improvement is selection of the parameters adaptive to input data .",
    "while , as we mentioned all results were obtained with the same set of parameters , the adaptation may bring significant increase of algorithm capability .",
    "one of such adaptive solution for error correction in compressed sensing was recently found by the authors in  @xcite .",
    "s.boyd , n.parikh , e.chu , b.peleato , and j. eckstein , distributed optimization and statistical learning via the alternating direction method of multipliers , foundations and trends in machine learning , v.3 ( 2010 ) , no 1 , 1122 .",
    "s.rao , r.tron , r.vidal , yi ma , motion segmentation in the presence of outlying , incomplete , or corrupted trajectories , ieee transactions on pattern analysis and machine intelligence , 32(2010 ) , 18321845 .",
    "j.wright , a. y. yang , a. ganesh , s. s. sastry , and yi ma , robust face recognition via sparse representation , pattern analysis and machine intelligence , ieee transactions on , feb 2009 , volume : 31 , issue : 2 page(s ) : 210227 ."
  ],
  "abstract_text": [
    "<S> we describe the greedy sparse subspace clustering ( gssc ) algorithm providing an efficient method for clustering data belonging to a few low - dimensional linear or affine subspaces from incomplete corrupted and noisy data .    </S>",
    "<S> we provide numerical evidences that , even in the simplest implementation , the greedy approach increases the subspace clustering capability of the existing state - of - the art ssc algorithm significantly .    </S>",
    "<S> * keywords : * subspace clustering , sparse representations , greedy algorithm , law - rank matrix completion , compressed sensing </S>"
  ]
}