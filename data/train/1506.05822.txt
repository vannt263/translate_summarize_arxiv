{
  "article_text": [
    "predicting the future behavior of complex systems from measured time series constitutes a major goal in many fields of science .",
    "traditionally , this problem has been mainly addressed in terms of model - based approaches , often using linear models @xcite . as an alternative , since the late 1980s",
    "also model - free predictions have been developed using nearest neighbors in state space @xcite or neural networks @xcite . in the nearest - neighbor technique ,",
    "states similar to the present state are searched for in the past of a time series @xmath0 and a future value @xmath1 at a prediction step @xmath2 is forecasted by simply averaging the @xmath0 values @xmath2-steps ahead of the nearby past states or using local - linear models @xcite .",
    "model - free predictions have so far been mostly univariate where states are usually reconstructed from embedding a single time series using takens theorem @xcite .",
    "however , the intertwined nature of complex systems calls for multivariate approaches taking into account more available information .",
    "now the problem is that the curse of dimensionality makes useful nearest - neighbor predictions almost impossible for more than a few predictors @xcite , especially if many of these predictors carry redundant information .    from an information - theoretic perspective",
    ", the minimal set of variables that maximizes the ( multivariate ) mutual information @xcite with a target variable is most predictive @xcite .",
    "minimality is required to avoid the curse of dimensionality .",
    "it is important to note that this set of variables can be different from those with _",
    "individually _ large mutual information with the target variable .",
    "indeed , sometimes the right combination of predictors matters .",
    "for example , if @xmath0 is driven multiplicatively by @xmath3 , the mutual information of each of these predictors with @xmath0 can be very low and only the mutual information of the combined set @xmath4 with @xmath0 is very high .",
    "in general , such _ synergetic _ sets can only be detected by searching through all subsets of variables .",
    "however , the number of possible combinations for taking into account more variables and larger time lags grows exponentially making such a search strategy prohibitive due to computational constraints .",
    "therefore , simple search strategies such as ranking the predictors by their mutual information with a target variable or the _ cmi - forward selection _ using conditional mutual information ( cmi ) have been proposed recently @xcite . here",
    "we demonstrate that such approaches can fail already in simple cases where one can not avoid to test different subsets of predictors .",
    "however , we information - theoretically prove that the search can be restricted to _ causal _ drivers . to obtain these drivers , we exploit a recently developed model - free algorithm to consistently reconstruct causal drivers from multivariate time series that keeps the estimation dimension as low as possible with typically low computational complexity @xcite",
    ". the much smaller set of causal drivers then allows for a globally optimizing search strategy which is optimal also for synergetic cases . in this contribution",
    ", we additionally provide a practical criterion for selecting the optimal size of the subset of predictors which compares well even with computationally expensive cross - validation approaches . in numerical experiments we found that our optimal scheme yields much improved predictions and often even runs faster than forward selection .",
    "our method suggests a general framework not only for prediction , but also for general statistical inference problems for datasets ( not only time series ) where the underlying mechanisms are poorly understood : firstly , the optimal model - free approach can be applied for selecting not only causal , but also possibly synergetic driving variables and , secondly , these variables can be used to fit a model to learn the functional form of the dependencies on these causal predictors .",
    "this approach combines the advantage of a model - free approach to detect relevant variables with the advantage of model - based methods to efficiently harness these variables to further improve predictions or understand mechanisms .",
    "our framework is illustrated on a sea - surface temperature based index of the el nio southern oscillation ( enso ) in the tropical pacific .",
    "this paper is organized as follows . after deriving the optimality of causal predictors in sect .",
    "[ sec : optimal_prediction ] , we discuss common approaches for information - theoretic variable selection for predictions in sect .",
    "[ sec : common_prediction_scheme ] .",
    "the optimal scheme is explained in sect .  [",
    "sec : optimal_prediction_scheme ] including the causal pre - selection algorithm and selection criteria .",
    "section  [ sec : computational_complexity ] discusses the computational complexity of the different schemes . in sect .",
    "[ sec : model_example ] we compare our scheme with other approaches in a model example . extensive numerical experiments on multivariate nonlinear stochastic delay processes are conducted in sect .  [ sec : numerical_experiments ] . in sect .",
    "[ sec : optimal_plus_model ] we analyze the combination of the model - free selection with a model - based prediction scheme which is applied in sect .  [ sec : enso_prediction ] to predict future values of the considered enso index .",
    "the discrete - time evolution of a subprocess @xmath0 of a multivariate @xmath5-dimensional stochastic process @xmath6 can be described by @xmath7 with some function @xmath8 , where @xmath9 are the deterministically driving variables including the past of @xmath0 at possibly different time lags and @xmath10 represents stochastic noise driving @xmath0 .",
    "the uncertainty about the outcome of @xmath11 can be quantified by the shannon entropy @xcite which decomposes into @xmath12 where the latter term is the _ source entropy _ @xcite .",
    "this conditional entropy quantifies the minimum level of uncertainty that can not be predicted even if the whole past ( and present ) @xmath13 is known .",
    "if the dependency of @xmath14 on the noise term @xmath10 is additive , the source entropy equals the entropy of the noise : @xmath15 .",
    "the infinite - dimensional multivariate mutual information ( mmi ) @xmath16 , on the other hand , quantifies the predictable part by measuring by how much the uncertainty about the outcome of @xmath11 can be maximally reduced if @xmath13 was perfectly measured .    in practice",
    ", a prediction using the entire set @xmath13 ( truncated at some maximal lag ) would severely suffer from the curse of dimensionality and _ overfitting _",
    "@xcite , which means that many variables do not actually carry useful information , but merely fit the noise in the time series , and the prediction  trained on a learning set  would perform poorly on a test set .",
    "the goal is , thus , to select a small subset of predictors from @xmath13 that carries a maximum of information about @xmath11 and still generalizes well on new data .",
    "however , for an @xmath5-variate process @xmath6 truncated to a maximum delay @xmath17 the number of possible subsets grows exponentially in @xmath5 and @xmath17 .    to avoid this combinatorial explosion , simple search strategies such as ranking the individual predictors by their mutual information ( mi - scheme ) with the target variable",
    "can be used which , however , is prone to include redundant variables that do not improve a prediction .",
    "forward selection , a more advanced technique , iteratively determines predictors based on how much information they contain _ additionally _ to the already chosen variables using conditional mutual information ( cmi - scheme ) @xcite leading to a polynomial computational complexity . these strategies will be discussed in sect .",
    "[ sec : common_prediction_scheme ] .",
    "but forward selection is not a globally optimal strategy , one reason being that it might select variables that are not deterministically driving @xmath11 , the other that it fails to detect synergetic cases as demonstrated in our model example in sect .",
    "[ sec : model_example ] .",
    "the unknown deterministic drivers @xmath18 in eq .",
    "( [ eq : evolution ] ) are , however , key to arrive at optimal predictors as can be shown by decomposing the mmi in eq .",
    "( [ eq : entropy_decomp ] ) using the chain rule @xcite @xmath19 where @xmath20 denotes the set subtraction .",
    "the second term is zero for processes satisfying the _ markov property _ which states that @xmath11 is independent of the remaining past given its _ causal parents _",
    "@xmath18 , a term that originates from the theory of _ graphical models _ @xcite . for multivariate time series these",
    "are called _ time series graphs _ @xcite .",
    "this proves that , theoretically , all information is already contained in the causal parents . adding a variable from @xmath13 would not increase the information , but removing a parent from @xmath18 would decrease the mmi in eq .",
    "( [ eq : mmi_decomp ] ) .",
    "the causal parents can be efficiently estimated by the algorithm described in sect .",
    "[ sec : optimal_algo ] .",
    "the markovity rests on the assumption that the noise term @xmath10 driving @xmath0 is independent of the noise terms driving the other variables .",
    "while this assumption is not strictly fulfilled in many real world systems , it often at least approximately holds .",
    "this assumption is also not as crucial for the prediction task as it is for the causal inference problem .",
    "however , for finite time series , some predictors in @xmath18 , even though causal , could only be weakly driving and lead to overfitting since they do not generalize well on new data .",
    "it is , therefore , crucial to optimize the selection of a minimal subset of causal parents .",
    "since the set of causal parents @xmath18 is much smaller than @xmath13 , this can now be done using a global optimization strategy . in sect .",
    "[ sec : selection_criteria ] we present such a strategy to select the",
    "_ optimal subset @xmath21 of causal predictors_.    .",
    "even though @xmath22 could have a high mutual information with @xmath0 , its influence is only indirect through the parents @xmath23 and @xmath11 of @xmath24 . among these",
    "the latter already lies in the unobserved future , but part of its information can still be recovered by measuring @xmath25 and @xmath26 which share information along the paths marked with black arrows .",
    "these variables form the causal predictors @xmath27 ( blue boxes ) for @xmath28 , which can be found by determining the markov set . a suitable algorithm for this task",
    "will be discussed in sect .",
    "[ sec : optimal_algo ] .",
    "all paths from nodes further in the past to @xmath24 have to pass through this set .",
    "( ii ) selection of optimal subset from causal predictors .",
    "for all numbers of predictors @xmath29 , the multivariate mutual information @xmath30 of all possible subsets is estimated .",
    "( iii ) the optimal predictors are those where the estimated mmi takes its maximum or can be obtained from cross - validation .",
    "for example , if the optimal predictors were @xmath31 , only time series samples ( blue shaded ) of these predictors are used with a nearest - neighbor ( sec .",
    "iv c ) or model - based ( sec .",
    "viii ) prediction scheme to forecast the future value t+h of the time series of y. ]    for predictions @xmath32 steps into the future , the set of causal predictors @xmath27 for @xmath1 is not identical with the parents anymore as for @xmath33 because predictors can only be chosen among the observed variables @xmath34 prior to @xmath35 .",
    "still the same algorithm as for the causal parents can be used to obtain the set of variables that separates @xmath1 from @xmath36 in the time series graph . in fig .",
    "[ fig : prediction_scheme](i ) an example of such a graph is given .",
    "as defined in @xcite , each node in that graph represents a subprocess of a multivariate discrete time process @xmath6 at a certain time @xmath37 .",
    "nodes are connected by a directed link if they are not independent conditionally on the past of the whole process , which implies a lag - specific _ granger causality _ with respect to @xmath6 @xcite . using these causal predictors ,",
    "the only uncertainty left comes from the source entropy of @xmath1 and the entropy from the unobserved ancestors of @xmath1 between @xmath38 and @xmath39 ( see fig .  [",
    "fig : prediction_scheme](i ) ) .    in the following sections we discuss and",
    "numerically compare the four prediction schemes mentioned above : ( 1 )  mi selection , ( 2 )  cmi - forward selection , ( 3 )  cmi - forward selection of only causal predictors , and ( 4 )  our optimal scheme .",
    "in the first prediction scheme , mi - selection , the respective mi of each variable in @xmath13 up to a maximum lag @xmath17 with the target variable @xmath1 is estimated .",
    "then the predictors @xmath40 are ranked by their mi : @xmath41 . to determine the best number @xmath29 of the ranked predictors that should be used",
    ", one can either apply a heuristic criterion or _ cross - validation _ @xcite . in the model experiments in sects .",
    "[ sec : model_example ] and [ sec : numerical_experiments ] we evaluate both approaches , employing the heuristic criterion that the mi of the ranked predictor @xmath42 should be at least a fraction @xmath43 of the mi of the previous predictor @xmath44 , i.e. , @xmath45 with @xmath46 .",
    "the last of the ranked predictors that satisfies this criterion determines the selected number @xmath47 of predictors .",
    "this scheme has the drawback that two or more predictors with high mi with the target variable might contain highly redundant information .",
    "then the inclusion of redundant predictors leads to overfitting which will be discussed in sect .  [",
    "sec : model_example ] .",
    "the second prediction scheme , cmi - forward selection , overcomes this drawback by excluding information already contained in the previous predictors @xcite : first the mis @xmath48 for all @xmath49 are estimated .",
    "the first predictor @xmath50 is the one that maximizes the mi with the target variable ( i.e. , the same one as in the mi - selection scheme ) .",
    "the next predictor @xmath51 , however , is chosen according to the maximal cmi @xmath52 among all remaining predictors , the third predictor is the maximum cmi conditional on the two previously selected predictors , etc . in each step",
    "@xmath29 , the cmi gives the gain to the mmi if this predictor is included because @xmath53 here the heuristic criterion @xcite is to select as the best @xmath47 the last @xmath29 with at least a gain of @xmath54 where @xmath46 as before .",
    "in @xcite also an adaptive choice of @xmath47 using a shuffle test is discussed .",
    "this scheme has been proposed to infer causal drivers in @xcite .",
    "however , it can be shown to fail for this task already in simple cases which will be shown in sect .",
    "[ sec : model_example ] .    rather than with a heuristic criterion , at the cost of additional computation time , the best number @xmath29 of predictors can also be chosen by cross - validation . here",
    "we use an @xmath55-fold cross - validation where the available observed set of time indices @xmath56 is partitioned into @xmath55 complementary segments . for each validation",
    "round , a fold @xmath55 is retained as the _ validation set _",
    "@xmath57 on which the prediction performance is evaluated .",
    "the nearest - neighbors are searched for in the complementary set @xmath58 from which also the prediction estimate is generated .",
    "then the number @xmath47 of predictors where the average prediction error across all @xmath55 folds is minimal is chosen .",
    "for the prediction of @xmath1 given the multivariate time series @xmath6 , our proposed optimal prediction scheme consists of the following steps ( fig .",
    "[ fig : prediction_scheme ] ) : ( i )  estimate the causal predictors @xmath59 using the causal algorithm described in the next sect .",
    "[ sec : optimal_algo ] , ( ii )  check all subsets ( except the empty one ) and select the @xmath29 causal predictors @xmath60 with the maximal _ estimate _ of the mmi @xmath30 with the target variable as the optimal ones ( sect .  [ sec : selection_criteria ] ) , ( iii )  use these predictors to forecast the target variable with nearest - neighbor prediction ( sect .",
    "[ sec : nnp ] ) . in the following ,",
    "we explain the causal pre - selection algorithm and discuss criteria for selecting the optimal subset . while here the actual prediction is conducted using a nearest - neighbor scheme @xcite , in sect .",
    "[ sec : optimal_plus_model ] we will also discuss how a model - based prediction based on the inferred optimal predictors can further improve a forecast .",
    "the causal pre - selection algorithm is a modification of the algorithm introduced in @xcite , which is based on the pc algorithm @xcite ( named after its inventors peter spirtes and clark glymour ) .",
    "the main idea is to iteratively unveil the causal predictors by testing for independence between pairs of nodes in the time series graph conditional on a subset of the remaining nodes .",
    "since these conditions are efficiently chosen , the dimension stays as low as possible in every iteration step .",
    "this important feature helps to alleviate the curse of dimensionality in estimating cmis @xcite affecting the computation time as well as the reliability of conditional independence tests . under some mild assumptions discussed below",
    ", the algorithm yields a consistent estimate of @xmath27 . instead of the commonly used binning estimators where the curse of dimensionality is especially severe , here we utilize an advanced nearest - neighbor estimator @xcite that is most suitable for variables taking on a continuous range of values .",
    "this estimator has as a free parameter the number of nearest neighbors @xmath61 which determines the size of hyper - cubes around each ( high - dimensional ) sample point .",
    "small values of @xmath61 lead to a lower estimation bias but higher variance and vice versa .",
    "therefore , we choose different values in the algorithm ( @xmath62 ) and the subsequent selection schemes ( @xmath63 ) .",
    "note that for an estimation from ( multivariate ) time series stationarity is required .",
    "the algorithm starts with no _ a priori _ knowledge about the drivers and iteratively learns the set of predictors of @xmath0 : first , estimate unconditional dependencies @xmath64 and initialize the preliminary predictors @xmath65 .",
    "this set contains also non - causal predictors which are now iteratively removed by testing whether the dependence between @xmath1 and each @xmath66 conditioned on the incrementally increased subset @xmath67 vanishes :    1 .",
    "iterate @xmath68 over increasing number of conditions , starting with some @xmath69 : 1 .",
    "iterate @xmath70 through all combinations of picking @xmath68 nodes from @xmath27 to define the conditions @xmath71 in this step , and estimate @xmath72 for all @xmath73 .",
    "after each step the nodes @xmath74 with @xmath75 are removed from @xmath27 and the iteration over @xmath70 stops if all possible combinations have been tested .",
    "( in the implementation we limit the number @xmath76 of combinations and check relevant combinations first , see @xcite for details . )",
    "+ if the cardinality @xmath77 , the algorithm converges , else , increase @xmath68 by one and iterate again .",
    "( in the implementation we limit the dimensionality up to some @xmath78 .",
    "if the initial number of conditions is @xmath79 to speed up the algorithm , also previously skipped combinations with @xmath80 need to be checked before convergence can be assessed . )",
    "the main assumptions underlying the identification of the conditional independence structure with the pc algorithm are the _ causal markov condition _ , i.e. , markovity of the process of any finite order , and _ faithfulness _ , which guarantees that the graph entails all conditional independence relations true for the underlying process and can be violated only in certain rather pathological cases @xcite . if these assumptions are fulfilled , the causal algorithm is universally consistent , implying that the algorithm will converge to the true causal predictors with probability 1 for infinite sample size @xcite . on the other hand ,",
    "the cmi forward selection algorithm proposed for causal inference in @xcite is not consistent since it yields non - causal drivers already in simple cases which will be analyzed in sect .",
    "[ sec : model_example ] .",
    "but the cmi forward selection scheme can be ` repaired ' by a further backward elimination step @xcite .",
    "practically , the causal algorithm involves conditional independence tests for @xmath75 . in @xcite a shuffle test is proposed for testing whether @xmath81 : an ensemble of @xmath82 values of @xmath83 is generated where @xmath84 is a shuffled sample of @xmath74 , i.e. , with the indices permuted . then the cmi values are sorted and for a test at a given @xmath85-level , the @xmath86-th value is taken as a significance threshold . in @xcite a numerical study on the detection and false positive rates of the algorithm are given .",
    "the shuffle test comes at the additional cost , that for each conditional independence test @xmath82 surrogates of cmi have to be estimated .",
    "an alternative is to apply a fixed threshold @xmath87 , which has the drawback that it does not adapt to the negative bias for higher - dimensional cmis @xcite .",
    "the algorithm yields different numbers of predictors for different chosen fixed thresholds or significance levels and the value should be low enough to include weak but possibly synergetic causal predictors , but high enough to limit computational complexity in the optimization step ( sect .",
    "[ sec : computational_complexity ] ) .      ) , for details see sect .",
    "[ sec : model_example ] . for each number of predictors @xmath29 ,",
    "the number of possible combinations varies according to the binomial coefficient @xmath88 .",
    "the predictor combinations are sorted by their prediction error .",
    "the maximum of mmi and also smaller values match the minimum prediction error very well . note that mmi is estimated on the learning set while the prediction error is evaluated out - of - sample on the test set indicating that the bias of higher - dimensional mmis here serves as a good proxy for overfitting as discussed in the text . ]",
    "once the causal predictors are determined , the optimal subset needs to be chosen ( possibly containing all causal predictors ) .",
    "here we also discuss a scheme using forward selection on the causal drivers ( causal cmi - scheme ) .    for the third prediction scheme , causal cmi - selection , the forward selection",
    "ranking discussed in sect .",
    "[ sec : common_prediction_scheme ] is applied not to the entire set @xmath89 , but only to the pre - selected causal predictors @xmath27 . also here",
    ", the same heuristic criterion as for the non - causal forward selection or cross - validation can be used .    for the optimal scheme ( fig .  [",
    "fig : prediction_scheme](ii ) ) , the mmi @xmath90 for all subsets of the causal predictors from @xmath27 is estimated . in fig .",
    "[ fig : prediction_optimization_step ] the mmi values ( ensemble average ) are plotted for the model example discussed later in sect .",
    "[ sec : model_example ] with @xmath91 causal predictors .",
    "even though all seven predictors are causal drivers , the _ estimated mmi _ is highest for just three of them and even decreases for more predictors ( according to eq .",
    "( [ eq : mmi_decomp ] ) , the theoretical mmi should be maximal for seven predictors ) .",
    "the reason is that the estimated mmi is negatively biased for higher dimensions @xcite and if the additional information contained in the predictors does not outweigh this bias , the mmi decreases .",
    "the bias of the mmi estimator , therefore , implies a penalty that avoids overfitting . in our heuristic criterion",
    "we exploit this property and simply select the subset @xmath92 with maximal estimated @xmath90 .",
    "this model - free data - based criterion could be seen as an analogue to model - based criteria like the akaike information criterion ( aic ) @xcite where the penalty is derived from some measure of model complexity , but our criterion needs to be further investigated .",
    "we choose as the nearest - neighbor parameter in the mmi estimator @xmath93 , where @xmath61 is the nearest - neighbor parameter in the prediction ( see next sect .",
    "[ sec : nnp ] ) .    in our numerical experiments in sect .",
    "[ sec : numerical_experiments ] , we additionally test a combination of the mmi - criterion with cross - validation : for each number of predictors @xmath29 we check the mmi - criterion to select the optimal combination and then use cross - validation to pick the optimal @xmath29 .",
    "this approach gives always a slightly better prediction performance than using the maximum criterion alone  at the cost of much longer computation time .",
    "asymptotically , for model - based predictions the aic criterion is equivalent to cross - validation and in our numerical experiments ( see appendix fig .  [",
    "fig : robustness_length ] ) we also find that our heuristic mmi - criterion well matches the cross - validation choice for longer time series . note that our use of cross - validation treats @xmath29 as a tuning parameter as is typically done @xcite .",
    "one could also treat the choice of a subset @xmath94 as a tuning parameter and run steps ( i)(iii ) of the prediction scheme for every fold .",
    "@xmath94 is , however , not a numeric ` tuning parameter ' and different folds in the cross - validation might not contain the same subsets .",
    "as a result the variance across the folds is considerable and it is hard to find the subset with minimal cross - validation error .      once the optimal predictors are selected , the actual prediction is conducted here using a scheme with a fixed number of nearest neighbors @xmath61 @xcite : for the optimal set of predictors @xmath95 , we first determine the distances @xmath96 where @xmath97 denotes some norm . here",
    "we apply the maximum norm as in the nearest - neighbor estimator of the ( conditional ) mutual information @xcite .",
    "@xmath17 is the maximum lag used to estimate @xmath95 .",
    "next , we sort the distances in increasing order @xmath98 yielding an index sequence @xmath99 .",
    "now there are two approaches to use these distances : ( i )  a fixed distance @xmath100 is chosen and all points @xmath101 with distance smaller than @xmath100 are taken into account to predict @xmath1 . then the coarse - graining level is consistent for all sample points , but sometimes there might not be any point within a distance @xmath100 @xcite .",
    "( ii )  here we use a fixed number of nearest neighbors which has the advantage that the same number of points contributes to a prediction making the estimate more reliable . for a chosen fixed number of nearest neighbors @xmath61 the future value @xmath1",
    "is then estimated by the conditional expectation and its prediction interval by its standard deviation : @xmath102 another option , instead of the expectation , is to fit an autoregressive model giving a _ local - linear prediction _ @xcite .",
    "the summation can also be weighted with a function of the distance of the neighbors , different norms , or kernel - based methods can be chosen @xcite .",
    "for the number of nearest neighbors @xmath61 , we use @xmath103 where @xmath63 is the nearest - neighbor parameter in the estimation of cmi or mmi in the selection schemes .",
    "this choice approximately yields a consistent level of coarse - graining in the information - theoretic selection step and the nearest - neighbor prediction .",
    "alternatively , at the cost of computation power , one can also utilize cross - validation for this choice @xcite .",
    "the number of nearest neighbors needs to be balanced to guarantee that only nearby values are used as predictors , but still enough values are available to confidently estimate @xmath1 and possibly the prediction interval .",
    "the value will typically strongly depend on the data . as a skill",
    "metric we compute the standardized root mean squared error @xmath104 where @xmath105 is the variance of @xmath0 in the testing set @xmath106 .",
    "for @xmath107 .",
    "solid lines will be used for @xmath108 , whereas dashed lines denote @xmath109 .",
    "the curve for the causal cmi - selection scheme ( black ) is only slightly different from the constant curve for the mi - selection scheme ( blue ) .",
    "the causal schemes have the advantage that their complexity rather depends on how many causal drivers are found , while the complexity of the mi- and cmi - selection schemes depends on the number of processes and the maximum lag . for the non - causal cmi - selection scheme ( grey lines )",
    "we fixed the maximum number of predictors to @xmath110 . to include the complexity due to the preselection step in the causal schemes we added @xmath111 ( @xmath112 ) for @xmath113 ( @xmath114 ) for these schemes according to eq .",
    "( [ eq : algo_complexity ] ) for @xmath115 and @xmath116 as in our numerical experiments ( see sect .  [",
    "sec : numerical_experiments ] ) .",
    "the plot shows that if the number of causal predictors can be reduced below @xmath117 ( @xmath118 ) here , the optimal scheme even takes less computation time than the non - causal cmi - selection scheme . ]",
    "an important criterion for the practical applicability of the different prediction schemes discussed in sects .",
    "[ sec : common_prediction_scheme ] and [ sec : optimal_prediction_scheme ] is their respective computational complexity .",
    "the estimator for the ( c)mi @xmath119 employed here ( nearest - neighbor technique with maximum norm @xcite ) has a computational complexity of @xmath120 @xcite , where @xmath121 is the time series length and @xmath122 the dimensionality of the respective variable .",
    "fast neighbor searching algorithms can further reduce the dependency on @xmath121 , but here we are interested in the relative complexity of the different predictor selection schemes and , therefore , only consider the linear scaling with the number of dimensions . in this case the first prediction scheme , mi - selection , clearly is the cheapest option . for a @xmath5-dimensional process @xmath6 , this procedure involves just @xmath123 estimates of mis with a dimensionality of @xmath124 .",
    "the second scheme , cmi - forward selection , is more demanding the more possible predictors are included . for cross - validation",
    ", a maximum number of @xmath125 predictors has to be selected , increasing the dimension to maximally @xmath126 due to more conditions in eq .",
    "( [ eq : cmi_decomp ] ) .",
    "the cmi - forward selection then involves @xmath127 estimates of cmi with iteratively increasing dimensionality .",
    "then the complexity of the cmi - selection scheme scales as @xmath128 for @xmath129 , i.e. , with a high linear dependency on @xmath123 and a polynomial dependency @xmath130 .",
    "note that for a @xmath55-fold cross - validation step using nearest - neighbor prediction an additional computational complexity of @xmath131 has to be added .    in fig .",
    "[ fig : computational_complexity ] we compare the complexity of the different prediction schemes for @xmath108 and @xmath107 as in our numerical experiments in sect .",
    "[ sec : numerical_experiments ] .",
    "while one can fix @xmath125 to a small number for which nearest - neighbor predictions yield acceptable results , the main problem here is that the computation time quickly increases with @xmath123 ( linear , but with a large pre - factor ) .",
    "one advantage of a causal pre - selection step is to reduce this number before the computationally expensive selection procedure is invoked .    since typically the set of causal predictors is small , i.e. , @xmath132",
    ", the third prediction scheme , causal cmi - selection , has a drastically smaller computational complexity than the non - causal cmi - selection scheme if the additional complexity due to the causal algorithm is not that large . if we rank all causal predictors ( not limiting to some @xmath125 as in the non - causal scheme ) , the complexity scales as @xmath133 where @xmath134 denotes the number of causal predictors . in fig .",
    "[ fig : computational_complexity ] , the complexity ( black lines ) shows only a very moderate increase with @xmath134 .    for the optimal scheme the computational complexity grows exponentially as @xmath135 .",
    "however , fig .",
    "[ fig : computational_complexity ] shows that if the number of causal predictors can be restricted , the optimal scheme even takes less computation time than the non - causal cmi - selection scheme .",
    "the number of causal predictors can be reduced by adjusting the significance level @xmath85 or fixed threshold @xmath87 in the conditional independence tests of the causal pre - selection algorithm .",
    "most important , the causal scheme s complexity only scales with the number of causal drivers and _ not _ directly with the number of processes @xmath5 or the maximal lag @xmath17 as the non - causal schemes ( dashed lines in fig .",
    "[ fig : computational_complexity ] ) .",
    "the dependence of the causal schemes on @xmath5 and the maximal lag @xmath17 is only via the algorithm .",
    "the additional time complexity of the causal algorithm varies with the graph structure .",
    "the number of iterations can be limited by starting with a higher number of initial conditions @xmath136 and limiting the maximum dimensionality @xmath78 .",
    "this number determines up to which dimension of @xmath71 the conditional independence is checked .",
    "also the number of combinations @xmath76 in the @xmath70-loop can be restricted . in a worst case scenario where",
    "the spurious links only vanish if the maximum number of conditions is used , the computational complexity scales as @xmath137 however , for sparse graphs and the conditional set being efficiently chosen @xcite , typically links get removed already for an @xmath136-dimensional condition with a complexity of @xmath138 and @xmath139 or @xmath140 .",
    "this is also confirmed in numerical experiments in @xcite and sect .",
    "[ sec : numerical_experiments ] .",
    "often the complexity is even lower because the mi value in the first iteration is already non - significant .",
    "realizations of model  ( [ eq : model ] ) for @xmath141 with time series graph given in the inset ( learning set length @xmath142 , test set length @xmath143 , nearest - neighbor prediction with @xmath144 neighbors , ( c)mis estimated from the learning set with parameter @xmath145 in the algorithm and @xmath146 for the optimization ) .",
    "the box and whiskers plots give the ensemble median and the interquartile range of the standardized root mean squared prediction errors in the test set for each iteration step @xmath29 in the four schemes ( from left to right : mi , cmi , causal - cmi , optimal scheme ) .",
    "the black line gives the median of the true minimal prediction error obtained by minimizing the out - of - sample prediction error for each number of predictors @xmath29 taken from the true causal drivers .",
    "for @xmath147 , only the optimal scheme ( red ) selects the best ( synergetic ) predictors @xmath148 and reaches the minimum possible error while the causal forward selection ( black ) first picks one of the less predictive @xmath149 and the pure forward selection ( grey ) and mi - based schemes first pick the two non - causal drivers @xmath150 .",
    "the non - optimal schemes include the synergetic predictors only when the higher dimensionality is already worsening the prediction due to overfitting . ]",
    "the following nonlinear discrete - time stochastic delay equation provides an illustrative example where a simple mi- or cmi - forward selection procedure yields non - causal variables that deteriorate a prediction .",
    "consider @xmath151 where the causal drivers @xmath152 , @xmath153 , and @xmath154 are independent gaussian processes with zero mean and unit variance [ @xmath155 .",
    "this model illustrates why the mi and cmi prediction schemes fail to yield good predictions due to ( 1 )  selecting non - causal predictors and ( 2 )  missing out synergetic predictors . here",
    "the @xmath156 are synergetic causal drivers , which  for certain parameters @xmath157  are _ individually _ less predictive than the drivers @xmath158 . but selected all _ together _ , their combined information @xmath159 is much larger than the single mutual informations .",
    "this synergetic effect can only be detected if all subsets of causal drivers are tested in a globally optimal scheme . in the following we",
    "analyze why the mi- and cmi - selection schemes fail to provide good predictions for such cases .",
    "regarding the problem of selecting non - causal drivers , for certain parameter combinations of @xmath160 the mutual information @xmath161 is larger than any @xmath162 or @xmath163 for all @xmath70 .",
    "the non - causal schemes based on iteratively selecting predictors with maximal mi or cmi ( blue and grey box plots in fig .",
    "[ fig : model_example ] ) will , therefore , choose a non - causal @xmath164 prior to the true causal predictors @xmath158 and @xmath156 .",
    "since the predictors @xmath158 have the largest mi after the @xmath164 , these are included next in the mi - selection scheme . in the cmi - forward selection scheme , on the other hand , the synergetic variables @xmath156 are selected after the second iteration step .",
    "this leads to a drastic decrease in the prediction error at @xmath165 ( grey box plot in fig .",
    "[ fig : model_example ] ) .",
    "the problem is that now the dimension of the predictors is already @xmath166 and the two spuriously causal variables @xmath164 deteriorate the prediction .",
    "the causal pre - selection avoids this pitfall .",
    "the black and red box plots in fig .",
    "[ fig : model_example ] denote the schemes based on causal predictors using forward selection ( black ) and the optimal scheme ( red ) . also for causal drivers",
    "the forward selection scheme is not optimal because the selection of one predictor at a time fails for synergetic cases and selects the weak drivers @xmath152 prior to the synergetic drivers @xmath153 .",
    "only the optimal scheme correctly identifies these drivers for the dimension @xmath147 and reaches the minimal prediction error possible for this model ( black line ) for each @xmath29 . in fig .",
    "[ fig : prediction_optimization_step ] we show that the three synergetic drivers @xmath153 are chosen with our heuristic optimal criterion since they have the largest mmi with the target variable .",
    "trials of the synergetic model class  ( [ eq : model_numerical_experiments ] ) with time series length @xmath167 ( @xmath143 in the test set ) for the four different prediction schemes ( from left to right in the panels : mi in blue , cmi in gray , causal - cmi in black , optimal in red ) .",
    "( a ) the four box plots show the ensemble interquartile range of ( i )  computational complexity ( the green box on the right shows the added complexity due to the causal algorithm ) , ( ii )  the range of numbers of predictors @xmath47 selected by cross - validation ( cv , here the green bar shows the number of causal predictors @xmath134 in the pre - selection step ) , ( iii )  the true positive rate ( tpr ) and ( iv )  false discovery rate ( fdr ) .",
    "the latter are evaluated for each scheme at @xmath168 , corresponding to the true number of causal drivers ( or less if fewer causal drivers are estimated in the pre - selection step ) .",
    "( b ) box plots showing the median and interquartile range of the prediction error _ relative _ to the true minimal error obtained by minimizing the out - of - sample prediction error over all subsets of true causal drivers . on the left are the results if cross - validation is used to optimize @xmath29 for each scheme ( whiskers show the 5% and 95% quantiles ) .",
    "the red box in the center shows the result for the heuristic optimality criterion .",
    "the range of boxes on the right shows the results for different thresholds @xmath43 for the other schemes ( only interquartile range ) .",
    "( c ) box and whiskers plots ( showing the 5% and 95% quantiles ) for the absolute prediction error of the optimal scheme at the cross - validated choice of @xmath29 for different numbers of nearest neighbors @xmath61 . ]    , but with @xmath169 processes and @xmath170 . the orange box plot in ( b )",
    "shows the relative prediction error if the optimal predictors from the model - free selection scheme are used in conjunction with the linear auto - regressive prediction model  ( [ eq : linear_prediction ] ) ( only the interquartile range shown ) . ]",
    "we next compare the four schemes including the causal pre - selection algorithm on a larger class of synergetic nonlinear discrete - time stochastic processes with different coupling configurations : @xmath171 where we are interested in predicting @xmath172 ( i.e. , @xmath33 ) .",
    "the linear function @xmath173 is simply the sum of @xmath166 randomly chosen subprocesses @xmath174 ( excluding process @xmath175 ) at random lags @xmath176 .",
    "the nonlinear function @xmath177 , on the other hand , is the product of @xmath178 randomly chosen subprocesses ( excluding process @xmath175 and the ones already included in the linear term ) .",
    "the other subprocesses for @xmath179 are linearly driven by @xmath140 other randomly chosen subprocesses , also at random lags .",
    "the coefficients are fixed to @xmath180 , and @xmath108 . with this setup",
    "we generate an ensemble of @xmath142 realizations .",
    "we run the four schemes at different choices of the heuristic parameter @xmath43 and using cross - validation checking up to @xmath181 predictors in the non - causal schemes . for the causal schemes , we use cross - validation for all",
    "estimated causal predictors ( up to maximally @xmath181 , ranked by their cmi value in the algorithm @xcite ) .",
    "the causal drivers are estimated using the algorithm parameters @xmath182 , and @xmath107 with a fixed significance threshold @xmath183 ( analyses for other thresholds are shown in the appendix ) .    in fig .",
    "[ fig : numerical_experiments ] , we show various statistics comparing the computational complexities and the prediction errors . here",
    "the number of possible predictors is @xmath184 yielding a computational complexity of @xmath185 for the mi - selection scheme ( blue ) and @xmath186 for the cmi - selection scheme ( grey ) using @xmath181 .",
    "the causal algorithm reduces the number of possible predictors to about @xmath187 ( median ) .",
    "this corresponds to a true positive rate ( tpr ) of roughly @xmath188 ( there are @xmath117 true drivers in model  ( [ eq : model_numerical_experiments ] ) , but several are only weakly driving ) and a zero false discovery rate ( fdr ) , while the mi- and cmi - selection schemes detect fewer causal drivers and much more false positives .",
    "fewer predictors result in a lower computational complexity for the causal prediction schemes .",
    "the causal cmi - selection scheme runs extremely fast ( black ) and the complexity of the optimal scheme ( red ) strongly varies among the different realizations , since it depends exponentially on how many causal predictors are pre - selected ( fig .",
    "[ fig : computational_complexity ] ) , but still typically even stays below the non - causal cmi - selection scheme . using cross - validation , the mi - selection scheme uses typically ( median ) @xmath165 , the cmi - selection schemes both @xmath189 , and the optimal scheme only @xmath147 out of the @xmath117 true causal drivers for this model .    finally , the relative prediction errors show that only the optimal scheme reaches the lowest possible errors with a median of zero relative error and even 90% of the ensemble below an error of @xmath190 .",
    "this demonstrates the large improvements due to the global optimization scheme that is only possible after reducing the set of variables to the few causal predictors .",
    "the aforementioned results have been obtained using cross - validation to select the optimal @xmath29 .",
    "the computationally cheaper alternative using a heuristic criterion here yields drastic differences in the prediction performance depending on the choice of @xmath43 . while here values in the range @xmath191 give good results , in another experiment ( fig .",
    "[ fig : prl_model ] ) we found good predictions only for @xmath192 making it hard to provide rules of thumb in practical applications . in the appendix we show that also the length of the time series results in different optimal ranges for @xmath43 . on the other hand , for the optimal scheme the heuristic choice leads to almost the same minimal errors as in cross - validation .",
    "to test the robustness of our results , we also compare the prediction schemes on a class of non - synergetic , but still nonlinearly coupled models ( generalized additive models @xcite with @xmath169 processes and polynomials of linear and quadratic degree ) as analyzed in the supplement of ref .",
    "@xcite . for each ensemble member",
    ", we choose as a target variable the one with the largest sum of ` incoming ' coefficients ( absolute values ) .",
    "the results shown in fig .  [ fig : prl_model ] demonstrate that for this case also the causal cmi - selection scheme reaches optimal prediction errors . in the appendix",
    ", we show that the optimality is robust also for different significance thresholds and other time series lengths .    in figs .",
    "[ fig : numerical_experiments](c ) and fig .",
    "[ fig : prl_model](c ) we evaluate the prediction for different phase space resolutions . to this end",
    "we use the causal predictors and run steps ( ii ) and ( iii ) of the prediction scheme from fig .",
    "[ fig : prediction_scheme ] ( using cross - validation to choose @xmath47 ) for varying nearest - neighbor parameters @xmath61 and mmi estimation parameter @xmath93 , both set to the same value for consistency . while in the first ensemble ( fig .",
    "[ fig : numerical_experiments](c ) ) the error is minimal for very few neighbors and sharply rises if too many neighbors are used , in the second ensemble ( fig .",
    "[ fig : prl_model](c ) ) too few neighbors yield worse results . in practice",
    ", the choice will very much depend on the process under study , but here we use a value in the range @xmath193 which constitutes a balance between local information and enough neighbors to reliably estimate @xmath194 .",
    "up to now we have stayed in a model - free framework with information - theoretic optimal selection of predictors and a nearest - neighbor prediction . while nearest - neighbor prediction is a flexible method that will adapt to any function @xmath14 in eq .",
    "( [ eq : evolution ] ) , it will in many cases be outperformed by a model - based prediction  if the right model class is chosen .",
    "if a misspecified model is chosen for variable selection and fitting , it might miss out nonlinear combinations of predictors .",
    "for example , in our synergetic model  ( [ eq : model ] ) a linear selection method would only include the weakly predictive variables @xmath152 and largely miss out the highly predictive variables @xmath153 .",
    "the functional dependency on the @xmath152 is , on the other hand , much better fitted with a linear model than with nearest neighbors .    to take advantage of improved model - based predictions and at the same time not miss out synergetic predictor combinations , we propose to apply our optimal predictor selection scheme and conduct the final prediction step by fitting a model on the optimal set of predictors .",
    "here we demonstrate this approach on the non - synergetic model ensemble from ref .",
    "@xcite . to predict @xmath1 from the optimal subset of predictors @xmath195 ( chosen by cross - validation or the heuristic criterion )",
    ", we use the ordinary least squares regression technique",
    ". then the prediction interval is given by the variance @xmath196 of the regression residual plus the errors in the estimated regression coefficients @xmath197 @xcite : @xmath198 in fig .",
    "[ fig : prl_model](b ) the results for this approach are shown as the orange box plot . the prediction improvement varies strongly for the different realizations which include nonlinear and linear drivers .",
    "about half of the realizations are better fitted using the optimized linear approach with prediction improvements of up to 10% compared to the nearest - neighbors prediction .",
    "more advanced techniques such as generalized additive models can further improve a prediction @xcite .",
    "in addition to facilitating the prediction task , the knowledge of the functional forms of dependencies can also help to better understand coupling mechanisms .",
    "testing up to @xmath199 months .",
    "( a )  prediction error using nearest - neighbor prediction ( solid red line ) and linear prediction ( dotted orange line ) versus prediction step @xmath2 . for both approaches the same optimal predictors obtained from the model - free scheme with cross - validated ( 5-fold within the learning set ) choice of predictors are used .",
    "( b )  nino3.4 index with el nio and la nia events marked in red and blue , respectively .",
    "the black lines denote selected hindcasts and their @xmath200-prediction intervals ( grey ) using the linear prediction .",
    "the dots mark the starting time @xmath37 in may of each year , and the predicted values range from june ( @xmath33 ) to october ( @xmath201 ) .",
    "the arrows mark correct ( red ) , missed ( grey ) and false ( black ) hindcasts of the nino3.4 index exceeding with more than 49% probability .",
    "the green line marks a real forecast starting in december 2014 giving a probability for the nino3.4 index to stay above of 55 - 70% for the months until may 2015 . ]",
    "the combined framework developed in the last section is now illustrated on a sea - surface temperature index of the el nio southern oscillation ( enso ) in the tropical pacific which has been the focus of prediction research for many decades due to its far - reaching climatic and economic impacts @xcite .",
    "the nino3.4 index is defined as the average sea - surface temperature over the region n - s , -w @xcite . as another possible predictor variable ,",
    "we use an atmospheric index based on sea - level pressure , the southern oscillation index ( soi ) , which is computed from the surface air pressure difference between tahiti and darwin , australia .    in fig .",
    "[ fig : enso_prediction](a ) , we employ the model - free causal algorithm and predictor selection ( steps ( i)-(ii ) in fig .",
    "[ fig : prediction_scheme ] , here optimized using cross - validation ) to obtain the optimal predictors and compare the skill of the nearest - neighbor and the linear prediction using the auto - regressive model  ( [ eq : linear_prediction ] ) fitted on the optimal predictors . trained on the period 1951 - 2002 , we test the prediction on the last decade 2003 - 2014 . from the 24 possible predictors ,",
    "the optimal predictor for @xmath33 month is only nino3.4 at one month lag , while for @xmath28 months the three predictors @xmath202 are relevant indicating that the atmospheric coupling , including a long memory , constitutes an important predictive mechanism . here",
    ", the linear auto - regressive model significantly reduces the prediction error by about @xmath203 compared to the nearest - neighbor approach using the same predictors , at least for a few months ahead . for steps larger than 5 months",
    ", the error in both approaches quickly reaches 1 which implies that the prediction is merely a persistence forecast .",
    "the better linear prediction is a sign that exploiting the nonlinearities in nino3.4 @xcite does not improve the prediction much while the linear fit using the optimal predictors better harnesses the linear drivers of enso  at least on these time scales @xcite .    to give an impression of selected predictions from the linear model ( actually hindcasts )",
    ", we show in fig .",
    "[ fig : enso_prediction](b ) the predictions up to 5 months ahead starting from may in each year .",
    "the important onsets of el nio events are determined by expert assessment , but one definition is the 3-month - running - mean smoothed nino3.4 index exceeding , here marked by a red line ( la nias , where the index decreases below are marked in blue ) . with our hindcasts starting in may of each year , one can compute the probability of an el nio event as the part of the prediction distribution exceeding ( assuming a gaussian distribution with mean and standard deviation given by eq .",
    "( [ eq : linear_prediction ] ) ) .",
    "if this probability is larger than 49% for any of the 5 months ahead ( until october ) , we predict an el nio event . with this scheme",
    "we would have correctly predicted the moderate el nio event in 2009 and the onset of the weak el nino in current 2014 - 2015 season ( red arrows ) , but missed the weak events of 2004 and 2006 ( grey arrows , the latter being almost predicted with a probability of 48% ) . on the other hand , in 2011 ( black arrow )",
    "a false alarm is given .",
    "the overall weak predictability of the recent el nio events is also found in other studies using statistical as well as physical model predictions @xcite and suggests that the mechanism of enso could be changing .",
    "finally , our real forecast ( green line ) starting from december 2014 suggests that the weak current el nio condition persists with a probability of 55 - 70% for the months until may 2015 .",
    "in this article we have shown that the combinatorial explosion to search for globally optimal subsets of predictors can be overcome by restricting the search to causal drivers .",
    "globally optimal predictors detect also synergetic mechanisms where the combination of multiple predictors strongly improves a prediction .",
    "analytical considerations and numerical experiments indicate that such an approach is superior to schemes using mi - ranking or forward selection with conditional mutual information .",
    "another advantage is that the computational complexity only scales with the number of causal predictors and not directly with the number of processes included in the analysis .",
    "if the set of causal predictors is not that large , the optimal scheme is even computationally less expensive than the non - causal cmi - selection scheme . to determine the optimal size of this set",
    ", we have found that a parameter - free heuristic criterion performs almost as good as a computationally much more demanding cross - validation .",
    "note that , even though theoretically only causal drivers can yield optimal predictions , non - causal variables could still be better predictors .",
    "consider the case that a very high - dimensional process @xmath204 drives @xmath0 and @xmath205 .",
    "then the prediction of @xmath0 from the causal drivers @xmath204 is deteriorated due to the curse of dimensionality for finite samples , while the non - causal process @xmath205 could potentially better aggregate this information .",
    "the same effect also explains why in fig .",
    "[ fig : model_example ] the cmi prediction using the non - causal @xmath164 together with the synergetic drivers has a slightly smaller prediction error than the causal cmi - selection scheme for @xmath165 ( grey box plot ) .",
    "while we propose the model - free selection of predictors for processes where the underlying mechanisms are poorly understood , the actual prediction can be much improved using suitable model - based techniques compared to a pure model - free nearest - neighbor prediction .",
    "this approach combines the advantage of a model - free approach to detect relevant variables with the smaller prediction variance of model - based methods and can also be used to better understand coupling mechanisms .",
    "the application of this combined approach significantly improves the prediction of an enso index compared to a nearest - neighbor scheme .",
    "the combined approach can be further improved by optimizing the number of predictors @xmath47 with a different criterion than the model - free criteria discussed in sect .",
    "[ sec : selection_criteria ] . especially linear models can harness much more predictors before the problem of overfitting becomes severe .    here",
    "the scope of application was the prediction of future values of a time series . in a forthcoming paper",
    "we will investigate how the scheme can be adapted if , for example , only forecasts for the emergence of extreme events like el nios @xcite are needed .",
    "a _ python _ script to estimate the causal predictors can be obtained from the author s website at ` www.pik-potsdam.de/members/jakrunge ` .",
    "we acknowledge the financial support by the german national academic foundation ( studienstiftung des deutschen volkes ) , the humboldt graduate school , and the german federal ministry of education and research ( young investigators group cosy - cc@xmath206 , grant no . 01ln1306a ) .",
    "in fig .  [ fig : robustness_sig ] we show the results as in fig .",
    "[ fig : prl_model ] for different values of the significance threshold @xmath87 .",
    "obviously this threshold affects the true positive and false discovery rate , which are , however , not directly of interest for the prediction task ( as opposed to the causal inference problem ) . but",
    "a too low significance level in the causal pre - selection algorithm leads to a high computational complexity and also increases the variance in the optimal subset selection step , which results in higher prediction errors . if , on the other hand , the significance level is too high , too few predictors are available to optimize the prediction such that the resulting optimal predictors equal the pre - selected causal predictors @xmath27 .",
    "if the significance level is adjusted to yield just a few predictors more than the number of optimal predictors @xmath47 ( obtained through cross - validation or the optimal heuristic criterion ) , the prediction error is minimal and also the computational complexity is lower than for the non - causal cmi - forward selection scheme .",
    "we also evaluate the prediction schemes for time series lengths @xmath207 and @xmath208 .",
    "the results shown in fig .  [ fig : robustness_length ] demonstrate that the optimal scheme also works for very short time series and is even better for longer time series .",
    "for @xmath208 and the synergetic model  ( [ eq : model_numerical_experiments ] ) the optimal scheme even results in 75% of the realizations reaching the true minimal prediction error .",
    "f.  takens . .",
    "in d.  a. rand and l .- s .",
    "young , editors , _ dynamical systems and turbulence , warwick 1980 : proceedings of a symposium held at the university of warwick 1979 - 80 _ , volume 898 of _ lecture notes in mathematics _ , pages 366381 .",
    "springer , new york , 1981 ."
  ],
  "abstract_text": [
    "<S> forecasting a time series from multivariate predictors constitutes a challenging problem , especially using model - free approaches . </S>",
    "<S> most techniques , such as nearest - neighbor prediction , quickly suffer from the curse of dimensionality and overfitting for more than a few predictors which has limited their application mostly to the univariate case . </S>",
    "<S> therefore , selection strategies are needed that harness the available information as efficiently as possible . since often the right combination of predictors matters , ideally </S>",
    "<S> all subsets of possible predictors should be tested for their predictive power , but the exponentially growing number of combinations makes such an approach computationally prohibitive . here a prediction scheme that overcomes this strong limitation is introduced utilizing a causal pre - selection step which drastically reduces the number of possible predictors to the most predictive set of causal drivers making a globally optimal search scheme tractable . </S>",
    "<S> the information - theoretic optimality is derived and practical selection criteria are discussed . as demonstrated for multivariate nonlinear stochastic delay processes , the optimal scheme can even be less computationally expensive than commonly used sub - optimal schemes like forward selection . </S>",
    "<S> the method suggests a general framework to apply the optimal model - free approach to select variables and subsequently fit a model to further improve a prediction or learn statistical dependencies . </S>",
    "<S> the performance of this framework is illustrated on a climatological index of el nio southern oscillation . </S>"
  ]
}