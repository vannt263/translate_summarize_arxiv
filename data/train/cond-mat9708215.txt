{
  "article_text": [
    "in the past decade attractor neural networks were the subject of an intense study as a model of associative memory .",
    "the `` ancestor '' of these models , the hopfield model @xcite , was defined as follows : there is a set of @xmath2 neurons , each one associated with a binary variable @xmath3 , representing its activity .",
    "the synaptic couplings between these model neurons , @xmath4 , are chosen at random at the beginning and kept fixed , and then the system evolves deterministically according to the equation _",
    "i(t+1)=sign(_j j_ij_j(t ) ) [ evol](parallel updating ; alternatively , one can consider sequential updating when @xmath5 is determined by the state of @xmath6 for @xmath7 and by @xmath8 for @xmath9 ) .",
    "this procedure defines a disordered dynamical system : the evolution is deterministic , but its rules are chosen at random at the beginning and kept fixed . in other words , we can rewrite the dynamic law in the form    ( t+1)=f_j(c(t ) ) , [ dis]where @xmath10 represents a configuration of the system , _",
    "i.e. _ a set of values of the @xmath2 variables @xmath11 , @xmath12 is a random realization of a deterministic map and the set of indices @xmath13 labels the realization of the dynamic rules .",
    "the most natural distance in configuration space is the normalized hamming distance , defined as d(c , c)=1n_i _",
    "i-_i.    we are interested in the statistical properties of the motion asymptotically in time and system size . as the motion is deterministic and configuration space is finite , asymptotically in time the dynamics takes place on periodic orbits , and the quantities of interest are the lengths and the number of such orbits as well as the size of their attraction basins .",
    "such quantities are random variables , depending on the realization of the dynamical rules , and we will study their probability distribution .",
    "when the couplings are symmetric ( @xmath14 ) it is possible to define a hamiltonian so that equation ( [ evol ] ) represents the zero temperature dynamics of a thermodynamic system . in particular ,",
    "if the @xmath4 are chosen from a distribution with zero mean and variance @xmath15 ( for instance a gaussian distribution ) we are dealing with the zero temperature dynamics of the sk model ( in the case of sequential updating : the parallel updating does not imply a relaxational dynamics ) . in the hopfield model the couplings are symmetric , too , but they are chosen according to the hebbian rule :    j_ij=_=1^p _ i^_j^ , where the @xmath16 vectors of @xmath2 binary variables , @xmath17 , represent the memorized patterns .",
    "the system is able to memorize , in the sense that the patterns are fixed points of the dynamics and they are stable against random perturbations if their number does not exceed the capacity of the network , _",
    "i.e. _ if @xmath16 is not larger than @xmath18 , with @xmath19 .",
    "so , given a number of microscopic states growing as @xmath20 , the hopfield model is able to memorize a number of patterns growing linearly with @xmath2 .",
    "asymmetric neural networks received a large attention in the literature in the late 80s @xcite . in 1986",
    "it was proposed to generalize the hopfield model by taking into account also asymmetric couplings @xcite.this generalization appears more realistic , since synapses in nature are in general not symmetric , and it suggests a possible way to distinguish between a network that has remembered a learned pattern and a network which is in a confused state ( such a distinction is not possible in the hopfield model ) .",
    "in fact , in asymmetric neural networks , two kind of attractors are present :  ordered \" attractors , that are either short cycles or fixed points , and  chaotic \" attractors , whose length grows exponentially with system size .",
    "the first numerical observations of this twofold nature of the attractors are due to gutfreund , reger and young @xcite and to ntzel @xcite .    in this note",
    "we are mainly interested in the study of the properties of the attractors , such as the probability distributions of their lengths , of their number and of the size of their attraction basins .",
    "we will consider only the case of fully asymmetric couplings , _",
    "i.e. _ @xmath4 and @xmath21 are independent random variables . in this case analytical results",
    "have already been obtained about the correlation functions @xcite and about the number of attractors @xcite , but more about the attractors can be said using a simple stochastic scheme based on the annealed approximation .",
    "this approximation was introduced in the study of disordered dynamical systems by derrida and pomeau @xcite to study damage spreading in kauffman networks ( a disordered dynamical system proposed as a model of the genetic regulation in cells @xcite ) . in @xcite",
    "we showed that it can also be used to obtain information about the attractors of that model .",
    "a reason of interest of this study is that asymmetric neural networks are the limit case of a one parameter family of models , the parameter @xmath22 representing the symmetry of the synaptic couplings :    = j_ijj_jij_ij^2 .",
    "the case @xmath23 represents the present model ( fully asymmetric couplings ) , while for @xmath24 the couplings are fully symmetric and we obtain the mean field model of spin glasses .",
    "thus the parameter @xmath22 connects with continuity asymmetric neural networks to a disordered system of statistical mechanics .",
    "it was suggested through numerical simulations that the model with generic correlation undergoes a dynamical transition when @xmath22 is changed @xcite .",
    "the transition seems to take place when the absolute value of @xmath22 crosses the value @xmath25 .",
    "for @xmath26 the dynamics is chaotic and the typical length of the cycles increases exponentially with the number of neurons @xmath2 , while for @xmath27 the dynamics is frozen and the typical length of the cycles does not increase with system size ( most of the cycles have length 2 , for positive @xmath22 , and 4 for negative @xmath22 ) .",
    "this transition is reminiscent of the dynamical transition taking place in kauffman networks .",
    "also in that case the typical length of the cycles grows exponentially with @xmath2 in the so called chaotic phase , remains finite in the frozen phase and grows less than exponentially with @xmath2 on the critical line @xcite .",
    "it was claimed by kauffman that the critical line of his model can be a good model of the genetic regulatory systems acting in cell differentiation , thus showing that such systems do not need to be tuned in the very details by natural selection but behave similarly to typical realizations of an ensemble of random regulatory networks @xcite .",
    "it is possible that , analogously , also the supposed critical point in attractor neural networks , where chaotic and ordered cycles coexist , can suggest something interesting from a biological point of view .",
    "we think that our method can be modified to give information about systems with generic asymmetry and about the supposed phase transition that they undergo , though this probably requires to go beyond the annealed approximation .",
    "our strategy for the study of attractors in disordered dynamical systems has as starting point the probability distribution of the distance at different time steps .",
    "actually , the information contained in the distribution of the distance is much more than what we need and this distribution is in principle a very complicated object , so that our approach may seem to complicate the problem .",
    "but , in some cases , the distance can be well approximated by a suitably defined stochastic process and the computation becomes much easier .",
    "the simplest possibility is to approximate the distance with a markovian stochastic process .",
    "this is what we call here the _",
    "annealed approximation_.    an apparently paradoxical aspect of this approach is that in disordered dynamical systems attractors exist due to the fact that the motion is deterministic .",
    "stochastic processes , on the other hand , have nothing similar to a limit cycle .",
    "nevertheless , all the properties of the attractors can be derived from the distribution of distances , which is a well defined object in both kinds of models .",
    "we can not pursue this analogy up to times larger than the time of first recurrence of a configuration already visited , when the deterministic motion becomes periodic .",
    "but this is enough , since the first recurrence provides us with every information about the length of the cycles and the transient time .",
    "the fundamental object of our study will be then the distribution of distances between configurations at time steps @xmath28 and @xmath29 on the same trajectory , restricted to trajectories that have not yet visited twice any configuration up to the larger time @xmath30 ( thus the effects of periodicity do not yet appear ) . we will call this condition the opening condition , and denote it by the symbol @xmath31 . the closing probability",
    "@xmath32 is the probability that configurations at time steps @xmath28 and @xmath30 are equal ( @xmath33 ) , subject to the opening condition :    _ n ( t , t)=\\ { d(t , t)=0 a_t } ( the subscript @xmath2 is there to remember the dependence on system size ) .    after the closing time @xmath30",
    "the trajectory enters a periodic orbit of length @xmath34 , where @xmath28 is the transient time . in terms of the closing probabilities ,",
    "the probability to find such a trajectory is easily computed .",
    "first we have to know the probability @xmath35 that the trajectory was not closed before time @xmath36 .",
    "this obeys the equation @xmath37 , whence , introducing a continuous time variable , we get    f_n(t)=(-_0^t dt_0^t dt^  _ n(t,t^))[fn ] ( to have a slightly simpler formula we made the hypothesis that the typical closing times are long , which is normally the case in the chaotic phase , where they grow exponentially with system size @xmath2 , and we transformed the sum into an integral ) .    the probability to find a trajectory that , after a transient time @xmath28 , enters a cycle of length @xmath38 is then obtained multiplying @xmath39 times the closing probability @xmath40 .      regarding the distance as a markovian stochastic process is a very drastic approximation . in our case",
    "it sounds reasonable when the temporal distance @xmath38 is large , since the model that we study is known to have a behavior very reminiscent of chaos @xcite .",
    "however in this way we neglect some memory effects , which can play a fundamental role in systems with nonzero symmetry .    this approximation was first used in this context by derrida and pomeau @xcite , who studied the damage spreading in kauffman networks .",
    "they showed that the average value of the hamming distance between two different trajectories is equivalent , in the infinite system limit , to the average value of the markovian stochastic process obtained extracting new dynamical rules at every time step and keeping memory only of the value of the distance at time step @xmath41 .",
    "thus the disorder is treated as annealed rather than as quenched . in other words , instead of considering an ensemble of trajectories , each one taking place on a fixed realization of the dynamical rules , they consider an ensemble of trajectories moving from one realization of the dynamical rules to another one , in the same spirit in which the annealed average is used for disordered thermodynamical systems .    the above procedure can be shown to describe exactly the evolution of the average distance up to time of order @xmath42 in disordered systems with finite connectivity @xcite , but we think that its validity is more general . in systems with infinite connectivity like the one that we are studying here , or when one is interested in the whole distribution of the distance ,",
    "the equivalence between the two dynamics has not been proved , and we have to assume that a typical trajectory of the quenched system loses memory of the details of the realization of dynamical rules under which it evolves . in the random map model @xcite this is trivially true . in other cases",
    "this can be thought of as a maximal ignorance hypothesis , whose consequences must then be compared with numerical simulations .",
    "let us state some of these consequences .",
    "a markovian stochastic process , if its transition probability is ergodic which is an absorbing point ( if @xmath43 , we must have with probability one @xmath44 for every @xmath45 ) , that is we have to impose the condition that the trajectory is not yet closed , as we did . ] , converges to a stationary stochastic variable independent of the initial distribution .",
    "this means that the closing probability @xmath40 converges to a stationary value @xmath46 .",
    "this is also independent of @xmath38 if the transition probability does not depend on this quantity .",
    "we will show that this happens in the present case , at least for @xmath38 large enough .",
    "it is then easy to compute the probability of a trajectory which , after a transient time @xmath28 , enters a cycle of length @xmath38 ( with @xmath38 and @xmath28 large enough , so that the closing probability has reached its asymptotic value ) : using the results of last section , we get    \\{t = t , l = l}= 1 ^ 2_n(-12(t+l _",
    "n)^2 ) , [ pcic]where @xmath47 is the typical time scale of the problem , in the sense that the random variable @xmath48 has a well defined density of probability even in the limit where @xmath49 goes to infinity .",
    "all the dependence on system size is contained into the factor @xmath50 , which is expected to decrease exponentially with @xmath2 in the chaotic phase .",
    "for instance , for a uniform random map @xcite , which is the most chaotic disordered dynamical system , it holds @xmath51 , and consequently the typical time scale of the attractors grows as @xmath52 .",
    "the properties of random maps can be easily generalized starting from equation ( [ pcic ] ) .",
    "an interesting quantity is the distribution of the attraction basin weights .",
    "this was analytically computed by derrida and flyvbjerg for the case of the uniform rm @xcite .",
    "the weight of the attraction basin of cycle @xmath53 , @xmath54 , is defined as the probability to extract at random an initial configuration which will reach asymptotically the attractor @xmath53 .",
    "the statistical information about the distribution of the weights can be expressed through the  moments \" @xmath55 , defined as    y_n=_w_^n .",
    "@xmath56 is equal to 1 due to the normalization of the weights and @xmath57 represents the average weight in a given dynamical system .",
    "its extreme values , 1 and 0 , correspond respectively to the  ergodic \" case where there is only one relevant attractor and to the case where there is an infinite number of relevant attractors , while a finite value of @xmath57 means that there is a finite number of attractors with non vanishing weight .",
    "this quantity fluctuates from sample to sample , so it is necessary to consider an average over the realizations of the dynamical rules , that is represented by the angular brackets .",
    "the method used in @xcite to compute the distribution of the weight can be applied without modifications to all disordered dynamical systems where the closing probability reaches an asymptotic value , @xmath50 , and the result do not depend on this value in the large size limit .",
    "thus the distribution of the attraction basin weights is _ universal _ for all the disordered dynamical systems where the closing probability reaches a stationary value @xcite , apart for systems which possess some symmetry .",
    "the result for the average value of the @xmath58 is @xcite    y_n=4^n-1[(n-1)!]^2(2n-1)!. [ yn ]    the fluctuations from sample to sample can also be computed .",
    "for example the fluctuations of @xmath57 are measured by @xmath59 , and do not cancel even in the infinite size limit @xmath60 .",
    "the average number of attractors of length @xmath38 can be computed starting with the relation    n_a(l)=2^nl\\{t=0,l = l}.[nl ]    in this formula , the probability @xmath61 should be computed multiplying @xmath62 , given in equation ( [ fn ] ) , times the closing probability @xmath63 .",
    "this is different from the asymptotic value ( for large @xmath28 ) of @xmath40 .",
    "according to the hypothesis , on which the annealed approximation relies , that the system is going to lose memory of the details of the evolution , we expect no correlations between the initial configuration and a configuration at a large time @xmath38 or , in other words , we expect the closing probability to be , asymptotically in @xmath38 , @xmath64 . for chaotic kauffman networks",
    "this can be explicitly computed in the framework of the annealed approximation , which is then consistent under this point of view .",
    "moreover , we find in this case @xmath65 , where @xmath66 does not depend on @xmath2 .",
    "thus it holds n_a(l)(-l^2/2 ^ 2 ) .",
    "summing over @xmath38 we obtain the average value of the total number of cycles , whose leading term in @xmath2 is equal to @xmath67 . since in the chaotic phase",
    "the time scale grows exponentially with system size , the number of attractors is proportional to @xmath2 in this case .",
    "this computation holds for chaotic kauffman networks in the framework of the annealed approximation , but we expect it to hold more generally under the hypothesis discussed above .      the general scheme described above must be modified in the case studied in this note , to take into account the symmetry of the problem .",
    "let us define the reversal operator , @xmath68 , which reverses all the spins .",
    "this operator commutes with the dynamics of the system . using the notation defined in equation ( [ dis ] )",
    ", we can write :    f_j  ( rc)=r f_j(c ) , [ sym ]    this implies that we can define two different closing times :    1 .",
    "the first time @xmath28 when @xmath69 is equal to @xmath70 .",
    "2 .   the first time when",
    "@xmath70 is equal to @xmath71 : then equation ( [ sym ] ) implies @xmath72 .",
    "in other words , the trajectory has reached , after a transient time @xmath28 , a cycle of length @xmath73 .",
    "these closing events can be described in terms of the hamming distance between configurations : the first one corresponds to @xmath43 , while the second one corresponds to @xmath74 .",
    "thus two closing probabilities must be defined :    _ n^(0)(t , t)=\\{d(t , t)=0a_t } , + _",
    "n^(1)(t , t)=\\{d(t , t)=1 a_t } , and the opening condition , @xmath75 , has the meaning that up to time @xmath28 it never occurred either @xmath76 or @xmath77 .",
    "our task is now to compute the master equation for the distribution of the distance under the opening condition ( this means that we consider only trajectories not yet closed ) and under the hypothesis that the distribution of @xmath78 depends only on the distribution of @xmath79 .",
    "this is not a difficult task . to simplify slightly the formulas we will consider , instead of the distance , the overlap @xmath80 , which is measured by the number of elements whose state is the same in the two configurations , divided by @xmath2 .",
    "an element @xmath11 is in the same state at time @xmath81 and @xmath82 if its local field has the same sign at time steps @xmath28 and @xmath30 .",
    "thus it holds    _ i(t+1)_i(t+1)=sign(_jkj_ijj_ik_k(t)_j(t ) ) .",
    "let us consider separately the contribution to this sum coming from the spins whose state is the same at time steps @xmath28 and @xmath30 , whose number is @xmath83 , and that belong to a set that we indicate with the name @xmath84 .",
    "we can then write    _",
    "i(t+1)_i(t+1)=sign((h_i^+(t))^2-(h_i^-(t))^2 ) , where & & h_i^+(t)=_ji(t , t)j_ij_j(t ) , [ h+- ] + & & h_i^-(t)=_j / i(t , t ) ) j_ij_j(t ) .",
    "the annealed approximation consists in considering the local fields as random variables , correlated to the previous story of the system only through the value of @xmath85 . in this spirit",
    ", we consider a dynamics in which the local fields are extracted at random at every time step , under the following assumptions :    1 .",
    "the local fields at different points are independent random variables ; 2 .",
    "the value of @xmath8 is independent on the synaptic coupling @xmath4 .",
    "both these assumptions have troubles when the synaptic couplings are correlated with each other , but they are quite reasonable for @xmath23 , which is the case that we are studying now .",
    "assumption 1 implies that the transition probability is a binomial one :    \\{q(t+1,t+1)=q_nq(t , t)=q_m}=nn((q_m))^n",
    "(1-(q_m))^n - n , where @xmath86 , and @xmath87 is the probability that @xmath88 , where , following assumption 2 , @xmath89 is a gaussian variable with mean value zero and variance @xmath90 ( this result is independent on the details of the distribution of the couplings , provided that they are all independent variables with mean value zero and with the same variance ) .",
    "a straightforward computation shows that    ( q)=2q .    the markov process associated to this transition probability is ergodic if we exclude as starting points the values @xmath91 and @xmath92 , as we do imposing the opening condition , and the distribution of the distance evolves towards a stationary distribution .",
    "moreover , since the transition probability is independent on @xmath34 , also the stationary distribution is independent on @xmath38 , which appears only in the initial distribution of the variable @xmath93 .",
    "it is also evident from the symmetry of the problem that it must hold @xmath94 , so , if also the initial distribution is symmetric ( _ e.g. _ a binomial distribution around @xmath95 ) , the overlap distribution will be symmetric at every time step and it will be concentrated around the value @xmath96 ( the distributions of the overlap and of the distance are perfectly equivalent in this case ) .",
    "the stationary distribution is , independently on the initial one , concentrated around the value @xmath97 solution of the self - consistent equation :    q^*=(q^*)=2.[fix]this equation has three solutions : @xmath25 , 1 and 0 , but only the first one can be accepted , according to the criterion @xmath98 , which can be obtained either as the stability condition of the fixed point @xmath97 of the map @xmath99 , or as the condition that the variance of the stationary distribution is positive ( see equation ( [ var ] ) below )",
    ".    equation ( [ fix ] ) is equivalent to the equation for the stationary value of the correlation function rigorously derived in @xcite through a functional integral approach , so that one can see from this comparison that the annealed approximation gives an exact ( though trivial ) result concerning the average overlap .",
    "but our task here is to compute the whole stationary distribution of the overlap , and we can not prove that the annealed approximation is correct to this extent , so we have to rely upon simulations to control its validity .    though it is concentrated around @xmath100 ,",
    "the stationary distribution is much broader than a binomial one and thus the closing probability is exponentially larger than @xmath101 . in order to compute its value ,",
    "we proceed in this way @xcite : since the transition probability is exponentially concentrated , we look for a solution of the form :    p_n(q(t , t)=q_n)=c_n(q_n , t)(-n_t(q_n ) ) , where we have dropped the @xmath38 dependence of the probability , which disappears at stationarity . using stirling approximation for the binomial coefficient and the saddle point approximation to average over the distribution at time step @xmath41 , we get the following equation for the evolution of the exponent of the distribution , @xmath102 : _ t(x)=_t-1(q_t(x))+x(x(q_t(x)))+ ( 1-x)(1-x1-(q_t(x ) ) ) , [ alp ] where the function @xmath103 must be determined self consistently solving the equation _t-1(q_t(x))-(q_t(x))(x(q_t(x))-1-x1-(q_t(x ) ) ) , [ sella]with the conditions @xmath104 and @xmath105 .    at stationarity",
    "the most probable overlap ( the point where @xmath106 has a minimum ) is given by equation ( [ fix ] ) , and the variance of the distribution can be obtained taking the second derivative of equation ( [ alp ] ) and solving it together with the first derivative of the saddle point condition ( [ sella ] ) .",
    "the result is    v^*=q^*(1-q^*)1-((q^*))^2=1/4 1-(2/)^20.4204 , [ var]where @xmath107 is the variance of the stationary distribution multiplied times @xmath2 .",
    "thus the variance is larger than in the case of a binomial distribution , since the dynamics has produced correlations between different elements .",
    "the value of the closing probability can not be computed analytically : we need for this the whole function @xmath108 , and to obtain it we should solve a transcendent non local equation .",
    "thus we had to solve numerically equation ( [ alp ] ) , obtaining the stationary distribution reported in figure 1 .",
    "the asymptotic closing probability , defined as @xmath109 , is thus ^*_n=2 (n ) , with @xmath110 .",
    "as discussed in the previous section , the exponent of the average length of the cycles should be equal to @xmath111 .",
    "this prediction is in good agreement with the numerical simulations that will be reported in section 4 .      in order to compute the average number of cycles we have to know the distribution of the overlap with the initial configuration , @xmath93 , which plays the role of the initial distribution for the stochastic process studied in the previous subsection .",
    "although the number of cycles in fully asymmetric neural networks was already exactly studied by schreckenberg @xcite , we want to sketch the annealed computation of it , since it is much simpler and it can be generalized to more complex situations .",
    "our aim is to compute the distribution of @xmath93 .",
    "after one time step the annealed approximation is exact ( we have still to extract all the couplings ) and trivial : every spin can be either in its initial state or in the reversed one with probability 1/2 , and the overlap @xmath112 multiplied times @xmath2 has a binomial distribution with @xmath113 .",
    "after two time steps we distinguish two contributions in the local field : one coming from the set @xmath114 of the spins which are in the same state at @xmath115 and at @xmath116 and another one coming from all the other spins .",
    "we write    _ i(0)_i(2)=sign(_ji_1_i(0 ) j_ij_j(0)- _ j / i_1_i(0 ) j_ij_j(0 ) ) .",
    "[ pl1 ]    since the states @xmath117 and @xmath118 are independent both one on each other and on the couplings , we can set them equal to 1 .",
    "if we change the sign of the last sum , we obtain @xmath119 .",
    "thus , depending on whether this is positive or negative , there are two possibilities :    [ pl2 ] _",
    "i(0)_i(2)= \\ {    lrsign((_ji_1j_ij)^2- (_j / i_1j_ij)^2 ) , & if _ i(0)_i(1)>0 + sign((_j / i_1j_ij)^2- (_ji_1j_ij)^2 ) , & if _",
    ".(there is indeed in this formula a small imprecision , which becomes negligible in the infinite size limit : since the coupling @xmath120 is set equal to zero , we have not to take into account the spin @xmath121 itself , which contributes to the first sum in both lines ) .",
    "the probability that the sum of @xmath122 gaussian variables has a module larger than that of the sum of @xmath123 other gaussian variables was already computed in the previous section , where it received the name @xmath124 . taking all this into account ,",
    "we come to the transition probability    & & \\{q(0,2)=m / nq(0,1)=n / n}= + & & = _ k nk n - nm - k ((n / n))^n - n - m+2k(1-(n / n))^n+m-2k , where , as usual , the opening condition imposes to exclude as starting points @xmath125 and @xmath126 , and the sum runs over all the values of @xmath127 for which the factorial is well defined . the closing probability which can be deduced from this formula setting either @xmath128 or @xmath129 coincides with the one exactly computed in ref .",
    "it can be easily seen that it is proportional to @xmath101 , as expected ( the system loses memory of the initial configuration quite fast ) , and the proportionality coefficient can be computed with the saddle point method @xcite    in the general case , the information about @xmath93 is not enough to compute the distribution of @xmath130 : we have also to know the value of @xmath112 , as it can be seen from equation ( [ pl2 ] ) where we have to substitute 1 with @xmath38 and 2 with @xmath131 in the equations but we have to keep memory of @xmath119 in the conditions . in the general case",
    "the transition probability has thus the form    & & \\{q(0,l+1)=m / nq(0,1)=n_1/n , q(0,l)=n / n}= + & & = _ k n_1kn - n_1m - k ((n / n))^n - n_1-m+2k(1-(n / n))^n_1+m-2k , and we have to consider the evolution of the joint distribution of the variables @xmath112 and @xmath93 .",
    "as expected , the correlations between these two variables vanish very fast as @xmath38 grows , and the stationary distribution is the product of two binomial distributions , as it can be easily checked , so that for large @xmath38 the closing probability is @xmath132 ( with @xmath133 equal either to 1 or to 0 ) , consistently with the supposed loss of memory and in agreement with the exact results of ref @xcite . for small values of @xmath38 it can be seen that @xmath134 , where @xmath66 goes to a finite value in the infinite size limit , so that the total number of cycles increases only proportionally to system size .",
    "the computations shown in section [ ann ] must be modified to take into account the twofold nature of the closing probability .",
    "we have to distinguish between two kinds of cycles , with different properties under the reversal operation :    1 .",
    "cycles that close when @xmath135 ( or , in other words , @xmath136 ) , whose length is @xmath73 . they are invariant under the reversal operation : each configuration is present together with its reversed one .",
    "cycles that close when @xmath137 . in this case the reversal operator applied to the cycle @xmath138 produces a new cycle @xmath139 with equal length and equally large attraction basin .    taking this into account",
    ", we have to distinguish between cycles of even length , which can be of one of the two kinds , and cycles of odd length , which can be only of the first kind .",
    "cycle length distribution is then    \\{t = t , l = l } & & = 12 ^ 2(-(t+l)^22 ^ 2 ) , l ; + & & = 12 ^ 2(-(t+l)^22 ^ 2)+12 ^ 2 (-(t+l/2)^22 ^ 2 ) , l , with @xmath140 .",
    "the cycles of the first type have only even length , so that their number is half of the number of the cycles of the second type . using the result of subsection [ ann ] and summing up the contributions of both types of cycles we obtain , at the leading order in @xmath2 ,    _",
    "ln_a(l)=34n , which is @xmath141 times larger than in a random map with the same closing probability .",
    "the most important difference between attractors in asymmetric neural networks and in a random map involves the distribution of the attraction basins weights .",
    "let us consider separately cycles of the first type and cycles of the second type ( taking only one cycle to represent each pair of cycles of the second type ) .",
    "we then get the expression of the moments @xmath55 of the distribution of the weights :    y_n=12_ w_^n + 2_^(w_^/2)^n , where the sum over @xmath142 and @xmath143 of the weights are both normalized to one . under the hypothesis that each of the two sets of weights is distributed as in a random map ,",
    "we get y_n = (12 + 12^n ) y_n_rm , [ weigh]or , using ( [ yn ] ) , y_n+1=12 (n!)^2(2n+1 ) !",
    "(4^n+2^n ) .",
    "thus the moments of the distribution of the weights are smaller than in the usual random map , for instance @xmath144 instead of 2/3 .",
    "these results are in very good agreement with numerical simulations .    to prove equation ( [ weigh ] ) let us recall that @xmath55 can be interpreted as the probability that @xmath122 randomly chosen trajectories reach the same attractor .",
    "we can compute such quantity using the closing probabilities and following exactly the same lines as in @xcite , but we have to remember that a closing event has two different meanings : either a closure on an identical configuration ( @xmath92 ) or a closure on a reversed configuration ( @xmath91 ) . thus not all the events which represent the closure of the @xmath122 trajectories , and whose probability is exactly @xmath145 , have the meaning that the trajectories will ultimately meet .",
    "if the first trajectory closes with @xmath91 ( this happens with probability 1/2 ) , its attraction basin contains also all of the reversed configurations , and the following @xmath146 trajectories which close on it will then go to the same attractor , regardless on how they close . on the contrary , if the first trajectory closes with @xmath92 , the following @xmath146 trajectories have also to close with @xmath92 in order to go to the same attraction basin ( if they close with @xmath91 , they go to the reversed basin ) . in this case ,",
    "whose probability is again @xmath25 , a closing event is equivalent to an asymptotic meeting of the @xmath122 trajectories only with probability @xmath147 .",
    "equation ( [ weigh ] ) is thus proved .",
    "the above picture of the distribution of the attraction basins weights is completely destroyed by the introduction of a magnetic field , however small , in the equations of motion ( [ evol ] ) , which restores the distribution typical of a random map .",
    "we considered the dynamic rules _",
    "i(t+1)=sign(_j j_ij_j(t)-h ) [ evolh ]    the magnetic field @xmath148 has the biological meaning of the threshold of activation of the neurons . in real neurons , such non - zero threshold exists and",
    "can be different from one neuron to another one . in our simplified model ,",
    "we take a threshold which is constant among the different neurons .",
    "its introduction explicitly breaks the symmetry respect to the reversal of all the neural activities .    in the framework of the annealed approximation , the conditional probability that the activity of a neuron is the same in two different time steps is not more symmetric , _",
    "i.e. _ @xmath149 is different from @xmath150 and @xmath151 increases very fast respect to the above case , thus making very unlikely a reversed closure , while @xmath152 decreases . after a straightforward calculation",
    "we get    ( q)=1 - 2_arcsin q^/2 (-12(h / t)^2 ) dt . for large threshold the closing probability differs from 1 by a value that cancels very fast , as @xmath153 .",
    "the attractors of the first type ( such that @xmath154 ) are completely destroyed in this way , while attractors of the second type do not live in pairs anymore , and the distribution of the weights is of the random map type .",
    "our first aim was to compare the distribution of the overlap predicted by the annealed approximation with the same distribution in the quenched system .",
    "as we wrote , the analogy holds if we measure the overlap between configurations only along the trajectories that are not yet closed when we do the measurement . under this condition , we computed the distribution of the overlap @xmath155 for @xmath38 fixed and @xmath28 large enough to suppose that the distribution has attained stationarity .",
    "the exponent @xmath156 of the distribution of the overlap is defined by the equation @xmath157 , where the factor @xmath158 , proportional to @xmath159 , comes from the stirling expansion of the binomial coefficient .",
    "thus we computed @xmath156 using the formula    ( q)=-1n((p_n(q ) ) + 12n ) .",
    "the logarithmic term must not be subtracted when @xmath90 is equal to 0 or 1 , because in this case the @xmath159 factor is no more present in the expansion of the binomial coefficient , and so we did not consider it for @xmath91 and 1 , interpolating linearly between the two formulas for values of @xmath90 between 0 and @xmath160 and between @xmath161 and 1 . in such data analysis",
    "we neglect terms of order @xmath15 ( there is also an unknown coefficient in the expression of the probability @xmath162 ) , and the agreement between the annealed prediction for @xmath156 and the quenched data , compared in figure 1 , is then very satisfactory even for a system of such a small size ( we considered @xmath163 ) .",
    "when @xmath38 , the temporal distance between configurations , is small , there are some discrepancies ( for instance , for @xmath164 the quenched distribution is much broader than expected , and the closing probability is consequently much higher ) , but when @xmath38 is large the agreement improves ( in particular , the variance of the distribution and the exponents @xmath165 and @xmath166 of the closing probabilities coincide within the errors with the predicted values ) .",
    "this fact sustains our interpretation that the annealed approximation is valid when the temporal distance is large , so that the system has lost memory of the details of its evolution @xcite .",
    "= 12.0 cm = 15.0 cm    next we measured the closing probability @xmath40 .",
    "figure 2 represents this quantity as a function of @xmath28 for different values of @xmath38 , kept fixed .",
    "the statistic errors are large , but it appears that @xmath167 reaches a value approximately stationary in @xmath28 , in agreement with the annealed prediction , when @xmath38 is large ( in figure 2b we have @xmath168 ) but when @xmath38 is small ( in figure 2a @xmath164 ) the closing probability reaches a maximum value and then decreases , as a function of @xmath28 .",
    "we already observed this kind of non monotonic behavior of the closing probabilities in simulations of kauffman model . in both cases",
    "we interpret the decreasing part of @xmath169 as due to the opening condition : the condition that the trajectory is not closed up to time @xmath28 selects , as @xmath28 grows , trajectories which are more and more unlikely to close .",
    "the opening condition can not be imposed in the annealed scheme , because we consider the stochastic process @xmath170 with @xmath38 fixed and we can not control @xmath79 for generic @xmath28 and @xmath30 . so the annealed scheme must be modified to take into account this fact @xcite .",
    "but in asymmetric neural networks , differently from what we observed in kauffman networks , the opening condition seems to be irrelevant when the temporal distance @xmath38 is large , and the closing probability appears to reach in this case an approximately stationary value .",
    "= 12.0 cm = 15.0 cm    the non - stationarity of the distribution of @xmath171 shows the existence of memory effects in the model : the statistical properties of @xmath171 still depend on @xmath28 , even after an arbitrarily long transient time .",
    "it would be interesting to find out whether the lack of time translation invariance in the system with completely uncorrelated couplings has some relation with aging in the relaxational dynamics of the sk spin glass model @xcite . in the present case , however , the lack of time translation invariance is only a minor effect and does not prevent the overlap @xmath155 from reaching a stationary distribution for @xmath38 large enough .",
    "the macroscopic properties of the dynamics can be predicted , in good agreement with numerical results , also neglecting this effect at all .",
    "we conclude this subsection showing a plot of the integral closing probability @xmath172 , defined as _",
    "n(t)=_t=0^t-1 _ n(t,t)this is the probability that a trajectory not closed at time @xmath41 closes at time @xmath28 ) . in kauffman networks ,",
    "this quantity is non - monotonic as a function of @xmath28 : it increases to a maximum value and then decreases with @xmath28 .",
    "on the other hand , from the annealed approximation we would expect it to increase linearly with @xmath28 in the stationary state . in asymmetric neural networks we found that the integral closing probability increases monotonically with @xmath28 .",
    "after a transient phase of very fast increase it slows down , and asymptotically it appears to behave as a power law .",
    "for the largest systems that we simulated our data are very noisy , and we could fit the asymptotic @xmath28 behavior only for @xmath163 , finding that the best fit exponent of the power law is approximately @xmath173 .",
    "so , at least for systems of not very large size , deviations from the annealed approximation are present also in this case .",
    "= 12.0 cm = 15.0 cm      to obtain the first three moments of the distribution of the attraction basins we followed the method indicated in @xcite . for every value of the parameters @xmath2 and @xmath174 we generated at random 2000 networks , extracting the synaptic couplings with gaussian distribution , and",
    "we simulated four randomly chosen trajectories on each of them .",
    "the average weight of the basins , @xmath175 , was estimated from the probability that two different trajectories end up on the same periodic orbit . in general @xcite",
    ", @xmath55 can be measured as the probability that @xmath122 different initial configurations evolve to the same attractor . simulating four initial configurations it is also possible to measure @xmath176 as the probability that each of two pairs of configurations end up on a same attractor , the two attractors being either different or equal .",
    "figure 4 shows data which report the behavior of the moments of attraction basin distribution for systems of different size , @xmath2 .",
    "it can be seen that they rapidly converge to the predictions of the annealed approximation , corrected to take into account the reversal symmetry .",
    "= 12.0 cm = 15.0 cm    the average cycles length increases exponentially with system size , @xmath177 . the exponent @xmath178 is less than @xmath179 , as it would be in a completely random map .",
    "its value @xmath180 is in good agreement with the prediction of the annealed approximation , @xmath181 ( the small discrepancy could be a finite size effect , as the exponent estimated from numerical data increases when only the largest systems are considered ) .",
    "figure 5 shows the average length of the cycles versus system size .",
    "= 12.0 cm = 15.0 cm    the distribution of cycle length is much broader than it is expected on the basis of the annealed approximation , and asymptotically behaves as a stretched exponential :    \\ { l > l}(-(l/_n)^_n).[distr ]    as discussed in the previous sections , the distribution is different for the two different types of cycles .",
    "we considered only odd cycles , in order to select only attractors of the second type , and we checked that the scale of the distribution , @xmath182 , increases exponentially with @xmath2 , @xmath183 , where the exponent @xmath184 coincides with @xmath185 within the errors . on the other hand the exponent @xmath186 of the stretched exponential ,",
    "for which the annealed approximation predicts the value 2 , is instead less than 1 for all of the system sizes that we examined , but it appears to increase slightly as @xmath2 grows ( though are data about this point are very noisy ) , so that it is possible that this discrepancy shall disappear in the infinite size limit .",
    "the fact that we find @xmath186 less than 1 appears challenging also because the distribution of the closing time ( _ i.e. _ the sum of the transient time plus the length of the cycle ) , which should have the same behavior of the distribution of cycle length , according to the annealed approximation , is indeed much steeper : it can be fitted to a stretched exponential of the same form ( [ distr ] ) , but with a much larger exponent @xmath187 .",
    "for instance , for @xmath163 , we find @xmath188 , in good agreement with the annealed prediction , while the value of @xmath186 is @xmath189 .      when we consider the evolution equation ( [ evolh ] ) with a threshold @xmath148 , the reversal symmetry is explicitly broken and the distribution of the weights is the same as in the usual random map .",
    "figure 6 shows the behavior of the first moments of the distribution of the weights as a function of system size @xmath2 for @xmath190 .",
    "such threshold is so small that it modifies the value of the exponent @xmath53 by less than 2 percent .",
    "the annealed approximation predicts in this case @xmath191 , to be compared to the value @xmath192 found with zero threshold .",
    "the prediction is in good agreement with numerical simulations : a fit of the average length of the cycles gives @xmath193 .",
    "for such a small threshold we can observe traces of the broken symmetry present as finite size effects : the moments of the distribution at small @xmath2 fall below the random map values , even if of a very small amount , and then increase to those values , which are maintained asymptotically in system size .",
    "= 12.0 cm = 15.0 cm    on the other hand , when the threshold is larger , we do not see at all the signs of the symmetry on the distribution : for @xmath194 , the average basin weight decreases monotonically from the value 1 at small @xmath2 toward the random map value @xmath195 .",
    "for such a threshold the average length of the cycles still behaves exponentially with @xmath2 , but the exponent @xmath53 is very small and power law corrections have important effects also for systems large to simulate , as it appears from the fact that the best fit exponent depends significantly on system size ( it decrease as system size increases ) , and we could not estimate it accurately .",
    "nevertheless , the agreement between the annealed approximation , which predicts @xmath196 , and the numerical result @xmath197 , is worse than in the previous cases but still not bad .",
    "in this work we used a stochastic scheme , based on the closing probabilities and on their approximation by means of a markovian stochastic process , in order to compute the properties of the attractors in fully asymmetric neural networks .",
    "the fundamental hypothesis behind this approximation is that the system forgets fast enough the details of its past evolution , so that a one step memory is already enough to describe the gross features of the dynamics .",
    "our method is able to predict very satisfactorily the @xmath2 behavior of the typical lengths of the cycles and typical transient times , the number of cycles , the distribution of their attraction basin weights and also the main features of the distribution of the distances . on the other hand , the approximation fails to predict the shape of the distribution of cycle length , which is much broader than we would expect .",
    "the average number of cycles had already been exactly computed , and perhaps other quantities can be exactly computed in this model , but the present method has the advantage of being very simple , and we hope that it can be applied to more complex situations . in particular with this method we argue that the distribution of attraction basins is , for disordered dynamical systems that are  chaotic enough \" , always equal to the one computed by derrida and flyvbjerg for the case of an uniform random map @xcite .",
    "a possible extension of our method , that we consider very interesting and that we plan to pursue further , is towards the study of neural networks with finite symmetry .",
    "numerical studies suggest that such systems undergo an abrupt change of dynamical regime when the symmetry @xmath22 is changed @xcite , but a ",
    "mean field \" description of this transition from an ordered behavior to chaos is still lacking .",
    "the possibility that such a change can be characterized as a transition between memory and loss of memory is very appealing .",
    "memory effects are more and more important for networks with non - zero coupling symmetry ( till the well - known aging properties of the sk model are approached ) .",
    "because of these effects , it is necessary to modify our method also to study the chaotic regime of the model ( low symmetry ) . technically this is not an easy task , since for non zero symmetry correlations arise both between the local fields ( see equations ( [ h+- ] ) ) of different neurons and , more difficult to treat , between synaptic couplings and dynamical variables .",
    "the latter introduce an effective interaction between the state of an element at two different time steps @xmath28 and @xmath198 , so that in the dynamics also an effective gradient flow is present , and , if the annealed approximation can describe this situation , it will be necessary to take into account also this information , aside the crude distance , to make the annealed scheme useful .    studying this family of models",
    "it is also possible , varying the continuous parameters @xmath22 and @xmath148 , to go from the distribution of the attraction basins typical of the random map to the one typical of spin glasses , thus the study of the general model would shed some light on the relation between the two kinds of distributions .",
    "memory effects are probably responsible of the discrepancy between the prediction of the annealed approximation and the observed distribution of cycle length .",
    "in fact , the distance @xmath79 does not reach a stationary distribution , if we impose the condition that the trajectory is not yet closed before the measure .",
    "we think that this condition , which can not be imposed in our computation , selects trajectories that are less and less likely to close . as a result , the integral closing probability , which is our main tool in the computation , increases as a function of @xmath28 slower than expected .",
    "it is possible that this effect shows up only at small @xmath38 and disappears in the infinite size limit ( an hint of this could be the fact that the distribution of cycle length decays faster in this limit ) , but it is also possible that , as in the case of kauffman model that we previously studied , some corrections to the annealed picture are necessary also in the infinite size limit .",
    "however , we think that these results show that the annealed approximation is an useful tool to investigate in a simple way the properties of attractors in disordered dynamical systems .",
    "we are indebted to angelo vulpiani for addressing us to this model .",
    "u.b . is pleased to thank peter grassberger and heiko rieger for interesting discussions .",
    "hopfield ( 1982 ) , neural networks and physical systems with emergent collective computational abilities , _ proc .",
    "usa _ , * 79 * , 2554 g. parisi ( 1986 ) , asymmetric neural networks and the process of learning , _",
    "a : math . gen . _ * 19 * , l675 a. crisanti and h. sompolinsky ( 1987 ) , dynamics of spin systems with randomly asymmetric bonds : langevin dynamics and a spherical model , _ phys .",
    "a _ * 36 * 4922 .",
    "a. crisanti and h. sompolinsky ( 1988 ) , _ phys .",
    "rev . a _ * 37 * 4865",
    ". h. gutfreund , j.d .",
    "reger and a.p .",
    "young ( 1988 ) , the nature of attractors in an asymmetric spin glass with deterministic dynamics , _ j. phys . a : math .",
    "* 21 * , 2775 h. rieger , m. schreckenberg , and j. zittartz ( 1989 ) , glauber dynamics of the asymmetric sk model , _ z. phys .",
    "b - condensed matter _ * 74 * , 527 h. rieger , m. schreckenberg , p. spitzner and w. kinzel ( 1991 ) , alignment in the fully asymmetric sk model , _ j. phys .",
    "a : math . gen . _ * 24 * , 3399 t. pfenning , h. rieger and m. schreckenberg ( 1991 ) , numerical investigation of the asymmetric sk model with deterministic dynamics , _ j. phys .",
    "i _ * 1 * , 323 k. ntzel ( 1991 ) , the length of attractors in asymmetric random neural networks with deterministic dynamics , _ j. phys . a : math .",
    "* 24 * , l151 .",
    "k. ntzel , u. krey ( 1993 ) , subtle dynamical behavior of finite size sherrington - kirkpatrick spin glasses with non symmetric couplings , _ j. phys . a : math .",
    "* 26 * , l591    m. schreckenberg ( 1992 ) , attractors in the fully asymmetric sk model , _ z. phys .",
    "b - condensed matter _ * 86 * , 453",
    ". b. derrida and y. pomeau ( 1986 ) , random networks of automata : a simple annealed approximation , _ biophys . lett . _ * 1*(*2 * ) , 45 - 49 s.a . kauffman ( 1969 ) , _ j. theor .",
    "* 22 * , 437 homoeostasis and differentiation in random genetic control networks , nature * 244 * , 177 - 178 .",
    "kauffman ( 1993 ) , _ origins of order : self - organization and selection in evolution _ , oxford university press u. bastolla and g. parisi ( 1966 ) , closing probabilities in the kauffman model : an annealed computation , _ physica d _ * 98 * , 1 a. crisanti , m. falcioni and a. vulpiani ( 1993 ) , transition from regular to complex behavior in a discrete deterministic asymmetric neural network model , _ j. phys . a : math .",
    "* 26 * , 3441 b. derrida and g. weisbuch ( 1986 ) , evolution of overlaps between configurations in random boolean networks , _ j. physique _ * 47 * , 1297 h.j .",
    "hilorst and m. nijmajer ( 1987 ) , on the approach of the stationary state in kauffman s random boolean network , _",
    "j. physique _ * 48 * , 185      b. derrida , h. flyvbjerg ( 1986 ) , the random map model : a disordered model with deterministic dynamics , _ journal de physique _ * 48 * , 971 - 978 b. derrida and h. flyvbjerg ( 1986 ) , multivalley structure in kauffman s model : analogy with spin glasses , _",
    "j.phys.a : math.gen._ * 19 * , l1003-l1008 u. bastolla and g. parisi ( 1997 ) , attraction basins in discretized maps , _ j. phys .",
    "a : math . gen .",
    "_ , /bf 30 , 3757 j.p .",
    "bouchod , l.f .",
    "cugliandolo , j. kurchan and m. mezard ( 1997 ) , out of equilibrium dynamics in spin - glasses and other glassy systems , cond - mat/9702070"
  ],
  "abstract_text": [
    "<S> the statistical properties of the length of the cycles and of the weights of the attraction basins in fully asymmetric neural networks ( _ i.e. _ with completely uncorrelated synapses ) are computed in the framework of the annealed approximation which we previously introduced for the study of kauffman networks . </S>",
    "<S> our results show that this model behaves essentially as a random map possessing a reversal symmetry . </S>",
    "<S> comparison with numerical results suggests that the approximation could become exact in the large size limit .    </S>",
    "<S> 48 pt -1.5truecm -1.2 truecm 16.5 cm 23 cm        @xmath0dipartimento di fisica , universit `` la sapienza '' , p.le aldo moro 2 , i-00185 roma italy    @xmath1hlrz , forschungszentrum jlich , d-52425 jlich germany    keywords : disordered systems , attractor neural networks </S>"
  ]
}