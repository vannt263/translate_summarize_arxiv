{
  "article_text": [
    "the estimation of density derivatives has full potential for applications .",
    "this has been noted even in the first seminal papers on density estimation , as @xcite , which was also concerned with the estimation of the mode of a unimodal distribution , the value that makes zero the first density derivative . in the multivariate case ,",
    "the pioneering work of @xcite showed how the estimation of the gradient vector can also be used for clustering and data filtering , leading to a substantial amount of literature on the subject , under the name of the _ mean shift algorithm_. looking further afield , @xcite made use of the mean shift idea for image analysis , and the highly - cited paper by @xcite showed how these techniques can be useful for low - level vision problems , discontinuity preserving smoothing and image segmentation .",
    "a further very popular use of the mean shift algorithm is for real - time object tracking , as described in @xcite .    from the perspective of statistical data analysis , in the multidimensional context the estimation of the first and second derivatives of",
    "the density is crucial to identify significant features of the distribution , such as local extrema , valleys , ridges or saddle points . in this sense , @xcite developed methods for determining and visualizing such features in dimension two , extending previous work on scale space ideas introduced in @xcite for the univariate case ( the sizer approach ) , and the same authors also explored the application of this methodology to digital image analysis in @xcite .",
    "@xcite generalized these results for multivariate data in arbitrary dimensions and provided a novel visualization for three - dimensional data .",
    "these techniques have been widely applied recently in the field of flow cytometry ; see @xcite , @xcite or @xcite .",
    "another relatively new problem that is closely related to gradient estimation is that of finding filaments in point clouds , which has applications in medical imaging , remote sensing , seismology and cosmology .",
    "this problem is rigorously stated and analyzed in @xcite .",
    "filaments are one - dimensional curves embedded in a point process , and it can be shown that steepest ascent paths ( i.e. , the paths from each point following the gradient direction ) concentrate around them , so gradient estimation appears as a useful tool for filament detection .",
    "in this paper we focus on kernel estimators of multivariate density derivatives of arbitrary order , formally defined in section [ sec:2 ] below . as for any kernel estimator",
    ", it is well known that the crucial factor that determines the performance of the estimator in practice is the choice of the bandwidth matrix . in the multivariate setting",
    "there are several levels of sophistication at the time of specifying the bandwidth matrix to be used in the kernel estimator ( see * ? ? ?",
    "* chapter 4 ) .",
    "the most general bandwidth type consists of a symmetric positive definite matrix ; it allows the kernel estimator to smooth in any direction whether coordinate or not .",
    "this general class of bandwidth matrices can be constrained to consider positive definite diagonal matrices , allowing for different degrees of smoothing along each of the coordinate axis , or even further to consider a bandwidth matrix involving only a positive scalar multiple of the identity matrix , meaning that the same smoothing is applied to every coordinate direction . as noted in @xcite in the density estimation context",
    ", the single - parameter class should not be used for unscaled data or , as stated by @xcite in terms of feature space analysis , to use this bandwidth class at least the validity of an euclidean metric for the feature space should be previously checked .",
    "the simpler parameterizations are in general more widely used than the unconstrained counterpart for two reasons : first , in practice they need less smoothing parameters to be tuned , and second , due to the difficulties encountered in the mathematical analysis of estimators with unconstrained bandwidths .",
    "however , @xcite provided a detailed error analysis of kernel density derivative estimators with unconstrained bandwidths and showed that the use of the simpler parameterizations can lead to a substantial loss in terms of efficiency , and that this problem becomes more and more important as the order of the derivative to be estimated increases .",
    "@xcite also proposed an optimal bandwidth selector for the normal case , but they did not develop more sophisticated data - driven choices of the bandwidth matrix with applicability to more general densities , which is crucial to make density derivative estimation useful in practice . along the same lines , @xcite argue that most existing bandwidth selection methods for the mean shift algorithm , all of them for the single - parameter class of bandwidths , are based on empirical arguments .",
    "in the univariate case there exist some approaches to bandwidth selection for density derivative estimation : @xcite introduced a cross validation method and showed its optimality ; @xcite derived the relative rate of convergence of this method and also for a plug - in proposal ; @xcite studied two root @xmath0 selectors in the fourier domain , and more recently @xcite focused on the smoothed cross validation bandwidth selector for the density derivative . in the multivariate case , however , the issue of automatic bandwidth selection for density derivative estimation has remained largely unexplored .",
    "given the smaller body of multivariate density estimation research as compared to their univariate cousins , it is not surprising that multivariate density derivative estimation suffers equally ( if not more so ) from a lack of solid results . to the best of our knowledge , in the literature",
    "the only published approaches to bandwidth selection for multivariate kernel estimation of density derivatives are the recent papers @xcite and @xcite , but both focus exclusively on the first derivative .",
    "this paper proposes three new methods for unconstrained bandwidth matrix selection for the multivariate kernel density derivative estimator , and explores their applicability to other related statistical problems . in section [ sec:2 ]",
    ", we introduce the mathematical framework for the analysis of multivariate derivatives . in section [ sec:3 ]",
    "we show that the relative rate of convergence of these unconstrained selectors is the same as for the classes of simpler bandwidth matrices , so that from an asymptotic point of view our methods can be as successful as ( and more flexible than ) those needing less smoothing parameters .",
    "the finite - sample behaviour of the new bandwidths is investigated in section [ sec:5 ] , and their application to develop new data - driven nonparametric clustering methods via the mean shift algorithm is explored in section [ sec : ms ] , and for feature significance in section [ sec : feature ] .",
    "finally , the proofs of the results are given in an appendix .",
    "the problem of estimating the @xmath1-th derivative of a multivariate density is considered in this section . from a multivariate point of view",
    ", the @xmath1-th derivative of a function is understood as the set of all its partial derivatives of order @xmath1 , rather than just one of them .",
    "notice that , for instance , in a multivariate taylor expansion of order @xmath1 all of the partial derivatives of order @xmath1 are needed to compute the @xmath1-th order term . or , in another related example , all the second order partial derivatives are involved in the computation of the hessian matrix .",
    "all the @xmath1-th partial derivatives can be neatly organized into a single vector as follows : if @xmath2 is a real @xmath3-variate density function and @xmath4 , denote by @xmath5 the first derivative ( gradient ) operator . all the second order",
    "partial derivatives can be organized into the hessian matrix @xmath6 , and the hessian operator can be formally written as @xmath7 if the usual convention @xmath8 is taken into account . for @xmath9 , however , it is not that clear how to organize the set containing all the @xmath10 partial derivatives of order @xmath1 .",
    "here we adopt the unified approach used in @xcite or ( * ? ? ? *",
    "section 1.4 ) , where the @xmath1-th derivative of @xmath2 is defined to be the vector @xmath11 . in the previous equation @xmath12 denotes the @xmath1-th kronecker power of the operator @xmath13 ; see , e.g. , @xcite for the definition of the kronecker product . naturally , @xmath14 , @xmath15 and , for example , @xmath16 , where @xmath17 denotes the operator which concatenates the columns of a matrix into a single vector .    here",
    "we study the problem of estimating the @xmath1-th derivative @xmath18 from a sample @xmath19 of independent and identically distributed random variables with common density @xmath2 .",
    "the usual kernel estimator of @xmath2 is defined as @xmath20 , where the kernel @xmath21 is a spherically symmetric density function , the bandwidth @xmath22 is a symmetric positive definite matrix and @xmath23 .",
    "thus , the most straightforward estimator of @xmath18 is just the @xmath1-th derivative of @xmath24 , given by @xmath25 , where the roles of @xmath21 and @xmath22 can be separated for implementation purposes by noting that @xmath26 , as shown in @xcite , where for any matrix @xmath27 it is understood that @xmath28 and @xmath29 .",
    "see , however , @xcite for other possible estimators in the univariate context .    for the density estimation case ( @xmath30 ) , @xcite showed that the use of bandwidths belonging to the class @xmath31 , with @xmath32 the @xmath33 identity matrix , or the class @xmath34 , may lead to dramatically less efficient estimators than those based on bandwidth matrices drawn from @xmath35 , the space of all positive definite symmetric matrices .",
    "moreover @xcite showed that the issue of efficiency loss is even more severe for @xmath36 .",
    "so the development of unconstrained bandwidth selectors for density derivative estimation , which is achieved in this paper , may also represent an important improvement in practice .    to measure the error committed by the kernel estimator for the sample at hand it is natural to consider the integrated squared error ( ise ) , defined as @xmath37 where @xmath38 denotes the usual euclidean norm .",
    "this quantity depends on the data , so it is common to consider the mean integrated squared error @xmath39 $ ] as a non - stochastic measure of error , and its minimizer @xmath40 as the non - stochastic optimal bandwidth choice . a more detailed discussion of the advantages and disadvantages of the ise and mise viewpoints can be found in @xcite .",
    "standard calculations lead to the integrated variance plus integrated squared bias decomposition @xmath41 , where @xmath42d{{\\boldsymbol x}}$ ] and @xmath43-{\\mathsf{d}}^{\\otimes r}f({{\\boldsymbol x}})\\|^2d{{\\boldsymbol x}}$ ] . by expanding each of these two terms",
    ", @xcite showed that a more explicit form of the mise is given by @xmath44 where @xmath45 , and @xmath46 for a vector valued function @xmath47 and a real valued function @xmath48 , with @xmath49 standing for a component - wise application of the convolution operator .",
    "moreover , writing @xmath50 , under some smoothness assumptions @xcite also showed that an asymptotic approximation of the mise is given by @xmath51 and that the minimizer of this amise function , @xmath52 , has entries of order @xmath53 , leading to a minimum achievable amise of order @xmath54 .",
    "although these expressions provide an insightful error analysis of multivariate kernel density derivative estimators , they are not directly implementable as software since they all involve the unknown density @xmath2 . in the next section ,",
    "we examine strategies to estimate these unknown quantities which lead to optimal data - based selectors for density derivative estimation .",
    "in this section we propose three new methods to select the bandwidth matrix for kernel density derivative estimation from the data and study their asymptotic properties .",
    "these methods are inspired by the cross validation , plug - in and smoothed cross validation methodologies for the estimation of the density in the univariate case , hence their names .",
    "cross validation ( cv ) techniques for bandwidth selection for univariate density estimation were introduced in @xcite and @xcite , and studied in detail in seminal papers like @xcite , @xcite and @xcite .",
    "they can be motivated in terms of either ise - oriented or mise - oriented considerations .    in the case of multivariate density derivative estimation notice that , for a random variable @xmath55 having density @xmath2 and independent of @xmath19 , using integration by parts it is possible to write @xmath56\\bigg\\}\\\\ & \\quad+\\operatorname{tr}{\\mathbf{r}}({\\mathsf{d}}^{\\otimes r}f)\\end{aligned}\\ ] ] provided that the kernel @xmath21 is sufficiently smooth .",
    "the last term is irrelevant for minimizing concerns , and the two first terms can be unbiasedly estimated by @xmath57^{-1}\\sum_{i\\neq j}{\\mathsf{d}}^{\\otimes 2r}k_{{\\bf h}}({{\\bf x}}_i-{{\\bf x}}_j)\\bigg\\}\\end{aligned}\\ ] ] where @xmath58 denotes the kernel estimator based on the sample with the @xmath59-th observation deleted .    from the mise point of view notice that , for any smooth enough function @xmath60 , @xmath61,\\end{aligned}\\ ] ] so that @xmath62 can be unbiasedly estimated by @xmath63^{-1}{\\operatorname{vec}}^\\top{{\\bf i}}_{d^r}\\sum_{i\\neq j}{\\mathsf{d}}^{\\otimes 2r}l_{{\\bf h}}({{\\bf x}}_i-{{\\bf x}}_j).\\ ] ] therefore , in view of ( [ exactmise ] ) , @xmath64 can be unbiasedly estimated by @xmath65^{-1}{\\operatorname{vec}}^\\top{{\\bf i}}_{d^r}\\sum_{i\\neq j}^n\\big\\{(1-n^{-1}){\\mathsf{d}}^{\\otimes 2r}\\bar{k}_{{\\bf h}}-2{\\mathsf{d}}^{\\otimes 2r}k_{{\\bf h}}\\big\\}({{\\bf x}}_i-{{\\bf x}}_j),\\end{aligned}\\ ] ] where @xmath66 . to check that these two cv expressions coincide ,",
    "take into account that @xmath67 , so that using some properties of the kronecker product and the vec operator ( see , e.g. , * ? ? ?",
    "* ) , the sum of the diagonal terms in the first @xmath68 expression equals @xmath69 where the third line follows by noting that @xmath70 .",
    "surely the simplest formulation for cv ( useful for implementation purposes ) is @xmath71^{-1}\\sum_{i\\neq j}{\\mathsf{d}}^{\\otimes 2r}k\\big({{\\bf h}}^{-1/2}({{\\bf x}}_i-{{\\bf x}}_j)\\big)\\bigg\\}.\\end{aligned}\\ ] ] we denote by @xmath72 the bandwidth matrix in @xmath73 which minimizes @xmath74 .",
    "plug - in ( pi ) bandwidth selection techniques are based on estimating the unknown quantities that appear in an asymptotic error formula and minimizing the resulting empirical criterion .",
    "basic plug - in selectors for univariate density estimation are described in @xcite and @xcite . in the multivariate case , introducing the vector integrated density derivative functional , defined as @xmath75 allows us to rewrite the amise formula ( [ amise ] ) for the @xmath1-th derivative as @xmath76 thus , the plug - in bandwidth selector @xmath77 is defined to be the bandwidth in @xmath78 minimizing the criterion @xmath79 where @xmath80 is a suitable estimator of @xmath81 .",
    "@xcite analyzed the problem of estimating @xmath82 for an arbitrary @xmath1 . noting that @xmath83 $ ] , they proposed the kernel estimator @xmath84 using a kernel @xmath60 with pilot bandwidth @xmath85 , possibly different from @xmath21 and @xmath22 . for the selection of the pilot bandwidth matrix @xmath85 , the same authors showed that the leading term of the mean squared error @xmath86 $ ] is given by the squared norm of the asymptotic bias vector @xmath87 so that the asymptotically optimal choice of the pilot bandwidth for the estimation of @xmath82 is @xmath88 , which depends on @xmath89 .    hence , to select the pilot bandwidth @xmath85 from the data we could substitute @xmath89 by another kernel estimator in ( [ eq : omega_pi ] ) and minimize the squared norm of the resulting vector , but of course then the optimal bandwidth for the kernel estimator of @xmath90 depends on @xmath81 , and so on .",
    "the usual strategy to overcome this problem is to use an @xmath91-stage algorithm as described in @xcite , involving @xmath91 successive kernel functional estimations with the initial bandwidth matrix chosen on the basis of a normal scale approximation .",
    "the resulting bandwidth obtained by minimizing @xmath92 when @xmath81 is estimated using an @xmath91-stage algorithm is called an @xmath91-stage plug - in bandwidth selector for the @xmath1-th derivative .",
    "the smoothed cross validation ( scv ) methodology for univariate density estimation was introduced in @xcite , and a thorough study of this technique was made in @xcite .",
    "however , it has not been until recently that its multivariate counterpart has been developed , in @xcite and @xcite , and its use for univariate density derivative estimation has been explored ( see * ? ? ? * ) .",
    "a possible derivation of the scv criterion for the problem of multivariate density derivative estimation is based on the approximation of the mise obtained by replacing the exact integrated variance in equation ( [ exactmise ] ) by its asymptotic approximation ( the first term ) , while keeping the exact form for the integrated squared bias , so that @xmath93 with @xmath94 where @xmath95 ( here @xmath96 denotes the dirac delta function ) and @xmath97",
    ". the scv criterion is obtained by replacing the unknown target @xmath98 in the mise2 formula with a pilot estimator @xmath99 , leading to @xmath100 where @xmath101 . when all the @xmath102 are distinct and the diagonal terms ( @xmath103 ) are omitted in the previous sum it can be shown , using the properties of the dirac delta function ( see , e.g. , * ? ? ?",
    "* chapter i.2 ) , that the scv criterion coincides with the cv criterion for @xmath104 .",
    "the minimizer of @xmath105 is defined to be @xmath106 .",
    "its value depends on the pilot selector @xmath85 .",
    "@xcite showed that in the case @xmath30 the leading term of the mean squared error @xmath107 is given by the squared norm @xmath108 where @xmath109 is the same as the aforementioned @xmath110 except that @xmath60 is replaced by @xmath111 .",
    "thus it is straightforward to define , analogously to the plug - in algorithm , the required optimal @xmath112-th stage pilot bandwidth of an @xmath91-stage scv selector .",
    "let @xmath113 be an arbitrary data - based bandwidth selector , built up on the basis of an estimated criterion @xmath114 .",
    "following @xcite , @xmath115 is said to converge to @xmath116 at relative rate @xmath117 if @xmath118 where @xmath119 denotes element - wise order in probability and @xmath120 is the @xmath121 matrix of ones .",
    "this order in probability statement can be difficult to derive directly .",
    "the next lemma provides a more tractable indirect method of calculating convergence rates .",
    "[ lem : asymhr ] suppose that assumptions ( a1)(a3 ) given in the appendix hold .",
    "the discrepancy @xmath122 is asymptotically equivalent to @xmath123({{\\bf h}}_{\\mise , r})$ ] , where @xmath124 is shorthand for @xmath125 .",
    "furthermore , the relative rate of convergence of @xmath126 is @xmath117 if @xmath127({{\\bf h}}_{\\mise , r } ) \\rbrace \\operatorname{\\boldsymbol{\\mathbb{e}}}\\lbrace { \\mathsf{d}}_{{\\bf h}}[\\widehat{\\mathrm{mise}}_r-\\mathrm{mise}_r]({{\\bf h}}_{\\mise , r } ) \\rbrace^\\top \\\\ & + \\operatorname{var}\\lbrace { \\mathsf{d}}_{{\\bf h}}[\\widehat{\\mathrm{mise}}_r-\\mathrm{mise}_r]({{\\bf h}}_{\\mise , r } ) \\rbrace = o(n^{-2\\alpha } { \\mathbf{j}}_{d^2 } ) { \\operatorname{vec}}{{\\bf h}}_{\\mise , r }   { \\operatorname{vec}^\\top}{{\\bf h}}_{\\mise , r}.\\end{aligned}\\ ] ]    the convergence rates of the three bandwidth selectors considered here are given in the following theorem , whose proof is deferred to the appendix .",
    "[ reroc ] suppose that assumptions ( a1)(a5 ) given in the appendix hold .",
    "the relative rate of convergence to @xmath116 is @xmath128 for the cross validation selector @xmath129 , and @xmath130 for the plug - in selector @xmath131 and the smoothed cross validation selector @xmath106 when @xmath132 .",
    "@xcite computed the relative rate of convergence for the cv and pi selectors for the estimation of a single partial derivative , using a single - parameter bandwidth matrix ( i.e. , @xmath133 ) .",
    "the previous theorem shows that the unconstrained cv bandwidth attains the same rate as its constrained counterpart , yet with added flexibility that should be captured in the constant coefficient of the asymptotic expression , although the computation of an explicit form for this coefficient does not seem possible in general .",
    "the convergence rate of the pi selector is @xmath134 within the single - parameter bandwidth class @xmath135 , yielding a slightly faster convergence to the optimal constrained bandwidth .",
    "as explained in @xcite for the density case , this is due to the fact that the very special cancellation in the bias term which is achievable when using a single - parameter bandwidth is not possible in general for the unconstrained estimator .",
    "nevertheless , the aforementioned papers showed that this slight loss in convergence rate terms is negligible in practice as compared with the fact that the targeted constrained optimal bandwidth is usually much less efficient than the unconstrained one ( see also section [ sec:5 ] below ) .",
    "theorem [ reroc ] also shows that the similarities noted in @xcite about the asymptotic properties of the pi and scv methods for the density estimation problem persist for @xmath136 , since both selectors exhibit the same relative rate of convergence .",
    "@xcite exemplified how slow is the rate @xmath137 of the cv selector for @xmath138 , @xmath30 by noting that @xmath0 has to be as large as @xmath139 so that @xmath140 . in the same spirit , to compare the rates obtained in theorem [ reroc ] , table [ rates - ratio ] shows the values of @xmath128 ( cv ) and @xmath130 ( pi and scv ) divided by @xmath141 , that is the rate for the cv selector @xmath142 which is used as a base case , for all the different combinations of @xmath143 , @xmath144 and @xmath145 .",
    "ratios which are lower than 1 indicate the rate is faster than the base case , and ratios greater than 1 a slower rate . for @xmath146 ,",
    "these ratios in table [ rates - ratio ] tend to be greater than 1 , indicating that using this sample size will lead to a deteriorating convergence rate . on the other hand for the larger sample sizes , @xmath147 , these ratios tend to be less than 1 .",
    "this implies that convergence rates better than the cv selector for bivariate density estimation can be attained , even with higher dimensions and higher order derivatives , provided that sufficiently large ( although still realistic ) sample sizes are used .",
    "of course this comparison only takes into account the asymptotic order of the convergence rates by ignoring the associated coefficients since explicit formulas for the latter are not available for @xmath148 .",
    "the finite sample behaviour of the bivariate case for moderate sample sizes is examined more closely in the next section .",
    ".comparison of the relative rate of convergence for the cv , pi and scv selectors . for each combination of @xmath1 , @xmath0 and @xmath3 in the table ,",
    "the left entry in the corresponding cell shows @xmath128 ( cv selector ) and the right entry @xmath130 ( pi and scv selectors ) divided by @xmath141 ( i.e. the rate for the cv selector with @xmath142 ) . [ cols=\"<,<,^,^,^,^,^,^,^,^ \" , ]     in view of table [ tab : ari ] , none of the methods compared is uniformly the best . in the group of the mean shift procedures",
    ", the use of the pi bandwidth seems to exhibit the best overall performance .",
    "the cv choice can be rated second best , with similar or even slightly ( but not significantly ) better average ari in some cases .",
    "the scv bandwidth shows an unexpectedly inferior performance for the normal mixture models , but it has an acceptable behaviour for the models with non - standard cluster shapes",
    ". finally , the normal scale rule nr is clearly inferior in four out of the five models , but it performs surprisingly well for the broken ring model ; since it is the least intensive method in computational terms , it could be useful at least to provide a quick initial analysis , especially in higher dimensions .    the comparison with the parametric method mclust followed the expected guidelines : for the normal mixture models mclust showed good results , especially for the difficult quadrimodal density , but it seems unable to adapt itself to the non - standard cluster shape situations .",
    "on the contrary , clues is not very powerful for a standard setup with ellipsoidal clusters , but seems to performs reasonably well for non - standard problems .",
    "finally , pdfc shows remarkable results in the simulation study , in spite of the ad hoc choice of the bandwidth in which it is based , and its performance is comparable to that of the best mean shift procedure , with the only exception of the eye model .",
    "surely a more careful study of the bandwidth selection problem would improve the quality of the pdfc method further .",
    "the mean shift algorithm in conjunction with the new proposed bandwidth selection rules was also applied to some real data sets .",
    "it is well - known that the kernel density estimator tends to produce spurious bumps ( i.e. , unimportant modes caused by a single observation ) in the tails of the distribution , and that this problem seems enhanced in higher dimensions , due to the empty space phenomenon and the curse of dimensionality ( see , for instance , * ? ? ?",
    "* chapter 4 ) . for real data sets",
    ", this may result in a number of data points forming singleton clusters after applying the mean shift algorithm .",
    "furthermore , in some applications the researcher may be interested in forming more homogeneous groups so that , say , insignificant groups of size less than @xmath149 of the biggest group are not allowed in the outcome of the clustering algorithm .",
    "this goal can be achieved as follows : apply the mean shift algorithm to the whole data set and identify all the data points forming groups of size less than @xmath149 of the biggest group , then leave those singular data points out of the estimation process in the mean shift algorithm and re - compute the data - based bandwidth and the density and density gradient estimators in ( [ eq : meanshift ] ) using only non - singular data points . since the mean shift algorithm produces a partition of the whole space , these left - out data points can be naturally assigned to any of the corresponding newly obtained clusters .",
    "if this new assignment again contains insignificant clusters then iterate the process until the eventual partition satisfies the desired requirements .",
    "this correction is similar ( although a little different ) to the stage called  merging clusters based on the coverage rate \" in @xcite , and will be referred henceforth as _",
    "correction for insignificant groups_.      the _ e.  coli _ data set is provided by the uci machine learning database repository @xcite .",
    "the original data were contributed by kenta nakai at the institute of molecular and cellular biology of osaka university .",
    "the data represent seven features calculated from the amino acid sequences of @xmath150 e.coli proteins , classified in eight classes according to their localization sites , labeled iml ( 2 observations ) , oml ( 5 ) , ims ( 2 ) , om ( 20 ) , pp ( 52 ) , imu ( 35 ) , i m ( 77 ) , cp ( 143 ) . a more detailed description of this data set can be found in @xcite . since two of the original seven features are binary variables , only the remaining five continuous variables ( @xmath151 ) , scaled to have unit variance , were retained for the cluster analysis .",
    "the number of groups identified by the mean shift procedure with correction for insignificant groups ( using @xmath152 as a default ) was 5 for pi and scv bandwidths , which is the natural choice if the insignificant clusters iml , oml and ims are merged into bigger groups .",
    "the mean shift algorithm found 6 groups using the nr bandwidth and 7 with the cv bandwidth . since in this example",
    "the true cluster membership is available from the original data , it is also possible to compare the performance of the methods using the ari .",
    "the aris for these configurations were 0.63 ( nr bandwidth ) , 0.671 ( cv ) , 0.667 ( pi ) and 0.559 ( scv ) .",
    "in contrast , clues and pdfc indicated a severely underestimated number of groups in the data , namely 3 and 2 , respectively , and whereas clues obtains a remarkably high ari anyway ( 0.697 ) , the performance of pdfc is poor for this data set in ari terms ( 0.386 ) .",
    "mclust also gives a reasonable answer , with 6 groups and an ari of 0.642 .",
    "these data were introduced in @xcite , and consist of eight chemical measurements on @xmath153 olive oil samples from three regions of italy .",
    "the three regions r1 , r2 and r3 are further divided into nine areas , with areas a1 ( 25 observations ) , a2 ( 56 ) , a3 ( 206 ) and a4 ( 36 ) in region r1 ( totalling 323 observations ) ; areas a5 ( 65 ) and a6 ( 33 ) in region r2 ( totalling 98 ) ; and areas a7 ( 50 ) , a8 ( 50 ) and a9 ( 51 ) in region r3 ( totalling 151 ) .",
    "detailed cluster analyses of this data set are given in @xcite and @xcite .",
    "taking into account the compositional nature of these data , they were transformed following the guidelines in the latter reference , first dealing with the effect of rounding zeroes when the chemical measurement was below the instrument sensitivity level and then applying the additive log - ratio transform to place the data in a 7-dimensional euclidean space ( see * ? ? ? * for a recent monograph on compositional data ) .",
    "then , cluster analysis was carried out over the first five principal components of the scaled euclidean variables .",
    "the results of the analysis indicated that whereas some methods seemed to target the partition of the data into major regions , others tried hard to discover the sub - structure of areas .",
    "this was clearly recognized when the aris of the groupings were computed either with respect to one classification or the other . naturally , if a method produced a grouping which was accurate with respect to major regions , it had lower ari with respect to the division into areas .",
    "clues , pdfc and the mean shift algorithm using the nr bandwidth clearly favoured grouping the data into major categories .",
    "the pdfc method obtained a remarkable ari of 0.841 by clustering the data into 3 groups , whereas clues only found 2 groups resulting in an ari of 0.680 . using the nr bandwidth the mean shift algorithm achieved an ari of 0.920 with respect to the true grouping into major regions",
    "; it correctly identified all the data points in regions r1 and r2 , although region r3 appeared divided into several subregions .",
    "in contrast , mclust and the mean shift algorithm combined with all the more sophisticated bandwidth selectors tended to produce groupings closer to the assignment into smaller areas .",
    "mclust showed the existence of 8 groups and achieved an ari of 0.739 with respect to the true distribution into areas .",
    "the mean shift analyses with the cv , pi and scv bandwidths all found 7 groups , leading to aris of 0.741 ( cv bandwidth ) , 0.791 ( pi ) and 0.782 ( scv ) .",
    "it is not always easy to interpret visually estimates of multivariate derivatives . to assist us",
    ", we use the significant negative curvature regions of @xcite , defined as the set containing the values of @xmath154 such that the null hypothesis that the hessian @xmath155 is positive definite is significantly rejected .",
    "the appropriate kernel test statistic , null distribution and adjustment for multiple testing is outlined in @xcite and implemented in the ` feature ` library in ` r ` .",
    "significant negative curvature regions corresponds to a modal region in the density function , and hence a local maxima in data density .",
    "these authors focused on the scale space approach of smoothing and so did not develop optimal bandwidth selectors for their density derivative estimates .    here , we compare the significant curvature regions obtained using a usual @xmath30 bandwidth selector to those with an @xmath156 optimal bandwidth in figure  [ fig : earthquake ] on the earthquake data from @xcite .",
    "the recorded measurements are the latitude and longitude ( in degrees ) and depth ( in km ) of epicenters of 510 earthquakes . here , negative latitude indicates west of the international date line , and negative depth indicates distances below the earth s surface .",
    "the depth is transformed using log(depth ) .",
    "for these transformed data , we use pi selectors @xmath157 and @xmath158 and scv selectors @xmath159 and @xmath160 .",
    "as expected from asymptotic theory , bandwidths for hessian estimation are larger in magnitude than bandwidths for density estimation .",
    "moreover only the central modal region is present using @xmath157 , whereas with @xmath158 , the three local modal regions are more clearly delimited from the surrounding space , confirming the three modes obtained with subjective bandwidth selection by @xcite .    .",
    "( upper right ) plug - in selector @xmath156 .",
    "( lower left ) scv selector @xmath30 .",
    "( lower right ) scv selector @xmath156 .",
    "the significant curvature regions or modal regions are more clearly delimited from the surrounding scatter point cloud with the selectors corresponding to second derivative.,title=\"fig:\",scaledwidth=50.0% ] .",
    "( upper right ) plug - in selector @xmath156 .",
    "( lower left ) scv selector @xmath30 .",
    "( lower right ) scv selector @xmath156 .",
    "the significant curvature regions or modal regions are more clearly delimited from the surrounding scatter point cloud with the selectors corresponding to second derivative.,title=\"fig:\",scaledwidth=50.0% ] + .",
    "( upper right ) plug - in selector @xmath156 .",
    "( lower left ) scv selector @xmath30 .",
    "( lower right ) scv selector @xmath156 .",
    "the significant curvature regions or modal regions are more clearly delimited from the surrounding scatter point cloud with the selectors corresponding to second derivative.,title=\"fig:\",scaledwidth=50.0% ] .",
    "( upper right ) plug - in selector @xmath156 .",
    "( lower left ) scv selector @xmath30 .",
    "( lower right ) scv selector @xmath156 .",
    "the significant curvature regions or modal regions are more clearly delimited from the surrounding scatter point cloud with the selectors corresponding to second derivative.,title=\"fig:\",scaledwidth=50.0% ] +    * acknowledgments . *",
    "grant mtm2010 - 16660 ( both authors ) from the spanish ministerio de ciencia e innovacin , and various fellowships ( second author ) from the institut curie , france , and the institute of translational sciences , france have supported this work .",
    "henceforth the following assumptions are made :    1 .",
    "@xmath21 is a symmetric @xmath3-variate density such that @xmath161 and all its partial derivatives up to order @xmath162 are bounded , continuous and square integrable .",
    "2 .   @xmath2 is a density function with all its partial derivatives up to order @xmath163 bounded , continuous and square integrable .",
    "3 .   @xmath164 is a sequence of bandwidth matrices such that all entries of @xmath165 and @xmath22 tend to zero as @xmath166 .",
    "these do not form a minimal set of assumptions , but they serve as useful starting point for the results that we subsequently develop . besides , in this section integrals without any integration limits are assumed to be integrated over the appropriate euclidean space .",
    "we also assume that suitable regularity conditions are satisfied so that the exchange of term - by - term integration and differentiation of taylor expansions are well - defined .",
    "reasoning as in lemma  1 in @xcite , it follows that @xmath122 is asymptotically equivalent to @xmath167^{-1}{\\mathsf{d}}_{{\\bf h}}[\\widehat{\\mathrm{mise}}_r-\\mathrm{mise}_r]({{\\bf h}}_{\\mise , r})$ ] , where @xmath168 denotes the hessian operator corresponding to @xmath124 .",
    "therefore , it suffices to show that @xmath169 .",
    "but for any @xmath22 with entries of order @xmath53 , as @xmath116 , the results in @xcite imply that @xmath170 is equivalent to @xmath171 and , moreover , the smoothness assumptions ensure that @xmath172 is of the same order as @xmath173 . and",
    "it is not hard to show that for the asymptotic integrated squared bias term we have @xmath174 and similarly for the asymptotic integrated variance , thus finishing the proof .",
    "lemma  [ lem : asymhr ] shows that @xmath175 is asymptotically equivalent to @xmath176({{\\bf h}}_{\\mise , r})$ ] . since @xmath177=\\mise_r({{\\bf h}})-\\operatorname{tr}{\\mathbf{r}}({\\mathsf{d}}^{\\otimes r}f)$ ] for all @xmath22",
    ", it follows that the order of @xmath175 is given by the ( root ) order of @xmath178({{\\bf h}}_{\\mise , r})\\big\\}\\\\ & \\sim\\operatorname{var}\\bigg\\{[n(n-1)]^{-1}\\sum_{i\\neq j}^n{\\mathsf{d}}_{{\\bf h}}\\big[{\\operatorname{vec}}^\\top({{\\bf h}}^{-1})^{\\otimes r}({\\mathsf{d}}^{\\otimes 2r}\\tilde k)_{{\\bf h}}({{\\bf x}}_i-{{\\bf x}}_j)\\big]\\big\\vert_{{{\\bf h}}={{\\bf h}}_{\\mise , r}}\\bigg\\},\\end{aligned}\\ ] ] where @xmath179 .",
    "so denoting @xmath180 $ ] , by standard @xmath181-statistics theory the previous variance is of the same order as @xmath182 where @xmath183\\\\ { \\mathbf{\\xi}}_2&=\\operatorname{\\boldsymbol{\\mathbb{e}}}[\\boldsymbol{\\varphi}_{{\\bf h}}({{\\bf x}}_1-{{\\bf x}}_2)\\boldsymbol{\\varphi}_{{\\bf h}}({{\\bf x}}_1-{{\\bf x}}_2)^\\top]\\\\ { \\mathbf{\\xi}}_0&=\\operatorname{\\boldsymbol{\\mathbb{e}}}[\\boldsymbol{\\varphi}_{{\\bf h}}({{\\bf x}}_1-{{\\bf x}}_2)]\\operatorname{\\boldsymbol{\\mathbb{e}}}[\\boldsymbol{\\varphi}_{{\\bf h}}({{\\bf x}}_1-{{\\bf x}}_2)]^\\top\\end{aligned}\\ ] ] with @xmath22 of the order of @xmath116 , namely having all its entries of order @xmath53",
    ". the following lemma provides an explicit expression for the function @xmath184 that will be helpful to evaluate @xmath185 .    [",
    "lem : varphi ] the function @xmath184 can be explicitly expressed as @xmath186 where the function @xmath187 is given by @xmath188 and the matrices @xmath189 , @xmath190 are defined as @xmath191\\end{aligned}\\ ] ] where we understand that @xmath192 .    since @xmath193",
    ", its differential is decomposed into three terms @xmath194 from @xcite , the differentials involved in the first two terms can be expressed as @xmath195 d { \\operatorname{vec}}{{\\bf h}},\\end{aligned}\\ ] ] where @xmath196 is a matrix such that @xmath197 .",
    "for the third term , @xmath198 since @xmath199{\\operatorname{vec}}{{\\bf h}}^{\\otimes -r}={\\operatorname{vec}}\\big({{\\bf i}}_d[{\\mathsf{d}}({\\mathsf{d}}^\\top)^{\\otimes 2r}]{\\operatorname{vec}}{{\\bf h}}^{\\otimes -r}\\big)=({\\operatorname{vec}^\\top}{{\\bf h}}^{\\otimes - r}\\otimes{{\\bf i}}_d){\\mathsf{d}}^{\\otimes 2r+1}$ ] . finally , using @xmath200 from @xcite , it follows that @xmath201 .",
    "thus the derivative reads @xmath202 the central factors of the third term on the right hand side can be rewritten as @xmath203\\\\ & = [ { \\operatorname{vec}^\\top}{{\\bf h}}^{\\otimes -r } \\otimes   ( { { \\bf h}}^{1/2 } \\otimes { { \\bf h}}^{1/2 } + { { \\bf i}}_d \\otimes { { \\bf h}})^{-1 } ] ( { { \\bf i}}_{d^{2r } } \\otimes { { \\bf h}}^{-1/2}{{\\boldsymbol x}}\\otimes { { \\bf i}}_d),\\end{aligned}\\ ] ] as desired .",
    "we now return to the task of finding the asymptotic order of @xmath204 , @xmath205 and @xmath206 . for that , some preliminary notation is needed . for any real function @xmath48",
    "we denote its vector moment of order @xmath207 as @xmath208 .",
    "for instance , @xcite showed that @xmath209 , @xmath210 and @xmath211 , where @xmath212 denotes the symmetrizer matrix of order @xmath1 ( see * ? ? ? * ) , defined as the ( only ) matrix such that pre - multiplying a kronecker product of any @xmath1 vectors in @xmath213 by @xmath214 results in the average of all possible permutations of the @xmath1-fold product .",
    "we also introduce here the notation @xmath215 for the commutation matrix of order @xmath216 @xcite .    so taking this into account , for the calculation of the asymptotic order of @xmath204 , a fourth order taylor expansion of @xmath217 , in the form of ( * ? ? ?",
    "* theorem 1.4.8 ) or @xcite , gives @xmath218{\\mathsf{d}}^{\\otimes 2r+p}f({{\\boldsymbol x}})d{{\\boldsymbol z}}\\\\ & = ( { { \\bf h}}^{1/2})^{\\otimes 2r}\\sum_{p=0}^4\\frac{(-1)^p}{p!}\\big[{{\\bf i}}_{d^{2r}}\\otimes({{\\boldsymbol \\mu}}_p(\\tilde k)^\\top ( { { \\bf h}}^{1/2})^{\\otimes p})\\big]{\\mathsf{d}}^{\\otimes 2r+p}f({{\\boldsymbol x}})\\\\ & = -({{\\bf h}}^{1/2})^{\\otimes 2r}{\\mathsf{d}}^{\\otimes 2r}f({{\\boldsymbol x}})+\\tfrac{1}{4}m_2(k)^2({{\\bf h}}^{1/2})^{\\otimes 2r}\\big[{{\\bf i}}_{d^{2r}}\\otimes((\\operatorname{vec}^\\top   { { \\bf h}})^{\\otimes 2}\\boldsymbol{\\mathcal s}_{d,4})\\big]{\\mathsf{d}}^{\\otimes",
    "2r+4}f({{\\boldsymbol x}})\\\\ & = -({{\\bf h}}^{1/2})^{\\otimes 2r}{\\mathsf{d}}^{\\otimes 2r}f({{\\boldsymbol x}})+\\tfrac{1}{4}m_2(k)^2\\big[({{\\bf h}}^{1/2})^{\\otimes 2r}\\otimes(\\operatorname{vec}^\\top   { { \\bf h}})^{\\otimes 2}\\big]{\\mathsf{d}}^{\\otimes 2r+4}f({{\\boldsymbol x}}),\\end{aligned}\\ ] ] therefore , since @xmath219 and @xmath220 , we obtain @xmath221\\nonumber\\\\ & \\sim{\\mathsf{d}}_{{\\bf h}}\\big\\{\\tfrac{1}{4}m_2(k)^2\\big[{\\operatorname{vec}}^\\top { { \\bf i}}_{d^r}\\otimes(\\operatorname{vec}^\\top   { { \\bf h}})^{\\otimes 2}\\big]{\\mathsf{d}}^{\\otimes 2r+4}f({{\\boldsymbol x}})\\big\\}\\nonumber\\\\ & = \\tfrac{1}{4}m_2(k)^2\\big({\\operatorname{vec}}^\\top { { \\bf i}}_{d^r}\\otimes{{\\bf i}}_{d^2}\\otimes{\\operatorname{vec}}^\\top { { \\bf h}}\\big)[{{\\bf i}}_{d^{2r}}\\otimes({{\\bf i}}_{d^2}+{\\mathbf{k}}_{d^2,d^2})]{\\mathsf{d}}^{\\otimes 2r+4}f({{\\boldsymbol x}})\\nonumber\\\\ & = \\tfrac{1}{2}m_2(k)^2\\big({\\operatorname{vec}}^\\top { { \\bf i}}_{d^r}\\otimes{{\\bf i}}_{d^2}\\otimes{\\operatorname{vec}}^\\top { { \\bf h}}\\big){\\mathsf{d}}^{\\otimes 2r+4}f({{\\boldsymbol x}}).\\label{phihf}\\end{aligned}\\ ] ] using this , @xmath222&=\\int\\boldsymbol\\varphi_{{\\bf h}}*f({{\\boldsymbol x}})f({{\\boldsymbol x}})d{{\\boldsymbol x}}\\sim\\tfrac{1}{2}m_2(k)^2\\big({\\operatorname{vec}}^\\top { { \\bf i}}_{d^r}\\otimes{{\\bf i}}_{d^2}\\otimes{\\operatorname{vec}}^\\top { { \\bf h}}\\big)\\boldsymbol\\psi_{2r+4}\\end{aligned}\\ ] ] and @xmath223    similarly , @xmath224 finally , note that @xmath225 $ ] , where @xmath226 , which also depends on @xmath22 through @xmath27 and @xmath227 . besides , @xmath228=\\iint(\\boldsymbol\\varphi\\boldsymbol\\varphi^\\top)({{\\boldsymbol z}})f({{\\boldsymbol y}})f({{\\boldsymbol y}}+{{\\bf h}}^{1/2}{{\\boldsymbol z } } ) d{{\\boldsymbol y}}d{{\\boldsymbol z}}\\sim r(f)\\int\\boldsymbol\\varphi({{\\boldsymbol z}})\\boldsymbol\\varphi({{\\boldsymbol z}})^\\top d{{\\boldsymbol z}}\\ ] ] which , in view of lemma  [ lem : varphi ] , leads to @xmath229 .          1 .",
    "@xmath60 is a symmetric @xmath3-variate density such that @xmath232 and all its partial derivatives up to order @xmath233 are bounded , continuous and square integrable .",
    "2 .   @xmath234 is a sequence of bandwidth matrices such that all entries of @xmath235 and @xmath85 tend to zero as @xmath166 .    to make use of lemma [ lem : asymhr ] once more ,",
    "notice that the difference between the mise and its estimate is @xmath236 so taking into account @xmath220 again , we come to @xmath237&\\sim(-1)^r \\tfrac{m_2(k)^2}{2}(\\operatorname{vec}^\\top { { \\bf i}}_{d^r}\\otimes { { \\bf i}}_{d^2 } \\otimes \\operatorname{vec}^\\top { { \\bf h}})(\\hat{\\boldsymbol{\\psi}}_{2r+4}({{\\bf g } } ) - \\boldsymbol{\\psi}_{2r+4}),\\end{aligned}\\ ] ] so that the performance of @xmath238 is determined by the performance of @xmath239 as an estimator of @xmath240",
    ".    from theorem 2 in @xcite the optimal pilot bandwidth @xmath85 for the estimator @xmath239 is of order @xmath130 , leading to @xmath241=o(n^{-4/(d+2r+6)})$ ] , and then @xmath242=o_p(n^{-2/(d+2r+6)}{\\mathbf{j}}_{d^2})\\operatorname{vec}{{\\bf h}}.$ ] so finally we arrive to @xmath243 by applying lemma  [ lem : asymhr ] .      as in @xcite , it can be shown that the function @xmath244 can be replaced for @xmath245 everywhere in the asymptotic analysis , since the difference between their respective minimizers is of relative order faster than @xmath246 , which is the fastest attainable rate in bandwidth selection @xcite .",
    "so to apply lemma [ lem : asymhr ] it is also possible consider @xmath245 instead of @xmath244 , hence we focus on analyzing the difference @xmath247 at @xmath22 of the same order as @xmath116 . to begin with ,",
    "note that using a fourth order taylor expansion of @xmath248 results in @xmath249   { \\mathsf{d}}^{\\otimes 2r+p } \\bar{l}({{\\bf g}}^{-1/2}{{\\boldsymbol x } } ) \\ , d{{\\boldsymbol z}}\\\\ & = \\tfrac{1}{4 } m_2(k)^2 |{{\\bf g}}|^{-1/2 } ( { { \\bf g}}^{-1/2})^{\\otimes 2r } [ { { \\bf i}}_{d^{2r } } \\otimes ( { \\operatorname{vec}^\\top}{{\\bf h}})^{\\otimes 2 } ( { { \\bf g}}^{-1/2})^{\\otimes 4 } ] { \\mathsf{d}}^{\\otimes 2r+4 } \\bar{l}({{\\bf g}}^{-1/2}{{\\boldsymbol x}})\\\\ & = \\tfrac{1}{4 } m_2(k)^2|{{\\bf g}}|^{-1/2 } [ { { \\bf i}}_{d^{2r } } \\otimes ( { \\operatorname{vec}^\\top}{{\\bf h}})^{\\otimes 2 } ] ( { { \\bf g}}^{-1/2})^{\\otimes ( 2r+4 ) } { \\mathsf{d}}^ { \\otimes 2r+4 } \\bar{l}({{\\bf g}}^{-1/2}{{\\boldsymbol x}})\\\\ & = \\tfrac{1}{4 } m_2(k)^2 [ { { \\bf i}}_{d^{2r } } \\otimes ( { \\operatorname{vec}^\\top}{{\\bf h}})^{\\otimes 2 } ]   { \\mathsf{d}}^ { \\otimes 2r+4 } \\bar{l}_{{\\bf g}}({{\\boldsymbol x}}),\\end{aligned}\\ ] ] where we have made use of the fact that @xmath250 and @xmath251 , and that the entries of @xmath252 tend to zero as a consequence of ( a3 ) and ( a5 ) .    this asymptotic approximation is then used to expand the terms in @xmath253= ( -1)^r { \\operatorname{vec}^\\top}{{\\bf i}}_{d^r } \\bigg\\ { n^{-1 } \\bar{\\delta}_{{\\bf h}}*{\\mathsf{d}}^{\\otimes 2r } \\bar{l}_{{\\bf g}}(0 ) \\\\ + ( 1-n^{-1})\\operatorname{\\boldsymbol{\\mathbb{e}}}\\big [   ( \\bar{\\delta}_{{\\bf h } } * { \\mathsf{d}}^{\\otimes 2r } \\bar{l}_{{\\bf g}})({{\\bf x}}_1 - { { \\bf x}}_2)\\big ] - \\int \\bar{\\delta}_{{\\bf h } } * { \\mathsf{d}}^{\\otimes 2r } f({{\\boldsymbol x } } ) f({{\\boldsymbol x } } ) \\ , d{{\\boldsymbol x}}\\bigg\\}.\\end{gathered}\\ ] ] precisely , for the first term we have @xmath254({{\\bf g}}^{-1/2})^{\\otimes 2r+4 } { \\mathsf{d}}^{\\otimes 2r+4 } \\bar{l}(0),\\end{aligned}\\ ] ] and for the second term @xmath255 \\\\ & \\sim \\tfrac{1}{4}m_2(k)^2[{{\\bf i}}_{d^{2r } } \\otimes ( { \\operatorname{vec}^\\top}{{\\bf h}})^{\\otimes 2 } ] \\iint { \\mathsf{d}}^{\\otimes 2r+4 } \\bar{l}_{{\\bf g}}({{\\boldsymbol x}}- { { \\boldsymbol y } } ) f({{\\boldsymbol x } } ) f({{\\boldsymbol y } } ) \\ , d{{\\boldsymbol x}}d{{\\boldsymbol y}}\\\\ & = \\tfrac{1}{4}m_2(k)^2 [ { { \\bf i}}_{d^{2r } } \\otimes ( { \\operatorname{vec}^\\top}{{\\bf h}})^{\\otimes 2 } ] \\iint   \\bar{l}_{{\\bf g}}({{\\boldsymbol x}}- { { \\boldsymbol y } } ) { \\mathsf{d}}^{\\otimes 2r+4}f({{\\boldsymbol x } } ) f({{\\boldsymbol y } } ) \\ , d{{\\boldsymbol x}}d{{\\boldsymbol y}}\\\\ & \\sim \\tfrac{1}{4}m_2(k)^2 [ { { \\bf i}}_{d^{2r } } \\otimes ( { \\operatorname{vec}^\\top}{{\\bf h}})^{\\otimes 2 } ] \\iint   \\bar{l } ( { { \\boldsymbol w } } ) \\sum_{p=0}^2 \\frac{(-1)^p}{p!}[{{\\bf i}}_{d^{2r+4 } } \\otimes ( { { \\boldsymbol w}}^\\top { { \\bf g}}^{1/2})^{\\otimes p } ] \\\\",
    "& \\quad \\times { \\mathsf{d}}^{\\otimes 2r+4+p } f({{\\boldsymbol y } } ) f({{\\boldsymbol y } } ) \\ , d{{\\boldsymbol w}}d{{\\boldsymbol y}}\\\\ & = \\tfrac{1}{4}m_2(k)^2 [ { { \\bf i}}_{d^{2r } } \\otimes ( { \\operatorname{vec}^\\top}{{\\bf h}})^{\\otimes 2 } ] \\sum_{p=0}^2   \\frac{(-1)^p}{p!}[{{\\bf i}}_{d^{2r+4 } } \\otimes \\{{{\\boldsymbol \\mu}}_p(\\bar{l})^\\top ( { { \\bf g}}^{1/2})^{\\otimes p}\\}]{{\\boldsymbol \\psi}}_{2r+4+p}\\\\ & = \\tfrac{1}{4}m_2(k)^2 [ { { \\bf i}}_{d^{2r } } \\otimes ( { \\operatorname{vec}^\\top}{{\\bf h}})^{\\otimes 2}]{{\\boldsymbol \\psi}}_{2r+4 } + \\tfrac{1}{4}m_2(k)^2 m_2(l)[{{\\bf i}}_{d^{2r } } \\otimes ( { \\operatorname{vec}^\\top}{{\\bf h}})^{\\otimes 2 } \\otimes { \\operatorname{vec}}^\\top { { \\bf g } } ] { { \\boldsymbol \\psi}}_{2r+6},\\end{aligned}\\ ] ] since @xmath256 and @xmath257 . finally , noting that @xmath258 and making use of the previously obtained expansion for @xmath259 , the third term is @xmath260 { { \\boldsymbol \\psi}}_{2r+4}.\\end{aligned}\\ ] ] thus",
    ", @xmath261 & \\sim \\tfrac{1}{4 } m_2(k)^2 n^{-1}|{{\\bf g}}|^{-1/2}[{\\operatorname{vec}}^\\top{{\\bf i}}_{d^r } \\otimes ( { \\operatorname{vec}^\\top}{{\\bf h}})^{\\otimes 2}]({{\\bf g}}^{-1/2})^{\\otimes 2r+4 } { \\mathsf{d}}^{\\otimes 2r+4 } \\bar{l}(0 ) \\\\ & \\quad +   \\tfrac{1}{4}m_2(k)^2 m_2(l)[{\\operatorname{vec}}^\\top{{\\bf i}}_{d^r } \\otimes ( { \\operatorname{vec}^\\top}{{\\bf h}})^{\\otimes 2 } \\otimes { \\operatorname{vec}}^\\top { { \\bf g } } ] { { \\boldsymbol \\psi}}_{2r+6}\\end{aligned}\\ ] ] calculations in section  [ sec:3 ] give @xmath85 is order @xmath262 , as for the plug - in selector , so substituting to this into the derivative of the previous equation yields @xmath263 \\ } & = o ( [ n^{-1 } |{{\\bf g}}|^{-1/2}(\\operatorname{tr}{{\\bf g}})^{-r-2 } + \\operatorname{tr}{{\\bf g } } ] { \\mathbf{j}}_{d^2 } ) { \\operatorname{vec}}{{\\bf h}}\\\\ & = o(n^{-2/(2r+d+6 ) } { \\mathbf{j}}_{d^2 } ) { \\operatorname{vec}}{{\\bf h}}.\\end{aligned}\\ ] ] lemma  [ lem : asymhr ] shows that @xmath264 is asymptotically equivalent to @xmath265({{\\bf h}}_{\\mise , r})$ ] .",
    "since it was stated in section  [ sec:3 ] that @xmath266 $ ] is dominated by its squared bias term , then @xmath267 .",
    "forina m. , armanino c. , lanteri s. and tiscornia e. ( 1983 ) classification of olive oils from their fatty acid composition .",
    "in : h. martens and h. j. russwurm ( eds . ) , _ food research and data analysis _ , applied science publishers , london , pp .",
    "189214 .",
    "horton , p. and nakai , k. ( 1996 ) a probabilistic classification system for predicting the cellular localization sites of proteins .",
    "proceedings of _ intelligent systems in molecular biology ( ismb-96 ) _ , 109115 .",
    "zeng , q.t . ,",
    "pratt , j.p .",
    ", pak , j. , ravnic , d. , huss , h. and mentzer , s.j .",
    "( 2007 ) feature - guided clustering of multi - dimensional flow cytometry datasets .",
    "_ journal of biomedical informatics _",
    ", * 40 * , 325331 ."
  ],
  "abstract_text": [
    "<S> important information concerning a multivariate data set , such as clusters and modal regions , is contained in the derivatives of the probability density function . despite this importance , nonparametric estimation of higher order derivatives of the density functions </S>",
    "<S> have received only relatively scant attention . </S>",
    "<S> kernel estimators of density functions are widely used as they exhibit excellent theoretical and practical properties , though their generalization to density derivatives has progressed more slowly due to the mathematical intractabilities encountered in the crucial problem of bandwidth ( or smoothing parameter ) selection . </S>",
    "<S> this paper presents the first fully automatic , data - based bandwidth selectors for multivariate kernel density derivative estimators . </S>",
    "<S> this is achieved by synthesizing recent advances in matrix analytic theory which allow mathematically and computationally tractable representations of higher order derivatives of multivariate vector valued functions . </S>",
    "<S> the theoretical asymptotic properties as well as the finite sample behaviour of the proposed selectors are studied . </S>",
    "<S> in addition , we explore in detail the applications of the new data - driven methods for two other statistical problems : clustering and bump hunting . </S>",
    "<S> the introduced techniques are combined with the mean shift algorithm to develop novel automatic , nonparametric clustering procedures which are shown to outperform mixture - model cluster analysis and other recent nonparametric approaches in practice . </S>",
    "<S> furthermore , the advantage of the use of smoothing parameters designed for density derivative estimation for feature significance analysis for bump hunting is illustrated with a real data example .    </S>",
    "<S> _ keywords : _ adjusted rand index , cross validation , feature significance , nonparametric kernel method , mean integrated squared error , mean shift algorithm , plug - in choice </S>"
  ]
}