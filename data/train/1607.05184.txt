{
  "article_text": [
    "the analysis of noisy data , for instance obtained from sensors monitoring a signal , is a well studied but still challenging problem .",
    "this especially applies in applications such as brain imaging based on magnetic resonance imaging , where increasing the precision of the physical measurement system , which is given by squids making use of quantum effects to record weak magnetic fields , is very expensive . in other applications",
    "the preservation of discontinuities in terms of their heights and/or their locations matters . to reduce the noise",
    "one may apply a denoising procedure such as a low pass filter , but this often leads to a blurred signal corresponding to a substantial loss of information .",
    "therefore , if discontinuities such as jumps are expected in the signal , one should apply jump - preserving procedures , which are often called edge - preserving in the literature due to their importance when processing two - dimensional image data .",
    "various approaches have been studied in the literature such as wavelets as studied by @xcite , point - wise adaptive approaches proposed by @xcite , kernel smoothing , see @xcite and @xcite for its application to image analysis , or jump - preserving regression , @xcite , to mention just a few works .",
    "a large amount of smoothing statistics aiming at denoising can be ( approximately ) written as weighted averages of the observations @xmath0 , @xmath1 , attaining values in the range @xmath2 , so that the definition of the weights attached to the data points @xmath3 , @xmath4 , of the corresponding scatterplot matters . in order to preserve discontinuities , methods such as the bilateral filter ,",
    "see @xcite , use weights which depend on both the ( geometric ) distance in the domain space @xmath5 ( horizontal weighting ) and the range space @xmath6 ( vertical weighting ) , whereas , for example , local polynomial estimators use weights only depending on the distance in the domain space .    in this article , we focus on a specification of the vertical weighting approach where the weight attached to each observation is a function of its difference to the observation of interest , resulting in a vertically weighted average .",
    "this approach has been extended to vertically weighted regressions for image analysis , @xcite , and jump - preserving monitoring procedures , see e.g. @xcite , @xcite and @xcite .",
    "a related clipping median statistic has been investigated in @xcite for a general mixture model . in the present work ,",
    "we confine our discussion to the univariate setting and study the problem how to evaluate the accuracy of the vertically weighted average in terms of confidence intervals .",
    "the computational costs of the vertically weighted average are of the order @xmath7 , if @xmath8 denotes the sample size , which makes it attractive for realtime applications .",
    "hence it is much faster to compute than other popular methods .",
    "for example , the computational costs of the fastest algorithm to calculate a @xmath9-penalized @xmath10-estimator are of the order @xmath11 , see @xcite .    basically , there are two approaches to construct a confidence interval .",
    "the more common approach is the fixed ( but large ) sample confidence interval , whose width is random , such that the precision indicated by the interval is random and can not be specified . in order to set up a fixed width confidence interval",
    ", one needs to rely on a random sample size , which is determined in such a way that the confidence interval attains the preassigned coverage probability , asymptotically .",
    "we investigate a two stage approach .",
    "the first stage sample ( small ) is used to estimate the dispersion of the estimator on which the interval is based . using that estimator one estimates the optimal sample size , which is therefore random .",
    "there is a rich literature on such two stage approaches to construct fixed width intervals , we refer to @xcite and the references given there .    to estimate the variability of the vertically weighted average , we propose to rely on resampling techniques , since they are easy to apply and typically lead to convincing results .",
    "we study the jackknife variance estimator as a versatile and fast method , whose consistency for the vertically weighted average has been shown in @xcite , and the bootstrap as a general tool .    the question arises how the proposed confidence intervals work in practice . to address this issue ,",
    "extensive simulations have been conducted to study the accuracy in terms of the coverage probablity .",
    "it turns out that the accuracy is , in general , very good , but the coverage may be lower than nominal in the tails of the distribution for the conditional approach .",
    "the organisation of the paper is as follows : section  [ sec : defs ] introduces the vertically weighted average , establishes some properties justifying its interpretation as a signal estimator and discusses the fixed sample intervals .",
    "section  [ sec : fixed - width ci ] provides some further background on fixed width confidence intervals and introduces the proposed procedure in detail .",
    "the simulation studies are presented in section  [ sec : simulations ] .",
    "let us assume for a moment that interest focuses on @xmath12 , its mean @xmath13 and the relationship of @xmath14 to the means @xmath15 , @xmath16 of the remaining sample of size @xmath17 . to keep the presentation simple ,",
    "we focus on this one - dimensional problem formulation and notice that it also covers a simplified version of the bilateral filter as studied in image processing : here @xmath0 , @xmath18 , are the gray values of @xmath19 pixels of a connected ( usually rectangular ) area of the image , where @xmath12 is the gray value of the center and @xmath20 represent the gray values of the neighboring pixels .",
    "however , the spatial distance of the pixels to the center and other issues which are of some importance in image processing are not taken into account and should be addressed by future research .",
    "let @xmath21 be independent real  valued observations following the model @xmath22 where @xmath23 are i.i.d .",
    "random variables with common distribution function @xmath24 , denoted by @xmath25 i.i.d(@xmath26 ) , symmetrically distributed around @xmath27 and having finite second moment .",
    "@xmath28 , @xmath18 , specifying the underlying true signal as @xmath15 , @xmath18 .",
    "it is common to assume that the observed data are obtained by sampling equidistantly at time instants @xmath29 , @xmath18 , an underlying function @xmath30 , where @xmath31 the sampling period ( either constant or given by , say , @xmath32 ) , and the domain is @xmath33 ( if @xmath34 is fixed ) or @xmath35 $ ] ( for @xmath36 ) , see e.g. @xcite . in this case",
    "@xmath37 is regarded as the true signal .",
    "the methods and results discussed in the present paper , however , do not need to assume this sampling model but are valid for the more general model ( [ modelofpaper ] ) .",
    "let us assume that interest focuses on @xmath12 and we want to dampen the noise by some averaging procedure in such a way that large differences in the means , @xmath38 , are preserved .",
    "this can be achieved by averaging those data points @xmath0 whose values are close to the observation of interest , @xmath12 .",
    "therefore , the weights used to define a weighted mean should depend on the differences @xmath39 between the observations @xmath0 and @xmath12 .",
    "it is natural to evaluate the difference @xmath39 by means of a nonnegative and symmetric kernel function @xmath40 which is assumed to satisfy the conditions @xmath41 the _ vertically weighted average _ is now defined as @xmath42 typical kernels used in applications are the gaussian kernel , i.e. the density of the @xmath43 distribution , or kernels with support @xmath44 $ ] such as the uniform kernel @xmath45 } $ ] .",
    "further , often one incorporates a scale parameter and considers the choice @xmath46 for some generic kernel @xmath47 .",
    "if @xmath47 is the uniform kernel , only those observations @xmath48 are averaged which are close to @xmath12 in the sense that @xmath49 .",
    "figure  [ illustration ] illustrates the denoising effect and the jump  preserving property of the approach .    .",
    "the observations are shrunken towards the true mean to obtain a denoised estimate.,width=453 ]    in what follows , it will be sometimes convenient to write @xmath50 . in the same vain , the corresponding vertically weighted average associated to @xmath0",
    "is given by @xmath51 for @xmath18 .",
    "then @xmath52 is regarded as the denoised reconstructed signal .    the statistic @xmath53 is related to the vertically weighted mean squared functional @xmath54 where @xmath55 . as shown in @xcite ,",
    "see also @xcite , the functional @xmath56 is minimized by the true mean @xmath28 and @xmath28 is a fix point of the nonlinear equation , i.e. a solution of the associated fix point equation @xmath57 for extensions to hilbert  valued random elements and further discussion see @xcite . the statistic @xmath53 is obtained by replacing the expectations by their empirical sample analogs based on the sample @xmath58 and substituting the true mean , @xmath59 , by the current observation @xmath12 .",
    "the question arises how one may evaluate the accuracy of this statistic .",
    "a common approach is to consider confidence intervals for the underlying parameter when the sample is homogeneous , i.e. if the null hypothesis of a constant signal , @xmath60 holds true .",
    "this requires appropriate large sample asymptotics .",
    "but the statistic @xmath53 is not a member of standard classes of statistics , which hinders the application of known limit theorems to construct procedures for statistical inference , and therefore requires a special treatment . in @xcite",
    "general invariance principles have been established which imply the following central limit theorem : if @xmath61 are i.i.d . with existing fourth moment , then @xmath62 / \\sigma_\\xi ( y_n ) \\stackrel{d}{\\to } n(0,1),\\ ] ] as @xmath63 , where @xmath64 with @xmath65 and @xmath66 for @xmath67 .",
    "the clt ( [ cltcond ] ) holds true under the conditional law given @xmath12 and therefore also unconditionally .    what is the relation between the centering term in ( [ cltcond ] ) , i.e. @xmath68 , and the true underlying signal , i.e. the constants in model ( [ modelofpaper ] ) ?",
    "the following theorem , which is related to the results in @xcite and @xcite , shows that the centering term vanishes in median and expectation , respectively , for i.i.d",
    ". samples .",
    "[ lemmatheta ] assume that @xmath69 are i.i.d . with @xmath70 and @xmath71 , @xmath72 . if @xmath73 is a symmetric kernel , then @xmath74 and @xmath75 if @xmath76 exists .",
    "the result can be shown using equal - in - distribution arguments . if @xmath77 and @xmath78 are two random variables or random vectors of the same dimension with distributions @xmath79 and @xmath80 , we write @xmath81 , if @xmath82 , i.e. if @xmath83 for all measurable sets @xmath84 . if @xmath81 , then @xmath85 for any measurable mapping @xmath86 which does not depend on @xmath87 .",
    "further , in what follows , we write @xmath88 when the expectation is taken with respect to ( the distribution of ) @xmath89 . then , e.g. , @xmath90 , if @xmath77 and @xmath91 are independent",
    ". we shall apply those basic results to the representation @xmath92 } { e_{y_2 } k(y_2-y_1 ) } , \\ ] ] where one has to take into account that numerator and denominator are depend .",
    "first notice that @xmath93 } { e_{y_2 } k(y_2-y_1 ) } \\right ) \\\\    & = \\text{med}_{\\epsilon_1 } \\left ( \\frac { e_{\\epsilon_2 } [ k(\\epsilon_2 - \\epsilon_1 ) ( \\epsilon_2 + m ) ] } { e_{\\epsilon_2 } k(\\epsilon_2 - \\epsilon_1 ) } \\right )   \\\\    & = \\text{med}_{\\varepsilon_1 } \\left ( m + \\frac { e_{\\epsilon_2 } [ k(\\epsilon_2-\\epsilon_1 ) \\epsilon_2 ] } { e_{\\epsilon_2 } k(\\epsilon_2 - \\epsilon_1 ) } \\right ) \\\\    & = m + \\text{med}_{\\epsilon_1 } \\left ( \\frac { g(\\epsilon_1 ) } { h(\\epsilon_1 ) } \\right)\\end{aligned}\\ ] ] where @xmath94 , \\\\    h ( \\epsilon_1 ) & = e_{\\epsilon_2 } k(\\epsilon_2 - \\epsilon_1 ) .\\end{aligned}\\ ] ] we show that @xmath95 which implies @xmath96 thus showing @xmath97 .",
    "the corresponding property for the expectation follows now easily . to verify ( [ toverify ] ) observe that @xmath98 , e_{\\epsilon_2 } k(\\epsilon_1-\\epsilon_2 ) )     \\\\      & = ( - e_{\\epsilon_2 } [ k ( \\epsilon_1 + \\epsilon_2 ) \\epsilon_2 ] , e_{\\epsilon_2 } k ( \\epsilon_1 + \\epsilon_2 ) ) \\end{aligned}\\ ] ] since @xmath99 and @xmath100 .",
    "next , using @xmath101 , we obtain @xmath102 , e_{\\epsilon_2 } k(\\epsilon_1 - \\epsilon_2 ) ) .\\ ] ] but @xmath103 implies @xmath104 , e_{\\epsilon_2 } k(\\epsilon_2 - \\epsilon_1 ) ) , \\ ] ] which completes the proof .    to the best of the author s knowledge",
    ", the following result also does not appear in the literature .",
    "let @xmath21 be i.i.d .",
    "following the model @xmath71 , @xmath18 , for some @xmath105 with symmetric error terms @xmath23 . if @xmath106 is symmetric , then @xmath107 for @xmath18 .",
    "it suffices to show the result for @xmath108 .",
    "notice that @xmath109 by symmetry and independence we have @xmath110 .",
    "hence we obtain @xmath111 therefore @xmath112 but this implies @xmath113 .",
    "note that the existence of @xmath76 and @xmath114 depends on the kernel @xmath73 and the error distribution .",
    "but it can be easily guaranteed by adding a positive but arbitrarily small constant to the kernel @xmath106 .",
    "although the validity of the central limit theorem greatly eases the interpretation of an estimator and its variation , the above formulas are cumbersome to construct practical procedures . a simple and effective approach to estimate the variance of an estimator is efron s jackknife , which dates back to the works of @xcite and @xcite . for the vertically weighted average @xmath108 it",
    "is given by @xmath115 where @xmath116 are the leave ",
    "one  out estimates , @xmath16 , and @xmath117 .",
    "the jackknife variance estimator is consistent if ( [ h0cond ] ) holds and the r.v.s have a finite fourth moment , see @xcite",
    ".    having a consistent estimator for @xmath118 at our disposal , one may calculate fixed sample asymptotic confidence intervals for @xmath119 , namely @xmath120,\\ ] ] for @xmath121 , in order to asses the estimator s precision given @xmath122 . in ( [ cifixedsample ] ) and in what follows , @xmath123 , @xmath124 , is the distribution function of the standard normal distribution and @xmath125 its quantile function .",
    "below we shall discuss how one can determine the sample size to obtain uniform accuracy in terms of the width of the interval , for any @xmath122 .",
    "the coverage probability of those confidence intervals are investigated in section  [ sec : simulations ] .",
    "the unconditional asymptotic normality of @xmath108 under the null hypothesis , which follows from the conditional central limit theorem , combined with lemma  [ lemmatheta ] suggests that @xmath126 as @xmath127 , for any consistent estimator @xmath128 of @xmath129 .",
    "hence , we shall investigate in section  [ sec : simulations ] in greater detail the coverage probabilities of the bootstrap confidence interval @xmath130\\ ] ] where @xmath131 is the bootstrap variance of @xmath53 estimated from @xmath132 replications @xmath133 , @xmath134 , @xmath135 where @xmath136 are @xmath137 with @xmath138 , @xmath139 .",
    "by construction and due to the very basic idea of the vertical weighting approach , the ( conditional ) variance of the estimator @xmath108 strongly depends on the value @xmath12 .",
    "this can be easily seen if @xmath140}(z / m ) $ ] for some fixed @xmath141 .",
    "then @xmath108 is the sample mean of all @xmath0 with @xmath49 .",
    "the effective sample size @xmath142 is random and typically large , if @xmath12 is located in the center of the distribution , but @xmath143 will be small if @xmath12 is in the tail .",
    "the following approach to construct a fixed width confidence interval for the vertically weighted average , introduced in @xcite , overcomes that drawback and allows to determine a sample size that leads to a simple and sound interpretation , namely that the resulting estimator has a specified precision .",
    "the basic idea is to determine the sample size in such a way that , for a preassigned accuracy @xmath144 , the two  sided fixed width confidence interval @xmath145\\ ] ] has _",
    "conditional asymptotic coverage _",
    "@xmath146 , @xmath147 given , i.e. @xmath148 \\ni \\theta(y_n ) | y_n \\bigr ) = 1-\\alpha + o(1),\\ ] ] as @xmath127 , a.s .",
    "if the conditional asymptotic distribution of @xmath108 is normal and were completely known , one could determine the asymptotically optimal sample size @xmath149 required to ensure ( [ condfwci ] ) .",
    "but this fails in practice , since the asymptotic variance is unknown . in a two - stage procedure one",
    "starts with a deterministic initial sample size @xmath150 , depending on the precision parameter @xmath151 and the confidence level @xmath152 , and uses that initial sample to estimate the asymptotically optimal sample size @xmath19 required to achieve a fixed width confidence interval of length @xmath153 with asymptotic coverage @xmath146 .",
    "as the sample size is estimated using the first  stage sample , the final sample size is random .",
    "the two - stage procedure studied in @xcite adopts the two - stage procedure from @xcite and works as follows : let us fix the current observation and denote it by @xmath12 , although the sample size is not yet determined we shall first determine an initial sample size @xmath150 for the first stage and set up the sample @xmath154 using @xmath155 observations in addition to the current one . then the final sample size @xmath8 ( or equivalently @xmath156 ) will be determined and , in the same way , a sample of size @xmath8 will be set up with the current observation @xmath122 put at the end of the sample .",
    "this is necessary because the formula for the vertically weighted average regards the last observation as the current one .",
    "at the first stage , one first draws a first - stage ( initial ) sample of size @xmath157 given by @xmath158 at the second stage calculate the random final sample size @xmath159 where @xmath160 with @xmath161 is the jackknife estimator of the asymptotic variance of the vertically weighted average calculated from the first - stage initial sample of size @xmath150 ( augmented by @xmath122 ) ) ; notice that we have @xmath155 leave - on - out estimates ( @xmath122 is fixed ) leading to the formula ( [ jacksdas ] ) .",
    "this means , if @xmath162 , we sample additional @xmath163 observations , where @xmath164 , to obtain the final sample @xmath165 .",
    "in @xcite it has been shown that the resulting confidence interval @xmath166 has the following properties , when ( [ h0cond ] ) holds :    * @xmath167 has asymptotic coverage @xmath152 , as @xmath168 .",
    "* @xmath166 is consistent for the asymptotically optimal fixed sample interval using the asymptotic optimal sample size @xmath169 , i.e. @xmath170 , as @xmath171 .",
    "* @xmath166 is first - order asymptotic efficient in the sense of chow and robbins , i.e. @xmath172 as @xmath171 .",
    "notice that the asymptotics is for @xmath173 , which implies @xmath174 .",
    "our simulations reported below show that the coverage probability of the two  stage fixed  width confidence interval is fairly good .",
    "its construction is based on a central limit theorem , which suggests to use a resampling approach such as the nonparametric bootstrap , in order to improve the approximation .",
    "more specifically , a closer look at the proof behind the validity of the two  stage procedure discussed above reveals that it is based on the central limit theorem @xmath175 as @xmath171 .",
    "the sample size @xmath176 is then determined such that @xmath177 leading to the formula for @xmath8 given in the previous section .",
    "( [ aclt ] ) and ( [ aclt2 ] ) suggest to bootstrap the distribution of the statistic @xmath178 to improve upon the central limit theorem .",
    "this is achieved by substituting the bootstrap distribution estimator for the distribution function @xmath179 in ( [ aclt2 ] ) .",
    "let us recall how the nonparametric simulation - based bootstrap works : draw @xmath180 independent resamples @xmath181 of size @xmath182 , where @xmath183 , @xmath67 .",
    "alternatively , one can use the smooth bootstrap which convolves @xmath184 with a gaussian law , @xmath185 . here",
    "one may use the cross validated bandwidth selector for the kernel density estimator or the asymptotically optimal choice , @xmath186 , for gaussian data , where @xmath187 is the sample variance . for simplicity of presentation ,",
    "let us denote the bootstrap observations by @xmath188 , whatever kind of bootstrap has been used .",
    "then form the bootstrap samples @xmath189 and calculate @xmath190 where @xmath191",
    "lastly , estimate the @xmath192quantile of the distribution of @xmath193 by the corresponding order statistic @xmath194 .",
    "this leads to the bootstrap final sample size @xmath195",
    "the simulations aim at studying the dispersion of the vertically weighted average and , especially , the performance of the proposed methods in terms of the coverage probability .",
    "let us start with a first experiment to examine how the dispersion of the vertically weighted average depends on the location of the current observation @xmath12 .",
    "figure  [ sdest ] depicts the standard deviation given @xmath196 , @xmath197 $ ] , for the sample size @xmath198 when using a gaussian kernel with standard deviation @xmath199 .",
    "the interval @xmath200 was discretized using a step size of @xmath201 and each resulting case was simulated using @xmath202 repetitions .",
    "it can be seen that the dispersion is substantially larger in the tails than in the center of the distribution .",
    "those simulations as well as all studies presented in the following subsections are based on observations following a standard normal distribution , as the focus of the simulations is to investigate the performance for the three different confidence intervals ( classical fixed sample , fixed width with clt asymptotics and fixed width with bootstrap ) when varying the method parameter @xmath10 , the confidence level @xmath146 and the sample size ( for fixed sample intervals ) and the precision parameter @xmath151 ( for the fixed width intervals ) , respectively .     for @xmath203 $ ] under @xmath204 errors .",
    "]      let us start our investigation by studying the coverage probability of the proposed asymptotic fixed sample confidence intervals ( [ cifixedsample ] ) .",
    "the parameter @xmath10 has levels @xmath205 and the coverage probability was simulated across the distribution of @xmath206 by conditioning on @xmath196 for @xmath207 . additionally , the coverage probability of the confidence interval as used in practice ( real ci ) , i.e. for @xmath208 ( randomly drawn ) was considered .",
    "the sample sizes under investigation are @xmath209 .",
    "each case was simulated using @xmath210 simulation runs .",
    "the required true value @xmath211 was simulated based on a random sample of size @xmath212 .",
    "the results for a confidence level of @xmath213 are shown in table  [ simtableclassi ] .",
    "it can be seen that the coverage probabilities are fairly good in the center of the distribution , but decrease in the tails , especially for small sample sizes such as @xmath214 .",
    "this results in a lower than nominal coverage of the real ci .",
    ".coverage of classical fixed sample confidence intervals for the vertically weighted average .",
    "[ cols=\"^,^,>,>,>,>,>,>,>,>\",options=\"header \" , ]      +",
    "the vertically weighted average represents an interesting jump - preserving signal estimator which denoises without corrupting finer details of a signal . since the effective number of observations used by the estimator to calculate the signal estimate depends on the location of the current observation ,",
    "the assessment of the precision of this estimator , both in terms of variance estimation and construction of confidence intervals , has been a delicate open problem .",
    "recent theoretical results about the asymptotic behavior of the vertically weighted average allow the consideration of confidence intervals based on that jump - preserving signal estimator .",
    "we discuss two common statistical approaches to construct confidence intervals : first , asymptotic fixed sample confidence intervals relying on the asymptotic normality of the appropriately standardized estimator , leading to confidence intervals of a random width but based on a sample of fixed sample size .",
    "second , fixed width confidence intervals where one specifies the precision ( i.e. width of the interval ) in advance . here",
    "the sample size has to be estimated and hence becomes random .",
    "we propose to rely on a two stage procedure , where the first stage sample ( of a fixed initial sample size ) is used to estimate the standard error of the vertically weighted average , in order to calculate an estimator of the required final sample size for the second stage . at the second stage",
    "the confidence interval is then calculated based on the vertically weighted average using the final sample , which consists of the initial sample and , if needed , additional observations .    to estimate the dispersion of the vertically weighted estimator",
    ", we propose to use jackknife variance estimation and the bootstrap , respectively .",
    "the bootstrap provides convincing results , but it is more demanding from a computational point of view .",
    "our simulations indicate that unconditional fixed sample confidence intervales based on bootstrap variance estimation perform well . for fixed width confidence intervals using jackknife variance estimation ,",
    "the coverage is generally accurate except when the current observation is located in the tails of the underlying distribution , which results in lower than nominal coverage probabilities .",
    "the accuracy can be further improved by the bootstrap for high precision intervals ."
  ],
  "abstract_text": [
    "<S> vertically weighted averages perform a bilateral filtering of data , in order to preserve fine details of the underlying signal , especially discontinuities such as jumps ( in dimension one ) or edges ( in dimension two ) . in homogeneous regions of the domain </S>",
    "<S> the procedure smoothes the data by averaging nearby data points to reduce the noise , whereas in inhomogenous regions the neighboring points are only taken into account when their value is close to the current one . </S>",
    "<S> this results in a denoised reconstruction or estimate of the true signal without blurring finer details .    </S>",
    "<S> this paper addresses the lack of results about the construction and evaluation of confidence intervals based on the vertically weighted average , which is required for a proper statistical evaluation of its estimation accuracy . </S>",
    "<S> based on recent results we discuss and investigate in greater detail fixed sample as well as fixed width ( conditional ) confidence intervals constructed from this estimator . </S>",
    "<S> the fixed width approach allows to specify explicitly the estimator s accuracy and determines a random sample size to ensure the required coverage probability . </S>",
    "<S> this also fixes to some extent the inherent property of the vertically weighted average that its variability is higher in low - density regions than in high - density regions . to estimate the variances required to construct the procedures , we rely on resampling techniques , especially the bootstrap and the jackknife .    </S>",
    "<S> extensive monte carlo simulations show that the proposed confidence intervals are reliable in terms of their coverage probabilities for a wide range of parameter settings . </S>",
    "<S> 0.5 cm msc : 62l10 , 62g09 , 62g15 </S>"
  ]
}