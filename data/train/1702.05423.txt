{
  "article_text": [
    "motivated by the need to solve large - scale optimization problems and increasing capabilities in parallel computing , block coordinate update ( bcu ) methods have become particularly popular in recent years due to their low per - update computational complexity , low memory requirements , and their potentials in a distributive computing environment . in the context of optimization , bcu first appeared in the form of block coordinate descent ( bcd ) type of algorithms which can be applied to solve unconstrained smooth problems or those with separable nonsmooth terms in the objective ( possibly with separable constraints ) .",
    "more recently , it has been developed for solving problems with nonseparable nonsmooth terms and/or constraint in a primal - dual framework .    in this paper , we consider the following linearly constrained multi - block structured optimization model : @xmath2 where @xmath3 is partitioned into disjoint blocks @xmath4 , @xmath5 is a smooth convex function with lipschitz continuous gradient , and each @xmath6 is proper closed convex and possibly non - differentiable .",
    "note that @xmath6 can include an indicator function of a convex set @xmath7 , and thus can implicitly include certain separable block constraints in addition to the nonseparable linear constraint .",
    "many applications arising in statistical and machine learning , image processing , and finance can be formulated in the form of including the basis pursuit @xcite , constrained regression @xcite , support vector machine in its dual form @xcite , portfolio optimization @xcite , just to name a few .    towards finding a solution for",
    ", we will first present an accelerated proximal jacobian alternating direction method of multipliers ( algorithm [ alg : ajadmm ] ) , and then we generalize it to an accelerated _ randomized _ primal - dual block coordinate update method ( algorithm [ alg : arpdc ] ) . assuming strong convexity on the objective function , we will establish @xmath1 convergence rate results of the proposed algorithms by adaptively setting the parameters , where @xmath8 is the total number of iterations .",
    "in addition , if further assuming smoothness and the full - rankness we then obtain linear convergence of a modified method ( algorithm [ alg : rpdc - lin ] ) .",
    "our algorithms are closely related to randomized coordinate descent methods , primal - dual coordinate update methods , and accelerated primal - dual methods . in this subsection ,",
    "let us briefly review three classes of methods and discuss their relations to our algorithms .",
    "in the absence of linear constraint , algorithm [ alg : arpdc ] specializes to randomized coordinate descent ( rcd ) , which was first proposed in @xcite for smooth problems and later generalized in @xcite to nonsmooth problems .",
    "it was shown that rcd converges sublinearly with rate @xmath0 , which can be accelerated to @xmath1 for weakly convex problems and achieves a linear rate for strongly convex problems . by choosing multiple block variables at each iteration",
    ", @xcite proposed to parallelize the rcd method and showed the same convergence results for parallelized rcd .",
    "this is similar to setting @xmath9 in algorithm [ alg : arpdc ] , allowing parallel updates on the selected @xmath3-blocks .      in the presence of linear constraints ,",
    "coordinate descent methods may fail to converge to a solution of the problem because fixing all but one block , the selected block variable may be uniquely determined by the linear constraint . to perform coordinate update to the linearly constrained problem , one effective approach is to update both primal and dual variables . under this framework ,",
    "the alternating direction method of multipliers ( admm ) is one popular choice .",
    "originally , admm @xcite was proposed for solving two - block structured problems with separable objective ( by setting @xmath10 and @xmath11 in ) , for which its convergence and also convergence rate have been well - established ( see e.g.  @xcite ) . however , directly extending admm to the multi - block setting such as may fail to converge ; see @xcite for a divergence example of the admm even for solving a linear system of equations .",
    "lots of efforts have been made to establish the convergence of multi - block admm under stronger assumptions ( see e.g.  @xcite ) such as strong convexity or orthogonality conditions on the linear constraint . without additional assumptions ,",
    "modification is necessary for the admm applied to multi - block problems to be convergent ; see @xcite for example .",
    "very recently , @xcite proposed a randomized primal - dual coordinate ( rpdc ) update method . applied to , rpdc is a special case of algorithm [ alg : arpdc ] with fixed parameters .",
    "it was shown that rpdc converges with rate @xmath0 under weak convexity assumption .",
    "more general than solving an optimization problem , primal - dual coordinate ( pdc ) update methods have also appeared in solving fixed - point or monotone inclusion problems @xcite . however , for these problems , the pdc methods are only shown to converge but no convergence rate estimates are known unless additional assumptions are made such as the strong monotonicity condition .",
    "it is possible to accelerate the rate of convergence from @xmath0 to @xmath1 for gradient type methods .",
    "the first acceleration result was shown by nesterov  @xcite for solving smooth unconstrained problems .",
    "the technique has been generalized to accelerate gradient - type methods on possibly nonsmooth convex programs @xcite .",
    "primal - dual methods on solving linearly constrained problems can also be accelerated by similar techniques . under weak convexity assumption ,",
    "the augmented lagrangian method ( alm ) is accelerated in @xcite from @xmath0 convergence rate to @xmath1 by using a similar technique as that in @xcite to the multiplier update , and @xcite accelerates the linearized alm using a technique similar to that in @xcite . assuming strong convexity on the objective , @xcite accelerates the admm method , and the assumption is weakened in @xcite to assuming the strong convexity for one component of the objective function .",
    "on solving bilinear saddle - point problems , various primal - dual methods can be accelerated if either primal or dual problem is strongly convex @xcite . without strong",
    "convexity , partial acceleration is still possible in terms of the rate depending on some other quantities ; see e.g.  @xcite .",
    "we accelerate the proximal jacobian admm @xcite and also generalize it to an accelerated primal - dual coordinate updating method for linearly constrained multi - block structured convex program , where in the objective there is a nonseparable smooth function . with parameters fixed during all iterations , the generalized method reduces to that in @xcite and enjoys @xmath0 convergence rate under mere weak convexity assumption .",
    "by adaptively setting the parameters at different iterations , we show that the accelerated method has @xmath1 convergence rate if the objective is strongly convex .",
    "in addition , if there is one block variable that is independent of all others in the objective ( but coupled in the linear constraint ) and also the corresponding component function is smooth , we modify the algorithm by treating that independent variable in a different way and establish a linear convergence result .",
    "numerically , we test the accelerated method on quadratic programming and compare it to the ( nonaccelerated ) rpdc method in @xcite .",
    "the results demonstrate that the accelerated method performs efficiently and stably with the parameters automatically set in accordance of the analysis , while the rpdc method needs to tune its parameters for different data in order to have a comparable performance .",
    "* notations . *",
    "we let @xmath12 denote the subvector of @xmath3 with blocks indexed by @xmath13 .",
    "namely , if @xmath14 , then @xmath15 . similarly , @xmath16 denotes the submatrix of @xmath17 with columns indexed by @xmath13 , and @xmath18 denotes the sum of component functions indicated by @xmath13 .",
    "we reserve @xmath19 for the identity matrix and use @xmath20 for euclidean norm . given a symmetric positive semidefinite ( psd ) matrix @xmath21 , for any vector @xmath22 of appropriate size , we define @xmath23 .",
    "also , we denote @xmath24    * preparations . *",
    "since there are only linear constraints in , a point @xmath25 is a solution to _ if and only if _ the karush - kuhn - tucker ( kkt ) conditions hold , i.e. , there exists a @xmath26 such that    [ eq : kkt - conds ] @xmath27    together with the convexity of @xmath28 , implies @xmath29    we will use the following lemmas as basic facts , the proofs of which can be found in @xcite .    for any vectors @xmath30 and symmetric psd matrix @xmath21 of appropriate sizes , it holds that @xmath31.\\ ] ]    [ lem : xy - rate ] given a function @xmath32 , for a given @xmath3 and a random vector @xmath33 , if for any @xmath34 ( that may depend on @xmath33 ) it holds @xmath35 then for any @xmath36 , we have @xmath37\\le \\sup_{\\|{\\lambda}\\|\\le \\gamma}\\phi({\\lambda}).\\ ] ]    [ lem : equiv - rate ] suppose @xmath38 \\le \\epsilon.$ ] then , @xmath39 \\leq \\epsilon,\\ ] ] where @xmath40 satisfies the optimality conditions in , and we assume @xmath41",
    ".    * outline .",
    "* the rest of the paper is organized as follows .",
    "section [ sec : ajadmm ] presents the accelerated proximal jacobian admm and its convergence results . in section [ sec : arpdc ] , we propose an accelerated primal - dual block coordinate update method with convergence analysis .",
    "section [ sec : linear ] assumes more structure on the problem and modifies the algorithm in section [ sec : arpdc ] to have linear convergence .",
    "numerical results are provided in section [ sec : numerical ] .",
    "finally , section [ sec : conclusion ] concludes the paper .",
    "in this section , we propose an accelerated proximal jacobian admm for solving . at each iteration ,",
    "the algorithm updates all @xmath42 block variables in parallel by minimizing a linearized proximal approximation of the augmented lagrangian function , and then it renews the multiplier .",
    "specifically , it iteratively performs the following updates :    [ eq : ajadmm ] @xmath43    where @xmath44 and @xmath45 are scalar parameters , @xmath46 is a block diagonal matrix , and @xmath47 denotes the residual .",
    "note that consists of @xmath42 independent subproblems , and they can be solved in parallel .",
    "algorithm [ alg : ajadmm ] summarizes the proposed method .",
    "it reduces to the proximal jacobian admm in @xcite if @xmath48 and @xmath49 are fixed for all @xmath50 and there is no nonseparable function @xmath5 .",
    "we will show that adapting parameters to the iterations can accelerate the convergence of the algorithm .",
    "* initialization : * choose @xmath51 , set @xmath52 , and let @xmath53      throughout the analysis in this section , we make the following assumptions .",
    "[ assump : saddle - pt ] there exists @xmath40 satisfying the kkt conditions in .",
    "[ assump : full - lip - f ] @xmath54 is lipschitz continuous with modulus @xmath55 .",
    "[ assump : str - cvx - f ] the functions @xmath5 and @xmath56 are convex with moduli @xmath57 and @xmath58 respectively , and @xmath59 .",
    "the first two assumptions are standard , and the third one is for showing convergence rate of @xmath1 , where @xmath8 is the number of iterations . with only weak convexity , algorithm [ alg : ajadmm ] can be shown to converge in the rate @xmath0 with parameters fixed for all iterations , and the order @xmath60 is optimal as shown in the very recent work @xcite .      in this subsection",
    ", we show the @xmath1 convergence rate result of algorithm [ alg : ajadmm ] .",
    "first , we establish a result of running one iteration of algorithm [ alg : ajadmm ] .",
    "[ lem:1iter - ajadmm ] under assumptions [ assump : full - lip - f ] and [ assump : str - cvx - f ] , let @xmath61 be the sequence generated from algorithm [ alg : ajadmm ] . then for any @xmath50 and any @xmath62 such that @xmath63 , it holds that @xmath64-\\beta_k\\|r^{k+1}\\|^2 \\nonumber \\\\ & & + \\frac{\\beta_k}{2}\\left[\\|a(x^{k+1}-x)\\|^2-\\|a(x^k - x)\\|^2+\\|a(x^{k+1}-x^k)\\|^2\\right ] \\nonumber \\\\ & & -\\frac{1}{2}\\left[\\|x^{k+1}-x\\|_{p^k}^2-\\|x^k - x\\|_{p^k}^2+\\|x^{k+1}-x^k\\|_{p^k}^2\\right ] \\nonumber \\\\ & & + \\frac{l_f}{2}\\|x^{k+1}-x^k\\|^2-\\frac{\\mu_g}{2}\\|x^{k+1}-x\\|^2-\\frac{\\mu_f}{2}\\|x^k - x\\|^2 .",
    "\\label{eq:1iter - ajadmm}\\end{aligned}\\ ] ]    using the above lemma , we are able to prove the following theorem .",
    "[ thm : ajadmm - g ] under assumptions [ assump : full - lip - f ] and [ assump : str - cvx - f ] , let @xmath61 be the sequence generated by algorithm  [ alg : ajadmm ] .",
    "suppose that the parameters are set to satisfy @xmath65 and there exists a number @xmath66 such that for all @xmath67 , @xmath68 then , for any @xmath62 satisfying @xmath63 , we have @xmath69    in the next theorem , we provide a set of parameters that satisfy the conditions in theorem [ thm : ajadmm - g ] and establish the @xmath1 convergence rate result .",
    "[ thm : ajadmm - spc ] under assumptions [ assump : saddle - pt ] through [ assump : str - cvx - f ] , let @xmath61 be the sequence generated by algorithm [ alg : ajadmm ] with parameters set to : @xmath70 where @xmath71 is a block diagonal matrix satisfying @xmath72 . then , @xmath73 where @xmath74 and @xmath75 in addition , letting @xmath76 and @xmath77 we have    [ eq : ajadmm - erg - rate - spc ] @xmath78",
    "in this section , we generalize algorithm [ alg : ajadmm ] to a randomized setting where the user may choose to update a subset of blocks at each iteration . instead of updating all @xmath42 block variables , we randomly choose a subset of them to renew at each iteration .",
    "depending on the number of processors ( nodes , or cores ) , we can choose a single or multiple block variables for each update .",
    "our algorithm is an accelerated version of the randomized primal - dual coordinate update ( rpdc ) method that is recently proposed in @xcite . at each iteration",
    ", it performs a block proximal gradient update to a subset of randomly selected primal variables while keeping the remaining ones fixed , followed by an update to the multipliers .",
    "specifically , at iteration @xmath50 , it selects an index set @xmath79 with cardinality @xmath80 and performs the following updates :    [ eq : fw - arpdc ] @xmath81 x_i^k , & \\text { if } i\\not\\in s_k\\end{array}\\right.\\label{eq : fw - arpdc - x}\\\\ & r^{k+1}=r^k+\\sum_{i\\in s_k}a_i(x_i^{k+1}-x_i^k),\\\\ & \\lambda^{k+1}=\\lambda^k - \\rho_k r^{k+1},\\label{eq : fw - arpdc - lam}\\end{aligned}\\ ] ]    where @xmath82 and @xmath83 are algorithm parameters , and their values will be determined later . note that we use @xmath84 in for simplicity .",
    "it can be replaced by a psd matrix weighted norm square term as in , and our convergence results still hold .",
    "algorithm [ alg : arpdc ] summarizes the above method .",
    "if the parameters @xmath82 and @xmath83 are fixed during all the iterations , i.e. , constant parameters , the algorithm reduces to the rpdc method in @xcite . adapting these parameters to the iterations",
    ", we will show that algorithm [ alg : arpdc ] enjoys faster convergence rate than rpdc if the problem is strongly convex .",
    "* initialization : * choose @xmath51 , set @xmath52 , and let @xmath53      in this subsection , we establish convergence results of algorithm [ alg : arpdc ] under assumptions [ assump : saddle - pt ] and [ assump : str - cvx - f ] , and also the following partial gradient lipschitz continuity assumption .",
    "[ assump : lip - f ] for any @xmath85 with @xmath86 , @xmath87 is lipschitz continuous with a uniform constant @xmath88 .",
    "note that if @xmath54 is lipschitz continuous with constant @xmath55 , then @xmath89 .",
    "similar to the analysis in section [ sec : ajadmm ] , we first establish a result of running one iteration of algorithm [ alg : arpdc ] . throughout this section ,",
    "we denote @xmath90 .",
    "[ lem:1iter ] under assumptions [ assump : str - cvx - f ] and [ assump : lip - f ] , let @xmath61 be the sequence generated from algorithm [ alg : arpdc ] . then for any @xmath3 such that @xmath63 , it holds @xmath91 \\nonumber \\\\ & \\le & ( 1-\\theta){\\mathbb{e}}\\left[\\phi(x^k , x,\\lambda^k)+\\beta_k\\| r^k\\|^2+\\frac{\\mu_g}{2}\\|x^k - x\\|^2\\right]+\\frac{l_m}{2}{\\mathbb{e}}\\|x^{k+1}-x^k\\|^2\\nonumber \\\\ & & -\\frac{\\theta\\mu_f}{2}{\\mathbb{e}}\\|x^k - x\\|^2+\\frac{\\beta_k}{2}{\\mathbb{e}}\\left(\\|a(x^{k+1}-x)\\|^2-\\|a(x^k - x)\\|^2+\\|a(x^{k+1}-x^k)\\|^2\\right)\\nonumber \\\\ & & -\\frac{\\eta_k}{2}{\\mathbb{e}}\\left(\\|x^{k+1}-x\\|^2-\\|x^k - x\\|^2 + \\|x^{k+1}-x^k\\|^2\\right ) .",
    "\\label{eq : sum - bd1}\\end{aligned}\\ ] ]    when @xmath92 ( meaning is only weakly convex ) , algorithm [ alg : arpdc ] has @xmath0 convergence rate with fixed @xmath93 during all the iterations .",
    "this can be shown from and has been established in @xcite . for ease of referencing ,",
    "we give the result below without proof .",
    "[ thm : naccl ] under assumptions [ assump : saddle - pt ] and [ assump : lip - f ] , let @xmath61 be the sequence generated from algorithm [ alg : arpdc ] with @xmath94 for all @xmath50 , satisfying @xmath95 where @xmath96 denotes the spectral norm of @xmath17 . then    @xmath97\\big|\\le \\frac{1}{1+\\theta t}\\max_{\\|\\lambda\\|\\le \\gamma}\\phi(x^*,\\lambda),\\\\ & { \\mathbb{e}}\\|a\\bar{x}^t - b\\|\\le \\frac{1}{(1+\\theta t)\\max\\{1 , \\|\\lambda^*\\|\\ } } \\max_{\\|\\lambda\\|\\le \\gamma}\\phi(x^*,\\lambda),\\end{aligned}\\ ] ]    where @xmath40 satisfies the kkt conditions in , @xmath98 , and @xmath99    when @xmath28 is strongly convex , the above @xmath0 convergence rate can be accelerated to @xmath1 by adaptively changing the parameters at each iteration .",
    "the following theorem is our main result .",
    "it shows @xmath1 convergence result under certain conditions on the parameters .",
    "based on this theorem , we will give a set of parameters that satisfy these conditions and thus provide an implementable way to choose the paramenters .",
    "[ thm : rate0 ] under assumptions [ assump : saddle - pt ] , [ assump : str - cvx - f ] and [ assump : lip - f ] , let @xmath61 be the sequence generated from algorithm [ alg : arpdc ] with parameters satisfying the following conditions for a certain number @xmath66 :    [ eq : para - conds ] @xmath100    then for any @xmath62 such that @xmath63 , we have @xmath101+\\frac{k_0 + 2}{2}(\\eta_1-\\theta\\mu_f){\\mathbb{e}}\\|x^1-x\\|^2 \\nonumber \\\\ & & + \\frac{\\theta(k_0 + 3)-1}{2\\rho_1}{\\mathbb{e}}\\|\\lambda^1-\\lambda\\|^2-\\frac{t+k_0 + 1}{2}{\\mathbb{e}}\\|x^{t+1}-x\\|_{(\\mu_g+\\eta_t ) i-\\beta_t a^\\top a}^2 .",
    "\\label{eq : sum - bd4}\\end{aligned}\\ ] ]    specifying the parameters that satisfy , we show @xmath1 convergence rate of algorithm [ alg : arpdc ] .",
    "[ thm : rate1 ] under assumptions [ assump : saddle - pt ] , [ assump : str - cvx - f ] and [ assump : lip - f ] , let @xmath61 be the sequence generated from algorithm [ alg : arpdc ] with parameters taken as    [ eq : paras ] @xmath102 \\frac{t+k_0 + 1}{\\theta(t+k_0 + 1)-1}\\rho_{t-1 } , & \\text { for } k = t \\end{array } \\right.\\label{eq : paras - rho}\\\\ & \\beta_k= \\frac{\\mu(\\theta k+2+\\theta)}{2\\rho\\|a\\|_2 ^ 2},\\,\\forall k\\ge 1,\\label{eq : paras - beta}\\\\ & \\eta_k=\\frac{\\mu}{2}\\left(\\theta k+2+\\theta\\right)+l_m,\\,\\forall k\\ge 1,\\label{eq : paras - eta}\\end{aligned}\\ ] ]    where @xmath103 and @xmath104 then @xmath105\\big|\\le \\frac{1}{t}\\max_{\\|\\lambda\\|\\le \\gamma}\\phi(x^*,\\lambda),\\quad { \\mathbb{e}}\\|a\\bar{x}^{t+1}-b\\|\\le \\frac{1}{t\\max\\{1,\\|\\lambda^*\\|\\}}\\max_{\\|\\lambda\\|\\le \\gamma}\\phi(x^*,\\lambda),\\ ] ] where @xmath106 , @xmath107 \\right . \\\\ & & \\left .",
    "+ \\frac{k_0 + 2}{2}(\\eta_1-\\theta\\mu_f)\\|x^1-x\\|^2+\\frac{\\theta(k_0 + 3)-1}{2\\rho_1}\\|\\lambda\\|^2\\right\\}\\end{aligned}\\ ] ] and @xmath108 in addition , @xmath109",
    "in this section , we assume some more structure on and show that a linear rate of convergence is possible . if there is no linear constraint , algorithm [ alg : arpdc ] reduces to the rcd method proposed in @xcite .",
    "it is well - known that rcd converges linearly if the objective is strongly convex .",
    "however , with the presence of linear constraints , mere strong convexity of the objective of the primal problem only ensures the smoothness of its lagrangian dual function , but not the strong concavity of it .",
    "hence , in general , we do not expect linear convergence by only assuming strong convexity on the primal objective function . to ensure linear convergence on both the primal and dual variables , we need additional assumptions .    throughout this section ,",
    "we suppose that there is at least one block variable being absent in the nonseparable part of the objective , namely @xmath5 . for convenience ,",
    "we rename this block variable to be @xmath110 , and the corresponding component function and constraint coefficient matrix as @xmath111 and @xmath112 . specifically , we consider the following problem @xmath113 towards a solution to , we modify algorithm [ alg : arpdc ] by updating @xmath110-variable after the @xmath3-update .",
    "since there is only a single @xmath110-block , to balance @xmath3 and @xmath110 updates , we do not renew @xmath110 in every iteration but instead update it in probability @xmath90 .",
    "hence , roughly speaking , @xmath3 and @xmath110 variables are updated in the same frequency .",
    "the method is summarized in algorithm [ alg : rpdc - lin ] .    * initialization : * choose @xmath114 , set @xmath52 , and choose parameters @xmath115 .",
    "let @xmath116 and @xmath90 .",
    "assume @xmath111 to be differentiable .",
    "then any pair of primal - dual solution @xmath117 to satisfies the kkt conditions :    [ kkt ] @xmath118    besides assumptions [ assump : str - cvx - f ] and [ assump : lip - f ] , we make two additional assumptions as follows .",
    "[ assump : saddle - pt - lin ] there exists @xmath119 satisfying the kkt conditions in .",
    "[ assump : str - cvx - h ] the function @xmath111 is strongly convex with modulus @xmath120 , and its gradient @xmath121 is lipschitz continuous with constant @xmath122 .",
    "the strong convexity of @xmath28 and @xmath111 implies    [ scvx - fh ] @xmath123    where @xmath124 is a subgradient of @xmath28 at @xmath25 .",
    "similar to lemma [ lem:1iter ] , we first establish a result of running one iteration of algorithm [ alg : rpdc - lin ] .",
    "it can be proven by similar arguments to those showing lemma [ lem:1iter ] .",
    "[ lem : linear-1step ] under assumptions [ assump : str - cvx - f ] , [ assump : lip - f ] , and [ assump : str - cvx - h ] , let @xmath125 be the sequence generated from algorithm [ alg : rpdc - lin ] .",
    "then for any @xmath50 and any @xmath126 such that @xmath127 , it holds @xmath128 + ( \\beta -\\rho){\\mathbb{e}}\\|r^{k+1}\\|^2\\nonumber\\\\ & & + { \\mathbb{e}}\\left[\\langle x^{k+1}- x , ( \\eta_x i-",
    "\\beta a^\\top a ) ( x^{k+1}- x^k)\\rangle+\\langle y^{k+1}- y,(\\eta_y i- \\beta   b^\\top b)(y^{k+1}-y^k)\\rangle\\right]\\nonumber\\\\ & & + \\frac{1}{\\rho}{\\mathbb{e}}\\langle\\lambda^{k+1}-\\lambda,\\lambda^{k+1}-\\lambda^k\\rangle-\\frac{l_m}{2}{\\mathbb{e}}\\|x^{k+1}-x^k\\|^2 + \\frac{\\theta\\mu_f}{2}{\\mathbb{e}}\\|x^k - x\\|^2+\\frac{\\mu_g}{2}{\\mathbb{e}}\\|x^{k+1}-x\\|^2\\nonumber\\\\ & \\le & ( 1-\\theta){\\mathbb{e}}\\left[f(x^k)-f(x)+\\big\\langle y^k- y,{\\nabla } h(y^k)\\big\\rangle-\\big\\langle \\lambda , ax^k+by^k - b\\big\\rangle\\right]+\\beta(1 -\\theta){\\mathbb{e}}\\|r^k\\|^2 \\nonumber \\\\ & & + ( 1-\\theta)\\frac{\\mu_g}{2}{\\mathbb{e}}\\|x^k - x\\|^2+\\frac{1-\\theta}{\\rho}{\\mathbb{e}}\\langle\\lambda^{k}-\\lambda,\\lambda^{k}-\\lambda^{k-1}\\rangle \\nonumber \\\\ & & + \\beta { \\mathbb{e}}\\langle a(x^{k+1}- x ) ,   b ( y^{k+1}- y^k)\\rangle+\\beta(1 -\\theta){\\mathbb{e}}\\langle b(y^k- y),a(x^{k+1}-x^k)\\rangle .",
    "\\label{lin - ineq1 - 1y}\\end{aligned}\\ ] ]    for ease of notation , we denote @xmath129 and @xmath130 let @xmath131 and also let @xmath132 .",
    "\\label{eq : def - psi}\\end{aligned}\\ ] ]    the following theorem is key to establishing linear convergence of algorithm [ alg : rpdc - lin ] .",
    "[ thm - pre ] under assumptions [ assump : str - cvx - f ] through [ assump : str - cvx - h ] , let @xmath125 be the sequence generated from algorithm [ alg : rpdc - lin ] with @xmath133 .",
    "let @xmath134 and @xmath135 .",
    "choose @xmath136 such that @xmath137\\succeq \\left[\\begin{array}{cc}\\theta & 1-\\theta\\\\ 1-\\theta & \\frac{1}{\\theta}-(1-\\theta)\\end{array}\\right],\\ ] ] and positive numbers @xmath138 such that    [ eq : paras - lin ] @xmath139    then it holds that @xmath140+{\\mathbb{e}}\\left[\\frac{1}{2}\\|y^{k+1}- y^*\\|_q^2+\\frac{3\\alpha\\nu}{4}\\|y^{k+1}-y^*\\|^2 - 4cl_h^2\\|y^{k+1}-y^*\\|^2\\right]\\nonumber\\\\ & & + \\left(\\frac{1}{2\\rho}+\\frac{c}{2}\\sigma_{\\min}(bb^\\top)\\right){\\mathbb{e}}\\left[\\|\\lambda^{k+1}-\\lambda^*\\|^2 -(1-\\theta)\\|\\lambda^{k}-\\lambda^*\\|^2+\\frac{1}{\\theta}\\|\\lambda^{k+1}-\\lambda^k\\|^2\\right]\\nonumber \\\\ & \\le & \\psi(z^k , z^*;p , q,\\beta,\\rho , c,\\tau_2 ) .",
    "\\label{lin - ineq5 - 1y}\\end{aligned}\\ ] ]    using theorem [ thm - pre ] , a linear convergence rate of algorithm [ alg : rpdc - lin ] follows .",
    "[ thm - linear ] under assumptions [ assump : str - cvx - f ] through [ assump : str - cvx - h ] , let @xmath125 be the sequence generated from algorithm [ alg : rpdc - lin ] with @xmath133 .",
    "let @xmath134 and @xmath135 .",
    "assume that @xmath112 is full row - rank and @xmath141 .",
    "choose @xmath142 satisfying and , and in addition ,    [ eq : paras - lin2 ] @xmath143    then @xmath144 where @xmath145    we can always rescale @xmath146 and @xmath147 without essentially altering the linear constraints . hence , the assumption @xmath141 can be made without losing generality . from , it is easy to see that @xmath148 converges to @xmath149 r - linearly in expectation .",
    "in addition , note that @xmath150 hence , also implies an r - linear convergence from @xmath151 to @xmath26 in expectation .",
    "in this section , we test algorithm [ alg : arpdc ] on nonnegative quadratic programming @xmath152 and we compare it to the rpdc method proposed in @xcite . note that when applied to , rpdc can be regarded as a special case of algorithm [ alg : arpdc ] with nonadaptive parameters .",
    "the data was generated randomly as follows .",
    "we let @xmath153 , where @xmath154 is gaussian randomly generated orthogonal matrix and @xmath155 is a diagonal matrix with @xmath156 . hence ,",
    "the smallest and largest singular values of @xmath157 are 1 and 100 respectively , and the objective of is strongly convex with mudulus @xmath158 .",
    "the components of @xmath159 follow standard gaussian distribution , and those of @xmath147 follow uniform distribution on @xmath160 $ ] .",
    "we let @xmath161\\in{\\mathbb{r}}^{p\\times n}$ ] to guarantee the existence of feasible solutions , where @xmath112 was generated according to standard gaussian distribution .",
    "in addition , we normalized @xmath17 so that it has a unit spectral norm .    in the test , we fixed @xmath162 and varied @xmath163 among @xmath164 . for both algorithm [ alg : arpdc ] and rpdc , we evenly partitioned @xmath3 into @xmath165 blocks ,",
    "i.e. , each block consists of 50 coordinates , and we set @xmath166 , i.e. , all blocks are updated at each iteration .",
    "the parameters of algorithm [ alg : arpdc ] were set according to , and those for rpdc were set based on theorem [ thm : naccl ] with @xmath167 where @xmath168 varied among @xmath169 . figures [ fig : qp - p200 ] and [ fig : qp - p500 ] plot the objective values and feasibility violations by algorithm [ alg : arpdc ] and rpdc . from these results",
    ", we see that algorithm [ alg : arpdc ] performed well for both datasets with a single set of parameters while the performance of rpdc was severely affected by the penalty parameter .    [",
    "cols=\"^,^,^,^ \" , ]",
    "in this paper we propose an accelerated proximal jacobian admm method and generalize it to an accelerated randomized primal - dual coordinate updating method for solving linearly constrained multi - block structured convex programs .",
    "we show that if the objective is strongly convex then the methods achieve @xmath1 convergence rate where @xmath8 is the total number of iterations . in addition , if one block variable is independent of others in the objective and its part of the objective function is smooth , we have modified the primal - dual coordinate updating method to achieve linear convergence .",
    "numerical experiments on quadratic programming have shown the efficacy of the newly proposed methods .    10    a.  beck and m.  teboulle . a fast iterative shrinkage - thresholding algorithm for linear inverse problems .",
    ", 2(1):183202 , 2009 .",
    ". local linear convergence of the alternating direction method of multipliers on quadratic or linear programs .",
    ", 23(4):21832207 , 2013 .",
    "k.  bredies and h.  sun . accelerated douglas - rachford methods for the solution of convex - concave saddle - point problems .",
    ", 2016 .",
    "x.  cai , d.  han , and x.  yuan .",
    "the direct extension of admm for three - block separable convex minimization models is convergent when one function is strongly convex . , 2014 .",
    "a.  chambolle and t.  pock .",
    "a first - order primal - dual algorithm for convex problems with applications to imaging .",
    ", 40(1):120145 , 2011 .",
    "c.  chen , b.  he , y.  ye , and x.  yuan . the direct extension of admm for multi - block convex minimization problems is not necessarily convergent .",
    ", 155(1 - 2):5779 , 2016 .",
    "s.  s. chen , d.  l. donoho , and m.  a. saunders .",
    "atomic decomposition by basis pursuit .",
    ", 43(1):129159 , 2001 .",
    "y.  chen , g.  lan , and y.  ouyang .",
    "optimal primal - dual methods for a class of saddle point problems . , 24(4):17791814 , 2014 .",
    "p.  l. combettes and j .- c .",
    "stochastic quasi - fejr block - coordinate fixed point iterations with random sweeping .",
    ", 25(2):12211248 , 2015 .    c.  cortes and v.  vapnik",
    ". support - vector networks . , 20(3):273297 , 1995 .    c.  dang and g.  lan .",
    "randomized methods for saddle point computation . , 2014 .",
    "w.  deng , m .- j .",
    "lai , z.  peng , and w.  yin .",
    "parallel multi - block admm with o ( 1/k ) convergence . , 2013 .",
    "w.  deng and w.  yin .",
    "on the global and linear convergence of the generalized alternating direction method of multipliers . , 66(3):889916 , 2015 .",
    "d.  gabay and b.  mercier . a dual algorithm for the solution of nonlinear variational problems via finite element approximation .",
    ", 2(1):1740 , 1976 .",
    "x.  gao , y.  xu , and s.  zhang .",
    "randomized primal - dual proximal block coordinate updates .",
    ", 2016 .",
    "x.  gao and s.  zhang .",
    "first - order algorithms for convex optimization with nonseparate objective and coupled constraints .",
    ", 3:5 , 2015 .",
    "r.  glowinski and a.  marrocco .",
    "sur lapproximation , par elments finis dordre un , et la rsolution , par pnalisation - dualit dune classe de problmes de dirichlet non linaires .",
    ", 9(r2):4176 , 1975 .",
    "t.  goldstein , b.  odonoghue , s.  setzer , and r.  baraniuk .",
    "fast alternating direction optimization methods . , 7(3):15881623 , 2014 .    b.  he , l.  hou , and x.  yuan . on full jacobian decomposition of the augmented lagrangian method for separable convex programming .",
    ", 25(4):22742312 , 2015 .    b.  he , m.  tao , and x.  yuan .",
    "alternating direction method with gaussian back substitution for separable convex programming .",
    ", 22(2):313340 , 2012 .",
    "b.  he and x.  yuan . on the acceleration of augmented lagrangian method for linearly constrained optimization . , 2010 .",
    "b.  he and x.  yuan . on the @xmath170 convergence rate of the douglas - rachford alternating direction method .",
    ", 50(2):700709 , 2012 .",
    "g.  m. james , c.  paulson , and p.  rusmevichientong . penalized and constrained regression",
    ". technical report , 2013 .",
    "h.  li and z.  lin . optimal nonergodic @xmath171 convergence rate : when linearized adm meets nesterov s extrapolation . , 2016 .",
    "m.  li , d.  sun , and k .- c",
    ". toh . a convergent 3-block semi - proximal admm for convex minimization problems with one strongly convex block .",
    ", 32(04):1550024 , 2015 .",
    "t.  lin , s.  ma , and s.  zhang .",
    "on the global linear convergence of the admm with multiblock variables . ,",
    "25(3):14781497 , 2015 .",
    "z.  lu and l.  xiao . .",
    ", 152(1 - 2):615642 , aug 2015 .",
    "h.  markowitz .",
    "portfolio selection .",
    ", 7(1):7791 , 1952 .",
    "r.  d. monteiro and b.  f. svaiter .",
    "iteration - complexity of block - decomposition algorithms and the alternating direction method of multipliers .",
    ", 23(1):475507 , 2013 .",
    "y.  nesterov . a method of solving a convex programming problem with convergence rate @xmath172 . ,",
    "27(2):372376 , 1983 .",
    "y.  nesterov .",
    "efficiency of coordinate descent methods on huge - scale optimization problems .",
    ", 22(2):341362 , 2012 .",
    "y.  nesterov .",
    "gradient methods for minimizing composite functions .",
    ", 140(1):125161 , 2013 .",
    "y.  ouyang , y.  chen , g.  lan , and e.  pasiliao  jr . an accelerated linearized alternating direction method of multipliers . , 8(1):644681 , 2015 .",
    "z.  peng , t.  wu , y.  xu , m.  yan , and w.  yin .",
    "coordinate friendly structures , algorithms and applications .",
    ", 1(1):57119 , 2016 .",
    "z.  peng , y.  xu , m.  yan , and w.  yin .",
    "arock : an algorithmic framework for asynchronous parallel coordinate updates .",
    ", 38(5):a2851a2879 , 2016 .",
    "pesquet and a.  repetti . a class of randomized primal - dual algorithms for distributed optimization . , 2014",
    "p.  richtrik and m.  tak .",
    "parallel coordinate descent methods for big data optimization . , pages 152 , 2012 .",
    "p.  richtrik and m.  tak .",
    "iteration complexity of randomized block - coordinate descent methods for minimizing a composite function . , 144(1 - 2):138 , 2014 .",
    "y.  xu . accelerated first - order primal - dual proximal methods for linearly constrained composite convex programming .",
    ", 2016 .",
    "hybrid jacobian and gauss - seidel proximal block coordinate update methods for linearly constrained convex programming .",
    "in this section , we give the detailed proofs of the lemmas and theorems in section [ sec : ajadmm ] .      from , we have the optimality conditions @xmath173 or equivalently there is @xmath174 , @xmath175 hence , for any @xmath3 such that @xmath63 , @xmath176 where the first inequality uses the strong convexity of @xmath56 , and the second inequality is from @xmath63 and the following arguments using gradient lipschitz continuity and strong convexity of @xmath5 : @xmath177 rearranging gives ( by noting @xmath178 ) @xmath179 using the fact @xmath180 , we have @xmath181.\\end{aligned}\\ ] ] in addition , @xmath182\\end{aligned}\\ ] ] substituting and into and also applying to the cross term @xmath183 gives the inequality in .",
    "first note that @xmath184 since @xmath185 , it holds that @xmath186 in addition , from the assumption in , we have @xmath187\\nonumber \\\\ & = & \\frac{k_0 + 2}{2\\rho_1}\\|\\lambda-\\lambda^1\\|^2-\\frac{t+k_0 + 1}{2\\rho_t}\\|\\lambda-\\lambda^{t+1}\\|^2+\\sum_{k=2}^t \\left(\\frac{k+k_0 + 1}{2\\rho_k}-\\frac{k+k_0}{2\\rho_{k-1}}\\right)\\|\\lambda-\\lambda^k\\|^2 \\nonumber \\\\",
    "& \\le & \\frac{k_0 + 2}{2\\rho_1}\\|\\lambda-\\lambda^1\\|^2 .",
    "\\label{eq : ajadmm - g - lam}\\end{aligned}\\ ] ] finally , note that @xmath188-\\frac{1}{2}\\big[\\|x^{k+1}-x\\|_{p^k}^2-\\|x^k - x\\|_{p^k}^2\\big]\\right.\\nonumber \\\\ & & \\hspace{3.5cm}\\left.-\\frac{\\mu_g}{2}\\|x^{k+1}-x\\|^2-\\frac{\\mu_f}{2}\\|x^k - x\\|^2\\right)\\nonumber \\\\ & = & \\frac{k_0 + 2}{2}\\|x^1-x\\|_{p^1-\\beta_1 a^\\top a-\\mu_f i}^2-\\frac{t+k_0 + 1}{2}\\|x^{t+1}-x\\|_{p^t-\\beta_t a^\\top a+\\mu_g i}^2\\nonumber \\\\ & & + \\sum_{k=2}^t\\left((k+k_0 + 1)\\|x^k - x\\|_{p^k-\\beta_k a^\\top a-\\mu_f i}^2-(k+k_0)\\|x^k - x\\|_{p^{k-1}-\\beta_{k-1}a^\\top a+\\mu_g i}^2\\right)\\nonumber \\\\ & \\overset{\\eqref{eq : ajadmm - para-3}}\\le & ~\\frac{k_0 + 2}{2}\\|x^1-x\\|_{p^1-\\beta_1 a^\\top a-\\mu_f i}^2-\\frac{t+k_0 + 1}{2}\\|x^{t+1}-x\\|_{p^t-\\beta_t a^\\top a+\\mu_g i}^2 .",
    "\\label{eq : ajadmm - g - x}\\end{aligned}\\ ] ] now multiplying @xmath189 to both sides of and adding it over @xmath50 , we obtain by using through .      from the choice of @xmath66 , it follows that @xmath190 since @xmath191 the above equation implies @xmath192 which is equivalent to @xmath193\\preceq ( k+k_0)\\left[kp - k\\beta a^\\top a+(l_f+\\mu_g)i\\right].\\ ] ] hence , the condition in holds .",
    "in addition , it is easy to see that all conditions in and also hold . therefore , we have , which , by taking parameters in and @xmath194 , reduces to @xmath195 where we have used the fact @xmath52 .",
    "letting @xmath196 , we have from and that ( by dropping nonnegative terms on the left hand side ) : @xmath197 which indicates . in addition , from the convexity of @xmath28 , we have from that for any @xmath198 , @xmath199 which together with lemmas [ lem : xy - rate ] and [ lem : equiv - rate ] implies .",
    "the @xmath3-update in can be written more compactly as @xmath200 we have the optimality condition @xmath201 where @xmath202 is one subgradient of @xmath203 at @xmath204 .",
    "hence , for any @xmath3 , it holds that @xmath205 recall @xmath90 .",
    "note that @xmath206 ; therefore , @xmath207\\cr & & + \\theta{\\mathbb{e}}\\left[f(x^k)-f(x)+\\frac{\\mu_f}{2}\\|x^k - x\\|^2+\\big\\langle x^k - x ,- a^\\top(\\lambda^k-\\beta_k r^k)\\big\\rangle\\right]\\cr & = & { \\mathbb{e}}\\left [ f(x^{k+1})-f(x)+\\big\\langle x^{k+1}-x ,-",
    "r^k)\\big\\rangle\\right]- \\frac{l_m}{2}{\\mathbb{e}}\\|x^{k+1}-x^k\\|^2\\cr & & -(1-\\theta){\\mathbb{e}}\\left [ f(x^k)-f(x)+\\big\\langle x^k - x ,- a^\\top(\\lambda^k-\\beta_k r^k)\\big\\rangle\\right]+\\frac{\\theta\\mu_f}{2}{\\mathbb{e}}\\|x^k - x\\|^2\\cr & = &   { \\mathbb{e}}\\left [ f(x^{k+1})-f(x)+\\big\\langle x^{k+1}-x ,- a^\\top\\big(\\lambda^{k+1}-\\lambda^{k+1}+\\lambda^k-\\beta_k ( r^{k+1}-r^{k+1}+r^k)\\big)\\big\\rangle\\right]- \\frac{l_m}{2}{\\mathbb{e}}\\|x^{k+1}-x^k\\|^2\\cr & & -(1-\\theta){\\mathbb{e}}\\left [ f(x^k)-f(x)+\\big\\langle x^k - x ,- a^\\top(\\lambda^k-\\beta_k r^k)\\big\\rangle\\right]+\\frac{\\theta\\mu_f}{2}{\\mathbb{e}}\\|x^k - x\\|^2\\cr & = & { \\mathbb{e}}\\left [ f(x^{k+1})-f(x)-\\big\\langle a(x^{k+1}-x),\\lambda^{k+1}\\big\\rangle+(\\beta_k-\\rho_k)\\big\\langle a(x^{k+1}-x),r^{k+1}\\big\\rangle\\right.\\cr & & \\hspace{1cm}\\left.-\\beta_k\\big\\langle a(x^{k+1}-x),a(x^{k+1}-x^k)\\big\\rangle \\right]- \\frac{l_m}{2}{\\mathbb{e}}\\|x^{k+1}-x^k\\|^2\\cr & & -(1-\\theta){\\mathbb{e}}\\left [ f(x^k)-f(x)-\\big\\langle a(x^k - x),\\lambda^k\\big\\rangle+\\beta_k\\big\\langle a(x^k - x ) , r^k\\big\\rangle\\right]+\\frac{\\theta\\mu_f}{2}{\\mathbb{e}}\\|x^k - x\\|^2,\\label{eq : term1}\\end{aligned}\\ ] ] where in the inequality , we have used the strong convexity of @xmath5 and lipschitz continuity of @xmath54 , and the last equality follows from the definition of @xmath208 and the update of @xmath151 . also , from the strong convexity of @xmath56 , we have @xmath209\\cr & = & { \\mathbb{e}}\\left [ g(x^{k+1 } ) - g(x^k ) + g_{s_k}(x_{s_k}^k)- g_{s_k}(x_{s_k } ) + \\frac{\\mu_g}{2}\\|x^{k+1}-x\\|^2-\\frac{\\mu_g}{2}\\|x^k - x\\|^2 + \\sum_{i\\in",
    "s_k}\\frac{\\mu_g}{2}\\|x_i^k - x_i\\|^2\\right]\\cr & = & { \\mathbb{e}}\\left [ g(x^{k+1 } ) - g(x^k ) + \\theta\\big(g(x^k)- g(x)\\big ) + \\frac{\\mu_g}{2}\\|x^{k+1}-x\\|^2-\\frac{\\mu_g}{2}\\|x^k - x\\|^2 + \\frac{\\theta\\mu_g}{2}\\|x^k - x\\|^2\\right]\\cr & = & { \\mathbb{e}}\\left [ g(x^{k+1 } ) - g(x)+\\frac{\\mu_g}{2}\\|x^{k+1}-x\\|^2\\right]-(1-\\theta)\\left [ g(x^k ) - g(x)+\\frac{\\mu_g}{2}\\|x^k - x\\|^2\\right ] .",
    "\\label{eq : term2}\\end{aligned}\\ ] ] in addition , note that @xmath210    plugging through into and rearranging terms gives @xmath211\\cr & \\le & ( 1-\\theta){\\mathbb{e}}\\left[f(x^k)-f(x)-\\big\\langle a(x^k - x),\\lambda^k\\big\\rangle+\\beta_k\\big\\langle a(x^k - x ) , r^k\\big\\rangle+\\frac{\\mu_g}{2}\\|x^k - x\\|^2\\right]+\\frac{l_m}{2}{\\mathbb{e}}\\|x^{k+1}-x^k\\|^2\\cr & & -\\frac{\\theta\\mu_f}{2}{\\mathbb{e}}\\|x^k - x\\|^2+{\\mathbb{e}}\\left[\\beta_k\\big\\langle a(x^{k+1}-x),a(x^{k+1}-x^k)\\big\\rangle-\\eta_k \\big\\langle x^{k+1}-x , x^{k+1}-x^k\\big\\rangle\\right].\\end{aligned}\\ ] ] since @xmath63 , the above inequality reduces to by using .",
    "multiplying @xmath189 to both sides of and summing it up from @xmath215 through @xmath8 gives @xmath216\\cr & & + \\sum_{k=2}^t\\big(\\theta(k+k_0 + 1)-1\\big){\\mathbb{e}}\\left[\\phi(x^k , x,\\lambda^k)+\\frac{\\mu_g}{2}\\|x^k - x\\|^2\\right]\\cr & & + \\sum_{k=2}^t\\big((\\beta_{k-1}-\\rho_{k-1})(k+k_0)-(1-\\theta)(k+k_0 + 1)\\beta_k\\big){\\mathbb{e}}\\| r^k\\|^2\\cr & \\le & ( 1-\\theta)(k_0 + 2){\\mathbb{e}}\\left[\\phi(x^1,x,\\lambda^1)+\\beta_1\\| r^1\\|^2+\\frac{\\mu_g}{2}\\|x^1-x\\|^2\\right ] \\label{eq : sum - bd2 } \\\\ & & + \\sum_{k=1}^t\\frac{\\beta_k(k+k_0 + 1)}{2}{\\mathbb{e}}\\left(\\|a(x^{k+1}-x)\\|^2-\\|a(x^k - x)\\|^2+\\|a(x^{k+1}-x^k)\\|^2\\right)\\cr & & -\\sum_{k=1}^t\\frac{k+k_0 + 1}{2}{\\mathbb{e}}\\left(\\eta_k\\|x^{k+1}-x\\|^2-(\\eta_k-\\theta\\mu_f)\\|x^k - x\\|^2 + ( \\eta_k - l_m)\\|x^{k+1}-x^k\\|^2\\right ) .",
    "\\nonumber\\end{aligned}\\ ] ] from the update of @xmath198 in , we have @xmath217 and thus @xmath218 adding to and rearranging terms yields @xmath219 \\label{eq : sum - bd3 } \\\\ & & + \\sum_{k=1}^t\\frac{\\beta_k(k+k_0 + 1)}{2}{\\mathbb{e}}\\left(\\|a(x^{k+1}-x)\\|^2-\\|a(x^k - x)\\|^2+\\|a(x^{k+1}-x^k)\\|^2\\right)\\cr & & -\\sum_{k=1}^t\\frac{k+k_0 + 1}{2}{\\mathbb{e}}\\left(\\eta_k\\|x^{k+1}-x\\|^2-(\\eta_k-\\theta\\mu_f)\\|x^k - x\\|^2 + ( \\eta_k - l_m)\\|x^{k+1}-x^k\\|^2\\right)\\cr & & -\\frac{\\mu_g(t+k_0 + 1)}{2}{\\mathbb{e}}\\|x^{t+1}-x\\|^2-\\sum_{k=2}^t\\frac{\\mu_g\\big(\\theta(k+k_0 + 1)-1\\big)}{2}{\\mathbb{e}}\\|x^k - x\\|^2\\cr & & -\\frac{t+k_0 + 1}{2\\rho_t}{\\mathbb{e}}\\left(\\|\\lambda^{t+1}-\\lambda\\|^2-\\|\\lambda^t-\\lambda\\|^2+\\|\\lambda^{t+1}-\\lambda^t\\|\\right)\\cr & & -\\sum_{k=2}^t\\frac{\\theta(k+k_0 + 1)-1}{2\\rho_{k-1}}{\\mathbb{e}}\\left(\\|\\lambda^k-\\lambda\\|^2-\\|\\lambda^{k-1}-\\lambda\\|^2+\\|\\lambda^k-\\lambda^{k-1}\\|^2\\right ) .",
    "\\nonumber\\end{aligned}\\ ] ] therefore , we obtain by substituting through into and also noting the summation in the second line of is nonnegative from the condition in .",
    "we first show that the parameters given in satisfy the conditions in .",
    "note that implies @xmath220 , and thus must hold . also , it is easy to see that holds with equality from the second equation of .",
    "since @xmath221 , we can easily have by plugging in @xmath44 and @xmath83 defined in and respectively .        note that @xmath225 for @xmath226 from their definition given in and . hence , becomes @xmath227 where the sufficient condition in uses the fact @xmath228 , and the last inequality apparently holds .",
    "hence , is satisfied .",
    "therefore , we have the inequality in that , as @xmath52 , reduces to @xmath230+\\frac{k_0 + 2}{2}(\\eta_1-\\theta\\mu_f){\\mathbb{e}}\\|x^1-x\\|^2 \\label{eq : sum - bd4 - 1 } \\\\ & & + \\frac{\\theta(k_0 + 3)-1}{2\\rho_1}{\\mathbb{e}}\\|\\lambda\\|^2-\\frac{t+k_0 + 1}{2}{\\mathbb{e}}\\|x^{t+1}-x\\|_{(\\mu_g+\\eta_t ) i-\\beta_t a^\\top a}^2\\nonumber.\\end{aligned}\\ ] ]    for @xmath231 , we have @xmath232 letting @xmath194 and using the convexity of @xmath28 , we have from and the above inequality that @xmath233\\le \\frac{1}{t}{\\mathbb{e}}\\phi(x^*,\\lambda),\\,\\forall \\lambda,\\end{aligned}\\ ] ] which together with lemmas [ lem : xy - rate ] and [ lem : equiv - rate ] with @xmath234 indicates .",
    "similar to , we have @xmath238- \\frac{l_m}{2}{\\mathbb{e}}\\|x^{k+1}-x^k\\|^2\\cr & & -(1-\\theta){\\mathbb{e}}\\left [ f(x^k)-f(x)-\\big\\langle a(x^k - x),\\lambda^k\\big\\rangle+\\beta\\big\\langle a(x^k - x ) , r^k\\big\\rangle\\right]+\\frac{\\theta\\mu_f}{2}{\\mathbb{e}}\\|x^k - x\\|^2 .",
    "\\label{gradf - term2}\\end{aligned}\\ ] ]      plugging , and with @xmath244 into with @xmath245 and summing together with , we have , by rearranging the terms , that @xmath246\\cr & & -\\frac{l_m}{2}{\\mathbb{e}}\\|x^{k+1}-x^k\\|^2+\\frac{\\theta\\mu_f}{2}{\\mathbb{e}}\\|x^k - x\\|^2+\\frac{\\mu_g}{2}{\\mathbb{e}}\\|x^{k+1}-x\\|^2\\cr & & + ( \\beta -\\rho){\\mathbb{e}}\\left[\\langle x^{k+1}- x ,   a^\\top r^{k+1}\\rangle + \\langle y^{k+1}- y , b^\\top r^{k+1}\\rangle\\right ] \\cr & & + { \\mathbb{e}}\\left\\langle x^{k+1}- x , ( \\eta_x i- \\beta a^\\top a ) ( x^{k+1}- x^k)\\right\\rangle+{\\mathbb{e}}\\left\\langle y^{k+1}- y,(\\eta_y i-",
    "\\beta   b^\\top b)(y^{k+1}-y^k)\\right\\rangle\\cr & \\le & ( 1-\\theta){\\mathbb{e}}\\left[f(x^k)-f(x)+\\langle y^k- y,\\nabla h(y^k)\\rangle+\\langle x^k- x , - a^\\top{\\lambda}^k\\rangle+\\langle y^k- y,- b^\\top{\\lambda}^k\\rangle\\right]\\cr & & + \\frac{(1-\\theta)\\mu_g}{2}{\\mathbb{e}}\\|x^k - x\\|^2+\\beta(1 -\\theta){\\mathbb{e}}\\left[\\langle x^{k}- x ,   a^\\top{r}^k\\rangle+\\langle y^k- y , b^\\top r^k\\rangle\\right]\\cr & & + \\beta { \\mathbb{e}}\\left\\langle a(x^{k+1}- x ) ,   b ( y^{k+1}- y^k)\\right\\rangle+\\beta(1 -\\theta){\\mathbb{e}}\\left\\langle b(y^k- y),a(x^{k+1}-x^k)\\right\\rangle.\\end{aligned}\\ ] ] now use and the fact that @xmath127 to have the desired result .",
    "[ lem : bd - lam - by - y ] for any @xmath250 , we have @xmath251 + 2(\\beta-\\rho)^2{\\mathbb{e}}\\|b^\\top r^{k+1}\\|^2\\\\ & & + 2\\rho^2(1-\\theta)(1+\\frac{1}{\\delta}){\\mathbb{e}}\\big[\\|b^\\top r^{k+1}\\|^2+\\|b^\\top b(y^{k+1}-y^k)\\|^2\\big].\\nonumber\\end{aligned}\\ ] ]          letting @xmath255 in , plugging into it , noting @xmath256 , and using , we have @xmath257\\nonumber\\\\ & & + \\frac{1}{2}{\\mathbb{e}}\\big[\\|y^{k+1}- y^*\\|_q^2-\\|y^{k}- y^*\\|_q^2+\\|y^{k+1}- y^k\\|_q^2\\big]-\\frac{l_m}{2}{\\mathbb{e}}\\|x^{k+1}-x^k\\|^2+\\frac{\\theta\\mu_f}{2}{\\mathbb{e}}\\|x^k - x^*\\|^2\\nonumber\\\\ & & + \\frac{\\mu_g}{2}{\\mathbb{e}}\\|x^{k+1}-x^*\\|^2+\\frac{1}{2\\rho}{\\mathbb{e}}\\big[\\|\\lambda^{k+1}-\\lambda^*\\|^2-\\|\\lambda^{k}-\\lambda^*\\|^2+\\|\\lambda^{k+1}-\\lambda^k\\|^2\\big]\\nonumber\\\\ & \\le & ( 1-\\theta){\\mathbb{e}}\\psi(z^k , z^*)+\\beta(1 -\\theta){\\mathbb{e}}\\|r^k\\|^2+\\frac{1-\\theta}{2\\rho}{\\mathbb{e}}\\big[\\|\\lambda^{k}-\\lambda^*\\|^2-\\|\\lambda^{k-1}-\\lambda^*\\|^2+\\|\\lambda^{k}-\\lambda^{k-1}\\|^2\\big ] \\label{lin - ineq2 - 1y } \\\\ & & + \\beta { \\mathbb{e}}\\big\\langle a(x^{k+1}- x^ * ) ,   b ( y^{k+1}- y^k)\\big\\rangle+\\beta(1 -\\theta){\\mathbb{e}}\\big\\langle b(y^k- y^*),a(x^{k+1}-x^k)\\big\\rangle+\\frac{\\mu_g(1-\\theta)}{2}{\\mathbb{e}}\\|x^k - x^*\\|^2 , \\nonumber\\end{aligned}\\ ] ] where @xmath258 is defined in . adding @xmath259 to both sides of and using",
    "gives @xmath260\\nonumber\\\\ & & -\\frac{l_m}{2}{\\mathbb{e}}\\|x^{k+1}-x^k\\|^2+\\frac{\\theta\\mu_f}{2}{\\mathbb{e}}\\|x^k - x^*\\|^2+\\frac{\\mu_g}{2}{\\mathbb{e}}\\|x^{k+1}-x^*\\|^2\\nonumber\\\\ & & + \\frac{1}{2}{\\mathbb{e}}\\big[\\|y^{k+1}- y^*\\|_q^2-\\|y^{k}- y^*\\|_q^2+\\|y^{k+1}- y^k\\|_q^2\\big]\\nonumber\\\\ & & + \\frac{1}{2\\rho}{\\mathbb{e}}\\big[\\|\\lambda^{k+1}-\\lambda^*\\|^2-(1-\\theta)\\|\\lambda^{k}-\\lambda^*\\|^2 + \\frac{1}{\\theta}\\|\\lambda^{k+1}-\\lambda^k\\|^2\\big]-\\frac{\\rho}{2}(\\frac{1}{\\theta}-1){\\mathbb{e}}\\|r^{k+1}\\|^2\\cr & \\le & ( 1-\\theta){\\mathbb{e}}\\psi(z^k , z^*)+\\beta(1 -\\theta){\\mathbb{e}}\\|r^k\\|^2\\\\ & & + \\frac{1}{2\\rho}{\\mathbb{e}}\\big[\\|\\lambda^{k}-\\lambda^*\\|^2-(1-\\theta)\\|\\lambda^{k-1}-\\lambda^*\\|^2 + \\frac{1}{\\theta}\\|\\lambda^{k}-\\lambda^{k-1}\\|^2\\big]-\\frac{\\rho}{2}(\\frac{1}{\\theta}-(1-\\theta)){\\mathbb{e}}\\|r^k\\|^2\\nonumber\\\\ & & + \\beta { \\mathbb{e}}\\big\\langle a(x^{k+1}- x^ * ) ,   b ( y^{k+1}- y^k)\\big\\rangle+\\beta(1 -\\theta){\\mathbb{e}}\\big\\langle b(y^k- y^*),a(x^{k+1}-x^k)\\big\\rangle+\\frac{\\mu_g(1-\\theta)}{2}{\\mathbb{e}}\\|x^k - x^*\\|^2.\\nonumber\\end{aligned}\\ ] ]    adding , , and multiplied by the number @xmath261 to , and also noting @xmath133 , we have @xmath262\\nonumber\\\\ & & -\\frac{l_m}{2}{\\mathbb{e}}\\|x^{k+1}-x^k\\|^2+\\frac{\\theta\\mu_f}{2}{\\mathbb{e}}\\|x^k - x^*\\|^2+\\frac{\\mu_g}{2}{\\mathbb{e}}\\|x^{k+1}-x^*\\|^2\\nonumber\\\\ & & + \\frac{1}{2}{\\mathbb{e}}\\big[\\|y^{k+1}- y^*\\|_q^2-\\|y^{k}- y^*\\|_q^2+\\|y^{k+1}- y^k\\|_q^2\\big]\\nonumber\\\\ & & - 4c{\\mathbb{e}}\\left(\\big[l_h^2\\|y^{k+1}-y^*\\|^2+\\|q(y^{k+1}-y^k)\\|^2\\big]+\\frac{\\rho^2}{2}(1-\\theta)(1+\\frac{1}{\\delta})\\|b^\\top b(y^{k+1}-y^k)\\|^2\\right)\\cr & & + \\frac{1}{2\\rho}{\\mathbb{e}}\\left[\\|\\lambda^{k+1}-\\lambda^*\\|^2-(1-\\theta)\\|\\lambda^{k}-\\lambda^*\\|^2+\\frac{1}{\\theta}\\|\\lambda^{k+1}-\\lambda^k\\|^2\\right]\\cr & & + c{\\mathbb{e}}\\left[\\|b^\\top(\\lambda^{k+1}-\\lambda^*)\\|^2-(1-\\theta)(1+\\delta)\\|b^\\top(\\lambda^k-\\lambda^*)\\|^2+\\kappa\\|b^\\top(\\lambda^{k+1}-\\lambda^k)\\|^2\\right]\\cr & & -c\\kappa{\\mathbb{e}}\\|b^\\top(\\lambda^{k+1}-\\lambda^k)\\|^2 - 2c\\rho^2(1-\\theta)(1+\\frac{1}{\\delta}){\\mathbb{e}}\\|b^\\top r^{k+1}\\|^2 - 2c(\\beta-\\rho)^2{\\mathbb{e}}\\|b^\\top r^{k+1}\\|^2\\cr & \\le & ( 1-\\theta){\\mathbb{e}}\\psi(z^k , z^*)+\\frac{\\beta(1 -\\theta-\\theta^2)}{2}{\\mathbb{e}}\\|r^k\\|^2+\\beta { \\mathbb{e}}\\left(\\frac{1}{2\\tau_1}\\|a(x^{k+1}-x^*)\\|^2+\\frac{\\tau_1}{2}\\|b(y^{k+1}-y^k)\\|^2\\right)\\nonumber\\\\ & & + \\frac{1}{2\\rho}{\\mathbb{e}}\\big[\\|\\lambda^{k}-\\lambda^*\\|^2-(1-\\theta)\\|\\lambda^{k-1}-\\lambda^*\\|^2+\\frac{1}{\\theta}\\|\\lambda^{k}-\\lambda^{k-1}\\|^2\\big]\\cr & & + \\beta(1 -\\theta){\\mathbb{e}}\\left(\\frac{1}{2\\tau_2}\\|b(y^k- y^*)\\|^2+\\frac{\\tau_2}{2}\\|a(x^{k+1}-x^k)\\|^2\\right)+\\frac{\\mu_g(1-\\theta)}{2}{\\mathbb{e}}\\|x^k - x^*\\|^2.\\end{aligned}\\ ] ] from and , we have @xmath263 and @xmath264 and from , it follows that @xmath265 in addition , note that @xmath266 adding through and multiplierd by @xmath159 to and rearranging terms gives .",
    "first , from @xmath134 , the full row - rankness of @xmath112 , and , it is easy to see that @xmath267 .",
    "second , we note the following inequalities : @xmath268\\\\ & \\ge & \\frac{\\eta}{2\\rho}\\left[\\|\\lambda^{k+1}-\\lambda^*\\|^2-(1-\\theta)\\|\\lambda^{k}-\\lambda^*\\|^2+\\frac{1}{\\theta}\\|\\lambda^{k+1}-\\lambda^k\\|^2\\right].\\end{aligned}\\ ] ] summing the above inequalities and using gives and completes the proof",
    ".        then @xmath271+(1-\\theta)(1+\\delta){\\mathbb{e}}\\|b^\\top(\\lambda^k-\\lambda^*)\\|^2\\cr & & + 2\\rho^2(1-\\theta)(1+\\frac{1}{\\delta}){\\mathbb{e}}\\big[\\|b^\\top r^{k+1}\\|^2+\\|b^\\top b(y^{k+1}-y^k)\\|^2\\big]+2(\\beta-\\rho)^2{\\mathbb{e}}\\|b^\\top r^{k+1}\\|^2,\\end{aligned}\\ ] ] which completes the proof .",
    "note that @xmath272^\\top\\left[\\begin{array}{cc}(1-(1-\\theta)(1+\\delta))i & ( 1-\\theta)(1+\\delta)i\\\\(1-\\theta)(1+\\delta)i&(\\kappa-(1-\\theta)(1+\\delta))i\\end{array}\\right]\\left[\\begin{array}{c}b^\\top(\\lambda^{k+1}-\\lambda^*)\\\\b^\\top(\\lambda^{k+1}-\\lambda^k ) \\end{array}\\right]\\cr & \\overset{\\eqref{eq : cond - kappa - del}}\\ge&\\frac{1}{2}\\left[\\begin{array}{c}b^\\top(\\lambda^{k+1}-\\lambda^*)\\\\b^\\top(\\lambda^{k+1}-\\lambda^k ) \\end{array}\\right]^\\top\\left[\\begin{array}{cc}\\theta i & ( 1-\\theta)i\\\\ ( 1-\\theta)i & ( \\frac{1}{\\theta}-(1-\\theta))i\\end{array}\\right]\\left[\\begin{array}{c}b^\\top(\\lambda^{k+1}-\\lambda^*)\\\\b^\\top(\\lambda^{k+1}-\\lambda^k ) \\end{array}\\right]\\cr & = & \\frac{1}{2}\\left[\\begin{array}{c}\\lambda^{k+1}-\\lambda^*\\\\\\lambda^{k+1}-\\lambda^k \\end{array}\\right]^\\top\\left[\\begin{array}{cc}\\theta bb^\\top & ( 1-\\theta)bb^\\top\\\\ ( 1-\\theta)bb^\\top & ( \\frac{1}{\\theta}-(1-\\theta))bb^\\top\\end{array}\\right]\\left[\\begin{array}{c}\\lambda^{k+1}-\\lambda^*\\\\ \\lambda^{k+1}-\\lambda^k \\end{array}\\right]\\cr & \\ge & \\frac{\\sigma_{\\min}(bb^\\top)}{2}\\left[\\begin{array}{c}\\lambda^{k+1}-\\lambda^*\\\\\\lambda^{k+1}-\\lambda^k \\end{array}\\right]^\\top\\left[\\begin{array}{cc}\\theta i & ( 1-\\theta)i\\\\ ( 1-\\theta)i & ( \\frac{1}{\\theta}-(1-\\theta))i\\end{array}\\right]\\left[\\begin{array}{c}\\lambda^{k+1}-\\lambda^*\\\\ \\lambda^{k+1}-\\lambda^k \\end{array}\\right]\\cr & = & \\frac{\\sigma_{\\min}(bb^\\top)}{2}\\left[\\|\\lambda^{k+1}-\\lambda^*\\|^2-(1-\\theta)\\|\\lambda^{k}-\\lambda^*\\|^2+\\frac{1}{\\theta}\\|\\lambda^{k+1}-\\lambda^k\\|^2\\right],\\end{aligned}\\ ] ] which completes the proof ."
  ],
  "abstract_text": [
    "<S> block coordinate update ( bcu ) methods enjoy low per - update computational complexity because every time only one or a few block variables would need to be updated among possibly a large number of blocks . </S>",
    "<S> they are also easily parallelized and thus have been particularly popular for solving problems involving large - scale dataset and/or variables . in this paper </S>",
    "<S> , we propose a primal - dual bcu method for solving linearly constrained convex program in multi - block variables . </S>",
    "<S> the method is an accelerated version of a primal - dual algorithm proposed by the authors , which applies randomization in selecting block variables to update and establishes an @xmath0 convergence rate under weak convexity assumption . </S>",
    "<S> we show that the rate can be accelerated to @xmath1 if the objective is strongly convex . </S>",
    "<S> in addition , if one block variable is independent of the others in the objective , we then show that the algorithm can be modified to achieve a linear rate of convergence . </S>",
    "<S> the numerical experiments show that the accelerated method performs stably with a single set of parameters while the original method needs to tune the parameters for different datasets in order to achieve a comparable level of performance .    </S>",
    "<S> * keywords : * primal - dual method , block coordinate update , alternating direction method of multipliers ( admm ) , accelerated first - order method .    </S>",
    "<S> * mathematics subject classification : * 90c25 , 95c06 , 68w20 . </S>"
  ]
}