{
  "article_text": [
    "randomized nomination sampling ( rns ) is a rank - based sampling scheme .",
    "rank - based sampling schemes are data collection techniques which utilize the advantage of additional information available in the population to provide an artificially stratified sample with more structure .",
    "providing more structured sample enables us to direct our attention toward units that represent the underlying population .",
    "let @xmath3 be an absolutely continuous random variable distributed according to the cumulative distribution function ( cdf ) @xmath4 and the probability density function ( pdf ) @xmath5 , where @xmath6 is known and @xmath7 ( p - dimensional euclidean space ) , is unknown .",
    "further , let @xmath8 be a sequence of independent random variables taking values in @xmath9 ( the natural numbers ) with probabilities @xmath10 so that @xmath11 , @xmath12 .",
    "let @xmath13 be a sequence of independent bernoulli random variables with success probability @xmath14 $ ] , independent of @xmath15 and @xmath3 .",
    "the rns design consists of drawing @xmath16 random sample sets of size @xmath15 , @xmath17 , from the population for the purpose of ranking and finally nominating @xmath16 sampling units ( one from each set ) for final measurement .",
    "the nominee from the @xmath18-th set is the largest unit of the set with probability @xmath19 or the smallest unit of the set with probability @xmath20 .",
    "the rns observation @xmath21 can be written as @xmath22 where @xmath23 and @xmath24 are respectively the smallest and the largest units of the @xmath25 set of size @xmath15 .",
    "rns was introduced by @xcite and applied to the problem of estimating population total in finite population .",
    "later , this sampling scheme was applied in constructing confidence intervals for quantiles in finite populations @xcite and infinite populations @xcite , as well as in constructing tolerance intervals @xcite .",
    "some well - known examples of rns are given below :    * the choice of @xmath26 , @xmath17 , results in the srs design with observations denoted by @xmath27 , @xmath17 . *",
    "the choice of @xmath28 nominates the maximum from each set and results in a maxima nomination sampling ( mans ) design ( see @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; and @xcite ) . *",
    "the choice of @xmath29 nominates the minimum from each set and results in a minima nomination sampling ( mins ) design ( see @xcite and @xcite ) . *",
    "the choice of @xmath30 and @xmath31 , for a constant @xmath32 , results in a randomized extreme ranked set sampling ( rerss ) design ( see @xcite ) . * the choice of @xmath33 and @xmath34 , and @xmath35 , where @xmath17 , for a constant @xmath32 and an even number @xmath16 , results in an extreme ranked set sampling ( erss ) design ( see @xcite , and @xcite ) * the choice of @xmath33 and @xmath34 , and @xmath36 , where @xmath17 , for an even number @xmath16 results in a moving extreme ranked set sampling ( mrss ) design ( see @xcite ) .",
    "note that @xmath37 , @xmath17 , in ( 5 ) and ( 6 ) are no longer independent and identically distributed ( iid ) .",
    "rns is a cost - effective method of selecting data in situations in which measuring the characteristic of interest is difficult , expensive and/or destructive , but a small number of sampling units can be cheaply ranked and the minimum and the maximum observations can be easily identified . unlike the regular ranked set sampling ( rss ) , rns allows for an increase of the set size without introducing too much ranking error . identifying only the extremes , rather than providing a complete ranking on the units in the set , is more practical , since we need to identify successfully only the first or the last ordered unit . regarding the randomness of the set size , while the rns technique allows one to select the sets of fixed size @xmath38 , i.e. @xmath39 for some fixed @xmath38 and @xmath17 , providing the flexibility of choosing the sizes in random helps to apply the rns scheme in circumstances where the set size might be random ( see @xcite and @xcite ) .",
    "another advantage of allowing the set size to be random is that , when @xmath40 , randomized nomination sample is expected to contain a simple random sample of size @xmath41 in addition to a collection of extremal order statistics from various set sizes , which contain more information about the underlying population than srs .",
    "rns also has the flexibility to adjust the proportion of maximums and minimums by choosing an appropriate value for @xmath14 $ ] based on the population shape .",
    "this reduces the concern in working with extremes when the underlying population is skewed .",
    "the rns - based statistical inference may be made under various situations .",
    "for example , there might be the case where @xmath42 s are the only available information and no further information is provided on either @xmath43 or @xmath44 , @xmath17 .",
    "there might also be situations in which the size of sets or the number of maximums ( and subsequently the number of minimums ) , or both are chosen in advance , instead of getting involved in a randomized process .",
    "in the situation where @xmath43 and/or @xmath44 are known , the cdf of @xmath21 can be found by conditioning on @xmath43 and @xmath44 , or both .",
    "the conditioning argument makes the theoretical investigation more complicated , but it provides more efficient statistical inference . in this paper ,",
    "both unconditional and conditional rns are studied .",
    "four types of rns data are introduced , corresponding to situations where , for any set @xmath17 , ( 1 ) the triplet @xmath45 are all known , ( 2 ) only @xmath46 are known , ( 3 ) only @xmath47 are known , or ( 4 ) only @xmath42 are known .",
    "these types of rns data are , respectively , called rns complete - data , type i , type ii , and type iii rns incomplete - data .",
    "we discuss rns - based maximum likelihood ( ml ) and method of moments ( mm ) estimates of the population parameters when the underlying random variable follows the proportional hazard rate ( phr ) or proportional reverse hazard rate ( prhr ) model .",
    "let @xmath48 be an absolutely continuous probability distribution function with density @xmath49 , possibly depending on an unknown vector of parameters @xmath1 , and let @xmath50 and @xmath51 ( where , by convention , we take @xmath52 ) so that , if @xmath53 , we have @xmath54 with @xmath55 the family of phr models ( based on @xmath48 ) is given by the family of distributions @xmath56^{1/\\gamma({\\pmb{\\theta}})},\\end{aligned}\\ ] ] where @xmath57 . similarly , the family of prhr models is given by the family of distributions @xmath58^{1/\\gamma({\\pmb{\\theta}})}.\\end{aligned}\\ ] ] the hazard rate function @xmath59 and the reverse hazard rate function @xmath60 at @xmath61 are given respectively by @xmath62 the phr and prhr models in ( [ hr - cdf ] ) and ( [ ihr - cdf ] ) are well - known in lifetime experiments .",
    "the lifetime distribution of a system and its components are of interest in reliability testing .",
    "statistical analysis of the lifetime of a system or its components is an important topic in many research areas such as engineering , marketing and biomedical sciences .",
    "see , for example , @xcite , @xcite , @xcite , @xcite and @xcite .",
    "the phr and prhr models include several well - known lifetime distributions . in the sequel , we are interested in estimating @xmath63 with specific choices of @xmath48 . some examples of hazard and reverse hazard rate models are presented in table [ examples ] .",
    "the remainder of the paper is organized as follows . in section [ mle ]",
    ", we investigate the rns complete - data and provide the pdf and cdf of an rns observation in the form of complete - data .",
    "we also derive the ml estimators of @xmath63 in the phr and prhr model when the triplet @xmath45 , @xmath17 , is available . in section [ mle - incom ] , we present the ml estimation for the parameters based on incomplete rns data .",
    "we provide the pdf and cdf of observations in each rns incomplete - data and use the em algorithm to obtain the ml estimators of the parameters of interest . in section [ mme ] , we derive the rns - based mm estimation in the phr and prhr models ; when the rns data are from either complete- or incomplete - data scenarios . in section [ numeric ]",
    ", we illustrate the numerical results in detail and compare the performance of the rns - based estimators with the corresponding srs estimators for the exponential and beta distributions .",
    "moreover , in section [ numeric ] , the performance of rns - based ml estimators in a more complicated situation is investigated using a real life dataset on fish mercury contamination measurement .",
    "let @xmath64 be a simple random sample of size @xmath16 from a continuous distribution with cdf @xmath4 and pdf @xmath5 .",
    "if it exists , the srs - based ml estimator of @xmath1 , denoted by @xmath65 , satisfies the ml equations @xmath66 where @xmath67 let @xmath68 be a randomized nomination sample of size @xmath16 from @xmath6 .",
    "the forms of the cdfs and pdfs of @xmath21 s , in addition to the rns - based ml equations , are determined by the available rns data . in this section ,",
    "we use the rns complete - data to derive the ml estimator of @xmath1 . in the rns complete - data case ,",
    "the triplets @xmath45 , @xmath69 , are known . in other words",
    ", one knows that , for @xmath17 , the observed value @xmath70 is from a set of size @xmath71 with the value @xmath72 and the rank @xmath73 in the @xmath18-th set , where @xmath43 and @xmath44 are both known .",
    "an rns observation @xmath74 given @xmath75 and @xmath76 , where @xmath77 and @xmath78 , has the cdf @xmath79 and the pdf @xmath80 as follows @xmath81 and @xmath82 the log likelihood function based on the rns complete - data is given by @xmath83    upon differentiation of ( [ loglcomplete ] ) with respect to @xmath84 and equating the result to zero , the ( complete ) ml estimator of @xmath0 , denoted by @xmath85 , is obtained from @xmath86 where @xmath87 . since both @xmath88 and @xmath89",
    "are involved in the rns likelihood , equation ( [ dlc ] ) is more complicated to solve for @xmath0 than ( [ srs - mle ] ) , and for most distributions there is no closed form expressions for the ml estimators .",
    "following the idea proposed by @xcite , we consider the modified ml ( mml ) estimators of parameters . depending on the underlying distribution , one may need to replace one or both of the second and third terms on the left - hand side of ( [ dlc ] ) by their corresponding expected values .",
    "the obtained mml estimator of @xmath1 is denoted by @xmath90 .",
    "let @xmath27 , @xmath17 , be a sequence of iid random variables from the family of phr models in ( [ hr - cdf ] ) .",
    "the srs - based ml estimator of @xmath91 , denoted by @xmath92 , can be expressed as @xmath93 which is an unbiased estimator of @xmath63 with variance @xmath94=\\gamma^2({{\\pmb{\\theta}}})/m$ ] . under the model ( [ hr - cdf ] ) , the pdf of a random variable @xmath21 , @xmath17 , from the rns complete - data is @xmath95^{\\frac{1}{\\gamma({{\\pmb{\\theta}}})}}\\right)^{\\alpha_i-1}\\left(1-[\\bar f_0(y_i)]^{\\frac{1}{\\gamma({{\\pmb{\\theta}}})}}\\right)^{\\beta_i},\\end{aligned}\\ ] ] where @xmath96 and @xmath97 .",
    "the rns - based complete ml estimator of @xmath63 , denoted by @xmath98 , is obtained by solving the equation @xmath99^{\\frac{1}{\\gamma({{\\pmb{\\theta}}})}}}{1-[\\bar f_0(y_i)]^{\\frac{1}{\\gamma({{\\pmb{\\theta}}})}}}\\log \\bar f_0(y_i)+(\\alpha_i-2)\\log \\bar f_0(y_i)\\right\\}=0.\\end{aligned}\\ ] ] note that if @xmath100 , for all @xmath17 , one can easily obtain a complete rns - based ml estimator of @xmath91 as described in the following lemma .",
    "let @xmath64 be iid random variables from ( [ hr - cdf ] ) , and suppose @xmath101=@xmath102 , ",
    ", @xmath103=@xmath102 is the corresponding mins sample of size @xmath16 .",
    "the complete ml estimator of @xmath63 is given by @xmath104 which is an unbiased estimator of @xmath63 with the variance equal to its srs - based counterpart , i.e. , @xmath105= \\frac{\\gamma^2({{\\pmb{\\theta}}})}{m}.\\end{aligned}\\ ] ]    from ( [ hr - complete ] ) , by replacing @xmath100 for @xmath17 , the complete ml estimate for @xmath63 in ( [ hr - min ] ) is obtained .",
    "noting that in the phr model we have @xmath106&=k_i\\text{b}(\\alpha_i-1 , \\beta_i+1){\\mathrm{e}}(\\log w_i)\\gamma({\\pmb{\\theta}}),\\end{aligned}\\ ] ] and @xmath107=k_i\\text{b}(\\alpha_i-1 , \\beta_i+1){\\mathrm{e}}(\\log^2 w_i)\\gamma^2({\\pmb{\\theta}}),\\end{aligned}\\ ] ] where @xmath108 , @xmath109 and @xmath110=\\mathfrak{d}(\\alpha_i-1)-\\mathfrak{d}(\\alpha_i+\\beta_i),\\end{aligned}\\ ] ] and @xmath111=\\mathfrak{d}'(\\alpha_i-1)-\\mathfrak{d}'(\\alpha_i+\\beta_i).\\end{aligned}\\ ] ] the function @xmath112 is the digamma function and , for @xmath113 , @xmath114 , where @xmath115 is the euler - mascheroni constant .",
    "the function @xmath116 is the trigamma function , which is defined as @xmath117 where @xmath118 gives the result .",
    "now , the expected value and the variance of @xmath119 follow immediately .    in the general case , to construct the mml estimator of @xmath91 in phr models",
    ", one needs to replace the second term in ( [ hr - complete ] ) by its expected value for any @xmath120 and @xmath121 .",
    "note that for @xmath100 or @xmath122 the second term in ( [ hr - complete ] ) equals zero .",
    "the rns - based mml estimator of @xmath91 , denoted by @xmath123 , and the corresponding expected value and variance are given in the following theorem .",
    "[ hr - com - mle ] let @xmath64 be iid random variables from ( [ hr - cdf ] ) , and suppose @xmath124 , ",
    ", @xmath125 is the corresponding rns sample of size @xmath16 with at least one @xmath126 .",
    "further , let @xmath127 and @xmath108 , where @xmath96 , @xmath97 and @xmath128^{\\frac{1}{\\gamma({{\\pmb{\\theta}}})}}}{1-[\\bar f_0(y_i)]^{\\frac{1}{\\gamma({{\\pmb{\\theta}}})}}}\\log\\bar f_0(y_i)\\right)=   -k_ib(\\alpha_i , \\beta_i){\\mathrm{e}}(\\log u_i),\\quad i=1 , \\ldots , m.\\ ] ]    a.   the mml estimator of @xmath91 based on rns complete - data of size @xmath16 is given by @xmath129 b.   the expected value and the variance of @xmath130 are respectively @xmath131=\\frac{-\\sum_{i=1}^m(\\alpha_i-1){\\mathrm{e}}[\\log \\bar f_0(y_i)]}{m+\\sum_{i=1}^m\\beta_i\\mathcal{e}_i}\\gamma({\\pmb{\\theta}}),\\end{aligned}\\ ] ] and @xmath132=\\frac{\\sum_{i=1}^m(\\alpha_i-1)^2{\\mathrm{var}}(\\log\\bar f_0(y_i))}{(m+\\sum_{i=1}^m\\beta_i\\mathcal{e}_i)^2}\\gamma^2({\\pmb{\\theta}}),\\end{aligned}\\ ] ] where @xmath133 $ ] and @xmath134 $ ] are obtained from ( [ exp - hpr ] ) and ( [ exp^2-hpr ] ) .",
    "for part ( a ) , the value @xmath135 is derived using the pdf of @xmath21 in ( [ hr - pdf - y ] ) .",
    "substituting @xmath135 for the second term in ( [ hr - complete ] ) results in ( [ hr - mmle ] ) .",
    "parts ( b ) is trivial .    considering ( [ com - exp ] ) , @xmath136\\widehat{\\gamma_m({{\\pmb{\\theta}}})}\\end{aligned}\\ ] ] is an unbiased estimator of @xmath63 .",
    "the mml estimator of @xmath63 based on a mans sample of size @xmath16 from ( @xmath137 ) is given by @xmath138 which is an unbiased estimator of @xmath63 with variance @xmath139=\\frac{\\sum_{i=1}^m\\sum_{j=1}^{k_i}\\frac{1}{j^2}}{(\\sum_{i=1}^m\\sum_{j=1}^{k_i}\\frac1j)^2}\\gamma^2({\\pmb{\\theta}}),\\end{aligned}\\ ] ] which is always smaller than its srs counterpart .",
    "[ exp - exp ] let @xmath140 , be a srs sample of size @xmath16 from an exponential distribution with parameter @xmath141 .",
    "further , let @xmath142 be an rns sample of size @xmath16 obtained from the same exponential distribution .",
    "noting that @xmath143 the srs - based ml estimator of @xmath141 is @xmath144 with the expected value @xmath145=\\lambda$ ] and the variance @xmath146=\\lambda^2/m$ ] . assuming @xmath108 , where @xmath96 and @xmath97 , we have @xmath147=-\\lambda k_ib(\\alpha_i-1 , \\beta_i+1){\\mathrm{e}}(\\log w_i)$ ] and @xmath148 the rns - based complete ml estimator of @xmath141 , denoted by @xmath149 , is obtained from the equation @xmath150 the mins complete - data ml estimator of @xmath141 is given by @xmath151 which is unbiased for @xmath141 with the variance @xmath152=\\lambda^2/m$ ] . also , the mml estimator of @xmath141 based on mans complete - data is @xmath153 noting that @xmath147= \\lambda\\sum_{j=1}^{k_i}\\frac{1}{j } $ ] and @xmath154=\\lambda^2\\sum_{j=1}^{k_i}\\frac{1}{j^2 } , $ ] the mml estimator @xmath155 in ( [ exp - mans ] ) is unbiased with variance @xmath156=\\frac{\\sum_{i=1}^m\\sum_{j=1}^{k_i}\\frac{1}{j^2}}{(\\sum_{i=1}^m\\sum_{j=1}^{k_i}\\frac1j)^2 } \\lambda^2.\\end{aligned}\\ ] ]    it can be shown that the above mentioned results hold with minor modifications for the family of prhr model in ( [ ihr - cdf ] ) . for example , the rns - based complete ml estimator of the parameter @xmath91 , is obtained from @xmath157^{\\frac{1}{\\gamma({{\\pmb{\\theta}}})}}}{1-[f_0(y_i)]^{\\frac{1}{\\gamma({{\\pmb{\\theta}}})}}}(\\log f_0(y_i))\\right)\\right\\}=0.\\end{aligned}\\ ] ] also , the rns - based complete ml estimator of @xmath63 in ( [ ihr - cdf ] ) using mans sample is given by @xmath158 which is an unbiased estimator of @xmath63 with variance equal to its srs - based counterpart , i.e. , @xmath159 .",
    "note that the ml estimator of @xmath63 in prhr models based on srs is given by @xmath160 the mml estimator of @xmath63 in the case where at least one @xmath100 is given by @xmath161 where @xmath162 and @xmath163 .",
    "the expected value and the variance of @xmath123 are , respectively , @xmath164=\\frac{-\\sum_{i=1}^m(\\beta_i+1)k_i\\text{b}(\\beta_i+1 , \\alpha_i-1){\\mathrm{e}}[\\log e_i^*]}{m+\\sum_{i=1}^m(\\alpha_i-2)\\mathcal{e}_i^*}\\gamma({\\pmb{\\theta}})\\end{aligned}\\ ] ] and @xmath165=\\frac{\\sum_{i=1}^m(\\beta_i+1)^2{\\mathrm{var}}[\\log\\bar f_0(y_i)]}{(m+\\sum_{i=1}^m(\\alpha_i-2)\\mathcal{e}_i^*)^2}\\gamma^2({\\pmb{\\theta}}),\\end{aligned}\\ ] ] where , assuming @xmath166 , we have @xmath167&=\\gamma({{\\pmb{\\theta}}})k_ib(\\beta_i+1 , \\alpha_i-1){\\mathrm{e}}(\\log w^*_i ) , \\end{aligned}\\ ] ] and @xmath168&=\\gamma^2({{\\pmb{\\theta}}})k_ib(\\beta_i+1 , \\alpha_i-1){\\mathrm{e}}(\\log^2 w^*_i).\\end{aligned}\\ ] ]    in the mins design , the mml estimator of @xmath91 is given by @xmath169 one can easily show that @xmath170 is an unbiased estimator of @xmath63 with the variance given by ( [ var - max - hr ] ) .",
    "[ beta - beta ] let @xmath64 be an srs sample of size @xmath16 from a beta(@xmath171 ) distribution with the corresponding cdf and pdf as follow @xmath172 further , let @xmath21 , @xmath17 , denote an rns sample from ( [ beta - cdf ] ) .",
    "the rns - based complete mle , @xmath173 , is obtained from the equation @xmath174 for a mans sample , the unbiased rns - based ml estimate for @xmath175 using the complete - data is given by @xmath176 with @xmath177 .",
    "also , based on the mins design , the parameters @xmath178 and @xmath179 are , respectively , 1 and @xmath43 .",
    "it is seen that the unbiased mml estimator of @xmath175 based on rns complete - data is given by @xmath180 with the variance provided in ( [ var - max - hr ] ) .",
    "in rns complete - data the triplet @xmath45 are all assumed to be observed . in practice , for @xmath17 , one of @xmath43 or @xmath44 , or both , may be unknown . in this section",
    ", we investigate the cdf and pdf of the rns random variable @xmath21 , the ml equations and corresponding em algorithms associated with each type of rns incomplete - data .",
    "a.   * type i rns incomplete - data : * here , we assume that @xmath46 , for @xmath17 , are known .",
    "in other words , the @xmath18-th observed unit is from a set of size @xmath43 .",
    "this unit may be the maximum or minimum with probability @xmath19 or @xmath20 , respectively . the cdf @xmath181 and pdf @xmath182 for the observed values @xmath183 given @xmath75 are , respectively , as follow @xmath184 the log likelihood function based on @xmath185 and @xmath186 is given by @xmath187 upon differentiating ( [ t1logl ] ) with respect to @xmath0 and equating the result to zero , we have + @xmath188 + which does not yield explicit solutions for @xmath189 . in order to find the ml estimators , we use the em algorithm . in order to pose this problem in the incomplete - data type i context ,",
    "we introduce the unobservable data @xmath190 , where @xmath126 or @xmath100 according to whether the selected unit in the @xmath18-th set is the maximum or the minimum , respectively .",
    "we find the ml estimator of @xmath0 by adding the unobservable data to the problem via working with the current conditional expectation of the complete - data log likelihood ( [ loglcomplete ] ) given the observed data and proceed as follow .",
    "let @xmath191 be the value specified for @xmath0 in the @xmath192-th iteration .",
    "then on the @xmath193-th iteration , the conditional expectation of @xmath194 given @xmath195 and @xmath196 using @xmath191 , i.e. , @xmath197 $ ] is computed .",
    "this step is called the e - step . as @xmath194 is a linear function of the unobservable data @xmath198 , the e - step",
    "is performed simply by replacing @xmath44 by their current conditional expectations given the observed data @xmath195 and @xmath196 .",
    "therefore , for a known parameter @xmath19 , we have @xmath199= { \\mathbb{p}}_{{{\\pmb{\\theta}}}^{(t)}}(z_i=1 | y_i , k_i)=z_i^{(t)},\\end{aligned}\\ ] ] where @xmath200 the next step on the @xmath193-th iteration , which is called the m - step , requires replacing @xmath44 s by @xmath201 s in ( [ loglcomplete ] ) to obtain @xmath202 by maximizing @xmath203 .",
    "we keep alternating between @xmath201 and @xmath204 until @xmath204 converges to a fixed point .",
    "+ when the parameter @xmath19 is unknown , the procedure may be started with the initial value of @xmath205 ( in phr model ) or @xmath206 ( in prhr model ) , and continued by updating @xmath19 using @xmath207 .",
    "b.   * type ii rns incomplete - data : * here , we consider the case where @xmath47 are known , but the set size @xmath43 is unknown . in other words , we observed the value @xmath42 and we know if the observed unit is the maximum or the minimum unit of the set , but the set size is unknown .",
    "the cdf @xmath208 and pdf @xmath209 for the observed value @xmath183 given @xmath76 are , respectively , as follow @xmath210 and @xmath211 from ( [ t2-cdf ] ) , the log likelihood function for @xmath0 obtained from the observed data @xmath185 and @xmath212 is expressed as @xmath213.\\end{aligned}\\ ] ] upon equating the derivatives of ( [ l2 ] ) with respect to @xmath0 to zero , we have @xmath214 which apparently do not yield explicit solutions for the incomplete - data mle of @xmath0 . since the vector @xmath186 is unobservable , we are unable to estimate @xmath0 by the maximizing ( [ loglcomplete ] ) .",
    "so we again use the em algorithm . in the e - step ,",
    "we substitute the unobservable data in ( [ loglcomplete ] ) by averaging the complete - data log likelihood over its conditional distribution given the observed @xmath215 and @xmath216 . as @xmath194 is a linear function of the unobservable data @xmath196 , the e - step",
    "is performed simply by replacing @xmath43 by their current conditional expectations given the observed data @xmath195 and @xmath198 .",
    "therefore , for a known parameter @xmath217 , @xmath218= \\sum_{k=1}^\\infty k{\\mathbb{p}}_{{{\\pmb{\\theta}}}^{(t)}}(k_i = k | { \\boldsymbol y } , { \\boldsymbol z})= k_i^{(t)},\\end{aligned}\\ ] ] where @xmath219 + the m - step on the @xmath193-th iteration requires replacing @xmath43 s by @xmath220 s in ( [ loglcomplete ] ) to obtain @xmath202 by maximizing @xmath203 .",
    "we keep alternating between @xmath221 and @xmath204 until @xmath204 converges to a fixed point .",
    "+ when the parameter @xmath217 is unknown , the procedure can be started with the initial value @xmath222 , where the length of @xmath223 is a relatively large but arbitrary @xmath224 , and continued by calculating @xmath225 using the frequencies of the @xmath220 over @xmath16 . c.   * type iii rns incomplete - data : * here , we study the case where only @xmath42 is observed and no more information on the set size and the rank of the selected unit is available .",
    "the cdf @xmath226 and pdf @xmath227 for the observed value @xmath183 are given , respectively , by @xmath228 and @xmath229 the log likelihood function for @xmath0 formed on the basis of @xmath230 is given by @xmath231 upon equating the derivatives of ( [ l3 ] ) with respect to @xmath1 to zero , the following results are obtained : @xmath232 similar to type i and type ii incomplete - data , no explicit ml estimator for the parameter @xmath0 can be obtained from ( [ l3der ] ) . in this type of rns incomplete - data , two unobservable data sets in the form of @xmath212 and @xmath186 are introduced . in order to perform the em algorithm assuming @xmath217 and @xmath19",
    "are known , we first calculate @xmath233= \\sum_{k=1}^\\infty k{\\mathbb{p}}_{{\\pmb{\\theta}}^{(t)}}(k_i = k | { \\boldsymbol y})=k_i^{(t)},\\end{aligned}\\ ] ] where @xmath234 then , we obtain @xmath235={\\mathbb{p}}_{{{\\pmb{\\theta}}}^{(t)}}(z_i=1 | { \\boldsymbol y})=z_i^{(t)},\\end{aligned}\\ ] ] where @xmath236 + the m - step on the @xmath193-th iteration requires replacing @xmath44 s by @xmath201 s and @xmath43 s by @xmath220 s in ( [ loglcomplete ] ) to obtain @xmath202 by maximizing @xmath203 .",
    "we keep alternating between @xmath201 , @xmath220 and @xmath204 until @xmath204 converges to a fixed point .",
    "when the parameters @xmath19 and @xmath217 are unknown , similar procedures proposed in type i incomplete - data ( for @xmath19 ) and in type ii incomplete - data ( for @xmath217 ) are used .",
    "finding the ml estimators of @xmath1 for complete - data case requires finding the roots of the nonlinear equations ( [ hr - complete ] ) and ( [ ihr - complete ] ) , which are cumbersome and computationally expensive . when the available data is incomplete ,",
    "the iterative em algorithm for calculating the ml estimator of @xmath1 is not easy - to - use . in this section",
    ", we briefly study the mm estimation based on rns data for @xmath63 in phr and prhr models .",
    "the srs - based mm estimate of @xmath63 , denoted by @xmath237 is equal to the srs - based ml estimate , @xmath238 , obtained from ( [ srs - hr - ml ] ) . in phr ,",
    "considering the random variable @xmath239 , the mm estimator of @xmath63 can be obtained by equating the first moment of the population to the sample moment as follow @xmath240 similarly , in prhr model , the mm estimator of @xmath63 is expressed as @xmath241 now , we present the rns - based complete- and incomplete - data mm estimators of @xmath63 in both phr and prhr models .",
    "[ mmestim ] let @xmath68 be an rns sample of size @xmath16 obtained from a continuous cdf of the family of phr model or prhr model .",
    "further , let @xmath108 and @xmath166 , where @xmath96 and @xmath97 .",
    "then , the unbiased mm estimators of @xmath63 in phr and prhr models are , respectively , obtained as @xmath242 where the value of @xmath243 depends on the rns data type and the underlying model as presented in table [ mm ] .",
    "note that for the rns complete - data , the variance of @xmath170 in both phr and prhr models provided in theorem [ mmestim ] are derived using ( [ exp - hpr ] ) , ( [ exp^2-hpr ] ) , ( [ exp - prh ] ) , and ( [ exp^2-prh ] ) .",
    "in this section , we perform numerical studies to compare the performance of the proposed rns methods with srs in estimating some parameters .",
    "first , we perform some simulations to examine the performance of rns compared with srs under different scenarios of available information about the observations , set sizes , and rank of observations . then , in a case study we evaluate the precision of the rns design over the srs design in a more complicated scenario in both perfect and imperfect settings .",
    "we first discuss the reduction in the mean square error ( mse ) of the ml estimators in the rns complete - data in the phr and prhr models using the relative precision .",
    "the relative precision is defined as the ratio of the rns - based mse over the srs - based mse such that values less than one are desired . for the incomplete - data settings ,",
    "the performance of mle s of the population parameters in two distributions are examined ; the parameter @xmath141 in the exponential distribution introduced in example [ exp - exp ] and the parameter @xmath175 in the beta distribution introduced in example [ beta - beta ] .",
    "note that the expected value and the variance of the rns complete - data in the phr and prhr models presented in ( [ com - exp ] ) , ( [ com - var ] ) , ( [ e - prhr ] ) , and ( [ v - prhr ] ) do not depend on the observed data and are only functions of @xmath244 and @xmath245 .",
    "in addition , we investigate the role of the rns design parameters in improving the performance of the rns - based estimators compared with their srs counterparts .    in figure [ msefixedk - phr ] , we provide the mse of @xmath119 , the estimator of @xmath63 in the rns setting when the data is complete , over the mse of @xmath246 , the estimator of @xmath63 in the srs setting , for the phr ( left panel ) and prhr ( right panel ) models .",
    "the relative precision is calculated for four rns designs with fixed set sizes as @xmath247 and the proportion of maximums varies in @xmath248 . from the results we can compare rns complete - data with different fixed set sizes and proportion of maximums among themselves and with srs in terms of the performance of estimators of @xmath63 . for example , in the left panel of figure [ msefixedk - phr ] , which shows the relative precision for the phr models , it is seen that any rns design , even with @xmath249 and proportion of maximums @xmath250 , outperforms srs .",
    "increasing the set size and the proportion of maximums improves the performance of the rns complete - data .",
    "the best performance pertains to mans with @xmath251 . in the right panel of figure [ msefixedk - phr ] , which shows the relative precision for the prhr models , similar results",
    "are obtained except that the best performance pertains to mins with @xmath251 .",
    "based on the rns complete - data over their srs counterparts in the phr ( left panel ) and prhr ( right panel ) models when @xmath247 and the proportion of maximums is @xmath248 .",
    "values less than one show rns performs better than srs .",
    ", width=576,height=240 ]    in figure [ phr - t123 ] , we provide the relative precision of the ml estimators of @xmath141 as the parameter of the exponential distribution in example [ exp - exp ] in three rns incomplete - data type i , type ii , and type iii .",
    "the relative precision is calculated by the mean square of the rns - based ml estimate of @xmath252 over its srs - based counterpart , so values less than one are desired .",
    "the top left panel shows the relative precision of the rns - based ml estimator of @xmath141 in the incomplete - data type i. the relative precision is calculated for the ml estimators of @xmath253 and @xmath14 $ ] .",
    "it is seen that for the larger values of @xmath141 , the rns incomplete - data type i outperforms srs for any @xmath254 $ ] . as @xmath19 approaches to 1 , regardless of the value of the parameter of interest , the performance of rns incomplete - data type i becomes better than srs .",
    "the top right panel presents the relative precision of the rns incomplete - data type ii for the range of @xmath255 and for four distributions of the set size @xmath244 as follows @xmath256 it is seen that the rns incomplete - data type ii with the assumed @xmath257 , @xmath258 , @xmath259 , and @xmath260 improves the precision of the estimators of @xmath141 especially when the set sizes get larger . as the value of @xmath141 increases , the performance of the rns incomplete - data type ii is improved more and the distributions of @xmath244 perform similarly .",
    "the next four panels in figure [ phr - t123 ] present the relative precision of the rns incomplete - data type iii for @xmath255 and @xmath261 .",
    "the relative precision for small @xmath141 depends on @xmath19 .",
    "the last four panels in figure [ phr - t123 ] show that , for all the considered distributions on @xmath244 , by increasing @xmath19 , rns outperforms srs and the relative precision reaches the lowest value when @xmath28 .    $ ] and @xmath262 , type ii ( top right panel ) for four distributions on @xmath244 and @xmath255 , and type iii ( middle and lower panels ) for @xmath261 and @xmath255 in an exponential distribution with parameter @xmath141 and @xmath263 .",
    "values of the relative precision less than one show rns performs better than srs.,width=576,height=672 ]    in figure [ prhr - t123 ] we present the relative precision of the ml estimators of @xmath175 as the parameter of the beta distribution in the form of example [ beta - beta ] for the rns incomplete - data types i , ii and iii .",
    "the top left panel shows the relative precision of the rns - based ml estimator of @xmath264 and 4 in the incomplete - data type i for @xmath14 $ ] .",
    "it is seen that for the examined values of @xmath175 , @xmath29 improves the rns incomplete - data type i over srs .",
    "the top right panel presents the relative precision of the rns incomplete - data type ii for the range of @xmath265 and for four distributions on @xmath244 , which are shown by @xmath257 , @xmath258 , @xmath259 , and @xmath260 . for the examined @xmath217 s , rns",
    "incomplete - data type ii outperforms srs .",
    "the next four panels in figure [ prhr - t123 ] present the relative precision of the rns incomplete - data type iii for @xmath265 and @xmath261 .",
    "it is seen that for @xmath29 the rns incomplete - data type iii performs better than srs .",
    "the relative precision of the estimators obtained from the rns design with @xmath19 other than zero might works good for some values of @xmath175 , especially when @xmath19 is close to zero .",
    "$ ] and @xmath266 , type ii ( top right panel ) for four distributions on @xmath244 and @xmath265 , and type iii ( middle and lower panels ) for @xmath261 and @xmath265 in an exponential distribution with parameter @xmath175 and @xmath263 .",
    "values of the relative precision less than one shows rns performs better than srs.,width=576,height=672 ]    we also evaluated the performance of the rns - based mm estimators of @xmath141 and @xmath175 .",
    "figure [ mme - c ] shows the precision of @xmath267 , the mm estimators of @xmath63 in the rns setting when the data is complete , relative to their srs counterparts for the phr ( left panel ) and prhr ( right panel ) models .",
    "the relative precision is calculated for four rns designs with fixed set sizes when the sets are of sizes @xmath247 and the proportion of maximums varies in @xmath248 .",
    "the results show that the rns design outperforms srs for all considered distributions of @xmath244 ( @xmath257 , @xmath258 , @xmath259 , and @xmath260 ) and for all proportions of maximums @xmath268 $ ] .",
    "we observe in the left panel that , similar to the ml estimators in the rns - based complete - data , increasing the set size and the proportion of maximums improve the performance of the rns complete - data in the phr model . in the phr model , the best performance",
    "is obtained from the mans design , where all the selected units are maximums , and with the set size @xmath251 . in the prhr model",
    "( right panel ) , the best performance belongs to the mins design , where all the selected units are minimums , with the set size @xmath251 .     based on the rns complete - data over their srs counterparts in the phr ( left panel ) and prhr ( right panel ) models when @xmath269 and 5 , and the proportion of maximums is @xmath270 .",
    "the relative precision less than one shows rns performs better than srs.,width=576,height=240 ]    figure [ mme - t123 ] provides the relative precision of the mm estimators of parameter @xmath141 in the exponential distribution introduced in example [ exp - exp ] based on the rns incomplete - data type i , type ii , and type iii .",
    "the top left panel shows the relative precision of the rns - based mm estimators in the incomplete - data type i. it shows that @xmath28 , regardless of the parameter value @xmath141 , is the optimum value of @xmath19 which improves the rns - based mm estimator in the incomplete - data type i scenario compared with srs .",
    "looking at the top right panel , which presents the performance of the mm estimator of @xmath141 in the rns incomplete - data type ii , shows that rns designs with design parameters @xmath257 , @xmath258 , and @xmath259 do not perform better than srs for all the examined parameters @xmath141 . for @xmath260 , the performance of the rns incomplete - data type ii improves over srs .",
    "the next four panels in the middle and down in figure [ mme - t123 ] present the performance of the mm estimators of @xmath141 in the rns incomplete - data type ii for @xmath261 , where @xmath28 has the best impact on the performance of this type of the rns design especially when the parameter value @xmath141 increases .",
    "for small @xmath141 the proportion of srs , samples from the sets of size @xmath271 , should be small .    $ ] and @xmath262 , type ii ( top right panel ) for four distributions on @xmath244 and @xmath265 , and type iii ( middle and lower panels ) for @xmath261 and @xmath265 in a beta distribution with parameter @xmath272 , the shape parameter @xmath273 , and @xmath263 .",
    "values of the relative precision less than one shows rns performs better than srs.,width=576,height=672 ]    $ ] and @xmath266 , type ii ( top right panel ) for four distributions on @xmath244 and @xmath265 , and type iii ( middle and lower panels ) for @xmath261 and @xmath265 in a beta distribution with parameter @xmath272 , the shape parameter @xmath273 , and @xmath263 .",
    "values of the relative precision less than one shows rns performs better than srs .",
    ", width=576,height=672 ]    in figure [ prhr - mm - t123 ] , the performance of the rns - based mm estimators of @xmath175 in the introduced beta distribution are compared with their corresponding estimators in the srs design . to evaluate the relative precision in the rns incomplete - data type i , the rns - based mm estimators of @xmath274 for",
    "@xmath14 $ ] are examined .",
    "the top left panel shows that , no matter what the value of @xmath175 is , @xmath29 provides the best performance of the rns incomplete - data type i compared with the srs scheme . considering the top right panel , which shows the relative precision of the rns - based mm estimators in the incomplete - data type ii , it is seen that for all assumed distributions on @xmath244 with larger and fixed set sizes , i.e. , @xmath260 and regardless of the parameter value @xmath175 , the rns outperforms srs .",
    "the next panels in figure [ prhr - mm - t123 ] confirm that @xmath29 provides the most efficient rns - based mm estimators of @xmath175 in the incomplete - data type iii .",
    "as @xmath19 increases , for all values of @xmath175 the performance of this type of rns gets worse .",
    "fish products have been shown to contain varying amounts of heavy metals , particularly mercury from water pollution .",
    "mercury is dangerous to both natural ecosystems and humans because it is a metal known to be highly toxic , especially due to its ability to damage the central nervous system .",
    "children , as well as women who are pregnant or planning to become pregnant , are the most vulnerable to mercury s harmful effects .",
    "many studies have been performed to determine the mercury concentration in the fish species and evaluate the performance of the proposed remedial actions ( e.g. @xcite , @xcite and @xcite ) .     based on rns incomplete - data type iii over their srs counterparts in the perfect ranking setting ( left panel ) and imperfect ranking setting ( right panel )",
    "when the parameters @xmath217 and @xmath19 are assumed to be unknown for @xmath275 and @xmath257 , @xmath258 , @xmath259.,width=624,height=240 ]    the mercury grows in concentration within the bodies of fish .",
    "it is well - known that measuring the mercury level in fish needs an expensive and complex laboratory procedure .",
    "however , a small group of fish can be ranked based on their mercury contamination levels before taking the actual measurements on them .",
    "this can be done using either the fish weight or length which have positive correlations with mercury contamination levels in fish . in this section ,",
    "the performance of the rns - based modified maximum likelihood estimator of the distribution parameter is compared with its corresponding srs counterpart .",
    "the dataset contains mercury levels , weights , and lengths of 3033 _ sandra vitreus _ ( walleye ) caught in minnesota .",
    "the original database contains 102,850 fish tissue mercury records compiled by the united states geological survey from various sources .",
    "selected records were those that : had a non - zero mercury concentration ; had a valid species identifier ; had non - zero length and weight measurements ; and had a sample type of  filet skin on \" .    in this study",
    "both perfect and imperfect ranking settings are considered . in the perfect ranking setting the study variable , i.e. the fish mercury level , is used for ranking the sample units in the sets and for the imperfect ranking setting , ranking is performed using the fish weights .",
    "@xmath276 between the mercury level and weight values is about 0.4 , which is relatively small .",
    "figure [ rd - fig1 ] ( left panel ) shows the population shape for the mercury levels of 3033 fish records and it looks the fish mercury level follows an exponential distribution , a member of phr model , with parameter @xmath277 .",
    "the choice of exponential distribution for fish mercury level is justified by the kolmogorov - smirnov test .",
    "figure [ rd - fig1 ] ( right panel ) shows the scatterplot between the mercury level and weight .",
    "the rns complete - data and three rns incomplete - data scenarios were considered in section [ simulation ] . in the examined three types",
    "rns incomplete - data , the parameters @xmath278 and/or @xmath19 are assumed to be known . in this section",
    "we examine the performance of rns incomplete - data type iii in which the parameters @xmath278 and @xmath19 are unknown .",
    "the performance of the design in estimating the population parameter is evaluated using the relative precision , i.e. the mean square of rns - based mle of @xmath141 over the mean square of its srs counterpart .",
    "the relative precision is calculated for the mle s of @xmath141 for @xmath275 and three distributions of the set size @xmath244 as follow @xmath279 figure [ rd - fig2 ] provides the relative precision of the ml estimators of parameter @xmath141 based on the rns incomplete - data type iii .",
    "it shows that @xmath28 , regardless of the parameter value @xmath278 , is the optimum value of @xmath19 which improves the rns - based ml estimator in the incomplete - data type ii scenario compared with their srs counterparts .",
    "the relative precision of rns - based estimators presented in figure [ rd - fig2 ] is obtained in an em algorithm when the initial values of @xmath217 and @xmath19 are @xmath280 and @xmath281 .",
    "considering the sensitivity of the em algorithm to the initial values of the unknown parameters @xmath217 and @xmath19 , we also examined @xmath282 , @xmath283 , @xmath284 and @xmath285 . except @xmath282 and @xmath284 , for",
    "the other examined initial values of @xmath217 and @xmath19 , rns outperforms srs for larger values of true @xmath19 s , i.e. @xmath286 $ ] , and it shows the best performance of rns over srs at @xmath28 .",
    "randomized nomination sampling ( rns ) was introduced by @xcite and it has been shown to perform better than simple random sampling ( srs ) in constructing nonparametric confidence and tolerance intervals .",
    "rns has potentials for a wide range of applications in medical , environmental and ecological studies . in this paper",
    ", we described the rns - based ml and mm estimators of the population parameters when the underlying study variable follows phr or prhr model .",
    "various conditions on the type of information , ranking error settings and the design parameters including distribution of the set size ( @xmath217 ) and probability of taking the maximum observation of the set ( @xmath19 ) have been investigated .",
    "we introduced four types of rns data , corresponding to situations in which all the observations , set sizes and observations ranks in the sets are known , only observations and the set sizes are known , only the observations and their ranks in the sets are known , or finally only the observations are known . considering all the situations , we also provided the pdf and cdf of an rns observation .",
    "we showed that there is always a range of @xmath19 on each rns is superior to srs in terms of the relative precision .",
    "the rns design has this advantage regardless of the ranking setting .",
    "the relative precision of the estimators obtained in the rns design becomes better when more weight is given to the larger set size and @xmath28 ( in phr model ) or @xmath29 ( in prhr model ) .",
    "the authors gratefully acknowledge the partial support of the nserc canada .                                                wells , m.  t. , r.  c. tiwari , et  al .",
    "( 1990 ) . estimating a distribution function based on minima - nomination sampling . in _ topics in statistical dependence _",
    ", pp .   471479 .",
    "institute of mathematical statistics ."
  ],
  "abstract_text": [
    "<S> randomized nomination sampling ( rns ) is a rank - based sampling technique which has been shown to be effective in several nonparametric studies involving environmental and ecological applications . in this paper </S>",
    "<S> , we investigate parametric inference using rns design for estimating the unknown vector of parameters @xmath0 in the proportional hazard rate and proportional reverse hazard rate models . </S>",
    "<S> we examine both maximum likelihood ( ml ) and method of moments ( mm ) methods and investigate the relative precision of our proposed rns - based estimators compared with those based on simple random sampling ( srs ) . </S>",
    "<S> we introduce four types of rns - based data as well as necessary em algorithms for the ml estimation , and evaluate the performance of corresponding estimators in estimating @xmath1 . </S>",
    "<S> we show that there are always values of the design parameters on which rns - based estimators are more efficient than those based on srs . </S>",
    "<S> inference based on imperfect ranking is also explored and it is shown that the improvement holds even when the ranking is imperfect . </S>",
    "<S> theoretical results are augmented with numerical evaluations and a case study .    </S>",
    "<S> mohammad nourmohammadi , mohammad jafari jozanijafari@xmath2jozani@umanitoba.ca . </S>",
    "<S> tel : 1 204 272 1563 . ] , and brad c.  johnson   +    _ university of manitoba , department of statistics , winnipeg , mb , canada , r3 t 2n2 _    * keywords : * randomized nomination sampling ; method of moments ; maximum likelihood ; modified maximum likelihood ; proportional hazard rate ; proportional reverse hazard rate ; em algorithm . </S>"
  ]
}