{
  "article_text": [
    "relationships between information theory and statistical physics have been extensively recognized over the last few decades , and they are drawn from many different aspects .",
    "we mention here only a few of them .",
    "one such aspect is characterized by identifying structures of optimization problems pertaining to certain information ",
    "theoretic settings as being analogous to parallel structures that arise in statistical physics , and then borrowing statistical  mechanical insights , as well as powerful analysis techniques ( like the replica method ) from statistical physics to the dual information ",
    "theoretic setting of interest . a very partial list of works along this line includes @xcite , @xcite , @xcite , @xcite , @xcite , @xcite @xcite , @xcite , @xcite ( and references therein ) , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , and @xcite .",
    "another aspect pertains to the philosophy and the application of the maximum entropy principle , which emerged in statistical mechanics in the nineteenth century and has been advocated during the previous century in a wide variety of more general contexts , by jaynes @xcite,@xcite,@xcite , and by shore and johnson @xcite , as a general guiding principle to problems in information theory ( see , e.g. , ( * ? ? ?",
    "11 ) and references therein ) and other areas , such as signal processing , in particular , speech coding ( see , e.g. , @xcite ) spectrum estimation ( see , e.g. , @xcite ) , and others .",
    "yet another aspect is related to ideas and theories that underly the notion of ` trading ' between information bits and energy , or heat . in particular ,",
    "landauer s erasure principle @xcite is argued to provide a powerful link between information theory and physics and to suggest a physical theory of information ( comprehensive overviews are included in , e.g. , @xcite and @xcite ) . according to landauer s principle",
    ", the erasure of every bit of information increases the thermodynamic entropy of the world by @xmath0 , where @xmath1 is boltzmann s constant , and so , information is actually physical .    finally , to shift gears more to the direction of this paper , we should mention the aspect of the interface between statistical physics and large deviations theory , a line of research advocated most prominently by ellis @xcite,@xcite , and developed also by oono @xcite , mcallester @xcite , and others .",
    "the main theme here evolves around the identification of chernoff bounds and more general large deviations rate functions with free energies ( along with their related partition functions ) , thermodynamical entropies , and the underlying maximum ",
    "entropy / equilibrium principle associated with them . in particular , ellis book @xcite is devoted largely to the application of large deviations theory to the statistical physics pertaining to models of ferromagnetic spin arrays , like ising spin glasses and others , in order to explore phase transitions phenomena of spontaneous magnetization ( see also @xcite ) .",
    "this paper , which is mostly expository in character , lies in the intersection of information theory , large deviations theory , and statistical physics .",
    "in particular , we establish a simple identity between two quantities as they can both be interpreted as the rate function of a certain large deviations event that involves multiple distributions of sets of independent random variables ( as opposed to the usual , single set of i.i.d .",
    "random variables ) .",
    "the analysis of this large deviations event is of a general form that is frequently encountered in numerous applications in information theory ( cf .",
    "section 4 ) .",
    "its informal description is as follows : let @xmath2 be an arbitrary ( deterministic ) sequence whose components take on values in a finite set @xmath3 , and let @xmath4 be a sequence of random variables where each component is generated independently according to a distribution @xmath5 , @xmath6 . for a given function @xmath7 and a constant @xmath8 , we are interested in the large deviations analysis ( chernoff bound ) of the probability of the event @xmath9 assuming that the relative frequencies of the various symbols in @xmath10 stabilize as @xmath11 grows without bound , and assuming that @xmath8 is sufficiently small to make this a rare event for large @xmath11",
    ".    there are ( at least ) two ways to drive a chernoff bound on the probability of this event .",
    "the first is to treat the entire sequence of rv s , @xmath12 as a whole , and the second is to partition it according to the various symbols @xmath13 , i.e. , to consider the separate large deviations events of the partial sums , @xmath14 , @xmath15 , for all possible allocations of the total ` budget ' @xmath16 among the various @xmath17 .",
    "these two approaches lead to two ( seemingly ) different expressions of chernoff bounds , but since they are both exponentially tight , they must agree .",
    "as will be described and discussed in section 2 , the identity between these two chernoff bounds has a natural interpretation in statistical physics : it is viewed as a situation of thermal equilibrium ( maximum entropy ) in a system that consists of several subsystems ( which can be of different kinds ) , each of them with many particles .    as will be shown in section 4 , the above  described problem of large deviations analysis of the event ( [ event ] ) is encountered in many applications in information theory , such as rate  distortion coding , channel capacity , hypothesis testing ( signal detection , in particular ) , and others .",
    "the above mentioned statistical mechanical interpretation then applies to all of them .",
    "accordingly , section 4 is devoted to expository descriptions of each of these applications , along with the underlying physics that is inspired by the proposed thermal equilibrium interpretation .",
    "the reader is assumed to have very elementary background in statistical physics .",
    "the remaining part of this paper is organized as follows . in section 2",
    ", we establish some notation conventions . in section 3 ,",
    "we assert and prove our main result , which is the identity between the above described chernoff bounds .",
    "finally , in section 4 , we explore the application examples .",
    "throughout this paper , scalar random variables ( rv s ) will be denoted by the capital letters , like @xmath18,@xmath19,@xmath20 , and @xmath21 , their sample values will be denoted by the respective lower case letters , and their alphabets will be denoted by the respective calligraphic letters .",
    "a similar convention will apply to random vectors and their sample values , which will be denoted with same symbols superscripted by the dimension .",
    "thus , for example , @xmath22 will denote a random @xmath11-vector @xmath23 , and @xmath24 is a specific vector value in @xmath25 , the @xmath11-th cartesian power of @xmath26 .",
    "the notations @xmath27 and @xmath28 , where @xmath29 and @xmath30 are integers and @xmath31 , will designate segments @xmath32 and @xmath33 , respectively , where for @xmath34 , the subscript will be omitted ( as above ) .",
    "sequences without specifying indices are denoted by @xmath35 .",
    "sources and channels will be denoted generically by the letter @xmath36 or @xmath37 .",
    "specific letter probabilities corresponding to a source @xmath36 will be denoted by the corresponding lower case letter , e.g. , @xmath38 is the probability of a letter @xmath15 .",
    "a similar convention will be applied to a channel @xmath37 and the corresponding transition probabilities , e.g. , @xmath39 , @xmath40 , @xmath15 .",
    "the cardinality of a finite set @xmath41 will be denoted by @xmath42 .",
    "information theoretic quantities like entropies , and mutual informations will be denoted following the usual conventions of the information theory literature .",
    "notation pertaining to statistical physics will also follow , wherever possible , the customary conventions .",
    "i.e. , @xmath1 will denote boltzmann s constant ( @xmath43 joules per kelvin degree ) , @xmath44  absolute temperature ( in kelvin degrees ) , @xmath45  the inverse temperature ( in units of @xmath46 or @xmath47 ) , @xmath8  energy , the letter @xmath48 will be used to denote partition functions , etc .",
    "let @xmath49 and @xmath3 be finite is finite , is made mostly for the sake of convenience and simplicity .",
    "most of our results extend straightforwardly to the case of a continuous alphabet @xmath49 .",
    "the extension to a continuous alphabet @xmath3 is somewhat more subtle , however . ]",
    "sets and let @xmath50 be a given function .",
    "let @xmath51 be a probability mass function on @xmath3 and let @xmath52 be a matrix of conditional probabilities from @xmath3 to @xmath49 .",
    "next , let us define for each @xmath15 , the partition function : @xmath53 and for a given @xmath54 in the range @xmath55 let @xmath56.\\ ] ] further , for a given constant @xmath8 in the range @xmath57 let @xmath58.\\ ] ] let @xmath59 denote the set of all @xmath60dimensional vectors @xmath61 , where each component @xmath54 satisfies ( [ range ] ) , and where @xmath62 .",
    "our main result , in this section , is the following :    @xmath63    the expression on the right  hand side is , of course , more convenient to work with since it involves minimization w.r.t .",
    "one parameter only , as opposed to the left ",
    "hand side , where there is a minimization over @xmath64 for every @xmath65 , as well as a maximization over the @xmath60dimensional vector @xmath66 .",
    "while the proof of theorem 1 below is fairly short , in the appendix ( subsection a.1 ) , we outline an alternative proof which , although somewhat longer , provides some additional insight , we believe .",
    "as described briefly in the introduction , it is based on two different approaches to the analysis of the rate function , @xmath67 , pertaining to the probability of the event : @xmath68 where @xmath69 are rv s taking values in @xmath49 and drawn according to @xmath70 , and @xmath71 is a given deterministic vector whose components are in @xmath3 , with each @xmath15 appearing @xmath72 times ( @xmath73 ) , and the related relative frequency , @xmath74 is exactly @xmath38 .",
    "it should be noted that the proof in the appendix pertains to a slightly different definition of the set @xmath59 , where the individual upper bound to each @xmath54 is enlarged to @xmath75 .",
    "thus , @xmath59 is extended to a larger set , which will be denoted by @xmath76 in the appendix .",
    "but the maximum over @xmath76 is always attained within the original set @xmath59 ( as is actually shown in the proof below ) .    _ proof .",
    "_ here we prove the identity of theorem 1 directly , without using large deviations analysis and chernoff bounds .",
    "we first prove that for every @xmath77 , we have @xmath78 and then , of course , @xmath79 as well .",
    "this follows from the following chain of inequalities : @xmath80\\nonumber\\\\ & = & \\sum_{v\\in{{\\cal v}}}\\min_{\\beta\\ge 0}[\\beta p(v)e_v+p(v)\\ln z_v(\\beta)]\\nonumber\\\\ & \\le&\\min_{\\beta\\ge 0}\\left[\\beta\\sum_{v\\in{{\\cal v}}}p(v)e_v+ \\sum_{v\\in{{\\cal v}}}p(v)\\ln z_v(\\beta)\\right]\\nonumber\\\\ & \\le&\\min_{\\beta\\ge 0}\\left[\\beta e+\\sum_{v\\in{{\\cal v}}}p(v)\\ln z_v(\\beta)\\right]\\nonumber\\\\ & = & \\bar{s}(e),\\end{aligned}\\ ] ] where in the second inequality we used the postulate that @xmath62 .    in the other direction ,",
    "let @xmath81 be the achiever of @xmath82 , i.e. , @xmath81 is the solution to the equation : @xmath83_{\\beta=\\beta^*}.\\ ] ] for each @xmath15 , let @xmath84 $ ] be chosen such that @xmath81 would be the achiever of @xmath85 , i.e. , @xmath86_{\\beta=\\beta^*}$ ] .",
    "obviously , the vector @xmath87 lies in @xmath59 , and @xmath88_{\\beta=\\beta^*}\\nonumber\\\\ & = & -\\left[\\frac{\\partial}{\\partial\\beta}\\sum_vp(v ) \\ln z_v(\\beta)\\right]_{\\beta=\\beta^*}\\nonumber\\\\ & = & e.\\end{aligned}\\ ] ] thus , @xmath89\\nonumber\\\\ & = & \\beta^*\\sum_{v\\in{{\\cal v}}}p(v)e_v^*+\\sum_vp(v)\\ln z_v(\\beta^*)\\nonumber\\\\ & = & \\beta^*e+\\sum_vp(v)\\ln z_v(\\beta^*)\\nonumber\\\\ & = & \\bar{s}(e).\\end{aligned}\\ ] ] this completes the proof of theorem 1 .",
    "@xmath90    the function @xmath91 is similar to the well  known partition function pertaining to the boltzmann distribution w.r.t .  the hamiltonian ( energy function ) @xmath92 , except that each exponential term is weighted by @xmath39 , as opposed to the usual form , which is just @xmath93 . before describing the statistical mechanical interpretation of eq .",
    "( [ identity ] ) , we should note that @xmath91 defined in ( [ zvb ] ) can easily be related to the ordinary partition function , without weighting , as follows : suppose that @xmath94 are rational and hence can be represented as ratios of two positive integers , @xmath95 , where @xmath96 is common to all @xmath40 ( and @xmath15 ) .",
    "now , imagine that every value of @xmath97 actually represents a ` quantization ' of a more refined microstate ( call it a `` nanostate '' ) @xmath98 , @xmath99 , so that @xmath100 , where @xmath101 is a many  to  one function , for which the inverse image of every @xmath97 consists of @xmath102 many values of @xmath103 .",
    "suppose further that the hamiltonian depends on @xmath103 only via @xmath104 , i.e. , @xmath105 .",
    "then , the ( ordinary ) partition function related to @xmath103 is given by @xmath106 thus , the weighted partition function is , within a constant factor @xmath107 , the same as the ordinary partition function of @xmath103 .",
    "this factor cancels out when probabilities are calculated since it appears both in the numerator and the denominator .",
    "moreover , it affects neither the minimizing @xmath64 that achieves @xmath108 or @xmath82 , nor the derivatives of the log  partition function .",
    "we now move on to our interpretation of eq .",
    "( [ identity ] ) from the viewpoint of elementary statistical physics : consider a physical system which consists of @xmath60 subsystems of particles .",
    "the total number of particles in the system is @xmath11 and the total amount of energy is @xmath16 joules . for each @xmath15 ,",
    "the subsystem indexed by @xmath65 ( subsystem @xmath65 , for short ) contains @xmath109 particles , each of which can lie in any microstate within a finite set of microstates @xmath49 ( or an underlying nanostate in a set @xmath110 ) , and it is characterized by an additive hamiltonian @xmath111 .",
    "the total amount of energy possessed by subsystem @xmath65 is given by @xmath112 joules .",
    "as long as the subsystems are in thermal isolation from each other , each one of them may have its own temperature @xmath113 , where @xmath114 is the achiever of the normalized ( per  particle ) entropy associated with an average per  particle energy @xmath54 , i.e. , @xmath56.\\ ] ] the above  mentioned rate function @xmath67 of @xmath115 is then given by the negative maximum total per  particle entropy , @xmath116 , where the maximum is over all energy allocations @xmath117 such that the total energy is conserved , i.e. , @xmath118 .",
    "this maximum is attained by the expression of the r.h.s .",
    "( [ identity ] ) , where there is _ only one _ temperature parameter , and hence it corresponds to _ thermal equilibrium_. in other words , the whole system then lies in the same temperature @xmath119 , where @xmath81 is the minimizer of @xmath82 .",
    "thus , the energy allocation among the various subsystems in equilibrium is such that their temperatures are the same ( cf .",
    "the above proof of theorem 1 ) .",
    "theorem 1 is then interpreted as expressing the second law of thermodynamics .    at this point ,",
    "a few comments are in order :    1 .",
    "it should be pointed out that in the above physical interpretation , we have implicitly assumed that the particles within each subsystem are distinguishable , and so the partition function corresponding to a set of @xmath72 particles is given by the partition function of a single particle raised to the power of @xmath72 , without dividing by @xmath120 .",
    "this differs then from the indistinguishable case only by a constant factor ( as long as @xmath72 is indeed constant ) and hence the difference between the distinguishable and the indistinguishable cases is not essential for the most part of our discussion .",
    "as mentioned in the above paragraph , our conclusion is that @xmath121 . at first glance",
    ", this may seem peculiar as it appears that @xmath67 may be negative .",
    "however , one should keep in mind that @xmath82 is induced by a ( convex ) combination of weighted partition functions , rather than ordinary partition functions , like @xmath122 . referring to eq .",
    "( [ wpf ] ) , the ordinary notion of entropy @xmath123 as the normalized log  number of ( nano)states with normalized energy @xmath8 , is given by @xmath124\\nonumber\\\\ & = & \\min_{\\beta\\ge 0}\\left[\\beta e+\\sum_vp(v)\\ln z_v(\\beta)\\right]+\\ln m\\nonumber\\\\ & = & \\bar{s}(e)+\\ln m.\\end{aligned}\\ ] ] thus , @xmath125 which is always non  negative .",
    "3 .   the identity ( [ identity ] )",
    "can be thought of as a generalized concavity property of the entropy : had all the entropy functions @xmath126 been the same , this would have been the usual concavity property .",
    "what makes this equality less trivial and more interesting is that it continues to hold even when @xmath126 , for the various @xmath15 , are different from each other .",
    "4 .   on the more technical level , since this paper draws analogies with physics , we should say a few words about physical units .",
    "the products @xmath127 , @xmath128 , @xmath129 , etc .",
    ", should all be pure numbers , of course . since @xmath45 , where @xmath1 is boltzmann s constant and @xmath44 is absolute temperature , and since @xmath130 has units of energy ( joules or ergs , etc . )",
    ", it is understood that @xmath8 , @xmath54 , @xmath131 and the like , should all have units of energy as well . in the applications described below ,",
    "whenever this is not the case , i.e. , the latter quantities are pure numbers rather than physical energies , we will sometimes reparametrize @xmath64 by @xmath132 , where @xmath133 is an arbitrary constant possessing units of energy ( e.g. , @xmath134 joule or @xmath134 erg ) , and we absorb @xmath133 in the hamiltonian , i.e. , redefine @xmath135 . thus , in this case , @xmath136 , where @xmath8 is the now the energy in units of @xmath133 , is redefined as @xmath137.\\ ] ] this kind of modification is not essential , but it may help to avoid confusion about units when the picture is viewed from the aspects of physics .",
    "equipped with the main result of the previous section and its statistical mechanical interpretation , we next introduce a few applications that fall within the framework considered . in all these applications , there is an underlying large deviations event of the type of eq .",
    "( [ ld ] ) , whose rate function is of interest .",
    "the above described viewpoint of statistical physics is then relevant in all these applications .",
    "let @xmath138 designate the vector of letter probabilities associated with a given discrete memoryless source ( dms ) , and for a given reproduction alphabet @xmath139 , let @xmath140 denote a single  letter distortion measure .",
    "let @xmath141 denote the rate ",
    "distortion function of the dms @xmath36 .",
    "one useful way to think of the rate ",
    "distortion function is inspired by the classical random coding argument : let @xmath142 be drawn i.i.d .  from the optimum random coding distribution @xmath143 and consider the event @xmath144 , where @xmath145 is a given source vector , typical to @xmath36 , i.e. , the composition of @xmath145 consists of @xmath146 occurrences of each @xmath147 .",
    "this is exactly an event of the type ( [ ld ] ) with @xmath148 , @xmath149 , @xmath6 , @xmath150 independently of @xmath151 , @xmath152 , and @xmath153 . i.e. , the hamiltonian @xmath154 is given by @xmath155 and the total energy is @xmath156 in units of @xmath133 .",
    "suppose that this probability is of the exponential order of @xmath157 .",
    "then , it takes about @xmath158}$ ] ( @xmath159 , however small ) independent trials to ` succeed ' at least once ( with high probability ) in having some realization of @xmath160 within distance @xmath156 from @xmath145 .",
    "this is the well  known the classical random coding achievability argument that leads to @xmath161 .",
    "thus , the large  deviations rate function of interest agrees exactly with the rate  distortion function ( cf .",
    "3.4 ) ) , which is : @xmath162.\\ ] ] interestingly , in ( * ? ? ?",
    "* , corollary 4.2.3 ) ) , the rate  distortion function is shown , using completely different considerations , to have a parametric representation which can be written exactly in this form .",
    "the fact that the rate  distortion function has an interpretation of an isothermal equilibrium situation in statistical thermodynamics is not quite new ( cf .",
    "e.g.  ( * ? ? ?",
    "6.4 ) , @xcite ) . here , however , we obtain it in a more explicit manner and as a special case of a more general principle .",
    "a simple example is that of the binary symmetric source with the hamming distortion measure .",
    "it is easy to see that , in this example , the relationship between distortion and temperature is : @xmath163}~~\\mbox{or , equivalently,}~~d=\\frac{1}{1+e^{\\epsilon_0/(kt)}}\\ ] ] and , of course , @xmath164 , where @xmath165 is the binary entropy function .",
    "a slightly more involved example pertains to the regime of high resolution ( small distortion ) and it turns out to be related to ( a generalized version of ) the law of equipartition of energy in statistical physics : consider the @xmath166 distortion measure , @xmath167 ( most commonly encountered are the cases @xmath168 and @xmath169 ) .",
    "let us assume that @xmath170 is very small and consider the ( continuous ) uniform random coding distribution @xmath171 in the interval @xmath172 $ ] and zero elsewhere .",
    "this random coding distribution is suboptimal , but it corresponds , and hence is well motivated , by many results in high  resolution quantization using uniform quantizers ( see , e.g. , @xcite and references therein ) . for every @xmath147 , the partition function is given by @xmath173 when @xmath174 is very small , @xmath64 is very large , and then the finite  interval integral pertaining to @xmath175 can be approximated by an infinite one , provided that the support of @xmath176 is included is negligibly small . ] in the interval @xmath172 $ ] : @xmath177 which then becomes independent of @xmath151 .",
    "the average distortion ( internal energy ) associated with this partition function can be evaluated using the same technique as the one that leads to the law of equipartition in statistical physics : @xmath178\\nonumber\\\\ & = & -\\frac{\\partial}{\\partial \\beta}\\ln\\left [ \\beta^{-1/\\theta}\\cdot\\int_{-\\infty}^\\infty \\exp\\{-\\epsilon_0|\\beta^{1/\\theta}({\\hat{x}}-x)|^\\theta\\ } \\mbox{d}(\\beta^{1/\\theta}({\\hat{x}}-x))\\right]\\nonumber\\\\ & = & -\\frac{\\partial}{\\partial \\beta}\\ln\\left [ \\beta^{-1/\\theta}\\cdot\\int_{-\\infty}^\\infty \\exp\\{-\\epsilon_0|z|^\\theta\\ } \\mbox{d}z\\right]\\nonumber\\\\ & = & -\\frac{\\mbox{d}}{\\mbox{d}\\beta}\\ln \\left(\\beta^{-1/\\theta}\\right)- \\frac{\\partial}{\\partial \\beta}\\ln\\left [ \\int_{-\\infty}^\\infty \\exp\\{-\\epsilon_0|z|^\\theta\\ } \\mbox{d}z\\right]\\nonumber\\\\ & = & \\frac{1}{\\beta \\theta}-0 = \\frac{kt}{\\theta}\\end{aligned}\\ ] ] [ note that for @xmath169 , where the hamiltonian is quadratic in the integration variable @xmath179 , this is exactly the law of equipartition .",
    "] thus , for low temperatures , the distortion is given by @xmath180 , i.e. , distortion is linear in temperature in that regime , and the constant of proportionality is related to the heat capacity , @xmath181 .",
    "since the temperature is proportional to the negative local slope of the distortion  rate function ( as the reciprocal , @xmath64 , is proportional to the negative local slope of the rate  distortion function ) , this means that the distortion is proportional to its derivative w.r.t .",
    "@xmath182 , which means an exponential relationship of the form @xmath183 ( @xmath184  constant ) . for @xmath169 ( mean square error )",
    ", this is recognized as the well  known characterization of distortion as function of rate in the high resolution regime .",
    "specifically , in this case , the factor of @xmath185 at the denominator of @xmath186 , the universal expression of the internal energy per degree of freedom according to the equipartition theorem , has the same origin as the factor of @xmath185 that appears in the exponent of @xmath187 ( decay of 6db per bit ) .",
    "thus the law of equipartition in statistical physics is related to the behavior of rate distortion codes in the high resolution regime .    to compute the rate associated with this temperature more explicitly ,",
    "note that the minimizing @xmath81 is given by @xmath188 , and so @xmath189\\nonumber\\\\ & = & -\\frac{1}{\\theta}-\\ln\\left[\\frac{1}{2a } \\cdot\\frac{2\\gamma(1/\\theta)}{\\theta(1/\\theta d)^{1/\\theta}}\\right]\\nonumber\\\\ & = & \\ln\\left[\\frac{a\\theta}{\\gamma(1/\\theta ) ( \\theta ed)^{1/\\theta}}\\right]\\nonumber\\\\ & = & \\ln\\left[\\frac{a\\theta}{\\gamma(1/\\theta)}\\right]- \\frac{1}{\\theta}\\ln(\\theta ed).\\end{aligned}\\ ] ]      in complete duality to the random coding argument that puts the rate  distortion function in the framework discussed in section 3 , a parallel argument can be made with regard to channel capacity .    given a discrete memoryless channel ( dmc ) with a finite input alphabet @xmath26 , and a finite output alphabet @xmath190",
    ", we can obtain capacity using the following argument .",
    "let @xmath191 be the optimum random coding distribution according to which , each codeword @xmath22 is drawn independently .",
    "let @xmath192 be a given channel output sequence which is typical to the output distribution @xmath193 , where @xmath194 are the channel transition probabilities .",
    "that is , each symbol @xmath195 appears @xmath196 times in @xmath192 .",
    "consider now the large deviations event @xmath197 where @xmath198 . by the union bound , as long as the number of randomly chosen codewords is exponentially less than @xmath199 , where @xmath200 is the rate function of the large ",
    "deviations event ( [ td ] ) , then the average error probability still vanishes as @xmath201 . ) , which although suboptimum , is still good enough to achieve capacity . ] since this is the exactly the achievability argument of the channel coding theorem , then @xmath202 , where @xmath203 the channel capacity .",
    "again , this complies with our model setting with the assignments , @xmath204 , @xmath205 , @xmath6 , @xmath206 independently of @xmath195 , @xmath207 and @xmath208 units of @xmath133 . in other words , channel capacity can be represented as @xmath209}\\right)\\right].\\ ] ] it is easy to see that , in this case , the equilibrium temperature always corresponds to @xmath210 , namely , @xmath211 .    by the same token , one can derive an expression of the random coding capacity pertaining to mismatched decoding , where the decoder uses an additive metric @xmath212 other than the optimum metric , @xmath213 ( see , e.g. , @xcite , @xcite , @xcite , @xcite , @xcite , and references therein ) . the only modifications to the above expression",
    "would be to replace the hamiltonian by @xmath214 and to replace @xmath215 by the expectation of @xmath216 w.r.t .",
    "the new optimum random coding distribution might change as well . here , it is no longer necessarily true that the equilibrium temperature is @xmath211 .",
    "consider the following binary hypothesis testing problem : given a deterministic signal , which is repreresented by a sequence @xmath218 with elements taking on values in a ( finite ) set @xmath26 and relative frequencies @xmath219 , and given an observation sequence @xmath220 , we are required to decide between two hypotheses :    * the observation vector @xmath221 is `` pure noise , '' distributed according to some product measure @xmath222 , i.e. , @xmath223 , which is unrelated to @xmath145 .",
    "* the observation vector @xmath221 is a `` noisy version '' of @xmath145 , distributed according to @xmath224 .    the optimum detector ( under both the bayesian and the neyman ",
    "pearson criterion ) compares the likelihood ratio @xmath225 $ ] to a threshold @xmath226 , and decides in favor of @xmath227 if this threshold is exceeded , otherwise , it decides in favor of @xmath228 .",
    "the false  alarm probability then is the probability of the event @xmath229\\le ne_0\\ ] ] under @xmath37 .",
    "this , again , fits our scenario with the substitutions @xmath230 , @xmath149 , @xmath6 , @xmath231 , independently of @xmath232 , @xmath233 $ ] , and @xmath234 .",
    "similarly , the analysis of the missed  detection probability corresponds to the assignments : @xmath230 , and @xmath149 , @xmath6 , as before , but now @xmath235 , @xmath236 $ ] and @xmath237 . note that when @xmath238 is the uniform distribution over @xmath190 , the missed - detection event can also be interpreted as the probability of excess code  length of an arithmetic lossless source code w.r.t .",
    "@xmath239 .",
    "another situation of hypothesis testing that is related to our study in a similar manner is one where the signal @xmath145 is always underlying the observations , but the decision to be made is associated with two hypotheses regarding the noise level , or the temperature . in this case , there is a certain hamiltonian @xmath240 for each @xmath147 , and we assume a boltzmann  gibbs distribution parametrized by the temperature @xmath241 where @xmath242 note that here @xmath243 is an ordinary partition function , without weighting ( cf .",
    "( [ wpf ] ) ) .",
    "we shall also denote @xmath244.\\ ] ] as @xmath245 is induced by a convex combination of non - weighted partition functions , it has the significance of the normalized logarithm of the number of microstates with energy about @xmath16 .",
    "thus , @xmath246 , where @xmath1 is boltzmann s constant , is the thermodynamic entropy .    given two values @xmath247 and @xmath248 ( say , @xmath249 ) , the hypotheses now are the following :    * @xmath221 is distributed according to @xmath250 . * @xmath221 is distributed according to @xmath251 .",
    "it then follows that the error exponent @xmath266 under @xmath253 is given by @xmath267\\nonumber\\\\ & = & \\frac{1}{k}\\int_{e_0}^{e_2}\\left[\\frac{1}{t(e)}-\\frac{1}{t_2}\\right]\\mbox{d}e \\nonumber\\\\ & = & \\frac{1}{k}\\int_{t_0}^{t_2}\\left(\\frac{1}{t}-\\frac{1}{t_2}\\right ) \\bar{c}(t)\\mbox{d}t,\\end{aligned}\\ ] ] where @xmath268 is the temperature corresponding to energy @xmath8 , @xmath269 , @xmath270 , and @xmath271 is the average heat capacity per particle of the system , which is the weighted average of heat capacities of all subsystems , i.e. , @xmath272 where @xmath273_{\\beta= 1/(kt)}.\\ ] ] thus , @xmath274 which is interpreted as the weighted average of the relative contributions of all subsystems , which all lie in the same temperature @xmath275 .    in a similar manner ,",
    "the rate function @xmath276 of the probability of error under @xmath228 is given by : @xmath277\\nonumber\\\\ & = & \\frac{1}{k}\\int_{e_1}^{e_0}\\left[\\frac{1}{t_1}- \\frac{1}{t(e)}\\right]\\mbox{d}e\\nonumber\\\\ & = & \\frac{1}{k}\\int_{t_1}^{t_0}\\left(\\frac{1}{t_1}- \\frac{1}{t}\\right)\\bar{c}(t)\\mbox{d}t.\\end{aligned}\\ ] ]    the expression in the square brackets of the second line pertaining to @xmath266 has a simple graphical interpretation ( see fig .",
    "1 ) : it is the vertical distance ( corresponding to the vertical line @xmath234 ) between the curve @xmath245 and the line tangent to that curve at @xmath278 ( whose slope is @xmath279 ) .",
    "the two other expressions of @xmath266 , in the last chain of equalities , describe the error exponent @xmath266 in terms of slow heating from temperature @xmath275 to temperature @xmath280 .",
    "similar comments apply to @xmath276 ( cf .",
    "fig .  1 ) .",
    "thus , the error exponents are linear functionals of the average heat capacity , @xmath281 , in the range of temperatures @xmath282 $ ] .",
    "the higher is the heat capacity , the better is the discrimination between the hypotheses .",
    "this is related to the fact that fisher information of the parameter @xmath64 is given by @xmath283 namely , again , a linear function of @xmath281 .",
    "however , while the fisher information depends only on one local value of @xmath281 ( as it measures the sensitivity of the likelihood function to the parameter in a local manner ) , the error exponents depend on @xmath284 in a cumulative manner , via the above integrals . the tradeoff between @xmath276 and @xmath266 is also obvious : by enlarging the threshold @xmath254 , or , correspondingly , @xmath275 , the range of integration pertaining to @xmath276 increases at the expense of the one of @xmath266 and vice versa . in the extreme case ,",
    "where @xmath285 , we get @xmath286      in this application example , we are back to the problem area of lossy data compression , but this time , it is about scalar ( symbol  by  symbol ) compression .",
    "this setup is motivated by earlier results about the optimality of time  shared scalar quantizers within the class of causal source codes for memoryless sources , both under the average rate / distortion criteria @xcite and large  deviations performance criteria @xcite . in particular , it was shown that under both criteria , optimum time ",
    "sharing between at most two ( entropy coded ) scalar quantizers is as good as any causal source code for memoryless sources . here",
    ", we will focus on the large deviations performance criteria , namely , source coding exponents .",
    "consider a time  varying scalar quantizer @xmath287 , acting on a dms @xmath288 , @xmath289 , drawn from @xmath290 , where @xmath291 is an arbitrary ( deterministic ) sequence of quantizers from a given finite set @xmath292 , where @xmath293 , @xmath294 being the reproduction alphabet corresponding to @xmath295 , @xmath296 .",
    "in other words , for every @xmath297 , @xmath298 , for a certain arbitrary sequence of ` states ' , @xmath299 ( known to the decoder ) with components in @xmath300 .",
    "the distortion incurred by such a time  varying scalar quantizer , over @xmath11 units of time , is @xmath301 .",
    "the total code length is @xmath302 , where the per  symbol length functions @xmath303 may correspond to either fixed  rate coding , where @xmath304 for all @xmath179 , or any other length function satisfying the kraft inequality , @xmath305 . for the sake of simplicity of the exposition , let us assume fixed ",
    "rate coding .",
    "we will denote by @xmath306 , @xmath307 , the number of times that @xmath308 occurs in @xmath309 , and @xmath310 is the corresponding relative frequency .    in @xcite , among other results , the rate function of the excess distortion event @xmath311 was optimized across the class of all time  varying scalar quantizers ( each one corresponding to a different sequence @xmath312 ) subject to a code ",
    "length constraint @xmath313 , or equivalently , @xmath314 , for a given pair @xmath315 .    in the notation of our generic model , here we have @xmath204 , @xmath316 , @xmath6 , @xmath317 independently of @xmath318 , @xmath319 , and @xmath320 . and @xmath321 , where @xmath322 , in order to work with non  negative quantities . ] and the excess distortion exponent is of the same form as before ( see also @xcite ) . here",
    ", however , unlike the previous application examples , we have a degree of freedom to select the relative frequency of usage , @xmath323 , of each member of @xmath324 , i.e. , the time  sharing protocol , but we also have the constraint @xmath325 .    from the statistical physics point of view , these additional ingredients mean that we have a freedom to select the number of particles in each subsystem ( though the total number , @xmath11 , is still fixed ) , and the additional constraint , @xmath325 , which is actually equivalent to the equality constraint @xmath326 ( in the interesting region of @xmath327 pairs ) can be viewed as an additional conservation law with respect to some other constant of motion , in addition to the energy ( e.g. , the momentum ) , where in subsystem @xmath318 , the ( average ) value of the corresponding physical quantity per particle is @xmath328 .",
    "while in @xcite , we have considered the problem of maximizing the rate function ( the source coding exponent ) of the excess distortion event @xmath329 , a related objective ( although somewhat less well motivated , but still interesting ) is to minimize the rate function ( or maximize the probability ) of the small distortion event @xmath330 in this case , the optimum performance is given by @xmath331,\\ ] ] where @xmath332 is the class of all probability distributions @xmath333 with @xmath325 . from the viewpoint of statistical physics , this corresponds to a situation where the various subsystems are allowed to interact , not only thermally , but also chemically , i.e. , an exchange of particles is enabled in addition to the exchange of energy , and the maximization over @xmath332 ( maximum entropy ) is achieved when the chemical potentials of the various subsystems reach a balance . as the maximization over @xmath334 subject to the constraint @xmath325 , for a given @xmath64 , is a linear programming problem with one constraint ( in addition to @xmath335 ) , then as was shown in @xcite , for each distortion level ( or energy ) @xmath174 , the optimum @xmath334 may be non ",
    "zero for at most two members of @xmath336 only , which means that at most two subsystems are populated by particles in thermal and chemical equilibrium under the two conservation laws ( of @xmath174 and of @xmath182 ) . however , the choice of these two members of @xmath336 depends , in general , on @xmath174 , which in turn depends on the temperature .",
    "thus , when the system is heated gradually , certain _ phase transitions _ may occur , whenever there is a change in the choice of the two populated subsystems .    finally , referring to comment no .  1 of section 3",
    ", we should point out that here , in contrast to our discussion thus far , the difference between the ensemble of distinguishable particles and indistinguishable particles becomes critical since the factors @xmath337 are no longer constant .",
    "had we assumed indistinguishability , the normalized log ",
    "partition function would no longer be affine in @xmath36 , thus the maximization over @xmath36 would no longer be a linear programming problem , and the conclusion might have been different . in the source coding problem , the indistinguishable case corresponds to a situation where the sequence of states @xmath309 is chosen uniformly at random ( with the decoder being informed of the result of the random selection , of course ) . in this case , the chernoff bound corresponding to each composition @xmath338 of @xmath309 should be weighed by the probability of this composition , which is @xmath339 .",
    "now , each factor of @xmath340 can be absorbed in the corresponding partition function @xmath341 of subsystem @xmath318 , with the interpretation that in each subsystem the particles are now indistinguishable .",
    "the maximum over @xmath36 would now correspond to the dominant contribution in this weighted average of chernoff bounds .",
    "one can , of course , extend the discussion to any i.i.d .",
    "distribution on @xmath309 , thus introducing additional bias and preferring some compositions over others .",
    "in this subsection , we outline another proof of theorem 1 using a large deviations analysis approach .",
    "in particular , consider the large deviations event @xmath342 , as described in section 2 .",
    "assuming that the relative frequencies @xmath343 all stabilize as @xmath201 , let us compute the rate function @xmath67 of the probability of this event in two different methods , where one would yield the left",
    " hand side of ( [ identity ] ) and the other would give the right  hand side of ( [ identity ] ) .",
    "in the first method , we partition the sequence @xmath344 according to its different letters . specifically , let @xmath345 where @xmath72 is the number of occurrences of the symbol @xmath15 along @xmath344 .",
    "let @xmath346 denote the set of all possible vector values that can be taken on by the vector @xmath61 .",
    "now , obviously , @xmath347 if and only if there exists a vector @xmath348 such that @xmath349 for all @xmath15 and @xmath350 .",
    "the `` if '' part follows from @xmath351 the `` only if '' part follows by setting @xmath352 for all @xmath15 .",
    "therefore , denoting @xmath353 ( where @xmath76 is defined as in section 2 ) , we have : @xmath354 and on the other hand , @xmath355 at this point , the only gap between the upper bound ( [ ub1 ] ) and the lower bound ( [ lb1 ] ) is the factor @xmath356 . the number of different values that @xmath357 can take does not exceed the number of different type classes of sequences of length @xmath72 over the alphabet @xmath49 , which is upper bounded by @xmath358 .",
    "thus , @xmath359\\right)\\right\\}\\nonumber\\\\ & = & \\exp\\left\\{|{{\\cal v}}|\\cdot(|{{\\cal u}}|-1)\\log\\left ( \\frac{n}{|{{\\cal v}}|}+1\\right)\\right\\}\\nonumber\\\\ & = & \\left(\\frac{n}{|{{\\cal v}}|}+1\\right)^{|{{\\cal v}}|\\cdot(|{{\\cal u}}|-1)},\\end{aligned}\\ ] ] and therefore @xmath356 is only polynomial in @xmath11 , and hence does not affect the exponential behavior .",
    "now , each one of the terms @xmath360 is bounded exponentially tightly by an individual chernoff bound , @xmath361\\right\\},\\ ] ] and so , the dominant term of their product is of the exponential order of @xmath362=\\max_{\\tilde{e}\\in{{\\cal h}}_g(e)}\\sum_vp(v)s_v(e_v).\\ ] ] finally , as @xmath363 , the set @xmath364 becomes dense in the continuous set @xmath76 , and by simple continuity arguments , the maximum over @xmath364 tends to the maximum over @xmath76 .",
    "the other method to evaluate the rate function @xmath67 is as follows .",
    "let @xmath365 be a fixed positive integer that divides @xmath11 , and denote @xmath366 , @xmath15 ( assume that @xmath365 is chosen large enough that @xmath367 is well approximated by the closest integer with a very small relative error ) .",
    "now , re  order the pairs @xmath368 ( periodically ) , according to the following rule : assuming , without loss of generality , that @xmath369 , the first @xmath370 symbol pairs of each @xmath365block of @xmath371 are such that @xmath372 , the next @xmath373 symbol pairs of each @xmath365block are such that @xmath374 , and so on . in other words , each @xmath365block , @xmath375 , @xmath376 , consists of the same relative frequencies @xmath343 as the entire sequence , @xmath344 .",
    "now , for the re  ordered sequence of pairs , let us define @xmath377 , @xmath376 .",
    "obviously , @xmath378 are i.i.d .  and therefore the probability of the large deviations event @xmath379 can be assessed exponentially tightly by the chernoff bound as follows : @xmath380",
    "\\right\\}\\nonumber\\\\ & = & \\exp\\left\\{\\frac{n}{\\ell}\\cdot\\min_{\\beta\\ge 0 } \\left[\\beta\\cdot\\ell e+\\ln\\left(\\prod_{v\\in{{\\cal v } } } \\sum_{u^{\\ell_v}}q(u^{\\ell_v}|v^{\\ell_v } ) \\exp\\left\\{-\\beta\\sum_{i=1}^{\\ell_v}f(u_i , v)\\right\\}\\right)\\right ] \\right\\}\\nonumber\\\\ & = & \\exp\\left\\{\\frac{n}{\\ell}\\cdot\\min_{\\beta\\ge 0 } \\left[\\beta\\cdot\\ell e+\\ln\\left(\\prod_{v\\in{{\\cal v } } } \\left[\\sum_{u\\in{{\\cal u}}}q(u|v)e^{-\\beta f(u , v)}\\right]^{\\ell_v}\\right)\\right ] \\right\\}\\nonumber\\\\ & = & \\exp\\left\\{\\frac{n}{\\ell}\\cdot\\min_{\\beta\\ge 0 } \\left[\\beta\\cdot\\ell e+\\ell\\cdot\\sum_{v\\in{{\\cal v } } } p(v)\\ln\\left(\\sum_{u\\in{{\\cal u}}}q(u|v)e^{-\\beta f(u , v)}\\right)\\right ] \\right\\}\\nonumber\\\\ & = & \\exp\\left\\{n\\cdot\\min_{\\beta\\ge 0}\\left[\\beta e+\\sum_{v\\in{{\\cal v } } } p(v)\\ln\\left(\\sum_{u\\in{{\\cal u}}}q(u|v)e^{-\\beta f(u , v)}\\right)\\right ] \\right\\}\\nonumber\\\\ & = & e^{n\\bar{s}(e)}.\\end{aligned}\\ ] ] since both approaches yield exponentially tight evaluations of @xmath67 , they must be equal .",
    "the exact derivation of eq .",
    "( [ equipartition ] ) for the finite interval integration , is as follows : @xmath381\\nonumber\\\\ & = & -\\frac{\\partial}{\\partial \\beta}\\ln\\left [ \\beta^{-1/\\theta}\\cdot\\int_{-\\beta^{1/\\theta}(a+x)}^{\\beta^{1/\\theta}(a - x ) } \\exp\\{-\\epsilon_0|\\beta^{1/\\theta}({\\hat{x}}-x)|^\\theta\\ } \\mbox{d}(\\beta^{1/\\theta}({\\hat{x}}-x))\\right]\\nonumber\\\\ & = & -\\frac{\\partial}{\\partial \\beta}\\ln\\left [ \\beta^{-1/\\theta}\\cdot\\int_{-\\beta^{1/\\theta}(a+x)}^{\\beta^{1/\\theta}(a - x ) } \\exp\\{-\\epsilon_0|z|^\\theta\\ } \\mbox{d}z\\right]\\nonumber\\\\ & = & -\\frac{\\partial}{\\partial \\beta}\\ln \\left(\\beta^{-1/\\theta}\\right)-\\frac{\\partial}{\\partial \\beta}\\ln \\left[\\int_{-\\beta^{1/\\theta}(a+x)}^{\\beta^{1/\\theta}(a - x ) } \\exp\\{-\\epsilon_0|z|^\\theta\\}\\mbox{d}z\\right]\\nonumber\\\\ & = & \\frac{1}{\\beta\\theta}\\left\\{1-\\frac{\\beta^{1/\\theta } [ ( a - x)\\exp\\{-\\beta\\epsilon_0|a - x|^\\theta\\ } + ( a+x)\\exp\\{-\\beta\\epsilon_0|a+x|^\\theta\\ } ] } { \\int_{-\\beta^{1/\\theta}(a+x)}^{\\beta^{1/\\theta}(a - x ) } \\exp\\{-\\epsilon_0|z|^\\theta\\}\\mbox{d}z}\\right\\}.\\end{aligned}\\ ] ] when @xmath64 is very large , the denominator of the second term of the expression in the curly brackets of the right  most side , goes to @xmath382 , which is a constant . now if , in addition , @xmath383 , then the numerator tends to zero as @xmath64 grows without bound .",
    "thus , the dominant term , for low temperatures , is @xmath384 .",
    "an exact closed ",
    "form expression , for every finite @xmath64 , can be derived for the case @xmath168 , since in this case , the integral at the denominator has a simple expression .",
    "for example , setting @xmath168 , and @xmath385 in the above expression , yields : @xmath386 note that this expression is valid only in the range where it is monotonically increasing in @xmath44 .",
    "( beyond this point , the minimizing @xmath64 is no longer the point of zero derivative ) .",
    "r.  s.  ellis , `` the theory of large deviations and applications to statistical mechanics , '' lectures for international seminar on extreme events in complex dynamics , october 2006 .",
    "available on  line at : [ http://www.math.umass.edu/@xmath387rsellis/pdf-files/dresden-lectures.pdf ] .",
    "r. m. gray , a. h. gray , g. rebolledo , and j. e. shore , `` rate distortionspeech coding with a minimum discrimination information distortion measure '' , _ ieee trans .  inform .",
    "theory _ , vol .",
    "it27 , no .  6 , pp .  708721 ,",
    "november 1981 .",
    "d.  guo and s.  verd , `` multiuser detection and statistical physics , '' in _ communications , information and network security _ , v.  bhargava , h.  v.  poor , v.  tarokh , and s.  yoon , eds . , chap .  13 , pp .",
    "229 - 277 , kluwer academic publishers , norwell , mass , usa , 2002 .",
    "a. lapidoth and s. shamai ( shitz ) , `` a lower bound on the bit - error rate resulting from mismatched viterbi decoding , '' technical report , cc pub no .  163 , department of electrical engineering , technion ",
    "i.i.t . , august 1996 .",
    "j.  e.  shore and r.  w.  johnson , `` axiomatic derivation of the principle of maximum entropy and the principle of minimum cross - entropy , '' _ ieee trans .",
    "theory _ , vol .",
    "it26 , no .  1 , pp .  2637 , january 1980 ."
  ],
  "abstract_text": [
    "<S> an identity between two versions of the chernoff bound on the probability a certain large deviations event , is established . </S>",
    "<S> this identity has an interpretation in statistical physics , namely , an isothermal equilibrium of a composite system that consists of multiple subsystems of particles . several information  </S>",
    "<S> theoretic application examples , where the analysis of this large deviations probability naturally arises , are then described from the viewpoint of this statistical mechanical interpretation . </S>",
    "<S> this results in several relationships between information theory and statistical physics , which we hope , the reader will find insightful .    </S>",
    "<S> * index terms : * large deviations theory , chernoff bound , statistical physics , thermal equilibrium , equipartition , thermodynamics , phase transitions .    </S>",
    "<S> department of electrical engineering + technion - israel institute of technology + haifa 32000 , israel + </S>"
  ]
}