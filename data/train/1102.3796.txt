{
  "article_text": [
    "the array processor experiment ( ape ) is a custom design for hpc targeting the field of lattice qcd , started by the istituto nazionale di fisica nucleare and partnered by a number of physics institutions all over the world , that since its start in 1984 has developed four generations of custom machines  @xcite .",
    "leveraging on the acquired know - how in networking and re - employing the gained insights , a spin - off project called apenet  @xcite developed an interconnect board that allows assembling a pc cluster  la apewith off - the - shelf components .",
    "following further developments funded by eu projects ( fp 6 shapes  @xcite and fp 7 euretile ) , the apenetproject evolved into apenet+  @xcite ; its achievement is the design of the apelink+host adapter , which integrates both a network interface and a switching component , bringing in state - of - the - art wire speeds for the links and a pcie x8 gen2 host interface . with this latest push to higher bandwidth , low power and low cost of the data transmission system",
    ", we are encompassing not only a broader range of intensive numerical algorithms ( lattice qcd is our primary but not exclusive concern ) , but also the field of acquisition systems for modern particle and astroparticle experiments ( slhc , ilc , clic , na62  ) .",
    "the outlook of this article is as follows : the first section explains the global network architecture ; the second one gives the details of the host board ; the third one outlines the software stack provided by the programming environment ; the fourth one sketches the current deployment of apenet+hardware in the framework of our quonghpc initiative ; the fifth and final one gives conclusions and outlines to future work .",
    "the apenet+interconnect is our low latency , high bandwidth packet - based direct network , supporting state - of - the - art link wire speeds and a pcie x8 gen2 host connection . on this network , the computing host  _",
    "e.g._a multi - core cpu optionally paired with gpu  is equipped with one apelink+board and made into a node of the cluster .",
    "the nodes are connected by point - to - point links to form a 3d torus in a cubic mesh ; each node communicates with each of its 6 neighbours along the @xmath0 , @xmath1 , @xmath2 , @xmath3 , @xmath4 and @xmath5 directions by bi - directional full - duplex communication channels .",
    "size envelope ( header+footer ) of packets is hard - coded and fixed , while payload size is variable ; packets are auto - routed to their final destinations according to wormhole dimension - ordered static routing , with the system taking care of dead - lock avoidance .",
    "the hardware block structure , depicted in figure  [ fig : internals ] , is split into a so called _ network interface _  the packet injection and processing logic comprising pcie , tx / rx logic , _",
    "router _ component and multiple _",
    "torus links_.    internal fpga block architecture . ]",
    "the apelink+__network interface _ _ has basically two main tasks :    * on the transmit data path , it gathers data coming in from the pciexpress port , fragmenting the data stream into packets which are forwarded to the relevant destination ports , depending on the requested operation .",
    "* on the receive side , it provides hardware support for the rdma programming model , implementing the basic rdma capabilities ( put and get semantics ) at the firmware level .    within this block ,",
    "the addition of a nios ii 32 bit embedded micro - controller  a standard altera^^ intellectual property  simplifies some tasks along the path of the received packets .",
    "the _ routing block _ takes care of examining the packet header and resolving the destination address into a proper path across the switch according to the chosen routing algorithm .    the _ torus link block _ manages the data flow by encapsulating the apenet+packets into a light , low - level , _ word - stuffing _ protocol able to detect transmission errors via crc .",
    "it implements two virtual channels  @xcite and proper flow - control logic on each rx link block to guarantee deadlock - free operations .",
    "for the design of the building brick of the apenet+infrastructure we leveraged on the most recent advances in host interface technology , physical link speed and connector mechanics ; the result is the latest generation of our hardware , the apelink+card  see table  [ tab : card_features ] .",
    "    the apelink+card is a single fpga - based pci express board ; the employed fpga device is the ep4sgx290 , which is part of the altera^^ 40 nm stratix iv device family and comes equipped with 36 full - duplex cdr - based transceivers , supporting data rates up to 8.5 gbps each .",
    "it also provides a pcie x8 gen2 interface , which is complemented by a commercial pcie core to allow communication between the host processor and the network .",
    "moreover , an ethernet port is foreseen in order to build an additional , secondary network with an offload engine for collective communication tasks .      in the global network structure",
    ", each card stands as a vertex of a 3d torus mesh network with 6 independent point - to - point multiple links channel ( _ i.e._the links between mesh sites ) .",
    "each link is made up of 4 bi - directional lanes bonded together ; the automatic alignment logic is our original addition .",
    "four links out of six are hosted on the main board ; two more , say @xmath4 and @xmath5 , are located in a detachable , small daughter - card on the upper level . in this way",
    ", the complete card takes on two pci standard slots in a pc chassis , mantaining the chance , if four links are enough , to use it in a single slot wide configuration .",
    "the torus links are 6 independent blocks with 2 virtual channel receive buffers each , added to manage deadlock prevention .",
    "proper flow control is maintained via credits handshake between a local rx block and the remote tx block ; this handshake is embedded in the link protocol data layer .",
    "the torus link is able to autonomously re - transmit the header and the footer in case of transmission errors .",
    "therefore , the protocol assures the delivery of the packet , avoiding nonrecoverable situations where badly corrupted packets ( with errors in the header or footer ) pose a threat to the global routing .",
    "packets with payload errors ( signaled by the footer ) must be instead handled at the software level .",
    "the chosen crc polynomial generator is the industry - standard , well - known crc-32 .",
    "the router comprises a fully connected , 7-ports - in/7-ports - out switch , plus routing and arbitration blocks .",
    "the routing block examines the header of each packet and translates its destination address to a proper path across the switch ; the routing is dimension - ordered , with a measured latency of 60 ns .",
    ".[tab : card_features]evolution of the apelinkcards .",
    "[ cols= \" < , < , < , < \" , ]      a schematic view of the complete apenet+board is visible in figure  [ fig : board ] .",
    "the prototypes will be available at february 2011 .",
    "assembled apelink+test system . ]",
    "assembled apelink+test system . ]",
    "a test system has been built in order to develop the fpga firmware , the pcie interface and the physical layer interconnection technology  @xcite .",
    "we used a commercial altera^^ development kit ( equipped with a smaller altera^^ stratix iv gx 230 ) and a custom - designed daughter - card ( an hsmc mezzanine designed at labe in infn - roma ) hosting 3 qsfp+ connectors and some sma test points .",
    "this setup allows us to test the complete communication chain up to a bitrate of 24 gbps for each link .",
    "signal integrity was checked connecting to dedicated sma test points , straight at the output of the fpga transceivers ( see figure [ fig:5gbps_diretto ] ) and on the mezzanine card ( see figure [ fig2b ] ) after one samtec^^ connector , two qsfp+ connectors and 1 m of copper qsfp+ cable .",
    "the link was successfully tested up to 3 gbps data rate ( compared to 8.5 gbps achievable with the stratix iv embedded transceivers ) . above this limit",
    "we found some signal degradation probably caused by the tower connector between the altera development kit and the test mezzanine .",
    "investigation is in progress ; a likely culprit is the reduced bandwidth ( below 5 ghz ) of the 19 mm qth samtec^^ connector , which would be substituted anyway by higher bandwidth connectors in the production release of the communication card  @xcite .",
    "characterization of signal integrity ( and maximum achievable bandwidth ) versus serial transceivers pre - emphasis and equalization is still in progress .",
    "eye diagram at 3  gbps on the mezzanine card . ]",
    "eye diagram at 3  gbps on the mezzanine card . ]    recovered clock stability was checked transmitting a pseudorandom data stream organized in 128 bit wide words over 1 m copper qsfp+ cable and checking the relative phase between the input and the output clocks ( see figure [ fig3a ] ) .",
    "recovered clock was found stable and in phase with the input clock up to 400 mhz .",
    "latency was checked transmitting a pseudorandom sequence over 1 m qsfp+ copper cable and rising a flag every time a fixed test word is transmitted and received by the serializer and the deserializer respectively ( see figure [ fig3b ] ) .",
    "transmission system latency was found stable up to 160 mhz transmitting clock .",
    "latency measurement at 40 mhz . ]    latency measurement at 40 mhz . ]",
    "all apenet+software is available under the gnu gpl licence and spans across four major topics : the firmware software running on the fpga embedded processor , the linux kernel driver , the application level rdma library and a mpi implementation , these latter three developed and tested under redhat enterprise linux 5 .",
    "communication primitives ( ` rmda_put ( ) ` , ` rdma_get ( ) ` , ` rdma_send ( ) ` ) , buffer registration primitives ( ` register_buffer ( ) ` , ` unregister_buffer ( ) ` ) and synchronization primitives ( ` wait_event ( ) ` ) covering a custom subset of the low - level rdma apis are made available to the application programmer as a highly optimized c language library .",
    "on top of these , we built a native apenet+btl module for openmpi 1.x .",
    "work is underway  @xcite on the hardware and software features needed for gpu - initiated communications , _ e.g._providing , using so called pcie peer - to - peer transactions , a cuda - enabled  @xcite version of the ` rdma_put ( ) ` primitive , in order to avoid intermediate copies onto cpu memory buffers . to further reduce overhead , another development oversees the delivery of rdma events by the apelink+hardware in cpu memory in a way that is accessible from within cuda kernels .",
    "another research topic is exposing gpu memory areas as rdma buffers , in such a way they can be target of rdma put and get operations , even more reducing the latency of network operations . to this end , discussions are ongoing with some gpu vendors .",
    "the firmware software running on the fpga embedded processor is currently in charge of managing the rdma virtual - to - physical address translation table , but we are exploring new ways to exploit it for higher - level tasks .",
    "we are currently exploring interconnection of gpu - equipped systems by means of apenet+(quongproject ) to reach the petaflops range in aggregated computing power and working on some gpu - related driver optimizations .",
    "for the 2011 , our road - map foresees the integration of a `` quongrack '' , a mesh of computing nodes which are rack - mounted 1u systems  based on a commodity intel cpu xeon 5650  accelerated via high - end gpus ( nvidia tesla c1060/m2050 ) interconnected with the apenet+hardware .",
    "this system , housed in a single rack of 42u , will show a peak performance exceeding 60 teraflops and a power consumption of less than 26  kw .",
    "leveraging on apenet+network , multiple quongracks can be assembled to push up the complete system to petaflops scale .",
    "a first mini - cluster is being assembled together with gpus and the apelink+version with 3 links , for final validation of the firmware , the interconnection and the complete software stack on a small size network ( 2 - 8 nodes ) .",
    "synthetic tests , as well as real life simulations , will be performed , so to be ready with the 6-links prototype release and eventually a bigger cluster deployment .",
    "the presence on the apenet+ card of a programmable component of considerable power will allow us to explore reconfigurable computing , _ e.g._accelerating some tasks directly in hardware .",
    "the needs of a large scale deployment make it necessary for apenet+to employ fault - tolerance features ; we will be adding support for links self - diagnosis and the capability of routing around faulty nodes  @xcite .",
    "the authors would like to thank the electronics laboratory at infn sezione di roma @xcite for technical support with the design , production and assembly of the test board used in this work .",
    "this work was partially supported by the eu framework programme 7 project euretile under grant number 247846 .",
    "99 ` http://apegate.roma1.infn.it/ape `          ammendola r , guagnelli m , mazza g , palombi f , petronzio r , rossetti d , rossetti a , salamon a , vicini p , apenet : lqcd clusters a la ape ( 2005 ) , _ proceedings of lattice2004 , nucl.phys.b - proc.suppl._ * 140 * 826 - 828[arxiv : hep - lat/0409071v1 ] .",
    "paolucci p s , jerraya a a , leupers r , thiele l and vicini p , shapes : a tiled scalable software hardware architecture platform for embedded systems ( 2006 ) , _ proceedings of the 4th international conference on hardware / software codesign and system synthesis codes+isss 06 ( seoul , korea ) , acm press _ , 167 - 72 .",
    "ammendola r et al .",
    ", apenet+ : a 3d toroidal network enabling petaflops scale lattice qcd simulations on commodity clusters , to appear in proceedings of _ xxviiith international symposium on lattice field theory , 14 - 19 june 2010 villasimius , sardinia , italy _"
  ],
  "abstract_text": [
    "<S> we describe herein the apelink+board , a pcie interconnect adapter featuring the latest advances in wire speed and interface technology plus hardware support for a rdma programming model and experimental acceleration of gpu networking ; this design allows us to build a low latency , high bandwidth pc cluster , the apenet+network , the new generation of our cost - effective , tens - of - thousands - scalable cluster network architecture .    </S>",
    "<S> some test results and characterization of data transmission of a complete testbench , based on a commercial development card mounting an altera^^ fpga , are provided . </S>"
  ]
}