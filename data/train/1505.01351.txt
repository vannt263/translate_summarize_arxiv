{
  "article_text": [
    "the gompertz ( g ) distribution , generalizing exponential ( e ) distribution , is a popular distribution that has been commonly used in many applied problems for modeling data in biology @xcite , gerontology @xcite , engineering and marketing studies @xcite .",
    "a significant progress has been made towards the generalization and construction flexible distributions to facilitate better modeling of well - known lifetime data .",
    "the book by @xcite provides some applications of the g distribution .    in recent years , many authors have proposed distributions which can arise as special submodels within the mcdonald generated or generalized beta generated ( gbg ) class of distributions . @xcite introduce a class of generalized beta - generated distributions that have three shape parameters in the generator .",
    "they considered eleven different parents : normal , log - normal , skewed student-__t__ , laplace , exponential , weibull , gumbel , birnbaum - saunders , gamma , pareto and logistic distributions .",
    "other generalizations are mcdonald gamma distribution by @xcite , mcdonald inverted beta distribution by @xcite , mcdonald normal distribution by @xcite , mcdonald extended exponential distribution by @xcite , mcdonald half - logistic distribution by @xcite , mcdonald dagum by @xcite , mcdonald generalized beta - binomial distribution by @xcite , mcdonald log - logistic distribution by @xcite , mcdonald arcsine distribution by @xcite , and mcdonald weibull distribution by @xcite .",
    "one of the advantages of the mcdonald generated distribution lies in its ability of fitting skewed data such as other well - known distributions in the literature @xcite . in this paper",
    ", we introduce a new five - parameter model called the __mcdonald gompertz__ ( mcg ) distribution that includes as special sub - models some recent distributions in the literature .",
    "this distribution offers a more flexible distribution for modeling lifetime data in terms of its hazard rate shapes that are decreasing , increasing , upside - down bathtub and bathtub shaped .",
    "several mathematical properties of this new model in order to attract wider applications in reliability , engineering and in other areas of research are provided .",
    "this paper is organized as follows . in section [ sec.model ]",
    ", we introduce the mcg distribution , density and hazard functions .",
    "some special models of the new distribution are described in this section . in section [ sec.pro ] ,",
    "we present useful expansions and properties of the cumulative distribution function ( cdf ) , probability density function ( pdf ) , __k__th moment and moment generating function of the mcg distribution .",
    "moreover , order statistics and their moments , entropy and quantile measures are provided in this section .",
    "estimation of the mcg parameters by maximum likelihood ( ml ) method is described in section [ sec.est ] .",
    "finally , application of the mcg model using two real data sets are considered in section [ sec.exa ] .",
    "the generalized beta distribution of the first kind ( or beta type i ) or mcdonald distribution was introduced by @xcite .",
    "the cdf of the mcdonald distribution is given by @xmath0 where @xmath1 is the incomplete beta function ratio of type i and @xmath2 is the beta function .",
    "the cdf of mcg model can be defined by @xmath3^c ; a / c , b ) ,   \\ \\ y>0,\\ ] ] where @xmath4 .",
    "the pdf corresponding to is given by @xmath5^{a-1 } \\nonumber\\\\ & & \\times\\{1- [ 1-\\exp ( -\\frac{\\theta}{\\gamma}(e^{\\gamma y}-1))]^c \\}^{b-1}.\\end{aligned}\\ ] ] here after , we denote a random variable @xmath6 with pdf in by @xmath7 .",
    "indeed , the mcg distribution belongs to mcdonald - generalized class of distributions with cdf and pdf as @xmath8 and @xmath9 respectively .",
    "the cdf in can be expressed in terms of the hypergeometric function as @xmath10 where @xmath11 is ascending factorial , and @xmath12 is the cdf of g distribution .",
    "let @xmath13 be the pdf of mcg distribution given by .",
    "the limiting behavior of @xmath13 for different values of its parameters is given below :    @xmath14 . if @xmath15 , then @xmath16    @xmath17 . if @xmath18 , then @xmath19    @xmath20 . if @xmath21 , then @xmath22    @xmath23 . @xmath24    the parts ( i)-(iii ) are obviously proved .",
    "for part ( iv ) , we have @xmath25^{c}]^{b-1}<1   \\longrightarrow \\\\ & & 0<f(y ; a , b , c , \\theta , \\gamma)<{c \\theta e^{\\gamma y } \\exp(-\\frac{\\theta}{\\gamma}(e^{\\gamma y}-1))[1-\\exp(-\\frac{\\theta}{\\gamma}(e^{\\gamma y}-1))]^{a-1}}/{b(a / c , b)}.\\end{aligned}\\ ] ] it can be easily shown that @xmath26^{a-1}}=0,\\ ] ] and the proof is completed .            from equations and , it is easy to verify that the hazard rate function ( hrf ) of the mcg distribution is given by @xmath27^c}(a / c , b ) } \\\\ & &\\times [ 1-\\exp(-\\frac{\\theta}{\\gamma}(e^{\\gamma y}-1))]^{a-1 } [ 1-(1-\\exp(-\\frac{\\theta}{\\gamma}(e^{\\gamma y}-1)))^{c}]^{b-1 } , \\",
    "\\ y>0,\\end{aligned}\\ ] ] and the corresponding reversed hazard rate function reduces to @xmath28^c } ( a / c , b ) } [ 1-\\exp(-\\frac{\\theta}{\\gamma}(e^{\\gamma y}-1))]^{a-1}\\\\ & & \\times  [ 1-(1-\\exp(-\\frac{\\theta}{\\gamma}(e^{\\gamma y}-1)))^{c}]^{b-1 } ,   \\ \\ \\   \\ y>0.\\end{aligned}\\ ] ]    figure [ plot.denh ] illustrates some of the possible shapes of density and hazard functions for selected values of parameters . for instance , these plots show the hazard function of the new model is much more flexible than the beta gompertz ( bg ) and g distributions . the hazard rate function can be bathtub shaped , monotonically increasing or decreasing and upside - down bathtub shaped depending on the parameter values .",
    "the mcg distribution contains as sub - models the kumaraswamy gompertz ( kumg ) , the bg @xcite , and the beta generalized exponential ( bge ) or mce @xcite distributions for @xmath29 , @xmath30 and @xmath31 , respectively .",
    "it also contains the beta exponential ( be ) @xcite , the generalized gompertz ( gg ) @xcite , and the kumaraswamy exponential ( kume ) @xcite distributions .",
    "the ge @xcite , the g and e distributions are also sub - models .",
    "the classes of distributions that are included as special sub - models of the mcg distribution are displayed in figure [ plot.relation ] .        if the random variable @xmath6 has the mcg distribution , then it has the following properties : + 1 .",
    "the random variable @xmath32^{c}$ ] satisfies the beta distribution with parameters @xmath33 and @xmath34 .",
    "therefore , the random variable @xmath35 has the bge ( or mcg ) distribution @xcite .",
    "furthermore , the random variable @xmath36 $ ] follows mcg distribution .",
    "this result helps us in simulating data from mcg distribution .",
    "the plots comparing the exact mcg density function and the histogram from a simulated data set with size 100 for some parameter values are given in figure [ plot.sim ] ( left ) . also , the plots of empirical distribution function and exact distribution function are given in figure [ plot.sim ] ( right ) .",
    "these plots indicate that the simulated values are consistent with the mcg distribution .",
    "if @xmath37 and @xmath38 , where @xmath14 and @xmath39 are positive integer values , then the @xmath40 is the cdf of the @xmath14th order statistic of gg distribution .",
    "in this section , some properties of mcg distribution are considered .",
    "we derive some expansions for the cdf , @xmath41th moment and moment generating function of the mcg distribution .",
    "the binomial series expansion is defined by @xmath42 where @xmath43 and @xmath44 is a positive real non - integer .",
    "the following proposition reveals that the mcg distribution can be expressed as a mixture of distribution function of gg distribution , whereas proposition [ prop.mixgg ] provides a useful expansion for the pdf in .",
    "the cdf in is a mixture of distribution function of gg distribution on the form @xmath45^{a+jc}=\\sum_{j=0}^{\\infty } p_{j } g_{j}(y),\\ ] ] where @xmath46 and @xmath47 is the distribution function of a random variable which has a gg distribution with parameters @xmath48 , @xmath49 and @xmath50 .",
    "using binomial expansion to the term @xmath51^{a+jc}$ ] in , we have @xmath52^{a + jc } & = & \\sum_{k=0}^{\\infty } ( -1)^k \\binom { a+jc } { k } ( 1-g(y))^k \\\\ & =  & \\sum_{r=0}^{\\infty } \\sum_{k = r}^{\\infty } ( -1)^k \\binom { a+jc } { k } \\binom { k } { r } [ g(y)]^r.\\end{aligned}\\ ] ] now , becomes @xmath53^r = \\sum_{r=0}^{\\infty } b_r [ g(y)]^r,\\end{aligned}\\ ] ] where @xmath54 .",
    "[ prop.mixgg ] the pdf of mcg can be expressed as an infinite mixture of gg densities with parameters @xmath48 , @xmath49 and @xmath55 given by @xmath56^{a+jc-1 } =  \\sum_{r=0}^{\\infty } p_j g_j ( y),\\end{aligned}\\ ] ] where @xmath57^{a+jc-1}$ ] .",
    "we can write the pdf of mcg as @xmath58^r,\\ ] ] where @xmath59 .",
    "in this section , we deal with the basic statistical properties of mcg distribution such as the @xmath41-th moment and generating function in the following propositions .",
    "the @xmath41-th moment of mcg distribution can be expressed as a infinite mixture of the @xmath41-th moment of gg distributions as follows : @xmath60^{a+jc-1 } = \\sum_{j=0}^{\\infty } p_{j } e(y_{j}^{k}),\\end{aligned}\\ ] ] where @xmath61^r [ \\frac{-1}{\\gamma ( r+1)}]^{k+1},\\ ] ] and @xmath62 .",
    "an explicit expression for the moment generating function of mcg distribution follows from proposition [ prop.mixgg ] , @xmath63^{a+jc-1 } =  \\sum_{j=0}^{\\infty } p_j   m_{y_{j } } ( t),\\end{aligned}\\ ] ] where @xmath64^{k+1}} .\\ ] ]      order statistics make their appearance in many areas of statistical theory and practice .",
    "let the random variable @xmath65 be the @xmath14th order statistic ( @xmath66 ) in a sample of size @xmath39 from the mcg distribution .",
    "the pdf and cdf of @xmath65 for @xmath67 are given by @xmath68^{i-1 } [ 1-f(y)]^{n - i } \\nonumber\\\\ & = & \\frac{1}{b(i , n - i+1)}\\sum_{k=0}^{n - i } \\dbinom{n - i}{k } ( -1)^{k } f(y ) [ f(y)]^{k+i-1},\\end{aligned}\\ ] ] and @xmath69^{k+i},\\end{aligned}\\ ] ] respectively , where @xmath70 .",
    "we use throughout an equation by ( @xcite , page 17 ) for a power series raised to a positive integer @xmath44 given by @xmath71 where the coefficients @xmath72 ( for @xmath73 ) are easily determined from the recurrence equation @xmath74\\,b_k\\,c_{m , r - k},\\ ] ] where @xmath75 .",
    "hence , the coefficients @xmath72 can be calculated from @xmath76 and therefore , from the quantities @xmath77 .",
    "using , the equations and can be written as @xmath78^{r-1},\\\\ & & f_{i : n}(y)=\\frac{1}{b(i , n - i+1 ) } \\sum_{k=0}^{n - i } \\sum_{r=0}^{\\infty } \\frac{1}{k+i } ( -1)^{k } \\dbinom{n - i}{k } c_{i+k , r } [ g(y)]^{r}.\\end{aligned}\\ ] ] an explicit expression for the @xmath79th moments of @xmath65 can be obtained as @xmath80 & = & \\frac{1}{b(i , n - i+1 ) } \\sum_{k=0}^{n - i } \\sum_{r=1}^{\\infty } \\frac{r}{k+i } ( -1)^{k } \\dbinom{n - i}{k } c_{i+k , r } \\int_{0}^{+\\infty } t^{s } g(t ) [ g(t)]^{r-1 } dt \\nonumber\\\\ & = & \\frac{\\theta \\gamma(s+1)}{b(i , n - i+1 ) } \\sum_{k=0}^{n - i } \\sum_{r=1}^{\\infty } \\frac{r}{k+i } ( -1)^{k } \\dbinom{n - i}{k } c_{i+k , r}\\nonumber\\\\ & & \\times \\sum_{i_{1}=0}^{\\infty } \\sum_{i_{2}=0}^{\\infty } \\dbinom { r\\lambda-1 } { i_{1 } } \\frac{(-1)^{i_{1}+i_{2}}}{\\gamma(i_{2}+1 ) } e^{\\frac{\\theta } { \\gamma}(i_{1}+1 ) } [ \\frac{\\theta(i_{1}+1 ) } { \\gamma}]^{i_{2}}[\\frac{-1}{\\gamma(i_{2}+1)}]^{s+1}.\\end{aligned}\\ ] ]      in this section , we consider the effect of each shape parameters @xmath81 , @xmath34 and @xmath82 on the skewness and kurtosis of the mcg distribution .",
    "to illustrate this effect , we use measures based on quantiles .",
    "the quantile function of the @xmath83 distribution say @xmath84 can be obtained as @xmath85 where @xmath86 denotes the @xmath87th quantile of beta distribution with parameters @xmath33 and @xmath34 .",
    "the bowley skewness ( see @xcite ) based on quantiles can be calculated by @xmath88 and the moors kurtosis ( see @xcite ) is defined as @xmath89 where @xmath90 denotes the quantile function .",
    "these measures are less sensitive to outliers and they exist even for distributions without moments . for the standard normal and the classical standard @xmath87 distributions with 10 degrees of freedom ,",
    "the bowley measure is 0 .",
    "the moors measure for these distributions is 1.2331 and 1.27705 , respectively .    in figure",
    "[ plot.skur1 ] , we plot bowley measure ( holding @xmath91 , @xmath92 and @xmath93 fixed ) as a function of @xmath82 for fixed values of @xmath81 ( left ) and bowley measure ( holding @xmath94 , @xmath92 and @xmath93 fixed ) as a function of @xmath82 for some values of @xmath34 ( right ) . in figure",
    "[ plot.skur2 ] , we plot moors measure ( for @xmath91 , @xmath92 and @xmath93 ) as a function of @xmath82 for selected values of @xmath81 ( left ) and moors measure ( for @xmath94 , @xmath92 and @xmath93 ) as a function of @xmath82 for some values of @xmath34 ( right ) .",
    "these plots indicate that these measures can be sensitive to the three shape parameters @xmath81 , @xmath34 and @xmath82 .",
    ".,title=\"fig : \" ] .,title=\"fig : \" ]    .,title=\"fig : \" ] .,title=\"fig : \" ]      the entropy of random variable is defined in terms of its probability distribution and can be shown to be a good measure of randomness or uncertainty .",
    "the shannon s entropy of a continuous random variable @xmath6 with pdf @xmath95 is defined by @xcite as @xmath96= - \\int_{0}^{\\infty } f(y ) \\log f(y ) dy.\\ ] ] hence , the shannon entropy for mcg distribution can be expressed in the form @xmath97 where @xmath98 and @xmath99 represents the digamma function .",
    "the last two terms in follows immediately from the first two conditions in lemma 1 of @xcite .",
    "the rnyi entropy is defined by @xmath100^{\\rho } dy ) , \\ ] ] where @xmath101 and @xmath102 .",
    "the shannon entropy is derived from @xmath103 .",
    "an explicit expression of rnyi entropy for mcg distribution is obtained as @xmath104 where @xmath105 has a beta distribution with parameters @xmath106 and @xmath107 .",
    "let @xmath108 be a random sample of size @xmath39 from the @xmath109 distribution and @xmath110 be the unknown parameter vector .",
    "the log - likelihood function is given by @xmath111 where @xmath112 .",
    "the maximum likelihood estimation ( mle ) of @xmath113 is obtained by solving the nonlinear equations , @xmath114 , where @xmath115 + \\sum_{i=1}^{n } \\log ( 1-t_{i}),\\\\ u_{b}({\\boldsymbol\\theta})&=&\\frac{\\partial l(\\boldsymbol\\theta)}{\\partial b}= n [ \\psi ( a / c+b ) -\\psi(b ) ] + \\sum_{i=1}^{n } \\log(1-(1-t_{i}^{c})),\\\\ u_{c}({\\boldsymbol\\theta})&= & \\frac{\\partial l(\\boldsymbol\\theta)}{\\partial c}= n / c - na/{c^2 } [ \\psi ( a / c+b ) -\\psi(a / c ) ] \\\\ & & - ( b-1 ) \\sum_{i=1}^{n } \\frac{(1-t_{i}^{c } ) \\log(1-t_{i})}{1- ( 1-t_{i})^{c}} , \\\\ u_{{\\theta}}(\\boldsymbol\\theta)&= & \\frac{\\partial l(\\boldsymbol\\theta)}{\\partial \\theta}= { n}/{\\theta } - 1/{\\gamma } \\sum_{i=1}^{n } ( e^{\\gamma y_i}-1 ) + ( a-1)/{\\gamma } \\sum_{i=1}^{n}\\frac{t_i ( e^{\\gamma y_i}-1)}{1-t_i}\\\\ & & - c(b-1)/{\\gamma } \\sum_{i=1}^{n } \\frac{t_i ( 1-t_i)^{c-1 } ( e^{\\gamma y_i}-1)}{1-(1-t_i)^c},\\\\ u_{\\gamma}({\\boldsymbol\\theta } ) &",
    "= & \\frac{\\partial l(\\boldsymbol\\theta)}{\\partial \\gamma}= \\sum_{i=1}^{n } y_i + \\theta / { \\gamma ^2 } \\sum_{i=1}^{n } ( e^{\\gamma y_i } - \\gamma y_i e^{\\gamma y_i } -1 ) \\\\ & & + \\theta ( a-1 ) / { \\gamma ^2 } \\sum_{i=1}^{n}\\frac{t_i ( \\gamma y_i e^{\\gamma y_i}- e^{\\gamma y_i}+1 ) } { 1-t_i}\\\\ & & + \\theta ( b-1 ) c/ { \\gamma ^2 } \\sum_{i=1}^{n}\\frac{t_i ( 1-t_i)^{c-1 } ( e^{\\gamma y_i } - \\gamma y_i e^{\\gamma y_i } -1 ) } { 1- ( 1-t_i)^c}.\\end{aligned}\\ ] ] we need the observed information matrix for interval estimation and hypotheses tests on the model parameters .",
    "the @xmath116 fisher information matrix , @xmath117 , is given by @xmath118,\\ ] ] where the expressions for the elements of @xmath119 are @xmath120,\\\\   j_{ab}&=&\\frac{\\partial^{2 } l(\\boldsymbol\\theta)}{\\partial a \\partial b } = \\frac{n}{c } \\psi'(a / c+b),\\\\   j_{ac}&=&\\frac{\\partial^{2 } l(\\boldsymbol\\theta)}{\\partial a \\partial c } = -\\frac{na}{c^3 } [ \\psi'(a / c+b ) - \\psi'(a / c)],\\\\   j_{a \\theta}&=&\\frac{\\partial^{2 } l(\\boldsymbol\\theta)}{\\partial a \\partial \\theta } = \\frac{1}{\\gamma}\\sum_{i=1}^{n}\\frac{{t_i } ( e^{\\gamma y_i}-1)}{1-t_i},\\\\   j_{a \\gamma}&= & \\frac{\\partial^{2 } l(\\boldsymbol\\theta)}{\\partial a \\partial \\gamma } = \\frac{\\theta}{{\\gamma}^2}\\sum_{i=1}^{n } \\frac{t_i ( \\gamma y_i e^{\\gamma y_i}-e^{\\gamma y_i}+1)}{1-t_i},\\\\   j_{bb}&= & \\frac{\\partial^{2 } l(\\boldsymbol\\theta)}{\\partial b^2 } = n [ \\psi'(a / c+b ) - \\psi'(b)],\\\\   j_{bc}&=&\\frac{\\partial^{2 } l(\\boldsymbol\\theta)}{\\partial b \\partial c } = -\\frac{na}{c^2 } \\psi'(a / c+b ) - \\sum_{i=1}^{n } \\frac{(1-t_i)^c \\log ( 1-t_i)}{1-(1-t_i)^c},\\\\   j_{b \\theta}&= & \\frac{\\partial^{2 } l(\\boldsymbol\\theta)}{\\partial b \\partial \\theta } = -\\frac{c}{\\gamma } \\sum_{i=1}^{n } \\frac { t_i ( 1-t_i)^{c-1 } ( e^{\\gamma y_i}-1)}{1-(1-t_i)^c},\\\\   j_{b\\gamma}&= & \\frac{\\partial^{2 } l(\\boldsymbol\\theta)}{\\partial b \\partial \\gamma } = - \\frac{c \\theta}{\\gamma ^2}\\sum_{i=1}^{n } \\frac { t_i ( 1-t_i)^{c-1 } ( \\gamma y_i e^{\\gamma y_i}-e^{\\gamma y_i}+1)}{1-(1-t_i)^c},\\\\   j_{cc}&= & \\frac{\\partial^{2 } l(\\boldsymbol\\theta)}{\\partial c^{2 } } = - \\frac{n}{c^2 } + \\frac{2na}{c^3 } [ \\psi(a / c+b )   -\\psi(a / c ) ] \\\\ & & + \\frac{na^2}{c^4 } [ \\psi'(a / c+b ) - \\psi'(a / c)]- ( b-1 ) \\sum_{i=1}^{n } \\frac{(1-t_i)^c ( \\log ( 1-t_i))^2}{[1-(1-t_i)^c]^2},\\\\   j_{c \\theta}&= & \\frac{\\partial^{2 } l(\\boldsymbol\\theta)}{\\partial c \\partial \\theta } = -\\frac{b-1}{\\gamma } \\sum_{i=1}^{n } \\frac{t_i(1-t_i)^{c-1 } ( e^{\\gamma y_i}-1 ) [ c \\log ( 1-t_i)+1- ( 1-t_i)^c]}{[1-(1-t_i)^c]^2},\\\\ j_{c \\gamma}&= & \\frac{\\partial^{2 } l(\\boldsymbol\\theta)}{\\partial c \\partial \\gamma}= -\\frac{\\theta(b-1)}{\\gamma^2 } \\sum_{i=1}^{n } \\frac{t_i ( \\gamma y_i e^{\\gamma y_i}-e^{\\gamma y_i}+1 ) [ c \\log ( 1-t_i)+1- ( 1-t_i)^c]}{(1-t_i)^{1-c}[1-(1-t_i)^c]^2},\\\\   j_{\\theta \\theta}&= & \\frac{\\partial^{2 } l(\\boldsymbol\\theta)}{\\partial \\theta^{2 } } = -\\frac{n}{\\theta ^2 } - \\frac{a-1}{\\gamma ^2 } \\sum_{i=1}^{n }    \\frac{{t_i } ( e^{\\gamma y_i}-1)^2}{(1-t_i)^2 } -\\frac{c ( b-1)}{\\gamma ^2}\\\\   & & \\times \\sum_{i=1}^{n } \\frac{{t_i } ( 1-t_i)^{c-2 } ( e^{\\gamma y_i}-1)^2 \\lbrace { c t_i + ( 1-t_i)^c - 1 } \\rbrace}{(1-(1-t_i)^c)^2},\\\\   j_{\\theta \\gamma}&= & \\frac{\\partial^{2 } l(\\boldsymbol\\theta)}{\\partial \\theta \\partial \\gamma}= \\frac{1}{\\gamma ^2 } \\sum_{i=1}^{n } ( e^{\\gamma y_i } - \\gamma y_i e^{\\gamma y_i } -1)-\\dfrac{c(b-1)}{\\gamma ^3 } \\sum_{i=1}^{n } \\frac{t_i ( e^{\\gamma y_i } - \\gamma y_i e^{\\gamma y_i } -1)}{(1-t_i)^{2-c}(1-(1-t_i)^c)^2}\\\\   & & \\times \\left [ ( 1-t_i)^c ( \\theta e^{\\gamma y_i}+ t_i \\gamma - \\gamma - \\theta ) + c \\theta t_i ( e^{\\gamma y_i}-1 ) + \\gamma ( 1-t_i ) + \\theta ( 1-e^{\\gamma y_i } ) \\right]\\\\   & & + \\dfrac{(a-1)}{\\gamma ^3 } \\sum_{i=1}^{n } \\dfrac{t_i ( e^{\\gamma y_i }",
    "- \\gamma y_i e^{\\gamma y_i } -1 ) ( \\theta e^{\\gamma y_i}+ \\gamma t_i - \\gamma - \\theta)}{(1-t_i)^2},\\\\   j_{\\gamma\\gamma}&= & \\frac{\\partial^{2 } l(\\boldsymbol\\theta)}{\\partial \\gamma^{2}}= \\dfrac{2 \\theta}{\\gamma ^3} \\sum_{i=1}^{n } ( \\gamma y_i e^{\\gamma y_i } - e^{\\gamma y_i } - \\gamma ^2 y_{i}^2e^{\\gamma y_i}/2 + 1)\\\\   & & - \\dfrac{c(b-1)\\theta ^2}{\\gamma ^4 } \\sum_{i=1}^{n } \\dfrac{t_{i}^2 ( 1-t_i)^{c-2}(\\gamma y_i e^{\\gamma y_i}-e^{\\gamma y_i}+1)^2 ( 2c(1-t_i)^c+(1-t_i)^c - c-1)}{(1-(1-t_i)^c)^2}\\\\   & & + \\dfrac{c(b-1)\\theta}{\\gamma ^4} \\sum_{i=1}^{n } \\dfrac{t_{i } ( 1-t_i)^{c-1}}{1-(1-t_i)^c } [ -\\gamma ^3 y_{i}^2 e^{\\gamma y_i } + 2 \\gamma ^2 y_{i } e^{\\gamma y_i}- 2 \\gamma e^{\\gamma y_i } + 2 \\gamma",
    "+ \\theta \\gamma ^2 y_{i}^2 e^{2 \\gamma",
    "y_i}\\\\   & & + \\theta ( e^{\\gamma y_i}-1)^2 - 2 \\theta \\gamma y_{i } e^{\\gamma y_i } ( e^{\\gamma y_i}-1 ) ] \\\\   & & -  \\dfrac{(a-1)\\theta}{\\gamma ^4 } \\sum_{i=1}^{n } \\dfrac{t_i}{1-t_i } \\left [ -\\gamma ^3",
    "y_{i}^2 e^{\\gamma y_i } + 2 \\gamma ^2 y_{i } e^{\\gamma y_i}-2 \\gamma ( e^{\\gamma y_i}-1)+\\theta ( \\gamma y_{i } e^{\\gamma y_i}- e^{\\gamma y_i}+1)^2 \\right]\\\\   & & - \\dfrac{(a-1)\\theta ^2}{\\gamma ^4 } \\sum_{i=1}^{n}\\dfrac{t_{i}^2 ( \\gamma y_{i } e^{\\gamma y_i}- e^{\\gamma y_i}+1)^2}{(1-t_i)^2}.\\end{aligned}\\ ] ] under conditions that are fulfilled for parameters in the interior of the parameter space but not on the boundary , asymptotically @xmath121 where @xmath122 is the expected information matrix .",
    "this asymptotic behavior is valid if @xmath123 replaced by @xmath124 , i.e. , the observed information matrix evaluated at @xmath125 @xcite .",
    "in this section , two real data sets are considered to illustrate that the mcg model can be a good lifetime distribution comparing with main three submodels ; bg , kumg and mce distributions . in both examples ,",
    "we obtain the mle and their corresponding standard errors ( in parentheses ) of the model parameters .",
    "the model selection is carried out using minus of log - likelihood function ( @xmath126 ) , kolmogorov - smirnov ( k - s ) statistic with its p - value , akaike information criterion ( aic ) , akaike information criterion corrected ( aicc ) , bayesian information criterion ( bic ) and likelihood ratio test ( lrt ) with its p - value .",
    "furthermore , we plot the histogram for each data set and the estimated pdf of the four models .",
    "moreover , the plots of empirical cdf of the data sets and estimated cdf of four models are displayed .",
    "the data set have been obtained from @xcite and represents the lifetimes of 50 devices .",
    "also , it is analyzed by @xcite and @xcite .",
    "the results which are given in table 1 indicate that the mce model is not suitable for this data set based on k - s statistic .",
    "the mcg model has the lowest @xmath126 , aic and aicc values among all fitted models , but the bg model has the lowest bic value among all other fitted models .",
    "a comparison of the proposed distribution with some of its submodels using p - value of lrt shows that the mcg model yields a better fit than the other three distributions to this real data set .",
    "it is also clear from figure [ plot.ex1 ] that the mcg distribution provides a better fit and therefore be one of the best models for this data set .    the data set represents the strengths of 1.5 cm glass fibers , measured at the national physical laboratory , england .",
    "unfortunately , the units of measurement are not given in the paper .",
    "it is obtained from @xcite and also analyzed by @xcite .",
    "the k - s statistic indicates that all fitted models are good candidates for this data set , but based on all other criteria : @xmath126 , aic , aicc , bic and p - value of lrt , we can infer that the best model is the mcg distribution .",
    "similar results are concluded from figure [ plot.ex2 ] .",
    "manoj , c. , wijekoon , p. , and yapa , r.  d. ( 2013 ) .",
    "the mcdonald generalized beta - binomial distribution : a new binomial mixture distribution and simulation based comparison with its nested distributions in handling overdispersion .",
    ", 2(2):p24 ."
  ],
  "abstract_text": [
    "<S> this paper introduces a five - parameter lifetime model with increasing , decreasing , upside -down bathtub and bathtub shaped failure rate called as the mcdonald gompertz ( mcg ) distribution . this new distribution </S>",
    "<S> extend the gompertz , generalized gompertz , generalized exponential , beta gompertz and kumaraswamy gompertz distributions , among several other models . </S>",
    "<S> we obtain several properties of the mcg distribution including moments , entropies , quantile and generating functions . </S>",
    "<S> we provide the density function of the order statistics and their moments . </S>",
    "<S> the parameter estimation is based on the usual maximum likelihood approach . </S>",
    "<S> we also provide the observed information matrix and discuss inferences issues . in the end , the flexibility and usefulness of the new distribution is illustrated by means of application to two real data sets .    </S>",
    "<S> _ keywords _ : gompertz distribution ; mcdonald distribution ; maximum likelihood estimation ; kumaraswamy distribution ; moment generating function ; entropy . </S>",
    "<S> 62e15 , 60e05 , 62f10 . </S>"
  ]
}