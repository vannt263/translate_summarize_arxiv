{
  "article_text": [
    "wyner , in his well  known paper on the wiretap channel @xcite , studied the problem of secure communication across a degraded broadcast channel , without using a secret key , where the legitimate receiver has access to the output of the good channel and the wiretapper receives the output of the bad channel . in that paper ,",
    "wyner characterized the optimum trade  off between reliable coding rates and the equivocation at the wiretapper , which was defined in terms of the conditional entropy of the source given the output of the bad channel , observed by the wire ",
    "tapper . among other things , wyner establised and characterized , in the same paper , the notion of the _ secrecy capacity _ , which is the maximum coding rate that still allows full secrecy , where the equivocation is equal to the ( unconditional ) entropy of the source , thus rendering the information available to the wiretapper , virtually useless for learning anything about the source . by applying good codes at rates close to the secrecy capacity ,",
    "the channel is fully exploited in the sense that the `` excess noise '' , that is sufferred at the bad channel output ( beyond the noise at the good channel output ) , plays the role of securing the message with maximum efficiency .",
    "the idea behind the construction of a good code for the wiretapped channel is essentially similar to the idea of binning .",
    "one creates a relatively large code , which is reliably decodable at the legitimate receiver , and which is thought of as an hierarchy of randomized sub ",
    "codes , each of which being reliably decodable individually by the wiretapper . however , the bits that are decodable by the wiretapper are only those of the randomization , and thus carry information that is irrelevant with regard to the source .    throughout the three decades that have passed since @xcite was published ,",
    "the results of that paper have been extended in quite many directions , and we mention here only a few .",
    "csiszr and krner @xcite have generalized wyner s setting to a broadcast channel that is not necessarily degraded ( allowing also a common message to both receivers ) . very shortly afterwards , leung  yan  cheong and hellman @xcite , studied the gaussian wiretap channel , and have shown , among other things that its secrecy capacity is simply the difference between the capacities of the main ( legitimate ) channel and the wiretap channel . in @xcite , ozarow and",
    "wyner studied another model , referred to as the type ii wiretap channel , where the main communication channel is noiseless , but the wiretapper has access to a subset of the coded bits , and optimal tradeoffs were characterized . in @xcite ,",
    "the wiretap channel model was extended to have two parallel broadcast channels , connecting one encoder and one legitmate decoder , where both channels are wiretapped by non ",
    "collaborating wiretappers , and again , optimum tradeoffs where given in terms of single  letter expressions . in @xcite ,",
    "the scope of @xcite was extended in two ways : first , by allowing a secret key to be shared between the encoder and the legitimate receiver , and secondly , by allowing a certain distortion in the reconstruction of the source at the legitimate receiver .",
    "the main coding theorem of @xcite suggests a separation principle , which asserts that no asymptotic optimality is lost if the encoder , first , applies a rate  distortion source code , then encrypts the compressed bits , and finally , applies a good code for the wiretap channel .",
    "more recently , the gaussian wiretap channel model of @xcite was further extended in two directions : one is the gaussian multiple access wiretap channel of @xcite , and the other is gaussian intereference wiretap channel of @xcite , @xcite , where the encoder has access to the interference signal as side information , similarly as in costa s dirty paper channel @xcite .    in this paper , we extend the setting of the wiretap channel in a different direction . for simplicity",
    ", we adopt the structure of a degraded broadcast channel , as in @xcite ( though it is plausible that the results are generalizable to more general broadcast channels ) , and similarly as in @xcite , we allow a secret key shared between the encoder and the authorized decoder , as well as lossy reconstruction of the source within a prescribed distortion level , but we , moreover , allow also side informations , correlated to the source , to be available both to the legitimate decoder and the wiretapper . we assume that the wiretapper receives its side information via a channel that is degraded relative to the side information channel of the innocent decoder ( see fig .",
    "[ gen ] ) .",
    "our main result is a single ",
    "letter characterization of the optimum tradeoff among five figures of merit : the equivocation at the wiretapper , the distortion level in reconstructing the source at the authorized decoder , the bandwidth expansion factor of the coded channels , the rate of the secret key relative to the source , and the average tranmission cost .",
    "one of the motivations for this study is that it establishes a framework for deriving performance limits of systematic codes for wiretapped channels and assessing their loss in performance compared to general codes ( as was done in @xcite in a different context ) : the side information channels ( @xmath0 and @xmath1 in fig .",
    "[ gen ] ) can be thought of as conveying the systematic ( uncoded ) part of the codeword .",
    "we compare the best achievable performance of systematic codes to that of general codes at the same coding rates , in several aspects , like the maximum achievable equivocation in the absence of a secret key , the maximum achievable equivocation in the presence of a full  rate key , the key rate needed to achieve the maximum achievable equivocation , and the distortion achieved when the channel is utilized at rate close to the secrecy capacity .",
    "a few examples are given for situations where systematic codes are as good as ( and sometimes even better than ) general codes .",
    "the outline of the remaining parts of this paper is as follows : in section 2 , we set up the notation , formulate the problem , present the main result , and make a few comments . in section 3 ,",
    "we discuss the implications on systematic coding , and we make comparisons with general codes , as described in the previous paragraph . in section 4 , we prove the converse part of the main result , and finally , in section 5 , we prove the direct part .",
    "we begin by establishing some notation conventions . throughout this paper , scalar random variables ( rv s )",
    "will be denoted by capital letters , their sample values will be denoted by the respective lower case letters , and their alphabets will be denoted by the respective calligraphic letters .",
    "a similar convention will apply to random vectors and their sample values , which will be denoted with same symbols superscripted by the dimension , or by the bold face font , if there is no room for confusion regarding the dimension .",
    "thus , for example , @xmath2 ( @xmath3  positive integer ) or @xmath4 will denote a random @xmath3vector @xmath5 , and @xmath6 is a specific vector value in @xmath7 , the @xmath3th cartesian power of @xmath8 .",
    "sources and channels will be denoted generically by the letter @xmath9 , subscripted by the name of the rv and its conditioning , if applicable , e.g. , @xmath10 is the probability function of @xmath11 at the point @xmath12 , @xmath13 is the conditional probability of @xmath14 given @xmath15 , and so on . whenever clear from the context , these subscripts will be omitted .",
    "information theoretic quantities like entropies and mutual informations will be denoted following the usual conventions of the information theory literature , e.g. , @xmath16 , @xmath17 , and so on .",
    "for single ",
    "letter information quantities ( i.e. , when @xmath18 or @xmath19 ) , subscripts will be omitted , e.g. , @xmath20 will be denoted by @xmath21 , similarly , @xmath22 will be denoted by @xmath23 , and so on . for three random variables , generically denoted @xmath24 , @xmath25 , and @xmath26 , the notation @xmath27 will designate the fact that they form , in this order , a markov chain .",
    "the extension of this notation to longer markov chains will be straightforward .",
    "the cardinality of a finite set @xmath28 will be denoted by @xmath29 .",
    "the notation @xmath30_+$ ] will stand for @xmath31 . finally , for @xmath32",
    ", @xmath33 will denote the modulo 2 sum ( xor ) of @xmath34 and @xmath35 , and for two general positive integers , @xmath34 and @xmath35 , the notation @xmath33 will designate the positive integer whose binary representation is given by the bit  wise modulo 2 sum of the corresponding bits of the binary representations of @xmath34 and @xmath35 .",
    "we now turn to the formal description of the model and the problem setting .",
    "a source @xmath36 generates a sequence of @xmath3 ( @xmath3  positive integer ) independent copies , @xmath37 , of a finite  alphabet rv , @xmath38 . at the same time , a discrete memoryless channel ( dmc ) , symbolized by @xmath0 generates from @xmath2 , another @xmath3-vector @xmath39 , with components in a finite ",
    "alphabet @xmath40 , and another dmc , denoted @xmath1 , produces from @xmath41 , yet another @xmath3-vector @xmath42 , with components in a finite  alphabet @xmath43 .",
    "thus , the joint probability distribution of @xmath44 is given by @xmath45.\\ ] ] at the same time and independently , another source @xmath46 , henceforth referred to as the _ key source _ , generates a random variable ( or vector ) @xmath47 taking values in a finite alphabet @xmath48 .    two additional cascaded dmc s operate at a bandwidth expansion factor of @xmath49 channel uses per source symbol .",
    "this means that during the time that the source generates a block @xmath2 of @xmath3 symbols , the first channel receives a block @xmath50 of @xmath51 channel input symbols taking on values in a finite alphabet @xmath52 , and outputs a block @xmath53 of @xmath51 channel output symbols in a finite alphabet @xmath54 , according to @xmath55 whereas the second dmc receives @xmath53 as an input vector and outputs a block @xmath56 of @xmath51 channel output symbols in a finite alphabet @xmath57 , according to @xmath58    given @xmath3 and @xmath51 , a block encoder is a mapping @xmath59 , whose output is @xmath60 .",
    "the channel input vector should satisfy an average transmission cost ( generalized power ) constraint : @xmath61 where @xmath62 is the generalized power function and @xmath63 is a given positive real . the corresponding block decoder ( of the authorized party ) is a mapping @xmath64 , whose output is @xmath65 , where @xmath66 is the reproduction alphabet of the decoder output symbols .",
    "let @xmath67 denote a single ",
    "letter distortion measure between source symbols and reproduction symbols , and let the distortion between the vectors , @xmath68 and @xmath69 , be defined additively across the corresponding components , as usual .",
    "let @xmath70 denote the wyner  ziv rate",
    " distortion function @xcite of the soure @xmath11 with respect to the distortion measure @xmath71 , and a decoder side information @xmath72 , i.e. , @xmath73,\\ ] ] where the infimum is over all rv s @xmath24 with alphabet size @xmath74 , that form a markov chain @xmath75 and that satisfy @xmath76 . given the degraded broadcast channel @xmath77 , we will also define the function @xmath78\\ ] ] which is similar to wyner s @xmath79 function @xcite , but with the additional generalized power constraint .",
    "an @xmath80 codec is an encoder ",
    "decoder pair with parameters @xmath3 and @xmath51 , that satisfies the following requirements :    * the bandwidth expansion factor is @xmath81 . * the expected distortion between the source and the reproduction satisfies @xmath82 * the equivocation of the message source satisfies @xmath83 * the rate of the secret key is @xmath84 . *",
    "the generalized transmission power satisfies @xmath85 .",
    "a quintuple @xmath86 is said to be _ achievable _ if for every @xmath87 , there is a sufficiently large @xmath3 and @xmath51 for which @xmath88 codecs exist .",
    "the _ achievable region _ of quintuples @xmath89 is the set of all achievable quintuples @xmath86 .",
    "the following theorem characterizes the region of achievable quintuples @xmath86 .",
    "a quintuple @xmath86 is achievable iff @xmath90_+.\\ ] ]    * discussion : * a few comments are in order at this point .    as mentioned in the introduction , theorem 1 generalizes earlier results reported in @xcite , @xcite , and @xcite .",
    "the generalization relative to ( * ? ? ?",
    "* theorem 1 ,  case of ldbc  ) is primarily in the presence of side informations at the authorized decoder as well as the wiretapper .",
    "it should be also noted that in @xcite , there is no full proof of the direct part , but only an intuitive argument .",
    "here , we provide complete proofs for both the converse part and the direct part , which are both based on the corresponding proofs in @xcite , but there are a few twists that are necessary in order to incorporate the secret key , @xmath47 , the side informations , @xmath41 and @xmath91 , and the generalized power constraint . for example , one of the additional ingredients in the proof of the direct part , that is not present in the direct part of @xcite , is that we need to show that the key @xmath47 can be estimated reliably from @xmath2 , @xmath91 , and @xmath56 , so that @xmath92 is small .    as in @xcite ,",
    "theorem 1 here suggests a _ separation principle _",
    ", that guarantees no loss in asymptotically optimum performance , if one separates source coding , encryption , and channel coding .",
    "as will be seen in the proof of the direct part , the proposed achievability scheme consists of wyner  ziv rate - distortion source coding , followed by encryption of the compressed bits , followed in turn by good channel coding for the wiretapped channel , as in @xcite . as is demonstrated in @xcite , the separation principle does not always hold in situations that involve source coding , encryption , and channel coding .",
    "a few words about the intuition behind the achievable upper bound on the equivocation , @xmath93 : for @xmath94 , there is enough randomness to achieve the maximum possible secrecy of @xmath95 , which can not be exceeded even if the wiretapper did not have access to @xmath56 . for the more interesting case where @xmath96 ( which in turn means that @xmath97 is above the secrecy capacity ) , and @xmath98 , we can express @xmath93 as the sum of four terms : @xmath99+[h(u|v)-r_{u|v}(d)]+ \\lambda\\gamma\\left(\\frac{r_{u|v}(d)}{\\lambda},q\\right)+r,\\ ] ] where we have added and subtracted @xmath100 .",
    "now , the first bracketed term designates the fact that the wiretapper has side information whose quality is lower than that of the authorized user , a fact which contributes to the equivocation .",
    "the second bracketed term designates uncertainty due to the information loss at the source encoder ( although a general coding scheme may not necessarily use a source encoder explicitly ) .",
    "out of the @xmath101 bits of the description of the source , @xmath102 bits are covered by the key and another @xmath103 bits are covered by good channel coding for the wiretapped channel , as in @xcite , @xcite . in designing a good coding scheme ,",
    "it should be kept in mind then , that there should be no overlap between the set of bits encrypted by the key and those that are `` hidden '' by coding .",
    "it is interesting to note that in the above decomposition of @xmath93 , the first term depends solely on the joint distribution of @xmath104 , and not on any other factor of the problem , the second term depends only on the joint distribution of @xmath105 and the allowed distortion ( but no longer on the joint distribution with @xmath106 ) , and the third term depends also the coded channels .",
    "referring to the previous comment , it is interesting to note that even in the lossless case ( @xmath107 ) and even if the coded channels are clean ( i.e. , @xmath108 with probability one ) , the presence of side information at the legitimate decoder , which is of better quality than the one at the wiretapper , gives rise to `` inherent secrecy , '' that is present even without a secret key .",
    "in such a case , the last three terms in the above representation of @xmath93 all vanish , but the first term is still positive .",
    "for example , a slepian  wolf encoder for a source @xmath11 and side information @xmath72 , which is based on random binning , has the maximum achievable inherent secrecy of @xmath109 bits / symbol if a wiretapper that observes the compressed bits has no side information .",
    "this is in contrast to the case without side information , where there is no inherent secrecy at all .",
    "an interesting question that arises is about optimum strategies and performance limits if one is interested to maximize the equivocation of @xmath110 instead of , or in addition to that of @xmath2 ( see also @xcite ) , which is reasonable because it is @xmath110 that is the information conveyed from the source .",
    "in contrast to @xcite , where the problem was fully solved using ordinary rate  distortion coding considerations , here , because of the presence of side information , the problem remains open .",
    "finally , as mentioned already in the abstract and the introduction , theorem 1 provides a framework for studying the fundamental performance limits of _ systematic _ ( not necessarily linear ) codes , in the same manner as in @xcite , for the wiretap channel .",
    "the next section is devoted to such a study .",
    "if @xmath111 , @xmath112 , and the uncoded channel , @xmath0 , is understood as an additional use of the same physical channel as the coded channel , @xmath113 , and if @xmath114 , then the uncoded path @xmath115 may be thought of as corresponding to the transmission and reception of the systematic ( uncoded ) part of a systematic code , where the information symbols are sent directly to the channel .",
    "the total bandwidth expansion factor of this systematic code , when the uncoded part is viewed as part of the code , is then @xmath116 , assuming that @xmath117 . for a",
    "fully coded ( general , non ",
    "systematic ) system with the same bandwidth expansion factor , we can use the formula of @xmath93 , but replace @xmath118 by @xmath119 and eliminate the side informations , @xmath41 and @xmath91 .",
    "the resulting maximum achievable equivocation of a general code , is therefore : @xmath120_+,\\ ] ] where @xmath121 is the ordinary rate",
    " distortion function of @xmath11 ( without side information ) , and we are interested to compare this to the original expression of @xmath93 , given in theorem 1 , which will be denoted by @xmath122 throughout this section .",
    "quite obviously , @xmath122 can not exceed @xmath123 , but it is interesting to identify cases of equality , simply by comparing the two expressions .",
    "we will , however , focus here on a few specfic aspects of comparison between optimum systematic codes and optimum general codes :    1 .",
    "the _ full equivocation _ , that is , the maximum equivocation that can be achieved in the absence of limitations on the key rate ( in which case , the bracketed term of @xmath124 vanishes ) .",
    "the _ zero key  rate equivocation _ , which is defined as @xmath124 for @xmath125 .",
    "this quantity manifests the `` inherent '' security that is already present in the system even without a key .",
    "it should be noted that whenever @xmath126 , then , in general ( as can be seen from the expressions of @xmath122 and @xmath123 ) , there is a range of @xmath127 , where @xmath128 since , in that range , both @xmath122 and @xmath123 grow linearly with a slope of 45 degrees , starting from their respective values at @xmath125 .",
    "3 .   the _ saturation key rate _ , which is the smallest value of @xmath127 , for which @xmath124 achieves the full equivocation .",
    "when the saturation key rate is small , then so are the randomization resources required .",
    "the _ secrecy distortion _ , which is the value of @xmath129 for which the channel coding rate equals the secrecy capacity , in other words , the first argument of the function @xmath79 agrees with the secrecy capacity .",
    "this is an interesting working point , because it is the point where the full equivocation is achieved without using a key at all .",
    "in other words , using the terminology that we have already defined , the zero key  rate equivocation is equal to the full equivocation , and the saturation key rate vanishes .    while under the first two criteria , systematic codes can never be strictly better than general codes , this is not necessarily the case with the last two criteria , because codes that are optimum in the maximum equivocation sense may be suboptimal under other criteria .",
    "we next compare optimum systematic codes to optimum codes from the above four aspects .    *",
    "1 . the full equivocation : * obviously , this quantity is @xmath130 for systematic codes and @xmath21 for general codes , thus the difference , @xmath131 , depends only on the joint distribution of @xmath11 and @xmath106 . in this respect ,",
    "optimum systematic codes are as good as optimum general codes only if the side information @xmath106 is independent of @xmath11 and hence useless .",
    "* 2 . the zero key  rate equivocation : * for @xmath125",
    ", we have @xmath132_+\\ ] ] for general codes , and @xmath133_+\\ ] ] for systematic codes .",
    "let us assume that the bracketed terms in both expressions are positive ( otherwise , we are back to the comparison of the previous paragraph ) . comparing the two expressions , we see that equality is achieved if @xmath134 as is shown in ( * ? ? ?",
    "* eqs .  ( 2.12 ) , ( 2.13 ) ) , the difference @xmath135 is never larger than @xmath109 , but there are cases of equality , most notably , the lossless case @xmath107 , as @xmath136 and @xmath137 . , the gaussian channel @xmath0 , and the squared error distortion measure , where @xmath138-[h(u|v)-\\frac{1}{2}\\log(2\\pi e d)]=i(u;v)$ ] throughout the entire interesting range of distortion levels .",
    "] thus , at least in the lossless case , eq .  ( [ equality ] ) boils down to @xmath139 now , in quite a few examples of interest , @xmath140 is equal to a constant , @xmath141 , throughout the entire interesting range of @xmath142 .",
    "one such example occurs when @xmath143 ( i.e. , no generalized power constraint ) , @xmath113 is the noiseless binary channel and @xmath144 is a binary symmetric channel ( bsc ) with crossover probability @xmath145 ( cf .",
    "@xcite ) , in which case , @xmath146 , where @xmath147 is the binary entropy function . in this case , the left",
    " hand side of eq .",
    "( [ equality0 ] ) becomes @xmath148 independently of @xmath118 .",
    "now , if @xmath0 has the same characteristics as @xmath113 , and similatry @xmath1 has the same characteristics as @xmath144 ( which is indeed the case in systematic coding applications ) , and if @xmath36 is the binary symmetric source ( bss ) , then it achieves the maximum of @xmath149 , which is , again , @xmath146 . in this case , therefore , the equality ( [ equality0 ] ) is achieved .",
    "similarly , if @xmath113 is noiseless as before , but @xmath144 is an erasure channel with erasure probability @xmath145 , then @xmath150 , and once again , equality is achieved if @xmath36 is the bss .",
    "yet another example of this type occurs when both @xmath113 and @xmath144 are independent gaussian channels with an input power constraint defined in terms of @xmath151 ( and hence , so are @xmath0 and @xmath1 ) . in this case",
    ", as was shown in @xcite , @xmath152 , the difference between the capacities of the channels @xmath113 and @xmath153",
    ". here , equality in ( [ equality0 ] ) is achieved if @xmath11 is a zero  mean gaussian random variable whose variance coincides with the maximum allowable input power , @xmath63 .",
    "thus , we have demonstrated a few non  trivial examples where optimum systematic codes are as good as optimum codes in the absence of a secret key .    *",
    "the saturation key rate : * here , we obtain @xmath154 for general codes , and @xmath155 for systematic codes . the condition for having a smaller saturation key rate for systematic codes",
    "is @xmath156 namely , the comparison is similar to the one made with regard to the zero key  rate equivocation criterion , but without the term @xmath131 .",
    "as we have previously shown examples of equality , even in the presence of the term @xmath131 , then the same examples can serve now for the desired inequality in the absence of this term . in these examples , as well as in many others ,",
    "optimum systematic codes are advantageous over optimum codes in general .",
    "* 4 . the secrecy distortion : * as mentioned earlier ,",
    "wyner @xcite has established the notion of the secrecy capacity , @xmath157 , which is the maximum coding rate for which full secrecy is still achieved even without a key . here",
    "we ask how do systematic and non ",
    "systematic codes compare in terms of the distortion , @xmath129 , for which the rate of the channel code meets the secrecy capacity . for non ",
    "systematic codes , this distortion level is given by the solution to the equation @xmath158 which is @xmath159 where @xmath160 is the ordinary distortion ",
    "rate function of @xmath11 ( without side information ) . for systematic coding , on the other hand ,",
    "it is the solution to the equation @xmath161 which is @xmath162 where @xmath163 is the wyner  ziv distortion  rate function of @xmath11 with side information @xmath72 .",
    "the answer to the question : which class of codes is better in terms of the secrecy distortion , depends on the parameters of the problem .",
    "one simple extreme example pertains to the case @xmath164 ( which happens , e.g. , when the channel @xmath144 is clean and hence @xmath165 with probability one ) . in this case",
    ", @xmath166 is clearly smaller than @xmath167 while the case where @xmath157 is strictly zero , clearly trivializes the whole problem altogether , it is , of course , conceivable that for small enough positive values of @xmath157 , continuity arguments imply that systematic codes still outperform non ",
    "systematic codes in the secrecy distortion sense .    as a somewhat less trivial example , consider the case where @xmath11 is zero  mean , gaussian , with variance @xmath168 , the channels are gaussian and independent , and @xmath71 is the squared error criterion . then , @xmath169 whereas @xmath170 where @xmath171 is the minimum mean squared error associated with optimum ( linear ) estimation of @xmath11 based on @xmath72 .",
    "thus , @xmath172 whenever @xmath173 where @xmath174 is the variance of the noise of the ( gaussian ) channel from @xmath11 to @xmath72 .",
    "note that the dependence upon @xmath118 disappeared .",
    "the last inequality is clearly met if , for example , the channel @xmath113 is the same as the channel @xmath0 and @xmath175 ( in which case , the right  hand side becomes @xmath176 ) .",
    "note that in this aspect of the secrecy distortion , our comparison between systematic codes and non ",
    "systematic codes is of the same spirit as in @xcite , in the sense that both are about equating rate ",
    "distortion functions to capacities .",
    "the only difference is that here , as opposed to @xcite , @xmath157 replaces @xmath176 in the these equations ( as there is only one coded channel and one uncoded channel in @xcite ) .",
    "obviously , in the comparisons carried out in @xcite , systematic codes can never outperform non  systematic codes .",
    "by contrast , as we have seen here , when the secrecy capacity is the working point , this becomes possible .",
    "finally , one more comment is in order regarding systematic codes : in a real systematic code for the wiretap channel , there is , in principle , the freedom to use part of the secret key in order to encrypt the systematic symbols as well .",
    "this freedom has not been exploited thus far , and the question is whether there is any advantage in doing so .",
    "suppose that the source @xmath11 is binary and the key rate is @xmath127 bits per source symbol ( @xmath177 ) .",
    "consider the following coding scheme .",
    "we select @xmath178 , and for each block @xmath2 , we use @xmath179 key bits to encrypt the systematic part and @xmath180 key bits to encrypt the wyner  ziv rate  distortion codeword before it is fed into the channel encoder of @xcite ( see also the proof of the direct part in section 5 ) . then , by a slight extension of the analysis in section 5 to follow , the resulting equivocation is essentially @xmath181 since the coefficient of @xmath182 , in this expression , is @xmath183 , the best choice of @xmath182 , in this example , is @xmath184 , namely , secret key bits should better not be used for encrypting the systematic bits , but only the coded bits , as we assumed thus far .",
    "let an @xmath88 codec be given .",
    "consider first the following chain of inequalities , which will be used later on .",
    "@xmath185\\nonumber\\\\ & { \\stackrel{\\mbox{(b)}}{\\ge}}&\\sum_{i=1}^n[h(u_i|v_i)-h(u_i|y^n , k , v^n)]\\nonumber\\\\ & { \\stackrel{\\mbox{(c)}}{=}}&\\sum_{i=1}^n[h(u_i|v_i)-h(u_i|v_i , a_i)]\\nonumber\\\\ & = & \\sum_{i=1}^ni(u_i;a_i|v_i)\\nonumber\\\\ & = & \\sum_{i=1}^n[h(a_i|v_i)-h(a_i|u_i , v_i)]\\nonumber\\\\ & { \\stackrel{\\mbox{(d)}}{=}}&\\sum_{i=1}^n[h(a_i|v_i)-h(a_i|u_i)]\\nonumber\\\\ & = & \\sum_{i=1}^n[i(u_i;a_i)-i(v_i;a_i)]\\nonumber\\\\ & { \\stackrel{\\mbox{(e)}}{\\ge}}&\\sum_{i=1}^nr_{u|v}({\\mbox{\\boldmath $ e$}}d(u_i,[g_{n , n}(a_i , v_i)]_i))\\nonumber\\\\ & { \\stackrel{\\mbox{(f)}}{\\ge}}&nr_{u|v}\\left(\\frac{1}{n}\\sum_{i=1}^n{\\mbox{\\boldmath $ e$}}d(u_i,[g_{n , n}(a_i , v_i)]_i)\\right)\\nonumber\\\\ & { \\stackrel{\\mbox{(g)}}{\\ge}}&nr_{u|v}(d+\\epsilon),\\end{aligned}\\ ] ] where ( a ) follows from the fact that @xmath186 is a markov chain , ( b ) is because conditioning reduces entropy , in ( c ) ",
    "@xmath187 is defined as @xmath188 , ( d ) is because @xmath189 is a markov chain , ( e ) is by definition of the wyner",
    " ziv rate ",
    "distortion function , where @xmath190_i$ ] is the projection of @xmath191 to the @xmath192th component , ( f ) is due to the convexity of the wyner  ziv rate  distortion function @xcite , ( * ? ? ?",
    "* lemma 14.9.1 , p.  439 ) , and ( g ) is due to its monotonicity , and the hypothesis that the codec achieves distortion @xmath193 .",
    "we next derive two upper bounds on @xmath194 .",
    "the first one is trivial : @xmath195 and so , @xmath196 due to the arbitrariness of @xmath197 . the other , more interesting , upper bound on @xmath194 is obtained as follows : first , we observe that @xmath198 next , we bound from above each one of the terms on the right  most side .",
    "as for the first term , we have @xmath199,\\end{aligned}\\ ] ] where in the second inequality we have used the fact that @xmath200 is a markov chain . as for the second term on the r.h.s .  of ( [ 2terms ] )",
    ", we have : @xmath201-i(u^n;k , z^n|w^n , v^n)\\nonumber\\\\ & = & nh(u|v , w)-i(u^n;k , y^n|w^n , v^n)+\\nonumber\\\\ & & [ i(u^n;k , y^n|w^n , v^n)-i(u^n;k , z^n|w^n , v^n)].\\end{aligned}\\ ] ] we proceed by deriving a lower bound to @xmath202 and an upper bound to the bracketed term in the last expression . as for the former",
    ", we have : @xmath203 where the second inequality has been proven above ( compare the right  hand side of the first line of eq .",
    "( [ 2nd ] ) with the right  most side of that equation ) .",
    "as for the upper bound to the bracketed term of the right  most side of ( [ 3terms ] ) , we have : @xmath204\\nonumber\\\\ & { \\stackrel{\\mbox{(g)}}{\\le}}&\\sum_{i=1}^n[h(y_i|y^{i-1},k , w^n , v^n)-h(z_i|z^{i-1},y^{i-1},k , w^n , v^n)+\\nonumber\\\\ & & h(z_i|x_i , k , w^n , v^n)-h(y_i|x_i , k , w^n , v^n)]\\nonumber\\\\ & { \\stackrel{\\mbox{(h)}}{=}}&\\sum_{i=1}^n[h(y_i|y^{i-1},k , w^n , v^n)-h(z_i|y^{i-1},k , w^n , v^n)+\\nonumber\\\\ & & h(z_i|x_i , y^{i-1},k , w^n , v^n)-h(y_i|x_i , y^{i-1},k , w^n , v^n)]\\nonumber\\\\ & = & \\sum_{i=1}^n[i(x_i;y_i|y^{i-1},k , w^n , v^n)-i(x_i;z_i|y^{i-1},k , w^n , v^n)]\\nonumber\\\\ & = & \\sum_{i=1}^n[h(x_i|z_i , y^{i-1},k , w^n , v^n)-h(x_i|y_i , y^{i-1},k , w^n , v^n)]\\nonumber\\\\ & { \\stackrel{\\mbox{(i)}}{=}}&\\sum_{i=1}^n[h(x_i|z_i , y^{i-1},k , w^n , v^n)-h(x_i|y_i , z_i , y^{i-1},k , w^n , v^n)]\\nonumber\\\\ & = & \\sum_{i=1}^ni(x_i;y_i|z_i , y^{i-1},k , w^n , v^n)\\end{aligned}\\ ] ] where ( a ) is by adding and subtracting @xmath205 , ( b ) is by adding @xmath206 and subtracting @xmath207 , ( c ) is by the fact that @xmath50 is a function of @xmath2 and @xmath47 , ( d ) is by adding and subtracting @xmath208 , ( e ) is by the fact that @xmath47 is degenerate as it appears in the conditioning , ( f ) is by the fact that @xmath209 is a markov chain , ( g ) is because conditioning reduces entropy , ( h ) is because @xmath210 and @xmath211 are markov chains , and ( i ) is because @xmath212 is a markov chain .    at this point",
    ", we are after an upper bound to @xmath213 , subject to the fact that @xmath214\\nonumber\\\\ & = & \\sum_{i=1}^ni(x_i;y_i|y^{i-1},k , v^n , w^n)\\end{aligned}\\ ] ] where , once again , the first inequality has been proved already in ( [ 2nd ] ) .",
    "for given @xmath215,@xmath216 , @xmath217 , and @xmath218 , @xmath219 , let @xmath220 and @xmath221 obviously , by definition of the function @xmath79 , @xmath222 thus , @xmath223 where ( a ) follows from the concavity of @xmath140 jointly in both arguments , together with its non  increasing monotonicity in @xmath142 and non  decreasing monotonicity in @xmath224 , ( b) from ( [ arg ] ) and the non  increasing monotonicity of the function @xmath225 , and ( c ) and ( d )  from the postulate that the bandwidth expansion factor of the codec does not exceed @xmath226 .",
    "combining eqs .",
    "( [ 1stbound ] ) , ( [ 2terms ] ) , ( [ 15 ] ) , ( [ 3terms ] ) , ( [ 17 ] ) , ( [ 18 ] ) , and ( [ 22 ] ) , and using the arbitrariness of @xmath197 with continuity considerations , we get @xmath227_+\\nonumber\\\\ & = & \\delta^*(\\lambda , r , d , q),\\end{aligned}\\ ] ] which establishes the converse part of theorem 1 .",
    "we begin with the following chain of equalities and inequalities : @xmath228-h(z^n|w^n)\\nonumber\\\\ & = & nh(u|w)+h(z^n|x^n , k , u^n , w^n)+\\nonumber\\\\ & & i(x^n , k;z^n|u^n , w^n)-h(z^n|w^n)\\nonumber\\\\ & { \\stackrel{\\mbox{(a)}}{\\ge}}&nh(u|w)+h(z^n|x^n)+\\nonumber\\\\ & & i(x^n , k;z^n|u^n , w^n)-h(z^n)\\nonumber\\\\ & = & nh(u|w)+i(x^n , k;z^n|u^n , w^n)-i(x^n;z^n)\\nonumber\\\\ & = & nh(u|w)+i(k;z^n|u^n , w^n)+i(x^n;z^n|u^n , w^n , k)-i(x^n;z^n)\\nonumber\\\\ & = & nh(u|w)+h(k|u^n , w^n)-h(k|u^n , w^n , z^n)+\\nonumber\\\\ & & i(x^n;z^n|u^n , w^n , k)-i(x^n;z^n)\\nonumber\\\\ & { \\stackrel{\\mbox{(b)}}{=}}&nh(u|w)+h(k)-h(k|u^n , w^n , z^n)+\\nonumber\\\\   & & i(x^n;z^n|u^n , w^n , k)-i(x^n;z^n)\\nonumber\\\\ & { \\stackrel{\\mbox{(c)}}{=}}&nh(u|w)+nr - h(k|u^n , w^n , z^n)+\\nonumber\\\\ & & i(x^n;z^n|u^n , w^n , k)-i(x^n;z^n),\\end{aligned}\\ ] ] where ( a ) follows from the fact that @xmath229 is a markov chain , ( b )  from the fact that @xmath47 is independent of @xmath230 , and ( c )  by assuming that @xmath231 .",
    "while this chain of equalities and inequalities holds for any codec , then in order to proceed , we will have to be specific , from now on , about the structure and the properties of the codec .",
    "in particular , referring to the right  most side of the above lower bound to @xmath232 , then in order to prove the direct part , we will have to prove that for our proposed codec ( and as long as @xmath127 is not too large ) : ( i ) @xmath92 is small , ( ii ) @xmath233 is essentially smaller than @xmath234 , and ( iii ) @xmath235 is essentially larger @xmath236 $ ] , where in ( ii ) and ( iii ) the distribution of the random variable @xmath237 is the achiever of @xmath238 .",
    "fix an arbitrarily small @xmath87 , and let @xmath129 satisfy @xmath239 , where @xmath176 is the capacity of the channel @xmath113 .",
    "given such @xmath129 and @xmath87 , let @xmath240 denote the channel input variable that achieves @xmath241 .",
    "let @xmath242 and @xmath243 denote the channel output variables induced by @xmath240 and the channels @xmath113 and @xmath144 , respectively .",
    "thus , @xmath244 and @xmath245 let us further suppose now that for the resulting optimal rv s @xmath240 , @xmath242 , and @xmath243 , we have : @xmath246 in the sequel , we will handle separately the case where ( [ pos ] ) does not hold .",
    "further , let @xmath247 denote the set of @xmath248typical @xmath51sequences with components in @xmath52 , i.e. , the set of sequences for which the relative frequency of each @xmath249 differs from @xmath250 by no more than @xmath248 .",
    "the following lemma , which is lemma 8 of @xcite , guarantees that if the encoder is such that , with high probability @xmath251 , then condition ( ii ) above is essentially satisfied :      note that whenever @xmath251 , the generalized power constraint is also essentially satisfied .",
    "it remains to handle conditions ( i ) and ( iii ) .",
    "consider next the encoder and the decoder of the legitimate receiver , depicted in fig .",
    "the source vector @xmath2 is first    compressed by a wyner ",
    "ziv encoder , designed for distortion level @xmath129 and side information @xmath41 , to a string of bits , @xmath255 , whose length does not exceed @xmath256\\le n[i(x^*;y^*)-\\epsilon/(2\\lambda)]$ ] . now , let us select @xmath127 in the range @xmath257-\\epsilon,\\ ] ] where the right  most side is positive due to ( [ pos ] ) .",
    "the key @xmath47 is a string of @xmath102 purely random bits , which are xored with ( the first ) @xmath102 bits of @xmath258 ( one time pad ) .",
    "the resulting ( partially ) encrypted bit string , @xmath259 , which will be represented by @xmath260 ( although it is possible that only some of the bits of @xmath258 are xored with those of @xmath47 ) , is the message to be conveyed across the channel .",
    "now , let @xmath261}.\\ ] ] next , let @xmath262 , where @xmath263 is a positive integer to be specified in the sequel .",
    "let @xmath264 be a subset of @xmath265 , which can be viewed as a code for the channel @xmath113 or @xmath153 . the channel encoder and decoder in fig .",
    "[ sys ] work as follows .",
    "they both share a partition of @xmath264 into @xmath266 sub  codes , @xmath267 , each of size @xmath263 .",
    "let @xmath268 , @xmath269 .",
    "when @xmath270 , the channel encoder outputs a vector @xmath50 which is a ( uniformly ) randomly chosen member of sub  code @xmath271 .",
    "thus , for @xmath269 , @xmath272 , @xmath273 and @xmath274 as mentioned earlier , the set @xmath264 can be thought of as a code for the channel @xmath113 , where the prior probabilities of the codewords are given by ( [ prior ] ) . let @xmath275 denote the bayes  optimal decoder for this code and these prior probabilities , which estimates the index @xmath276 of the sub ",
    "code @xmath271 that contains the transmitted codeword @xmath50 .",
    "let @xmath277 . obviously , if @xmath278 is small , namely , if @xmath279 with high probability , then the wyner",
    " ziv decoder @xmath280 would output the `` correct '' reconstruction vector within distortion @xmath129 , with the same probability .",
    "next , observe that each sub  code @xmath271 may serve as a channel code for the degraded channel @xmath153 , provided that the corresponding decoder is informed of @xmath276 .",
    "let @xmath281 , @xmath269 , denote the error probability of code @xmath271 w.r.t .",
    "the channel @xmath153 when the decoder that observes @xmath56 is informed of @xmath276 .",
    "finally , let @xmath282 . with these definitions ,",
    "we next make our first step to handle condition ( iii ) .",
    "let @xmath2 and @xmath47 be such that @xmath283 .",
    "then , the channel input , given @xmath270 , is distributed according to ( [ condprior ] ) , that is , @xmath50 is a randomly chosen member of @xmath271 , thus , @xmath284 . since @xmath285 is the probability of error associated with @xmath271 , fano s inequality yields : @xmath286 where @xmath147 is the binary entropy function @xmath287 .",
    "it follows then that @xmath288 which upon averaging over @xmath289 with weights @xmath290 , yields @xmath291 on the other hand , @xmath292 where the second equality is due to the markov relation @xmath293 .",
    "thus , we have established the inequality @xmath294 in the sequel , we will choose @xmath263 so as to meet condition ( iii ) .",
    "we next move on to handle condition ( i ) . for every @xmath295 ,",
    "let @xmath296 denote the union of all @xmath297 codebooks @xmath298 , and let @xmath299 denote the error probability of @xmath296 w.r.t .",
    "the channel @xmath153 when the decoder is informed of @xmath300 .",
    "let @xmath301 finally , let @xmath302 . with these definitions ,",
    "let us now derive an upper bound on @xmath92 : @xmath303\\nonumber\\\\ & \\le&1+\\bar{\\delta}'(nr+\\log m_2),\\end{aligned}\\ ] ] where the third inequality is again fano s inequality , and where we have also used the fact that @xmath304 is an upper bound of the probability of error in estimating @xmath259 , since @xmath259 is only the index @xmath276 of the codebook @xmath271 to which the estimated codeword belongs .    to summarize our findings thus far , we substitute eqs .",
    "( [ condition1 ] ) , ( [ condition2 ] ) and ( [ condition3 ] ) into ( [ genlowerbound ] ) , divide by @xmath3 , and get : @xmath305.\\end{aligned}\\ ] ] now , let us select @xmath306}\\ ] ] and @xmath307}.\\ ] ] applying this to ( [ 2ndlb ] ) , we get @xmath308-\\frac{2}{n}- \\bar{\\delta}'(r+\\log |{{\\cal x}}|)-\\nonumber\\\\ & & \\lambda[i(x^*;z^*)+\\mbox{pr } \\{x^n\\in { { \\cal t}}_n^c\\}\\cdot\\log|{{\\cal x}}|+f_1(n)]\\nonumber\\\\ & \\ge&h(u|w)+r+\\lambda[i(x^*;y^*)-i(x^*;z^*)]-r_{u|v}(d)-\\nonumber\\\\ & & \\left\\{\\epsilon+\\frac{2}{n}+ ( \\bar{\\delta}+\\bar{\\delta}')(r+\\log |{{\\cal x}}|)+\\lambda[\\mbox{pr } \\{x^n\\in { { \\cal t}}_n^c\\}\\cdot\\log|{{\\cal x}}|+f_1(n)]\\right\\}\\nonumber\\\\ & = & h(u|w)+r+\\lambda\\gamma\\left(\\frac{r_{u|v}(d)+\\epsilon}{\\lambda},q\\right)-r_{u|v}(d)-\\nonumber\\\\ & & \\left\\{\\epsilon+\\frac{2}{n}+ ( \\bar{\\delta}+\\bar{\\delta}')(r+\\log |{{\\cal x}}|)+\\lambda[\\mbox{pr } \\{x^n\\in { { \\cal t}}_n^c\\}\\cdot\\log|{{\\cal x}}|+f_1(n)]\\right\\}.\\end{aligned}\\ ] ] finally , to prove that the expected distortion of @xmath110 relative to @xmath2 is essentially @xmath129 , and to prove that @xmath194 essentially meets the upper bound @xmath93 ( namely , that the last term on the right  most side of ( [ almostdone ] ) is arbitrarily small for large @xmath3 ) , we have to prove the existence of a code @xmath264 for which @xmath278 , @xmath309 , @xmath310 and @xmath311 are all simultaneously arbitrarily small for large @xmath3 .",
    "to this end , let us define @xmath312 , and for a given code @xmath264 , let @xmath313 denote the error probability w.r.t .  the channel @xmath113 with prior probabilities @xmath290 as given in ( [ prior ] ) , when @xmath314 is transmitted . then",
    ", @xmath315 further , let @xmath281 , @xmath299 , @xmath309 , and @xmath310 be defined as above . then ,",
    "@xmath316.\\end{aligned}\\ ] ] now , suppose that @xmath264 are selected at random , with each @xmath314 chosen independently according to @xmath317 .",
    "to prove that there exists a sequence of codes for which @xmath318 as @xmath254 , all we have to show is that @xmath319 . but",
    "@xmath320 where the indices @xmath300 , @xmath215 , and @xmath321 are now immaterial .",
    "the first term tends to zero by the weak law of large numbers .",
    "the second term tends to zero by the ordinary random channel coding argument as the rate of the code @xmath264 is less than @xmath322 ( cf .  the choice of @xmath323 above ) .",
    "by the same token , the fourth term vanishes with @xmath51 , as @xmath296 is a random code of size @xmath324 , and so its rate ( cf .",
    "( [ keyrate ] ) ) is @xmath325 which means that it is reliable for the channel @xmath153 on the average .",
    "fortiori , the third term decays with @xmath51 as @xmath271 is even a smaller random code . by a simple application of the chebychev inequality , with probability of at least @xmath326 , the random of selection of the code yields @xmath327 , which is still vanishingly small . on the other hand , since the codeword components are selected i.i.d .  under @xmath328 ,",
    "then by the weak law of large numbers , for every @xmath87 and large enough @xmath51 and @xmath323 , we have , with probability that tends to unity , and in particular , larger than @xmath329 from some point on : @xmath330(j ) ) \\le q+\\epsilon\\ ] ] where @xmath331(j)$ ] is the @xmath332th component of the codeword @xmath333 .",
    "since @xmath334 , it follows then that there exist codes for which both @xmath335 ( and hence all components of @xmath336 must be small ) and the power constraint ( [ power ] ) holds at the same time .",
    "finally , for completeness , we give a sketchy description of how the proof of the direct part should be slighlty modified in the ( simpler ) case where eq .",
    "( [ pos ] ) does not hold , namely , @xmath338 note that in this case , the achievable upper bound on @xmath194 , asserted in theorem 1 , becomes @xmath130 even for @xmath125 , as the bracketed term therein is non  positive . in the case",
    ", we will not use the key at all , i.e. , @xmath125 and @xmath47 is degenerate .",
    "thus , ( [ genlowerbound ] ) becomes now : @xmath339 as before , @xmath233 is essentially upper bounded by @xmath340 using lemma 1 , and so , we only have to deal with the term @xmath341 and show that it is essentially lower bounded by @xmath340 . to this end , let us re  define @xmath323 as @xmath342},\\ ] ] and @xmath266 as before , so , @xmath343}.\\ ] ] now , since @xmath344 ( cf .",
    "( [ neg ] ) ) , the full codeword @xmath314 can be reliably decoded at the legitimate decoder , as before . also , since each sub  code @xmath271 is , again , of rate less than @xmath345 , then it can be decoded reliably by the wiretapper , provided that s / he is informed of @xmath276 , thus @xmath341 is again , essentially lower bounded by @xmath346 $ ] .",
    "this completes the proof of direct part of theorem 1 ."
  ],
  "abstract_text": [
    "<S> shannon s secrecy system is studied in a setting , where both the legitimate decoder and the wiretapper have access to side information sequences correlated to the source , but the wiretapper receives both the coded information and the side information via channels that are more noisy than the respective channels of the legitmate decoder , which in turn , also shares a secret key with the encoder . a single  </S>",
    "<S> letter characterization is provided for the achievable region in the space of five figures of merit : the equivocation at the wiretapper , the key rate , the distortion of the source reconstruction at the legitimate receiver , the bandwidth expansion factor of the coded channels , and the average transmission cost ( generalized power ) . beyond the fact that this is an extension of earlier studies </S>",
    "<S> , it also provides a framework for studying fundamental performance limits of systematic codes in the presence of a wiretap channel . the best achievable performance of systematic codes </S>",
    "<S> is then compared to that of a general code in several respects , and a few examples are given . + </S>",
    "<S> * index terms : * wiretap channel , encryption , shannon s cipher system , separation theorem , systematic codes .    </S>",
    "<S> department of electrical engineering + technion - israel institute of technology + technion city , haifa 32000 , israel + e  mail : merhav@ee.technion.ac.il + </S>"
  ]
}