{
  "article_text": [
    "standard technique for estimating sparse signals is through the formulation of an inverse problem with the @xmath0 norm as convex proxy for sparsity . in particular , consider the problem of estimating a signal @xmath1 from a noisy observation @xmath2 , @xmath3where @xmath4 represents awgn .",
    "we assume the underlying signal to be sparse with respect to an overcomplete tight frame @xmath5 , @xmath6 , which satisfies the tight frame condition , i.e. , @xmath7using an analysis - prior , we formulate the signal denoising problem as @xmath8_i ; a_i \\right)\\biggr\\rbrace,\\end{aligned}\\ ] ] where @xmath9 are the regularization parameters , and @xmath10 is a non - smooth sparsity inducing penalty function .",
    "the parameters @xmath11 control the non - convexity of @xmath12 in case it is non - convex .",
    "the analysis prior is used in image processing and computer vision applications @xcite .",
    "commonly , the @xmath0 norm is used to induce sparsity , i.e. , @xmath13 @xcite . in that case , problem is strictly convex and the global optimum can be reliably obtained .",
    "the @xmath0 norm is not the tightest envelope of sparsity @xcite .",
    "it under - estimates the non - zero values of the underlying signal @xcite .",
    "non - zero values can be more accurately estimated using suitable non - convex regularizers .",
    "non - convex regularization in an analysis model has been used for mri reconstruction @xcite , eeg signal reconstruction @xcite , and for computer vision problems @xcite .",
    "however , the use of non - convex regularizers comes at a price : the objective function is generally non - convex .",
    "consequently , several issues arise ( spurious local minima , a perturbation of the input data can change the solution unpredictably , convergence is guaranteed to the local minima only , etc . ) .    in order to maintain convexity of the objective function while using non - convex regularizers ,",
    "we propose to restrict the parameter @xmath11 of the non - convex regularizer @xmath12 . by controlling the degree of non - convexity of the regularizer we guarantee that the total objective function @xmath14 is convex .",
    "this idea which dates to blake and zisserman @xcite and nikolova @xcite , has been applied to image restoration and reconstruction @xcite , total variation denoising @xcite , and wavelet denoising @xcite .    in this letter",
    "we provide a critical value of parameter @xmath15 to ensure @xmath14 in is strictly convex ( even though @xmath12 is non - convex ) .",
    "in contrast to the above works , we consider transform domain regularization and prove that admm @xcite applied to the problem converges to the global optimum",
    ". the convergence of admm is guaranteed , provided the augmented lagrangian parameter @xmath16 , satisfies @xmath17 .",
    "in order to induce sparsity more strongly than the @xmath0 norm , we use non - convex penalty functions @xmath10 parameterized by the parameter @xmath18 . we make the following assumption of such penalty functions .",
    "[ theorem::assumption 1 ] the non - convex penalty function @xmath19 satisfies the following    1 .",
    "@xmath12 is continuous on @xmath20 , twice differentiable on @xmath21 and symmetric , i.e. , @xmath22 2 .",
    "@xmath23 3 .",
    "@xmath24 4 .",
    "@xmath25 5 .",
    "@xmath26 6 .",
    "@xmath27 .     and the function @xmath28 , @xmath29 . ]    since @xmath27",
    ", the @xmath0 norm is recovered as a special case of the penalty function @xmath12 .",
    "the parameter @xmath15 controls the degree of non - convexity of @xmath12 .",
    "note that the @xmath30 norm does not satisfy assumption [ theorem::assumption 1 ] .",
    "the rational penalty function @xcite , @xmath31the logarithmic , and the arctangent penalty functions @xcite are examples that satisfy assumption [ theorem::assumption 1 ] .",
    "the rational penalty @xmath12 for @xmath32 is shown in fig .",
    "[ fig::phi minus absolute ] .",
    "the proximity operator of @xmath12 @xcite , @xmath33 , is defined as @xmath34 for @xmath35 satisfying assumption [ theorem::assumption 1 ] , with @xmath36 , the proximity operator is a continuous non - linear threshold function with @xmath37 as the threshold value , i.e. , @xmath38 .",
    "the proximity operator of the absolute value function is the soft - thresholding function .",
    "there is a constant gap between the identity function and the soft - threshold function due to which the non - zero values are underestimated @xcite . on the other hand ,",
    "non - convex penalty functions satisfying assumption [ theorem::assumption 1 ] are specifically designed so that the threshold function approaches identity asymptotically .",
    "these non - convex penalty functions do not underestimate large values .      in order to benefit from convex optimization principles in solving ,",
    "we seek to ensure @xmath14 in is convex by controlling the parameter @xmath11 .",
    "for later , we note the following lemma .",
    "[ theorem::lemma 1 ] let @xmath10 satisfy assumption [ theorem::assumption 1 ] .",
    "the function @xmath39 defined as @xmath40is twice continuously differentiable and concave with @xmath41    since @xmath12 and the absolute value function are twice continuously differentiable on @xmath42 , we need only show @xmath43 and @xmath44 . from assumption [ theorem::assumption 1 ] , we have @xmath45 , hence @xmath46 . again by assumption",
    "[ theorem::assumption 1 ] we have @xmath47 , hence @xmath48 .",
    "further , @xmath49 and @xmath50 .",
    "thus the function @xmath51 is twice continuously differentiable .",
    "the function @xmath51 is concave since @xmath52 . using assumption [ theorem::assumption 1 ]",
    "it follows that @xmath53 .    figure  [ fig::phi minus absolute ] displays the function @xmath54 , which is twice continuously differentiable even though the penalty function @xmath12 is not differentiable .",
    "the following theorem states the critical value of parameter @xmath11 to ensure the convexity of @xmath14 in .",
    "[ theorem::theorem 1 ] let @xmath55 be a non - convex penalty function satisfying assumption [ theorem::assumption 1 ] and @xmath56 be a transform satisfying @xmath57 , @xmath58 .",
    "the function @xmath59 defined in is strictly convex if @xmath60    consider the function @xmath61 defined as @xmath62_i ; a_i ) . \\end{aligned}\\ ] ] since @xmath63 is twice continuously differentiable ( using lemma [ theorem::lemma 1 ] ) , the hessian of @xmath63 is given by @xmath64where @xmath65_i;a_i\\right)$ ] .",
    "using , we write the hessian as @xmath66the transform @xmath56 has full column rank , from , hence @xmath67 is positive definite if @xmath68 thus , @xmath67 is positive definite if @xmath69_i;a_i ) > -\\dfrac{1}{r\\lambda_i}.\\end{aligned}\\ ] ] using lemma [ theorem::lemma 1 ] , we obtain the critical value of @xmath11 to ensure the convexity of @xmath63 , i.e. , @xmath70it is straightforward that @xmath71_i|.\\end{aligned}\\ ] ] thus , being a sum of a strictly convex function and a convex function , @xmath14 is strictly convex .",
    "note that if @xmath72 , then the function @xmath73 is not convex , as the hessian of @xmath73 is not positive definite . as a result , @xmath74 is the critical value of @xmath11 to ensure the convexity of the function @xmath14 .",
    "the following corollary provides a convexity condition for the situation where the same regularization parameter is applied to all coefficients .",
    "[ theorem::corollary 1 ] for @xmath75 , the function @xmath14 in is strictly convex if @xmath76 @xmath77    we illustrate the convexity condition using a simple example with @xmath78 .",
    "we set @xmath79 , \\quad { \\mathnormal{a}}^{t}{\\mathnormal{a } } = 4{\\mathnormal{i } } , \\end{aligned}\\]]and @xmath80 .",
    "theorem [ theorem::theorem 1 ] states that the function @xmath63 defined in is convex for @xmath81 and non - convex for @xmath82 .",
    "it can be seen in fig .",
    "[ fig::convexity condition ] that the function @xmath63 is convex for @xmath83 , even though the penalty function is not convex .",
    "however , when @xmath84 , the function @xmath63 ( hence @xmath14 ) is non - convex .",
    "a benefit of ensuring convexity of the objective function is that we can utilize convex optimization approaches to obtain the solution . in particular , for @xmath13 , the widely used methods for solving are proximal methods @xcite and admm @xcite .",
    "the convergence of admm to the optimum solution is guaranteed when the functions appearing in the objective function are convex @xcite .",
    "the following theorem states that admm can be used to solve with guaranteed convergence , provided the augmented lagrangian parameter @xmath16 is appropriately set .",
    "such a condition on @xmath16 was also given in @xcite .",
    "note that @xmath16 does not affect the solution to which admm converges , rather the speed at which it converges .",
    ".iterative algorithm for the solution to .",
    "[ cols= \" < \" , ]     [ table::algorithm ]    let @xmath12 satisfy assumption [ theorem::assumption 1 ] and the transform @xmath56 satisfy the parseval frame condition .",
    "let @xmath85 .",
    "the iterative algorithm [ table::algorithm ] converges to the global minimum of the function @xmath14 in if @xmath86    we re - write the problem using variable splitting @xcite as    [ eq::variable split ] @xmath87    the minimization is separable in @xmath88 and @xmath89 .",
    "applying admm to yields the following iterative procedure with the augmented lagrangian parameter @xmath16 .",
    "[ eq::iterative procedure ] @xmath90    the sub - problem for @xmath88 can be solved explicitly as @xmath91using .",
    "the sub - problem for @xmath92 can be solved using @xmath93 , provided the function @xmath94 is convex .",
    "consider the function @xmath95 defined as @xmath96 from lemma [ theorem::lemma 1 ] and the proof of theorem [ theorem::theorem 1 ] , @xmath97 is positive definite if @xmath98 since @xmath99 , it follows that @xmath100 is positive definite if @xmath17 .",
    "hence @xmath101 is strictly convex for @xmath17 .",
    "note that @xmath102 .",
    "hence , the function @xmath94 , being the sum of a convex and a strictly convex function , is strictly convex . as",
    "such , the minimization problem in is well - defined and its solution can be efficiently computed using the proximity operator of @xmath12 , i.e. , @xmath103_i ; \\lambda_i/\\mu_i , a_i \\bigr).\\end{aligned}\\ ] ]    since @xmath56 has full column rank , admm converges to a stationary point of the objective function ( despite having a non - convex function in the objective ) @xcite ; see also @xcite .",
    "moreover , the function @xmath14 is strictly convex ( by theorem [ theorem::theorem 1 ] ) and the sub - problems of the admm are strictly convex for @xmath17 . as a result , the iterative procedure converges to the global minimum of @xmath14 .    a globally convergent algorithm based on a different splitting",
    "is presented in @xcite . in that approach , the objective function",
    "is split into two functions , both of which are convex regardless of the auxillary parameter value . hence , no parameter constraint is required to ensure convergence .",
    "we consider the problem of denoising a 1d signal that is sparse with respect to the undecimated wavelet transform ( udwt ) @xcite , which satisfies the condition with @xmath104 .",
    "in particular , we use a 4-scale udwt with three vanishing moments .",
    "the noisy signal is generated using wavelab ( http://www-stat.stanford.edu/%7ewavelab/ ) with awgn of @xmath105 .",
    "we set the regularization parameters @xmath106 .",
    "we use the same @xmath107 for all the coefficients in scale @xmath108 .",
    "the value of @xmath109 is chosen to obtain the lowest rmse for convex and non - convex regularization respectively . to maximally",
    "induce sparsity we set @xmath110 . for the 1d signal denoising example",
    ", we use the non - convex arctangent penalty and its corresponding threshold function @xcite . for comparison",
    "we use reweighted @xmath0 minimization @xcite , with @xmath109 chosen in order to obtain the lowest rmse .         for the 1d signal denoising example . ]",
    "figure  [ fig::1d example ] shows that the denoised signal obtained using non - convex regularization has the lowest rmse and preserves the discontinuities .",
    "further , the peaks are less attenuated using non - convex regularization in comparison with @xmath0 norm regularization .    for further comparison",
    ", we generate the noisy signal in fig .",
    "[ fig::1d example ] for @xmath111 , and denoise it with non - convex and convex regularization .",
    "we also denoise the noisy signal by direct non - linear thresholding of the noisy wavelet coefficients and by reweighted @xmath0 minimization .",
    "we use the same @xmath109 values as in fig .",
    "[ fig::1d example ] .",
    "the value of @xmath109 for direct non - linear thresholding is also chosen to obtain the lowest rmse . as seen in fig .  [ fig::rmse comparison ] , the non - convex regularization outperforms the three methods by giving the lowest rmse .",
    "the rmse values are obtained by averaging over 15 realizations for each @xmath112 .",
    "we consider the problem of denoising a 2d image corrupted with awgn .",
    "we use the 2d dual - tree complex wavelet transform ( dt - cwt ) @xcite , which is 4-times expansive and satisfies with @xmath113 .",
    "the noisy ` peppers ' image has peak signal - to - noise ratio ( psnr ) value of 14.6 db .",
    "we use the same @xmath37 for all the sub - bands . as in the previous example",
    ", we set the value of @xmath37 for each case ( convex and non - convex ) as a constant multiple of @xmath112 that gives the highest psnr .",
    "norm regularization . ]    .",
    "( b ) psnr as a function of @xmath112 . ]",
    "figure  [ fig::2d example ] shows that the denoised image ( non - convex case ) contains fewer wavelet artifacts and has a higher psnr .",
    "[ fig::psnr values](a ) shows the psnr values ( convex and non - convex ) for different values of @xmath37 . to further assess the performance of tight - frame non - convex regularization",
    ", we realize several noisy ` peppers ' images with @xmath114 . as in the case of the 1d signal denoising , fig .",
    "[ fig::psnr values ] shows that non - convex regularization offers higher psnr across different noise - levels .",
    "this letter considers the problem of signal denoising using a sparse tight - frame analysis prior .",
    "we propose the use of parameterized non - convex regularizers to maximally induce sparsity while maintaining the convexity of the total problem .",
    "the convexity of the objective function is ensured by restricting the parameter @xmath15 of the non - convex regularizer .",
    "we use admm to obtain the solution to the convex objective function ( consisting of a non - convex regularizer ) , and guarantee its convergence to the global optimum , provided the augmented lagrangian parameter @xmath16 , satisfies @xmath17 .",
    "the proposed method outperforms the @xmath0 norm regularization and reweighted @xmath0 minimization methods for signal denoising .",
    "s.  boyd , n.  parikh , e.  chu , b.  peleato , and j.  eckstein , `` distributed optimization and statistical learning via the alternating direction method of multipliers , '' _ found .",
    "trends mach .",
    "_ , vol .  3 , no .  1 ,",
    "pp . 1122 , 2010 .",
    "p.  l. combettes and j .- c .",
    "pesquet , `` proximal splitting methods in signal processing , '' in h.  h bauschke et al . , editors , _ fixed - point algorithms for inverse problems in science and engineering _ , springer - verlag , pp . 185212 , 2011 .",
    "j.  eckstein and d.  p. bertsekas , `` on the douglas - rachford splitting method and the proximal point algorithm for maximal monotone operators , '' _ math",
    ". program .",
    "_ , vol .",
    "55 , no . 1 - 3 , pp . 293318 , apr . 1992 .",
    "m.  hong , z .- q .",
    "lo , and m.  razaviyayn , `` convergence analysis of alternating direction method of multipliers for a family of nonconvex problems , '' _ proc .",
    "speech signal process .",
    "( icassp ) _ , pp . 15 , 2015 .",
    "v.  jojic , s.  saria , and d.  koller , `` convex envelopes of complexity controlling penalties : the case against premature envelopment , '' _ proc .",
    "( aistats ) _ , vol .",
    "15 , pp . 399406 , 2011 .",
    "a.  lanza , s.  morigi , and f.  sgallari , `` convex image denoising via non - convex regularization , '' in j .-",
    "aujol , m.  nikolova , and n.  papadakis , editors , _ scale space and variational methods in computer vision _ , ser .",
    "lecture notes in computer science , springer , vol .",
    "9087 , pp .",
    "666677 , 2015 .",
    "s.  magnusson , p.  c. weeraddana , m.  g. rabbat , and c.  fischione , `` on the convergence of alternating direction lagrangian methods for nonconvex structured optimization problems , '' _",
    "preprint _ , pp .",
    "113 , 2014 .",
    "a.  majumdar and r.  k. ward , `` non - convex row - sparse multiple measurement vector analysis prior formulation for eeg signal reconstruction , '' _ biomed . signal process .",
    "control _ , vol .  13 , pp . 142147 , sep .",
    "2014 .",
    "m.  nikolova , m.  k. ng , and c.  p. tam , `` fast nonconvex nonsmooth minimization methods for image restoration and reconstruction , '' _ ieee trans",
    ". image process .",
    "_ , vol .  19 , no .  12 , pp .",
    "30733088 , dec . 2010 .",
    "p.  ochs , a.  dosovitskiy , t.  brox , and t.  pock , `` on iteratively reweighted algorithms for nonsmooth nonconvex optimization in computer vision , '' _ siam j. imaging sci .",
    "_ , vol .  8 , no .  1 , pp .",
    "331372 , 2015 ."
  ],
  "abstract_text": [
    "<S> this letter considers the problem of signal denoising using a sparse tight - frame analysis prior . </S>",
    "<S> the @xmath0 norm has been extensively used as a regularizer to promote sparsity ; however , it tends to under - estimate non - zero values of the underlying signal . </S>",
    "<S> to more accurately estimate non - zero values , we propose the use of a non - convex regularizer , chosen so as to ensure convexity of the objective function . </S>",
    "<S> the convexity of the objective function is ensured by constraining the parameter of the non - convex penalty . </S>",
    "<S> we use admm to obtain a solution and show how to guarantee that admm converges to the global optimum of the objective function . </S>",
    "<S> we illustrate the proposed method for 1d and 2d signal denoising . </S>"
  ]
}