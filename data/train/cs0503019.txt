{
  "article_text": [
    "this paper addresses the computation of a function that is key to the evaluation of both the random coding and sphere packing error exponents .",
    "this function , often denoted @xmath0 , is usually expressed as a maximization problem over input distributions .",
    "consequently , it is conceptually easily bounded from below : any feasible input distribution gives rise to such a bound . in this paper",
    "we propose to use a dual expression for @xmath0  an expression that involves a minimization over output distributions  in order to derive _ upper _ bounds on @xmath0 .",
    "we shall demonstrate this approach by studying the cutoff rate of non - coherent ricean fading channels . to that end",
    "we shall have to study the appropriate modifications to the function @xmath0 that are needed to account for input constraints and when the channel input and output alphabets are infinite .",
    "it should be noted that the dual expression we propose to use is not new @xcite , ( * ? ? ?",
    "* ex .  23 in ch . 2.5 ) .",
    "we merely extend it here to input constrained channels over infinite alphabets and demonstrate how it can be used to derive _ analytic _ upper bounds on the random coding and sphere packing error exponents .",
    "for _ numerical _ procedures ( for unconstrained finite alphabet channels ) see @xcite .",
    "the rest of this introductory section is dedicated to the introduction of the function @xmath0 for discrete memoryless channels .",
    "we first treat unconstrained channel and then introduce the modifications that are needed to account for input constraints .",
    "we describe both the `` method of types '' approach and gallager s approach .",
    "we pay special attention to the modification that gallager introduced to account for cost constraints and to the duality between the expressions derived using the two approaches .",
    "this introduction is somewhat lengthy because , while the results are not new , we had difficulty pointing to a publication that introduces the two approaches side by side and that compares the two in the presence of cost constraints .    in section  [ sec :",
    "continuous ] we extend the discussion to infinite alphabets and prove the basic inequality on which our approach to upper bounding @xmath0 is based ; see proposition  [ prop : upper ] . in section  [ sec : ricean ]",
    "we introduce the discrete - time memoryless ricean fading channel with and without full or partial side information at the receiver , and we describe our asymptotic results on this channel s cutoff rate .",
    "these asymptotic results are derived using duality in section  [ sec : derivation ] , which concludes the paper .      to motivate the interest in the function @xmath0 we shall begin by addressing the case where there are no input constraints .",
    "the reliability function @xmath1 corresponding to rate-@xmath2 unconstrained communication over a discrete memoryless channel ( dmc ) of capacity @xmath3 is the best exponential decay in the blocklength @xmath4 of the average probability of error that one can achieve using rate-@xmath2 blocklength-@xmath4 codebooks .",
    "that is , @xmath5 where @xmath6 denotes the average probability of error of the best rate-@xmath2 blocklength-@xmath4 codebook for the given channel .",
    "the problem of computing the reliability function of a general dmc over the finite input and output alphabets @xmath7 and @xmath8 and of a general law @xmath9 is still open .",
    "various upper and lower bounds are , however , known . to derive lower bounds on the reliability function one must derive upper bounds on the probability of error of the best rate-@xmath2 blocklength-@xmath4 code .",
    "this is typically done by demonstrating the existence of good codes for which the average probability of error is small .",
    "one such lower bound on @xmath1 is the random coding lower bound @xcite . by considering an ensemble of codebooks",
    "whose codewords are chosen independently , each according to a product distribution of marginal law @xmath10 , gallager derived the lower bound @xmath11 where @xmath12 and @xmath13 since the law @xmath10 from which the ensemble of codebooks is constructed is arbitrary , gallager obtained the bound @xmath14 where @xmath15 is gallager s random coding error exponent @xmath16    a different random coding lower bound on the reliability function can be derived using the ensemble of codebooks where the codewords are still chosen independently , but rather than according to a product distribution , each is now chosen uniformly over a type class @xcite , @xcite , @xcite . with this approach one obtains @xcite , @xcite the lower bound @xmath17 where @xmath18 here the minimization is over all conditional laws @xmath19 @xmath20 the term @xmath21 denotes the mutual information corresponding to the channel @xmath22 and the input distribution @xmath10 ; and @xmath23 stands for @xmath24",
    ". again , since the type @xmath10 according to which the ensemble is generated is arbitrary , one obtains @xmath25 where @xmath26    there is an alternative form for @xmath27 that will be of interest to us @xcite , ( * ? ? ?",
    "* ex .  23 in ch .",
    "this form is more similar to : @xmath28 where @xmath29 and where the minimization in the latter is over the set of all distributions @xmath30 on the output alphabet @xmath8 .    in general , for any dmc @xmath9 and any input distribution @xmath10 @xcite , ( * ? ?",
    "* ex .  23 in ch .",
    "2.5 ) @xmath31 and hence @xmath32 with the inequalities typically being strict .",
    "these inequalities are a consequence of the fact that the `` average constant composition code '' performs better than the `` average independent and identically distributed code '' @xcite . however ,",
    "when optimized over the input distributions , the inequalities turn into equalities @xcite , @xcite , ( * ? ? ?",
    "* ex .  23 in ch .",
    "2.5 ) @xmath33 and @xmath34 i.e. , @xmath35 in fact , as shown in appendix  [ app : lagrange ] , the optimization problems appearing on the lhs and on the rhs of are lagrange duals .",
    "consequently , we shall henceforth denote @xmath36 ( @xmath37 ) by @xmath0 and refer to @xmath15 ( @xmath38 ) as the random coding error exponent and denote it by @xmath39 . in terms of the function",
    "@xmath40 the random coding error exponent @xmath39 is thus given by @xmath41    the cut - off rate @xmath42 is defined by @xmath43    the function @xmath0 also plays an important role in the study of upper bounds to the reliability function .",
    "in fact , the sphere packing error exponent @xmath44 is given by @xcite @xmath45    combining with and we obtain the two equivalent expressions for @xmath0 @xmath46 @xmath47 we refer to the former expression as the `` primal '' expression and to the latter as the `` dual '' expression .",
    "the primal expression is useful for the derivation of lower bounds on @xmath0 .",
    "indeed , any distribution @xmath10 on the input alphabet @xmath48 induces the lower bound @xmath49 on the other hand , the dual expression is useful for the derivation of upper bounds .",
    "any distribution @xmath30 on the output alphabet @xmath50 yields the upper bound @xmath51      before we can use the above bounds for fading channels we need to extend the discussion to cost constrained channels and to channels over infinite input and output alphabets where the method of types can not be directly used .",
    "for now we continue our assumption of finite alphabets and address the cost constraint .",
    "suppose we limit ourselves to blockcode transmissions where we only allow codewords @xmath52 that satisfy @xmath53 where @xmath54 is a cost function on the input alphabet @xmath48 , @xmath55 is some pre - specified non - negative number , and @xmath4 , as before , is the blocklength .",
    "the reliability function @xmath1 is defined as in with the modification that @xmath6 should be now understood as the lowest average probability of error that can be achieved using a rate-@xmath2 blocklength-@xmath4 codebook all of whose codewords satisfy the cost constraint .    to obtain lower bounds on @xmath1 gallager @xcite ,",
    "@xcite modified his random coding argument in two ways .",
    "he introduced a new ensemble of codebooks and introduced an improved technique to analyze the average probability of error over this ensemble . for any probability law @xmath10 on the input alphabet satisfying @xmath56 } \\leq \\upsilon\\ ] ] where @xmath57 } \\triangleq \\sum_{x \\in { \\mathcal{x } } } { \\mathsf{q}}(x ) g(x)\\ ] ] define @xmath58 } <      \\upsilon$ } \\\\",
    "{ \\displaystyle      \\max_{r \\geq 0 } e_{0}(\\varrho , { \\mathsf{q } } , r )      } & \\text{if      $ { \\textnormal{\\textsf{e}}_{{\\mathsf{q}}}\\!\\left[{g(x)}\\right ] } = \\upsilon$ }     \\end{cases}\\ ] ] where @xmath59 note that @xmath60 and hence @xmath61 thus , gallager s `` modification '' can only tighten the bound .",
    "gallager then showed that for any @xmath62 the exponent @xmath63 is achievable using block codes that satisfy the constraint .",
    "( to prove this result when @xmath64 } < \\upsilon$ ] he considered an ensemble of codebooks where the codewords are chosen independently of each other , each according to the a - posteriori law of a sequence @xmath65 drawn iid according to @xmath10 conditional on @xmath66 .",
    "to prove the result when @xmath64 } = \\upsilon$ ] he considered an ensemble similarly constructed but with the distribution being conditional on @xmath67 . )",
    "consequently the error exponent @xmath68 where @xmath69 } \\leq \\upsilon } { e_{\\textnormal{g,0}}^{\\textnormal{m}}}(\\varrho , { \\mathsf{q}})\\ ] ] is achievable .",
    "it is instructive to distinguish between two types of constraints .",
    "we say that the cost constraint is _ inactive _ if there exists some input distribution @xmath70 satisfying the constraint that achieves the global unconstrained maximum of @xmath71 .",
    "that is , @xmath72 } \\leq \\upsilon   \\quad \\text{and } \\quad    { e_{\\textnormal{g},0}}(\\varrho , { \\mathsf{q}}^ { * } ) =   \\max_{{\\mathsf{q } } } { e_{\\textnormal{g},0}}(\\varrho , { \\mathsf{q}})\\ ] ] or equivalently @xmath73 } \\leq \\upsilon } { e_{\\textnormal{g},0}}(\\varrho ,    { \\mathsf{q } } ) =    \\max_{{\\mathsf{q } } } { e_{\\textnormal{g},0}}(\\varrho , { \\mathsf{q}}).\\ ] ] otherwise , we say that the cost constraint is _",
    "active_. with these definitions it can be shown that simplifies to @xmath74 } = \\upsilon } \\ ;         \\max_{r \\geq 0 } e_0(\\varrho,{\\mathsf{q}},r ) } & \\text{cost active } \\\\    { \\displaystyle \\max_{{\\mathsf{q } } } { e_{\\textnormal{g},0}}(\\varrho , { \\mathsf{q } } ) }       & \\text{cost inactive }      \\end{cases}.\\ ] ] ( the case where the cost constraint is active follows from gallager s observation that when the cost constraint is active , the maximum of @xmath75 over all @xmath76 and over all laws @xmath10 satisfying is achieved by an input distribution @xmath77 satisfying the constraint with equality .",
    "the case where the cost constraint is inactive follows by noting that by starting from we have for inactive cost constraints @xmath78 } \\leq \\upsilon } { e_{\\textnormal{g,0}}^{\\textnormal{m}}}(\\varrho , { \\mathsf{q } } ) & \\geq   \\max_{{\\mathsf{q } } :    { \\textnormal{\\textsf{e}}_{{\\mathsf{q}}}\\!\\left[{g(x)}\\right ] } \\leq \\upsilon } { e_{\\textnormal{g},0}}(\\varrho , { \\mathsf{q } } ) \\\\ & = \\max_{{\\mathsf{q } } } { e_{\\textnormal{g},0}}(\\varrho , { \\mathsf{q } } ) \\\\ & = \\max_{{\\mathsf{q } } } { e_{\\textnormal{ck},0}}(\\varrho , { \\mathsf{q } } ) \\\\ & \\geq \\max_{{\\mathsf{q } } : { \\textnormal{\\textsf{e}}_{{\\mathsf{q}}}\\!\\left[{g(x)}\\right ] } \\leq \\upsilon }    { e_{\\textnormal{g,0}}^{\\textnormal{m}}}(\\varrho , { \\mathsf{q } } ) \\end{aligned}\\ ] ] so that all inequalities must hold with equalities . here",
    "the first inequality follows from ; the subsequent equality because the cost constraint is assumed inactive ; the subsequent equality from ; and the final inequality from ahead . )    an achievable error exponent can also be demonstrated using constant composition codes .",
    "this yields that the error exponent @xmath79 is achievable where @xmath80 } \\leq \\upsilon } { e_{\\textnormal{ck},0}}(\\varrho , { \\mathsf{q}}).\\ ] ]    the relation not withstanding , it can be shown that for any law @xmath10 satisfying and any @xmath81 @xmath82 with the inequality being , in general , strict . }",
    "< \\upsilon$ ] this follows directly from . for a proof in the case @xmath64 } =    \\upsilon$ ] see proposition  [ prop : upper ] ahead , which proves that the rhs of is greater or equal @xmath83 . ] consequently , by and we have @xmath84 .",
    "however , as shown in appendix  [ app : equal_rc ] this holds with equality @xmath85 thus , denoting the two identical functions @xmath86 and @xmath87 by @xmath88 and the two identical functions @xmath89 and",
    "@xmath90 by @xmath91 we have @xmath92 where @xmath93 can be expressed either by ( [ eq : max_simple ] ) as @xmath94 } = \\upsilon } \\ ; \\max_{r \\geq 0 }      e_0(\\varrho,{\\mathsf{q}},r ) } & \\text{cost active } \\\\      { \\displaystyle \\max_{{\\mathsf{q } } } { e_{\\textnormal{g},0}}(\\varrho , { \\mathsf{q } } ) }       & \\text{cost inactive }      \\end{cases}\\ ] ] or , using , as @xmath95 } \\leq      \\upsilon }     \\min_{{\\mathsf{r } } } \\left\\ {       -(1+\\varrho ) \\sum_{x \\in \\x } { \\mathsf{q}}(x ) \\log \\left ( \\sum_{y \\in \\y }        { \\mathsf{w}}(y|x)^{\\frac{1}{1+\\varrho } }        { \\mathsf{r}}(y)^{\\frac{\\varrho}{1+\\varrho } } \\right )                                     \\right\\}.\\end{gathered}\\ ] ] the former , to which we refer as the `` primal '' expression , is useful for the derivation of lower bounds on @xmath93 whereas the latter , the `` dual '' , is useful for upper bounds .",
    "we next extend the discussion to channels over infinite input and output alphabets . consider a channel @xmath96 whose inputs and outputs take value in the separable metric spaces @xmath7 and @xmath8 respectively .",
    "thus for any input @xmath97 and any borel set @xmath98 the probability that in response to the input @xmath99 the channel will produce an output @xmath100 that lies in the set @xmath101 is @xmath102 .",
    "we assume that the mapping @xmath103 from @xmath48 to the interval @xmath104 $ ] is borel measurable .",
    "finally assume the existence of an underlying positive measure @xmath105 on @xmath8 with respect to which all the probability measures @xmath106 are absolutely continuous .",
    "denote the radon - nykodim derivative of @xmath107 with respect to @xmath108 by @xmath109 thus , @xmath110 is the density at @xmath111 of the channel output corresponding to the input @xmath97 . for any input @xmath112 and any borel set @xmath113 @xmath114    as to the cost",
    ", we shall assume that the function @xmath115 is measurable and consider block codes that satisfy .",
    "we extend the definition to infinite alphabets as @xmath116 } \\triangleq \\int_{\\x } g(x ) { \\,\\textnormal{d}}{\\mathsf{q}}(x).\\ ] ]    definition   is extended for any probability law @xmath10 on @xmath7 as @xmath117    for any input distribution @xmath10 satisfying the constraint @xmath64 } \\leq \\upsilon$ ] we extend as follows : @xmath118 } = \\upsilon$ and $ { \\textnormal{\\textsf{e}}_{{\\mathsf{q}}}\\!\\left[{g^{3}(x)}\\right ] } <    \\infty$ } \\\\ { \\displaystyle    \\bigl . e_{0}(\\varrho , { \\mathsf{q } } , r ) \\bigr|_{r=0 } } &   \\text{otherwise } \\end{cases}.\\ ] ] ( note that following gallager @xcite , @xcite we allow for the optimization over @xmath119 only when under the law @xmath10 the random variable @xmath120 has a finite third moment . )    with this definition we can now define @xmath121 } \\leq    \\upsilon } { e_{\\textnormal{g,0}}^{\\textnormal{m}}}(\\varrho , { \\mathsf{q}})\\ ] ] and the cut - off rate as @xmath122 the random coding error exponent @xmath123 is achievable with block codes satisfying the constraint @xcite , @xcite .",
    "the following proposition proves in the more general case where the alphabets may be continuous .",
    "it is particularly useful for the derivation of upper bounds on @xmath124 .",
    "[ prop : upper ] consider as above a discrete - time memoryless infinite alphabet channel @xmath110 , an output measure @xmath108 , a measurable cost function @xmath125 , and some arbitrary allowed cost @xmath55 .",
    "let @xmath126 be an arbitrary density with respect to @xmath108 on the output alphabet @xmath8 .",
    "then for any distribution @xmath10 on @xmath7 satisfying the cost constraint @xmath64 } \\leq \\upsilon$ ] @xmath127    distinguish between the case where @xmath64 } < \\upsilon$ ] and the case where @xmath64 } = \\upsilon$ ] and @xmath128 } < \\infty$ ] . in the former case , by , @xmath129 and the result follows by an application of jensen s inequality and hlder s inequality : @xmath130    as for the case where @xmath64 } = \\upsilon$ ] ( and @xmath128 } < \\infty$ ] ) we have for any @xmath76 @xmath131 } - \\upsilon \\right )   \\nonumber \\\\    & & -\\ : ( 1+\\varrho)\\int_{x \\in \\x}\\log     \\left ( \\int_{y \\in \\y } e^{r(g(x)-\\upsilon ) }          w(y|x)^\\frac{1}{1+\\varrho }          f_{r}(y)^\\frac{\\varrho}{1+\\varrho } { \\,\\textnormal{d}}\\mu(y ) \\right){\\,\\textnormal{d}}{\\mathsf{q}}(x )           \\nonumber \\\\     & = & -(1+\\varrho ) \\int_{x \\in \\x } \\log \\left(\\int_{y \\in \\y }       e^{r(g(x)-\\upsilon ) } w(y|x)^\\frac{1}{1+\\varrho }       f_{r}(y)^\\frac{\\varrho}{1+\\varrho } { \\,\\textnormal{d}}\\mu(y ) \\right){\\,\\textnormal{d}}{\\mathsf{q}}(x )     \\nonumber \\\\     & \\geq & -(1+\\varrho ) \\log \\int_{x \\in \\x } \\int_{y \\in \\y }     e^{r(g(x)-\\upsilon ) } w(y|x)^\\frac{1}{1+\\varrho }     f_{r}(y)^\\frac{\\varrho}{1+\\varrho } { \\,\\textnormal{d}}\\mu(y){\\,\\textnormal{d}}{\\mathsf{q}}(x )     \\nonumber\\\\     & \\geq &   - \\log \\int_{y \\in \\y } \\left ( \\int_{x \\in \\x }       e^{r(g(x ) -    \\upsilon ) } w(y|x)^\\frac{1}{1+\\varrho } { \\,\\textnormal{d}}{\\mathsf{q}}(x ) \\right)^{1+\\varrho } { \\,\\textnormal{d}}\\mu(y ) \\nonumber \\\\    & = & e_{0}(\\varrho , { \\mathsf{q } } , r).\\end{aligned}\\ ] ] where the second equality follows because in the case we are considering now @xmath64 } = \\upsilon$ ] ; the first inequality by jensen s inequality , and the subsequent by hlder s inequality .",
    "the result for this case now follows because @xmath76 in the above is arbitrary .    to conclude , to derive lower bounds on @xmath93 we can choose any input distribution @xmath10 satisfying the constraint @xmath64 } \\leq \\upsilon$ ] to obtain the lower bound : @xmath132 where @xmath133 is defined in .",
    "to derive upper bounds on @xmath93 we can use the above proposition by choosing some arbitrary output density @xmath134 to obtain @xmath135 } \\leq    \\upsilon}\\left\\ { -(1+\\varrho ) \\int_{x",
    "\\in \\x } \\log   \\left ( \\int_{y \\in \\y } w(y|x)^{\\frac{1}{1+\\varrho } }      f_{r}(y)^{\\frac{\\varrho}{1+\\varrho } }        { \\,\\textnormal{d}}\\mu(y ) \\right ) { \\,\\textnormal{d}}{\\mathsf{q}}(x)\\right\\}.\\end{gathered}\\ ] ]",
    "the discrete - time memoryless ricean fading channel with partial receiver side information is a channel whose input @xmath99 takes value in the complex field @xmath136 and whose corresponding output constitutes of a pair of complex random variables @xmath100 and @xmath137 .",
    "we shall refer to @xmath100 as `` the received signal '' and to @xmath137 as the `` side information ( at the receiver ) '' .",
    "the joint distribution of @xmath138 corresponding to the input @xmath139 is best described using the fading complex random variable @xmath140 and the additive noise complex random variable @xmath141 .",
    "the joint distribution of @xmath140 , @xmath137 , and @xmath141 does not depend on the input @xmath99 .",
    "the additive noise @xmath141 is independent of the pair @xmath142 and has a circularly symmetric complex gaussian distribution of positive variance @xmath143 .",
    "the fading @xmath140 is of mean @xmath144  the `` specular component ''  and it is assumed that @xmath145 is a unit - variance circularly symmetric complex gaussian random variable . is real and non - negative .",
    "the more general complex case can be treated by rotating the output . ]",
    "the pair @xmath137 and @xmath146 are jointly circularly symmetric gaussian random variables .",
    "we denote the conditional variance of @xmath140 given @xmath137 by @xmath147 .",
    "the received signal @xmath100 corresponding to the input @xmath139 is given by @xmath148    the case where @xmath149 corresponds to the case where @xmath140 and @xmath137 are independent , in which case the receiver can discard @xmath137 without loss in information rates .",
    "this case corresponds to `` non - coherent '' fading . in the case",
    "@xmath150 the receiver can precisely determine the realization of @xmath140 from @xmath137 .",
    "this corresponds to `` coherent detection '' .",
    "finally , the case @xmath151 corresponds to `` partially coherent '' communication . in this case",
    "@xmath137 carries some information about @xmath140 , but it does not fully determine @xmath140 . in this paper we shall only consider the case where @xmath152 .",
    "the case @xmath150 is much easier to analyze and has already received considerable attention in the literature .",
    "see for example , @xcite , @xcite , @xcite and the references in the latter .",
    "the special case of ricean fading with zero specular component @xmath153 is called `` rayleigh fading '' . the non - coherent ( @xmath149 ) capacity of this channel",
    "was studied in @xcite , @xcite and @xcite . the coherent case ( @xmath150 )",
    "was studied in @xcite .",
    "the capacity of the non - coherent ricean channel ( @xmath149 and @xmath154 ) was studied in @xcite-@xcite and @xcite .    unless some restrictions are imposed on the input @xmath99 , the capacity and cut - off rate of this channel are infinite .",
    "two kinds of restrictions are typically considered .",
    "the first corresponds to an average power constraint . here",
    "only blockcodes where each codeword satisfies with @xmath155 are allowed . in this context rather than denoting the allowed cost by @xmath55 we shall use the more common symbol @xmath156 , which stands here for the average energy per symbol .",
    "that is , we only allow blocklength-@xmath4 codes in which every codeword @xmath157 satisfies @xmath158    the second type of constraint is a peak power constraint . here",
    "we only allow channel inputs that satisfy @xmath159 where @xmath156 now stands for the allowed peak power .",
    "such a constraint is best treated by considering the channel as being free of constraints but with the input alphabet now being @xmath160    any codebook satisfying the peak power constraint also satisfies the average power constraint hence the capacity and reliability function under the peak constraint can not exceed those under the average constraint .    irrespective of whether an average power or a peak power constraint is imposed , at high snr the capacity @xmath161 of this channel is given asymptotically as @xmath162 where the correction term @xmath163 depends on the snr and tends to zero as the snr tends to infinity . here",
    "@xmath164 denotes the exponential integral function @xmath165 and we define the value of the function @xmath166 at @xmath167 as @xmath168 , where @xmath169 denotes euler s constant .",
    "( with this definition the function @xmath170 is continuous from the right at @xmath167 . )    here we shall study the cutoff rate in two cases .",
    "first , in the absence of side information ( @xmath149 ) we will show that irrespective of whether a peak or average power constraint is imposed @xmath171 here @xmath172 denotes the zero - th order modified bessel function of the first kind , which is given by @xmath173 and the @xmath163 term is a correction term that depends on the snr and that approaches zero as the snr tends to infinity .",
    "figure  [ fig : plot1 ] depicts the second order term ( the constant term ) in the high snr expansion of channel capacity and of the cutoff rate as a function of the specular component @xmath153 in the absence of side information .",
    "for a zero specular component the difference between the two second order terms is @xmath174 nats ; for very large specular components ( @xmath175 ) this difference approaches @xmath176 nats .",
    "[ cc][cc]second order terms ( nats ) [ cc][bc]the specular component @xmath177 [ cc][cc]of @xmath178 [ cc][cc]of @xmath179 [ cc][cc]@xmath180    for the case where the side information is present but is not perfect ( @xmath181 ) we only treat the case of zero specular component ( @xmath182 , i.e. , rayleigh fading ) .",
    "we obtain the expansion @xmath183 where @xmath184 is the complete elliptic integral of the first kind : @xmath185    for the case of rayleigh fading with perfect side information ( @xmath150 ) see @xcite . for the case of `` almost perfect side information '' ( @xmath186 ) we note the expansion @xmath187 which follows from the approximation @xcite @xmath188 for some @xmath189    figure  [ fig : plot2 ] depicts the second order terms of channel capacity and the cutoff rate as a function of the estimation error @xmath147 in estimating the fading from the side information for rayleigh fading channels ( @xmath190 ) .",
    "[ fig : plot ] [ cc][cc]second order terms ( nats ) [ cc][bc]the estimation error @xmath191 [ cc][cc]of @xmath178 [ cc][cc]of @xmath179 [ cc][cc]@xmath180",
    "to derive an upper bound on the cut - off rate of the ricean channel in the absence of side information we use proposition  [ prop : upper ] with the density ( w.r.t .",
    "the lebesgue measure @xmath108 on @xmath136 ) @xmath192 here the parameters @xmath193 , @xmath194 , and @xmath195 can be chosen freely in order to obtain the tightest bound , and @xmath196 denotes the incomplete gamma function , @xmath197 ( this family of densities was introduced in @xcite for the purpose of studying the fading number . )    by proposition  [ prop : upper ] applied with @xmath198 we obtain for any law @xmath10 under which @xmath199 } \\leq { { \\mathcal e}}\\ ] ] the upper bound @xmath200 where @xmath201 and from @xmath202{grad_ryz_94}$ ] @xmath203    for our high snr analysis it will suffice to consider ( for sufficiently large powers @xmath156 ) the possibly sub - optimal choice of the parameters @xmath204 and to consider the limiting behavior of the bound as @xmath205 . after taking this limit with @xmath206 held fixed we shall consider the additional limit of @xmath207 .",
    "the analytic computation of @xmath208 is difficult .",
    "note , however , that any lower bound to this quantity will yield an upper bound on @xmath209 .",
    "also , the integral is computable when both @xmath210 and @xmath211 are formally set to zero . be set to zero . ]",
    "we can thus use a limiting argument to study @xmath212 for @xmath213 very small .",
    "indeed , in appendix  [ app : mim ] it is shown that @xmath214 where @xmath215 @xmath216 being some constant .",
    "as we shall see , the term @xmath217 will have a negligible asymptotic contribution to our bound .",
    "the term @xmath218 can be computed analytically @xcite : @xmath219    we thus conclude from ( [ eq : lab_star ] ) , ( [ eq : lab_b ] ) , , and    rcl + & + & ( , ) - ( 2 ) + & + & _ + & + & |d|^2 _ + & + & _ .    the expectations in the above can not be computed without knowledge of the law @xmath10 .",
    "we thus proceed to upper bound the expectations using the average power constraint .",
    "the expectation of the logarithm is upper bounded using jensen s inequality and the power constraint ; the following expectation is upper bounded using the point - wise upper bound @xmath220 , jensen s inequality , and the power constraint ; and the final expectation by noting that the function @xmath221 is monotonically increasing and by noting that @xmath222    we thus conclude that with the allowed average power @xmath156 the cut - off rate satisfies : @xmath223 holding @xmath206 ( small ) and @xmath224 ( large ) fixed , and letting @xmath225 with @xmath226 and @xmath227 as in we obtain from the above and @xmath228 where in computing the limiting difference between the incomplete gamma function and @xmath229 we used ( * ? ? ?",
    "* appendix xi ) .",
    "holding @xmath230 fixed and letting @xmath231 we obtain @xmath232 letting now @xmath230 tend to infinity we obtain the desired asymptotic upper bound @xmath233      any input distribution satisfying the cost constraint ( possibly strictly ) induces a lower bound on the cut - off rate . indeed , for any input distribution @xmath234 satisfying the cost constraint @xmath235 where the first inequality follows by the definition of the cut - off rate ( and holds with equality if @xmath234 achieves the cut - off rate ) and where the second inequality follows from ( and holds with equality if @xmath234 satisfies the cost constraint with strict inequality ) .    we thus proceed to lower bound @xmath236 for a law @xmath234 of our choice . under this law",
    ", @xmath237 is a circularly symmetric random variable with @xmath238 the motivation for using this law is that it is known to achieve the asymptotic capacity @xcite .",
    "moreover , this law also satisfies the peak power constraint @xmath239 , so that the lower bound on the cut - off rate we compute will also be valid as a lower bound for the cut - off rate under a peak constraint . finally , as the next proposition shows , the fact that under @xmath234 the input @xmath237 satisfies , with probability one , @xmath240 , where @xmath241 greatly simplifies our analysis .",
    "it allows us to asymptotically ignore the additive noise .",
    "[ prop : devdeal ] let @xmath242 denote the function @xmath243 evaluated at @xmath244 for the input law @xmath10 to the ricean channel of specular component @xmath153 and additive noise variance @xmath143 .",
    "let @xmath245 be similarly defined for the ricean channel with the same specular component but without any additive noise .",
    "if under the law @xmath10 the input @xmath246 satisfies with probability one @xmath247 for some @xmath248 then @xmath249    for any input probability distribution @xmath10 , the term @xmath250 can be expressed @xmath251 where @xmath252 and where for the ricean fading channel with additive noise of variance @xmath253 @xmath254 comparing @xmath255 with the corresponding term in the absence of noise @xmath256 we obtain    rcl + & & b(x , x;0 ) e^|d|^2 ^2   + & & b(x , x;0 ) e^|d|^2 ^ 2 [ eq : simon1 ]    where the last inequality follows by the triangle inequality .",
    "it thus follows from and that if under the law @xmath10 the random variable @xmath237 satisfies with probability one @xmath240 then @xmath257    using this proposition with the law @xmath234 under which @xmath237 is distributed according to we obtain that @xmath258 computing @xmath259 from and we obtain @xmath260 the last term on the rhs of the above is difficult to evaluate precisely . however , since the integrand is positive , the double integral can be upper bounded by inflating the region of integration to the region @xmath261 the integral over this larger set can be now computed analytically by changing to polar coordinates to obtain @xmath262 where we have used the identity @xmath263 which follows from @xcite . consequently , by and @xmath264 so that by @xmath265      we next consider the case where the fading @xmath140 is of zero - mean ( rayleigh ) and where the receiver has access to some side - information @xmath137 that is jointly gaussian with @xmath140",
    "we assume that the pair @xmath142 is independent of the additive noise @xmath141 and that the joint law of @xmath142 and @xmath141 does not depend on the channel input @xmath139 .",
    "we denote the conditional mean of @xmath140 given @xmath266 by @xmath267}\\ ] ] and the estimation error by @xmath268}.\\ ] ] note that unconditionally , @xmath269 is a zero - mean circularly - symmetric gaussian random variable of variance @xmath270 : @xmath271 recall also that we only treat here the case @xmath152 . denoting the conditional density of @xmath272 corresponding to the input @xmath273 by @xmath274 , we have by the independence of the side information @xmath137 and the input that @xmath275 where @xmath276 is the density of the side information and where @xmath277 is the conditional law of @xmath100 given the input @xmath99 and the side information @xmath278 .",
    "note that , because @xmath142 are jointly gaussian , the density @xmath277 is the gaussian density of mean @xmath279 and variance @xmath280 .",
    "consequently ,    rcl +   + & = & - _ y _ s ( _ x e^r(|x|^2 - e ) d ( x))^2 d(y)d(s ) + & = & - _ s f_s(s ) _ y ( _ x e^r(|x|^2 - e ) d ( x))^2 d(y)d(s ) [ eq : jazz17 ] + & = & - _ s f_s(s ) ( - e_0(1 , , r | s ) ) ds [ eq : jazz18 ]    where follows from and where follows by defining @xmath281 as the @xmath282 function corresponding to the channel @xmath277 for @xmath283 fixed .",
    "( this channel is a ricean fading channel , except that the fading is not normalized to have unit variance . )",
    "the cut - off rate @xmath284 in the presence of the side information @xmath137 can be thus upper bounded by @xmath285 } \\leq { { \\mathcal e } } }     \\sup_{r \\geq 0 } \\left\\ {    - \\log \\int_{s } f_{s}(s ) \\cdot \\exp \\left ( - e_{0}(1 , { \\mathsf{q } } , r | s )    \\right ) { \\,\\textnormal{d}}{s}\\right\\ } \\\\    & \\leq - \\log \\int_{s }",
    "f_{s}(s ) \\cdot \\exp \\left ( - \\sup_{{\\mathsf{q}}:{\\textnormal{\\textsf{e}}_{{\\mathsf{q}}}\\!\\left[{|x|^{2}}\\right ] } \\leq { { \\mathcal e } } }     \\sup_{r \\geq 0 } e_{0}(1 , { \\mathsf{q } } , r | s ) \\right ) { \\,\\textnormal{d}}{s } \\\\    & = - \\log \\int_{s } f_{s}(s ) \\cdot \\exp \\left ( - r_{0}({{\\mathcal e}}|s = s )    \\right ) \\label{eq : lady1}\\end{aligned}\\ ] ] where @xmath286 } \\leq { { \\mathcal e } } }     \\sup_{r \\geq 0 } e_{0}(1 , { \\mathsf{q } } , r | s ) \\\\\\ ] ] is the cut - off rate corresponding to power @xmath156 communication over the channel @xmath277 for fixed @xmath266 . ) since the cost constraint on the cut - off rate is always active for the ricean fading channel . ]",
    "it now follows from that @xmath287 and consequently    rcl + & & _ e \\ { - _ s f_s(s ) ( - ( r_0(e|s = s ) - ) ) ds } + & = & - _ e _ s f_s(s ) ( - ( r_0(e|s = s ) - ) ) ds + & & - _ s f_s(s ) _",
    "e ( - ( r_0(e|s = s ) - ) ) ds + & = & - _ s f_s(s ) ( - _ e \\ { r_0(e|s = s ) - } ) ds + & = & - _ s f_s(s ) ( - + ( 2 ) + 2 0 ( ) ) ds + & = & - ( ) -4 .",
    "[ eq : jazz56 ]    here the swapping of the limit and the expectation ( second inequality ) is justified using fatou s lemma and we use the result @xmath288 which follows from applied to the un - normalized ricean fading channel whose specular component is @xmath269 and whose granular component is of variance @xmath147 .",
    "the evaluation of the last integral is based on an identity combining @xcite and @xcite @xmath289 and the identity for the elliptic function ( * ? ? ?",
    "* eq .  ( 3.2.4 ) ) @xmath290 in view of , to establish it now suffices to show @xmath291 to this end we note that by and evaluated at @xmath292 @xmath293 for any law @xmath234 satisfying @xmath294 } \\leq { { \\mathcal e}}$ ] .",
    "we next choose , as before , @xmath234 to be a law under which @xmath237 is circularly symmetric with @xmath295 whence by proposition  [ prop : devdeal ] and applied to the ricean channel of fading mean @xmath269 and granular component @xmath147 and the tightness of the lower bound @xmath296 for every @xmath278 .",
    "the desired result now follows from and using the dominated convergence theorem and .",
    "in this appendix we prove the following lagrange duality :    [ prop : lagdual ] for any discrete memoryless channel and any @xmath297 , the problem @xmath298    is a lagrange dual of the problem @xmath299 where @xmath10 is a distribution on the input alphabet . in particular , since strong duality holds , @xmath300    consider a discrete memoryless channel @xmath9 with input @xmath301 , @xmath302 and output @xmath303 , @xmath304 .",
    "we henceforth introduce the more standard , for optimization problems , vector notation for functions on discrete domains .",
    "hence , let @xmath305 be a probability distribution on @xmath48 and @xmath306 be a matrix whose ( i , j)-th element is given by @xmath307 hence , ( [ eq : expgal ] ) can be written as : @xmath308 where @xmath309 is an auxiliary vector that we introduce in this problem .",
    "the domain @xmath310 of this optimization problem is @xmath311 . for any @xmath312 the objective function",
    "is convex in @xmath310 .",
    "furthermore , all equality and inequality constraints are affine .",
    "hence , the problem is a _ convex optimization problem_. we will perform a relaxation , which is nevertheless tight for the optimal values of @xmath313 and @xmath314 , to the constraint @xmath315 , namely @xmath316 the lagrangian function of this problem is @xmath317 where @xmath318 , @xmath319 , @xmath309 , @xmath320 and @xmath321 .",
    "since the lagrangian function is affine with respect to @xmath314 , we impose the dual inequality constraint @xmath322 , minimize the lagrangian over @xmath313 and obtain the lagrange dual problem @xmath323 this is a concave problem , with the objective function being monotonic with respect to all the optimization variables .",
    "since we maximize it in a polyhedron , the optimum will be on the boundary , of maximum distance from the hyperplane @xmath324 and of minimum distance from all hyperplanes that define the polyhedron .",
    "therefore , some dual constraint has to be active , i.e. , @xmath325 consequently , the dual problem becomes @xmath326 we perform the transformation of variables @xmath327 , where @xmath328 is chosen to be a probability distribution and @xmath329 is the appropriate normalizing scalar .",
    "optimizing over @xmath210 yields @xmath330 which , because of the fact that @xmath331 is concave with respect to @xmath332 and monotonic with respect to @xmath333 , concludes the proof .",
    "we begin with the case where the cost constraint is active . fix some @xmath81 and let @xmath77 and @xmath334 achieve @xmath335}=\\upsilon } \\max_{r \\geq 0 } e_{0}(\\varrho , { \\mathsf{q } } , r)\\ ] ] so that @xmath336}=\\upsilon } \\max_{r \\geq 0 } e_{0}(\\varrho , { \\mathsf{q } } , r).\\ ] ] following ( * ? ? ?",
    "* eq .  ( 7.3.26 ) ) we define @xmath337 with this definition we have by and @xmath338}=\\upsilon } \\max_{r \\geq 0 } e_{0}(\\varrho , { \\mathsf{q } } , r )       & = e_{0}(\\varrho , { \\mathsf{q } } _ { * } , r _ { * } ) \\nonumber \\\\",
    "\\label{eq : a17 }      & = - \\log \\sum_{y \\in { \\mathcal{y } } } \\alpha^{1+\\varrho}(y ) .",
    "\\end{aligned}\\ ] ] also , by ( * ? ? ?",
    "* eq .  ( 7.3.28 ) ) @xmath339 consider now the distribution @xmath340 on @xmath50 given by @xmath341 we now have by that for any distribution @xmath10 @xmath342 and if @xmath64 } = \\upsilon$ ] then    rcl + & & -(1 + ) _ x ( x ) ( _ y e^r _ * ( g(x ) - ) ( y|x)^ _ * ( y)^ ) + & = & -(1 + ) _ x ( x ) ( _ y e^r _ * ( g(x ) - ) ( y|x)^ ^(y ) ) + _ y ^1+(y ) + & & - _ y ^1+(y ) + & = & _ : _ = _ r 0 e_0 ( , , r ) .    here",
    "the first inequality follows from because the condition @xmath64 } = \\upsilon$ ] guarantees that the introduction of the exponential term @xmath343 has zero net effect ; the subsequent equality by ; the subsequent inequality by ; and the final equality by",
    ". it thus follows upon taking the supremum in the above over all laws @xmath10 satisfying @xmath64 } = \\upsilon$ ] that @xmath344 } = \\upsilon } { e_{\\textnormal{ck},0}}(\\varrho ,    { \\mathsf{q } } ) \\leq \\max_{{\\mathsf{q } } : { \\textnormal{\\textsf{e}}_{{\\mathsf{q}}}\\!\\left[{g(x)}\\right]}=\\upsilon } \\max_{r \\geq 0 } e_{0}(\\varrho , { \\mathsf{q } } , r).\\ ] ] on the other hand , by we obtain @xmath345 } = \\upsilon } { e_{\\textnormal{ck},0}}(\\varrho ,    { \\mathsf{q } } ) \\geq \\max_{{\\mathsf{q } } : { \\textnormal{\\textsf{e}}_{{\\mathsf{q}}}\\!\\left[{g(x)}\\right]}=\\upsilon } \\max_{r \\geq 0 } e_{0}(\\varrho , { \\mathsf{q } } , r)\\ ] ] which combines with to prove the claim for active cost constraints .    for the case of inactive cost constraints we have @xmath346 } \\leq \\upsilon } { e_{\\textnormal{ck},0}}(\\varrho ,    { \\mathsf{q } } ) & \\leq \\max_{{\\mathsf{q } } } { e_{\\textnormal{ck},0}}(\\varrho , { \\mathsf{q } } )   \\\\    & = \\max_{{\\mathsf{q } } } { e_{\\textnormal{g},0}}(\\varrho , { \\mathsf{q } } ) \\\\    & = \\max_{{\\mathsf{q } } : { \\textnormal{\\textsf{e}}_{{\\mathsf{q}}}\\!\\left[{g(x)}\\right ] } \\leq \\upsilon }     { e_{\\textnormal{g,0}}^{\\textnormal{m}}}(\\varrho , { \\mathsf{q}}).\\end{aligned}\\ ] ] here the first inequality follows by relaxing the constraint ; the subsequent equality by ; and the final equality by .",
    "this combines with to conclude the proof .",
    "to derive we begin by noting that for @xmath347 the integrand can be lower bounded by its value when @xmath348 because @xmath349 in the region @xmath350 we can use the inequality @xmath351 combining the above two bounds we obtain that throughout the region of integration @xmath352 and hence @xmath353 we next relate @xmath354 to @xmath355 . to that end",
    "denote the integrand in @xmath356 by @xmath357 we now write the integral as @xmath358 in the region @xmath359 we have @xmath360 and hence @xmath361    we next show that when @xmath362 is small , the integral over the interval @xmath363 $ ] is also small .",
    "indeed , @xmath364 which combines with the monotonicity of @xmath172 and the fact that the argument to the exponential function is negative to demonstrate that @xmath365 and hence that @xmath366 on the other hand a straightforward calculation demonstrates that @xmath367 where the first inequality follows from the monotonicity of @xmath172 and the final inequality follows from simple algebra .",
    "we thus conclude that"
  ],
  "abstract_text": [
    "<S> we propose a technique to derive upper bounds on gallager s cost - constrained random coding exponent function . applying this technique to the non - coherent peak - power or average - power limited discrete time memoryless ricean fading channel , we obtain the high signal - to - noise ratio ( snr ) expansion of this channel s cut - off rate . at high snr the gap between channel capacity and the cut - off rate approaches a finite limit . </S>",
    "<S> this limit is approximately 0.26 nats per channel - use for zero specular component ( rayleigh ) fading and approaches 0.39 nats per channel - use for very large specular components .    </S>",
    "<S> we also compute the asymptotic cut - off rate of a rayleigh fading channel when the receiver has access to some partial side information concerning the fading . </S>",
    "<S> it is demonstrated that the cut - off rate does not utilize the side information as efficiently as capacity , and that the high snr gap between the two increases to infinity as the imperfect side information becomes more and more precise .    </S>",
    "<S> keywords : asymptotic , channel capacity , cut - off rate , fading , high snr , ricean fading . </S>"
  ]
}