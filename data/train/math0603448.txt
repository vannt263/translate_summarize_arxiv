{
  "article_text": [
    "let @xmath4 be a measurable space and @xmath5 be a @xmath6-finite measure on @xmath4 .",
    "let @xmath7 be a sample of @xmath3 i.i.d .",
    "observations drawn from an unknown probability of density @xmath8 on @xmath9 with respect to @xmath5 .",
    "consider the estimation of @xmath8 from @xmath10 .",
    "suppose that we have @xmath11 different estimators @xmath12 of @xmath8 .",
    "@xcite , @xcite , @xcite , @xcite , @xcite , @xcite and @xcite have studied the problem of model selection type aggregation .",
    "it consists in construction of a new estimator @xmath13 ( called _ aggregate _ ) which is approximatively at least as good as the best among @xmath12 . in most of these papers ,",
    "this problem is solved by using a kind of cross - validation procedure .",
    "namely , the aggregation is based on splitting the sample in two independent subsamples @xmath14 and @xmath15 of sizes @xmath16 and @xmath17 respectively , where @xmath18 and @xmath19 .",
    "the size of the first subsample has to be greater than the one of the second because it is used for the true estimation , that is for the construction of the @xmath0 estimators @xmath12 .",
    "the second subsample is used for the adaptation step of the procedure , that is for the construction of an aggregate @xmath20 , which has to mimic , in a certain sense , the behavior of the best among the estimators @xmath21 .",
    "thus , @xmath22 is measurable w.r.t .",
    "the whole sample @xmath23 unlike the first estimators @xmath12 .",
    "one can suggest different aggregation procedures and the question is how to look for an optimal one . a way to define optimality in aggregation in a minimax sense for a regression problem",
    "is suggested in @xcite .",
    "based on the same principle we can define optimality for density aggregation . in this paper",
    "we will not consider the sample splitting and concentrate only on the adaptation step , i.e. on the construction of aggregates ( following @xcite , @xcite , @xcite ) .",
    "thus , the first subsample is fixed and instead of estimators @xmath12 , we have fixed functions @xmath24 . rather than working with a part of the initial sample we will use , for notational simplicity , the whole sample @xmath10 of size @xmath3 instead of a subsample @xmath25 .",
    "the aim of this paper is to prove the optimality , in the sense of @xcite , of the aggregation method proposed by yang , for the estimation of a density on @xmath26 where @xmath27 is the lebesgue measure on @xmath28 .",
    "this procedure is a convex aggregation with weights which can be seen in two different ways .",
    "yang s point of view is to express these weights in function of the likelihood of the model , namely @xmath29where the weights are @xmath30 and @xmath31 and the second point of view is to write these weights as exponential ones , as used in @xcite , @xcite , @xcite , @xcite , @xcite and @xcite , for different statistical models . define the empirical kullback loss @xmath32 ( keeping only the term independent of the underlying density to estimate ) for all density @xmath8 .",
    "we can rewrite these weights as exponential weights : @xmath33    most of the results on convergence properties of aggregation methods are obtained for the regression and the gaussian white noise models .",
    "nevertheless , @xcite , @xcite , @xcite , @xcite and @xcite have explored the performances of aggregation procedures in the density estimation framework .",
    "most of them have established upper bounds for some procedure and do not deal with the problem of optimality of their procedures . to our knowledge , lower bounds for the performance of aggregation methods in density estimation are available only in @xcite .",
    "their results are obtained with respect to the mean squared risk . @xcite and @xcite construct procedures and give convergence rates w.r.t . the kl loss .",
    "one aim of this paper is to prove optimality of one of these procedures w.r.t . the kl loss .",
    "lower bounds w.r.t . the hellinger s distance and @xmath1-distance ( stated in section [ sectionlowerbound ] ) and",
    "some results of @xcite and @xcite ( recalled in section [ sectionupperbound ] ) suggest that the rates of convergence obtained in theorem [ theohellinger ] and [ theol1 ] are optimal in the sense given in definition [ definitionoptimality ] .",
    "in fact , an approximate bound can be achieved , if we allow the leading term in the rhs of the oracle inequality ( i.e. in the upper bound ) to be multiplied by a constant greater than one .",
    "the paper is organized as follows . in section [ sectiondefinitionresult ]",
    "we give a definition of optimality , for a rate of aggregation and for an aggregation procedure , and our main results .",
    "lower bounds , for different loss functions , are given in section [ sectionlowerbound ] .",
    "in section [ sectionupperbound ] , we recall a result of @xcite about an exact oracle inequality satisfied by the aggregation procedure introduced in ( [ esti ] ) .",
    "to evaluate the accuracy of a density estimator we use the kullback - leiber ( kl ) divergence , the hellinger s distance and the @xmath1-distance as loss functions .",
    "the _ kl divergence _ is defined for all densities @xmath8 , @xmath34 w.r.t . a @xmath35finite measure @xmath5 on a space @xmath9 , by @xmath36 where @xmath37 ( respectively @xmath38 ) denotes the probability distribution of density @xmath8 ( respectively @xmath34 ) w.r.t . @xmath5 .",
    "_ hellinger s distance _ is defined for all non - negative measurable functions @xmath8 and @xmath34 by @xmath39 where the @xmath40-norm is defined by @xmath41 for all functions @xmath42 .",
    "@xmath1-distance _ is defined for all measurable functions @xmath8 and @xmath34 by @xmath43    the main goal of this paper is to find optimal rate of aggregation in the sense of the definition given below .",
    "this definition is an analog , for the density estimation problem , of the one in @xcite for the regression problem .",
    "[ definitionoptimality ] take @xmath44 an integer , @xmath45 a set of densities on @xmath46 and @xmath47 a set of functions on @xmath9 with values in @xmath48 such that @xmath49 .",
    "let @xmath50 be a loss function on the set @xmath47 .",
    "a sequence of positive numbers @xmath51 is called * optimal rate of aggregation of m functions in @xmath52 w.r.t . the loss @xmath50 * if :    a.   there exists a constant @xmath53 , depending only on @xmath54 , such that for all functions @xmath55 in @xmath47 there exists an estimator @xmath56 ( aggregate ) of @xmath8 such that @xmath57 - \\min_{i=1 , \\ldots , m } d(f , f_i ) \\right ] \\leq c \\psi_n(m ) , \\quad \\forall n\\in\\mathbb{n}^{*}.\\ ] ] b.   there exist some functions @xmath55 in @xmath47 and @xmath58 a constant independent of @xmath0 such that for all estimators @xmath59 of @xmath8 , @xmath60 - \\min_{i=1 , \\ldots , m } d(f , f_i )   \\right ] \\geq c \\psi_n(m ) , \\quad \\forall n\\in\\mathbb{n}^*.\\ ] ]    moreover , when the inequalities ( [ upperbound ] ) and ( [ lowerbound ] ) are satisfied , we say that the procedure @xmath61 , appearing in ( [ upperbound ] ) , is an * optimal aggregation procedure w.r.t . the loss @xmath50 .",
    "*    let @xmath62 be a given number .",
    "in this paper we are interested in the estimation of densities lying in @xmath63and , depending on the used loss function , we aggregate functions in @xmath47 which can be :    1 .",
    "@xmath64 for kl divergence , 2 .",
    "@xmath65 for hellinger s distance , 3 .",
    "@xmath66 for the @xmath1-distance .    the main result of this paper , obtained by using theorem [ theoyang ] and assertion ( [ ineq1 ] ) of theorem [ theokullback ] , is the following theorem .    [ theooptimal ]",
    "let @xmath62 .",
    "let @xmath0 and @xmath3 be two integers such that @xmath67 .",
    "the sequence @xmath68 is an optimal rate of aggregation of @xmath0 functions in @xmath69 ( introduced in ( [ set ] ) ) w.r.t . the kl divergence loss .",
    "moreover , the aggregation procedure with exponential weights , defined in ( [ esti ] ) , achieves this rate .",
    "so , this procedure is an optimal aggregation procedure w.r.t . the kl - loss .",
    "moreover , observing theorem [ theobirge ] and the result of @xcite ( recalled at the end of section [ sectionupperbound ] ) , the rates obtained in theorems [ theohellinger ] and [ theol1 ] : @xmath70 are near optimal rate of aggregation for the hellinger s distance and the @xmath1-distance to the power @xmath71 , where @xmath72 , if we allow the leading term `` @xmath73 '' to be multiplied by a constant greater than one , in the upper bound and the lower bound .",
    "to prove lower bounds of type ( [ lowerbound ] ) we use the following lemma on minimax lower bounds which can be obtained by combining theorems 2.2 and 2.5 in @xcite .",
    "we say that @xmath50 is a _",
    "_ s__emi - distance on @xmath74 if @xmath50 is symmetric , satisfies the triangle inequality and @xmath75 .",
    "[ lemlowerbound ] let @xmath50 be a semi - distance on the set of all densities on @xmath46 and @xmath76 be a non - decreasing function defined on @xmath77 which is not identically @xmath78 .",
    "let @xmath79 be a sequence of positive numbers .",
    "let @xmath80 be a finite set of densities on @xmath46 such that @xmath81,@xmath82 and the kl divergences @xmath83 , between the product probability measures corresponding to densities @xmath8 and @xmath34 respectively , satisfy , for some @xmath84 , @xmath85 then , @xmath86\\geq c_1,\\ ] ] where @xmath87denotes the infimum over all estimators based on a sample of size @xmath3 from an unknown distribution with density @xmath8 and @xmath88 is an absolute constant .",
    "now , we give a lower bound of the form ( [ lowerbound ] ) for the three different loss functions introduced in the beginning of the section .",
    "lower bounds are given in the problem of estimation of a density on @xmath28 , namely we have @xmath89 and @xmath5 is the lebesgue measure on @xmath28 .",
    "[ theohellinger ] let @xmath0 be an integer greater than @xmath90 , @xmath62 and @xmath72 two numbers .",
    "we have for all integers @xmath3 such that @xmath91 , @xmath92-\\min_{j=1,\\ldots , m } h(f_j , f)^q \\right ] \\geq c \\left(\\frac{\\log m}{n}\\right)^{q/2},\\ ] ] where @xmath93 is a positive constant which depends only on @xmath94 and @xmath71 . the sets @xmath95 and @xmath96 are defined in ( [ set ] ) when @xmath89 and the infimum is taken over all the estimators based on a sample of size @xmath3 .",
    "* proof :* for all densities @xmath55 bounded by @xmath94 we have , @xmath97-\\min_{j=1,\\ldots , m}h(f_j , f)^q \\right ]   \\geq \\inf_{\\hat f_n } \\sup_{f\\in \\{f_1,\\ldots , f_m \\ } }   \\mathbb{e}_f \\left [ h(\\hat{f}_n , f)^q \\right].\\ ] ] thus , to prove theorem 1 , it suffices to find @xmath0 appropriate densities bounded by @xmath94 and to apply lemma 1 with a suitable rate .",
    "we consider @xmath98 the smallest integer such that @xmath99 and @xmath100 .",
    "we set @xmath101 for all @xmath102 , where @xmath103 and @xmath104}(y)-{{\\rm 1}\\kern-0.24em{\\rm i}}_{(1/2,1]}(y)$ ] for all @xmath102 and @xmath105 will be chosen later .",
    "we consider @xmath106^d}(x)\\left ( 1+\\sum_{j=1}^{d}\\delta_j h_j(x_1)\\right ) , \\quad\\forall x=(x_1,\\ldots , x_d)\\in\\mathbb{r}^d,\\ ] ] for all @xmath107 .",
    "we take @xmath108 such that @xmath109 thus , for all @xmath110 , @xmath111 is a density bounded by @xmath94 .",
    "we choose our densities @xmath55 in @xmath112 , but we do not take all of the densities of @xmath113 ( because they are too close to each other ) , but only a subset of @xmath113 , indexed by a separated set ( this is a set where all the points are separated from each other by a given distance ) of @xmath114 for the _ hamming distance _ defined by @xmath115 for all @xmath116 . since @xmath117 , we have @xmath118for all @xmath119 .",
    "on the other hand the function @xmath120 , where @xmath121 , is convex on @xmath122 $ ] and we have @xmath123 so , according to jensen , @xmath124 .",
    "therefore @xmath125 , and we have @xmath126for all @xmath127 . according to varshamov - gilbert , cf .",
    "@xcite or @xcite , there exists a @xmath128-separated set , called @xmath129 , on @xmath114 for the hamming distance such that its cardinal is higher than @xmath130 and @xmath131 . on the separated set @xmath132 we have , @xmath133    in order to apply lemma [ lemlowerbound ] , we need to control the kl divergences too .",
    "since we have taken @xmath129 such that @xmath134 , we can control the kl divergences w.r.t .",
    "@xmath135 , the lebesgue measure on @xmath136^d$ ] .",
    "we denote by @xmath137 the probability of density @xmath138 w.r.t . the lebesgue s measure on @xmath28 , for all @xmath110 .",
    "we have , @xmath139^d } \\log \\left ( f_{\\delta}(x)\\right)f_{\\delta}(x ) dx \\\\ & = & n \\sum_{j=1}^{d}\\int_{\\frac{j-1}{d}}^{j / d}\\log \\left ( 1+\\delta_j h_j(x)\\right)\\left ( 1+\\delta_j h_j(x)\\right)dx \\\\ & = & n \\left(\\sum_{j=1}^{d}\\delta_j\\right)\\int_{0}^{1/d}\\log(1+h(x))(1+h(x))dx,\\end{aligned}\\ ] ] for all @xmath140 . since @xmath141 , we have , @xmath142    since @xmath143 , we can take @xmath108 such that @xmath144 and still having @xmath145 .",
    "thus , for @xmath146 , we have for all elements @xmath147 in @xmath129 , @xmath148 and @xmath149    applying lemma 1 when @xmath50 is @xmath150 , the hellinger s distance , with @xmath0 densities @xmath55 in @xmath151 where @xmath152^d}$ ] and the increasing function @xmath153 , we get the result .",
    "@xmath154    the construction of the family of densities @xmath155 is in the same spirit as the lower bound of @xcite , @xcite but , as compared to @xcite , we consider a different problem ( model selection aggregation ) and as compared to @xcite , we study a different model ( density estimation ) . also , our risk function is different from those considered in these papers .    now",
    ", we give a lower bound for kl divergence .",
    "we have the same residual as for square of hellinger s distance .",
    "[ theokullback]let @xmath156 be an integer , @xmath62 and @xmath72 .",
    "we have , for any integer @xmath3 such that @xmath157 , @xmath158   -\\min_{j=1,\\ldots , m } ( k(f|f_j))^q \\right ] \\geq c \\left(\\frac{\\log m}{n}\\right)^q,\\]]and @xmath159 -\\min_{j=1,\\ldots , m } ( k(f_j|f))^q \\right]\\geq c \\left(\\frac{\\log m}{n}\\right)^q,\\]]where @xmath93 is a positive constant which depends only on @xmath94 .",
    "the sets @xmath95 and @xmath160 are defined in ( [ set ] ) for @xmath89 .",
    "* proof :* proof of the inequality ( [ ineq2 ] ) of theorem [ theokullback ] is similar to the one for ( [ ineq1 ] ) .",
    "since we have for all densities @xmath8 and @xmath34 , @xmath161 ( a proof is given in * ? ? ?",
    "* ) , it suffices to note that , if @xmath55 are densities bounded by @xmath94 then , @xmath162 -\\min_{j=1,\\ldots , m } ( k(f|f_i))^q \\right]\\ ] ] @xmath163 \\right ] \\geq   \\inf_{\\hat f_n } \\sup_{f\\in \\{f_1,\\ldots , f_m \\ } } \\left [ \\mathbb{e}_f \\left [ h^{2q}(f,\\hat{f_n } ) \\right ] \\right],\\]]to get the result by applying theorem [ theohellinger ] .",
    "@xmath154    with the same method as theorem 1 , we get the result below for the @xmath1-distance .",
    "[ theol1 ] let @xmath156 be an integer , @xmath164 and @xmath72 .",
    "we have for any integers @xmath3 such that @xmath165 , @xmath166   -\\min_{j=1,\\ldots , m } v(f , f_i)^q \\right]\\geq c \\left(\\frac{\\log m}{n}\\right)^{q/2}\\ ] ] where @xmath93 is a positive constant which depends only on @xmath94 .",
    "the sets @xmath95 and @xmath167 are defined in ( [ set ] ) for @xmath89 .    * proof :* the only difference with theorem [ theohellinger ] is in the control of the distances . with the same notations as the proof of theorem [ theohellinger ]",
    ", we have , @xmath168^d }    \\rho(\\delta^1,\\delta^2)\\int_0^{1/d } |h(x)|dx = \\frac{l}{d^2}\\rho(\\delta^1,\\delta^2),\\]]for all @xmath169 .",
    "thus , for @xmath170 and @xmath129 , the @xmath128-separated set of @xmath114 introduced in the proof of theorem [ theohellinger ] , we have , @xmath171 therefore , by applying lemma 1 to the @xmath1-distance with @xmath0 densities @xmath55 in @xmath172 where @xmath152^d}$ ] and the increasing function @xmath153 , we get the result .",
    "in this section we use an argument in @xcite ( see also * ? ? ? * ) to show that the rate of the lower bound of theorem [ theokullback ] is an optimal rate of aggregation with respect to the kl loss .",
    "we use an aggregate constructed by yang ( defined in ( [ esti ] ) ) to attain this rate .",
    "an upper bound of the type ( [ upperbound ] ) is stated in the following theorem .",
    "remark that theorem [ theoyang ] holds in a general framework of a measurable space @xmath4 endowed with a @xmath6-finite measure @xmath5 .",
    "[ theoyang ] let @xmath173 be @xmath3 observations of a probability measure on @xmath4 of density @xmath8 with respect to @xmath5 .",
    "let @xmath55 be @xmath0 densities on @xmath174 .",
    "the aggregate @xmath13 , introduced in ( [ esti ] ) , satisfies , for any underlying density @xmath8 , @xmath175 \\leq \\min_{j=1,\\ldots , m}k(f|f_j ) + \\frac{\\log(m)}{n+1}.\\ ] ]    * proof :* proof follows the line of @xcite , although he does not state the result in the form ( [ upperbound ] ) , for convenience we reproduce the argument here .",
    "we define @xmath176 ( where @xmath177 is defined in ( [ weights ] ) and @xmath178 for all @xmath179 and @xmath180 ) and @xmath181 for all @xmath182 .",
    "thus , we have @xmath183 let @xmath8 be a density on @xmath46 .",
    "we have @xmath184 & = & \\sum_{k=0}^{n } \\int_{{{\\cal x}}^{k+1 } } \\log \\left(\\frac{f(x_{k+1})}{\\hat{f}_k(x_{k+1};x^{(k ) } ) } \\right)\\prod_{i=1}^{k+1}f(x_i)d\\nu^{\\otimes ( k+1)}(x_1,\\ldots , x_{k+1})\\\\ & = & \\int_{{{\\cal x}}^{n+1 } } \\left ( \\sum_{k=0}^{n}\\log \\left(\\frac{f(x_{k+1})}{\\hat{f}_k(x_{k+1};x^{(k ) } ) } \\right ) \\right)\\prod_{i=1}^{n+1}f(x_i)d\\nu^{\\otimes ( n+1)}(x_1,\\ldots , x_{n+1})\\\\ & = & \\int_{{{\\cal x}}^{n+1}}\\log \\left ( \\frac{f(x_1)\\ldots f(x_{n+1 } ) } { \\prod_{k=0}^{n}\\hat{f}_k(x_{k+1};x^{(k)})}\\right)\\prod_{i=1}^{n+1}f(x_i)d\\nu^{\\otimes ( n+1)}(x_1,\\ldots , x_{n+1}),\\end{aligned}\\ ] ] but @xmath185 thus , @xmath186=\\int_{{{\\cal x}}^{n+1}}\\log \\left ( \\frac{f(x_1)\\ldots f(x_{n+1})}{\\frac{1}{m } \\sum_{j=1}^{m}f_j(x_1)\\ldots f_j(x_{n+1})}\\right)\\prod_{i=1}^{n+1}f(x_i)d\\nu^{\\otimes ( n+1)}(x_1,\\ldots , x_{n+1}),\\ ] ] moreover @xmath187 is a decreasing function so , @xmath186 \\leq \\min_{j=1,\\ldots , m } \\left\\ { \\int_{{{\\cal x}}^{n+1}}\\log \\left ( \\frac{f(x_1)\\ldots f(x_{n+1})}{\\frac{1}{m } f_j(x_1)\\ldots f_j(x_{n+1})}\\right ) \\prod_{i=1}^{n+1}f(x_i)d\\nu^{\\otimes ( n+1)}(x_1,\\ldots , x_{n+1})\\right\\}\\ ] ] @xmath188 finally we have , @xmath189 \\leq \\log m + ( n+1 ) \\inf_{j=1,\\ldots , m } k(f|f_j).\\ ] ] on the other hand we have , @xmath190=   \\int_{{{\\cal x}}^{n+1 } } \\log \\left ( \\frac{f(x_{n+1})}{\\frac{1}{n+1 } \\sum_{k=0}^{n}\\hat{f}_{k } ( x_{n+1};x^{(k)})}\\right )    \\prod_{i=1}^{n+1}f(x_i)d\\nu^{\\otimes ( n+1)}(x_1,\\ldots , x_{n+1}),\\ ] ] and @xmath187 is convex , thus , @xmath191 \\leq \\frac{1}{n+1 } \\sum_{k=0}^{n } \\mathbb{e}_f \\left [ k(f|\\hat{f}_k ) \\right].\\ ] ] theorem [ theoyang ] follows by combining ( [ demoyang1 ] ) and ( [ demoyang2 ] ) .",
    "birg constructs estimators , called _",
    "t - estimators _ ( the  t  is for  test  ) , which are adaptive in aggregation selection model of @xmath0 estimators with a residual proportional at @xmath192 when hellinger and @xmath1-distances are used to evaluate the quality of estimation ( cf .",
    "but it does not give an optimal result as yang , because there is a constant greater than 1 in front of the main term @xmath193 where @xmath50 is the hellinger distance or the @xmath1 distance .",
    "nevertheless , observing the proof of theorem [ theohellinger ] and [ theol1 ] , we can obtain @xmath194 - c(q)\\min_{i=1 , \\ldots , m } d(f , f_i ) ^q \\right ] \\geq c\\left(\\frac{\\log m}{n}\\right)^{q/2},\\ ] ] where @xmath50 is the hellinger or @xmath1-distance , @xmath72 and @xmath62 .",
    "the constant @xmath195 can be chosen equal to the one appearing in the following theorem .",
    "the same residual appears in this lower bound and in the upper bounds of theorem [ theobirge ] , so we can say that @xmath196is near optimal rate of aggregation w.r.t .",
    "the hellinger distance or the @xmath1-distance to the power @xmath71 .",
    "we recall birg s results in the following theorem .",
    "[ theobirge ] if we have @xmath3 observations of a probability measure of density @xmath8 w.r.t .",
    "@xmath5 and @xmath55 densities on @xmath46 , then there exists an estimator @xmath13 ( t - estimator ) such that for any underlying density @xmath8 and @xmath72 , we have @xmath197\\leq c(q ) \\left ( \\min_{j=1,\\ldots , m } h(f , f_j)^q+\\left(\\frac{\\log m}{n}\\right)^{q/2 } \\right),\\ ] ] and for the @xmath1-distance we can construct an estimator @xmath13 which satisfies : @xmath198\\leq c(q ) \\left ( \\min_{j=1,\\ldots , m } v(f , f_j)^q+\\left(\\frac{\\log m}{n}\\right)^{q/2 } \\right),\\ ] ] where @xmath199 is a constant depending only on @xmath71 .",
    "an other result , which can be found in @xcite , states that the minimum distance estimate proposed by yatracos ( 1985 ) ( cf .",
    "@xcite ) achieves the same aggregation rate as in theorem [ theobirge ] for the @xmath1-distance with @xmath200 .",
    "namely , for all @xmath201 , @xmath202\\leq 3\\min_{j=1,\\ldots , m } v(f , f_j)+\\sqrt{\\frac{\\log m}{n}},\\ ] ] where @xmath203 is the estimator of yatracos defined by @xmath204 and @xmath205          l.  birg .",
    "model selection via testing : an alternative to ( penalized ) maximum likelihood estimators .",
    "_ to appear in annales of ihp _ , 2004 .",
    "available at http://www.proba.jussieu.fr/mathdoc/textes/pma-862.pdf .",
    "a.  juditsky , a.  nazin , a.b .",
    "tsybakov and n.  vayatis .",
    "online aggregation with mirror - descent algorithm .",
    "preprint n.987 , laboratoire de probabilits et modle alatoires , universits paris 6 and paris 7 ( available at http://www.proba.jussieu.fr/mathdoc/preprints/index.html#2005 ) .",
    "optimal rates of aggregation .",
    "_ computational learning theory and kernel machines .",
    "b.schlkopf and m.warmuth , eds .",
    "lecture notes in artificial intelligence _ , 2777:0 303313 , 2003 .",
    "springer , heidelberg ."
  ],
  "abstract_text": [
    "<S> in this paper we prove the optimality of an aggregation procedure . </S>",
    "<S> we prove lower bounds for aggregation of model selection type of @xmath0 density estimators for the kullback - leiber divergence ( kl ) , the hellinger s distance and the @xmath1-distance . </S>",
    "<S> the lower bound , with respect to the kl distance , can be achieved by the on - line type estimate suggested , among others , by @xcite . combining these results </S>",
    "<S> , we state that @xmath2 is an optimal rate of aggregation in the sense of @xcite , where @xmath3 is the sample size .    </S>",
    "<S> aggregation , optimal rates , kullback - leiber divergence . </S>"
  ]
}