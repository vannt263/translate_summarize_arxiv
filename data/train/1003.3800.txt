{
  "article_text": [
    "since the publication of the seminal treatise of tong @xcite , the field of nonlinear time series has been receiving considerable attention in the literature . today , nonlinear time series has been widely applied to subjects such as ecology , engineering , chaos , finance and econometrics . from a statistical perspective",
    ", nonlinear time series also furnishes an exciting platform for nonstandard statistical inference both parametrically and nonparametrically . for a comprehensive survey on some of these recent developments , see fan and yao @xcite .    among many different developments in nonlinear time",
    "series , estimation and testing of the threshold parameter constitute one of most challenging tasks .",
    "one of the main reasons of the difficulty arises from the fact that tricky and nonstandard asymptotic techniques are required to handle the threshold estimation , see chan @xcite , hansen @xcite and @xcite .",
    "a comprehensive theory for this type of problems seems to be lacking from the literature so far , however .    on the other hand , a relative complete theory for the statistical inference for diffusion processes in continuous time is available ,",
    "see for example kutoyants @xcite and @xcite .",
    "in particular , these two books demonstrate that both the maximum likelihood and the bayesian approaches to diffusion processes can be put under a general context and an asymptotic theory can be developed , albeit to its non standard nature .",
    "one of the main purposes of this paper is to make use of this general theory and apply it to the nonlinear time series context .",
    "related contributions to continuous time arma and threshold arma models can be found , for example , in brockwell @xcite , chan and tong @xcite , stramer , brockwell and tweedie @xcite , and tong @xcite and the references therein .    although likelihood inference for the threshold parameter of nonlinear time series was considered by chan @xcite and hansen @xcite previously , the asymptotic machineries employed were of special nature which can not be easily generalized to other situations . for further background on likelihood tests of non - linearity ,",
    "see li and li @xcite . from a bayesian perspective ,",
    "geweke and teuri @xcite considered a bayesian threshold ar model and derived the posterior distribution of the threshold parameter .",
    "however , a detailed description of the asymptotic properties of the bayesian estimator and its moment convergence were lacking .    by incorporating the developments in diffusion",
    ", this paper illustrates a general methodology to tackle both the maximum likelihood and bayesian estimation problems from which simulations can be efficiently conducted .",
    "moreover , the proposed approach is sufficiently transparent and can be easily adopted to other nonlinear time series context .",
    "a second but equally important goal of this study is to develop an implementable scheme for simulating and computing the limit likelihood statistics . by linking the integral equation of the underlying invariant density of the nonlinear time series and the intensity of the limiting compound commission process",
    ", one can compute the form of the limiting likelihood explicitly . to the best of our knowledge ,",
    "this has never been conducted before and results obtained in this paper can greatly enhance the inference for the threshold parameter of a nonlinear time series and extend its applications .",
    "this paper is organized as follows .",
    "background introduction together with the statement of the problem and the main result are given in section 2 .",
    "section 3 consists of simulations .",
    "section 4 discusses the extension to cover the usual one - sided threshold setting while conclusions and possible extensions are given in section 5 .",
    "consider the model @xmath0 where @xmath1 are i.i.d .",
    "@xmath2 , @xmath3 and @xmath4 .",
    "note that model  ( [ 2 - 1 ] ) appears to be different from the standard setting , where the threshold is usually partitioned as @xmath5 and @xmath6 .",
    "we choose the current setting because it is more general and mathematically more convenient .",
    "our results can be easily extended to encompass the standard setting as demonstrated in section 4.1 .",
    "we suppose that @xmath7 are known and @xmath8 is the unknown threshold parameter . our goal is to estimate @xmath9 from observations @xmath10 and to describe the asymptotic behavior of the estimators as @xmath11 .",
    "recall that @xmath12 is geometrically mixing ( see chen and tsay @xcite ) and denote its stationary density function by @xmath13 , see also fan and yao @xcite .    in this paper",
    ", we consider both the maximum likelihood and bayesian approaches .",
    "recall that the likelihood function is written as @xmath14 and the maximum likelihood estimator ( mle ) @xmath15 is defined by the equation @xmath16 .\\ ] ] if this equation has many solutions , we can , for example , call the mle to be the value which is at the center of the gravity . note that the function @xmath17 has jumps at the points @xmath18 where @xmath19 . clearly , if @xmath20 , then @xmath21 .    to apply the bayesian approach ,",
    "suppose that the unknown parameter is a random variable with a known prior density @xmath22 , which is continuous and positive .",
    "using the quadratic loss function , the bayesian estimator ( which minimizes the mean squares error ) is the conditional mathematical expectation @xmath23    properties of the least squares estimator ( lse ) of @xmath24 were studied in chan @xcite .",
    "the lse coincides with the mle for gaussian @xmath1 .",
    "we therefore recall properties of mle and compare them with properties of the bayesian estimators .",
    "first , introduce the stochastic process @xmath25 where @xmath26 and @xmath27 are two independent poisson processes of intensities @xmath28 ( @xmath29 is the stationary density function of @xmath30 ) and @xmath31 are independent gaussian @xmath32 random variables .",
    "it is easy to see that @xmath33,\\qquad u\\geq 0\\ ] ] @xmath34,\\qquad u\\geq 0\\ ] ] are compound poisson processes .",
    "the random process @xmath35 is piecewise constant and as a result , the points @xmath36 of the maximum of the process @xmath35 is defined by @xmath37 where @xmath38 here @xmath39 and @xmath40 are two consecutive events of the process @xmath26 , or of the process @xmath27 , or they are respectively the first event of @xmath26 and @xmath27 .",
    "simulated realizations of @xmath35 are given in section 3 .",
    "the center of gravity of the interval is given by the point @xmath41 such a choice of @xmath42 is explained in section 4 below .",
    "it follows from the result of chan @xcite that the mle @xmath15 is consistent and @xmath43 introduce the random variable @xmath44 the main result is the following theorem .",
    "[ t1 ] the bayesian estimator @xmath45 constructed by the observations @xmath46 of the threshold autoregressive process is consistent , the normalized difference @xmath47 converges in distribution : @xmath48 and for any @xmath49 @xmath50    * proof .",
    "* the proof of this theorem is based on the general result by ibragimov and khasminskii @xcite , theorem 1.10.2 . to apply it we study the normalized likelihood ratio process @xmath51,\\end{aligned}\\ ] ] where @xmath52 is the true value .",
    "recall the main steps .",
    "write the bayesian estimator @xmath53 as @xmath54    suppose that we proved the convergence of the process @xmath55 to the process @xmath35 providing the convergence of these integrals .",
    "then @xmath56 this convergence together with an estimate on the large deviations of the tails of the process @xmath55 allow us to prove the convergence of the moments .",
    "now check the conditions of the theorem 1.10.2 in @xcite .",
    "we need to prove    1 .   the convergence of the finite dimensional distributions of @xmath55 to the finite dimensional distributions of @xmath35 , that is , @xmath57 2 .   to establish the estimate : @xmath58 ^ 2\\leq c\\,\\left|u_2-u_1\\right|\\,,\\ ] ] 3 .   and to establish the estimate : for any @xmath59 @xmath60    the convergence of finite - dimensional distributions follows from the proposition 2 of @xcite .",
    "instead of repeating a technical argument as in @xcite , we offer a different intuitive ( but rigorous ) explanation as follows .",
    "rewrite the process as @xmath61 where we use @xmath62 and denote @xmath63 .    put @xmath64 and study the process @xmath65 for positive values of @xmath66 .",
    "@xmath67\\\\ & = \\sum_{j=0}^{n-1}\\left ( \\rho\\,x_j\\,\\left[\\1_{\\left\\{\\left|x_j\\right|>\\vartheta\\right\\}}- \\1_{\\left\\{\\left|x_j\\right|>\\vartheta+\\frac{u}{n}\\right\\}}\\right ] \\right)\\\\ & \\qquad \\times \\left(2x_{j+1}-2\\rho_1 \\,x_j- \\rho\\,x_j\\,\\left[\\1_{\\left\\{\\left|x_j\\right|>\\vartheta\\right\\}}+ \\1_{\\left\\{\\left|x_j\\right|>\\vartheta+\\frac{u}{n}\\right\\}}\\right ]   \\right).\\end{aligned}\\ ] ] note that @xmath68 hence , @xmath69\\;\\1_{\\left\\{\\vartheta",
    "< \\left|x_j\\right|\\leq\\vartheta+\\frac{u}{n}\\right\\}}\\nonumber\\\\ & = \\sum_{j=0}^{n-1}\\left(\\rho ^2\\,x_j^2 + 2\\rho \\,x_j\\,\\varepsilon _ { j+1 } \\right)\\;\\1_{\\left\\{\\vartheta < \\left|x_j\\right|\\leq\\vartheta+\\frac{u}{n}\\right\\ } }   . \\label{lr}\\end{aligned}\\ ] ]    next introduce another process @xmath70 and put @xmath71 .",
    "we show that this process is asymptotically equivalent to the process @xmath65 .",
    "we have @xmath72 for the first term we write @xmath73 the second term is ( recall that @xmath30 and @xmath74 are independent ) @xmath75 hence , @xmath76 for any fixed @xmath66 .",
    "therefore , it is sufficient to study the limit distribution of the random function @xmath77 and to show the convergence @xmath78.\\ ] ]    to see that the limit of @xmath77 is a compound poisson process , first note that the characteristic function @xmath79   } \\nonumber\\\\ & = \\left.\\ex_\\vartheta \\ex_\\vartheta \\left ( e^{{\\rm i}v\\sum_{l=0}^{n_+\\left(u\\right)}\\left[\\rho^2\\ , \\vartheta^2 + 2\\rho\\ , \\vartheta \\,\\varepsilon_l^+ \\right ]   } \\right|{\\cal f}_{n_+}\\right)\\nonumber\\\\ & = \\ex_\\vartheta   e^{\\left[{\\rm i}v\\rho^2\\ , \\vartheta^2 - 2v^2\\rho^2\\ , \\vartheta^2 \\,\\sigma ^2\\right]n_+\\left(u\\right ) } \\nonumber\\\\ & = \\exp\\left\\{u\\left(e^ { { \\rm i}v\\rho^2\\ , \\vartheta^2 - 2v^2\\rho^2\\ , \\vartheta^2 \\,\\sigma ^2 } -1\\right)2f\\left(\\vartheta , \\vartheta \\right)\\right\\},\\end{aligned}\\ ] ] where we denote @xmath80 to be the @xmath81-algebra related to the poisson process and make use of the independence of @xmath82 and @xmath26 .",
    "the desired convergence will be proved if the convergence of the characteristic function of the process @xmath83 to is established .",
    "fix @xmath84 , then as @xmath11 the band @xmath85 $ ] becomes narrower and the events , when @xmath86 $ ] , become more rare .",
    "this means that the distance between two consecutive events @xmath86 $ ] and @xmath87 $ ] tends to infinity .",
    "as the process @xmath12 is geometrically mixing , these events become asymptotically independent . under such circumstances ,",
    "the characteristic function @xmath88 can be calculated explicitly as @xmath89 further , @xmath90 hence , @xmath91 that is , it coincides with and as a result , ( [ fdd ] ) is proved .",
    "@xmath92    to establish conditions and we need the following two lemmas .    [ l1 ] there exists a constant @xmath93 such that for all values @xmath94 we have the inequality @xmath95    * proof . *",
    "the first step is @xmath96^{1/2}\\\\ & = 2 - 2\\;\\ex_{\\vartheta+\\frac{u_1}{n } } \\left[\\frac{z_n\\left(u_2\\right)}{z_n\\left(u_1\\right ) } \\right]^{1/2},\\end{aligned}\\ ] ] where the measure is changed from @xmath97 to @xmath98 .",
    "we have ( @xmath99 ) @xmath100^{1/2}\\\\ & \\quad = \\ex_{\\vartheta_1}\\exp\\left\\{-\\frac{1}{4\\sigma ^2}\\sum_{j=0}^{n-1}\\left[\\rho ^2\\,x_j^2 + 2\\rho\\,x_j\\,\\varepsilon _ { j+1}\\right ] \\,\\left[\\1_{\\left\\{\\bb_j\\left(u_2\\right)\\right\\ } } -\\1_{\\left\\{\\bb_j\\left(u_1\\right)\\right\\ } }   \\right]\\right\\ } .\\end{aligned}\\ ] ] note that @xmath101 further as @xmath102 , @xmath103 \\ , \\1_{\\left\\{\\cc_j\\right\\ } }   \\right\\}\\nonumber \\\\ & \\quad \\leq \\frac{1}{2\\sigma ^2}\\ex_{\\vartheta_1}\\left\\{\\sum_{j=0}^{n-1}\\left[\\rho ^2\\,x_j^2 + 2\\rho\\,x_j\\,\\varepsilon _ { j+1}\\right ] \\ , \\1_{\\left\\{\\cc_j\\right\\ } }   \\right\\}\\nonumber \\\\ &",
    "\\quad=\\frac{n\\rho ^2}{2\\sigma ^2}\\ex_{\\vartheta_1}\\,x_j^2\\,\\1_{\\left\\{\\cc_j\\right\\}}=\\frac{n\\rho ^2}{2\\sigma ^2}\\ ; \\int_{\\cc_j}^{}x^2\\;f\\left(\\vartheta + \\frac{u_1}{n},x\\right)\\,{\\rm d}x\\nonumber\\\\ & \\quad=\\frac{n\\rho",
    "^2}{2\\sigma ^2 } \\left(\\vartheta + \\frac{u_2}{n}\\right)^2\\,\\frac{u_2-u_1}{n}\\,\\left [ f\\left(\\vartheta + \\frac{u_1}{n},\\tilde\\vartheta_+   \\right)+f\\left(\\vartheta + \\frac{u_1}{n},\\tilde\\vartheta _ -\\right)\\right]\\nonumber \\\\ & \\quad\\leq c\\,\\left|u_2-u_1\\right| . \\label{dva}\\end{aligned}\\ ] ] we see that is fulfilled .",
    "[ l2 ] for any @xmath49 there exists a constant @xmath104 such that for all values @xmath105 we have the inequality @xmath106    * proof .",
    "* we have to study the following expectation @xmath107 \\ , \\1_{\\left\\{\\bb_j\\left(u\\right)\\right\\ } }   \\right\\}\\,.\\ ] ] start with the probability @xmath108 \\ , \\1_{\\left\\{\\bb_j\\left(u\\right)\\right\\ } } > -c \\left|u\\right|\\right\\}\\,.\\end{aligned}\\ ] ] write @xmath109 for the last probability , by markov inequality we have @xmath110 because @xmath111 the last equality follows from the following property of the conditional expectation @xmath112 hence , it is sufficient to study the probability @xmath113 where @xmath114 .",
    "fix some @xmath115 and consider first the _ local values _",
    "@xmath66 satisfying the condition @xmath116 .",
    "suppose that @xmath84 ( for @xmath117 the consideration is similar ) .",
    "then we have @xmath118 \\1_{\\left\\{\\bb_j\\left(u\\right ) \\right\\ } }    -\\vartheta ^2\\sum_{j=0}^{n-1 }   \\1_{\\left\\{\\bb_j\\left(u\\right ) \\right\\ } }       \\geq -c_1\\,u\\right\\}\\\\ & \\quad = \\pb_\\vartheta \\left\\{\\sum_{j=0}^{n-1 } \\left[x_j^2-\\vartheta ^2\\right ] \\1_{\\left\\{\\bb_j\\left(u\\right ) \\right\\ } }    + \\vartheta ^2\\sum_{j=0}^{n-1 }   \\1_{\\left\\{\\bb_j\\left(u\\right ) \\right\\ } }       \\leq c_1\\,u\\right\\}\\\\ & \\quad \\leq \\pb_\\vartheta \\left\\{\\sum_{j=0}^{n-1 }   \\1_{\\left\\{\\bb_j\\left(u\\right ) \\right\\ } }       \\leq c_2\\,u\\right\\},\\qquad c_2=\\frac{c_1}{\\vartheta ^2},\\end{aligned}\\ ] ] because @xmath119 \\1_{\\left\\{\\bb_j\\left(u\\right ) \\right\\}}\\geq 0 $ ] .",
    "recall that the sum @xmath120 converges to the poisson process of intensity @xmath121 , hence the last probability has to be small for the values @xmath122 .",
    "let @xmath123 and note that @xmath124 \\\\ & = 2\\,\\frac{u}{n}\\,f\\left(\\vartheta , \\vartheta \\right)\\left(1+o\\left(1\\right)\\right ) \\geq \\frac{u}{n}\\,f\\left(\\vartheta , \\vartheta \\right)\\,,\\end{aligned}\\ ] ] where the last inequality holds for @xmath125 . recall that the function @xmath13 is even and @xmath126 .",
    "further , @xmath127 where we chose such @xmath128 that @xmath129 . to estimate the last expectation we apply the inequality of dedeker and doukhan ( see ( 8.1 ) in @xcite ) : @xmath130^{\\frac{1}{p}}\\right)^p.\\ ] ] write @xmath131 and let @xmath132 , then @xmath133\\\\ & \\quad + \\frac{u}{n}\\ ; \\left[f \\left(\\vartheta , \\vartheta + \\frac{\\bar u}{n}\\right)+ f \\left(\\vartheta ,",
    "\\vartheta -\\frac{\\bar{\\bar u}}{n}\\right)\\right]=\\frac{u}{n}\\,a\\left(x_{j-1}\\right),\\end{aligned}\\ ] ] where @xmath134 is the density function of the gaussian r.v .",
    "@xmath1 , i.e. , @xmath135 ) and @xmath136 are some values between @xmath137 and @xmath66 .",
    "note that the function @xmath138 defined by the last equality is bounded and @xmath139 .",
    "hence @xmath140 and @xmath141^{\\frac{1}{p}}=\\frac{u}{n}\\ , \\sum_{j=0}^{n-1 } \\left[\\ex_\\vartheta \\left|y_0\\,\\left.\\ex_\\vartheta \\left(a\\left(x_{j-1}\\right)\\right|{\\cal f}_0\\right)\\right|^p\\right]^{\\frac{1}{p}}\\\\ & \\qquad \\leq c\\;\\frac{u}{n}\\ , \\sum_{j=0}^{n-1}\\alpha \\left(j-1\\right)=c\\;\\frac{u}{n}\\ , \\sum_{j=0}^{n-1}\\gamma ^{j-1 } \\leq c\\;\\frac{u}{n}\\,,\\end{aligned}\\ ] ] where we used the geometrical ergodicity of @xmath12 : @xmath142 , @xmath143 ( see @xcite ) and the inequality of ibragimov @xmath144 ( see bradley @xcite , theorem 4.4 , ( a2 ) ) .",
    "finally , we obtain ( for @xmath145 ) the estimate @xmath146    consider now the case @xmath147 . of course , @xmath148 .",
    "we have @xmath149 because @xmath150 and the constant @xmath128 is chosen such that @xmath151 . the last probability in",
    "can be estimated with the help of the following lemma .",
    "( rosenthal s moment inequality ) [ ros ] let @xmath152 be zero mean mixing series satisfying the condition : there exist @xmath153 and @xmath154 , @xmath155 , such that @xmath156^\\frac{\\varepsilon } { c+\\varepsilon } < \\infty   , \\ ] ] where @xmath157 is the @xmath158-mixing coefficient , then @xmath159 .\\ ] ]    for the proof see @xcite , p.26 .",
    "as the process @xmath30 is geometrically mixing ( see @xcite , theorem 2.4 ) , hence condition is fulfilled with any @xmath160 and @xmath153 .",
    "we apply with @xmath161 obviously , @xmath162 and @xmath163 .",
    "we suppose for simplicity that @xmath84 , @xmath164 and similarly @xmath165 hence ,",
    "@xmath166 where we have used the relations @xmath167 , @xmath148 and have chosen sufficiently small @xmath168 ( or sufficiently large @xmath169 ) .    by chebyshev inequality @xmath170    the estimates obtained above",
    "allow us to write the following expression : for any @xmath171 and all @xmath172 , there exist constants @xmath160 and @xmath93 such that @xmath173 for the expectation , note that @xmath174 recall that this estimate is valid for any @xmath171 , hence is verified .",
    "therefore the required conditions are fulfilled and the bayesian estimate satisfied all of the properties stipulated in theorem 1 ( see theorem 1.10.2 , @xcite ) . @xmath92",
    "we obtain the density functions of limit distributions of the mle and bayesian estimators by the following simulations .",
    "the limit likelihood ratio is @xmath175 for @xmath176 and @xmath177 for @xmath178 . here",
    "@xmath26 and @xmath27 are independent poisson processes of intensity @xmath179 and the gaussian random variables @xmath180 , @xmath181 are independent , @xmath182 .",
    "denote @xmath183 then the poisson processes @xmath184 and @xmath185 have intensity 1 and the limit likelihood ratio @xmath186 now the limit process @xmath187 only depends on one parameter ( @xmath188 ) and the limit random variables @xmath42 and @xmath189 can be written as @xmath190 in obvious notation .",
    "the next problem is to find the function @xmath13 , where @xmath13 is the stationary density function of @xmath30 .",
    "as @xmath191 where @xmath30 and @xmath74 are independent , we obtain the convolution equation @xmath192 herein , we denote the density function of @xmath193 by @xmath194 .",
    "this density can be expressed as a function of @xmath195 , which is a solution to a corresponding integral equation , see also chan and tong @xcite .",
    "specifically , observe that @xmath196 hence , the integral equation is @xmath197 \\varphi \\left(y - x\\right){\\rm d}x\\nonumber\\\\ & \\qquad = \\int_{-\\infty } ^{\\infty } f\\left(\\vartheta , x\\right ) \\left[\\varphi \\left(y - x\\rho _",
    "1\\right)\\;\\1_{\\left\\{\\left|x\\right|<\\vartheta \\right\\ } } + \\varphi \\left(y - x\\rho _ 2\\right)\\;\\1_{\\left\\{\\left|x\\right|\\geq \\vartheta \\right\\ } } \\right ] { \\rm d}x .",
    "\\label{dens}\\end{aligned}\\ ] ]    solution to this equation at the point @xmath52 is the intensity @xmath198 of the corresponding poisson processes .",
    "therefore the value @xmath199 satisfies the integral equation @xmath200 { \\rm d}x\\,,\\ ] ] where @xmath201 is gaussian @xmath32 density .",
    "we see that @xmath202 . to visualize the properties of the sample path @xmath203 and the invariant density @xmath204",
    ", we conduct a simulation experiment by taking @xmath205 to be i.i.d .",
    "standard normal random variables .",
    "the parameters used are @xmath206 and @xmath207 .",
    "a gaussian kernel is used to estimate the form of @xmath208 based on @xmath209 observations of @xmath210 generated from model   with @xmath211 being i.i.d .",
    "@xmath212 random variables .",
    "the plots of @xmath203 and @xmath213 are given in figures 1 and 2 respectively .",
    "for the maximum likelihood estimate , note that the maximum values of @xmath214 form an interval [ @xmath215 with length @xmath216 , where @xmath217 is an exponential random variable with probability density @xmath218 , @xmath219 .",
    "we can take any value of @xmath66 from this interval , the middle point @xmath220 , say . to have its density function we only need to simulate the exponential and the gaussian independent random variables which will generate @xmath221 .",
    "the historgram of @xmath222 based on @xmath223 simulated values of @xmath222 is plotted in figure 3 .",
    "as can be seen clearly , the mle performs reasonably well and converges to zero very fast .",
    "the sample mean of the simulated @xmath222 is @xmath224 with a standard deviation @xmath225 . for the bayesian estimators we first calculate the integral @xmath226 here @xmath227 and @xmath228 . by a similar way we have @xmath229",
    "the limit random variable is @xmath230 with obvious notation . to understand the behavior of the bayesian estimator",
    ", we simulate the bayesian estimator for @xmath223 times with the histogram of @xmath231 given in figure 4 . from this figure",
    ", it is clearly seen that the bayesian estimator converges to the expected value zero .",
    "the sample mean is @xmath232 with a standard deviation @xmath233 .",
    "it is interesting to see that this simulation results are consistent with the theory that the limit variances of the mle and be @xmath234 satisfy @xmath235 note that it follows from the symmetry of the limit process , the random variables @xmath42 and @xmath189 satisfy @xmath236 .        to examine the finite sample performance of the test statistics , we computed the critical values of the limit distributions based on the mle and the be using the same set of parameters as given in figures 3 .",
    "the sizes are chosen for commonly used test statistics and the limiting values are given in the first two rows of table 1 .",
    "as can be seen , both the mle and be procedures perform reasonably well and are in close agreement .",
    "furthermore , the numbers in the last row of table 1 are the critical values computed from the test statistics in ( [ 2 - 2 ] ) , which are directly simulated from model ( [ 2 - 1 ] ) using the same set of parameters .",
    "it is seen that the critical values generated from the simulated statistics agree remarkably well with the critical values computed from the limit distributions in table 1 based on mle . in summary ,",
    "table 1 demonstrates the usefulness of the limit distributions in computing the critical values .",
    "if one needs to conduct a test for another set of parameters , then a similar table can be computed and the programming code is available from the authors upon request ."
  ],
  "abstract_text": [
    "<S> this paper studies the threshold estimation of a tar model when the underlying threshold parameter is a random variable . </S>",
    "<S> it is shown that the bayesian estimator is consistent and its limit distribution is expressed in terms of a limit likelihood ratio . </S>",
    "<S> furthermore , convergence of moments of the estimators is also established . </S>",
    "<S> the limit distribution can be computed via explicit simulations from which testing and inference for the threshold parameter can be conducted . </S>",
    "<S> the obtained results are illustrated with numerical simulations . </S>",
    "<S> + * key words and phrases : * bayesian estimator , continuous - time diffusion , compound poisson process , limit distribution , limit likelihood ratio and nonlinear threshold models . </S>",
    "<S> + _ ams 1991 subject classifications : primary 62g30 ; secondary 62m10 . _ </S>"
  ]
}