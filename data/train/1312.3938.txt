{
  "article_text": [
    "infiniband is the preferred network for most of high performance computing and for certain cloud applications , due to its low latency .",
    "we present a new approach to checkpointing over infiniband .",
    "this is the first efficient and transparent solution for _ direct _ checkpoint - restart over the infiniband network ( without the intermediary of an mpi implementation - specific checkpoint - restart service ) .",
    "this also extends to other language implementations over infiniband , such as unified parallel  c ( upc ) .",
    "this work contains several subtleties , such as whether to drain the `` in - flight data '' in an infiniband network prior to checkpoint , or whether to force a re - send of data after resume and restart .",
    "a particularly efficient solution was found , which combines the two approaches .",
    "the infiniband completion queue is drained and refilled on resume or restart , and yet data is never re - sent , _ except _ in the case of restart .",
    "historically , transparent ( system - initiated ) checkpoint - restart has typically been the first technology that one examines in order to provide fault tolerance during long - running computations .",
    "for example , transparency implies that a checkpointing scheme works independently of the programming language being used ( e.g. , fortran , c , or c++ ) . in the case of distributed computations over ethernet",
    ", several distributed checkpointing approaches have been proposed  @xcite .",
    "unfortunately , those solutions do not extend to supporting the infiniband network .",
    "other solutions for distributed checkpointing are specific to a particular mpi implementation  @xcite .",
    "these mpi - based checkpoint - restart services `` tear down '' the infiniband connection , after which a single - process checkpoint - restart package can be applied .    finally , checkpoint - restart is the process of saving to stable storage ( such as disk or ssd ) the state of the processes in a running computation , and later re - starting from stable storage .",
    "_ checkpoint _ refers to saving the state , _ resume _ refers to the original process resuming computation , and _ restart",
    "_ refers to launching a new process that will restart from stable storage .",
    "the checkpoint - restart is _ transparent _ if no modification of the application is required .",
    "this is sometimes called _ system - initiated _ checkpointing .",
    "prior schemes specific to each mpi implementation had : ( i )  blocked the sending of new messages and waited until pending messages have been received ; ( ii )  `` torn down '' the network connection ; ( iii )  checkpointed each process in isolation ( without the network ) , typically using the blcr package  @xcite based on a kernel module ; ( iv )  and then re - builds the network connection .",
    "those methods have three drawbacks :    1 .",
    "checkpoint - resume can be slower due to the need to tear down and re - connect the network .",
    "pgas languages such as upc  @xcite must be re - factored to run over mpi instead of directly over infiniband in order to gain support for transparent checkpoint - restart .",
    "this can produce additional overhead .",
    "the use of a blcr kernel module implied that the restart cluster must use the same linux kernel as the checkpoint cluster .",
    "the current work is implemented as a plugin on top of dmtcp ( distributed multithreaded checkpointing )  @xcite .",
    "the experimental evaluation demonstrates dmtcp - based checkpointing of open  mpi for the nas lu benchmark and others . for 512  processes , checkpointing to a local disk drive ,",
    "occurs in 232  seconds , whereas it requires 36  seconds when checkpointing to back - end lustre - based storage .",
    "checkpointing of up to 2,048 mpi processes ( 128  nodes with 16  cores per node ) is shown to have a run - time overhead between 0.8% and 1.7% .",
    "this overhead is shown to be a little less than the overhead when using the checkpoint - restart of open  mpi using blcr .",
    "tests were also carried out on berkeley upc  @xcite over gasnet s ibv conduit  @xcite , with similar results for checkpoint times and run - time overhead .",
    "the new approach can also be extended to provide capabilities similar to the existing interconnection - agnostic use of the open  mpi checkpoint - restart service  @xcite .",
    "specifically , we demonstrate an additional ib2tcp plugin that supports cost - effective use of a symbolic debugger ( e.g. , gdb ) on a large production computation .",
    "the ib2tcp plugin enables checkpointing over infiniband and restarting over ethernet .",
    "thus , when a computation fails after on a production cluster ( perhaps after hours or days ) , the checkpoint image can be copied to an inexpensive , smaller , ethernet - based cluster for interactive debugging .",
    "an important contribution of the ib2tcp plugin , is that unlike the blcr kernel - based approach , the dmtcp / ib2tcp approach supports using an ethernet - based cluster that uses a different linux kernel , something that occurs frequently in practice .",
    "finally , the approach of using dmtcp plugins provides is easily maintainable , as measured by lines of code .",
    "the primary plugin , supporting checkpointing over infiniband , consists of 2,700  lines of code .",
    "the additional ib2tcp plugin consists of 1,000  lines of code .",
    "[ [ organization - of - paper . ] ] organization of paper .",
    "+ + + + + + + + + + + + + + + + + + + + + +    the rest of this paper includes the background of dmtcp ( section  [ sec : dmtcp ] . and the algorithm for checkpointing over infiniband ( section  [ sec : algorithm ] )",
    ". limitations of this approach are discussed in section  [ sec : limitations ] .",
    "an experimental evaluation is presented in section  [ sec : experiments ] .",
    "section  [ sec : ib2tcp ] is of particular note , in reporting on the ib2tcp plugin for migrating a computation from a production cluster to a debug cluster .",
    "finally , the related work ( section  [ sec : relatedwork ] ) and conclusions ( section  [ sec : conclusion ] ) are presented .",
    "section  [ sec : infiniband ] reviews some concepts of infiniband , necessary for understanding the checkpointing approach described in section  [ sec : algorithm ] .",
    "section  [ sec : plugins ] describes the use of plugins in dmtcp .",
    "infiniband concepts ]    in order to understand the algorithm , we review some concepts from the verbs api of infiniband . while there are several references that describe infiniband , we recommend one of  @xcite as a gentle introduction for a general audience .",
    "recall that the infiniband network uses _ rdma _",
    "( remote dma to the ram of a remote computer ) .",
    "each computer node must have a host channel adapter ( hca ) board with access to the system bus ( memory bus ) .",
    "with only two computer nodes , the hca adapter boards may be connected directly to each other . with three or more nodes ,",
    "communication must go through an infiniband switch in the middle .",
    "note also that the bytes of an infiniband message may be delivered out of order .",
    "figure  [ fig : ibconcepts ] reviews the basic elements of an infiniband network . a hardware host channel adapter ( hca ) and the software library and driver together maintain at least one _ queue pair _ and a _ completion queue _ on each node .",
    "the queue pair consists of a send queue on one node and a receive queue on a second node . in a bidirectional connection , each node will contain both a send queue and a receive queue . sending a message across a queue pair",
    "causes an entry to be added to the completion queue on each node .",
    "however , it is possible to set a flag when posting a work request to the send queue , such that no entry is added to the completion queue on the `` send '' side of the connection .    although not explicitly introduced as a standard , libibverbs ( provided by the linux ofed distribution )",
    "is the most commonly used infiniband interface library .",
    "we will describe the model in terms of the functions prefixed by ibv _ for the _ verbs library _ ( libibverbs ) .",
    "many programs also use ofed s convenience functions , prefixed by rdma_. ofed also provides an optional library , librdmacm ( rdma connection manager ) for ease of connection set - up and tear - down in conjunction with the verbs interface . since this applies only to set - up and tear - down",
    ", this library does not affect the ability to perform transparent checkpoint - restart .",
    "we assume the reliable connection model ( end - to - end context ) , which is by far the most commonly used model for infiniband .",
    "there are two models for the communication :    * send - receive model * rdma ( remote dma ) model ( often employed for efficiency , and serving as the inspiration for the one - sided communication of the mpi-2 standard )    our infiniband plugin supports both models , and a typical mpi implementation can be configured to use either model .",
    "we first describe the steps in processing the send - receive model for infiniband connection",
    ". it may be useful to examine figure  [ fig : ibconcepts ] while reading the steps below .    1 .",
    "initialize a hardware context , which causes a buffer in ram to be allocated .",
    "all further operations are with respect to this hardware context .",
    "2 .   create a protection domain that sets the permissions to determine which computers may connect to it .",
    "3 .   register a memory region , which causes the virtual memory to be pinned to a physical address ( so that the operating system will not page that memory in or out ) .",
    "a completion queue is created for each of the sender and the receiver .",
    "this completion queue will be used later .",
    "create a queue pair ( a send queue and a receive queue ) associated with the completion queue .",
    "an end - to - end connection is created between two queue pairs , with each queue pair associated with a port on an hca adapter .",
    "the sender and receiver queue pair information ( several ids ) is exchanged , typically either using either tcp ( through a side channel that is not non - infiniband ) , or by using an rdmacm library whose api is transport - neutral .",
    "the receiver creates a work request and posts it to the receive queue .",
    "( one can post multiple receive buffers in advance . )",
    "the sender creates one or more work requests and posts them to the send queue .",
    "[ step : postreceive ] _ note : _ the application must ensure that a receive buffer has been posted before it posts a work request to the send queue .",
    "it ia an application error if this is not the case .",
    "the transfer of data now takes place between a posted buffer on the send queue and a posted buffer on the receive queue . the posted send and receive buffers have now been used up , and further posts are required for further messages . 11 . [",
    "step : completion ] upon completion , work completions are generated by the hardware and appended to each of the completion queues , one queue on the sender s node and one queue on the receiver s node .",
    "[ step : polling ] the sender and receiver each poll the completion queue until a work completion is encountered .",
    "( a blocking request for work completion also exists as an alternative .",
    "a blocking request must be acknowledged on success . )",
    "polling causes the work completion to be removed from the completion queue .",
    "hence , further polling will eventually see further completion events .",
    "both blocking and non - blocking versions of the polling calls exist .",
    "we also remark that a work request ( a wqe or work queue entry ) points to a list of scatter / gather elements , so that the data of the message need not be contiguous .",
    "the rdma model is similar to the send - receive model .",
    "however , in this case , one does not post receive buffers .",
    "the data is received directly in a memory region .",
    "an efficient implementation of mpi s one - sided communication ( mpi_put , mpi_get , mpi_accumulate ) , when implemented over infiniband , will typically employ the rdma model  @xcite .    as a consequence , step  [ step : postreceive ] of section  [ sec : sendreceivemodel ]",
    "does not appear in the rdma model .",
    "similarly , steps  [ step : completion ] and  [ step : polling ] are modified in the rdma model to refer to completion and polling solely for the send end of the end - to - end connection .",
    "other variations exist , which are supported in our work , but not explicitly discussed here . in one example , an infiniband application may choose to send several messages without requesting a work completion in the completion queue . in these cases , an application - specific algorithm will follow this sequence with a message that includes a work completion . in a second example ,",
    "an rdma - based work request may request an immediate mode , in which the work completion is placed only in the remote completion queue and not in the local completion queue .",
    "dmtcp is a transparent , checkpoint - restart package that supports third - party plugins .",
    "the current work on infiniband support was implemented as a dmtcp plugin  @xcite .",
    "the plugin is used here to virtualize the infiniband resources exposed to the end user , such as the queue pair struct ( ibv_qp ) ( see figure  [ fig : ibconcepts ] ) .",
    "this is needed since upon restart from a checkpoint image , the plugin will need to create a new queue pair for communication . as a result , the infiniband driver will create a new queue pair struct at a new address in user space , with new ids .",
    "plugins provide three core features used here to support virtualization :    1 .",
    "wrapper functions around functions of the infiniband library : these wrappers translate between virtual resources ( see by the target application ) and real resources ( seen within the infiniband library , driver and hardware ) .",
    "the wrapper function also records changes to the queue pair and other resources for later replay during restart .",
    "event hooks : these hooks are functions within the plugin that dmtcp will call at the time of checkpoint and restart .",
    "hence , the plugin is notified at the time of checkpoint and restart , so as to update the virtual - to - real translations , to recreate the network connection upon restarting from a checkpoint image , and to replay some information from the logs .",
    "3 .   [ step : publishsubscribe]a publish / subscribe facility : to exchange ids among plugins running on the different computer nodes whenever new device connections are created .",
    "examples of such ids are local and remote queue pair numbers and remote keys of memory regions .",
    "figure  [ fig : qp ] presents an overview of the virtualization of a queue pair .",
    "this is the most complex of the subsystems being checkpointed . in overview , observe that the dmtcp plugin library interposes between most calls from the target application to the infiniband ibverbs library .",
    "this allows the dmtcp infiniband plugin to intercept the creation of a queue pair by the infiniband kernel driver , and to create a shadow queue pair .",
    "the target application is passed a pointer only to the virtual queue pair created by the plugin .",
    "thus , any further ibverbs calls to manipulate the queue pair will be intercepted by the plugin , and appropriate fields in the queue pair structure can be appropriately virtualized before the real ibverbs call .",
    "similarly , any ibverbs calls to post to the send or receive queue or to modify the queue pair are intercepted and saved in a log .",
    "this log is used for internal bookkeeping by the plugin , to appropriately model work requests as they evolve into the completion queue . in particular , note that a call to ibv_post_send may request that no work completion be entered on the completion queue .",
    "queue pair resources and their virtualization .",
    "( the plugin keeps a log of calls to post to or to modify the queue pair . ) ]      as the user base code makes calls to the verbs library , we will use dmtcp plugin wrapper functions around these library functions to interpose .",
    "hence the user call goes first to our dmtcp plugin library .",
    "we then extract parameters describing how the resources were created , before passing on the call to the verbs library , and later passing back the return value .",
    "this allows us to recreate semantically equivalent copies of those same resources on restart _ even if we restart on a new computer_.    in particular , we record any calls to modify_qp and to modify_srq . on restart ,",
    "those calls are replayed in order to place the corresponding data structures in a semantically equivalent state to pre - checkpoint .",
    "while the description above appears simple , several subtleties arise , encapsulated in the following principles .",
    "[ [ principle-1-never - let - the - user - see - a - pointer - to - the - actual - infiniband - resource . ] ] principle 1 : never let the user see a pointer to the actual infiniband resource .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a verbs call that creates a new infiniband resource will typically create a struct , and return a pointer to that struct .",
    "we will call this struct created by the verbs library a _",
    "real struct_. if the end user code create an infiniband resource , we interpose to copy that struct to a a new _ shadow struct _ , and then pass back to the end user the pointer to this shadow struct . some examples of infiniband resources for which this is done are : a context , a protection domain , a memory region , and a queue pair .",
    "the reason for this is that many implementations of infiniband libraries contain additional undocumented fields in these structs , in addition to those documented by the corresponding `` man page '' .",
    "when we restart after checkpoint , we can not pass the original pre - checkpoint struct to the verbs library .",
    "the undocumented ( hidden ) fields would not match the current state of the infiniband hardware on restart .",
    "( new device - dependent ids will be in use after restart . )",
    "so , on restart , we create an entirely new infiniband resource ( using the same parameters as the original ) .",
    "this new struct should be semantically equivalent to the pre - checkpoint original , and the hidden fields will correspond to the post - restart state of the hardware .",
    "one can think of this as a form of virtualization .",
    "the user is passed a pointer to a _ virtual struct _",
    ", the shadow struct .",
    "the verbs library knows only about the _",
    "real struct_. so , we will guarantee that the verbs library only sees real structs , and that the end user code will only see virtual structs .    to do this , we interpose our dmtcp plugin library function if a verbs library function refers to one of these structs representing infiniband resources .",
    "if the end user calls a verbs library function that returns a pointer to a real struct , then our interposition will replace this and return a pointer to a corresponding virtual struct .",
    "if the user code passes an argument pointing to a virtual struct , we will replace it by a pointer to a real struct before calling the verbs library function .",
    "[ [ principle-2-inline - functions - almost - always - operate - through - pointers - to - possibly - device - dependent - functions .- add - wrappers - around - the - pointers - and - not - the - inline - functions . ] ] principle 2 : inline functions almost always operate through pointers to possibly device - dependent functions .",
    "add wrappers around the pointers , and not the inline functions .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the previous principle depends on wrapper functions that interpose in order to translate between real and virtual structs .",
    "but in the ofed ibverbs implementation , some of the apparent library calls to the verbs library are in fact inline functions .",
    "a dmtcp plugin can not easily interpose on inline functions .",
    "luckily , these inline functions are often associated with possibly device - dependent functions .",
    "so , the ofed software design expands the inline functions into calls to pointers to other functions .",
    "those pointers can be found through the ops field of a struct ibv_context .",
    "the ops field is of type struct ibv_context_ops and contains the function pointers .",
    "this gives us access , and we modify the function pointers to point to our own functions , which can then interpose and finally call the original function pointers created by the verbs library .",
    "[ [ principle-3-carry - out - bookkeeping - on - posts - of - work - queue - entries - to - the - send - and - receive - queue . ] ] principle 3 : carry out bookkeeping on posts of work queue entries to the send and receive queue .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    as work requests are entered onto a send queue or receive queue , the wrapper functions of the dmtcp plugin record those work requests ( which have now become work queue entries )",
    ". when the completion queue is polled , if a completion event corresponding to that work queue entry is found , then the dmtcp plugin records that the entry has been destroyed . at the time of checkpoint",
    ", there is a log of those work queue entries that have been posted and not yet destroyed . at the time of restart , the send and receive queues will initially be empty .",
    "so , those work queue entries are re - posted to their respective queues .",
    "( in the case of resume , the send and receive queues continue to hold their work queue entries , and so no special action is necessary . )",
    "[ [ principle-4-at - the - time - of - checkpoint - drain - the - completion - queue - of - its - completion - events . ] ] principle 4 : at the time of checkpoint , `` drain '' the completion queue of its completion events .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    at the time of checkpoint , and after all user threads have been quiesced , the checkpoint thread polls the completion queue for remaining completion events not yet seen by the end user code .",
    "a copy of each completion event seen is saved by the dmtcp plugin .",
    "note that we must drain the completion queue for each of the sender and the receiver . recall also that the verbs library function for polling the completion queue will also remove the polled completion event from the completion queue as it passes that event to the caller .",
    "[ [ principle-5-at - the - time - of - restart - or - resume - refill - a - virtual - completion - queue . ] ] principle 5 : at the time of restart or resume , `` refill '' a virtual completion queue .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    at the time of restart or resume and before any user threads have been re - activated , we must somehow refill the completion queue , since the end user has not yet seen the completion events from the previous principle . to do this , the dmtcp plugin stores the completion events of the previous principle in its own private queue .",
    "the dmtcp plugin library then interposes between any end user calls to a completion queue and the corresponding verbs library function .",
    "if the end user polls the completion queue , the dmtcp wrapper function passes back to the end user the plugin s private copy of the completion events , and the verbs library function for polling is never called . only after the private completion queue becomes empty",
    "are further polling calls passed on to the verbs library function .",
    "hence , the plugin s private queue becomes part of a _ virtual completion queue_.    [ [ principle-6-any - infiniband - messages - still - in - flight - can - be - ignored . ] ] principle 6 : any infiniband messages still `` in flight '' can be ignored .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    if data from an infiniband message is still in flight ( has not yet arrived in the receive buffer ) , then infiniband will not generate a completion event .",
    "note that the infiniband hardware may continue to transport the data of a message , and even generate a completion event _",
    "after all user threads have been quiesced for checkpoint_. nevertheless , a simple rule operates .",
    "if our checkpoint thread has not seen a completion event that arrived late , then we will not have polled for that completion event .",
    "therefore , our bookkeeping in principle  3 will not have removed the send or receive post from our log .",
    "further , this implies that the memory buffers will continue to have the complete data , since it was saved on checkpoint and restored on restart .",
    "therefore , upon restart ( which implies a fresh , empty completion queue ) , the checkpoint thread will issue another send or receive post ( again following the logic of principle  3 ) .    [ [ remark . ] ] remark .",
    "+ + + + + + +    blocking requests for a completion event ( ibv_get_cq_event ) and for shared receive queues create further issues .",
    "while those details add some complication , their solution is straightforward and is not covered here .",
    "a number of infiniband objects and associated ids will change on restart .",
    "all of these must be virtualized . among these objects and ids",
    "are ibv contexts , protection domains , memory regions ( the local and remote keys ( lkey / rkey ) of the memory regions ) , completion queues , queue pairs ( the queue pair number , qp_num ) , and the local i d of the hca port being used ( lid ) .",
    "note that the lid of an hca port will not change if restarting on the same host , but it may change needed when restarting on a new host , which may have been configured to use a different port .    in all of the above cases ,",
    "the plugin assigns a virtual i d and maintains a translation table between virtual and real i d .",
    "the application sees only the virtual  i d .",
    "any infiniband calls are processed through the plugin , where virtual ids are translated back to real  ids .    on restart",
    ", the infiniband hardware may assign new real  ids for a given infiniband object . in this case , the real  ids are updated within the translation tables maintained by the plugin .",
    "however , a more difficult issue occurs in the case of remote memory keys ( rkey ) , queue pair numbers ( qp_num ) and local ids ( lid ) . in all three cases",
    ", an infiniband application must pass these ids to a remote node for communication with the local node .",
    "the remote need will need the qp_num and lid when calling ibv_modify_qp to initialize a queue pair that connects to the local node .",
    "the remote node will need the rkey when calling ibv_post_send to send a message to the local node .",
    "since the plugin allows the application to see only virtual ids , the application will employ a virtual i d when calling ibv_modify_qp and ibv_post_send .",
    "the plugin will first replace the virtual i d by the real  i d , which is known to the infiniband hardware . to do this",
    ", the plugin within each remote node must contain a virtualization table to translate all virtual ids by real ids .",
    "next , we recall how a remote node received a virtual  i d in the first place .",
    "the infiniband specification solves this bootstrapping problem by requiring the application to pass these three ids to the remote node through some out - of - band mechanism .",
    "when the application employs this out - of - band mechanism , the remote node will `` see '' the virtual ids that the plugin passed back to the application upon completion of an infiniband call .",
    "the solution chosen for the infiniband plugin is that assigns a virtual  i d , which is the same as the real  i d at the time of the initial creation of the infiniband object .",
    "after restart , the infiniband hardware may change the real  i d . at the time of restart",
    ", the plugin uses the dmtcp coordinator and the publish - subscribe feature to exchange the new real  ids , associated with a given virtual  i d .",
    "since the application continues to see only the virtual  ids , the plugin can continue to translate between virtual and real  ids through any wrapper by which the application communicates to the infiniband hardware ( see figure  [ fig : qp ] .",
    "( a subtle issue can arise if a queue pair or memory region is created after restart .",
    "this is a rare case .",
    "although we have not seen this in the current work , section  [ sec : limitations ] discusses two possible solutions . )      next , the case of rkeys ( remote memory region keys ) poses a particular problem that does not occur for queue pair numbers or local ids .",
    "this is because an rkey is guaranteed unique by infiniband only with respect to the protection domain within which it was created .",
    "thus , if a single infiniband node has received rkeys from many remote nodes , then the rkeys for two different remote nodes may conflict .",
    "normally , infiniband can resolve this conflict because a queue pair must be specified in order to send or receive a message .",
    "the local queue pair number determines a unique queue pair number on the remote node .",
    "the remote queue pair number then uniquely determines an associated protection domain  @xmath0 . with the remote  @xmath0 ,",
    "all rkeys are unique .",
    "hence , the infiniband driver on the remote node uses the ( @xmath0 , rkey ) pair , to determine a unique memory address on the remote node .    in the case of the infiniband plugin ,",
    "the vrkey and rkey are identical if no restart has taken place .",
    "( it is only after restart that the rkey may change , for a given vrkey ) .",
    "hence , prior to the first checkpoint , translation from vrkey to rkey is trivial .    after a restart",
    ", the infiniband plugin must employ a strategy motivated by that of the infiniband driver . in a call to ibv_post_send",
    ", the target application will provide both a virtual queue pair number and a virtual rkey ( vrkey ) .",
    "unlike infiniband , the plugin must translate the vrkey into the real rkey on the local node .",
    "however , during a restart , each node has published its locally generated rkey , the corresponding  @xmath0 ( as a globally unique i d ; see above ) , and the corresponding vrkey .",
    "similarly , each node has published the virtual queue pair number and corresponding  @xmath0 for any queue pair generated on that node .",
    "each node has also subscribed to the above information published by all other nodes .",
    "hence , the local node is aware of the following through publish - subscribe during restart : @xmath1    the call to ibv_post_send provided the local virtual qp_num , and the vrkey .",
    "this allows us to translate into the remote virtual qp_num , and hence the remote real qp_num .",
    "this allows us to derive the globally unique @xmath0 from the tuples above .",
    "the @xmath0 and @xmath2 together then allow us to use the second tuple to derive the necessary rkey , to be used when calling the infiniband hardware .",
    "recall that dmtcp copies and restores all of user - space memory . in reviewing figure  [ fig : qp ] ,",
    "one notes that the user - space memory includes a low - level device - dependent driver in user space . if , for example , one checkpoints on a cluster partition using intel / qlogic , and if one restarts on a mellanox partition , then the mellanox low - level driver will be missing .",
    "this presents a restriction for heterogeneous computing centers in the current implementation .    in future work",
    ", we will consider one of two alternative implementations .",
    "first , it is possible to implement a generic `` stub '' driver library , which can then dispatch to the appropriate device - dependent library .",
    "second , it is possible to force the infiniband library to re - initialize itself by restoring the pre - initialization version of the infiniband library data segment , instead of the data segment as it existed just prior to checkpoint .",
    "this will cause the infiniband library to appear to be uninitialized , and it will re - load the appropriate device - dependent library .",
    "another issue is that the infiniband hardware may post completions to the sender and receiver at slightly different times .",
    "thus , after draining the completion queue , the plugin waits for a fraction of a second , and then drains the completion queue one more time .",
    "this is repeated until no completions are seen in the latest period .",
    "thus , correctness is affected only if the infiniband hardware posts corresponding completions relatively far apart in time , which is highly unlikely .",
    "( note that this situation occurs in two cases : infiniband send - receive mode ; and infiniband rdma mode for the special case of ibv_post_send while setting the immediate data flag . )    in a related issue , when using the immediate data flag or the inline flag in the rdma model , a completion is posted only on the receiving node .",
    "these flags are intended primarily for applications that send small messages .",
    "hence , the current implementation sleeps for a small amount of time to ensure that such messages complete .",
    "a future implementation will use the dmtcp coordinator to complete the bookkeeping concerning messages sent and received , and will continue to wait if needed .",
    "next , the current implementation does not support unreliable connections ( the analog of udp for tcp / ip ) .",
    "most target applications do not use this mode , and so this is not considered a priority .    a small memory leak on",
    "the order of hundreds of bytes per restart can occur because the memory for the queue pair struct and other data structures generated by by infiniband driver are not recovered .",
    "while techniques exist to correct this issue , it is not considered important , given the small amount of memory .",
    "( note that this is not an issue for registered or pinned memory , since on restart , a fresh process does not have any pinned memory . )",
    "the implementation described here can be viewed as interposing a shadow device driver between the end user s code and the true device driver .",
    "this provides an opportunity to virtualize the fields of the queue pair struct seen by the end user code .",
    "thus , the infiniband driver is modelled without the need to understand its internals .",
    "this is analogous the idea of using a shadow kernel device by swift  @xcite . in that work ,",
    "after a catastrophic failure by the kernel device driver , the shadow device driver was able to take over and place the true device driver back in a sane state . in a similar manner , restarting on a new host with a new hca adapter can be viewed as a catastrophic failure of the infiniband user - space library .",
    "our virtual queue pair along with the log of pending posts and modifications to the queue pair serves as a type of shadow device driver .",
    "this allows us to place back into a sane state the hca hardware , the kernel driver and the device - dependent user - space driver .",
    "this work is based on dmtcp ( distributed multithreaded checkpointing )  @xcite .",
    "the dmtcp project began in 2004  @xcite . with the development of dmtcp versions  2.x ,",
    "it has emphasized the use of plugins  @xcite for more modular maintainable code .",
    "currently , blcr  @xcite is widely used as one component of an mpi dialect - specific checkpoint - restart service .",
    "this design is fundamentally different , since an mpi - specific checkpoint - restart service calls blcr , whereas dmtcp transparently invokes an arbitrary mpi implementation .",
    "since blcr is kernel - based , it provides direct support only on one computer node .",
    "most mpi dialects overcome this in their checkpoint - restart service by disconnecting any network connections , delegating to blcr the task of a single - node checkpoint , and then reconnect the network connection . among the mpi implementations using blcr",
    "are open  mpi  @xcite ( crcp coordination protocol ) , lam / mpi  @xcite , mpich - v  @xcite , and mvapich2  @xcite .",
    "other mpi implementations provide their own analogs  @xcite . in some cases , an mpi implementation may support an _ application - initiated _ protocol in combination with blcr ( such as self  @xcite ) . for application - initiated checkpointing , the application writer guarantees that there are no active messages at the time of calling for a checkpoint .",
    "some recommended technical reports for further reading on the design of infiniband are  @xcite , along with the earlier introduction to the c  api  @xcite .",
    "the report  @xcite was a direct result of the original search for a clean design in checkpointing over infiniband , and @xcite represents a talk on interim progress .",
    "in addition to dmtcp , there have been several packages for transparent , distributed checkpoint - restart of applications running over tcp sockets  @xcite .",
    "the first two packages ( @xcite and  @xcite ) are based on the zap package  @xcite .",
    "the berkeley language unified parallel  c ( upc )  @xcite is an example of a pgas language ( partitioned global address space ) .",
    "it runs over gasnet  @xcite and evolved from experience with earlier languages for dsm ( distributed shared memory ) .",
    "the experiments are divided into four parts : scalability with more nodes in the case of open  mpi ( section  [ sec : scalability ] ) ; comparison between blcr and dmtcp for mpi - based computations ( section  [ sec : comparison ] ) ; tests on unified parallel  c ( upc ) ( section  [ sec : upc ] ) ; and demonstration of migration from infiniband to tcp ( section  [ sec : ib2tcp ] ) .",
    "the lu benchmark from the nas parallel benchmarks was used except for tests on upc , for which there was no available port of the nas lu benchmark .",
    "note that the default behavior of dmtcp is to compress checkpoint images using gzip .",
    "all of the experiments used the default gzip invocation , unless otherwise noted .",
    "as shown in section  [ sec : scalability ] , gzip produced almost no additional compression , but it is also true that the additional cpu time in running gzip was less than 5% of the checkpoint and restart times .",
    "for dmtcp , all checkpoints are to a local disk ( local to the given computer node ) , except as noted .",
    "open  mpi / blcr checkpointing invokes an additional step to copy all checkpoint images to a single , central node .",
    "[ [ experimental - configuration . ] ] experimental configuration .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    two clusters were employed for the experiments described here .",
    "sections  [ sec : comparison ] and  [ sec : upc ] refer to a cluster at the center for computational research at the university of buffalo .",
    "it uses slurm as its resource manager , and a common nfs - mounted filesystem .",
    "each node is equipped with either a mellanox or qlogic ( now intel ) hca , although a given partition under which an experiment was run was always homogeneous ( either all mellanox or all qlogic ) .",
    "the operating system is redhat enterprise linux  6.1 with linux kernel version  2.6.32 .",
    "experiments were run using one core per computer .",
    "hence , the mpi rank was equal to the number of computers , and so mpi processes were on separate computer nodes .",
    "since the memory per node was not fixed , we set the memory limit per cpu to 3  gb . each cpu has a clock rate ranging from 2.13  ghz to 2.40  ghz , and the specific cpus can vary .    in terms of software , we used open  mpi  1.6 , dmtcp  2.1 ( or a pre - release version in some cases ) , berkeley upc  2.16.2 , and blcr  0.8.3 , respectively .",
    "open  mpi was run in its default mode , which typically used the rdma model for infiniband , rather than the send - receive model .",
    "although , dmtcp version  2.1 was used , the plugin included some additional bug fixes appearing after that dmtcp release .",
    "for the applications , we used berkeley upc ( unified parallel  c ) version  2.18.0 . the nas parallel benchmark was version  3.1 .",
    "tests of blcr under open  mpi were run by using the open  mpi checkpoint - restart service  @xcite .",
    "tests of dmtcp for open  mpi did not use the checkpoint - restart service . under both dmtcp and blcr , each node produced a checkpoint image file in a local , per - node scratch partition . however , the time reported for blcr includes the time to copy all checkpoint images to a single node , preventing a direct comparison .    in the case of blcr ,",
    "the current open  mpi checkpoint - restart service copies each local checkpoint - image to a central coordinator process .",
    "unfortunately , this serializes part of the parallel checkpoint .",
    "hence , checkpoint times for blcr are not directly comparable to those for dmtcp .    in the case of dmtcp",
    ", all tests were run using the dmtcp default parameters , which include dynamic gzip compression .",
    "gzip compression results in little compression for numerical data , and increases the checkpoint time several fold .",
    "further , dmtcp is checkpointing to a local `` tmp '' directory on each node .",
    "blcr checkpoints to a local `` tmp '' directory , and then the open mpi service copies them to a central node that saves them in a different `` tmp '' directory .",
    "scalability tests for the dmtcp plugin are presented , using open  mpi as the vehicle for these tests .",
    "all tests in this section were run at the massachusetts green high - performance computing center ( mghpcc ) .",
    "intel xeon e5 - 2650 cpus running at 2  ghz .",
    "each node is dual - cpu , for a total of 16  cores per node .",
    "it employs mellanox hca adapters .",
    "in addition to the front - end infiniband network , there is a lustre back - end network .",
    "all checkpoints are written to a local disk , except for a comparison with lustre , described later in table  [ tbl : lustre ] .",
    "table  [ tbl : scalability ] presents a study of scalability for the infiniband plugin .",
    "the nas mpi test for lu is mmployed . for a given number of processes , each of classes c , d , and e",
    "are tested provided that the running time for the test is of reasonable length .",
    "the overhead when using dmtcp is analyzed further in table  [ tbl : overhead ] .",
    ".[tbl : scalability ] demonstration of scalability : running times without dmtcp ( natively ) and with dmtcp [ cols= \" < , > , > , > \" , ]",
    "the current work supports only a homogeneous infiniband architecture . for example",
    ", one can not checkpoint on a node using an intel / qlogic hca and restart on a different node that uses a mellanox hca .",
    "this is because the checkpoint image already includes a low - level library to support a particular hca .",
    "future work will extend this implementation to support heterogeneous infiniband architectures by re - loading the low - level library , as described in section  [ sec : limitations ] .",
    "further , the experimental timings reported here did not employ any particular tuning techniques .",
    "there are opportunities to reduce the run - time overhead by reducing the copying of buffers .",
    "[ [ avoiding - conflict - of - virtual - ids - after - restart ] ] avoiding conflict of virtual ids after restart : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in typical mpi implementations memory region keys ( rkey ) , queue pair numbers ( qp_num ) , and local ids ( lid ) are all exchanged out - of - band . since virtualized ids are passed to the target application",
    ", it is the virtualized ids that are passed out - of - band .",
    "the remote plugin is then responsible for translating the virtual ids to the real ids known to the infiniband hardware , on succeeding infiniband calls .",
    "the current implementation ensures that this is possible , and that there are no conflicts prior to the first checkpoint , as described in section  [ sec : virtualization ] . in typical infiniband applications ,",
    "queue pairs are created only during startup , and so all rkeys , qp_nums and lids will be assigned prior to the first checkpoint . however , it is theoretically possible for an application to create a new queue pair , memory region , or to query its local i d after the first restart .",
    "the current implementation assigns the virtual  i d to be the same as the real  i d at the time of the initial creation of the infiniband object .",
    "( after restart , the infiniband hardware may assign a different real  i d , but the virtual  i d for that object will remain the same . )",
    "if an object is created after restart , the real  i d assigned by infiniband may be the same as for an object created prior to checkpoint .",
    "this would create a conflict of the corresponding virtual  ids .",
    "two solutions to this problem are possible .",
    "the simplest is to use dmtcp s publish - subscribe feature to generate globally unique virtual rkeys , and update a global table of virtual - to - real rkeys . in particular , one could use the existing implementation before the first checkpoint , and then switch to a publish - subscribe implementation after restart .",
    "a second solution is to choose the virtual rkeys in a globally unique manner , similarly to the globally unique protection domain ids of the current plugin .",
    "a new approach to distributed transparent checkpoint - restart over infiniband has been demonstrated .",
    "this direct approach accommodates computations both for mpi and for upc ( unified parallel  c ) .",
    "the approach uses a mechanism similar to that of a shadow device driver  @xcite . in tests on the nas lu parallel benchmark ,",
    "a run - time overhead of between 0.7% and 1.9% is found on a computation with up to 2,048 mpi processes .",
    "startup overhead is up to 13  seconds , and grows as the cube root of the number of mpi processes .",
    "checkpoint times are roughly proportional to the total size of all checkpoint images on a single computer node . in one example , checkpoint times varied by a factor of  6.5 ( from 232  seconds to 36  seconds ) , depending on whether checkpoint images were written to a local disk or to a faster , lustre - based back - end . in each case , 16  mpi processes per node ( 512 processes in all ) wrote a total of approximately 5.8  gb per node .",
    "further , the new approach provides a viable checkpoint - restart mechanism for running upc natively over infiniband  something that previously did not exist .",
    "finally , an ib2tcp plugin was demonstrated that allows users to checkpoint a computation on a production cluster under infiniband , and to restart on a smaller debug cluster under ethernet  in a manner suitable for attaching an interactive debugger .",
    "this mode is analogous to the interconnection - agnostic feature of the open  mpi checkpoint restart service  @xcite , but it has an added benefit in that the production cluster and the debug cluster do not have to have the same linux kernel image .",
    "we are grateful to facilities provided at several institutions with which to test over a variety of configurations .",
    "we would like to thank : shawn matott (  of buffalo , development and benchmarking facilities ) ; henry neeman ( oklahoma university , development facilities ) ; larry owen and anthony skjellum ( the university of alabama at birmingham , facilities based on nsf grant cns-1337747 ) ; and the massachusetts green high performance computing center ( facilities for scalability testing ) .",
    "dotan barak provided helpful advice on the implementation of openfabrics infiniband .",
    "jeffrey  squyres and joshua hursey provided helpful advice on the interaction of open  mpi and infiniband .",
    "artem polyakov provided advice on using the dmtcp batch - queue ( resource manager ) plugin .",
    "j.  ansel , g.  cooperman , and k.  arya . : scalable user - level transparent checkpointing for cluster computations and the desktop . in _ proc . of ieee international parallel and distributed processing symposium ( ipdps-09 , systems track)_. ieee press , 2009 . published on  cd ; version also available at http://arxiv.org/abs/cs.dc/0701037 ; software available at http://dmtcp.sourceforge.net .",
    "t.  bedeir .",
    "building an rdma - capable application with ib verbs .",
    "technical report , http://www.hpcadvisorycouncil.com/ , august 2010 . http://www.hpcadvisorycouncil.com / pdf / building - an - rdma - capable- application-with-ib-verbs.pdf .",
    "d.  bonachea .",
    "et specification ,  v1.1 .",
    "technical report ucb / csd-02 - 1207 , u. of california , berkeley , october 2002 .",
    "/ techreports / ucb / text / csd-02 - 1207.% pdf[http://digitalassets.lib.berkeley.edu / techreports / ucb / text / csd-02 - 1207.% pdf ] .",
    "w.  w. carlson , j.  m. draper , d.  e. culler , k.  yelick , e.  brooks , and k.  warren .",
    "introduction to upc and language specification .",
    "technical report ccs - tr-99 - 157 , ida center for computing sciences , 1999 .",
    "http://upc.lbl.gov/publications/upctr.pdf .",
    "g.  cooperman , j.  ansel , and x.  ma .",
    "adaptive checkpointing for master - worker style parallelism ( extended abstract ) . in _ proc . of 2005 ieee computer society international conference on cluster computing_. ieee press , 2005 .",
    "conference proceedings on cd .",
    "g.  cooperman , j.  ansel , and x.  ma .",
    "transparent adaptive library - based checkpointing for master - worker style parallelism . in _ proceedings of the 6@xmath3 ieee international symposium on cluster computing and the grid ( ccgrid06 ) _ , pages 283291 , singapore , 2006 .",
    "ieee press .    .",
    "tutorial for dmtcp plugins , accessed dec .  10 , 2013 .",
    "http://sourceforge.net / p / dmtcp / code / head / tree / trunk / doc / plugin - tutorial% .pdf[http://sourceforge.net / p / dmtcp / code / head / tree / trunk / doc / plugin - tutorial% .pdf ] .      t.  el - ghazawi and f.  cantonnet . performance and potential : a npb experimental study . in _ proceedings of the 2002 acm / ieee conference on supercomputing _ , supercomputing 02 , pages 126 , los alamitos , ca , usa , 2002 .",
    "ieee computer society press .",
    "q.  gao , w.  yu , w.  huang , and d.  k. panda .",
    "application - transparent checkpoint / restart for mpi programs over infiniband . in _",
    "icpp 06 : proceedings of the 2006 international conference on parallel processing _ ,",
    "pages 471478 , washington , dc , usa , 2006 .",
    "ieee computer society .",
    "j.  hursey , t.  i. mattox , and a.  lumsdaine . interconnect agnostic checkpoint / restart in open mpi . in _",
    "hpdc 09 : proceedings of the 18th acm international symposium on high performance distributed computing _ , pages 4958 , new york , ny , usa , 2009 .",
    "j.  hursey , j.  m. squyres , t.  i. mattox , and a.  lumsdaine .",
    "the design and implementation of checkpoint / restart process fault tolerance for open mpi . in _ proceedings of the 21@xmath4 ieee international parallel and distributed processing symposium ( ipdps ) / 12@xmath3 ieee workshop on dependable parallel , distributed and network - centric systems_. ieee computer society , march 2007 .",
    "g.  janakiraman , j.  santos , d.  subhraveti , and y.  turner .",
    "cruz : application - transparent distributed checkpoint - restart on standard operating systems . in _ dependable systems and networks ( dsn-05 ) _ , pages 260269 , 2005 .",
    "g.  kerr , a.  brick , g.  cooperman , and s.  bratus .",
    "checkpoint - restart : proprietary hardware and the ` spiderweb api ' , july 810 2011 .",
    "talk : abstract at http://recon.cx/2011/schedule/events/112.en.html ; video at https://archive.org/details/recon_2011_checkpoint_restart .",
    "p.  lemarinier , a.  bouteillerand , t.  herault , g.  krawezik , and f.  cappello .",
    "improved message logging versus improved coordinated checkpointing for fault tolerant mpi . in _",
    "cluster 04 : proceedings of the 2004 ieee international conference on cluster computing _ , pages 115124 , washington , dc , usa , 2004 .",
    "ieee computer society .",
    "s.  osman , d.  subhraveti , g.  su , and j.  nieh . the design and implementation of zap : a system for migrating computing environments . in _ prof . of 5@xmath3 symposium on operating systems design and implementation ( osdi-2002 )",
    "_ , 2002 .",
    "o.  o. sudakov , i.  s. meshcheriakov , and y.  v. boyko . : transparent checkpointing system for linux clusters . in _",
    "ieee international workshop on intelligent data acquisition and advanced computing systems : technology and applications _ , pages 159164 , 2007 .",
    "software available at http://freshmeat.net / projects / chpox/.    m.  m. swift , m.  annamalai , b.  n. bershad , and h.  m. levy . recovering device drivers . in _ proceedings of the 6th conference on symposium on operating systems design and implementation _ , osdi04 , berkeley , ca , usa , 2004 .",
    "usenix association ."
  ],
  "abstract_text": [
    "<S> infiniband is widely used for low - latency , high - throughput cluster computing . saving the state of the infiniband network as part of distributed checkpointing </S>",
    "<S> has been a long - standing challenge for researchers . because of a lack of a solution , </S>",
    "<S> typical mpi implementations have included custom checkpoint - restart services that `` tear down '' the network , checkpoint each node as if the node were a standalone computer , and then re - connect the network again . </S>",
    "<S> we present the first example of transparent , system - initiated checkpoint - restart that directly supports infiniband . </S>",
    "<S> the new approach is independent of any particular linux kernel , thus simplifying the current practice of using a kernel - based module , such as blcr . </S>",
    "<S> this direct approach results in checkpoints that are found to be faster than with the use of a checkpoint - restart service . </S>",
    "<S> the generality of this approach is shown not only by checkpointing an mpi computation , but also a native upc computation ( berkeley unified parallel  c ) , which does not use mpi . </S>",
    "<S> scalability is shown by checkpointing 2,048  mpi processes across 128  nodes ( with 16  cores per node ) . </S>",
    "<S> in addition , a cost - effective debugging approach is also enabled , in which a checkpoint image from an infiniband - based production cluster is copied to a local ethernet - based cluster , where it can be restarted and an interactive debugger can be attached to it . </S>",
    "<S> this work is based on a plugin that extends the dmtcp ( distributed multithreaded checkpointing ) checkpoint - restart package . </S>"
  ]
}