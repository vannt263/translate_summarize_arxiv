{
  "article_text": [
    "estimation of dependency networks for high dimensional datasets is especially desirable in many scientific areas such as biology and sociology .",
    "gaussian graphical model ( ggm ) has proven to be a very powerful formalism to infer dependence structures of various datasets .",
    "ggm is an equivalent representation of conditional dependence of jointly gaussian random variables .",
    "inference on the structure of ggm is challenging when the dimension is greater than the sample size .",
    "many classical methods do not work any more .",
    "let @xmath0 be a multivariate normal random vector with mean @xmath1 and covariance matrix @xmath2 .",
    "ggm is a graph @xmath3 , where @xmath4 is the set of vertices and @xmath5 is the set of edges between vertices .",
    "there is an edge between @xmath6 and @xmath7 if and only if @xmath6 and @xmath7 are conditional dependent given @xmath8 .",
    "it is well - known that estimating the structure of ggm is equivalent to recovering the support of precision matrix @xmath9 ; see lauritzen ( 1996 ) .",
    "the typical way on ggm estimation depends on regularized optimizations .",
    "the past decade has witnessed significant developments on the regularization method for various statistical problems .",
    "for example , in the context of variable selection , tibshirani ( 1996 ) introduced lasso , which selects important variables in regression by solving the least squares optimization with the @xmath10 regularization .",
    "graphical - lasso , an extension of lasso to ggm estimation , was introduced by yuan and lin ( 2007 ) , friedman et al .",
    "( 2008 ) and daspremont et al .",
    "graphical - lasso estimates the support of precision matrix by an @xmath10 penalized likelihood method .",
    "theoretical properties of graphical - lasso can be found in rothman et al .",
    "( 2008 ) and ravikumar et al .",
    "other methods , based on the @xmath10-minimization technique , can be found in meinshausen and buhlmann ( 2006 ) , yuan ( 2010 ) , zhang ( 2010 ) , cai , et al .",
    "( 2011 ) , liu , et al .",
    "( 2012 ) , xue and zou ( 2012 ) .",
    "the nonconvex penalties , such as scad function penalty ( fan et al .",
    "( 2009 ) ) , have also been considered in the context of ggm estimation .",
    "it is well known that regularization approaches often require the choice of tuning parameters .",
    "large tuning parameters often lead to sparse networks and they are powerless on finding the edges with small weights . on the other hand , small tuning parameters will generate many false edges and result in high false discovery rates . the theory of the precise relationship between the number of false edges and the tuning parameter is very difficult to be derived .",
    "a different way on ggm estimation relies on simultaneous tests @xmath11 for @xmath12 , where @xmath13 .",
    "an edge between @xmath6 and @xmath7 is included into the estimated network if and only if @xmath14 is rejected . when the dimension @xmath15 is fixed , drton and perlman ( 2004 ) proposed a multiple testing procedure to estimate ggm .",
    "they used the fisher s z transformations of the sample partial correlation coefficients ( spccs ) . a procedure on controlling the family - wise error",
    "was developed .",
    "however , when the dimension @xmath15 is greater than the sample size , the sample partial correlation matrix is not even well defined .",
    "hence , we do not have a natural pivotal estimator as spccs so that the asymptotic null distribution is easy to be derived . in high dimensional settings ,",
    "it becomes very challenging to estimate ggm by tests on the entries of precision matrix .    in the present paper ,",
    "we study the estimation of ggm by multiple tests ( [ test ] ) .",
    "we are particularly interested in high dimensional settings .",
    "the false discovery rate ( fdr ) is a useful measure on evaluating the performance of ggm estimation .",
    "we will introduce a procedure called ggm estimation with fdr control ( gfc ) .    a basic step in hypothesis tests",
    "is the construction of test statistics .",
    "the sample partial correlation coefficients are not well defined when @xmath16 .",
    "hence , we introduce new test statistics suitable for high dimensional settings . the new test statistics are based on a bias correction version of the sample covariance coefficients of residuals .",
    "they are shown to be asymptotically normal distributed under some sparsity conditions on @xmath17 .",
    "in addition to new test statistics , gfc carries out large - scale tests simultaneously .",
    "to this end , an adjustment for significance levels is necessary . in this paper",
    ", we develop a multiple testing procedure with an adjustment for significance levels and it controls the false discovery rate .",
    "the proposed procedure thresholds test statistics directly rather than p - values which were widely used ( cf .",
    "benjamini and hochberg ( 1995 ) ) .",
    "it is convenient for us to develop novel theoretical properties on fdr .",
    "we show that gfc method controls both fdr and false discovery proportion ( fdp ) asymptotically .",
    "in addition to its desirable theoretical properties , gfc method is computationally very attractive for high dimensional data .",
    "the computational cost is the same as the neighborhood selection method by meinshausen and buhlmann ( 2006 ) or the clime method by cai , et al .",
    "we only need to solve @xmath15 regression equations with lasso or dantzig selector .",
    "numerical performance of gfc is investigated by simulated data .",
    "results show that the procedure performs favorably in controlling fdr and fdp .",
    "the rest of the paper is organized as follows . in section 2.2 ,",
    "we introduce new test statistics for conditional dependence .",
    "gfc procedure is introduced in section 2.3 . in section 3",
    ", we give limiting distributions of our test statistics .",
    "theoretical results on gfc are also stated .",
    "since gfc needs initial estimations of regression coefficients , we provide their detailed implementations in section 4 .",
    "numerical performance of the procedure is evaluated by simulation studies in section 5 .",
    "the proofs of main results are delegated to section 6 .",
    "we begin this section by introducing basic notations . for any vector @xmath18 ,",
    "let @xmath19 denote @xmath20 dimensional vector by removing @xmath21 from @xmath22 .",
    "for any @xmath23 matrix @xmath24 , let @xmath25 denote the @xmath26-th row of @xmath24 with its @xmath27th entry being removed and @xmath28 denote the @xmath27-th column of @xmath24 with its @xmath26th entry being removed .",
    "@xmath29 denote a @xmath30 matrix by removing the @xmath26-th row and @xmath27-th column of @xmath24 . throughout ,",
    "define @xmath31 , @xmath32 and @xmath33 . for a matrix @xmath34",
    ", we define the element - wise @xmath35 norm @xmath36 , the spectral norm @xmath37 and the matrix @xmath38 norm @xmath39 .",
    "let @xmath40 and @xmath41 denote the largest eigenvalue and the smallest eigenvalue of @xmath2 respectively .",
    "@xmath42 denotes a @xmath43 identity matrix .",
    "let @xmath44 and @xmath45 .",
    "it is well known that , for @xmath46 , we can write @xmath47 where @xmath48 is independent of @xmath49 , @xmath50 and @xmath51 ; see anderson ( 2003 ) . the regression coefficients vector @xmath52 and the error terms @xmath53 satisfy @xmath54 we estimate ggm by recovering the support of @xmath55 , the covariance matrix of @xmath56 .      in this subsection , we introduce new test statistics for @xmath14 .",
    "let @xmath57 , where @xmath58 , @xmath59 , be independent and identically distributed random samples from @xmath60 . by ( [ reg ] ) , we can write @xmath61 where @xmath62 is the @xmath63-th row of @xmath64 with its @xmath26th entry being removed and @xmath65 is independent with @xmath62 .",
    "let @xmath66 be any estimators of @xmath52 satisfying @xmath67 and @xmath68 for some convergence rates @xmath69 and @xmath70 , where @xmath71 and @xmath72 .",
    "define the residuals by @xmath73 and the sample covariance coefficients between the residuals by @xmath74 where @xmath75 and @xmath76 .",
    "our test statistics are based on a bias correction of @xmath77 . to this end , for @xmath12 , define @xmath78 it should be noted that the index is @xmath79 in @xmath80 and @xmath81 is a @xmath20 dimensional vector .",
    "let @xmath82 where @xmath83 , @xmath84 and @xmath85 .",
    "we will prove that @xmath86 and under @xmath87 we will prove that @xmath88 note that , under @xmath14 , the limiting distribution in ( [ a1 ] ) does not depend on any unknown parameter . also , @xmath89 in probability , uniformly in @xmath90 .",
    "hence , for the hypothesis test @xmath14 , we shall use the following test statistic @xmath91    the estimators @xmath81 , @xmath92 , can be lasso estimators or dantizg selectors .",
    "theoretical results on the convergence rates in ( [ c1 ] ) have been proved by many papers under various conditions .",
    "for example , for dantizg selector , it can be proved by ( 46 ) and ( 47 ) that , under ( c1 ) in section 3 , ( [ c1 ] ) is satisfied when @xmath93 .",
    "the same conclusion holds for the lasso estimators .",
    "the detailed choices of @xmath81 will be given in section 4 .    * remark 1 .",
    "* there are a number of recent papers in the regression context where bias correction is used to derive p - values or confidence intervals for the regression coefficients in the high - dimensional case ; see zhang and zhang ( 2011 ) , bhlmann ( 2012 ) , van de geer , bhlmann and ritov ( 2013 ) , javanmard and montanari ( 2013 ) .",
    "when applying their methods in ggm estimation , we briefly discuss the difference between our method and theirs . for every @xmath26 , to get the p - values for the components of @xmath52 , their methods need to estimate the @xmath94 precision matrix of @xmath49 .",
    "so , to derive the p - values for all of the components of @xmath52 , @xmath92 , their methods need to estimate @xmath15 precision matrices with dimension @xmath94 .",
    "this requires a huge computational cost .",
    "our method only needs the initial estimators for @xmath52 .",
    "no additional precision matrix estimator is required .      with the new test statistic @xmath95",
    ", we can carry out @xmath96 tests ( [ test ] ) simultaneously and control fdr as follow .",
    "let @xmath97 be the threshold level such that @xmath14 is rejected if @xmath98 .",
    "the false discovery rate and false discovery proportion are defined by @xmath99.\\end{aligned}\\ ] ] a `` good '' threshold level @xmath97 makes as many as true alternative hypothesis be rejected and remains the fdr / fdp be controlled at a pre - specified level @xmath100 .",
    "so an ideal choice of @xmath97 is    @xmath101    where @xmath44 . in the definition of @xmath102 , @xmath97 is restricted to @xmath103 $ ] because @xmath104 by the proof in section 6 . since @xmath105 is unknown , we shall use an estimator of @xmath106 . as we will prove in section 6 ,",
    "an accurate approximation for @xmath106 is @xmath107 , where @xmath108 .",
    "moreover , @xmath105 can be estimated by @xmath96 due to the sparsity of @xmath17 .",
    "this leads to the following procedure .",
    "1.0 * gfc procedure . * calculate test statistics @xmath95 in ( [ d1 ] ) .",
    "let @xmath100 and @xmath109 where @xmath110 .",
    "if @xmath111 in ( [ a3 ] ) does not exist , then let @xmath112 . for @xmath12 , we reject @xmath14 if @xmath113 .    in gfc procedure , the estimators @xmath81 , @xmath92 , are needed . as mentioned earlier",
    ", we can use the lasso estimators or the dantizg selectors .",
    "both of them require the choice of tuning parameters . in section 4",
    ", we will propose a method on the choice of tuning parameters , which is particularly suitable for our multiple testing problem .    for general multiple testing problems ,",
    "liu and shao ( 2012 ) developed a procedure that controls the false discovery rate .",
    "they proposed to threshold test statistics directly rather than the true p - values as in benjamini and hochberg ( 1995 ) , because the true p - values are unknown in practice .",
    "additionally , to control fdr , the benjamini - hochberg method requires the independence or some kind of positive regression dependency between p - values .",
    "our test statistics do not meet such conditions . by thresholding the test statistics directly as in liu and shao ( 2012 )",
    ", we shall show that @xmath114 and @xmath115 in probability .",
    "it should be pointed out that liu and shao ( 2012 ) imposed the dependence condition among the test statistics . in ggm estimation , it is more natural to impose the dependence condition on the precision matrix . to this end , we need many novel techniques in the proof .",
    "in this section , we will show that gfc procedure can control the false discovery rate asymptotically at any pre - specified level . + * ( c1 ) .",
    "* let @xmath116 .",
    "suppose that @xmath117 and @xmath118 for some constant @xmath119 .",
    "assume that @xmath120 .",
    "since @xmath121 , ( c1 ) implies that @xmath122 and @xmath123 .",
    "we give the asymptotic distribution of @xmath95 , which is useful in testing a single @xmath124 @xmath125 .",
    "suppose that ( c1 ) holds .",
    "let @xmath81 be any estimator satisfying ( [ bt1 ] ) , ( [ bt2 ] ) and ( [ c1 ] ) .",
    "then , we have @xmath126 as @xmath127 , where the convergence in distribution is uniformly in @xmath12 .",
    "let the false discovery proportion and false discovery rate of gfc be defined by @xmath128 recall that @xmath44 .",
    "let @xmath129 be the cardinality of @xmath105 and @xmath130 .",
    "for a constant @xmath131 and @xmath92 , define @xmath132 theorem [ th1 ] shows that gfc controls fdp and fdr at level @xmath133 asymptotically .",
    "[ th1 ] let @xmath134 for some @xmath135 .",
    "suppose that for some @xmath136 , @xmath137 assume that @xmath138 for some @xmath139 and @xmath81 satisfies ( [ bt1 ] ) , ( [ bt2 ] ) and @xmath140 under ( c1 ) and @xmath141 for some @xmath142 and @xmath131 , we have @xmath143 as @xmath127 .",
    "the dimension @xmath15 can be much larger than the sample size because @xmath144 can be arbitrarily large . note that @xmath138 is a natural condition . if @xmath145 , then almost all of @xmath146 are nonzero .",
    "hence , rejecting all the hypothesis tests leads to @xmath147 .",
    "the condition @xmath141 is also mild .",
    "for example , if @xmath148 for some @xmath149 and @xmath17 is a @xmath150-sparse matrix with @xmath151 ( i.e. the number of nonzero entries in each row is no more than @xmath150 ) , then this condition holds .",
    "the sparsity @xmath151 is often imposed in the literature on precision matrix estimation .",
    "the technical condition ( [ a5 ] ) is used to ensure @xmath152 which is almost necessary for @xmath153 in probability .",
    "we believe ( [ c2 ] ) is nearly necessary for the false discovery proportion @xmath154 in probability .",
    "on the other hand , the condition for controlling fdr may be weaker than that for controlling fdp .",
    "even if ( [ c2 ] ) is violated , the false discovery rate may still be controlled at level @xmath133 .",
    "hence , it is possible that ( [ a5 ] ) is not needed for fdr results . in addition , ( [ a5 ] ) is not strong because the total number of hypothesis tests is @xmath96 and we only require a few standardized off - diagonal entries of @xmath17 have magnitudes exceeding @xmath155 .",
    "gfc requires to choose the estimators of @xmath52 .",
    "there are lots of literature on the estimation of high dimensional regression coefficients . in this paper",
    ", we use the popular dantizg selector and lasso estimator . some other recent procedures such as scaled - lasso ( sun and zhang , 2012 ) and square - root lasso ( belloni , chernozhukov and wang , 2011 ) can also be used and similar theoretical results as proposition [ pro2 ] and [ pro3 - 3 ] can be established .    * dantizg selector for @xmath81 .",
    "* dantizg selector estimates @xmath81 by solving the following optimization problems @xmath156 for @xmath92 , where @xmath157 , @xmath158 and @xmath159 for @xmath136 , where @xmath160 .",
    "we can let @xmath161 which is fully specified and has theoretical interest . for finite sample sizes , we will propose a more useful data - driven choice for @xmath162 in ( [ ats ] ) .",
    "[ pro2 ] suppose that ( c1 ) holds and @xmath163 .",
    "for @xmath161 in ( [ so1 ] ) , we have @xmath164 , @xmath92 , satisfy ( [ bt1 ] ) , ( [ bt2 ] ) and ( [ cd2 ] ) .    * lasso estimator for @xmath81 . *",
    "the coefficients @xmath81 can be estimated by lasso as follow : @xmath165 where @xmath166 the following proposition shows that for any @xmath167 , ( [ cd2 ] ) is satisfied .",
    "the data - driven choice for @xmath162 is given in ( [ ats ] ) .",
    "[ pro3 - 3 ] suppose that ( c1 ) holds and @xmath163 .",
    "for any @xmath167 in ( [ so2 - 2 ] ) , we have @xmath168 , @xmath92 , satisfy ( [ bt1 ] ) , ( [ bt2 ] ) and ( [ cd2 ] ) .",
    "* data - driven choice of @xmath162 .",
    "* as in many regularization approaches , the choice @xmath169 is often large . hence , in this paper , we propose to select @xmath162 adaptively by data .",
    "we let @xmath168 be the solution to ( [ so1 ] ) or ( [ so2 - 2 ] ) and then obtain the statistics @xmath170 , @xmath12 .",
    "as noted in section 2.3 , gfc works because for good estimators @xmath168 , @xmath92 , @xmath171 will be close to @xmath172 .",
    "hence , an oracle choice of @xmath162 can be @xmath173 where @xmath174 .",
    "@xmath105 is unknown , however .",
    "since @xmath17 is sparse , @xmath175 is close to @xmath96 .",
    "so a good choice of @xmath162 should minimize the following error @xmath176 where @xmath177 is a fixed number bounded away from zero . the constraint @xmath178 aims to ensure the nonzero entries part @xmath179 . in our choice , we let @xmath180 .",
    "this leads to the final choice of @xmath162 by discretizing the integral as follow : @xmath181 where @xmath182 is an integer number that can be pre - specified .",
    "finally , we use @xmath183 as the estimator of @xmath52 .",
    "deriving theoretical properties for @xmath184 is important .",
    "we leave this as a future work .",
    "in this section , we carry out simulations to examine the performance of gfc by the following graphs .    * * band graph*. @xmath185 , where @xmath186 , @xmath187 , @xmath125 for @xmath188 .",
    "@xmath17 is a @xmath189-sparse matrix . *",
    "* hub graph*. there are @xmath190 rows with sparsity @xmath191 .",
    "the rest every row has sparsity @xmath192 .",
    "to this end , we let @xmath193 , @xmath194 for @xmath195 and @xmath196 , @xmath197 . the diagonal @xmath198 and others entries are zero . finally , we let @xmath199 to make the matrix be positive definite . *",
    "* erds - rnyi random graph*. there is an edge between each pair of nodes with probability @xmath200 independently .",
    "let @xmath201 , where @xmath202 is the uniform random variable and @xmath203 is the bernoulli random variable with success probability 0.05 .",
    "@xmath204 and @xmath203 are independent . finally , we let @xmath199 such that the matrix is positive definite .    for each model",
    ", we generate @xmath205 random samples with @xmath206 , @xmath207 and @xmath208 .",
    "we use the dantizg selector and lasso to estimate @xmath52 in gfc and denote the corresponding procedures by gfc - dantizg and gfc - lasso .",
    "the tuning parameter @xmath209 is given in section 4 with @xmath210 .",
    "the simulation results are based on 100 replications . as we can see from table 1 , the fdrs of gfc - dantizg for band graph and erds - rnyi ( e - r ) random graph are close to @xmath133 .",
    "the fdrs for hub graph are somewhat smaller than @xmath133 . for all three graphs , the fdrs can be effectively controlled below the level @xmath133 .",
    "similarly , gfc - lasso can control fdr at the level @xmath133 .",
    "the fdps of gfc - dantizg in 100 replications are plotted in figure 1 with @xmath211 .",
    "for the reason of space , we give the other figures for @xmath212 and gfc - lasso in the supplemental material liu ( 2013 ) .",
    "we can see from these figures that most of fdps are concentrated around the fdrs .    in figure 2 , we plot the fdps for all gfc - dantizg estimators with @xmath211 , @xmath213 and @xmath214 , @xmath215 .",
    "the histograms of @xmath216 are plotted in figure 3 .",
    "we use @xmath217 to denote the false discovery rates for gfc - dantizg with @xmath218 . as we can see from figure 2 ,",
    "there always exist several @xmath27 such that @xmath217 are well controlled at level @xmath213 . from the histograms of @xmath216 in figure 3 , we see that @xmath216 in section 4 can always take the values of these @xmath27 s for all three graphs .",
    "similar phenomenon can be observed in gfc - lasso ; see the supplemental material liu ( 2013 ) .",
    "we examine the power of gfc on controlling fdr .",
    "based on 100 replications , the average powers are defined by @xmath219 we state the numerical results in table 2 .",
    "the power increases when @xmath133 increases . for the hub graph ,",
    "the powers are close to one . for the band graph , gfc - dantizg can also effectively detect the edges and gfc - lasso is more powerful than gfc - dantizg . for the erds - rnyi random graph , gfc has non - trivial powers when @xmath220 , @xmath221 and @xmath222 .",
    "the powers are low when @xmath223 .",
    "this mainly dues to the very small magnitude of @xmath146 .",
    "actually , all of @xmath224 belong to the interval @xmath225 when @xmath223 .",
    "so it is very difficult to detect such small nonzero entries .    finally , we compare gfc with the graphical lasso ( glasso ) which estimates the graph by solving the following optimization problem : @xmath226 as in rothman , et al .",
    "( 2008 ) , fan , feng and wu ( 2009 ) and cai , liu and luo ( 2011 ) , the tuning parameter @xmath227 is selected by the popular cross validation method . to this end",
    ", we generate another @xmath205 training samples from @xmath60 and let @xmath228 be the sample covariance matrix from the training samples .",
    "we choose the following the tuning parameter @xmath229 the empirical false discovery rates and the standard deviations are stated in table 3 .",
    "we can see that for all three graphs the fdrs of glasso are quite close to 1 .",
    "this indicates that glasso with the cross validation method fails to control the false discovery rate .",
    "we next examine the power of glasso .",
    "since the power of glasso depends on the choice of @xmath227 , we plot all of the fdrs and the average powers for @xmath230 with @xmath231 in figure 4 with @xmath211 . other figures for @xmath212 are given in the supplemental material liu ( 2013 ) . as we can see from these figures , for the band graph and er graph , the powers are quite low ( @xmath232 ) if the fdrs @xmath233 .",
    "hence , for these two graphs , gfc significantly outperforms glasso even we know the oracle choice of the tuning parameter for glasso .",
    "it is also interesting to see that , for the hub graph , the power of glasso is close to one even the fdrs are small .",
    "this phenomenon is similar to that of gfc which also performs quite well for the hub graph .",
    ".*empirical false discovery rates * [ cols=\"^,^,^,^,^,^,^,^,^ \" , ]     [ tb : simu3 ]",
    "put @xmath234 , where @xmath235 . recall the definitions of @xmath236 and @xmath237 in section 2.1 . note that @xmath238 for the last term in ( [ p1 ] ) , we have @xmath239 it is easy to show that , for any @xmath240 , there exists @xmath241 such that @xmath242 hence @xmath243 moreover , latexmath:[\\ ] ] which are the conditions of lemma [ le3 ] .",
    "* proof of proposition [ pro2 ] .",
    "* we first show that the true @xmath52 belongs to the region @xmath413 .",
    "it suffices to prove that @xmath414 uniformly in @xmath415 , with probability tending to one . by the independence between @xmath416 and @xmath417 ,",
    "we have @xmath418 since @xmath419 , we prove ( [ a25 ] ) . by the definition of @xmath81 ,",
    "@xmath420 for some @xmath139 .",
    "actually , the re assumption follows from @xmath421 and the inequality @xmath422 for any @xmath423 . by the proof of theorem 7.1 in bickel , ritov and tsybakov ( 2009 )",
    ", we obtain that @xmath424 and @xmath425 this implies proposition [ pro2 ] .",
    "* proof of proposition [ pro3 - 3 ] .",
    "* by the proof of proposition [ pro2 ] , we have for any @xmath167 and some @xmath426 , @xmath427 with probability tending to one . for a vector @xmath428 and an index set @xmath429 ,",
    "let @xmath430 be the vector with @xmath431 for @xmath432 and @xmath433 for @xmath434 .",
    "let @xmath435 be the support of @xmath52 . then by the proof of theorem 1 in belloni , chernozhukov and wang ( 2011 )",
    ", we can get @xmath436 for @xmath437 .",
    "also @xmath438 ) and ( [ tp2 ] ) hold for @xmath439 .",
    "cai , t. t. , liu , w. and xia , y. ( 2013 ) , two - sample covariance matrix testing and support recovery in high - dimensional and sparse settings .",
    "_ journal of the american statistical association _ , 108 : 265 - 277 .",
    "ravikumar , p. , wainwright , m. , raskutti , g. and yu , b. ( 2011 ) . high - dimensional covariance estimation by minimizing @xmath10-penalized log - determinant divergence .",
    "_ electronic journal of statistics _ 5 : 935 - 980 ."
  ],
  "abstract_text": [
    "<S> this paper studies the estimation of high dimensional gaussian graphical model ( ggm ) . typically , the existing methods depend on regularization techniques . as a result </S>",
    "<S> , it is necessary to choose the regularized parameter . </S>",
    "<S> however , the precise relationship between the regularized parameter and the number of false edges in ggm estimation is unclear . </S>",
    "<S> hence , it is impossible to evaluate their performance rigorously . in this paper </S>",
    "<S> , we propose an alternative method by a multiple testing procedure . based on our new test statistics for conditional dependence , </S>",
    "<S> we propose a simultaneous testing procedure for conditional dependence in ggm . </S>",
    "<S> our method can control the false discovery rate ( fdr ) asymptotically . </S>",
    "<S> the numerical performance of the proposed method shows that our method works quite well . </S>"
  ]
}