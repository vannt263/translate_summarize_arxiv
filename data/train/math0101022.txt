{
  "article_text": [
    "many problems in science and engineering are described by nonlinear differential equations whose solutions are too complicated to be properly resolved .",
    "the problem of predicting the evolution of systems that are not well resolved has been addressed by the present authors and others in @xcite .",
    "nothing can be predicted without some knowledge about the unresolved ( `` subgrid '' ) degrees of freedom . in the optimal prediction methods",
    "just cited it is assumed that one possesses , as one often does , prior statistical information about the system in the form of an invariant measure ; what is sought is a mean solution with respect to this prior measure , compatible with the information initially at hand as well as with the limitations on the computing power one can bring to bear .",
    "the simplest version of this idea , markovian optimal prediction , generates an approximating system of ordinary differential equations and works well for a time that depends on the degree of underresolution and on the uncertainty in the data .",
    "this version is optimal in the class of markovian approximations @xcite , but it eventually exhibits errors , because the influence of partial initial data on the distribution of the solutions weakens in time if the system is ergodic , and this loss of information is not captured in full , see @xcite . to obtain an accurate approximation of a subset of variables without solving the full problem requires the addition of a  memory \" term , and the resulting prediction scheme becomes a generalized langevin equation , similar to those in irreversible statistical mechanics @xcite .",
    "we present a general formalism for separating resolved and unresolved degrees of freedom , analogous to the nonlinear projection formalism of zwanzig @xcite but using the language of probability theory .",
    "we find a zero - th order approximate solution of the equation for the orthogonal unresolved dynamics and find its statistics by monte - carlo integration ; we use the results to construct a prediction scheme with memory .",
    "we apply the scheme to a simple model problem . in the conclusion",
    "we indicate how the construction is generalized to more complicated problems , and the new perspectives it opens for prediction as well as for irreversible statistical mechanics .",
    "consider a problem of the form @xmath0 where @xmath1 and @xmath2 are @xmath3-dimensional vectors ( @xmath3 may be infinite ) , with components @xmath4 and @xmath5 ; @xmath6 is time .",
    "when @xmath3 is finite ( [ eq : system ] ) is a system of ordinary differential equations .",
    "our goal is to calculate the average values of @xmath7 components of @xmath2 , @xmath8 without calculating all the components ; the average is over all the values that the missing , unresolved , components may assume ; our prior information allows us to make statistical statements about these missing components .",
    "we denote the phase space ( the vector space in which @xmath2 resides ) by @xmath9 ; in classical statistical physics this phase space is the @xmath10 dimensional space of coordinates and momenta @xmath11 , where @xmath12 is the number of particles ; the @xmath13 at time @xmath6 are then entries of the vector @xmath2 .",
    "a solution of equation ( [ eq : system ] ) is defined when an initial value @xmath14 is given ; to each initial condition @xmath15 corresponds a trajectory , @xmath16 ; the initial value @xmath15 is emphasized by this notation in view of its key role in what follows .",
    "a phase variable @xmath17 is a function on @xmath9 ; @xmath17 may be a vector , whose components are labeled as @xmath18 .",
    "a phase variable varies when its argument varies in time , so that a phase variable whose value at @xmath19 was @xmath20 acquires at time @xmath6 the value @xmath21 .",
    "it is useful to examine the evolution of @xmath17 in a more abstract setting : introduce an evolution operator @xmath22 for phase variables by the relation @xmath23 differentiation of ( [ eq : pullback ] ) with respect to time yields @xmath24 where @xmath25 , the liouvillian , is the linear differential operator @xmath26 .",
    "thus the phase variable @xmath27 can be calculated in either of two ways : ( i ) for each @xmath15 integrate to time @xmath6 the equations of motion @xmath28 with initial conditions @xmath29 and evaluate the phase variable @xmath17 at the point @xmath30 ; or ( ii ) solve the equation @xmath31 it is convenient to write @xmath32 ; we do not inquire here as to the conditions under which this symbolic notation can be taken literally .",
    "the significant thing about equation ( [ liouville ] ) is that it is linear .",
    "one can check from the definitions that @xmath33 , @xmath34 for any function @xmath35 , and @xmath36 . in this notation",
    ", the symbol @xmath17 standing alone refers to the data @xmath20 at @xmath19 ; the time dependence is described by the exponential .",
    "suppose that the initial data @xmath15 are drawn from a probability distribution @xmath37 ; each initial datum gives rise to a solution of equation ( [ eq : system ] ) and the measure @xmath37 evolves into a measure @xmath38 at time @xmath6 .",
    "the evolution of @xmath38 is defined by the conditions @xmath39 for all sufficiently smooth phase variables @xmath17 .",
    "we assume that the measure @xmath37 is invariant under the flow ( [ eq : system ] ) : @xmath40 .",
    "many systems have invariant measures , in particular , any hamiltonian system with hamiltonian @xmath41 leaves invariant the canonical measure with density @xmath42 , where @xmath43 is a normalization constant and @xmath44 is the variance of the samples , which in physics is the temperature .",
    "given a phase variable @xmath17 , we denote by @xmath45 $ ] the expected value of @xmath17 with respect to the invariant measure @xmath46 , @xmath47 = \\int_{{\\boldsymbol \\gamma}}u(x)\\,\\mu(dx ) .",
    "\\nonumber\\ ] ] we endow the space of phase variables with the inner product @xmath48 $ ] , which makes them elements of the hilbert space @xmath49 $ ] ( @xmath50 for brevity ) .",
    "the representations ( [ eq : system ] ) and ( [ liouville ] ) of the dynamics are equivalent ; in particular one can retrieve ( [ eq : system ] ) from ( [ liouville ] ) by setting @xmath51 and therefore @xmath52 ( the i - th coordinate function ) . in @xmath50 the liouvillian operator @xmath25 is antisymmetric .",
    "we wish to calculate the means of a small number of variables @xmath53 , averaged over all the values the other variables may take initially when they are drawn from the invariant distribution , without calculating any of these other variables .",
    "we write @xmath54 for the vector whose entries are the variables we actually calculate . to find equations for @xmath55 we need projections of @xmath50 on subspaces of functions of @xmath56 or of @xmath57 ; these projections are not unique and we shall consider two different ones :    \\(i ) consider the conditional expectation @xmath58 $ ] , where both @xmath17 and @xmath59 are phase variables ; it satisfies :    1 .",
    "@xmath58 $ ] is a function of @xmath17 ; 2 .",
    "@xmath58 $ ] is linear in @xmath59 : @xmath60 = \\alpha\\,{{\\mathbb e}}[v_1|u ] + \\beta\\,{{\\mathbb e}}[v_2|u ] .",
    "\\nonumber\\ ] ] 3 .",
    "@xmath58 $ ] is the best approximation of @xmath59 by a function of @xmath17 : @xmath61|^2 ] \\le { { \\mathbb e}}[|v - f(u)|^2 ] \\label{best}\\ ] ] for all functions @xmath35 .    see chung @xcite .    @xmath58 $ ] is the orthogonal projection of @xmath59 on the space of functions of @xmath17 , and we can write @xmath62 $ ] .",
    "this is the  nonlinear projection \" used in @xcite with a different interpretation , as well as in @xcite .",
    "ii)for a function @xmath63 in @xmath50 , define @xmath64 this is the  linear projection \" used in most of irreversible statistical mechanics ( see e.g. ( @xcite ) .",
    "clearly the projection based on conditional expectations puts more information into the subspace of functions of the smaller set of variables and is thus preferable for our purposes , but we shall need both projections for technical reasons . there is a large set of projections intermediate between these two , obtained by spanning the space of functions of @xmath65 by additional elements ; we shall not discuss these intermediate projections here .",
    "we now follow the mori - zwanzig procedure ( @xcite ) and split the time derivative of @xmath66 into its projection on the functions of @xmath67 plus a complement : @xmath68 where @xmath69 is either of the two projection just described and @xmath70 .",
    "if @xmath17 is a scalar function of @xmath2 , it is projected in @xmath50 on a function of @xmath55 ; if @xmath17 is a vector function of @xmath2 , each of its components is projected .",
    "the first term in ( [ eq : split ] ) can be evaluated in terms of @xmath17 and we denote it by @xmath71 : @xmath72    to understand the second term , consider an evolution operator , @xmath73 , defined for a phase variable @xmath59 by the equation : @xmath74 with @xmath75 .",
    "this is the orthogonal dynamics equation , which should be read as follows : let @xmath76 at @xmath19 ; let @xmath77 be a time - dependent phase variable that satisfies @xmath78 with initial datum @xmath79 , and set @xmath80 , defining the orthogonal evolution operator @xmath73 .",
    "note that if @xmath59 is orthogonal to the range of @xmath69 then so is @xmath81 for all times @xmath6 .",
    "the operator @xmath73 is the solution operator of the orthogonal dynamics .",
    "the evolution operators @xmath82 and @xmath73 satisfy the dyson formula @xcite : @xmath83 which can be checked by differentiation . with the help of the dyson formula the second term on the right hand side of ( [ eq : split ] ) can be written as : @xmath84 where @xmath85    putting all the terms together , we obtain the generalized langevin equation @xmath86 this is an identity between phase variables , the starting point for our approximations .",
    "if one projects this equation on the space spanned by the initial data for @xmath17 , the last term drops out and the other terms acquire a projection operator as prefactor .",
    "the various terms in equation ( [ eq : langevin ] ) have conventional interpretations .",
    "the first term on the right - hand side is a function only of @xmath66 and represents the self - interaction of the components of @xmath66 ; it is the markovian contribution to @xmath87 .",
    "the second term depends on @xmath17 through the values of @xmath66 at all times @xmath88 between @xmath89 and @xmath6 , and embodies a non - markovian memory . finally , the third term has no component in the range of @xmath69 and if we know nothing outside this range it can be viewed as random , with statistics determined by the initial data .    in the special case of a linear projection",
    "@xmath90 the second term in equation ( [ eq : langevin ] ) acquires a particularly simple form , well - known in statistical physics where it is the only form in general use . by definition , see above , @xmath91 where @xmath92 , @xmath93 $ ] , @xmath94 , are normalized coordinate functions .",
    "a short manipulation converts this sum into @xmath95 where @xmath96 , i.e. , @xmath97 is the correlation of the solution of the orthogonal dynamics equation that starts from @xmath98 with @xmath99 .",
    "substitution into the integral produces the term @xmath100",
    "the generalized langevin equation ( [ eq : langevin ] ) expresses the rate of change of the phase variable @xmath17 as a sum of terms that depend on @xmath17 and on the orthogonal dynamics .",
    "these expressions can not be used directly for approximation when the equations are nonlinear .",
    "indeed , the evaluation of a term such as @xmath101=e[le^{tl}u|e^{tl}u]$ ] requires the evaluation of @xmath66 , i.e. , requires a solution of the full set of @xmath3 equations . in terms of @xmath2",
    ", this term becomes    @xmath102_{{{\\hat{y}}}=\\hat{{\\varphi}}(x , t)},\\ ] ]    where the last expression is the expected value of @xmath103 given the value @xmath104 of @xmath105 .",
    "( we first find the best approximation of @xmath106 by a function of @xmath105 , then substitute the value @xmath107 into the result ) .",
    "the vector @xmath15 has components @xmath108 not included in @xmath65 and not known ; @xmath109 . to obtain something that can be evaluated we move the expectation into the condition , i.e.",
    ", we approximate @xmath110_{{{\\hat{y}}}={{\\varphi}}({{\\hat{x}}},\\tilde{x},t)]}$ ] by @xmath110_{{{\\hat{y}}}={{\\mathbb e}}[{{\\varphi}}(x , t)|{{\\hat{x}}}]}$ ] , which is a function @xmath111 of @xmath65 .    assume for a moment that the second and third terms in the langevin equation ( [ eq : langevin ] ) are negligible .",
    "equation ( [ eq : langevin ] ) becomes :    @xmath112_{{{\\hat{y}}}=\\hat{{\\varphi}}(x , t)}. \\label{outer}\\ ] ]    the approximation we just made , replacing @xmath104 by its projection , gives : @xmath113_{{{\\hat{y}}}={{\\hat{y}}}({{\\hat{x}}})};\\ ] ] projecting this equation on the space of functions of @xmath65 , we find , @xmath114_{{{\\hat{y}}}={{\\hat{y}}}({{\\hat{x}}})}. \\label{op}\\ ] ] the initial data @xmath115 make @xmath116 a function of @xmath65 .",
    "these are the first - order optimal prediction equations .",
    "first - order optimal prediction neglects however the effect of the fluctuations due to the variation of the components @xmath108 of @xmath15 , as we already know from @xcite ; all that is left the uncomputed components is their conditional expectations .    as an example , consider the model equations that arise from the hamiltonian @xmath117 ( @xmath118 can be viewed as position variables @xmath119 and @xmath120 can be viewed as momenta @xmath121 ) .",
    "the resulting equations of motion are : @xmath122 with @xmath29 .",
    "the probability density of @xmath15 is @xmath42 , and that of @xmath2 is @xmath123 . for simplicity set @xmath124 .",
    "keeping @xmath125 out of the @xmath126 equations , and omitting all but the first term in the langevin equation ( [ eq : langevin ] ) , we find : @xmath127 as can be deduced from the definition of conditional expectation : @xmath128 these are the first - order optimal prediction equations as presented in @xcite .",
    "now consider the full langevin equation ( [ eq : langevin ] ) .",
    "the evaluation of the memory term requires a solution of the orthogonal dynamics equation , which we shall present in detail elsewhere . in the present paper",
    "we shall be content with less , yet the result is instructive .    with the appropriate substitutions , the dyson formula ( [ dyson ] ) yields the identity @xmath129 which can be in principle be the starting point of a perturbative evaluation of @xmath73 .",
    "the zero - th order approximation of this equation is : @xmath130 i.e. , one replaces the true flow in the orthogonal complement of the range of @xmath69 ( or @xmath90 ) by the real flow induced by @xmath25 ( which of course has a component in the orthogonal complement )",
    ". this approximation can be sufficient if the correlations in ( [ kernels ] ) do not decay too slowly ; to see this , consider the second term in equation ( [ eq : langevin ] ) :    @xmath131    adding and subtracting equal quantities , we find : @xmath132 a taylor series yields : @xmath133 and therefore , using @xmath134 , we find : @xmath135 if the correlations @xmath136 , @xmath137 , are significant only over short times , we have an acceptable approximation .    at @xmath19",
    "the orthogonal dynamics start from @xmath138 $ ] ; at later time , in our approximation , @xmath139_{{{\\hat{y}}}=\\hat{{\\varphi}}(x , t)}$ ] .",
    "the correlations in ( [ kernels ] ) can be calculated by monte - carlo , and provides a better approximation than first - order optimal prediction .",
    "this is still not a very good approximation , and the reason , as we shall show elsewhere , is that the linear projection @xmath90 used to derive ( [ kernels ] ) projects onto a set of functions that fails to span the whole space of functions of @xmath65 ; this can be remedied by devising more elaborate approximations to the conditional expectation @xmath69 .",
    "here we propose a heuristic alternative : we interpret @xmath140_{{{\\hat{y}}}=\\hat{{\\varphi}}(x , t)}$ ] in @xmath141 as the mean of the right - hand - side of equations ( [ eq : system ] ) and @xmath142 as the fluctuation around that mean .",
    "this mean varies less than the fluctuations as @xmath15 spans @xmath9 , and we make it vary even less by anchoring it to the initial data for the specific problem we wish to solve , i.e. , first approximate @xmath140_{{{\\hat{y}}}=\\hat{{\\varphi}}(x , t)}$ ] by @xmath140_{{{\\hat{y}}}={{\\mathbb e}}[\\hat{{\\varphi}}(x , t)|{{\\hat{x}}}]}$ ] and then further fix @xmath65 at the specific initial value for which we want to solve the @xmath143equation approximation of the system ( [ eq : system ] ) .",
    "let @xmath77 be the function @xmath144_{{{\\hat{y}}}={{\\mathbb e}}[\\hat{{\\varphi}}(x , t)|{{\\hat{x}}}]}$ ] obtained in this way for a specific @xmath65 ( in the example , @xmath145 , where @xmath146 is the specific solution we are seeking ) .",
    "note that @xmath147 is no longer a stationary stochastic process . the heuristic rationale for this  freezing \" of @xmath148 is as follows : write as before @xmath109 ; when , in the averaging that determines the kernels @xmath97 , we fix @xmath65 and vary @xmath108 alone , thus finding the contribution of the subspace orthogonal to @xmath65 , the  frozen \" and the true variations of @xmath141 are the same ; however , if we keep @xmath108 fixed and vary @xmath65 for the evaluation of @xmath149 we relate the fluctuations to the specific solution we are seeking more thoroughly than is possible otherwise when the projection is @xmath90 . with these approximations ,",
    "the kernels @xmath97 in ( [ kernels ] ) , @xmath150 $ ] , become @xmath151 where @xmath152 $ ] can be evaluated by monte - carlo : sample initial data from the invariant distribution over and over , solve the resulting system of equations , whose solutions are statistically stationary in time , and average .",
    "note that this calculation does not depend on the particular known initial values @xmath65 , and thus can be done once and for all for a given equation and a given level of truncation .",
    "the time evolution of the @xmath153 need be accurate only as long as @xmath153 and @xmath154 are correlated .",
    "substitute this into the langevin equation : there is no impediment to using the true @xmath73 in the third term ; then average with respect to @xmath15 ; note that the @xmath97 are functions of time only and commute with a projection on the initial data , while the average of the third term is 0 .",
    "we find the following equation : @xmath155 where @xmath156 and @xmath157 .",
    "equation ( [ oops ] ) states that the solution of the problem depends not only on the mean of the @xmath1 but also on the autocorrelation of the fluctuations .",
    "the solution is thus ",
    "renormalized \" ( see e.g @xcite ) .",
    "the sources of error in ( [ oops ] ) are : an inconsistent use of projections , a simplistic solution of the orthogonal dynamics , and possibly an inaccurate monte - carlo evaluation of the kernels @xmath158 .",
    "all can be remedied .    in the example , ( [ oops ] ) reduces to : @xmath159 the kernel @xmath158 is @xmath160 , r_1({{\\varphi}})= -{{\\varphi}}_2-{{\\varphi}}_2{{\\varphi}}_4 ^ 2 $ ] , @xmath161 , and the expected value in @xmath158 is evaluated over all choices of @xmath15 in @xmath126 dimensions drawn from the invariant distribution with density @xmath162 .    in figure 1",
    "we display some numerical results for @xmath163 in the example , with data @xmath164 .",
    "we show the  truth \" found by averaging many solutions of the full @xmath165dimensional system , the galerkin approximation that sets all unknown functions to zero , the first - order optimal prediction , and the solution of equation ( [ example2 ] ) .",
    "the solutions are shown in a rather favorable case ( they look less striking if one exchanges for example the values of @xmath166 ) .",
    "at first sight , the results in figure 1 are interesting but not overwhelming : the cost of evaluating @xmath158 is comparable to the cost of evaluating the `` truth '' by monte - carlo , and the gain is not obvious .",
    "one may note however , that once @xmath158 has been evaluated , the cost of rerunning the calculation with any other initial data @xmath65 is negligible .",
    "this is the significant fact : @xmath158 is evaluated  at equilibrium \" , i.e. , with all components of @xmath15 , the initial data , sampled from the invariant measure ; @xmath158 does not depend on any specific initial value . as we shall show elsewhere",
    ", the analogous statement is true for an accurate solution of the orthogonal dynamics .",
    "once the heavy work of determining memory functions has been done , the solution of a specific problem is plain sailing . at equilibrium",
    ", one can bring to bear the panoply of scaling methods and equilibrium statistical mechanics .",
    "one can say that the mori - zwanzig formalism makes possible the use of  universal \" ( non problem specific ) results to solve specific problems . in some of the applications we have in mind , the ",
    "large \" ( @xmath3-dimensional ) problems are partial differential equations , and then imperfections in the evaluation of memory terms are immaterial as long as the rate of convergence of finite - dimensional approximations is enhanced ( see @xcite ) .    it is taken for granted in the physics literature that the memory kernels are autocorrelations of the  noise \" ( i.e. , the orthogonal dynamics ) .",
    "this is true also in the example we have presented here .",
    "however , it should be obvious from the discussion that this is an artifact of the use of @xmath90 , the linear projection , and that the full truth is more complicated and interesting .",
    "furthermore , in the physics literature one usually deals with memory by separating ",
    "fast `` and ' ' slow \" variables and assuming that the orthogonal dynamics generated by  fast \" variables generate  noise \" with delta - function memory ; note in contrast that in our problem the unresolved and the resolved variables have exactly the same time scale .    finally , the heuristic arguments of the present paper will be replaced in general by a systematic approximation of the langevin equation , including a systematic evaluation of the orthogonal dynamics , as we will explain in future publications .",
    "we would like to thank prof .",
    "barenblatt , dr .",
    "e. chorin , mr .",
    "e. ingerman , dr . a. kast , mr .",
    "k. lin , for helpful discussions and comments , and mr .",
    "p. okunev for help in programming .",
    "this work was supported in part by by the applied mathematical sciences subprogram of the office of energy research of the us department of energy under contract de - ac03 - 76-sf00098 , and in part by the national science foundation under grant dms98 - 14631 .",
    "r.k . was supported by the israel science foundation founded by the israel academy of sciences and humanities .",
    "99 j bell , aj chorin and w crutchfield , stochastic optimal prediction with application to averaged euler equations , proc .",
    "fluid mech .",
    "lin ( ed ) , pingtung , taiwan , ( 2000 ) , pp . 1 - 13 ."
  ],
  "abstract_text": [
    "<S> optimal prediction methods compensate for a lack of resolution in the numerical solution of complex problems through the use of prior statistical information . </S>",
    "<S> we know from previous work that in the presence of strong underresolution a good approximation needs a non - markovian  memory \" , determined by an equation for the  orthogonal \" , i.e. , unresolved , dynamics . </S>",
    "<S> we present a simple approximation of the orthogonal dynamics , which involves an ansatz and a monte - carlo evaluation of autocorrelations . </S>",
    "<S> the analysis provides a new understanding of the fluctuation - dissipation formulas of statistical physics . </S>",
    "<S> an example is given .    </S>"
  ]
}