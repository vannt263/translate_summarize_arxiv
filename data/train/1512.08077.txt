{
  "article_text": [
    "in this paper , we propose a model prior probability distribution for variable selection problems in linear regression models .",
    "we focus on the general approach where , given @xmath0 covariates , the task is to derive a posterior probability on the space of regression models .",
    "the prior we propose is based on losses and it is compared with both the uniform prior ( which assigns equal prior mass to each model ) and the scott & berger prior @xcite through a simulation study . we show that , in absence of any prior information about the true regression model , the model prior we propose has an overall better performance than the other two .",
    "we also compare the model prior distributions on two well - known data sets : the us crime data @xcite and the hald data @xcite .",
    "variable selection problems , in the bayesian framework , are ( and should be ) in line with any other inferential procedure .",
    "that is , a posterior distribution for the space of models should be obtained in order to represent the ( posterior ) uncertainty about the true regression model ( see @xcite )",
    ". there may be instances where the above is not appropriate , for example if there are models with a negligible posterior probability , in which case a subset of all the possible regression models can be considered . with a prior distribution on the space of models , representing the model uncertainty related to variable selection , one way to proceed",
    "is by using bayesian model averaging @xcite . when the model posterior distribution tends to be spread across many of the possible regression models , and",
    "when prediction is an important part of the statistical analysis , @xcite show that bayesian model averaging performs superiorly than choosing the regression model with the highest posterior probability . also ,",
    "although one may decide to explore a different route than model averaging , @xcite show that the median probability model , under certain conditions , has a predictive power at least as good as the one of the highest probability model .",
    "the median probability model derives from an equivalent `` answer '' to the above problem obtained by estimating the marginal posterior probability that each variable has , independently from the others , of being included in the regression model .",
    "an important component of the bayesian variable selection approach is the definition of the prior for the regression coefficients , including the intercept , and the regression variance .",
    "in fact , we can comfortably say that the majority of literature related to variable selection is focussed on the identification of appropriate prior distributions for the model specific parameters .",
    "this is particularly the case when the approach has to be minimally informative ; however , due to the large number of potential covariates examined in modern problems , a minimally informative prior is possibly the most wise ( or the sole ) solution as it would be not feasible to elicit priors for moderate to large models .",
    "although the importance of parameter specific priors , in this paper we will focus on the model prior distribution only , referring to the specific literature on the subject .",
    "see , for example , @xcite and the references therein .",
    "the prior we propose is based on the idea that , given a variable selection problem , where all the potential covariates have been identified , every possible linear regression model has a _ worth _ depending on the fact that that model has been chosen to be part of the problem @xcite . in other words , the choice of including a model in the model space conveys information , and we use that information to assign a prior probability to the model .",
    "it is important to highlight that the prior we propose is intended to be applied within the bayesian framework , in the sense that the prior represents the initial uncertainty about the true model which is updated , by including the information coming from the data , to obtain a posterior distribution representing the uncertainty at the end of the process @xcite .",
    "the obtained prior depends on the model size , that is on the number of covariates included . as such",
    ", we will compare the proposed prior with the uniform and the scott & berger priors on the basis of the frequentist performances of posterior for the model size .",
    "the paper is organised as follows . in section [ sc_notation ]",
    "we define the notation used throughout the paper and formalise the problem of variable selection for linear regression models in a bayesian framework . in section [ sc_priors ]",
    "we discuss the current objective model priors for variable selection ( the uniform prior and the scott & berger prior ) , and we also present the proposed prior based on losses .",
    "the results of simulation studies are provided in section [ sc_simulation ] . in the section",
    "we examine the performance of the considered priors on the basis of the frequentist results of the corresponding model size posterior distributions .",
    "section [ sc_realdata ] reports the analysis results using two real data sets widely discussed in literature .",
    "the final section [ sc_conclusion ] concludes and provides some discussion points on the proposed prior and its comparison with the other two priors .",
    "the bayesian set up for variable selection problems is , as follows .",
    "the task is to explain a response variable by means of a set of @xmath0 possible covariates , with a sample of observations of size @xmath1 . given the vector @xmath2 of @xmath1 responses , the design matrix @xmath3 of size @xmath4 , an intercept @xmath5 and a vector of coefficients @xmath6 of dimension @xmath0 , the response outcome @xmath7 is expressed as @xmath8 where @xmath9 , for @xmath10 , are the i.i.d .",
    "normally distributed errors with unknown variance @xmath11 .",
    "we assume that the number of observations @xmath1 is larger than the number of covariates ( i.e. @xmath12 ) , and the design matrix is of full rank .",
    "the variable selection problem can be seen as identifying which of the possible @xmath0 covariates has impact on @xmath2 .",
    "in other words , we aim to identify which of the regression parameters @xmath13s are different from zero .",
    "let us consider the binary vector @xmath14 , where the @xmath15-th element is zero if @xmath16 and one if @xmath17 .",
    "then , the generic bayesian regression model is indicated by @xmath18 where @xmath19 and @xmath20 represents the prior distribution for the parameters of the model , the so - called model - specific parameter prior .",
    "note that the cardinality of @xmath21 gives the number of covariates included in model @xmath22 .",
    "there are @xmath23 possible regression models , each one of them identified by @xmath21 .    in the bayesian framework ,",
    "inference about model uncertainty is based on the model posterior probability @xmath24 where @xmath25 is the prior probability for model @xmath26 and @xmath27 is the marginal likelihood of the observations under model @xmath26 .",
    "the model posterior distribution can then be used to either estimate a specific regression model or perform model averaging .",
    "the number of possible regression models grows exponentially with @xmath0 .",
    "when @xmath0 is large the posterior probabilities in are often small for most of models , and posterior inclusion probabilities could give a better idea of the posterior uncertainty in comparison to model posterior probabilities .",
    "the posterior inclusion probability of the @xmath15-th covariate is defined as @xmath28 prior and posterior inclusion probabilities originate from the common idea in bayesian variable selection to consider variable inclusions as exchangeable bernoulli trials , with the common inclusion probability @xmath29 , implying @xmath30    [ [ model - specific - parameter - prior ] ] model - specific parameter prior + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    prior choice on model - specific parameters has received much attention , and well - received priors include the zellner ",
    "siow prior @xcite , the zellner s @xmath31-prior @xcite , the mixtures of @xmath31-priors @xcite , and the more recent _ robust _ prior by @xcite , among others . the robust prior has the advantage of yielding closed - form marginal likelihoods and to not suffer from the information paradox @xcite .",
    "the robust prior is defined as @xmath32 where @xmath33 is the covariance matrix of the maximum likelihood estimator of @xmath34 .",
    "the distribution of @xmath31 is given by @xmath35^a(g+b)^{-(a+1 ) } \\cdot 1_{\\left\\{g>\\rho_{\\pmb\\gamma}(b+n)-b\\right\\}},\\ ] ] where @xmath36 and @xmath37 .",
    "the prior is called _ robust _ as its tails behave like the tails of a multivariate @xmath38 density , therefore less sensible to outliers .    in this paper ,",
    "as the focus is solely on model priors , every analysis is performed by using the same model - specific parameter prior , the robust prior with hyperparameter values @xmath39 , @xmath40 and @xmath41 , as recommended in @xcite , so differences in the results can be ascribed to differences in the model prior .",
    "to the best of our knowledge , the choice of model prior probabilities which convey minimal information is limited to two options : the uniform prior and the so - called scoot & berger prior .",
    "the model uniform prior is obtained by assigning equal prior mass to each regression model , that is @xmath42 for any @xmath21 , and it yields a prior inclusion probability of @xmath43 .",
    "@xcite discuss the following model prior for variable selection @xmath44 their model prior is obtained by assigning a beta prior to @xmath29 , with both the hyper - parameters equal to one , and then marginalising over @xmath29 .",
    "this prior was previously discussed in the literature , see for example @xcite , with the aim of representing prior minimal information .",
    "the motivation behind the choice of the model prior by @xcite lies in its property to correct for multiplicity , which can be seen as an issue when model choice is performed by multiple statistical testing with respect to a reference model ( typically the null model or the full model ) .",
    "the scott & berger prior induces a marginal prior inclusion probability of @xmath43 for each covariate , same as the uniform model prior .",
    "however , as thoroughly discussed in @xcite , given that their prior is function of @xmath0 , it allows the multiplicity correction .",
    "it may also be worthwhile to mention that the choice of a uniform prior for @xmath29 , which is a probability of success of bernoulli trials , somehow contradicts the usual choice of a @xmath45 as default prior in parameter inference @xcite .",
    "the model prior based on losses has been introduced by @xcite .",
    "the basic idea is that we can assign a _",
    "worth _ to each model by objectively measuring what is lost if the model is removed from the space of models , and it is the true one .    to derive the prior for @xmath26",
    "we then follow the framework detailed in @xcite .",
    "first , we derive the loss in information when the true model is removed , which we indicate by @xmath46 .",
    "we apply the well - known bayesian result in @xcite , which states that if a model is misspecified the posterior asymptotically accumulates on the nearest model , where the nearest model is the one which minimises the kullback ",
    "leibler divergence @xcite from the true model . to illustrate ,",
    "let us assume that we want to assess the _ worth _ of the regression model @xmath26 , with the simplified notation @xmath47 .",
    "the loss in information by removing model @xmath22 , when it is the true model , is given by @xmath48 where @xmath49 represents the regression distribution of @xmath50 , that is , the regression model which is the most similar to @xmath51 .",
    "we then see that the loss in information associated to model @xmath26 is the expected minimum kullback ",
    "leibler divergence between @xmath26 and the nearest one , where the expectation is taken with respect to the prior @xmath52 , representing the prior uncertainty about the true values of @xmath5 , @xmath34 and @xmath53 .    to link the _ worth _ of a model to its prior probability",
    ", we use the _ self - information _ loss function , as discussed in @xcite .",
    "briefly , this particular type of loss function ( also known as the _ log - loss _ function ) measures the performance of a probability statement with respect to an outcome .",
    "thus , for every probability assignment @xmath54 , the self - information loss function is defined as @xmath55 more details and properties of this particular loss function can be found , for example , in @xcite . as both the expected minimum kullback ",
    "leibler divergence between model @xmath26 and the nearest model , and the self - information loss function measure the same quantity , i.e. the loss in information , we can equate the two measurements yielding @xmath56 however , as the number of covariates may easily be moderate to large , it is also necessary to take into considerations the complexity of the model .",
    "for the regression model @xmath26 , we denote the loss due to complexity by @xmath57 , and it is determined as follows . if we keep model @xmath26 in the space of models , the loss would be proportional to the number of covariates that have to be considered and measured . therefore , the loss of keeping a linear regression model increases with the number of covariates it contains , and we have @xmath58 and @xmath59 where @xmath60 represents a loss and @xmath61 a utility .",
    "the loss component due to complexity is easily fit in our framework and the model prior for @xmath26 is @xmath62 - c\\cdot|\\pmb\\gamma| \\right\\}.\\ ] ] in other words , the prior is constructed based upon a cumulative loss with a component representing the loss in information and a component representing the loss due to complexity .",
    "the following theorem [ teo_klreg1 ] ( which proof is in the appendix ) shows the expression of the minimum kullback ",
    "leibler divergence between regression models .",
    "[ teo_klreg1 ] let @xmath63 and @xmath64 be linear normal regression models as in , with design matrices , respectively , @xmath65 and @xmath66 .",
    "if @xmath67 is invertible , the minimum kullback ",
    "leibler divergence between @xmath51 and @xmath49 is @xmath68    theorem [ teo_klreg1 ] shows that the minimum kullback ",
    "leibler divergence between any two linear regression models is zero , regardless to the number of covariates in the models .",
    "this means that , in variable selection problems for linear regression models , there is no loss in information in selecting the `` wrong '' model , as such the model prior in becomes @xmath69    figure [ fig : priorcomp_first ] compares the proposed prior with the scott & berger prior , in terms of prior mass versus model size , for the case of @xmath70 covariates .",
    "the uniform prior has been omitted from the plot as it would be represented by a horizontal line , reflecting a constant prior probability independent from the number of covariates included in the regression model . whilst scott & berger prior has a symmetrical behaviour , the proposed",
    "prior assigns more mass to the more simple models than to the more complex ones , as expected from expression . +",
    "it is important to discuss some remarks about the proposed prior .",
    "first , the prior in has practical relevance , especially in modern variable selection problems where the number of covariates can be significantly large .",
    "in fact , assigning less prior mass on large models reflects a parsimonious prior position , where the inclusion of additional covariates has to be supported by relatively strong evidence in the data .",
    "the constant @xmath71 plays an important role . as @xmath72 ,",
    "the prior tends to be uniform , that is @xmath73 .",
    "if , on the other hand , we increase the value of @xmath71 , the proposed prior will put higher mass on small models , yielding more parsimonious regression models .",
    "a natural choice is to set @xmath74 , as if there is no prior information about the true model this appears to be the less informative choice .",
    "we empirically show in the simulation studies in section [ sc_simulation ] that @xmath74 leads better results , when compared to the uniform and the scott & berger priors , if there is no prior information about the model size .",
    "the proposed prior with the recommended value @xmath74 does not correct the multiplicity .",
    "one way of solving this issue , if required , would be to set constant @xmath71 as a function of the number of covariates .",
    "however , as mentioned in section [ sc_intro ] , we believe that for variable - selection problems the approach should be in line with the bayesian framework of having prior and posterior probability representing , respectively , prior and posterior uncertainty . although we agree that multiplicity correction is one way to define model prior probabilities in an objective sense , we argue that this is not a necessary condition for a model prior to satisfy , in particular , if the problem of interest is related to prediction .    as a final remark",
    ", we note that the prior based on loses induces a binomial prior for model size @xmath75 : @xmath76 therefore , the marginal prior inclusion probability is given by @xmath77 the immediate consequence of the above result is that the prior inclusion probability depends on the value of the constant @xmath71 .",
    "if we set , as recommended for a minimally informative prior , @xmath74 , then we obtain @xmath78",
    ". a smaller values of @xmath71 will lead a prior inclusion probability close to 1/2 , that is @xmath79 for @xmath72 . on the other hand , as @xmath71 increases",
    ", the prior inclusion probability becomes negligible and the model prior mass tends to be concentrated on the null model .",
    "in this section we show the results of a simulation study designed to compare the performance of the proposed model prior based on losses with the uniform prior and the scott & berger prior .",
    "it is important to highlight here that the value of @xmath71 considered for this simulation is one .",
    "the reason lies in the predictability of the behaviour of our prior for different values of @xmath71 .",
    "it is well known that a variable selection problem is driven by both the choice of the model prior and of the model - specific parameter prior .",
    "however , here the interest is in the effects on variable selection determined by the prior probability on the space of models . as such",
    ", the simulation exercise described has the purpose to analyse the frequentist properties of the posterior distribution on the model size and , to minimise any possible effects of the model - specific parameters prior , we choose to use the robust prior of @xcite in conjunction with the three model priors .",
    "we obtained very similar results using the zellner ",
    "siow prior for the parameter of the regression ( results are not included in this paper ) .",
    "in line with objective bayesian analysis , we have examined two key frequentist properties of the posterior distribution for the model size : the coverage and the mean squared error .",
    "the general idea is to perform repeated simulations from similar regression models . for the coverage ,",
    "we compute the proportion of times the true model size is included in the 95% credible interval of the posterior .",
    "we also compute the mean squared error from the mean and from the median of the true model size .",
    "the simulation study encompasses a wide range of different scenarios , in terms of sample size , number covariates and model size , so to perform a thorough analysis . in detail",
    ", we have considered three sample sizes ( @xmath80 , @xmath81 , and @xmath82 ) and four maximum number of covariates ( @xmath83 , @xmath84 , @xmath85 and @xmath86 ) so to be able to compare the priors for different ratios number of covariates to sample size . to avoid cumbersome and difficult to interpret results ,",
    "we have generated the repeated samples from models with , relatively to @xmath0 , a small , medium and large number of covariates .",
    "this has been accomplished by randomly selecting the number of covariates according to a fixed prior inclusion probability @xmath29 of @xmath87 , @xmath88 and @xmath89 , respectively .",
    "the above values of @xmath1 , @xmath0 and @xmath29 give a total of 36 cases .",
    "for each case we repeat the following procedure for 100,000 times :    * generate a design matrix @xmath3 of size @xmath90 where each element is an independent realisation of a standard normal distribution ; * generate a binary vector @xmath21 from a sequence of @xmath0 independent bernoulli experiments with probability of success equal to @xmath29 ; * from the robust prior in , we generate the vector of coefficients @xmath34 ; * we generate the response vector from the regression model in , considering @xmath91 ; * finally , by using the above values of the design matrix and the vector of responses , we have computed the necessary quantities , including the marginal likelihoods , the model posteriors and the model size posterior distribution .",
    "the last step of the above procedure has been performed under the three model priors considered in this paper : the uniform prior , the scott & berger prior and the proposed prior .    with our simulation experience",
    ", the true model was equally identified under all the three considered priors .",
    "this part of the study has been performed by computing the average number of false positive and false negative in detecting the coefficients , both considering the highest probability model ( hpm ) and the median probability model ( mpm ) . by false positive",
    "we consider a covariate which was included in any of the above models when it was not present in the true model . and",
    "by false negative we consider a covariate not in the true model but included in the mpm or the hpm . as",
    "the focus of this paper is on model prior and its performance on model size estimation the above results are not reported in detail .",
    "instead , we discuss the effect of the model prior choice on the posterior model size distribution .    the detailed results of the simulations on the model size are presented in table [ tab : simulation_summry ] . under each model",
    "prior , we report the coverage of the @xmath92 credible interval of the posterior , the mean squared error from the mean and the mean squared error from the median . from the experiments , the following aspects emerge .",
    "the coverage under the scott & berger prior appears to be the most stable across sample size and model size , although it is constantly above the nominal value of @xmath92 .",
    "the uniform prior shows a coverage too low with respect to the nominal value when @xmath93 and @xmath94 .",
    "this is more obvious for sample sizes of @xmath80 and @xmath81 . the prior we propose appears to have a relatively low coverage when @xmath95 .",
    "however , it shows an overall performance in the coverage closer to the nominal value than the scott & berger s .    to study the variability of posterior estimations , the mean squared errors from the posterior mean values and posterior median values are summarized in table [ tab : simulation_summry ] .",
    "as in terms of behaviour both mean squared errors have similar pattern , we examine in detail the mean squared error from the mean , which is also detailed in figure [ fig_simulationsummary ] .",
    "as one would expect , across all values of @xmath0 and @xmath29 the mean squared errors decrease as @xmath1 increases .",
    "if we focus on @xmath29 , we note the following . for @xmath94 ,",
    "the uniform prior has the worse performance , while our proposed prior has generally the best performance . the only exception being the scenario with @xmath82 and @xmath86 .",
    "the three priors have very similar performance for expected medium size regression models , i.e. @xmath96 .",
    "finally , for relatively large models , our prior is outperformed by both the uniform prior and the scott & berger prior .",
    "however , in this latter case , the differences are not as prominent as the case of relatively small models and tend to disappear for large values of @xmath0 .",
    "max width=    lcccccccccc @xmath97 & & & & + @xmath0 & coverage & mse mean & mse median & coverage & mse mean & mse median & coverage & mse mean & mse median & @xmath29 + & 0.995 & 0.515 & 0.445 & 0.998 & 0.371 & 0.297 & 0.999 & 0.162 & 0.163 & 0.15 + & 0.995 & 0.382 & 0.507 & 0.998 & 0.421 & 0.605 & 0.967 & 0.515 & 0.685 & 0.50 + & 0.989 & 0.545 & 0.746 & 0.997 & 0.529 & 0.802 & 0.908 & 0.930 & 1.166 & 0.75 + & 0.980 & 1.770 & 1.640 & 0.995 & 0.969 & 0.784 & 0.997 & 0.363 & 0.323 & 0.15 + & 0.992 & 0.652 & 0.763 & 0.997 & 0.793 & 1.059 & 0.950 & 0.886 & 1.163 & 0.50 + & 0.960 & 1.021 & 1.221 & 0.995 & 0.843 & 1.163 & 0.852 & 1.682 & 2.014 & 0.75 + & 0.773 & 7.464 & 7.444 & 0.993 & 2.438 & 1.760 & 0.989 & 1.179 & 1.011 & 0.15 + & 0.990 & 0.398 & 0.475 & 0.991 & 0.421 & 0.480 & 0.966 & 0.401 & 0.493 & 0.50 + & 0.957 & 0.561 & 0.706 & 0.990 & 0.460 & 0.593 & 0.886 & 0.840 & 0.976 & 0.75 + & 0.786 & 11.266 & 11.314 & 0.992 & 4.501 & 3.656 & 0.973 & 2.074 & 1.926 & 0.15 + & 0.996 & 0.107 & 0.129 & 0.996 & 0.102 & 0.117 & 0.974 & 0.113 & 0.130 & 0.50 + & 0.972 & 0.177 & 0.212 & 0.993 & 0.153 & 0.185 & 0.918 & 0.222 & 0.249 & 0.75 +   + @xmath98 & & & & + @xmath0 & coverage & mse mean & mse median & coverage & mse mean & mse median & coverage & mse mean & mse median & @xmath29 + & 0.997 & 0.363 & 0.298 & 0.999 & 0.252 & 0.206 & 0.999 & 0.126 & 0.134 & 0.15 + & 0.995 & 0.312 & 0.417 & 0.998 & 0.347 & 0.478 & 0.971 & 0.399 & 0.524 & 0.50 + & 0.988 & 0.445 & 0.618 & 0.995 & 0.427 & 0.635 & 0.932 & 0.693 & 0.867 & 0.75 + & 0.988 & 1.220 & 1.131 & 0.997 & 0.603 & 0.478 & 0.998 & 0.267 & 0.248 & 0.15 + & 0.992 & 0.546 & 0.665 & 0.995 & 0.678 & 0.888 & 0.954 & 0.719 & 0.956 & 0.50 + & 0.963 & 0.830 & 1.045 & 0.993 & 0.710 & 0.989 & 0.878 & 1.263 & 1.548 & 0.75 + & 0.882 & 5.536 & 5.382 & 0.997 & 1.104 & 0.746 & 0.996 & 0.773 & 0.629 & 0.15 + & 0.991 & 0.378 & 0.456 & 0.993 & 0.404 & 0.463 & 0.970 & 0.383 & 0.482 & 0.50 + & 0.960 & 0.519 & 0.659 & 0.990 & 0.448 & 0.581 & 0.896 & 0.732 & 0.856 & 0.75 + & 0.784 & 9.115 & 9.133 & 0.996 & 1.259 & 0.808 & 0.990 & 1.255 & 1.076 & 0.15 + & 0.995 & 0.115 & 0.133 & 0.996 & 0.113 & 0.128 & 0.974 & 0.119 & 0.138 & 0.50 + & 0.975 & 0.171 & 0.201 & 0.994 & 0.156 & 0.188 & 0.918 & 0.203 & 0.231 & 0.75 +   + @xmath99 & & & & + @xmath0 & coverage & mse mean & mse median & coverage & mse mean & mse median & coverage & mse mean & mse median & @xmath29 + & 0.999 & 0.223 & 0.174 & 0.999 & 0.151 & 0.120 & 0.999 & 0.083 & 0.084 & 0.15 + & 0.995 & 0.245 & 0.323 & 0.995 & 0.277 & 0.364 & 0.978 & 0.298 & 0.382 & 0.50 + & 0.987 & 0.336 & 0.461 & 0.993 & 0.325 & 0.471 & 0.949 & 0.482 & 0.596 & 0.75 + & 0.990 & 0.741 & 0.619 & 0.997 & 0.347 & 0.269 & 0.998 & 0.183 & 0.170 & 0.15 + & 0.993 & 0.418 & 0.532 & 0.992 & 0.531 & 0.661 & 0.967 & 0.499 & 0.658 & 0.50 + & 0.967 & 0.613 & 0.810 & 0.994 & 0.537 & 0.764 & 0.909 & 0.851 & 1.051 & 0.75 + & 0.954 & 3.270 & 2.989 & 0.998 & 0.420 & 0.279 & 0.997 & 0.449 & 0.340 & 0.15 + & 0.991 & 0.372 & 0.441 & 0.993 & 0.415 & 0.476 & 0.970 & 0.375 & 0.471 & 0.50 + & 0.958 & 0.508 & 0.647 & 0.990 & 0.451 & 0.580 & 0.894 & 0.701 & 0.821 & 0.75 + & 0.866 & 5.810 & 5.716 & 0.997 & 0.269 & 0.152 & 0.994 & 0.660 & 0.530 & 0.15 + & 0.996 & 0.102 & 0.120 & 0.996 & 0.108 & 0.123 & 0.977 & 0.109 & 0.125 & 0.50 + & 0.975 & 0.160 & 0.187 & 0.994 & 0.149 & 0.178 & 0.922 & 0.187 & 0.210 & 0.75 +        overall , it appears that the proposed prior with @xmath74 has the most stable frequentist performance , making it appealing in scenarios of minimal prior knowledge about the true model size .",
    "on the other hand , if one had prior information about the true models size being close to the full model , a value of @xmath71 smaller than one could be selected ( e.g. @xmath100 ) so to obtain a model prior more similar to the uniform prior and increase performance .",
    "in this section we investigate the properties of objective model priors for variable selection in real data sets .",
    "the considered data sets are us crime data @xcite and the hald data @xcite , which have been extensively used in literature ( see @xcite and @xcite , for example ) .",
    "at last , we perform a robustness analysis to highlight the sensitivity of the inferential results to relatively small changes to the data .      the first data set consists of @xmath101 observations for @xmath102 covariates related to crime data in the us for 1960 @xcite .",
    "the aim of the analysis is to understand which variables , such as population type , police expenditure and ethnicity ( among others ) , impact on the crime rate per head in the population ( the response variable ) .",
    "the summary statistics of the model size posterior distributions are presented in table [ tab : us1 ] , where we considered the proposed prior with different values of the constant @xmath71 .",
    "the plot of the posterior distributions for the model size are represented in figure [ fig : us_post ] .",
    "the mean and the median obtained by the use of the uniform prior and the scott & berger prior are similar .",
    "the proposed prior , with @xmath74 , gives mean and median model size smaller than the ones obtained by using the other two priors .",
    "we note that the posterior mean and median model size increase when @xmath71 is smaller than one , as expected , and for @xmath103 these values decrease as the prior puts more mass on smaller models . again , this result is in line on what discussed in sections [ sc_priors ] and [ sc_simulation ] . from the table , and by inspecting the the posterior plots , we see that the scott & berger prior yields a posterior for the model size with higher uncertainty than the one obtained by using either the uniform prior or the proposed prior .",
    "the hpm identified by the uniform prior includes 6 covariates , unlike the one determined by scott & berger prior and the proposed prior , which both assign the highest mass to a model of size 3 .",
    "the posterior probability for the hpm is of 0.019 for the uniform prior , 0.021 for the scott & berger prior , and 0.056 when the proposed prior ( with @xmath74 ) is used . in table",
    "[ tab : us_postinclu ] we can see what covariates are included in the hpm under each one of the considered priors , and that both scott & berger and our prior agree in the result . in table",
    "[ tab : us_postinclu ] , the proposed model prior tends to assign lower probabilities compared to the other two priors , particularly for covariates which are unlikely to be included ( posterior inclusion probabilities less than 1/2 ) .",
    "the main result is that the covariates concerning age of males , unemployment rate among male and probability of imprisonment are included in the mpm under the uniform and the scott & berger priors , but not under the proposed prior .",
    "although the proposed prior and scott & berger s disagree on the size of the mpm , we note that both values , 3 and 6 , are well within the 95% credible interval of the respective posteriors .",
    "the same can not be said for the uniform prior , as the corresponding credible interval does not contain the size 3 .",
    ".comparison of the posterior summary statistics for the us crime data set .",
    "four statistics for the number of covariates are measured : mean , median , standard deviation ( sd ) and the @xmath92 confidence interval ( @xmath92 c.i . ) .",
    "the number of covariates included in the highest posterior probability model ( hpm ) and the median probability model ( mpm ) are reported . [ cols=\"<,^,^,^,^,^,^\",options=\"header \" , ]      to analyse the robustness of the proposed prior to small changes in the data , and to compare the results with the uniform and the scott & berger priors , we look at the performance in estimating the model size and the posterior inclusion probabilities for repeated random sub - samples of the original data , both for the us crime data and the hald data .    the general idea is to extract 500 sub - samples of a size that is the 85% of the numerosity of the data set , and perform variable selection as done in section [ sc_realdata ] using these sub - samples as observations .",
    "then we compare the performance of the model priors by considering the histogram of the posterior mean model size , and the box - plots of the posterior inclusion probabilities for the considered priors .",
    "for the us crime data set analysed in section [ sc_uscrime ] , we consider 500 samples of size 40 .",
    "the analysis takes also into consideration the results for different values of @xmath71 for the proposed prior .",
    "figure [ fig : us_ra_hists ] shows the histogram of the mean model size when we apply the uniform prior and the scott & berger ; for the proposed prior , we consider @xmath104 , @xmath105 , @xmath106 and @xmath107 . in the plots we have indicated with a vertical line the estimated model size when the whole data set is considered .",
    "we note the following .",
    "first , the uncertainty in the model size is larger when the scott & berger prior is used .",
    "all the remaining cases appear to lead similar variability in the posterior model size estimates .",
    "second , there is a sensitivity of our prior to @xmath71 , with an impact on the mean location of the distribution , in the sense that when @xmath71 increases the mean model size decreases .",
    "this agrees with what discussed in section [ sc_priors ] about the parsimony of the prior when @xmath71 increases .",
    "also , note that proposed prior tends to yield a posterior similar to the posterior yielded by the uniform prior for @xmath100 , as expected .    for what it concerns the posterior inclusion probabilities , in figure [ fig : us_ra_box ] ,",
    "we see a relatively small impact of @xmath71 on the variability of the probabilities , except when @xmath107 when we see a larger variability for covariate number 13 , for example .",
    "effects of different values of the constant @xmath71 appear in the mean of the estimates .",
    "for example , covariates 1 and 3 .",
    "when we compare our prior with @xmath74 , to the uniform prior and , in particular , scott & berger s , it is interesting to focus on the three covariates that are included in the mpm under these two last priors but not under our prior . in figure [ fig : us_ra_box ] these covariates are indicated by numbers 1 , 11 and 14 .",
    "we note that the size of the variability of their posterior inclusion probabilities under the proposed prior admits possible inclusion of the covariates in the mpm .",
    "this seems to suggest that , although our prior has undoubtedly a feature of estimating regression models with less covariates than the one defined by the other two priors , it possibly reflects a low level of certainty in the data on whether the covariates have to be considered or not .",
    "considering the covariates included in the mpm , the covariates 3 , 4 and 13 in figure [ fig : us_ra_box ] , we can make the following comments .",
    "for the covariates 4 and 13 , there is little to argue as the distributions of the respective posterior inclusion probabilities show strong information in the data in favour of their inclusion in the mpm model .",
    "however , with similar arguments for the above non included covariates , the variability of the posterior inclusion probability of covariate 3 expresses a less strong support from the observations on whether the covariate has to be part of the regression model .",
    "the robustness analysis of the hald data set , with a similar procedure as above , consisted in 500 repeated samples of size 10 .",
    "the histograms of the posterior mean model size are in figure [ fig : hald_ra_hist ] , while the box - plots of the posterior inclusion probabilities are in figure [ fig : hald_ra_box ] .",
    "as already discussed in section [ sc_hald ] , the information about the covariates that should be part of the regression model contained in the data appears to be strong , and this is reflected in the robustness of the approach . in particular , the histograms of the posterior mean model size are concentrated around the mean obtained by considering the whole data set ( vertical line ) .",
    "the inspection of the box - plots of the posterior inclusion probabilities for this data set is straightforward .",
    "the impact of @xmath71 is more noticeable on the covariates with either a high posterior inclusion probability ( i.e. number 1 ) or with a low posterior inclusion probability ( i.e. number 3 ) .",
    "in fact , for @xmath71 increasing , these probabilities become more prominent . in general , however , it does nt seem that different priors lead to significantly different variability or shape of the distribution of the posterior inclusion probabilities .",
    "this paper introduces a novel prior distribution for the model space in variable selection for linear regression .",
    "the prior is based on the idea that , if the `` wrong '' model is chosen , we incur in a loss that has two components : one represents the loss in information objectively measured by considering the _ worth _ of the model , and one related to the complexity of the model expressed by its size .",
    "the obtained model has the following two appealing properties .",
    "first , it has a simple expression .",
    "second , it has the flexibility of representing minimal prior information about the model as well as to allow the inclusion of appropriate prior information with regard to the model size .",
    "the simulations studies carried out show good frequentist performance of the model prior when compared to other two existing minimally informative priors : the uniform prior and the scott & berger prior @xcite .",
    "in particular , when we set the constant @xmath74 , which is our recommendation for a prior carrying minimal prior information , the proposed prior outperforms the other two priors when the size of the true model is relatively small , with respect to the full model . in cases where the true model is close to the full model our prior is not performing as well as the other two priors .",
    "however , the difference in the performance is small and , if prior information is available , this lack of performance can be compensated by an appropriate choice of the constant @xmath71 reflecting the prior knowledge .",
    "when it comes to real data analysis , we note a fair closeness of the results obtained by using the proposed prior with the ones of scott & berger s .",
    "both of them are more parsimonious than the uniform prior , unless the data itself is highly informative about which covariates should be included in the regression model .",
    "it is however important to highlight that the observed discrepancies between the proposed prior and the scott & berger prior are limited to the median probability model .",
    "in fact , the estimated highest probability models are the same for both the analysed data sets , and it is noteworthy to mention that the posterior probability associated to the highest probability model under our prior is the largest .",
    "let @xmath21 and @xmath108 be non - identical binary vectors . with the corresponding design matrices @xmath65 and @xmath66 and the vectors of coefficients @xmath34 and @xmath109 ,",
    "the models are defined as                                                                        ( 1986 ) .",
    "on assessing prior distributions and bayesian regression analysis with @xmath31-prior distributions . in _",
    "bayesian inference and decision techniques : essays in honour of bruno de finetti _ , goel pk , zellner a ( eds ) .",
    "north - holland : amsterdam , 233243    ( 1980 ) .",
    "posterior odds ratios for selected regression hypotheses .",
    "( with discussion ) . in _ bayesian statistics _ , bernardo jm , degroot mh , lindley dv , smith afm ( eds ) .",
    "university press : valencia , 585603"
  ],
  "abstract_text": [
    "<S> in this work we discuss a novel model prior probability for variable selection in linear regression . the idea is to determine the prior mass in an objective sense , by considering the _ worth _ of each of the possible regression models , given the number of covariates under consideration . through a simulation study </S>",
    "<S> , we show that the proposed prior outperforms the uniform prior and the scott & berger prior in a scenario of no prior knowledge about the size of the true regression models . </S>",
    "<S> we illustrate the use of the prior using two well - known data sets with , respectively , 15 and 4 covariates .    * keywords * bayesian variable selection , linear regression , loss functions , objective priors . </S>"
  ]
}