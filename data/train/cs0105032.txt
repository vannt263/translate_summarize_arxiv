{
  "article_text": [
    "the interaction of decision makers who share an environment is traditionally studied in game theory and economics .",
    "the game theoretic formalism is very general , and analyzes the problem in terms of solution concepts such as nash equilibrium  @xcite , but usually works under the assumption that the environment is perfectly known to the agents .    in reinforcement learning  @xcite , no explicit model of the environment",
    "is assumed , and learning happens through trial and error .",
    "recently , there has been interest in applying reinforcement learning algorithms to multi - agent environments .",
    "for example , littman  @xcite describes and analyzes a _ q - learning_-like algorithm for finding optimal policies in the framework of zero - sum markov games , in which two players have strictly opposite interests .",
    "hu and wellman  @xcite propose a different multi - agent q - learning algorithm for _ general - sum _ games , and argue that it converges to a nash equilibrium .",
    "a simpler , but still interesting case , is when multiple agents + share the same objectives .",
    "a study of the behavior of agents employing q - learning individually was made by claus and boutilier  @xcite , focusing on the influence of game structure and exploration strategies on convergence to nash equilibria . in boutilier",
    "s later work  @xcite , an extension of value iteration was developed that allows each agent to reason explicitly about the state of coordination .",
    "however , all of this research assumes that the agents have the ability to completely and reliably observe both the state of the environment and the reward received by the whole system .",
    "schneider et al .",
    "@xcite investigate a case of distributed reinforcement learning , in which agents have complete and reliable state observation , but only receive a local reinforcement signal .",
    "they investigate rules that allow individual agents to share reinforcement with their neighbors . in this paper",
    "we investigate the complementary problem in which the agents all receive the shared reward signal , but have incomplete , unreliable , and generally different perceptions of the world state .",
    "in such environments , value - search methods are generally inappropriate , causing us to turn to policy - search methods  @xcite which we have applied previously to single - agent partially observable domains  @xcite .    in this paper",
    "we describe a gradient - descent policy - search algorithm for cooperative multi - agent domains . in this",
    "setting , after each agent performs its action given its observation according to some individual strategy , they all receive the same payoff .",
    "our objective is to find a learning algorithm that makes each agent independently find a strategy that enables the group of agents to receive the optimal payoff .",
    "although this will not be possible in general , we present a distributed algorithm that finds _ local _ optima in the space of the agents policies .",
    "the rest of the paper is organized as follows . in section  [ ipg ] ,",
    "we give a formal definition of a cooperative multi - agent environment . in section  [ gd ]",
    ", we review the gradient descent algorithm for policy search , then develop it for the multi - agent setting . in section  [ bs ] , we discuss the different notions of optimality for strategies . finally , we present empirical results in section  [ experiments ] .",
    "an _ identical payoff stochastic game _",
    "( )  @xcite describes the interaction of a set of agents with a markov environment in which they all receive the same payoffs .",
    "an ( ) is a tuple @xmath0 , where @xmath1 is a discrete state space ; @xmath2 is a probability distribution over the initial state ; @xmath3 is a collection of agents , where an _ agent _",
    "@xmath4 is a 3-tuple , @xmath5 , of its discrete action space @xmath6 , discrete observation space @xmath7 , and observation function ; @xmath8 is a mapping from states of the environment and actions of the agents to probability distributions over states of the environment denotes the set of probability distributions defined on some space @xmath9 . ] ; and is the payoff function , where @xmath10 is the joint action space of the agents . when all agents in @xmath3 have the identity observation function @xmath11 for all @xmath12 , the game is _",
    "completely observable_. otherwise , it is a _ partially observable _  ( ) .    in",
    "a , at each time step : each agent observes @xmath13 corresponding to @xmath14 and selects an action @xmath15 according to its strategy ; a compound action @xmath16 from the joint action space @xmath17 is performed , inducing a state transition of the environment ; and the identical reward @xmath18 is received by all agents .",
    "the objective of each agent is to choose a strategy that maximizes the _ value of the game_. for a discount factor and a set of strategies @xmath19 , the value of the game is @xmath20\\;\\;.\\ ] ] in the general case , a _ strategy _ for some agent is a mapping from the history of all observations from the beginning of the game into the current action @xmath21 .",
    "we limit our consideration in this paper to cases in which the agent s actions may depend only on the current observation , or in which the agent has a finite internal memory . when actions depend only on the current observation , the policy is called a _ memoryless _ or",
    "_ reactive policy_. when this dependence is probabilistic , we call it a _ stochastic reactive policy _ , otherwise a _ deterministic reactive policy_.    note that in a completely observable , reactive policies are sufficient to implement the best possible joint strategy . this follows directly from the fact that every mdp  has an optimal deterministic reactive policy  @xcite .",
    "therefore an mdp  with the product action space @xmath22 corresponding to a completely observable   also has one , representable by deterministic reactive policies for each agent .",
    "however , it has been shown that in partially observable environments , the best reactive policy can be arbitrarily worse than the best policy using memory  @xcite .",
    "this statement can also be easily extended to poipsgs .",
    "there are many possibilities for constructing policies with memory  @xcite . in this work",
    "we use a _ finite state controller _",
    "( fsc ) for each agent .",
    "a more detailed description of fscs and derivation of algorithms for learning them may be found in a previous paper  @xcite ; we simply state the definition here .    _ a finite state controller _ ( fsc ) for an agent with action space @xmath23 and observation space @xmath24 is a tuple @xmath25 , where @xmath26 is a finite set of internal controller states , @xmath27 is a probability distribution over the initial internal state , @xmath28 is the internal state transition function that maps an internal state and observation into a probability distribution over internal states , and @xmath29 is the action function that maps an internal state into a probability distribution over actions .",
    "figure  [ mapg ] depicts an influence diagram for two agents controlled by fscs .",
    "note that in partially observable environments , agents controlled by fscs might not have enough memory to even represent an optimal policy which could , in general , require infinite memory , as in a partially observable markov decision process ( pomdp )  @xcite . in this paper , we concentrate on the problem of finding the ( locally ) optimal controller from the class of fscs with some fixed size of memory .    to better understand ipsgs ,",
    "let us consider an example from boutilier  @xcite , illustrated in figure  [ craig ] .",
    "there are two agents , @xmath30 and @xmath31 , each of which has a choice of two actions , @xmath32 and @xmath33 , at any of three states .",
    "all transitions are deterministic and are labeled by the joint action that corresponds to the transition .",
    "for instance , the joint action @xmath34 corresponds to the first agent performing action @xmath32 and the second agent performing action @xmath33 . here , @xmath35 refers to any action taken by the corresponding agent .",
    "the starting state is @xmath36 , where the first agent alone decides whether to move the environment to state @xmath37 by performing action @xmath32 or to state @xmath38 by performing action @xmath33 . in state @xmath38 ,",
    "no matter what both agents do as the next step , they receive a reward of @xmath39 in state @xmath40 risk - free .",
    "in state @xmath37 , the agents have a choice of cooperating  choosing the same action , whether @xmath41 or @xmath42with reward @xmath43 in state @xmath44 , or not  choosing different actions , whether @xmath34 or @xmath45and getting @xmath46 in state @xmath47 .",
    "we will represent a joint policy with parameters @xmath48 , denoting the probability that an agent will perform action @xmath32 in the corresponding state .",
    "only three parameters are important for the outcome : @xmath49 . the optimal joint policies are @xmath50 or @xmath51 , which are deterministic reactive policies .",
    "in this section , we first introduce a general method for using gradient descent in policy spaces , then show how it can be applied to multi - agent problems .",
    "williams introduced the notion of policy search for reinforcement learning in his reinforce   algorithm  @xcite , which was generalized to a broader class of error criteria by baird and moore  @xcite .",
    "we will start by considering the case of a single agent interacting with a pomdp .",
    "the agent s policy @xmath52 is assumed to depend on some internal state taking on values in finite set @xmath26 .",
    "we will not make any further commitment to details of the policy s architecture , as long as it defines the probability of action given past history as a continuous differentiable function of some set of parameters @xmath53 .",
    "first , we will establish some notation .",
    "we denote by @xmath54 the set of all possible experience sequences of length @xmath55 : @xmath56 . in order to specify that some element is a part of the history @xmath57 at time @xmath58 , we write , for example , @xmath59 and @xmath60 for the @xmath61 reward and action in the history @xmath57 .",
    "we will also use @xmath62 to denote a prefix of the sequence @xmath63 truncated at time @xmath64 @xmath65 @xmath66 . the value defined by equation  ( [ value ] )",
    "can be rewritten as @xmath67    let us assume the policy is expressed parametrically in terms of a vector of weights @xmath68 . if we could calculate the derivative of @xmath69 for each @xmath70 , it would be possible to do an exact gradient descent on value @xmath69 by making updates @xmath71 we can compute the derivative for each weight @xmath70 , @xmath72 \\\\   = & \\sum_{t = 1}^\\infty \\gamma^t \\sum_{h \\in h_t } \\pr(h \\mid \\mu ) r(t , h ) \\\\",
    "\\times&\\!\\sum_{\\tau=1}^t \\frac{\\partial \\ln \\pr\\left(n(\\tau , h ) ,   a(\\tau , h)\\!\\mid\\!h^{\\tau-1 } , \\mu \\right ) } { \\partial w_k}\\;\\;.    \\end{split}\\ ] ]    but , in the spirit of reinforcement learning , we can not assume the knowledge of a world model that would allow us to calculate @xmath73 , so we must retreat to stochastic gradient descent instead .",
    "we sample from the distribution of histories by interacting with the environment , and calculate during each trial an estimate of the gradient , accumulating the quantities : @xmath74 for all @xmath55 . for a particular policy architecture , this can be readily translated into a gradient descent algorithm that is guaranteed to converge to a local optimum of @xmath69 .",
    "now let us consider the case in which the action is factored , meaning that each action @xmath75 consists of several components @xmath76 .",
    "we can consider two kinds of controllers : a _ joint controller _ is a policy mapping observations to the complete joint distribution @xmath77 ; a _ factored controller _ is made up of independent sub - policies @xmath78 ( possibly with a dependence on individual internal state ) for each action component .",
    "factored controllers can represent only a subset of the policies represented by joint controllers .",
    "obviously , any product of policies for the factored controller @xmath79 can be represented by a joint controller @xmath80 , for which @xmath81 . however , there are some stochastic joint controllers that can not be represented by any factored controller , because they require coordination of probabilistic choice across action components , which we illustrate by the following example .",
    "the first action component controls the liquid component of a meal @xmath82 and the second controls the solid one @xmath83 . for",
    "the sake of argument , let us assume that sticking to one combination or another is not as good as a `` mixed strategy '' , meaning that for a healthy diet , we sometimes want to eat milk with cereal , other times vodka with pickles . the optimal policy is randomized , say @xmath84 of the time @xmath85 and @xmath86 of the time @xmath87 .",
    "but when the components are controlled independently , we can not represent this policy .",
    "with randomization , we are forced to drink vodka with cereal or milk with pickles on some occasions .    because we are interested in individual agents learning independent policies , we concentrate on learning the best factored controller for a domain , even if it is suboptimal in a global sense . requiring a controller to be factored simply puts constraints on the class of policies , and therefore distributions @xmath88 , that can be represented .",
    "the stochastic gradient - descent techniques of the previous section can still be applied directly in this case to find local optima in the controller space .",
    "we will call this method _ joint gradient descent_.      the next step is to learn to choose action components not centrally , but under the distributed control of multiple agents .",
    "one obvious strategy would be to have each agent perform the same gradient - descent algorithm in parallel to adapt the parameters of its own local policy @xmath89 .",
    "perhaps surprisingly , this _ distributed gradient descent _ ( dgd ) method is very effective .    for factored controllers , distributed gradient descent is equivalent to joint gradient descent .",
    "_ proof : _ we will show that for both controllers the algorithm will be stepwise the same , so starting from the same point in the search space , on the same data sequence , the algorithms will converge to the same locally optimal parameter setting . for simplicity",
    "we assume a degenerate set of internal controller states @xmath26 .",
    "then a factored controller , @xmath90 can be described as @xmath91 .",
    "the corresponding history for an individual agent @xmath4 is @xmath92 .",
    "it is clear that a collection @xmath93 of individual histories , one for each agent , specifies the joint history @xmath90 .",
    "the joint gradient descent algorithm requires that we draw sample histories from @xmath94 and that we do gradient descent on @xmath95 with a sample of the gradient at each time step @xmath55 in the history equal to @xmath96    whether a factored controller is being executed by a single agent , or it is implemented by agents individually executing policies @xmath89 in parallel , joint histories are generated from the same distribution @xmath97 .",
    "so the distributed algorithm is sampling from the correct distribution .",
    "now , we must show that the weight updates are the same in the distributed algorithm as in the joint one .",
    "let @xmath98 be the set of parameters controlling action component @xmath99 .",
    "then @xmath100 that is , the action probabilities of agent @xmath101 are independent of the parameters in other agents policies . with this in mind , for factored controllers , the derivative in expression  ( [ eqe ] ) becomes @xmath102 thus , the same weight updates will be performed by dgd as by joint gradient descent on a factored controller .    ' '' ''",
    "this theorem shows that policy learning and control over component actions can be distributed among independent agents who are not aware of each others choice of actions .",
    "an important requirement , though , is that agents perform simultaneous learning ( which might be naturally synchronized by the coming of the rewards ) .",
    "in game theory , the nash equilibrium is a common solution concept . because gradient descent methods can often be guaranteed to converge to local optima in the policy space , it is useful to understand how those points are related to nash equilibria .",
    "we will limit our discussion to the two - agent case , but the results are generalizable to more agents .",
    "a nash equilibrium is a pair of strategies such that deviation by one agent from its strategy , assuming the other agent s strategy is fixed , can not improve the overall performance .",
    "formally , in an , a _ nash equilibrium _",
    "point is a pair of strategies @xmath103 such that : @xmath104 for all @xmath105 . when the inequalities are strict , it is called a _ strict nash equilibrium_.    every discounted stochastic game has at least one nash equilibrium point  @xcite .",
    "it has been shown that under certain convexity assumptions about the shape of payoff functions , the gradient - descent process converges to an equilibrium point  @xcite .",
    "it is clear that the optimal nash equilibrium point ( the nash equilibrium with the highest value ) in an   also is a possible point of convergence for the gradient descent algorithm , since it is the global optimum in the policy space .",
    "let us return to the game described in figure  [ craig ] .",
    "it has two + optimal strict nash equilibria at @xmath50 and @xmath51 .",
    "it also has a set of sub - optimal nash equilibria @xmath106 , where @xmath107 can take on any value in the interval @xmath108 $ ] and @xmath109 can take any value in the interval @xmath110 $ ] .",
    "the sub - optimal nash equilibria represent situations in which the first agent always chooses the bottom branch and the second agent acts moderately randomly in state @xmath37 . in such cases , it is strictly better for the first agent to stay on the bottom branch with expected value @xmath39 . for the second agent ,",
    "the payoff is @xmath39 no matter how it behaves , so it has no incentive to commit to a particular action in state @xmath37 ( which is necessary for the upper branch to be preferred ) .    in this problem ,",
    "the nash equilibria are also all local optima for the gradient descent algorithm .",
    "unfortunately , this equivalence only holds in one direction in the general case .",
    "we state this more precisely in the following theorems .",
    "every strict nash equilibrium is a local optimum for gradient descent in the space of parameters of a factored controller .    _",
    "proof : _ assume that we have two agents and denote the strategy at the point of strict nash equilibrium as @xmath111 encoded by parameter vector @xmath112 . for simplicity , let us further assume that @xmath111 is not on the boundary of the parameter space , and each weight is locally relevant : that is , that if the weight changes , the policy changes , too .    by the definition of nash equilibrium ,",
    "any change in value of the parameters of one agent without change in the other agent s parameters results in a decrease in the value @xmath69 .",
    "in other words , we have that @xmath113 _ and _",
    "@xmath114 for all @xmath115 and @xmath4 at the equilibrium point .",
    "thus , @xmath116 for all @xmath117 at @xmath111 , which implies it is a singular point of @xmath69 .",
    "furthermore , because the value decreases in every direction , it must be a maximum .    in the case of a locally irrelevant parameter @xmath117 , @xmath69 will have a ridge along its direction .",
    "all points on the ridge are singular and , although they are not strict local optima , they are essentially local optima for gradient descent .    ' '' ''    the problem of nash equilibria on the boundary of the parameter space is an interesting one .",
    "whether or not they are convergence points depends on the details of the method used to keep gradient descent within the boundary .",
    "a particular problem comes up when the equilibrium point occurs when one or more parameters have infinite value ( this is not uncommon , as we shall see in section  [ experiments ] ) . in such cases ,",
    "the equilibrium can not be reached , but it can usually be approached closely enough for practical purposes .",
    "some local optima for gradient descent in the space of parameters of a factored controller are not nash equilibria .    _",
    "proof : _ consider a situation in which each agent s policy has a single parameter @xmath118 , so the policy space can be described by @xmath119 .",
    "we can construct a value function @xmath120 such that for some @xmath121 , @xmath122 has two modes , one at @xmath123 and the other at @xmath124 , such that @xmath125 .",
    "further assume that @xmath126 and @xmath127 each have global maxima @xmath123 and @xmath124 .",
    "then @xmath123 is a local optimum that is not a nash equilibrium .    ' '' ''",
    "there are no established benchmark problems for multi - agent learning . to illustrate our method we present empirical results for two problems : the simple coordination problem of figure  [ craig ] and a small multi - agent soccer domain .",
    "we originally discussed policies for this problem using three weights , one corresponding to each of the @xmath48 probabilities .",
    "however , to force gradient descent to respect the bounds of the simplex , we used the standard boltzmann encoding , so that for agent @xmath4 in state @xmath128 there are two weights , @xmath129 and @xmath130 , one for each action .",
    "the action probability is coded as a function of these weights as @xmath131    we ran dgd with a learning rate of @xmath132 and a discount factor of @xmath133 ; the results are shown in figure  [ empir1 ] .",
    "the graph of a sample run illustrates how the agents typically initially move towards a sub - optimal policy .",
    "the policy in which the first agent always takes action @xmath33 and the second agent acts fairly randomly is a nash equilibrium , as we saw in section  [ bs ] .",
    "however , this policy is not exactly representable in the boltzmann parameterization because it requires one of the weights to be infinite to drive a probability to either @xmath134 or @xmath135 .",
    "so , although the algorithm moves toward this policy , it never reaches it exactly .",
    "this means that there is an advantage for the second agent to drive its parameter toward @xmath134 or @xmath135 , resulting in eventual convergence toward a global optimum ( note that , in this parameterization , these optima can not be reached exactly , either ) .",
    "the average of @xmath136 runs shows that the algorithm always converges to a pair of policies with value very close to the maximum value of @xmath136 .",
    "we have also conducted experiments on a small soccer domain adapted from littman  @xcite .",
    "the game is played on a @xmath137 grid as shown in figure  [ fig : soccer_field ] .",
    "there are two learning agents on one team and a single opponent with a fixed strategy on the other . every time the game begins , the learning agents are randomly placed in the right half of the field , and the opponent in the left half of the field .",
    "each cell in the grid contains at most one player .",
    "every player on the field ( including the opponent ) has an equal chance of initially possessing the ball .    at each time step ,",
    "a player can execute one of the six actions : @xmath138 .",
    "when an agent passes , the ball is transferred to the other agent on its team on the next time step .",
    "once all players have selected actions , they are executed in a random order .",
    "when a player executes an action that would move it into the cell occupied by some other player , possession of the ball goes to the stationary player and the move does not occur .",
    "when the ball falls into one of the goals , the game ends and a reward of @xmath139 is given .",
    "we made a partially observable version of the domain to test the effectiveness of dgd : each agent can only obtain information about which player possesses the ball and the status of cells to the north , south , east and west of its location .",
    "there are 3 possible observations for each cell : whether it is open , out of the field , or occupied by someone . in figure",
    "[ fig : soccer_graphs ] , we compare the learning curves of dgd to those of q - learning with a central controller for both the completely observable and the partially observable cases .",
    "we also show learning curves of dgd without the action _",
    "pass _ in order to measure the _ cooperativeness _ of the learned policies .    the graphs in the figure summarize simulations of the game against three different fixed - strategy opponents : + @xmath140 random : executes actions uniformly at random .",
    "+ @xmath140 greedy : moves toward the player possessing the ball and stays there .",
    "whenever it has the ball , it rushes to the goal .",
    "+ @xmath140 defensive : rushes to the front of its own goal , and stays or moves at random , but never leaves the goal area .",
    "we show the average performance over @xmath136 runs with error bars for standard deviation .",
    "the learning rate was @xmath141 for dgd and @xmath142 for q - learning , and the discount factor was @xmath143 , throughout the experiments .",
    "each agent in the dgd team learned a reactive policy .",
    "the policy s parameters were initialized by drawing uniformly at random from the appropriate domains .",
    "we used @xmath144-greedy exploration with @xmath145 for q - learning .",
    "the performance in the graph is reported by evaluating the greedy policy derived from the q - table .    because , in the completely observable case , this domain is an mdp  ( the opponent s strategy is fixed , so it is not really an adversarial game )",
    ", q - learning can be expected to learn the optimal joint policy , which it seems to do .",
    "it is interesting to note the slow convergence of completely observable q - learning against the random opponent .",
    "we conjecture that this is because , against a random opponent , a much larger part of the state space is visited .",
    "the table - based value function offers no opportunity for generalization , so it requires a great deal of experience to converge .",
    "as soon as observability is restricted , q - learning no longer reliably converges to the best strategy .",
    "the joint q - learner now has as its input the two local observations of the individual players .",
    "it behaves quite erratically , with extremely high variance because it sometimes converges to a good policy and sometimes to a bad one .",
    "this unreliable behavior can be attributed to the well - known problems of using value - function approaches , and especially q - learning , in pomdps .",
    "the individual dgd agents have stochasticity in their action choices , which gives them some representational abilities unavailable to the q - learner .",
    "we tried additional experiments in which the dgd agents had 4-state fsc s .",
    "their performance did not improve appreciably .",
    "we expect that , in future experiments on a larger field with more players , it will be important for the agents to have internal state .",
    "despite the fact that they learn independently , the combination of policy search plus a different policy class allows them to gain considerably improved performance .",
    "we can not tell how close this performance is to the optimal performance with partial observability , because it would be computationally impractical to solve the   exactly .",
    "bernstein et .",
    "@xcite show that in the finite - horizon case two - agent poipsgs are _ harder _ to solve than pomdps ( in the worst - case complexity sense ) .",
    "it is also important to see that the two dgd agents have learned to cooperate in some sense : when the same algorithm is run in a domain without the `` pass '' action , which allows one agent to give the ball to its teammate , performance deteriorates significantly against both defensive and greedy opponents . since the agents do nt know where they are with respect to the goal",
    ", they probably choose to pass whenever they are faced with the opponent . against a completely random opponent , both strategies do equally well .",
    "it is probably sufficient , in this case , to simply run straight for the goal , so cooperation is not necessary .",
    "we performed some additional experiments in a two - on - two domain in which one opponent behaved greedily and the other defensively . in this domain ,",
    "the completely observable state space is so large that it is difficult to even store the q table , let alone populate it with reasonable values .",
    "thus , we just compare two 4-state dgd agents with a limited - view centrally controlled q - learning algorithm . not surprisingly , we find that the dgd agents are considerably more successful .",
    "finally , we performed informal experiments with an increasing number of opponents .",
    "the opponent team was made up of one defensive agent and an increasing number of greedy agents .",
    "for all cases in which the opponent team had more than two greedy agents , dgd led to a defensive strategy in which , most of the time , the agents all rushed to the front of their goal and stayed there forever .",
    "we have presented an algorithm for distributed learning in cooperative multi - agent domains .",
    "it is guaranteed to find local optima in the space of factored policies .",
    "we can not show , however , that it always converges to a nash equilibrium , because there are local optima in policy space that are not nash equilibria .",
    "the algorithm performed well in a small simulated soccer domain .",
    "it will be important to apply this algorithm in more complex domains , to see if the gradient remains strong enough to drive the search effectively , and to see whether local optima become problematic .",
    "an interesting extension of this work would be to allow the agents to perform explicit communication actions with one another to see if they are exploited to improve performance in the domain .",
    "in addition , there may be more interesting connections to establish with game theory , especially in relation to solution concepts other than nash equilibrium , which may be more appropriate in cooperative games .",
    "craig boutilier and michael schwarz gave relevant ideas and comments .",
    "franjo ivanci pointed out an inaccuracy in the description of the basic algorithm .",
    "leonid peshkin was supported by grants from nsf and ntt ; nicolas meuleau in part by research grant from ntt ; kee - eung kim in part by afosr / rlf 30602 - 95 - 1 - 0020 ; and leslie pack kaelbling in part by a grant from ntt and in part by darpa contract # dabt 63 - 99 - 1 - 0012 .",
    "d.  bernstein , s.  zilberstein , and n.  immerman .",
    "the compplexity of decentralized control of markov decision processes . in _ proceedings of the sixteenth conference on uncertainty in artificial intelligence _ , 2000 .",
    "c.  claus and c.  boutilier .",
    "the dynamics of reinforcement learning in cooperative multiagent systems . in _ proceedings of the tenth innovative applications of artificial intelligence conference _ , pages 746752 , madison , wisconsin ,",
    "usa , july 1998 .",
    "n.  meuleau , l.  peshkin , k .- e .",
    "kim , and l.  p. kaelbling .",
    "learning finite - state controllers for partially observable environments . in _ proceedings of the fifteenth conference on uncertainty in artificial intelligence _ , pages 427436 .",
    "morgan kaufmann , 1999 .",
    "l.  peshkin , n.  meuleau , and l.  p. kaelbling .",
    "learning policies with external memory . in i.",
    "bratko and s.  dzeroski , editors , _ proceedings of the sixteenth international conference on machine learning _ , pages 307314 , san francisco , ca , 1999 .",
    "morgan kaufmann .",
    "j.  schneider , w .- k .",
    "wong , a.  moore , and m.  riedmiller .",
    "distributed value functions . in i.",
    "bratko and s.  dzeroski , editors , _ proceedings of the sixteenth international conference on machine learning _ , pages 371378 , san francisco , ca , 1999 .",
    "morgan kaufmann .",
    "s.  singh , t.  jaakkola , and m.  jordan . learning without state - estimation in partially observable markovian decision processes . in _",
    "machine learning : proceedings of the eleventh international conference_. 1994 .",
    "r.  j. williams . a class of gradient - estimating algorithms for reinforcement learning in neural networks . in _ proceedings of the ieee first international conference on neural networks _ , san diego , california , 1987"
  ],
  "abstract_text": [
    "<S> cooperative games are those in which both agents share the same payoff structure . </S>",
    "<S> value - based reinforcement - learning algorithms , such as variants of q - learning , have been applied to learning cooperative games , but they only apply when the game state is completely observable to both agents . </S>",
    "<S> policy search methods are a reasonable alternative to value - based methods for partially observable environments . in this paper </S>",
    "<S> , we provide a gradient - based distributed policy - search method for cooperative games and compare the notion of local optimum to that of nash equilibrium . </S>",
    "<S> we demonstrate the effectiveness of this method experimentally in a small , partially observable simulated soccer domain . </S>"
  ]
}