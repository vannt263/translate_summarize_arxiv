{
  "article_text": [
    "in the last decade , some very effective frameworks for image restoration have been proposed that ( a ) exploit long - distance correlations in natural images , and ( b ) use patches instead of pixels to robustly compare photometric similarities .",
    "this includes the classical non - local means ( nlm ) algorithm @xcite , and the more sophisticated bm3d algorithm @xcite .",
    "the latter combines the nlm framework with other classical algorithms , and is widely considered as the state - of - the - art in image denoising .",
    "we refer the reader to @xcite for a comprehensive review of patch - based algorithms .",
    "let @xmath5 be some linear indexing of the input noisy image . in nlm ,",
    "the restored image @xmath6 is computed using the simple formula @xmath7 here , @xmath8 is some weight ( affinity ) assigned to pixels @xmath9 and @xmath10 , and @xmath11 is some non - local ( sufficiently large ) neighborhood of pixel @xmath9 over which the averaging is performed @xcite . in particular , for a given pixel @xmath9 , let @xmath12 denote the restriction of @xmath13 to a square window around @xmath9 . letting @xmath14 be the length of this window , this associates every pixel @xmath9 with a point @xmath12 in @xmath15 ( the patch space ) .",
    "the weights in nlm are set to be @xmath16 , where @xmath17 is the euclidean distance between @xmath12 and @xmath18 , and @xmath19 is a smoothing parameter .",
    "recently , it was demonstrated in @xcite that the robustness of the non - local means ( nlm ) algorithm @xcite can be improved by incorporating @xmath4 regression into the nlm framework .",
    "the idea was to fix some @xmath20 , and consider the following unconstrained optimization on the patch space : @xmath21 where @xmath8 are the weights used in nlm ( one could also use other weights , e.g. , see @xcite ) .",
    "the denoised pixel @xmath22 was then set to be the center pixel in @xmath23 .",
    "note that this reduces to the simple formula in when @xmath24 . in this case",
    ", the optimization is performed pixel - wise .",
    "for any other value of @xmath1 , the optimization in becomes a generic optimization on the patch space  the regression needs to be performed on patches and not just pixels . in particular , when @xmath25 , the resulting estimator turns out to be more robust to `` outliers '' in the patch space ( compared to standard nlm ) , and this leads to significant improvement in the denoising quality .",
    "we refer the reader to @xcite for an intuitive understanding of the robustness in nlpr , and for denoising results on synthetic and natural images .    note that we can generally write the optimization problem in as @xmath26 where @xmath27 are given points in @xmath28 , and @xmath29 are some positive weights .",
    "motivated by the work on algorithms for @xmath4 minimization in @xcite , the authors in @xcite proposed to optimize using iteratively reweighted least - squares ( irls ) .",
    "fixing some small @xmath30 , and initializing @xmath31 as the nlm output , the update rule was set to be @xmath32 where @xmath33",
    "we refer the reader to @xcite for the heuristics behind the irls update in .",
    "extensive numerical experiments show us the that the algorithm is globally convergent ( for arbitrary @xmath31 ) in the convex regime @xmath2 , and locally convergent ( fails very rarely compared to , say , the gradient or newton method ) in the non - convex regime @xmath3 , provided that @xmath34 is set as the nlm estimate .",
    "the former observation can be explained using the existing literature on the weiszfeld algorithm @xcite , which is perhaps less well - known in the signal processing community . in this letter",
    ", we adapt the `` majorize - minimize '' framework introduced in @xcite to specifically analyze when @xmath3 .",
    "the analysis automatically covers the case @xmath35 .",
    "this is the content of section [ analysis ] . in particular",
    ", we will show that the algorithm forces the cost to be non - increasing as the iteration progresses , and that it exhibits linear convergence both in the convex and non - convex ( when convergence does occur ) regime .",
    "the key question is what is the cost associated with the irls iterations in ? this must be resolved even before we ask questions about convergence .",
    "it turns out that the iterations corresponds to a regularized version of the original cost .",
    "this is given by @xmath36 where @xmath37 is the regularized version of the euclidean norm , @xmath38 note that @xmath39 corresponds to the original cost when @xmath40 .",
    "it can be argued that the minimizers of @xmath39 converge to the minimizer of the original problem as @xmath41 tends to zero . in other words , for sufficiently small @xmath41 , the minimizer of @xmath39 is close to that of the original problem . henceforth , to simplify notation , we fix some small @xmath42 and denote @xmath39 by @xmath43 .",
    "we note that in @xcite , the authors proposed to start with , say , @xmath44 , and then gradually shrink it to zero as the iteration progresses .",
    "while this does tend to speed up the convergence , the associated analysis becomes quite complicated .",
    "the advantage we get by considering the regularized problem is that the function @xmath45 is smooth ( infinitely differentiable ) .",
    "this allows us to use the powerful tools of smooth optimization .",
    "moreover , @xmath43 inherits the convex nature of the original problem , namely , that it is strictly convex for @xmath35 .",
    "since @xmath43 is smooth , it suffices to show that its hessian is positive definite .",
    "in fact , the gradient ( denoted by @xmath46 ) and the hessian ( denoted by @xmath47 ) are given by @xmath48 and @xmath49.\\ ] ] here , @xmath50 is the identity matrix of size @xmath51 . for any non - zero @xmath52 , @xmath53.\\ ] ] since @xmath54 , the quadratic form is strictly larger than @xmath55 this is non - negative if and only if @xmath35 .",
    "therefore , @xmath43 is strictly convex in this case , and has a unique global minimizer @xmath56 for which @xmath57 . on the other hand , it is not difficult to see that @xmath43 need not be convex when @xmath58 .",
    "the best we can hope for in this case is that the iterates in converge to some local stationary point .",
    "in fact , we can show that    [ thm1 ] the irls update in guarantees the following :    1 .   for @xmath59 ,",
    "the sequence @xmath60 is strictly monotonic , @xmath61 for all @xmath62 .",
    "when @xmath35 , @xmath63 converges linearly to the unique global minimizer of @xmath43 .",
    "3 .   for @xmath3 , under some additional assumptions , the convergence is again linear and the limit of @xmath63 is a stationary point of @xmath43 .    by linear convergence , we mean that the convergence happens at an exponential rate .",
    "the relaxation property is particularly important for the non - convex setting , providing the guarantee that the cost at the end of the iterations is less than that obtained from the initial estimate @xmath31 . in optimization literature ,",
    "one calls @xmath63 a _ relaxation sequence _ if @xmath61 .",
    "since @xmath43 is trivially bounded below , this implies convergence of the sequence @xmath60 . as we will see , the iterates in _ unconditionally _ generate a relaxation sequence in both the convex and non - convex regime",
    "this property turns out to be a central ingredient in the guarantees provided by theorem [ thm1 ] .",
    "one of the cases can however be ruled out immediately :    [ bdd ] for @xmath20 , the irls iterates @xmath63 are bounded ( do not escape to infinity ) .",
    "this is a simple consequence of the observation that @xmath64 as @xmath65 . indeed ,",
    "if @xmath63 does escape to infinity , then we would have a contradiction since we just showed that @xmath60 is bounded .    in the convex regime",
    ", we will show that oscillations can also be ruled out .",
    "to do so in the non - convex regime , we will need additional assumptions .",
    "to establish theorem [ thm1 ] , we will use the majorize - minimize ( mm ) framework from @xcite . in this framework",
    ", the key idea is to globally approximate @xmath43 from above using a sequence of quadratic functions .",
    "more precisely , after having found @xmath66 , we construct a function @xmath67 such that    * @xmath68 for all @xmath69 . * @xmath70 .",
    "* @xmath71 has @xmath72 in as its unique global minimizer .",
    "once we have @xmath73 with the above properties , we immediately see that @xmath74 that is , we are guaranteed that @xmath75 is a relaxation sequence .",
    "we now need to specify @xmath71 .",
    "the following choice will suffice : @xmath76 where @xmath77 is as defined in .",
    "note that the linear part of @xmath71 is simply the linear approximation of @xmath43 at @xmath66 , and the quadratic form is derived from the dominant part of @xmath78 . by construction , @xmath79 .",
    "moreover , it is clear that @xmath80 is strictly convex ( for all @xmath1 ) , and has a global unique minimizer .",
    "setting @xmath81 to be this minimizer , we have @xmath82 substituting for @xmath46 , we get the update rule in .",
    "to complete the proof , we need to show that @xmath68 .",
    "note that we can write @xmath83 as @xmath84 we substitute the following above : @xmath85 and @xmath86 this allows us to simplify the expression to @xmath87 where @xmath88 and @xmath89 .",
    "it can be verified that each term in the sum is non - negative for any @xmath59 ( for @xmath90 and @xmath91 this is obvious ) .",
    "this shows that @xmath92 , concluding the proof of .      since the sequence @xmath93 is monotonic and bounded below",
    ", it is convergent . in particular , @xmath94 as @xmath95 .",
    "so , what can we say about the sequence @xmath63 ?    [ diff ] we claim that @xmath96 as @xmath62 gets large .    to do so , we use , @xmath97 and the majorizing property , @xmath98 combining these , we see that @xmath99 now , from we have the trivial bound @xmath100 .",
    "we can then write @xmath101,\\ ] ] where @xmath102 since @xmath93 is convergent , we arrive at our claim .    note that can not directly conclude from proposition [ diff ] that @xmath63 is convergent .",
    "however , since @xmath63 is bounded , it has convergent subsequences ( by compactness ) . in the convex regime",
    ", we can say something more :    for @xmath35 , every convergent subsequence has the same limit , and this limit is the unique stationary point of @xmath43 . in particular , it is necessary that @xmath63 converges to @xmath56    we now establish the above claim .",
    "let @xmath103 be a subsequences that converges to @xmath56 .",
    "we know that @xmath104 , so that the limit of both @xmath103 and @xmath105 must be @xmath56 .",
    "note that @xmath106 since both @xmath107 and @xmath108 are smooth , letting @xmath109 , we have @xmath110 that is , @xmath56 is a stationary point of @xmath43 . in the convex regime @xmath35 , we know that @xmath43 has a unique stationary point @xmath56 .",
    "therefore , every convergent subsequence of @xmath63 must have the same limit @xmath56 .    for @xmath3 ,",
    "the above claim is true only under certain local assumptions .",
    "the problem in this case is that there can be multiple stationary points of @xmath43 , and the previous argument breaks down ( as is typical with non - convex problems ) .",
    "all we know in this case is that @xmath63 is bounded , and that the entire @xmath63 can be restricted to a ball @xmath111 of radius @xmath112 around @xmath56 .",
    "suppose we assume that that the initialization @xmath31 is `` good '' , in that it is situated sufficiently close to a local ( probably global ) minimizer @xmath56 .",
    "it is then possible that @xmath112 is small enough and @xmath111 contains no other stationary points of @xmath43 . in this case , we are guaranteed that every convergent subsequence , and hence the whole sequence @xmath63 , converges to @xmath56 .",
    "plots of @xmath113 versus @xmath62 for suggests a linear trend both for the convex and non - convex cases ( assuming convergence for the latter ) . for the convex regime",
    ", we can indeed guarantee that    [ linconv ] for @xmath35 , @xmath114    in other words , the convergence is exponential , where the convergence rate is controlled by @xmath115 . to establish our claim , we being by comparing @xmath43 at the points @xmath72 and the linear combination @xmath116",
    "( we will define @xmath117 later ) , @xmath118 the first inequality follows from majorization , and the second from the optimality of @xmath72 . from",
    ", we can write @xmath119 as @xmath120 on the other hand , @xmath121 using this to eliminate the term containing @xmath122 , we can write @xmath119 as @xmath123 now , set @xmath124 then @xmath125 , and hence @xmath126 by construction , @xmath127 for all @xmath62 .",
    "we are done if we can show that @xmath128 for some constant @xmath115 . by taylor",
    "s theorem , @xmath129 where @xmath130 is some point on the segment joining @xmath56 and @xmath66 . plugging and in , @xmath131 where @xmath132 denotes the smallest eigenvalue of the matrix @xmath133 .",
    "now , since @xmath47 is continuous , @xmath134 approaches @xmath135 as @xmath136 .",
    "moreover , @xmath56 is a ( local ) minimizer .",
    "hence , @xmath137 is necessarily positive semidefinite , that is , @xmath138 .",
    "this is true for @xmath20 .",
    "therefore , for all sufficiently large @xmath62 , @xmath139 , where @xmath140 and where @xmath141 .",
    "for the convex regime @xmath35 , @xmath137 is guaranteed to be positive definite , so that @xmath142 .",
    "this completes the proof of proposition [ linconv ] .    for the non - convex",
    "setting , the above argument holds under additional assumptions :    for @xmath143 , assume that @xmath63 converges to the local minimizer @xmath56 , and that @xmath137 is positive definite",
    ". then @xmath60 converges linearly to @xmath144 .",
    "the linear convergence of irls is typical of first - order methods .",
    "we have also tried second - order newton methods for optimizing , which are guaranteed to exhibit quadratic convergence ( locally ) . indeed , newton methods typically require less than half the number of iterations needed by the updates in to reach a given accuracy .",
    "however , the cost of a single newton step ( computation of @xmath47 and its inversion ) is significantly more than that of the simple update in . as a result",
    ", the total execution time of irls turns out to be smaller than that of newton methods .",
    "the other point that we noticed from the numerical simulations is that irls is much more stable than newton ( or gradient descent ) methods in the non - convex regime . in particular ,",
    "the iterates in the newton method often diverge to infinity if the initialization is not `` close '' to a local minimum .",
    "however , the irls iterates never escape to infinity ( this is clear from theorem [ thm1 ] ) , and almost always converge to the global optimum when we initialize using the nlm output .",
    "in rare cases when gets `` stuck '' in a local minimum ( say , due to bad initialization ) , newton methods are found to have the same problem .",
    "it would be interesting to see if one could give a more accurate analysis of the irls algorithm in the non - convex regime .    .",
    "the author would like to thank prof .",
    "gilad lerman and prof .",
    "amit singer for interesting discussions .",
    "i. daubechies , r. devore , m. fornasier , c. s. gunturk `` iteratively reweighted least squares minimization for sparse recovery , '' communications on pure and applied mathematics , vol .",
    "63 , pp . 1 - 38 , 2009"
  ],
  "abstract_text": [
    "<S> recently , it was demonstrated in @xcite that the robustness of the classical non - local means ( nlm ) algorithm @xcite can be improved by incorporating @xmath0 regression into the nlm framework . </S>",
    "<S> this general optimization framework , called non - local patch regression ( nlpr ) , contains nlm as a special case . </S>",
    "<S> denoising results on synthetic and natural images show that nlpr consistently performs better than nlm beyond a moderate noise level , and significantly so when @xmath1 is close to zero . an iteratively reweighted least - squares ( irls ) </S>",
    "<S> algorithm was proposed for solving the regression problem in nlpr , where the nlm output was used to initialize the iterations . </S>",
    "<S> based on exhaustive numerical experiments , we observe that the irls algorithm is globally convergent ( for arbitrary initialization ) in the convex regime @xmath2 , and locally convergent ( fails very rarely using nlm initialization ) in the non - convex regime @xmath3 . in this letter , we adapt the `` majorize - minimize '' framework introduced in @xcite to explain these observations </S>",
    "<S> .    non - local means , non - local patch regression , @xmath4 minimization , non - convex optimization , iteratively reweighted least - squares , majorize - minimize , stationary point , relaxation sequence , linear convergence . </S>"
  ]
}