{
  "article_text": [
    "assume we have some data matrix of the form @xmath0 for @xmath1 and @xmath2 , with @xmath0 observed for @xmath3 and missing elsewhere .",
    "row features @xmath4 and column features @xmath5 are available . without loss of generality assume throughout @xmath6 .    as motivation , consider estimating the following hierarchical bayesian model for @xmath7 with row- and column - specific latent variables @xmath8 with @xmath9 : @xmath10 where",
    "the covariance matrices are fixed but all other parameters  @xmath11 and @xmath12  are to be estimated .",
    "is an appealing model for collaborative filtering with side information , since latent parameters are regularized toward feature - based predictions .",
    "indeed , it has proven a popular model for collaborative filtering problems , with variants of it appearing in e.g. @xcite and @xcite .",
    "because of the dimensionality of the problem inference is often done via maximum a posteriori ( map ) estimation or variational methods instead of sampling from the posterior .    despite the modeling appeal of , the map estimation problem is forbidding at first blush . writing @xmath13",
    ", map inference requires minimizing the objective @xmath14 which is highly nonconvex as given .",
    "for example , we could negate @xmath15 and @xmath16 or permute the @xmath17 factors without altering the objective value .",
    "a typical approach is to use alternating methods to look for local modes @xcite .",
    "we show that after reformulating the problem map inference for can be carried out scalably using convex optimization , provided @xmath18 is log - concave . using generalized nuclear norm regularization",
    "we show how to estimate a broad class of models exploiting various kinds of row and column side information .",
    "many if not most classical techniques in multivariate statsitics amount to least - squares approximation of @xmath7 or some appropriately preprocessed @xmath19 by a matrix @xmath20 with rank @xmath21 ( e.g. for pca , we obtain @xmath22 by centering and scaling the columns of @xmath7 ) @xcite . that is , we solve @xmath23 if @xmath22 is fully observed then has a closed - form solution in terms of the singular value decomposition of @xmath22 , often with simple postprocessing afterward .",
    "despite the apparent simplicity of , this framework encompasses principal components analysis , correspondence analysis , linear discriminant analysis , canonical correlation analysis , reduced - rank regression , multidimensional scaling , and spectral clustering .",
    "unfortunately , finding the best rank-@xmath24 approximation to @xmath7 is in general a non - convex problem .",
    "if @xmath7 is not fully observed or our objective is not squared - error loss , the problem becomes intractible @xcite .",
    "this difficulty has been addressed in recent years by including a penalty term proportional to the nuclear norm of @xmath20 , which acts as a convex relaxation for the rank - constrained problem @xcite ; i.e. one can replace with @xmath25 the minimizer has low rank for sufficiently large @xmath26 .",
    "although is convex , convex optimization approaches are routinely dismissed in the literature as impractical for large problems ; see e.g. @xcite .",
    "one can generalize the penalty in to obtain @xmath27 for generic positive semidefinite @xmath28 and @xmath29 . as with , the apparent simplicity in masks the immense diversity of learning tasks to which it may be applied .",
    "for example , @xcite propose a robust form of pca by replacing the squared - error loss with a huberized loss .",
    "a few authors have considered replacing the nuclear norm with the generalized nuclear norm as in .",
    "notably @xcite advocate a special case of with diagonal @xmath28 and @xmath29 , and @xcite apply the generalized nuclear norm to the structure - from - motion problem in computer vision .",
    "@xcite frame collaborative filtering in very general terms of estimating compact linear operators in hilbert space . their proposals for regularizing @xmath20 overlap in part with ours",
    "but they do not address the relationship with bayesian map estimation or discuss scalable computation , two central elements of our work .",
    "the relationship between standard nuclear norm penalization in and gaussian priors on latent variables has been discussed in @xcite and elsewhere , but we have not encountered discussion of the connection between hierarchical models such as and .",
    "the starting point for reduced - rank modeling is some data matrix .",
    "interesting examples include :    collaborative filtering : :    @xmath0 stores the result of some interaction between    user @xmath30 and item @xmath31 ; e.g. ,    @xmath32 if user @xmath30 clicked on    advertisement @xmath31 and 0 otherwise .",
    "contingency tables : :    for two categorical variables with @xmath33 and @xmath34    levels , @xmath0 counts the number of times    @xmath35 co - occurred .",
    "a special case is the incidence    matrix for a bipartite graph .",
    "sparsely observed functional data : :    given noisy observations @xmath36 for function    @xmath37 at random locations @xmath38 , reconstruct    @xmath37 .",
    "@xmath38 could be values on a real    interval or some other metric space .",
    "here we have one row per function    and one column per @xmath39 that occurs at least once .",
    "multi - task learning : :    given feature vector @xmath40 predict the vector    @xmath41 .",
    "@xcite propose estimating    a vector of parameters for @xmath42 via low - rank matrix of    coefficients . in the statistics literature",
    "@xcite call such models    reduced - rank vector glms .    the starting point for modeling matrix data is choosing an appropriate model for the response variable in each entry .",
    "the most common choice is an exponential family model of the form @xmath43 we assume that , conditionally on row and column information , the @xmath0 are independent across entries with parameter @xmath44 for the @xmath35 entry .",
    "this assumption leads to an additive negative log - likelihood @xmath45 we assume further that the loss in is convex and differentiable .",
    "the model in is generally overparameterized unless we impose some structure on @xmath20 , and the art of modeling @xmath7 lies mostly in our choices about how to do so .",
    "reduced - rank modeling is a powerful technique for regularizing @xmath20 . as a very simple example",
    ", we might assume a model with row and column effects as well as a matrix of interactions : @xmath46 where @xmath47 is regularized to be of low rank , say @xmath24 . then becomes @xmath48 thus @xmath49 and @xmath50 can be seen as representations of row @xmath30 and column @xmath31 in a latent @xmath24-dimensional space , where the @xmath35 interaction term is large when @xmath49 and @xmath50 are close . besides aiding in prediction , informative @xmath51 or @xmath52 could measure quantities of intrinsic scientific interest such as a supreme court justice s ideology @xcite .",
    "the connection between nuclear - norm regularization and bayesian modeling arises from an important identity from @xcite : @xmath53 it follows that nuclear norm regularization is exactly equivalent to maximum a posteriori ( map ) estimation in the simplest bayesian hierarchical model with @xmath54 , as observed in @xcite and elsewhere .",
    "one thing we gain from viewing the model in terms of @xmath51 and @xmath52 is that the latent variables can be natural objects to model .",
    "for example , in the model @xmath55 is not centered at 0 but rather at some other prediction @xmath56 based on features . if rows represent users and some user has not been observed much , we may want to shrink his @xmath49 toward a prediction informed by demographic information such as age or sex .",
    "below we reproduce the map optimization criterion : @xmath57 the first step in reformulating as a convex problem is to partially minimize with respect to @xmath16 .",
    "@xmath58\\end{aligned}\\ ] ] fixing the other variables , solving for the optimal @xmath59 amounts to generalized ridge regression of response @xmath60 on features @xmath61 .",
    "writing @xmath62 and @xmath63 , the minimizer for the @xmath17th term is @xmath64 , so @xmath65 if we had modeled @xmath52 in a similar way , we would have obtained a similar expression of the form @xmath66 .",
    "continuing , we have @xmath67 suppressing the arguments of @xmath68 , we can rewrite the optimal objective value of the original problem as @xmath69 one interpretation of is that instead of penalizing the latent variables @xmath60 we are now penalizing the _ residuals _ of @xmath60 with respect to our model .",
    "we are shrinking @xmath60 toward the column space of @xmath61 instead of toward 0 .",
    "note that we would arrive at the same expression if we marginalized @xmath60 with respect to @xmath59 and carried out map inference on the marginal . in that case",
    "we would obtain marginal variance @xmath70 , whose inverse is exactly @xmath71 , as we can see by applying woodbury s matrix inversion lemma .    before continuing",
    "we will need to generalize the relationship between the problem in terms of @xmath20 vs. @xmath51 and @xmath52 :    [ propproj ] let @xmath72 and @xmath73 be positive semidefinite with @xmath6 .",
    "then for any function @xmath74 , we have @xmath75 where @xmath76 and @xmath77 are the moore - penrose pseudoinverses of @xmath28 and @xmath29 , and @xmath78 and @xmath79 are projections onto their respective null spaces .",
    "the implication of proposition  [ propproj ] is that we can replace the rhs of with the rhs of , which is convex in the three @xmath20 variables .",
    "if @xmath78 and @xmath79 are low - rank  in other words , if there are not too many unpenalized directions of @xmath51 and @xmath52  then we can take the optimization variables @xmath80 and @xmath81 to be small matrices .",
    "furthermore , proximal steps for @xmath82 will be with respect to the standard nuclear norm , so they can be solved in closed form using a soft - thresholded .",
    "let @xmath83 and @xmath84 denote projections onto the images of @xmath28 and @xmath29 .",
    "then @xmath85 we can see by changing variables to @xmath86 , @xmath87 , and @xmath88 . and follow from the fact that @xmath89 for any projection @xmath90 ( in this case @xmath83 ) , and similarly @xmath91 .    for any @xmath20 we can find @xmath92 for which @xmath93 . then @xmath94 holds because @xmath95 . holds because @xmath96 and we can attain the minimum by replacing @xmath82 with @xmath97 , which does not change the @xmath98 term .",
    "now we can further reformulate by replacing our interaction matrix @xmath99 with @xmath47 ; that is @xmath100 applying proposition [ propproj ] we obtain @xmath101 which we can simplify further as in for easier optimization .",
    "hierarchical bayesian modeling is one framework we might adopt to model the latent variables , but other quadratic regularization frameworks might be as or more appropriate for a given problem . as a simple example , the degenerate case @xmath102 corresponds to an improper ( flat ) prior on @xmath16 .",
    "in that case , the penalty on @xmath16 goes away and partially minimizing @xmath103 gives @xmath104 for @xmath105 ( note here @xmath106 since @xmath107 is a projection matrix ) .",
    "another limiting case is @xmath108 which constrains @xmath51 , and hence @xmath47 , to lie in the column space of @xmath61 .",
    "another interesting possibility is to replace the hat matrix @xmath107 in with some other linear smoother matrix on the latent variables , such as local regression , smoothing splines , or kernel regression on the features @xmath61 .",
    "as long as @xmath107 is low - rank and positive semidefinite with @xmath109 , our framework in section  [ seccomputation ] will apply .",
    "we will refer to a metric obtained in this way as residual - shrinking .",
    "it is also straightforward to impose other smoothness constraints on @xmath51 and @xmath52 such as smoothness with respect to a fourier basis , graph laplacian , or other covariance kernel .",
    "realizing the statistical gains of our modeling framework on large - scale data sets requires an optimization algorithm that is both general enough to accommodate our modeling framework , and scalable .",
    "in section [ secmodeling ] we saw that in general our task reduces to solving a problem of the form @xmath110 with @xmath28 and @xmath29 semidefinite .",
    "assume that after fixing @xmath47 it is easy to optimize over @xmath111 and @xmath112 .",
    "our general algorithmic framework is block proximal gradient descent . in general the proximal map for",
    "the generalized nuclear norm in on @xmath47 can be computationally awkward , but for the sorts of @xmath28 and @xmath29 that arise in section  [ secmodeling ] we can often massage into a friendlier form .",
    "proposition  [ propproj ] plays a crucial role , by allowing us to reformulate the problem as one with a standard nuclear norm penalty , so that proximal steps may be solved in closed form via a soft - thresholded singular value decomposition",
    ".    however , proximal gradient descent may still not be scalable since the svd of an @xmath113 matrix in general costs @xmath114 operations , which is unacceptable for very large problems . however , under certain circumstances , the complexity of a low - rank svd has a more favorable computational complexity , as we explore below",
    ".      it will be convenient here to introduce a shorthand descriptor for a matrix @xmath115 for which we can rapidly evaluate @xmath116 and @xmath117 .",
    "the meaning of `` rapid '' is relative and may vary across different contexts , but a good working definition will be that the total number of floating point operations scales with @xmath118 instead of @xmath119 .",
    "we will call such a matrix `` easy to apply , '' henceforth abbreviated ea .    the class of ea matrices is closed under transposition , addition and multiplication , so long as we do not combine too many of these operations .",
    "reasons @xmath115 might be ea include :    small : :    at least one of the dimensions of @xmath115 scales slowly , low - rank : :    @xmath120 where @xmath51 and @xmath52 are    small , or sparse : :    the time to apply @xmath115 is linear in the number of nonzero    entries , so if @xmath121 for most @xmath35    then @xmath115 is ea .",
    "this class includes diagonal and banded    matrices .",
    "similarly , we call @xmath115 easy - to - solve ( henceforth es ) if we can rapidly solve the systems @xmath122 and @xmath123 . examples of easy - to - solve matrices include diagonal or banded matrices , or any es matrix plus a low - rank matrix .",
    "this form is especially important because it applies to matrices of the form @xmath71 for low - rank @xmath107 with @xmath124 .",
    "our interest in ea matrices arises from the fact that the computational bottleneck of our algorithm is a soft - thresholded singular value decomposition of an @xmath113 matrix . in general",
    ", this operation requires @xmath114 time , but it can be much faster if the matrix is ea and we only need the top few singular values and vectors . in particular , given a good warm start  which is available for us  a high - precision singular value can be obtained after only a few applications of the matrix .    using from proposition  [ propproj ]",
    "we can reformulate as @xmath125    for simplicity of exposition we assume @xmath126 and that we have reduced to the form from after proposition  [ propproj ] . specifically , if @xmath127 is a sparse matrix containing the gradients of the loss wrt @xmath44 for @xmath128 and 0 otherwise , then the gradient wrt @xmath47 is @xmath129 where @xmath130 is the last iterate and @xmath39 is the step size .",
    "hence we can do fast proximal mappings for @xmath131 and @xmath132 .",
    "our first data example involves a data set from the `` what do you know '' prediction competition hosted on the website kaggle . here",
    "the rows correspond to 139,000 users of the test preparation web site grockit , and columns correspond to 6000 practice questions . in total",
    "there are 4.2 million binary observations with @xmath32 if student @xmath30 answered question @xmath31 correctly .",
    "the goal is to predict whether a user is likely to answer a question correctly , for instance to gauge whether a student has mastered a topic or question type .    in our first experiment",
    "we compare our optimization method with several state of the art methods for standard nuclear norm minimization that seem to be scalable to the problem sizes arising in this paper :    * proximal gradient descent  @xcite ( gd ) as implemented in the tfocs package  @xcite with the large - scale low - rank svd computation feature enabled * accelerated gradient descent  @xcite ( agd ) , as described in the tfocs package with similar tricks as in ( a ) for the svd computations and * the frank - wolfe ( fw ) or conditional gradient  @xcite algorithm adapted to the logistic loss function .",
    "we used the theoretical step - size @xmath133 in our experiments . for this method , at every iteration one needs to compute the top singular triple of a sparse matrix .",
    "tthe fw algorithm operates on the constrained version of the nuclear norm regularized problem i.e. @xmath134 .",
    "so , to make fair comparisons possible with fw , we used gd and agd adapted to the constrained version of the nuclear norm regularized problem .    for our algorithm , which we refer to by the moniker ",
    "ours \" in the plots  we used a projected gradient descent algorithm with a fixed step - size of @xmath135 .",
    "of course , the main algorithm bottleneck is in performing the low - rank svds for which we exploited the sparsity of the gradient ( the sparsity structure being inherited from the pattern of sparsely observed responses ) .",
    "we used a significant novelty in the performing the svd computations  @xcite  we used subspace iterations or orthogonal block qr decompositions to compute the low - rank svd  @xcite ; and during the course of the proximal gradient scheme we employed the left singular subspace obtained from the previous svd step as a warm - start for the next proximal gradient step .",
    "this seems play an instrumental role in the superior performance of our algorithm .",
    "the reason behind this performance gain is not surprising  during the course of the proximal gradient steps as the successive difference of the objective functions diminish , the left and right singular subspaces of the matrices obtained from two successive iterates get closer .",
    "our proposal is able to exploit this observation in a systematic way  this significantly speeds up the cost for evaluating the nuclear norm thresholded operator .",
    "we report three different examples in our timing comparisons  .",
    "for all the examples , we first compute the minimum objective value of the function by running our algorithm for a maximum of 10 hours ( cpu ) time and taking the best solution thus obtained , we call it @xmath136 .",
    "we considered two subsets of the kaggle dataset with sizes @xmath137 ( kaggle-(a ) ) and @xmath138 ( kaggle-(b ) ) respectively .",
    "the data - sets are obtained by taking the heaviest users and the most popular questions of the respective sizes . in each of these two instances we obtained the optimal tuning parameter apriori by cross - validation . with the selected tuning parameter",
    ", we estimated @xmath136 as described above .",
    "all algorithms were allowed to run for a maximum of 2 hours each .",
    "in addition we used a synthetic data - set for a matrix of size @xmath139 with approximately 300 entries per row / column .",
    "we generated data based on a logistic model , the latent factors @xmath140 were of rank @xmath141and the entries were generated from iid gaussian data .",
    "all algorithms were allowed to run for a maximum of 2 hours of cputime .",
    "our second experiment illustrates the statistical benefits of using side information in this data set .",
    "there are many options for using side information but we used a simple framework , using the test and section from which the ( e.g. , sat verbal , gmat quantitative , etc . ) .",
    "we use the ridge regression formulation where @xmath142 is a dummy for the @xmath17th test , and we take @xmath143 , so the test effects are independent of each other and of the same order of magnitude as the student effects . given the corresponding @xmath107 we then solve @xmath144 we choose @xmath145 and @xmath146 by optimizing test error with the @xmath47 term removed , then fit a path of models by varying @xmath147 .",
    "because we are fitting so many models , we subsample 10,000 users before performing the analysis",
    ". the left panel of figure  [ fig:2 ] shows that this side information helps our predictive accuracy .",
    "our second example is of a more nonparametric flavor .",
    "we begin with 200 noisy functions measured at 256 equally spaced frequency points , representing measured log - periodograms of several phonemes spoken by various subjects .",
    "this data set was analyzed by @xcite to demonstrate a variant of discriminant analysis with smoothness penalty applied .",
    "we artificially construct a sparsely - observed data set by sampling each curve at 26 random points , and set ourselves the objective of reconstructing the functions based on these relatively few samples , exploiting smoothness and a low - rank assumption about the curves .",
    "we impose a simple model on the column variables @xmath50 ; they are constrained to lie in the natural spline basis with 12 degrees of freedom , and we shrink them toward the natural spline basis with 4 degrees of freedom .",
    "we estimate the principal components analysis model with unpenalized marginal column ( time ) effects .",
    "@xmath148 where @xmath149 represents a @xmath150-dimensional natural spline basis .",
    "this can be framed ( somewhat artificially ) as a bayesian prior on @xmath52 with flat variance in the directions of @xmath151 , finite positive variance in the directions of @xmath152 , and zero variance on other directions .",
    "although this data set is relatively small , it can be computationally advantageous to constrain @xmath47 as we have done here , since it reduces the size of our optimization variables ( e.g. @xmath153 is @xmath154 instead of @xmath155 ) .",
    "we compare our proposed method to matrix completion using the standard nuclear norm , which does not exploit smoothness .",
    "the right panel of figure  [ fig:2 ] shows that side information cuts mse by a sizeable fraction .",
    "we presented a framework for scalable convex optimization on matrix completion problems incorporating side information .",
    "the information can be diverse in its source , as long as it can be represented ultimately as some quadratic penalty which can be applied or inverted with ease .",
    "we have seen two examples where side information of different kinds is advantageous for predictive performance .",
    "although the bottleneck in our algorithm is an svd of a large matrix , we can attain rapid convergence by exploiting the structure of the svd target , which is often easy to apply .",
    "our method is competitive with state - of - the - art convex optimization algorithms and scales favorably .",
    "the authors are grateful to trevor hastie for inspiration , discussion and suggestions , and to lester mackey for references and suggestions .",
    "william fithian was supported by nsf vigre grant dms-0502385 ."
  ],
  "abstract_text": [
    "<S> we propose a general framework for reduced - rank modeling of matrix - valued data . by applying a generalized nuclear norm penalty we directly model low - dimensional latent variables associated with rows and columns . </S>",
    "<S> our framework flexibly incorporates row and column features , smoothing kernels , and other sources of side information by penalizing deviations from the row and column models . under general conditions </S>",
    "<S> these models can be estimated scalably using convex optimization . </S>",
    "<S> the computational bottleneck  one singular value decomposition per iteration of a large but easy - to - apply matrix  can be scaled to large problems by representing the matrix implicitly and using warm starts @xcite . </S>",
    "<S> our framework generalizes traditional convex matrix completion and multi - task learning methods as well as map estimation under a large class of popular hierarchical bayesian models .    </S>",
    "<S> [ theorem]corollary [ theorem]lemma [ theorem]observation [ theorem]proposition [ theorem]definition [ theorem]claim [ theorem]fact [ theorem]assumption </S>"
  ]
}