{
  "article_text": [
    "part of speech ( pos ) tagging is a quite well defined nlp problem , which consists of assigning to each word in a text the proper morphosyntactic tag for the given context .",
    "although many words are ambiguous regarding their pos , in most cases they can be completely disambiguated taking into account an adequate context .",
    "successful taggers have been built using several approaches , such as statistical techniques , symbolic machine learning techniques , neural networks , etc .",
    "the accuracy reported by most current taggers ranges from 9697% to almost 100% in the linguistically  motivated constraint grammar environment .",
    "unfortunately , there have been very few direct comparisons of alternative taggers on identical test data .",
    "however , in most current papers it is argued that the performance of some taggers is better than others as a result of some kind of indirect comparisons between them .",
    "we think that there are a number of not enough controlled / considered factors that make these conclusions dubious in most cases .    in this direction , the present paper aims to point out some of the difficulties arising when evaluating and comparing tagger performances against a reference test corpus , and to make some criticism about common practices followed by the nlp researchers in this issue .",
    "the above mentioned factors can affect either the evaluation or the comparison process .",
    "factors affecting the evaluation process are : ( 1 ) training and test experiments are usually performed over noisy corpora which distorts the obtained results , ( 2 ) performance figures are too often calculated from only a single or very small number of trials , though average results from multiple trials are crucial to obtain reliable estimations of accuracy @xcite , ( 3 ) testing experiments are usually done on corpora with the same characteristics as the training data usually a small fresh portion of the training corpus but no serious attempts have been done in order to determine the reliability of the results when moving from one domain to another @xcite , and ( 4 ) no figures about computational effort space / time complexity are usually reported , even from an empirical perspective .",
    "a factors affecting the comparison process is that comparisons between taggers are often indirect , while they should be compared under the same conditions in a multiple  trial experiment with statistical tests of significance .    for these reasons",
    ", this paper calls for a discussion on pos taggers evaluation , aiming to establish a more rigorous test experimentation setting / designing , indispensable to extract reliable conclusions . as a starting point",
    ", we will focus only on how the noise in the test corpus can affect the obtained results .",
    "from a machine learning perspective , the relevant noise in the corpus is that of non systematically mistagged words ( i.e. different annotations for words appearing in the same syntactic / semantic contexts ) .",
    "commonly used annotated corpora have noise .",
    "see , for instance , the following examples from the wall street journal ( wsj ) corpus :    verb participle forms are sometimes tagged as such ( vbn ) and also as adjectives ( jj ) in other sentences with no structural differences :    * ... failing_vbg to_to voluntarily_rb submit_vb",
    "the_dt _ requested_vbn _ information_nn ... * ... a_dt large_jj sample_nn of_in _ married_jj _ women_nns with_in at_in least_jjs one_cd child_nn ...    another structure not coherently tagged are noun chains when the nouns ( nn ) are ambiguous and can be also adjectives ( jj ) :    * ... mr._nnp hahn_nnp , _ , the_dt 62-year - old_jj chairman_nn and_cc _",
    "chief_nn executive_jj officer_nn _ of_in georgia - pacific_nnp corp._nnp ... * ... burger_nnp king_nnp s_pos _ chief_jj executive_nn officer_nn _ , _ , barry_nnp gibbons_nnp , _ , stars_vbz in_in ads_nns",
    "saying_vbg ...    the noise in the test set produces a wrong estimation of accuracy , since correct answers are computed as wrong and vice - versa . in following sections we will show how this uncertainty in the evaluation may be , in some cases , larger than the reported improvements from one system to another , so invalidating the conclusions of the comparison .",
    "to study the appropriateness of the choices made by a pos tagger , a reference tagging must be selected and assumed to be correct in order to compare it with the tagger output .",
    "this is usually done by assuming that the disambiguated test corpora being used contains the right pos disambiguation .",
    "this approach is quite right when the tagger error rate is larger enough than the test corpus error rate , nevertheless , the current pos taggers have reached a performance level that invalidates this choice , since the tagger error rate is getting too close to the error rate of the test corpus .    since we want to study the relationship between the tagger error rate and the test corpus error rate , we have to establish an absolute reference point .",
    "although @xcite questions the concept of _ correct analysis _ , @xcite establish that there exists a statistically significant _ absolute _ correct disambiguation , respect to which the error rates of either the tagger or the test corpus can be computed .",
    "what we will focus on is how distorted is the tagger error rate by the use of a noisy test corpus as a reference .",
    "the cases we can find when evaluating the performance of a certain tagger are presented in table [ taula - cassos ] .",
    "ok/@xmath0ok stand for a right / wrong tag ( respect to the absolute correct disambiguation ) .",
    "when both the tagger and the test corpus have the correct tag , the tag is correctly evaluated as _",
    "right_. when the test corpus has the correct tag and the tagger gets it wrong , the occurrence is correctly evaluated as _ wrong_. but problems arise when the test corpus has a wrong tag : if the tagger gets it correctly , it is evaluated as _",
    "wrong _ when it should be _ right _ ( false negative ) .",
    "if the tagger gets it wrong , it will be rightly evaluated as _",
    "wrong _ if the error commited by the tagger is other than the error in the test corpus , but wrongly evaluated as _ right _ ( false positive ) if the error is the same .    .",
    "[ cols=\"^,^,^,^\",options=\"header \" , ]     the same information is included in figure  [ f - intervals ] which presents the reasonable accuracy intervals for both taggers , for @xmath1 ranging from @xmath2 to 1 ( the shadowed part corresponds to the overlapping region between intervals ) .",
    "the obtained intervals have a large overlap region which implies that there are _ reasonable _ parameter combinations that could cause the taggers to produce different observed performances though their real accuracies were very similar . from this conservative approach",
    ", we would not be able to conclude that the tagger @xmath3 is better than @xmath4 , even though the @xmath5 confidence intervals for the observed performances did allow us to do so .",
    "the presented analysis of the effects of noise in the test corpus on the evaluation of pos taggers leads us to conclude that when a tagger is evaluated as better than another using noisy test corpus , there are _ reasonable _ chances that they are in fact very similar but one of them is just adapting better than the other to the noise in the corpus .",
    "we believe that the widespread practice of evaluating taggers against a noisy test corpus has reached its limit , since the performance of current taggers is getting too close to the error rate usually found in test corpora .    an obvious solution and maybe not as costly as one might think , since small test sets properly used may yield enough statistical evidence is using only error  free test corpora . another possibility is to further study the influence of noise in order to establish a criterion e.g . a threshold depending on the amount of overlapping between intervals to decide whether a given tagger can be considered better than another .",
    "there is still much to be done in this direction .",
    "this paper does not intend to establish a new evaluation method for pos tagging , but to point out that there are some issues such as the noise in test corpus that have been paid little attention and are more important than what they seem to be .",
    "some of the issues that should be further considered are : the effect of noise on unambiguous words ; the reasonable intervals for _ overall _ real performance ; the probably different values of @xmath6 , @xmath1 , @xmath7 and @xmath8 for ambiguous / unambiguous words ; how to estimate the parameter values of the evaluated tagger in order to constrain as much as possible the intervals ; the statistical significance of the interval overlappings ; a more informed ( and less conservative ) criterion to reject / accept the hypothesis that both taggers are different , etc .",
    "aquest article versa sobre lavaluaci de desambiguadors morfosintctics .",
    "normalment , lavaluaci es fa comparant la sortida del desambiguador amb un corpus de referncia , que se suposa lliure derrors",
    ". de tota manera , els corpus que susen habitualment contenen soroll que causa que el rendiment que sobt dels desambiguadors sigui una distorsi del valor real .",
    "en aquest article analitzem fins a quin punt aquesta distorsi pot invalidar la comparaci entre desambiguadors o la mesura de la millora aportada per un nou sistema .",
    "la conclusi principal s que cal establir procediments alternatius dexperimentaci ms rigorosos ,",
    "per poder avaluar i comparar fiablement les precisions dels desambiguadors morfosintctics .",
    "artikulu hau desanbiguatzaile morfosintaktikoen ebaluazioaren inguruan datza .",
    "normalean , ebaluazioa , desanbiguatzailearen irteera eta ustez errorerik gabeko erreferentziako corpus bat konparatuz egiten da .",
    "hala ere , maiz corpusetan erroreak egoten dira eta horrek desanbiguatzailearen emaitzaren benetako balioan eragina izaten du .",
    "artikulu honetan , hain zuzen ere , horixe aztertuko dugu , alegia , zer neurritan distortsio horrek jar dezakeen auzitan desanbiguatzaileen arteko konparazioa edo sistema berri batek ekar dezakeen hobekuntza - maila .",
    "konklusiorik nagusiena hauxe da : desanbiguatzaile morfosintaktikoak aztertzeko eta modu ziurrago batez konparatu ahal izateko , azterketa - bideak sakonagoak eta zehatzagoak izan beharko liratekeela ."
  ],
  "abstract_text": [
    "<S> this paper addresses the issue of pos tagger evaluation . </S>",
    "<S> such evaluation is usually performed by comparing the tagger output with a reference test corpus , which is assumed to be error - free . </S>",
    "<S> currently used corpora contain noise which causes the obtained performance to be a distortion of the real value . </S>",
    "<S> we analyze to what extent this distortion may invalidate the comparison between taggers or the measure of the improvement given by a new system . </S>",
    "<S> the main conclusion is that a more rigorous testing experimentation setting / designing is needed to reliably evaluate and compare tagger accuracies . </S>"
  ]
}