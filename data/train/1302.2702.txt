{
  "article_text": [
    "with synchronization errors have been familiar to information and coding theorists and practitioners alike ever since the advent of the digital information era . although dobrushin @xcite established the coding theorem for such channels as early as 1967 , tackling these channels in terms of estimating information rates and constructing codes with good performance have proved to be very tough . in the last decade ,",
    "significant progress has been made in estimating achievable information rates for certain channels with synchronization errors .",
    "however , a coding scheme with provably `` good '' performance remains elusive thus far .    in this paper",
    ", we start with dobrushin s model of channels with synchronization errors , henceforth referred to as the _ synchronization error channel _ ( sec ) , and convert it into an equivalent channel with states . using this alternative model , we construct a sequence of channels that `` approximate '' the sec and whose limit is the sec .",
    "we use these approximate channels to derive some results about information rates achievable over the sec .",
    "although the motivation behind the alternative model is straightforward , its use to obtain non - trivial bounds on the capacity of the sec has , to the best of our knowledge , not been found in literature . while the present paper concerns only a few asymptotic results on information rates of the sec , we think that the model presented here can be utilized to design codes for secs in general .",
    "the remainder of this paper is organized as follows . in section [ sec_sec ] ,",
    "we revisit dobrushin s model of an sec and recall the main results on capacity of secs . through much of the paper",
    ", we consider a special case of the generic sec  the deletion , replication channel ( drc)and construct an equivalent channel by viewing the drc as a channel with states in section [ sec_eqchn ] . under further special cases of channels with only deletions or only replications , we give some simple , non - trivial and sometimes tight bounds on the capcity in sections [ ssec_bdc ] and [ ssec_bdupc ] .",
    "we then construct a sequence of finite state channels that approximate the drc and establish certain properties of this sequence of channels that serve as bounds for the capacity of the drc in section [ sec_appchn ] . in section [ sec_gen ] ,",
    "we note the application of similar strategies to more general secs , and we conclude with summary and remarks in section [ sec_conc ] .",
    "non - random variables are written as lowercase letters , e.g. , @xmath1 .",
    "we denote sets by double - stroke uppercase letters , e.g. , @xmath2 .",
    "we will reserve @xmath3 , @xmath4 and @xmath5 to denote the sets of natural numbers , integers and real numbers , respectively .",
    "@xmath6 denotes the set of non - negative integers .",
    "we define @xmath7 & \\triangleq \\{1 , 2 , \\cdots , n\\ } , n \\in \\mathbb{n } , \\notag \\\\ [ 0 ] & \\triangleq \\emptyset,\\notag \\\\ [ m : n ] & \\triangleq   \\begin{cases } \\{m , m + 1 , \\cdots , n\\ } , & m \\leq n , \\\\",
    "\\emptyset , & n < m. \\end{cases } \\text { and } \\notag \\\\ \\mathbb{z}_{\\pm m } & \\triangleq \\{-m , -m + 1 , \\cdots , 0 , 1 , \\cdots , m\\}{\\ } \\forall{\\ } m \\in \\mathbb{z}^+",
    ". \\notag\\end{aligned}\\ ] ]    for some @xmath8 , we will let @xmath9 denote the set of vectors of dimension @xmath1 with elements from @xmath10 .",
    "we will write @xmath11 to denote a string , and @xmath12 to denote the _",
    "empty string_. the _ length _ of a string , denoted @xmath13 , is the number of symbols in it , and by definition , @xmath14 . with some abuse of notation",
    ", we will use `` vectors of dimension @xmath1 '' and `` strings of length @xmath1 '' interchageably .",
    "the set of all strings of length @xmath1 over the alphabet @xmath10 is hence also denoted @xmath9 , and @xmath15 .",
    "we write @xmath16 to denote the set of all strings over the set @xmath10 , i.e. , @xmath17 the bar `` @xmath18 '' will denote the concatenation operation , so that @xmath19 is the concatenation of strings @xmath11 and @xmath20 .    throughout the paper , we assume an underlying probability space @xmath21 over which random variables , denoted by uppercase letters , e.g. , @xmath22 , are defined .",
    "random vectors are denoted by uppercase letters with the _ multiset _ of indices as subscripts , e.g. , @xmath23 } = ( x_1 , x_2 , \\cdots , x_n)$ ] , or @xmath24}}$ ] when the multiset of indices is itself the elements of a random vector @xmath25}$ ] .",
    "random processes ( assumed discrete - time ) are denoted by script letters @xmath26 , or subscripted by the set of natural numbers , @xmath27 .",
    "we will use the asymptotic notations @xmath28 , @xmath29 , @xmath30 as in @xcite.@xmath31    we start by defining the synchronization error channels as considered by dobrushin @xcite .",
    "[ def_msec ] let @xmath2 be a finite set .",
    "a _ memoryless _",
    "synchronization error channel is specified by a stochastic matrix @xmath32 where @xmath33 is the output alphabet . from the properties of a stochastic matrix",
    ", we have @xmath34 further , we will assume that the mean value of the length of the output string arising from one input symbol is strictly positive and finite , i.e. , @xmath35 for @xmath36 } = ( x_1 , x_2 , \\cdots , x_n ) \\in { \\mathbb}{x}^n$ ] and @xmath37 } = ( \\overline{y}_1 , \\overline{y}_2 , \\cdots , \\overline{y}_n ) \\in \\overline{{\\mathbb}{y}}^n$ ] , we write @xmath38 } | x_{[n ] } ) = \\prod_{i = 1}^n q(\\overline{y}_i | x_i).\\ ] ] let @xmath39}}$ ] denote the concatenation of strings @xmath40 $ ] .",
    "then the transition probabilities of the memoryless sec are defined as @xmath41 } ) = \\sum_{\\overline{\\overline{y}_{[n ] } } = \\overline{y } } q_n(\\overline{y}_{[n ] } | x_{[n]})\\ ] ] for @xmath42 and @xmath36 } \\in { \\mathbb}{x}^n$ ] .",
    "the memoryless sec is given by the triplet @xmath43 , the input and the output alphabets , and the transition probabilities between input strings of length @xmath1 and all output strings.@xmath31    consider the sequence of memoryless secs @xmath44 . then",
    ", we have the following .    [ thm_cap ]",
    "let @xmath23}$ ] and @xmath45 denote the input and the output of the sec @xmath46 .",
    "let @xmath47 } ) } \\frac{1}{n}i(x_{[n ] } ; \\overline{y}).\\ ] ] then , @xmath48 exists and is equal to the capacity of the sequence of secs.@xmath49    the quantity @xmath50 represents the maximum rate at which information can be transferred over the sec with vanishing error probability .",
    "furthermore , the following result shows that , in estimating the capacity of the sec , we can restrict ourselves to a subclass of possible input processes @xmath26 .",
    "[ prop_marcap ] let @xmath51 be a stationary , ergodic , markov process over @xmath2 . then the capacity of the sequence @xmath44 is @xmath52 } ; \\overline{y}).\\ ] ] the capacity is therefore the supremum of the rates achievable through stationary , ergodic , markov processes @xmath53 .",
    "@xmath49    we will now give an example of a memoryless sec . throughout the paper",
    ", we will assume that the input alphabet for the secs is @xmath54 , i.e. , the channels considered are _ binary _ memoryless secs",
    ". however , we note here that all the results in the paper can be straightforwardly extended to the case where @xmath10 is any finite set .",
    "[ eg_ddc ] consider the binary sec with @xmath55 and the following stochastic matrix .",
    "@xmath56 intuitively , we can think of @xmath57 as the deletion probability , @xmath58 as the transmission probability , and @xmath59 as the replication probability , i.e. , when @xmath60 is sent , it is either deleted with probability @xmath61 , or transmitted and replicated @xmath62 times with probability @xmath63 for @xmath64 . from , we get for @xmath65 @xmath66 or equivalently @xmath67 from , @xmath68 where we use equation .",
    "hence @xmath69 .",
    "note that when @xmath70 , the drc is the same as the _ binary deletion channel _ ( bdc ) ; and when @xmath71 , it is the _ binary replication channel _ ( brc ) , also referred to as the _ geometric binary sticky channel _ @xcite.@xmath31    @xmath72 } , z_0 = 0 ) & = \\sum_{\\{\\overline{z } : |\\overline{z}| = |\\overline{y}|\\ } } { \\mathsf}{p}(\\overline{z } = \\overline{z } , \\overline{y } = \\overline{y } | x_{[n ] } = x_{[n ] } , z_0 = 0 ) \\notag \\\\ & = \\sum_{\\{\\overline{z } : |\\overline{z}| = |\\overline{y}|\\ } } { \\mathsf}{p}(\\overline{z } = \\overline{z } | z_0 = 0){\\mathsf}{p}(\\overline{y } = \\overline{y } | x_{[n ] } = x_{[n ] } , z_0 = 0 , \\overline{z } = \\overline{z } ) \\notag \\\\ & = \\sum_{\\{\\overline{z } : |\\overline{z}| = |\\overline{y}|\\ } } \\prod_{i = 1}^{|\\overline{y}| } \\big({\\mathsf}{p}(z_i = z_i | z_{i - 1 } = z_{i - 1 } , z_0 = 0){\\mathsf}{p}(y_i = y_i | x_{[n ] } = x_{[n ] } , z_i = z_i)\\big ) \\label{eq_eqchntrnp } \\\\ & = \\sum_{\\{\\overline{z } : |\\overline{z}| = |\\overline{y}|\\ } } \\prod_{i = 1}^{|\\overline{y}| } \\big(\\mathsf{p}(z_i",
    "= z_i | z_{i - 1 } = z_{i - 1})\\mathds{1}_{\\{y_i = x_{i - z_i}\\}}\\big ) . \\notag\\end{aligned}\\ ] ]    ' '' ''    the bdc has been the most well - studied sec . in @xcite ,",
    "the author surveys the results that were known prior to 2009 . to summarize ,",
    "the best known lower bounds were obtained , chronologically , through bounds on the cutoff rate for sequential decoding @xcite , bounding the rate with a first - order markov input @xcite , reduction to a poisson - repeat channel @xcite , analyzing a `` jigsaw - puzzle '' coding scheme @xcite , or by directly bounding the information rate by analyzing the channel as a joint renewal process @xcite .",
    "recently , @xcite and @xcite independently gave the capacity of a bdc with small deletion probabilities , and showed that it is achieved by independent and uniformly distributed ( i.u.d . ) inputs .",
    "the known upper bounds for the bdc have been obtained by genie - aided decoder arguments @xcite .",
    "an idea from @xcite was extended to obtain some analytical lower bounds on the capacity of channels that involve substitution errors as well as insertions or deletions @xcite .",
    "the idea in @xcite was extended to obtain a better approximtion for the capacity of the bdc with small deletion probabilities in @xcite .",
    "in contrast to these existing results , our approach explicitly characterizes the achievable information rates in terms of `` subsequence - weights '' , which is a measure relevant in ml decoding for the bdc @xcite .",
    "additionally , the method proposed here gives the tight bound on capacity for small deletion probabilities obtained in @xcite more directly .",
    "for the brc , @xcite obtained lower bounds on the capacity by numerically estimating the capacity per unit cost of the equivalent channel of runs through optimization of @xmath73 and @xmath74 bit codes . here , we obtain direct analytical lower bounds on the capacity . these",
    ", to the best of our knowledge , represent the only analytical bounds for the capacity of the brc .",
    "moreover , we obtain an exact expression for the markov-@xmath0 rate for the brc which conclusively disproves the conjecture that the capacity of secs is a convex function of the channel parameter .",
    "we will use the drc as a running example of an sec . in section",
    "[ sec_gen ] , we discuss the extension of the results presented to a more general class of secs .",
    "we now construct a channel with states that is equivalent to the drc introduced in example [ eg_ddc ] .",
    "dobrushin s model of sec ( cf . definition [ def_msec ] ) tracks the output string generated by each input symbol . in our model",
    ", we track the input symbol that gave rise to each output symbol .",
    "[ def_ddcstate ] for a fixed @xmath8 , we write @xmath75\\ ] ] where @xmath76 is the `` state '' of the channel and @xmath77 we will refer to the random variable @xmath78 as the _ output length _ corresponding to @xmath1 input symbols .",
    "the state process @xmath79 is independent of the channel input process , and is a first - order markov process over the set of integers @xmath80 with transition probabilities for each @xmath81 given by @xmath82 where we define @xmath83 as in equation assuming @xmath84 .",
    "we will refer to the process @xmath85 where @xmath86 as the _ index process_.    we also assume the boundary condition that @xmath87 , i.e. , the channel is perfectly synchronized before transmission commences .",
    "note that the transition probabilities in indeed are well - defined since @xmath88 , as @xmath89 , @xmath90 with the above definition , for @xmath42 and @xmath36 } \\in { \\mathbb}{x}^n$ ] , the channel transition probabilities are given as in equation .",
    "note that in the terms within the parenthesis on the right hand side of equation , the first term is completely specified by the transition probabilities of the channel state process @xmath79 , and the second term is @xmath91 or @xmath0 accordingly as @xmath92 or @xmath93 respectively .    for each @xmath8 , we define the drc with states as the channel @xmath94.@xmath31    we will start by proving a few properties of the output length @xmath78 and the channel state @xmath79 and index processes @xmath95 which will be made use of subsequently .",
    "we will relegate the proofs of the following two lemmas to appedices [ app_nnprop ] and [ app_ztprop ] respectively .",
    "[ lem_nnprop ] the output length @xmath78 satisfies the following properties :    a.   for any @xmath8 , @xmath96 a.s .. b.   @xmath97 as @xmath98 a.s .. c.   @xmath99 a.s . as @xmath98.@xmath49    [ lem_ztprop ] the channel state process @xmath100 and the index process",
    "@xmath95 satisfy the following properties :    a.   @xmath100 and @xmath95 are first - order , time - homogeneous , shift - invariant markov chains .",
    "further , @xmath100 is irreducible and aperiodic .",
    "b.   @xmath95 is almost surely non - decreasing , i.e. , @xmath101 for any @xmath8 , a realization of @xmath102}$ ] such that the corresponding @xmath103}$ ] realization satisfies the above monotonicity property is called a _",
    "compatible _ state path .",
    "c.   for every @xmath81 , @xmath104 where @xmath105 , for @xmath106 $ ] , is the _ binary entropy function _ @xcite . here",
    ", we assume from continuity that @xmath107 .",
    "consequently , for every @xmath8 , @xmath108 } ) = h(\\gamma_{[n ] } ) = n\\big(h_2(\\p{r } ) + \\frac{1 - \\p{r}}{1 - \\p{d}}h_2(\\p{d})\\big).\\tag*{$\\blacksquare$}\\ ] ]    note that the @xmath79 process is _ not _ stationary because we fix @xmath109 .",
    "the @xmath110 process is clearly not stationary since @xmath111 depends on @xmath112 . from lemma [ lem_ztprop ] ( ii ) , we can show that for @xmath113 , @xmath114    [ prop_chneq ] for each @xmath8 , the channels @xmath46 and @xmath115 are equivalent .",
    "both @xmath46 and @xmath115 have the same input and output alphabets @xmath10 and @xmath116 , respectively .",
    "the correspondence between the transition probabilities @xmath117 and @xmath118 in equations and is evident by the following observations :    a.   for every parsing of @xmath42 as @xmath37}$ ] in equation , there is a corresponding state path @xmath119 in equation .",
    "b.   for every compatible state path @xmath119 in equation ( see lemma [ lem_ztprop ] ) , there is a corresponding parsing of @xmath120 in equation . c.   for these corresponding parsings of @xmath20 and compatible state paths @xmath121 , the terms within the parenthesis on the right hand side of equation [ eq_eqchntrnp ] , when grouped according to the output symbols arising from the same input symbol , spell out exactly the same probability as the terms @xmath122 .    therefore , except on a set of zero probability ( state paths that are not compatible ) , the probability measures @xmath117 and @xmath118 are equal .",
    "this implies the equivalence of the channels @xmath46 and @xmath115 .    as a consequence of the above equivalence , the results of theorem [ thm_cap ] and proposition [ prop_marcap ] carry forward to the sequence of channels @xmath123 specified by equations and .",
    "[ cor_eqchn ] for input @xmath23}$ ] and output @xmath124}$ ] of the channel @xmath125 , the quantity @xmath126 } ) } \\frac{1}{n } i(x_{[n ] } ; y_{[n_n ] } ) \\notag \\\\ & = \\sup_{{\\mathcal}{x}_{\\mathcal}{m } } \\lim_{n \\rightarrow \\infty } \\frac{1}{n } i(x_{[n ] } ; y_{[n_n ] } ) , \\notag\\end{aligned}\\ ] ] where @xmath51 represents stationary , ergodic , markov processes over @xmath2 , exists and is equal to the capacity of the sequence of channels @xmath127.@xmath49    we will henceforth restrict our attention to this class of input processes .",
    "the following is a useful result whose proof is deferred to appendix [ app_ystat ] .    [ prop_ystat ] the channel output process @xmath128 is stationary for stationary input processes @xmath26.@xmath49    as a consequence of the above result , the _ entropy rate",
    "_ @xmath129 of the output process is well - defined @xcite .",
    "the formulation of the drc as a channel with states allows us to immediately establish the following .",
    "[ prop_capbounds ] for the drc , @xmath130    we can write @xmath131 } ; & y_{[n_n ] } ) = i(x_{[n ] } ; y_{[n_n ] } , z_{[n_n ] } ) - i(x_{[n ] } ; z_{[n_n ] } | y_{[n_n ] } ) \\notag \\\\ & \\stackrel{(a)}{= } i(x_{[n ] } ; y_{[n_n ] } | z_{[n_n ] } ) - i(x_{[n ] } ; z_{[n_n ] } | y_{[n_n ] } ) \\notag \\\\ & \\stackrel{(b)}{= } ( 1 - \\p{d})h(x_{[n ] } ) - i(x_{[n ] } ; z_{[n_n ] } | y_{[n_n ] } ) , \\label{eq_capeq}\\end{aligned}\\ ] ] where @xmath132 is true because @xmath133 and @xmath134 from the fact that the drc , given the @xmath79 process realization , is equivalent to a binary erasure channel ( bec ) with erasure rate @xmath61 .",
    "then , @xmath135 } ; y_{[n_n ] } ) \\geq ( 1 - \\p{d})h(x_{[n ] } ) - h(z_{[n_n]}).\\ ] ] from lemma [ lem_ztprop ] ( iii ) , and since , for any finite @xmath1 , we have the extra knowledge that @xmath136 by definition of @xmath78 , we can show that @xmath137 } ) \\leq { \\mathsf}{e}(n_n)\\big(h_2(\\p{r } ) + \\frac{1 - \\p{r}}{1 - \\p{d}}h_2(\\p{d})\\big).\\ ] ] note that the extra information @xmath136 becomes tautological when @xmath98 , and hence @xmath138})}{n } = \\big(\\lim_{n \\rightarrow \\infty } \\frac{{\\mathsf}{e}(n_n)}{n}\\big)\\big(h_2(\\p{r } ) + \\frac{1 - \\p{r}}{1 - \\p{d}}h_2(\\p{d})\\big).\\ ] ] from lemma [ lem_nnprop ] , and for independent uniformly distributed inputs , the claim follows .",
    "proposition [ prop_capbounds ] gives bounds on the capacity for @xmath84 .",
    "three special cases of the drc are of particular interest : the binary deletion channel ( bdc ) with @xmath139 ; the _ symmetric _ deletion - replication channel ( sdrc ) with @xmath140 ; and the binary replication channel ( brc ) with @xmath141 .",
    "specializing proposition [ prop_capbounds ] to these cases gives us the following results .",
    "[ cor_capbndsspl ] we have @xmath142    although the bounds in corollary [ cor_capbndsspl ] have simple closed - form expressions with well known information theoretic functions , they are loose compared to the best known ( analytical or numerical ) bounds for the capacity of these channels .",
    "we can , however , improve these bounds .",
    "we have from equation , @xmath131 } ; y_{[n_n ] } ) & = ( 1 - \\p{d})h(x_{[n ] } ) + i(y_{[n_n ] } ; z_{[n_n ] } ) \\notag \\\\ & \\hspace{5 mm } - h(z_{[n_n ] } ) + h(z_{[n_n ] } | x_{[n ] } , y_{[n_n ] } ) .",
    "\\label{eq_ixy}\\end{aligned}\\ ] ] writing the entropy rate of the input process @xmath26 as @xmath143 and defining @xmath144})}{n},\\quad\\hat{\\mathcal{h}}(\\mathcal{y } ) \\triangleq \\lim_{n \\rightarrow \\infty } \\frac{h(y_{[n_n]})}{n } , \\notag \\\\ \\text{and}\\quad \\hat{\\mathcal{h}}(\\mathcal{z } | & \\mathcal{x } , \\mathcal{y } ) \\triangleq \\lim_{n \\rightarrow \\infty } \\frac{h(z_{[n_n ] } | x_{[n ] } , y_{[n_n]})}{n } , \\notag\\end{aligned}\\ ] ] from lemma [ lem_ztprop ] and equation , we can bound @xmath145    [ lem_hnmon ] let @xmath146 } | x_{[n ] } , y_{[n_n]})$ ] for @xmath8 . then , for the sequence @xmath147 , @xmath148    the proof is given in appendix [ app_hnsupadd ] .",
    "the above result implies that if we could evaluate ( or lower bound ) @xmath149 for some @xmath1 , that could be used to estimate a lower bound on @xmath50 .",
    "[ prop_caplb ] for the drc ,",
    "@xmath150    we have @xmath151 } | x_{[n ] } , y_{[n_n ] } ) \\notag \\\\ & = \\frac{1}{n } { \\mathsf}{e}\\big(\\sum_{i = 1}^{n_n } h(z_i | z_{[i - 1 ] } = z_{[i - 1 ] } , x_{[n ] } , y_{[n_n ] } ) \\big ) \\notag \\\\ & = \\frac{1}{n } { \\mathsf}{e}\\big(\\sum_{i = 1}^{n_n } h(z_i | z_{i - 1 } = z_{i - 1 } , x_{[i - 1 - z_{i - 1 } :",
    "n ] } , y_{[i : n_n]})\\big ) \\notag\\end{aligned}\\ ] ] where the last equality follows from the conditional independence of @xmath152 on @xmath153 } , x_{[i - 2 - z_{i - 1}]}$ ] and @xmath154}$ ] given @xmath155 . from the time - homogeneity and shift - invariance of the @xmath79 process ( see lemma [ lem_ztprop ] ) , as @xmath98 , the summand in the above expression @xmath156 } , y_{[i : n_n ] } )   \\notag \\\\",
    "& \\qquad\\rightarrow h(z_1 | z_0 = 0 , x_{{\\mathbb}{n } } , y_{{\\mathbb}{n } } )",
    "\\triangleq h(z_1 | { \\mathcal}{x } , { \\mathcal}{y } ) .",
    "\\notag\\end{aligned}\\ ] ] since @xmath157 , optimizing over input processes @xmath26 gives us the desired result .",
    "it is not easy to evaluate the bound in proposition [ prop_caplb ] .",
    "however , we can further lower bound the capacity by introducing some conditioning .",
    "[ lem_monli ] the sequence of lower bounds @xmath158 , where @xmath159 is non - decreasing .",
    "since we have introduced extra conditioning , the @xmath160s are lower bounds .",
    "we have @xmath161 } ) - h(z_i | z_1 , z_{i + 1 } ) \\notag \\\\ & \\stackrel{(a)}{= } h(z_i |",
    "z_{i + 1 } ) + h(z_1 |",
    "z_i ) - h(z_i | z_1 , z_{i + 1 } ) \\notag \\\\ & = h(z_1 | z_i ) + i(z_1 ; z_i | z_{i + 1 } ) \\notag \\\\ & \\geq h(z_1 | z_i ) \\notag\\end{aligned}\\ ] ] where @xmath132 follows from the markovity of the @xmath79 process .",
    "since conditioning on @xmath26 and @xmath128 preserves the above chain of inequalities , we have @xmath162 .",
    "hence @xmath163 is non - decreasing .",
    "optimizing @xmath164 over stationary , ergodic , markov input processes @xmath26 gives the bound in proposition [ prop_capbounds ] .",
    "therefore , for increasing @xmath112 , we have bounds better than the one in proposition [ prop_capbounds ] .",
    "in particular , as @xmath165 , following the proof of lemma [ lem_nnprop ] ( iii ) , we can see that @xmath166 a.s .",
    ", so that the knowledge of @xmath152 becomes tautological in the limit , and consequently , @xmath167 gives us the bound in proposition [ prop_caplb ] .",
    "@xmath168 } \\in",
    "{ \\mathbb}{x}^{m + i - 1}}\\frac{1}{2^{m + i - 1}}{\\mathsf}{h}(x_{[m + i - 1 ] } ) , \\notag \\\\ { \\mathsf}{h}(x_{[m + i - 1 ] } ) &",
    "= \\sum_{y_{[i - 1 ] } \\in { \\mathbb}{y}^{i - 1}}\\frac{w_{y_{[i - 1]}}(x_{[m + i - 1]})}{{m + i - 1 \\choose m}}\\mathfrak{h}(x_{[m + i - 1 ] } , y_{[i - 1 ] } ) , \\\\",
    "\\mathfrak{h}(x_{[m + i -1 ] } , y_{[i - 1 ] } ) = -\\sum_{z = -m}^0&{\\mathds}{1}_{\\{x_{1 - z } = y_1\\}}\\frac{w_{y_{[2 : i - 1]}}(x_{[2 - z : m + i - 1]})}{w_{y_{[i - 1]}}(x_{[m + i - 1]})}\\log_2\\big({\\mathds}{1}_{\\{x_{1 - z } = y_1\\}}\\frac{w_{y_{[2 : i - 1]}}(x_{[2 - z : m + i - 1]})}{w_{y_{[i - 1]}}(x_{[m + i - 1]})}\\big ) . \\notag\\end{aligned}\\ ] ]    ' '' ''    alternatively , instead of bounding the information rate as in proposition [ prop_caplb ] , we can write the following as an immediate consequence of equation and an argument similar to the one made in the proof of proposition [ prop_caplb ] .    [ prop_irdrc ] for the drc , @xmath169",
    ". \\notag \\blacksquare\\end{aligned}\\ ] ]    following arguments similar to the ones used in lemma [ lem_monli ] , we can show the following .",
    "[ lem_monri ] the sequence of lower bounds @xmath170 , where @xmath171 is non - decreasing , and @xmath172    the task of finding the rate - maximizing input distributions appears to be tough , with no theoretical insights or efficient numerical algorithms",
    ". often , to establish lower bounds on achievable rates , special classes of input processes are considered , and we will resort to a similar strategy here to obtain some expressions for the bounds we have so far developed",
    ". the following section will consider special cases of the drc wherein there are either only deletions , i.e. , the bdc , or only replications , i.e. , the brc . in a subsequent section",
    ", the symmetric drc will be studied .",
    "all bounds developed in the next section are similar to the generic bounds developed thus far .",
    "for the case of the bdc or the brc , evaluating some of the bounds developed in the previous section is somewhat easy , owing to the fact that the @xmath79 process is monotonic in these two special cases , i.e. , it is non - increasing or non - decreasing with increments of at most one , respectively .",
    "this monotonicity in @xmath79 implies that the @xmath110 process is strictly increasing for the bdc and non - decreasing with increments of at most one for the brc .",
    "this translates to the output being a subsequence of the input sequence for the bdc and vice versa for the brc .      in this subsection",
    "we estimate the information rates possible over the bdc , i.e. , @xmath139 , when the input process is either i.u.d . or when it is a first - order markov process .    for the bdc with i.u.d .",
    "inputs , we can easily show that @xmath128 is also an i.u.d . sequence .",
    "consequently , @xmath173 } ; z_{[n_n]})}{n } \\rightarrow 0\\text { as } n \\rightarrow \\infty\\ ] ] because the only information obtained from @xmath124}$ ] about @xmath174}$ ] is the length of the vector , and this information vanishes in the limit as @xmath98 .",
    "therefore , we have from equation that the lower bound in proposition [ prop_caplb ] is actually the _ symmetric information rate _",
    "we are hence interested in evaluating @xmath175 as defined in lemma [ lem_monli ] , where the superscript `` iud '' stands for independent , uniformly distributed inputs .",
    "in particular , we have the sir @xmath176    we start with some definitions and notation .",
    "we call a vector @xmath177 a _ subsequence _ of a vector @xmath178 if @xmath179 and the order of the elements in @xmath180 is the same as the order in which those elements appear in @xmath181 .",
    "for ease of notation , we will write @xmath182}}(x_{[j]})$ ] to denote the number of subsequences of @xmath183 } \\in \\mathbb{x}^j$ ] that are the same as @xmath184 } \\in \\mathbb{x}^i$ ] , which is referred to as the @xmath184}$]-_subsequence weight _ of the vector @xmath183}$ ] .",
    "we can write @xmath185}}(x_{[j ] } ) = \\sum_{s \\subset [ j ] : |s| = i } \\mathds{1}_{\\{x_s = y_{[i]}\\}}\\ ] ] where the elements of the set @xmath186 are arranged in ascending order .",
    "clearly , @xmath182}}(x_{[j ] } ) = 0 $ ] for @xmath187 .",
    "we define @xmath188 } ) = 1{\\ } \\forall{\\ } x_{[j ] } \\in { \\mathbb}{x}^j$ ] for @xmath189.@xmath31    for a binary sequence , a _ run _ is a maximal block of contiguous @xmath91s or @xmath0s .",
    "the _ run - length _ of a run is the number of symbols in it .",
    "we denote by @xmath190})$ ] the length of the first run in the vector @xmath183}\\ \\in { \\mathbb}{x}^j , j \\geq 1 $ ] .",
    "clearly , @xmath191 } ) \\leq |x_{[j]}| = j$].@xmath31    we will denote by @xmath192 and @xmath193 the sets of non - decreasing and non - increasing vectors of length @xmath112 , respectively , for @xmath194 .    [ thm_sirbdc ] for the bdc , @xmath195 where @xmath196 , with @xmath197 is as given in equation .    for the bdc , we have from lemma [ lem_monli ] that @xmath198 from equation , we need to show that @xmath199 we first note that @xmath200 } , y_{[i - 1]}).\\ ] ] clearly , the above entropy term is zero for @xmath201 . for @xmath202 ,",
    "given @xmath203 } = x_{[m + i - 1]}$ ] and @xmath154 } = y_{[i - 1]}$ ] , it is easy to see that @xmath204}}(x_{[2 - z : m + i - 1 ] } ) > 0\\}. \\notag\\end{aligned}\\ ] ] that is , @xmath205 only if @xmath206 and @xmath207 match , and the subsequent part of the output vector @xmath208}$ ] is a subsequence of the subsequent part of the input vector @xmath209}$ ] . also , for @xmath210 } \\in { \\mathbb}{z}_{\\downarrow}^{i - 1}$ ] ( which , as noted earlier , is true for the bdc ) , @xmath211 } & = z_{[i - 1 ] } , z_i = -m | x_{[m + i - 1 ] } = x_{[m + i - 1 ] } , \\notag \\\\ & y_{[i - 1 ] } = y_{[i - 1 ] } ) = { \\mathds}{1}_{\\{x_{[i - 1 ] - z_{[i - 1 ] } } = y_{[i - 1]}\\ } } \\p{t}^i\\p{d}^m , \\notag\\end{aligned}\\ ] ] where @xmath212 , so that for @xmath213 , @xmath214 } = x_{[m + i - 1 ] } , y_{[i - 1 ] } = y_{[i - 1 ] } ) \\notag \\\\ & = { \\mathds}{1}_{\\{x_{1 - z } = y_1\\ } } \\p{t}^i\\p{d}^m\\cdot w_{y_{[2 : i - 1]}}(x_{[2 - z : m + i - 1 ] } ) , \\notag \\\\",
    "{ \\mathsf}{p}(z_i & = -m | x_{[m + i - 1 ] } = x_{[m + i - 1 ] } , y_{[i - 1 ] } = y_{[i - 1 ] } ) \\notag \\\\ & = w_{y_{[i - 1]}}(x_{[m + i - 1]})\\p{t}^i\\p{d}^m \\notag\\end{aligned}\\ ] ] and hence , when @xmath215}}(x_{[m + i - 1 ] } ) > 0 $ ] , @xmath216 } = x_{[m + i - 1 ] } , y_{[i - 1 ] } = y_{[i - 1 ] } ) \\notag \\\\ & = \\frac{{\\mathds}{1}_{\\{x_{1 - z } = y_1\\}}w_{y_{[2 : i - 1]}}(x_{[2 - z_1 : m + i - 1 ] } ) \\p{t}^i\\p{d}^m}{w_{y_{[i - 1]}}(x_{[m + i - 1]})\\p{t}^i\\p{d}^m } \\notag \\\\ & = \\frac{{\\mathds}{1}_{\\{x_{1 - z } = y_1\\}}w_{y_{[2 : i - 1]}}(x_{[2 - z_1 : m + i - 1]})}{w_{y_{[i - 1]}}(x_{[m +",
    "i - 1]})}. \\notag\\end{aligned}\\ ] ] since , with i.u.d .",
    "inputs , @xmath217 } = x_{[m + i - 1 ] } | z_i = -m ) = 2^{-(m + i - 1)}$ ] and @xmath218 } & = y_{[i - 1 ] } | x_{[m + i - 1 ] } = x_{[m + i - 1 ] } , z_i = -m ) \\notag \\\\ & = \\frac{w_{y_{[i - 1]}}(x_{[m + i - 1]})}{{m + i - 1 \\choose m } } , \\notag\\end{aligned}\\ ] ] we have that @xmath219 as in equation . by noting that @xmath220 from equation ( with @xmath221 ) , we have the desired result .",
    "although evaluating @xmath222 in general is hard since we are required to count subsequence weights of sequences , we can evaluate it in two specific cases : for every @xmath223 when @xmath224 ( when all but a single bit are deleted ) and for all @xmath112 when @xmath225 ( when only a single bit is deleted ) .",
    "we examine these two cases in detail in appendix [ ssec_i2 ] , [ ssec_m1 ] and state the results here .",
    "[ cor_l2 ] for the bdc , @xmath226    [ cor_sdpsir ] for the bdc , @xmath227 where @xmath228.@xmath49    @xmath229 . \\label{eq_m1rbrc}\\end{aligned}\\ ] ]    ' '' ''    similar bounds for symmetric first - order markov input processes are considered in appendix [ ssec_bdcm1 ] .",
    "[ fig_bdc ] plots the bounds for @xmath230 .    .",
    "@xmath231 ( cf .",
    "equation ) is shown as the long - dashed blue line and @xmath232 ( or equivalently @xmath233 ) with the @xmath234 term dropped as the solid red line ( cf .",
    "equation ) .",
    "the best known numerical lower @xcite and upper bounds @xcite are shown as black and white circles respectively . the best known lower bound as @xmath235 approaches @xmath0 @xcite is shown as the dash - dotted green line .",
    "the inset shows the bounds for small @xmath235 values where the red solid curve is known to be tight from @xcite . ]      in this subsection , we will consider information rates for the brc , i.e. , @xmath141 . as in the previous subsection",
    ", we will consider i.u.d . and",
    "symmetric first - order markov inputs .    for the brc , the @xmath79 process is non - decreasing .",
    "moreover , when it increases , the increment is at most @xmath0 at each time instant .",
    "this simplifies the evaluation of information rates and we will , in fact , be able to write exact expressions for the markov-@xmath0 rates , as will be shown shortly . in this case , even when the input is i.u.d .",
    ", the term @xmath173 } ; z_{[n_n]})}{n } \\nrightarrow 0\\text { as } n \\rightarrow \\infty\\ ] ] in the normalized version of equation .",
    "hence the expression for the information rate in proposition [ prop_irdrc ] will prove to be more useful in this subsection .    [ thm_m1rbrc ] for the brc , the markov-@xmath0 rate is given as in equation .",
    "first we note that since @xmath236 , we have @xmath237 $ ] and @xmath238 $ ] .",
    "further we have for the brc that whenever @xmath239 , we must have @xmath240 or equivalently @xmath241 .",
    "this means that @xmath242 is independent of subsequent runs of @xmath128 ( and @xmath26 ) given the first run of @xmath128 ( and @xmath26 ) since we can achieve synchronization at the end of each run .",
    "thus we can write the conditional probabilities @xmath243 and @xmath244 in terms of the first runs of @xmath26 and @xmath128 , i.e. , @xmath245 and @xmath246 . note that we assume that @xmath109 so that @xmath247 .",
    "thus , if @xmath248 , then @xmath242 is @xmath91 or @xmath0 accordingly as @xmath207 is not equal or equal to @xmath249 , respectively .",
    "this means that there is no uncertainty in @xmath242 given the output sequence ( and the assumption that @xmath250 , which can be made without loss of generality ) .",
    "therefore , in estimating the entropy of @xmath242 given the output sequence , or the output and the input sequences , we can confine our attention to those sequences @xmath251 and @xmath252 whose first runs are comprised of zeros .",
    "we shall denote such runs as @xmath253 .",
    "for a first - order markov input process , we have , for @xmath254 @xmath255 and we can get from the definition of the brc that @xmath256 for @xmath257 .",
    "consequently , we have @xmath258 since @xmath259 excludes the first bit in the received sequence from being a replication , we can easily obtain @xmath260 for @xmath261 . for @xmath262 , @xmath263 therefore , @xmath264 and @xmath265 substituting these in proposition [ prop_irdrc ] specialized to the brc and first - order markov inputs , we have the desired result .",
    "the following results are shown in appendix [ app_brc ] .",
    "[ cor_brclb ] for the brc , @xmath266 for @xmath267.@xmath49    [ cor_srpbrc ] for the brc , @xmath268 where @xmath269.@xmath49    fig .",
    "[ fig_brc ] plots these bounds .     from corollary [ cor_brclb ]",
    "is shown as the long - dashed blue line and the markov-@xmath0 rate in equation is shown as the solid red line .",
    "the sir ( @xmath270 in equation ) is the dash - dotted green line .",
    "the numerical lower bounds in @xcite are shown as black circles .",
    "the inset shows the bounds for small replication probabilities . ]",
    "note that the sir and the markov-@xmath0 rate are non - convex in @xmath235 .",
    "further , it appears that the markov-@xmath0 rate ( and the sir ) are zero for some values of @xmath271 .",
    "however , this behavior is due to the fact that the term @xmath272 in equation is computed only up to a finite value of @xmath273 ( the curves in fig .",
    "[ fig_brc ] are , therefore , lower bounds for the markov-@xmath0 rate and the sir ) . for values of @xmath235 close to @xmath0 ,",
    "more terms in this sum need to be considered to get a better estimate of the achievable rates .",
    "[ rem_convex ] it was expected that the capacity of a memoryless sec was a convex function of the channel parameters .",
    "although this conjecture seems to be true for the bdc @xcite , we see that this conjecture is false for the brc .",
    "note that the lower bounds in @xcite themselves lead one to question the conjecture ( cf .",
    "[ fig_brc ] ) .",
    "however , the markov-@xmath0 rate for the brc in equation settles this conjecture as being false for the brc .",
    "this is because if the capacity were convex in the replication probability , no rate larger than @xmath274 would be achievable , which is clearly not the case as can be seen from fig .",
    "[ fig_brc ] .",
    "this implies that , in general , in presence of synchronization errors , the capacity is not convex in the channel parameters . in particular",
    ", it is possible that the capacity for the bdc is non - convex as well .",
    "although the bounds in the previous section provide us some idea of the achievable information rates for the bdc and the brc , they do not generalize in a straightforward manner for an sec with both deletions and replications for a first - order markov input process for a drc .",
    "we shall omit this here . ] . in order to obtain bounds when both deletions and replications are present , we take a different approach .",
    "we construct a sequence of channels that approximate the drc @xmath115 . to this end , we fix @xmath275 and let @xmath276 for @xmath152s given by the @xmath79 process defined in section [ sec_eqchn ] with @xmath277 defined as @xmath278 we then define the channel model for the @xmath279-approximating channel @xmath280 as was done for @xmath115 , @xmath281\\ ] ] where @xmath282 .",
    "it is clear that @xmath283 .",
    "the input and output alphabets of the channel @xmath284 are @xmath10 and @xmath116 respectively , same as those of @xmath115 .",
    "the transition probability @xmath285 for the channel @xmath284 is defined as in equation , but with the channel states defined by the process @xmath286 .",
    "the transition probability of the @xmath287 process itself is defined by equations and .",
    "we now establish a few properties of the @xmath287 process and the approximating channels @xmath284 .",
    "we start with some properties of the state process @xmath287 and the index process @xmath288 that will be useful in proving subsequent results .",
    "the following property , proved in appendix [ app_zmprop ] , establishes the non - stationarity of the sequence of channels @xmath289 .",
    "[ lem_zmprop ] the state process @xmath287 is a finite , time - inhomogeneous markov chain .",
    "moreover , the boundary states @xmath290 are eventually _ absorbing _ states , under the measure @xmath291 , in the following two cases .    a.   for @xmath292 when @xmath293 . b.   for @xmath294 when @xmath295.@xmath49    [ lem_subset ] for a fixed @xmath8 , for every @xmath296 , @xmath297}\\ } \\cap [ n ] \\subset \\{\\gamma^{(m)}_{[n^{(m)}_n]}\\ } \\cap [ n]\\text { a.s . } \\ ] ] and @xmath298}\\ } \\cap [ n ] \\subset \\{\\gamma^{(m)}_{[n^{(m)}_n]}\\ } \\cap [ n]\\text { a.s.},\\ ] ] where @xmath299 denotes the set of elements of the random vector @xmath300 , i.e. , where the random variables are not repeated.@xmath49    the proof is given in appendix [ app_subset ] .",
    "[ prop_fscbet ] for every @xmath275 , @xmath301 } ; y_{[n_n ] } ) & \\leq i(x_{[n ] } ; y^{(m)}_{[n^{(m)}_n ] } ) , \\text { and } \\notag \\\\ i(x_{[n ] } ; y^{(m + 1)}_{[n^{(m + 1)}_n ] } ) & \\leq i(x_{[n ] } ; y^{(m)}_{[n^{(m)}_n]}).\\vspace*{2mm}\\hspace*{18mm}\\blacksquare \\notag\\end{aligned}\\ ] ]    the proof is left to appendix [ app_fscbet ] . intuitively , the above result is true because the `` drift '' between the input and the output processes is bounded by @xmath223 for the approximating channel @xmath284 , whereas it is unbounded for the drc @xmath115 ( or equivalently @xmath46 ) .",
    "the result below , which gives a total ordering of the sequence of channels @xmath289 in terms of their mutual information rates , follows immediately from proposition [ prop_fscbet ] .",
    "[ cor_decm ] for any @xmath8 , the sequence @xmath302 , where @xmath303 } ; y_{[n_n^{(m)}]}^{(m)}),\\ ] ] is non - increasing .",
    "since @xmath304{\\ } \\forall{\\ } n \\in \\mathbb{n}$ ] and @xmath296 , @xmath305 exists and is equal to @xmath306.@xmath49    [ prop_ininm ] for any @xmath8 , we have @xmath307 } ; y_{[n_n ] } ) = i_n^\\dagger \\triangleq \\lim_{m \\rightarrow \\infty}\\downarrow i_{n , m}^\\dagger = \\inf_{m \\geq 0 } i_{n , m}^\\dagger.\\ ] ] consequently , for a stationary , ergodic input process @xmath308 , @xmath309 so that @xmath310 for stationary , ergodic , markov processes @xmath308 .",
    "the last equality in the first line is from corollary [ cor_decm ] . from proposition [ prop_fscbet ] , we have @xmath311 , from which @xmath312 follows .",
    "the equality is true because of the following .",
    "if we let @xmath313 } , y_{[n_n^{(m)}]}^{(m)}\\})$ ] , the sigma - algebra generated by the random variables @xmath314 } , y_{[n_n^{(m)}]}^{(m)}\\}$ ] , then @xmath315 is a _ filtration _",
    "@xcite , i.e. , @xmath316 thus , @xmath317 } , y_{[n_n^{(m)}]}^{(m)})$ ] is the restriction of @xmath291 to @xmath318 . from ( * ? ? ?",
    "* theorem 2 ) , we have that @xmath319 .    the limit of @xmath320 as @xmath1 goes to infinity exists and is equal to the infimum of the sequence from the subadditivity of the sequence @xmath321 and fekete s lemma ( cf .",
    "* appendix ii ) ) .",
    "the last claim made is true from proposition [ prop_marcap ] .",
    "[ cor_cncnd ] for any @xmath8 , we have that @xmath47})}i_n = c_n^\\dagger \\triangleq \\sup_{\\mathsf{p}(x_{[n]})}i_n^\\dagger\\ ] ] where @xmath322 is as defined in theorem [ thm_cap ] .",
    "therefore @xmath323    although @xmath289 is a sequence of channels that approximate @xmath115 , and have the properties discussed so far in this subsection , they are not useful as finite - state channels ( fscs ) , as shown below .",
    "[ lem_fscpnm ] for any @xmath296 , for a stationary , ergodic input process @xmath308 , @xmath324 so that @xmath325    from lemma [ lem_zmprop ] , the states @xmath290 are eventually absorbing for any @xmath296 .",
    "hence , in the limit as @xmath98 , the channel only has a delay of @xmath326 , and hence the result .",
    "we now attempt to obtain approximate channels that are stationary and useful as fscs .",
    "let @xmath296 .",
    "fix @xmath8 . consider the channel @xmath327 where @xmath328\\ ] ] with @xmath329}^{(m)}$ ] and @xmath330}^{(m)}$ ] as defined for the channel @xmath284 .",
    "the difference will be in the underlying measure @xmath331 .",
    "let the measure @xmath331 be such that the @xmath287 process is a finite , time - homogeneous , first - order markov chain with transition probabilities @xmath332 when @xmath333 , @xmath334 and @xmath335 note that the measure @xmath331 differs from @xmath291 only for state paths that reach beyond the states @xmath290 .",
    "the transition probabilities @xmath336 for the channel @xmath337 can now be defined as in equation , but under the measure @xmath331 .",
    "the stationarity of the channels @xmath337 follows from the time - homogeneity of the @xmath287 process .",
    "note that the sequence of sigma - algebras @xmath338 where @xmath339 forms a filtration .",
    "the sequence of measures @xmath331 as defined above seem to be defined only on the corresponding sigma - algebras @xmath340s for each @xmath296 .",
    "however , we can extend these measures to the sigma - algebra @xmath341 as in appendix [ app_extmeas ] , and will henceforth consider @xmath342 $ ] for each @xmath296.@xmath31    the lemma below shows that for a fixed @xmath296 , the fsc @xmath337 is an indecomposable fsc @xcite .",
    "[ lem_indec ] the fsc @xmath337 is indecomposable for every @xmath296 for @xmath343 .",
    "fix @xmath296 .",
    "we need to make a couple of modifications to put the channels @xmath344 in the parlance of discrete fscs .",
    "first , we set @xmath345.\\ ] ] note that @xmath346 $ ] , and hence the channel producing @xmath347}^{(m)}$ ] is `` causal '' .",
    "let the `` state '' @xmath348 of the channel @xmath337 at time @xmath349 $ ] be defined as @xmath350 } , \\acute{z}_i^{(m ) } ) \\in \\mathbb{x}^{2 m } \\times [ 0 : 2m],\\ ] ] where we set @xmath351 for @xmath352 $ ] .",
    "note that we need to redefine the state of the channel in this case to keep the factorization @xmath353    since @xmath354 is a finitely delayed , finitely shifted version of @xmath287 , and because @xmath287 is an _ irreducible , aperiodic _ markov chain under the measure @xmath331 ( * ? ? ?",
    "* chapter 1 ) as long as @xmath343 , so is @xmath354 .",
    "in particular , we have that for every @xmath355 , @xmath356}\\mathsf{p}_{\\langle m\\rangle}(\\acute{z}_i^{(m ) } = z | \\acute{z}_0^{(m ) } = z^\\prime ) > 0{\\ } \\forall{\\ } z^\\prime \\in [ 0 : 2m].\\ ] ] this implies that for @xmath357 and @xmath358 , by choosing by convention . ]",
    "@xmath359 } , z)$ ] for any @xmath360 $ ] , we see that @xmath361 for every @xmath362 $ ] . from (",
    "* theorem 4.6.3 ) , we have the desired result .    [ rem_infrate ]",
    "note that in the description of the causal channel in the proof above , we have discarded the part of the output @xmath363}^{(m)}$ ] by considering the causal output @xmath347}^{(m)}$ ] .",
    "this will however not matter in the estimation of the information rate since @xmath364 } ; y_{[n_n^{(m)}]}^{(m ) } ) - \\frac{1}{n}i(x_{[n ] } ; \\acute{y}_{[n]}^{(m ) } ) \\leq \\frac{m}{n},\\ ] ] and since @xmath296 is fixed , the rates are the same in the limit as @xmath1 goes to infinity.@xmath31    [ cor_capfsc ] for @xmath296 and the sequence of fscs @xmath344 with @xmath343 , the capacity is given by @xmath365 } ) } \\frac{1}{n}i(x_{[n ] } ; y_{[n_n^{(m)}]}^{(m ) } ) \\notag \\\\ & \\quad\\triangleq \\lim_{n \\rightarrow \\infty } \\sup_{\\mathsf{p}_{\\langle m\\rangle}(x_{[n ] } ) } i_{n , m}^\\star . \\notag\\end{aligned}\\ ] ]    from lemma [ lem_indec ] and remark [ rem_infrate ] , we have @xmath366 as defined in the statement can be written as @xmath367})}\\frac{1}{n}i(x_{[n ] } ; \\acute{y}_{[n]}^{(m)}).\\ ] ] now since @xmath131 } ; \\acute{y}_{[n]}^{(m ) } ) & = i(x_{[n ] } ; w_0^{(m ) } ) + i(x_{[n ] } ; \\acute{y}_{[n]}^{(m ) } | w_0^{(m ) } ) \\notag \\\\ & \\quad- i(x_{[n ] } ; w_0^{(m ) } | \\acute{y}_{[n]}^{(m ) } ) , \\notag\\end{aligned}\\ ] ] we have @xmath368 therefore @xmath367})}\\frac{1}{n}i(x_{[n ] } ; \\acute{y}_{[n]}^{(m ) } | w_0^{(m)}).\\ ] ] from ( * ? ? ?",
    "* theorem 4.6.4 ) , the quantity on the right hand side of the above equality exists and is the capacity of the indecomposable fscs @xmath344 .",
    "[ cor_marcapfsc ] for the fscs @xmath344 , the capacity @xmath366 can be written @xcite as @xmath369 } ; y_{[n_n^{(m)}]}^{(m ) } ) \\notag \\\\ & \\quad= \\sup_\\mathcal{x } \\lim_{n \\rightarrow \\infty } i_{n , m}^\\star \\triangleq \\sup_{\\mathcal{x } } i_\\mathcal{x}^\\star(m ) \\notag\\end{aligned}\\ ] ] where the supremum is over all stationary , ergodic input sources @xmath308.@xmath49    from lemma [ lem_indec ] , since @xmath344 are indecomposable fscs , we have from @xcite that @xmath370 } , y_{[n_n^{(m)}]}^{(m ) } ) & \\rightarrow \\lim_{n \\rightarrow \\infty}\\frac{h(x_{[n ] } , y_{[n_n^{(m)}]}^{(m)})}{n } \\notag \\\\ & = \\hat{\\mathcal{h}}(\\mathcal{x } , \\mathcal{y}^{(m ) } ) , \\notag \\\\ -\\frac{1}{n}\\log_2\\mathsf{p}_{\\langle m\\rangle}(y_{[n_n^{(m)}]}^{(m ) } ) & \\rightarrow \\lim_{n \\rightarrow \\infty}\\frac{h(y_{[n_n^{(m)}]}^{(m)})}{n } \\notag \\\\ & = \\hat{\\mathcal{h}}(\\mathcal{y}^{(m ) } ) , \\notag\\end{aligned}\\ ] ] as @xmath98 a.s . , where the entropies are calculated with respect to the measure @xmath331 .",
    "therefore @xmath371 can be estimated numerically using the forward passes of the bcjr algorithm @xcite to estimate @xmath372 and @xmath373 , as in @xcite . moreover , optimizing markov input sources numerically is possible @xcite for these fscs .    in fig .",
    "[ fig_sdrc ] , we plot the sirs , @xmath374 , for the indecomposable fscs @xmath344 obtained through numerical simulations for @xmath375 and @xmath376 $ ] .     with @xmath376 $ ] for different @xmath223 values are shown in solid lines .",
    "the lower bound on the capacity of the sdrc from corollary [ cor_capbndsspl ] is also shown as the dashed line . ]",
    "the value of @xmath1 used for the estimation was @xmath377 .",
    "the error in estimation is consequently upper bounded by @xmath378 .",
    "a couple of observations are worthwhile noting .",
    "first , the sirs @xmath379 are non - increasing .",
    "this hints at a total ordering of the fscs @xmath380 with respect to the information rates similar to what we had in corollary [ cor_decm ] .",
    "second , we see that for small values of @xmath235 , the sirs get bunched up as @xmath223 increases , i.e. , the sirs @xmath374 converge quickly , so that we have a good estimate of @xmath381 for @xmath235 close to @xmath91 .",
    "[ prop_inmstar ] for @xmath8 , we have @xmath382 thus , @xmath383    for a fixed @xmath8 , since we have that @xmath384 } , y_{[n_n^{(m)}]}^{(m ) } ) \\rightarrow \\mathsf{p}(x_{[n ] } , y_{[n_n]})$ ] as @xmath385 for every @xmath386 , @xmath384 } , y_{[n_n^{(m)}]}^{(m)})$ ] converges to @xmath317 } , y_{[n_n]})$ ] in total variation as @xmath385 .",
    "consequently , from ( * ? ? ?",
    "* corollary 1@xmath387 ) , we have the desired result .    [ rem_conj2 ]",
    "we conjecture that @xmath388 as @xmath385 . from ( * ? ? ?",
    "* corollary 1@xmath387 ) , one needs to show uniform integrability of the _ information densities _",
    "@xmath389 } , y_{[n_n^{(m)}]}^{(m)})$ ] for the conjecture to be true .",
    "if the sequence of channels @xmath380 is totally ordered for every @xmath8 with respect to the mutual information rates , as was the case for the sequence @xmath289 ( cf . corollary [ cor_decm ] ) , i.e. , if @xmath390 is a non - increasing sequence for every @xmath8 , then we know that @xmath391 and from proposition [ prop_inmstar ] , @xmath392 follows .",
    "unfortunately , we are not able to show this monotonicity in the sequence @xmath390 as we argued in the case of the sequence @xmath302 . although the refinedness property of the @xmath95 process ( cf .",
    "lemma [ lem_subset ] ) still holds , the different measures @xmath331 being used for each @xmath296 do not allow us to generalize the result of corollary [ cor_decm ] .",
    "however , fig .",
    "[ fig_sdrc ] provides sufficient empirical evidence for this monotonicity conjecture.@xmath31      in this subsection , we consider the sdrc , i.e. , the case when @xmath393 .",
    "this channel is of interest since in practice , systems prone to mis - synchronization are usually not biased to produce more deletions or replications . for the case of the sdrc , we can fix @xmath223 to be a function of @xmath1 satisfying a simple condition and define a sequence of approximating channels .",
    "[ lem_prub ] for the sdrc , for every @xmath8 , let @xmath394 .",
    "then , @xmath395    we relegate the proof to appendix [ app_prub ] .",
    "the significance of the above result can be seen by noticing that , for the sdrc , the probability ( under measure @xmath291 or @xmath331 ) with which the approximating channels introduced in the previous two subsections differ from the actual channel can be made arbitrarily small by setting @xmath396 , i.e. , @xmath397 , and choosing a large enough @xmath1 .",
    "for the so - chosen sequence of approximating channels , we can conclude that the limiting channel characterizes the sdrc from the following result whose proof is left to appendix [ app_sdrclim ] .",
    "[ prop_sdrcap ] for the sdrc , @xmath398 where @xmath396 , for stationary , ergodic input process",
    "@xmath308.@xmath49    the channels @xmath380 give us a way to approach the problems of optimizing input distributions as well as designing coding schemes for the sdrc .",
    "we can optimize the inputs of @xmath337 , starting with small values of @xmath223 , under some input assumptions , e.g. , for fixed - order markov inputs @xcite .",
    "note that the numerical estimation of @xmath399 is possible ( as described in the previous subsection ) only when @xmath400 , since setting the channels as indecomposable fscs ( cf .",
    "lemma [ lem_indec ] ) is possible only in this case .",
    "moreover , for a good estimate of the information rate , we will require @xmath401 .",
    "for the sdrc , proposition [ prop_sdrcap ] allows us to consider some @xmath402 , where @xmath403 is both @xmath404 as well as @xmath405 , for which a good estimate of the information rate @xmath406 can be obtained .",
    "note that due to the lack of a result analogous to lemma [ lem_prub ] in the case of a general drc for @xmath400 , generalizing these arguments when @xmath293 is not completely justified .",
    "starting with some small values of @xmath223 , we expect that the information rates and optimal distributions quickly converge ( in @xmath223 ) , giving us a way to characterize optimal inputs for the sdrc @xmath115 . for small values of @xmath235 , as in fig .",
    "[ fig_sdrc ] , the information rates for the sdrc can be characterized numerically for moderate values of @xmath223 ( much smaller than @xmath404 guaranteed by lemma [ lem_prub ] ) . for optimizing the input distribution for an approximation @xmath337",
    ", we can start with optimizing inputs that are @xmath407-order markov processes , for @xmath408 . as was observed of the input markov process",
    "is expected to be true . ] in @xcite , the convergence of optimal information rates as a function of the order @xmath409 of the input markov process is expected to be rapid .",
    "the authors in @xcite hypothesized that this convergence was exponential in @xmath409 .",
    "similar `` diminishing returns '' on increasing @xmath409 has also been observed by others @xcite .",
    "we think that a similar rapid convergence of @xmath410 to @xmath411 also holds for @xmath223 , where @xmath410 is the optimal information rate achieved by a @xmath407-order markov input process on the fsc @xmath337 and @xmath411 is the optimal information rate achieved by a @xmath407-order markov input process on the sdrc @xmath115 .",
    "we use the _ generalized blahut - arimoto algorithm _ presented in @xcite to evaluate @xmath410 for some small values of @xmath223 and @xmath409 .",
    "figure [ fig_sdrccap ] plots these estimates , which illustrates the aformentioned observations .     for @xmath412 and @xmath413 , and @xmath414 ( solid lines ) . for the case",
    "where @xmath415 , only the estimates for small deletion - replication probabilities have been evaluated .",
    "we chose @xmath416 for @xmath412 and @xmath417 for @xmath418 .",
    "the smaller value of @xmath1 in the case of @xmath418 was chosen for computational convenience . also shown ( in dashed lines ) for comparison are the corresponding estimates of the sir ( @xmath419 ) from figure [ fig_sdrc ] . ]",
    "note that it is clear from the plots above that bernoulli equiprobable inputs achieve rates ( sir ) comparable to higher - order markov inputs for small proababilities of deletion and replication . however , unlike isi channels @xcite , it is not clear that markov sources of increasing orders achieve capacity .",
    "it is also evident from the figure that for small values of the deletion - replication probabilities , the information rates seem to converge as @xmath223 increases even for small values of @xmath223 ( @xmath418 ) .",
    "this suggests that figure [ fig_sdrccap ] plots good estimates of @xmath406 for such channels , and since the values of @xmath1 chosen are large , that they indeed represent good estimates of the markov capacity of the sdrc .",
    "apart from the above advantage of facilitating numerical estimation of information rates , the approximating channels @xmath337 have another important advantage .",
    "this is that since they have immediate factor - graph interpretations , there is a possibility of constructing sparse graph - based coding schemes and decoding over the joint graphical model representing the channels as well as the codes , as was done for joint detection and decoding of ldpc codes on partial response channels @xcite . instead of trying to build codes for the sdrc @xmath115",
    ", the problem can be reduced to designing good codes and efficient decoding schemes for the fscs @xmath337 for small values of @xmath223 . for small deletion - replication probabilities @xmath235 , which is the case of interest in practice",
    ", we can expect these codes to perform well for the sdrc @xmath115 as well .",
    "in this section , we discuss different scenarios that can be modeled using the channel model introduced in section [ sec_eqchn ] with appropriate modifications . wherever possible",
    ", methods for information theoretic analysis for these cases through generalizations of the channel model presented in this paper are highlighted .",
    "the channel model of equation allows us to handle deletions as well as replications .",
    "however , the class of secs that introduce random insertions can not be written in the form of equation .",
    "a suitable modification for our model in this scenario is to let @xmath420 where @xmath421 , and @xmath422 is a bernoulli sequence with parameter @xmath423 ( for `` flip '' ) .",
    "this means that the probability of a random insertion is @xmath424 and that of a replication is @xmath425 .",
    "note that this can be easily generalized to any finite sets @xmath10 , and arbitrary sets @xmath116 and @xmath426 ( with an appropriate notion of the addition operation `` @xmath427 '' ) .",
    "analysis of this channel is , however , more complicated than analyzing the drc itself due to the cascaded additive noise channel which also depends on the `` shared '' state process @xmath100 .",
    "however , when the channel produces deletions , replications , random insertions as well as substitutions , we can write @xmath428 which is just a cascade of the drc and an additive noise channel . in the binary setting , this corresponds to a channel that deletes a bit @xmath429 with probability @xmath61 , or inserts a sequence @xmath20 with probability @xmath430 .",
    "this implies that the substitution error probability for a bit is given by @xmath431 .    the capacity ( or the information rate achievable by a given input process ) and coding for a cascade of binary , memoryless channels without synchronization errors has been studied in , e.g. , @xcite .",
    "some lower bounds on the capacity of a cascade of a bdc and an additive noise channel were given in @xcite .",
    "the possibility of extending these results to general memoryless secs using the model presented here remains a problem worth exploring .",
    "an sec with memory is defined as in definition [ def_msec ] , with the only difference being that the transition probabilities @xmath432 } | x_{[n]})$ ] do not factorize as products of individual probabilities @xmath122 . as an example , one could think of an sec where having a deletion for a symbol influences the likelihood of the following symbol being deleted , i.e. , a channel that introduces a burst of deletions .",
    "similarly , channels that introduce a limited number of deletions in every input subsequence of a certain length could also occur in practice .",
    "these have been studied under the name of _ segmented _ deletion channels @xcite .",
    "note that the definition of the segmented deletion channel is slightly different in the references cited , where it is assumed that the input is divided into blocks of a certain length and at most one deletion occurs within each block .",
    "our definition is more general and corresponds closer to reality .",
    "the channel model in equation generalizes readily to the case of drc with memory .",
    "consider the @xmath100 process to be a non - increasing ( so that only deletions occur ) , time - homogeneous , shift - invariant markov process of order @xmath433 such that @xmath434 } = z_{[i - \\mathsf{z } : i - 1 ] } ) > 0\\ ] ] only for @xmath435}$ ] such that @xmath436 for some @xmath437 , and @xmath438 .",
    "then , clearly at most one deletion occurs for every input subsequence of length @xmath439 . the model in equation",
    "will then correspond to a segmented deletion channel where no more than @xmath0 deletion occurs for every @xmath439 input symbols .",
    "similarly , we can model other drc with memory by suitably considering the @xmath100 process to be a markov process of some order with specific transitions occuring with non - zero probability .",
    "although we have let @xmath433 , not all second - order markov processes @xmath100 result in secs with memory .",
    "one example worth noting is when the @xmath100 process is non - decreasing with increments of at most @xmath0 , and is such that two consecutive increments occur with probability @xmath91 .",
    "this results in a replication channel where each symbol is transmitted noiselessly and possibly replicated once  this is exactly the _ elementary sticky channel _ introduced in @xcite , which is a memoryless sec",
    ". we will refer to such channels that introduce a bounded number of inserted symbols per input symbol as _ bounded _ , memoryless secs .",
    "this particular channel has also been studied in @xcite , where some analytical lower bounds on the capacity were given .",
    "another example where @xmath440 does not result in an sec with memory but is a bounded , memoryless sec is gallager s model @xcite of the insertion - deletion channel .",
    "some analytical lower bounds for the capacity of this channel ( without deletions ) were given in @xcite .",
    "achievable rates for a bounded , memoryless sec were studied in @xcite , and those for a cascade of a bounded , memoryless sec with an inter - symbol interference ( isi ) channel in @xcite .",
    "some bounds on the capacity of a bounded , memoryless sec with substitution errors were given in @xcite .",
    "note that the channel coding theorem for secs with memory has not been established .",
    "the various works on the `` capacity '' of such channels is an indication of such secs occurring widely in practice .",
    "establishing the channel coding theorem for secs with memory is , therefore , important both for the theory and in practice .",
    "for secs with memory , since the channel model will have the transition probabilities that still factorize as in equation ( with the channel state transition probabilities replaced by the higher - order transition probabilities ) , it is more amenable to analysis and could potentially be used to establish the channel coding theorem .",
    "channels that consider mis - synchronization due to _ jitter _ or _ bit - shifts _ have been studied in the context of magnetic recording and constrained coding @xcite .",
    "these represent a variant of the general model of the drc presented in the present paper . in particular , they are characterized by a @xmath100 process where each valid state path @xmath441 has increments and decrements of size at most @xmath0 , and the transition probabilities are data - dependent . the zeros and ones in the input correspond to the absence and presence , respectively , of a transition in the signal .",
    "thus , the presence of a transition can not be deleted , i.e. , a @xmath0 in the input stream can not be deleted , whereas the @xmath91s can be deleted or replicated ( at most once ) .",
    "the authors of @xcite gave bounds on the capacity and the zero - error capacity of bit - shift channels and also present some bounds on achievable rates over a concatenation of the bit - shift channel with a binary symmetric channel .",
    "similar analysis was performed in @xcite for discrete and continuous channels with timing jitter .",
    "numerical upper and lower bounds on the capacity of a binary channel with jitter where transitions could `` cancel '' each other were given in @xcite .",
    "another class of channels that resemble these channels are the `` paired '' insertion - deletion channels studied in the context of bit - patterned media recording @xcite . here",
    ", the channel is similar to the approximating fsc given in section [ ssec_appsc ] with @xmath225 . in @xcite ,",
    "the authors give bounds on the capacity and the zero - error capacity of the channel for varying sizes of the state space .",
    "a further specialization of this channel is the one - dimensional _ graunlar media _ recording channel .",
    "this has also been studied in @xcite , where some bounds on capacity and coding constructions have been proposed .",
    "the _ trapdoor channel _ introduced by blackwell ( see @xcite ) is a channel where the input stream is fed to a buffer at the same rate as symbols from within the buffer are randomly drawn as the output stream . using our model",
    ", we can define the trapdoor channel as follows .",
    "the multiset of indices of the buffer contents at time @xmath194 is denoted as @xmath442 , which is of size @xmath443 .",
    "we initialize @xmath444 and define the output at the @xmath445 instant as @xmath446 for @xmath349 $ ] , where @xmath111 has the distribution @xmath447 .",
    "the buffer multiset is updated as @xmath448 . in this case",
    ", a further simplification of the channel model might be more useful since the channel depends not on the indices of the inputs in the buffer , but on the _ type _ @xcite of the buffer contents at any time .",
    "this channel was generalized to define _ permuting channels _ in @xcite .",
    "although the trapdoor channel is described easily , its capacity , even in the simplest case of @xmath449 and @xmath450 , has been an open problem ever since its introduction . in @xcite ,",
    "the authors considered coding schemes for certain non - probabilistic models of the trapdoor channel .",
    "the capacity of the probabilistic trapdoor channel when @xmath449 and @xmath450 is known to satisfy @xcite @xmath451 it is worthwhile to explore the possibility of obtaining better bounds on the capacity of the trapdoor and permuting channels using the model presented in this paper .",
    "a simple model for molecular communication or chemical channels is as follows .",
    "the channel state @xmath152 at time instant @xmath112 is a random variable on the alphabet @xmath452 $ ] and represents the delay introduced to the input at time @xmath112 .",
    "the output at time @xmath112 is given as @xmath453 i.e. , the output is the sum of all the channel inputs that arrive at time @xmath112 .",
    "this channel was studied in @xcite as a _ delay selector channel _ and a lower bound on the capacity was given assuming that the state process is i.i.d .. in general , the state process can be modeled as a markov process , and the channel might be amenable to a similar analysis as presented here .",
    "there is a link between discrete _ timing channels _",
    "@xcite , where information is communicated not only in the signals but also in the timing of the signals and the randomness is in the arrival times of the signals , and `` good '' transmission sequences for secs .",
    "this is in the sense that a information - bearing transmission sequence for an sec must not only be able to carry information within the sequence , but also contain information in the ordering of the symbols within the sequence , such that even in the presence of synchronization errors , the information about the symbol ordering is not completely lost .",
    "that is to say that the sequences @xmath23}$ ] must be such that under limited number of synchronization errors , the received sequence @xmath124}$ ] must convey adequate information about the state sequence @xmath174}$ ] .",
    "therefore , it might be of importance to study whether methods of coding over timing can be used to obtain efficient codes and decoding schemes for the secs .",
    "we introduced a new channel model for a class of secs which formulated the sec as a channel with states .",
    "this allowed us to obtain analytical lower bounds for the capacity of secs with only deletions or only replications .",
    "for the case of the bdc , we were able to write the sir in terms of subsequence weights of binary sequences .",
    "subsequence weights are known to be a quantity of interest in the maximum - likelihood decoding of sequences for the bdc ( cf .",
    "equation ) .",
    "moreover , it is clear from equation that the dependence of information rates for the bdc on the input statistics only appears in the term @xmath454 , whereas the subsequence weights influence @xmath455 independently of the input statistics .",
    "thus , our result establishes a natural link between the capacity of the bdc and the metric relevant for ml decoding .",
    "we were also able to obtain lower bounds on the capacity of the bdc that are known to be tight for small deletion probabilities .",
    "for the brc , we were able to exactly characterize the markov-@xmath0 rate , which is , to the best of our knowledge , the first analytic lower bound on the capacity of the brc .",
    "in doing so , we were able to disprove the conjecture that the capacity of secs is a convex function of the channel parameters , at least in the case of the brc .",
    "for the case of an sec with deletions and replications , we were able to provide a sequence of approximating fscs that are totally ordered with respect to the mutual information rates achievable , and therefore , with respect to capacities .",
    "these approximating fscs were shown to be such that the mutual information rate achievable for the sec was equal to the limit of the mutual information rates achievable for the sequence of fscs . to obtain numerical estimates of achievable rates on the drc",
    ", we defined another sequence of indecomposable fscs .",
    "computing the mutual information rates for this sequence of fscs allows us to relate the mutual information rate for the drc to the limiting value of the mutual information rates of the sequence . for the particular case of the sdrc",
    ", we were able to show a stronger form of convergence of these mutual information rates .    the formulation in this paper not only allows us to get estimates of mutual information rates achievable on secs but also gives some insight into possible code constructions and decoding schemes for such channels .",
    "the approximations introduced for the drc gives us a natural way to reduce these problems .",
    "one would therefore obtain progressively better performing codes for the drc by designing good codes for the sequence of approximating fscs .",
    "we expect that for a small values of the deletion - replication probability , a code constructed for an approximation with a moderate value of @xmath223 will perform well over the drc as well .",
    "some coding schemes for special cases of the fscs ( with @xmath225 ) have been known in various contexts ( see section [ ssec_jbsge ] ) .",
    "extending these schemes to better approximations ( larger @xmath223 values ) will prove crucial in designing good codes for the drc .",
    "we emphasize that although the present paper considers only _ binary _ secs , the results extend naturally to the case of larger finite alphabets .",
    "the expressions for information rates will perhaps become more complicated , but the methods to arrive at their bounds or numerical estimates remain unchanged .",
    "the present formulation of the secs also allows us to make the following remarks on the bdc .    * in @xcite ,",
    "the authors conjectured that the capacity of the bdc has a taylor - like series expansion .",
    "we see from theorem [ thm_sirbdc ] that this is true for the sir of the bdc .",
    "we expect that the capacity also has a similar formulation .",
    "* the capacity of a general sec might not be convex in the channel parameters ( see remark [ rem_convex ] ) .",
    "it was shown in @xcite that the capacity @xmath456 of the bdc satisfies @xmath457 } \\frac{c_{\\mathrm{bdc}}(p)}{1 - p } = \\lim_{p \\rightarrow 1 } \\frac{c_{\\mathrm{bdc}}(p)}{1 - p}.\\ ] ] it is therefore expected that @xmath458 is convex in @xmath235 . from theorem [ thm_sirbdc ] , we see that the sir @xmath459 of the bdc can be written as @xmath460 where @xmath461 is non - convex in @xmath235 for @xmath462 .",
    "it is interesting to see if this double limit turns out to be convex despite @xmath463 being non - convex for every @xmath464 .",
    "extending this to the case of the capacity @xmath230 is also of interest . * in order to obtain bounds for the capacity of a bdc for @xmath235 close to @xmath0 , one might typically consider the case where all but one ( or a few ) symbols are lost .",
    "the lower bound @xmath465 presented here ( cf .",
    "lemma [ lem_monli ] ) corresponds to this situation .",
    "however , since we considered this bound for a first - order markov input , the bounds we obtained did nt prove to be useful for @xmath235 close to @xmath0 .",
    "it might therefore be of interest to generalize this bound for a high - order markov input which might give us a strictly positive ( and thereby non - trivial ) achievable rate .",
    "a.   this is true since @xmath466 .",
    "b.   since @xmath89 . c.   notice",
    "that , for each @xmath8 , we can write @xmath467 where the @xmath468 s are i.i.d . with @xmath469 from the strong law of large numbers ( slln ) , we therefore have @xmath470 a.s . as @xmath98 .",
    "we also have @xmath97 a.s . as @xmath98 from point ( ii ) above .",
    "therefore , @xmath471 a.s . as @xmath98 .",
    "further , by definition , we have @xmath472 , i.e. , @xmath473 thus @xmath474 a.s . as @xmath98 .",
    "a.   by definition , @xmath100 is a first - order markov chain .",
    "time - homogeneity implies that @xmath475 this is true for the state process @xmath100 from the definition since the transition probabilities in equation do not depend on the time index @xmath112 .",
    "shift - invariance implies @xmath476 this is true because the state transition probabilities in depend only on the difference @xmath477 .",
    "+ the @xmath95 process inherits these properties from @xmath100 through the bijection @xmath478 , where with some abuse of notation , we write @xmath103 } = \\zeta(z_{[n ] } ) = ( \\zeta(z_i ) ) , i \\in [ n]$ ] , with @xmath479 , { \\ } \\forall{\\ } n \\in \\mathbb{n}$ ] .",
    "+ the irreducibility and aperiodicity of the @xmath100 process follow from the definition .",
    "b.   note that from equation , @xmath480 a.s . for every @xmath481 .",
    "hence @xmath482 a.s .",
    "for every @xmath483 . since @xmath86",
    ", we have @xmath484 with probability @xmath0 .",
    "c.   from the bijection @xmath485 ( see point ( i ) above ) and appendix [ app_nnprop ] , we have @xmath486 hence @xmath487 } ) & = \\sum_{i = 1}^n h(z_i | z_{[i - 1 ] } ) = \\sum_{i = 1}^n h(z_i | z_{i - 1 } ) \\notag \\\\ & = n\\big(h_2(\\p{r } ) + \\frac{1 - \\p{r}}{1 - \\p{d}}h_2(\\p{d})\\big ) .",
    "\\notag\\end{aligned}\\ ] ]      from equation , we can write @xmath492 } | x_{[i + j ] } , y_{[n_{i + j } ] } ) \\notag \\\\ & = h(z_{[n_i ] } | x_{[i + j ] } , y_{[n_{i + j } ] } ) \\notag \\\\ & \\qquad+ h(z_{[n_i + 1:n_{i + j } ] } | x_{[i + j ] } , y_{[n_{i + j } ] } , z_{[n_i ] } ) \\notag \\\\ & \\geq h(z_{[n_i ] } | x_{[i + j ] } , y_{[n_{i + j } ] } , n_i ) \\notag \\\\ & \\qquad+ h(z_{[n_i + 1:n_{i + j } ] } | x_{[i + j ] } , y_{[n_{i + j } ] } , z_{[n_i ] } , n_i ) \\notag \\\\ & \\stackrel{(a)}{= } h(z_{[n_i ] } | x_{[i ] } , y_{[n_i ] } ) \\notag \\\\ & \\qquad+ h(z_{[n_i + 1:n_{i + j } ] } | x_{[i + 1:i + j ] } , y_{[n_i + 1 : n_{i + j } ] } , z_{n_i } ) \\notag \\\\ & = ih_i + jh_j . \\notag\\end{aligned}\\ ] ] in the above , the equality labeled @xmath132 follows from the conditional independence of @xmath493}$ ] and @xmath494}$ ] on @xmath495 } , y_{[n_i + 1:n_{i + j}]})$ ] and @xmath496 } , y_{[n_i ] } , z_{[n_i - 1]})$ ] , respectively , given @xmath497 . from fekete s lemma ( *",
    "* appendix ii ) , this superadditivity proves the claim .      from the definition of the @xmath287 process in equation",
    ", it is clear that @xmath589 for every @xmath81 , and that it is a markov chain .",
    "it is therefore a finite markov chain .",
    "the time - inhomogeneity follws by noting the transition probabilities between states , which can be easily shown to be given as follows . for @xmath333 , @xmath194 , @xmath590 from the states @xmath290 ,",
    "the transition probabilities are @xmath591 where @xmath592 and for @xmath112 such that @xmath593 , @xmath594 where @xmath595 note that it is only transitions from the boundary states @xmath290 that have time - dependent probabilities .    as was noted in the proofs of lemmas [ lem_nnprop ] and [ lem_monli ] , we can write the @xmath100 process as @xmath596 where @xmath597 is an i.i.d .",
    "process with @xmath598 as noted before , from the slln , @xmath599 = \\frac{\\p{r } - \\p{d}}{1 - \\p{d } } \\triangleq \\chi$ ] a.s . as @xmath98 .",
    "let us write @xmath600 = \\mathsf{e}[\\xi_1 ^ 2 ] - \\mathsf{e}[\\xi_1]^2 = \\frac{\\p{r } + \\p{d } + \\p{d}^2 - 3\\p{d}\\p{r}}{(1 - \\p{d})^2 } \\triangleq \\nu^2.\\ ] ] from the central limit theorem ( clt ) , we have @xmath601 where @xmath602 with @xmath603 , and @xmath604 writing @xmath605 , we can say that when @xmath606 , @xmath607 and when @xmath608 , @xmath609 when @xmath610 , for @xmath294 , @xmath611 hence @xmath612 the result then follows by noting that @xmath613 is equal ( or not equal ) to @xmath91 accordingly as @xmath61 is equal ( or not equal , respectively ) to @xmath614 .",
    "we first note that although @xmath78 and @xmath615 might themselves differ , both sets @xmath616}\\}$ ] and @xmath617}\\}$ ] are subsets of @xmath618 \\cup \\{0\\}$ ] .",
    "therefore , assuming that all random variables @xmath619 where @xmath352 $ ] are constants ( in particular , we assume that these random variables are all equal to @xmath91 ) , we can consider the above sets of indices to be @xmath620}\\}$ ] and @xmath621}\\}$ ] respectively , where we define @xmath622 we have @xmath623 a.s .",
    "for every @xmath624 .",
    "let @xmath625 and @xmath626 , where we define @xmath627 .",
    "then , @xmath628 $ ] is the set of instances where @xmath152 and @xmath629 differ for the first time as a result of @xmath152 exceeding @xmath223 . in this case , @xmath630 with probability @xmath0 from the definition of the @xmath100 process .",
    "further , @xmath631 a.s . , implying for this range of @xmath632s that @xmath633 a.s .. but , by definition , @xmath634 , and hence @xmath635 .",
    "thus , if we write @xmath636 , then we have @xmath637 since @xmath638 , @xmath639 a.s . from lemma [ lem_ztprop ]",
    "similarly , since @xmath640 , @xmath641 a.s . from lemma [ lem_ztprop ]",
    "it follows that the indices in @xmath642 can not appear in @xmath299 for any @xmath643 \\setminus { \\mathbb}{u}^+_1 $ ] .",
    "using similar arguments , by recursively defining for @xmath202 @xmath644 and letting @xmath645 , we can show that @xmath646    similarly , consider @xmath647 then , @xmath648 and @xmath649 with probability @xmath0 .",
    "further , with probability @xmath0 , @xmath650 a.s . , implying @xmath651 a.s .. by definition , @xmath652 , and consequently @xmath653 where @xmath654 .",
    "as before , the missing indices can not appear in @xmath300 for any @xmath643 \\setminus { \\mathbb}{u}^-_i$ ] .",
    "therefore , writing @xmath655 , we have @xmath656 let @xmath657 \\setminus \\mathbb{u}^{\\pm}$ ] .",
    "since @xmath658 consists of all indices @xmath112 where @xmath111 and @xmath659 differ , we have from the above relation that almost surely , @xmath660}\\ } & \\subset \\{\\gamma_{[n(m .",
    "n)]}^{(m)}\\ } \\notag \\\\",
    "\\rightarrow \\{\\gamma_{[n_n]}\\ } \\cap [ n ] & \\subset \\{\\gamma_{[n_n^{(m)}]}^{(m)}\\ } \\cap [ n ] .",
    "\\notag\\end{aligned}\\ ] ] we are interested in the intersection in the last step above since only indices in the set @xmath618 $ ] are indices of non - constant random variables",
    ".    we can use an argument similar to the one above to show that @xmath661 , @xmath298}\\ } \\cap [ n ] \\subset \\{\\gamma^{(m)}_{[n^{(m)}_n]}\\ } \\cap [ n]\\text {   a.s .. }\\ ] ]      as noted in the proof of lemma [ lem_zmprop ] , we have for every @xmath8 that @xmath690 is the @xmath691 partial sum of the i.i.d . process @xmath692 . for the sdrc , we have @xmath693 = \\chi = 0 $ ] and @xmath694 = \\nu^2 = \\frac{2p}{1 - p } < \\infty$ ] since @xmath695 .",
    "let @xmath696 , the sigma - algebra generated by @xmath690 , for every @xmath8 .",
    "clearly , @xmath697}\\})$ ] so that @xmath698 is a filtration , and @xmath699 by definition .",
    "let @xmath700 as @xmath98 .",
    "then for every @xmath8 , @xmath701 since @xmath702 & = \\mathsf{e}[z_n^2 ] = \\mathsf{e}\\big[\\big(\\sum_{i = 1}^n\\xi_i\\big)^2\\big ] \\notag \\\\ & = \\sum_{i = 1}^n \\mathsf{e}[\\xi_i^2 ] + \\sum_{i = 1}^n\\sum_{j = 1 , j \\neq i}^n\\mathsf{e}[\\xi_i\\xi_j ] \\notag \\\\ & = n\\cdot\\mathsf{var}[\\xi_1 ] + \\sum_{i = 1}^n\\sum_{j = 1 , j \\neq i}^n\\mathsf{e}[\\xi_i]\\mathsf{e}[\\xi_j ] \\notag \\\\ & = n\\frac{2p}{1 - p } < \\infty .",
    "\\notag \\end{aligned}\\ ] ] further , @xmath703 = \\mathsf{e}[z_{n - 1 } + \\xi_n | \\mathscr{s}_{n - 1 } ] = z_{n - 1}$ ] .",
    "therefore , @xmath704 is a martingale under the measure @xmath291 .",
    "consequently , @xmath705 is a submartingale .",
    "since @xmath706 , from doob s submartingale inequality @xcite , we have @xmath707}{m^2 } = \\big(\\frac{2p}{1 - p}\\big)\\frac{n}{m^2}.\\ ] ] we have ( cf . section [ sec_appchn ] ) that @xmath708 .",
    "the result then follows by noting that @xmath709 and the above result .",
    "the bound with respect to the measure @xmath710 is true because @xmath711 from the definition of the measure @xmath331 ( see section [ ssec_appsc ] and appendix [ app_extmeas ] ) .",
    "let @xmath109 and consider semi - infinite input , state and output processes .",
    "we first note that @xmath488 , @xmath489 } = y_{[k : l ] } | z_{k - 1 } = 0 ) \\notag \\\\ & = \\sum_{\\gamma_{[k : l ] } } { \\mathsf}{p}(\\gamma_{[k : l ] } = \\gamma_{[k : l ] } , x_{\\gamma_{[k : l ] } } = y_{[k : l ] } | \\gamma_{k - 1 } = k - 1 ) \\notag \\\\ & \\stackrel{(a)}{= } \\sum_{\\gamma_{[k : l ] } } { \\mathsf}{p}(\\gamma_{[k : l ] } = \\gamma_{[k : l ] } - z , x_{\\gamma_{[k : l ] } } = y_{[k : l ] } | \\notag \\\\ & \\hspace*{40mm}\\gamma_{k - 1 } = k - 1 - z ) \\notag \\\\ & \\stackrel{(b)}{= } \\sum_{\\gamma_{[k : l ] } } { \\mathsf}{p}(\\gamma_{[k : l ] } = \\gamma_{[k : l ] } - z , x_{\\gamma_{[k : l ] } - z } = y_{[k : l ] } | \\notag \\\\ & \\hspace*{40mm}\\gamma_{k - 1 } = k - 1 - z ) \\notag \\\\ & = { \\mathsf}{p}(y_{[k : l ] } = y_{[k : l ] } | \\gamma_{k - 1 } = k - 1 - z ) \\notag \\\\ & = { \\mathsf}{p}(y_{[k : l ] } = y_{[k : l ] } | z_{k - 1 } = z){\\ } \\forall{\\ } z \\leq k - 1 .",
    "\\notag\\end{aligned}\\ ] ] here , @xmath132 follows from the shift - invariance of @xmath95 ( see lemma [ lem_ztprop ] ) and @xmath134 from the stationarity of @xmath26 . therefore , we have @xmath490 } = y_{[k ] } ) = \\sum_{z \\in { \\mathbb}{z}}{\\mathsf}{p}(z_0 = z){\\mathsf}{p}(y_{[k ] } = y_{[k ] } | z_0 = z ) \\notag \\\\ & = { \\mathsf}{p}(y_{[k ] } = y_{[k ] } | z_0 = 0 ) \\notag \\\\ & = { \\mathsf}{p}(y_{[k ] } = y_{[k ] } | \\gamma_0 = 0 ) \\notag \\\\ & = \\sum_{\\gamma_{[k ] } } { \\mathsf}{p}(\\gamma_{[k ] } = \\gamma_{[k ] } , x_{\\gamma_{[k ] } } = y_{[k ] } | \\gamma_0 = 0 ) \\notag \\\\ & \\stackrel{(c)}{= } \\sum_{\\gamma_{[k ] } } { \\mathsf}{p}(\\gamma_{[j + 1 : j + k ] } = \\gamma_{[k ] } , x_{\\gamma_{[k ] } } = y_{[k ] } | \\gamma_j = 0 ) \\notag \\\\ & = { \\mathsf}{p}(y_{[j + 1 : j + k ] } = y_{[k ] } | z_j = 0 ) \\notag \\\\ & = { \\mathsf}{p}(y_{[j + 1 : j + k ] } = y_{[k]}){\\ } \\forall{\\ } j , k \\in { \\mathbb}{n } \\notag\\end{aligned}\\ ] ] where @xmath491 follows form the time - homogeneity of @xmath95 ( lemma [ lem_ztprop ] ) .",
    "the last equality above follows from the observation made in the beginning of the proof .",
    "we use the result from lemma [ lem_subset ] .",
    "let us define @xmath662}\\ } \\cap [ n ] \\subset \\{\\gamma^{(m)}_{[n^{(m)}_n]}\\ } \\cap [ n]\\ } , \\text { and } \\notag \\\\ \\hat{\\mathbb{s}}_m & = \\bigcap_{n \\geq 1 } \\{\\varsigma \\in \\mathbb{s } : \\{\\gamma^{(m + 1)}_{[n^{(m + 1)}_n]}\\ } \\cap [ n ] \\subset \\{\\gamma^{(m)}_{[n^{(m)}_n]}\\ } \\cap [ n]\\ } \\notag\\end{aligned}\\ ] ] and",
    "let @xmath663 clearly , @xmath664 . then , confining the expectations over the set @xmath665 , @xmath131 } ; & y^{(m)}_{[n^{(m)}_n ] } ) - i(x_{[n ] } ;",
    "y_{[n_n ] } ) \\notag \\\\ & = i_{\\mathbb{s}^*}(x_{[n ] } ; x_{\\gamma^{(m)}_{[n^{(m)}_n ] } \\setminus \\gamma_{[n_n ] } } | x_{\\gamma_{[n_n ] } } ) \\geq 0 , \\notag\\end{aligned}\\ ] ] where @xmath666 denotes the mutual information obtained after confining the expectations to the set @xmath665 .",
    "similarly , we have @xmath667 } ; y^{(m + 1)}_{[n^{(m + 1)}_n ] } ) \\leq i(x_{[n ] } ; y^{(m)}_{[n^{(m)}_n]})$ ] .",
    "we start with a small lemma .",
    "[ lem_wc ] let @xmath712 be a measurable space , and let @xmath713 , @xmath714 all be probability measures on this space .",
    "suppose that    a.   for every @xmath715 , there is a set @xmath716 such that @xmath717 for every @xmath718 , @xmath719 .",
    "b.   @xmath720 as @xmath98 .",
    "then the measures @xmath721 converge in total variation to @xmath714 , i.e. , @xmath722 as @xmath98 .    from ii ) , for every @xmath723 , there exists @xmath724 such that @xmath725 from i ) , @xmath726 for every @xmath715 , @xmath719 .",
    "therefore , for every @xmath723 , @xmath727 hence @xmath722 as @xmath98 .",
    "note that @xmath728 is the subset of @xmath729 in @xmath341 where @xmath384 } , y_{[n_n^{(m)}]}^{(m)})$ ] differs from @xmath317 } , y_{[n_n]})$ ] . from lemma [ lem_prub ]",
    ", we have @xmath730 as @xmath98 , whenever @xmath396 .",
    "consider henceforth that @xmath403 satisfies this condition .",
    "in lemma [ lem_wc ] above , set @xmath731 , @xmath732 , @xmath733 and @xmath734 .",
    "note that although @xmath735 is only defined on @xmath736 ( cf .",
    "proposition [ prop_ininm ] ) , we can extend it to @xmath341 such that it agrees with the measure @xmath291 on every subset of @xmath737 for each @xmath715 .",
    "then for each @xmath8 , we see that by setting @xmath738 , both conditions i ) and ii ) in lemma [ lem_wc ] are satisfied . from this and (",
    "* corollary 1@xmath387 ) , we have the desired result .",
    "it is easy to see that when @xmath224 , equation reduces to @xmath500 } } h_2\\big(\\frac{w(x_{[m + 1]})}{m + 1}\\big ) \\notag \\\\ & = \\log_2(m + 1 ) - \\frac{1}{2^{m + 1}}\\sum_{j = 0}^{m + 1}{m + 1\\choose j}h_2\\big(\\frac{j}{m + 1}\\big ) , \\label{eq_h2m}\\end{aligned}\\ ] ] where @xmath501 denotes _ hamming weight_. hence @xmath502    for numerically estimating @xmath503 for large @xmath223 , we can use the upper bound @xcite @xmath504 to get a further lower bound since this will be a lower bound for @xmath232 as well ( cf . equation ) . ] on @xmath505 . on the other hand , to obtain a looser analytic lower bound , we can bound @xmath506 to get @xmath507 .",
    "this gives us @xmath508 unfortunately , it is not easy to evaluate the series @xmath509 on the right hand side of the above inequality .",
    "consider the function @xmath510 where @xmath511 is the natural logarithm .",
    "the @xmath512 term in the series can then be written as @xmath513 .",
    "it turns out that for @xmath514 we can lower bound the series @xmath515 by the integral @xmath516 where @xmath517 is the _ exponential integral _ function defined as @xmath518 which can be numerically evaluated to arbitrary accuracy through a taylor series expansion .",
    "therefore , for @xmath519 , @xmath520    with @xmath503 as given in equation , we can write @xmath521 for small @xmath235 .",
    "this is loose compared to the bound obtained in @xcite .",
    "this can be attributed to the fact that we evaluated @xmath522 rather than @xmath523 to obtain @xmath499 .",
    "in fact , this small-@xmath235 series expansion of @xmath499 is no better than that of the lower bound for the bdc in corollary [ cor_capbndsspl ] .",
    "we will improve this bound for small @xmath235 in the next subsection .",
    "we now pursue the other case where is easy to evaluate . instead of evaluating @xmath524 exactly ,",
    "we can further lower bound it as follows .",
    "@xmath525 we are essentially writing a series expansion for @xmath498 and lower bounding it by the @xmath526 partial sum .",
    "note that we can write @xmath527 where @xmath528 was defined in theorem [ thm_sirbdc ] .",
    "clearly , the sequence @xmath529 is non - decreasing , and , in turn , so is the sequence @xmath530 . since @xmath531 , we have @xmath532 .",
    "further , by definition , @xmath533 thus for every @xmath189 , we can write @xmath534 where @xmath132 is true since @xmath535{\\ } \\forall{\\ } i \\geq 1 , j \\geq 0 $ ] .    from the channel model , @xmath536 } = x_{[i ]",
    "} | z_i = -1 ) = 2^{-i}$ ] since @xmath133 and @xmath26 is i.u.d . , and @xmath537 } = y_{[i - 1 ] } | x_{[i ] } = x_{[i ] } , z_i = -1 ) = \\frac{w_{y_{[i - 1]}}(x_{[i]})}{i}.\\ ] ] for @xmath538 } = x_{[i - 1 ] - z_{[i - 1]}}$ ] for some realization @xmath210}$ ] with the boundary conditions @xmath539 and @xmath540 , @xmath541 } = x_{[i ] } , y_{[i - 1 ] } = y_{[i - 1 ] } ) \\notag \\\\ & = h_2\\big(\\frac{1}{r_1(x_{[i]})}\\big)\\mathds{1}_{{\\mathcal}{r}_1(x_{[i ] } , y_{[i - 1 ] } ) } \\notag\\end{aligned}\\ ] ] where @xmath542 }",
    ", y_{[i - 1]})$ ] is the event that the single deletion occurred in the first run of @xmath543}$ ] to result in @xmath538}$ ] . to see this ,",
    "let @xmath538}$ ] represent a received word resulting from a single deletion upon transmission of @xmath543}$ ] .",
    "consider the two mutually exclusive and exhaustive cases in this scenario :    * the single deletion occurs in a run other than the first run of @xmath543}$ ] . in this case",
    ", there is no ambiguity that @xmath259 , and the first run of @xmath538}$ ] is either the same or larger than}$ ] disappears . ] that of @xmath543}$ ] . *",
    "the single deletion occurs in the first run of @xmath543}$ ] . * * if @xmath544 } ) = 1 $ ] , there is no ambiguity that @xmath545 . * * if @xmath544 } ) > 1 $ ] , the deleted symbol could be , with equal likelihood , one of the symbols comprising the first run of @xmath543}$ ]",
    ". the uncertainty in @xmath242 is @xmath546})}\\big)$ ] . + in both the above cases ,",
    "the uncertainty can be written as @xmath546})}\\big)$ ] .    therefore ,",
    "@xmath547}}\\frac{1}{2^i}\\sum_{y_{[i - 1]}}\\frac{w_{y_{[i - 1]}}(x_{[i]})}{i}h_2\\big(\\frac{1}{r_1(x_{[i]})}\\big){\\mathds}{1}_{{\\mathcal}{r}_1(x_{[i ] } , y_{[i - 1 ] } ) } \\notag \\\\ & = i\\sum_{x_{[i]}}\\frac{1}{2^i}\\frac{r_1(x_{[i]})}{i}h_2\\big(\\frac{1}{r_1(x_{[i]})}\\big ) \\notag \\\\ & = \\frac{1}{2^i}\\sum_{j = 1}^i j2^{i - j}h_2\\big(\\frac{1}{j}\\big ) + \\frac{1}{2^i}h_2\\big(\\frac{1}{i}\\big ) \\notag \\\\ & = \\sum_{j = 1}^i \\frac{j}{2^j}h_2\\big(\\frac{1}{j}\\big ) + \\frac{1}{2^i}h_2\\big(\\frac{1}{i}\\big ) \\notag \\\\ & = \\frac{1}{2}\\sum_{j = 1}^{i - 2 } \\frac{j}{2^j}\\log_2 j + 2\\frac{i}{2^i}\\log_2i .",
    "\\label{eq_aiterm } \\ ] ] we observe that @xmath548 is non - decreasing in @xmath112 , and converges exponentially to the value @xmath549 . from and , we have @xmath550 since @xmath551 , we have @xmath552 thus , from equation @xmath553 where @xmath554 .",
    "we note here that this is exactly the same bound obtained in @xcite with a completely different technique . since this bound was shown to be tight for small @xmath235 , we have that the capacity of the bdc itself is given by the above expression for small @xmath235 .",
    "the advantage in the evaluation of the above bound was that , when we restrict to the case of a single deletion , the ambiguity in the first channel state @xmath242 arises only when @xmath544 } ) > 1 $ ] , in which case the uncertainty is exactly @xmath546})}\\big)$ ] .",
    "this , however , is not true when there are @xmath555 or more deletions , wherein we will have to count subsequence weights of sequences",
    ".      we can get similar bounds as in the previous two subsections for first - order markov inputs .",
    "further , since the channel has no bias for the input symbols , we can confine our attention to _ symmetric _ markov inputs .",
    "but these calculations will have to keep track of _ ascents _ and _ descents _ in sequences , and are therefore more tedious .",
    "proceeding along the same lines as in appendix [ ssec_i2 ] , we can write for @xmath556 $ ] , @xmath557 \\notag \\\\ \\qquad\\qquad\\qquad\\times ( 1 - p ) - h_2(p ) \\text { , where } \\notag \\\\ \\ell_m(\\alpha ) = \\log_2(m + 1 ) - \\sum_{j = 0}^{m + 1}h_2\\big(\\frac{j}{m + 1}\\big)\\eta(\\alpha , j , m + 1 ) , \\notag\\end{aligned}\\ ] ] and @xmath558 is defined recursively as @xmath559 with @xmath560 , @xmath561 and @xmath562 $ ] for @xmath563 .",
    "similarly , we can also evaluate @xmath564 . \\notag\\end{aligned}\\ ] ] however , both @xmath565 and @xmath566 turn out to be better than their sir counterparts by less than @xmath567 .",
    "although first - order markov inputs are expected to perform better than i.u.d .",
    "inputs , we see that the bounds we obtained are almost the same in the two cases .",
    "this is because we are considering two special cases , the first when @xmath224 wherein all but a single symbol were deleted , and the second when @xmath225 wherein a single symbol was deleted ; and in these cases , a first - order markov input is not significantly different than i.u.d .",
    "we have from proposition [ prop_irdrc ] and lemma [ lem_monri ] that @xmath568 \\notag\\end{aligned}\\ ] ] where we have used the expression for @xmath569 from the proof of theorem [ thm_m1rbrc ] .",
    "observe that @xmath570 , and among these possibilities , the only event wherein there is an ambiguity in the value of @xmath242 is when @xmath571 .",
    "thus , we can see easily that @xmath572 . hence @xmath573 .",
    "\\notag\\end{aligned}\\ ] ] it can be shown that the optimal @xmath574 in the above is given by @xmath575 note that @xmath576 is always larger than @xmath577 , and @xmath578 for @xmath579 where @xmath580 plugging this back in the expression for @xmath581 ends the proof .      from proposition [ prop_irdrc ] and lemma [ lem_monri ] , for i.u.d .",
    "inputs , @xmath582 where we have used the expression for @xmath569 from the proof of theorem [ thm_m1rbrc ] for @xmath270 .",
    "the last equality is true since @xmath583 .    as shown in the proof of theorem [ thm_m1rbrc ] , we can write @xmath584 . \\notag\\end{aligned}\\ ] ] further , there is no ambiguity in @xmath242 if the single replication does not occur in the first run of @xmath251 .",
    "therefore , for a first - order markov input process , @xmath585 \\notag \\\\ & = \\sum_{l = 1}^{i - 1}(1 - \\alpha)^l\\alpha\\frac{l + 1}{i}h_2\\big(\\frac{1}{l + 1}\\big ) + ( 1 - \\alpha)^ih_2\\big(\\frac{1}{i}\\big ) .",
    "\\notag\\end{aligned}\\ ] ] for @xmath270 , we get @xmath586 from equation .",
    "thus , @xmath587 where @xmath588 . as was the case for the bdc",
    ", we expect this to be a tight bound for the capacity for small @xmath235 .",
    "we will first assume that @xmath668 and that @xmath669 with @xmath670 and @xmath671 , i.e. , the space @xmath672 is a product space . since in our model @xmath673 , there is no loss of generality in this assumption .    by defining the stationary transition probabilities",
    "@xmath674 as in section [ ssec_appsc ] , the measures @xmath331 are well - defined over @xmath675 .",
    "let @xmath676 for @xmath677 , and similarly @xmath678 . then , clearly @xmath679 and @xmath680 then , we define @xmath681 this will imply that for every @xmath677 , @xmath682 by definition , we also have for @xmath683 that @xmath684 so that the associated probability is zero under any measure @xmath685 .",
    "we can now consider the space @xmath686 to be obtained from @xmath687 along with the definition and subsequent _ completion _",
    "@xcite .    by now defining @xmath688 independent of @xmath223",
    ", we can extend the measure @xmath331 to @xmath689 for each @xmath296 as required .",
    "a. r. iyengar would like to thank aman bhatia , suhas diggavi , patrick fitzsimmons , henry pfister , bharath sriperumbudur & ruth williams for stimulating discussions .",
    "the authors thank eleni drinea and michael mitzenmacher , respectively , for providing the numerical lower bounds for the bdc from @xcite and the brc from @xcite ( that appear as black circles in figs . [ fig_bdc ] and [ fig_brc ] , respectively ) .",
    "a.  r. iyengar , p.  h. siegel , and j.  k. wolf , `` modeling and information rate for synchronization error channels , '' in _ proc .",
    "inf . theory _ , st .",
    "petersburg , russia , 31 jul.-5 aug .",
    "2011 , pp . 518522 .",
    "a.  kalai , m.  mitzenmacher , and m.  sudan , `` tight asymptotic bounds for the deletion channel with small deletion probability , '' in _ proc .",
    "inf . theory _",
    ", austin , tx , usa , jun .",
    "13 - 18 , 2010 , pp . 9971001 .",
    "m.  dalai , `` a new bound on the capacity of the binary deletion channel with high deletion probabilities , '' in _ proc .",
    "inf . theory _ , st .",
    "petersburg , russia , 31 jul.-5 aug .",
    "2011 , pp . 410413 .",
    "d.  blackwell , l.  breiman , and a.  j. thomasian , `` proof of shannon s transmission theorem for finite - state indecomposable channels , '' _ the annals of mathematical statistics _",
    "29 , no .  4 , pp .",
    "pp . 12091220 , 1958 .",
    "[ online ] .",
    "available : http://www.jstor.org/stable/2236957      d.  arnold , h .- a .",
    "loeliger , p.  vontobel , a.  kavcic , and w.  zeng , `` simulation - based computation of information rates for channels with memory , '' _ ieee trans .",
    "inf . theory _ , vol .",
    "52 , no .  8 , pp .",
    "3498 3508 , aug .",
    "h.  pfister , j.  soriaga , and p.  siegel , `` on the achievable information rates of finite state isi channels , '' in _ proc .",
    "ieee globecom 2001 _ , vol .  5 , san antonio , tx , usa , 25 - 29 nov .",
    "2001 , pp .",
    "29922996 vol.5 .",
    "p.  vontobel , a.  kavcic , d.  arnold , and h .- a .",
    "loeliger , `` a generalization of the blahut - arimoto algorithm to finite - state channels , '' _ ieee trans .",
    "inf . theory _ ,",
    "54 , no .  5 , pp .",
    "1887 1918 , may 2008 .",
    "j.  chen and p.  siegel , `` markov processes asymptotically achieve the capacity of finite - state intersymbol interference channels , '' _ ieee trans . inf .",
    "54 , no .  3 , pp .",
    "1295 1303 , march 2008 .",
    "a.  r. iyengar , p.  h. siegel , and j.  k. wolf , `` ldpc codes for the cascaded bsc - bawgn channel , '' in _ proc .",
    "47th annual allerton conf . on communication , control and computing _ , sep .",
    "30 - oct . 2 , 2009 , pp .",
    "620627 .",
    "f.  wang , d.  aktas , and t.  m. duman , `` on capacity and coding for segmented deletion channels , '' in _ proc .",
    "49th annual allerton conf . on communication , control and computing _ , 28 - 30 sep . , 2011 , pp .",
    "14081413 .",
    "j.  hu , t.  duman , m.  erden , and a.  kavcic , `` achievable information rates for channels with insertions , deletions , and intersymbol interference with i.i.d .",
    "inputs , '' _ ieee trans .",
    "_ , vol .",
    "58 , no .  4 , pp .",
    "1102 1111 , apr . 2010 .",
    "d.  arnold , a.  kavcic , r.  kotter , h .- a .",
    "loeliger , and p.  vontobel , `` the binary jitter channel : a new model for magnetic recording , '' in _ proc .",
    "inf . theory _ ,",
    "sorrento , italy , 25 - 30 jun .",
    "2000 , p. 433 ."
  ],
  "abstract_text": [
    "<S> we consider a new formulation of a class of synchronization error channels and derive analytical bounds and numerical estimates for the capacity of these channels . for the binary channel with only deletions , </S>",
    "<S> we obtain an expression for the symmetric information rate in terms of subsequence weights which reduces to a tight lower bound for small deletion probabilities . </S>",
    "<S> we are also able to exactly characterize the markov-@xmath0 rate for the binary channel with only replications . for a channel that introduces deletions as well as replications of input symbols , </S>",
    "<S> we design two sequences of approximating channels with favourable properties . in particular , we parameterize the state space associated with these approximating channels and show that the information rates approach that of the deletion - replication channel as the state space grows . for the case of the channel where deletions and replications occur with the same probabilities , a stronger result in the convergence of mutual information rates is shown . </S>",
    "<S> the numerous advantages this new formulation presents are explored .    </S>",
    "<S> synchronization errors , deletions , insertions , replications , channel capacity . </S>"
  ]
}