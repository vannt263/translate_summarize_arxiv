{
  "article_text": [
    "carraro et al ( 1998 ) developed a pure particle code , combining barnes & hut ( 1986 ) octo - tree with sph , and applying this code to the formation of a spiral galaxy like the milky way .",
    "the code is similar to herniquist & katz ( 1989 ) treesph .",
    "it uses sph to solve the hydro - dynamical equations . in sph",
    "a fluid is sampled using particles , there is no resolution limitation due to the absence of grids , and great flexibility thanks to the use of a time and space dependent smoothing length .",
    "shocks are captured by adopting an artificial viscosity tensor , and the neighbors search is performed using the octo - tree .",
    "the octo - tree , combined with sph , allows a time scaling of @xmath3 .",
    "a good advantage of such codes is that it is easy to introduce new physics , like cooling and radiative processes , magnetic fields and so forth .",
    "finally the kernel , which is utilised to performe hydro - dynamical quantities estimates , can be made adaptive by using anisotropic smoothing lengths .",
    "it is widely recognized that treesph codes , although deficient in some aspects , can give reasonable answers in many astrophysical situations , like in simulations of fragmentation and star formation in giant molecular clouds ( gmc ) ( ) , supenovexplosions globular clusters formation , merging of galaxies , galaxies and clusters formation and lyman alpha forest .",
    "galaxy formation in particular requires a huge dynamical range ( dav et al 1997 ) .",
    "in fact an ideal galaxy formation simulation would start from a volume as large as the universe to follow the initial growth of the cosmic structures , and at the same time would be able to resolve regions as small as gmc , where stars form and drive the galaxy evolution through their interaction with ism .",
    "this ideal simulation would encompass a dynamic range of @xmath4 ( from gpc to parsec ) , @xmath5 time smaller than that achievable with present day codes .",
    "big efforts have been made in the last years to enlarge as much as possible the dynamical range of numerical simulations , mainly using more and more powerful supercomputers .",
    "scalar and vector computers indeed can not handle efficiently a number of particles greater than half a million .",
    "dav et al ( 1997 ) for the first time developed a parallel implementation of a treesph code ( ptreesph ) which can follow both collision - less and collisional matter .",
    "they report results of simulations run on a cray t3d computer of the adiabatic collapse of an initially isothermal gas sphere ( using 4096 particles ) , of the collapse of a zeldovich pancake ( 32768 particles ) and of a cosmological simulation ( 32768 gas and 32768 dark particles ) .",
    "their result are quit encouraging , being quite similar to those obtained with the scalar treesph code ( hernquist & katz 1989 ) .",
    "porting a scalar code to a parallel machine is far from being an easy task .",
    "a massively parallel computer ( like the silicon graphics t3e ) links together hundreds or thousands of processors aiming at increasing significantly the computational power . for this reason they are very attractive ,",
    "although several difficulties can arise in adapting a code to these machines .",
    "any processor possesses its own memory , and can assess other processors memory by means of communications which are handled by an hard - ware network , and are slower than the computational speed .",
    "great attention must be paid to avoid situations in which a small number of processors are actually working while most of them are standing idle .",
    "usually one has to invent a proper data distribution scheme which allows to subdivide particles into processors in a way that any processor handles about the same number of particles and does not need to make heavy communications",
    ". moreover the computational load must be shared between processors , ensuring that processors exchange informations all together , in a synchronous way , or that any processors is performing different kinds of work when it is waiting for informations coming from other processors , in an asynchronous fashion ( dav et al 1997 ) . in this paper",
    "we present a parallel implementation of the treesph code dscribed in carraro et al ( 1998 ) .",
    "the numerical ingredients are the same as in the scalar version of the code .",
    "however the design of the parallel implementations required several changes to the original code .",
    "the key idea that guided us in building the parallel code was to avoid continuous communications , limiting the informations exchange at a precise moment along the code flow .",
    "this clearly reduces the communication overheard .",
    "we have also decided to tailor the code to the machine , improving its efficiency . since we are using a t3e massively parallel computer , a natural choise was to handle communications using the shmen libraries , which permits asynchronous communications , and",
    "are intrinsically very fast , being produced directly by cray for the t3e super - computer . at present",
    "the code is also portable to other machine , like sgi origin 2000 , and will be portable to any other machine with the advent of the second release of message passing interface ( mpi ) .",
    "we consider the adiabatic collapse of an initially non - rotating isothermal gas sphere .",
    "this is a standard test for sph codes ( hernquist & katz 1989 . in particular it is an ideal test for a parallel code , due to the large dynamical range and high density contrast . to facilitate the comparison of our results with those by the above authors",
    ", we adopt the same initial model and the same units ( @xmath6 ) .",
    "the system consists of a @xmath7 gas sphere , with an initially isothermal density profile : @xmath8 where m(r ) is the total mass inside the sphere of radius r. the density profile is obtained stretching an initially regular cubic grid .",
    "the total number of particles used in this simulation is @xmath9 .",
    "all the particles have the same mass .",
    "the specific internal energy is set to @xmath10 . for this test the viscosity parameters @xmath11 and",
    "@xmath12 adopted are 0.5 , in agreement with dav et a ( 1997 ) .",
    "the gravitational softening parameter @xmath13 adopted for this simulation is @xmath14 .",
    "the state of the system at the time of the maximum compression is shown in the various panels of fig .",
    "1 , which displays the density , radial velocity , pressure and specific internal energy profiles . each panel shows the variation of the physical quantity under consideration ( in suitable units ) as a function of the normalized radial coordinate at time equal to 0.88 .",
    "the initial low internal energy is not sufficient to support the gas cloud which starts to collapse .",
    "approximately after one dynamical time scale a bounce occurs .",
    "the system afterwards can be described as an isothermal core plus an adiabaticlly expanding envelope pushed by the shock wave generated at the stage of maximum compression .",
    "after about three dynamical times the system reaches virial equilibrium with total energy equal to a half of the gravitational potential energy ( hernquist & katz 1989 ) .",
    "the present results agree fairly well with the mean values of the hernquist & katz ( 1989 ) simulations , which in turns agrees with the 1-d finite difference results .",
    "we run the adiabatic collapse test up to the time of the maximum compression ( t @xmath15 ) using @xmath16 particles on 1 , 2 , 4 , 8 , 16 , 32 and 64 processors , and looked at the performances in the following code sections ( see also table  1 ) :    @xmath17 : :    total wall - clock time ; @xmath17 : :    data up - dating data ; @xmath17 : :    parallel computation , which consists of barriers , the construction of    the _ ghost - tree _ and the distribution of data between processors ; @xmath17 : :    search for neighbour particles ; @xmath17 : :    evaluation of the hydro - dynamical quantities ; @xmath17 : :    evaluation of the gravitational forces ; @xmath17 : :    miscellaneous , which encompasses i / o and kernel computation .",
    "the results summarized in table  1 present the total wall - clock time per timestep per processor , averaged over 50 time - steps , together with the time spent in each of the 5 subroutines ( data updating , neighbour searching , sph computation , gravitational interaction and parallel computation ) .",
    "the gravitation interaction takes about one - third of the total time , while the search for neighbours takes roughly comparable time .",
    "the evaluation of hydrodynamical quantities takes about one - fourth of the time , the remaining time being divided between i / o and data up - dating .",
    "the parallel over - head does not appear to be a problem , being always less than @xmath18 of the total time .",
    "this timing refers , as indicated above , to simulations stopped at roughly the time of maximum compression .",
    "a run with 8 processors up to @xmath19 , the time at which the system is almost completely virialized , took 3800 secs .",
    "one of the most stringest requirement for a parallel code is the capability to distribute the computational work equally between all processors .",
    "this can be done defining a suitable work - load criterium , as discussed in section  3.2 .",
    "this is far from being an easy task ( dav et al 1997 ) , and in practice some processors stand idly for some time waiting that the processors with the greatest computational load accomplish their work .",
    "this is true also when an asynchronous communications scheme is adopted , as in our treesph code . to evaluate the code load - balance we adopted the same strategy of dav et al ( 1997 ) , measuring the fractional amount of time spent idle in a time - step while another processor performs computation : @xmath20 here",
    "@xmath21 is the time spent by the slowest processor , while @xmath22 is the time taken by the @xmath23 processors to perform computation .",
    "the results are shown in fig  2 , where we plot the load - balance for simulations at increasing number of processors , from 1 to 64 .",
    "the load balance maintains always above @xmath24 , being close to 1 up to 8 processors . for the kind of simulation we are performing",
    ", the use of 8 processors is particulary advantageous for symmetry reasons . at increasing number of processors",
    ", a parallel code should ideally speeds up linearly in practice the increase of the processors number casuses an increase of the communications between processors , and a degradation of the code performances . to test this",
    ", we used the same simulations discussed above , ruuning the adiabatic collapse test with @xmath16 particles at increasing processors number .",
    "we estimated how the code speed scales computing the wall - clock time per processor spent to execute a single time - step , averaged over 50 time - steps . in fig .",
    "3 we plot the speed ( in @xmath25 against the number of processors . + the code scalabilty keeps very close to the ideal scalability up to 8 processors , where it shows a minimum .",
    "this case in fact is the most symmetricone .",
    "then the scalability deviates significantly only when using more that 16 processors .",
    "looking also at fig .",
    "4 , it is easy to recognize that mainly the gravitational interaction is responsible for this deviation .",
    "+ in the near future we are going to investigate whether the code overall scalability might improve adding new physics ( like cooling and star formation ) which is necessary to describe the evolution of real systems , like galaxies .",
    "barnes j.e . , hut p. , 1986",
    ", nature 324 , 446 carraro g. , lia c. , chiosi c. , 1998 , mnras 297 , 1029 dav r. , dubinski j. , hernquist l. , 1997 , new astronomy 2(3 ) , 277 hernquist l. , katz n. , 1989 , apjs 70 , 419"
  ],
  "abstract_text": [
    "<S> we describe a new implementation of a parallel tree - sph code with the aim to simulate galaxy formation and evolution . </S>",
    "<S> the code has been parallelized using shmem , a cray proprietary library to handle communications between the 256 processors of the silicon graphics t3e massively parallel supercomputer hosted by the cineca super - computing center ( bologna , italy ) . </S>",
    "<S> + the code combines the smoothed particle hydrodynamics ( sph ) method to solve hydro - dynamical equations with the popular barnes and hut ( 1986 ) tree - code to perform gravity calculation with a @xmath0 scaling , and it is based on the scalar tree - sph code developed by carraro et al ( 1998)[mnras 297 , 1021 ] . </S>",
    "<S> + parallelization is achieved distributing particles along processors according to a work - load criterium . </S>",
    "<S> + benchmarks , in terms of load - balance and scalability , of the code are analised and critically discussed against the adiabatic collapse of an isothermal gas sphere test using @xmath1 particles on 8 processors . </S>",
    "<S> the code results balanced at more than @xmath2 level . increasing the number of processors </S>",
    "<S> , the load balance sligthly worsens . </S>",
    "<S> the deviation from perfect scalability at increasing number of processors is negligible up to 64 processors . </S>",
    "<S> additionally we have incorporated radiative cooling , star formation , feed - back and an algorithm to follow the chemical enrichment of the interstellar medium . </S>"
  ]
}