{
  "article_text": [
    "parallel algorithms are an essential ingredient when making proper use of high - performance computing ( hpc ) infrastructures .",
    "the parallelism of the underlying hardware has increased particularly fast during the last decade , and more importantly , has entered the realm of small to medium - sized computing infrastructures , extending even to the hardware that serves our every - day use . the habitual increase in cpu frequency",
    "has factually halted and moore s law is only kept reasonably valid via an increase of parallel on - chip execution units . therefore , concerning scientific applications we can not rely anymore on an `` automatic speed - up '' by means of an increase in cpu frequency . instead , it is rather the transfer of substantial , and sometimes even revolutionary , advances in hpc into the branches of fundamental science which in the near future can leverage and accelerate theoretical and experimental investigations which would otherwise remain unreachable .",
    "harvesting the potential of parallel computing systems has never been a straight - forward task , with hpc being a scientific branch and community of its own for that very reason .",
    "we here report on an example of such a transfer and demonstrate that current hpc architectures can prove extremely helpful in accelerating a specific type of numerical application in quantum condensed matter physics , namely functional renormalisation group calculations .",
    "we stress from the very beginning that the concepts and the route taken to arrive at an enormous speed - up were not particularly complicated , and the actual code modifications turn out to be comparatively moderate and simple .",
    "an all - important lesson we learned from this example is that we can avoid a complicated and intense code porting phase , something which scientific code developers hardly ever wish to engage in .      in recent times , functional renormalisation group methods",
    "have been established in quantum condensed matter physics as a powerful tool to detect and treat various phenomena which arise in correlated electron systems . in particular , they can be used as an unbiased detector for competing correlations and quite a range of results is available for various models , parameter sets and specifically targeted observables @xcite @xcite @xcite @xcite @xcite @xcite @xcite @xcite @xcite @xcite @xcite . a pioneering work in this context",
    "is given in @xcite , for a comprehensive review see @xcite .",
    "the generic equation which governs the method is a functional differential equation which arises when an explicit scale - dependence is introduced in functionals of interest and a derivative with respect to this scale is taken subsequently . upon expanding the functional of interest in terms of correlation functions ,",
    "one obtains an infinite hierarchy of ordinary differential equation for these functions ( frg - ode ) .",
    "the right - hand side ( rhs ) of this equation is in general given by multi - dimensional integrals and/or summations over weighted products of these functions , which stem from standard diagrammatic expansions in quantum field theory , see e.g. @xcite .",
    "+ in practice , the formal representation of these functions and equations is discretised and subsequently treated numerically .",
    "the resulting algorithms and codes can become very demanding in their computational intensity and thereby inhibitory to the overall scientific potential of the method , which in principle extends far beyond current applications . in this work",
    "we shall tackle this very obstacle by means of hpc methods , namely we wish to illustrate how a parallelisation at different levels can speed up a typical code of the frg family such that ultimately a speed - up of several orders of magnitude is accomplished .",
    "+ we feel that this step forward can be of considerable importance for one reason in particular : in very recent times an increasing amount of attention in computational science is paid to `` virtual scanning techniques '' in the area of materials design , the general idea of which is to pre - select potential candidates for new materials by first calculating relevant properties via numerical methods .",
    "we believe that the frg has the potential to become a very helpful computational kernel within such strategies .",
    "however , proper scanning of `` theoretical materials '' relies on the capability to generate results for a large variety of parameter sets in a short amount of time and by this virtue such quantitative improvements are a necessary ingredient for qualitative progress . + that said , the resulting motivation for this study is to provide a proof - of - concept that frg calculations can massively profit from small and large - scale multi - parallel hpc environments .",
    "we thereby intend to offer a blueprint to enable frg developers to best benefit from such architectures .",
    "+      the key aspects when parallelising the numerical treatment of the frg - ode are imposed by a hierarchy of available means for parallelisation which is common to many contemporary hpc systems . in a bottom - up sequence",
    "this hierarchy can essentially be summarised as follows :    * on - core single - instruction - multiple - data ( simd ) vector units , with a tendency of increasing width * possibility / necessity to execute two or more processes / threads per core * multiple cores per cpu , in part massively increasing in number ( e.g. intel xeon phi ) * multiple cpus per compute node with access to the same physical memory * an ever increasing number of compute nodes connected by faster and faster networks    as an in - house example for a standard small - sized cluster we will refer to a general purpose intel - based development cluster at jsc . as a representative system at the petascale level we will refer to juqueen , the ibm bg / q supercomputer at jsc which offers @xmath0 nodes and @xmath1 cores in total @xcite .",
    "each core can be overloaded by four processes such that ultimately @xmath2 concurrent threads can be executed .",
    "the set - up is summarised in table [ hardware_set - up ] .",
    "+    .hpc systems under consideration in this work [ cols=\"<,<,>,>,^,^\",options=\"header \" , ]",
    "as already mentioned , the general set - up of the frg - ode constitutes a scenario which seems ideal for very efficient and at the same time simple parallelisation techniques over a wide range of relevant parameters .",
    "we started with an initial code that had originally been optimised for serial performance and which scaled perfectly only for a small number of mpi ranks .",
    "the outcome is a code which now benefits from two additional levels of parallelisation at the shared - memory and simd / core overload level and scales to very large numbers of nodes at the distributed memory level .",
    "the evolution involved various means of redesign and restructuring , including changes in the low - lying algorithms and continuous evaluations of the appropriate locations where parallelisation - specific pragmas and functions had to be placed .",
    "some examples of such an approach along with generic issues which are also valid in our context are e.g. given in @xcite , a discussion along with general conceptual details on efficient hybrid implementations of master - worker parallelism is given in @xcite .",
    "+ the final code which scales over several orders of magnitude is then again rather simple , which was a core statement already in the introduction .",
    "we thus hope that the obstacles we had to overcome on the way to this simple code structure may motivate others to engage in a similar strategy and to help making the transition to a scalable codes an easier task than it may seem . + an obligatory step when encountering a break - down in scaling is to analyse the application with respect to its usage of the underlying hardware infrastructure .",
    "this is possible thanks to a number of hpc - specific tools which , when used in combination , allow for correctness checking and performance analysis at nearly all technical and algorithmic levels . in this section we will outline the two most relevant issues we encountered together with a brief description of how these issues were analysed and which tools were of great use in doing so .",
    "the initial code neither involved shared - memory multi - threading , nor did it offer simple means for a compiler to make efficient use of on - core vector units and other parallelism . the first step was to start from the very bottom of the algorithm , which essentially means at the very lowest level of basic evaluations of computational kernels within a multi - dimensional numerical quadrature . for that purpose we needed to ensure that loops run over a minimum of iterations for vectorisation to become reasonably effective , and",
    "we wanted to end up with code that makes it sufficiently easy for compilers to automatically vectorise these loops without forcing us to resort to intrinsics . while it can be argued that this approach is possibly less efficient than optimisation `` by hand '' we wanted to keep the code generically portable .",
    "an inevitable subtask when trying to find and evaluate means for efficient vectorisation are harware - based tools that shed sufficient light on the inner workings of the actual binary . in case of the intel - based x86 cluster we mainly relied on intel s native performance analysis tools vtune and intel advisor , which provide information on various aspects and potential bottlenecks .",
    "in particular , on knc hardware it allows for an explicit analysis of the actual vectorisation intensity which is achieved in reality . in combination with compiler diagnostics offered by the intel compiler",
    ", we were able to combine the information obtained from the tools and the reported vectorisation level of the compiler to efficiently analyse bottlenecks and potential locations for code changes .",
    "this eventually allowed us to alter the code as much as needed but no more than necessary to obtain a high level of vectorisation intensity .",
    "+ porting and tuning the code on juqueen relied mainly on the score - p / scalasca / vampir toolset . here , it actually turned out that the compiler decides to not explicitly vectorise the code but rather to optimise it for on - core parallelism as it can be used when overloading a core . yet , it proved to be an efficient algorithm also in this case and showed that the path we were guided to on the x86 cluster was a valid direction also for lifting the code to a bg / q environment .    while it was very valuable to enable compiler - based vectorisation , the gain at this level of parallelisation was a little lower than we anticipated on the standard x86 hardware . in case of the xeon phi",
    "it was much lower , and in the case of juqueen it was even preferable for the compiler to resort to other means of optimisation .",
    "this tells us that there is still room for improvement , since in principle and also in ideal examples vectorisation can be much more efficient .",
    "considering the current hardware evolution with wider and wider simd units appearing in standard cpus , there is a big potential in this area to further improve the code .",
    "when designing a similar code from scratch we would strongly suggest to address this at a very early stage of code development .      when distributing work over a large number of nodes , already slight differences in work load can completely spoil the overall efficiency of parallelisation .",
    "therefore it is imperative to construct the overall algorithm in such a way that the load balancing amongst all nodes is extremely well equalised .",
    "the same argument holds for the intra - node load balancing , where it applies to the work distribution among the available threads .",
    "this of course means that in summary we need to ensure a near - to - perfect load balancing between all available threads on all participating nodes , i.e. ultimately for full machine runs on juqueen between @xmath2 threads ( @xmath0 nodes @xmath3 @xmath4 cores @xmath3 @xmath5-fold thread overload ) .",
    "this requirement translates into the necessity to find the best location for both levels of parallelisation , mpi and openmp - based , in the numerical algorithm : if the level in the algorithm is too high up , we will spoil the load balancing since we can not smoothen out differences in compute time .",
    "therefore , from this point of view we would place both handles as low as possible while keeping sufficient space between them .",
    "if we followed this paradigm we would , however , eventually run into difficulties at the other end of the scale .",
    "if we reduce the thread - specific numerical effort at the shared - memory openmp level by too much , the logistical overhead that is intrinsic to the openmp runtime framework will itself turn into a bottle - neck and act as a limiting factor for any further scaling when increasing the intensity of parallelisation . similarly , if we reduce the individual work packages per node by too much , i.e. if each node is `` too quick '' , it is the mpi communication overhead which will constitute a bottleneck in an analogous manner .",
    "these requirements impose a lower limit to the level in the algorithm where it is sensible to place the two parallelisation techniques mpi and openmp",
    ". we could even be unlucky and hit the case when there is no sweet - spot or sweet - region between the aforementioned approaches , and for very small problem sizes we indeed observed this when making use of the full juqueen .",
    "the process of locating such a sweet - spot would be a more or less impossible task without dedicated tools which have been designed and developed over many years for the specific purpose of analysing massively parallel code .",
    "in our case we mainly relied on the tools score - p , scalasca and vampir @xcite for the analysis of mpi and openmp load balancing as well as overhead analysis . in order to illustrate a typical part of the workflow when analysing the quality of the load balancing we show three figures for the case of using eight mpi worker nodes , each of which employs @xmath6 openmp threads .",
    "figure [ static_mpi_outer_omp ] shows a sample taken via score - p / vampir which shows the load distribution for the case when each mpi rank is assigned a fixed set of components at the beginning of each rhs evaluation .",
    "the openmp distribution within an mpi rank is such that each openmp thread calculates the rhs for one single component .",
    "while this was appropriate for certain cases and a small number of nodes , the graphical analysis immediately unveals the draw - backs .",
    "the white regions show that half of the nodes run idle for a substantial period of time during each calculation of the rhs .",
    "the seven evaluations of the rhs necessary to execute one ode step can clearly be seen .",
    "this deficiency was first tackled by changing the work scheduling at the openmp level by moving the openmp parallelisation one level down .",
    "namely , instead of having each thread compute the full rhs for a single component we make use of all threads already for the computation of the outer numerical quadrature in the two - dimensional integration , as described above .",
    "this leads to a load balancing picture shown in figure [ static_mpi_inner_omp ] where it can be seen that we obtain quite a reasonable level .",
    "we can further improve the situation by switching to the dynamical master - worker concept of assigning only one component of the rhs at the time to each rank , receiving the result and reassigning the next task to this now available rank . as shown in figure [ dynamic_mpi_inner_omp ]",
    "this further increases the efficiency of the algorithm and leads to a nearly ideal load balancing . on the other hand",
    ", this strategy increases the intensity of communication between master and workers and is known not to scale beyond a certain number of nodes , depending on the specific application .",
    "yet , we note that it is well worth considering and benchmarking this standard approach as we found that it constitutes the most efficient algorithm up to @xmath7 nodes . beyond this limit",
    "it is more efficient to resort to a static mpi load distribution .",
    "but for static mpi work load scheduling and inner - level openmp ]     but for dynamic mpi work load scheduling and inner - level openmp ]",
    "the central purpose of this work - and more generally of hpc in science - is to provide means to numerically tackle scientific problems of interest which would otherwise be unfeasible or extremely time consuming .",
    "however , hpc systems can not  automagically  speed up any scientific application .",
    "this is due to the fact that the enormous nominal peak performance relies on an extremely high level of hardware parallelism .",
    "therefore , a key task to harvest the power of hpc consists in designing applications in a manner that they scale on such parallel infrastructures . in many cases",
    "this task appears to be a major obstacle to code developers , the result often being the implicit decision to  maybe do it at a later stage  .",
    "+ in case of a particular example of a functional renormalisation group ( frg ) code we find that an hpc - ready design can indeed be accomplished , while at the same time the necessary code modifications can be kept at a very moderate level . in our case legacy code needed to be altered and the final resulting code is simple .",
    "arriving at this result involved a number of intermediate steps of analysis and code redesign , which we also sketched in this paper .",
    "+ in summary , we hope to motivate the frg community to use parallel computing architectures more intensely and eventually move to full - fledged hpc systems in order to most efficiently harvest the potential of the method . + as the closing bottom line we note that we achieved an overall speed - up of about five orders of magnitude , coming from a single core computation on an intel - based compute node to a full run on a 28 rack bg / q system .",
    "this is an enormous spread in compute time and in our eyes justifies the common conjecture that ",
    "quantity is quality  in this context .",
    "it became clear at the very beginning of this project that the task of efficiently parallelising the code would need regular input from several sides with respect to specific details at several stages .",
    "the successful outcome demonstrates that a cooperation between experts from different scientific fields can greatly improve the efficiency of a particular code , and by this the efficiency of science . along this line",
    ", the author greatly acknowledges regular support and assistance by hpc experts at jsc , in particular brian wylie from the cross - sectional team `` performance analysis '' , whose help was invaluable for many of the individual tasks of locating and removing `` the next bottleneck '' .",
    "equally helpful was the assistance accessible through the partnership with intel , mainly offered by heinrich bockhorst and zakhar matveev , which leveraged the value of intel tools and helped to identify many crucial aspects at the interface between code and actual hardware . in a similar manner we received input and advice on behalf of",
    "ibm / lenovo through christoph pospiech .",
    "+ data acquisition and analysis were greatly facilitated by using the jlich benchmarking environment ( jube ) @xcite , and a special thanks goes to sebastian lhrs for implementing some very helpful features at occasional phases of the project .",
    "http://dx.doi.org/10.1007/pl00011117[c .",
    "honerkamp , electron - doping versus hole - doping in the 2d t - t-hubbard model , the european physical journal b - condensed matter and complex systems 21  ( 1 ) ( 2001 ) 8191 . ]    http://dx.doi.org/10.1088/0953-8984/16/29/019[r .",
    "hedden , v.  meden , t.  pruschke , k.  schnhammer , a functional renormalization group approach to zero - dimensional interacting systems , journal of physics : condensed matter 16  ( 29 ) ( 2004 ) 5279 . ]      http://dx.doi.org/10.1103/physrevlett.93.106406[a .",
    "katanin , a.  kampf , quasiparticle anisotropy and pseudogap formation from the weak - coupling renormalization group point of view , physical review letters 93  ( 10 ) ( 2004 ) 106406 . ]    http://dx.doi.org/10.1088/0953-8984/20/34/345205[c .",
    "karrasch , r.  hedden , r.  peters , t.  pruschke , k.  schnhammer , v.  meden , a finite - frequency functional renormalization group approach to the single impurity anderson model , journal of physics : condensed matter 20  ( 34 ) ( 2008 ) 345205 . ]    http://dx.doi.org/10.1103/physrevb.79.195125[c .",
    "husemann , m.  salmhofer , efficient parametrization of the vertex function , @xmath8 scheme , and the t , t hubbard model at van hove filling , physical review b 79  ( 19 ) ( 2009 ) 195125 . ]",
    "http://dx.doi.org/10.1103/physrevb.81.144410[j .",
    "reuther , p.  wlfle , j 1-j 2 frustrated two - dimensional heisenberg model : random phase approximation and functional renormalization group , physical review b 81  ( 14 ) ( 2010 ) 144410 . ]        http://dx.doi.org/10.1088/1367-2630/10/4/045003[r .",
    "gersch , c.  honerkamp , w.  metzner , superconductivity in the attractive hubbard model : functional renormalization group analysis , new journal of physics 10  ( 4 ) ( 2008 ) 045003 . ]      http://dx.doi.org/10.1103/revmodphys.84.299[w .",
    "metzner , m.  salmhofer , c.  honerkamp , v.  meden , k.  schnhammer , functional renormalization group approach to correlated fermion systems , reviews of modern physics 84  ( 1 ) ( 2012 ) 299 . ]    http://dx.doi.org/10.1016/s0370-1573(01)00098-9[j .",
    "berges , n.  tetradis , c.  wetterich , non - perturbative renormalization flow in quantum field theory and statistical physics , physics reports 363  ( 46 ) ( 2002 ) 223  386 , renormalization group theory in the new millennium .",
    "\\{iv}. ]              http://dx.doi.org/10.1109/n-ssc.2007.4785615[g .",
    "m. amdahl , validity of the single processor approach to achieving large scale computing capabilities , afips conference proceedings , vol .",
    "30 , 1967 , pp . 483485 . ]",
    "http://dx.doi.org/10.1109/hpcc.and.euc.2013.39[a .",
    "castellanos , a.  moreno , j.  sorribes , t.  margalef , performance model for master / worker hybrid applications , ieee international conference on high performance computing and communications , 2013 ieee international conference on embedded and ubiquitous computing ( 2013 ) 210217 . ]        http://dx.doi.org/10.1016/j.parco.2011.02.002[h .",
    "jin , d.  jespersen , p.  mehrotra , r.  biswas , l.  huang , b.  chapman , high performance computing using mpi and openmp on multi - core parallel systems ] , parallel computing 37  ( 9 ) ( 2011 ) 562  575 , emerging programming paradigms for large - scale scientific computing .    http://dx.doi.org/10.14529/jsfi140207[b .",
    "mohr , scalable parallel performance measurement and analysis tools - state - of - the - art and future challenges , supercomputing frontiers and innovations 1  ( 2 ) ( 2014 ) 108123 . ]"
  ],
  "abstract_text": [
    "<S> the functional renormalisation group ( frg ) has evolved into a versatile tool in condensed matter theory for studying important aspects of correlated electron systems . </S>",
    "<S> practical applications of the method often involve a high numerical effort , motivating the question in how far high performance computing ( hpc ) can leverage the approach . </S>",
    "<S> + in this work we report on a multi - level parallelisation of the underlying computational machinery and show that this can speed up the code by several orders of magnitude . </S>",
    "<S> this in turn can extend the applicability of the method to otherwise inaccessible cases . </S>",
    "<S> + we exploit three levels of parallelisation : distributed computing by means of message passing ( mpi ) , shared - memory computing using openmp , and vectorisation by means of simd units ( single - instruction - multiple - data ) . </S>",
    "<S> results are provided for two distinct high performance computing ( hpc ) platforms , namely the ibm - based bluegene / q system juqueen and an intel sandy - bridge - based development cluster . </S>",
    "<S> we discuss how certain issues and obstacles were overcome in the course of adapting the code . </S>",
    "<S> most importantly , we conclude that this vast improvement can actually be accomplished by introducing only moderate changes to the code , such that this strategy may serve as a guideline for other researcher to likewise improve the efficiency of their codes . </S>"
  ]
}