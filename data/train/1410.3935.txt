{
  "article_text": [
    "conditional random fields ( crfs ) @xcite are probabilistic models for discriminative modeling defining a conditional distribution @xmath0 over output @xmath1 given input @xmath2 .",
    "they are quite popular for labeling sequence data such as text data and biological sequences @xcite .",
    "although they are usually specified by graphical models , we here propose to use probabilistic logic programs and specify them generatively .",
    "our intension is first to provide a unified approach to crfs for complex modeling through the use of a turing complete language and second to offer a convenient way of realizing generative - discriminative pairs @xcite in machine learning to compare generative and discriminative models and choose the best model .",
    "the use of logical expressions to specify crfs is not new but they have been used solely as feature functions @xcite .",
    "for example in markov logic networks ( mlns)@xcite , weighted clauses are used as feature functions to define ( conditional ) markov random fields and probabilities are obtained by gibbs sampling . in contrast , our approach is implemented by a generative modeling language prism @xcite where clauses have no weights ; they simply constitute a logic program @xmath3 computing possible output @xmath1 from input @xmath2 by proving a top - goal @xmath4 that relates @xmath2 to @xmath1 .",
    "in addition probabilities are exactly computed by dynamic programming .",
    "@xmath3 however contains special atoms of the form @xmath5 having weights @xmath6 where @xmath7 and @xmath8 are arbitrary terms .",
    "they are called @xmath9 atoms here as in prism .",
    "we define the weight @xmath10 of a top - goal @xmath4 as a sum - product of such weights associated with @xmath11 atoms appearing in a proof of @xmath4 and consider @xmath10 as an unnormalized distribution . by modifying the dynamic programming mechanism of prism slightly , we can efficiently compute , when possible and feasible , the unnormalized marginal distribution @xmath12 and obtain a crf @xmath13 .",
    "we implemented our idea by modifying prism and termed the resulting language d - prism ( discriminative prism ) .",
    "d - prism is a general programming language that generatively defines crfs and provides built - in predicates for parameter learning and viterbi inference of crfs .",
    "our approach to crfs is general in the sense that , like other statistical relational learning ( srl ) languages for crfs @xcite , programs in d - prism have no restriction such as the exclusiveness condition in prism @xcite except for the use of binary features and we can write any program , i.e.   we can write arbitrary crfs as long as they are described by d - prism",
    ". we point out that binary features are the most common features and they can encode basic crf models such as logistic regression , linear - chain crfs and crf - cfgs @xcite . furthermore by dynamic programming , probabilistic inference can be efficiently carried out with the same time complexity as their generative counterparts as exemplified by linear - chain crfs and hidden markov models ( hmms ) .    in machine learning",
    "it is well - known that naive bayes and logistic regression form a generative - discriminative pair @xcite .",
    "that is , any conditional distribution @xmath14 computed from a joint distribution @xmath15 defined generatively by naive bayes , where @xmath1 is a class and @xmath16 is a feature vector , can also be defined directly by logistic regression and vice versa .",
    "as is empirically demonstrated in @xcite , classification accuracy by discriminative models such as logistic regression is generally better than their corresponding generative models such as naive bayes when there is enough data but generative models reach their best performance more quickly than discriminative ones w.r.t .   the amount of available data .",
    "also the theoretical analysis in @xcite suggests that when a model is wrong in generative modeling , the deterioration of prediction accuracy is more severe than in discriminative modeling .",
    "it seems therefore reasonable to say `` ... for any particular data set , it is impossible to predict in advance whether a generative or a discriminative model will perform better '' @xcite . hence what is desirable is to provide a modeling environment in which the user can test both types of modeling smoothly without pain and d - prism provides such an environment that makes it easy to test and compare discriminative modeling and generative modeling for the same class or related family of probabilistic models .    in what follows ,",
    "we review crfs in section  [ sec : crfs ] and also review three basic models , i.e.   logistic regression , linear - chain crfs and crf - cfgs in section  [ sec : models ] .",
    "we then introduce d - prism in section  [ sec : d - prism ] .",
    "we empirically verify the effectiveness of our approach in section  [ sec : experiments ] using the three models .",
    "section  [ sec : new - models ] introduces new crf models , crf - bncs and crf - lcgs , both easily implementable in d - prism . in section  [ sec : trans ] , we discuss program transformation which derives a program for incomplete data from one for complete data .",
    "section  [ sec : discussion ] contains related work and discussion and section  [ sec : conclusion ] is the conclusion .",
    "conditional random fields ( crfs ) @xcite are popular probabilistic models defining a conditional distribution @xmath17 over the output sequence @xmath18 given an input sequence @xmath16 which takes the following form :    @xmath19    here @xmath20 and @xmath21 ( @xmath22 ) are respectively a real valued function ( _ feature function _ ) and the associated weight ( _ parameter _ ) and @xmath23 a normalizing constant . as @xmath23 is the sum of exponentially many terms , the exact computation is generally intractable and takes @xmath24 time where @xmath25 is the maximum number of possible values for each component of @xmath18 and hence approximation methods have been developed @xcite .",
    "however when @xmath17 has recursive structure of specific type as a graphical model like linear - chain crfs , @xmath23 is efficiently computable by dynamic programming .",
    "now let @xmath26 be a training set .",
    "the regularised conditional log - likelihood @xmath27 of @xmath28 is given by    @xmath29    where @xmath30 are parameters and @xmath31 is a penalty term .",
    "parameters are then estimated as the ones that maximize @xmath27 by newton s method or quasi - newton methods .",
    "the gradient required for parameter learning is computed as    @xmath32    the problem here is that the expectation @xmath33 is difficult to compute and hence a variety of approximation methods such as stochastic gradient descent ( sdg ) @xcite have been proposed .",
    "however in this paper we focus on cases where exact computation by dynamic programming is possible and use an algorithm that generalizes inside probability computation in probabilistic context free grammars ( pcfgs ) @xcite .",
    "after parameter learning , we apply our model to prediction tasks and infer the most - likely output @xmath34 for an input sequence @xmath16 using @xmath35    as naively computing @xmath34 is straightforward but too costly , we again consider only cases where dynamic programming is feasible and apply a variant of the viterbi algorithm for hmms .",
    "logistic regression specifies a conditional distribution @xmath36 over a class variable @xmath1 given the input @xmath37 , a vector of attributes .",
    "it assumes @xmath38 is a linear function of @xmath16 and given by    @xmath39    we here confirm that logistic regression is a crf .",
    "rewrite @xmath40 and @xmath41 and substitute them for @xmath42 and @xmath43 in the above formula is a binary function of @xmath1 taking @xmath44 if @xmath45 , otherwise @xmath46 . ] .",
    "we obtain    @xmath47    by considering @xmath48 and @xmath49 as feature functions ( of @xmath1 and @xmath16 ) , we can see logistic regression is a crf .",
    "crfs @xcite are generally intractable and a variety of approximation methods such as sampling and loopy bp have been developed .",
    "there is however a tractable subclass called _ linear - chain crf_s .",
    "they are of the following the form :    @xmath50    where @xmath23 is a normalizing constant .",
    "they define , as crfs , a conditional distribution @xmath51 over output sequences @xmath18 given an input sequence @xmath16 such that @xmath52 ( @xmath53 denotes the length of vector @xmath16 ) but feature functions are restricted to the form @xmath54 ( @xmath55 ) which only refers to two consecutive components in @xmath18 .",
    "thanks to this local reference restriction exact probability computation is possible for linear - chain crfs in time linear in the input length @xmath53 by a variant of the forward - backward algorithm for hmms .",
    "linear - chain crfs are considered as a generalized and undirected version of hmms which enable us to use far richer feature functions other than transition probabilities and emission probabilities used in hmms .",
    "pcfgs @xcite are a basic class of probabilistic grammars extending cfgs by assigning selection probabilities @xmath56 to production rules . in pcfgs ,",
    "the probability of a sentence is the sum of probabilities of parse trees and the probability of a parse tree is the product of probabilities associated with production rules used in the tree .",
    "pcfgs are generative models and parameters are usually learned by maximum likelihood estimation ( mle ) .",
    "so given parse trees @xmath57 and the corresponding sentences @xmath58 , parameters are estimated as @xmath59 .",
    "seeking better parsing accuracy , johnson attempted parameter learning by maximizing conditional likelihood : @xmath60 but found the improvement is not statistically significant @xcite .",
    "later finkel et al .",
    "generalized pcfgs to _ conditional random field context free grammar_s ( crf - cfgs ) @xcite where the conditional probability @xmath61 of a parse tree @xmath62 given a sentence @xmath63 is defined by @xmath64 here @xmath65 is a normalizing constant .",
    "@xmath66 are parameters and @xmath67 is a cfg rule ( possibly enriched with other information ) appearing in the parse tree @xmath62 of @xmath63 and @xmath68 is a feature function .",
    "finkel et al .   conducted learning experiments with a crf - cfg using the penn treebank @xcite .",
    "they learned parameters from parse trees @xmath57 and the corresponding sentences @xmath58 in the corpus by maximizing conditional likelihood just like @xcite but this time they obtained a significant gain in parsing accuracy @xcite .",
    "their experiments clearly demonstrate the advantage of extensive use of features and discriminative parameter learning .",
    "having seen basic models of crfs , we next show how they are uniformly subsumed by a logic - based modeling language prism @xcite with a simple modification of its probability computation . the modified language is termed _ d - prism _",
    "( discriminative prism ) .      before proceeding we",
    "quickly review prism .",
    "prism is a high - level generative modeling language based on prolog , extended by a rich array of probabilistic built - in predicates for various types of probabilistic inference and parameter learning .",
    "specifically it offers , in addition to mle by the em algorithm , viterbi training ( vt ) , variational bayes ( vb ) , variational vt ( vb - vt ) and mcmc for bayesian inference .",
    "prism has been applied to music and bioinformatics @xcite .",
    "syntactically a prism program @xmath3 is a prolog program and runs like prolog .",
    "[ prog : nb ] is an example of prism program for naive bayes .",
    "@xmath3 is basically a set of definite clauses .",
    "the difference from usual prolog programs is that the clause body may contain special atoms , @xmath9 atoms stands for `` multi - valued switch . '' ] , of the form msw(@xmath7,@xmath8 ) representing a probabilistic choice made by ( analogically speaking ) rolling a die @xmath7 and choosing the outcome @xmath8 . here",
    "@xmath7 is a ground term naming the @xmath9 atom and @xmath8 belongs to a set @xmath69 of possible outcomes declared by values/2 declaration .",
    "the probability of msw(@xmath7,@xmath8 ) ( @xmath70 ) being true is denoted by @xmath71 and called a _ parameter _ for msw(@xmath7,@xmath8 ) . naturally @xmath72 holds . executing @xmath73 with a variable @xmath74 returns a value @xmath70 in @xmath75 with probability @xmath71 .",
    "so msw(season , s ) in fig .",
    "[ prog : nb ] probabilistically returns one of \\{spring , summer , fall , winter } in s.    ' '' ''     + [ -1em ]    .... values(season,[spring , summer , fall , winter ] ) .",
    "values(attr(temp,_),[high , mild , low ] ) .",
    "values(attr(humidity,_),[high , low ] ) .",
    "nb([t , h],s):-                    % defines p(x , y ) where x = [ t , h ] and y = s      msw(season , s ) ,               % choose s from { spring , summer , fall , winter }      msw(attr(temp , s),t ) ,         % choose t from { high , mild , low }      msw(attr(humidity , s),h ) .",
    "% choose h from { high , low } nb([t , h]):- nb([t , h ] , _ ) .",
    "% defines p(x ) where x = [ t , h ] ....    ' '' ''     +    @xmath3 defines a probability measure @xmath76 over herbrand interpretations ( possible worlds)@xcite .",
    "the probability @xmath77 of a top - goal @xmath78 then is computed as a sum - product of parameters in two steps .",
    "first @xmath78 is reduced using @xmath3 by sld search to a disjunction @xmath79 such that each @xmath80 ( @xmath81 ) is a conjunction of @xmath82 atoms representing a sequence of probabilistic choices .",
    "@xmath80 is called an _ explanation _ for @xmath78 because it explains why @xmath78 is true or how @xmath78 is proved as a result of probabilistic choices encoded by @xmath80 .",
    "let @xmath83 be the set of all explanations for @xmath78 .",
    "@xmath77 is computed as @xmath84 and @xmath85 for @xmath86 , where @xmath87 is a parameter for @xmath88 ( @xmath89 ) .",
    "let @xmath90 be a joint distribution over input @xmath2 ( or observation ) and output @xmath1 ( or hidden state ) which we wish to compute by a prism program .",
    "we write a program @xmath3 that probabilistically proves @xmath4 , a top - goal that relates @xmath2 to @xmath1 , using @xmath82 atoms , in such a way that @xmath91 holds .",
    "since @xmath92 forms complete data , @xmath4 has only one explanation @xmath93 for @xmath4 , so we have @xmath94 where @xmath95 is the count of msw(@xmath7,@xmath8 ) in @xmath93 .",
    "introduce @xmath96 . then the marginal probability @xmath97 is obtained as @xmath98 because @xmath99 holds .",
    "hence the conditional distribution @xmath0 is computed as @xmath100    we next apply ( [ eq : crfprism ] ) to the naive bayes program @xmath101 in fig .",
    "[ prog : nb ] .",
    "@xmath101 is intended to infer a season s from temperature t and humidity h and generatively defines a joint distribution @xmath102},\\mbox{\\tt     s } )     =     p_{{d\\!b}_0}({\\tt    nb([t , h],s)})$ ] . to draw a sample from @xmath103},\\mbox{\\tt s})$ ] or equivalently from @xmath104,s)})$ ] , it first samples a season s by executing msw(season , s ) , then similarly samples a value t of temperature and a value h of humidity , each conditioned on s , by executing msw(attr(temp , s),t ) and msw(attr(humidity , s),h ) in turn . ] .",
    "note that the program also includes a clause nb([t , h]):-nb([t , h ] , _ ) to compute a marginal distribution @xmath105 } ) }   ) $ ] (= @xmath103})$ ] ) .",
    "the correspondence to ( [ eq : crfprism ] ) is that @xmath2 = , @xmath1 = s , @xmath4 = nb([t , h],s ) and @xmath106 = nb([t , h ] ) .",
    "the conditional distribution @xmath107})$ ] is computed as @xmath108,s ) }   ) /p_{{d\\!b}_0 } ( \\mbox{\\tt nb({[t , h ] } ) } ) $ ] .",
    "the basic idea of our approach to discriminative modeling is to generalize ( [ eq : crfprism ] ) by replacing probability @xmath71 for msw(@xmath7,@xmath8 ) with arbitrary _ weight _ @xmath109 . in d - prism",
    "we further perform normalization to obtain a crf .",
    "more precisely , we first introduce an _",
    "unnormalized distribution _",
    "@xmath110 defined by : @xmath111 assuming that for any complete data @xmath92 and the corresponding top - goal @xmath4 , our program , @xmath3 , always has only one explanation @xmath93 . by setting @xmath112 , @xmath113",
    "is reduced to @xmath114 again .",
    "next we rewrite ( [ eq : crfprism ] ) as follows by putting @xmath115 and using @xmath116 .",
    "@xmath117    ( [ eq : crf - def ] ) and ( [ eq : crf - def - z ] ) are fundamental equations for d - prism describing how a crf @xmath118 is defined and computed . by comparing ( [ eq : crfprism ] ) to ( [ eq : crf - def ] ) and ( [ eq : crf - def - z ] ) , we notice that the most computationally demanding task in d - prism , computing @xmath119 in ( [ eq : crf - def - z ] ) , can be carried out efficiently by dynamic programming just by replacing probability @xmath71 in prism with weight @xmath120 , resulting in the same time complexity for probability computation as in prism .",
    "it is also seen from ( [ eq : crf - def ] ) and ( [ eq : crf - def - z ] ) that in our formulation of crfs by d - prism , @xmath95 , the count of msw(@xmath7,@xmath8 ) in @xmath93 , works as a ( default ) feature function has only one explanation @xmath93 for a complete data @xmath92 , @xmath92 uniquely determines @xmath95 . ] over the input @xmath2 and output @xmath1 .",
    "@xmath95 becomes binary when occurs at most once in @xmath93 . for a binary feature function @xmath121 in general ,",
    "let be a dummy msw atom which is unique to @xmath121 and always true .",
    "we assume that corresponding to @xmath121 , there is a goal provable in prism if and only if @xmath122 .",
    "then it is easy to see that a prism goal realizes @xmath121 . from the viewpoint of modeling",
    ", we emphasize that for the user , d - prism programs are just prism programs that proves two top - goals , @xmath4 for complete data @xmath92 and @xmath106 for incomplete data @xmath2 .",
    "for example , the prism program in fig .",
    "[ prog : nb ] for naive bayes is also a d - prism program defining logistic regression .",
    "+ in d - prism , parameters are learned discriminatively from complete data .",
    "consider the regularised ( log ) conditional likelihood @xmath27 of a set of observed data @xmath123 where @xmath124 ( @xmath125 ) .",
    "@xmath126 is given by @xmath127 and parameters @xmath128 are determined as the ones that maximize @xmath129 .",
    "currently we use l - bfgs @xcite to maximize @xmath129 .",
    "the gradient used in the maximization is computed as @xmath130    finally , viterbi inference , computing the most likely output @xmath1 for the input @xmath2 , or the most likely explanation @xmath131 for the top - goal @xmath106 , is formulated as ( [ eq : crfviterbi ] ) in d - prism and computed by dynamic programming just like prism . @xmath132",
    "in this section , we conduct learning experiments with crfs2 cpu and 72 gb ram running opensuse 11.2 . ] .",
    "crfs are encoded by d - prism programs while their generative counterparts are encoded by prism programs .",
    "we compare their accuracy in discriminative tasks .",
    "we consider three basic models , logistic regression , a linear - chain crf and a crf - cfg , and learn their parameters by l - bfgs .",
    "we select four datasets with no missing data from the uci machine learning repository @xcite and compare prediction accuracy , one by logistic regression written in d - prism and the other by a naive bayes model ( nb ) written in prism .",
    "we use the program in fig .",
    "[ prog : nb ] with an appropriate modification of values/2 declarations .",
    "the result by ten - fold cross - validation is shown in table  [ tab : uci - nb ] with standard deviation in parentheses .",
    "table  [ tab : uci - nb - time ] contains learning time for each dataset .",
    "we can see , except for the zoo dataset , logistic regression by d - prism outperforms naive bayes by prism at the cost of considerably increased learning time for larger datasets .",
    ".[tab : uci - nb - time ] logistic - regression and naive bayes : learning time ( sec ) [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]",
    "as explained in section  [ sec : d - prism ] , in d - prism the user needs to define two top - goals , @xmath4 for complete data @xmath92 and @xmath133 for incomplete data @xmath2 , each defining unnormalized distributions @xmath134 and @xmath135 respectively",
    ". then @xmath0 is computed as @xmath136 .",
    "however since @xmath4 and @xmath106 are logically connected as @xmath137 , it is theoretically enough and correct to add a clause `` g(x):-g(x , y ) '' to the program for @xmath4 to obtain a program for @xmath135 and `` g(x ) '' for @xmath106 respectively . ] .",
    "this is what we did for the naive bayes program in fig .",
    "[ prog : nb ] .",
    "unfortunately this simple approach does not work in general .",
    "the reason is that the search for all explanations for @xmath138 causes an exhaustive search for proofs of @xmath139 for all possible values of @xmath140 .",
    "consequently when a subgoal occurring in the search process that carries @xmath141 is proved with some value @xmath142 and tabled , i.e.  stored in the memory for reuse , it has little chance of being reused later because @xmath140 in the subgoal mostly takes different values from @xmath143 . as a result the effect of tabling is almost nullified , causing an exponential search time for all explanations for @xmath138 .    to avoid this negative effect of the redundant argument , @xmath140 , we often have to write a specialized program for @xmath138 , independently of a program for @xmath139 , that does not refer to @xmath140 ; in the case of linear - chain crf program in fig .",
    "[ prog : crf - hmm ] , we wrote two programs , one for @xmath144 ( complete data ) and the other for @xmath145 ( incomplete data ) .",
    "the latter is a usual hmm program and efficient tabling is possible that guarantees linear time search for all explanations .",
    "however , writing a specialized program for @xmath138 invites another problem of program correctness .",
    "when we independently write two programs , @xmath146 for @xmath139 and @xmath147 for @xmath138 , they do not necessarily satisfy @xmath148 which is required for sound computation of the conditional distribution of @xmath149 .",
    "it is therefore hoped to find a way of obtaining @xmath147 satisfying this property .    ' '' ''     + [ -1em ]    ....   ( 1 ) hmm0([x0|xs],[y0|ys]):- msw(init , y0),msw(out(y0),x0),hmm1(y0,xs , ys ) .",
    "( 2 ) hmm1 ( _ , [ ] , [ ] ) .",
    "( 3 ) hmm1(y0,[x|xs],[y|ys]):-",
    "msw(tr(y0),y),msw(out(y),x),hmm1(y , xs , ys ) .",
    "( 4 ) hmm0(x):-",
    "hmm0(x , y ) .",
    "( 5 ) hmm1(y0,xs):- hmm1(y0,xs , ys ) .     ( 6 ) hmm0([x0|xs ] ) : - msw(init , y0),msw(out(y0),x0),hmm1(y0,xs , ys ) .         -- from unfolding ( 4 ) by ( 1 )   ( 7 ) hmm0([x0|xs ] ) : - msw(init , y0),msw(out(y0),x0),hmm1(y0,xs ) .         -- from folding ( 6 ) by ( 5 )   ( 8) hmm1(y0 , [ ] ) .         -- unfolding ( 5 ) by ( 2 ) and ( 3 ) giving ( 8) and ( 9 )   ( 9 ) hmm1(y0,[x|xs]):- msw(tr(y0),y),msw(out(y),x),hmm1(y , xs , ys ) . ( 10 ) hmm1(y0,[x|xs]):- msw(tr(y0),y),msw(out(y),x),hmm1(y , xs ) .         -- from folding ( 9 ) by ( 5 ) ....    ' '' ''     +    one way to achieve this is to use meaning preserving unfold / fold transformation for logic programs @xcite .",
    "it is a system of program transformation containing rules for unfolding and folding operations .",
    "unfolding replaces a goal with the matched body of a clause whose head unifies with the goal and folding is a reverse operation .",
    "there are conditions on transformation that must be met to ensure that the transformation is meaning preserving , i.e.  the least model of programs is preserved through transformation ( see @xcite for details ) .",
    "note that meaning preserving unfold / fold program transformation also preserves the set of all explanations for a goal .",
    "so if @xmath150 is obtained from @xmath151 by such transformation , both programs have the same set of all explanations for g(x ) , and hence the desired property @xmath152 holds .",
    "in addition , usually , @xmath150 does not refer to the cumbersome @xmath140 .",
    "[ fig : dprism : trans ] illustrates a process of such program transformation .",
    "it derives an hmm program for hmm0(x ) computing incomplete data from a program for hmm0(x , y ) computing complete data using a transformation system described in @xcite .",
    "it starts with the initial program defining hmm0(x , y ) consisting of \\{(1 ) , ( 2 ) , ( 3 ) } together with two defining clauses for new predicates , i.e.  ( 4 ) for hmm0(x ) and ( 5 ) for hmm1(y0,xs ) . the transformation process begins by unfolding the body goal hmm0(x , y ) in ( 4 ) by ( 1 ) and folding hmm1(y0,xs , ys ) by ( 5 ) follows , resulting in ( 7 ) . the defining clause ( 5 ) for hmm1(y0,xs ) is processed similarly .",
    "the final program obtained is \\{(7 ) , ( 8) , ( 10 ) } which coincides with the hmm program for hmm0(x ) in fig .",
    "[ prog : crf - hmm ] .",
    "this example exemplifies that unfold / fold transformation has the power of eliminating the redundant argument @xmath153 in and deriving a specialized program for @xmath138 that does not refer to @xmath141 and hence is suitable for tabling .",
    "however how far this transformation is generally applicable and how far it can be automated is future work .",
    "there are already discriminative modeling languages for crfs such as alchemy @xcite based on mlns and factorie @xcite based on factor graphs . to define potential functions and hence models , the former uses weighted clauses whereas the latter uses imperatively defined factor graphs .",
    "both use markov chain monte - carlo ( mcmc ) for probabilistic inference .",
    "d - prism differs from them in that although programs are used to define crfs like mlns and factorie , they are purely generative , computing output from input , and probabilities are computed exactly by dynamic programming .",
    "tildecrf @xcite learns crfs over sequences of ground atoms .",
    "potential functions are computed by weighted sums of relational regression trees applied to an input sequence with a fixed size window .",
    "tildecrf is purely discriminative and unlike d - prism uses a fixed type of potential function .",
    "also it is designed for linear - chain crfs and more complex crfs such as crf - cfgs are not intended or implemented .",
    "d - prism is interesting from the viewpoint of statistical machine learning in that it builds discriminative models from generative models and offers a general approach to implementing generative - discriminative pairs .",
    "this unique feature also makes it relatively easy and smooth to develop new discriminative models from generative models as we demonstrated in section  [ sec : new - models ] .",
    "in addition , as is shown by every experiment in this paper , there is a clear trade - off between accuracy ( by discriminative models ) and learning time ( by generative models ) , and hence we have to choose which type of model to use , depending on our purpose .",
    "d - prism assists our choice by providing a unified environment to test both types .",
    "+ compared to prism , d - prism has no restriction on programs such as the uniqueness condition , exclusiveness condition and independence condition @xcite . consequently non - exclusive or is permitted in a program .",
    "also probability computation is allowed to fail by constraints .",
    "for example it is straightforward to add linguistic constraints such as subject - verb agreement to a pcfg by adding an extra argument carrying such agreement information to the program .",
    "although loss of probability mass occurs due to disagreement in the generating process , normalization recovers a distribution and we obtain a constraint crf - cfg as a result . of course this freedom",
    "is realized at the cost of normalization which may be prohibitive even when dynamic programming is possible .",
    "this would happen when adding too many constraints , e.g. , agreement in number , gender , tense and so on to a pcfg .",
    "thanks to the removal of restrictive conditions however , d - prism is now more amenable to structure learning in ilp than prism , which is expected to open up a new line of research of learning crfs in ilp .",
    "+ in this paper we concentrated on learning from complete data in crfs and missing value is not considered .",
    "when there are missing values , for example when some labels on a sequence in a linear - chain crf are missing , the data is incomplete and parameter learning becomes much harder , if not impossible .",
    "there is a method of parameter learning from incomplete data for conditional distributions using em .",
    "it is developed for prism programs with failure @xcite and learns parameters from a conditional distribution of the form @xmath154 where @xmath155 and @xmath133 is a goal for incomplete data @xmath2 that may fail .",
    "the point in @xcite is to automatically synthesize failure predicate such that @xmath156 and rewrite the conditional distribution as an infinite series @xmath157 to which em is applicable ( the fam algorithm @xcite ) . although whether the adaptation of this technique to em learning of crfs with incomplete data is possible or not is unknown , it seems worth pursuing considering the simplicity of em compared to complicated gradient - based parameter learning algorithms for incomplete data . + in section  [ sec : trans ] , the unfold / fold program transformation is used to remove the redundant argument @xmath153 from @xmath144 .",
    "@xmath140 is a non - discriminating argument in the sense of @xcite .",
    "christiansen and gallagher gave a deterministic algorithm to eliminate such non - discriminating arguments without affecting the program s run - time behavior @xcite . actually deleting non - discriminating arguments from clauses for @xmath144 in fig .",
    "[ prog : crf - hmm ] results in the same hmm program obtained by program transformation .",
    "compared to their approach however , our approach is based on non - deterministic unfold / fold program transformation and allows for an introduction of new predicates .",
    "clarifying the relationship between these two approaches is future work .",
    "+ currently only binary features or their counts are allowed in d - prism .",
    "introducing real - valued features is also a future work and so is a mechanism of parameter tying .",
    "finally , d - prism is experimentally implemented at the moment and we hope it is part of the prism package in the future .",
    "we have introduced d - prism , a logic - based generative language for discriminative modeling .",
    "as examples show , d - prism programs are just prism programs with probabilities replaced by weights .",
    "it is the first modeling language to our knowledge that generatively defines crfs and their extensions to probabilistic grammars .",
    "we can freely build logistic regression , linear - chain crfs , crf - cfgs or new models generatively with almost the same modeling cost as prism while achieving better performance in discriminative tasks .",
    "biba , m. , xhafa , f. , esposito , f. , ferilli , s. : stochastic simulation and modelling of metabolic networks in a machine learning framework .",
    "simulation modelling practice and theory * 19*(9 ) , 19571966 ( 2011 )          finkel , j. , kleeman , a. , manning , c. : efficient , feature - based , conditional random field parsing . in : proceedings of the 46th annual meeting of the association for computational linguistics ( acl08 ) , pp .",
    "959967 ( 2008 )            : the alchemy system for statistical relational ai .",
    ", department of computer science and engineering , university of washington ( 2005 ) .",
    "{ http://www.cs.washington.edu/ai/alchemy}[\\{http://www.cs.washington.edu/ai/alchemy } ]    lafferty , j. , mccallum , a. , pereira , f. : conditional random fields : probabilistic models for segmenting and labeling sequence data .",
    "in : proceedings of the 18th international conference on machine learning ( icml01 ) , pp . 282289 ( 2001 )",
    "liang , p. , jordan , m. : an asymptotic analysis of generative , discriminative , and pseudolikelihood estimators . in : proceedings of the 25th international conference on machine learning ( icml08 ) , pp .",
    "584591 ( 2008 )                            sneyers , j. , vennekens , j. , de  schreye , d. : probabilistic - logical modeling of music . in : proceedings of the 8th international symposium on practical aspects of declarative languages ( padl06 ) , vol.3819 , lncs , pp .",
    "6072 ( 2006 )      tamaki , h. , sato , t. : unfold / fold transformation of logic programs . in",
    ": proceedings of the 2nd international conference on logic programming ( iclp84 ) , lecture notes in computer science , pp . 127138 . springer ( 1984 )      van  uytsel , d. , van  compernolle , d. , wambacq , p. : maximum - likelihood training of the plcg - based language model . in : proceedings of the ieee automatic speech recognition and understanding workshop 2001 ( asru01 ) , pp .",
    "210213 ( 2001 )"
  ],
  "abstract_text": [
    "<S> conditional random fields ( crfs ) are usually specified by graphical models but in this paper we propose to use probabilistic logic programs and specify them generatively . our intension is first to provide a unified approach to crfs for complex modeling through the use of a turing complete language and second to offer a convenient way of realizing generative - discriminative pairs in machine learning to compare generative and discriminative models and choose the best model . </S>",
    "<S> we implemented our approach as the d - prism language by modifying prism , a logic - based probabilistic modeling language for generative modeling , while exploiting its dynamic programming mechanism for efficient probability computation . </S>",
    "<S> we tested d - prism with logistic regression , a linear - chain crf and a crf - cfg and empirically confirmed their excellent discriminative performance compared to their generative counterparts , i.e.  naive bayes , an hmm and a pcfg . </S>",
    "<S> we also introduced new crf models , crf - bncs and crf - lcgs . </S>",
    "<S> they are crf versions of bayesian network classifiers and probabilistic left - corner grammars respectively and easily implementable in d - prism . </S>",
    "<S> we empirically showed that they outperform their generative counterparts as expected . </S>"
  ]
}