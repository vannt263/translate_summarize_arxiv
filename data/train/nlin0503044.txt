{
  "article_text": [
    "the entropy rate is a key parameter associated with stochastic processes , information sources and dynamical systems . roughly speaking",
    ", the entropy rate quantifies the average uncertainty , disorder or irregularity generated by a process or system per ` time ' unit and , it is the primary subject of fundamental results in information and coding theory ( shannon s noiseless coding theorem ) and statistical mechanics ( second law of thermodynamics ) .",
    "it is not surprising , therefore , that this notion , appropriately generalized and transformed , is ubiquitous in many fields of mathematics and science when randomness or ` random - like ' behavior is at the heart of the theory or model being studied .    for definiteness consider a stationary information source emitting a time - series of observed values @xmath1 in a continuous state space formally , draws from the random variables @xmath2 . since the realization of a non - discrete random variable",
    "can not be observed exactly ( this would mean an infinite amount of information ) , the observer has to content himself with a finite degree of accuracy . generally speaking ,",
    "the metric or shannon entropy rate of an information source is the rate of new information it generates per unit time ( as the metric or kolmogorov - sinai entropy rate of a deterministic dynamical system is a measure of its pseudo - randomness or chaotic behavior ) . given a certain discretization scale @xmath3 of the state space , the metric ( shannon ) entropy rate @xmath4 of the discretized information source @xmath5 is @xmath6with @xmath7 a length @xmath8 word of symbols @xmath9 discretized at resolution @xmath3 from @xmath10 .",
    "we use @xmath11 for the entropy of the discrete random variable @xmath12 , i.e. , @xmath13 for the probability distribution @xmath14 of @xmath12 .",
    "we come back to the metric entropy and entropy rate in the next section , where we set the conceptual background of this paper on a more formal footing .",
    "consider a length @xmath15 word of observables @xmath16 .",
    "assuming there exists a natural order relation on the state space of the source @xmath17 ( e.g. , real scalars or vectors with a defined lexicographic ordering ) , each block of observations @xmath16 selects one particular _",
    "permutation _ @xmath18 out of the @xmath19 possible permutations .",
    "for example , if @xmath20 , then the corresponding permutation can be expressed explicitly as @xmath21 .",
    "note that the mapping from @xmath9-orderings to permutations can be many - to - one when there are repeated values ; to overcome this shortcoming , we will use ` ranks ' below ( see sect .",
    "3 ) , so that words defining the same permutation have the same rank variables which , in turn , can be identified with the corresponding permutation . bandt and pompe @xcite defined the _  permutation entropy of order _ @xmath15 as is used instead @xmath22 because @xmath23 contributes nothing to the entropy .",
    "this choice is , of course , inconsequential when @xmath24 , but it is preferable for numerical simulations and the applications we discuss in the last section . ]",
    "@xmath25with @xmath26 being the probability of observing any particular permutation given a block of observables . in direct analogy to the shannon entropy rate , the _ permutation entropy rate _ at resolution @xmath3 is hence defined as ( following the notation of bandt ) @xmath27    for _ deterministic _ maps @xmath28 of a proper interval @xmath29 with a finite number of monotony segments , bandt , keller and pompe @xcite analytically and numerically investigated a permutation entropy rate we denote by @xmath30 , based on the entropy of certain partitions , proving that it exists and , in fact , equals the metric ( kolmogorov - sinai ) entropy rate @xmath31 .",
    "they also prove this equality for the topological versions of permutation and ordinary entropy rates .",
    "relative changes in @xmath32 estimated numerically from time - series from the logistic map tended to track very well , over a wide range of varying nonlinearity parameter , the behavior of @xmath4 ( estimated from the positive lyapunov exponent of the map directly ) .",
    "there remained a substantial bias , though it was nearly constant over parameters .",
    "the correspondence observed in @xcite between permutation entropy and metric entropy rates of time series is not coincidental , nor restricted to one - dimensional dynamics . under only the assumption of ergodicity",
    ", we show that the permutation entropy rate of stationary , finite - alphabet random processes equals the metric entropy rate .",
    "a similar result follows for the permutation and metric _ differential _ entropy rates of non - discrete sources . with these results on stochastic processes in the hand ,",
    "we further show that for _ ergodic _ maps on @xmath33-dimensional intervals @xmath34 the two entropy rates are also equal . in doing so",
    ", we define the permutation entropy rate as @xmath35 , where @xmath36 stands now for the ` simple observations ' of @xmath28 supplied by a discretization of @xmath34 with resolution @xmath3 a finite - state stochastic process .",
    "the generality of all these results gives a strong support to our approach , which provides a unified treatment for stochastic and deterministic dynamical systems .",
    "this paper is organized as follows .",
    "for the reader s convenience we review in sect .",
    "ii the theoretical background and fix the notation . sect .",
    "iii contains one of the main results of this paper , namely , @xmath37 for _ ergodic _ finite - alphabet stochastic processes ( theorem 1 ) .",
    "this result is generalized in sect .",
    "iv to non - discrete ergodic information sources using the differential entropy rate ( theorem 2 ) and , in sect .",
    "v , to maps on @xmath33-dimensional intervals ( theorem 3 ) .",
    "we also mention in sect .",
    "iii that @xmath38 for _ non - ergodic _ finite - alphabet sources ; the proof can be found in appendix b. sect .",
    "v contains the main result on finite - dimensional maps , and sect .",
    "vi , a discussion of the two definitions of permutation entropy . finally , in sect",
    ". vii we show some numerical examples and discuss open practical issues in using permutation entropies in time - series analysis .",
    "let @xmath39 , @xmath40 the product sigma - algebra of @xmath41 generated by the borel sets of @xmath42 , and @xmath43 the ( left ) shift transformation on @xmath44 , @xmath45 .",
    "let @xmath46 be a probability space , i.e. , @xmath47 is a nonempty set , @xmath48 is a sigma - algebra of subsets of @xmath47 and @xmath49 is a ( positive ) measure on @xmath50 .",
    "any _ stationary _ stochastic ( or random ) process in discrete time @xmath51 on the probability space @xmath46 with values in @xmath42 corresponds in a standard way to the shift dynamical system @xmath52 via the map @xmath53 defined by @xmath54 , @xmath55 .",
    "the probability measure @xmath56 is defined on the borel sets @xmath57 of @xmath44 by@xmath58(@xmath59 because @xmath60 is @xmath48-measurable for all @xmath0 ) and it is @xmath43-invariant ( i.e. , @xmath61 ) because of the stationarity of @xmath62 .",
    "the measure @xmath56 is sometimes called the induced probability measure or distribution on the space of possible outputs of the random process .",
    "moreover , if @xmath63 is the projection onto the @xmath0th component , @xmath64 ( or @xmath65 ) , then the ` sampling function ' @xmath66 has the same joint distributions on @xmath44 as @xmath67 on @xmath47 , i.e. , both processes are equivalent .",
    "any point @xmath68 of the _ state space _",
    "@xmath44 is a possible realization ( or ` sample path ' ) of the whole process .",
    "such one - sided random processes provide better models than the two - sided processes @xmath69 for physical information sources that must be turned on at some time and thus we will use both denominations interchangeably in this paper .",
    "we will also refer to the shift dynamical system @xmath70 as the ( sequence space ) _ model _ of the stochastic process or information source @xmath62 .",
    "sometimes @xmath71 is used instead of @xmath72 to number the random variables @xmath60 and their samples @xmath73 ( we do so in sect .",
    "models allow to focus on the random process itself as given by the probability distribution on their outputs , dispensing with a perhaps complicated underlying probability space .",
    "as usual , we will also identify @xmath60 with @xmath74    finite - state or finite - alphabet sources @xmath75 on @xmath46 , where @xmath76 with alphabet @xmath77 , are dealt with in a similar way to the previous , non - discrete sources and , as a matter of fact , most of the general setup , properties and observations above apply _ mutatis mutandis _ to this simpler case .",
    "the sequence space of the corresponding model is now @xmath78 , @xmath79 being endowed with the discrete topology ; let @xmath80 be the product sigma - algebra of @xmath81 generated by the elements of @xmath79 . since no confusion will arise , we continue denoting by @xmath82 the shift on @xmath81 , @xmath83 , and by @xmath56 the @xmath43-invariant measure on @xmath84 defined as the pushforward of @xmath49 by the map @xmath85 , @xmath86 . the finite order probability distribution of @xmath87 , @xmath88 , can be alternatively expressed by means of the probability distribution on the outputs of @xmath87 , @xmath89for any @xmath90 and @xmath91 .    in this paper",
    "we will consider mostly finite - alphabet sources , although these will also occasionally arise as discretizations or quantizations @xmath92 of sources @xmath62 taking values on a proper interval @xmath34 of @xmath42 ( @xmath93 in symbols ) endowed with lebesgue measure @xmath94 .",
    "formally , this means that there exists a ( usually , uniform ) partition @xmath95 of @xmath34 into a finite number of @xmath94-measurable subsets such that @xmath96 is the discrete random variable defined by@xmath97where @xmath98 is the common distribution function to all @xmath96 ( in case @xmath96 is a vector random variable , the inequality is understood component - wise ) , @xmath56 is the induced probability measure on the outputs and @xmath99 is the alphabet of @xmath100 .",
    "if @xmath101 has a density function @xmath102 ( formally , the radon - nykodim derivative of @xmath103 with respect to @xmath94 ) , then @xmath104 . distribution functions and densities of higher finite order are analogously defined . for @xmath105 , the ` discretization scale '  or ` resolution '  we referred to in the introduction , one can take any measure of the ` coarseness ' of @xmath106 , say , the largest diameter of its elements , also called the norm of @xmath106 , @xmath107 .",
    "let @xmath46 be a probability space and @xmath108 a @xmath49-preserving transformation , i.e. , @xmath109 for all @xmath110 .",
    "given the dynamical system @xmath111 and a finite partition @xmath112 of @xmath113 , the entropy of @xmath28 with respect to @xmath114 is defined as @xmath115where @xmath116 is the least common refinement of the partitions @xmath117 and @xmath118 for any finite partition @xmath119 .",
    "the metric or kolmogorov - sinai entropy rate of map @xmath120 is then defined as : @xmath121the convergence in ( [ hmu ] ) can be proved to be monotonically decreasing @xcite .",
    "assuming logarithms base 2 everywhere herein , @xmath122 has units of bits per symbol or time unit , if @xmath0 is interpreted as discrete time . by convention , @xmath123 . in an information - theoretical setting , @xmath124 represents the long - term average of the information gained per unit time with respect to a certain partition and @xmath122 the maximum information per unit time available from any stationary process generated by the source , typically equal to the sum of the positive lyapunov exponents by the pesin theorem . if there exists finite @xmath125 such that @xmath126 , then @xmath125 is called a generator , or generating partition , of @xmath28 .",
    "given a discrete alphabet source @xmath127 with model @xmath128 , the ( shannon ) entropy of the random variables @xmath129 is@xmath130where @xmath131 @xmath132 is the partition of @xmath81 consisting of the basic ` cylinder sets ' @xmath133 , @xmath134 . according to ( [ p = p]),@xmath135and , correspondingly , the _ entropy rate _ ( or uncertainty ) of the source is defined as @xmath136 since @xmath131 is a ( one - sided ) generator of @xmath137 , i.e. , @xmath138 in other words , the shannon entropy rate of @xmath87 is , by definition , the kolmogorov - sinai entropy rate of its sequence space model .",
    "this explains our using ` metric ' to refer to both concepts , independently of the random or deterministic nature of the system considered .",
    "sometimes we will also use the @xmath0__th order entropy _ _ of @xmath87 , @xmath139so that @xmath140 . in general",
    ", @xmath141 stands for the string @xmath142 .",
    "other dynamical , statistical or information - theoretical concepts like conditional entropy , mutual information , ergodicity , mixing properties , etc .",
    ", are also defined via the sequence space model .",
    "for example , @xmath143 is said to be _ ergodic _ if @xmath144 is ergodic , i.e. , for @xmath145 with @xmath146 , @xmath147 , there exists @xmath148 such that @xmath149 .    if , more generally , @xmath62 is a non - discrete scalar or vector source with outcomes on an interval @xmath93 , define its _ differential entropy rate _",
    "as@xmath150where @xmath36 is a uniform discretization of @xmath62 with resolution scale @xmath3 .",
    "the differential entropy shows how the average rate of information furnished by a quantization of resolution @xmath105 differs from @xmath151 when @xmath152 . if @xmath16 happens to have a density function @xmath153  for every @xmath154 , then @xmath155",
    "given a finite - alphabet source @xmath127 with model @xmath156 , each possible permutation of a block of length @xmath15 , e.g. , @xmath129 , can be indexed as a word of _ ranks _ , each an integer in successively larger alphabets .",
    "in particular , define for @xmath157 the rank variable @xmath158 , where , as usual , the @xmath106-function of a proposition is @xmath159 if it holds and @xmath160 otherwise . by definition , @xmath161 is a _ discrete _ random variable on @xmath47 with range @xmath162 and the sequence @xmath163 builds a discrete - time non - stationary process . then the permutation @xmath164 in ( [ hperemut ] ) can also be viewed as the word @xmath165 , the relation between both being one - to - one .",
    "the many - to - one relation between @xmath166 and @xmath167 is written as @xmath168 .",
    "for example , consider a source @xmath87 over the alphabet @xmath169 .",
    "suppose we observe the word @xmath170 .",
    "then , @xmath171 , ( of course other strings , e.g. , @xmath172 or @xmath173 , also map to @xmath174 ) and @xmath175 .",
    "the string @xmath176 could be counted as matching both the ordering @xmath177 and @xmath178 . by using ranks , by contrast",
    ", the measure associated with each word is unambiguously associated with one permutation , and the rest of our development follows this approach .",
    "the _ permutation entropy rate _ of @xmath87 is then defined as @xmath179alternatively to the definition ( [ hperemut ] ) , with @xmath180defined to be the _ permutation entropy of order _",
    "@xmath181 of @xmath182 .",
    "remember that the overbar notation @xmath183 means that the relevant factor of @xmath22 or @xmath184 has been included for the entropy of a block of length @xmath15 .",
    "let @xmath185 denote the set of permutations of @xmath186 for the time being .",
    "we say that the word @xmath166 is of type @xmath187 if @xmath168 defines the permutation @xmath188 .",
    "it follows @xmath189 .",
    "the cylinder sets @xmath190such that @xmath191 build a partition of @xmath81 with @xmath192 , @xmath193 for @xmath194 .",
    "therefore@xmath195that is , the permutation entropy is sensitive to the measures of non - trivial order relationships observed in a word , as the shannon entropy is sensitive to the measures of the different word values themselves .",
    "observe as a technical point for later reference that , if @xmath196then @xmath197 due to words @xmath198 with repeated letters : if @xmath199 for every @xmath200 , then @xmath201 if and only if @xmath202 .",
    "[ lemma1 ] given an ergodic information source @xmath87 , @xmath203for all @xmath204 .",
    "that is , given a sufficiently long tail of previously observed symbols , the later ranks can be predicted virtually as well as the symbols themselves .",
    "heuristically , this is because the distribution of rank variable @xmath205 for @xmath206 sufficiently large depends effectively on only the cumulative distribution function of the source , approximated by the normalized sum of @xmath207 . in turn",
    "this means that the information contained in @xmath205 is the same as the information in @xmath208 . the proof , and an elementary example , is given in appendix  [ sec : appendix1 ] .",
    "with lemma  [ lemma1 ] in hand , we turn to our first main result , the equality between permutation and metric entropy for finite - alphabet stochastic processes .    [",
    "theorem : finite - alphabet ] for finite - alphabet ergodic sources @xmath87 the permutation entropy rate exists and equals the metric entropy rate : @xmath209 .",
    "we prove inequalities in both directions .",
    "+ ( a ) @xmath210 . given @xmath166 ,",
    "the corresponding rank variables are uniquely determined via @xmath168 . by  @xcite ( ch 2 , exercise 5 ) , @xmath211 for any discrete random variable @xmath12 , so @xmath212 and thus @xmath213 .",
    "\\(b ) @xmath214 .",
    "there are several ways to prove this inequality .",
    "consider , for instance , @xmath215\\end{aligned}\\]]for any @xmath216 , where we have applied the chain rule for entropy . as @xmath217",
    "we apply the data processing inequality @xmath218 @xcite to all elements of the first term on the rhs : @xmath219.\\ ] ] by lemma  [ lemma1 ] , for any @xmath220 there is some @xmath221 such that @xmath222 for @xmath223 , so @xmath224 \\right .",
    "\\\\ & & \\,\\,\\,\\left .",
    "+ \\frac{1}{l}\\left [ h_{m}(r_{1}^{l^{\\ast } } ) -h_{m}(s_{1}^{l^{\\ast } } ) \\right ] -\\left ( \\frac{l - l^{\\ast } } { l}\\right ) \\varepsilon \\right ) \\\\ & = & h_{m}(\\mathbf{s})-\\varepsilon .\\end{aligned}\\]]the existence of the limit and equality follows from ( a ) and ( b ) .",
    "more generally , we can only show an inequality for _ non - ergodic _ cases , namely,@xmath225the proof of ( [ nonergodic ] ) uses the ergodic decomposition of the entropy rate and is given in appendix b.",
    "information sources can have also non - discrete alphabets , although their outcomes are only observable with a finite precision . in this case , it is well - known that shannon s entropy rate , defined as the limit over ever finer uniform quantizations of the source , diverges logarithmically with the quantization scale . in order to obtain a finite measure of the asymptotic behavior of such quantizations",
    ", one has to resort to the differential entropy rate ( [ diffentropy ] ) instead .",
    "it turns out that theorem  [ theorem : finite - alphabet ] can be extended to scalar and vector ergodic non - discrete sources if entropy is replaced by differential entropy .",
    "let @xmath226 be a scalar or vector ergodic source taking values on an interval @xmath93 , @xmath227 . in case @xmath228 ( vector sources ) , @xmath34 is supposed to be endowed with the product ( or lexicographical ) order : @xmath229 if @xmath230 for @xmath231 and @xmath232 ( other conventions are also possible ) . with the equality between permutation and metric entropy rates for ergodic finite - alphabet sources , we now consider the source @xmath233 _ uniformly _ discretized to an alphabet @xmath234 by means of a partition @xmath235 of @xmath34 with @xmath236 for @xmath237 , where @xmath94 is , as before , lebesgue measure .",
    "one can then define the ranks @xmath238 of blocks of discretized symbols @xmath16 in the known way : @xmath239 , @xmath240 .",
    "if @xmath241 is the sequence space model for @xmath242 , we define the permutation entropy rate at resolution @xmath3 as usual : @xmath243 .",
    "we can take now the limit @xmath244 and , analogously to ( [ diffentropy ] ) , define the _ differential permutation entropy rate _ of @xmath62 as , @xmath245this yields :    [ theorem : non - discrete ] suppose @xmath62 is an ergodic non - discrete source .",
    "then @xmath246 , that is , the differential permutation and metric entropy rates of @xmath62 are equal .    if @xmath247 is ergodic , so is @xmath248 . by theorem  [ theorem : finite - alphabet ] ,",
    "@xmath249 , so @xmath250where @xmath251 is the metric differential entropy rate of @xmath252 .",
    "in this section we will use our result on finite - alphabet stochastic processes to show that the equality between permutation and kolmogorov - sinai entropy rate applies to ergodic maps on finite - dimensional intervals .",
    "let @xmath34  be a proper interval of @xmath42 endowed with the sigma - algebra @xmath253 , the restriction of borel sigma - algebra of @xmath42 to @xmath34 , and let @xmath254 be a @xmath49-preserving transformation , with @xmath255 being a measure on @xmath256 . in order to define the permutation entropy of @xmath28 , we consider first product partitions @xmath257of",
    "@xmath34 into @xmath258 subintervals of lengths @xmath259 , @xmath260 , in each coordinate @xmath206 , defining @xmath261 .",
    "the intervals are lexicographically ordered in each dimension , i.e. , points in @xmath262 are smaller than points in @xmath263 and for the multiple dimensions a lexicographic order is defined , @xmath264 , so there is an order relation between all the @xmath265 partition elements , and we can enumerate them with a single index @xmath266$]:@xmath267    next define a collection of _ simple observations _ @xmath268 with respect to @xmath28 with precision @xmath269 : @xmath270 if @xmath271 , @xmath272 then @xmath273 is an ergodic stationary @xmath265-state random process or , equivalently , an ergodic source on @xmath274 with finite alphabet @xmath275 and output probability distribution @xmath276 , with @xmath277 , so that @xmath278 in fact , @xmath28 and the left shift @xmath43 on the sequences @xmath279 are conjugate .",
    "a simple implementation of @xmath273 for @xmath280 and @xmath281 is the following : @xmath282 with @xmath283 for @xmath237 .",
    "we see that using simple observations as a finite alphabet measurement with respect to @xmath28 provides a direct link between the entropies of @xmath273 and @xmath28 .",
    "accordingly , we define the _ permutation entropy rate of @xmath28 _ as @xmath284 provided the limit exists . with this definition , and theorem  [ theorem :",
    "finite - alphabet ] , we may prove the principal result on ergodic dynamical systems .    [",
    "theorem : dynamical - systems ] if @xmath254 is ergodic , then @xmath285 . in words",
    ", the permutation entropy rate of ergodic maps equals the metric entropy rate .",
    "if @xmath286 , the statement follows in general ( also for non - ergodic maps ) from ( [ nonergodic ] ) .",
    "if @xmath287 , we have ( see ( [ pr ] ) ) @xmath288on the other hand , @xmath289 by theorem  [ theorem : finite - alphabet ] ( since @xmath290 is ergodic with respect to the measure @xmath56 ) .    let @xmath125 denote the finite generating partition of @xmath28 that , according to krieger s theorem @xcite , must exist ( due to @xmath28 s ergodicity and finite metric entropy ) , so that @xmath291 .",
    "we claim that @xmath292and , hence , @xmath293    _ case 1_. suppose that the elements of @xmath125 are ( @xmath33-dimensional ) intervals or , more generally , that all elements of @xmath125 consist of a finite number of intervals . in either case , taking if necessary a refinement of @xmath125 ( thus , also a generator that we call @xmath125 as well ) so that @xmath125 becomes a product partition @xmath294 of @xmath34 , we deduce @xmath295 and the same is true for any further refinement of @xmath294 .    _",
    "case 2_. if , otherwise , some component of @xmath125 consists ( modulo @xmath160 ) of infinitely many intervals , we can define a sequence of ever finer partitions @xmath296 of @xmath34 that , after an hypothetical refinement can be assumed without restriction to be a product partition ( _ case 1 _ ) such that @xmath297 , the finite sigma - algebras generated by the @xmath298 , build an increasing sequence and @xmath299 ( @xmath300 ) . then @xmath301 @xcite .",
    "this proves our claim and the theorem .",
    "the original definition of bandt , keller and pompe ( bkp ) @xcite of the permutation entropy of maps on intervals @xmath302 involves partitions of the form@xmath303where @xmath187 , here the set of permutations of @xmath304 , @xmath181 .",
    "in fact , if @xmath28 is supposed to be piecewise monotone as in @xcite or just ergodic , as in our case , it is easy to show that@xmath305is a partition of @xmath306 ( except maybe for a set of points of measure zero ) .",
    "bkp define then the permutation entropy of order @xmath15 as @xmath307(compare to ( [ qpi ] ) ) and their permutation entropy rate of @xmath28 to be @xmath308provided the limit exists .",
    "they prove @xmath309 for piecewise monotone maps on intervals of @xmath310 , but in the more general case , ergodic maps it seems that only the inequality @xmath311formally similar to ( [ nonergodic])can be proved , which we have done in appendix c for ergodic maps on @xmath33-dimensional intervals .",
    "comparing such particular results to the generality of theorem  [ theorem : dynamical - systems ] , we may conclude that our definition ( [ defa ] ) of permutation entropy rate offers a substantial advantage .",
    "note that the central distinction , which makes our formulation easier and more natural , is that ( [ defa ] ) takes the limit of infinite long conditioning ( @xmath312 first , and the discrete limit ( @xmath152 ) last , similarly to kolmogorov - sinai entropy rate , and as opposed to  ( [ defb ] ) , where an explicit discretization was not taken .",
    "we conjecture that for non - pathological dynamical systems of the sort one might observe in nature the two formulations are equivalent , but there are likely to be some non - trivial technicalities involved in a rigorous analysis . for example",
    ", @xcite shows a 1-dimensional map with an infinite number of monotonicity intervals , where the _ topological _ entropy rate and the permutation version of the _ topological _ entropy rate ( i.e. , counting simply the number of distinct permutations with non - zero measure , and not weighting them by their measure ) are unequal : @xmath313 .",
    "as a by - product of our result , the practitioner of time - series analysis will find an alternative way to envision or , eventually , numerically estimate the entropy rate of real sources .",
    "it is worth reminding that the entropy of information sources can be measured by a variety of techniques that go beyond counting word statistics and comprise different definitions of ` complexities ' such as , for example , counting the patterns along a digital ( or digitalized ) data sequence @xcite .",
    "bandt and pompe refer , in @xcite , to the permutation entropy of time series as complexity . that the entropy rate can also be computed by counting permutations",
    "shows once again that it is a so general concept that can be captured with different and seemingly blunt approaches .     for @xmath314 length time series from the map ( red and black thin lines ) .",
    "the permutation entropy estimate tracks changes in the lyapunov exponent ( equal to the kolmogorov - sinai entropy rate where nonnegative ) well , with a nearly constant bias .",
    "periodic orbits give a finite permutation entropy , but the rate estimate would tend to zero given a sufficiently long word . ]     and strong et al  @xcite fitted estimate ( black points ) as a function of @xmath315 wherever @xmath316 .",
    "the scaling region ansatz yields lower bias at cost of increased variance . the block length and scaling region were chosen by hand , a significant limitation . ]",
    "we demonstrate numerical results on time series from the logistic map @xmath317 .",
    "figure 1 shows an estimate of the permutation entropy rate estimate on noise - free data as a function of @xmath79 , comparing the lyapunov exponent ( computed from the orbit knowing the equation of motion ) to the permutation entropy .",
    "to be precise , we are estimating @xmath318 with @xmath87 discretized from the logistic map iterated at the discretization of double - precision numerical representation , i.e. , @xmath182 is the output of a standard numerical iteration .",
    "the entropy estimator of the block ranks was the plug - in estimator ( substituting observed frequencies for probabilities ) plus the classical bias correction , first order in @xmath319 .",
    "the key unresolved issue in using permutation entropies for empirical data analysis is , as with standard shannon entropy rate estimation , balancing the tension between larger word lengths @xmath15 , to capture more dependencies , and the loss of sufficient sampling for good statistics in the ever larger discrete space .",
    "the finite @xmath15 performance and convergence rate and bias of any specific computational method are key issues when it comes to accurately estimating the entropy rate of a source from observed data .",
    "it is now appreciated that numerically estimating the shannon block entropy from finite data and , especially , the asymptotic entropy rate , can be surprisingly tricky @xcite .",
    "the theoretical definitions of entropy rate do not necessarily lead to good statistical methods , and superior alternatives have been developed over the many years since shannon .",
    "we believe that some of these ideas may similarly be applicable to the permutation entropy situation .",
    "figure  2 shows a very simple application of the part of the method of  @xcite , fitting an empirical asymptotic scaling @xmath320 for @xmath321 , comparing to the block estimate .",
    "this procedure shows a lower bias , but the specific choice of scaling region @xmath15 ( as with block entropy ) is a key empirical issue , and does not have a generally satisfactory resolution .     for @xmath322 length time series from the map ( blue , red and black thin lines ) , contaminated with uniform zero - mean observational noise of width 0.1 . here",
    ", the entropy of the underlying map is nearly obliterated by the effect of the noise . ]     for @xmath323 length time series from the map ( blue , red and black thin lines ) , contaminated with uniform zero - mean observational noise of width 0.1 , and discretized to @xmath324 . with this discretization",
    "the entropy estimate tracks the macroscopic entropy from the dynamics much better , though the bias is increased , as expected , since the entropy due to noise still has some effect . ]    also important for practical time - series analysis is the usual situation where observations of a predominantly deterministic source is contaminated with a small level of observational noise . here ,",
    "we recommend that the user _ fix _ some discretization level @xmath3 characteristic of the noise , and evaluate the permutation entropies via entropies of rank words evaluated from the discretized observables .",
    "figure 3 shows analysis of permutations on significantly noise - contaminated signals , with no explicit @xmath3 ( i.e. , it is the size of the numerical precision of the computations ) .",
    "the consequence is the permutation entropy is heavily dominated by the noise .",
    "figure  4 shows the restoration of monotonic scaling with @xmath325 when an explicit , finite @xmath326 is used to discretize the data before rank variables are computed .",
    "note that as computing ranks involves looking at the difference between noise contaminated variables , when the characteristic noise size is @xmath327 , as in this example , an appropriate discretization scale is @xmath328 .    for vector - valued sources",
    ", we applied lexicographic ordering and construction of outer product variables in the proof . for analyzing chaotic",
    "observed data , however , it may be acceptable to still use but one scalar projection , subject to the traditional caveats of time - delay embedology .",
    "we would expect that for appropriately mixing sources and generic observation functions , the kolmogorov - sinai entropy estimated through that scalar still equals the true value , and likewise so might permutation entropy rate .",
    "we have found that numerically this appears to work in practice . with a direct higher - dimensional product space ,",
    "the undersampling issue becomes even more difficult with increasing @xmath15 , hence using scalars , as in a time - delay embedding , may turn out to be a superior approach for observed time - series of higher - dimensional sources .",
    "* acknowledgments .",
    "* we thank domingo morales ( universidad miguel hernndez ) for proof - reading some parts of the paper .",
    "was partially supported by the spanish ministry of education and science , grant grupos 04/79 .",
    "m.k . and l.k .",
    "thanks nsf for partial support .",
    "proof of lemma  [ lemma1 ] given an ergodic information source @xmath87 , @xmath203for all @xmath204 .",
    "consider @xmath329 .",
    "for @xmath330 define the _ sample frequency _ of the letter @xmath331 in the word @xmath332 to be @xmath333with the help of @xmath334 we may express @xmath205 in terms of @xmath335 , @xmath336 , namely , @xmath337where we assume the outcomes @xmath332 to be known .",
    "then , the identity @xmath338give us the probability for observing some @xmath205 with value @xmath339 by means of @xmath340 , @xmath341 . since , given @xmath342 , @xmath205 is a deterministic function of the random variable @xmath208 , i.e. , @xmath343 , eq .",
    "( [ prr ] ) can be seen as an application of the law of total probability .    without loss of generality",
    ", we may first rearrange the sum in ( [ prr ] ) to consider only those symbol values @xmath344 with non - zero @xmath345 , summing to @xmath346 .",
    "expand the sum , @xmath347 \\\\ & & + \\pr \\left ( s_{k+1}=2\\right ) \\delta \\left [ y=(k+1)(\\vartheta _ { k+1}(1)+\\vartheta _ { k+1}(2))\\right ] \\\\ & & + \\ldots + \\pr \\left ( s_{k+1}=n^{\\prime } \\right ) \\\\ & & \\times \\delta \\left [ y=(k+1)(\\vartheta _ { k+1}(1)+\\ldots + \\vartheta _ { k+1}(n^{\\prime } ) ) \\right ] .\\end{aligned}\\]]suppose all the relevant sample frequencies @xmath348 are greater than zero .",
    "this means that for any @xmath349 , only a single one of the @xmath106-functions can be nonzero , and hence we have a one - to - one transformation taking non - zero elements from the distribution @xmath350 without change into some bin for @xmath351 . since entropy is invariant to a renaming of the bins , and",
    "the remaining zero probability bins add nothing to the entropy , we conclude that , if @xmath352 for all @xmath331 where the true probability @xmath353 ( i.e. , @xmath354 after a hypothetical rearrangement ) , then @xmath355 . because of the assumed ergodicity , we can make the probability that @xmath356 when @xmath357 to be arbitrarily small by taking @xmath206 to be sufficiently large , and the claim follows for @xmath358 .",
    "this construction can be extended without change to words @xmath359 of arbitrary length @xmath204 via @xmath360 .\\end{aligned}\\]]observe that if @xmath352 for @xmath361 , then the same happens with @xmath362, ... ,@xmath363 and @xmath364 follows .",
    "again , ergodicity guarantees that there exist realizations @xmath365 whose sample frequencies fulfill the said condition .    as way of illustration",
    ", suppose that @xmath366 are independent random variables with probability @xmath367 .",
    "given @xmath368 , set @xmath369 , @xmath370 . consider the case @xmath371 in lemma 1 .",
    "there are two possibilities :    \\(i ) @xmath370 .",
    "then@xmath372each of these events has the joint probability@xmath373and conditional probability@xmath374where @xmath375 and @xmath376 , @xmath377 , @xmath378 or @xmath379 .",
    "\\(ii ) @xmath380 .",
    "then@xmath381these events have the joint probabilities@xmath382and conditional probabilities@xmath383 from ( i ) and ( ii ) , we get @xmath384on the other hand ,",
    "@xmath385and so @xmath386 and @xmath387 are equal in the limit @xmath388 , as guaranteed by lemma 1 .",
    "in order to deal with the general , non - ergodic case , we appeal to the theorem on ergodic decompositions @xcite : if @xmath47 is a compact metrizable space and @xmath389 is continuous , then there is a partition of @xmath47 into @xmath28-invariant subsets @xmath390 , each equipped with a sigma - algebra @xmath391 and a probability measure @xmath392 , such that @xmath28 acts ergodically on each @xmath393 , the indexing set being another probability space @xmath394 ( in fact , a lebesgue space ) .",
    "furthermore,@xmath395the family @xmath396 is called the ergodic decomposition of @xmath397    if @xmath43 is the shift on the ( compact , metric ) sequence space @xmath398 , the indexing set can be taken to be itself , i.e.,@xmath399where @xmath400 @xcite .",
    "this result shows that any source which is not ergodic can be represented as a mixture of ergodic subsources .",
    "the next lemma states that such a decomposition holds also for the entropy rate .",
    "[ lemma : ergodic - decomposition ] let @xmath144 be the sequence space model of a stationary finite alphabet source @xmath127 .",
    "let @xmath401 be the ergodic decomposition of @xmath56 .",
    "if @xmath402 is @xmath56-integrable , then @xmath403    [ theorem : non - ergodic ] under the assumptions of lemma  [ lemma : ergodic - decomposition ] , @xmath404 holds for any finite alphabet source @xmath87 .    fix @xmath181 . from ( [ qpi ] ) and ( [ decomp ] ) , @xmath405where in ( [ jensen ] )",
    "we have used jensen s inequality,@xmath406with @xmath407 convex in @xmath408 and @xmath409 .",
    "therefore ,    @xmath410    where we have applied fatou s lemma in ( [ fatou ] ) to the sequence of positive and @xmath56-measurable functions @xmath411 .",
    "observe that @xmath412 exists for all @xmath413 ( and is @xmath56-integrable as a function of @xmath414 ) since @xmath415 by theorem 1 ( @xmath87 acts ergodically on @xmath416 ) .",
    "therefore,@xmath417by ( [ ergodicdecomp ] ) .    theorem  [ theorem : non - ergodic ] and eqs .",
    "( [ fatou-2 ] ) and ( [ fatou+1 ] ) yield :    if @xmath418 exists for a non - ergodic finite - alphabet source @xmath87 , then @xmath419 and @xmath420 .",
    "suppose first that @xmath306 is a one - dimensional interval and @xmath421 an ergodic and @xmath49-preserving transformation , where @xmath49 is a measure on @xmath422 @xmath40 being borel sigma - algebra of @xmath423 .",
    "_ case 1_. suppose that the elements of @xmath125 are connected sets ( intervals ) or , more generally , that all elements of @xmath125 consist of a finite number of intervals . in either case",
    ", taking if necessary a refinement of @xmath125 ( thus , also a generator ) that we call @xmath125 as well , we write without restriction @xmath428 , were @xmath429 are intervals .",
    "this being the case , let @xmath430 be the points that subdivide the interval @xmath431 $ ] into the @xmath432 intervals @xmath433 of the generator @xmath125 .",
    "we consider a fixed @xmath434 and show that it can intersect at most @xmath435 sets of the partition @xmath436 with @xmath437 . for @xmath438 , let @xmath439 $ ] denote the set in @xmath440 that contains @xmath68 .",
    "thus , @xmath441 $ ] can be written as @xmath442 with @xmath443 , so that it can be specified by the @xmath0-tuple @xmath444=(j_{0}, ... ,j_{l-1})\\in \\{1, ... ,\\left\\vert \\gamma \\right\\vert \\}^{l}$ ]",
    ".    now , @xmath188 is given by inequalities @xmath445 with @xmath446 and @xmath447 .",
    "for each @xmath448 we can extend these inequalities so that they give the common order of the @xmath449 and the @xmath450 , where @xmath451 and @xmath452 @xmath15 .",
    "it follows that there are at most @xmath435 possible extended orders since each @xmath449 has @xmath453 possible bins to go among the @xmath450 ( as @xmath454 varies in @xmath455 , the @xmath15 points @xmath450 defining the bins move but do not cross each other ) .",
    "moreover , when we know the common order of the @xmath449 and @xmath450 , then @xmath444 $ ] is uniquely determined ( since @xmath456 , implies @xmath457 and thus @xmath458 with @xmath459 , @xmath460 and @xmath461 ) .",
    "each @xmath434 is then the union of at most @xmath462 sets @xmath463 @xmath464 with total measure @xmath465 .",
    "hence,@xmath466and therefore , summing over all @xmath187,@xmath467it follows @xmath468\\]]and@xmath469since @xmath125 is a generator of @xmath28 .",
    "definition ( [ hstar ] ) completes the proof in this case .    _",
    "case 2_. if some component of @xmath125 consists of infinitely many intervals , we can define a sequence of interval partitions @xmath470 ( _ case 1 _ ) such that @xmath471 , the finite sigma - algebras generated by the @xmath472 , build an increasing sequence and @xmath473 ( @xmath300 )",
    ". then @xmath474 @xcite .",
    "we claim that , also in this case , eq .",
    "( [ hmu(f ) ] ) holds .",
    "otherwise , for every @xmath220 and for every @xmath181 , there exists @xmath475 such that @xmath476take now @xmath477 such that @xmath478 for all @xmath479 . from ( [ h - eps ] ) it follows@xmath480because @xmath481 decreases monotonically to @xmath482 .",
    "use now ( [ case1 ] ) to deduce@xmath483but the last term can be made arbitrarily small because the @xmath484 fulfilling ( [ h - eps ] ) form an unbounded subsequence and @xmath477 is independent of @xmath484 .",
    "this contradiction proves our claim and completes the proof .",
    "let @xmath28 be an ergodic interval map in @xmath42 fulfilling the above assumptions .",
    "if @xmath424 , then @xmath485 @xmath486 , where the permutation entropy is defined by means of the product order of @xmath42 .",
    "proof outline as in lemma  [ lemma : interval - inequality ] , we split again its proof in two cases . if ( _ case 1 _ ) the generating partition is a product partition or can be refined to a product partition@xmath487\\times ... \\times \\lbrack a_{d}^{(i)},b_{d}^{(i)}],\\]](whose elements are , without restriction , lexicographically ordered ) , then the same approach used for one - dimensional intervals works through to eq .",
    "( [ hmu(f ) ] ) .",
    "otherwise ( _ case 2 _ ) , each element of @xmath125 is the countable union of disjoint intervals .",
    "they allow to define ( after an eventual refinement ) a sequence of product partitions @xmath488 ( _ case 1 _ ) such that @xmath489 the proof that @xmath490 @xmath491 is then completed again by contradiction .",
    "kontoyiannis i , algoet ph , suhov ym and wyner aj 1998 nonparametric entropy estimation for stationary processes and random fields , with applications to english text .",
    "_ ieee trans .",
    "inform . theory _ * 44 * 1319 - 1327 ."
  ],
  "abstract_text": [
    "<S> permutation entropy quantifies the diversity of possible orderings of the values a random or deterministic system can take , as shannon entropy quantifies the diversity of values . </S>",
    "<S> we show that the metric and permutation entropy rates  measures of new disorder per new observed value  are equal for ergodic finite - alphabet information sources ( discrete - time stationary stochastic processes ) . with this result </S>",
    "<S> , we then prove that the same holds for deterministic dynamical systems defined by ergodic maps on @xmath0-dimensional intervals . </S>",
    "<S> this result generalizes a previous one for piecewise monotone interval maps on the real line ( bandt , keller and pompe , entropy of interval maps via permutations ,  _ nonlinearity _ * 15 * , 1595 - 602 , ( 2002 ) ) , at the expense of requiring ergodicity and using a definition of permutation entropy rate differing in the order of two limits . </S>",
    "<S> the case of non - ergodic finite - alphabet sources is also studied and an inequality developed . </S>",
    "<S> finally , the equality of permutation and metric entropy rates is extended to ergodic non - discrete information sources when entropy is replaced by differential entropy in the usual way .    </S>",
    "<S> pacs : 02.50.ey , 05.45.vx , 89.70.+c </S>"
  ]
}