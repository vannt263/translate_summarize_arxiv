{
  "article_text": [
    "in this paper , we describe the implementation and performance of the multiplication of sparse matrix and vector ( hereafter the spmv multiplication ) on the pezy - sc processor .",
    "in particular , we focus on the effect of various data compression schemes on the performance .",
    "the multiplication of sparse matrix and vector is the most time consuming part of many real applications which use irregular grids .",
    "the best - known example is the fem ( finite element method ) for structural analysis and many other cae applications .",
    "irregular grids are essential to allow the analysis of objects with complex shapes . though it is not impossible to apply regular grids to complex shapes ,",
    "generally irregular grids offer more accurate results with much smaller number of freedoms .",
    "however , it has become very difficult to achieve even reasonable efficiency on the spmv multiplication on modern hpc systems .",
    "there are two main reasons for this difficulty .",
    "the first one is the memory bandwidth .",
    "consider the multiplication of matrix @xmath0 and vector @xmath1 , @xmath2 for real applications , the matrix @xmath0 is too large to fit to the cache memory . on the other hand , vectors @xmath1 and @xmath3",
    "are much smaller , and there is always the possibility of extensive data reuse for them .",
    "thus , the dominant part of memory access for an spmv operation is the reading of the ( sparse ) matrix @xmath0 .",
    "the exact data size of the matrix @xmath0 depends on the used data format , but it can not be smaller than the number of non - zero elements of @xmath0 .",
    "the number of floating - point operation per one non - zero element of @xmath0 is two .",
    "thus , if the data format is used is the double - precision format , memory read of eight bytes takes place for every two floating point operations . in other words ,",
    "the `` required '' b / f ( byte per flops ) number is @xmath4 .",
    "note that here we ignored the memory read for the indices .",
    "if we use the ell format , which usually is the most efficient format for storing the matrix in fem applications , required bandwidth can increase by 50 - 100% .",
    "thus , the required memory bandwidth , in terms of the b / f number can be between six and eight .    in the 1980s , vector supercomputers had the memory subsystem which could support at least a fair fraction of the the memory bandwidth requirement of the spmv operation , since the hardware b / f number of vector machines in 1980s ranged between 4 and 12 . here ,",
    "the hardware b / f number of a machine is defined as the theoretical ( or measured ) memory bandwidth in bytes / second divided by the theoretical peak performance of the floating - point operation measured in the number of floating - point operations per second .",
    "vector machines in 1980s had sufficient memory bandwidth to keep the floating - point unit busy for spmv multiplications .    however , the b / f numbers of microprocessors used for modern hpc systems are much smaller . for example , the b / f number of k computer is 0.5 , and this is rather exceptionally high in today s standard .",
    "recent xeon - based systems have the b / f numbers of around 0.2 . if the required b / f number is six , this means that the theoretical maximum efficiency of modern hpc systems would be around or less than 5% .",
    "in fact , the ratio between the measured hpl performance and measured hpcg performance of machines in the june 2016 top 10 list of hpcg benchmark ranges between 0.4 and 5% , and the numbers of xeon - based systems are 2 - 3% .",
    "thus , the low memory bandwidth of modern hpc systems is clearly the primary reason for the very low efficiency of spmv multiplication on them .",
    "the second reason is the tendency of designers of modern to adopt simd arithmetic unit with rather wide width ( 4 - 8 words ) .",
    "the computing kernel of spmv operation , for irregular matrices , requires the indirect access of the elements of either the vector or the matrix .",
    "the performance of indirect access on modern processors with wide simd units are very low .",
    "some processors do not support simd operations for indirect memory access .",
    "even on machines with simd instructions for indirect memory access , their throughput is much lower than simple simd load / store instructions . on some machines this inefficiency can cause further degradation of performance of spmv operations .",
    "one way to reduce the required memory read is the element - by - element ( ebe ) method . in the ebe method ,",
    "the sparse matrix @xmath0 is constructed from the original physical and topological data of each element on the fly .",
    "since the total amount of data for all elements is significantly smaller than the size of the generated matrix , we can reduce the amount of memory access .",
    "even though the calculation cost of on - the - fly construction of the matrix is fairly high , the total computing time can be significantly reduced by moving to the ebe method .",
    "ebe method has become widely used in many fem applications , since it can achieve quite significant improvement in the actual speed of calculation , even though the calculation cost is increased .",
    "another way which can potentially be useful in reducing the required amount of memory access is to compress the matrix using some data compression algorithm . however , even though there are many research papers on the use of data compression in hpc applications , there seems to be little work on the application of data compression to spmv multiplications .",
    "one possible reason is that in order to achieve actual speedup , data decompression algorithm must be extremely efficient , since the number of floating - point operations per one matrix element is only two .",
    "if the decompression algorithm requires more than a few instructions , it will cause quite significant increase of the total cost .",
    "moreover , generally the decompression algorithm requires some table - lookup operations , in other words , the indirect memory access for which modern microprocessors are not particularly efficient .    on the other hand ,",
    "if the hardware b / f number of a system is extremely low , we might be able to achieve significant performance improvement on spmv multiplications by using data compression / decompression .    in this paper",
    ", we report the performance of the zettascaler-1.5 supercomputer@xcite on the spmv part of hpcg benchmark@xcite , with and without the use of data compression / decompression .    the zettascaler ( previously called exascaler ) system is based on the first - generation pezy - sc 1024-core processor chip .",
    "it appeared in the top500 list of november 2014 and ranked # 2 in the green500 list . in the june 2015 green500 list ,",
    "three exascaler systems occupied top three ranks .",
    "the system listed # 1 achieved the performance per watt exceeding 7 gflops / w , significantly higher than the number of the # 1 system for november 2014 green500 list . as of june 2016 , it still keeps the # 1 position in the green500 list .",
    "the reason why we used the zettascaler system as the testbed for the data compression algorithm is that its hardware b / f number is rather low , around 0.05 .",
    "thus , it is ideal as the testbed of algorithms which will be useful for processors in the near future .",
    "in addition , its processor cores do not have simd units .",
    "thus , we might be able to achieve pretty good speedup for spmv multiplication using data compression .",
    "the zettascaler system is rather similar to modern gpgpu - based systems , in which the gpgpus are connected to intel xeon processors through pcie interface , and xeon processors are connected using infiniband network .",
    "there are , however , two unique features of the zettascaler system .",
    "the first one is of course the use of pezy - sc processor chip , which is a 1024-core mimd processor with physically shared memory and hierarchical cache .",
    "it was developed by a japanese venture company , pezy computing .",
    "the second feature is the immersion cooling system in which fluorocarbon ( 3 m fluorinert fc-43 ) is used .",
    "for the zettascaler system , they designed motherboards for xeon processors and processor cards for pezy - sc processors to achieve high - density packing .",
    "the use of immersion cooling has the potential advantage of reducing the pue number and also reducing the junction temperature of the processor chips , resulting in more energy efficient operation .",
    "however , the primary reason of the high performance - per - watt number of the zettascaler system is the design of the pezy - sc processor itself .",
    "the pezy - sc processor integrates 1024 mimd cores , each with fully pipelined double - precision multiply - and - add ( mad ) unit , into a die of size 400 mm@xmath5 , using tsmc s 28hpm process .",
    "its nominal power consumption is only 65 w for the operation with 733 mhz clock .",
    "at least for the hpl benchmark , or more specifically the dgemm operation ( double precision dense matrix multiplication ) , the pezy - sc processor has achieved quite impressive performance per watt , even though the efficiency compared to the theoretical peak performance is still rather low ( slightly better than 50% for hpl ) . on the other hand",
    ", the porting of applications could be relatively easy , since the pezy - sc processor is an mimd manycore processor with hierarchical ( but non - coherent ) cache and physically shared memory .",
    "also , a fairly well designed subset of opencl , pzcl , is supported .",
    "in this paper we first present the performance of hpcg on pezy - sc processor , with usual optimizations applied in previous works .",
    "then we proceed to present the performance of `` optimized '' implementations of spmv operation with on - the - fly data compression and decompression .",
    "this paper is organized as follows .",
    "first , in section [ sect : pezy_sc_description ] , we present the overview of the pezy - sc processor and the zettascaler system .",
    "in section [ sect : hpcg_on_pezy_sc ] , we describe our implementation of hpcg on pezy - sc processor . in section [",
    "sect : hpcg_result ] , we present the performance result . finally , in section [ sect : spmv_tuning ] , we present data compression / decompression algorithms we implemented and its measured performance on pezy - sc . in section [ sect : discussion ] , we summarize the paper and discuss the future directions for research and development .",
    "in this section , we overview the architecture of the pezy - sc processor and the zettascaler system . in subsection [ sect : pezy_sc_processor ] , we give a brief overview of the pezy - sc processor . in subsection",
    "[ sect : zettascaler ] , we give a brief overview of the zettascaler system .",
    "the pezy - sc processor@xcite integrates 1024 mimd cores with three levels of cache memory . in this section",
    ", we describe the structure bottom - up , starting from the processor core .",
    "each of the pezy - sc core can do one double - precision mad operation or two single - precision mad operations per cycle .",
    "it has the usual load - store architecture .",
    "the core is a quite simple dual - issue , in - order core with four - way ( can be eight - way ) smt .",
    "thus , the impact of the latency of data caches to the performance is relatively small , even though the in - order core is used .",
    "one rather unusual feature of the pezy - sc processor core is that each core has 16 kb of the local memory , accessible only by that core .",
    "it has the separate local address space , and can be used to store the data which is repeatedly used by the core . since this local memory provides the largest on - chip storage ( 16 mb in total ) with very high bandwidth , it is essential to take advantage of this local storage to achieve high efficiency , in particular for compute - intensive applications .",
    "for example , there is well - known tradeoff between the required memory bandwidth and required on - chip storage , for the performance of the dgemm operation .",
    "thus , the use of this local memory is essential to achieve high efficiency for dgemm .",
    "two cores share one l1d cache of 2 kb .",
    "the size of l2d cache is 64 kb and one l2d cache is shared by 16 cores . in the pezy terminology ,",
    "cores that share the l2d cache form a `` city '' .",
    "the size of the l3d cache is 2 mb , and 16 `` cities '' share one l3d cache , to form a `` prefecture '' .",
    "thus , each prefecture consists of 256 cores , and the one pezy - sc chip consists of four prefectures .",
    "finally , the chip is connected to eight channels of either ddr3 or ddr4 drams .    thus , the pezy - sc processor has three levels of cache memories .",
    "these cache memories are _ not _ coherent .",
    "cores which share the same l2d cache can read the data written by other cores only after explicit flush operation and barrier synchronization , and the same is true for the l3d cache and the main memory .",
    "both the flush and barrier synchronization instructions are provided to each levels of cache . to be precise",
    ", the barrier instruction is available also for one core ( multiple threads ) .",
    "clearly , this removal of the cache coherency has greatly simplified the processor design , and made it possible to construct a 1024-core processor with three levels of cache .",
    "the line sizes of the l1 , l2 , and l3 data caches are 64 , 256 , and 1024 bytes . by changing the line size",
    ", the designers of the pezy - sc processor kept the bandwidth of l2 and l3 data caches very high .",
    "the bandwidth of l2d cache is the same as that for l1d , and l3d offers around half of them .    from the application programmer s point of view",
    ", the lack of the cache coherency does not seem to pose severe limitation , as far as hpc applications ( or their computing kernels ) are concerned .",
    "for many applications , programmers know at which moments processors need to communicate .",
    "moreover , they try to minimize the communication between cores to achieve high efficiency .",
    "thus , from the point of view of tuning , the lack of the cache coherency can be regarded as the ability of the programmer to control the traffic between cores . moreover , the barrier synchronization is almost always necessary before the communication , since otherwise what core a expects that core b has updated might not be actually updated yet .",
    "efficient hardware - supported flush and barrier synchronization is thus quite useful .    the cache for instruction is also multi - level . for i caches ,",
    "the line size and the bandwidth are essentially the same for all levels . as far as all cores",
    "run the same and relatively small kernels , this structure works fine .",
    "each pezy - sc chip has 32 lanes of pcie ( gen3 ) interfaces , which is controlled by integrated two arm 926 processors .",
    "pcie interfaces can be used to transfer the data between the main memory of pezy - sc processor and the host processor , either by dma or pio read / write of the host processor .",
    "pezy - sc processor supports a language called pzcl , a dialect of opencl .",
    "it supports most of the features of opencl , but there are some limitations in particular when the performance is important ( which is of course almost always the case ) .",
    "the number of software threads created should be _ same _ as the maximum number of hardware threads ( 8192 per chip ) to achieve best efficiency .",
    "another difference comes from the fact that the cache is not coherent .",
    "functions to flush appropriate levels of cache should be inserted manually to guarantee the correct result . for small computing kernels ,",
    "this is not too difficult , but of course can be a source of hard - to fix bugs .    as one pezy - sc processor has eight channels of ddr4 drams , the theoretical peak memory bandwidth is 85gb / s when the ddr4 clock is 1333 mhz .",
    "the actual read bandwidth is around 75 gb / s , and stream copy performance is 40 gb / s .",
    "the copy performance is low because the write bandwidth is 1/2 of the read bandwidth .",
    "the read bandwidth of l1 , l2 and l3 caches ( chip total ) are 2000 , 2000 , and 700 gb / s , respectively .",
    "the current generation of the zettascaler system ( zettascaler-1.5 ) consists of multiple computing nodes , each of which consists of one xeon ( e5-v3 ) processor and four pezy - sc processors .",
    "the xeon processor is mounted to a specially - designed motherboard , and pezy - sc processors are mounted to also specially - designed module boards .",
    "the connection between the host xeon processor and one pezy - sc processor is an 8-lane gen3 pcie channel .",
    "the network between computing nodes is a standard fdr infiniband .",
    "the largest existing configuration of zettascaler system is a 320-node system called `` shoubu '' , installed at riken accc .",
    "smaller systems are installed at kek as well as riken aics .",
    "a very unique feature of the zettascaler system is the use of immersion cooling with fluorocarbon ( 3 m fluorinert fc-43 ) coolant .",
    "compared to previously used oil - based coolant , fluorocarbon coolant has several advantages like the ease of handling , safety ( it is nonflammable ) , and smaller coefficient of thermal expansion .",
    "the major disadvantages are the price and potential greenhouse effect , though the latter is not so severe because of the high vaporization temperature of the particular coolant actually used .",
    "in this section , we briefly describe the hpcg benchmark itself and our reference implementation on the pezy - sc processor . in subsection [ sect : hpcg ] , we describe the hpcg benchmark and in subsection [ sect : implementation_hpcg_on_pezy_sc ] , our implementation of hpcg on pezy - sc .      as we ve already discussed in section [ sect : introduction ] , the hpcg benchmark @xcite is , according to its designers , `` designed to measure performance that is representative of many important scientific calculations , with low computation - to - data - access ratios . '' as such , it mimics the major operations of fem using the cg with multigrid solver , on irregular grid .",
    "unfortunately , the currently available official specification of hpcg @xcite is rather old , and the algorithm described there and what is used in the current benchmark code are quite different . in the following , we first follow @xcite and then summarize the changes made .    from the mathematical point of view , the problem solved in hpcg is a 3d diffusion equation discretized using 27-point stencil on a regular grid of size @xmath6 , where @xmath7 is the size of the grid on each mpi process and @xmath8 is the mpi process grid .",
    "thus , the total number of the mpi processes is @xmath9 .    in the original specification",
    ", hpcg solves this problem using the symmetric gauss ",
    "seidel preconditioned cg iteration , and the users are not allowed to change this basic cg algorithm . in particular , the multigrid method , which is essential if one wants to solve large 3d problems , is not included .",
    "thus , not surprisingly , this is changed in the current specification .",
    "four - stage v - cycle geometric multigrid preconditioner is used .",
    "what is measured in the hpcg benchmark is the weighted average of the computing speed of major operations , in particular symgs , spmv , restriction , prolongation , dotproduct , and waxpby .",
    "usually , two functions , computespmv and computesymgs , dominate the total computing time and thus determine the performance .",
    "our reference implementation of hpcg on pezy - sc is pretty straightforward .",
    "the following six procedures are ported to pezy - sc ( rewritten using the pzcl language ) :    * symgs * spmv * restriction * prolongation * dotproduct * waxpby    both the matrix data and vector data are kept on the memory of pezy - sc .",
    "therefore , only a small amount of data to be transferred for convergence check and other operations and the boundary data to be exchanged between nodes are exchanged between the host xeon processor and the pezy - sc processors .",
    "the rewrite using pzcl is pretty straightforward .",
    "as noted earlier , the main point currently we need to care is that the total number of threads should be actually equal to the available number of hardware threads .",
    "since the changes which directly take advantage of the regular structure of the grid are not allowed in the optimization phase , algebraic block multicolor ordering@xcite is used for the symgs part .",
    "table [ tab : cgiteration ] shows the operations performed in one cg iteration . since 4-level v - cycle multigrid method is used , symgs routine is called seven times per iteration , and spmv four times .    .",
    "operations and communication during one cg iteration .",
    "p and x indicate pezy - sc and xeon , @xmath10 the direction vector , and @xmath11 the preconditioned residual vector , respectively . [ cols=\"^,<,^,^,^,^\",options=\"header \" , ]     so far , we have actually implemented the first two compression schemes .",
    "table [ tab : compression ] show the resulting performance . the first approach of compressing the data array only should theoretically give around a factor of three speedup , and actually realized the speedup by 50% .",
    "the second one , in which both the index array and the data array are compressed , should theoretically give around a factor of 25 speedup , and actually achieved the speedup by a factor of 2.8 .",
    "the reason why the actual speedup is much smaller than the theoretical limit is simply that to estimate the theoretical limit we ignore the access cost of the input vector , which is currently accessed indirectly with rather large address offsets in the innermost loop . with reordering of the vector and matrix we might be able to improve the performance further",
    ".    list [ code : samplecode ] shows the conceptual code for data and index compression .",
    "the fact indices and data are compressed means that their actual values are obtained by table lookup operations .",
    "thus , on modern microprocessors with wide simd instruction sets , it would be difficult to achieve reasonable performance with the compression algorithm , since the throughput of indirect access operations are generally low .",
    "the fully - mimd , non - simd nature of the pezy - sc processor is critical to achieve the actual speedup .    ....",
    "for(int i = 0 ; i < n ; i++ ) {      y[i ] = 0 ;      const int type = columndifftype[i ] ;      int idx = 0 ;      for(int valueidx = 0 ; valueidx < valuecount ;          valueidx++ ) {          const double a_ij = value[valueidx ] ;          for(;idx < valueidxend[i][valueidx ] ; idx++ ) {              const int j = i + columndiff[type][idx ] ;              const double x_j = x[j ] ;              y[i ] + = a_ij * x_j ;          }      } } ....    note , however , that the problem here is the number of independent memory access per cycle , and not the difference between simd and mimd architecture .",
    "the gather / scatter functions of modern simd microprocessors are clearly still in their infancy , and might be improved in the future .",
    "in this paper , we report the effect of data compression / decompression algorithms for the spmv multiplication on the zettascaler system , in which the 1024-core , mimd ultra - many - core pezy - sc processors are used as accelerators .",
    "we have used the matrix generated by hpcg benchmark code as the example . usually , the performance of the well - optimized implementation of hpcg is limited by the bandwidth of the sequential read access of the external memory of the processor ( or accelerator ) . in the case of a pezy - sc processor ,",
    "the theoretical limit of the read bandwidth is 85 gb / s , and actual measured bandwidth is 75 gb / s .",
    "thus , the performance of spmv and symgs operations are limited to around 10 gflops .",
    "the actual performance achieved is close to this number .",
    "the theoretical speedup by data and index compression is as large as a factor of 25 .",
    "we actually achieved the speedup of a factor of 2.8 for the spmv operation .",
    "even though the actual achieved improvement is much smaller than the theoretical maximum , we have demonstrated that the used of data compression / decompression can actually improve the performance of spmv multiplication on the pezy - sc processor .",
    "we therefore conclude that the use of data compression / decompression will be quite useful technique to improve the performance of spmv operations in fem applications on the current and future high - performance processors .",
    "1=1    the authors would like to thank people in pezy computing / exascaler for their invaluable help in solving many problems we encountered while porting and tuning hpcg .",
    "part of the research covered in this paper research was funded by mext s program for the development and improvement for the next generation ultra high - speed computer system , under its subsidies for operating the specific advanced large research facilities .",
    "x.  zhang , c.  yang , f.  liu , y.  liu , and y.  lu , _ algorithms and architectures for parallel processing : 14th international conference , ica3pp 2014 , dalian , china , august 24 - 27 , 2014 .",
    "proceedings , part i_.1em plus 0.5em minus 0.4emcham : springer international publishing , 2014 , ch . optimizing and scaling hpcg on tianhe-2 : early experience , pp .",
    "[ online ] .",
    "available : http://dx.doi.org/10.1007/978-3-319-11197-1_3    j.  park , m.  smelyanskiy , k.  vaidyanathan , a.  heinecke , d.  d. kalamkar , x.  liu , m.  m.  a. patwary , y.  lu , and p.  dubey , `` efficient shared - memory implementation of high - performance conjugate gradient benchmark and its application to unstructured matrices , '' in _ proceedings of the international conference for high performance computing , networking , storage and analysis _ ,",
    "ser . sc 14.1em plus 0.5em minus 0.4em piscataway , nj , usa : ieee press , 2014 , pp . 945955 .",
    "[ online ] .",
    "available : http://dx.doi.org/10.1109/sc.2014.82    j.  j. dongarra , a.  heroux , michael , and p.  luszczek , `` hpcg benchmark : a new metric for ranking high performance computing systems , '' department of electrical engineering and computer science , university of tennessee system , tech .",
    "15 - 736 , 2015 .",
    "[ online ] .",
    "available : http://www.eecs.utk.edu/resources/library/594    a.  heroux , michael , j.  j. dongarra , and p.  luszczek , `` hpcg technical specification , '' sandia national laboratories , tech .",
    "rep . sand2013 - 8752 , 2013 .",
    "[ online ] .",
    "available : https://software.sandia.gov/hpcg/doc/hpcg-specification.pdf        t.  iwashita , h.  nakashima and y.  takahashi , `` algebraic block multi - color ordering method for parallel multi - threaded sparse triangular solver in iccg method , '' parallel & distributed processing symposium ( ipdps ) , 2012 ieee 26th international , shanghai , 2012 , pp .",
    "474 - 483 .",
    "doi : 10.1109/ipdps.2012.51    w.  hackbusch , b.  n. khoromskij , and r.  kriemann , `` direct schur complement method by domain decomposition based on h - matrix approximation , '' _ computing and visualization in science _",
    ", vol .  8 , no .  3 , pp .",
    "179188 , 2005 .",
    "[ online ] .",
    "available : http://dx.doi.org/10.1007/s00791-005-0008-3    s.  w. son , z.  chen , w.  hendrix , a.  agrawal , w.  keng liao , and a.  choudhary , `` data compression for the exascale computing era - survey , '' _ supercomputing frontiers and innovations _ , vol .  1 , no .  2 , 2014",
    "[ online ] .",
    "available : http://superfri.org/superfri/article/view/13    h.  kaneko , `` periodic pattern coding for last level cache data compression , '' _ ieice trans .",
    "fundamentals _ , vol .",
    "96 , no .",
    "12 , pp . 23512359 , 2013 .",
    "[ online ] .",
    "available : http://ci.nii.ac.jp/naid/130003385283/en/    a.  r. alameldeen and d.  a. wood , `` adaptive cache compression for high - performance processors , '' in _ proceedings of the 31st annual international symposium on computer architecture _ ,",
    "isca 04.1em plus 0.5em minus 0.4emwashington , dc , usa : ieee computer society , 2004 , pp .",
    "212. [ online ] .",
    "available : http://dl.acm.org/citation.cfm?id=998680.1006719    m.  ekman and p.  stenstrom , `` a robust main - memory compression scheme , '' in _ proceedings of the 32nd annual international symposium on computer architecture _ ,",
    "isca 05.1em plus 0.5em minus 0.4em washington , dc , usa : ieee computer society , 2005 , pp .",
    "[ online ] .",
    "available : http://dx.doi.org/10.1109/isca.2005.6"
  ],
  "abstract_text": [
    "<S> iterative methods on irregular grids have been used widely in all areas of comptational science and engineering for solving partial differential equations with complex geometry . </S>",
    "<S> they provide the flexibility to express complex shapes with relatively low computational cost . </S>",
    "<S> however , the direction of the evolution of high - performance processors in the last two decades have caused serious degradation of the computational efficiency of iterative methods on irregular grids , because of relatively low memory bandwidth . </S>",
    "<S> data compression can in principle reduce the necessary memory memory bandwidth of iterative methods and thus improve the efficiency . </S>",
    "<S> we have implemented several data compression algorithms on the pezy - sc processor , using the matrix generated for the hpcg benchmark as an example . for the spmv ( sparse matrix - vector multiplication ) </S>",
    "<S> part of the hpcg benchmark , the best implementation without data compression achieved 11.6gflops / chip , close to the theoretical limit due to the memory bandwidth . </S>",
    "<S> our implementation with data compression has achieved 32.4gflops . </S>",
    "<S> this is of course rather extreme case , since the grid used in hpcg is geometrically regular and thus its compression efficiency is very high . </S>",
    "<S> however , in real applications , it is in many cases possible to make a large part of the grid to have regular geometry , in particular when the resolution is high . </S>",
    "<S> note that we do not need to change the structure of the program , except for the addition of the data compression / decompression subroutines . </S>",
    "<S> thus , we believe the data compression will be very useful way to improve the performance of many applications which rely on the use of irregular grids .    1=1    finite element analysis , sparse matrices , data compression </S>"
  ]
}