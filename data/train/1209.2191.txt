{
  "article_text": [
    "mapreduce  @xcite has become an ubiquitous framework for large - scale data processing .",
    "the hadoop open - source implementation enjoys widespread adoption in organizations ranging from two - person startups to fortune 500 companies .",
    "it lies at the core of an emerging stack for data analytics , with support from industry heavyweights such as ibm , microsoft , and oracle . among the advantages of mapreduce",
    "are the ability to horizontally scale to petabytes of data on thousands of commodity servers , easy - to - understand programming semantics , and a high degree of fault tolerance .",
    "mapreduce , of course , is not a silver bullet , and there has been much work probing its limitations , both from a theoretical perspective  @xcite and empirically by exploring classes of algorithms that can not be efficiently implemented with it  @xcite .",
    "many of these empirical studies take the following form :  they present a class of algorithms for which the nave hadoop solution performs poorly , expose it as a fundamental limitation of the mapreduce programming model , and propose an extension or alternative that addresses the limitation .",
    "the algorithms are expressed in this new framework , and , of course , experiments show substantial ( an order of magnitude ! )  performance improvements over hadoop .",
    "this essay espouses a very different position , that mapreduce is `` good enough '' ( even if the current hadoop implementation could be vastly improved ) . while it is true that a large class of algorithms are not amenable to mapreduce implementations , there exist alternative solutions to the same underlying problems that _ can _ be easily implemented in mapreduce .",
    "staying in its confines allows more tightly - integrated , robust , end - to - end solutions to heterogeneous large - data challenges .    to apply a metaphor :  hadoop is currently the large - scale data processing hammer of choice .",
    "we ve discovered that , in addition to nails , there are actually screws  and it does nt seem like hammering screws is a good idea .",
    "so instead of trying to invent a screwdriver , let s just get rid of the screws .",
    "if there are only nails , then our mapreduce hammer will work just fine . to be specific , much discussion in the literature surrounds the fact that iterative algorithms are not amenable to mapreduce :  the ( simple ) solution , i suggest , is to avoid iterative algorithms !",
    "i will attempt to support this somewhat radical thesis by exploring three large classes of problems which serve as the poster children for mapreduce - bashing :  iterative graph algorithms ( e.g. , pagerank ) , gradient descent ( e.g. , for training logistic regression classifiers ) , and expectation maximization ( e.g. , for training hidden markov models , @xmath0-means ) .",
    "i begin with vague and imprecise notions of what `` amenable '' and `` good enough '' mean , but propose a concrete objective with which to evaluate competing solutions later .",
    "this essay captures my personal experiences as an academic researcher as well as a software engineer in a production analytics environment . as an academic ,",
    "i ve been fortunate enough to collaborate with many wonderful colleagues and students on `` big data '' since 2007 , primarily using hadoop to scale a variety of text- and graph - processing algorithms ( e.g. , information retrieval , statistical machine translation , dna sequence assembly ) .",
    "recently , i ve just returned from spending an extended two - year sabbatical at twitter `` in the trenches '' as a software engineer wrestling with various `` big data '' problems and trying to build scalable production solutions .    in earnest",
    ", i quip `` throw away everything not a nail '' tongue - in - cheek to make a point .",
    "more constructively , i suggest a two - pronged approach to the development of `` big data '' systems and frameworks . taking the metaphor a bit further ( and at the expense of overextending it ) :",
    "on the one hand , we should perfect the hammer we already have by improving its weight balance , making a better grip , etc . on the other hand",
    ", we should be developing jackhammers  entirely new `` game changers '' that can do things mapreduce and hadoop fundamentally can not do . in my opinion",
    ", it makes less sense to work on solving classes of problems for which hadoop is already `` good enough '' .",
    "everyone s favorite example to illustrate the limitations of mapreduce is pagerank ( or more generally , iterative graph algorithms ) .",
    "let s assume a standard definition of a directed graph @xmath1 consisting of vertices @xmath2 and directed edges @xmath3 , with @xmath4 and @xmath5 consisting of the set of all successors and predecessors of vertex @xmath6 ( outgoing and incoming edges , respectively ) .",
    "pagerank  @xcite is defined as the stationary distribution over vertices by a random walk over the graph .",
    "that is , for each vertex @xmath6 in the graph , pagerank computes the value @xmath7 representing the likelihood that a random walk will arrive at vertex @xmath6 .",
    "this value is primarily induced from the graph topology , but the computation also includes a damping factor @xmath8 , which allows for random jumps to any other vertex in the graph . for non - trivial graphs ,",
    "pagerank is generally computed iteratively over multiple timesteps @xmath9",
    "using the power method : @xmath10 the algorithm iterates until either a user defined maximum number of iterations has completed , or the values sufficiently converge .",
    "one common convergence criterion is : @xmath11 the standard mapreduce implementation of pagerank is well known and is described in many places ( see , for example ,  @xcite ) .",
    "the graph is serialized as adjacency lists for each vertex , along with the current pagerank value .",
    "mappers process all the vertices in parallel :  for each vertex on the adjacency list , the mapper emits an intermediate key - value pair with the destination vertex as the key and the partial pagerank contribution as the value ( i.e. , each vertex distributes its present pagerank value evenly to its successors ) .",
    "the shuffle stage performs a large `` group by '' , gathering all key - value pairs with the same destination vertex , and each reducer sums up the partial pagerank contributions .",
    "each iteration of pagerank corresponds to a mapreduce job .",
    "typically , running pagerank to convergence requires dozens of iterations .",
    "this is usually handled by a control program that sets up the mapreduce job , waits for it to complete , and then checks for convergence by reading in the updated pagerank vector and comparing it with the previous .",
    "this cycle repeats until convergence .",
    "note that the basic structure of this algorithm can be applied to a large class of `` message - passing '' graph algorithms  @xcite ( e.g. , breadth - first search follows exactly the same form ) .",
    "there is one critical detail necessary for the above approach to work :  the mapper must also emit the adjacency list with the vertex i d as the key .",
    "this passes the graph structure to the reduce phase , where it is reunited ( i.e. , joined ) with the updated pagerank values . without this step",
    ", there would be no way to perform multiple iterations .",
    "there are many shortcoming with this algorithm :    = 1em    mapreduce jobs have high startup costs ( in hadoop , can be tens of seconds on a large cluster under load ) .",
    "this places a lower bound on iteration time .",
    "scale - free graphs , whose edge distributions follow power laws , often create stragglers in the reduce phase .",
    "the highly uneven distribution of incoming edges to vertices produces significantly more work for some reduce tasks ( take , for example , the reducer assigned to sum up the incoming pagerank contributions to google.com in the webgraph ) .",
    "note that since these stragglers are caused by data skew , speculative execution  @xcite can not solve the problem .",
    "combiners and other local aggregation techniques alleviate but do not fully solve this problem .    at each iteration , the algorithm must shuffle the graph structure ( i.e. , adjacency lists ) from the mappers to the reducers . since in most cases",
    "the graph structure is static , this represents wasted effort ( sorting , network traffic , etc . ) .",
    "the pagerank vector is serialized to hdfs , along with the graph structure , at each iteration .",
    "this provides excellent fault tolerance , but at the cost of performance .    to cope with these shortcomings ,",
    "a number of extensions to mapreduce or alternative programming models have been proposed .",
    "pregel  @xcite implements the bulk synchronous parallel model  @xcite :  computations are `` vertex - centric '' and algorithms proceed in supersteps with synchronization barriers between each . in the implementation , all state , including the graph structure , is retained in memory ( with periodic checkpointing ) .",
    "haloop  @xcite is an extension of hadoop that provides support for iterative algorithms by scheduling tasks across iterations in a manner that exploits data locality and by adding various caching mechanisms . in twister",
    "@xcite , another extension of hadoop designed for iteration , intermediate data are retained in memory if possible , thus greatly reducing iteration overhead .",
    "priter  @xcite , in contrast , takes a slightly different approach to speeding up iterative computation :  it _ prioritizes _ those computations that are likely to lead to convergence .",
    "all the frameworks discussed above share in supporting iterative constructs , and thus elegantly solve one or more of the shortcomings of mapreduce discussed above .",
    "however , they all have one drawback :  they re not hadoop !",
    "the reality is that the hadoop - based stack ( e.g. , pig , hive , etc . )",
    "has already gained critical mass as the data processing framework of choice , and there are non - trivial costs for adopting a _ separate _ framework just for graph processing or iterative algorithms .",
    "more on this point in section  [ section : good - enough ] . for",
    "now , consider three additional factors :    first , without completely abandoning mapreduce , there are a few simple `` tweaks '' that one can adopt to speed up iterative graph algorithms . for example , the schimmy pattern  @xcite avoids the need to shuffle the graph structure by consistent partitioning and performing a parallel merge join between the graph structure and incoming graph messages in the reduce phase .",
    "the authors also show that great gains can be obtained by simple partitioning schemes that increase opportunities for partial aggregation .",
    "second , some of the shortcomings of pagerank in mapreduce are not as severe as the literature would suggest . in a real - world context ,",
    "pagerank ( or any iterative graph algorithm ) is almost never computed from scratch , i.e. , initialized with a uniform distribution over all vertices and run until convergence .",
    "typically , the previously - computed pagerank vector is supplied as a starting point on an updated graph .",
    "for example , in the webgraph context , the hyperlink structure is updated periodically from freshly - crawled pages and the task is to compute updated pagerank values .",
    "it makes little sense to re - initialize the pagerank vector and `` start over '' .",
    "initializing the algorithm with the previously - computed values significantly reduces the number of iterations required to converge .",
    "thus , the iteration penalties associated with mapreduce become much more tolerable .",
    "third , the existence of graph streaming algorithms for computing pagerank  @xcite suggests that there may be non - iterative solutions ( or at least approximations thereof ) to a large number of iterative graph algorithms .",
    "this , combined with a good starting distribution ( previous point ) , suggests that we can compute solutions efficiently , even within the confines of mapreduce .",
    "given these observations , perhaps we might consider mapreduce to be `` good enough '' for iterative graph algorithms ?",
    "but what exactly does `` good enough '' mean ?",
    "let s return to this point in section  [ section : good - enough ] .",
    "gradient descent ( and related quasi - newton ) methods for machine learning represent a second large class of problems that are poorly suited for mapreduce .",
    "to explain , let s consider a specific type of machine learning problem , supervised classification .",
    "we define @xmath12 to be an input space and @xmath13 to be an output space .",
    "given a set of training samples @xmath14 from the space @xmath15 , the task is to induce a function @xmath16 that best explains the training data .",
    "the notion of `` best '' is usually captured in terms of minimizing `` loss '' , via a function @xmath17 that quantifies the discrepancy between the functional prediction @xmath18 and the actual output @xmath19 , for example , minimizing the quantity : @xmath20 which is known as the _",
    "empirical risk_. usually , we consider a family of functions @xmath21 ( i.e. , the hypothesis space ) that is parameterized by the vector @xmath22 , from which we select : @xmath23 that is , we learn the parameters of a particular model . in other words , machine learning is cast as a functional optimization problem , often solved with gradient descent .    rewriting equation  ( [ loss ] ) as @xmath24 simplifies our notation .",
    "the gradient of @xmath25 , denote @xmath26 , is defined as follows : @xmath27\\ ] ] the gradient defines a vector field pointing to the direction in which @xmath25 is increasing the fastest and whose magnitude indicates the rate of increase .",
    "thus , if we `` take a step '' in the direction opposite to the gradient from an arbitrary point @xmath28 , @xmath29 , then @xmath30 , provided that @xmath31 ( known as the step size ) is a small value greater than zero .",
    "if we start with an initial guess of @xmath32 and repeat the above process , we arrive at gradient descent .",
    "more formally , let us consider the sequence @xmath33 , defined with the following update rule : @xmath34 we have : @xmath35 where the sequence converges to the desired local minimum . if the loss function is convex and @xmath31 is selected carefully ( which can vary per iteration ) , we are guaranteed to converge to a global minimum .",
    "based on the observation that our loss function decomposes linearly , and therefore the gradient as well , the mapreduce implementation of gradient descent is fairly straightforward .",
    "we process each training example in parallel and compute its partial contribution to the gradient , which is emitted as an intermediate key - value pair and shuffled to a single reducer .",
    "the reducer sums up all partial gradient contributions and updates the model parameters .",
    "thus , each iteration of gradient descent corresponds to a mapreduce job .",
    "two more items are needed to make this work :    = 1em    complete classifier training requires many mapreduce jobs to be chained in a sequence ( hundreds , even thousands , depending on the complexity of the problem ) . just as in the pagerank case",
    ", this is usually handled by a driver program that sets up a mapreduce job , waits for it to complete , and then checks for convergence , repeating as long as necessary .",
    "since mappers compute partial gradients with respect to the training data , they require access to the current model parameters . typically , the parameters are loaded in as `` side data '' in each mapper ( in hadoop , either directly from hdfs or from the distributed cache ) . however , at the end of each iteration the parameters are updated , so it is important that the updated model is passed to the mappers at the next iteration .    any number of fairly standard optimizations can be applied to increase the efficiency of this implementation , for example , combiners to perform partial aggregation or the in - mapper combining pattern  @xcite . as an alternative to performing gradient descent in the reducer",
    ", we can substitute a quasi - newton method such as l - bfgs  @xcite ( which is more expensive , but converges in few iterations ) .",
    "however , there are still a number of drawbacks :    = 1em    as with pagerank , hadoop jobs have high startup costs .",
    "since the reducer must wait for all mappers to finish ( i.e. , all contributions to the gradient to arrive ) , the speed of each iteration is bound by the _",
    "slowest _ mapper , and hence sensitive to stragglers .",
    "this is similar to the pagerank case , except in the map phase .",
    "the combination of stragglers and using only a single reducer potentially causes poor cluster utilization .",
    "of course , the cluster could be running other jobs ,",
    "so from a throughput perspective , this is only a minor concern .",
    "the shortcomings of gradient descent implementations in mapreduce have prompted researchers to explore alternative architectures and execution models that address these issues .",
    "all the systems discussed previously in the context of pagerank are certainly relevant , but we point out two more alternatives .",
    "spark  @xcite introduces the resilient distributed datasets ( rdd ) abstraction , which provide a restricted form of shared memory based on coarse - grained transformations rather than fine - grained updates to shared state .",
    "rdds can either be cached in memory or materialized from durable storage when needed ( based on lineage , which is the sequence of transformations applied to the data ) .",
    "classifier training is one of the demo applications in spark .",
    "another approach with similar goals is taken by bu et al .",
    "@xcite , who translate iterative mapreduce and pregel - style programs into recursive queries in datalog . by taking this approach , database query optimization techniques can be used to identify efficient execution plans .",
    "these plans are then executed on the hyracks data - parallel processing engine  @xcite .",
    "in contrast to these proposed solutions , consider an alternative approach .",
    "since the bottleneck in gradient descent is the iteration , let s simply get rid of it ! instead of running _ batch _ gradient descent to train classifiers , let us adopt _",
    "gradient descent , which is an _ online _ technique .",
    "the simple idea is that instead of updating the model parameters after only considering _ every _ training example , let us update the model after _ each _ training example ( i.e. , compute the gradient with respect to each example ) .",
    "online learning techniques have received renewed interest in the context of big data since they operate in a streaming fashion and are very fast  @xcite . in practice , classifiers trained using online gradient descent achieve accuracy comparable to classifiers trained using traditional batch learning techniques , but are an order of magnitude ( or more ) faster to train  @xcite .",
    "stochastic gradient descent addresses the iteration problem , but does not solve the single reducer problem .",
    "for that , ensemble methods come to the rescue  @xcite . instead of training a single classifier ,",
    "let us train an _ ensemble _ of classifiers and combine predictions from each ( e.g. , simple majority voting , weighted interpolation , etc . ) .",
    "the simplest way of building ensembles  training each classifier on a partition of the training examples  is both embarrassingly parallel and surprisingly effective in practice  @xcite .",
    "combining online learning with ensembles addresses the shortcomings of gradient descent in mapreduce . as a case study ,",
    "this is how twitter integrates machine learning into pig in a scalable fashion  @xcite :  folding the online learning inside storage functions and building ensembles by controlling data partitioning . to reiterate the argument :  if mapreduce is not amenable to a particular class of algorithms , let s simply find a different class of algorithms that will solve the same problem and _ is _ amenable to mapreduce .",
    "a third class of algorithms not amenable to mapreduce is expectation maximization ( em )  @xcite and em - like algorithms .",
    "since em is related to gradient descent ( both are first - order optimization techniques ) and many of my arguments are quite similar , the discussion in this section will be more superficial .",
    "em is an iterative algorithm that finds a successive series of parameter estimates @xmath32 , @xmath36 , @xmath37 that improve the marginal likelihood of the training data , used in cases where there is incomplete ( or unobservable ) data .",
    "the algorithm starts with some initial set of parameters @xmath32 and then updates them using two steps :  expectation ( e - step ) , which computes the posterior distribution over the latent variables given the observable data and a set of parameters @xmath38 , and maximization ( m - step ) , which computes new parameters @xmath39 maximizing the expected log likelihood of the joint distribution with respect to the distribution computed in the e - step .",
    "the process then repeats with these new parameters .",
    "the algorithm terminates when the likelihood remains unchanged .",
    "similar to iterative graph algorithms and gradient descent , each em iteration is typically implemented as a hadoop job , with a driver to set up the iterations and check for convergence . in broad strokes ,",
    "the e - step is performed in the mappers and the m - step is performed in the reducers .",
    "this setup has all the shortcomings discussed before , and em and em - like algorithms can be much more elegantly implemented in alternative frameworks that better support iteration ( e.g. , those presented above ) .",
    "let s more carefully consider terms that i ve been using quite vaguely :  what does it mean for an algorithm to be _ amenable _ to mapreduce ?",
    "what does it mean for mapreduce to be `` good enough '' ? and",
    "the point of comparison ? here are two case studies that build up to my point : dyer et al .",
    "@xcite applied mapreduce to training translation models for a statistical machine translation system  specifically , the word - alignment component that uses hidden markov models ( hmms ) to discover word correspondences across bilingual corpora  @xcite .",
    "the point of comparison was giza++ , a widely - adopted in - memory , single - threaded implementation ( the _ de facto _",
    "standard used by researchers at the time the work was performed , and still commonly used today ) .",
    "the authors built a hadoop - based implementation of the hmm word - alignment algorithm , which demonstrated linear scalability compared to giza++ , reducing per - iteration training time from hours to minutes .",
    "the implementation exhibited all the limitations associated with em algorithms ( high job startup costs , awkward passing of model parameters from one iteration to the next , etc . ) , yet compared to the previous single - threaded approach , mapreduce represented a step forward .",
    "here is the key point :  whether an algorithm is `` amenable '' to mapreduce is a relative judgment that is only meaningful in the context of an alternative .",
    "compared to giza++ , the hadoop implementation represented an advance .",
    "however , this is not inconsistent with the claim that em algorithms could be more elegantly implemented in an alternate model that better supports iteration ( e.g. , any of the work discussed above ) .",
    "the second example is the venerable lloyd s method for @xmath0-means clustering , which can be understood in terms of em ( not exactly em , but can be characterized as em - like ) .",
    "a mapreduce implementation of @xmath0-means shares many of the limitations discussed thus far .",
    "it is true that the algorithm can be expressed in a simpler way using a programming model with iterative constructs and executed more efficiently with better iteration support ( and indeed , many of the papers discussed above use @xmath0-means as a demo application ) .",
    "however , even within the confines of mapreduce , there has been a lot of work on optimizing clustering algorithms ( e.g. ,  @xcite ) .",
    "it is not entirely clear how these improvements would stack up against using an entirely different framework . here , is mapreduce `` good enough '' ?",
    "these two case studies provide the segue to my attempt at more clearly defining what it means for mapreduce to be `` good enough '' , and a clear objective for deciding between competing solutions .",
    "i propose a pragmatic , operational , engineering - driven criterion for deciding between alternative solutions to large - data problems .",
    "first , though , my assumptions :    = 1em    the hadoop stack , for better or for worse , has already become the _ de facto _ general - purpose , large - scale data processing platform of choice . as part of the stack",
    "i include higher - level layers such as pig and hive .",
    "complete , end - to - end , large - data solutions involve heterogeneous data sources and must integrate different types of processing :  relational processing , graph analysis , text mining , machine learning , etc .",
    "no single programming model or framework can excel at every problem ; there are always tradeoffs between simplicity , expressivity , fault tolerance , performance , etc .    given these assumptions ,",
    "the decision criterion i propose is this :  in the context of an end - to - end solution , would it make sense to adopt framework @xmath12 ( haloop , twister , priter , spark , etc . )  over the hadoop stack for solving the problem at hand ? has already been made production ready . ]",
    "put another way :  are the gains gotten from using @xmath12 worth the integration costs incurred in building the end - to - end solution ?",
    "if no , then operationally , we can consider the hadoop stack ( including pig , hive , etc . , and by extension , mapreduce ) to be `` good enough '' .",
    "note that this way of thinking takes a broader view of end - to - end system design and evaluates alternatives in a global context . considered in isolation",
    ", it naturally makes sense to choose the best tool for the job , but this neglects the fact that there are substantial costs in knitting together a patchwork of different frameworks , programming models , etc .",
    "the alternative is to use a common computing platform that s already widely adopted ( in this case , hadoop ) , even if it is nt a perfect fit for some of the problems .",
    "i propose this decision criterion because it tries to bridge the big gap between `` solving '' a problem ( in a research paper ) and deploying the solution in production ( which has been brought into stark relief for me personally based on my experiences at twitter ) .",
    "for something to `` work '' in production , the solution must be continuously running ; processes need to be monitored ; someone needs to be alerted when the system breaks ; etc . introducing a new programming model , framework , etc .",
    "significantly complicates this process  even mundane things like getting the data imported into the right format and results exported to the right location become non - trivial if it s part of a long chain of dependencies .",
    "a natural counter - argument would be :  why should academics be concerned with these ( mere ) `` production issues '' ?",
    "this ultimately comes down to what one s criteria for success are . for me personally , the greatest reward comes from seeing my algorithms and code `` in the wild '' :  whether it s an end - to - end user - facing service that millions are using on a daily basis or an internal improvement in the stack that makes engineers and data scientists lives better .",
    "i consider myself incredibly lucky to have accomplished both during my time at twitter .",
    "i firmly believe that in order for any work to have meaningful impact ( in the way that i define it , recognizing , of course , that others are guided by different utility functions ) , how a particular solution fits into the broader ecosystem is an important consideration .",
    "different programming models provide different ways of thinking about the problem .",
    "mapreduce provides `` map '' and `` reduce '' , which can be composed into more complex dataflows ( e.g. , via pig ) .",
    "other programming models are well - suited to certain types of problems _ precisely _ because they provide a different way of thinking about the problem .",
    "for example , pregel provides a vertex - centered approach where `` time '' is dictated by the steady advance of the superstep synchronization barriers .",
    "we encounter an impedance mismatch when trying to connect different frameworks that represent different ways of thinking .",
    "the advantages of being able to elegantly formulate a solution in a particular framework must be weighed against the costs of integrating that framework into an end - to - end solution .    to illustrate , i ll present a hypothetical but concrete example :  let s say we wish to run pagerank on the interaction graph of a social network ( i.e. , the graph defined by _ interactions _ between users ) .",
    "such a graph is _ implicit _ and needs to be constructed from behavior logs , which is natural to accomplish in a dataflow language such as pig ( in fact , pig was exactly designed for log mining ) .",
    "let s do exactly that .    with the interaction graph",
    "now materialized , we wish to run pagerank .",
    "consider two alternatives :  use giraph , the open - source implementation of pregel , or implement pagerank directly in pig .",
    "the advantage of the first is that the bsp model implemented by giraph / pregel is perfect for pagerank and other iterative graph algorithms ( in fact , that s exactly what pregel was designed to do ) .",
    "the downside is lots of extra `` plumbing '' : munging pig output into a format suitable for giraph , triggering the giraph job , waiting for it to finish , and figuring out what to do with the output ( if another pig job depends on the results , then we must munge the data back into a form that pig can use ) . in the second alternative",
    ", we simply write pagerank in pig , with all the shortcomings of iterative mapreduce algorithms discussed in this paper",
    ". each iteration might be slow due to stragglers , needless shuffling of graph structure , etc .",
    "but since we likely have the pagerank vector from yesterday to start from , the pig solution would converge mercifully quickly .",
    "and with pig , all of the additional `` plumbing '' issues go away . given these alternatives ,",
    "i believe the choice of the second is at least justifiable ( and arguably , preferred ) , and hence , in this particular context , i would argue that mapreduce is good enough .    in my opinion ,",
    "the arguments are even stronger for the case of stochastic gradient descent . why adopt a separate machine - learning framework simply for running batch gradient descent when it could be seamlessly integrated into pig by using stochastic gradient descent and ensemble methods  @xcite ?",
    "this approach costs nothing in accuracy , but gains tremendously in terms of performance . in the twitter case study ,",
    "machine learning is accomplished by _ just another pig script _ , which plugs seamlessly into existing pig workflows .",
    "to recap :  of course it makes sense to consider the right tool for the job , but we must also recognize the cost associated with switching tools  in software engineering terms , the costs of integrating heterogeneous frameworks into an end - to - end workflow are non - trivial and should not be ignored .",
    "fortunately , recent developments in the hadoop project promise to substantially reduce the costs of integrating heterogeneous frameworks :  hadoop nextgen ( aka yarn ) introduces a generic resource scheduling abstraction that allows multiple application frameworks to co - exist on the same physical cluster . in this context",
    ", mapreduce is just one of many possible application frameworks ; others include spark and mpi .",
    "this `` meta - framework '' could potentially reduce the costs of supporting heterogeneous programming models ",
    "an exciting future development that might let us have our cake and eat it too .",
    "however , until yarn proves itself in production environments , it remains an unrealized potential .",
    "building on the arguments above and reflecting on my experiences over the past several years working on `` big data '' in both academia and industry , i d like to make the following constructive suggestions :    * continue plucking low hanging fruit * , or , refine the hammer we already have .",
    "i do not think we have yet sufficiently pushed the limits of mapreduce in general and the hadoop implementation in particular . in my opinion",
    ", it may be premature to declare it obsolete and call for a fresh ground - up redesign  @xcite .",
    "mapreduce is less than ten years old , and hadoop is even younger .",
    "there has already been plenty of interesting work within the confines of hadoop , just from the database perspective :  integration with a traditional rdbms  @xcite , smarter task scheduling  @xcite , columnar layouts  @xcite , embedded indexes  @xcite , cube materialization  @xcite , and a whole cottage industry on efficient join algorithms  @xcite ; we ve even seen `` traditional '' hpc ideas such as work stealing make its way into the hadoop context  @xcite .",
    "much more potential remains untapped .",
    "the data management and distributed systems communities have developed and refined a large `` bag of tricks '' over the past several decades .",
    "researchers have tried applying many of these in the hadoop context ( see above ) , but there are plenty remaining in the bag waiting to be explored .",
    "many , if not most , of the complaints about hadoop lacking basic features or optimization found in other data processing systems can be attributed to immaturity of the platform , not any fundamental limitations .",
    "more than a `` matter of implementation '' , this work represents worthy research .",
    "hadoop occupies a very different point in the design space when compared to parallel databases , so the `` standard tricks '' often need to be reconsidered in this new context .",
    "so , in summary , let s fix all the things we have a good idea how to fix in hadoop ( low - risk research ) , and then revisit the issue of whether mapreduce is good enough .",
    "i believe this approach of incrementally refining hadoop has a greater chance of making impact ( at least by my definition of impact in terms of adoption ) than a strategy that abandons hadoop . to invoke another clich :",
    "let s pluck all the low - hanging fruit first before climbing to the higher branches .",
    "* work on game - changers * , or , develop the jackhammer . to displace ( or augment ) mapreduce ,",
    "we should focus on capabilities that the framework fundamentally can not support .",
    "to me , faster iterative algorithms , illustrated with pagerank or gradient descent are nt `` it''given my above arguments on how for those , mapreduce is `` good enough '' .",
    "i propose two potential game changers that reflect pain points i ve encountered during my time in industry :    first , real - time computation on continuous , large - volume streams of data is not something that mapreduce is capable of .",
    "mapreduce is fundamentally a batch processing framework  and despite efforts in implementing `` online '' mapreduce  @xcite , i believe solving the general problem requires something that looks very different from the current architecture . for example , let s say i want to keep track of the top thousand most - clicked urls posted on twitter in the last @xmath40 minutes .",
    "the current solution is to run batch mapreduce jobs with increasing frequency ( e.g. , every five minutes ) , but there is a fundamental limit to this approach ( job startup time ) , and ( near ) real - time results are not obtainable ( for example , if i wanted up - to - date results over the last 30 seconds ) .",
    "one sensical approach is to integrate a stream processing engine  a stream - oriented rdbms ( e.g. ,  @xcite ) , s4  @xcite , or storm  with hadoop , so that the stream processing engine handles real - time computations , while hadoop performs aggregate `` roll ups '' .",
    "more work is needed along these lines , and indeed researchers are already beginning to explore this general direction  @xcite .",
    "i believe the biggest challenge here is to seamlessly and efficiently handle queries across vastly - different time granularities :  from `` over the past 30 seconds '' ( in real time ) to `` over the last month '' ( where batch computations with some lag would be acceptable ) .",
    "second , and related to the first , real - time interactions with large datasets is a capability that is sorely needed , but is something that mapreduce fundamentally can not support .",
    "the rise of `` big data '' means that the work of data scientists is increasingly important  after all , the value of data lie in the insights that they generate for an organization .",
    "tools available to data scientists today are primitive :  write a pig script and submit a job .",
    "wait five minutes for the job to finish .",
    "discover that the output is empty because of the wrong join key .",
    "fix simple bug .",
    "wait another five minutes .",
    "rinse , repeat .",
    "it s fairly obvious that long debug cycles hamper rapid iteration . to the extent that we can provide tools to allow rich , interactive , incremental interactions with large data sets",
    ", we can boost the productivity of data scientists , thereby increasing their ability to generate insights for the organization .    *",
    "open source everything .",
    "* open source releasing of software should be the default for any work that is done in the `` big data '' space .",
    "even the harshest critic would concede that open source is a key feature of hadoop , which facilitates rapid adoption and diffusion of innovation .",
    "the vibrant ecosystem of software and companies that exist today around hadoop can be attributed to its open source license .",
    "beyond open sourcing , it would be ideal if the results of research papers were submitted as patches to existing open source software ( i.e. , associated with jira tickets ) .",
    "an example is recent work on distributed cube materialization  @xcite , which has been submitted as a patch in pig .",
    "of course , the costs associated with this can be substantial , but this represents a great potential for collaborations between academia and industry ; committers of open source projects ( mostly software engineers in industry ) can help shepherd the patch . in many cases , transitioning academic research projects to production - ready code",
    "make well - defined summer internships at companies .",
    "these are win - win scenarios for all :  the company benefits immediately from new features ; the community benefits from the open sourcing ; and the students gain valuable experience .",
    "the clich is `` if all you have is a hammer , then everything looks like a nail '' .",
    "i argue for going one step further :  `` if all you have is a hammer , throw away everything that s not a nail '' !",
    "it ll make your hammer look amazingly useful .",
    "at least for some time .",
    "soon or later , however , the flaws of the hammer will be exposed  but let s try to get as much hammering done as we can before then . while we re hammering , though , nothing should prevent us from developing jackhammers ."
  ],
  "abstract_text": [
    "<S> hadoop is currently the large - scale data analysis `` hammer '' of choice , but there exist classes of algorithms that are nt `` nails '' , in the sense that they are not particularly amenable to the mapreduce programming model . to address this , </S>",
    "<S> researchers have proposed mapreduce extensions or alternative programming models in which these algorithms can be elegantly expressed . </S>",
    "<S> this essay espouses a very different position :  that mapreduce is `` good enough '' , and that instead of trying to invent screwdrivers , we should simply get rid of everything that s not a nail . to be more specific , much discussion in the literature </S>",
    "<S> surrounds the fact that iterative algorithms are a poor fit for mapreduce :  the simple solution is to find alternative non - iterative algorithms that solve the same problem . </S>",
    "<S> this essay captures my personal experiences as an academic researcher as well as a software engineer in a `` real - world '' production analytics environment . from this combined perspective </S>",
    "<S> i reflect on the current state and future of `` big data '' research . + </S>",
    "<S> * author s note : * i wrote this essay specifically to be controversial . </S>",
    "<S> the views expressed herein are more extreme than what i believe personally , written primarily for the purposes of provoking discussion . </S>",
    "<S> if after reading this essay you have a strong reaction , then i ve accomplished my goal :) </S>"
  ]
}