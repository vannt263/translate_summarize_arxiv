{
  "article_text": [
    "in practice , one would like to determine a high - dimensional signal @xmath0 from its relatively low - dimensional linear measurements @xmath1 , where @xmath2 and @xmath3 . there exist infinite candidates of @xmath4 that satisfy the linear equation in general . to possibly recover the true signal",
    ", some additional information needs to be taken into account .",
    "fortunately , most signals of interest are sparse or compressible under appropriate bases , e.g. , an image under a wavelet basis . without loss of generality",
    ", we consider a signal that is sparse under the canonical basis since any sparsifying transform of @xmath4 can be absorbed into the matrix @xmath5 .",
    "so we need to search for the maximally sparse solution to the linear equation , i.e. , to minimize @xmath6 that counts the number of nonzero entries of @xmath4 subject to the constraint @xmath7 .",
    "since this combinatorial optimization problem is np - hard , its convex relaxation ( replacing @xmath6 by @xmath8 ) coined as basis pursuit ( bp ) has been extensively studied ( see @xcite and references therein ) . during the past several years",
    ", the sparse signal recovery problem has been developed into a well - known research area named as compressed sensing ( cs)@xcite , which has wide applications including medical imaging @xcite , source localization @xcite and single - pixel camera @xcite , to name just a few . in cs ,",
    "one wishes to recover a sparse signal @xmath4 from its compressive measurements @xmath9 .",
    "the sensing matrix @xmath5 is typically generated from a random distribution such that it satisfies some desirable properties such as restricted isometry property ( rip ) @xcite .",
    "it is shown that @xmath10 optimization can recover the true signal exactly under mild conditions .",
    "in addition , @xmath10 optimization is robust to measurement noises and works efficiently with compressible signals @xcite . while the @xmath10 norm used to promote sparsity",
    "is a convex approximation to the original @xmath11 norm , nonconvex optimizations have also been studied .",
    "it is shown in @xcite that improved performance can be obtained using the @xmath12 ( @xmath13 ) norm .",
    "the nonconvex objective function @xmath14 with @xmath15 is related to reweighted @xmath10 minimization in @xcite .",
    "another class of approaches to cs uses a greedy pursuit method including omp @xcite and stomp@xcite . in a greedy algorithm ,",
    "the support of the solution is modified sequentially with local optimization in each step .",
    "this paper is focused on bayesian approaches to cs , known as bayesian cs @xcite .",
    "this method was originated from the area of machine leaning and introduced by tipping @xcite for obtaining sparse solutions to regression and classification tasks that use models which are linear in the parameters , coined as relevance vector machine ( rvm ) or sparse bayesian learning ( sbl ) .",
    "sbl is built upon a statistical perspective where the sparsity information is exploited by assuming a sparsity - inducing prior for the signal of interest that is then estimated via bayesian inference .",
    "its theoretical performance is analyzed by wipf and rao @xcite .",
    "after being introduced into cs by ji _ et al . _",
    "@xcite , this technique has become a popular approach to cs and other sparsity - related problems .",
    "for example , a bayesian framework is presented in @xcite for meg / eeg source imaging . in @xcite ,",
    "the authors introduce a framework to unify the cs problems with multi- and one - bit quantized measurements and estimate the sparse signals of interest and quantization errors based on a bayesian formulation .",
    "currently , main research topics in sbl for cs include ( a ) developing efficient sparsity - inducing priors , ( b ) incorporating additional signal structures in the prior besides sparsity and ( c ) designing fast and accurate inference algorithms .",
    "this paper studies problems ( a ) and ( c )",
    ". examples of ( b ) include @xcite that exploits the wavelet structure of images and @xcite on cluster structured sparse signals .",
    "many sparsity - inducing priors have been studied in the literature . in @xcite ,",
    "a spike - and - slab prior @xcite is applied which is a mixture of a point mass at zero and a continuous distribution elsewhere and fits naturally for sparse signals .",
    "a typical inference scheme for such a prior is a markov chain monte carlo ( mcmc ) method @xcite due to the lack of closed - form expressions of bayesian estimators . as a result",
    ", the inference process may suffer from computational difficulties because a large number of samples are required to approximate the posterior distribution and the convergence is typically slow .",
    "a popular class of sparsity - inducing priors is introduced in a hierarchical framework where a complex prior is composed of two or more simple distributions .",
    "for example , a student s @xmath16-prior ( or gaussian - inverse gamma prior ) is used in the basic sbl @xcite that is composed of a gaussian prior in the first layer and a gamma prior in the second .",
    "a laplace ( gaussian - exponential ) prior is used in @xcite . a gaussian - gamma prior",
    "is recently studied in @xcite that generalizes the laplace prior .",
    "two popular inference methods for the hierarchical priors are evidence procedure @xcite , e.g. , in @xcite , and variational bayesian inference @xcite , e.g. , in @xcite .",
    "both the methods are approximations of bayesian inference since the exact inference is intractable . in an evidence procedure",
    ", the signal estimator has a simple expression in which some unknown hyperparameters are involved and estimated iteratively by maximizing their evidence . in a variational bayesian inference method",
    ", the posterior distribution is approximated using some family of tractable distributions followed by computation of an optimal distribution within the family . to circumvent high - dimensional matrix inversions , a fast algorithm framework is developed in @xcite for evidence procedure and also adopted in @xcite .",
    "but it is observed in this paper and @xcite that the fast algorithms in @xcite typically produce signal estimates with overestimated support sizes , especially in a low signal to noise ratio ( snr ) regime or in the case of a large sample size . a new sparsity - inducing prior and algorithm",
    "are proposed in this work to resolve this problem with improved convergence speed .",
    "though formulated from a different perspective , sbl is related to other approaches to cs .",
    "consider the observation model @xmath17 where @xmath18 represents an additive white gaussian noise ( awgn ) .",
    "let @xmath19 be the prior for @xmath4 .",
    "then , a maximum _ a posteriori _ ( map ) estimator of @xmath4 coincides with a solution to a regularized least - squares problem with @xmath20 ( up to a scale ) being the regularization term , which bridges sbl and optimization methods .",
    "for example , a laplace prior corresponds to the widely studied @xmath10 minimization .",
    "a prior corresponding to the nonconvex @xmath12 ( @xmath13 ) norm is studied in @xcite .",
    "the fast algorithm in @xcite is related to the greedy pursuit method .",
    "in fact , it is a greedy algorithm using a different support modification criterion . unlike omp and stomp , it allows deletion of irrelevant basis vectors that may have been added to the solution support in earlier steps .    in this paper",
    ", we introduce a new sparsity - inducing prior named as gaussian shifted - truncated - gamma ( g - stg ) prior that generalizes the gaussian - gamma prior in @xcite . the extended flexibility of the new prior promotes its capability of modeling sparse signals .",
    "in fact , it is shown that the gaussian - gamma prior can not work in the main algorithm of this paper . from the perspective of map estimation",
    ", the g - stg prior corresponds to a nonconvex objective function in optimization methods in general . for signal recovery",
    "we propose an iterative algorithm based on an evidence procedure and a fast greedy algorithm inspired by the algorithm in @xcite .",
    "we show that similar theoretical guarantees shown in @xcite hold for the new sbl method as for the basic sbl .",
    "specifically , we show that every local optimum of the sbl cost function is achieved at a sparse solution and that the global optimum is achieved at the maximally sparse solution .",
    "moreover , we show that the proposed algorithm produces a sparser solution than existing sbl methods .",
    "we provide simulation results with one - dimensional synthetic signals and two - dimensional images that verify our analysis .",
    "we compare our proposed method with state - of - the - art ones to illustrate its improved performance .",
    "notations used in this paper are as follows .",
    "bold - face letters are reserved for vectors and matrices .",
    "for ease of exposition , we do not distinguish a random variable from its numerical value .",
    "@xmath21 is the @xmath22th entry of a vector @xmath4 .",
    "@xmath6 counts the number of nonzero entries of @xmath4 .",
    "@xmath23 for @xmath24 denotes the @xmath12 norm ( or pseudo - norm ) of @xmath4 .",
    "@xmath25 denotes a diagonal matrix with diagonal entries being the elements of the vector @xmath4 .",
    "@xmath26 denotes the @xmath27th entry of a matrix @xmath5 .",
    "@xmath28 denotes the determinant of the matrix @xmath5 .",
    "@xmath29 denotes the expectation of a random variable @xmath30 .",
    "the rest of the paper is organized as follows .",
    "section [ sec : newsparseprior ] introduces the g - stg prior .",
    "section [ sec : algorithms ] presents the bayesian framework , an iterative procedure for signal recovery , and theoretical results of the new sbl method .",
    "section [ sec : greedyalg ] introduces a fast greedy algorithm with analysis .",
    "section [ sec : simulation ] provides empirical results to show the efficiency of the proposed solution .",
    "section [ sec : conclusion ] concludes this paper .",
    "we introduce the hierarchical gaussian shifted - truncated - gamma ( g - stg ) prior for a sparse signal @xmath0 as follows : where @xmath31 , @xmath32 , @xmath33 is a shifted - truncated - gamma ( stg ) distribution for @xmath34 with @xmath35 is the shape parameter , @xmath36 is the rate parameter , @xmath15 is the threshold parameter and @xmath37 denotes an incomplete gamma function . the first layer of the prior is a commonly used gaussian prior that leads to convenient computations as shown later . in the second layer @xmath38 , @xmath39 ,",
    "are assumed to be independent , and further @xmath40 , @xmath39 , are i.i.d .",
    "truncated gamma distribution ( that is why we say that @xmath33 is an stg distribution ) .",
    "by @xmath41 , @xmath39 , it is obvious that @xmath38 is favored to be zero in the second layer if @xmath42 , resulting in that @xmath21 is favored to be zero .",
    "thus the hierarchical prior is a sparsity - inducing prior . in general",
    ", there is no explicit expression for the marginal distribution    in the following we study some special cases and show that the g - stg prior generalizes those in @xcite .    _",
    "1 ) @xmath43 : _ in this case , the second layer is reduced to an exponential prior independent of @xmath44 since @xmath45 .",
    "then , the g - stg prior coincides with the laplace prior in @xcite with @xmath46 .",
    "_ 2 ) @xmath47 : _ the second layer becomes a gamma prior for each @xmath38 . as a result , the proposed prior becomes the gaussian - gamma prior in @xcite and @xmath48 where @xmath49 is the modified bessel function of the second kind and order @xmath50 .",
    "in addition , we have that @xmath51 if @xmath52 and @xmath53 if @xmath54 .",
    "though the g - stg prior generalizes the gaussian - gamma prior , it should be noted that the main algorithm based on the g - stg prior proposed in this paper works differently from that in @xcite .",
    "_ 3 ) @xmath55 : _ by lhospital s rule it can be shown that @xmath56 , i.e. , the prior for @xmath38 in the second layer approaches an exponential prior in such a case .",
    "consequently , the proposed g - stg prior coincides with the laplace prior as in _ case 1_.    to visualize the variation of the g - stg prior with respect to the two parameters @xmath44 and @xmath57 , we plot the pdf @xmath58 in fig . [",
    "fig : priorpdf ] with @xmath59 . fig .",
    "[ fig : priorpdf_tau ] is for the case of @xmath60 and varying @xmath44 .",
    "obviously , the g - stg prior is a sparsity - inducing prior with the pdfs highly peaked at the origin , especially when @xmath61 .",
    "the main difference between the cases @xmath47 and @xmath62 is near the origin where @xmath63 approaches infinity for @xmath47 while it is always finite for @xmath64 .",
    "as @xmath44 gets larger , less density concentrates near the origin and the resulting prior gets closer to the laplace prior that corresponds to @xmath65 .",
    "[ fig : priorpdf_epsilon ] is for the case of @xmath62 and varying @xmath57 .",
    "it is shown that the g - stg prior gets less sparsity - inducing as @xmath57 gets larger , and that it ceases to promote sparsity as @xmath66 .",
    "from the perspective of map estimation , the g - stg prior corresponds to a nonconvex optimization method as @xmath67 since the term @xmath20 is nonconvex in such a case .      strictly speaking , a continuous prior is not suitable for sparse signals since any vector generated from a continuous prior is only approximately sparse ( the probability of a zero - valued entry is zero ) . in the following ,",
    "we provide an intuitive explanation about why the g - stg prior works for sparse signals by setting the threshold parameter @xmath44 according to the noise level .",
    "we consider a gaussian ensemble sensing matrix @xmath5 ( the entries of @xmath5 are i.i.d .",
    "gaussian @xmath68 where the variance is set to @xmath69 to make columns of @xmath5 have expected unit norm ) .",
    "this matrix ensemble has been widely studied , e.g. , in @xcite .",
    "then , consider a compressible signal @xmath70 and the observation model @xmath71 . in the bayesian framework",
    ", we assume that @xmath70 is distributed according to some sparsity - inducing prior . here",
    "we adopt the gaussian - gamma prior in @xcite , i.e. , we assume that @xmath72 and @xmath73 where @xmath74 and @xmath75 refers to @xmath76 with @xmath47 . however , theoretical results @xcite state that only significant entries of @xmath70 can be recovered while insignificant ones play as noises .",
    "so we write @xmath70 into @xmath77 where @xmath4 denotes the significant , `` recoverable '' component and @xmath78 refers to the insignificant , `` unrecoverable '' part .",
    "then the observation model becomes @xmath79 with @xmath80 . by the structure of @xmath5",
    ", @xmath18 is a zero - mean awgn with the noise variance @xmath81 .",
    "we see that only the power of @xmath78 is reflected in the noise .",
    "that is , for any vector @xmath82 satisfying @xmath83 , @xmath84 and @xmath18 are identically distributed .",
    "so we may replace @xmath78 by @xmath82 in the observation model ( i.e. , @xmath85 and @xmath70 are indistinguishable for cs approaches ) .",
    "then we may model @xmath82 as an i.i.d .",
    "zero - mean gaussian vector with variance @xmath86 .",
    "in addition , @xmath82 is independent of @xmath4 .",
    "so , under the assumption of a gaussian ensemble matrix @xmath5 , a compressible signal @xmath70 is equivalent to the sum of its significant part @xmath4 plus a white gaussian noise @xmath82 . applying the gaussian - gamma prior for @xmath70 to @xmath85",
    ", we obtain that @xmath87 , @xmath39 , where @xmath88 is as defined in ( [ formu : prior_x ] ) .",
    "then we get @xmath89 as a conditional distribution , resulting in that @xmath90 is in the exact form of ( [ formu : prior_alpha ] ) .    in this paper",
    ", we mainly consider the observation model @xmath17 where @xmath4 is a sparse signal and @xmath18 is a zero - mean awgn with known variance @xmath91 .",
    "the same sparsity - inducing prior for @xmath4 can be obtained by a reverse procedure and the details are omitted .",
    "so we can set the threshold parameter @xmath92 in the g - stg prior .",
    "though this setting is only based on intuition without rigorous analysis , it indeed leads to good performance as to be reported via simulations in section [ sec : simulation ] , where it is also observed that this setting applies to other sensing matrix ensembles besides the gaussian one .",
    "in sbl , the signal of interest and noise are modeled as random variables . under the common assumption of zero - mean awgns ,",
    "i.e. , @xmath93 , where @xmath91 is the noise variance , the pdf of the compressive measurements is in this paper we assume that the noise variance @xmath91 is known _ a priori_.",
    "this assumption has been widely made in the cs literature , e.g. , @xcite .",
    "moreover , it is shown in @xcite that to estimate @xmath91 jointly with the signal recovery process ( e.g. , in @xcite ) can lead to very inaccurate estimate .    the g - stg prior introduced in section [ sec :",
    "newsparseprior ] is adopted as the sparsity - inducing prior for the sparse signal @xmath4 .",
    "the hyperparameters @xmath44 and @xmath57 are chosen manually according to the reasoning in section [ sec : newsparseprior ] and subsection [ sec : basisselectcondition ] .",
    "numerical simulations will be provided in section [ sec : simulation ] to illustrate their performance . to estimate @xmath94 from the measurements , we assume a gamma hyperprior for @xmath94 : @xmath95 , where we let @xmath96 to obtain a uniform hyperprior ( over a logarithmic scale ) .",
    "so the joint pdf of the observation model is @xmath97 , where @xmath98 is the observation , @xmath4 is the unknown signal of interest , @xmath88 and @xmath94 are unknown parameters , and @xmath91 , @xmath44 , @xmath57 , @xmath99 and @xmath100 are fixed .",
    "the task is to estimate @xmath4 .",
    "note that the exact bayesian inference is intractable since @xmath101 is computationally intractable .",
    "some approximations have to be made .",
    "following from @xcite , we decompose the posterior @xmath102 into two terms as the first term @xmath103 is the posterior for @xmath4 given the hyperparameter @xmath88 , which will be later shown to be a gaussian pdf .",
    "then , we compute the most probable estimates of @xmath88 and @xmath94 , say @xmath104 and @xmath105 , that maximize the second term @xmath106 .",
    "we use @xmath104 to obtain the posterior for @xmath4 . from the perspective of signal estimation , this is equivalent to requiring where @xmath107 refers to @xmath103 at @xmath104 .",
    "similar procedures have been adopted in @xcite .",
    "we will provide theoretical evidence to show that this approach leads to desirable properties in section [ sec : analysis ] .",
    "simulation results presented in section [ sec : simulation ] also suggest that the signal recovery based on this approximation is very effective .    since @xmath108 and @xmath109 are both gaussian , it is easy to show that the posterior for @xmath4 and the marginal distribution for @xmath98 are both gaussian with @xmath110 , @xmath111 , where the maximization of @xmath106 is equivalent to that of @xmath112 by the relation @xmath113 .",
    "we consider @xmath114 as the hidden variable instead of @xmath94 since the uniform hyperprior is assumed over a logarithmic scale .",
    "by @xmath115 we see that the hyperprior for @xmath94 leads to a noninformative prior by setting @xmath116 .",
    "so the log - likelihood function is where @xmath117 is a constant .",
    "the maximizer of @xmath118 will be analyzed in subsection [ sec : analysis ] . in the following we provide an iterative procedure to maximize @xmath118 by recognizing the identities @xmath119 and @xmath120 .",
    "for @xmath38 , @xmath39 , we have where @xmath121 for @xmath122 is a cubic function and @xmath123 , where @xmath124 is the @xmath22th diagonal entry of @xmath125 .",
    "we need the following lemma .    for a cubic function @xmath126 , if @xmath127 and @xmath128 , then @xmath129 has a unique root on @xmath130 .",
    "[ lem : rubicroot ]    by @xmath131 and @xmath132 there exists at least one root in @xmath130 .",
    "we show that this root is unique using contradiction .",
    "suppose there exists more than one positive root .",
    "then there must exist three positive roots and that @xmath133 has two positive stationary points .",
    "that is , the two solutions of @xmath134 are both positive , resulting in that @xmath135 ( contradiction ) .    by lemma",
    "[ lem : rubicroot ] , it is easy to show that the maximum of @xmath118 is achieved at the unique positive root , say @xmath136 , of @xmath137 .",
    "we note that explicit expressions are available for the roots of a cubic function and hence @xmath38 , @xmath39 , can be efficiently updated .      in general",
    ", there is no explicit expression for updating @xmath94 .",
    "since the first and second derivatives of @xmath118 with respect to @xmath114 can be easily computed , @xmath118 can be efficiently maximized with respect to @xmath114 using numerical methods , e.g. , gradient ascending method or newton s method .",
    "in addition , the computational complexity hardly depends on the cs problem dimension .",
    "the computation of the incomplete gamma function @xmath138 is involved in the update of @xmath94 .",
    "this term can be efficiently computed using functions provided in matlab if @xmath57 is properly bounded away from zero . but a numerical integration is needed if @xmath139 . in section [ sec : simulation ] , we observe through simulations that the update of @xmath94 may take considerably long time in the case of @xmath139 . but such differences are negligible in the case of a high - dimensional cs problem since the computation of @xmath94 hardly depends on the problem dimension unlike other computations , such as the update of @xmath88 . [",
    "rem : timeatepsilonis0 ]    as a result , an iterative algorithm can be implemented to obtain @xmath104 and @xmath105 by iteratively updating @xmath125 in ( [ formu : update_sigma ] ) , @xmath140 in ( [ formu : update_mu ] ) , @xmath88 and @xmath94 .",
    "it is easy to show that this algorithm can be implemented using an em algorithm @xcite .",
    "so the likelihood @xmath118 increases monotonically at each iteration and the algorithm is guaranteed to converge .",
    "after convergence , the signal @xmath4 is estimated using its posterior mean @xmath140 .",
    "one shortcoming of the iterative algorithm is that at each iteration a high - dimensional matrix inversion has to be calculated for updating @xmath125 though this computation can be possibly alleviated using the woodbury matrix identity .",
    "we analyze the global and local maxima of the likelihood @xmath118 in ( [ formu : likelihood ] ) in this subsection .",
    "our analysis is rooted in @xcite and shows that the theoretical results on the basic sbl in @xcite can be extended to our case with necessary modifications . in the following , we assume that @xmath116 and @xmath141 is fixed ( it is a similar case if @xmath94 is chosen to maximize @xmath118 as well ) .",
    "thus we may write @xmath118 ( with respect to @xmath88 ) as where @xmath142 is a constant independent of @xmath88 .",
    "we first consider the global maxima in the noise free case .",
    "let @xmath15 , @xmath143 , @xmath144 and @xmath145 .",
    "assume that @xmath146 satisfying @xmath147 is the maximally sparse solution to the linear equation @xmath9",
    ". then , there exists some @xmath148 with @xmath149 and @xmath150 such that at @xmath148 , @xmath118 is globally maximized and the corresponding posterior mean is @xmath151 .",
    "[ thm : globalmaxima ]    note first that @xmath152 by ( [ formu : update_mu ] ) . since @xmath153 there must exist @xmath154 , @xmath149 and @xmath150 , such that @xmath151 at @xmath155 .",
    "in fact , @xmath148 can be any vector satisfying that for @xmath156 , @xmath157 if @xmath158 , and @xmath159 otherwise . in the following",
    "we show that the global maximum of @xmath118 is achieved at @xmath155 .",
    "let @xmath160 and denote @xmath161 the minimum nonzero @xmath38 , @xmath39 .",
    "following from the proof of ( * ? ? ?",
    "* theorem 1 ) , we have @xmath162 and @xmath163 if @xmath164 . in addition",
    ", we have @xmath165 and @xmath166 .",
    "so we have @xmath167 at @xmath155 , which completes the proof .",
    "theorem [ thm : globalmaxima ] shows that the global maximum of the objective function is achieved at the maximally sparse solution .",
    "thus the proposed approach has no structural errors and the remaining question of whether the algorithm can produce this solution is a convergence issue .",
    "further , we have the following result which is similar to that in ( * ? ? ?",
    "* theorem 2 ) .",
    "let @xmath15 and @xmath143 .",
    "every local maximum of @xmath118 is achieved at a sparse @xmath88 with @xmath168 that leads to a sparse posterior mean @xmath140 with @xmath169 , regardless of the existence of noise .",
    "[ thm : localmaxima ]    note that @xmath170 since @xmath171 as @xmath172 , @xmath39 .",
    "so we need only to show that every local maximum of @xmath118 is achieved at a sparse @xmath88 with @xmath168 .",
    "let @xmath173 , which is convex with respect to @xmath174 if @xmath42 .",
    "suppose that @xmath175 is a local maximum point of @xmath118 .",
    "we may construct a closed , bounded convex polytope @xmath176 following from @xcite ( we omit the details ) such that @xmath177 and if @xmath178 , then the second term of @xmath118 , @xmath179 , equals a constant @xmath180 .",
    "in addition , all extreme points of @xmath181 are sparse with support size no more than @xmath182 . as a result",
    ", @xmath175 is a local maximum point of @xmath183 with respect to @xmath178 . by the convexity of @xmath183 , @xmath175 must be an extreme point of @xmath181 with @xmath184 .",
    "theorem [ thm : localmaxima ] states that all local maxima of @xmath118 are achieved at sparse solutions . since we can locally maximize @xmath118 efficiently in practice , based on theorem [ thm : localmaxima ] , we introduce a fast algorithm in section [ sec : greedyalg ] that searches for a sparse solution that locally maximizes @xmath118 .",
    "based on theorem [ thm : localmaxima ] in section [ sec : analysis ] , the following algorithm aims to find a sparse solution that locally maximizes @xmath118 .",
    "we consider the contribution of a single basis vector @xmath185 ( the @xmath186th column of @xmath5 ) , @xmath156 , and determine whether it should be included in the model ( or in the active set ) for the maximization of @xmath118 .",
    "denote @xmath187 that is independent of the @xmath186th basis vector @xmath185 .",
    "then we have @xmath188 and @xmath189 .",
    "using the two identities above , we rewrite the log - likelihood function ( with respect to @xmath88 ) into where @xmath190 denotes @xmath191 after removing the contribution of the @xmath186th basis vector , @xmath192 with @xmath193 and @xmath194 , and @xmath195 , @xmath196 are constants independent of @xmath88 .",
    "note that @xmath197 has been modified by a constant such that @xmath198 . in the following",
    "we compute the maximum point , say @xmath199 , of @xmath200 on @xmath201 .",
    "if @xmath202 , then the basis vector @xmath185 should be preserved in the active set since it gives positive contribution to the likelihood .",
    "otherwise , it should be removed from the active set ( by setting @xmath203 ) .",
    "we consider only the general case @xmath64 , @xmath143 and @xmath141 since it is simple for other cases .",
    "first we have where @xmath204 is a cubic function of @xmath205 with to compute the maximum point of @xmath200 , we need the following result .    for a cubic function @xmath126 ,",
    "let the discriminant @xmath206 . if @xmath207 , then @xmath129 has three distinct real roots . if @xmath208 , then the equation has three real roots including a multiple root .",
    "if @xmath209 , then the equation has one real root and two complex conjugate roots .",
    "[ lem : discriminant ]    note that @xmath210 for @xmath156 since @xmath211 and @xmath212 are positive definite , and then @xmath213 . we divide our discussions into three scenarios .",
    "this case is the same as the update of @xmath88 in subsection [ sec : bayesianinference ] . by lemma [ lem : rubicroot ]",
    ", @xmath215 has a unique solution @xmath199 on @xmath130 .",
    "then it is easy to show that @xmath200 increases monotonically on @xmath216 $ ] and decreases monotonically on @xmath217 .",
    "thus @xmath200 obtains the maximum at @xmath202 .",
    "let @xmath220 denote the determinant of @xmath221 .",
    "we see that @xmath215 has three real distinct roots by lemma [ lem : discriminant ] since @xmath207 .",
    "the two stationary points of @xmath221 lie on different sides of the @xmath222-axis since @xmath223 with @xmath219 . thus the three roots include one negative root and two distinct positive roots since @xmath224 .",
    "denote @xmath225 the largest root .",
    "we see that @xmath200 decreases from @xmath203 to some point , then increases until @xmath226 and decreases again . as a result , @xmath200 obtains the maximum at @xmath227 or @xmath228 .",
    "so we have    * if @xmath229 , then the maximum point is @xmath230 ; * if @xmath231 , then @xmath232 ;      in this scenario , the maximum of @xmath200 is obtained at @xmath232 .",
    "we divide our discussions into three cases : _ 1 ) _",
    "@xmath209 , _ 2 ) _",
    "@xmath208 , and _ 3 ) _ @xmath207 and @xmath233 . in _ case 1",
    "_ , @xmath215 has only one ( negative ) real root . in _ case 2",
    "_ , the equation has three negative roots ( two of them coincide ) , or one negative root and a multiple positive root . in _ case 3 _ the equation has three distinct negative roots .",
    "so in all the cases @xmath235 for @xmath236 , resulting in that @xmath200 decreases monotonically on @xmath201 .    based on the analysis above ,",
    "we can compute efficiently @xmath199 ( given @xmath237 and @xmath238 ) at which the likelihood is maximized with respect to a single basis vector @xmath185 , @xmath156 .",
    "so , the likelihood consistently increases if we update a single @xmath205 at one time with the basis @xmath185 , @xmath156 , properly chosen . as a result",
    ", a greedy algorithm can be implemented . at the beginning",
    ", no basis vectors are included in the model ( i.e. , the active set is empty or all @xmath203 ) .",
    "then choose a vector @xmath185 at each iteration such that it gives the largest likelihood increment by updating @xmath205 from its current value @xmath239 to the maximum point @xmath199 . if @xmath159 and @xmath202 , then the basis vector @xmath185 is added to the model with @xmath240 .",
    "if @xmath157 and @xmath202 , then @xmath205 is re - estimated in the model .",
    "if @xmath157 and @xmath232 , then @xmath185 is removed from the model with @xmath203 .",
    "after that , update @xmath94 as in subsection [ sec : bayesianinference ] .",
    "the process is repeated until convergence ( that is guaranteed since @xmath118 increases monotonically ) . based on the results of @xcite , we see that @xmath140 , @xmath125 , @xmath237 and @xmath238 , @xmath156 , can be efficiently updated without matrix inversions .",
    "consequently , the greedy algorithm is computationally efficient .",
    "the main differences between the proposed algorithm and those in @xcite are the updates of @xmath88 and @xmath94 .",
    "since roots of a cubic equation have explicit expressions and the update of @xmath94 hardly depends on the problem dimension , the proposed greedy algorithm has the same computational complexity as those in @xcite at each iteration .",
    "we study the basis selection condition of the fast algorithm in more details in this subsection .",
    "based on the analysis in subsection [ sec : greedyalgorithm ] , we have the following result .",
    "suppose that the log - likelihood @xmath118 has been locally maximized in the fast algorithm . if @xmath241 for some basis vector @xmath185 , @xmath156 , then @xmath242 where @xmath237 , @xmath238 are as defined in subsection [ sec : greedyalgorithm ] .",
    "[ prop : basisselcondition ]    note that the inequalities @xmath243 and @xmath244 are equivalent to @xmath214 and @xmath219 respectively .",
    "let us suppose that the conclusion does not hold .",
    "then we have @xmath218 and @xmath233 .",
    "it follows from the analysis in subsection [ sec : basisanalysis3 ] that the likelihood increases if @xmath185 is removed from the model , leading to contradiction .",
    "proposition [ prop : basisselcondition ] provides a necessary condition for the basis vectors in the final active set .",
    "note that this condition is generally insufficient since additional requirements are needed in the case of @xmath218 ( e.g. , @xmath207 ) according to the analysis in subsection [ sec : greedyalgorithm ] . in a special case where @xmath243 holds",
    ", we can conclude that @xmath185 is in the model according to subsection [ sec : basisanalysis1 ] .",
    "[ rem : necessarynotsuff ]    the basis selection condition concerns the sparsity level of the solution of the fast algorithm . we have illustrated that different settings of @xmath44 and @xmath57 lead to different sparsity - inducing priors in section [ sec : newsparseprior ] . in the following we will see how the parameters affect the sparsity of the algorithm solution .",
    "we discuss the effects of @xmath44 and @xmath57 separately .",
    "let us first fix @xmath64 and vary @xmath245 .",
    "it is easy to show that as @xmath57 decreases both the terms @xmath246 and @xmath247 increase .",
    "thus the necessary condition in proposition [ prop : basisselcondition ] becomes stronger . as a result ,",
    "the solution of the greedy algorithm will be sparser , which is consistent with the fact that a smaller @xmath57 leads to a more sparsity - inducing prior as shown in section [ sec : newsparseprior ] .",
    "in the case of @xmath43 , the inequality turns to be @xmath248 that coincides with the result in @xcite .",
    "so , the proposed algorithm will produce a sparser solution than that of @xcite by simply setting @xmath67 .",
    "it is not obvious for the case of fixed @xmath67 and varying @xmath44 ( the case @xmath43 has been discussed before ) since , as @xmath44 decreases , the first term @xmath246 increases while the second term @xmath247 decreases . to make a correct conclusion , we observe that @xmath200 in subsection [ sec : greedyalgorithm ] is a strictly increasing function with respect to @xmath64 for any fixed @xmath241 and keeps equal to zero at @xmath203 . as a result , as @xmath44 decreases , it is less likely that the maximum of @xmath200 is achived at a positive point , resulting in that the solution of the greedy algorithm gets sparser .",
    "this is consistent with that a smaller @xmath44 leads to a more sparsity - inducing prior as shown in section [ sec : newsparseprior ] .",
    "in fact , the greedy algorithm produces a zero solution if @xmath47 and @xmath67 since in such a case the maximum of the likelihood is always achieved at the origin with respect to every basis vector .",
    "so intuitively , a smaller @xmath44 should be used to obtain a more sparsity - inducing prior but too small @xmath44 may lead to inaccuracy for the greedy algorithm .",
    "numerical simulations in section [ sec : simulation ] will illustrate that the recommended @xmath44 in subsection [ sec : tau_setting ] is a good choice .",
    "in the case of @xmath47 , the proposed g - stg prior coincides with the gaussian - gamma prior in @xcite . the greedy algorithm using this prior developed in @xcite",
    "is claimed to follow from the same framework in @xcite and maximize the likelihood sequentially .",
    "however , it should be noted that the algorithm in @xcite does not really maximize the likelihood since , if it does , then it should produce a zero solution as discussed above .",
    "specifically , the authors of @xcite compute only a local maximum point of @xmath200 which can not guarantee to increase the likelihood @xmath118 while the global maximum of @xmath200 is always obtained at the origin .",
    "hence the algorithm in @xcite is technically incorrect because of the inappropriate basis update scheme .",
    "moreover , the algorithm in @xcite has not been shown to provide guaranteed convergence .",
    "in this section , we present numerical results to illustrate the performance of the proposed method ( we consider only the fast algorithm in subsection [ sec : greedyalgorithm ] ) .",
    "we consider both one - dimensional synthetic signals and two - dimensional images , and compare with existing methods , including @xmath10 optimization ( bp or bpdn ) , reweighted ( rw- ) @xmath10 optimization @xcite , stomp@xcite , the basic bcs @xcite and bcs with the laplace prior ( denoted by laplace)@xcite .",
    "bcs , laplace and the proposed method are sbl methods .",
    "@xmath10 optimization is a convex optimization method .",
    "rw-@xmath10 is related to nonconvex optimization .",
    "stomp is a greedy method",
    ". the matlab codes of bp , stomp and bcs are obtained from the sparselab package , and that of bpdn is from the @xmath10-magic packagejustin / l1magic . ] .",
    "the code of laplace is available at https://netfiles.uiuc.edu/dbabacan/www/links.html .",
    "the number of iterations is set to 5 for rw-@xmath10 ( i.e. , 5 @xmath10 minimization problems are solved iteratively ) . to make a fair comparison",
    ", we use the same convergence criterion in the proposed method as in bcs and laplace with the stopping tolerance set to @xmath249 .",
    "the performance metrics adopted include the relative mean squared error ( rmse , calculated by @xmath250 ) , the support size ( @xmath251 ) , the number of iterations ( for the three bcs methods ) and the cpu time , where @xmath252 and @xmath4 denote the recovered and original signals , respectively .",
    "an explicit determination of @xmath44 has been recommended in subsection [ sec : tau_setting ] . in the following ,",
    "we show that , indeed , this setting leads to good performance in the signal recovery . in our simulation",
    ", we set the signal length @xmath253 and the number of nonzero entries @xmath254 , and vary the sample size @xmath182 from 40 to 140 with step size of 5 .",
    "the nonzero entries of the sparse signal are randomly located with the amplitudes following from a zero - mean unit - variance gaussian distribution .",
    "we consider two matrix ensembles for the sensing matrix @xmath5 , including gaussian ensemble and uniform spherical ensemble ( with columns uniformly distributed on the sphere @xmath255 ) . to obtain the desired snr , zero - mean",
    "awgns are added to the linear measurements where the noise variance is set to @xmath256 .",
    "we set @xmath257 .",
    "the noisy measurements are used in the following signal recovery process . in the proposed algorithm",
    ", we set @xmath258 which results in both fast and accurate recovery ( this will be illustrated in the next experiment ) .",
    "denote @xmath259 .",
    "we set @xmath260 and consider six values of @xmath261 and @xmath262 .",
    "thus @xmath263 leads to the recommended value of @xmath44 in subsection [ sec : tau_setting ] . for each @xmath182 ,",
    "100 random problems are generated and solved respectively using the proposed method with different @xmath44 .",
    "the metrics are averaged results over the 100 trials .",
    "our simulation results are presented in fig .",
    "[ fig : choice_tau ] . fig .",
    "[ fig : choice_tau_rmse ] and fig .",
    "[ fig : choice_tau_suppsize ] plot the rmses and support sizes of the proposed algorithm with gaussian sensing matrices .",
    "it is shown that the recommended @xmath44 leads to approximately the smallest error with a reasonable number of measurements while the errors are almost the same when the sample size is small for different @xmath44 s .",
    "[ fig : choice_tau_suppsize ] shows that the recommended @xmath44 results in the most accurate estimation of the support size in most cases .",
    "in addition , it is shown that a sparser solution is obtained if a smaller @xmath44 is used in the algorithm as expected .",
    "almost identical performance is shown in fig .",
    "[ fig : choice_tau_usemat_rmse ] and [ fig : choice_tau_usemat_suppsize ] by using the uniform spherical ensemble .",
    "thus , we consider only the uniform spherical ensemble in the following experiments .",
    "we study now the performance of the proposed algorithm with respect to @xmath57 .",
    "we repeat the simulation above using the recommended @xmath44 and consider five values of @xmath264 and @xmath265 .",
    "note that the case @xmath43 corresponds to the laplace prior .",
    "our simulation results are presented in fig .",
    "[ fig : performance_wrt_epsilon ] .",
    "it is shown in fig .",
    "[ fig : performance_wrt_epsilon_rmse ] that the signal recovery error decays as the sample size increases in general . as the sample size is",
    "small the estimation errors differ slightly . but with a reasonable number of measurements a smaller @xmath57 results in a smaller error .",
    "it is shown in fig .",
    "[ fig : performance_wrt_epsilon_suppsize ] that a smaller @xmath57 leads to a sparser solution as expected and more accurate support size estimation .",
    "another advantage of adopting a small @xmath57 can be observed in fig .",
    "[ fig : performance_wrt_epsilon_niter ] where it is shown that a smaller @xmath57 leads to less number of iterations . in general ,",
    "the time consumption is proportional to the number of iterations since the computational workload is approximately the same at each iteration .",
    "[ fig : performance_wrt_epsilon_time ] shows an exception at @xmath139 as illustrated in remark [ rem : timeatepsilonis0 ] . in this case , the update of @xmath94 takes most of the computational time in our simulation . since it is shown in figs .",
    "[ fig : performance_wrt_epsilon_rmse ]  [ fig : performance_wrt_epsilon_niter ] that the performance at @xmath139 and @xmath266 is hardly distinguishable , we use @xmath258 in the rest simulations .",
    "we consider two simulation setups .",
    "in the first case we repeat the simulations above ( i.e. , we fix the @xmath257 and vary @xmath182 ) . in each trial , all methods share the same data .",
    "we adopt the far thresholding strategy in stomp .",
    "our simulation results are presented in fig .",
    "[ fig : comparison_case1 ] .",
    "it is shown in fig .",
    "[ fig : comparison_case1_rmse ] that the reconstruction errors of the three sbl methods ( bcs , laplace and our proposed method ) are very close to each other and larger than those of bpdn , rw - bpdn and stomp if the sample size is small . with a reasonable sample size",
    "it can be seen that our proposed method has the smallest error .",
    "[ fig : comparison_case1_suppsize ] shows the average support size of the recovered signal .",
    "the results of bpdn and rw - bpdn are omitted since they are global optimization methods and their numerical solutions have no exact zero entries . in general , the estimated support sizes of stomp , bcs and laplace increase with the sample size .",
    "as expected the proposed method produces sparser solutions than bcs and laplace .",
    "it is shown that the proposed method can accurately estimate the support of the sparse signal in most cases and has the best performance .",
    "[ fig : comparison_case1_niter ] plots the number of iterations of the three sbl methods , where it is shown that the proposed one uses the least number of iterations and thus is the fastest one in computational speed . on average ,",
    "stomp uses the least computation time ( about @xmath266s ) , followed by the sbl methods ( from @xmath267 to @xmath268s ) , and then bpdn ( about @xmath265s ) and rw - bpdn ( about @xmath269s ) .",
    "we note that a number of solvers have been proposed to solve the bpdn problem with improved speed , e.g. , spgl1@xcite .    in the next simulation",
    "we set the sample size @xmath270 and vary the snr from 0 to 50db with step size of 5db .",
    "the simulation results are presented in fig .",
    "[ fig : comparison_case2 ] . fig .",
    "[ fig : comparison_case2_rmse ] shows that the proposed method has consistently the smallest signal recovery error .",
    "[ fig : comparison_case2_suppsize ] shows that the proposed method produces the sparsest solution and the most accurate support size estimation .",
    "[ fig : comparison_case2_niter ] shows that among the three sbl methods the proposed one uses the least number of iterations at all snr levels . in the low snr regime",
    ", it can be 6 and 3 times less in comparison with laplace and bcs respectively , leading to that the proposed method is much faster than laplace and bcs .    in summary ,",
    "the proposed method has improved performance for sparse signals in comparison with existing ones .",
    "it outperforms its sbl peers in both signal recovery accuracy and computational speed .      in this section ,",
    "we revisit the widely used multiscale cs reconstruction @xcite of the @xmath271 mondrian image in sparselab .",
    "we use the same simulation setup , i.e. , we choose the `` symmlet8 '' wavelet as the sparsifying basis with a coarsest scale @xmath272 and a finest scale @xmath273 .",
    "the number of wavelet samples is @xmath274 and the sample size of cs methods is @xmath275 .",
    "the parameters of bp and stomp with the fdr and far thresholding strategies ( denoted by fdr and far respectively ) are set as in sparselab .",
    "since the wavelet expansion of the mondrian image is compressible but not exactly sparse , we set @xmath276 in laplace and our proposed method as in bcs , where @xmath277 denotes the variance of the entries of @xmath98 .",
    "table [ table : time ] presents the experimental results over 100 trials .",
    "linear reconstruction from @xmath278 wavelet samples has a reconstruction error of @xmath279 that represents a lower bound of the error of the considered cs methods .",
    "the global optimization method bp has the smallest error among the cs methods , followed by bcs , laplace , the proposed method and stomp .",
    "the presented results verify again that the proposed method produces a sparser solution than bcs and laplace .",
    "in fact , it produces the sparsest solution among all the methods .",
    "so it is reasonable that the proposed method has a slightly worse reconstruction error in comparison with bcs and laplace since the original signal is not exactly sparse .",
    "we note that the proposed method is faster than bcs and laplace .",
    "fdr uses the least time but has the worst accuracy . in comparison with",
    "far , the proposed method is slightly slower but more accurate .",
    "finally , it can be observed that the proposed method has the most stable performance among the cs methods except bp by comparing the standard deviation of the metrics .",
    "we note that bp can be accelerated using recently developed algorithms for @xmath10 optimization .",
    "[ fig : image ] shows examples of reconstructed images where faithful reconstructions of the mondrian image can be observed .    .averaged relative mses , cpu times and number of nonzero entries ( @xmath280 ) for multiscale cs reconstruction of the mondrian image . [",
    "cols=\"<,<,<,<\",options=\"header \" , ]     [ table : time ]",
    "the sparse signal recovery problem in cs was studied in this paper . within the framework of bayesian cs ,",
    "a new hierarchical sparsity - inducing prior was introduced and efficient signal recovery algorithms were developed .",
    "similar theoretical results on the global and local optimizations of the proposed method were proven as that for the basic sbl .",
    "the main algorithm was shown to produce sparser solutions than its existing sbl peers .",
    "numerical simulations were carried out to demonstrate the improved performance of the proposed sparsity - inducing prior and solution .",
    "the proposed g - stg prior preserves the general structure of existing hierarchical sparsity - inducing priors and can be implemented in other sbl - based methods with ease .",
    "z.  yang , c.  zhang , and l.  xie , `` robustly stable signal recovery in compressed sensing with structured matrix perturbation , '' _ ieee transactions on signal processing _ ,",
    "60 , no .  9 ,",
    "pp . 46584671 , 2012 .",
    "d.  donoho , y.  tsaig , i.  drori , and j.  starck , `` sparse solution of underdetermined linear equations by stagewise orthogonal matching pursuit , '' _ available online at http://www.cs.tau.ac.il/@xmath281idrori/stomp.pdf_ , 2006 .",
    "n.  pedersen , d.  shutin , c.  manchn , and b.  fleury , `` sparse estimation using bayesian hierarchical prior modeling for real and complex models , '' _ arxiv preprint , available at http://arxiv.org/pdf/1108.4324v2_ , 2011 .",
    "d.  shutin and b.  fleury , `` sparse variational bayesian sage algorithm with application to the estimation of multipath wireless channels , '' _ ieee transactions on signal processing _ , vol .",
    "59 , no .  8 , pp . 36093623 , 2011 .",
    "m.  tipping and a.  faul , `` fast marginal likelihood maximisation for sparse bayesian models , '' in _ proceedings of the ninth international workshop on artificial intelligence and statistics _ , vol .  1 , no .  3.1em plus",
    "0.5em minus 0.4emciteseer , 2003 .",
    "s.  babacan , l.  mancera , r.  molina , and a.  katsaggelos , `` non - convex priors in bayesian compressed sensing , '' in _ proceedings of the 17th european signal processing conference ( eusipco 2009 ) _ , 2009 , pp ."
  ],
  "abstract_text": [
    "<S> sparse bayesian learning ( sbl ) is a popular approach to sparse signal recovery in compressed sensing ( cs ) . in sbl , </S>",
    "<S> the signal sparsity information is exploited by assuming a sparsity - inducing prior for the signal that is then estimated using bayesian inference . in this paper , a new sparsity - inducing prior </S>",
    "<S> is introduced and efficient algorithms are developed for signal recovery . the main algorithm is shown to produce a sparser solution than existing sbl methods while preserving their desirable properties . </S>",
    "<S> numerical simulations with one - dimensional synthetic signals and two - dimensional images verify our analysis and show that for sparse signals the proposed algorithm outperforms its sbl peers in both the signal recovery accuracy and computational speed . </S>",
    "<S> its improved performance is also demonstrated in comparison with other state - of - the - art methods in cs .    </S>",
    "<S> compressed sensing , g - stg prior , greedy algorithm , sparse bayesian learning . </S>"
  ]
}