{
  "article_text": [
    "how many stocks are principal drivers of the whole stock market ?",
    "how many medical tests are sufficient to describe a person s health status ?",
    "these are all important questions being asked nowadays , and these types of questions can often be formulated as questions about whether there exists a low - dimensional linear dependency in a high - dimensional dataset . to be more specific ,",
    "suppose we observe a collection of @xmath0 variables @xmath7 , each @xmath8 , but possibly correlated .",
    "can we tell whether there exists a subset of the gaussian variables @xmath9 such that all other variables are almost simply linear combinations of them ?",
    "this question has been studied very carefully under the context of low - rank matrix approximation and subspace tracking , in particular under the setting when the number of variables is larger than the sample size , i.e. , @xmath10 . for a review of low - rank approximation methods ,",
    "see @xcite . for recent algorithms on subspace tracking ,",
    "see @xcite , @xcite , @xcite , and @xcite .",
    "one of the most important components of all these methods involves the estimation of the rank of the covariance matrix , or the dimension of the signal subspace , @xmath4 .",
    "this estimation problem has been first studied by @xcite , @xcite , @xcite , and @xcite .",
    "more recent approaches from statistics include @xcite , @xcite , and @xcite .",
    "one principal technique in the statistical approaches to the rank estimation problem is eigenvalue thresholding based on the principal component analysis ( pca ) , where we look for the `` cut off '' among singular values of the covariance matrix when they drop to nearly 0 . however , when @xmath11 , pca of the sample correlation matrix will return with @xmath5 positive eigenvalues , which is less than the truth @xmath4 .",
    "moreover , although we may get low - rank solutions to many problems , an important question hasnt been completely addressed : how low is the rank , really ?",
    "in other words , how can we justify that the low - rank results we obtain is in the correct dimension ?",
    "indeed , the inference on @xmath4 as a parameter was not clear . in the context of the above paragraphs",
    ", the question is that , given the data , can we make any probabilistic statements about @xmath4 as a parameter ?",
    "can we test if the correlation matrix of a joint distribution is low - rank ?",
    "can we build confidence intervals that covers @xmath4 with high probability ?",
    "even these basic questions have not been completely answered yet .",
    "our approach to this problem is through the study the low rank from the extreme value of the joint distribution . consider a gaussian vector @xmath12 where @xmath13 has @xmath14 s on the diagonal , so that each @xmath15 is standard normally distributed . in this case",
    ", classical extreme value theory suggests that for a large collection of @xmath13 s , the magnitude of the extreme value @xmath16 is about @xmath17 ( @xcite and @xcite ) .",
    "although it seems like that there is not much difference in this asymptotic rate among @xmath13 s , this is not the complete story yet . in @xcite",
    ", it is shown that the asymptotics of the extremes from a low - rank gaussian distribution is below the square root of the rank , which can be far below the dimension of the distribution .",
    "this result suggests the dependence of the gaussian extreme on the rank of the covariance matrix .",
    "a generalized version of such asymptotics is studied here , and it leads to an explicit description of such dependence , which we call the rank - extreme ( rex ) association .",
    "here are the heuristics of this rex approach : note that for any @xmath18 covariance matrix @xmath13 that is positive semi - definite , has ones on the diagonal and has rank @xmath4 . through its eigen - decomposition , we can write @xmath19 , where @xmath20 $ ] is a @xmath21 matrix with columns @xmath22 s such that @xmath23 . thus , we can write @xmath24 where @xmath25 based on this fact , we reduce the problem to one of sphere packing problems in @xmath26 , as described in section  [ sec_sp ] . in this case , with the help from the classical arguments from coding theory and sphere packing , we first derive a universal asymptotic bound for the maximal correlation between any collection of @xmath0 directions and a uniformly distributed direction in @xmath26 .",
    "the bound is found to be @xmath27 we show that the rate @xmath28 takes on a trichotomy phenomenon to tend toward @xmath14 , @xmath29 or @xmath30 , as the ratio @xmath31 converges to @xmath32 some fixed positive @xmath33 or @xmath34 respectively .",
    "based on these results , we propose a test of overall significance in regressions for general @xmath5 and @xmath0 as an extension of the universal threshold proposed in @xcite .",
    "we further show that this bound is sharp and is attained when the @xmath22 s are i.i.d .",
    "uniformly distributed unit vectors over the sphere @xmath35 .    as a consequence of the above sphere packing results , we derive a universal asymptotic upper bound for @xmath36 , the gaussian extreme with an arbitrary @xmath13 with @xmath37 as @xmath38 this upper bound explicitly describes the relationship between the magnitude of the extreme and the rank of the correlation matrix . as in the sphere packing problem described before , we show that this bound is sharp in the sense that if @xmath13 is generated by @xmath22 s that are i.i.d .",
    "uniformly distributed over the sphere @xmath39 the bound is attained .",
    "we were unable to find a similar sharp bound for extremes from low - rank gaussian distributions in the literature . due to the dependence of this bound on the rank , we call the bound @xmath2 the rank - extreme ( rex ) asymptotic bound .",
    "this association inspires us to use the extremes in drawing information about the underlying rank @xmath4 as a parameter .    one interesting fact about the rex rate @xmath2",
    "is that it takes on a trichotomy phenomenon too like its sphere packing counterpart .",
    "indeed , the rate @xmath2 is shown to grow below , at , or above @xmath40 asymptotically according to whether the ratio @xmath31 tends below , at , or above a fixed positive constant @xmath41 , which is the unique solution to the equation @xmath42 based on this rex trichotomy , we also propose inference methods under this situation and show that finite sample inference can be made about @xmath4 .",
    "it is noteworthy that the rex inference is based on the asymptotics in @xmath0 rather than in @xmath5 .",
    "thus , the method works even when @xmath43 when pca of the sample correlation matrix would return @xmath44 positive eigenvalues . in the extreme",
    ", the inference can be made even when there is only one sample but billions of variables .",
    "the freedom of the inference for any @xmath5 is one important advantage of the rex approach .",
    "this key feature and the easiness of finding the maximal entry in each sample allow fast detection of low - rank correlation structure in high - dimensional data .",
    "the paper is organized as follows . in section  [ sec_sp ]",
    "we introduce our sphere packing method and develop the rex asymptotic bound of the @xmath45 norm of gaussian vectors .",
    "we discuss two applications of the rex bound in section  [ subsec_overall ] and section  [ subsec_posi ] .",
    "we show that the rex bound is sharp when the correlation matrices are uniformly randomly generated in section  [ sec_gem ] . in this case , we study the rex trichotomy phenomenon of the @xmath45 norm of gaussian vectors in section  [ subsec_trichotomy ] , and we propose an inference method for low - dimensional linear dependency in a high - dimensional dataset in section  [ subsec_inference ] . in section  [ sec_noise ]",
    "we consider the case when we observe the data with noise .",
    "we illustrate through simulations both the rex trichotomy phenomenon and the performance of the rex inference in section  [ sec_simu ] . in section  [ sec_discussion ]",
    "we conclude our findings and discuss possible future work along this direction . we provide proofs of the key theorems in the appendix .",
    "our key observation is that for any @xmath46 , if we consider the spherical coordinates , then we have @xmath47 where @xmath48 and @xmath49 .",
    "moreover , it is well - known that @xmath50 and @xmath51 are independent",
    ". therefore , @xmath52 thus , since the distribution @xmath53 is well known , as long as we understand the distribution of @xmath54 , we will understand that of the absolute extremes @xmath16 .",
    "an asymptotic upper bound for @xmath54 is summarized below .",
    "[ thm.kstar.arb ] for arbitrary unit vectors @xmath55 and a uniformly distributed unit vector @xmath51 over @xmath35 , the random variable @xmath54 satisfies that for any fixed @xmath56 uniformly for any @xmath57 , @xmath58 in particular , if @xmath59 then @xmath60    we shall show the sharpness of this bound in section  [ subsec_trichotomy ] .",
    "note that the inner product @xmath61 is the cosine of the angle between @xmath22 and @xmath51 .",
    "therefore , a geometric interpretation of the theorem above is that : if we have @xmath0 directions in a @xmath4-dimension space , regardless of where and how they are located , the minimum acute angle between these directions to a uniformly random direction is at least @xmath62 interestingly , this fact indicates that a lower bound for the sine of the smallest angle between any collection of @xmath0 directions and a uniformly random direction in @xmath26 is about @xmath63 .",
    "one intuitive explanation of this interesting angle is from the fact that the surface area of a spherical cap with half - angle @xmath64 on @xmath35 is in the order of @xmath65 thus , with @xmath0 points on the sphere , the minimal angle between a uniform direction and these points should be in the order of the @xmath66-th root of @xmath0 .",
    "we note also that the bound holds uniformly for any @xmath57 , which includes the case when the dimension @xmath4 is larger than the number of vectors @xmath0 .",
    "in fact , the bound @xmath28 behaves differently according to whether the ratio @xmath3 converges to @xmath30 , a fixed positive @xmath67 , or @xmath68    a.   if @xmath69 , then as @xmath70 @xmath71 b.   if @xmath72 for fixed @xmath73 , then as @xmath70 @xmath74 c.   if @xmath75 , then as @xmath70 @xmath76    we can see that the above trichotomy cover the complete range of @xmath77 $ ] . as an intuitive explanation from the view of random packing , the above trichotomy can be attributed to two facts : ( 1 ) the more `` evenly '' distributed the @xmath22 s are on the sphere , the larger the @xmath54 is ; ( 2 ) the number of orthants in @xmath26 grows exponentially as @xmath4 grows . from ( 1 ) , we consider the situation when we have an `` evenly distributed mesh '' of @xmath0 points @xmath22 s on the sphere @xmath35 .",
    "suppose we now generate a uniformly random direction @xmath78 by ( 2 ) , if the size of the `` mesh '' grows slower than the exponential rate , then the points on the `` mesh '' are so `` sparse '' that a random direction @xmath51 can be almost orthogonal to them ; if the size of the `` mesh '' does grow exponentially as the rate of the number of orthants , then a random direction @xmath51 will stay around some angle to the points on the `` mesh '' ; on the other hand , if the size of the `` mesh '' grows faster than the exponential rate , then the points on the `` mesh '' are so `` dense '' that a random direction will `` hit '' the mesh with high probability .",
    "thus , the case @xmath79 is the balanced case in which @xmath54 converges to a constant between @xmath30 and @xmath14 .",
    "we shall note that the bound holds uniformly for any @xmath57 , which includes the case when the dimension @xmath4 is larger than the number of vectors @xmath0 .",
    "in particular , in the limit @xmath80 , this random direction will become almost orthogonal to any collection of directions , which includes all axes !",
    "this limit is consistent with the well - known result that a random direction is orthogonal to almost everything in high - dimensional euclidean spaces .",
    "consider a regression model with @xmath5 i.i.d observations of @xmath0 explanatory variables ( @xmath0 may be bigger than @xmath5 ) and one response variable .",
    "suppose the distribution of the error in the regression model is spherically symmetric .",
    "the problem of the overall significance of the model is to test whether the coefficients of the explanatory variables are all zeros . in this case",
    ", we can view the problem as observing @xmath81 vectors in @xmath82 .",
    "to apply theorem  [ thm.kstar.arb ] , we standardize each vector so that they all have length 1 as directions @xmath55 for the covariates and @xmath51 for the response variable .",
    "if the explanatory variables are `` irrelevant '' to the response , so that the distribution of @xmath51 does not depend on @xmath55 and is uniform over @xmath83 , then theorem  [ thm.kstar.arb ] says the followings : no matter where and how the @xmath22 s are located , the minimal angle between these @xmath22 s and @xmath51 is at least @xmath84 , and the maximal correlation between the response and the covariates is at most @xmath85 this fact thus leads to a test of the overall significance in regressions : the explanatory variables in the model are considered to be `` relevant '' to the response variable if and only if their maximal correlation is above @xmath85    the geometric interpretation of this threshold rule is easy to see : since the upper bound is for a uniformly distributed @xmath51 and arbitrary @xmath86 , if the correlations between @xmath22 s and @xmath51 are all below the threshold @xmath87 there is hardly evidence to believe that @xmath51 is different from a uniformly random direction to the @xmath22 s .",
    "thus there is hardly evidence to believe that the response is correlated to the covariates in the regression model .",
    "one interesting fact to note here is that if @xmath88 as @xmath89 , then this threshold is approximately @xmath90 which is the universal threshold @xcite .",
    "thus , the proposed threshold can be regarded as a generalization of the universal threshold for any @xmath5 and @xmath0 .",
    "moreover , it can be seen that this method works even when @xmath5 is finite and @xmath0 is large .",
    "therefore , the threshold at @xmath91 provides a simple test of the overall significance in a regression model for finite samples with large @xmath0 . we will report more details about this test in a future paper",
    "since @xmath92 and @xmath93 as @xmath94 , we derive the following rex universal asymptotic bound for the @xmath45 norm of gaussian vectors which holds uniformly over all correlation matrices .",
    "[ thm.k.arb ] for any vector of @xmath0 standard gaussian variables",
    "@xmath95 with @xmath96 , the random variable @xmath97 satisfies that for any fixed @xmath98 @xmath99 in particular , if @xmath100 then @xmath101    we shall show the sharpness of this bound in section  [ subsec_trichotomy ] .",
    "this theorem implies that for large @xmath0 and @xmath4 , an arbitrary @xmath0-dimensional gaussian vector will essentially stay within a hypercube which centers at the origin and has a radius of @xmath102 where @xmath4 is the rank of the correlation matrix . geometrically speaking ,",
    "this fact together with the fact that the @xmath103 norm of a @xmath0-dimensional gaussian vector is about @xmath104 indicate that a low - rank gaussian vector will be very likely to appear around the `` corners '' of the hypercube where it intersects with the sphere of radius @xmath105 .",
    "algorithms in search for low - dimensional structure can be designed based on this geometric observation .",
    "we should also note that the function @xmath2 is monotone increasing in both @xmath4 and @xmath0 .",
    "hence , the rex bound confirms our intuition that the higher the rank is , the higher the extreme value of gaussians can reach . on the other hand , if the extreme value is high , then its rank can not be low .",
    "thus theorem  [ thm.k.arb ] allows fast detection of a low - rank : if the maxima in the samples are large , then the underlying rank must be high or full ; on the other hand , if the maxima are small , then the full rank assumption is questionable and the small magnitude of the maxima may be due to an underlying low - rank structure .",
    "another observation of the bound is that it takes on a trichotomy phenomenon too for different ranges of @xmath4 , as its sphere packing counterpart :    a.   if @xmath69 , then as @xmath70 @xmath106 b.   if @xmath72 for fixed @xmath73 , then as @xmath70 @xmath107 c.   if @xmath75 , then as @xmath70 @xmath108    from the above trichotomy , we can see that the bound @xmath2 indeed cover the entire scope from constants to @xmath109 in particular , since the range of the function @xmath110 is @xmath111 for @xmath112 this case can be regarded as the intermediate step between the very low rank limits and the full rank limit @xmath113 .",
    "the bound @xmath2 explicitly describes the relationship between the possible extreme value from a gaussian vector and the rank of its covariance matrix . due to the dependence of the bound of extremes and the rank",
    ", we call this phenomenon the rex trichotomy",
    ".    we should note that the rex asymptotic bound is not always sharp for all covariance matrices .",
    "for example , consider the equicorrelation matrices with 1 s in the diagonal entries and @xmath114 s as off diagonals .",
    "this correlation matrix has full rank @xmath0 for any @xmath115 . on the other hand , as @xmath114 approaches 1 ,",
    "the extreme from the multivariate gaussian with this correlation matrix is essentially @xmath116 .",
    "nevertheless , the rex asymptotic bound reveals the fact that the extremes from gaussian vectors contains much information about the rank , and we can utilize this information in search of low - rank structure .",
    "statistical inference after variable selection in linear models has recently gained great interest .",
    "this interest is sparked by both the prevalence of data - driven variable selections and the severe bias that the selection process may impact on the inference in the selected submodel .",
    "for an excellent review on variable selection procedures , see @xcite . for recent discussions on the problems of the conventional inference after variable selection , see @xcite , @xcite and @xcite .",
    "the posi method in @xcite provides one way to protect the statistical inference in the selected model by considering simultaneous inference on the partial slopes in selected models .",
    "the simultaneity here is over the set of submodels that may be possible selected .",
    "thus , the resulting inference based on the simultaneity is valid and conservative .",
    "moreover , the important advantage of such inference is that it is universally valid over any data - driven variable selection rules .",
    "this robustness over selection procedures makes the posi method a practical way to achieve valid inference .",
    "the posi inference is based on the posi constant which is essentially the maximal @xmath117-statistics over all possible combinations of variables and submodels . for a regression model with @xmath0 explanatory variables ,",
    "this set of all possible combinations consists of @xmath118 @xmath117-statistics .",
    "when the variance of the noise is known or when @xmath119 is large , the @xmath117-statistics become standard gaussian variables .",
    "also , it is easily seen that the rank of the correlation matrix of these @xmath120 gaussian variables is at most @xmath0 .",
    "thus , by theorem  [ thm.k.arb ] , the rex bound of the posi constant for any design matrix grows at the rate of @xmath121 as @xmath89 , as developed in @xcite .",
    "more importantly , in practice people may set some integer @xmath122 and select a model with a size less than or equal to @xmath123 .",
    "when the number of variables in a selected submodel is restricted by @xmath123 , then combinatorics easily show that we have a total of @xmath124 gaussian variables .",
    "therefore , by theorem  [ thm.k.arb ] , the rex asymptotic bound for posi constants in this case , @xmath125 , follows that @xmath126 for any design matrix . hence , if we only consider submodels with sizes @xmath123 such that @xmath127 , then the posi method can be much more efficient than its upper bound rate @xmath128 some special cases of small @xmath123 s include :    1 .",
    "@xmath129 is a constant , @xmath130 2 .",
    "@xmath131 , @xmath132 3 .",
    "@xmath133 for fixed @xmath134 , @xmath135",
    "once the rex asymptotic bound is developed , it is of interest whether it is sharp . to this end",
    ", we study the following model in this section : the @xmath136-th variable ( @xmath137 ) in the @xmath138-th observation ( @xmath139 ) , @xmath140 , is generated as @xmath141 where    1 .",
    "@xmath22 s are i.i.d .",
    "uniformly distributed over the @xmath66-sphere @xmath142 2 .",
    "@xmath143 and is independent of @xmath22 s .    to fix ideas , for now we set @xmath144 and assume that we only have one observation of many variables",
    "thus , in this section we suppress the subscript @xmath138 and write @xmath140 as @xmath145      the next theorem shows that in this case , the asymptotics magnitude of @xmath54 attains the upper bound :    [ thm.kstar.gem ] if @xmath22 s and @xmath51 are i.i.d . uniformly distributed over the @xmath66-sphere @xmath146 then uniformly for all @xmath57 , as @xmath70 @xmath147    this result shows the limits of @xmath54 when @xmath22 s are i.i.d .",
    "uniformly distributed over the sphere :    a.   if @xmath69 , then @xmath148 b.   if @xmath72 for fixed @xmath73 , then @xmath149 c.   if @xmath75 , then @xmath150    the theorem is proved through a random packing argument that is inspired by @xcite .",
    "we also note here that a parallel version of the above asymptotics were independently developed in @xcite and @xcite , in a related but different context . in these papers ,",
    "the limiting distribution of @xmath54 is carefully studied .",
    "therefore , we shall not go into further details of the convergence , but concentrate on the application of the limit .",
    "the immediate application of this result is the uniform rex rate of @xmath97 :    [ thm.k.gem ] if @xmath151 where @xmath152 with @xmath22 s are i.i.d .",
    "uniformly distributed over the @xmath66-sphere @xmath146 then as @xmath153 and @xmath89 , @xmath154    the rex trichotomy with i.i.d .",
    "@xmath22 s shows the following phase transition of @xmath16 :    a.   if @xmath94 and @xmath69 , then @xmath155 b.   if @xmath156 for fixed @xmath73 , then @xmath157 c.   if @xmath75 , then @xmath158    for the case @xmath159 , the equation @xmath160 is of our particular interest . we should notice that the function @xmath161 is a monotone increasing function in @xmath67 .",
    "therefore , the solution to is unique and it is @xmath162 from , we see that if @xmath163 , then @xmath164 in probability .",
    "in fact , @xmath165 is the only asymptotic rate of @xmath4 such that @xmath166 . if @xmath79 with @xmath167 , then for large @xmath0 , @xmath168 with probability tending to 1 . on the other hand , if @xmath79 with @xmath169 then for large @xmath0 , @xmath170 with probability tending to 1 .",
    "indeed , for @xmath171 , @xmath172 , hence we are going back to the classical extreme value result of @xmath173 .",
    "the above facts provide us with the basis of a nature classification of low - ranks .",
    "in fact , if we simply set a threshold at @xmath40 , then by , if @xmath4 grows faster than @xmath174 or @xmath4 grows as @xmath175 with @xmath169 then with high probability @xmath176 will be above @xmath177 on the other hand , if @xmath4 grows slower than @xmath174 or @xmath4 grows as @xmath178 with @xmath179 then with high probability @xmath176 will go below @xmath180 with theorem  [ thm.k.gem ] , we are able to make these judgements accurately as long as @xmath0 is large , based on only a few observations .    based on the magnitude of the corresponding extremes , we call the case @xmath181 the `` super - low rank '' case ,",
    "we call the case @xmath182 the `` exact - low rank '' case , and we call the case when @xmath183 the `` moderately - low rank '' case . the solution to , @xmath41 ,",
    "will be called the rex separation constant .      in section  [ subsec_trichotomy ] ,",
    "we study the case when both @xmath89 and @xmath184 we now turn to the another question : if @xmath0 is large and @xmath4 is just a fixed number , can we make inference about @xmath4 ?",
    "the theorem below answers this question with a confirmation .",
    "the following rex inference is a simple application of the slutsky theorem : since @xmath185 and @xmath186 in probability as @xmath89 , the limiting distribution of @xmath176 is @xmath187    [ thm.fixed ] if @xmath151 where @xmath152 with @xmath22 s are i.i.d .",
    "uniformly distributed over the @xmath66-sphere @xmath146 and if @xmath4 is fixed as @xmath89 , then @xmath188    with this theorem , we convert the inference about @xmath4 to a simple inference problem on the degrees of freedom of a @xmath189 distribution .",
    "it is important to note here that the asymptotics in this theorem are in @xmath0 rather than in the sample size @xmath5 .",
    "thus , the classical finite sample theories can be directly applied here . in this case , many inference methods about @xmath4 are readily available in literature , either in the context of the @xmath189-distribution or as a problem of inference about the shape parameter in the gamma distribution .",
    "furthermore , we can utilize the square - root transformation as the variance stabilizing transformation for @xmath190-distribution ( section 3.2 .",
    "@xcite ) to achieve approximate confidence intervals for moderate @xmath191 since these inference procedures have been well developed , we shall not go into further theoretical details of the specific inference problems .",
    "the major advantage of this rex inference is that it is finite sample inference .",
    "thus , it allows fast detection of the low - rank . in practice ,",
    "we may face datasets with very large @xmath0 and completely unknown correlation structure . in this case",
    ", we may assume that the correlations are i.i.d . uniformly distributed .",
    "by quickly checking the maximal entry in each sample and applying theorem  [ thm.fixed ] , we may get a good sense of the fixed rank .",
    "in particular , the rex inference can be made with @xmath44 , when the eigenvalue thresholding procedures will fail . in the extreme",
    ", the inference can be made with only one sample .",
    "thus , fast detection of low - rank is possible through this approach .",
    "it is also noteworthy that the asymptotics in @xmath0 indicates that the higher @xmath0 is , the more accurate the inference is .",
    "thus , instead of the `` curse of dimensionality , '' the rex inference can be viewed as a `` blessing of dimensionality . ''",
    "in this section we consider the case when our observations may be subject to measurement errors , i.e. , when we do not observe @xmath24 directly but instead observe @xmath192 where @xmath193 with independent @xmath194 in this case , although the rank of the correlation matrix of @xmath195 is @xmath4 , the covariance matrix of @xmath196 is of rank @xmath0 .",
    "nevertheless , we show that as long as the noise is small , we can still draw information about @xmath4 by utilizing the extremes .    to see the argument above",
    ", we notice that if we let @xmath197 then @xmath198 thus , as long as @xmath199 as @xmath89 , then @xmath200 in this case , the asymptotic results in @xmath196 are identical to those in @xmath195 :    for any vector of @xmath0 standard gaussian variables @xmath95 with @xmath96 , and for @xmath201 where @xmath193 with independent @xmath202 if @xmath199 , then for any fixed @xmath98 @xmath203 in particular , if @xmath204 then @xmath205    we also have the corollary on the sharpness of the bound .    if @xmath199 , and if @xmath22 s are i.i.d . uniformly distributed over",
    "the @xmath66-sphere @xmath146 then as @xmath153 and @xmath89 , @xmath206    the rex inference is also valid in @xmath196 for a finite @xmath4 :    if @xmath22 s are i.i.d . uniformly distributed over the @xmath66-sphere @xmath146 if @xmath4 is fixed as @xmath89 , and if @xmath199 , then @xmath207",
    "we study @xmath208 simulated datasets .",
    "we set @xmath209 , and consider four different deficient rank scenarios : @xmath210 , @xmath211 , @xmath212 and @xmath213 .",
    "the data are generated as in section  [ sec_gem ] .",
    "we also consider the case when the @xmath15 s are i.i.d .",
    ", so that @xmath214 . note here @xmath215 so @xmath216 is about a fixed rank or super - low rank case , @xmath217 is about an exact - low rank case , while @xmath218 and @xmath219 are moderately - low rank cases . to emphasize the dependence of the distribution of extremes on the rank , we shall denote @xmath220 if the rank of the covariance matrix of @xmath195 is @xmath4 .",
    "we first show the plot of kernel density estimates ( using r default bandwidth ) of the @xmath221 s for different @xmath4 s in figure  1 below .     for @xmath209 . the brown vertical line at the threshold of @xmath40 almost separate the distribution @xmath222 in the super - low rank case from the distributions @xmath223 and @xmath224 in the moderately - low rank case.,width=288 ]    from the picture",
    ", we can clearly see the rex trichotomy .",
    "if we set a threshold at @xmath40 ( shown as the brown vertical line ) , then    1 .   for the super - low rank case @xmath216 ,",
    "the distribution of @xmath222 is almost all on the left hand side of the threshold of @xmath40 .",
    "it can be also easily checked that the distribution of @xmath222 matches the @xmath225 density very well ; 2 .",
    "for the exact - low rank case @xmath217 , the distribution of @xmath226 is around @xmath40 ; 3 .   for the moderately - low rank case @xmath218 and @xmath219 , the distributions of @xmath221 are almost on the right hand side of the threshold of @xmath227 and they have similar distributions even though the ranks substantially differ .",
    "their distributions are also close to the distribution of @xmath228 when @xmath15 s are i.i.d .",
    "gaussian .    from figure  1",
    "we see that there is a small right tail of the distribution @xmath222 that is on the right hand side of the threshold of @xmath40 and there are tiny left tails of the distributions @xmath223 and @xmath224 that are on the left hand side of the threshold .",
    "thus , the probability of correct classification here is close to 1 but not exactly 1 .",
    "however , note only one sample ( @xmath144 ) has been considered by far . with a few more samples our classification can be much more accurate .",
    "now consider we observe @xmath5 samples @xmath229 @xmath230 and @xmath231 if we let @xmath232 and simply look at the averages , @xmath233 , then we can do much better .",
    "figure  2 below illustrates the case @xmath234 .",
    "when we compare the distributions of @xmath235 with @xmath236 simulated datasets , we see that we have probability one for complete separation of the super - low rank case and moderately - low rank case .",
    "these categories can be classified by @xmath221 for large @xmath0 as described above .     for @xmath209 and @xmath234 .",
    "the distributions are more convergent , and the brown vertical line at the threshold of @xmath40 separates the distribution @xmath237 in the super - low rank case from the distributions @xmath238 and @xmath239 in the moderately - low rank case.,width=288 ]    we note here that since in the low - dimensional case the distribution of @xmath221 is approximately @xmath53 , for which the sufficient statistics are @xmath240 , the simple average @xmath235 is not even admissible as a point estimator .",
    "therefore , although @xmath235 is doing a great job , even better procedures should be available . since our focus is on showing only the possibility of this asymptotic classification , we will not go into further details of this problem .      in this section",
    "we study the coverage probability of the rex confidence interval proposed in section  [ subsec_inference ] .",
    "we set @xmath209 or @xmath241 for which @xmath242 or @xmath243 respectively .",
    "we then study the cases when @xmath4 ranges from @xmath244 to @xmath245 respectively .",
    "we make @xmath44 by setting @xmath5 to be the rounded number of @xmath246 we simulated @xmath247 datasets for each scenario .",
    "the confidence intervals are built through a slightly conservative variant of the square - root transformation of @xmath248 : since the asymptotic variance of @xmath248 is bounded by @xmath249 , we take the square - root transform of @xmath250 . however , we match the mean of @xmath251 at @xmath252 thus , the lower and upper ends of the @xmath253-confidence intervals , @xmath254 and @xmath255 , are obtained by solving equations @xmath256 and @xmath257 respectively .",
    "the coverage probabilities of these confidence intervals are summarized in table  1 .",
    ".coverage of @xmath258 rex confidence intervals for different @xmath4 s when @xmath259 and @xmath260 . [ cols=\"^,^,^,^,^,^,^,^,^ \" , ]     despite the difficult fact that the sample size @xmath44 when pca based methods would fail , the coverage probabilities of these confidence intervals are satisfactory .",
    "this simulation result of the rex method shows the feasibility of the finite sample inference for low - dimensional structures in high - dimensional datasets .",
    "this article provides an asymptotic upper bound , called the rank - extreme ( rex ) bound , for arbitrary gaussian distributions as @xmath2 .",
    "these results describe the relationship between @xmath45 norms of gaussian vectors and the ranks of their covariance matrices .",
    "we then use the rex association reversely towards the inference problem of low - dimensional linear dependency in high - dimensional data when the correlations are generated by inner products of i.i.d .",
    "uniformly distributed unit vectors .",
    "we show that if @xmath4 is fixed , it is possible to make inference about @xmath4 . we also show that if @xmath4 grows along with @xmath0 , it is possible to compare @xmath4 s rate of growth with @xmath261 .",
    "all the asymptotic results are in @xmath0 but not in @xmath5 , so that fast detection and finite sample inference of a low - rank structure is possible by extracting the information contained in the maximum norm of gaussian vectors .",
    "it is important to note that extreme values as statistics are well - known to be not resistant to outliers .",
    "the asymptotic results in this article rely on the gaussian assumption too .",
    "therefore , the robustness of the inference needs to be carefully studied .",
    "we would also like to generalize the gaussian assumptions to more general cases , for example , spherically symmetric distributions .",
    "we should note that in this paper we found the rex bound for the @xmath45 norm of gaussian vectors with arbitrary correlation .",
    "we are interested in more accurate upper and lower bound for gaussian vectors with special correlation structures .",
    "a related direction is that in this paper we consider rex inference under uniformly distributed @xmath22 s",
    ". it would be interesting to study the rex inference under a more general distribution of @xmath22 s , or under the framework of pca models .",
    "the optimality of this approach needs to be studied as well .",
    "we hope to borrow from bayesian methods in these studies .",
    "one indication of the asymptotic rate @xmath2 is that when @xmath22 s are i.i.d . uniformly distributed over the @xmath66-sphere @xmath35",
    ", we may estimate the rank of the correlation matrix @xmath4 .",
    "this estimator @xmath262 is easily defined as the rounded number of the solution to the equation @xmath263 since the function @xmath264 is monotone increasing in @xmath4 , @xmath262 is unique .",
    "we will study the asymptotic performance of this estimator in more details in a future paper .",
    "there is an interesting fact that the case @xmath265 covers the entire scope of that @xmath176 can take for arbitrary correlation matrices .",
    "indeed , the function @xmath161 is monotone increasing in @xmath67 and its range is from @xmath30 to @xmath266 .",
    "therefore , it seems that we could concentrate on the case @xmath265 and look at the inference about @xmath33 for example , the testing problem of @xmath267 or @xmath268 .",
    "the above fact also seems to be remotely related to the johnson - lindenstrauss theorem @xcite . in the j - l theorem",
    ", it is shown that a random projection from @xmath269 to a subspace of dimension @xmath270 can well preserve the @xmath103 norm of any vector in @xmath269 .",
    "it would be interesting to see how well such a random projection can preserve the @xmath45 norm of the vector .",
    "the author appreciates the great suggestions from l.  d.  brown , a.  buja , t.  cai , j.  fan , j.  s.  marron , and h.  shen .",
    "the author also thanks j.  berger , r.  berk , a.  budhiraja , e.  candes , l.  de haan , j.  galambos , e.  george , j.  hannig , t.  jiang , i.  johnstone , a.  krieger , r.  leadbetter , d.  lin , j.  liu , w.  liu , y.  liu , z.  ma , e.  pitkin , s.  provan , d.  small , l.  wang , d.  zeng , l.  zhao , and c.  zong for very helpful discussions .",
    "the author is particularly grateful for l.  a.  shepp for his inspiring introduction of the random packing literature .",
    "to show , it is enough to show that for any fixed @xmath271 that @xmath272 , @xmath273    we follow the proof of theorem  6.3 in @xcite .",
    "for , by considering the bonferroni bound that @xmath274 where @xmath275 is any coordinate of @xmath51 or projection of @xmath51 onto a unit vector .",
    "now we use the fact that @xmath276 i.e. , for any @xmath277 ,",
    "@xmath278 ~=~ \\frac{1}{{{\\mathrm{b}}}(1/2,(d-1)/2 ) } \\ , \\int_{a^2}^1 x^{-1/2 } ( 1-x)^{(d-3)/2 } dx.\\ ] ]    due to the bound that @xmath279 , for any @xmath280 , we have @xmath281 now we further bound by @xmath282 note that uniformly for any @xmath57 , @xmath283 as @xmath89 . thus ,",
    "@xmath284 as @xmath89 regardless of the rate of @xmath285 .    to see , note that if @xmath286 then @xmath287 thus we may let @xmath288 in   to get",
    ". also , if @xmath94 but @xmath289 then is further bounded by @xmath290 thus we have .",
    "it is easy to show . to show , note that for any @xmath291 , @xmath292 we will show each of the two summands in the last line can be made small with a proper choice of @xmath293    by the proof of theorem  [ thm.kstar.arb ]",
    ", we see that @xmath294 note also that @xmath295 .",
    "thus by the chernoff bound for @xmath296 distribution , @xmath297 due to and , we let @xmath298 in the case when @xmath299 both and converge to @xmath30 .",
    "since we already have the upper bound , it is enough to show that for any fixed @xmath271 such that @xmath300 , @xmath301    we use the independence of @xmath22 s as in @xcite and @xcite to bound @xmath302 we will lower - bound the absolute value of the exponent in by recalling . we first note that by integrating by parts , we have @xmath303 for any @xmath304 thus , by the important fact that @xmath305 , we have that for @xmath70 @xmath306 in the last step of , we used again the fact that uniformly for any @xmath57 , @xmath307 as @xmath308 it is now easy to see that @xmath309 as @xmath89 regardless of the rate of @xmath285 , which completes the proof of theorem  [ thm.kstar.gem ] .",
    "t.  hastie , r.  tibshirani , and j.  friedman . _ the elements of statistical learning  data mining , inference , and prediction_. 2nd ed .",
    "3rd printing .",
    "springer series in statistics , new york : springer , 2009 .",
    "w.  b.  johnson and j.  lindenstrauss .",
    "_ extensions of lipschitz mappings into a hilbert space _ , conference in modern analysis and probability ( new haven , conn .",
    "1982 ) , contemporary mathematics , vol .",
    "26 , providence , ri : american mathematical society , 189206 , 1984 ."
  ],
  "abstract_text": [
    "<S> it is important to detect a low - dimensional linear dependency in high - dimensional data . </S>",
    "<S> we provide a perspective on this problem , called the rank - extreme ( rex ) association , through studies of the maximum norm of a vector of @xmath0 standard gaussian variables that has a covariance matrix of rank @xmath1 . </S>",
    "<S> we find a simple asymptotic upper bound of such extreme values as @xmath2 . </S>",
    "<S> this upper bound is shown to be sharp when the entries of the correlation matrix are generated by inner products of i.i.d . </S>",
    "<S> uniformly distributed unit vectors . </S>",
    "<S> this upper bound also takes on an interesting trichotomy phenomenon depending on the limit of @xmath3 . </S>",
    "<S> based on this rex approach , we propose several methods for high - dimensional inference . </S>",
    "<S> these applications include a test of the overall significance in regressions , a refinement of valid post - selection inference when the size of selected models is restricted , a classification of deficient ranks based on the magnitude of the extreme , and an inference method for low - ranks . </S>",
    "<S> one advantage of this approach is that the asymptotics are in the dimensions @xmath4 and @xmath0 but not in the sample size @xmath5 . </S>",
    "<S> thus , the inference can be made even when @xmath6 , which allows fast detection of low - dimensional structure . </S>",
    "<S> furthermore , the higher the dimension is , the more accurate the inference is . </S>",
    "<S> therefore , these results can be regarded as a `` blessing of dimensionality . '' </S>"
  ]
}