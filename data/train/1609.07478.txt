{
  "article_text": [
    "[ sec : intro]optimization techniques for high - dimensional problems have become the work - horses for most data - analysis and machine - learning methods . with the rapid increase of available data , major challenges",
    "occur as the number of optimization variables ( weights ) grows beyond capacity of current systems .",
    "the idea of screening refers to eliminating optimization variables that are guaranteed to _ not _ contribute to any optimal solution , and can therefore safely be removed from problem .",
    "such screening techniques have received increased interest in several machine learning related applications in recent years , and have been shown to lead to very significant computational efficiency improvements in various cases , in particular for many types of sparse methods .",
    "screening techniques can be used either as a pre - processing before passing the problem to the optimizer , or also interactively during any iterative solver ( called dynamic screening ) , to gradually reduce the problem complexity during optimization .",
    "while existing screening methods were mainly relying on geometric and problem - specific properties , we in this paper take a different approach .",
    "we propose a new framework allowing screening on general convex optimization problems , using simple tools from convex duality instead of any geometric arguments .",
    "our framework applies to a very large class of optimization problems both for constrained as well as penalized problems , including most machine learning methods of interest .",
    "our main contributions in this paper are summarized as follows :    1 .",
    "we propose a new framework for screening for a more general class of optimization problem with a simple primal - dual structure .",
    "2 .   the framework leads to a large set of new screening rules for machine learning problems that could not be screened before .",
    "furthermore , it also recovers many existing screening rules as special cases .",
    "3 .   we are able to express all screening rules using general optimization complexity notions such as smoothness or strong convexity , getting rid of problem - specific geometric properties .",
    "4 .   our proposed rules are dynamic ( allowing any existing algorithm to be additionally equipped with screening ) and safe ( guaranteed to only eliminate truly unimportant variables ) .",
    "[ [ related - work . ] ] related work .",
    "+ + + + + + + + + + + + +    the concept of screening in the sense of eliminating non - influential data points to reduce the problem size has originated relatively independently in at least two communities .",
    "coming from computational geometry , @xcite has proposed a screening technique for the minimum enclosing ball problem for a given set of data points . here screening",
    "can be interpreted as simply removing points which are guaranteed to lie in the strict interior of the final ball .",
    "later @xcite improve the threshold for this rule in the minimum enclosing ball setting .",
    "independently , the breakthrough work of @xcite gave the first screening rules for the important case of sparse regression , as given in the lasso .",
    "since then , there have been many extensions and alterations of the general concept . while @xcite exploits geometric quantities to bound the the lasso dual solution within a compact region , we recommend the survey paper by @xcite for an overview of geometric methods for lasso screening .",
    "sphere - region based methods differ from dome - shaped regions as used in @xcite in choosing different centers and radii to bound the dual optimal point . apart from being geometry",
    "specific , most existing approaches such as @xcite are not agnostic to the regularization parameter used , but instead are restricted to perform screening along the entire regularization path ( as the regularization parameter changes ) .",
    "this is known as sequential screening , and restricts its usability to optimization algorithms obtaining paths .",
    "in contrast , our proposed framework here allows any internal optimization algorithms to be equipped with screening .    despite the importance of constrained problems in many applications ,",
    "much less is known about screening for constrained optimization , in contrast to the case of penalized optimization problems . for the dual of the hinge loss svm , which is a box - constrained optimization problem",
    ", @xcite proposed a geometric screening rule based on the intersection region of two spheres , in the sequential setting of varying regularizer .",
    "more recently ,  @xcite provided new screening rules for that case in the dynamic setting using a method similar to our approach .",
    "however their method is restricted to the svm case .    as a first step to allow screening for more general optimization objectives ,",
    "@xcite gives duality gap based screening rules for multi - task and multi - class problems ( in the penalized setting ) under for a wider class of objectives  @xmath1 .",
    "nevertheless , their approach is restricted to assume separability of @xmath1 over the group structure , which limits the screening rules , in the sense of not covering standard group lasso for example . also in @xcite , authors assume the similar problem formulation as in @xcite but a bit more general .",
    "the focus in @xcite is on screening rules for svm problems rather than general framework .",
    "they derive the screening rules for svm by considering standard hinge and @xmath2-insensitive loss with regularization formulation which is close to the empirical risk minimization framework here , but has more limited applications in terms of generalization of the screening rules .",
    "we here provide screening rules for a more general framework of box constrained optimization , while hinge - loss svm happens to be a special case of this .",
    "our proposed approach can be shown to recover many of the other existing rules including e.g. @xcite and @xcite , but significantly generalizing the method to general objectives and constraints as well as regularizers .",
    "the rest of the paper is organized as follows : in section  [ sec : primaldual ] , we discuss our framework for screening .",
    "section  [ sec : duality_gap_certificates ] is devoted to deriving the information about optimal points in terms of gap functions . sections [ sec : screening_constrained ] and [ sec : screening_penalized ] utilizes the framework and tools derived in previous sections to provide screening rules for constrained and penalized case respectively . in the end , we provide a small illustrative experiment for screening on simplex and @xmath0-constrained and also discuss that which of the existing results can be recovered using our algorithm in section [ sec : exp ] .",
    "in this paper , we consider optimization problems of the following primal - dual structure .",
    "as we will see , the relationship between primal and dual objectives has many benefits , including computation of the duality gap , which allows us to have a certificate for approximation quality .    a very wide range of machine learning optimization problems can be formulated as and , which are dual to each other : @xmath3\\   \\\\",
    "\\label{eq : dual}\\tag{b}\\ \\ \\ \\          \\min_{{\\bm{w}}\\in { \\mathbb{r}}^{d } } \\quad & \\big [ \\ \\       { \\mathcal{o}_{\\hspace{-1pt}b}}({\\bm{w } } ) : =   \\ \\ f^*({\\bm{w } } )      \\ + \\ g^*(-a^\\top{\\bm{w } } ) \\!\\!\\!&\\big]\\ \\end{aligned}\\ ] ] the two problems are associated to a given data matrix @xmath4 , and the functions @xmath5 and @xmath6 are allowed to be arbitrary closed convex functions .",
    "the functions @xmath7 in formulation   are defined as the _ convex conjugates _ of their corresponding counterparts @xmath8 in  . here",
    "@xmath9 and @xmath10 are the respective variable vectors . for a given function @xmath11 ,",
    "its conjugate is defined as @xmath12 the association of problems and is a special case of fenchel duality .",
    "more precisely , the relationship is called _ fenchel - rockafellar duality _ when incorporating the linear map @xmath13 as in our case , see e.g. ( * ? ? ?",
    "* theorem 4.4.2 ) or ( * ? ? ?",
    "* proposition 15.18 ) , see the appendix  [ app : primaldual ] for a self - contained derivation .",
    "the two main powerful features of this general duality structure are first that it includes many more machine learning methods than more traditional duality notions , and secondly that the two problems are fully symmetric , when changing respective roles of  @xmath1 and  @xmath14 . in typical machine learning problems ,",
    "the two parts typically play the roles of a data - fit ( or loss ) term as well as a regularization term .",
    "as we will see later , the two roles can be swapped , depending on the application .",
    "[ [ optimality - conditions . ] ] optimality conditions .",
    "+ + + + + + + + + + + + + + + + + + + + + +    the first - order optimality conditions for our pair of vectors @xmath15 in problems   and   are given as    @xmath16    @xmath17    see e.g. ( * ? ? ?",
    "* proposition 19.18 ) .",
    "the stated optimality conditions are equivalent to @xmath18 being a saddle - point of the lagrangian , which is given as @xmath19 if @xmath20 and @xmath21 , see appendix  [ app : primaldual ] for details .",
    "[ [ the - constrained - case . ] ] the constrained case .",
    "+ + + + + + + + + + + + + + + + + + + + +    any constrained convex optimization problem of the form @xmath22 for a constraint set @xmath23 can be directly written in the form by using the indicator function of the constraint set as the penalization term  @xmath14 .",
    "( the indicator function @xmath24 of a set @xmath25 is defined as @xmath26 if @xmath27 and @xmath28 otherwise . )",
    "[ [ the - partially - separable - case . ] ] the partially separable case .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a very important special case arises when one part of the objective becomes separable .",
    "formally , this is expressed as @xmath29 for univariate functions @xmath30 for @xmath31 $ ] . nicely in this case , the conjugate of @xmath14 also separates as @xmath32 .",
    "therefore , the two optimization problems and write as @xmath33 where @xmath34 denotes the @xmath35-th column of @xmath13 .    crucially in this case , the optimality conditions and   now become separable , that is    @xmath36    note that the two other conditions and are unchanged in this case .",
    "the duality gap for our problem structure provides an optimality certificate for our class of optimization problems",
    ". it will be the most important tool for us to provide guaranteed information about the optimal point ( as in section [ sec : info ] ) , which will then be the foundation for the second step , to perform screening on the optimal point ( as we will do in the later sections [ sec : screening_constrained ] and [ sec : screening_penalized ] ) .      for the problem structure and",
    "as given by fenchel - rockafellar duality , the _ duality gap _ for any pair of primal and dual variables @xmath37 and @xmath38 is defined as @xmath39 .",
    "non - negativity of the gap  that is weak duality  is satisfied by all pairs .",
    "most importantly , the duality gap acts as a certificate of approximation quality  the true optimum values @xmath40 and @xmath41 ( which are both unknown ) will always lie within the ( known ) duality gap .",
    "[ [ the - gap - function . ] ] the gap function .",
    "+ + + + + + + + + + + + + + + + +    for the special case of differentiable function @xmath1 , we can study a simpler duality gap @xmath42 purely defined as a function of @xmath43 , using the optimality relation , i.e. @xmath44 .    [ [ the - wolfe - gap - function . ] ] the wolfe - gap function .",
    "+ + + + + + + + + + + + + + + + + + + + + + +    for any constrained optimization problem defined over a bounded set @xmath23 and @xmath27 , the wolfe gap function ( also known as hearn gap or frank - wolfe gap ) is defined as the difference of @xmath1 to the minimum of its linearization over the same domain .",
    "formally , @xmath45 it is not hard to see that the convenient wolfe gap function is a special case of our above defined general duality gap @xmath46 , for @xmath14 being the indicator function of the constraint set  @xmath23 , and @xmath47 . for more details ,",
    "see appendix [ app : eq_fw_gd ] , or also ( * ? ? ?",
    "* appendix  d ) .",
    "as we have mentioned , any type of screening will crucially rely on first deriving safe knowledge about the unknown optimal points of our given optimization problem . here , we will use the duality gap to obtain such knowledge on the optimal points @xmath48 and @xmath49 of the respective optimization problems and respectively .",
    "proofs are provided in appendix  [ app : optimal_point ] .",
    "our first lemma shows how to bound the distance between any ( feasible ) current dual iterate and the solution @xmath50 using standard assumptions on the objective functions .",
    "[ lem : stronglyconvex ] consider the problem with optimal solution @xmath51 .",
    "for @xmath1 being @xmath52-smooth , we have @xmath53    the following corollary will be important to derive screening rules for penalized problems in section  [ sec : screening_penalized ] , as well as box - constrained problems ( section [ subsec : box_screening ] ) .",
    "[ cor : dgaprestriction ] we consider the problem setup and , and assume @xmath1 is @xmath52-smooth .",
    "then @xmath54 here @xmath55 is the duality gap function as defined in equation .",
    "the following two results hold for general constrained optimization problems of the form , where  @xmath14 is the indicator function of a constraint set @xmath56 and hence are useful for deriving screening rules for such problems .    [",
    "lem : fwgaprestriction ] consider problem and assume that @xmath1 is @xmath52-strongly convex over a bounded set @xmath23 .",
    "then it holds that @xmath57 where @xmath58 is an optimal solution and @xmath59 is the wolfe - gap function of @xmath1 over the bounded set @xmath23 .",
    "[ cor : restric : smooth ] assuming @xmath1 is @xmath60-smooth as well as @xmath52-strongly convex over a bounded set @xmath23 , we have @xmath61",
    "in the following , we will develop screening rules for constrained optimization problems of the form  , by exploiting the structure of the constraint set for a variety of sparsity - inducing problems .",
    "first of all , we give a general lemma which we will be using in rest of the paper to derive screening rules when any of the function in [ eq : primal ] and [ eq : dual ] is indicator function .",
    "the above equation also suggest that @xmath64 .",
    "lemma [ lem : general_constrained_lemma ] is very crucial in further deriving screening rules for constrained optimization problem as well as norm penalized problems whose conjugate is indicator function of the dual norm .",
    "optimization over unit simplex @xmath65 is a important class of constrained problems , as it includes optimization over any finite polytope . in this case",
    ", the columns of @xmath13 describe the vertices , and @xmath43 are barycentric coordinates representing the point @xmath66 .",
    "formally , @xmath67 is the indicator function of the unit simplex @xmath68 in this case .        in the following theorem [ thm : simplex_smooth ]",
    "we now assume smoothness and strong convexity of function @xmath1 to provide screening rules for simplex problems , in terms of an arbitrary iterate @xmath43 , without knowing @xmath58 .",
    "[ thm : simplex_smooth ] let @xmath1 be @xmath60-smooth and @xmath52-strongly convex over the unit simplex @xmath68 .",
    "then for simplex constrained optimization @xmath69 we have the following screening rule , for any @xmath70 $ ] @xmath72    our general screening rules for simplex constrained problems as in theorem [ thm : simplex_smooth ] allows many practical implications .",
    "for example , new screening rules for squared loss svm and minimum enclosing ball problem come as a direct consequence .",
    "the squared hinge - loss svm problem in its dual form is formulated as @xmath73 \\vspace{-1 mm }   \\ ] ] over a unit simplex constraint @xmath74 . here for given data examples @xmath75 and corresponding labels @xmath76 , the matrix @xmath13 collects the columns @xmath77 , see e.g. @xcite .",
    "we obtain the following novel screening rule for square loss svm :        the primal and dual for the minimum enclosing ball problem is given as the following pair of optimization formulations and respectively .",
    "@xmath79 \\label{eq : min_enclosing_ball } \\\\",
    "\\qquad & ~ \\min_{{\\bm{x}}\\in{\\triangle}\\subset{\\mathbb{r}}^n } \\ \\ \\ { \\bm{x}}^\\top a^\\top a{\\bm{x}}+ { \\bm{c}}^\\top { \\bm{x}}\\ , \\label{eq : dualball}\\end{aligned}\\ ] ] where @xmath80 is a vector whose @xmath81 element @xmath82 is @xmath83 , see for example @xcite or our appendix  [ app : simplex_screening ] .",
    "@xmath0-constrained formulations are very widely used in order to induce sparsity in the variables . here below",
    "we provide results for screening on general @xmath0-constrained problems , that is @xmath85 for @xmath86 ( or a scaled version of the @xmath0-ball ) .",
    "proofs are provided in appendix  [ app : l1ball_screening ] .",
    "[ thm : l1_constrained_screening ] for general @xmath0-constrained optimization @xmath87 , the optimality condition   gives rise to the following screening rule at the optimal point , for any @xmath70 $ ] @xmath88      [ thm : l1_smooth_constrained ] let @xmath1 be @xmath60-smooth and @xmath52-strongly convex over the @xmath0-ball .",
    "then for @xmath0-constrained optimization @xmath87 we have the following screening rule , for any @xmath70 $ ] @xmath89      elastic net regularization as an alternative to @xmath0 is often used in practice , and can outperform the lasso , while still enjoying a similar sparsity of representation @xcite .",
    "the elastic net is given by the expression @xmath90 here below we provide novel result for screening on general elastic net constrained problems , that is @xmath85 for  @xmath23 being the elastic net constraint , or a scaled version of it .",
    "proofs are provided in appendix [ app : elastic_net_screening ] .",
    "[ thm : elastic_net_constrained ] for general elastic net constrained optimization @xmath91 where @xmath92 , the optimality condition   gives rise to the following screening rule at the optimal point , for any @xmath70 $ ] latexmath:[\\ ] ] hence , whenever @xmath400 .",
    "+ the above arguments also show the significance of symmetry in our formulation as structure and .",
    "this formulation provides our framework more flexibility to be used in larger class of problem .",
    "+ now , @xmath402 equation comes directly from corollary [ cor : dgaprestriction ] .",
    "hence finally we get the screening rules for general elastic net penalty problem which is very similar to screening for @xmath403 problems :          [ lem : grplasso_basic ] if we use the same notation as mentioned in section [ subsec : l1_l2_genral ] to write a vector @xmath43 as a concatenation of smaller group vectors @xmath108 such that @xmath109 $ ] and correspondingly the matrix @xmath13 can be denoted as the concatenation of column groups @xmath110 $ ] . now",
    "if we consider an optimization problem of the form @xmath404 + at the optimal point @xmath58 and dual optimal points @xmath50 , we get rules according to the following equation : @xmath405    dual of the problem is given by @xmath406 hence for the indicator function @xmath407 by lemma [ lem : subgrad_ind ] @xmath408 now , by the optimality condition @xmath409 , and since this holds , hence @xmath410 should satisfy the required constrained which is needed to be in the set of subgradients of @xmath411 according to conditions given above .",
    "hence ,        another view on the screening of above optimization problem can be seen from the optimality condition . the optimization problem in lemma [ lem : grplasso_basic ]",
    "can be taken as partially separable problem and from the optimality condition @xmath416 from equations and , we conclude that if @xmath417      this is an explicit case of the optimization problem mentioned in lemma [ lem : grplasso_basic ] . by observation we know that , @xmath422 now applying the findings of theorem [ thm : smooth_grp_lasso ] , we get @xmath423      [ lem : gengrpnorm ] sparse multi - task and multi class model @xcite - if we consider general problem of the form @xmath424 where the regularization function @xmath425 is such that @xmath426 and @xmath427 $ ] .",
    "we write @xmath428 $ ] for variable of the dual problem .",
    "then the screening rule becomes @xmath429 here @xmath430 is the vector of the @xmath431 element group of each vector @xmath244",
    ".    equations pair and can be used interchangeably by replacing primal with dual and @xmath1 with @xmath14 .",
    "hence the partial separable primal - dual pair and can also be used interchangeably . by comparing equation with and",
    ", we observe that separable function @xmath432 takes the place of separable @xmath171 in and @xmath433 takes the place of @xmath162 .",
    "hence we apply the optimality condition to get ( with exchanged primal dual variable ) @xmath434 hence if , @xmath435 now , @xmath436 using equations and , the screening rule comes out to be @xmath437          the general idea behind the sphere test method @xcite is to consider the maximum value of desired function in a spherical region which contains the optimal dual variable . in context of our general framework and",
    ", we obtain this case when considering an @xmath440 penalty or @xmath107 penalty .",
    "that means @xmath14 is a norm and hence from lemma [ lem : conjugates ] , @xmath171 becomes the indicator function of the dual norm ball of @xmath441 .",
    "the dual norm function for @xmath440 norm is of the form @xmath442 and for @xmath107 norm , it is @xmath443 .",
    "hence , we try to find maximum value of the function of the forms @xmath444 where @xmath445 the ball @xmath446 also contains the optimal dual point @xmath50 .",
    "if the maximum value of @xmath447 is less than some particular value for all the @xmath448 in the ball hence @xmath449 will also be less than that particular value and that is the main reason we try to find maximum of @xmath447 over the ball @xmath446 .",
    "@xmath450 similar arguments can be given in the @xmath107-norm case .",
    "a variety of existing screening test for lasso and group lasso are of this flavor of sphere tests .",
    "the difference between these approaches mainly lie in the way of choosing the center and bounding the radius of the sphere , such that the optimal dual variables lie inside the sphere .",
    "our method can be seen as a general framework for such a sphere test based screening with dynamic screening rules .",
    "our method can be interpreted as a sphere test with the current iterate of the dual variable @xmath157 as a center of the ball , and we obtain the bound on the radius in terms of duality gap function ."
  ],
  "abstract_text": [
    "<S> we propose a new framework for deriving screening rules for convex optimization problems . </S>",
    "<S> our approach covers a large class of constrained and penalized optimization formulations , and works in two steps . </S>",
    "<S> first , given any approximate point , the structure of the objective function and the duality gap is used to gather information on the optimal solution . in the second step , this information is used to produce screening rules , i.e. safely identifying unimportant weight variables of the optimal solution . </S>",
    "<S> our general framework leads to a large variety of useful existing as well as new screening rules for many applications . </S>",
    "<S> for example , we provide new screening rules for general simplex and @xmath0-constrained problems , elastic net , squared - loss support vector machines , minimum enclosing ball , as well as structured norm regularized problems , such as group lasso . </S>"
  ]
}