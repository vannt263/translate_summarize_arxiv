{
  "article_text": [
    "in this paper , the discrete gaussian channel with intersymbol interference ( isi ) @xmath0 will be considered , where the vector @xmath1 represents the isi , and @xmath2 is white gaussian noise with variance @xmath3    a similar continuous time model has been studied in gallager @xcite .",
    "he showed that it could be reduced to the form @xmath4 where the @xmath5 are eigenvalues of the correlation operator .",
    "the same is true also for the discrete model ( [ modell ] ) , but the reduction requires knowledge of the covariance matrix @xmath6 whose eigenvectors should be used as new basis vectors .    here",
    "however such knowledge will not be assumed , our goal is to study universal coding for the class of isi channels of form ( [ modell ] ) .    as a motivation ,",
    "note that the alternate method of first identifying the channel by transmitting a known `` training sequences '' has some drawbacks . because the length of the training sequence is limited , the estimation of the channel can be imprecise , and the data sequence is thus decoded according to an incorrect likelihood function .",
    "this results in an increase in error rates @xcite , @xcite and in a decrease in capacity @xcite .",
    "as the training sequence contains no valuable information , the longer it is the less information bits can be carried .",
    "one can think this problem could be solved by choosing the training sequence sufficiently large to ensure precise channel estimation , and then choose the data block sufficiently long , but this solution seldom works due to the delay constraint , and to the slow change in time of the channel .",
    "so we will give a straightforward method of coding and decoding , without any channel side information ( csi ) to achieve this , we generalise the result of csiszr and krner @xcite to gaussian channel with white noise and isi , using an analogue of the maximal mutual information ( mmi ) decoder in @xcite",
    ".    we will show that our new method is not only universal , not depending on the channel , but its error exponent is better in many cases than gallager s @xcite lower bound for the case when complete csi is available to the receiver .",
    "previously , gallager s error exponent has been improved for some channels , using an mmi decoder , such as for discrete memoryless multiple - access channels @xcite .",
    "we do nt use generalised maximum likelihood decoding @xcite , but a generalised version of mmi decoding .",
    "this is done by firstly approximating the channel parameters by maximum likelihood estimation , and then adopt the message whose mutual information with the estimated parameters is maximal . by using an extension of the powerful method of types",
    ", we can simply derive the capacity region , and random coding error exponent . at the end",
    ", we have a more general result , namely : we show how the method of types can be extended to a continuous , non memoryless environment .    the structure of this correspondence is as follows . in section [ definitions ]",
    "we generalise typical sequences to isi channel environment .",
    "the main goal of section [ theorem ] is to give a new method of blind detection . in section [ numericalr ]",
    "we show by numerical results , that for some parameters the new error exponent is better than gallager s random coding error exponent . in section [ discussion ]",
    "we discuss the result , and give a general formula to the channels with fading .",
    "let @xmath7 be a sequence of positive numbers with limit 0 .",
    "the sequence @xmath8 is @xmath7-typical to an n - dimensional continuous distribution @xmath9 , denoted by @xmath10 , if @xmath11 where @xmath12 denotes the density function , and @xmath13 the differential entropy of @xmath9 .",
    "similarly sequences @xmath8,@xmath14 are jointly @xmath7-typical to @xmath15 dimensional joint random distribution @xmath16 denoted by @xmath17 , if @xmath18    in the same way , a sequence @xmath19 is @xmath7-typical to the conditional distribution @xmath20 , given that @xmath21 , denoted by @xmath22 if @xmath23    for simplifying the proof , in the following , @xmath24 is always the @xmath25 dimensional i.i.d .",
    "standard normal distribution , the optimal input distribution of a gaussian channel with power constrain 1 .",
    "the conditional distribution will be chosen as @xmath26 with density @xmath27 where @xmath28 and @xmath29 where @xmath30 is understood for @xmath31 .",
    "so in this case @xmath32\\ ] ]    the limit of the entropy of @xmath33 as @xmath34 , is @xmath35 where @xmath36 see @xcite ( here @xmath37 is the correlation matrix ) .",
    "so the limit of the average mutual information per symbol , that is @xmath38 is equal to @xmath39 moreover the sequence @xmath40 is non - increasing ( see @xcite ) .",
    "we will consider a finite set of channels that grows subexponentially with @xmath25 , and in the limit dense in the set of all isi channels . to this end , define the set of approximating isi , as @xmath41 where @xmath42 is the length of the isi , @xmath43 is the power constraint per symbol , and @xmath7 is the `` coarse graining '' , intuitively the precision of detection .",
    "similarly we define the set of approximating variances as @xmath44 these two sets form the approximating set of parameters , denoted by @xmath45 .",
    "below we set @xmath46 $ ] , @xmath47 , @xmath48    [ isitype ] the isi type of a pair @xmath49 is the pair @xmath50 defined by @xmath51    note that this type concept does not apply to separate input or output sequences , only to pairs @xmath52 .",
    "we summarise the result of this section : the first lemma shows that the above definition of isi type is consistent , in the sense that @xmath19 is conditionally @xmath53 typical given @xmath54 , at least in the case when @xmath55 is not too large .",
    "lemma [ lem0 ] gives the properties which we need for our method , and proves that almost all randomly generated sequences has these properties .",
    "lemma [ lem1 ] gives an upper bound to the set of output signals , which are `` problematic '' thus typical to two codewords , namely they can be result of two different codeword with different channel .",
    "lemma [ lem3 ] shows that if the channel parameters estimated via maximum likelihood ( ml ) , the codewords and the noise can not be very correlated .",
    "lemma [ lem4 ] gives the formula of the probability of the event that an output sequence is typical with another codeword with respect to another channel .",
    "all lemmas are used in theorem [ mainthm ] , which gives the main result , and defines the detection method strictly .    when @xmath56 so the detected variances is in the interior of the set of approximating variances , then @xmath57    indeed , if @xmath56 then @xmath58 with @xmath59 we get @xmath60    and by the definition @xmath61 if @xmath62    [ lem0 ] for arbitrarily small @xmath63 , if @xmath25 is large enough , there exists a set @xmath64 , with @xmath65 , where @xmath9 is the @xmath25-dimensional standard normal distribution , such that for all @xmath66 , @xmath67 @xmath68 @xmath69    take @xmath25 i.i.d , standard gaussian random variables @xmath70 .",
    "fix @xmath71 @xmath72 . by chebishev s inequality ,",
    "@xmath73 from this , with @xmath74 @xmath75 which means that , there exist a set in @xmath76 whose @xmath77 measure is at least @xmath78 and for all sequences from this set it is true that @xmath79 similarly there exist such sets for all @xmath68 in @xmath80 . by a completely analogous procedure we can make sets which satisfy [ lem0 - 1 ] and [ lem0 - 2 ] . the intersection of these sets @xmath9-measure at least @xmath81 . as @xmath82 ,",
    "this proves the lemma .",
    "the lebesgue measure will be denoted by @xmath83 ; its dimension is not specified , it will be always clear what it is .",
    "[ megj1 ] if @xmath25 is large enough then the set @xmath84 in lemma [ lem0 ] satisfies @xmath85 and for any @xmath86-dimensional continuous distribution @xmath87 @xmath88 where @xmath89 is the set of typical sequences to @xmath90 , see ( [ tipikussag ] )    since @xmath91 by the previous lemma , by using @xmath92 on @xmath93 , and @xmath94 , we get @xmath95 if @xmath25 is large enough . similarly from @xmath96 @xmath97    the next lemma is an analogue of the packing lemma in @xcite    [ lem1 ] for all @xmath98 , there exist at least @xmath99 different sequences in @xmath100 which are elements of the set @xmath101 from lemma [ lem0 ] , and for each pair of isi channels with @xmath102 , @xmath103 , and for all @xmath104    @xmath105 } \\label{pack - lem}\\end{aligned}\\ ] ]    provided that @xmath106    we shall use the method of random selection . for fixed @xmath107 constans ,",
    "let @xmath108 be the family of all ordered collections @xmath109 , of @xmath86 not necessarily different sequences in @xmath84 .",
    "notice that if some @xmath110 satisfies ( [ pack - lem ] ) for every @xmath111 and pair of gaussian channels @xmath112 , then @xmath113 s are necessarily distinct .",
    "for any collection @xmath114 , denote the left - hand side of ( [ pack - lem ] ) by @xmath115 .",
    "since for @xmath116 @xmath117 from lemma [ megj1 ] , a @xmath114 satisfy ( [ pack - lem ] ) , if for all @xmath118 @xmath119-\\operatorname{h}_{h,\\sigma}(y|x ) }   \\end{aligned}\\ ] ] is at most 1 , for every @xmath111 .",
    "notice that , if @xmath120 @xmath121 then @xmath122 for at least @xmath123 @xmath111 indices @xmath111 .",
    "further , if @xmath124 is the subcollection , states the above indexes , then @xmath125 for every such index @xmath111 .",
    "hence the lemma will be proved , if for an @xmath86 with @xmath126 we find a @xmath120 which satisfy [ pack - lem1 ] .",
    "choose @xmath120 at random , according to uniform distribution from @xmath101 .",
    "in other words , let @xmath127 be independent rv s , each uniformly distributed over @xmath101 . in order to prove that [ pack - lem1 ] is true for some @xmath120 , it suffices to show that    @xmath128    to this end",
    ", we bound @xmath129 . recalling that",
    ", @xmath115 denotes the left - hand side of [ pack - lem ] , we have @xmath130 as the @xmath131 are independent identically distributed the probability the integration is bounded above by @xmath132 as the @xmath131 s are uniformly distributed over @xmath101 , we have for all fixed @xmath133 @xmath134 the set in the enumerator is non - void only if @xmath135 . in this case it can be written as @xmath136 , where @xmath137 is a conditional distribution , which @xmath138 thus by lemma [ megj1 ] , and lemma [ lem0 ] @xmath139 if @xmath135 , and @xmath140 otherwise .",
    "so , if we upper bound @xmath141 by @xmath142 - with the use of lemma [ megj1 ] - from ( [ lem1 - 3 ] ) , ( [ lem1 - 2 ] ) and ( [ lem1 - 1 ] ) we get , @xmath143}\\\\ & \\leq 2^{-n[\\operatorname{i}(\\hat{h},\\hat{\\sigma})-r+\\delta-7\\gamma_n]+\\operatorname{h}_{h,\\sigma}(y|x)}\\end{aligned}\\ ] ] let @xmath25 be so large that @xmath144 , then we get @xmath145 which proves ( [ ujszam ] )    [ lem3 ] for @xmath66 from lemma [ lem0 ] , and @xmath19 as is ( [ modell ] ) , and @xmath146 , and @xmath147 , @xmath148    ( indirect ) suppose that @xmath149 for some @xmath150 .",
    "then let @xmath151 we will show , that @xmath152 , which contradicts to the definition of @xmath153 ) .    now , @xmath154 on account of ( [ lem0 - 0 ] ) , @xmath155    [ lem4 ] let @xmath63 , and @xmath66 from lemma [ lem0 ] .",
    "let @xmath156 and @xmath157 be two arbitrarily ( isi function , variance ) pairs .",
    "let @xmath19 and @xmath54 be such that @xmath158 then @xmath159-h_{h,\\sigma } ( y|x ) } \\ ] ] here @xmath160 is an information divergence for gaussion distributions , positive if @xmath161 .",
    "@xmath162+\\log(p_{y|x}^{h,\\sigma}({\\underline{y}}|{\\underline{x}}_i))]}\\label{lem4 - 0 } \\end{aligned}\\ ] ]    and @xmath163 by the definition , so : @xmath164 if @xmath25 is large enough . with this :",
    "@xmath165 introduce the following notation @xmath166 , then @xmath167 using lemma [ lem3 ] @xmath168 @xmath169 with this we can bound ( [ lem4 - 1 ] ) @xmath170 while @xmath171 .",
    "if @xmath25 large enough , then @xmath172 since @xmath173 . using ( [ szam ] ) we continue from ( [ lem4 - 2 ] ) @xmath174 substituting this and ( [ lem4 - 3 ] ) to ( [ lem4 - 0 ] ) gives the desired result .",
    "now we can state , and prove our main theorem    [ mainthm ] for arbitrarily given @xmath175 @xmath176 , and blocklength @xmath177 , there exist a code @xmath178 ( coding / decoding function pair ) , with rate @xmath179 such that for all isi channels , with parameters @xmath180 , the average error probability satisfies @xmath181 here @xmath182 where @xmath183 is the information divergence ( [ szam ] ) ,    [ megj2 ] the expression minimised above is a continuous function of @xmath184    let @xmath185 , and let @xmath186 the set of codesequences from lemma [ lem1 ] , so @xmath187 .",
    "the coding function sends the @xmath111-th codeword for message @xmath111 , @xmath188 .",
    "the decoding happens as follows : let denote the isi - type of @xmath189 by @xmath190 for all @xmath191 . using these parameters",
    "we define the decoding rule as follows @xmath192 in case of non - uniqueness , we declare an error .",
    "now we bound the error @xmath193 where @xmath194 denotes the probability of event @xmath195 that the detected variance , for some @xmath196 does not satisfy @xmath197 .",
    "bound the probability of this event .",
    "if @xmath195 occurs then @xmath198 is extremal point of the approximating set of parameters , so @xmath199 .",
    "since @xmath200 is element of the approximating set of isi , this means that the power of the incoming sequence is greater than @xmath201 , the probability of this @xmath202 @xmath203 where @xmath204 the complement normal error function .",
    "this probability converges to 0 faster that exponential , which - as we will see - means that in ( [ hibavsz ] ) the second term is the dominant .",
    "consider the second term from ( [ hibavsz ] ) .",
    "if we sent @xmath111 then @xmath205 occurs if and only if @xmath206 we know that @xmath207 while we supposed that we are not in the event @xmath195 .",
    "so the probability of the second term of ( [ hibavsz ] ) @xmath208 with lemma [ lem4 ] ( substituting @xmath209 , @xmath210 , @xmath185 ) and from lemma [ lem1 ] we get @xmath211}\\ ] ] if @xmath25 is large enough . from this - since the number of the approximating channel parameters grows subexponentially - we get @xmath212}\\ ] ]",
    "we compare the new error exponent with gallager s error exponent .",
    "gallager derived the method to send digital information through channel with continuous time and alphabet , with given channel function .",
    "this result can be easily modified to discrete time , as in e.g. @xcite .",
    "the linear gaussian channel with discrete time parameter , with fading vector @xmath1can be formulated as follows : @xmath213 where @xmath54 is the input vector and @xmath214\\end{aligned}\\ ] ] from @xcite we get the idea to define a right and a left eigenbasis for the matrix @xmath215 .",
    "so the right eigenbasis @xmath216 , is not else then the eigenbasis of @xmath217 where @xmath218 means the transpose .",
    "and @xmath219 is the left eigenbasis , where @xmath220 ( or basis of @xmath221 ) , and denoting @xmath222 . as in the work of gallager @xmath223 , because @xmath224-s form an orthonormal eigenbasis and @xmath225 .",
    "so write @xmath54 in a good basis @xmath226 we get @xmath227 - we know that @xmath228-s form also an i.i.d .",
    "gaussian sequence .",
    "write the output as @xmath229 , we get @xmath230 for all @xmath111 , where @xmath231 is the white gaussian noise in the basis of @xmath232 ( in which is also white ) . in many works",
    "this equation is used as a channel with fading , where @xmath233s are i.i.d .",
    "random variables .",
    "this is a false approach . if the receiver knows the isi",
    "@xmath234 then these constant can be computed .",
    "is the isi is a random vector from , e.g. , i.i.d .",
    "random variables , then @xmath233s are not necessarily i.i.d .    if we interlace our codeword , with this formula we can get @xmath235 parallel channels , each have snr @xmath236 . we know that the error exponent given by gallager for the @xmath111-th channel is @xmath237\\ ] ]",
    "if the input distribution @xmath238 is the optimal , gaussian distribution , with use of @xcite the above expression can be rewritten as @xmath239 now we can use the szeg theorem from @xcite , and we get that the average of the exponent , so the exponent of the system is @xmath240 dx\\ ] ] where @xmath241 , which is same as @xcite    in the simulation we simulated a 4 dimensional fading vector whose components was randomly generated with uniform random distribution in @xmath242 $ ] . for other randomly generated vectors , we get similar result .",
    "the two error exponent were positive in the same region , but for surprise the new error exponent was better ( higher ) than gallager s one .",
    "figure [ 2.abra ] shows , that the new error exponent is always as good , or better than the gallager s one .",
    "the new method gives better error exponent , however it can be hardly computed .",
    "we could estimate the difference only in 4 dimension , because of the computational hardness to give the global optimum of a 4 dimensional function .",
    "firstly our result can be used as a new lower bound to error exponents with no csi at the transmitter . note that , our error exponent ( [ sajatexp ] ) is positive if the rate is smaller than the capacity .",
    "secondly it gives a new idea for decoding incoming signals without any csi : maybe it is worth to perform a more difficult maximisation , but not dealing with channel estimation .",
    "this can be done because of the universality of the code , which means , the detection method does nt depend on the channel .",
    "we have proved that if the isi fullfills some criteria ( see theorem [ mainthm ] ) , then the message can be detected , with exponentially small error probability .",
    "however these criteria can be relaxed , because in the theorem @xmath243 and @xmath244 , so any isi with finite length and finite energy , and finite white noise variance , can be approximated well via the approximating set of parameters .",
    "it can be easily seen , that the lemmas and theorem remain true with small changes , if the input distribution is an arbitrarily chosen i.i.d .",
    "( absolute continuous or discrete ) distribution .",
    "only the functional form of the mutual information @xmath245 , and the entropy of the output variable @xmath246 changes .",
    "so , this result can be used for lower bounding the error exponent , if non - gaussian i.i.d .",
    "random variables are used for the random selection of the codebook . however in these cases the entropy of the output can hardly be expressed in closed form .    with the result of theorem [ mainthm ] ,",
    "we can define channel capacity for compound fading channels .",
    "if the fading remains unchanged during the transmission , and the fading length satisfies @xmath247 , we can state the following theorem :      in the limit of the set @xmath250 is dense in the space of the real sequences with any length , so for every @xmath251 there exists a sequence @xmath252 such that @xmath253 .",
    "we know from remark [ megj2 ] that the error exponent in theorem [ mainthm ] is a continuous function , so the theorem [ mainthm ] proves that @xmath254 is an achievable rate .",
    "for some @xmath257 our error exponent gives a better numerical result , than the random coding error exponent derived by gallager @xcite , improved by kaplan and shamai @xcite ( the random coding error exponent used here is deduced in section [ numericalr ] ) .",
    "this result is not so surprising , if we know that the maximum mutual information ( mmi ) decoder gives better exponent in some cases ( like in multiaccess environment ) than the random coding error exponent derived by gallager .",
    "this work does nt contradict with @xcite .",
    "we know , that in the discrete case the mmi decoder is not else than the generalised likelihood ( gml ) decoder @xcite , and also in @xcite was showed , that gml decoder is not optimal in the non - memoryless case .",
    "however this is not the case in the continuous case , where the entropy of the incoming signal depends of the used parameter @xmath257 .",
    " beitrage zur theorie der toplitzschen formen i  _ mathematische zeitschrift _ vol . 6 , pp 167 1920 .",
    "omura and b.k .",
    "levitt , `` coded error probability evaluation for antijam communication system '' _ ieee trans .",
    "com-30 , pp .",
    "869 - 903 , may 1982 .",
    "w. ahmed , p. mclane `` random coding error exponents for two - dimensional flat fading channels with complete channel state information '' _ ieee tranactions on information theory _",
    ", vol . 45 . no ."
  ],
  "abstract_text": [
    "<S> a new straightforward universal blind detection algorithm for linear gaussian channel with isi is given . </S>",
    "<S> a new error exponent is derived , which is better than gallager s random coding error exponent . </S>"
  ]
}