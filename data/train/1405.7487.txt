{
  "article_text": [
    "when applying data - driven execution models to parallel hierarchical n - body methods , it is important first to understand the significance of the dynamic load - balancing and data prefetching mechanisms that have existed in them for over two decades .",
    "parallel n - body methods start by partitioning the particles in a way that maximizes data locality while balancing the workload among the partitions .",
    "this is done by using the workload from the previous time step as weights when splitting a space filling curve that connects all particles @xcite .",
    "parallel n - body methods also have a mechanism for prefetching the data on remote processes by communicating all necessary parts of the remote trees upfront .",
    "the resulting tree is a subset of the entire global tree , which is called the local essential tree ( let ) @xcite .",
    "any data - driven execution model that provides features such as dynamic load - balancing and data prefetching / caching must augment these existing tailored mechanisms rather than compete with them .",
    "one area where the existing load - balancing and prefetching scheme can be improved is the granularity at which they are performed .",
    "figure  [ fig : granularity_partition ] shows the spectrum of granularity for the partitioning phase .",
    "currently , the partitioning phase is constrained to the granularity of a single time step .",
    "one could coarsen the granularity by delaying the update of the partition for a few time steps , thereby adding more room for asynchronous execution .",
    "it is also possible that a repartitioning could take place within a time step in case of a node failure .",
    "adding such flexibility to the partitioning granularity is a partial requirement for making the algorithm fault tolerant .",
    "figure  [ fig : granularity_let ] shows the spectrum of granularity for the let communication ( prefetching ) phase .",
    "conventional parallel n - body methods use a bulk - synchronous ` mpi_alltoallv ` to communicate the whole let at once , and overlap this communication with the local tree traversal to hide latency .",
    "one could over decompose the let down to a per cell request , and then aggregate the communication to the optimal granularity .",
    "the bulk - synchronous communication model can be thought of as an extreme case of aggregation , while something like an rdma per task per cell would be at the other end of the granularity spectrum .",
    "+    there have already been a few attempts to use data - driven execution models with parallel hierarchical n - body methods .",
    "_ use the ` charm++ ` execution model for their cosmological n - body code ` changa ` @xcite .",
    "they compare several different cosmological datasets on several different architectures , and show significant improvement in the scalability over another cosmological n - body code ` pkdgrav ` .",
    "they show that a nave load - balancing scheme based on work - stealing increases the amount of communication three - fold . `",
    "changa ` has also been extended to run on gpus @xcite .",
    "the tree construction and tree traversal are done on the cpu and only the force calculation is performed on the gpu .",
    "they report 3.82 tflops ( single precision ) on 896 cpu cores + 256 s1070 gpus , which is less than 2% of the theoretical peak .",
    "they are able to calculate approximately 10 million particles per second on 448 cpu cores + 128 gpus .",
    "however , state - of - the - art parallel n - body codes such as ` pfalcon ` and ` exafmm ` can calculate 10 million particles per second on a single cpu socket @xcite .    when assessing the usefulness of new data - driven runtime systems , it is problematic to use a code with orders of magnitude slower serial performance . as mentioned earlier ,",
    "data - driven execution models add value not by providing load - balancing or data - caching features to parallel n - body methods , but rather by adding flexibility to the granularity at which these mechanisms can be executed .",
    "however , slow serial performance of the code will skew the discussion on the optimal granularity .",
    "for example , techniques on the finer end of the spectrum in figure  [ fig : granularity ] will seem acceptable if the serial performance was slow enough , while in reality the communication latency could actually be too large for codes like ` pfalcon ` and ` exafmm ` .",
    "the same can be said to the case of dekate _",
    "@xcite , where they use the ` parallex ` execution model for the barnes - hut treecode @xcite and report a performance of 100k particles per second on a single cpu socket .",
    "this is exactly 100 times slower than the state - of - the - art n - body codes , which can do 10 million particle per second .",
    "+    there are a few other reports on the use of parallel n - body methods with data - driven execution models such as ` starpu ` @xcite and ` ompss`@xcite , but these only consider shared memory architectures .",
    "although there are qualitative similarities between inter - socket and inter - node data management , it is the quantitative difference that matters when discussing the granularity issues as mentioned before .",
    "the scope of the current work is on distributed memory data - driven execution models .",
    "previous work with good serial performance have focused on optimizing the bulk - synchronous all - to - all communication itself rather than data - driven execution models . with these optimizations lashuk _ et al . _",
    "were able to calculate 90 billion particles in approximately 300 seconds on 200k cores of jaguar and achieved 0.7 pflops @xcite .",
    "similarly , yokota _",
    "_ calculated 64 billion particles in approximately 100 seconds on 4000 gpus of tsubame2.0 and achieved 1.0 pflops @xcite .",
    "the base of comparison for the data - driven execution models should be such highly optimized codes .",
    "the present work performs a direct comparison between a highly scalable bulk - synchronous n - body code , ` exafmm ` , with and without ` charm++ ` .",
    "unlike studies where the comparison is made against a completely different code , the present work compares the same code with and without the data - driven execution model .",
    "in order to understand the amount of potential asynchronicity in fmm , one must understand the data - dependencies between the different phases of the fmm .",
    "fmm has six different mathematical operations that it performs , each with different dependencies and workloads .",
    "figure  [ fig : fmm ] shows the data - dependency of fmm in two separate schematics .",
    "the picture on the top shows a birds - eye view of the interaction between the red source particles and the blue target particles .",
    "each connection or arrow shown in this figure represents a data - dependency .",
    "the interactions are grouped hierarchically so that far particles interact more sparsely .",
    "this is quite different from fft , where even remote data points require equal amount of communication between them .",
    "the bottom picture in figure  [ fig : fmm ] shows a geometrical partitioning of a two - dimensional domain and the corresponding data - dependency between the location of the cells .",
    "the p2 m ( particle - to - multipole ) kernel takes the information of the particles ( coordinates and charges ) and calculates multipole expansion coefficients from this information ( shown in red ) . for details of the mathematical formulation of fmm",
    "see @xcite .",
    "the m2 m ( multipole - to - multipole ) kernel takes the information of the multipoles from its child cells and aggregates this information into a new multipole expansion at the center of a larger cell ( shown in orange ) .",
    "the m2l ( multipole - to - local ) kernel takes all the information of the multipole expansions in the tree and translates them to local expansions ( shown in yellow ) .",
    "there is a special rule for m2l kernels that it can only interact with cells that are sufficiently far compared to its cell size . the bigger the cell the further",
    "the other cell must be .",
    "the gray zones in figure  [ fig : fmm ] show the region where the m2l kernel is valid .",
    "the union of the gray zones on the three levels of m2l , m2l , and p2p shown in the center column add up to the entire domain . in a much deeper tree in 3-d",
    "this would be like peeling layers of m2l interaction lists until it reaches a ball of p2p neighbor lists , but they add up to the whole domain .",
    "the l2l ( local - to - local ) kernel takes the information given by the m2l kernel and cascades it down the tree ( shown in light - green ) , until it reaches the bottom at which point the l2p ( local - to - particle ) kernel is called to translate that information to each particle ( shown in dark - green ) .",
    "the data - flow of fmm is analogous to a mail delivery system , where the information is aggregated from local post office to a larger hub , the delivery between remote locations is done with cargo aircraft , and then distributed back to the local post offices before delivery to the individual .",
    "this is a very efficient delivery system for computational problems that require information to travel from everywhere to every other place .",
    "mathematically speaking , elliptical equations belong to this class of problems where some form of information must travel from one end of the domain to the other for the system to achieve a state of equilibrium .",
    "this could either be done by successive iteration with local halo communication over stencils , or it can be done more directly and asynchronously by packing / compressing everything and sending it over the network at once .    an important fact that can be overlooked by just looking at figure  [ fig : fmm ] is that the m2l and p2p phases are so much more expensive than the other phases .",
    "these two phases make up more than 90% of the total runtime , so navely integrating the data - flow with the remaining 10% will never result in a performance increase of more than 10% .",
    "furthermore , every m2l cell depends on hundreds of m2 m cells .",
    "it is not a clear data - path where each node in the dag has one arrow pointing to the next node .",
    "it is a dag with hundreds of arrows pointing to a single m2l node .",
    "this is another reason why data - flow programming at this level of granularity is not favorable .",
    "fmm has two major communication phases : the partitioning of particles ( load - balancing ) , and the let communication ( prefetching ) .",
    "we describe in the next two subsections the details of each of these two phases and how ` charm++ ` is used to add asynchronicity and granular flexibility .",
    "+   +    partitioning schemes for fast n - body methods can be categorized into orthogonal recursive bisections ( orb ) @xcite or hashed octrees ( hot ) @xcite .",
    "the orb forms a balanced binary tree by finding a geometric bisector that splits the number of particles equally at every bisection of the tree .",
    "the direction of the geometric bisector alternates orthogonally ( x , y , z , x , ... ) to form a cascade of rectangular subdomains that contain equal number of particles . for non - uniform distributions",
    "the aspect ratio of the subdomain could become large , which leads to suboptimal interaction list size and communication load .",
    "this problem can be solved by choosing the direction of the geometric bisector to always split in the longest dimension .",
    "the original method is limited to cases where the number of processes is a power of two , but the method can be extended to non - powers - of - two by using multi sections instead of bisections @xcite .",
    "the hot partitions the domain by splitting morton / hilbert ordered space filling curves into equal segments .",
    "morton / hilbert ordering maps the geometrical location of each particle to a single key .",
    "the value of the key depends on the depth of the tree at which the space filling curve is drawn .",
    "three bits of the key are used to indicate which octant the particle belongs to at every level of the octree .",
    "therefore , a 32-bit unsigned integer can represent a tree with 10 levels , and a 64-bit unsigned integer can represent a tree with 21 levels . directly mapping",
    "this key to the memory address is inefficient for non - uniform distributions since most of the keys will not be used .",
    "therefore , a hashing function is used to map the morton / hilbert key to the memory address of particles / cells .",
    "parallel sampling - based techniques have proven to be useful for both finding the bisectors in orb @xcite and finding the splitting keys in hot @xcite .",
    "both orb and hot are constructing parallel tree structures , but in different ways .",
    "there is an analogy between parallel tree construction and parallel sorting .",
    "the idea behind orb is analogous to merge sort , where a divide and conquer approach is taken .",
    "hot is analogous to radix sort , where each bit of the key is examined at each step .",
    "therefore , sampling - based techniques that are known to be effective for parallel sorting are also effective for parallel tree partitioning .",
    "the partitioning can be separated into two steps .",
    "the first step is to find the bisectors / key - splitters by using a sampling - based parallel sorting algorithm .",
    "an example of such sampling - based partitioning is shown in figure  [ fig : split ] .",
    "sorting is only performed among the buckets ( not within them ) and this is done only locally .",
    "the only global information that is communicated is the histogram counts , which is only a few integers and can be done efficiently with an ` mpi_allreduce ` operation .",
    "the bins can be iteratively refined to narrow the search for the splitter of the hot key or orb bisector .",
    "this will determine the destination process for each particle .",
    "the second step is to perform an all - to - all communication of the particles .",
    "since the orb bisector is one floating point number and the hot key is one integer , it is much less data than sending around particle data at each step of the parallel sort .    in our current implementation",
    "we choose orb over hot for a few different reasons .",
    "one of the main reasons is that we were able to improve a major defect of orb  partition - cell alignment issue .",
    "since geometrically closer points interact more densely with each other , it is crucial to keep the particles in the same cell on the same process in order to minimize communication .",
    "however , if a global morton / hilbert key is used to construct the local trees , the orb may place a bisector in the middle of a cell as shown in figure  [ fig : orb ] .",
    "this results in an increase in the interaction list size .",
    "we avoid this problem by using local morton / hilbert keys that use the bounds of the local partition .",
    "this may at first seem to increase the interaction list near the partition boundaries since two misaligned tree structures are formed .",
    "however , when one considers the fact that the present method squeezes the bounding box of each cell to tightly fit the particles as shown in figure  [ fig : orb2 ] , it can be seen that the cells are not aligned at all in the first place .",
    "furthermore , our flexible definition of the multipole acceptance criteria optimizes the interaction list length for a given accuracy regardless of the misalignment .",
    "there is one more important ingredient for an efficient partitioning scheme ",
    "weighting by the workload .",
    "particles have varying interaction list sizes , so equally splitting the bisection / key results in suboptimal load - balance . weighting the particles with the workload from the previous time step",
    "is a simple and effective load - balancing strategy .",
    "this technique was mentioned in the original hot paper @xcite and little has been done to improve it to this day",
    ". it would be nave to propose a work - stealing mechanism for fast n - body methods without understanding the significance of this practical solution that has stood the test of time .",
    "data - driven execution models should be able to augment this tailored feature rather than to reinvent it .",
    "although this weighting scheme was originally proposed for hot , it can obviously be used to determine weights for particles during the bisection in orb .",
    "+    one limitation of the weighting scheme is that it only balances the workload and not the communication .",
    "there have been efforts to use graph partitioning tools with the workload as node - weights and communication as edge - weights , in order to create partitions that have an optimal balance of both the workload and communication @xcite .",
    "this method has only been compared with morton key splitting without weights , so the advantage over morton key splitting with weights is unclear . in the present work we attempt to balance the workload and communication simultaneously by calculating the weight for the @xmath0 particle @xmath1 according to @xmath2 where @xmath3 is the local interaction size , @xmath4 is the remote interaction list size , and @xmath5 is a constant that is optimized over the time steps to minimize the total runtime .",
    "@xmath6 is the total interaction list size and represents the workload , while @xmath4 reflects the amount of communication . by adjusting the coefficient @xmath5",
    ", one can amplify / damp the importance of communication balance .",
    "making this an optimization problem to minimize the total runtime is what we prefer over minimizing the load - imbalance since the latter is not our final objective . moreover ,",
    "the variables @xmath3 , @xmath4 , and the total runtime are already measured in the present code so the information is available at no cost .    `",
    "charm++ ` provides an opportunity to augment this bulk - synchronous approach to load - balancing by offering control over the granularity .",
    "we are interested in the data - flow programming model of ` charm++ ` , which allows us to asynchronously execute the fmm kernels while the communication for partitioning is happening . to our knowledge , there have not been any attempts to overlap computation with the communication in the partitioning phase . ` charm++ ` also provides task migration capabilities , but we decided not to use this feature for the current study .",
    "this is because we believe that it is much more efficient to  strategically update \" the partitions so that they are well balanced , than to try to  steal work \" after they are partitioned poorly .",
    "once the particles are partitioned , the ones in the local domain are used to construct a local tree .",
    "we use a completely local construction of the octree using the local bounding box , instead of using a global morton / hilbert key that is derived from the global bounding box .",
    "this allows us to reuse all parts of the serial code and only add a few routines for the partitioning , grafting of trees , and communication .",
    "therefore , any modification in the serial code is immediately reflected in the parallel code .",
    "after the local tree structure is constructed , a post - order traversal is performed on the tree structure and p2 m and m2 m kernels are executed bottom up .",
    "the p2 m kernel is executed only at the leaf cells .",
    "it loops over all particles in the leaf cell to form the multipole expansion at the center of the leaf cell .",
    "the m2 m kernel is executed only for the non - leaf cells .",
    "it loops over all child cells and translates the multipole expansions from it s children s centers to its center .",
    "once the multipole expansions for all local cells have been determined , the multipole expansions are sent to the necessary processes in a sender - initiated fashion @xcite .",
    "this reduces the latency by communicating only once , rather than sending a request to remote processes and then receiving the data .",
    "such sender - initiated communication schemes were common in cosmological n - body codes since they tend to use only monopoles , and in this case the integer to store the requests is as large as the data itself if they were to use a request - based scheme .    in the present method , the let is formed from the information that is sent from the remote processes by simply grafting the root nodes of the remote trees as shown in figure  [ fig : tree ] . in conventional parallel fmm codes , a global octree is formed and partitioned using either hot or orb .",
    "therefore , the tree structure was severed in many places as shown in figure  [ fig : tree ] , which caused the merging of the let to become quite complicated .",
    "typically , code for merging the let would take a large portion of a parallel fmm code , and this made it difficult to implement new features such as periodic boundary conditions , mutual interaction , more efficient translation stencils , and dual tree traversals . ` exafmm ` is able to incorporate all these extended features and still maintain a fast pace of development because of this simplification in how the global tree structure is geometrically separated from the local tree structure .",
    "while the remote information for the let is being transferred , the local tree can be traversed .",
    "conventional fast n - body methods overlap the entire let communication with the entire local tree traversal .",
    "the let communication becomes a bulk - synchronous ` mpi_alltoallv ` type communication , where processes corresponding to geometrically far partitions send logarithmically less information , thus resulting in @xmath7 communication complexity where @xmath8 is the number of processes .",
    "nonetheless , in traditional fast n - body codes this part is performed in a bulk - synchronous manner .",
    ".... entry void alltoallcells ( ) {    atomic {      transportcellstoall ( ) ;    }    for ( count=0 ; count < numchares ; count++ ) {      when transportcells(int cellcount ,       cell b[cellcount ] , int sender ) atomic {        processcells(cellcount , b , sender ) ;      }    }     atomic {      finishalltoallcells ( ) ;    } } ; ....    ` exafmm`-`charm++ ` replaces the global synchronization points , which consist of aggregation of let data through all to all communication , with asynchronous sender - initiated entry functions that represent a coherent work entity called chare .",
    "the local partitions are mapped to chares that are accessible through entry functions .",
    "such entry constructs propagate cell / body data across nodes to traverse the corresponding let , rather than traverse all lets in one shot .",
    "the advantages are reflected at both cpu and network utilization levels .",
    "global communication is typically synchronous in nature and are dependent on two factors : network bandwidth and initialization cost .",
    "it is clear that for large messages the first factor dominates whereas latency hiding could overcome this problem in rare cases ; however , in general , such events act like global barriers . by supporting the traversal as an entry card to the localtree structure",
    ", the post - order traversal is triggered once the let is received .",
    "the function in figure  [ fig : alltoall ] shows a direct mapping of the synchronized global communication to ` charm++ ` , that undergoes the structured control flow abstraction .",
    "the  when \" construct controls the sequence at which messages are received , and the code inside it will be executed at the receiver side .",
    "it is clear from the abstraction that the next workload is triggered once all of messages are received .",
    "the function in figure  [ fig : alltoall ] is utilized by exafmm - charm to replace the global blocking receive of cells with a remote asynchronous call that will process the message and proceed based on the rank of the sender .    once the let is formed",
    "the m2l and p2p kernels can be calculated using this information from the remote processes .",
    "the calculation of these two kernels takes a large portion of the execution time of fmm .",
    "the p2p kernel only requires information from its neighbors , while the m2l kernel requires information from an intermediate range . besides these read - dependencies these two kernels",
    "do not have any level - wise dependency within the tree structure and can be processed in parallel on a per cell basis . in ` exafmm ` the m2l and p2p kernels are processes without forming an explicit interaction list by using the dual tree traversal @xcite .",
    "after the dual tree traversal is finished , a post - order traversal is performed and l2l and l2p kernels are executed to cascade the information down the tree to the particles .",
    ".fmm parameters [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]",
    "our tests were performed on the tacc stampede system without using the coprocessors .",
    "stampede has 6400 nodes , each with two xeon e5 - 2680 processors with eight physical cores and 32 gb of memory .",
    "we used the intel compiler ( ` module intel/13.0.2.146 , impi/4.1.0.030 ` ) and used the intel thread building blocks library for the threading model .",
    "the ` exafmm ` code that was used for the current study is publicly available on bitbucket .",
    "we compare the scalability of ` exafmm ` with and without the use of ` charm++ ` . `",
    "exafmm ` has many tunable parameters as shown in table  [ tab : parameters ] .",
    "@xmath9 is the number of bodies , @xmath8 is the order of multipole / local expansion , and @xmath10 is the multipole acceptance criteria . the fmm has a theoretical error bound of @xmath11 , while the computational complexity varies between @xmath12 and @xmath13 depending on the type of basis @xcite . unlike , previous treecodes that can only control @xmath10 or fmm codes that can only control @xmath8 , ` exafmm ` can achieve the optimal speed by controlling both @xmath8 and @xmath10 simultaneously .    in table  [ tab : parameters ] , @xmath14 represents the maximum number of particles per leaf cell , while @xmath15 is the minimum number of particles per spawned thread .",
    "using a large @xmath14 will create a shallower tree and decrease the number of m2l interactions , but will increase the number of particles per cell and therefore increase the number of p2p interactions . using an optimal @xmath14 value",
    "is essential to balance the workload between the m2l and p2p kernel , which are the two most expensive parts of the fmm . increasing @xmath15",
    "will allow less threads to be created and will decrease the overhead of the task spawning .",
    "decreasing @xmath15 will cause more threads to be created and will make it easier to load - balance , but may increase the runtime due to the overhead caused by spawning many tasks .",
    "these values where carefully chosen to maximize the performance on stampede .",
    "particles.,scaledwidth=45.0% ]    the final entry in table  [ tab : parameters ] is the distribution of particles .",
    "we have selected three different types of distributions , which are representative of the actual distributions in scientific application codes .",
    "an illustration of the three distributions is shown in figure  [ fig : distribution ] .",
    "cube \" distribution can be found in molecular dynamics simulations where water molecules are evenly distributed throughout a cubic domain .",
    "the  sphere \" distribution has points only on the surface of the sphere .",
    "this is representative of boundary integral problems , where a surface mesh is used to discretize the problem .",
    "plummer \" distribution is typical for cosmological n - body simulations , where the mass is distributed unevenly with very high concentration in certain areas .",
    "we perform a strong scalability test of the fmm by keeping the number of particles to the value shown in table  [ tab : parameters ] and increasing the number of cores .",
    "all the values in table  [ tab : parameters ] are kept constant throughout the strong scalability tests .",
    "we first run up to 16 cores per node and then increase the node count once the number of cores per node is saturated .",
    "the total number of cores used in the largest run was 4,096 .",
    "the results of the strong scalability test using ` exafmm ` with and without ` charm++ ` are shown in figure  [ fig : scaling ] .",
    "the divergence from ideal scaling is mainly caused by the increase in the interaction list size when splitting the constant - sized tree into smaller and smaller segments . by looking back at figure  [ fig :",
    "partition ] , one can see that all partitioning schemes will suffer from this problem because it is difficult to maintain a small surface to volume ratio when partitioning a constant domain into thousands of subdomains . for any partitioning scheme ,",
    "the shapes of the partitions tend to be neater at a macroscopic level , but the unevenness in the particle distribution at the microscopic scale tends to create oddly - shaped partitions as you go finer .    we will take a closer look at the strong scalability runs by plotting the breakdown of the runtime in figure  [ fig : breakdown ] .",
    "the breakdown in figure  [ fig : breakdown ] corresponds to the plot for  exafmm \" in figure  [ fig : scaling ] .",
    "the main difference between the two plots is that figure  [ fig : scaling ] is showing the speedup , whereas figure  [ fig : breakdown ] is showing the runtime multiplied by the number of cores .",
    "this is done so that the bar plot for larger core counts is clearly visible",
    ". therefore , in figure  [ fig : breakdown ] a constant bar height will mean perfect strong scalability .",
    "it can be seen that the  comm partition \" phase is consuming a large time on 4,096 cores .",
    "this is the communication of the partitioning stage , which is very large for the initial step .",
    "note that the two let communication phases  comm let bodies \" and  comm let cells \" are completely overlapped with the  traverse \" and can not be seen in figure  [ fig : breakdown ] .",
    "the original ` exafmm ` code overlaps the let communication with local tree traversal so adding ` charm++ ` does not improve the performance any further for this part .",
    "however , the communication for the initial partitioning phase is not overlapped with any computation in ` exafmm ` ( or any other fast n - body code as far as the authors are aware ) , so the asynchronous execution model of ` charm++ ` provides some benefit for this part .",
    "particles.,scaledwidth=45.0% ]      the increase in runtime of the  traverse \" phase shown in figure  [ fig : breakdown ] is mostly attributed to the increase in the interaction list length as mentioned earlier , but it is also partially caused by load - imbalance .",
    "we see this in figure  [ fig : loadbalance ] , where the runtime across all cores is shown with the same legend as figure  [ fig : breakdown ] but this time without multiplying the runtime by the number of cores .",
    "as can be seen from figure  [ fig : distribution ] the plummer distribution is highly non - uniform and is difficult to partition to thousand of subdomains .",
    "furthermore , the main difficulty of partitioning n - body codes is that the work - load is not directly proportional to the partition size . for mesh - based methods , partitioning into equal size subdomains would result in somewhat equal workload",
    "however , since each particle has a different interaction list size , partitioning the domain so that the number of particles are equal will result in suboptimal load - balance .",
    "it is difficult to assess the quality of our load - balancing scheme by just looking at a single case , especially if perfect balance is not a reasonable goal to aim for .",
    "therefore , we will compare the results for the different distributions shown in table  [ tab : parameters ] . if the most benign distribution is showing the same amount of load - imbalance as the most difficult case .",
    "then we should be able to conclude that there is little room for improvement .",
    "figure  [ fig : weights ] shows the distribution of the runtime on each core for six different cases .",
    "the top three cases are for the  cube \" ,  sphere \" , and ",
    "plummer \" distribution with the standard weighting scheme based on the interaction list length .",
    "the bottom three cases are for the same distributions but with our new weighting scheme that tries to optimize for both the work and communication load by using eq .  .",
    "the number of cores is set to 1,025 to create an environment where the number of processes is not a power of two .",
    "we used 16 threads per node except for the last node which used only 1 thread .",
    "therefore , we had to use one extra node , which makes the partitioning of mpi ranks not so straightforward .",
    "we see from figure  [ fig : weights ] that all distributions have somewhat similar load imbalance despite these difficult conditions .",
    "therefore , we conclude that our partitioning scheme can handle difficult distributions and difficult number of processes to the same degree that it can handle the easy ones .",
    "distributed memory parallelization models for fmm have traditionally been bulk - synchronous , but dynamic load - balancing and data prefetching mechanisms have existed in them for over two decades . for load - balancing ,",
    "the hashed - octree and orthogonal recursive bisection are both effective techniques for maximizing data locality while balancing the workload among the partitions by using the workload from the previous step as weights when partitioning . for data - prefetching ,",
    "the local essential tree is formed by communicating all necessary parts of the remote tree upfront .",
    "these two techniques are usually applied at the granularity of the time step , but the data - flow of fmm allows a more flexible granularity for both load - balancing and data prefetching .",
    "we have investigated the possibility of using ` charm++ ` to have a finer control over the granularity of the communication and asynchronous execution .    unlike previous work on asynchronous fast n - body methods such as ` changa ` and ` pepc ` , the present work performs a direct comparison against the traditional bulk - synchronous approach and the asynchronous approach using ` charm++ ` .",
    "furthermore , the serial performance of our fmm code is over an order of magnitude better than these previous codes , so it is much more challenging to hide the overhead of ` charm++ ` .",
    "we also propose a novel partitioning scheme , which allows us to geometrically separate the local tree from the global tree .",
    "this was only possible because our fmm uses the dual tree traversal , which does not require a global key nor cubic cells . by taking advantage of this feature of the dual tree traversal",
    ", we were able to simplify the grafting of the local essential tree greatly .",
    "this simplification of our code made it possible to readily integrate with frameworks such as ` charm++ ` with relative ease .    in order to demonstrate the effectiveness of the current combination of state - of - the - art load - balancing , data - prefetching , and data - flow execution models",
    ", we performed a strong scalability test that spans over three orders of magnitude without offsetting the problem size .",
    "as expected , the communication for the initial partitioning phase became a bottleneck at 4096 cores , but we were able to improve this by using asynchronous execution model of ` charm++ ` .",
    "this allows us to achieve over a 1000 times speedup for an highly non - uniform plummer distribution with @xmath16 particles .",
    "we confirmed that our weighting scheme for the partitioning works evenly well for various particle distributions .",
    "random distribution in a cube , points on a spherical shell , and the highly non - uniform plummer distribution all had a similar load - imbalance for @xmath16 particles on 1025 ( not 1024 ) cores .",
    "this also demonstrates that our partitioning scheme works equally well for non - powers of two .",
    "the ` exafmm ` code that was used for the current study is publicly available on bitbucket .",
    "this publication was based on work supported in part by award no kuk - c1 - 013 - 04 , made by king abdullah university of science and technology ( kaust ) .",
    "this work used the extreme science and engineering discovery environment ( xsede ) , which is supported by national science foundation grant number oci-1053575 .",
    "p.  jetley , f.  gioachin , c.  mendes , and l.  v. kal , `` massively parallel cosmological simulations with changa , '' in _ proceedings of the 2008 ieee international parallel and distributed processing symposium _ , 2008 , pp . 112 .",
    "p.  jetley , l.  wesolowski , f.  gioachin , l.  v. kal , and t.  r. quinn , `` scaling hierarchical n - body simulations on gpu clusters , '' in _ proceedings of the 2010 acm / ieee international conference for high performance computing , networking , storage and analysis _ , 2010 .",
    "c.  dekate , m.  anderson , m.  brodowicz , h.  kaiser , b.  adelstein - lelbach , and t.  sterling , `` improving the scalability of parallel n - body applications with an event - driven constraint - based execution model , '' _ international journal of high performance computing applications _ , vol .",
    "26 , no .  3 , pp .",
    "319332 , 2012 .",
    "c.  bordage , `` parallelization on heterogeneous multicore and multi - gpu systems of the fast multipole method for the helmholtz equation using a runtime system , '' in _ the sixth international conference on advanced engineering computing and applications in sciences _ , 2012 .",
    "i.  lashuk , a.  chandramowlishwaran , h.  langston , t .- a .",
    "nguyen , r.  sampath , a.  shringarpure , r.  vuduc , l.  ying , d.  zorin , and g.  biros , `` a massively parallel adaptive fast multipole method on heterogeneous architectures , '' _ communications of the acm _",
    "55 , no .  5 , pp . 101109 , 2012 .",
    "r.  yokota , t.  narumi , k.  yasuoka , and l.  a. barba , `` petascale turbulence simulation using a highly parallel fast multipole method on gpus , '' _ computer physics communications _ , vol .",
    "184 , pp . 445455 , 2013 .",
    "h.  cheng , w.  y. crutchfield , z.  gimbutas , l.  f. greengard , j.  f. ethridge , j.  huang , v.  rokhlin , n.  yarvin , and j.  zhao , `` a wideband fast multipole method for the helmholtz equation in three dimensions , '' _ journal of computational physics _",
    "300325 , 2006 .",
    "f.  cruz , m.  g. knepley , and l.  a. barba , `` petfmm  a dynamicallly load - balancing parallel fast multipole library , '' _ international journal for numerical methods in fluids _",
    "403428 , 2011 ."
  ],
  "abstract_text": [
    "<S> fast multipole methods ( fmm ) on distributed memory have traditionally used a bulk - synchronous model of communicating the local essential tree ( let ) and overlapping it with computation of the local data . </S>",
    "<S> this could be perceived as an extreme case of data aggregation , where the whole let is communicated at once . ` </S>",
    "<S> charm++ ` allows a much finer control over the granularity of communication , and has a asynchronous execution model that fits well with the structure of our fmm code . unlike previous work on asynchronous fast n - body methods such as ` changa ` and ` pepc ` , the present work performs a direct comparison against the traditional bulk - synchronous approach and the asynchronous approach using ` charm++ ` . furthermore , the serial performance of our fmm code is over an order of magnitude better than these previous codes , so it is much more challenging to hide the overhead of ` charm++ ` . </S>"
  ]
}