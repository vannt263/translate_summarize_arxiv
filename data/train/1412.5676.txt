{
  "article_text": [
    "unlike conventional control systems , the control loop is closed through a _ communication network _ with a _ limited bandwidth _",
    "@xcite in a _ networked control system _ ( ncs ) , @xcite as shown in fig . [ fig1_ncs ] .",
    "while a traditional feedback controller requires constant ( or periodic ) access to the sensor , and hence , to the network for receiving state measurements , several different tasks are _ competing _ for transmitting their data through the same network in an ncs .",
    "therefore , designing schemes which decrease the communication load of the network while maintaining the desired performance is beneficial , @xcite .",
    "r0.28        as a real - world example , smart power grids with small - scale electricity generators may be mentioned , in which not only the sensors located at points of common coupling are spatially distributed throughout the network , but also , the generators are distributed , @xcite .",
    "this leads to a flow of electricity and information in the power network . as another example , different control systems of an airplane , e.g. , flight control , engine control , etc .",
    ", can be considered . replacing the wire harnesses of the sensors , which are spatially distributed throughout the plane , with a unified , potentially wireless",
    ", communication medium leads to a dramatic decrease in the weight and hence , the operation cost .",
    "simultaneously , it will facilitate the maintenance and improve the monitoring and fault detection capabilities , @xcite . in a simple and small airplane ( cessna 310r )",
    "this change was reported to increase the range by 10% , @xcite .",
    "considering the literature , the main developed approaches are a ) decreasing the need for continuous state measurement through designing periodic @xcite or aperiodic event triggering schemes @xcite where the network is utilized only when an event is triggered , b ) decreasing the size of the data packets for reducing the network load @xcite , and c ) designing controllers which ` deal ' with the busy network and its consequences including induced delays @xcite or quantization errors @xcite which exist anyway in digital networks @xcite .",
    "while the idea of compressing data packets was observed not to be as effective as the idea of transmitting less frequently @xcite , the first and last approaches have been very popular , especially if the two approaches can be combined to both transmit less frequently and account for the losses and delays , @xcite .    event triggering , as opposed to periodic sampling , has been typically conducted through monitoring the error between the current state of the system and the state information expected to be available on the other side of the network and triggering the system ( or scheduling the network ) when the error exceeds a certain limit .",
    "the state information available on the other side of the network could be the last state measurement transmitted through the network as in zero - order - hold ( zoh ) based methods @xcite or an estimation of the current system state made based on the last transmitted state measurement , as in generalized zoh ( gzoh ) or model based ncs schemes @xcite . in any case , the controller is designed a priori with the assumption of constant access to the sensor , i.e. , without considering the event triggering nature , and the triggering / scheduling policy ( sometimes called the _ event function _ ) is designed such that the system is stabilized , in the cited papers .    reviewing the available literature , the area of triggering optimally as opposed to triggering for stability is rarely investigated due to its difficulties , despite its natural advantages .",
    "the published studies in this area , to the best of the author s knowledge , includes results in @xcite for linear systems .",
    "this study is aimed at this pursuit .",
    "the contribution of this study is extending the applications of approximate dynamic programming ( adp ) @xcite and reinforcement learning ( rl ) @xcite to ( near ) optimal triggering of ncss .",
    "initially the case of finite - horizon cost functions with the simplifying assumption of applying no control , when no state feedback is received by the controller is investigated .",
    "this assumption simplifies the problem to a switching problem , hence , the method developed in @xcite is directly applicable . once the idea behind the solution is clarified , the more advanced cases of zoh and gzoh are covered and the outline of the process for extending the results to stochastic networks with random delays and losses is given .",
    "afterwards , the schemes are extended to the case of infinite - horizon problems and both model - based and model - free schemes are developed .",
    "these methods are supported by rigorous analyses on convergence , optimality , and stability .",
    "the developed methods lead to low online computational load . also , they are valid for different initial conditions of the system .",
    "moreover , the schemes are _ scalable _ and _ decentralized_. that is , when implemented on systems with several sensor - controller - plant sets sharing the same network , the scheduling is based solely on the states of the respective plant for each set , so , it can be conducted _ in parallel _ for every set .    as for the approach of this study",
    ", adp has already been investigated for control of ncss in @xcite .",
    "however , adp was used for finding the control law in @xcite and @xcite under some adp-_independent _ triggering policies .",
    "also , the use of adp in adjusting the transmission power ( which changes continuously ) was presented in @xcite . in here , though , adp will be used for optimal design of the event function . finally , an adp based approach to optimal intermittent feedback was presented in @xcite .",
    "the approach involves an online gradient descent search for finding the next optimal time for triggering , based on the value function representing the cost .",
    "while the authors presented their initial and interesting developments to the problem in that paper , no further work appeared from them in deeper analysis of the method , to the best of this author s knowledge .",
    "the rest of this paper is organized as follows .",
    "the problem is formulated in the next section , followed by the idea for solving it .",
    "afterward , the extensions of the work to zoh , gzoh , stochastic networks , infinite - horizon problems , and problems with modeling uncertainty are presented in the subsequent sections .",
    "numerical examples are presented in section [ simulations ] , followed by some concluding remarks .",
    "the problem of _ scheduling _ an ncs can be presented as follows .",
    "let the discrete - time dynamics of the plant in fig .",
    "[ fig1_ncs ] be given by @xmath0 where @xmath1 is a continuous function versus its both inputs , i.e. , the state and control vectors , @xmath2 and @xmath3 , respectively , with @xmath4 .",
    "sub - index @xmath5 represents the discrete time index , @xmath6 denotes the fixed final time , and positive integers @xmath7 and @xmath8 denote the dimensions of the continuous state and control spaces .    let the piecewise constant function @xmath9 denote the _ scheduling / triggering policy _ , where @xmath10 means the event is triggered or equivalently , the network is _ scheduled _ to be utilized for transmitting state information through the dash lines in fig . [ fig1_ncs ] , and @xmath11 means the network is not scheduled ( event is not triggered ) . assume cost function @xmath12 where continuous positive semi - definite function @xmath13 assigns a _ final cost _ to the last state vector and positive semi - definite @xmath14 , continuous versus the first argument , assigns a _ running cost _ to the states during the horizon and to each network scheduling .",
    "for example , one may select @xmath15 for a some @xmath16 and a positive constant @xmath17 , where the latter assigns a weight to the cost of usage of the resource , i.e. , the network .",
    "the set of non - negative reals is denoted with @xmath18 .    moreover , let function @xmath19 denote a known continuous feedback control policy , i.e. , @xmath20 , such that it asymptotically stabilizes the system if the network is always scheduled .",
    "the problem is designing a feedback triggering policy @xmath21 , such that cost function ( [ costfunction_finhor ] ) is minimized , subject to plant ( [ dynamics ] ) under control policy @xmath22 .",
    "in order to demonstrate the basic idea behind the solution presented in this study , let the control be zero when the network is not scheduled , i.e. , @xmath23 , or simply @xmath24 , called no feedback , no control ( nfnc ) , in this study .",
    "also , the dynamics of the system are assumed to be known and the sensor node is assumed to have a copy of control policy @xmath22 .",
    "moreover , the network is assumed to be delay - free and loss - free",
    ".    motivated by switching systems @xcite , the operation of the system can be modeled by _ switching _ between two different _ modes _ ; @xmath25 and @xmath26 , respectively , for the case of @xmath27 and @xmath28 , @xmath29 the idea for solving this switching problem is using the bellman equation @xcite . let the _",
    "optimal value function _ , sometimes called _ optimal cost - to - go _ , at current state @xmath30 and time @xmath5 which leads to _ time - to - go _",
    "@xmath31 , be denoted with @xmath32 for some @xmath33 . by definition ,",
    "optimal value function @xmath32 represents the incurred cost from the current time to the final time if _ optimal decisions _ are made throughout the remaining @xmath34 time steps . considering eq .",
    "( [ costfunction_finhor ] ) the value function satisfies recursion @xmath35 , where @xmath36 denotes the optimal decision at time @xmath5 . based on the bellman equation , one has @xmath37 @xmath38 if the value function is available , then , the _ optimal triggering policy _ at time @xmath5 ( which leads to the time - to - go @xmath39 ) with current state @xmath2 is simply given in a feedback form by @xmath40 interestingly , eqs .",
    "( [ bellman_finhor_eq1 ] ) and ( [ bellman_finhor_eq2 ] ) can be used for approximating the desired value function . utilizing any parametric function approximator , e.g. , neural networks ( nn ) , with _ time - dependent parameters / weights _ for approximating the time - dependent value function ,",
    "the approximation can be done in a backward fashion , from @xmath41 to @xmath42 , or equivalently from @xmath43 to @xmath44 .",
    "( [ bellman_finhor_eq1 ] ) can be used for finding the parameters of @xmath45 approximator , e.g. , using least squares as detailed in @xcite , within some selected compact domain of interest @xmath46 , i.e. , @xmath47 . afterwards , having @xmath45 , eq .",
    "( [ bellman_finhor_eq2 ] ) can be used for approximating @xmath48 . repeating this process",
    "all the functions can be approximated in the offline phase .",
    "afterwards , the optimal triggering can be conducted in realtime using eq .",
    "( [ scheduler_eq1 ] ) .",
    "this calculation is as simple as evaluating two scalar valued functions and selecting the @xmath49 which corresponds to the smaller number .",
    "this method depends on the approximation accuracy of the function approximator .",
    "nn , as parametric function approximators , are known to provide _ uniform approximation _ with any desired accuracy providing the function subject to approximation is a _ continuous _ function , @xcite .",
    "considering eq .",
    "( [ bellman_finhor_eq1 ] ) , the continuity of the function subject to approximation follows from the continuity of @xmath50 . for eq .",
    "( [ bellman_finhor_eq2 ] ) , however , due to the switching between @xmath51 and @xmath52 , the continuity versus @xmath2 is not obvious .",
    "this continuity is established in an earlier work on optimal switching , in @xcite .",
    "in section [ basicproblem ] the simplified problem of having @xmath53 when @xmath27 , nfnc , was investigated . in most of the case , this is not as good as at least applying the previously calculated control when no new feedback information is received , called _ zero - order - hold _ ( zoh ) , @xcite .",
    "last _ state measurement received by the controller _ as of time _",
    "@xmath5 , be denoted with @xmath54 . in zoh",
    "one has @xmath55 when @xmath56 , and has @xmath57 when @xmath58 .",
    "also , let the finite - horizon cost - function ( [ costfunction_finhor ] ) be assumed .",
    "the objective is designing the scheduling policy @xmath59 such that cost function ( [ costfunction_finhor ] ) is minimized , under the zoh policy of the controller node .",
    "once the controller has a _ memory _ to store the last received state measurement @xmath54 , it can be seen that the value function will be dependent on the stored @xmath54 as well as on time and @xmath30 . to clarify this point ,",
    "two cases may be considered , where , in both cases the current states and times are the same , but , the last received state measurements , i.e. , @xmath54s are different .",
    "the cost - to - go can then be different , because , if the schedulers decide not to schedule the network in both cases , then , the controls that the controllers will apply on their respective plants will be different due to the different @xmath54s , and this can lead to different @xmath60s .",
    "another way of looking at this dependency , is considering @xmath54 also as a part of the _ state _ of the system .",
    "the _ overall state vector _ , denoted with @xmath61 , which is supposed to uniquely identify and characterize the current status of the system includes both the current physical state , @xmath30 , and the last transmitted state measurement , @xmath54 , i.e. , @xmath62^t$ ] . considering this argument ,",
    "the value function of the ncs with zoh may be denoted with @xmath63 for some @xmath64 .",
    "let the dynamics of @xmath65 under the two events of the network not being scheduled and being scheduled , be denoted with modes @xmath25 and @xmath26 , respectively .",
    "then , @xmath66 , \\\\",
    "y_{k+1}= f_1(y_k ) : = \\left [           \\begin{array}{c }              f\\big(x_k , h(x_k)\\big ) \\\\              x_k          \\end{array }          \\right ] .                  \\label{dynamics_zoh_y_k } \\end{split}\\ ] ] for example , as seen in @xmath26 , when the network is scheduled one has @xmath67 , i.e. , the memory will be updated . the bellman equation may be adapted as @xmath68 @xmath69 the abovementioned two equations can be used for calculating the parameters of the respective function approximators , step by step from @xmath70 to @xmath71 , as seen earlier in section [ basicproblem ] .",
    "once the functions are approximated , the following _ real - time _",
    "scheduler conducts _ feedback scheduling _ for each given @xmath72 and @xmath73 @xmath74    note that the scheduling will be conducted on the sensor side ( see fig .",
    "[ fig1_ncs ] ) .",
    "hence , the scheduler needs to know @xmath54 , that is the last successfully transmitted state information to the controller . utilizing an _ acknowledgment based network communication protocol _ , e.g. , transmission control protocol ( tcp ) @xcite , it can be assured that the @xmath54s will be identical on both sides of the network , at each instant ( of course if the acknowledgments themselves are not lost , @xcite ) .",
    "the reason is the sensor will be notified whether or not the controller receives each transmitted @xmath30 , in order to update its ( sensor s ) own copy of @xmath54 accordingly .",
    "one applies the _ constant control @xmath75 _ during the time interval in which no new sensor information is received , in zoh .",
    "this section is aimed at using a ( _ possibly imperfect _ ) model of the system on the controller side to _ update @xmath54 _ and hence the _ control _ in the no - communication periods , called _ generalized zoh _ ( gzoh ) , @xcite . to this end , let @xmath76 be the possibly imperfect model of the plant _ and _ its control policy , i.e. , @xmath77 .",
    "let @xmath78 denote the time at which the last state measurement was received , i.e. , @xmath79 .",
    "updating @xmath54 may be conducted using @xmath80 when no new state information is received .",
    "once new information is received , the variable resets to the received current value of the state .",
    "considering the reason for claiming the dependency of the value function on @xmath54 in zoh , it can be observed that if a model is used for updating @xmath54 , then , the value function will depend on the _ current ( updated ) value _ of @xmath54 . in other words ,",
    "the ` overall state ' of the system will include @xmath30 , which is the physical state of the system , and @xmath54 which is the _ updated _ value of @xmath81 , where @xmath78 is the last time that the network has been scheduled .",
    "the reason for the latter dependency is the fact that the control that the controller will apply , if no new sensor information is received , will be directly dependent on the current @xmath54 .",
    "the same relations and derivations of section [ zoh_finhor ] apply , except that eq .",
    "( [ dynamics_zoh_y_k ] ) will need to be changed to @xmath82,\\\\ y_{k+1}= f_1(y_k ) : = \\left [           \\begin{array}{c }              f\\big(x_k , h(x_k)\\big ) \\\\",
    "\\hat{f}\\big(x_k,\\hat{h}(x_k)\\big )          \\end{array }          \\right ] .",
    "\\label{dynamics_gzoh_y_k } \\end{split}\\ ] ] note that @xmath83 will be required in both the sensor and the controller nodes , because , not only the controller needs it to update its @xmath54 , but also , the sensor needs to know the current value of @xmath54 stored in the controller node . using acknowledgment - based network communication protocols as described in subsection [ zoh_finhor ] , this condition can be fulfilled .",
    "it is interesting to note that the idea presented in section [ basicproblem ] can be simply extended to zoh and gzoh , as done in the last two sections .",
    "this feature shows the potential of the idea of using adp for triggering ncss with their different challenging issues .",
    "the network communications have been assume to be delay - free and loss - free , so far .",
    "the developed theories , however , can be naturally extended to the case of stochastic ncss , e.g. , networks with random delays and packet losses .",
    "the idea is utilizing the potential of the adp / rl in handling stochastic processes @xcite , if the _ probability distribution functions _ of the delays and losses are known and using _ expected value operators _ in the bellman equation .",
    "while the details are skipped due to the page constraints , interested reader are referred to the available studies both for conventional systems @xcite and ncss @xcite .",
    "besides the mentioned analytical way of handling the stochastic behaviors of the network , it is interesting to note that the _ feedback _ scheduling nature of the presented ideas by itself is capable of handling moderate disturbances , including delays and packet losses , as shown in section [ simulations ] .",
    "the fact that the problems so far had a fixed and finite final time helped in developing the training algorithms .",
    "in other words , both having the final condition given by eq .",
    "( [ bellman_finhor_eq1 ] ) and the point that having @xmath84 , function @xmath85 can be found using eq .",
    "( [ bellman_finhor_eq2 ] ) helped in developing the solution proposed in the previous sections , i.e. , calculating the parameters / weights in a backward - in - time fashion . in many applications ,",
    "however , the cost function has an _ infinite horizon _ , e.g. , regulation of states .",
    "for such a case , with a cost function similar to @xmath86 the objective is developing an adp - based solution which optimizes the _ infinite - horizon _ cost function ( [ costfunction_infhor ] ) , subject to the dynamics given in eq .",
    "( [ dynamics ] ) and the given control policy @xmath22 .    considering the infinite horizon of the problem ,",
    "the concern of possible unboundedness of the cost function arises .",
    "for example , even if the network is always scheduled , a stabilizing , or even an asymptotically stabilizing , control policy @xmath22 may lead to an unbounded cost - to - go .",
    "therefore , motivated by the adp literature , the definition of _ admissibility _ is presented next and the control policy @xmath22 is assumed to be admissible .",
    "[ def1 ] a control policy @xmath22 is defined to be _",
    "admissible _ within a compact set if a ) it is a continuous function of @xmath2 in the set with @xmath87 , b ) it asymptotically stabilizes the system within the set , and ( c ) the respective _ ` cost - to - go ' _ or _",
    "` value function ' _ starting from any state @xmath88 in the set , denoted with @xmath89 and defined by @xmath90 is continuous in @xmath46 . in eq .",
    "( [ valuefunction_of_h ] ) one has @xmath91 and @xmath92 . in other words , @xmath93 denotes the @xmath5th element on the state trajectory / history initiated from @xmath88 and propagated using control policy @xmath22 .",
    "set @xmath94 denotes non - negative integers .",
    "the main difference between the defined admissibility and the ones typically presented in the adp / rl literature , including @xcite , is the assumption of continuity of the respective value function , instead of the assumption of its finiteness in the set .",
    "the continuity requirement is added to guarantee the possibility of _ uniform _ approximation of the function using parametric function approximators , @xcite .",
    "it should be noted that continuous functions are bounded in a compact set @xcite , hence , the continuity of the value function leads to its boundedness as well , which is an essential requirement for an admissible control .",
    "another concern is the infinite sum over the scheduling cost , that is , the contribution of @xmath52 in the running cost of @xmath95 .",
    "it should be noted that the running cost of @xmath96 for a constant @xmath17 , which was an option for fixed - final - time problems , is not desired for infinite - horizon problems .",
    "the reason is , such a cost function may become unbounded due to infinite sum of non - zero @xmath97 s .",
    "an option for infinite - horizon cost functions is discounting such a running cost , by multiplying the running cost evaluated at @xmath98 time step with @xmath99 for some @xmath100 , which leads to the contribution of @xmath101 s to be bounded by a convergent geometric series .",
    "however , it cancels the feature of utilizing the optimal value function as a lyapunov function for proof of stability of the resulting overall system .",
    "another option is utilizing a _ state - dependent scheduling weight _ , which changes continuously versus the state and vanishes at the origin .",
    "for example , let @xmath102 , where @xmath103 is a positive semi - definite function .",
    "if @xmath104 is such that there exists a finite @xmath105 which leads to @xmath106 , then , the admissibility of @xmath22 guarantees the finiteness of ( [ costfunction_infhor ] ) .",
    "denoting the optimal value function of the infinite - horizon problem at state @xmath2 with @xmath107 for some @xmath108 , the bellman equation for such a problem reads @xmath109 and the optimal triggering policy is given by @xmath110    it should be noted that in infinite - horizon problems the optimal value function is not a function of time . unlike eq .",
    "( [ bellman_finhor_eq2 ] ) , the unknown value function exists on both sides of eq .",
    "( [ bellman_infhor_eq1 ] ) .",
    "therefore , instead of a recursion , one ends up with an equation to solve for the unknown value function .",
    "motivated by the value iteration ( vi ) scheme in adp / rl for solving conventional problems @xcite , starting with a guess on @xmath111 , for example @xmath112 , the iterative relation @xmath113 may be used for obtaining the value function of the infinite - horizon problem within compact set @xmath46 , where the superscript on @xmath114 denotes the _ index of iteration_. considering the successive approximation nature of ( [ infhor_iteration ] ) , several fundamental questions arise ; 1 ) does the iterative relation converge ?",
    "2 ) what initial guess on @xmath111 guarantees convergence ?",
    "3 ) if it converges , does it converge to the optimal value function ?",
    "4 ) is the limit function , i.e. , the function to which sequence @xmath115 converges , a continuous function , in order to use nn for _ uniformly _ approximating it ?    in a previous work of the author on the convergence analysis of adp ,",
    "these questions are answered , @xcite .",
    "for example , it is shown that any @xmath116 leads to the convergence of the vi to the optimal value function .",
    "the idea is establishing an _ analogy _ between the @xmath117th iteration of eq .",
    "( [ infhor_iteration ] ) and the time - to - go of the respective finite - horizon problem , i.e. , eq .",
    "( [ bellman_finhor_eq2 ] ) . selecting @xmath118",
    ", it directly follows that @xmath119 , by comparing eq .",
    "( [ infhor_iteration ] ) with eq .",
    "( [ bellman_finhor_eq2 ] ) .",
    "hence , the convergence questions 1 to 3 simplify to whether or not the value function of the finite - horizon problem converges to the value function of the infinite - horizon problem , as the horizon extends to infinity .",
    "the answer is positive following the line of proof presented in @xcite . as for the fourth question ,",
    "even though each @xmath120 and hence , each @xmath114 is a continuous function , it should be noted that unless the convergence of the sequence is _ uniform _ , @xcite , the continuity of the limit function @xmath121 does not follow necessarily .",
    "this concern also is addressed through establishing a sufficient condition for uniform convergence of the sequence in @xcite , motivated by @xcite .    while , for simplicity , the abovementioned ( infinite - horizon ) results are presented for the case of nfnc , they are applicable to the more advances schemes of zoh and gzoh as well , by utilizing @xmath122^t$ ] as the state of the system instead of @xmath2 , in the vi .",
    "the reason is , as the horizon extends to infinity , the fixed - final - time solutions converge to the infinite - horizon solutions .",
    "this can be seen by noting that recursive relation ( [ bellman_finhor_zoh_eq2 ] ) , applicable to both zoh and gzoh , is actually a vi .",
    "some theoretical results regarding the stability of the triggering policy resulting from the vi are given in this section .",
    "the results address three different issues that will exist in almost any implementation ; 1 ) the dynamics of the system will not be perfectly known , 2 ) the iterations of vi will be terminated at a finite @xmath117 , and 3 ) approximation errors will exist in approximating the value functions . before going through the analyses , an assumption needs to be made .",
    "[ assum_invariantset ] the intersection of the set of n - vectors @xmath2 at which @xmath123 with the invariant set of @xmath124 only contains the origin , @xmath125 .",
    "assumption [ assum_invariantset ] assures that there is no set of states ( besides the set containing only the origin ) in which the state trajectory can _ hide _ forever , in the sense that the running cost evaluated at those states is zero , and hence the optimal value function is zero , without convergence of the states to the origin .    when using zoh and gzoh , however , it should be noted that the state vector will be @xmath126^t$ ] .",
    "if @xmath127 then for any @xmath122^t$ ] one has @xmath128 .",
    "hence , especial care needs to be taken for satisfaction of assumption [ assum_invariantset ] .",
    "for example , if @xmath129 and @xmath22 are such that @xmath130 for any non - zero @xmath131 , then , satisfaction of assumption [ assum_invariantset ] for nfnc case leads to its satisfaction for the cases of zoh and gzoh .",
    "the reason is , if @xmath132 but @xmath133 for some @xmath5 , then , selecting @xmath28 , leads to @xmath134 and selecting @xmath27 leads to @xmath135 , because @xmath136 , hence , the trajectory can not stay in @xmath137^t\\in \\mathbb{r}^{2n } : x = 0 , d \\neq 0\\}$ ] with @xmath138 .",
    "[ stab_model_uncertainty ] let the actual dynamics of the overall system , including the dynamics of the plant @xmath129 , the selected control policy @xmath22 , and the model of the plant and its control policy @xmath76 ( if gzoh is implemented ) be given by @xmath139 , where the imperfect model @xmath140 is used for approximating the value function through the value iteration given by ( [ infhor_iteration ] ) .",
    "if the resulting optimal value function for the imperfect model , is lipschitz continuous with the lipschitz constant of @xmath141 , in the compact set @xmath46 , then , the resulting triggering policy @xmath142 given by ( [ bellman_infhor_eq2 ] ) asymptotically stabilizes the system if @xmath143    _ proof _ : function @xmath121 , being the limit function to the recursion in ( [ infhor_iteration ] ) , satisfies ( [ bellman_infhor_eq1 ] ) . by lipschitz continuity of the value function one",
    "has @xmath144 .",
    "therefore , @xmath145 if ( [ eq1_stab_model_uncertainty ] ) holds , inequality ( [ eq2_stab_model_uncertainty ] ) leads to asymptotic stability , as @xmath121 will be a lyapunov function for @xmath142 .",
    "note that , by assumption [ assum_invariantset ] , no non - zero state trajectory can stay in @xmath146 , hence , the asymptotic stability of @xmath142 follows from negative semi - definiteness of the difference between the value functions in ( [ eq2_stab_model_uncertainty ] ) , using lasalle s invariance theorem , @xcite .",
    "assume the approximation of each value function @xmath147 through ( [ infhor_iteration ] ) leads to the approximation error of @xmath148 .",
    "denoting the _ approximated _ value function with @xmath149 , it is propagated using the _ approximate vi _ given by @xmath150 once the approximation errors exist , the convergence of the value functions to the optimal value function is no longer guaranteed . as a matter of fact , it is not even guaranteed that the iterations converge .",
    "following the line of proof in @xcite , it can be proved that if @xmath151 , for some @xmath152 then each @xmath149 is lower and upper bounded , respectively , by @xmath153 and @xmath154 at each @xmath117 , if @xmath155 where , @xmath156 @xmath157 in other words , @xmath153 and @xmath154 correspond to the _ exact _ value iteration for cost functions @xmath158 @xmath159 the next theorem provides a sufficient condition for stability of both having an approximation error and/or terminating the iterations after a finite @xmath117 .",
    "[ stab_approx_errors ] let the approximate value iteration given by ( [ eq1_avi ] ) be conducted using a continuous function approximator with the approximation error bounded by @xmath151 , for some @xmath160 let the iteration terminate at the @xmath161 iteration when @xmath162 for some positive ( semi-)definite real - valued function @xmath163 . the resulting triggering policy @xmath164 asymptotically stabilizes the system if @xmath165    _ proof _ : by ( [ eq1_avi ] ) and ( [ eq1_avi_lowerbound_costfunction ] ) one has @xmath166 or @xmath167 considering @xmath168 and ( [ eq1_stab_approx_errors ] ) , the foregoing inequality along with assumption [ assum_invariantset ] lead to the asymptotic stability of the system .",
    "note that the lower boundedness of @xmath149 by @xmath153 guarantees the positive definiteness of the candidate lyapunov function , and its continuity follows from the continuity of the selected function approximator .",
    "considering the point that the approximation errors exist , inequality ( [ eq1_avi_lowerbound_costfunction ] ) may never be satisfied . in this case , one needs to decrease the approximation error , through selecting a richer function approximator .",
    "such an action is helpful , because of the lower and upper boundedness of @xmath149 , by the exact value functions and the convergence of the exact value functions to the same optimal value function as @xmath169 and @xmath170 .",
    "all the previously discussed ideas require the knowledge of the dynamics of the plant .",
    "for example , considering the simple case of nfnc , section [ basicproblem ] , the model will be required in two places .",
    "1 ) in offline training , because of the existence of @xmath124 in eq . ( [ bellman_finhor_eq2 ] ) .",
    "2 ) in online scheduling , because of the existence of @xmath124 in eq .",
    "( [ scheduler_eq1 ] ) .",
    "the objective in this section is developing completely _ model free _ methods .",
    "an idea toward this goal is utilizing two separate concepts from the adp / rl literature in control of conventional problems ; a ) learning action - dependent value functions @xcite which is called q - function in q - learning @xcite and b ) conducting online learning @xcite .",
    "consider the infinite - horizon problem discussed in section [ basicproblem_infhor ] .",
    "let the _ action - dependent value function _",
    "@xmath171 denote the incurred cost if _ action _",
    "@xmath49 is taken at the current time and the _ optimal actions _ are taken for the future times , for some @xmath172 . then , by definition , one has @xmath173 motivated by vi and the previous section , the solution to ( [ bellman_actiondep_eq2 ] ) can be obtained through selecting an initial guess @xmath174 and conducting the successive approximation given by @xmath175 this learning process is called q - learning , @xcite .    for simplicity ,",
    "the idea of nfnc is being considered here for deriving a model free algorithm ; however , other schemes , e.g. , zoh , directly follow , by replacing @xmath2 with @xmath72 . the interesting feature of action dependent value functions is the fact that the scheduler does not require the model of the system , since , @xmath176 note that , @xmath177 , where @xmath107 is the ( action independent ) value function presented in section [ basicproblem_infhor ] which satisfies eq .",
    "( [ bellman_infhor_eq1 ] ) . therefore , using this idea , the need for the model in the scheduling stage can be eliminated . as for the need for the model in the training , i.e. , in eq .",
    "( [ eq_advi ] ) , the following idea can be utilized .",
    "if the learning is conducted ` on the fly ' , i.e. , _ online learning _ , then , no model of the system is required for training also .",
    "the reason is , instead of using @xmath178 for finding @xmath60 to be used in the right hand side of eq .",
    "( [ eq_advi ] ) , one can wait for one time step and measure @xmath60 directly . as a matter of fact ,",
    "this is how we learn , for example , to drive a car , i.e. , by waiting and observing the outcomes of the taken actions , @xcite .",
    "let @xmath179 and @xmath180 be approximated using two separate function approximators ( because of the possible discontinuity of @xmath181 with respect to its second argument ) .",
    "considering eq .",
    "( [ eq_advi ] ) , the outline of the online learning process is presented in algorithm 1 .    * algorithm 1 *    step 1 : select initial guesses @xmath182 and @xmath183 and set @xmath42 .",
    "step 2 : randomly select @xmath184 .",
    "step 3 : apply @xmath59 on the system , i.e. , trigger or do nt trigger based on the selected @xmath59 .",
    "step 4 : wait for one time step and measure @xmath60 .",
    "step 5 : update the action dependent value function corresponding to the selected @xmath59 using @xmath185 and keep the value function of the action which was not taken constant , i.e. , @xmath186    step 6 : go back to step 2 .",
    "an important point is , the proposed scheme is entirely model - free and does not need any _ model identification _",
    ", unlike the ( model - free ) methods that conduct a separate model identification phase to identify the model and then use the result in the learning or control process , e.g. @xcite .",
    "it is worth mentioning that the presented algorithm fits in the category of _ exploration _ in the rl literature , @xcite .",
    "this is equivalent of the condition of persistency of excitation @xcite in conventional optimal and adaptive control .",
    "the point is , through the random selection of the decisions , the algorithm gives the chance to the parameters of every function approximator to learn different ` behaviors ' of the system .",
    "section [ model_free ] presented the idea for a model - free scheduler , which calls for _ online learning_. however , the _ stability _ of the system during the online learning can be at stake , considering the point that the decisions are made randomly . motivated by adp / rl , one may conduct _ exploitation _ as well , through replacing step 2 of algorithm 1 with the following step , in some iterations .",
    "step 2 : select @xmath187 .",
    "+ it should be noted that one will still need to do exploration occasionally , through selecting different random @xmath59 s .",
    "a reason is , if a particular @xmath49 is not selected in step 2 of the exploitation algorithm , the respective value function will never get the chance to be learned , i.e. , the parameters of the function approximator corresponding to that action will never be updated in step 5 . while the selection of the decisions through the exploitation phase decreases the concern ( as compared with the random selection of the decisions ) , the stability concern still exists .",
    "the reason is , the decisions made in the exploitation actions are based on the current , possibly immature , version of the value function , @xmath188 as long as @xmath189 is not optimal , its resulting policy , @xmath190 , may not even be stabilizing . this concern can be addressed through utilizing the value function of a stabilizing triggering policy as the initial guess , presented in a previous work in @xcite .",
    "@xcite , however , investigated conventional optimal control problems ( where the decision variable changes continuously ) with an action - independent value function .",
    "the rest of this section is devoted to adapting the stability and convergence results presented in that work to both the decision making / switching case and the case of action dependence , i.e. , the problem at hand .",
    "the adaptation of the results is not trivial , hence , the proofs of the theorems , along with some required lemmas , are included in the appendex of this paper .",
    "these results can also be extended to the case of having approximation errors , using the derivations in @xcite as the basis ( skipped due to the page constraints ) .",
    "considering the definition of the ( action independent ) value function of a _ control _ policy @xmath22 assuming constant access to state measurement , given in definition [ def1 ] and denoted with @xmath191 , the action dependent value function of a _ triggering _ policy given by @xmath192 may be defined as @xmath193 where @xmath194 and @xmath195 . in other words , @xmath196 denotes the @xmath5th element on the state trajectory initiated from @xmath88 and propagated using triggering decision @xmath49 for the first time step and then using triggering policy @xmath192 for the rest of the times .",
    "obviously @xmath197 depends on the control policy @xmath22 as well , but , as long as it is clear , the inclusion of ` @xmath198 ' is skipped in the notation for the value function , for notational simplicity . considering ( [ eq_v0_definition ] ) , it can be seen that @xmath197 solves @xmath199    [ admissible_triggering_policy ] the triggering policy @xmath192 whose action dependent value function @xmath200 is continuous and hence bounded in a compact set , @xmath125 , is called an admissible triggering policy .",
    "[ stabilizingadvi_definition ] the action - dependent value iteration ( advi ) scheme given by recursive relation ( [ eq_advi ] ) which is initiated using the action dependent value function of an admissible triggering policy is called stabilizing action dependent value iteration ( sadvi ) .",
    "besides the theoretical stability analyses , presented next , for practice however , how can one find an initial admissible triggering policy , especially if the dynamics of the system is not known ? to address this question , the fact that we already have an admissible control policy @xmath22 on the controller side should be considered .",
    "the simple triggering policy of @xmath201 is an admissible triggering policy .",
    "once an admissible triggering policy is selected , its action dependent value function can be obtained using theorem [ thm_conv_v0 ] .",
    "let @xmath202 ( respectively , @xmath203 ) denote that function @xmath204 is continuous at point @xmath205 for the given @xmath206 , ( respectively , within @xmath46 for every given @xmath49 ) .",
    "[ thm_conv_v0 ] if @xmath192 is an admissible triggering policy within @xmath46 , then selecting any @xmath207 which satisfies @xmath208 , the iterations given by @xmath209 converge uniformly to the action dependent value function of @xmath192 , in compact set @xmath46 .",
    "the next theorem proves the convergence of sadvi to the optimal action dependent value function .",
    "[ theorem_vi_convergence ] the stabilizing action dependent value iteration converges to the optimal action dependent value function of the infinite - horizon problem within the selected compact domain .",
    "[ thm_stab_lyapfun ] let the compact domain @xmath210 for any @xmath211 be defined as @xmath212 and let @xmath213 be ( the largest @xmath214 ) such that @xmath215 . then , for every given @xmath216 , triggering policy @xmath217 resulting from stabilizing action dependent value iteration asymptotically stabilizes the system about the origin and @xmath218 will be an estimation of the region of attraction for the system .",
    "theorem [ thm_stab_lyapfun ] proves that each single @xmath190 if _ constantly _ applied on the system , will have the states converge to the origin .",
    "however , in online learning , the triggering policy will be subject to adaptation .",
    "in other words , if @xmath190 is applied at the current time , triggering policy @xmath219 will be applied at the next time - step .",
    "it is important to note that even though theorem [ thm_stab_lyapfun ] proves the asymptotic stability of the _ autonomous _ system @xmath220 for every fixed @xmath117 , it does not guarantee the asymptotic stability of the _ time varying _ system @xmath221 .",
    "therefore , following a similar discussion in @xcite for conventional problems , it is required to have a separate stability analysis to show that the trajectory formed under the _ adapting / evolving _ triggering policy also will converge to zero .",
    "an idea for doing that is finding a _ single _ function to be a lyapunov function for _ all _ the triggering policies .",
    "the proof of the following theorem , however , uses another approach .",
    "[ thm_stabil_sadvi_no_lyap ] if the system is operated using triggering policy @xmath222 at time @xmath5 , that is , the policy subject to adaptation in the stabilizing action dependent value iteration , then , the origin will be asymptotically stable and every trajectory contained in @xmath46 will converge to the origin .",
    "finally , the next theorem provides an idea for finding an estimation of the region of attraction ( eroa ) for the evolving triggering policy .",
    "[ thm_roa_evolving_vi ] let @xmath212 and @xmath223 for any @xmath211 .",
    "also , let the system be operated using triggering policy @xmath222 at time @xmath5 , that is , the control subject to adaptation in the stabilizing value iterations .",
    "if @xmath224 for an @xmath225 then @xmath226 is an estimation of the region of attraction of the closed loop system .    before concluding the theoretical analyses on the stability of the system operated under sadvi ( online learning ) , it should be added that algorithm 1 updates @xmath227 only at the current state at each time .",
    "but , the presented theory on the stability of the system under online learning is based on ( [ eq_advi ] ) , i.e. , each new value function is calculated based on the entire @xmath46 . a possible scenario for satisfying",
    "such a condition is having multiple similar sensor - controller - plant sets evolving together and sharing their ` observations ' at different respective states with each other , to end up with one new @xmath227 at each time step .",
    "if this condition is not satisfied , one may still utilize the _",
    "pointwise _ update given by algorithm 1 , however , the stability will then depend on the generalization capability of the function approximator and is not directly guaranteed . in this case ,",
    "an idea is monitoring the states to switch to an available stabilizing triggering policy within @xmath46 , ( e.g. , @xmath201 ) if the states approached the boundaries of @xmath46 , which is an eroa for the stabilizing policy .",
    "however , once the learning converges for different states within @xmath46 , the stability of the proposed scheme is guaranteed by theorem [ thm_stab_lyapfun ] , with the respective eroa .",
    "in order to demonstrate the potential of the schemes in practice , a few simple examples are simulated in this section .",
    "the simulated plant is a scalar system with the dynamics of @xmath228 and the selected control policy is @xmath229 .",
    "the problem is discretized with sampling time @xmath230 @xmath231 and @xmath232 .",
    "the zoh scheme is implemented using @xmath233 , @xmath234 , @xmath235 .",
    "the function approximator was selected in a polynomial form made of @xmath2 and @xmath131 , up to the fourth order , where the coefficients are the tunable parameters .",
    "the approximation domain was selected as @xmath236^t\\in [ -2,2 ] \\times [ -2,2]\\}\\subset \\mathbb{r}^2 $ ] .",
    "100 new random @xmath72s were selected from @xmath46 in each evaluation of ( [ bellman_finhor_zoh_eq2 ] ) to conduct least squares for finding the parameters .",
    "the approximation of the value functions took less than 3 seconds in a computer with the cpu of intel core i7 , 3.4 ghz running matlab in single threading mode .",
    "the result was utilized for controlling initial condition @xmath237 . in order to simulate real - world conditions , a random time - varying disturbance force uniformly distributed between @xmath238 and @xmath239",
    "was applied on the system through its control input .",
    "the disturbance is selected large enough to destabilize the system when operated in an open loop fashion , that is , when the control is calculated ahead of time , from an assumed disturbance - free state trajectory resulting from @xmath240 .",
    "the simulation results , as presented in fig .",
    "[ figsim1 ] , show that the scheduler has been able to control the state of the system through only 5 network transmissions . for comparison purposes , the state history if the network is always scheduled and the state history if it is operated in the described open loop fashion are also plotted .",
    "the proposed method calls for the communication load which is less than 2% of what the always scheduled case requires .          to see the performance of the method in gzoh and also in dealing with lossy networks even without incorporating the stochastic nature of the problem with random losses as in @xcite ,",
    "the previous example is modified as follows .",
    "the imperfect model @xmath241 and imperfect control policy @xmath242 , instead of the actual system and control policy , are utilized for gzoh .",
    "moreover , it is assumed that the transmitted packets will be dropped with a 90% chance .",
    "the random disturbance force of the previous simulation is also applied .",
    "the results , presented in fig .",
    "[ figsim2 ] , show the capability of the controller in controlling the state and dealing with the very high packet loss probability .",
    "as seen in the history of data transmission , when the scheduler tries to send a state measurement to the controller if it fails , the scheduler _ keeps trying _ to send until it is successful .",
    "another important feature of the result is utilization of the gzoh , which lead to updating @xmath54 in fig .",
    "[ figsim2 ] , instead of keeping it constant as in fig .",
    "[ figsim1 ] .          in order to simulate the performance of the model - free scheme ,",
    "the dynamics of van der pol s oscillator , @xmath243 , and the ( feedback linearization based ) policy @xmath244 was chosen .",
    "the problem was taken into state space by defining @xmath245^t:=[z,\\dot{z}]^t$ ] and discretized with sampling time @xmath246 @xmath231 .",
    "the gzoh scheme was implemented using @xmath247 and @xmath248 with the imperfect model @xmath249 and imprecise control policy @xmath250 .",
    "the function approximator was selected in a polynomial form made of elements of @xmath2 and @xmath251^t$ ] , up to the third order .",
    "the approximation domain was selected as @xmath236^t : x , d\\in [ -2,2 ] \\times [ -2,2]\\ } \\subset \\mathbb{r}^4 $ ] .",
    "the action dependent value function of @xmath22 was calculated , using theorem [ thm_conv_v0 ] , as the initial guess for our online sadvi .",
    "a disturbance , uniformly distributed between @xmath238 and @xmath239 and acting on @xmath252 as an additive term , was applied , which destabilizes the system in case of no feedback information . using @xmath253 the system initially at the origin , will be stabilized , as shown by the red plots in fig .",
    "[ figsim3 ] .",
    "this policy however requires network communications for the entire time . using online learning through sadvi , _ without _ using the dynamics of the system , the control policy , or their respective models used in gzoh",
    ", the value function was updated within the first 3 seconds through both the exploration and exploitation algorithms ( chosen randomly at each time step ) .",
    "afterward , the updated triggering policy was utilized for scheduling the network . the resulting state trajectory ( including the first 3 seconds of learning )",
    "is depicted in fig .",
    "[ figsim3 ] . by the black plot .",
    "the history of the respective @xmath254 is also plotted . as seen , after the end of the learning",
    ", the network communication is decreased to a fraction of the times .",
    "this leads to a considerable saving of the network bandwidth , compared with the initial admissible triggering policy .",
    "adp was shown to be very promising in designing triggering policies which provide ( near ) optimal solutions to the networked control problems .",
    "the approach is very versatile in extending to zoh , gzoh , stochastic networks , infinite - horizon problems , and problems with unknown / uncertain dynamics .",
    "these features along with the low realtime computational load of the scheme make it very desirable .",
    "however , the approach calls for more theoretical rigor , for proof of stability and optimality .",
    "while this study took several steps to this end , many questions are left to be answered , particularly for stability of systems during online learning and the generalization capability of the function approximators .      _ * proof of theorem [ thm_conv_v0 ] * _ : eq .",
    "( [ pi_value_eq2 ] ) leads to @xmath255 comparing ( [ v0_conv_lem_eq1 ] ) with ( [ eq_v0_definition ] ) and considering @xmath256 one has @xmath257 .",
    "therefore , sequence @xmath258 is upper bounded by @xmath259 , for each given @xmath2 and @xmath49 .",
    "the limit function @xmath260 is equal to @xmath261 , since , the admissibility of @xmath192 leads to @xmath262 , and hence , @xmath263 as @xmath170 , due to @xmath256 .",
    "hence , ( [ v0_conv_lem_eq1 ] ) converges to ( [ eq_v0_definition ] ) as @xmath170 .",
    "this proves _",
    "pointwise _ convergence of the sequence to @xmath261 .",
    "but , this convergence is monotonic .",
    "note that for any arbitrary positive integers @xmath264 and @xmath265 , if @xmath266 , then @xmath267 since @xmath268 , and the last term in the foregoing inequality is only one of the non - negative terms in the summation in the right hand side of ( [ pi_value_eq3_2 ] ) .",
    "therefore , sequence of functions @xmath269 is pointwise non - decreasing . on the other hand , the limit function is continuous , by admissibility of the triggering policy , and also , each element of the sequence is continuous , as it is a finite sum of continuous functions .",
    "these characteristics lead to uniform convergence of the sequence in the compact set , by dini s uniform convergence theorem ( ref .",
    "@xcite , theorem 7.13 ) .        _ * proof of lemma [ lemma_nondecreasing ] * _ : the proof is done by induction . considering ( [ eq_advi_v0 ] ) , which gives @xmath174 , and ( [ eq_advi ] ) , which ( for @xmath271 ) gives @xmath272 , one has @xmath273 because @xmath272 is the result of minimization of the right hand side of ( [ eq_advi ] ) instead of being resulted from a given triggering policy @xmath192 . now , assume that for some @xmath117 , we have @xmath274 considering the definition of @xmath275 given by ( [ eq_advi_vi ] ) and using ( [ lemma1_eq2 ] ) as well as the fact that @xmath276 is not necessarily the minimizer used in the definition of @xmath277 , lead to @xmath278 therefore , @xmath279 which completes the proof .",
    "_ * proof of theorem [ theorem_vi_convergence ] * _ : considering finite - horizon cost function ( [ costfunction_finhor ] ) , the action dependent optimal value function @xmath280 is defined as the cost of taking action @xmath49 at the first time step and taking the ( history of ) time - dependent optimal actions with respect to the cost function with the horizon of @xmath281 for the remaining time steps . in other words , @xmath282 .",
    "therefore , @xmath283 @xmath284    selecting @xmath285 such that @xmath286 , one has @xmath287 the foregoing equation corresponds to the analogy between the finite horizon and infinite horizon action - independent value functions , discussed earlier and detailed in @xcite .    by the non - increasing ( cf .",
    "lemma [ lemma_nondecreasing ] ) and non - negative ( by definition ) nature of value functions under sadvi , they converge to some limit function @xmath288 . considering ( [ ad_bellman_finhor_eq3 ] ) , the limit function is the optimal action dependent value function to cost function ( [ costfunction_infhor ] ) .",
    "this can be observed by noticing that due to the convergence of sadvi , one has @xmath289 using decision sequence @xmath290 which is the sequence of decisions taken in evaluating cost - to - go @xmath291 .",
    "otherwise , @xmath292 becomes unbounded .",
    "note that per assumption [ assum_invariantset ] the state trajectory can not hide in the invariant set of @xmath124 with zero cost , to lead to a finite cost - to - go without convergence to the origin .",
    "therefore , by @xmath289 one has @xmath293 in calculation of @xmath294 . comparing finite - horizon cost function ( [ costfunction_finhor ] ) with infinite - horizon cost function ( [ costfunction_infhor ] ) and considering ( [ thm2_eq2 ] )",
    ", one has @xmath295 otherwise , the smaller value among @xmath171 and @xmath296 will be both the optimal action dependent value function ( evaluated at @xmath2 ) for the infinite - horizon problem and the greatest lower bound of the sequence of value function of the fixed - final - time problems resulting from @xmath297 .        _ * proof of lemma [ lemma_continuity_sadvi ] * _ : the continuity of each action - independent value function @xmath114 initiated from a continuous initial guess and generated using ( [ infhor_iteration ] ) for the general case of switching problems is proved in @xcite . considering ( [ eq_advi_vi ] ) and comparing ( [ eq_advi ] ) with ( [ infhor_iteration ] ) one has @xmath299 from @xmath300 for every finite @xmath117 , @xmath301 , @xmath302 , and eq .",
    "( [ eq_advi_vs_vi ] ) , it follows that @xmath303 by eq .",
    "( [ eq_advi ] ) .    _ * proof of theorem [ thm_stab_lyapfun ] * _ : the proof is done by showing that @xmath304 is a lyapunov function for @xmath217 , for each given @xmath117 . denoting the value function of the initial admissible triggering policy with @xmath174 ,",
    "it is continuous by definition of admissibility , and positive definite by positive semi - definiteness of @xmath305 and assumption [ assum_invariantset ] .",
    "note that , there is no @xmath306 with the value function of zero under any triggering policy .",
    "if @xmath307 for some @xmath117 is positive definite , it directly follows from ( [ eq_advi ] ) that @xmath277 will also be positive definite , because , if @xmath138 for some @xmath306 and @xmath49 , then @xmath308 by assumption [ assum_invariantset ] .",
    "hence , by induction , @xmath277 is positive definite for every @xmath216 . also , as shown in the proof of lemma [ lemma_continuity_sadvi ] it is a continuous function in @xmath46 . by ( [ eq_advi ] )",
    "@xmath309 one has @xmath310 , by ( [ eq_advi_vi ] ) .",
    "moreover , @xmath311 by lemma [ lemma_nondecreasing ] .",
    "hence , @xmath312 therefore , @xmath313 which leads to @xmath314 hence , the asymptotic stability of the system operated by @xmath217 follows considering assumption [ assum_invariantset ] and lasalle s invariance theorem , @xcite .",
    "set @xmath218 is an estimation of the region of attraction ( eroa ) @xcite for the closed loop system , because , @xmath315 by ( [ thm1_eq3 ] ) , hence , @xmath316 leads to @xmath317 . finally ,",
    "since @xmath318 is contained in @xmath46 , it is bounded . also , the set is closed , because , it is the _ inverse image _ of a closed set , namely @xmath319 $ ] under a continuous function , @xcite .",
    "hence , @xmath318 is compact .",
    "the origin is an _",
    "interior _ point of the eroa , because @xmath320 , @xmath321 , and @xmath322 .    _ * proof of theorem [ thm_stabil_sadvi_no_lyap ] * _ : eqs .",
    "( [ eq_advi ] ) and ( [ eq_advi_vi ] ) and the monotonicity feature established in lemma [ lemma_nondecreasing ] lead to @xmath323 and similarly @xmath324 let @xmath325 and @xmath326 . replacing @xmath327 in the left hand side of the inequality in ( [ thm_stab_nolyap_1 ] ) with the left hand side of ( [ thm_stab_nolyap_2 ] ) , which is smaller per ( [ thm_stab_nolyap_2 ] ) , one has @xmath328 repeating this process by replacing @xmath329 in ( [ thm_stab_nolyap_3 ] ) using @xmath330 leads to @xmath331 similarly by repeating this process one has @xmath332 since @xmath189 is positive definite , it can be dropped from the left hand side of the foregoing inequality .",
    "the result is , the sequence of partial sums in the left hand side is upper bounded by the right hand side and because of being non - decreasing , it converges , as @xmath333 , @xcite .",
    "therefore , @xmath334 as @xmath335 .",
    "considering assumption [ assum_invariantset ] , this leads to @xmath336 , as long as the entire state trajectory is contained in @xmath46 .",
    "_ * proof of theorem [ thm_roa_evolving_vi ] * _ : as the first step we show that for any given @xmath214 one has @xmath337 from ( [ thm1_eq3 ] ) one has @xmath338 .",
    "therefore , @xmath339 by ( [ thm1_eq1_2 ] ) and the definition of @xmath340 one has @xmath341 .",
    "therefore , @xmath342 finally ( [ eq_thm_roa_evolving_vi_2 ] ) and ( [ eq_thm_roa_evolving_vi_3 ] ) lead to ( [ eq_thm_roa_evolving_vi_1 ] ) .",
    "now that ( [ eq_thm_roa_evolving_vi_1 ] ) is proved , one may use mathematical induction to see @xmath343 the next step is noting that @xmath344 .",
    "this inequality leads to @xmath345 , by definition of @xmath340 and @xmath346 .",
    "therefore , ( [ eq_thm_roa_evolving_vi_3_1 ] ) leads to @xmath347 the result given by ( [ eq_thm_roa_evolving_vi_4 ] ) proves the theorem , because , if @xmath214 is such that @xmath224 then any trajectory initiated within @xmath226 will remain inside @xmath46 , and hence , by theorem [ thm_stabil_sadvi_no_lyap ] will converge to the origin .",
    "y.  halevi and a.  ray , `` integrated communication and control systems . part 1 - analysis , '' _ journal of dynamic systems , measurement and control , transactions of the asme _ , vol .",
    "110 , no .  4 , pp .  367373 , 1988 .",
    "j.  yook , d.  tilbury , and n.  soparkar , `` trading computation for bandwidth : reducing communication in distributed control systems using state estimators , '' _ ieee transactions on control systems technology _ , vol .  10 , pp .  503518 , jul 2002 .",
    "e.  garcia and p.  antsaklis , `` model - based event - triggered control for systems with quantization and time - varying network delays , '' _ ieee transactions on automatic control _",
    "58 , pp .",
    "422434 , feb 2013 .",
    "r.  yedavalli and r.  belapurkar , `` application of wireless sensor networks to aircraft control and health management systems , '' _ journal of control theory and applications _",
    "9 , no .  1 ,",
    "pp .  2833 , 2011 .",
    "k.  astrom and b.  bernhardsson , `` comparison of riemann and lebesgue sampling for first order stochastic systems , '' in _ proceedings of the ieee conference on decision and control _ , vol .  2 , pp .",
    "20112016 , dec 2002 .",
    "a.  sahoo , h.  xu , and s.  jagannathan , `` neural network - based adaptive event - triggered control of affine nonlinear discrete time systems with unknown internal dynamics , '' in _ american control conference _",
    ", 2013 .",
    "u.  premaratne , s.  halgamuge , and i.  mareels , `` event triggered adaptive differential modulation : a new method for traffic reduction in networked control systems , '' _ ieee transactions on automatic control _",
    "58 , pp .",
    "16961706 , july 2013 .",
    "j.  wu , f .- q .",
    "deng , and j .-",
    "gao , `` modeling and stability of long random delay networked control systems , '' in _ international conference on machine learning and cybernetics _ , vol .  2 , pp .  947952 , aug 2005 .",
    "j.  yi , q.  wang , d.  zhao , and j.  t. wen , `` bp neural network prediction - based variable - period sampling approach for networked control systems , '' _ applied mathematics and computation _ , vol .  185 , pp .  976  988 , 2007 .      h.  xu , s.  jagannathan , and f.  lewis , `` stochastic optimal control of unknown linear networked control system in the presence of random delays and packet losses , '' _ automatica _ , vol .",
    "48 , pp .",
    "1017  1030 , 2012 .",
    "l.  repele , r.  muradore , d.  quaglia , and p.  fiorini , `` improving performance of networked control systems by using adaptive buffering , '' _ ieee transactions on industrial electronics _ , vol .",
    "61 , pp .  48474856 , 2014 .",
    "l.  montestruque and p.  antsaklis , `` stability of model - based networked control systems with time - varying transmission times , '' _ ieee transactions on automatic control _ , vol .",
    ".  15621572 , sept 2004 .",
    "x.  zhong , z.  ni , h.  he , x.  xu , and d.  zhao , `` event - triggered reinforcement learning approach for unknown nonlinear continuous - time system , '' in _ int .",
    "joint conf . on neural networks _ , pp .",
    "36773684 , 2014 .",
    "j. werbos , `` reinforcement learning and approximate dynamic programming ( rladp)-foundations , common misconceptions , and the challenges ahead , '' in _ reinforcement learning and approximate dynamic programming for feedback control _ ( f.  l. lewis and d.  liu , eds . ) , pp .  130 , john wiley & sons , 2012 .",
    "d.  p. bertsekas , `` lambda - policy iteration : a review and a new implementation , '' in _ reinforcement learning and approximate dynamic programming for feedback control _ ( f.  l. lewis and d.  liu , eds . ) , pp .  381406 , john wiley & sons , 2012 .",
    "a.  al - tamimi , f.  lewis , and m.  abu - khalaf , `` discrete - time nonlinear hjb solution using approximate dynamic programming : convergence proof , '' _ ieee trans .",
    "systems , man , and cybernetics , part b : cybernetics _ , vol .",
    "38 , pp .  943949 , aug 2008 .",
    "ali heydari received his phd degree from the missouri university of science and technology in 2013 .",
    "he is currently an assistant professor of mechanical engineering at the south dakota school of mines and technology .",
    "he was the recipient of the outstanding m.sc .",
    "thesis award from the iranian aerospace society , the best student paper runner - up award from the aiaa guidance , navigation and control conference , and the outstanding graduate teaching award from the academy of mechanical and aerospace engineers at missouri s&t .",
    "his research interests include optimal control , approximate dynamic programming , and control of hybrid and switching systems .",
    "he is a member of tau beta pi ."
  ],
  "abstract_text": [
    "<S> the problem of resource allocation of nonlinear networked control systems is investigated , where , unlike the well discussed case of triggering for stability , the objective is _ optimal _ triggering . </S>",
    "<S> an approximate dynamic programming approach is developed for solving problems with fixed final times initially and then it is extended to infinite horizon problems . </S>",
    "<S> different cases including zero - order - hold , generalized zero - order - hold , and stochastic networks are investigated . </S>",
    "<S> afterwards , the developments are extended to the case of problems with unknown dynamics and a model - free scheme is presented for _ learning _ the ( approximate ) optimal solution . after detailed analyses of convergence , optimality , and stability of the results , the performance of the method is demonstrated through different numerical examples . </S>"
  ]
}