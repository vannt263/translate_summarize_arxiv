{
  "article_text": [
    "suppose that we have price data @xmath0 of a certain asset in a financial market .",
    "let @xmath1 be the log - return process , defined by @xmath2 .",
    "it is commonly believed that stochastic volatility models of the form @xmath3 describe much of the observed behaviour of this type of data . here",
    "@xmath4 is typically an i.i.d .",
    "noise sequence ( often gaussian ) and at each time @xmath5 the random variables @xmath6 and @xmath7 are independent .",
    "we will assume that the process @xmath8 is strictly stationary and that the ( multivariate ) marginal distributions of @xmath8 have a density with respect to the lebesgue measure on @xmath9 .",
    "our aim is to construct a nonparametric estimator for the multivariate density of @xmath10 , and to study its asymptotic behaviour .",
    "models that are used in the literature to describe the volatility display rather different invariant distributions .",
    "this observation lies at the basis of our point of view , which we pursue in this paper , that nonparametric estimation procedures are by all means sensible tools to get some insight in the behaviour of the volatility",
    ". quite often in models that are used in practice , the invariant distributions of @xmath8 are unimodal . since it is known that volatility clustering is an often occurring phenomenon , it is hard to believe that this can be explained by any of these models .",
    "instead , one would expect in such a case for instance the distribution of @xmath11 to have a density that has concentration regions around the diagonal with possibly peaks at certain clusters of low and high volatility , a phenomenon that may lead to for instance bimodal one - dimensional marginal distributions .",
    "nonparametric density estimation could perhaps reveal such a shape of the invariant density of the volatility .",
    "we will distinguish two classes of models in this paper .",
    "in both of them we will assume that the noise sequence is standard gaussian and that @xmath8 is a strictly stationary , positive process satisfying a certain mixing condition .",
    "the way in which the bivariate process @xmath12 , in particular its dependence structure , is further modelled differs however . in the first class of models that we consider ,",
    "we assume that the process @xmath8 is predictable with respect to the filtration @xmath13 generated by the process @xmath4 .",
    "note that @xmath6 is independent of @xmath7 for each fixed time @xmath5 .",
    "we furthermore have that ( assuming that the unconditional variances are finite ) @xmath14 is equal to the conditional variance of @xmath15 given @xmath16 .",
    "this class of models has become quite popular in the econometrics literature .",
    "financial data such as log - returns of stock prices or exchange rates are believed to share a number of stylized features , including for instance heavy - tailedness and long - range dependence .",
    "models of the type ( [ eq : s ] ) have been proposed to capture those features .",
    "a well - known family included in the class ( [ eq : s ] ) is the family of garch - models , introduced by bollerslev ( 1986 ) .",
    "for the garch(@xmath17)-model the sequence @xmath18 in ( [ eq : s ] ) is assumed to satisfy the equation @xmath19 where the @xmath20 and @xmath21 are nonnegative constants . under suitable assumptions ,",
    "see bougerol and picard ( 1992 ) , garch processes are stationary and the statistical problem in this case would be to estimate the coeficients @xmath20 and @xmath21 in  ( [ eq : garch ] ) .    in the second class of models that we consider ,",
    "we assume that the whole process @xmath8 is independent of the noise process @xmath4 . in this case",
    ", the natural underlying filtration @xmath22 is generated by the two processes @xmath4 and @xmath8 in the following way . for each @xmath5",
    "the @xmath8-algebra @xmath23 is generated by @xmath24 , @xmath25 and @xmath26 , @xmath27 .",
    "this choice of the filtration enforces @xmath8 to be predictable . as in the first model",
    "the process @xmath1 becomes a martingale difference sequence and we have again ( assuming that the unconditional variances are finite ) that @xmath14 is the conditional variance of @xmath15 given @xmath16 .",
    "an example of such a model is given in de vries ( 1991 ) , where @xmath8 is generated as an ar(1 ) process with @xmath28-stable noise ( @xmath29 ) .",
    "as we said before , we do not want to make a parametric assumption such as ( [ eq : garch ] ) , but we still want to measure the volatility of the data somehow . in the present paper",
    "we propose a nonparametric statistical procedure for this problem .",
    "using ideas from deconvolution theory , we will propose a procedure for the estimation of the marginal density at a fixed point . to assess the quality of our procedure",
    ", we will derive expansions of the bias and bounds on the variance .",
    "this will be done separately for the two kinds of model classes outlined above .",
    "we briefly review the construction of the deconvolution kernel density estimator based on i.i.d .",
    "observations , see also wand and jones ( 1995 ) . for simplicity",
    "we consider in this section the univariate case only .",
    "recall that the _ characteristic function _ or _",
    "fourier transform _ of a density function @xmath30 is defined by @xmath31 where @xmath1 is a random variable with density function @xmath30 . in the standard deconvolution setting",
    "the random variable @xmath1 is equal to the sum of two independent random variables , say @xmath32 , with unknown density @xmath33 , and @xmath4 , with known density @xmath34 .",
    "so @xmath30 is the convolution of @xmath33 and @xmath34 and @xmath35    the objective is to estimate @xmath33 from i.i.d .",
    "observations of @xmath36 having density @xmath30 . in identity ( [ ft:2 ] ) we know @xmath37 and we can estimate @xmath38 by the characteristic function of a _ kernel estimator _",
    "@xmath39 of @xmath30 .",
    "so @xmath40 where @xmath41 is an integrable function with integral one , called the _ kernel function _ , and @xmath42 is a positive number , called the _ bandwidth _ , governing the curvature of the estimate .",
    "the kernel estimator itself is also a convolution of the empirical distribution function @xmath43 of the observations and the rescaled kernel function @xmath44 .",
    "so , with @xmath45 the fourier transform of @xmath41 , @xmath46 where @xmath47 is called the _ empirical characteristic function_. from ( [ ft:2 ] ) we see that @xmath48 is an obvious candidate to estimate @xmath49 . applying an inverse fourier transform we obtain an estimator of @xmath33 .",
    "define the estimator @xmath50 of @xmath33 as @xmath51    the inversion is allowed if the function ( [ ft:3 ] ) is integrable . in general this is not guaranteed .",
    "however , to enforce integrability , we assume that @xmath45 has a bounded support .",
    "note that ( [ fourest ] ) can be rewritten as @xmath52 where @xmath53 it is easy to see that the function @xmath54 , and hence the estimator @xmath55 , is real valued .",
    "indeed , taking complex conjugates , we get @xmath56    a popular performance measure for deconvolution kernel estimators is the _",
    "mean squared error _ ( mse ) .",
    "the mse of @xmath55 is defined as . to obtain asymptotic expansions for the mse",
    ", we need expansions for the bias and variance of the estimator .",
    "the expectation of @xmath55 is equal to the expectation of an ordinary kernel density estimator of @xmath33 based on observations from @xmath33 .",
    "we have @xmath57 as @xmath58 , @xmath59 and @xmath60 , provided that @xmath41 is symmetric and @xmath33 satisfies some smoothness conditions , essentially twice differentiability at @xmath61 .",
    "the asymptotic variance of @xmath55 depends on the tails of the characteristic function of the density @xmath34 .",
    "the smoother @xmath34 , the faster the tails of the characteristic function vanish and the larger the asymptotic variance , see for instance fan ( 1991 ) .",
    "we consider the model  ( [ eq : s ] ) , so @xmath62 if we square this equation and take logarithms we get @xmath63 recall that under our assumptions for each @xmath5 the random variables @xmath6 and @xmath7 are independent .",
    "the density of @xmath64 , denoted by @xmath34 , is given by @xmath65 its graph is given in figure 1 below .",
    "@xmath66    as in section  [ primer ] , it seems reasonable to use a _ deconvolution kernel density estimator _ to estimate the unknown density @xmath33 of @xmath67 .",
    "an estimate of the density of @xmath68 or @xmath6 can then be obtained by a simple transformation .",
    "computing the characteristic function @xmath69 of @xmath70 we get , with @xmath71 as in ( [ densityk ] ) , @xmath72 where the gamma function @xmath73 is defined for all complex @xmath74 with positive real part by @xmath75 the graphs of re@xmath76 , im@xmath76 and @xmath77 are given in figures 2 and 3 .",
    "@xmath78    @xmath79    for the model  ( [ eq : s ] ) this leads to the estimator @xmath80 of the density @xmath33 of @xmath81 , with @xmath82 as in ( [ fourkernel ] ) .",
    "note that , like in the previous section , this estimator is real valued .",
    "the expression for the estimator of the density of the @xmath83-dimensional random vector @xmath84 is similar .",
    "we first introduce some auxiliary notation .",
    "let @xmath83 be fixed and write @xmath85 for a vector @xmath86 .",
    "we use similar boldface expressions for other ( random ) vectors .",
    "the kernel @xmath87 that we will use in the multivariate case is just a product kernel , @xmath88 . likewise",
    "then with @xmath90 defined by @xmath91 where @xmath92 and @xmath93 denotes inner product , the multivariate density estimator is given by @xmath94 where we use @xmath95 to denote the vector @xmath96 .",
    "the bias of the deconvolution estimator described in section  [ primer ] will be seen to be the same as the bias of a kernel density estimator based on independent observations from @xmath33 .",
    "hence , under standard smoothness assumptions , it is of order @xmath97 as @xmath59 .",
    "the variance of this type of deconvolution estimator heavily depends on the rate of decay to zero of @xmath98 as @xmath99 .",
    "the faster the decay the larger the asymptotic variance . in other words ,",
    "the smoother @xmath34 the harder the estimation problem .",
    "this follows for instance for i.i.d .",
    "observations from results in fan ( 1991 ) and for stationary observations from the work of masry ( 1991 , 1993a , b ) .",
    "the rate of decay of @xmath98 for the density ( [ densityk ] ) is given by lemma [ phiexpan ] in section [ proofs ] , where we show that @xmath100 by the similarity of the tail of this characteristic function to the tail of a cauchy characteristic function we can expect the same order of the mean squared error as in cauchy deconvolution problems , where it decreases logarithmically in @xmath101 , cf .",
    "fan ( 1991 ) for results on i.i.d .",
    "note that this rate , however slow , is faster than the one for normal deconvolution .    in the model  ( [ eq : model2 ] )",
    "the sequence @xmath102 is not independent , so results on the asymptotic behavior of the kernel estimator of section  [ primer ] are not directly applicable . in the literature also",
    "more general deconvolution problems have been studied , where the i.i.d .",
    "assumption has been relaxed .",
    "for instance , the deconvolution model @xmath103 , where @xmath104 is a stationary sequence and the sequences @xmath105 @xmath106 are independent has been treated by e. masry ( 1991 , 1993a , b ) .",
    "expansions for the variance of the deconvolution kernel estimator have been derived under several mixing conditions . under the assumption that the volatility process is independent of the noise sequence ,",
    "the model ( [ eq : model2 ] ) fits into this scheme . we will obtain similar results for the estimator when @xmath8 ( as a process ) is not independent of @xmath4 , but only predictable with respect to the filtration generated by @xmath4 .",
    "let us define the mixing conditions .",
    "for a certain process @xmath107 let @xmath108 be the @xmath8-algebra of events generated by the random variables @xmath109 .",
    "let the mixing coefficient @xmath110 be defined by @xmath111 we call a process @xmath107 _ strongly mixing _ if @xmath112 as @xmath113 .    to obtain expansions for the bias and variance we also need conditions on the kernel function @xmath41 such as bounded support of its characteristic function @xmath114 .",
    "moreover , the rate of decay to zero of @xmath114 at the boundary of its support turns up in the asymptotics .",
    "the complete list of assumptions on @xmath41 that we use is the following .",
    "* condition w. * let @xmath41 be a real symmetric function satisfying    1 .   @xmath115 2 .",
    "@xmath116 3 .",
    "@xmath117 4 .",
    "@xmath118 , 5 .",
    "@xmath45 , the characteristic function of @xmath41 has support [ -1,1 ] , 6 .",
    "@xmath119 for some @xmath120",
    ".    note that by fourier inversion these conditions imply that @xmath41 is bounded and lipschitz .",
    "more precisely , we have @xmath121 an example of such a kernel , from wand ( 1998 ) , with @xmath122 and @xmath123 , is @xmath124 it has characteristic function @xmath125 the next theorem , whose proof can be found in section  [ proofs ] , establishes the expansion of the bias and an order bound on the variance of our estimator under a strong mixing condition . under broad conditions",
    "this mixing condition is satisfied if the process @xmath8 is a markov chain , since then convergence of @xmath110 to zero takes place at an _ exponential rate _ , see theorems 4.2 and theorem 4.3 of bradley ( 1985 ) for precise statements .",
    "similar behaviour occurs for arma processes with absolutely continuous distributions of the noise terms ( bradley ( 1985 ) , example 6.1 ) .",
    "[ discrasthm ] assume that the process @xmath1 is strongly mixing with coefficient @xmath110 satisfying @xmath126 for some @xmath127 .",
    "let the kernel function @xmath41 satisfy condition w and let the density @xmath33 of the @xmath83-vector @xmath128 be bounded and twice continuously differentiable with bounded second order partial derivatives .",
    "assume that @xmath8 is a predictable process with respect to the filtration generated by the process @xmath4 .",
    "then we have for the estimator of the multivariate density defined as in  ( [ eq : fnhp ] ) and @xmath59 @xmath129 and @xmath130    [ discrasthmp ] assume that the process @xmath8 is strongly mixing with coefficient @xmath110 satisfying @xmath126 for some @xmath127 . let the kernel function @xmath41 satisfy condition w and let the density @xmath33 of the @xmath83-vector @xmath128 be bounded and twice continuously differentiable with bounded second order partial derivatives .",
    "assume furthermore that @xmath8 and @xmath4 are independent processes .",
    "then the multivariate density estimator @xmath50 satisfies the same bias expansion as in theorem  [ discrasthm ] .",
    "for the variance we have the sharper bound @xmath131    because of the exponential factor in the variance bound , in order to obtain consistency , one has to take essentially @xmath132 , see also stefanski ( 1990 ) for a related problem . on the other hand we would like to minimize the bias , so the choice @xmath133 is optimal . both bias and variance decay at a logarithmic rate for this choice of bandwidth .",
    "this seems disappointing , however fan ( 1991 ) shows for the i.i.d .",
    "situation of section 2 that we can not expect anything better .",
    "notice that the results in masry ( 1993a , b ) establishing strong consistency , rates of convergence and asymptotic normality are not useful here , because the condition that @xmath69 has either purely real or purely imaginary tails is not satisfied .    note that our assumptions in theorem  [ discrasthm ] are slightly different from those of masry ( 1991 ) .",
    "one of the essential facts that are used in the proof is the mixing property of @xmath1 .",
    "if @xmath8 and @xmath4 are independent processes this is implied by a similar assumption on the @xmath8 process itself as in masry ( 1991 ) .",
    "[ rem : ftilde ] in the case where the processes @xmath8 and @xmath4 are independent , the estimators @xmath55 have the following property .",
    "@xmath134=\\frac{1}{nh}\\sum_{j=1}^n w\\big(\\frac{x-\\log\\sigma^2_j}{h}\\big),\\ ] ] where @xmath135 denotes the @xmath8-algebra generated by the whole process @xmath8 .",
    "thus the @xmath136 would be ordinary kernel density estimators , if the @xmath137 could be observed .",
    "equation  ( [ eq : fff ] ) is seen to be true as follows .",
    "write @xmath138 and use similar notation for @xmath139 and @xmath140",
    ". then @xmath141 & = & \\frac{1}{2\\pi}\\int \\ex e^{is\\zeta_j / h}\\frac{\\phi_w(s)}{\\phi_k(s / h ) } e^{-is(x-\\tau_j)/h}\\ , ds \\\\ & = & \\frac{1}{2\\pi}\\int \\phi_w(s ) e^{-is(x-\\tau_j)/h}\\ , ds \\\\ & = & w\\big(\\frac{x-\\tau_i}{h}\\big).\\end{aligned}\\ ] ] the result now follows .",
    "of course , the analogous statement for the multivariate density estimator is equally true .",
    "one has @xmath142=\\frac{1}{nh^p}\\sum_{j = p}^n w\\big(\\frac{{\\mathbf{x}}-(\\log\\sigma^2_j,\\ldots,\\log\\sigma^2_{j - p+1})}{h}\\big),\\ ] ]    better bounds on the asymptotic variance than in theorem  [ discrasthm ] can be obtained under stronger mixing conditions .",
    "consider for instance _",
    "uniform mixing_. in this case the mixing coefficient @xmath143 is defined for @xmath144 as @xmath145 for @xmath146 .",
    "obviously , uniform mixing implies strong mixing . as a matter of fact , one has the relation @xmath147 see doukhan ( 1994 ) for this inequality and many other mixing properties . if @xmath18 is uniform mixing with coefficient @xmath148",
    "satisfying @xmath149 , then the variance bound  ( [ discrasthm:2p ] ) can be replaced with @xmath150 the proof of the latter bound runs similarly to the strong - mixing bound as given in section  [ proofs ] .",
    "the essential difference is that in equation  ( [ eq : mnh ] ) we use theorem 17.2.3 of ibragimov and linnik ( 1971 ) with @xmath151 instead of deo s ( 1973 ) lemma , as in the proof of theorem 2 in masry ( 1983 ) .",
    "the result is that we can now bound the term @xmath152 of equation ( [ eq : mnh ] ) by a constant times @xmath153 . after this step",
    "the proof is essentially unchanged .",
    "use the estimate @xmath154 to finish the proof .",
    "notice that this bound on the variance is of the same order as the one we obtained in theorem  [ discrasthmp ] , where @xmath8 was only assumed to be strongly mixing .",
    "this bound can not be improved upon by strengthening the assumption to uniform mixing .",
    "an example of an observed process that is stongly mixing and that belong to the first model class is a garch@xmath155 process .",
    "it has been shown in carasso and chen ( 2002 ) ( see also boussama ( 1998 ) ) that such a process is @xmath156-mixing with exponentially decaying @xmath156-mixing coefficients .",
    "hence this process is also @xmath28-mixing , since the @xmath156-mixing coefficient @xmath157 ( see doukhan ( 1994 ) ) .",
    "notice that we also have that the assumption of theorem  [ discrasthm ] on the @xmath28 s is satisfied in this case .",
    "all the estimators that we proposed involve the functions @xmath69 and @xmath45 . for these functions and related ones we need expansions and order estimates . these are collected in the lemmas of this subsection .",
    "[ phiexpan ] for @xmath99 we have @xmath158,\\\\ \\im \\phi_k(t)&=&|\\phi_k(t)|[\\sin(t\\log(\\sqrt{1 + 4t^2}-t))+o(\\tfrac{1 } { |t|})].\\end{aligned}\\ ] ]    by the stirling formula for the complex gamma function , cf .",
    "abramowitz and stegun ( 1964 ) chapter 6 , we have @xmath159 as @xmath160 and @xmath161 for some @xmath162 .",
    "so for @xmath163 and @xmath99 we get @xmath164 here we have used the expansion @xmath165 , as @xmath5 tends to infinity . for negative @xmath5 a similar expansion holds . since @xmath166 has modulus one , substituting this expansion in ( [ phik ] ) now proves the first statement of the lemma .",
    "the argument of @xmath167 satisfies @xmath168 so , since @xmath169 , we have @xmath170 which proves the second and third statement of the lemma .",
    "consider now the function @xmath54 defined in  ( [ fourkernel ] ) .",
    "[ l2exp ] we have the following order estimate for the @xmath171 norm of @xmath54 . for @xmath59 @xmath172    by parseval s identity @xmath173 write @xmath174 the integral in ( [ een ] ) can be rewritten as @xmath175 by the dominated convergence theorem . omitting constants",
    ", we can rewrite the integral ( [ twee ] ) as @xmath176 by the dominated convergence theorem .",
    "we have used the fact that both the functions @xmath177 and ( see lemma  [ phiexpan ] ) @xmath178 are bounded and that the second function is of order @xmath179 as @xmath180 tends to infinity .",
    "this shows that the term ( [ twee ] ) is negligible with respect to ( [ een ] ) .",
    "[ cor : vp ] the @xmath171-norm of the function @xmath90 , defined in ( [ eq : bvh ] ) is of order @xmath181 .",
    "this follows from the product form of @xmath90 given by @xmath182 .",
    "* proof of theorem  [ discrasthm ] . * the expansion  ( [ discrasthm:1p ] ) follows from theorem 1 in masry ( 1991 ) . to prove the variance bound ( [ discrasthm:2p ] ) we argue as in the proof of theorem 2 in the same paper .",
    "first we give a bound on the variance in terms of the @xmath183-norm of the function @xmath90 and then we exploit the asymptotic expansion of the characteristic function @xmath69 as given in lemma  [ phiexpan ] to get a sharper bound on the @xmath183-norm of @xmath90 than masry in his proposition 3 by taking the behaviour of @xmath45 at the boundary of its support into account .",
    "some details follow .    argueing as in masry ( 1991 )",
    "we can show that @xmath184 with ( up to a multiplicative constant ) @xmath185 where @xmath186 .    applying a lemma by deo ( 1976 ) , we can bound for strong mixing process @xmath1 with mixing coefficients @xmath187 the term @xmath152 by a constant ( not depending on @xmath101 and @xmath188 ) times @xmath189 which , by stationarity , becomes @xmath190 observe now that , by boundedness of the density of @xmath191 , the term @xmath192 can be bounded by a constant times @xmath193 and that we can therefore write @xmath194 the proof will be finished by application of corollary  [ cor : vp ] , which gives the @xmath171-norm of @xmath90 , and an estimate of the @xmath195-norm of @xmath90 . for the latter one we have the inequalities @xmath196 and @xmath197 for some constant @xmath198 by the fact that @xmath45 has compact support . as a result we get @xmath199 and that @xmath152 is less than a constant times @xmath200 .",
    "the bound on @xmath201 of theorem  [ discrasthm ] now follows .",
    "@xmath202 + * proof of theorem  [ discrasthmp]*. let @xmath135 be the @xmath8-algebra generated by the process @xmath8 .",
    "we use the decomposition @xmath203 with @xmath204 as in remark  [ rem : ftilde ] .",
    "we now consider the first term in  ( [ eq : decvar ] ) .",
    "let @xmath205 and @xmath206 .",
    "since the @xmath207 are independent given @xmath135 we can bound the conditional variance by @xmath208\\ ] ] which is by conditional independence and stationarity equal to @xmath209 with @xmath198 the maximum of @xmath210 , the density of @xmath211 . therefore the first term in  ( [ eq : decvar ] ) is of order @xmath212 , so of order @xmath213 .",
    "the second term of ( [ eq : decvar ] ) is treated next .",
    "we have with @xmath214 @xmath215 the first term reduces by stationarity to @xmath216 which can be bounded by a constant times @xmath217 , since @xmath218 has by assumption a bounded density . for the second term",
    "we proceed as in the proof of theorem  [ discrasthm ] . using stationarity",
    "we write it as @xmath219 we split the summation into two parts . in the first part we consider @xmath220",
    "whose absolute value can be bounded in view of the cauchy - schwarz inequality and stationarity by @xmath221 , which is bounded by @xmath222 .",
    "the absolute value of the second part @xmath223 can be bounded by invoking once more deo s result by @xmath224 which is less than @xmath225 hence we have that @xmath226 is of order @xmath227 .",
    "combining the obtained order estimates for the two terms of  ( [ eq : decvar ] ) and using the @xmath171-norm of the function @xmath90 gives the desired result .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ abramowitz , m.  and stegun , i.  ( 1964 ) , _ handbook of mathematical functions , ninth edition _ , dover , new york",
    ".                                        wand , m.p .  and jones , m.c .",
    "( 1995 ) , kernel smoothing , chapman and hall , london . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _"
  ],
  "abstract_text": [
    "<S> we consider discrete time models for asset prices with a stationary volatility process . </S>",
    "<S> we aim at estimating the multivariate density of this process at a set of consecutive time instants .    </S>",
    "<S> a fourier type deconvolution kernel density estimator based on the logarithm of the squared process is proposed to estimate the volatility density . </S>",
    "<S> expansions of the bias and bounds on the variance are derived . + _ key words : _ stochastic volatility models , density estimation , kernel estimator , deconvolution , mixing + _ ams subject classification : _ </S>",
    "<S> 62g07 , 62m07 , 62p20 </S>"
  ]
}