{
  "article_text": [
    "ferrofluids are media composed of magnetic nanoparticles of diameters in the range @xmath0 nm which are dispersed in a viscous fluid ( for example , water or ethylene glycol ) @xcite .",
    "these physical systems combine the basic properties of liquids , i.e. a viscosity and the presence of surface tension , and those of ferromagnetic solids possessing , i.e. an internal magnetization and high permeability to magnetic fields .",
    "this synergetic duality makes ferrofluids an attractive candidate for performing different tasks , ranging from the delivery of rocket fuel into spacecraft s thrust chambers under zero - gravity conditions to the high - precision drug delivery during cancer therapy @xcite .",
    "moreover , ferrofluids have already found their way into commercial applications as em - controlled shock absorbers , dynamic seals in engines and computer hard - drives , and as key elements of high - quality loudspeakers , to name but a few @xcite .",
    "being liquids , ferrofluids can be modeled by using different macroscopic , continuum - media approaches . the corresponding field , ferrohydrodynamics @xcite , is a well - established area that has produced a lot of fundamental results .",
    "however , some important phenomena such as magnetoviscosity @xcite can not be described properly neither on the hydrodynamical level nor within the single - particle picture @xcite .",
    "the role of multiparticle aggregates  chains and clusters  living in the ferrofluid bulk , is important there .",
    "the evaluation of other non - newtonian features of ferrofluids , which can be controlled by external magnetic fields @xcite , also demands information about structure of aggregates and their dynamics .",
    "the key mechanism responsible for the formation of ferro - clusters is the dipole - dipole interaction acting between magnetic particles . under certain conditions",
    ", the interaction effects can overcome the destructive effects of thermal fluctuations and contribute tangibly to the ensemble dynamics .",
    "therefore , in order to get a deeper insight into the above - mentioned features of ferrofluids , the interaction effects should be explicitly included into the model .",
    "strictly speaking , the dipole - dipole interaction acts between all the possible pairs of @xmath1 ferromagnetic particles .",
    "therefore , the computational time of the straightforward sequential algorithm scales like @xmath2 . this is a fundamental drawback of many - body simulations and standard , cpu - based computational resources often limit the scales of the molecular - dynamics calculations . in order to run larger ensembles for longer times ,",
    "researchers either ( i ) advance their models and numerical schemes and/or ( ii ) rely on more efficient computers .",
    "the first track led to different methods developed to explore the equilibrium properties of bulk ferrofluids .",
    "most prominent are cut - off sphere approximations @xcite and the ewald summation technique @xcite , see also ref .",
    "@xcite for different modifications of the technique .",
    "however , both methods are of limited use for computational studies of confined ferrofluids , a problem that attracts special attention nowadays due to its practical relevance @xcite .",
    "an alternative approach is the search for ways to increase scalability and parallelism of computations , see ref .",
    "@xcite . until recently , this practically meant the use of either large computational clusters , consisting of many central processing units ( cpus ) , or massively parallel supercomputers , like blue gene supercomputers .",
    "the prices and the maintenance costs of such devices are high .",
    "the advent of the general - purpose computing on graphics processing units ( gp@xmath3u ) @xcite has changed the situation drastically and boosted simulations of many - body systems onto new level @xcite .",
    "gpus , initially designed to serve as the data pipelines for graphical information , are relatively small , much easier to maintain , and possess high computation capabilities allowing for parallel data processing .",
    "nowadays , the scientific gpu - computing is used in many areas of computational physics , thanks to the compute unified device architecture ( cuda ) developed by nvidia corporation @xcite .",
    "cuda significantly simplifies gpu - based calculations so now one could use a sony playstation 3 as a multi - core computer by programming it with @xmath4 , @xmath5 or @xmath6 @xcite languages .",
    "the typical scale of molecular - dynamics simulations in ferrofluid studies is within the range @xmath7 @xcite , while @xmath8 constitutes the current limit @xcite .",
    "it is evident that the increase of the ensemble size by several orders of magnitude would tangibly improve the statistical sampling and thus the quality of simulation results .",
    "to run numerically @xmath9 interacting magnetic particles is a task not much different from the running of an artificial universe @xcite .",
    "therefore , similar to the case of computational cosmology @xcite , the gp@xmath3u seems to be very promising also in the context of ferrofluid simulations . in this paper",
    "we report the performance of a recently proposed gpu - oriented modification of the barnes - hut algorithm @xcite used to simulate the dynamics of @xmath10 interacting ferromagnetic particles moving in a viscous medium .    although being mentioned as a potentially promising approach ( see , for example , ref .",
    "@xcite ) , the barnes - hut algorithm was never used in molecular - dynamics ferrofluid simulations , to the best of our knowledge .",
    "our main goal here is to demonstrate a sizable speed - up one can reach when modeling such systems on a gpu  _ and _ by using gpu - oriented numerical algorithms  compared to the performance of conventional , cpu - based algorithms .",
    "we also demonstrate the high accuracy of the barnes - hut approximation by using several benchmarks . the paper is organized as follows :",
    "first , in section ii we specify the model .",
    "then , in section iii , we describe how both , the all - pairs and barn - hut algorithms , can be efficiently implemented for gpu - based simulations .",
    "the results of numerical tests are presented and discussed in section v. finally , section vi contains conclusions .",
    "the model system represents an ensemble of @xmath1 identical particles of the radius @xmath11 , made of a ferromagnetic material of density @xmath12 and specific magnetization @xmath13 .",
    "each particle occupies volume @xmath14 , has magnetic moment @xmath15 of constant magnitude @xmath16 , mass @xmath17 , and moment of inertia @xmath18 .",
    "the ensemble is dispersed in a liquid of viscosity @xmath19 . based on the langevin dynamics approach",
    ", the equations of motions for @xmath20-th nanoparticle can be written in the following form @xcite @xmath21 \\label{eq : mot_rot_tet } i\\ddot{\\varphi_k } \\!&=&\\ ! - n_{kx } \\sin \\varphi_k + n_{ky } \\cos \\varphi_k - g_r \\dot \\varphi_k + \\xi^{r}_{\\varphi } , \\\\[6 pt ] \\label{eq : mot_rot_phi } m\\ddot { \\vec r}_k \\!&=&\\ ! \\mu_0(\\vec m_k \\nabla_k ) \\cdot \\vec h_k + \\vec f_k^{sr } - g_d \\dot { \\vec r}_k + \\vec{\\xi}^{d}_{\\vec r } , \\label{eq : red_mot_disp}\\end{aligned}\\ ] ] where @xmath22 and @xmath23 are the polar and azimuthal angles of the magnetization vector @xmath24 respectively .",
    "@xmath25 , @xmath26 denotes the cartesian coordinates , dots over the variables denote the derivatives with respect to time .",
    "@xmath27 is the radius - vector defining the nanoparticle position , and the gradient is given by @xmath28 , ( @xmath29 are the unit vectors of the cartesian coordinates ) .",
    "constants @xmath30 , and @xmath31 specify translational and rotational friction coefficients , @xmath32 is the magnetic constant .",
    "the resulting field acting on the @xmath20-th particle @xmath33 is the sum of external field @xmath34 and overall field exerted on the particle by the rest of the ensemble , @xmath35 \\label{eq : h } \\vec h_{kj}^{dip } \\!&=&\\ ! \\frac{3",
    "\\vec r_{kj } ( \\vec m_j \\vec r_{kj } ) - \\vec m_j { \\vec r_{kj } } ^{\\,2 } } { |\\vec r_{kj}| ^5 } , \\label{eq : h_dip}\\end{aligned}\\ ] ] where @xmath36 .",
    "@xmath37 in eq .",
    "[ eq : red_mot_disp ] denotes the force induced by a short - range interaction potential . in this paper",
    "we use lennard - jones potential @xcite ( though hard sphere @xcite , soft sphere @xcite and yukawa - type @xcite potentials can be used as alternatives ) , so that @xmath38}. \\label{eq : lj}\\ ] ] here @xmath39 is the depth of the potential well and @xmath40 is the equilibrium distance at which the inter - particle force vanishes .",
    "the interaction between a particle and container walls is also modeled with a lennard - jones potential of the same type .",
    "the random - force vector , representing the interaction of a particle with thermal bath , has standard white - noise components , @xmath41 ( @xmath42 ) , @xmath43 ( @xmath44 , and second moments satisfying @xmath45 @xmath46 @xcite . here",
    "@xmath47 is the boltzmann constant and @xmath48 is the temperature of the heat bath .    by rescaling the variables , @xmath49 , @xmath50 , the equations of motions for the @xmath51th particle can be rewritten in the reduced form , @xmath52 where @xmath53    @xmath54,\\end{aligned}\\ ] ]    @xmath55,\\end{aligned}\\ ] ]    here @xmath56",
    "random - force vector components are given now by white gaussian noises , with the second moments satisfying @xmath57 @xmath58 @xmath59 @xmath60 .",
    "in general case the characteristic relaxation time of particle magnetic moments to their equilibrium orientations is much smaller than @xmath61 , and one can neglect the magnetization dynamics by assuming that the direction of the vector @xmath62 coincides with the easy axis of the @xmath51th particle .",
    "we assume that this condition holds for our model .",
    "in this section we discuss two alternative approaches to the numerical propagation of the dynamical system given by eqs .",
    "( [ eq : red_mot_rot_tet ] - [ eq : red_mot_disp_final ] ) on a gpu .",
    "it is assumed that the reader is familiar with the basics of the gp@xmath3u , otherwise we address him to refs .",
    "( @xcite ) that contain crash - course - like introductions into the physically - oriented gpu computing .      the most straightforward approach to propagate",
    "a system of @xmath1 interacting particles is to account for the interactions between all pairs .",
    "although exact , the corresponding all - pairs ( ap ) algorithm is slow when performed on a cpu , so it is usually used to propagate systems of @xmath63 particles .",
    "however , even with this brute - force method one can tangibly benefit from gpu computations by noticing that the ap idea fits cuda architecture @xcite .",
    "one integration step of the standard ap algorithm is performed in two stages .",
    "they are :    1 .",
    "calculation of increment of particle positions and magnetic moments directions .",
    "2 .   update particle positions and magnetic moments directions .",
    "this structure remains intact in the gpu version of the algorithm .",
    "kernels responsible for the first stage compute forces that act on the particles , and calculate the corresponding increments for particle positions and magnetic moments , according to equations ( [ eq : red_mot_rot_tet ] - [ eq : red_mot_disp_final ] ) .",
    "the increments are then written into the global memory . finally , second - stage kernels update particle states with the obtained increments . there is , however , a need for the global synchronization of the threads that belong to the different blocks after every stage since the stages are performed on separate cuda kernels and all the information about the state of the system is kept in the shared memory .",
    "each thread is responsible for one particle of the ensemble , and thus it should account for the forces exerted on the particle by the rest of the ensemble . to speed up the computational process",
    "we keep the data vector of the thread particle in the shared memory , as well as the information on other particles , needed to compute the corresponding interaction forces .",
    "thus we have two sets of arrays of data in the shared memory , namely    1 .",
    "data of the particles assigned to the threads of the block ; 2",
    ".   data of particles to compute interaction with .",
    "the necessary data are the coordinates of the particles and projections of their magnetic moment vectors onto @xmath64 , @xmath65 and @xmath66 axis .",
    "size of the arrays are equal to the number of threads per block .",
    "the first set of the data is constant during one integration step , but the second set is changed .",
    "so , at the beginning we upload the information on particle coordinates and momenta to the second set of arrays in the shared memory . after computing the forces acting on the block particles ,",
    "the procedure is repeated , i. e. the information on another set of particles is written into the second set of arrays and the corresponding interactions are computed . the corresponding pseudo - code is presented the below , @xmath67 , @xmath68 , @xmath69 , @xmath70 denote increments of a particle s coordinates @xmath64 , @xmath65 , @xmath66 , and the direction angles of particle magnetic moment , @xmath22 and @xmath23 , needed to propagate the particle over one time step @xmath71 . ] .    cached @xmath72 upload to the shared memory particle positions and angles e.g. @xmath73 = x_{global}[ind]$ ] , etc .",
    "calculate force and dipole field for the particle number @xmath74 on current @xmath20-th particle , and add the result to the total cached @xmath75 , @xmath76 , @xmath77 .",
    "_ _ syncthreads ( ) ; @xmath78 update cached @xmath79 , @xmath67 , @xmath80 , @xmath69 , @xmath70 according to equations [ eq : red_mot_rot_tet ] - [ eq : red_mot_disp_final ] .",
    "copy increments to global memory .",
    "the advantage of the described approach is that it uses the global memory in the most optimal way .",
    "the access to global memory is coalesced and there are no shared memory bank conflicts . for an ensemble @xmath8 particles",
    "this leads to the gpu occupancy of 97.7% .",
    "the all - pairs algorithm is simple and straightforward for implementation on a gpu and perfectly fits cuda . yet",
    "this algorithm is purely scalable .",
    "the corresponding computation time grows like @xmath81 , and its performance is very slow already for an ensemble of @xmath82 particles .    the barnes - hut ( bh ) approximation @xcite exhibits a much better scalability and its computational time grows like @xmath83 . the key idea of the algorithm is to substitute a group of particles with a single pseudo - particle mimicking the action of the group .",
    "then the force exerted by the group on the considered particle can be replaced with the force exerted by the pseudo - particle . in order to illustrate the idea",
    "assume that all particles are located in a three - dimensional cube , which is named main cell. the main cell is divided then into eight sub - cells .",
    "each sub - cell confines subset of particles .",
    "if there are more than one particle in the given sub - cell then the last is again divided into eight sub - cells .",
    "the procedure is re - iterated until there is only one particle or none left in each sub - cell . in this way",
    "we can obtain an octree with leaves that are either empty or contain single particle only .",
    "a simplified , two - dimensional realization of this algorithm is sketched with fig .",
    "[ fig : bh ] . by following this recipe",
    ", we can assign to every cell , obtained during the decomposition , a pseudo - particle , with magnetic moment @xmath84 equals to the sum of magnetization vectors @xmath85 of all particles belonging to the cell , and position @xmath86 which is the position of the set geometric center , @xmath87 where @xmath88 is a number of particles in the cell and @xmath89 is the position of the @xmath90-th particle from the sub - set .",
    "forces acting on @xmath20-th particle can be calculated by traversing the octree .",
    "if the distance from the particle to the pseudo - particle that corresponds to the root cell is large enough , the influence of this pseudo - particle on the @xmath20-th particle is calculated ; otherwise pseudo - particles of the next sub - cells are checked etc ( sometime this procedure can lead finally to a leaf with only one particle in the cell left ) .",
    "thus calculated force is added then to the total force acting on the @xmath20-th particle .",
    "the bh algorithm allows for a high parallelism and it is widely employed in computational astrophysics problems @xcite . however , implementation of the barnes - hut algorithm on gpus remained a challenge until recently , because the procedure uses an irregular tree structure that does not fit the cuda architecture well .",
    "it is the main reason why the bh scheme was not realized entirely on a gpu but some part of calculations was always delegated and performed on a cpu @xcite .",
    "a realization of the algorithm solely on a gpu has been proposed in 2011 @xcite .",
    "below we briefly outline the main idea and specify its difference from the standard , cpu - based realization .    to build an octree on a cpu",
    "usually heap objects are used .",
    "these objects contain both child - pointer and data fields , and their children are dynamically allocated .",
    "to avoid time - consuming dynamic allocation and accesses to heap objects , an array - based data structure should be used .",
    "since we have several arrays responsible for variables , the coalesced global memory access is possible .",
    "particles and cells can have the same data fields , e.g. positions . in this case",
    "the same arrays are used .",
    "in contrast to the all - pairs algorithm , where only two kernels were involved , in the original gpu - bh algorithm has six kernels",
    "@xcite :    1 .   bounding box definition kernel .",
    "2 .   octree building kernel .",
    "3 .   computing geometrical center and total magnetic moment of each cell .",
    "sorting of the particles with respect to their positions .",
    "computing forces and fields acting on each particle .",
    "integration kernel .",
    "kernel 1 defines the boundaries of the root cell .",
    "though the ensemble is confined to a container and particles can not go outside , we keep this kernel .",
    "the size of the root cells can be significantly smaller than the characteristic size of the container .",
    "moreover , the computation time of this kernel is very small , typically much less than 1% of the total time of one integration step .",
    "the idea of this kernel is to find minimum and maximum values of particle positions .",
    "here we use atomic operations and built - in @xmath91 and @xmath92 functions .    kernel 2",
    "performs hierarchical decomposition of the root cell and builds an octree in the three - dimensional case . as well as in following kernels ,",
    "the particles are assigned to the threads in round - robin fashion . when a particle is assigned to a thread",
    ", it tries to lock the appropriate child pointer . in the case of success",
    ", the thread rewrites the child pointer and releases a lock . to perform a lightweight lock , that is used to avoid several threads access to the same part of the tree array",
    ", atomic operation should be involved . to synchronize the tree - building process we use the @xmath93 barrier .",
    "kernel 3 calculates magnetic moments and positions of pseudo - particles associated with cells by traversing un - balanced octree from the bottom up .",
    "a thread checks if magnetic moment and geometric center of all the sub - cells assigned to its cell have already been computed .",
    "if not then the thread updates the contribution of the ready cells and waits for the rest of the sub - cells . otherwise the impact of all sub - cells is computed .",
    "kernel 4 sorts particles in accordance to their locations .",
    "this step can speed up the performance of the next kernel due to the optimal global memory access .",
    "kernel 5 first calculates forces acting on the particles , and then calculates the corresponding increments .",
    "then , in order to compute the force and dipole field acting upon the particle , the octree is traversed . to minimize thread divergence",
    ", it is very important that spatially close particles belong to the same warp . in this case",
    "the threads within the warp would traverse approximately the same tree branches .",
    "this has already been provided by kernel 4 .",
    "the necessary data to compute interaction are fetched to the shared memory by the first thread of a warp .",
    "this allows to reduce number of memory accesses .",
    "finally , kernel 6 updates the state of the particles by using the position increments and re - orient particle magnetic moments .",
    "the above - described algorithm has many advantages .",
    "among them are minimal thread divergence and the complete absence of gpu / cpu data transfer ( aside of the transfer of the final results ) , optimal use of global memory with minimal number of accesses , data field re - use , minimal number of locks etc .",
    "all this allows to achieve a tangible speed - up . for more detailed description",
    "we direct the interested reader to ref .",
    "we performed simulations on ( i ) a pc with intel xeon x5670 @2.93ghz cpu(48 gb ram ) and ( ii ) a tesla m2050 gpu .",
    "though the cpu has six cores , only a single core was used in simulations .",
    "the programs were compiled with ` nvcc ` ( version 4.0 ) and ` gcc ` ( version 4.4.1 ) compilers . since there was no need in high - precision calculations , we used single - precision variables ( ` float ` ) and compiled the program with ` -use_fast_math ` key .",
    "we also used ` -o3 ` optimization flag to speedup our programs .",
    "finally , the euler - maruyama method with time step @xmath94 was used to integrate eqs .",
    "( [ eq : red_mot_rot_tet ] - [ eq : red_mot_disp_final ] ) .",
    "we measured the computation time of one integration step for both algorithms as functions of @xmath1 .",
    "the results are presented with table [ tbl : comptimetable ] .",
    "the benefits of the gpu computing increase with the number of particles . for an ensemble of @xmath95 particles the speed - up gained from the use of the barnes - hut algorithm is almost @xmath96 compared to the performance of the performance of the optimized all - pairs algorithm on the same gpu .",
    "however , for @xmath97 the all - pairs algorithm performs better .",
    "it is because the computational expenses for the tree - building phase , sorting etc . , overweight the speed - up effect of the approximation for small number of particles .",
    "here we remind that @xmath97 was the typical scale of the most of ferrofluid simulations to date @xcite .",
    ".duration of single integration step ( ms ) for the optimized all - pairs algorithm implemented on cpu ( @xmath98 ) and gpu ( @xmath99 ) , and for the cpu- ( @xmath100 ) and gpu - oriented ( @xmath101 ) barnes - hut algorithm . [ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]     fig .",
    "[ fig : fflsim ] shows instantaneous configurations obtained during the simulations for a cubic confinement and for a mono - layer . to simulate a mono - layer of particles we use a rectangular parallelepiped of the height @xmath102 as a confinement .",
    "the parameters of the simulations correspond to the regime when the average dipole energy is much larger than the energy of thermal fluctuations .",
    "the formation of chain - like large - scale clusters @xcite is clearly visible .",
    "in order to check the accuracy of the numerical schemes we calculated the reduced magnetization curve @xcite .",
    "reduced magnetization vector is given by the sum @xmath103 .",
    "the main parameters that characterize ferrofluid magnetic properties are the dipole coupling constant @xmath104 , which is the ratio of dipole - dipole potential and thermal energy , namely @xcite , @xmath105 and the volume fraction , which is the ratio between the volume occupied by particles and the total volume occupied by the ferrofluid , @xmath106 , i.e. , @xmath107 in the limit when @xmath108 and for small volume fraction , @xmath109 , the projection of the reduced magnetization vector on the direction of applied field , @xmath110 , can well be approximated by the langevin function @xcite : @xmath111 where @xmath112 denotes the ratio between magnetic energy and thermal energy , @xmath113    -particle ensembles obtained with the barnes - hut algorithm : ( a ) @xmath114 ( cubic confinement with the edge length @xmath115 ) and ( b ) mono - layer of @xmath116 particles .",
    "the parameters are : @xmath117 , @xmath118 k , @xmath119 a / m , @xmath120 @xmath121 , @xmath122 nm . ]    we simulated a system with the parameters corresponding to maghemite ( @xmath123 ) with a saturation magnetization of @xmath124 a / m and density @xmath120 @xmath121 , the carrier viscosity @xmath125 pa ( the latter corresponds to the water viscosity at @xmath126 k ) . the volume fraction is set at @xmath127 and the particle radius @xmath128 nm . the external magnetic field @xmath129 was applied along @xmath130axis .",
    "we initiated the system at time @xmath131 by randomly distributing particles in a cubic container .",
    "the orientation angles of particle magnetization vectors were obtained by drawing random values from the interval @xmath132 $ ] .",
    "[ fig : langevin ] presents the results of the simulations .",
    "after the transient @xmath133 , given to the system of @xmath134 particles to equilibrate , the mean reduced magnetization has been calculated by averaging @xmath135 over the time interval @xmath136 .",
    "it is noteworthy that even single - run results are very close to the langevin function , see fig .",
    "[ fig : langevin](a ) .",
    "the contribution of the magnetostatic energy grows with @xmath112 so that the strength of the dipole - dipole interaction is also increasing , see fig .",
    "[ fig : hdip ] .",
    "since the average value of dipole field projection on @xmath66 axis is positive and increases with @xmath112 , the dipole field amplifies the external magnetic field .",
    "this explains the discrepancy between the analytical and numerical results obtained for large values of @xmath112 . for an ensemble of @xmath97 particles the results obtained with two algorithms are near identical , fig .",
    "[ fig : langevin ] ( b ) .     particles and langevin function , eq .",
    "( [ eq : langevin ] ) ; b ) comparison of the results obtained with the all - pair and the barnes - hut algorithms for an ensemble of @xmath97 particles . ]",
    "it is important also to compare the average dipole fields , @xmath137 , calculated with the barnes - hut and the all - pairs algorithms .",
    "the outputs obtained for the above - given set of parameters are shown on fig.[fig : hdip ] .",
    "again , two algorithms produced almost identical results .",
    "-component of the reduced average dipole field as a function of @xmath112 @xcite .",
    "the parameters are the same as in fig .",
    "[ fig : langevin](b ) .",
    "each point was obtained by averaging over @xmath138 independent realizations . ]",
    "with this work we demonstrate that the barnes - hut algorithm can be efficiently implemented for large - scale , gpu - based ferrofluid simulations .",
    "overall , we achieved a speed - up more than two orders of magnitude when compared to the performance of a common gpu - oriented all - pairs algorithm .",
    "the proposed approach allows to increase the size of ensembles by two orders of magnitude compared to the present - day scale of simulations @xcite .",
    "the barnes - hut algorithm correctly accounts for the dipole - dipole interaction within an ensemble of @xmath95 particles and produces results that fit the theoretical predictions with high accuracy .",
    "our finding opens several interesting perspectives .",
    "first , it brings about possibilities to perform large - scale molecular - dynamics simulations for the time evolution of ferrofluids placed in confinements of complex shapes like thin vessels or tangled pipes , where the boundary effects play an important role @xcite .",
    "it is also possible to explore the _ non - equilibrium _ dynamics of ferrofluids , for example , their response to different types of externally applied magnetic fields , such as periodically alternating fields @xcite , or gradient fields @xcite .",
    "another direction for further studies is the exploration of the relationship between shape and topology of nano - clusters and different macroscopic properties of ferrofluids @xcite .",
    "finally , the gpu - based computational algorithms provide with new possibilities to study heat transport processes in ferrofluids @xcite , for example to investigate the performance of ferromagnetic particles as heat sources for magnetic fluid hyperthermia @xcite .",
    "a.yu.p . and t.v.l .",
    "acknowledge the support of the cabinet of ministers of ukraine obtained within the program of studying and training for students , phd students and professor s stuff abroad and the support of the ministry of education , science , youth , and sport of ukraine ( project no 0112u001383 ) .",
    "s.d . and p.h .",
    "acknowledge the support by the cluster of excellence nanosystems initiative munich ( nim ) .",
    "44 natexlab#1#1[1]`#1 ` [ 2]#2 [ 1]#1 [ 1]http://dx.doi.org/#1 [ ] [ 1]pmid:#1 [ ] [ 2]#2 , , , . , , , , , ( ) .",
    ", , , ( ) . , , ( ) .",
    ", , , ( ) . , , , , ( ) .",
    ", , , , in : , volume , , p.  .",
    ", , , , , ( ) . , , , , ( ) .",
    ", , , et  al . , , ( ) .",
    ", , , , , , , , ( ) . , , , , ( ) .",
    ", , , , ( ) . , , , et  al . , , ( ) .",
    ", , , , , , , ( ) . , , , , , , , , , , , , , , , , , , ( ) . , , , , ( ) . , , , , ( ) .",
    ", , in : , volume  , , p. .",
    ", , , in : , .",
    ", , in : , , .",
    ", , , ( ) . , , , ( ) .",
    ", , , , ( ) .",
    ", , , ( ) . , , ( ) .",
    ", , , ( ) . . , , , . , , , , ( ) . , , , in : , volume  , , pp . . .",
    ", , , , ( ) . , , , , ( ) .",
    ", , , ( ) . , , , , , ( ) . , , , ( ) .",
    ", , , , , , ( ) . , , , , ( ) ."
  ],
  "abstract_text": [
    "<S> we present an approach to molecular - dynamics simulations of ferrofluids on graphics processing units ( gpus ) . </S>",
    "<S> our numerical scheme is based on a gpu - oriented modification of the barnes - hut ( bh ) algorithm designed to increase the parallelism of computations . for an ensemble consisting of one million of ferromagnetic particles , </S>",
    "<S> the performance of the proposed algorithm on a tesla m2050 gpu demonstrated a computational - time speed - up of four order of magnitude compared to the performance of the sequential all - pairs ( ap ) algorithm on a single - core cpu , and two order of magnitude compared to the performance of the optimized ap algorithm on the gpu . </S>",
    "<S> the accuracy of the scheme is corroborated by comparing the results of numerical simulations with theoretical predictions .    </S>",
    "<S> ferrofluid , molecular dynamics simulation , the barnes - hut algorithm , gpus , cuda </S>"
  ]
}