{
  "article_text": [
    "undoubtedly , entropy is one of the most important quantity in physics  @xcite .",
    "it connects the thermodynamic behavior of a macroscopic system with the configurations of microscopic states  @xcite , giving rise to modern statistical physics .",
    "in particular , entropy , incorporated with information after the seminal work on information theory  @xcite , plays a central role in physics of complex systems  @xcite .",
    "for instance , information exchange dynamics was proposed as the underlying mechanism of self - organized criticality  @xcite ; the maximum entropy model was proposed to understand the physics of biological systems such as species abundance  @xcite and the collective behavior in neural networks  @xcite .",
    "notwithstanding the fundamental and practical importance as mentioned above , some properties of entropy still remain somewhat controversial .",
    "specifically , the second law of thermodynamics , which states the nondecreasing time evolution of entropy of an isolated system , is still an actively studied topic  @xcite . according to the fluctuation theorem in particular  @xcite ,",
    "the nondecreasing property of entropy is feasible only on the macroscopic scale while a decrease of entropy may indeed be observed in a small system .    here",
    "it should be noted that entropy may not be singly defined across the coarse - graining level or the scale of description . as a representative example , one may consider a system consisting of many elements , and define the entropy from the probability for the system to be in given configuration , i.e. , for each element to be in given state , or from the state distribution of all the elements in the system .",
    "henceforth , for convenience , we call the former entropy ( defined by the probability of the system configuration ) ` fine - grained entropy ' and the latter one ( by the state distribution function of elements ) ` coarse - grained entropy ' .",
    "the general master equation governing the time evolution of the probability allows one to probe the time evolution of the entropy as well . to clarify the difference between the two entropies",
    ", we analyze the simple growth model , with no production or with uniform size production  @xcite .",
    "the state of each element is specified by its ` size ' , which can in general take continuous values .",
    "nevertheless the description based on discrete values of the size can also be adopted ; both the continuous and the discrete descriptions are examined .",
    "this growth model bears skew distributions , as manifested by the time evolution obtained from the master equation , and thus provide a good framework to probe the issues mentioned above .",
    "naively , one may expect that the coarse - grained entropy is equivalent to the fine - grained entropy if the elements are independent of each other .",
    "in such a case , the whole system can be decomposed fully into single elements and the coarse - graining procedure should not introduce information loss .",
    "examining the system with uniform size production , however , we find that the independence between elements is not sufficient : heterogeneity of elements can serve as an additional source of the information loss in the coarse - graining procedure .",
    "further , resolution of the description is also proved to play a significant role .",
    "the expansion rate of the size space domain in the continuous description is qualitatively different from that in the discrete description and as a consequence , a term describing such a growing space domain is introduced in the continuous description of the system .    meanwhile , the evolution equations for the probability density functions of both systems are analytically tractable as shown in ref .",
    "@xcite , at least after a sufficiently long time .",
    "we may thus use the known analytic expression of entropy for those cases . on the other hand ,",
    "the time evolution at earlier stages can in general be obtained only via numerical methods . moreover",
    ", a system with more complex growth mechanism resists analytical treatment , compelling one to resort to numerical calculation .",
    "therefore , we perform extensive numerical simulations as well .",
    "this paper consists of five sections : in sec .",
    "ii , we formulate the entropy dynamics of the system governed by a general master equation .",
    "section iii describes the time evolution of entropy applied to the growth model , while numerical results are presented in sec .",
    "finally , a brief summary is given in sec .",
    "we consider a system of @xmath0 elements , the @xmath1th of which is characterized by its size @xmath2 ( @xmath3 ) .",
    "the configuration of the system is specified by the sizes of all elements , @xmath4 or shortly by @xmath5 .",
    "if @xmath2 is a continuous variable , the probability density @xmath6 for the system to be in configuration @xmath5 at time @xmath7 is governed by the master equation    @xmath8 , \\ ] ]    where @xmath9 is the transition rate for the @xmath1th element to change its size from @xmath2 to @xmath10 .",
    "we are also interested in the size distribution @xmath11 , related to the probability density @xmath6 via @xmath12 where @xmath13 .    using the probability density and the size distribution",
    ", one can define entropy in two ways and probe the time evolution of the two : the fine - grained entropy and the coarse - grained entropy , the relation between which is of interest here .",
    "first , we define the fine - grained entropy to be @xmath14 the time evolution of which is obtained from eq .",
    ":    @xmath15\\frac{d}{d t}p(\\{x_i\\};t ) { \\nonumber}\\\\ & = -\\int d^n x \\left [ 1+\\ln{p(\\{x_i\\};t ) } \\right ] \\sum_{i=1}^n \\int dx_i^\\prime   \\left [ \\omega ( x_i^\\prime \\to x_i)p(x_1 , \\ldots , x_i^\\prime , \\ldots , x_n;t)-\\omega ( x_i \\to x_i^\\prime)p(\\{x_i\\};t ) \\right ] { \\nonumber}\\\\ & = -\\int d^n x \\sum_{i=1}^n \\int dx_i^\\prime \\left [ \\omega ( x_i \\to x_i^\\prime ) p(\\{x_i\\};t ) \\ln{p(x_1 , \\ldots , x_i^\\prime , \\ldots , x_n;t ) } -\\omega ( x_i \\to x_i^\\prime ) p(\\{x_i\\};t ) \\ln{p(\\{x_i\\};t ) } \\right ] { \\nonumber}\\\\ & = -\\left\\langle \\sum_{i=1}^n \\int dx_i^\\prime \\omega ( x_i \\to x_i^\\prime ) \\ln{\\frac{p(x_1 , \\ldots , x_i^\\prime , \\ldots , x_n;t)}{p(\\{x_i\\};t ) } } \\right\\rangle\\end{aligned}\\ ] ]    with @xmath16 .",
    "this equation is generally applicable to the differential entropy of the system governed by the master equation .",
    "it is well known that the differential entropy of the system described by continuous variables suffers from the divergence of the information capacity .",
    "here we deal with the time evolution of the ( differential ) entropy , where such divergence cancels out .",
    "accordingly , there does not occur the problem of divergence .",
    "we next define the coarse - grained ( differential ) entropy according to @xmath17 which yields @xmath18.\\ ] ] equation   allows one to rewrite @xmath19 in the form : @xmath20 which is just the lyapunov exponent  @xcite of the mapping @xmath21 with @xmath22 defined by @xmath23 .",
    "it is thus manifested that the differential entropy is the lyapunov exponent or the dynamic entropy with the index @xmath24 counting the time step .",
    "note here that the probability density function quantifies the relation between adjacent stochastic variables @xmath25 and @xmath26 .",
    "if the correlations between them are small , the actual trajectory of the series @xmath5 should be rather unstable , characterized by sensitivity to the initial condition @xmath27 and accordingly by a large value of the lyapunov exponent .",
    "indeed , entropy is a representative measure for regularity of the system , and it is natural to interpret the differential entropy as the dynamic entropy of the stochastic process defined through the probability density function .    we can also extend the analysis to the system whose number of elements varies in time . in this case , the entropy evolves in time as follows :    @xmath28   -\\int d^n x\\ , p(\\{x_i\\};t ) \\ln{p(\\{x_i\\};t ) } \\right ] { \\nonumber}\\\\ & = \\frac{d s_p^{(0)}}{dt }      + \\frac{dn}{dt}\\frac{1}{\\delta n}\\int d^{n{+}\\delta n}x\\ , p(x_{1 } , \\ldots , x_{n{+}\\delta n } ; t+\\delta t ) \\ln{p(x_{n{+}1},\\ldots , x_{n{+}\\delta n } ; t{+}\\delta t| \\{x_i\\ } ; t ) } { \\nonumber}\\\\ & \\equiv \\frac{d s_p^{(0)}}{dt } + \\frac{dn}{dt}s^{(1)},\\end{aligned}\\ ] ]    where the first term in the last line represents the time evolution of the number conserving part @xmath29 and the second term corresponds to the production of conditional entropy @xmath30 ( per element ) associated with the birth of new elements .",
    "it is straightforward to apply this formulation to a system with discrete size variables : replacing the integration @xmath31 and the delta function @xmath32 by the summation @xmath33 and the kronecker delta @xmath34 , respectively , one can easily obtain the evolution equation for the information entropy ( instead of the differential entropy ) in a similar form .",
    "in addition , we here point out that @xmath35 and @xmath19 are in general not equivalent . in the system of elements coupled with each other ,",
    "the entropy of the system is not extensive and to replace the system configuration probability @xmath6 by the coarse - grained state distribution function @xmath11 would cause information loss arising from the ignorance of correlations between elements .",
    "in the case that the elements of a system are independent of each other , we have @xmath36 where @xmath37 is the probability for the size of the @xmath1th element to be @xmath38 .",
    "the fine - grained entropy is then given by the sum @xmath39 where extensiveness is obvious .",
    "however , such independence between elements does not guarantee the equivalence between the fine - grained entropy and the coarse - grained one . since eq .",
    "reduces to @xmath40 the coarse - grained entropy reads @xmath41}.\\ ] ] comparison of eqs .",
    "( [ sp ] ) and ( [ eq : s neq sf ] ) indeed shows that @xmath35 and @xmath19 are not necessarily equivalent : in fact the cauchy - schwarz inequality indicates that @xmath42 . here",
    "the equality requires additional assumption that every element has the same probability for the size : @xmath43 .",
    "this gives @xmath44 and the equality @xmath45 .",
    "accordingly , in the case of a heterogeneous system , the coarse - grained entropy is larger than the fine - grained entropy ; this reflects the loss of information in the coarse - graining procedure , arising from the disregard of details of the element sizes . equipped with these observations , we now consider the growth model and probe the time evolution of entropy in various cases .      for convenience , we begin with a brief summary of the growth model developed and analyzed in refs .  @xcite .",
    "first , we consider the system whose number of elements is fixed . in this case of a number conserving system without production , the only process involved is the size change ( growth ) by the amount proportional to the current size and the transition rate takes the form @xmath46\\ ] ] with the ( mean ) growth rate @xmath47 and the growth factor @xmath48 .",
    "making use of eq .",
    "( [ eq : master equation ] ) , we obtain the evolution equation for the size distribution @xmath11 : @xmath49 it is known that the log - normal distribution of the form @xmath50\\ ] ] provides an asymptotic solution of eq .",
    "( [ eq : number conserved ] ) .",
    "specifically , under the initial condition @xmath51 , we have the mean @xmath52 and the deviation @xmath53 @xcite .    we also probe the system in the discrete description , where the transition rate reads @xmath54 this in turn leads to the evolution equation in a slightly modified form @xmath55 when the initial size of every element is given by unity , the size at a later time can take only the discrete value @xmath56 for some integer @xmath57 .",
    "we thus write simply @xmath58 , which evolves in time according to @xmath59 it is easy to obtain the solution of eq .",
    "( [ eq : p_k ] ) : @xmath60 which is the poisson distribution  @xcite .",
    "note that the normalization condition is now given by @xmath61    in both continuous and discrete descriptions , elements grow independently of each other and the probability is the same for every element , leading to the relation @xmath62^n$ ] . in consequence , the time evolution is simplified to take the form @xmath63 where , along with @xmath64 , the only difference is the factor @xmath0 representing the extensive property .",
    "henceforth , one can safely probe the time evolution using @xmath11 instead of @xmath65 .",
    "this approach is not applicable to the system in which couplings between elements may not be neglected .",
    "note also that the above relations are valid for the discrete description as well , with the integration in the averaging procedure replaced by the summation .",
    "however , the entropy in the continuous description and that in the discrete one could be different , as they are governed by different time evolution equations , eqs .   and .",
    "we now use the solution of the time evolution equation to pursue specifically the time evolution of the entropy . inserting the log - normal distribution to eq .",
    ", we obtain the asymptotic behavior of the entropy in the form @xmath66 one can also compute the entropy directly from eq .  , and obtain the consistent result @xmath67 where @xmath68 is a constant .    in the discrete description ,",
    "entropy can be computed from the poisson distribution , similarly to the continuous one .",
    "the entropy for the poisson distribution is well known and behaves asymptotically as  @xcite @xmath69 where @xmath70 is a constant depending on the growth rate @xmath47 .",
    "note that the asymptotic behavior is free of the growth factor @xmath48 as expected .",
    "note also that the main difference between the continuous description and the discrete description is given by the term @xmath71 , which is a direct consequence of the @xmath72 factor in the log - normal distribution  @xcite . in deriving the log - normal distribution from the gaussian distribution , the factor @xmath72",
    "is brought by the change of the measure @xmath73 in the logarithmic transformation @xmath74 .",
    "therefore , we conclude that the term has its origin solely in the growing domain of the size space in the continuous description . we will return to this issue in sec .",
    "iv [ see eq .  ] .",
    "next , we consider the case that the total number of elements varies with time , i.e. , @xmath75 , and each element tends to produce a new one with rate @xmath76 ( thus the total number of elements increases in proportion to the current number : @xmath77 ) .",
    "the time evolution equation for the size distribution obtains the form : @xmath78 where @xmath79 is the size distribution function of newly produced elements . in this work",
    ", we deal with the case that new elements are produced in uniform size @xmath80 , i.e. , @xmath81 .",
    "the stationary distribution is then given by a power - law function for @xmath82 @xcite : @xmath83 with the exponent @xmath84 in the discrete description , the evolution equation for @xmath85 under uniform size production , corresponding to eq .",
    "( [ eq : f evolution 2 ] ) , reads @xmath86 of which the exact stationary solution is given by @xmath87    when new elements of uniform size are produced , the entropy can still be decomposed into the entropy component of each element . on the other hand ,",
    "the relation @xmath88 is not satisfied because the entropy of an element produced at time @xmath89 and that at @xmath90 are obviously different from each other .",
    "it is therefore expected that the fine - grained entropy @xmath91 and the coarse - grained entropy @xmath92 are not equivalent in this case ; this will be confirmed by computing the stationary entropy values specifically .",
    "further , to circumvent the extensiveness of the fine - grained entropy growing with the number of elements , we focus on the entropy per element @xmath93 rather than @xmath35 . of course , this is not the case for the coarse - grained entropy @xmath19 .",
    "we first probe the stationary value of @xmath94 which should be computed directly from eq .  .",
    "the time evolution is governed by @xmath95 where @xmath96 is the entropy per element in the number conserving system with the asymptotic behavior given by eq .  .",
    "the additional term @xmath97 originates from the last term in eq .  .",
    "note that in this model system , @xmath98 and accordingly , @xmath30 is simply the entropy per new element .",
    "if we further assume @xmath99 , the time evolution of entropy per element is described by : @xmath100 therefore , if we know the single - element entropy in the number conserving system , we can precisely compute the entropy in the uniform production case .",
    "fortunately , we have @xmath101 and also obtained the time evolution of @xmath102 in eq .  .",
    "neglecting the first term on the right hand side of eq .  , we approximate the stationary value of the entropy as follows : @xmath103_{t'=0}^{t'=t } { \\nonumber}\\\\ & = \\frac{\\lambda}{r}\\ln{(1{+}b)}-\\frac{1}{2}\\ln{r } -\\frac{1}{2}\\ln{\\lambda } + b_1 , \\label{eq : fine h stationary}\\end{aligned}\\ ] ] where ei is the exponential integral  @xcite and @xmath104 is a constant .",
    "similarly , in the discrete description , the stationary value obtains @xmath105 even though the constant shift @xmath106 remains unclarified , the dependency of the entropy on the model parameters @xmath47 , @xmath48 , and @xmath76 is fully specified .",
    "we then turn to the coarse - grained entropy @xmath19 . in the continuous description",
    ", we can compute the stationary value from the exact form of @xmath11 [ see eq .  ] . performing the integration",
    ", we thus obtain the coarse - grained entropy @xmath107 in the discrete description , on the other hand , we can exactly compute the stationary value of entropy from eq .  , to obtain the form @xmath108}.\\end{aligned}\\ ] ] denoting @xmath109 , we obtain @xmath110    we now ponder on the mechanism for the emergence of the stationary power - law distribution .",
    "if there is no production , the system evolves to the disordered state as the entropy increases indefinitely . in the presence of uniform size production , on the other hand ,",
    "the state of newly produced elements is fully ordered in the sense that the additional entropy contributions from the new elements vanish .",
    "as a result of appropriate mixing of these two components , there emerges a stationary state whose asymptotic entropy is finite .",
    "for this stationary state , we have confirmed that both @xmath94 and @xmath19 are finite .    finally , from eqs .   and   [ or from eqs .   and  ]",
    ", it is evident that @xmath111 even with the constant shift disregarded . as we could not specify the initial value of the entropy , it is still unclear whether the coarse - grained entropy is larger than the fine - grained one due to the information loss in the coarse - graining procedure .",
    "however , the dependency on the model parameters is clearly distinguished and we conclude that the coarse - grained entropy could differ from the fine - grained one even in the case of a non - interacting system .",
    "the issue associated with the information loss will be clarified by the numerical results in the next section .",
    "let us first describe briefly the algorithm to compute the time evolution of ( differential ) entropy .",
    "the procedure begins with the numerical integration of the evolution equation to obtain the distribution function as a function of time ( and @xmath38 ) . in the numerical integration",
    ", we use the fourth - order runge - kutta method for time integration with the time step @xmath112 , while dividing the positive @xmath38 space into segments of equal or variable length(s ) .",
    "finally , we calculate the differential entropy in the continuous description at each time , which is defined to be @xmath113 for numerical integration , we use the simplest approximation for eq .",
    "( [ eq : differ ] ) : @xmath114 in the discrete description , the entropy is computed directly from the definition of the information entropy : @xmath115     and @xmath116 are used .",
    "( a ) discrete size distribution @xmath117 versus @xmath57 and ( b ) ( continuous ) size distribution @xmath11 versus @xmath38 at time @xmath118 ( red squares ) and @xmath119 ( blue circles ) . solid lines in ( b ) depict the log - normal distributions with @xmath120 and @xmath121 , respectively .",
    "( c ) coarse - grained entropy @xmath122 versus time @xmath7 .",
    "red squares describe the entropy calculated from a narrow uniform initial distribution and blue triangles that from a gaussian initial distribution .",
    "green circles represent the entropy for the discrete size distribution .",
    "black solid and dashed lines present the analytical results given by eqs .",
    "( for the continuous description ) and   ( for the discrete description ) , respectively . , title=\"fig:\",width=302 ]   and @xmath116 are used .",
    "( a ) discrete size distribution @xmath117 versus @xmath57 and ( b ) ( continuous ) size distribution @xmath11 versus @xmath38 at time @xmath118 ( red squares ) and @xmath119 ( blue circles ) .",
    "solid lines in ( b ) depict the log - normal distributions with @xmath120 and @xmath121 , respectively .",
    "( c ) coarse - grained entropy @xmath122 versus time @xmath7 .",
    "red squares describe the entropy calculated from a narrow uniform initial distribution and blue triangles that from a gaussian initial distribution .",
    "green circles represent the entropy for the discrete size distribution .",
    "black solid and dashed lines present the analytical results given by eqs .",
    "( for the continuous description ) and   ( for the discrete description ) , respectively . , title=\"fig:\",width=302 ]   and @xmath116 are used .",
    "( a ) discrete size distribution @xmath117 versus @xmath57 and ( b ) ( continuous ) size distribution @xmath11 versus @xmath38 at time @xmath118 ( red squares ) and @xmath119 ( blue circles ) . solid lines in ( b ) depict the log - normal distributions with @xmath120 and @xmath121 , respectively .",
    "( c ) coarse - grained entropy @xmath122 versus time @xmath7 .",
    "red squares describe the entropy calculated from a narrow uniform initial distribution and blue triangles that from a gaussian initial distribution .",
    "green circles represent the entropy for the discrete size distribution .",
    "black solid and dashed lines present the analytical results given by eqs .",
    "( for the continuous description ) and   ( for the discrete description ) , respectively .",
    ", title=\"fig:\",width=302 ]    figure  [ fig : no production ] presents the results for the number conserving system . in fig .",
    "[ fig : no production](a ) we display the discrete size distribution @xmath85 at time @xmath118 and @xmath119 in a system with @xmath123 and @xmath116 .",
    "as time goes by , the poisson distribution in eq .",
    "( [ eq : poisson ] ) approaches the normal distribution peaked at @xmath124 with the standard deviation @xmath125 .",
    "the data in fig .",
    "[ fig : no production](a ) indeed fit well with these values of the peak position and the standard deviation ( results not shown in the figure ) .",
    "figure  [ fig : no production](b ) shows the size distribution @xmath11 at time @xmath118 and @xmath119 for the system with the same model parameters @xmath47 and @xmath48 . the data in fig .",
    "[ fig : no production](b ) may be obtained from those of @xmath85 via the relation @xmath126 with @xmath127 and @xmath128 $ ] . as addressed already",
    ", @xmath11 reduces to the log - normal distribution in the long - time limit . indeed ,",
    "starting from the normal distribution @xmath129\\ ] ] and putting @xmath130 with @xmath57 regarded as a continuous variable , one can also obtain eq .",
    "( [ eq : log - normal distribution ] ) with @xmath131 and @xmath132 . fitting the data in fig .",
    "[ fig : no production](b ) to eq .",
    "( [ eq : log - normal distribution ] ) , one finds excellent agreement with the theoretical values of @xmath133 and @xmath134 .",
    "figure  [ fig : no production](c ) shows the entropy , growing in time , for the same system .",
    "red squares represent the entropy obtained from a narrow uniform initial distribution ( labeled as # 1 ) via the simplified way of space integration described in sec .",
    "ii and blue triangles that from a gaussian initial distribution ( labeled as # 2 ) .",
    "green circles present the entropy for the discrete size distribution .",
    "also shown are black solid and dashed lines representing the analytical results given by eqs .",
    "( for the continuous description ) and   ( for the discrete description ) , respectively .",
    "agreement between analytical solutions and numerical results is manifested .",
    "note that the difference @xmath122 is displayed and the two data sets # 1 and # 2 fall in almost with each other eventually , except for the more rapid increase of the data set # 1 reflecting the lower entropy for the uniform distribution .",
    "if we compare the entropy of the continuous system designated by # 1 and that of the discrete system , the initial increases are similar but the latter grows more slowly .",
    "in particular , the increase becomes almost linear at large time @xmath7 and the slope computed analytically agree well with the numerical values , explaining the more rapid increase of @xmath92 ( and @xmath135 as well ) in fig .",
    "[ fig : no production](c ) .",
    "this rapid increase in the continuous description is attributed to the use of the domains of the real space growing exponentially in time , as confirmed easily by computing directly the difference . from eqs .   and , one obtains @xmath136 neglecting the constant term in the asymptotic limit ( @xmath137 ) , the difference is exactly given by the second term originating from the extension of the size space .",
    "this term turns out to be @xmath71 , which confirms the analytical results given in eqs .   and  .",
    "apart from the constant shift , the numerical results are shown to fit well with the analytical results . in general",
    "when the initial size distribution is very sharp , the size growing in time tends to take discrete values and rather a discrete size distribution is maintained in finite time , making the results for the discrete description applicable . on the other hand ,",
    "if the initial distribution is somewhat broad , diversity of size is generated by the growth process and the continuous size distribution should be relevant . another point to mention is that the success of the simplified method of integration for the differential entropy is related to the measurement scale on the element size .",
    "the logarithmic scale is thus more appropriate for the size in this growth problem .",
    "in addition , we have noticed two kinds of entropy for the systems studied . in the uniform production case ,",
    "we compute the fine - grained entropy in addition to the coarse - grained entropy , to check whether or not the two are equivalent . in this case , we trace the birth of a new element together with the time of birth . from the age of each element at given time , we compute the contribution of each element to entropy . at the end",
    ", we sum the contributions over all elements and obtain the total entropy , making use of the extensiveness of entropy , and present the results of both fine - grained and coarse - grained entropies in figs .",
    "[ fig : uniform ] and [ fig : stationary ] .            figure  [ fig : uniform](a ) shows the size distributions @xmath117 and @xmath11 for the system with parameters @xmath138 at time @xmath139 .",
    "red triangles present the data for @xmath85 in the semi - log scale while ( black ) solid line plots those for @xmath140 in the log - log scale . both plots",
    "are observed linear for not too large values of @xmath38 and @xmath57 . here",
    "the linear region tends to expand with the lapse of time , and a stationary state is reached finally . in fig .",
    "[ fig : uniform](b ) , we display how coarse - grained entropy as well as the fine - grained entropy evolves in time in the case of uniform - size production for the system of fig .",
    "[ fig : uniform](a ) . it is shown that the entropy becomes nearly saturated after the power - law distribution is established and accordingly the stationary state is reached .",
    "it is evident that the coarse - grained entropy is larger that the fine - grained entropy , as expected from eq .  .",
    "further , the entropy in the discrete description is smaller than that in the continuous description .    finally , the stationary values of @xmath94 and @xmath19 for various values of @xmath47 and @xmath76 are shown in fig .",
    "[ fig : stationary](a ) and ( b ) , respectively ( see the legend for the details ) .",
    "it is shown that the results fit very well with the analytical results .",
    "in particular , we observe that @xmath141 and it is confirmed that the coarse - grained entropy is larger than the fine - grained entropy .",
    "we thus conclude that the heterogeneity in addition to the correlations between elements can induce loss of information in the coarse - grained procedure .",
    "we have studied the entropy of a system of elements evolving according to the master equation . specifically , we consider the growth model in the fine - grained description , where the probability of the system configuration is governed by the master equation , and in the coarse - grained description , which deals with the evolution equation for the distribution function . the system which accommodates production of new elements as well as the number conserving system without production",
    "have been probed in detail .",
    "further , the difference between the two cases of the size variable , continuous and discrete ( size ) descriptions has also been examined . what has been revealed and its implications are summarized in the following :",
    "first , we have found that the growth rate of the size domain also provides an important factor for the time evolution of the entropy .",
    "such growth of the domain is closely related to the resolution of description of the system .",
    "indeed , in the discrete description , the domain is determined as the sum of the possible locations of elements in the size variable space , while the domain should span the whole space in the case of the continuous description . in some systems such as the classical random walk model ,",
    "the volumes of the domains in the continuous and discrete descriptions are proportional to each other . in the case of the growth model studied in this paper , however ,",
    "the resolution is directly connected with the size scale of the system via @xmath142 .",
    "therefore , the domain increases faster as the size scale of the system grows larger , leading to the information loss due to the expansion of the domain space to be probed in the continuous description .",
    "second , examining the uniform production case of the growth model , we have confirmed that the heterogeneity in addition to the correlations among elements can induce loss of information or increase of entropy in the coarse - graining procedure . in this case , the entropy is still extensive but the coarse - graining procedure blurs out the disparity between elements ( e.g. , ages of produced elements ) and as a consequence , causes the loss of information .",
    "the coarse - graining process , employed widely in the study of complex systems , may therefore yield a biased result unless heterogeneity is taken into account duly in the analysis . to quantify the amount of information loss accompanying the coarse - graining procedure should be very helpful for understanding the scale - dependent properties of complex systems .",
    "this is left for further study .",
    "this work was supported in part by the 2015 research fund of the university of ulsan ."
  ],
  "abstract_text": [
    "<S> entropy plays a key role in statistical physics of complex systems , which in general exhibit diverse aspects of emergence on different scales . </S>",
    "<S> however , it still remains not fully resolved how entropy varies with the coarse - graining level and the description scale . in this paper , we consider a yule - type growth model , where each element is characterized by its size being either continuous or discrete . </S>",
    "<S> entropy is then defined directly from the probability distribution of the states of all elements as well as from the size distribution of the system . </S>",
    "<S> probing in detail their relations and time evolutions , we find that heterogeneity in addition to correlations between elements could induce loss of information during the coarse - graining procedure . </S>",
    "<S> it is also revealed that the expansion of the size space domain depends on the description level , leading to a difference between the continuous description and the discrete one . </S>"
  ]
}