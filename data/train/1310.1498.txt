{
  "article_text": [
    "tagging is a popular methodology for many user - driven document organisation applications such as social bookmarking and publication sharing websites . on websites such as citeulike , delicious and bibsonomy",
    ", tags provide an unstructured organization method where each user has the liberty of choosing or making up any string of characters to be used as a tag for a document .",
    "the automatic generation of tag recommendations aids the social tagging process by reducing the effort required by the user , and making them aware of which existing tags are relevant to the document they are tagging .",
    "tag recommenders encourage the use of the system and lead to a more homogeneous document organisation overall .",
    "the task of tag recommendation is to automatically suggest a set of tags to a user for a document that he is in the process of tagging .",
    "the data contained in social tagging systems is often described as a folksonomy @xcite .",
    "a folksonomy is a tuple @xmath0 @xmath1 where @xmath2 is the set of users , @xmath3 is the set of documents , @xmath4 is the set of tags and @xmath5 is the set of tag assignments .",
    "a tag assignment @xmath6 is a triplet @xmath7 and indicates that user @xmath8 has assigned tag @xmath9 to document @xmath10 .",
    "thus a folksonomy can be modelled as a hyper - graph with the adjacency tensor given by a 3-dimensional binary matrix @xmath11_{|u| \\times |d| \\times |t|}$ ] where each entry @xmath12 specifies whether or not user @xmath13 tagged document @xmath14 with tag @xmath15 .",
    "a post in the tagging data consists of a set of tags assigned by a user to a document .",
    "posts themselves are only captured implicitly in the folksonomy model .",
    "the set of posts can be described as @xmath16 where each post @xmath17 is a triplet @xmath18 consisting of a user @xmath19 , a document @xmath20 , and a set of tags @xmath21 .",
    "we use the notation @xmath22 for query posts , where @xmath23 is the query user , @xmath24 is the query document and the set of tags is unknown and to be predicted , denoted by the empty set @xmath25 .",
    "tag recommendation algorithms are often evaluated on post - core datasets which are subsets of the full tagging data . in a post - core at level @xmath26 ,",
    "only posts are included where the user , document and all tags appear in at least @xmath26 posts .",
    "this provides for a denser dataset and also minimises the chance that new items ( users , documents or tags ) will be encountered in the test data .",
    "however , post - cores only make up a small fraction of the full real - world data and the majority of posts are not included in this subset .",
    "full tagging datasets have a large proportion of new / unseen documents . in order to recommend tags for these new documents ,",
    "approaches are required which model documents not only based on the tags assigned to them in the past ( if any ) , but also their content . while the inclusion of content data has been applied to keyword extraction and hybrid tag recommenders @xcite , graph - based approaches which model the full folksonomy graph as well as taking content data into account have not been widely explored .      in this paper",
    "we present an in - depth analysis of the folksonomy graph model , propose novel adaptations and extensions to folkrank @xcite , and evaluate our hypotheses on four datasets from popular social tagging websites .",
    "we propose two alternative extensions to include content data into folkrank s recommendation process and evaluate the predictive value of different content sources as well as varying amounts of content data .",
    "our extensions make folkrank successfully applicable to full unpruned datasets and address the new document problem in tag recommendation . in our extensive examination of the folksonomy graph model we highlight information that is lost and implicit assumptions that are made by the model , and propose a novel graph structure which captures the tagging data more accurately .",
    "furthermore , we conduct an in - depth analysis of folkrank s iterative weight spreading algorithm and address issues that exist therein .",
    "the outcome of this analysis is a novel weight spreading method which is much less computationally expensive while having a comparable recommendation accuracy to the iterative approach .",
    "finally , we provide an extensive theoretical discussion as well as practical evaluation of the value of exploring the deep folksonomy graph . we evaluate whether the potential benefit of considering the information contained in deeper levels of the graph is worth the added computational expense and present important insights regarding the applicability of deep graph - based methods to tagging data in general . in summary ,",
    "our main contributions are :    * content - aware extensions of folkrank which model the entire folksonomy as well as taking content data into account . * an improved graph data model which more accurately captures the tagging data . *",
    "a less expensive but equally accurate weight spreading method for graph - based tag recommendation . * an in - depth theoretical discussion as well as practical evaluation of the value of exploring the deeper folksonomy graph for tag recommendation , and of the applicability of graph - based methods to the domain of social tagging in general .",
    "existing tag recommendation solutions can be categorised into approaches which model and analyse the folksonomy in order to come up with recommendations , and content - based approaches where the textual content and/or meta - data of documents is considered .",
    "we give our view of the tag recommendation landscape in figure [ fig : approaches_overview ] .",
    "methodologies relying on the folksonomy data include hypergraph @xcite , graph @xcite , collaborative filtering @xcite and simple co - occurrence @xcite approaches .",
    "while hypergraph approaches try to capture and analyse all characteristics of the folksonomy in their models , graph - based and collaborative filtering approaches can be described as reductionist methods since they reduce the 3-dimensional folksonomy data to one or several 2-dimensional projections .",
    "a major difference between graph - based , collaborative filtering and co - occurrence approaches is that co - occurrence methods only consider the immediate neighbourhood of the query , corresponding to one hop in the folksonomy graph .",
    "collaborative filtering considers connections one level deeper into the folksonomy , for example comparing the query user to similar users based on their overlap in document sets or tag vocabulary .",
    "graph - based approaches have the possibility to explore the graph further and include information contained in the deep folksonomy into the recommendation process .    in content - based approaches , the textual content of the documents",
    "is used for either tag extraction and expansion @xcite , word - tag co - occurrence @xcite , or with document classification techniques @xcite .",
    "important aspects of content based approaches are the content source and the document representation used .",
    "experiments have shown that the most informative words generally appear in the title and url @xcite , and the document text @xcite .",
    "for structured text documents such as html , further sources such as anchors , links and paragraphs are available @xcite .",
    "heymann et al",
    ".  carried out experiments on html pages @xcite , comparing the value of page text , anchor text and text of surrounding hosts for tag prediction .",
    "they concluded that out of the three , the document text was most informative and anchor text was more informative than surrounding hosts .",
    "the document representation in content - based approaches is usually a bag - of - words .",
    "there are many different methods of determining the importance score of each word to the document , most of which include a tf - idf score in the calculation .",
    "use tf - idf scores with part - of - speech analysis , word clustering and a sentence importance score ; combines tf - idf scores and lexical tools ; and calculate the tf - idf scores on small word parts ( quad - grams consisting of 4 letters ) instead of whole words to overcome the stemming problem . in @xcite ,",
    "tf - idf scores as well as the position of the first appearance of a word in the document are used . alternatively to tf - idf , use word frequency and word - word co - occurence to calculate scores for words given only single documents rather than a document collection .        in @xcite ,",
    "lipczak et al .",
    "present their hybrid tag recommender which won the content - based tag recommendation task of the ecml pkdd discovery challenge 2009 @xcite .",
    "the part of the hybrid most comparable to our content - based approaches , for which lipczak et al .  give individual results , is `` user profile @xmath27 resource related '' , which is a combination of two tag recommendation sets : past tags of the query user and tags related to the content of the query document .",
    "the only source of content data in their approach is the document title . in order to generate the content - based tag recommendations ,",
    "the words in the title of the query document which have been used as tags before in the training data are first extracted ( word - tag overlap ) .",
    "the tag recommendation set is then expanded based on tag - tag co - occurence . due to the initial filtering , the content words considered only include words which also appear as tags in the training data .",
    "the differences between our approaches for including content data and the content - related part of the hybrid in @xcite are the content sources and the document representation used .",
    "we consider all content words ( after stopword removal ) in our document representation , not only words that are tags , and base our predictions on word - tag co - occurence as well as utilising a content - based word importance measure ( tf - idf ) .",
    "moreover , we consider and evaluate two different content sources in our approaches : document title and fulltext content .",
    "the most successful part of the hybrid presented in @xcite is the `` title recommender '' , which achieves better results than `` user profile @xmath27 resource related '' .",
    "it recommends words from the query document s title directly , choosing the words which have been observed to have a high global overlap in being a title word as well as a tag for documents .",
    "it is worth noting that these are words where the word string is equal to the tag string , which is not the case in our word - tag co - occurence approach .",
    "nevertheless , it is a simple and very successful tag recommendation strategy producing a limited number of tags with high precision .",
    "folkrank @xcite is a graph - based ranking algorithm which is modelled based on google s pagerank @xcite .",
    "similarly to pagerank , the key idea of folkrank is that a document which is tagged by important users with important tags becomes important itself .",
    "the same holds symmetrically for users and tags .",
    "users , documents and tags are represented as nodes @xmath28 in an undirected tri - partite graph @xmath29 , where all co - occurrences of users and documents , users and tags , and documents and tags are edges @xmath30 between the corresponding nodes .",
    "the weight of the edge between two nodes depends on the number of their co - occurrences , given as the number of tag assignments that both nodes appear in .",
    "for example if a user @xmath8 has used a tag @xmath9 for two documents , there would be two tag assignments @xmath31 and @xmath32 in the folksonomy , and in @xmath33 the weight of the edge between the two nodes representing @xmath8 and @xmath9 would be set equal to two .",
    "the importance or rank of each node in folkrank is calculated by an iterative weight - spreading algorithm , in a similar fashion to pagerank .",
    "the weights of all nodes are given in the weight vector @xmath34 which has one entry per node and is computed by the weight spreading function @xmath35 where @xmath36 is the row - stochastic version of the adjacency matrix of graph @xmath33 , @xmath37 is the preference vector , and the dampening factor @xmath38 determines the influence of @xmath37 .",
    "the preference vector @xmath37 is used as a means to personalise the recommendations to the query user and document , and to achieve that goal is set to give the nodes representing the query user and document in the graph a higher preference weight compared to other nodes . the dampening factor @xmath10 sets the balance between personal preference and global importance when calculating the node weights . after constructing the folksonomy graph ,",
    "the tag ranking procedure with folkrank is as follows for each test post .    1 .",
    "initialise each node in the graph with a random starting weight so that the total sum of node weights in the graph is equal to a predefined parameter @xmath39 .",
    "2 .   set the preference vector giving the query user and document a higher weight than other nodes in the graph , and so that the sum of weights in the preference vector is equal to the total weight in the graph @xmath39 .",
    "3 .   perform iterative weight spreading until node weights converge .",
    "the end condition is that the sum of absolute change in node weights during one iteration is smaller than a predefined fraction of the total weight @xmath39 .",
    "4 .   select the nodes which represent tags and rank them by node weight , where the tag node with the highest weight is given the best ranking .    when setting the weights of the preference vector it is important that the sum of preference weights is equal to the total sum of node weights in the graph .",
    "this ensures that the total weight in the graph stays constant over weight - spreading iterations ; that no factors other than parameter @xmath10 impact the amount of personalisation ; and that the end condition of iterative spreading will work as intended . folkrank can generate a global non - personalised ranking and a personalised ranking of all nodes in the graph , depending on the values set in the preference vector . for a global ranking",
    ", the entries in @xmath37 for all nodes are set to the same value . in order to generate personalised recommendations for a query post @xmath22 , @xmath37",
    "is set so that higher preference weights are given to the query user @xmath23 and query document @xmath24 , compared to other nodes in the graph which are set to have a uniformly small ( non - zero ) preference weight @xcite .",
    "the original folkrank utilises a differential approach @xcite to calculate the final tag scores for a query post .",
    "for each tag in the graph , the weight of the tag in the global ranking is subtracted from the weight of the tag in the query - personalised ranking to give the final prediction score .",
    "although this approach produces better results than using the personalised ranking alone , a simpler alternative strategy is discussed in @xcite .",
    "kim and el saddik conclude that setting the preference weights of all non - query nodes in the personalised preference vector to zero instead of the uniformly small values results in an equivalent ranking to the differential approach .",
    "an unexplored question in folkrank is whether the same amount of preference weight should be given to the query user and query document . in the original folkrank approach the balance of preference weight between the query user and document",
    "is not specified explicitly via a parameter but instead is determined implicitly by the ratio of user nodes to document nodes in the graph @xcite . in general this leads to the query document receiving more preference weight than the query user since there are usually more documents than users in a folksonomy , however it is determined entirely by the data .",
    "we introduce a parameter @xmath40 into our folkrank - based approaches which allows us to control how the total preference weight is distributed between the query user and document .",
    "the preference weights of the query user @xmath23 and query document @xmath24 are then given by @xmath41 @xmath42 where @xmath43 , @xmath44 is the total preference weight and @xmath45 since we set the total preference weight equal to the total ( starting ) weight in the graph .",
    "if we set @xmath46 , where @xmath2 is the set of users and @xmath3 is the set of documents , then we have the equivalent strategy to that used in the original folkrank .",
    "we conduct an in - depth analysis of the inner workings of folkrank , highlight issues which might reduce tag recommendation accuracy and propose novel adaptations to overcome these .",
    "we first examine the folksonomy graph model used in folkrank and the implicit assumptions made by the edge weights in the graph , and propose an alternative model which we call post graph .",
    "the second part of our analysis concerns itself with the weight - spreading iterations of folkrank and the utilisation of information contained in the deep graph .",
    "we also address and propose how to overcome the high complexity and runtime of the algorithm .",
    "the first issue we examine is the graph structure of the folksonomy model and the problem of setting the edge weights .",
    "folkrank uses a tri - partite graph of the folksonomy consisting of user , document and tag nodes . due to the fact that a post can contain a variable number of tags and since the post - membership information of tag nodes is not included explicitly in the folksonomy model , the user and document nodes in the graph can be connected to a variable number of tag nodes .",
    "the variable number of tags per post affects the outcome of weight spreading since in each spreading action the weight that is passed to each connected node depends on the total number and weight of edges of the active node .",
    "the difficulty is then setting the edge weights in the graph , where each alternative method of doing so makes different assumptions . in the following paragraphs",
    "we explore alternative graph construction methods and the implicit assumptions they make .",
    "we later evaluate each of these methods in section [ sec : evaluation_and_results ] . an assumption which holds in all alternatives",
    "is that the co - occurrence of users and tags , as well as documents and tags , should influence edge weights",
    ". the weight of a user - tag edge should be higher if the user has used the tag multiple times to tag multiple documents .",
    "similarly , if the same tag has been assigned to a document multiple times , so by multiple different users , the document - tag edge should be given a higher weight .",
    "in contrast , for user - document relationships the tagging data only provides one distinct co - occurrence since a user can only tag each document once ( with a set of tags ) .",
    "[ [ folksonomy - graph ] ] folksonomy graph + + + + + + + + + + + + + + + +        the folksonomy graph structure and edge weighting methodology used in original folkrank is given in figure [ fig : folkrank_alt1 ] .",
    "the weight between user and document nodes is set according to the number of tag assignments and thus the number of tags in the post ( see edge weights @xmath47-@xmath48 vs @xmath47-@xmath49 ) .",
    "this means that within the context of a post , all types of nodes ( user , document , and all tags together ) get the same amount of total weight . in the context of post @xmath50)$ ] only , ignoring the influence of post @xmath51)$ ] , the weight of the edge @xmath47-@xmath48 is the same as the sum of edge weights @xmath47-@xmath52 , @xmath47-@xmath53 and @xmath47-@xmath54 , which is 3 .",
    "however , another consequence of this graph construction method is that , if we spread weight from @xmath47 , then @xmath48 would get a higher weight than @xmath49 , and subsequently , the tags connected to @xmath48 , in this case @xmath55 , would get a higher weight than the tags connected to @xmath49 , namely @xmath56 .",
    "the implicit assumption made by this model is that documents to which a user has assigned many tags are more representative of the user s interest .",
    "another assumption is made with regard to the number of tags in a post . if we had a query post @xmath57 , the fraction of weight spread from @xmath47 to @xmath54 , which is the user s most used tag , would be @xmath58 ( times the dampening factor ) .",
    "however , the query document @xmath59 would spread @xmath60 of its weight to @xmath55 .",
    "assuming both the query user and query document have the same preference weight , @xmath55 would thus be ranked higher than @xmath54 even though @xmath54 has been used by the user multiple times and @xmath55 has only been assigned to @xmath59 once .",
    "the assumption which leads to this outcome is that if a post has multiple tags then each of the tags is proportionally less important to the user and document of the post . in summary , as a consequence of the graph model , the following implicit assumptions are made when spreading weight in the folksonomy graph .    * within the context of a post , all types of nodes ( user , document , tag ) have the same amount of relevance summed by node type . *",
    "the weight of the user - document relationship depends on the number of tags in the respective post .",
    "the more tags a user has assigned to the document , the stronger the user - document connection . *",
    "each tag in a post is proportionally less important to the user and document if the post contains multiple tags .",
    "[ [ folksonomy - adapted - graph ] ] folksonomy adapted graph + + + + + + + + + + + + + + + + + + + + + + + +        we propose an alternative edge weighting method for the folksonomy graph , illustrated in figure [ fig : folkrank_alt2 ] , which we refer to in our experiments as the adapted graph ( ag ) .",
    "the difference to the original folksonomy methodology is that we always keep user - document edges at a weight of 1 regardless of the number of tags in the post . by spreading weight in the adapted graph ,",
    "the following assumptions are made .",
    "* within the context of a post , all tag nodes together are more important than user or document nodes . *",
    "the weight of the user - document relationship is independent of of the number of tags in the respective post .",
    "all edges connecting users to documents have the same weight . *",
    "each tag in a post is proportionally less important to the user and document if the post contains multiple tags .",
    "[ [ sec : postrank ] ] post graph + + + + + + + + + +        since both aforementioned graph construction methods do not explicitly include the post - membership information of tag nodes , we believe that they produce an inaccurate model of the social tagging data , and propose a structurally different graph model which we call post graph ( pg ) .",
    "the post graph model includes an additional type of node representing posts themselves into the graph .",
    "figure [ fig : postrank1 ] shows the post graph for the same data as the previous folksonomy graph and adapted graph models . the user , document and tag nodes are only connected to post nodes instead of being directly connected to each other , and we set the weight of post - tag edges so that the edge weights to all tags of a single post sum to 1 .",
    "this makes the strength of the user - document relationships independent of the number of tags in the post , as well as ensuring that the same amount of total weight is spread to all types of nodes in the context of a post . to address the assumption that having multiple tags in a post implies less importance for each of them we evaluate an alternative method of retrieving tag scores from the graph . instead of directly retrieving tag scores as the weight of tag nodes ,",
    "we retrieve the weight of post nodes instead , and in a second step calculate the tag scores by summing up for each tag the weight of the post nodes that the tag is related to ( ignoring the total number of tags in each post ) .    * within the context of a post , all types of nodes ( user , document , tag ) have the same amount of relevance summed by node type . *",
    "the weight of the user - document relationship is independent of of the number of tags in the respective post .",
    "all edges connecting users to documents have the same weight . * by retrieving each tag s score as the sum of weights of post nodes it is connected to , the importance of each tag is independent of the total number of tags in its post .",
    "folkrank s iterative weight spreading algorithm has two potential advantages over approaches which only utilise the immediate neighbourhood of the query nodes , such as simple co - occurrence methods .",
    "firstly , by initialising all nodes with random starting weights , the general importance of tags is taken into account when generating recommendations .",
    "this can also be described as authority - based popularity , due to the characteristic that important user or document nodes will provide more weight to their connected tags .",
    "secondly , the weight spreading algorithm considers the information contained in the deep graph . in figure",
    "[ fig : folkrank_shallow_vs_deep ] we illustrate how folkrank utilises the deeper graph beyond the immediate neighbourhood of the query user and document .",
    "user @xmath47 and document @xmath59 make up the query post , the immediate neighbourhood of the query nodes is shown in solid lines , the deeper graph is shown in dashed lines , and the weights of all edges which are not explicitly labelled are set to 1 . in approaches considering only the immediate neighbourhood of @xmath47 and @xmath59 , the candidate tag set for this query post would consist of tags @xmath52 , @xmath53 , @xmath54 , @xmath55 , @xmath56 , and @xmath61 . a co - occurrence approach , such as our combination of user - related and document - related tags @xcite , would rank @xmath52 as the best recommendation as it is related to both @xmath47 and @xmath59 , followed by @xmath53 as the second best since it has a relatively strong relationship with @xmath47 .",
    "however , when trying to rank @xmath56 and @xmath61 , both of these tags would have the same prediction score and the algorithm would not have sufficient information to decide which of them should precede the other in the ranking . in the final tag predictions ,",
    "the ordering of @xmath56 and @xmath61 would be random . by utilising the deeper graph , folkrank s iterative weight spreading algorithm",
    "has the ability to provide a definitive ranking of @xmath56 and @xmath61 by trying to deduce which of them is more important to the query nodes .",
    "it would spread weight along the path @xmath62 and thus @xmath61 would be ranked higher than @xmath56 .",
    "the other method that folkrank has for breaking ties and re - ranking tags which would otherwise have equal prediction scores are the general importance weights .",
    "additionally , folkrank also spreads weight to tag @xmath63 found in the deeper graph and includes it in the candidate set , whereas @xmath63 would be omitted by approaches which only recommend tags co - occurring with the query user or document .",
    "it seems intuitive from the graph structure and from literature applying graph - based approaches to non - folksonomy data that @xmath63 should receive some weight and be included in the candidate tag set , and that @xmath61 is more related to the query and thus should be ranked higher than @xmath56 .",
    "however , the value of following this computationally expensive strategy and considering the connections of the deep folksonomy graph has not yet been directly evaluated .",
    "as there are other factors that impact the weight spreading calculation of folkrank , which we explore in the following paragraphs , it has not yet been established that considering the deep graph provides an increase to tag recommendation accuracy . as part of the theoretical discussion , the opposite argument to folkrank s assumption about the predictive indications of the deep graph could also be made .",
    "considering the query user @xmath47 , if we make the somewhat weak assumption that @xmath47 is aware of the existence of tag @xmath63 , then it would not make sense to spread weight to @xmath63 . in the graph in figure",
    "[ fig : folkrank_shallow_vs_deep ] , user @xmath47 has tagged @xmath49 with the tags @xmath53 , @xmath54 and @xmath55 . a different user @xmath64 has also tagged the same document @xmath49 with tag @xmath63 .",
    "if we assume that @xmath47 has , in his view , completely described @xmath49 with @xmath53 , @xmath54 and @xmath55 , this would suggest that @xmath63 was not required by the query user @xmath47 to describe @xmath49 .",
    "one could thus argue that this was a conscious decision and @xmath63 might not be considered to be a good descriptor by @xmath47 in general .",
    "the weak points of this argument are the generalisation and the assumption of completeness . rather than dismissing tag @xmath63 completely",
    ", @xmath47 might also think that @xmath63 is not appropriate for the documents he has tagged so far but generally a useful descriptor .",
    "more importantly , user @xmath47 might not be aware of tag @xmath63 at all .",
    "however , starting from the query document @xmath59 , a similar and more convincing argument can be made with regard to @xmath63 .",
    "document @xmath59 has been tagged with @xmath56 and @xmath61 by user @xmath65 , who has also tagged a different document @xmath66 with @xmath63 . in this case",
    "the argument against assigning a higher weight to @xmath63 as a candidate tag for @xmath59 is much stronger .",
    "since user @xmath65 has used @xmath63 , it is guaranteed that he is aware of its existence .",
    "however , @xmath65 has explicitly not assigned @xmath63 to @xmath59 , and is using the different tag sets of @xmath67 $ ] and @xmath68 $ ] to distinguish between documents @xmath59 and @xmath66 . if any deduction is made about the relevance of @xmath63 to @xmath59 , it should be that the graph indicates a negative relationship and the weight of @xmath63 with regard to @xmath59 should be reduced rather than increased .    the counter - argument to utilising even longer paths , which leads to folkrank s ranking of @xmath61 above @xmath56 ,",
    "is the highly personal tagging behaviour of users in ( broad ) folksonomies .",
    "folkrank uses the path @xmath62 to deduce that @xmath61 is more relevant than @xmath56 to the query consisting of user @xmath47 and document @xmath59 .",
    "however , this deduction is based on the fact that @xmath61 was used by a different user @xmath64 for a different document @xmath69 , and the only link to the query is given by @xmath64 having tagged @xmath49 which has also been tagged by the query user @xmath47 .",
    "the shared document @xmath49 is taken as an indication that @xmath47 and @xmath64 have similar interests and that @xmath47 should give some authority to all of the other opinions / tag assignments made by @xmath64 .",
    "argue that tag assignments can not be transferred as easily across users and provide evidence for the highly personalised tagging behaviour of users in broad folksonomies .",
    "they show that users who have tagged the same documents rarely assigned the same tags to these documents . even though the users",
    "areas of interest are similar due to the shared documents , only a small overlap can be observed in their tag vocabulary which indicates that the users views of the documents are highly personal .      in the following paragraphs",
    "we analyse the iterative weight spreading method of folkrank in detail and address issues which we believe to hinder or cascade its ability to effectively utilise the information contained in the deeper graph .",
    "an important preliminary observation about folkrank is that the impact of each preference node on the final weights in the graph is independent of the influence of other preference nodes .",
    "having multiple nodes with preference weight @xmath70 in the preference vector of folkrank is virtually equivalent to performing the weight spreading computation for each of the preference nodes separately , and then doing a linear combination of the resulting weight vectors to give the final ranking .",
    "as long as the end condition of the weight - spreading iterations is set sufficiently small , the only nodes which could end up with a different weight are the ones at the very bottom of the ranking .",
    "this method can be used to speed up the prediction time of folkrank in a live tag recommendation scenario .",
    "for each user in the system , the tag scores can be pre - calculated offline and stored .",
    "the same can be done for each document . during the online tag recommendation phase , the pre - calculated tag scores for the query user and query document",
    "would then be retrieved and a weighted average of the scores would be taken per tag in order to quickly create tag recommendations . for the following discussion",
    "we assume that this method of performing a separate weight spreading computation for each of the preference nodes is used , so that each individual weight spreading run will have only one node in the preference vector .",
    "[ [ swash - back - problem ] ] swash - back problem + + + + + + + + + + + + + + + + + +        a problem of folkrank as discussed in @xcite is `` swash - back '' of weights .",
    "since the graph is undirected , weight is spread from a node @xmath71 to a connected node @xmath72 in one iteration and then spread back from @xmath72 to @xmath71 in the next iteration .",
    "this means that the weight of @xmath71 , the node from which the weight - spreading originates , is adjusted in the second iteration based on the ( number of and weight of ) edges of @xmath72 , which does not seem intuitive and is not desirable .",
    "we illustrate the consequences of this in figure [ fig : folkrank_swashback ] .",
    "user @xmath47 has tagged documents @xmath48 and @xmath49 with tags @xmath52 and @xmath53 respectively .",
    "tag @xmath52 is also used by a very active user @xmath73 who is connected to a large number of other nodes , where @xmath53 is also used by user @xmath65 who has only one tag assignment .",
    "if we want to recommend tags for @xmath47 and activate that node in the graph , @xmath52 and @xmath53 would get the same weight in the first iteration . in the second iteration @xmath52",
    "spreads weight to @xmath73 , and @xmath53 spreads weight to @xmath65 ( as well as all of their other connected nodes ) , where the weight received by @xmath73 and @xmath65 is equal .",
    "the third iteration is when the swash - back with regard to @xmath52 and @xmath53 occurs , denoted by the empty arrows .",
    "tag @xmath53 gets half of the weight of @xmath65 ( times the dampening factor ) back since @xmath65 is connected to two nodes .",
    "however , @xmath73 is connected to many other nodes , and so the weight spread back from @xmath73 to @xmath52 would be much less than the weight spread back from @xmath65 to @xmath53 . in the final tag predictions for user @xmath47 ,",
    "tag @xmath53 would have a higher score than @xmath52 due to the behaviour of users @xmath73 and @xmath65 .",
    "this is contrary to our intuition that the weights should be equal up to this point since the query user @xmath47 has used both tags with equal frequency in the past . in the final ranking",
    ", when the node weights of the query user are combined with the node weights produced by the query document , the weights of @xmath52 and @xmath53 are expected to change to reflect the influence of the deeper graph . however , in this weight spreading operation for the query user only , the only source of preference weight is @xmath47 .",
    "the change in weights due to swash - back might outweigh the later influence of other preference nodes and prevent folkrank from utilising the information contained in the deeper graph .",
    "[ [ triangle - spreading - problem ] ] triangle - spreading problem + + + + + + + + + + + + + + + + + + + + + + + + + +        another issue , which we refer to as triangle - spreading of weights , is illustrated in figure [ fig : folkrank_triangle_spreading ] .",
    "user @xmath47 has tagged document @xmath48 and @xmath49 with tags @xmath52 and @xmath53 respectively .",
    "document @xmath48 is a popular document tagged by many other users , whereas @xmath49 has only been tagged by @xmath47 .",
    "if we activate @xmath47 in order to recommend tags for this user , tags @xmath52 and @xmath53 would get the same weight in the first iteration .",
    "in the second iteration , @xmath49 would spread half of its weight to @xmath53 ( times the dampening factor ) , however , @xmath48 would spread less of its weight to @xmath52 since @xmath48 is also connected to several other nodes .",
    "this would mean that in the tag weights for query user @xmath47 , tag @xmath53 would get a higher weight than @xmath52 even though the user has used both tags with equal frequency .",
    "a similar problem would arise with regard to the weight of documents @xmath48 and @xmath49 if one of the tags was very popular .",
    "due to graph being undirected and the folksonomy consisting of triplet relationships ( user , document , tag ) , if two nodes @xmath71 and @xmath72 are connected , there is always at least one indirect path from @xmath71 to @xmath72 via a third node @xmath74 .",
    "the weight spread from @xmath71 to @xmath72 over the indirect path via @xmath74 is influenced by the ( number of and weight of ) edges of @xmath74 .",
    "this is undesirable since the weight of the direct edge from @xmath71 to @xmath72 already completely describes the relationship between @xmath71 and @xmath72 .",
    "moreover , the influence of the triangle - spreading is likely to cascade the effect of the deeper graph on final tag weights , since the indirect path along which the undesired spread happens has a length of only two hops .      in order to address the swash - back and triangle spreading problems we present our adapted weight - spreading approach for undirected folksonomy graphs , which we call pathrank . rather than doing iterative weight spreading",
    ", pathrank assigns scores to each node in the graph based on the shortest path(s ) from the preference nodes .",
    "the weight spreading computation works in a similar manner to a breadth - first search , where edges which were already explored in previous iterations are not re - visited .",
    "pathrank is akin to spreading activation which is usually applied to directed graphs , and where nodes also spread their weight only once .",
    "however , pathrank is used on the un - directed folksonomy graph and gives the edges a personalised direction starting from the query nodes , where the edge direction can be different for each query .",
    "pathrank can be described as activation - directed weight spreading .",
    "in contrast to the original iterative weight spreading approach of folkrank , we set the initial weight of all nodes in the graph to zero instead of initialising nodes with random starting weights .",
    "pathrank thus only uses personalised weights , originating from the preference nodes , and there are no general importance weights in the graph ( which makes the dampening factor parameter obsolete ) .",
    "we compute node weights separately for each preference node and then take a weighted average of the resulting weights for each node in the graph to produce the final node weights , taking into account all preference nodes . because of the separate calculation per preference node and the setting of all starting weights in the graph to zero , each individual weight - spreading calculation has only one node , the preference node , from which all of the weight in the graph originates .",
    "the swash - back and triangle spreading of weights can then be prevented by adapting the iterative weight spreading algorithm with a simple rule : _ if the weight of a node has been updated in a previous iteration ( i.e.  is not equal to zero ) , then do not re - calculate the node s weight in subsequent iterations_. thus , the pathrank weight spreading algorithm is in effect not an iterative update calculation like in pagerank / folkrank , but rather assigns a weight to each node @xmath75 based on the edges of the shortest path(s ) from the each of the preference nodes @xmath76 to @xmath75 .",
    "the parameter @xmath77 specifies the maximum path length from the preference nodes to be explored by pathrank .",
    "the end condition of pathrank weight spreading is that either the maximum path length @xmath77 has been reached , or that all nodes in the graph have been explored and assigned a weight greater than zero .",
    "the benefits of pathrank are that the problems of swash - back and triangle - spreading of weights are removed , which allows the algorithm to fully utilise the information contained in the deeper graph",
    ". since there are no general importance weights , these also can not interfere with and cascade the influence of the weight spread through the deeper graph .",
    "intuitively we would assume that weights spread from preference nodes through the deeper graph would result in a better re - ranking of the tag nodes in comparison to using general importance weights , since the general importance of nodes is not personalised and constant across all query posts .",
    "setting different values for the maximum path length @xmath77 to be explored allows for a direct evaluation of the value of including the deeper graph in the recommendation process . in our evaluation in section [ sec : tuning_pl ] we address the question of how much value there is in exploring each step deeper into the graph when calculating tag predictions .    regarding runtime , as long as we only have one preference node ,",
    "the complexity of weight spreading is greatly reduced in pathrank compared to folkrank , since once a node s score is set it does not need to be re - calculated in every subsequent iteration . if we take the same graph , let @xmath78 denote the total number of iterations and @xmath26 denote the number of edges in the graph , folkrank s iterative weight spreading has a complexity of @xmath79 . in each iteration",
    ", weight is spread in both directions along each edge , partly because the nodes are initialised with random starting weights .",
    "pathrank has a worst - case complexity of @xmath80 if the weight spreading is performed until all nodes in the graph are explored .",
    "weight is only spread once along each edge in one direction .",
    "however , in the case that there are several preference nodes , pathrank needs a separate weight - spreading calculation for each of them , meaning the complexity would be @xmath81 where @xmath82 is the number of nodes with weight @xmath70 in the preference vector , whereas the runtime of folkranks s iterative algorithm would not change . for the expensive folkrank algorithm to be applicable in practice ,",
    "the individual tag scores per user and per document have to be pre - calculated offline , and then combined in the online recommendation phase to quickly generate predictions . in this scenario , where each of the pre - calculation runs has only one node in the preference vector , pathrank",
    "is guaranteed to outperform folkrank regarding runtime . moreover , by limiting the maximum path length via the parameter @xmath77 , the runtime can be further reduced .",
    "as we later show in the evaluation , @xmath77 can be set to almost minimal values without a decrease in prediction accuracy .",
    "here we present our methods for extending folkrank with content data .",
    "these content - aware recommenders include the textual content of documents in the recommendation process as well as utilising the full folksonomy graph .",
    "this allows us to relate new unseen documents to already tagged ( different ) documents in the system and make recommendations based on the tag assignments related to those known documents .",
    "we can thus overcome the new document problem and make the solely folksonomy - based recommenders applicable to full real - world datasets .",
    "for test posts where the query user is new ( as well ) , we have to default to the most popular tags found to be related to the content of the query document and can not personalise these to the user , which is acceptable since the user does not have a tagging profile yet . in the following sections we first describe the document content model we use and then present our content - aware graph recommenders .      for including data from the content of documents in the tag recommendation algorithms , we consider two sources of content words : document title and fulltext content .",
    "we convert all words to lower - case , remove stop - words as well as all words which have a length of less than 3 or more than 20 characters , and use the remaining words without stemming .",
    "each document is represented by a bag - of - words vector of content words with tf - idf scores for each word .",
    "tf - idf stands for _ term frequency - inverse document frequency _ and we compute it as @xmath83 where @xmath3 is the set of all documents , @xmath84 is the term count equal the number of occurrences of word @xmath85 in document @xmath14 , and @xmath86 is the document count equal to the number of documents in the collection containing word @xmath87 .",
    "we normalise the tf - idf scores to sum to 1 per document .",
    "a factor to consider is that the content data of websites can change over time .",
    "the title , content and meta - data of a website which is bookmarked can be updated and differ from one post to the next .",
    "this presents a problem , as well as additional data for analysis .",
    "the fulltext content of the bookmarked website itself is only available in the current version at the time of retrieval , however , the bibsonomy dataset provides different versions of metadata for a document at the time of each post . where available , we concatenate the title variations of a document from all its posts and treat the resulting text string as the single document title .",
    "this makes the term count measure @xmath84 in our tf - idf calculation more powerful as words which persist over several title variations will end up with a higher score than words which only appear in a few of the variations .",
    "contentfolkrank ( which we first presented in @xcite ) includes the content of documents directly into the graph .",
    "we adapt the original folksonomy graph of folkrank to model triplets @xmath88 @xmath89 instead of @xmath90 .",
    "each tag assignment in the training data @xmath91 is converted to a set of tag assignments with words instead of documents @xmath92 , @xmath93 , @xmath94 , @xmath95 where each of the words @xmath87 is in the content of @xmath10 .",
    "figure [ fig : folkrank_document_vs_word_nodes ] shows the standard folkrank graph with document nodes on the left , and the contentfolkrank graph modelling the same data on the right , where document @xmath48 is represented by word nodes @xmath96 , @xmath97 and @xmath98 , and document @xmath49 is represented by word nodes @xmath98 and @xmath99 .",
    "in contentfolkrank we use custom rules for setting the weights of different types of edges , namely user - word edges , word - tag edges and user - tag edges . to prevent the content length of a document , and thus the number of word nodes representing the document in the graph , from influencing the recommendation process , we utilise tf - idf scores when setting the edge weights .",
    "the tf - idf scores are normalised to sum to 1 per document .",
    "this provides suitable weights for the edges of word nodes representing the document in the graph and ensures that the number of words itself does not impact the generated recommendations .",
    "additionally , the tf - idf scores provide appropriate weights for capturing the varying importance of different content words to the document , and have been shown to be beneficial in @xcite . since several documents , in our example @xmath100 and @xmath101 , tagged by the same user @xmath102 can contain the same word @xmath103 , the weight of the edge between @xmath102 and @xmath103 is set to the sum of the normalised tf - idf scores of @xmath103 in @xmath100 and @xmath101 .",
    "the same holds for edges between word and tag nodes .",
    "the final edge weights thus take the content importance of words as well as tagging co - occurence in the folksonomy into account .",
    "the weight of the edges between user and tag nodes are solely based on co - occurrence since only complete documents ( and not individual words ) can be tagged by users , and are set to the number of posts in which the user has used the tag .",
    "the formulae for calculating the weights of the different types of edges are the following . for user - word edges ,",
    "the edge weight is given by @xmath104 where @xmath105 is the set of posts by user @xmath8 where the document contained word @xmath87 .",
    "similarly , for word - tag edges we calculate the weight using @xmath106 where @xmath107 is the set of posts tagged with @xmath9 ( by any user ) where the document contained word @xmath87 . for user - tag edges",
    "there is no need to include tf - idf scores as complete documents are tagged by users and words can not be tagged on their own , so we use the formula @xmath108 where @xmath109 is the set of posts where user @xmath8 used tag @xmath9 .",
    "the preference vector for each test post is given by @xmath110 where @xmath23 is the query user and each word @xmath87 is in the content of the query document @xmath24 .",
    "the preference weight for each word @xmath87 is set proportional to its tf - idf score in @xmath24 , and is given by @xmath111 where the tf - idf weights are normalised to sum to 1 per document , @xmath40 is the parameter for setting the balance in preference weight between the query user and document , and @xmath44 is the total preference weight .",
    "the preference weight of the query user is the same as before without content : @xmath112 .",
    "our second approach of including content into the recommendation process is to utilise a content - based document similarity measure and include content information implicitly rather than introducing words directly into the graph .",
    "the graph model of simfolkrank does not contain content data itself and documents are represented by document nodes , using either the original folksonomy graph ( simfolkrank ) or the post graph model ( simfolkrank_pg ) .",
    "however , for each test post , we construct the preference vector to include not only the query document ( if it already exists in the graph ) but also a predefined number of training documents most similar in content to the query document . in our experiments",
    "we evaluate the effects of including different numbers of most similar documents in the preference vector .",
    "the similarity between documents is calculated based on the words in either the title or the full text of the documents .",
    "the metric we use is cosine similarity of the bag - of - words document vectors with normalised tf - idf scores . due to the problem",
    "that document content data can vary over time ( as discussed in section [ sec : doc_model ] ) , it can be the case that a query document which also exists in the graph as a training document ends up with a low content similarity score with itself . to overcome this issue",
    ", we include an additional step where we set the similarity of query documents with themselves to 1 , provided that they appear as training documents as well .",
    "once the cosine similarity of a query document to each training document is calculated , we normalise the similarity scores to sum to 1 for the query document .",
    "this ensures that the number of similar training documents with cosine similarity greater than zero does not affect the recommendation process .",
    "the preference weight of each training document @xmath10 included in the preference vector is a function of its similarity to the query document @xmath24 and is given by @xmath113 where @xmath114 is the content - based similarity between @xmath10 and @xmath24 normalised to sum to 1 over all documents @xmath10 similar to @xmath24 , parameter @xmath40 determines the balance in preference weight between the query user and document , and @xmath44 is the total preference weight .",
    "the preference weight of the query user is the same as before : @xmath112 .",
    "we apply and evaluate this approach of including content with iterative weight spreading ( simfolkrank , simfolkrank_pg ) as well as our pathrank weight spreading algorithm ( simpathrank , simpathrank_pg ) .",
    "our datasets consist of tagging data from the social bookmarking websites citeulike , delicious and bibsonomy , and additionally downloaded content data for our content - aware recommenders .",
    "official snapshots of citeulike and bibsonomy are available on their respective websites .",
    "we use the citeulike 2012 - 05 - 01 snapshot , and the bibsonomy 2012 - 07 - 01 snapshot .",
    "the bibsonomy social bookmarking website and dataset is split into two separate sections : bibsonomy bookmark which are website bookmarks and bibsonomy bibtex which are publication bookmarks .",
    "we treat these two subsets of bibsonomy as separate datasets .",
    "delicious does not provide snapshots of their data .",
    "here we use a dataset that was obtained by crawling the delicious website in 2005 , the specifics of which are given in @xcite .    additionally , we downloaded all of the available pages from the urls in the delcious and bibsonomy bookmark datasets , and all of the bibtex entries for citeulike . for our content - aware recommenders",
    "the two content data sources are the title and fulltext for websites , and the title and abstract for publications . our delicious crawl and the bibsonomy bookmark and bibsonomy bibtex snapshots provide the titles of documents . for citeulike we extracted the titles from the downloaded bibtex entries",
    "the fulltext content for delicious and bibsonomy bookmark is the page text of the bookmarked websites , which we extracted from the downloaded pages . for citeulike and bibsonomy bibtex , where the bookmarked documents are publications",
    ", we use the abstracts from the bibtex entries as the fulltext content .",
    "we pre - processed all of the datasets by casting all tags to lower case , removing duplicate tag assignments that might occur as a result of this , and removing posts which have no tags . additionally , for citeulike there are some automatically generated tags which occur very frequently . in order to clean the dataset of these tags , we removed all tag assignments where the tag equals  no - tag \" or  bibtex - import \" , or matches the regular expressions  * file - import * \" or  * import- * \" . for bibsonomy bibtex we removed all tag assignments where the tag is  jabrefnokeywordassigned \" or ",
    "myown \" since these occur with disproportional frequently and can single - handedly skew results .",
    "we use recall@@xmath115 , precision@@xmath115 and f1@@xmath115 as our success measures , where @xmath115 is the predefined number of tags to be recommended . recall measures the ratio of correct recommendations to the number of true tags of a test post , whereas precision measures the ratio of correct to false recommendations made .",
    "recall and precision are given by @xmath116 @xmath117 where @xmath118 ( true positives ) is the number of correct tags recommended , @xmath119 ( false positives ) is the number of wrong recommendations and @xmath120 ( false negatives ) is the number of true tags which were not recommended .",
    "f1 is a combination of recall and precision and is given by @xmath121 since we believe recall to be more important than precision in the context of tag recommendation , as long as @xmath115 is kept reasonably low ( @xmath12210 ) , we use recall in the evaluation phase to identify the best recommenders and configurations .",
    "we then give recall as well as f1 for the final results .      to construct a training and test set for the experiments on the full / unpruned tagging data , we use the following date - split approach for each of the datasets . the test set consist of all posts in the most recent two months of the data which provides us with a large enough test set size .",
    "the resulting numbers of test posts are 76,491 for citeulike , 1.7 m for delicious , 9,506 for bibsonomy bookmark and 2,843 for bibsonomy bibtex .",
    "the training set is a sample of the data prior to the two test months .",
    "we use a sample and not the complete historical data for our training set since the folkrank - type algorithms have a high computational complexity and expense . note that we only apply sampling for the training dataset , while in the test set all posts made in the test time - frame",
    "are included .",
    "the aim of our sampling methodology for the training set is to achieve a small enough sample size for our models to generate recommendations within a reasonable time while introducing as little bias into the models as possible . to create the training sample we start by selecting the 12 months of data prior to the test months .",
    "social tagging datasets have been shown to be time - sensitive where popular post topics as well as users interests change over time , and we believe that posts which are older than a year from the test period provide less predictive data for generating recommendations . we then take a stratified sample of documents , where the stratification is based on the number of posts that the documents appears in .",
    "finally , we retrieve all posts related to the sampled documents to create our training posts sample .",
    "this approach ensures that our training sample contains documents which are tagged frequently as well as documents which are tagged infrequently , and reduces the bias towards documents which are only tagged once that would exist if sampling the documents uniformly at random .",
    "the resulting sample has the same distribution of documents over number of posts as the full dataset .",
    "we employ this approach of first sampling documents and then retrieving the related posts since the number of documents and the resulting size of the content data is the limiting factor which impacts recommendation speed the most in our content - based approaches .",
    "moreover , documents do nt suffer from other issues that exist when sampling users or tags and then retrieving all related posts . with users ,",
    "the number of posts per user varies much more than the number of posts per document , partly due to some users using bulk imports and automatic post submission plug - ins which make them much more frequent users of the system than others . with tags ,",
    "there is also much more variance in the number of posts per tag than with documents , where the issue is that tags such as  toread \" , which hold no collaborative value have , a high number of related posts .",
    "we aim to find a sample size which strikes a good balance between improving the recommendation speed of the algorithm and reducing sample bias .",
    "we want to select a sample size at which we achieve a low variation in prediction quality for different samples of the same size , and at which the increase to a larger sample is not justified by a significant increase in prediction quality .",
    "to find an appropriate sample size , we create 5 different training samples of the same size and evaluate models built on them against the test set to find the amount of variation in prediction results .",
    "we then increase the sample size to a larger value and repeat the same process , until we are confident that the sample size gives a low variation in different samples of the same size and the move to a larger sample does not significantly increase results .",
    "we start at a sample size of 100,000 posts , and increase the number of posts by 50,000 until we are satisfied with the resulting samples .",
    "figures [ fig : sample_sizes_citeulike ] and [ fig : sample_sizes_delicious ] give the results with standard folkrank for each of the examined training sample sizes on the citelike and delicious datasets respectively .",
    "the left side shows the recall graph of each of the individual sample runs , where runs of the same sample size are plotted in the same line style .",
    "the box plot on the right gives the average recall@5 per sample size .",
    "the more we increase the sample size , the less variation in results on samples of the same size , and the improvement in average recall is also smaller . as an outcome of this process we have found that a sample size of ( roughly ) 250,000 posts gives acceptable results . for bibsonomy bookmark and bibsonomy bibtex we do not sample the training data and use all posts from the year previous to the test time - frame , as the number of posts in these datasets is sufficiently small .",
    "the statistics of the final training and test sets used in our experiments are given in table [ tab : pcore1_train_test ] .",
    "the sampling does have the effect that some of the tags in the test set of citeulike and delicious will not be present in their respective training sets , and thus can not be recommended successfully by any of the evaluated recommenders . in figure",
    "[ fig : max_possible ] we show the theoretical maximum possible recall that could be achieved on the test set of each dataset with recommending tags that exist in the training set . for citeulike and delicious",
    "the maximum recall is given for our training set sample and as well as the full training data .",
    "the theoretical maximum at @xmath115 recommended tags is calculated by assuming that for each test post @xmath123 correct tags are recommended at each value of @xmath115 , where @xmath123 is the minimum of @xmath115 and the number of true tags for the test post which also exist in the training data .",
    "the extent of the problem of not including all training data tags in the samples is not too great , and we do not believe that this will impact the validity of our conclusions as all of the evaluated recommenders will suffer from this problem to the same degree .",
    "in addition to the training - test split , we create a separate evaluation split for each dataset that we use for comparison of individual methods and for parameter tuning . the evaluation test and training sets are created from the datasets prior to the two months of real test data , in the same fashion as described above .      for completeness we also evaluate our approaches on each of the datasets at post - core level 2 .",
    "post - cores at level @xmath26 have the constraint that each of the users , document and tags has to appear in at least @xmath26 posts . for each dataset",
    ", we create the post - core by iteratively removing posts where the user , document or one of the tags does not satisfy the condition that they appear in at least two posts .",
    "we then use a leave - one - out per user split to create the training and test sets by selecting the most recent post for each user and placing it in the test set . for parameter tuning",
    "we create an additional evaluation split from the resulting training data .",
    "in our evaluation we aim to find the best combination of our proposed approaches by answering the research questions given below . in order to achieve this we run experiments on the evaluation set where we set default values for the parameters of dampening factor ( @xmath124 ) and balance in query preference weight ( @xmath125 ) .",
    "having identified the best strategies , we evaluate the remaining parameters , and finally give results on the real test set with tuned parameters in section [ sec : results ]",
    ".    * content inclusion * * how should content be included : directly into the graph at the word level or indirectly at the document level ? * * what is the most predictive source of content : document title or fulltext content ? * * how much content should be included ?",
    "* folksonomy graph model * * which of the examined graph models provides the most accurate representation of the tagging data ?",
    "* deep folksonomy graph * * is iterative weight spreading worth the computational expense ? *",
    "* does exploring the deeper folksonomy graph provide an improvement to tag predictions ?            in figure",
    "[ fig : content_inclusion ] we show the results of evaluating our two methods for including content into folkrank . on all datasets ,",
    "the indirect content inclusion method of adding similar documents to the preference vector ( simfolkrank ) gives better results than incorporating the document content directly into the graph ( contentfolkrank ) .",
    "the biggest difference is on the bibsonomy datasets , while for citeulike the results are almost identical , with simfolkrank performing slightly better .",
    "we assume that contentfolkrank gives worse results due to the word nodes in the graph being connected to many more tags compared to the document nodes in the standard folksonomy graph used by simfolkrank .",
    "the same individual word can appear in a variety of documents from different domains and thus be connected to many tags which are themselves unrelated . to accurately capture the query document several words are required in combination .",
    "the predictions generated by contentfolkrank can be influenced by the edge configuration of individual words , which might be most connected to tags from a different domain than the query document whilst being connected to appropriate tags with less edge weight . in simfolkrank ,",
    "the similarities to training documents are calculated based on the whole representation of the query document , and in the graph each of the similar documents is likely to be connected to tags from one or a few domains only . in the larger datasets of citeulike and delicious",
    "the difference between the two approaches is smaller .",
    "contentfolkrank comes close in results to simfolkrank , but does not outperform it .",
    "this suggest that with more data the weighting methods used in contentfolkrank , which are based on tf - idf scores and include a co - occurrence element , can more accurately model the query document as well as the edge weights of words in the graph . with sufficient data",
    "the outcome of contentfolkrank is thus very similar to simfolkrank .",
    "however , in addition to producing better results simfolkrank is also computationally less expensive than contentfolkrank since the contentfolkrank graph is much larger due to the many word nodes .",
    "when comparing the title and fulltext content of documents as potential document representations ( figure [ fig : content_sources ] ) , the title performs better in most cases .",
    "the biggest difference is on the delicious and bibsonomy bookmark datasets , as here the fulltext content is the crawled page content of the bookmarked websites . in citeulike and bibsonomy bibtex",
    ", the fulltext representation is given by the abstract of the bookmarked research papers which we expect to be a more accurate document description .",
    "on bibsonomy bibtex , the fulltext content actually performs slightly better than the title .          to evaluate the amount of content to be included we vary the number of similar documents in the preference vector of simfolkrank and give the results in figure [ fig : simfolkrank_numsimdocs ] .",
    "the content source in these experiments is the document title .",
    "the x - axis gives the number of most similar documents included in the preference vector and the y - axis is recall when recommending five tags .",
    "the left - most point and the horizontal line in each graph gives the recall@5 without including content .",
    "the results indicate that prediction results improve the more content is added , where the biggest gain is achieved by the most similar documents .",
    "the only exception is bibsonomy bibtex where including content does not give a significant gain . except for bibsonomy bibtex",
    ", the shape of the plots and the fact that the results do not decrease at higher numbers of similar documents also confirms that normalised cosine similarity is an appropriate metric for measuring document similarity in our scenario .                before comparing the graph construction methods we first evaluate the two alternative scores retrieval methods of the post graph model described in section [ sec : postrank ] .",
    "the approach of retrieving post node weights from the graph and then calculating the tag scores based on these gives slightly better results than retrieving the tag node weights directly from the graph , although it does not seem to make a significant difference .",
    "since it also makes sense that the number of tags in each post should not influence the scores of the tags they contain , we use the strategy of calculating tag scores from post nodes for all approaches using the post graph model in the subsequent experiments .",
    "as shown in figure [ fig : graph_construction ] , without content data there is no real difference in results with the different models , and the folksonomy graph ( folkrank ) , adapted graph ( folkrank_ag ) and post graph ( folkrank_pg ) give almost identical results .",
    "however , when including content data , the post graph model performs consistently better than the folksonomy graph , indicated by simfolkrank_pg performing better than simfolkrank across all datasets .",
    "we believe the improved results to be due to the more accurate data representation of the post graph model , as discussed in section [ sec : graph_construction ] . with more nodes in the preference vector ,",
    "the implicit assumptions of the folksonomy model have a relatively greater impact on tag predictions scores and the post graph proves to be the more robust model .",
    "we compare the iterative spreading algorithm of folkrank to our pathrank weight spreading approach on the folksonomy graph ( figure [ fig : weight_spreading_folkrank ] ) and the post graph model ( figure [ fig : weight_spreading_postrank ] ) .",
    "the two weight spreading methods produce very similar results on both models across all datasets .",
    "however , pathrank is a much quicker weight spreading algorithm . it does not adjust the weight of each node in several iterations to find the optimal distribution of weights reflecting the overall edge connections in the graph .",
    "in other words , it does not consider the general ( non - personal ) importance weight of nodes which is implied by the graph structure itself .",
    "this suggests that the impact of the general importance ( or authority ) of nodes in the graph does not provide a significant benefit to the tag predictions , and the expensive iterative spreading of the non - personalised weights can be omitted to speed up the recommendation process .",
    "our evaluation of the dampening factor in section [ sec : tuning_d ] further confirms this conclusion as the best results with folkrank s iterative weight spreading are achieved at the lowest setting for @xmath10 , which translates to giving the least relevance to general importance weights .              to examine the value of including the general node weights in the recommendation process",
    ", we evaluate different settings for the dampening factor @xmath10 and give our results in figure [ fig : parameter_tuning_d ] . without the inclusion of content data",
    "there is not much impact on the results for the examined values of @xmath10 .",
    "this is because without content the whole preference weight is given to a maximum of two preference nodes , the query user and document , which means that there is a huge difference in weight between the preference nodes and any one of the other nodes in the graph .",
    "non - preference nodes , and thus general importance weights , do nt have a chance to impact the predictions except for extreme values of @xmath10 such as 0.9 , at which setting we observe a very slight decrease in results . with content data the preference weight is distributed among a maximum of 101 preference nodes , which include the query user and potentially 100 training documents similar to the query document . here",
    "the impact of the general non - personalised weights can be observed at lower values of @xmath10 . in all cases ,",
    "the best results are achieved with setting @xmath10 to the lowest examined value of 0.1 .",
    "this indicates that the general weights in the graph do not provide a benefit to the accuracy of tag predictions , and in fact have a negative impact when given too much relevance .",
    "we conclude that to maximise the tag prediction accuracy , @xmath10 should be set to the lowest value , in effect ignoring the general / non - personalised weights of nodes in the graph . with the lowest examined setting of @xmath126 , the general weights can still act as tie - breakers for tags in the candidate set which have otherwise equal personalised weights .",
    "however , our results in the comparison with pathrank weight spreading , which does not utilise general weights , suggest that there is no significant improvement over randomly ranking tags which have equal weights . this comparison is made in the previous section [ sec : eval_iterative_vs_pathrank ] and in the evaluation on the real test set with tuned parameters in section [ sec : results ] .",
    "on recall@5 ]      the parameter of maximum path length @xmath77 in our pathrank weight spreading approach is especially interesting since it allows us to examine the value of exploring the graph beyond the immediate neighbourhood of the query user and nodes related to the query document .",
    "we show the outcome of setting different values of @xmath77 in figure [ fig : parameter_tuning_pl ] on the folksonomy graph and post graph models .",
    "the x - axis gives the value of @xmath77 and the y - axis is recall@5 . with the lowest setting of @xmath77 only the immediate neighbourhood",
    "is explored , where as we move to the right of the x - axis longer paths are also traversed by the weight spreading algorithm . with the post graph model we retrieve tag scores as the sum of weights of post nodes they are connected to . here",
    ", the next posts and thus additional tags beyond the immediate neighbourhood ( of path length 1 ) are encountered at a path length of 3 .",
    "overall , our results suggest that there is actually not much value in considering the graph beyond the immediate neighbourhood of the preference nodes .",
    "there is a small difference that can be observed between path lengths 1 and 3 which we explore in detail below . in general , the conclusion that no significant increase is achieved is in line with our previously published results @xcite , where our less expensive co - occurrence recommender ( exploring only the immediate neighbourhood of the query user and document ) performed equally well to folkrank .",
    "moreover , with the pathrank weight spreading algorithm , we have now removed the other influences on the weight spreading calculation which could have cascaded or reduced the impact of the deeper graph . even without swash - back , triangle - spreading of weights and general importance scores , the weights spread through long paths in the deep graph do not provide a significant improvement .",
    "the results indicate that the deeper graph does not provide a beneficial re - ranking of existing candidate tags in the immediate user or document neighbourhood . with the setting of @xmath127 where only the immediate neighbourhood is considered",
    ", tag nodes which have equal weight will be ranked randomly in the final predictions .",
    "utilising the deep graph to re - rank these tags does not significantly improve results over this random ranking .",
    "[ sec : tuning_pl ]    ]    our first detailed observation is that after the first influence of the deeper graph at path length 3 , we can not observe any significant impact , positive or negative , caused by exploring longer paths . along the lines of @xcite",
    ", this suggest that users of ( broad ) folksonomies have a highly personal tagging behaviour .",
    "it is thus very difficult to traverse more than a few edges in the graph and still weigh the encountered nodes in a manner relevant to the the preference node at which the path started .",
    "the only small change that can be observed is up to a path length of three . as a side note",
    ", the post graph model gives better results than the folksonomy graph at a path length of one . at this setting",
    "the only difference in the tag scores calculation between the two models is that for the post graph the tag scores are given as the sum of weights of post nodes they are connected to . as discussed in section [ sec : graph_construction ] , this follows from the post graph model s assumption that the number of tags of each post should not influence tag scores , whereas the plain folksonomy graph assumes that if there are many tags in a post then each of them is less important .",
    "this again suggests that the assumptions made by the post graph model provide a more accurate representation of the underlying social bookmarking data .    another interesting observation in figure [ fig : parameter_tuning_pl ]",
    "can be made from the results with the folksonomy graph model on the delicious dataset . in this case",
    "there is a small improvement at a path length of 3 .",
    "what is interesting here is that the increase does not occur at @xmath128 but at @xmath129 . in the folksonomy graph , the tags found at a path length of 2",
    "have paths of the form @xmath130 or @xmath131 from the user preference node @xmath132 or the document s preference node(s ) @xmath133 respectively . including these additional tags is conceptually similar to tag expansion via the document or user nodes related to the preference node . at a path length of 3 , paths of the form @xmath134 and @xmath135",
    "are also included which is conceptually similar to performing tag expansion by using tag - tag co - occurence measure .",
    "the small improvement in prediction accuracy seems to be due to using tag - tag co - occurence , rather than giving weight to tags which are related to non - tag nodes from the preference node s immediate neighbourhood . on the bibsonomy bookmark dataset",
    "we can observe a small decrease at @xmath128 when including content with simpathrank . with the post graph model and content ( simpathrank_pg )",
    ", there is also a decrease on bibsonomy as well as citeulike when going from @xmath127 to @xmath129 .",
    "as there are no paths with length 2 leading to additional tags in the post graph , the influence of tag expansion both via non - tag nodes and tag - tag co - occurrence is included at the same time at @xmath129 .",
    "it seems to be the case that tag expansion via non - tag nodes decreases results . along the lines of our discussion in section [ sec : weight_spreading_discussion ] , this seems to suggest that tags found related to non - tag nodes of the preference node but not directly connected to the preference node itself should not be given an increased weight . as they seem to worsen results it might be appropriate to decrease their weight instead .",
    "this suggest a potential that negative feedback could be extracted via a more complex analysis of the graph , which we intend to investigate in the future .",
    "overall , we conclude that spreading weight into the deeper graph does not provide a significant benefit to tag recommendations and can in some cases even harm prediction scores . the only increase in scores is given by spreading weight from tags to further tag nodes , essentially performing a tag set expansion via tag - tag co - occurrence .",
    "given the complete graph model this is very difficult to separate from expanding the tag set via non - tag nodes , which seems to decrease prediction accuracy . to still utilise the tag - tag co - occurrence data we believe that separate approaches which directly model the tag - tag relationships would be more appropriate and produce better results",
    "however , even though the assumptions made by conventional positive - reinforcement weight spreading methods do not seem to hold for the social bookmarking domain , some useful information could potentially be gained from the deep folksonomy graph by different approaches .",
    "a rule - driven analysis of small subsections of the graph could be used to make deductions about implied negative feedback , to either aid the recommendation process directly or to improve the accuracy of a tag - tag similarity metric by including negative scores .         between query user and query document ]    in figure [ fig : parameter_tuning_b ] we present the results for different settings of @xmath40 , which determines the balance in preference weight between the query user and the query document .",
    "once again there is not much difference in results without including content data ( folkrank , folkrank_pg , pathrank_pg ) .",
    "since most of the query documents in the test sets are new , the preference vector without content will only include the query user in the majority of cases .",
    "for the cases where the document does exist in the graph , and thus will be included in the preference vector , each of the tags connected to the query document will usually receive more weight than each of the tags connected to the query user since users are usually connected to many more tags than documents are .",
    "the tags connected to the query user only have a chance to outweigh the tags connected to the query document for high values of @xmath40 , at which settings we see a slight decrease in results .",
    "however , with content data ( simfolkrank , simfolkrank_pg , simpathrank_pg ) the preference vector contains the query user as well as several documents related to the query document and we can clearly observe the impact of @xmath40 .",
    "the results confirm that there is value in introducing the parameter @xmath40 to explicitly set this balance instead of using the strategy of the original folkrank algorithm of setting @xmath46 , which results in values lower than 0.1 for all of the datasets except delicious where it would be 0.2 .",
    "the best results are achieved with setting @xmath40 to 0.5 for citeulike , 0.3 for delicious , and 0.6 for both bibsonomy bookmark and bibsonomy bibtex .      here",
    "we present our final results with our best approaches and with tuned parameters on the test set of each of the datasets .",
    "the content source in all of the content - aware approaches is the document title . for approaches using folkrank s iterative weight spreading the dampening factor",
    "is set to @xmath126 , and for approaches using pathrank the maximum path length is set to @xmath127 .",
    "the balance @xmath40 in preference weight is set per dataset to the best value that was found in the parameter tuning runs .",
    "figures [ fig : results_test_set ] and [ fig : results_test_set ] show the recall and f1 respectively , on the test set for each of the datasets with tuned parameters . including content into the recommendation process",
    "provides a significant increase in results .",
    "the results on the test set are in line with our previous conclusions on the evaluation set .",
    "simfolkrank_pg produces better results than simfolkrank over all datasets , suggesting that the post graph is a more accurate model of the tagging data than the folksonomy graph .",
    "furthermore , pathrank_pg and simpathrank_pg give almost equivalent results to folkrank_pg and simfolkrank_pg respectively which suggests that the iterative computation and general importance weights in folkrank s weight spreading approach do not provide a significant benefit to tag predictions . while producing comparable results , the pathrank weight spreading method is much less computationally expensive .",
    "furthermore , the results with simpathrank_pg , which is among the best recommenders across all datasets , are achieved with a parameter setting of @xmath127 . at this setting only the immediate neighbourhood of preference nodes is considered .",
    "none of the approaches improve results by utilising the deep graph over simpathrank_pg with @xmath127 which is essentially a user - tag and document - tag co - occurrence recommender at this setting . the results on bibsonomy bookmark without including content data are due to the fact that a large portion of the test posts in bibsonomy bookmark contain new users as well as new documents .",
    "for these test posts the algorithms which do nt include content data ( folkrank , folkrank_pg and pathrank_pg ) default to recommending the overall highly - ranked tags in the graph without personalisation .",
    "the three approaches have different rankings for the top three tags in the general recommendations which leads to the results shown .                      for completeness",
    ", we show the recall and f1 for each of the datasets post - cores at level 2 in figures [ fig : results_test_set_pc2 ] and [ fig : results_test_set_pc2_f1 ] respectively .",
    "the results with all methods are very similar except on the smaller bibsonomy datasets . however , there is no improvement with including content data .",
    "in fact , including content data makes results worse in most cases .",
    "since ( almost ) all of the documents in the test set for post - cores also exist in the training data , they all have previously assigned tags available which can be recommended .",
    "there is no need to additionally include similar documents in the preference vector as well since the exact query document exists in the graph . adding the content in this case has a negative effect on tag predictions",
    "this is an interesting result and suggests that the best strategy for the future might be to only include the content if the query document does not exist in the training data , for experiments on post - core 2 as well as unpruned datasets .",
    "in this paper we have presented novel adaptations and extensions to folkrank and conducted an in - depth analysis of the accuracy of the folksonomy graph model , the iterative weight spreading algorithm of folkrank and the value of exploring the deep folksonomy graph .",
    "the extension of folkrank with content data resulted in a significant increase in tag recommendation accuracy and addressed the new item problem in tag recommendation , as well as providing further insight into the folkrank algorithm .",
    "as part of our examination of the folksonomy graph structure , we have proposed an improved model which captures the tagging data more accurately and produces better tag recommendation results . in our analysis of the iterative weight spreading method of folkrank , we have shown that the general un - personalised node weights do not provide a positive impact on tag recommendations , and if given too much relevance hurt the accuracy of the algorithm . since the general node weights are one of the main reasons for folkrank s high complexity , we think it is an important finding that they can be safely omitted .",
    "furthermore , we have shown that a simpler weight spreading algorithm , pathrank , which works in a similar manner to breadth - first search , produces comparable results to the much more complex iterative weight spreading algorithm employed by folkrank while being computationally less expensive .",
    "the most intriguing result of our analysis was that even though both folkrank s iterative weight spreading and our simpler pathrank spreading algorithm have the potential to utilise the deep folksonomy graph , they do not benefit from doing so in practice .",
    "moreover , we have presented an in - depth discussion as well as a direct evaluation of the value of exploring the deep folksonomy graph .",
    "we conclude that exploring the graph beyond the immediate neighbourhood of the query nodes with conventional weight spreading methods does not provide a significant increase in tag recommendation accuracy and can in some cases even hurt recommendations .",
    "the assumption that closeness in the graph always implies a positive relationship does not hold beyond the immediate neighbourhood of nodes in social tagging graphs .",
    "this suggests that the foundation of graph - based recommenders ( and to a lesser extent collaborative filtering ) , which are traditionally applied to two - dimensional datasets , does not apply to the three - dimensional user - document - tag relationships found in social tagging data .",
    "in summary our main conclusions are as follows .",
    "* content inclusion in tag recommendation * * including content into the recommendation process addresses the new document problem and significantly increases results on full / unpruned datasets . * * the title of documents is a better content source and provides a more accurate description of documents than the fulltext content . *",
    "* including content at the document level produces a more accurate recommender than including content at the word level and constructing user - word and word - tag relationships , especially for smaller sized social tagging datasets .",
    "* folksonomy graph model * * explicitly including post - membership information into the graph provides a model which makes more accurate assumptions about the relationships in the tagging data and produces improved results over the traditional folksonomy model .    * deep graph exploration * * general importance / authority scores , which make iterative weight spreading computationally expensive , do not provide an improvement to the accuracy of tag recommendations and can be omitted to reduce complexity . *",
    "* the expensive exploration of the deep tagging data graph with conventional weight spreading methods does not provide an improvement to tag recommendations and can in some cases decrease results . * * the assumption that closeness in the graph always implies a positive relationship does not hold in social tagging datasets beyond the immediate neighbourhood of nodes .    in the future we plan to further explore methods to leverage the potential benefit of including the information contained in the deep folksonomy graph for tag recommendation .",
    "we think that by using rule - based methods which analyse smaller subgraphs of the folksonomy , implicit negative feedback could be extracted . this could be used to include negative scores in user - tag and especially document - tag relationships in order to reduce the scores of tags which are likely to be incorrect for a specific user or document .",
    "moreover , the negative feedback could be incorporated into tag - tag similarity measures to make these more accurate .",
    "another interesting research direction are the sampling methods used in tag recommendation . as social bookmarking websites and tagging datasets",
    "get larger , it is becoming infeasible to build models on and analyse all of the training data , especially with methods which examine complex relationships in the data .",
    "we plan to further explore this problem and evaluate different sampling methods in their ability to produce unbiased and predictive samples of training data .",
    "gemmell2009 jonathan gemmell , thomas schimoler , maryam ramezani , and bamshad mobasher . 2009 . .",
    "in _ proceedings of the 7th workshop on intelligent techniques for web personalization and recommender systems_.    heymann2008 paul heymann , daniel ramage , and hector garcia - molina . 2008 . .",
    "in _ sigir 08 : proceedings of the 31st annual international acm sigir conference on research and development in information retrieval_. acm , new york , ny , usa , 531538 .",
    "andreas hotho , robert jschke , christoph schmitz , and gerd stumme .",
    "2006 . . in _ the semantic web : research and applications",
    "_ ( lecture notes in computer science ) _ , vol . 4011 .",
    "springer , 411426 .",
    "jaeschke2007 robert jschke , leandro  balby marinho , andreas hotho , lars schmidt - thieme , and gerd stumme .",
    "in _ knowledge discovery in databases : pkdd 2007 , 11th european conference on principles and practice of knowledge discovery in databases_. 506514 .",
    "landia2012 nikolas landia , sarabjot  singh anand , andreas hotho , robert jschke , stephan doerfel , and folke mitzlaff .",
    "2012 . . in _ proceedings of the 4th acm recsys workshop on recommender systems and the social web",
    "_ ( rsweb 12)_. acm , new york , ny , usa , 18 .          liu2009 feifan liu , deana pennell , fei liu , and yang liu . 2009 . .",
    "naacl 09 : proceedings of human language technologies : the 2009 annual conference of the north american chapter of the association for computational linguistics_. association for computational linguistics , morristown , nj , usa , 620628 .",
    "rendle2009 steffen rendle , leandro balby  marinho , alexandros nanopoulos , and lars schmidt - thieme .",
    "2009 . . in _ proceedings of the 15th acm",
    "sigkdd international conference on knowledge discovery and data mining _ _",
    "( kdd 09)_. new york , ny , usa , 727736 .",
    "renz2003 ingrid renz , andrea ficzay , and holger hitzler .",
    "nldb 2003 : natural language processing and information systems , 8th international conference on applications of natural language to information systems , june 2003 , burg ( spreewald ) , germany_. 228234 .",
    "song2008 yang song , ziming zhuang , huajing li , qiankun zhao , jia li , wang - chien lee , and c.  lee giles .",
    "in _ sigir 08 : proceedings of the 31st annual international acm sigir conference on research and development in information retrieval_. 515522 .",
    "symeonidis2008 panagiotis symeonidis , alexandros nanopoulos , and yannis manolopoulos .",
    "2008 . . in _ proceedings of the 2008 acm conference on recommender systems _",
    "_ ( recsys 08)_. acm , new york , ny , usa , 4350 .",
    "wetzker2010 robert wetzker , carsten zimmermann , christian bauckhage , and sahin albayrak . 2010 . .",
    "in _ proceedings of the third acm international conference on web search and data mining _ _ ( wsdm 10)_. acm , new york , ny , usa , 7180 .    witten1999 ian  h. witten , gordon  w. paynter , eibe frank , carl gutwin , and craig  g. nevill - manning .",
    "1999 . . in _ proceedings of the 4th acm conference on digital libraries , august 11 - 14 , 1999 , berkeley , ca , usa_. acm , 254255"
  ],
  "abstract_text": [
    "<S> the information contained in social tagging systems is often modelled as a graph of connections between users , items and tags . </S>",
    "<S> recommendation algorithms such as folkrank , have the potential to leverage complex relationships in the data , corresponding to multiple hops in the graph . </S>",
    "<S> we present an in - depth analysis and evaluation of graph models for social tagging data and propose novel adaptations and extensions of folkrank to improve tag recommendations . </S>",
    "<S> we highlight implicit assumptions made by the widely used folksonomy model , and propose an alternative and more accurate graph - representation of the data . </S>",
    "<S> our extensions of folkrank address the new item problem by incorporating content data into the algorithm , and significantly improve prediction results on unpruned datasets . </S>",
    "<S> our adaptations address issues in the iterative weight spreading calculation that potentially hinder folkrank s ability to leverage the deep graph as an information source . </S>",
    "<S> moreover , we evaluate the benefit of considering each deeper level of the graph , and present important insights regarding the characteristics of social tagging data in general . </S>",
    "<S> our results suggest that the base assumption made by conventional weight propagation methods , that closeness in the graph always implies a positive relationship , does not hold for the social tagging domain .    </S>",
    "<S> [ information filtering ]    author s addresses : n. landia ( n.landia@warwick.ac.uk ) , s. s. anand ( sarabjot.singh@gmail.com ) and n. griffiths ( nathan.griffiths@warwick.ac.uk ) , department of computer science , university of warwick , coventry cv4 7al , uk ; s. doerfel ( doerfel@cs.uni-kassel.de ) , faculty of electrical engineering and computer science , university of kassel , wilhelmshher allee 73 , 34121 kassel , germany ; r. jschke ( jaeschke@l3s.de ) , l3s research center , appelstrasse 9a , 30167 </S>",
    "<S> hannover , germany ; a. hotho ( hotho@informatik.uni-wuerzburg.de ) , department of computer science , university of wrzburg , am hubland , wrzburg , germany . </S>"
  ]
}