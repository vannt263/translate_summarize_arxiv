{
  "article_text": [
    "analysis of complex high dimensional data is an exploding area of research , with applications in diverse fields , such as machine learning , statistical data analysis , bio - informatics , meteorology , chemistry and physics . in the first three application fields ,",
    "the underlying assumption is that the data is sampled from some unknown probability distribution , typically without any notion of time or correlation between consecutive samples .",
    "important tasks are dimensionality reduction , e.g. , the representation of the high dimensional data with only a few coordinates , and the study of the geometry and statistics of the data , its possible decomposition into clusters , etc @xcite .",
    "in addition , there are many problems concerning supervised learning , in which additional information , such as a discrete class @xmath3 or a continuous function value @xmath4 is given to some of the data points . in this paper",
    "we are concerned only with the unsupervised case , although some of the methods and ideas presented can be applied to the supervised or semi - supervised case as well @xcite .    in the later three above - mentioned application fields the data is typically sampled from a complex biological , chemical or physical _ dynamical _ system ,",
    "in which there is an inherent notion of time .",
    "many of these systems involve multiple time and length scales , and in many interesting cases there is a separation of time scales , that is , there are only a few `` slow '' time scales at which the system performs conformational changes from one meta - stable state to another , with many additional fast time scales at which the system performs local fluctuations within these meta - stable states . in the case of macromolecules",
    "the slow time scale is that of a conformational change , while the fast time scales are governed by the chaotic rotations and vibrations of the individual chemical bonds between the different atoms of the molecule , as well as the random fluctuations due to the frequent collisions with the surrounding solvent water molecules . in the more general case of interacting particle systems ,",
    "the fast time scales are those of density fluctuations around the mean density profiles , while the slow time scales correspond to the time evolution of these mean density profiles .",
    "although on the fine time and length scales the full description of such systems requires a high dimensional space , e.g. the locations ( and velocities ) of all the different particles , these systems typically have an intrinsic low dimensionality on coarser length and time scales .",
    "thus , the coarse time evolution of the high dimensional system can be described by only a few dynamically relevant variables , typically called reaction coordinates .",
    "important tasks in such systems are the reduction of the dimensionality at these coarser scales ( known as homogenization ) , and the efficient representation of the complicated linear or non - linear operators that govern their ( coarse grained ) time evolution .",
    "additional goals are the identification of the meta - stable states , the characterization of the transitions between them and the efficient computation of mean exit times , potentials of mean force and effective diffusion coefficients @xcite .    in this paper , following @xcite , we consider a family of diffusion maps for the analysis of these problems .",
    "given a large dataset , we construct a family of random walk processes based on isotropic and anisotropic diffusion kernels and study their first few eigenvalues and eigenvectors ( principal components ) . the key point in our analysis",
    "is that these eigenvectors and eigenvalues capture important geometrical and statistical information regarding the structure of the underlying datasets .",
    "it is interesting to note that similar approaches have been suggested in various different fields . in graph theory",
    ", the first few eigenvectors of the normalized graph laplacian have been used for spectral clustering @xcite , approximations to the optimal normalized - cut problem @xcite and dimensionality reduction @xcite , to name just a few .",
    "similar constructions have also been used for the clustering and identification of meta - stable states for datasets sampled from dynamical systems @xcite .",
    "however , it seems that the connection of these computed eigenvectors to the underlying geometry and probability density of the dataset has not been fully explored .    in this paper",
    ", we consider the connection of these eigenvalues and eigenvectors to the underlying geometry and probability density distribution of the dataset . to this end , we assume that the data is sampled from some ( unknown ) probability distribution , and view the eigenvectors computed on the finite dataset as discrete approximations of corresponding eigenfunctions of suitably defined continuum operators in an infinite population setting . as the number of samples goes to infinity , the discrete random walk on the set converges to a diffusion process defined on the @xmath5-dimensional space but with a non - uniform probability density . by explicitly studying the asymptotic form of the chapman - kolmogorov equations in this setting ( e.g. , the infinitesimal generators ) , we find that for data sampled from a general probability distribution , written in boltzmann form as @xmath6 , the eigenfunctions and eigenvalues of the standard normalized graph laplacian construction correspond to a diffusion process with a potential @xmath7 ( instead of a single @xmath2 ) .",
    "therefore , a subset of the first few eigenfunctions are indeed well suited for spectral clustering of data that contains only a few well separated clusters , corresponding to deep wells in the potential @xmath2 .    motivated by the well known connection between diffusion processes and schrdinger operators @xcite , we propose a different novel non - isotropic construction of a random walk on the graph , that in the asymptotic limit of infinite data recovers the eigenvalues and eigenfunctions of a diffusion process with the same potential @xmath2 .",
    "this normalization , therefore , is most suited for the study of the long time behavior of complex dynamical systems that evolve in time according to a stochastic differential equation .",
    "for example , in the case of a dynamical system driven by a bistable potential with two wells , ( e.g. with one slow time scale for the transition between the wells and many fast time scales ) the second eigenfunction can serve as a parametrization of the reaction coordinate between the two states , much in analogy to its use for the construction of an approximation to the optimal normalized cut for graph segmentation .",
    "for the analysis of dynamical systems , we also propose to use a subset of the first few eigenfunctions as reaction coordinates for the design of fast simulations .",
    "the main idea is that once a parametrization of dynamically meaningful reaction coordinates is known , and lifting and projection operators between the original space and the diffusion map are available , detailed simulations can be initialized at different locations on the reaction path and run only for short times , to estimate the transition probabilities to different nearby locations in the reaction coordinate space , thus efficiently constructing a potential of mean field and an efficient diffusion coefficient on the reaction path @xcite .",
    "finally , we describe yet another random walk construction that in the limit of infinite data recovers the laplace - beltrami ( heat ) operator on the manifold on which the data resides , regardless of the possibly non - uniform sampling of points on the manifold .",
    "this normalization is therefore best suited for learning the geometry of the dataset , as it separates the geometry of the manifold from the statistics on it .",
    "our analysis thus reveals the intimate connection between the eigenvalues and eigenfunctions of different random walks on the finite graph to the underlying geometry and probability distribution @xmath8 from which the dataset was sampled .",
    "these findings lead to a better understanding of the advantages and limitations of diffusion maps as a tool to solve different tasks in the analysis of high dimensional data .",
    "consider a finite dataset @xmath9 .",
    "we consider two different possible scenarios for the origin of the data . in the first scenario ,",
    "the data is not necessarily derived from a dynamical system , but rather it is randomly sampled from some arbitrary probability distribution @xmath10 . in this case",
    "we define an associated potential @xmath11 so that @xmath12 .    in the second scenario , we assume that the data is sampled from a dynamical system in equilibrium .",
    "we further assume that the dynamical system , defined by its state @xmath13 at time @xmath14 , satisfies the following generic stochastic differential equation ( sde ) @xmath15 where a dot on a variable means differentiation with respect to time , @xmath16 is the free energy at @xmath17 ( which , with some abuse of nomenclature , we will also call the potential at @xmath17 ) , and @xmath18 is an @xmath5-dimensional brownian motion process . in this case",
    "there is an explicit notion of time , and the transition probability density @xmath19 of finding the system at location @xmath17 at time @xmath14 , given an initial location @xmath20 at time @xmath21 ( @xmath22 ) , satisfies the forward fokker - planck equation ( fpe ) @xcite @xmath23 with initial condition @xmath24 similarly , the backward fokker - planck equation for the density @xmath25 , in the backward variables @xmath26 ( @xmath27 ) is @xmath28 where differentiations in ( [ backward_fpe ] ) are with respect to the variable @xmath20 , and the laplacian @xmath29 is a negative operator , defined as @xmath30 .    as time",
    "@xmath31 the steady state solution of ( [ fpe ] ) is given by the equilibrium boltzmann probability density , @xmath32 where @xmath33 is a normalization constant ( known as the partition function in statistical physics ) , given by @xmath34 in what follows we assume that the potential @xmath2 is shifted by the suitable constant ( which does not change the sde ( [ sde ] ) ) , so that @xmath35",
    ". also , we use the notation @xmath36 interchangeably to denote the ( invariant ) probability measure on the space .    note that in both scenarios , the steady state probability density , given by ( [ mu_x ] ) is identical .",
    "therefore , for the purpose of our initial analysis , which does not directly take into account the possible time dependence of the data , it is only the features of the underlying potential @xmath2 that come into play .    the langevin equation ( [ sde ] ) or the corresponding fokker - planck equation ( [ fpe ] ) are commonly used to describe mechanical , physical , chemical , or biological systems driven by noise .",
    "the study of their behavior , and specifically the decay to equilibrium has been the subject of much theoretical research @xcite . in general , the solution of the fokker - planck equation ( [ fpe ] ) can be written in terms of an eigenfunction expansion @xmath37 where @xmath38 are the eigenvalues of the fp operator , with @xmath39 , @xmath40 are their corresponding eigenfunctions , and the coefficients @xmath41 depend on the initial conditions .",
    "obviously , the long term behavior of the system is governed only by the first few eigenfunctions @xmath42 , where @xmath43 is typically small and depends on the time scale of interest . in low dimensions ,",
    "e.g. @xmath44 for example , it is possible to calculate approximations to these eigenfunctions via numerical solutions of the relevant partial differential equations . in high dimensions",
    ", however , this approach is in general infeasible and one typically resorts to simulations of trajectories of the corresponding sde ( [ sde ] ) . in this case , there is a need to employ statistical methods to analyze the simulated trajectories , identify the slow variables , the meta - stable states , the reaction pathways connecting them and the mean transition times between them .",
    "let @xmath45 , denote the @xmath46 samples , either merged from many different simulations of the stochastic equation ( [ sde ] ) , or simply given without an underlying dynamical system . in @xcite , coifman and",
    "lafon suggested the following method , based on the definition of a markov chain on the data , for the analysis of the geometry of general datasets :    for a fixed value of @xmath47 ( a metaparameter of the algorithm ) , define an isotropic diffusion kernel , @xmath48 assume that the transition probability between points @xmath49 and @xmath50 is proportional to @xmath51 , and construct an @xmath52 markov matrix , as follows @xmath53 where @xmath54 is the required normalization constant , given by @xmath55 for large enough values of @xmath47 the markov matrix @xmath56 is fully connected ( in the numerical sense ) and therefore has an eigenvalue @xmath57 with multiplicity one and a sequence of additional @xmath58 non - increasing eigenvalues @xmath59 , with corresponding eigenvectors @xmath60 .",
    "the diffusion map at time @xmath61 is defined as the mapping from @xmath17 to the vector @xmath62 for some small value of @xmath43 . in @xcite",
    ", it was demonstrated that this mapping gives a low dimensional parametrization of the geometry and density of the data . in the field of data analysis ,",
    "this construction is known as the _ normalized graph laplacian_. in @xcite , shi and malik suggested using the first non - trivial eigenvector to compute an approximation to the optimal normalized cut of a graph , while the first few eigenvectors were suggested by weiss et al .",
    "@xcite for clustering .",
    "similar constructions , falling under the general term of kernel methods have been used in the machine learning community for classification and regression @xcite . in this paper",
    "we elucidate the connection between this construction and the underlying potential @xmath2 .      to analyze the eigenvalues and eigenvectors of the normalized graph laplacian",
    ", we consider them as a finite approximation of a suitably defined diffusion operator acting on the continuum probability space from which the data was sampled .",
    "we thus consider the limit of the above markov chain process as the number of samples approaches infinity . for a finite value of @xmath47",
    ", the markov chain in discrete time and space converges to a markov process in discrete time but continuous space .",
    "then , in the limit @xmath63 , this jump process converges to a diffusion process on @xmath64 , whose local transition probability depends on the non - uniform probability measure @xmath65 .",
    "we first consider the case of a fixed @xmath66 , and take @xmath67 .",
    "using the similarity of ( [ k_epsilon ] ) to the diffusion kernel , we view @xmath47 as a measure of time and consider a discrete jump process at time intervals @xmath68 , with a transition probability between points @xmath20 and @xmath17 proportional to @xmath69 .",
    "however , since the density of points is not uniform but rather given by the measure @xmath70 , we define an associated normalization factor @xmath71 as follows , @xmath72 and a forward transition probability @xmath73 equations ( [ p_ve ] ) and ( [ m_f ] ) are the continuous analogues of the discrete equations ( [ p_ve_discrete ] ) and ( [ m_discrete ] ) . for future use",
    ", we also define a symmetric kernel @xmath74 as follows , @xmath75 note that @xmath76 is an estimate of the local probability density at @xmath17 , computed by averaging the density in a neighborhood of radius @xmath77 around @xmath17 . indeed , as @xmath78",
    ", we have that @xmath79    we now define forward , backward and symmetric chapman - kolmogorov operators on functions defined on this probability space , as follows , @xmath80 ( { { \\mbox{\\boldmath$x$ } } } ) = \\int m_f ( { { \\mbox{\\boldmath$x$ } } } | { { \\mbox{\\boldmath$y$ } } } ) \\varphi ( { { \\mbox{\\boldmath$y$ } } } ) d\\mu ( { { \\mbox{\\boldmath$y$ } } } ) \\ ] ] @xmath81 ( { { \\mbox{\\boldmath$x$ } } } ) = \\int m_f ( { { \\mbox{\\boldmath$y$ } } } | { { \\mbox{\\boldmath$x$ } } } ) \\varphi ( { { \\mbox{\\boldmath$y$ } } } ) d\\mu ( { { \\mbox{\\boldmath$y$ } } } ) \\ ] ] and @xmath82 ( { { \\mbox{\\boldmath$x$ } } } ) = \\int m_s ( { { \\mbox{\\boldmath$x$ } } } , { { \\mbox{\\boldmath$y$ } } } ) \\varphi ( { { \\mbox{\\boldmath$y$ } } } ) d\\mu ( { { \\mbox{\\boldmath$y$ } } } ) \\ ] ] if @xmath83 is the probability of finding the system at location @xmath17 at time @xmath84 , then @xmath85 $ ] is the evolution of this probability to time @xmath86 .",
    "similarly , if @xmath87 is some function on the space , then @xmath88 ( { { \\mbox{\\boldmath$x$ } } } ) $ ] is the mean ( average ) value of that function at time @xmath47 for a random walk that started at @xmath17 , and so @xmath89 ( { { \\mbox{\\boldmath$x$ } } } ) $ ] is the average value of the function at time @xmath90 .    by definition , the operators @xmath91 and @xmath92 are adjoint under the inner product with weight @xmath93 , while the operator @xmath94 is self adjoint under this inner product , @xmath95 moreover , since @xmath94 is obtained via conjugation of the kernel @xmath96 with @xmath97 all three operators share the same eigenvalues .",
    "the corresponding eigenfunctions can be found via conjugation by @xmath98 .",
    "for example , if @xmath99 , then the corresponding eigenfunctions for @xmath91 and @xmath92 are @xmath100 and @xmath101 , respectively . since @xmath98 is the first eigenfunction with @xmath102 of @xmath94 , the steady state of the forward operator",
    "is simply @xmath76 , while for the backward operator it is the uniform density , @xmath103 .",
    "obviously , the eigenvalues and eigenvectors of the discrete markov chain described in the previous section are discrete approximations to the eigenvalues and eigenfunctions of these continuum operators .",
    "rigorous mathematical proofs of this convergence as @xmath67 under various assumptions have been recently obtained by several authors @xcite .",
    "therefore , for a better understanding of the finite sample case , we are interested in the properties of the eigenvalues and eigenfunctions of either one of the operators @xmath104 or @xmath94 , and how these relate to the measure @xmath70 and to the corresponding potential @xmath2 . to this end",
    ", we look for functions @xmath83 such that @xmath105 where @xmath106 .    while in the case of a finite amount of data",
    ", @xmath47 must remain finite so as to have enough neighbors in a ball of radius @xmath77 near each point @xmath17 , as the number of samples goes to infinity , we can take smaller and smaller values of @xmath47 .",
    "therefore , it is instructive to look at the limit @xmath78 . in this case , the transition probability densities of the continuous in space but discrete in time markov chain converge to those of a diffusion process , whose time evolution is described by a differential equation @xmath107 where @xmath108 is the infinitesimal generator or propagator of the forward operator , defined as @xmath109 as shown in the appendix , by computing the asymptotic expansion of the corresponding integrals in the limit @xmath63 , we obtain that @xmath110 similarly , the inifinitesimal operator of the backward operator is given by @xmath111 as expected , @xmath112 is the eigenfunction with @xmath113 of the backward infinitesimal operator , while @xmath114 is the eigenfunction of the forward one .",
    "a few important remarks are due at this point .",
    "first , note that the backward operator ( [ h_b ] ) has the same functional form as the backward fpe ( [ backward_fpe ] ) , but with a potential @xmath1 instead of @xmath2 .",
    "the forward operator ( [ h_f ] ) has a different functional form from the forward fpe ( [ fpe ] ) corresponding to the stochastic differential equation ( [ sde ] ) .",
    "this should come as no surprise , since ( [ h_f ] ) is the differential operator of an isotropic diffusion process on a space with non - uniform probability measure @xmath70 , which is obviously different from the standard anisotropic diffusion in a space with a uniform measure , as described by the sde ( [ sde ] ) @xcite .",
    "interestingly , however , the form of the forward operator is the same as the schrdinger operator of quantum physics @xcite , e.g. @xmath115 where in our case the potential @xmath116 has the following specific form @xmath117 therefore , in the limit @xmath118 , the eigenfunctions of the diffusion map are the same as those of the schrdinger operator ( [ schrodinger ] ) with a potential ( [ vx ] ) .",
    "the properties of the first few of these eigenfunctions have been extensively studied theoretically for simple potentials @xmath116 @xcite .    in order to see why the forward operator @xmath108 also corresponds to a potential @xmath119 instead of @xmath2 , we recall that there is a well known correspondence @xcite , between the schrdinger equation with a sypersymmetric potential of the specific form ( [ vx ] ) and a diffusion process described by a fokker - planck equation of the standard form ( [ fpe ] ) .",
    "the transformation @xmath120 applied to the original fpe ( [ fpe ] ) yields the schrdinger equation with imaginary time @xmath121 comparing ( [ eq : schrodinger imaginary time ] ) with ( [ vx ] ) , we conclude that the eigenvalues of the operator ( [ h_f ] ) are the same as those of a fokker - planck equation with a potential @xmath1 .",
    "therefore , in general , for data sampled from the sde ( [ sde ] ) , there is no direct correspondence between the eigenvalues and eigenfunctions of the normalized graph laplacian and those of the corresponding fokker - planck equation ( [ fpe ] ) .",
    "however , when the original potential @xmath2 has two metastable states separated by a large barrier , corresponding to two well separated clusters , so does @xmath119 .",
    "therefore , the first non - trivial eigenvalue is governed by the mean passage time between the two barriers , and the first non - trivial eigenfunction gives a parametrization of the path between them ( see also the analysis of the next section ) .",
    "we note that in @xcite , horn and gottlieb suggested a clustering algorithm based on the schrdinger operator ( [ qm ] ) , where they constructed an approximate eigenfunction @xmath122 as in our eq .",
    "[ p_ve_discrete ] ) , and computed its corresponding potential @xmath116 from eq .",
    "( [ qm ] ) .",
    "the clusters were then defined by the minima of the potential @xmath123 . employing the same asymptotic analysis of this paper",
    ", one can show that in the appropriate limit , the computed potential @xmath123 is given by ( [ vx ] ) . this asymptotic analysis and the connection between the quantum operator and a diffusion process",
    "thus provides a mathematical explanation for the success of their method .",
    "indeed , when @xmath16 has a deep parabolic minima at a point @xmath17 , corresponding to a well defined cluster , so does @xmath123 .",
    "in the previous section we showed that asymptotically , the eigenvalues and eigenfunctions of the normalized graph laplacian operator correspond to the fokker - planck equation with a potential @xmath119 instead of the single @xmath2 . in this section",
    "we present a different normalization that yields infinitesimal generators corresponding to the potential @xmath2 without the additional factor of two .",
    "in fact , following @xcite we consider in more generality a whole family of different normalizations and their corresponding diffusions , and we show that , in addition to containing the graph laplacian normalization used in the previous section , this collection of diffusions includes at least two other laplacians of interest : the laplace - beltrami operator , which captures the riemannian geometry of the data set , and the backward fokker - planck operator of equation ( [ backward_fpe ] ) .    instead of applying the graph laplacian normalization to the isotropic kernel @xmath124",
    ", we first appropriately renormalize the kernel into an anisotropic one to obtain a new weighted graph , and then apply the graph laplacian normalization to this graph .",
    "more precisely , we proceed as follows : start with a gaussian kernel @xmath124 and let @xmath125 be a parameter indexing our family of diffusions . define an estimate for the local density as @xmath126 and consider the family of kernels @xmath127 we now apply the graph laplacian normalization by computing the normalization factor @xmath128 and forming a forward transition probability kernel @xmath129 similar to the analysis of section [ sec : continuum diffusion process ] , we can construct the corresponding forward , symmetric and backward diffusion kernels .",
    "it can be shown ( see appendix [ infinitesimal computations ] ) that the forward and backward infinitesimal generators of this diffusion are @xmath130 we mention three interesting cases :    * for @xmath131 , this construction yields the classical normalized graph laplacian with the infinitesimal operator given by equation ( [ h_f ] ) @xmath132 * for @xmath133 , the backward generator gives the laplace - beltrami operator : @xmath134 in other words , this diffusion captures only the geometry of the data , in which the density @xmath135 plays absolutely no role . therefore",
    ", this normalization separates the geometry of the underlying manifold from the statistics on it .",
    "* for @xmath136 , the infinitesimal operator of the forward and backward operators coincide and are given by @xmath137 which is exactly the backward fpe ( [ backward_fpe ] ) , with a potential @xmath2 .",
    "therefore , the last case with @xmath138 provides a consistent method to approximate the eigenvalues and eigenfunctions corresponding to the stochastic differential equation ( [ sde ] ) .",
    "this is done by constructing a graph laplacian with an appropriately anisotropic weighted graph .    as explained in @xcite ,",
    "the euclidian distance between any two points after the diffusion map embedding into @xmath139 is almost equal to their diffusion distance on the original dataset .",
    "thus , for dynamical systems with only one or two slow time scales , and many fast time scales , only a small number of diffusion map coordinates need be retained for the coarse grained representation of the data at medium to long times , at which the fast coordinates have equilibrated .",
    "therefore , the diffusion map can be considered as an empirical method to perform data - driven or equation - free homogenization .",
    "in particular , since this observation yields a computational method for the approximation of the top eigenfunctions and eigenvalues , this method can be applied towards the design of fast and efficient simulations that can be initialized on arbitrary points on the diffusion map .",
    "this application will be described in a separate publication @xcite .",
    "in this section we present the potential strength of the diffusion map method by analyzing , both analytically and numerically a few toy examples , with simple potentials @xmath2 .",
    "more complicated high dimensional examples of stochastic dynamical systems are analyzed in @xcite , while other applications such as the analysis of images for which we typically have no underlying probability model appear in @xcite .",
    "we start with the simplest case of a parabolic potential in one dimension , which in the context of the sde ( [ sde ] ) , corresponds to the well known ornstein - uhlenbeck process .",
    "we thus consider a potential @xmath140 , with a corresponding normalized density @xmath141 .",
    "the normalization factor @xmath54 can be computed explicitly @xmath142 where , for convenience , we multiplied the kernel @xmath143 by a normalization factor @xmath144 . therefore , the eigenvalue / eigenfunction problem for the symmetric operator @xmath94 with a finite @xmath47 reads @xmath145 the first eigenfunction , with eigenvalue @xmath57 is given by @xmath146 the second eigenfunction , with eigenvalue @xmath147 is , up to normalization @xmath148 in general , the sequence of eigenfunctions and eigenvalues is characterized by the following lemma :    * _ lemma : _ * the eigenvalues of the operator @xmath94 are @xmath149 , with the corresponding eigenvectors given by @xmath150 where @xmath151 is a polynomial of degree @xmath43 ( even or odd depending on @xmath43 ) .    in the limit @xmath78 , we obtain the eigenfunctions of the corresponding infinitesimal generator .",
    "for the specific potential @xmath152 , the eigenfunction problem for the backward generator reads @xmath153 the solutions of this eigenfunction problem are , up to scaling of @xmath154 , the well known hermite polynomials , which by the correspondence of this operator to the schrdinger eigenvector / eigenvalue problem , are also the eigenfunctions of the quantum harmonic oscillator ( after multiplication by the appropriate gaussian ) @xcite .    note that plotting the second vs. the first eigenfunctions ( with the convention that the zeroth eigenfunction is the constant one , which we typically ignore ) , is the same as plotting @xmath155 vs @xmath154 , e.g. a parabola .",
    "therefore , we expect that for a large enough and yet finite data - set sampled from this potential , the plot of the corresponding discrete eigenfunctions should lay on a parabolic curve ( see next section for a numerical example ) .",
    "we now consider a harmonic potential in @xmath5-dimensions , of the form @xmath156 where , in addition , we assume @xmath157 , so that @xmath158 is a slow variable in the context of the sde ( [ sde ] ) .    we note that for this specific potential , the probability density has a separable structure , @xmath159 , and so does the kernel @xmath69 , and consequently , also the symmetric kernel @xmath74 .",
    "therefore , there is an outer - product structure to the eigenvalues and eigenfunctions .",
    "for example , in two dimensions the eigenfunctions and eigenvalues are given by @xmath160 where @xmath161 and @xmath162 .",
    "since by assumption @xmath163 , then upon ordering of the eigenfunctions by decreasing eigenvalue , the first non - trivial eigenfunctions are @xmath164 , which depend only on the slow variable @xmath158 . note that indeed , regardless of the value of @xmath47 , as long as @xmath165 , we have that @xmath166 .",
    "therefore , in this example the first few coordinates of the diffusion map give a ( redundant ) parametrization of the slow variable @xmath158 in the system .    in figure",
    "[ f : u1 ] we present numerical results corresponding to a 2-dimensional potential with @xmath167 . in the upper left some 3500 points sampled from the distribution @xmath8 are shown . in the lower right and left panels , the first two non - trivial backward eigenfunctions @xmath168 and @xmath169 are plotted vs. the slow variable @xmath158 .",
    "note that except at the edges , where the statistical sampling is poor , the first eigenfunction is linear in @xmath158 , while the second one is quadratic in @xmath158 . in the upper right panel",
    "we plot @xmath169 vs. @xmath168 and note that they indeed lie on a parabolic curve , as predicted by the analysis of the previous section .",
    "+      we now consider a double well potential @xmath170 with two minima , one at @xmath171 and one at @xmath172 .",
    "for simplicity of the analysis , we assume a symmetric potential around @xmath173 , with @xmath174 ( see figure [ f : u2 ] ) . in the context of data clustering , this can be viewed as approximately a mixture of two gaussian clouds , while in the context of stochastic dynamical systems , this potential defines two meta - stable states .",
    "we first consider an approximation to the quantity @xmath175 , given by eq .",
    "( [ p_ve ] ) . for @xmath154 near @xmath171 , @xmath176 , while for @xmath154 near @xmath172 , @xmath177 . therefore , @xmath178 and @xmath179\\end{aligned}\\ ] ] where @xmath180 and @xmath181 are the first forward eigenfunctions corresponding to a single well potential centered at @xmath171 or at @xmath172 , respectively .",
    "as is well known both in the theory of quantum physics and in the theory of the fokker - planck equation , an approximate expression for the next eigenfunction is @xmath182\\ ] ] therefore , the first non - trivial eigenfunction of the backward operator is given by @xmath183 this eigenfunction is roughly @xmath184 in one well and @xmath185 in the other well , with a sharp transition between the two values near the barrier between the two wells .",
    "therefore , this eigenfunction is indeed suited for clustering .",
    "moreover , in the context of a mixture of two gaussian clouds , clustering according to the sign of @xmath186 is asymptotically equivalent to the optimal bayes classifier .",
    "* example : * consider the following potential in two dimensions , @xmath187 in the @xmath154 direction , this potential has a double well shape with two minima , one at @xmath188 and one at @xmath189 , separated by a potential barrier with a maximum at @xmath190 .    in figure",
    "[ f : u2 ] we show some numerical results of the diffusion map on some 1200 points sub - sampled from a stochastic simulation with this potential which generated about 40,000 points . on the upper right panel",
    "we see the potential @xmath191 , showing the two wells . in the upper left , a scatter plot of all the points , color coded",
    "according to the value of the local estimated density @xmath54 , ( with @xmath192 ) is shown , where the two clusters are easily observed . in the lower left panel ,",
    "the first non - trivial eigenfunction is plotted vs. the first coordinate @xmath154 .",
    "note that even though there is quite a bit of variation in the @xmath193-variable inside each of the wells , the first eigenfunction @xmath168 is essentially a function of only @xmath154 , regardless of the value of @xmath193 . in the lower right",
    "we plot the first three backward eigenfunctions .",
    "note that they all lie on a curve , indicating that the long time asymptotics are governed by the passage time between the two wells and not by the local fluctuations inside them .",
    "+      we now consider the following two dimensional potential energy with three wells , @xmath194    -5\\beta    e^{-y^2 }    \\left[e^{-(x-1)^2 } + e^{-(x+1)^2}\\right]\\ ] ] where @xmath195 is a thermal factor .",
    "this potential has two deep wells at @xmath196 and at @xmath197 and a shallower well at @xmath198 , which we denote as the points @xmath199 , respectively , the transitions between the wells of this potential have been analyzed in many works @xcite . in figure",
    "[ f : three_wells ] we plotted on the left the results of 1400 points sub - sampled from a total of 80000 points randomly generated from this potential confined to the region @xmath200",
    "^ 2\\subset \\mathbb{r}^2 $ ] at temperature @xmath201 , color - coded by their local density . on the right we plotted the first two diffusion map coordinates @xmath202 . notice how in the diffusion map space one can clearly see a triangle where each vertex corresponds to one of the points @xmath199 .",
    "this figure shows very clearly that there are two possible pathways to go from @xmath203 to @xmath204",
    ". a direct ( short ) way and an indirect longer way , that passes through the shallow well centered at @xmath205 .",
    "+      we conclude this section with a diffusion map analysis of one of the most popular multivariate datasets in pattern recognition , the iris data set .",
    "this set contains 3 distinct classes of samples in four dimensions , with 50 samples in each class . in figure [ f : iris ] we see on the left the result of the three dimensional diffusion map on this dataset .",
    "this picture clearly shows that all 50 points of class 1 ( blue ) are shrunk into a single point in the diffusion map space and can thus be easily distinguished from classes two and three ( red and green ) . in the right plot",
    "we see the results of re - running the diffusion map on the 100 remaining red and green samples .",
    "the 2-d plot of the first two diffusion maps coordinates shows that there is no perfect separation between these two classes .",
    "however , clustering according to the sign of @xmath206 gives misclassifications rates similar to those of other methods , of the order of 6 - 8 samples depending on the value chosen for the kernel width @xmath47 .",
    "in this paper , we introduced a mathematical framework for the analysis of diffusion maps , via their corresponding infinitesimal generators .",
    "our results show that diffusion maps are a natural method for the analysis of the geometry and probability distribution of empirical data sets .",
    "the identification of the eigenvectors of the markov chain as discrete approximations to the corresponding differential operators provides a mathematical justification for their use as a dimensional reduction tool and gives an alternative explanation for their empirical success in various data analysis applications , such as spectral clustering and approximations of optimal normalized cuts on discrete graphs .",
    "we generalized the standard construction of the normalized graph laplacian to a one - parameter family of graph laplacians that provides a low - dimensional description of the data combining the geometry of the set with the probability distribution of the data points .",
    "the choice of the diffusion map depends on the task at hand .",
    "if , for example , data points are known to approximately lie on a manifold , and one is solely interested in recovering the geometry of this set , then an appropriate normalization of a gaussian kernel allows to approximate the laplace - beltrami operator , regardless of the density of the data points .",
    "this construction achieves a complete separation of the underlying geometry , represented by the knowledge of the laplace operator , from the statistics of the points .",
    "this is important in situations where the density is meaningless , and yet points on the manifold are not sampled uniformly on it . in a different scenario ,",
    "if the data points are known to be sampled from the equilibrium distribution of a fokker - planck equation , the long - time dynamics of the density of points can be recovered from an appropriately normalized random walk process . in this case",
    ", there is a subtle interaction between the distribution of the points and the geometry of the data set , and one must correctly account for the density of the points .",
    "while in this paper we analyzed only gaussian kernels , our asymptotic results are valid for general kernels , with the appropriate modification that take into account the mean and covariance matrix of the kernel .",
    "note , however , that although asymptotically in the limit @xmath67 and @xmath78 , the choice of the isotropic kernel is unimportant , for a finite data set the choice of both @xmath47 and the kernel can be crucial for the success of the method .",
    "finally , in the context of dynamical systems , we showed that diffusion maps with the appropriate normalization constitute a powerful tool for the analysis of systems exhibiting different time scales .",
    "in particular , as shown in the different examples , these time scales can be separated and the long time dynamics can be characterized by the top eigenfunctions of the diffusion operator .",
    "last , our analysis paves the way for fast simulations of physical systems by allowing larger integration steps along slow variable directions .",
    "the exact details required for the design of fast and efficient simulations based on diffusion maps will be described in a separate publication @xcite .",
    "* acknowledgments : * the authors would like to thank the referee for helpful suggestions and for pointing out ref . @xcite .",
    "in this appendix , we present the calculation of the infinitesimal generators for the different diffusion maps characterized by a parameter @xmath207 .",
    "suppose that the data set @xmath208 consists of a riemannian manifold with a density @xmath0 and let @xmath69 be a gaussian kernel .",
    "it was shown in @xcite that if @xmath209 is scaled appropriately , then for any function @xmath210 on @xmath208 , @xmath211 where @xmath212 is a function that depends on the riemannian geometry of the manifold and its embedding in @xmath64 . using the notations introduced in section [ ref : anisotropic diffusion maps ] , it is easy to verify that @xmath213 and consequently , @xmath214 let @xmath215 then , the normalization factor @xmath216 is given by @xmath217\\ ] ] therefore , the asymptotic expansion of the backward operator gives @xmath218 and its infinitesimal generator is @xmath219 inserting the expression @xmath8 into the last equation gives @xmath220 similarly , the form of the forward infinitesimal operator is @xmath221        r. r. coifman , s. lafon , a. b. lee , m. maggioni , b. nadler , f. warner and s. zucker , geometric diffusions as a tool for harmonic analysis and structure definition of data , part i : diffusion maps , _ proc .",
    "_ , in press .",
    "m. saerens , f. fouss , l. yen and p. dupont , _ the principal components analysis of a graph and its relationships to spectral clustering _ , proceedings of the 15th european conference on machine learning ( ecml 2004 ) , lecture notes in artificial intelligence , vol . 3201 , springer - verlag , berlin , 2004 , pp 371 - 383 .",
    "kevrekidis , c.w .",
    "gear , j. m. hyman , p.g .",
    "kevrekidis , o. runborg , c. theodoropoulos , `` equation free multiscale computation : enabling microscopic simulators to perform system - level tasks '' , _ comm .",
    "_ , submitted ."
  ],
  "abstract_text": [
    "<S> a central problem in data analysis is the low dimensional representation of high dimensional data , and the concise description of its underlying geometry and density . in the analysis of large scale simulations of complex dynamical systems , where the notion of time evolution comes into play , </S>",
    "<S> important problems are the identification of slow variables and dynamically meaningful reaction coordinates that capture the long time evolution of the system . in this paper </S>",
    "<S> we provide a unifying view of these apparently different tasks , by considering a family of _ diffusion maps _ , defined as the embedding of complex ( high dimensional ) data onto a low dimensional euclidian space , via the eigenvectors of suitably defined random walks defined on the given datasets . assuming that the data is randomly sampled from an underlying general probability distribution @xmath0 , we show that as the number of samples goes to infinity , the eigenvectors of each diffusion map converge to the eigenfunctions of a corresponding differential operator defined on the support of the probability distribution . </S>",
    "<S> different normalizations of the markov chain on the graph lead to different limiting differential operators . </S>",
    "<S> for example , the normalized graph laplacian leads to a backward fokker - planck operator with an underlying potential of @xmath1 , best suited for spectral clustering . </S>",
    "<S> a specific anisotropic normalization of the random walk leads to the backward fokker - planck operator with the potential @xmath2 , best suited for the analysis of the long time asymptotics of high dimensional stochastic systems governed by a stochastic differential equation with the same potential @xmath2 . </S>",
    "<S> finally , yet another normalization leads to the eigenfunctions of the laplace - beltrami ( heat ) operator on the manifold in which the data resides , best suited for the analysis of the geometry of the dataset , regardless of its possibly non - uniform density . </S>"
  ]
}