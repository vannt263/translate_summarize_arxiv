{
  "article_text": [
    "ontologies in different natural languages often differ in quality in terms of richness of schema or richness of internal links .",
    "bizer et al .",
    "@xcite refers to various ontology - related problems like maintaining quality , relevancy and trustworthiness .",
    "when dealing with such problems in non - english ontologies , we can take help from richness of english ontologies . as an example",
    ", we can generate data quality tests from english schema data ( using approach in kontokostas et al .",
    "@xcite ) and use cross - lingual mappings to test quality of non - english instance data . in general , ontology mapping in cross - lingual domain still continues to be a challenge as maintained by shvaiko et al . @xcite .",
    "the results of oaei 2013 @xcite show that the precision values of the mappings in cross - lingual domain are , in general , poorer as compared to the precision values of mappings in monolingual domain .",
    "a similar observation has been made in fu et al.s study @xcite .",
    "most current approaches for cross - lingual ontology mapping ( clom ) are based on machine translation ( mt ) as pointed out in fu et al . @xcite .",
    "such approaches first use mt on the source ontology to convert a cross - lingual mapping problem into a monolingual ontology mapping ( mom ) problem .",
    "then mom techniques are used to find mappings to the target ontology .",
    "however , a drawback to such an approach ( as fu et al .",
    "@xcite points out ) is that the mapping performance is critically dependent on the translation step . in recent years , several studies @xcite have attempted to overcome this challenge by devising ways to choose an appropriate machine translation for the labels in the ontologies .    in this paper ,",
    "our goal is to obtain a @xmath0 cross - lingual _ owl : equivalentproperty _ correspondence from a given predicate in source ontology to a predicate in the target ontology .",
    "we focus only on the limited problem of predicate mapping rather than the broader ontology mapping problem . yet",
    ", many applications can make use of such mappings .",
    "for example ,",
    "cross - lingually mapped predicates can exchange owl property restrictions from each other .",
    "we emphasize that this work on predicate mapping can also be seen as a sub - part of the broader problem of clom .",
    "our work , in particular , was motivated by the current scenario in dbpedia instance - data where inter - language links are nearly absent for the predicates but are present only for uris of individuals .",
    "our main contribution in this paper is the indirect links method for cross - lingual predicate mapping .",
    "our system stresses on semantic similarity between mapped predicates rather than lexical similarity .",
    "we achieve this mapping by using pre - existing inter - language links of resources connected by the given predicate . also , we made use of wikipedia inter - language links .",
    "the merit and high - quality of such links have been pointed out in niu et al .",
    "bouma @xcite has also attempted to use such links for cross - lingual mapping tasks .",
    "additionally , our system uses a lightweight mt+mom approach as a fallback mechanism .",
    "the rest of the paper is structured as follows .",
    "section 2 describes how our methodology for cross - lingual predicate mapping works in detail .",
    "section 3 provides explanations of the experiment performed for evaluating our methodology .",
    "further , section 4 describes the evaluation results and section 5 concludes our work with pointers to possible future research .",
    "our methodology uses two methods , method 1 and method 2 , which are used in combination with method 1 as the primary driver .",
    "it must be noted that our methodology uses only instance - data for the predicate mapping task .",
    "we shall denote the set of all uris in source ontology ( published in language @xmath1 ) by @xmath2 .",
    "also , we shall denote the set of all uris in target ontology ( published in language @xmath3 ) by @xmath4 .",
    "both the methods of our methodology take an input predicate @xmath5 and map it to an output predicate belonging to target ontology .",
    "we shall denote the output predicate from method 1 as @xmath6 and that from method 2 as @xmath7 .",
    "our main contribution lies in the method 1 ( indirect links method ) which we shall see in detail next .",
    "further , if the confidence for the returned predicate @xmath6 is low , then we switch to method 2 as a fallback mechanism and report @xmath7 as output mapping .",
    "we denote this finally reported output predicate by @xmath8 and we say that @xmath9 and @xmath8 can be mutually connected by an _ owl : equivalentproperty _ link .",
    "basic intuition behind this method is that the predicates that link the same pair of resources must have essentially similar meanings .",
    "thus we find the subject - object pairs linked by the given predicate in the source ontology .",
    "then we look for the same or similar subject - object pairs in the target ontology . and",
    "finally we extract the predicates that link these subject - object pairs in the target ontology .",
    "the idea is represented diagrammatically in figure 1 .",
    "a formal description of this algorithm consists of 3 steps and is given below .          for the given predicate @xmath5 , we obtain set of all the subject - object pairs @xmath10 such that triple @xmath11 exists in the source ontology .",
    "this is implemented using a sparql query as follows :    .... select distinct ?",
    "b   where   {      ?",
    "? b . } ....    we will denote this set of subject - object pairs thus obtained by @xmath12 .      in this step ,",
    "we obtain cross - ontology mappings for the subjects and objects in the @xmath12 obtained in previous step .",
    "this step refers to the block m in figure 1 .",
    "we treat mapping of each subject or object ( which we denote by a dummy variable @xmath13 ) as an individual mapping problem where we attempt to find a mapping @xmath14 such that @xmath15 .",
    "such mappings may be obtained in following possible ways using some pre - existing inter - language link :    * using high quality _ owl : sameas _ links from @xmath2 to @xmath4 . * using string similarity methods on uri @xmath13 to get corresponding wikipedia entry title @xmath16 in language @xmath1 .",
    "then following the wikipedia inter - language link to get corresponding wikipedia entry with title @xmath17 in language @xmath3 . then again employing string similarity techniques to get uri @xmath15 corresponding to @xmath17 .",
    "we noted that such inter - language links are of high quality as also pointed out in niu et al .",
    "@xcite .",
    "so we obtain the set of subject - object pairs @xmath18 such that each pair is mapped from some @xmath19 .",
    "we shall denote this set by @xmath20 .",
    "let @xmath21 be @xmath22 subject - object pair in @xmath20 and let @xmath23 be the set of all predicates @xmath24 such that the triple @xmath25 exists in the target ontology .",
    "we can find this set @xmath23 for a subject - object pair @xmath21 using a simple sparql query of the following form .",
    ".... select ?",
    "p    where   {      < a > ?",
    "p < b > . } ....    we take a union of all such @xmath23 s obtained for each @xmath26 .",
    "let this union be called @xmath27 .",
    "thus , @xmath27 is the set of all the candidate output predicates . in order to select the best predicate suggestion ,",
    "we shall keep a count ( denoted by @xmath28 ) of the number of subject - object pairs @xmath29 that each predicate @xmath30 links .",
    "we score each predicate in @xmath27 as follows : @xmath31 where @xmath32 is the number of triples containing predicate @xmath33 in the target ontology .",
    "we conjectured that @xmath34 acts like a `` bayesian prior '' and would help reject the deprecated and noisy predicates from occurring in the returned mappings .",
    "confidence for this mapping was calculated as @xmath35 we chose the predicate @xmath33 with the highest confidence as the output mapping @xmath6 .",
    "it must be noted this confidence value is calculated only for deciding whether to use method 2 or not . we do not assign a confidence score for the final output predicate @xmath8 .",
    "we believe and argue that this method has the following positive features .    * this method should preserve semantic similarity rather than lexical similarity between mapped predicates .",
    "this is because the predicates that link the same pair of resources must have similar meaning .",
    "also , because wikipedia inter - language links are manually created , they are expected to have high semantic similarity .",
    "* this method should allow us to find a correspondence even when the number of pre - existing anchor links are sparse / few .",
    "this is because multiple subject - object pairs ( for any predicate ) increase the likelihood of finding at least a few inter - language links . *",
    "mapping a large number of subject / object resources for one predicate should cause an averaging / cancelling out of random errors in individual mappings .",
    "this is a lightweight mapping approach based on standard mt+mom approach . for a given predicate @xmath5",
    ", we first obtain label translations using google translate api .",
    "next we use edit distance to find the closest string match of the label translation with the labels of predicates in the target ontology .",
    "tie between many closest matches was broken using a `` prior '' for each predicate .",
    "it was calculated as @xmath34 where @xmath32 is the total number of triples containing predicate @xmath33 in the target ontology .",
    "we use this method as a fallback mechanism when the confidence score for the mapping from the method 1 is low .",
    "we prioritized method 1 over method 2 because of greater accuracy of the former when the confidence score was high .",
    "to demonstrate the merit and usability of our methodology , we have applied this approach on a pair of following data sets :    * source ontology : korean dbpedia ( _ http://ko.dbpedia.org/ _ ) * target ontology : english dbpedia ( _ http://dbpedia.org/ _ )    evaluation was performed by mapping 1000 ( out of nearly 16000 ) predicates which occurred most frequently in the triples of korean dbpedia data set .",
    "this choice of our test - set is justified because 97.5% of all the triples in korean dbpedia data set use only these 1000 predicates .",
    "we did not extend our test set further because :    * remaining predicates form only 2.5% of the korean dbpedia data - set and hence are of low significance . *",
    "computational effort of testing and human evaluation of the results on remaining data set was high . *",
    "low number of triples for such predicates provided lesser anchor links .",
    "this underestimated the performance and merit of the algorithm .",
    "evaluation of the predicate mappings obtained from our experiment was performed by two non - author bilingual evaluators with a reasonably high inter - evaluator agreement .",
    "out of 1000 predicate mappings , 200 mappings were rated by both evaluators and a cohen s kappa coefficient @xcite was calculated , which had a value of 98.38% .",
    "further , the remaining set of 800 mappings was divided into two equal parts and were given to one evaluator each for evaluation .",
    "evaluators were asked to rate each mapping as one of the following cases :    1 .",
    "@xmath36 i.e. source predicate and target predicate are equivalent .",
    "@xmath37 i.e. source predicate is a hyponym ( i.e. , sub - property ) of target predicate .",
    "@xmath38 i.e. source predicate is a hyponym ( i.e. , sub - property ) of target predicate .",
    "@xmath39 i.e. source predicate and target predicate are unrelated .    in our implementation , we invoked method 2 when the confidence on suggested predicate from method 1 was lower than a particular threshold @xmath40 .",
    "we ran our algorithm by taking this threshold @xmath40 as 0.1 , 0.2 and 0.3 .",
    "we also ran our algorithm purely using method 1 only and method 2 only , respectively . the last case ( method 2 only ) served as the baseline for comparison .    in the list",
    "below , we mention a few implementation - related intricacies for method 1 .    1 .   in step 1 ,",
    "the extraction of subject - object pairs in korean dbpedia was done by executing the corresponding sparql query as depicted in section 2 .",
    "a sparql endpoint for korean dbpedia version 3.9 was used for the purpose .",
    "+ 2 .   in step 2",
    ", we encountered multiple types of resources while mapping korean dbpedia subject - object pairs to english dbpedia subject - object pairs .",
    "these multiple cases have been given below along with details on how they were tackled .",
    "+ * * the subject / object belongs to a standard data - type such as integer , double , date , time etc . *",
    "+ we noted that no mapping from korean to english was needed because data - typed resources are universal . +",
    "* * the subject / object is a string label * + in such cases , we looked for the one - word matches with title labels in korean wikipedia and then followed the corresponding korean - english wikipedia inter - language link to get an english wikipedia entry .",
    "then we searched the title label of this wikipedia entry for one - word matches in the english dbpedia .",
    "thus , we attempt to overcome the language barrier using the wikipedia inter - language links .",
    "evaluation using a better string matching technique instead of one - word match can be a task for a future work . + * * subject / object has a well - formed uri * + in such cases , we attempted to look for the inter - language _ owl : sameas _ links .",
    "it must be noted here that in localised versions of dbpedia ( after version 3.7 ) , inter - language _ owl : sameas _ links are derived from wikipedia inter - language links .",
    "+ 3 .   in step 3 , retrieval of predicates linking the mapped subject - object pairs was done using the corresponding sparql query as depicted in section 2 . a sparql endpoint for english dbpedia version 3.9",
    "was used for the purpose .",
    "in this section , we look at the evaluation results for our predicate mapping methodology . in order to put our result in perspective",
    ", we shall also compare our results with a baseline matcher based on simple mt+mom approach .",
    "this baseline matcher is based on using google translate api on the uri labels in source ontology and then using simple string - based edit distance matcher on this translated ontology .",
    "we argue that this is a fair choice because many recent cross - lingual ontology matchers ( as in @xcite ) use mt+mom approaches .",
    "we observed that in these matchers , the common choice of machine translation ( mt ) module is the google translate api .",
    "further , for the monolingual ontology mapping ( mom ) module , we chose a string - based edit distance matcher because it is simple , lightweight and is still being used by oaei campaigns for the evaluation of monolingual ontology matchers as in @xcite .",
    "one of the demerits of our methodology is that it returns only @xmath0 predicate mappings from source to target ontology .",
    "furthermore , in our current evaluation setup , we have performed only one - way predicate mapping instead of two - way mapping .",
    "these issues may be tackled in a future study .",
    "thus , currently , we report only the precision scores and not recall .",
    "table 1 shows a few examples of mappings given by our system that were rated under different categories .",
    "utf8    .examples of different types of mappings [ cols=\"<,^,^ \" , ]     we have calculated 2 kinds of precision scores based on whether partially correct mappings of type @xmath37 or @xmath38 are considered or not . it should be noted that mappings rated as @xmath37 or @xmath38 can not be thoughtlessly disregarded because it reflects that input and output predicates had some semantic similarity , prompting us to report precision scores that take these partially correct mappings into account .",
    "these two kinds of precision values are described below :    * precision 1 refers to fraction of total mappings that were rated @xmath36 . *",
    "precision 2 refers to fraction of total mappings that were rated @xmath36 or @xmath37 or @xmath38 .",
    "we summarise some of the inferences as follows .    1 .   proposed methodology ( having precision 0.56 ) is an improvement over the baseline system ( having precision 0.44 ) as evidenced by the 22% higher precision of the former .",
    "proposed methodology is able to identify many possible mappings with partial semantic linkage .",
    "this is evidenced by the high number of mappings rated @xmath37 and @xmath38 furnished by our methodology as compared to the baseline system .",
    "this supports our hypothesis that our method stresses on semantic similarity rather than lexical similarity .",
    "thus , taking partially correct mappings into consideration , we see a huge improvement in precision ( of nearly 45% ) from 0.47 ( for baseline system ) to 0.68 ( for proposed methodology ) .",
    "a drawback of method 1 is that nearly one in ten mappings could not be found .",
    "this generally occurred due to the lack of wikipedia entry for the subjects / objects in korean or english dbpedia .",
    "also , it occurred due to the lack of wikipedia inter - language links . also , it was manually observed that method 1 returned more mappings rated @xmath39 when confidence was low as compared to when it was high .",
    "these drawbacks have been successfully overcome by using method 2 as a fallback mechanism .",
    "setting confidence threshold @xmath41 yields the maximum precision value of 0.56 ( disregarding partially correct mappings ) .",
    "this is an improvement over the precision value of 0.50 ( disregarding partially correct mappings ) , which was obtained by using only method 1 .",
    "we have presented a successful ad - hoc system for cross - lingual predicate mapping . we have successfully demonstrated that our proposed methodology outperforms the baseline system .",
    "further , we have demonstrated that our system stresses on semantic similarity rather than lexical similarity as our system outperforms the baseline system in furnishing partially correct mappings .",
    "this work still leaves a lot of scope for future work .",
    "currently , the evaluation has been performed on a limited data set between just one pair of ontologies . so performing evaluation on more pairs of ontologies",
    "can shed more lights on merits and demerits of our methodology .",
    "one of the key issues with this work is that the algorithm returns only @xmath0 predicate mappings from source to target ontology .",
    "thus , algorithm misses out on other possible predicates in the target ontology that might capture the same role .",
    "hence , this work may be extended to include this additional aspect and to get @xmath42 mappings",
    ".    there can be various ways is which the current implementation may be improved .",
    "one possibility is to explore other ways to cross the natural language barrier instead of using wikipedia inter - language links .",
    "another possibility is to have greater and better support for different data - types for cross - lingual mapping of subjects and objects .",
    "additionally , we can have a more sophisticated measure of confidence for the predicate suggestion because current metric is based on a simple ratio - based heuristic .",
    "a future work may also attempt on distinctly identifying not just @xmath36 mappings but also mappings of type @xmath37 and @xmath38 . notwithstanding these limitations and future research issues",
    ", the current study provides a novel approach and important insights towards overcoming the disparity between cross - lingual ontologies .",
    "5 bizer , c. , t. heath , and t. berners - lee .",
    " linked data - the story so far .",
    "\" international journal on semantic web and information systems 5.3 ( 2009 ) : 1 - 22 .",
    "shvaiko , p. and j. euzenat .",
    " ontology matching : state of the art and future challenges . \" knowledge and data engineering , ieee transactions on 25.1 ( 2013 ) : 158 - 176 .",
    "niu , x. , et al . ",
    "zhishi.me-weaving chinese linking open data . \" the semantic web ",
    "iswc 2011 .",
    "springer berlin heidelberg , 2011 .",
    "205 - 220 .",
    "kontokostas , d. , et al .",
    " test - driven evaluation of linked data quality . \" proceedings of the 23rd international conference on world wide web .",
    "international world wide web conferences steering committee , 2014 .",
    "fu , b. , r. brennan , and d. osullivan .",
    " a configurable translation - based cross - lingual ontology mapping system to adjust mapping outcomes .",
    "\" web semantics : science , services and agents on the world wide web 15 ( 2012 ) : 15 - 36 .",
    "fu , b. , r. brennan , and d. osullivan .",
    " cross - lingual ontology mapping  an investigation of the impact of machine translation . \" the semantic web .",
    "springer berlin heidelberg , 2009 . 1 - 15 .",
    "fu , b. , r. brennan , and d. osullivan .",
    " using pseudo feedback to improve cross - lingual ontology mapping . \" the semantic web : research and applications .",
    "springer berlin heidelberg , 2011 .",
    "336 - 351 .",
    "koukourikos , a. , p. karampiperis , and g. stoitsis .",
    " cross - language ontology alignment utilizing machine translation models . \" metadata and semantics research .",
    "springer international publishing , 2013 .",
    "bouma , g.  cross - lingual dutch to english alignment using eurowordnet and dutch wikipedia . \" proceedings of the 4th international workshop on ontology matching , ceur - ws .",
    "grau , b. c. , et al .",
    " results of the ontology alignment evaluation initiative 2013 . `` proc .",
    "8th iswc workshop on ontology matching ( om ) .",
    "cohen , j. ' ' a coefficient of agreement for nominal scales .",
    "\" educational and psychological measurement 20(1 ) , 1960 , 37 - 46 ."
  ],
  "abstract_text": [
    "<S> ontologies in different natural languages often differ in quality in terms of richness of schema or richness of internal links . </S>",
    "<S> this difference is markedly visible when comparing a rich english language ontology with a non - english language counterpart . </S>",
    "<S> discovering alignment between them is a useful endeavor as it serves as a starting point in bridging the disparity . in particular , our work is motivated by the absence of inter - language links for predicates in the localised versions of dbpedia . in this paper , we propose and demonstrate an ad - hoc system to find possible _ owl : equivalentproperty _ links between predicates in ontologies of different natural languages . </S>",
    "<S> we seek to achieve this mapping by using pre - existing inter - language links of the resources connected by the given predicate . </S>",
    "<S> thus , our methodology stresses on semantic similarity rather than lexical . </S>",
    "<S> moreover , through an evaluation , we show that our system is capable of outperforming a baseline system that is similar to the one used in recent oaei campaigns . </S>"
  ]
}