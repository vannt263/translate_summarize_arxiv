{
  "article_text": [
    "one of the classical problems in the streaming literature is that of computing the @xmath9-frequency moments ( or @xmath9-norm ) @xcite . in particular ,",
    "the question is to compute the norm @xmath10 of a vector @xmath11 , up to @xmath12 approximation , in the streaming model using low space . here , we assume the most general model of streaming , where one sees updates to @xmath7 of the form @xmath13 which means to add a quantity @xmath14 to the coordinate @xmath15 of @xmath7 . ,",
    "although more refined bounds can be stated otherwise .",
    "note that in this case , a `` word '' ( or measurement in the case of linear sketch  see definition below ) is usually @xmath16 bits . ] in this setting , linear estimators , which store @xmath6 for a matrix @xmath8 , are particularly useful as such an update can be easily processed due to the equality @xmath17 .",
    "the frequency moments problem is among the problems that received the most attention in the streaming literature .",
    "for example , the space complexity for @xmath18 has been fully understood .",
    "specifically , for @xmath19 , the foundational paper of @xcite showed that @xmath20 words ( linear measurements ) suffice to approximate the euclidean norm words ; since in this paper we concentrate on the case of @xmath21 only , we drop dependence on @xmath22 . ] .",
    "later work showed how to achieve the same space for all @xmath23 norms @xcite .",
    "this upper bound has a matching lower bound @xcite .",
    "further research focused on other aspects , such as algorithms with improved update time ( time to process an update @xmath13 ) @xcite . in constrast ,",
    "when @xmath2 , the exact space complexity still remains open . after a line of research on both upper bounds @xcite , @xcite and lower bounds @xcite , we presently know that the best space upper bound is of @xmath3 words , and the lower bound is @xmath4 bits ( or linear measurements ) .",
    "( very recently also , in a _ restricted _ streaming model  when @xmath24  @xcite achieves an improved upper bound of nearly @xmath25 words . ) in fact , since for @xmath26 the right bound is @xmath27 ( without the log factor ) , it may be tempting to assume that there the right upper bound should be @xmath25 in the general case as well .    in this work , we prove a tight lower bound of @xmath5 for the case of _ linear estimator_. a linear estimator uses a distribution over @xmath28 matrices @xmath8 such that with high probability over the choice of @xmath8 , it is possible to calculate the @xmath0 moment @xmath10 from the sketch @xmath6 .",
    "the parameter @xmath29 , the number of words used by the algorithm , is also called the number of measurements of the algorithm .",
    "our new lower bound is of @xmath30 measurements / words , which matches the upper bound from @xcite .",
    "we stress that essentially all known algorithms in the general streaming model are in fact linear estimators .",
    "[ thm : fklowerbound ] fix @xmath31 .",
    "any linear sketching algorithm for approximating the @xmath32 moment of a vector @xmath11 up to a multiplicative factor @xmath33 with probability @xmath34 requires @xmath30 measurements .    in other words , for any @xmath35 there is a constant @xmath36 such that for any distribution on @xmath37 matrices @xmath8 with @xmath38 and any function @xmath39 we have    @xmath40    the proof uses similar hard distributions as in some of the previous work , namely all coordinates of an input vector @xmath7 have random small values except for possibly one location . to succeed on these distributions ,",
    "the algorithm has to distinguish between a mixture of gaussian distributions and a pure gaussian distribution .",
    "analyzing the optimal probability of success directly seems too difficult .",
    "instead , we use the @xmath41-divergence to bound the success probability , which turns out to be much more amenable to analysis .    from a statistical perspective",
    ", the problem of linear sketches of moments can be recast as a minimax statistical estimation problem where one observes the pair @xmath42 and produces an estimate of @xmath10 .",
    "more specifically , this is a functional estimation problem , where the goal is to estimation some functional ( in this case , the @xmath32 moment ) of the parameter @xmath7 instead of estimating @xmath7 directly . under this decision - theoretic framework",
    ", our argument can be understood as le cam s two - point method for deriving minimax lower bounds @xcite .",
    "the idea is to use a binary hypotheses testing argument where two priors ( distributions of @xmath7 ) are constructed , such that 1 ) the @xmath32 moment of @xmath7 differs by a constant factor under the respective prior ; 2 ) the resulting distributions of the sketches @xmath6 are indistinguishable .",
    "consequently there exists no moment estimator which can achieve constant relative error .",
    "this approach is also known as the method of fuzzy hypotheses ( * ? ? ?",
    "* section 2.7.4 ) .",
    "see also @xcite for the method of using @xmath41-divergence in minimax lower bound .",
    "we remark that our proof does not give a lower bound as a function of @xmath22 ( but @xcite independently reports progress on this front ) .",
    "we use the following definition of divergences .",
    "let @xmath43 and @xmath44 be probability measures .",
    "the _ @xmath41-divergence _ from @xmath43 to @xmath44 is @xmath45 the _ total variation distance _ between @xmath43 and @xmath44 is @xmath46    the operational meaning of the total variation distance is as follows : denote the optimal sum of type - i and type - ii error probabilities of the binary hypotheses testing problem @xmath47 versus @xmath48 by @xmath49 where the infimum is over all measurable sets @xmath8 and the corresponding test is to declare @xmath50 if and only if @xmath51 .",
    "then @xmath52    the total variation and the @xmath41-divergence are related by the following inequality ( * ? ? ?",
    "* section 2.4.1 ) : @xmath53 therefore , in order to establish that two hypotheses can not be distinguished with vanishing error probability , it suffices to show that the @xmath41-divergence is bounded .",
    "one additional fact about @xmath54 and @xmath41 is the data - processing property  @xcite : if a measurable function @xmath55 carries probability measure @xmath43 on @xmath8 to @xmath56 on @xmath57 , and carries @xmath44 to @xmath58 then @xmath59",
    "in this section we prove theorem  [ thm : fklowerbound ] for arbitrary fixed measurement matrix @xmath8 . indeed , by yao s minimax principle , we only need to demonstrate an input distribution and show that any deterministic algorithm succeeding on this distribution with probability 99/100 must use @xmath30 measurements",
    ".    fix @xmath31 .",
    "let @xmath60 be a fixed matrix which is used to produce the linear sketch , where @xmath61 is the number of measurements and @xmath36 is to be specified .",
    "next , we construct distributions @xmath62 and @xmath63 for @xmath7 to fulfill the following properties :    1 .",
    "@xmath64 on the entire support of @xmath62 , and @xmath65 on the entire support of @xmath63 , for some appropriately chosen constant @xmath66 .",
    "2 .   let @xmath67 and @xmath68 denote the distribution of @xmath6 when @xmath7 is drawn from @xmath62 and @xmath63 respectively",
    ". then @xmath69",
    ".    the above claims immediately imply the desired via the relationship between statistical tests and estimators . to see this , note that any moment estimator @xmath70 induces a test for distinguishing @xmath67 versus @xmath68 : declare @xmath63 if and only if @xmath71 .",
    "in other words , @xmath72 where the last line follows from the characterization of the total variation in . the idea for constructing the desired pair of distributions",
    "is to use the gaussian distribution and its sparse perturbation .",
    "since the moment of a gaussian random vector takes values on the entire @xmath73 , we need to further truncate by taking its conditioned version . to this end , let @xmath74 be a standard normal random vector and @xmath75 a random index uniformly distributed on @xmath76 and independently of @xmath77 .",
    "let @xmath78 denote the standard basis of @xmath79 .",
    "let @xmath80 and @xmath81 be input distributions defined as follows : under the distribution @xmath80 , we let the input vector @xmath7 equal to @xmath77 . under the distribution @xmath81 , we add a one - sparse perturbation by setting @xmath82 with an appropriately chosen constant @xmath83 .",
    "now we set @xmath62 to be @xmath80 conditioned on the event @xmath84 , i.e. , @xmath85 , and set @xmath63 to be @xmath81 conditioned on the event @xmath86 . by the triangle inequality , @xmath87 where the second inequality follows from the data - processing inequality   ( applied to the mapping @xmath88 ) .",
    "it remains to bound the three terms in .",
    "first observe that for any @xmath15 , @xmath89 = t_p$ ] where @xmath90 .",
    "thus , @xmath91 = nt_p$ ] . by markov inequality",
    ", @xmath92 holds with probability at most @xmath93 .",
    "now , if we set @xmath94 we have @xmath95 with probability at least @xmath34 , and hence the third term in   is also smaller than @xmath96 .",
    "it remains to show that @xmath97 .    without loss of generality , we assume that the rows of @xmath8 are orthonormal since we can always change the basis of @xmath8 after taking the measurements .",
    "let @xmath22 be a constant smaller than @xmath98 .",
    "assume that @xmath99 .",
    "let @xmath100 denote the @xmath101 column of @xmath8 .",
    "let @xmath102 be the set of indices @xmath15 such that @xmath103 .",
    "let @xmath104 be the complement of @xmath102 . since @xmath105 , we have @xmath106 .",
    "let @xmath107 be uniformly distributed on @xmath102 and @xmath108 the distribution of @xmath109 . by the convexity of @xmath110 and",
    "the fact that @xmath111 , we have @xmath112 . in view of , it suffices to show that @xmath113 for some sufficiently small constant @xmath114 . to this end , we first prove a useful fact about the measurement matrix @xmath8 .    [",
    "lem : eqn1 ] for any matrix @xmath8 with @xmath115 orthonormal rows , denote by @xmath102 the set of column indices @xmath15 such that @xmath116 .",
    "then @xmath117    because @xmath118 , we have @xmath119 } \\langle a_i , a_j\\rangle^2 = \\sum_{i , j\\in [ n ] } ( a^t a)_{ij}^2 = \\|a^ta\\|_f^2 = \\operatorname{tr}(a^taa^ta ) = \\operatorname{tr}(a^ta ) = \\|a\\|_f^2 = m.\\ ] ]    we consider the following relaxation : let @xmath120 where @xmath121 and @xmath122 .",
    "we now upper bound @xmath123 .",
    "we have @xmath124 } x_i)^{j-2}}{j!}\\\\ & \\le 1+|s|^{-2}\\sqrt{|s|^2\\sum_i x_i^2}+|s|^{-2}(c_1 ^ 4mn^{4/p})\\left(\\frac{e^{{\\epsilon}\\log n}}{({\\epsilon}\\log n)^2}\\right)\\\\ & \\le 1 + 1.03c_1 ^ 2 \\sqrt{m}n^{2/p-1 } + 1.03 c_1 ^ 4n^{-2 + 4/p+{\\epsilon}}m.\\end{aligned}\\ ] ] the last inequality uses the fact that @xmath125 . applying the above upper bound to @xmath126 , we conclude the lemma .",
    "we also need the following lemma @xcite which gives a formula for the @xmath41-divergence from a gaussian location mixture to a standard gaussian distribution :    let @xmath43 be a distribution on @xmath127 .",
    "then @xmath128 -1 \\ , , \\ ] ] where @xmath129 and @xmath130 are independently drawn from @xmath43 .",
    "[ lmm : chi2 ]    we now proceed to proving an upper bound on the @xmath41-divergence between @xmath131 and @xmath132 .",
    "@xmath133    let @xmath134 be the probability @xmath135 . recall that @xmath107 is the random index uniform on the set @xmath136 : \\|a_i\\|_2 \\le 10\\sqrt{m / n } \\}$ ] .",
    "note that @xmath137 . since @xmath138 , we have @xmath139",
    ". therefore @xmath140 , a gaussian location mixture .",
    "applying lemma [ lmm : chi2 ] and then lemma [ lem : eqn1 ] , we have @xmath141    finally , to finish the lower bound proof , since @xmath142 we have @xmath143 , implying   for all sufficiently large @xmath144 and completing the proof of @xmath145 .",
    "while theorem  [ thm : fklowerbound ] is stated only for constant @xmath9 , the proof also gives lower bounds for @xmath9 depending on @xmath144 . at one extreme",
    ", the proof recovers the known lower bound for approximating the @xmath146-norm of @xmath147 .",
    "notice that the ratio between the @xmath148-norm and the @xmath146-norm of any vector is bounded by @xmath149 so it suffices to consider @xmath150 with a sufficiently small constant @xmath151 . applying the stirling approximation to the crude value of @xmath83 in the proof ,",
    "we get @xmath152 .",
    "thus , the lower bound we obtain is @xmath153 .    at the other extreme , when @xmath154 , the proof also gives super constant lower bounds up to @xmath155 .",
    "notice that @xmath22 can be set to @xmath156 instead of a positive constant strictly smaller than @xmath98 . for this value of @xmath9 ,",
    "the proof gives a @xmath157 lower bound .",
    "we leave it as an open question to obtain tight bounds for @xmath158 .",
    "hn was supported by nsf ccf 0832797 , and a gordon wu fellowship .",
    "yp s work was supported by the center for science of information ( csoi ) , an nsf science and technology center , under grant agreement ccf-0939370 .",
    "alexandr andoni , robert krauthgamer , and krzysztof onak . streaming algorithms from precision sampling . in _ proceedings of the symposium on foundations of computer science ( focs ) _",
    "full version appears on arxiv:1011.1263 .",
    "lakshminath bhuvanagiri , sumit ganguly , deepanjan kesh , and chandan saha .",
    "simpler algorithm for estimating frequency moments of data streams . in _ proceedings of the acm - siam symposium on discrete algorithms ( soda ) _ , pages 708713 , 2006 .",
    "amit chakrabarti , subhash khot , and xiaodong sun .",
    "near - optimal lower bounds on the multi - party communication complexity of set disjointness . in _",
    "ieee conference on computational complexity _ , pages 107117 , 2003 .",
    "hossein jowhari , mert saglam , and gbor tardos .",
    "tight bounds for @xmath159 samplers , finding duplicates in streams , and related problems . in _ proceedings of the acm symposium on principles of database systems ( pods )",
    "_ , pages 4958 , 2011 .",
    "previously ` http://arxiv.org/abs/1012.4889 ` .",
    "daniel  m. kane , jelani nelson , ely porat , and david  p. woodruff .",
    "fast moment estimation in data streams in optimal space . in _ proceedings of the symposium on theory of computing ( stoc ) _ , 2011 .",
    "a previous version appeared as arxiv:1007.4191 , ` http://arxiv.org/abs/1007.4191 ` .",
    "eric price and david  p. woodruff .",
    "applications of the shannon - hartley theorem to data streams and sparse recovery . in _ proceedings of the 2012 ieee international symposium on information theory _ , pages 18211825 , 2012 ."
  ],
  "abstract_text": [
    "<S> the problem of estimating frequency moments of a data stream has attracted a lot of attention since the onset of streaming algorithms  @xcite . while the space complexity for approximately computing the @xmath0 moment , for @xmath1 $ ] </S>",
    "<S> has been settled @xcite , for @xmath2 the exact complexity remains open . </S>",
    "<S> for @xmath2 the current best algorithm uses @xmath3 words of space @xcite , whereas the lower bound is of @xmath4 @xcite .    in this paper , we show a tight lower bound of @xmath5 words for the class of algorithms based on linear sketches , which store only a sketch @xmath6 of input vector @xmath7 and some ( possibly randomized ) matrix @xmath8 . </S>",
    "<S> we note that all known algorithms for this problem are linear sketches . </S>"
  ]
}