{
  "article_text": [
    "with the increasing precision enabled by modern cosmological observations ( e.g. @xcite ) , there is increasing interest in making their statistical analysis as rigorous as the measurements themselves . in this brief paper",
    "we review the propagation of errors in the covariance matrix to the parameter errors , extending recent work ( @xcite ) to cover errors estimated by marginalising over the likelihood recovered for each mock , and errors measured from the distribution of mocks that are also used to estimate the covariance matrix .",
    "these situations arose in our recent analysis measuring the baryon acoustic oscillation ( bao ) postion in the baryon oscillation spectroscopic survey ( boss ; @xcite ) data release 9 ( dr9 ; @xcite ) galaxy samples @xcite , and in related analyses .",
    "many cosmological observations are well described as being drawn from a multi - variate gaussian distribution with inverse covariance matrix @xmath1 , where the superscript @xmath2 denotes the true matrix , so that parameter inferences ( such as finding the bao position ) can be based on a likelihood @xmath3,\\ ] ] where @xmath4\\psi^t_{ij}\\left[x_j^d - x_j({\\bf p})\\right].\\ ] ] in the example of bao fitting , the data @xmath5 , and model for the data @xmath6 , would be power spectra or correlation functions , with the parameter @xmath7 being the bao position .    in many experiments , it is common to use mock , or simulated , data to estimate the inverse covariance matrix @xmath1 .",
    "suppose we have @xmath8 data measurements such as power spectrum band - powers , and wish to estimate the covariance matrix using @xmath9 simulations .",
    "assuming that the mock data can be written as @xmath10 , with @xmath11 and @xmath12 , the mean of each value over all simulations is @xmath13 and an unbiased estimate of the true covariance matrix @xmath14 from these data is @xmath15 the distribution of matrices recovered from multiple , independent sets of simulations follows the statistics of a wishart distribution , and its inverse @xmath16 , from an inverse - wishart distribution with true inverse covariance matrix @xmath1 ( e.g. @xcite ) .    because we do not know @xmath1 , we can not use eq .",
    "( [ eq : like ] ) directly , but should instead make parameter inferences using a joint likelihood @xmath17 where @xmath18 is given by an inverse wishart distribution , while @xmath19 is the standard distribution given in eq .",
    "( [ eq : like ] ) , after replacing the true inverse covariance matrix with the estimate",
    ". we can subsequently marginalise over @xmath1 to obtain @xmath20 , which can be used to derive parameter measurements .",
    "the marginalisation over all elements in @xmath1 is computationally challenging ; this limitation has led to an approximate approach , where the estimate of @xmath1 is used instead of the true inverse covariance matrix in eq .",
    "( [ eq : like ] ) , and the method and results from this approach are corrected .",
    "marginalising over the distribution of measured covariance matrices in eq .",
    "( [ eq : like_full ] ) leads to two important corrections to this simplified approach :    1 .",
    "the inverse wishart distribution has a form such that @xmath21 , with @xmath22 determined as in eq .",
    "( [ eq : cov_estimate ] ) , is a biased estimate of the inverse covariance matrix .",
    "2 .   the marginalisation over possible true inverse covariance matrices increases the width of the error on any measured parameter from that recovered from @xmath19 .",
    "the first effect can be corrected by using an unbiased estimate of the inverse covariance matrix in the likelihood calculation @xmath23 where the factor @xmath24 accounts for the skewed nature of the inverse wishart distribution ( for the first cosmological application of this , see @xcite ) .",
    "changing the covariance matrix in this manner does not correct for errors in the covariance matrix , which propagate through to errors on estimated parameters , so the second effect is still apparent .",
    "suppose that the inverse covariance matrix estimate has an error @xmath25 compared with the true matrix @xmath1 , with @xmath26 . for simulations drawn from a multi - variate gaussian , these errors can be calculated @xcite , @xmath27 where @xmath28    in the following three sections , we consider how to use these error estimates to correct various parameter error calculations in order to fully account for the errors in the covariance matrix . in section  [ sec : full_err ] , we first follow the derivation of @xcite , calculating the true error for measurements made from data that are independent from that used to estimate the covariance matrix . in section  [ sec :",
    "like ] , we consider how the covariance - matrix errors propagate through to an estimate of the confidence interval derived from an individual likelihoods , and how measurements made from this approach must be corrected to give the true error .",
    "section  [ sec : dist_same_data ] considers the distribution of values recovered when fitting the same simulated data used to estimate the covariance : this exercise serves as a test of the method , allowing the full set of simulations to be used to both create and test the covariance matrix estimate . for consistency and brevity in these sections ,",
    "we follow the notation of @xcite as closely as possible .",
    "the derived formulae are tested using monte - carlo simulations in section  [ sec : mc_tests ] .    while following the propagation of errors in the covariance matrix through to parameter errors",
    "ensures that the estimated parameter errors are unbiased , this calculation does not mean that the corrected @xmath19 provides a maximum likelihood estimator for @xmath7 . instead , the corrected parameter errors depend on the number of bins used when modelling the data , which can be considered as part of the methodology : smaller values of @xmath8 give rise to less noisy estimates of the covariance matrix elements , while we can not determine the elements of larger covariance matrices with the same precision . using larger covariance matrices",
    "leads to increasingly large deviations in the accuracy of the parameter measurements compared with those that would have been made using the true likelihood . in section",
    "[ sec : boss ] we provide a practical demonstration of the corrections , calculating the optimal number of bins to use when performing cosmological analyses of the latest boss galaxy clustering data .",
    "suppose that we have estimated the covariance matrix using a sample of simulations , and wish to know the full error that we should expect on a measurement made using this covariance matrix and the standard gaussian likelihood , or equivalently the expected distribution of best - fit parameter values that would be recovered from an independent set of simulations .",
    "this calculation was performed by @xcite , and corresponds to determining the combined error on a measurement including both the data and covariance matrix errors .",
    "we assume that the likelihood is calculated using the inverse covariance matrix estimate of eq .",
    "( [ eq : psi ] ) . following equation  24 of @xcite , and using the standard summation convention , we can write the estimator for parameter @xmath29 as @xmath30^{-1}_{\\alpha\\alpha ' }      \\frac{\\partial x_i}{\\partial p_{\\alpha'}}\\psi_{ij}(x_j^d - x_j^t),\\ ] ] where @xmath31 is the true fisher matrix linearly relating the fitted parameter @xmath32 to the measurements around the true likelihood peak @xmath33 and similarly for @xmath34 as a function of @xmath25 . also following @xcite , and without loss of generality , we assume that the true values of the parameters @xmath35 .",
    "@xcite expanded eq .",
    "( [ eq : p_alpha ] ) to find the second order ( s.o . ) contribution to the expected distribution of recovered values .",
    "@xmath36 where @xmath37 was given in eq .",
    "( [ eq : ab ] ) , and @xmath38 is the number of parameters measured .",
    "thus , the corrected variance is @xmath39f^{-1}_{\\alpha\\beta}.\\ ] ] this result , which was a key conclusion of @xcite , describes the additional contribution to the data error from a covariance matrix calculated from simulations .",
    "it matches the distribution of best - fit parameter measurements made from a set of simulations that is independent of those used to estimate the covariance matrix .",
    "however , this correction can not be directly applied to an error derived from the likelihood derived from a particular mocks ( as made in @xcite , for example ) , as is demonstrated in the next section .",
    "in order to propagate the uncertainty in the covariance matrix through to errors estimated from the recovered likelihood for a particular fit , we first review how these errors are usually calculated .",
    "the best - fit measurement can be made by integrating over the likelihood @xmath40 with @xmath41 defined as in eq .",
    "( [ eq : chisq ] ) . in the multi - variate gaussian approximation around the best - fit solution",
    ", this expression reduces to eq .",
    "( [ eq : p_alpha ] ) .",
    "the ( squared ) error on the measurement can also be estimated by integrating over the likelihood , @xmath42 if @xmath16 were known perfectly ( i.e. , we replace @xmath16 with @xmath1 ) , eq .",
    "( [ eq : sig_from_like ] ) would recover a parameter variance of @xmath43^{-1}_{\\alpha\\beta}$ ] , from the definition of the fisher matrix .",
    "the error in @xmath16 instead leads to a revised variance estimate @xmath44^{-1}_{\\alpha\\beta}.\\ ] ] a taylor series expansion then gives @xmath45 ignoring first order terms that will lead to zero expectation . using the analogue of eq .",
    "( [ eq : f ] ) for @xmath34 as a function of @xmath25 , and substituting in eq .",
    "( [ eq : dpsi ] ) , we see that the error from the covariance matrix estimation increases the recovered variance to yield @xmath46f^{-1}_{\\alpha\\beta}.\\ ] ] thus , the error in the covariance matrix has a biased effect on errors derived from the likelihood from any particular fit : on average they are larger than the errors would have been if we knew the true inverse covariance matrix .",
    "unfortunately , the increase in size does not match the increase required to correct the distribution of best - fit values recovered from independent data as derived by @xcite , and presented in the previous section . to obtain an unbiased estimate of the full variance on parameter @xmath29 , given a measurement of",
    "the error made using the standard method of integrating over the likelihood , we therefore must apply a factor of @xmath47 to the measured parameter covariance , and the square root of this expression to the measured standard deviation .",
    "because the correction to the measured parameter covariance is independent of the value of the parameters around which the variance is measured , this correction should be applied even if we wish to estimate errors from the recovered likelihood , calculated by fitting to the same data used to estimate the covariance matrix .",
    "in general , one wants to construct the best covariance matrix possible , in order to minimise the additional error .",
    "thus , if this matrix is to be based on simulations , it is strongly desirable to use all available simulations .",
    "a classical approach is to apply any data analysis pipeline to mock data in order to test for any problems .",
    "if all mocks have already been used to estimate the covariance matrix , however , we should not expect to recover a distribution of best - fit solutions that matches the equations derived in section  [ sec : full_err ] .",
    "consequently , it is worth examining how the expected error changes when we analyse the distribution of best - fit values recovered from the same data set used to estimate the covariance matrix . in this case , we can write @xmath48 from eqns .",
    "( [ eq : cov_estimate ] ) &  ( [ eq : psi ] ) . substituting this equation into an expansion of @xmath49 , with @xmath29 as in eq .",
    "( [ eq : p_alpha ] ) , we find @xmath50^{-1}_{\\alpha\\beta}.\\ ] ] using the same approach that led from eq .",
    "( [ eq : var_step1 ] ) to eq .",
    "( [ eq : var_like ] ) , yields @xmath51f^{-1}_{\\alpha\\beta}.\\ ] ] therefore , the distribution of best - fit parameter values recovered from data that was also used to estimate the covariance matrix is biased in a different way to that of an independent set of data , and from the covariance estimate made from the measured likelihood .",
    "however , we can still use the recovered distribution to test the methodology provided we include the revised bias when analysing the result . here",
    "we need a corrective factor @xmath52 with @xmath53 defined as in eq .",
    "( [ eq : m_1 ] ) .",
    "in order to test the relative methods for determining errors , we have created monte - carlo simulations for a model matching that of @xcite . here",
    "we assume that each data vector comprised of @xmath8 values are independently drawn from a standard gaussian distribution ( mean=0 , variance=1 ) , and that @xmath9 of these data vectors are used to calculate a covariance matrix .",
    "the covariance matrix is allowed to include `` apparent '' correlations between different data points , even though the true covariance matrix is diagonal . from any set of data ,",
    "the parameter we wish to estimate is the average @xmath29 , which has the expected value @xmath54 , and true variance @xmath55 .",
    "the 1-dimensional true fisher matrix and its inverse are therefore @xmath56 , @xmath57 .",
    "we have created @xmath58 monte - carlo _ runs _ for every @xmath8 and @xmath9 tested , averaging the measurements over all runs to provide our results . for each run",
    ", we created a set of @xmath9 data vectors from which we calculated the covariance matrix and a set of @xmath9 independent data vectors , which we used to test the fit .",
    "all of these data ( both dependent and independent data sets ) were fitted using the estimated covariance matrix , using eq .",
    "( [ eq : like ] ) to estimate the likelihood .",
    "we therefore performed 2@xmath9 likelihood fits for each run , finding the mean and variance as described in section  [ sec : like ] .",
    "estimates of the variance derived in different ways from these fits are shown in fig .",
    "[ fig : var_toy ] .",
    "we do not apply any bias corrections to these data , but instead plot them as if they had been naively used to estimate the true variance .",
    "the average variance of the distribution of best - fit parameters recovered from the fits to the independent sets of data are shown by the open circles in fig .",
    "[ fig : var_toy ] , and are well matched to the formula derived by @xcite ( dot - dash line , given by eq .",
    "[ eq : var_idist ] ) .",
    "these data represent the true error that should be quoted on measurements . the difference between these data and the solid line shows the extra variance introduced by the noisy covariance matrix estimate .",
    "if we estimate the variance using the likelihood , or using the distribution of data also used to estimate the covariance matrix , we find a biased value .",
    "the average variance recovered by integrating over the likelihood as in eq .",
    "( [ eq : sig_from_like ] ) is plotted in fig .",
    "[ fig : var_toy ] ( solid circles ) - the root of these values are commonly quoted as parameter errors in analyses . as described in section  [ sec : like ] , for parameters that linearly depend on the data ( or in the standard approximation around the likelihood maxima ) , the best - fit value around which we measure the variance does not matter .",
    "thus we recover exactly the same likelihood errors in our model whether we use the independent data , or the data also used to estimate the covariance matrix .",
    "these estimates are biased and the offset is well matched by eq .",
    "( [ eq : var_like ] ) , which is indicated by the dashed line in the plots .",
    "the solid triangles show the variance estimated from the distribution of best - fit values recovered from the same data set used to calculate the covariance matrix .",
    "these points are well matched to the dotted line , calculated using the formula given in section  [ sec : dist_same_data ] .",
    "as can be seen , this estimate of the variance is biased low , as a consequence of the offset between the estimated covariance and the inverse covariance matrix as given by the extra factor in @xmath59 compared with @xmath53 .    in this plot ,",
    "the factor @xmath53 is the ratio between the dashed and dot - dashed lines , and @xmath59 is the ratio between dotted and dot - dashed lines .",
    "these factors correct these estimates to produce the true combined error ( dot - dash line ) including both the standard variance and the effect of the noisy covariance matrix .",
    "we now apply the calculations described above to investigate cosmological measurements made with the power spectrum and correlation function from boss . in this work ,",
    "we focus on the cmass galaxy sample , although our results could also be applied to the lowz sample .",
    "boss @xcite is part of the sloan digital sky survey - iii ( sdss - iii ; @xcite ) project , which used the sdss telescope @xcite to obtain imaging @xcite and spectroscopic @xcite data , which was then reduced @xcite to provide a sample of galaxy redshifts , with known mask .",
    "we focus on the bao methodology described in @xcite , and the rsd methodology of @xcite .    in @xcite and @xcite , we used @xmath60 pthaloe mock catalogues to analyse the boss dr9 sample , both to understand the analysis methodology and to determine covariance matrices for the 2-point measurements .",
    "these mock catalogues were created as described in @xcite .",
    "briefly , @xmath60 2nd - order lagrangian perturbation theory ( 2lpt ) matter fields were created in boxes of size @xmath61 , sampled by @xmath62 dark matter particles . within these boxes , haloes were found with a friends - of - friends group finder @xcite with appropriate linking length , and their masses were calibrated by detailed comparisons with n - body simulations .",
    "the halos were populated with mock galaxies using a halo occupation distribution ( hod ; @xcite ) prescription , which was calibrated to reproduce the clustering measurements on scales between @xmath63 and @xmath64 .",
    "mock catalogues were then created by sampling these boxes to match the geometry and efficiency of the project .",
    "mock catalogues have also been drawn from these boxes for the dr10 @xcite and dr11 samples used in @xcite to measure the bao positions @xcite .    in order to create the mocks",
    ", we treat the northern galactic cap ( ngc ) and southern galactic cap ( sgc ) components of the survey as being independent , and sample them separately from the same set of boxes . for the dr9 analysis , we could easily sample the north and south components of the survey from the 600 boxes without overlap , giving 600 ngc mocks and 600 sgc mocks that are independent .",
    "given the volume covered by the dr10 and dr11 boss cmass galaxy samples , we could not easily sample both parts of the survey from each box without overlap , meaning that the ngc and sgc mocks drawn from the same box are not independent .",
    "to construct joint ngc+sgc mocks , we sample the ngc from one subset of 300 simulations and combine these with samples of the sgc from the remaining independent simulations .",
    "an equivalent set of combined mocks can be created by instead sampling the sgc from the first subset of 300 simulations and the ngc from the remaining 300 simulations .",
    "while both of these sets should provide unbiased estimates of the covariance matrix , they are in principle correlated with each other , as the set of ngc mocks used to calculate one is correlated with the set of sgc mocks used to calculate the other .",
    "we then estimate the covariance matrix for the joint ngc+sgc power spectrum as the average from these two , each calculated from 300 ( ngc+sgc ) mock power spectra . the final equation for our covariacne matrix is @xmath65[p^m_j(k)-\\bar{p_j}(k ) ]    \\nonumber\\\\    & + &     \\frac{1}{299}\\sum_{m>300}[p^m_i(k)-\\bar{p_i}(k)][p^m_j(k)-\\bar{p_j}(k ) ] ,    \\label{eq : c_pk}\\end{aligned}\\ ] ] where @xmath66 is the measured power spectrum from mock @xmath67 in bin @xmath68 , and @xmath69 is the mean calculated separately for each set of 300 mocks .",
    "a similar equation is used to calculate the covariance matrix for the correlation function .",
    "although this approach produces an unbiased estimate of the covariance matrix , the two contributions are correlated , so the sum would not produce the @xmath70 reduction in noise in the covariance matrix as would be expected for the combination of independent estimates , if we could approximate components as being gaussian .",
    ".the effective areas of the dr9 , dr10 and dr11 boss cmass galaxy samples , and the overlap areas when mocks are sampled from the same parent box .",
    "@xmath71 is the correlation coefficient between estimators and @xmath72 reflects the reduction of the covariance errors when the estimators are combined .",
    "[ cols=\"^,^,^,^,^,^ \" , ]     in fact , for dr9 , when projected into the mock boxes , the ngc and sgc components of the survey only have a small overlap and we were therefore justified in treating both sets of mocks as independent .",
    "however , for dr10 the overlap is approximately 75 per cent of the area covered by the sgc , while for dr11 the entire southern component is also covered by the ngc ( see table  [ tab : areas ] ) . if we assume that the variance on the measurement is proportional to the inverse of the effective volume @xmath73 , the correlation coefficient between ( ngc+sgc ) mock measurements with overlapping ngc and sgc components , so that the ngc for one overlaps with the sgc of the other , and vice - versa , is given by @xmath74 .",
    "the degree of overlap above results in @xmath75 for dr10 and @xmath76 for dr11 .",
    "again , to be explicit , this correlation coefficient represents how strongly the power spectrum error in one mock correlates with that in another mock , where the two mocks sample either ncg+scg or sgc+ngc from two simulations .",
    "ultimately we are interested in how these correlations impact the covariance error resulting from the combined estimator of eq .",
    "( [ eq : c_pk ] ) , compared to the covariance error which arises from using a single set of @xmath77 mock catalogues .",
    "we find that the power spectrum correlation propagates into the combined covariance error , which we effectively model by rescaling the error terms given by @xmath78 , @xmath37 and @xmath24 in eqns .",
    "( [ eq : ab ] ) &  ( [ eq : psi ] ) by a factor of @xmath72 , which is that standard formula for the variance of the average of two correlated random variables . in the limit of large @xmath9 ,",
    "the @xmath37 term dominates over the @xmath78 term in eq .",
    "( [ eq : ab ] ) ; this is equivalent to rescaling the number of simulations by a factor of @xmath79 .",
    "thus , for dr9 , where the correlations are negligible , the effect is simply to increase the effective number of simulations by a factor of two , as one might expect .",
    "error - bars for the bao measurements presented in @xcite were derived from the likelihood calculated from fitting either the isotropically averaged power spectrum or correlation function with a model that marginalises out the broadband components of the 2-point functions , leaving the bao whose scale can be measured . for the power spectrum analysis of @xcite , we fitted 70 band - powers with a model including 11 parameters , and neither the correction shown in eq .",
    "( [ eq : psi ] ) nor the factor in eq .",
    "( [ eq : m_1 ] ) were applied to the inverse covariance matrix .",
    "both the factors are of the order 10 per cent , and act to increase the size of the variance from the raw value measured .",
    "the quoted errors on the power - spectrum based bao position measurements provided in @xcite should therefore be increased by 12 per cent given the current analysis : i.e. post reconstruction , we quoted @xmath80 , but these will change with the current error analysis to @xmath81 .    for the correlation function analysis of @xcite , we fitted 44 binned points @xmath82 , using a model with five free parameters .",
    "as with the fits based on the power spectrum , error - bars were derived from the likelihood , and neither the correction to the inverse covariance matrix estimate ( eq .  [ eq : psi ] ) nor the correction because of the error in the covariance matrix ( eq .",
    "[ eq : m_1 ] ) were applied .",
    "because of the reduced number of bins and degrees of freedom , the corrections are slightly smaller than in the power spectrum case , and are of order 4 per cent and 3 per cent respectively for the error . the errors on the correlation - function based bao position measurements provided in @xcite would therefore need to be increased by 7 per cent given the current analysis : i.e. , post reconstruction , @xcite quoted @xmath83 , but these values will change with the current error analysis to @xmath84 .      the default boss dr10 analysis presented in @xcite uses 600 mocks , calculated as for dr9 , but with an updated angular mask .",
    "we have measured the power spectrum for each of these mocks after reconstruction , using the standard pipeline described in @xcite .",
    "each power spectrum was binned into a large number of fine bins , which were then combined to produce results for various numbers of bins within the range of scales fitted @xmath85 . for each binning choice , we have estimated the covariance matrix , and window function , and used these to fit the data with a model given by @xmath86,\\ ] ] where the bao scale @xmath87 , and the damping @xmath88 are parameters , and @xmath89 is a smooth model for the broad - band shape of the power spectrum , and @xmath90 are the baryon acoustic oscillations extracted from the linear power spectrum @xmath91 .",
    "we have changed the fitting method from that in @xcite in two key ways :    1 .",
    "we fit band - powers in @xmath92 , which was shown to be close to having a multi - variate gaussian distribution in @xcite , as expected in the sample - variance limited regime .",
    "we use a model for the broad band power spectrum @xmath93 which is better matched to that used for the correlation function , than the @xmath94 model used in @xcite .",
    "our final model has six `` nuisance '' parameters , @xmath95 , @xmath96 , @xmath97 , @xmath98 , @xmath99 , and @xmath100 ; see @xcite for further discussion of this issue .    for each mock ,",
    "we have determined the best fit value of @xmath87 and @xmath101 by marginalising over the other parameters using the derived likelihood . in this calculation",
    "we assumed a gaussian prior on @xmath88 of @xmath102 centred on the best - fit values determined by fitting the average recovered power spectrum . in principle , the boss data alone can measure this parameter simultaneously with the bao scale measurement , albeit at the expense of an increase in the error .",
    "in fact we have a strong prior from theory about the amplitude of this damping , which we include to reduce the impact on the bao scale error ( for more details see @xcite ) .",
    "we need to apply the corrections determined in section  [ sec : like ] to the errors derived from the likelihood and , as the covariance matrix was also calculated from the same mocks used to determine the covariance matrix , the correction of section  [ sec : dist_same_data ] to the distribution of best - fit values .",
    "the resulting measurements of the expected error on @xmath87 are shown in fig .",
    "[ fig : pk_results ] as a function of the number of bins in the fitted range @xmath103 .",
    "the upper sets of points and lines are pre - reconstruction , with the lower set , corresponding to the more accurate fits , post - reconstruction .",
    "the raw errors from these calculations are shown as the open ( from the likelihood ) and solid ( from the distribution ) circles , with the results after correction represented by the lines .",
    "the lower panel presents the percentage deviation of the mean , calculated from all of the mocks for different numbers of bins .",
    "this represents a systematic error on the recovered value of @xmath87 .    from fig .",
    "[ fig : pk_results ] we see that , after correction , the values of the errors recovered from the distribution and from the likelihood agree to a higher degree than before correction , particularly for small values of the bin width .",
    "there is an error on this match that results from the error in the covariance matrix , with the data from different bin widths being highly correlated .",
    "we expect this error to be of the same order as the difference between the corrections applied to the likelihood and distribution based errors as this difference results from the offset within the wishart distribution from which the covariance matrix is derived ( see section  [ sec : intro ] ) .",
    "it therefore gives a crude estimate for the width of this distribution .",
    "this reasoning shows that the differences between corrected errors derived in the different ways are consistent .",
    "without correction , the statistical errors recovered from both methods decrease with increasing bin width , naively suggesting that increasing the number of bins increases the information content .",
    "in fact , the post - correction errors increase for small bin widths , demonstrating that we are simply transferring data noise into covariance matrix noise as we increase the number of bins , which is not appearing in the raw error calculation . after correction",
    "the recovered errors are reassuringly independent of bin width for a wide range of bin widths . for small numbers of bins ,",
    "the mean offset in the bao location measured is small compared with the statistical errors , and is of order 0.4% for the pre - reconstruction fits , while it is consistent with zero post - reconstruction , with an error of 0.04% for all bin widths .",
    "the size of the systematic offset is not dependent on the bin width , giving us confidence that we are correctly modelling the binning effects .",
    "the low amplitude of the systematic errors post - reconstruction strongly suggests that we do not have any systematic biases due to the survey mask , our modelling of the resulting window function , or effects from the galaxy bias as implemented within the pthaloe methodology @xcite .    comparing both the offset in the mean value recovered and",
    "the recovered errors indicates that the optimum number of bins for the power spectrum analysis over @xmath103 is approximately @xmath104 with bin width @xmath105 , half the number of bins used in the dr9 analysis of @xcite .",
    "for such a small number of bins , the corrections required for the derived errors are small : eq .",
    "( [ eq : m_1 ] ) suggests that the likelihood derived errors need to increase by @xmath106 per cent .",
    "we have performed a similar analysis to determine the optimum bin size for the bao fits to the isotropic correlation function . following the methodology adopted for @xcite , we fix the bao damping scale , leaving a 5 parameter model composed of @xmath87 and a 4-parameter broad - band model that is similar to that described for the power spectrum in the previous section @xmath107 here",
    ", @xmath108 is the fourier transform of a linear model for the correlation function with damped bao ( see @xcite for more details ) , and @xmath109 with ( @xmath110 ) are free parameters that marginalise over the broadband signal . because the bao signal in the correlation function does not extend to non - linear scales to the same extent as in the power spectrum",
    ", the broad - band model can be added to the linear correlation function , which includes the bao signal , rather than multiplying the bao as in the @xmath94 model ( eq .  [ eq : mod_pk ] ) .",
    "this leaves a correlation - function model with more freedom to dampen the bao .",
    "thus , for our correlation function fits , we fix @xmath111 rather than including it as a free parameter with a gaussian prior as for the power spectrum .",
    "the consequences of this difference are discussed further in @xcite .    as for the power spectrum",
    ", we have fitted all 600 mocks using this model to determine a likelihood distribution for each . from this exercise",
    "we have derived best - fit values and expected errors on @xmath87 , marginalising over other parameters .",
    "we have also estimated the width of the distribution of recovered best - fit values , taking care to produce an unbiased estimate by splitting the mocks into two sets of 300 independent measurements .",
    "the resulting errors , plotted as a function of number of bins , are shown in fig .",
    "[ fig : xi_results ] .",
    "the difference between results from the likelihood and the distribution are similar to those for the power spectrum fits .",
    "the size of these discrepancies are similar to the correction applied , and as such may simply be a statistical deviation within the expected distribution .",
    "the results from different bin choices are obviously correlated to a high degree .",
    "we do not attempt to estimate the error on the correction we are applying to the error - i.e. , the error on the error on the error .",
    "[ fig : xi_results ] reveals a flat minimum , with bins of width @xmath112@xmath113 all providing similar final errors on the bao scale .",
    "we therefore recommend that the monopole of the correlation function , when fitted independently , be binned with width @xmath114 .",
    "there is no evidence that binning on these scales induces a systematic error due to the coarseness of the averaging .",
    "we have also considered fits to the monopole and quadrupole moments of the correlation function using the methodology applied in @xcite and @xcite . for simplicity we only present results from the dr11 data , although similar results are produced for dr10 . additionally , similar results are observed for fits to `` wedges '' : top - hat averages of the anisotropic correlation function in the cosine of the angle to the line - of - sight ( for more information see * ? ? ?",
    "[ fig : bao2d_results ] presents the average errors on @xmath115 and @xmath116 from the fits to the 600 mocks as a function of bin size . as in fig .",
    "[ fig : pk_results ] , these errors are shown with and without the correction factors for the error in the covariance matrix .",
    "the behaviour of the fits in the anisotropic case is quite similar to those from just fitting the monopole of the correlation function ( fig .",
    "[ fig : xi_results ] ) .",
    "the minimum is quite broad , just pushing to slightly larger bin sizes than the monopole - only fits . given our preference for simplicity ,",
    "we adopt a bin size of @xmath114 for fits to both monopole only , or monopole and quadrupole , rather than using a different bin for the two measurements .",
    "the likelihood - based and distribution - based results are well matched after correcting for the covariance matrix effects , as for the monopole only fits .",
    "there is some evidence for a small @xmath00.5% systematic offset on @xmath116 , which was also seen in @xcite .",
    "there is also evidence for `` oscillatory behaviour '' of the errors as a function of bin width , which is particularly apparent for the post - reconstruction fits . for our binning scheme ,",
    "as we increase the bin width , we also alter the positions of the bin centres .",
    "the ability to fit the position of the bao is very sensitive to the bin centre for bins that cover the bao signal , and are large compared to that signal .",
    "this leads to variations in the recovered errors as seen .",
    "we also see an increase in the systematic offset for large bins , which is coupled to this lack of resolution .",
    "clearly it is desirable that this region is avoided .",
    "we now extend the analysis to consider rsd measurements made from joint fits to the monopole and quadrupole moments of the correlation function .",
    "we limit the analysis to have the same bin width for both , and consider how this choice affects the error on the final measurement . for this analysis",
    ", we have three free parameters :    1 .   the amplitude of the real - space galaxy power spectrum , quantified by @xmath117 , where @xmath118 is the root - mean - square amplitude of overdensity fluctuations in spheres of radius @xmath114 .",
    "the amplitude of the velocity field , which controls the rsd amplitude , and is quantified by @xmath119 where @xmath120 is the linear growth rate 3 .",
    "the width of the gaussian probability distribution function assumed to model the non - linear fingers - of - god , @xmath121 .",
    "further details about these parameters can be found in @xcite . for speed ,",
    "given the number of fits to be performed , unlike in @xcite , we do not allow the shape of the real - space power spectrum or the two dilation parameters @xmath116 and @xmath115 that control the radial and angular projections to vary , and fix them at their true values .",
    "we do not expect this decision to alter our conclusions significantly given that this shape is highly constrained by the recent planck results @xcite .",
    "the results of our fits can be seen in fig .",
    "[ fig : rsd_results ] , where we compare the standard deviations of the distribution of recovered values of @xmath122 against bin width . for each fit , we do not attempt to map the full likelihood , but instead use a minimisation routine to find the maximum of the likelihood in parameter space .",
    "thus we only present results from the distribution of recovered best - fit values . given the similarity between results derived from individual likelihood distributions , and from the distributions presented in sections  [ sec : bao_pk ] &  [ sec : bao_xi ] , we believe that this approach is sufficient to determine the best bin width .    as can be seen in fig .",
    "[ fig : rsd_results ] , for narrow bin width where large numbers of bins are used in the covariance matrix , there is an increase in the corrected error as for the bao fits .",
    "there is no increase to large bin widths because the rsd measurement is effectively an amplitude determination unlike bao fitting , which is a centroiding problem and therefore large bin widths are more detrimental .",
    "thus rsd measurements are less sensitive to the bin width chosen .",
    "most rsd determinations ( e.g. , * ? ? ?",
    "* ) perform a joint fit including the shape of the 2-point measurement , and therefore the best - fit bao bin width of @xmath123 remains an optimal choice .",
    "the systematic errors shown in the lower panel of fig .  [ fig : rsd_results ] are relatively large compared with those from bao measurements .",
    "the large errors are partially due to the 2lpt mocks not reproducing the nonlinear evolution of the growth rate exactly .",
    "the systematic offset would decrease if we fitted a 2lpt model to the measurements instead of the nonlinear streaming model , which is more accurate for the data ( see @xcite for more details ) .",
    "in this paper we have reviewed the calculations being performed using the latest boss data in order to extract cosmological measurements .",
    "building upon a series of recent papers examining the errors in the inverse covariance matrix used in cosmological applications @xcite , we have had to derive and understand the effect of two further errors in two further situations - where the error on final parameters is calculated by integrating over the derived likelihood and , in order to test the method , the distribution of best - fit values recovered from the same set of mocks used to determine the covariance matrix .",
    "these derivations have been tested , and shown to be accurate using monte - carlo simulations .",
    "to summarise , there are two corrections that must be applied to the `` naive '' analysis simply inverting the covariance matrix derived from eq .",
    "( [ eq : cov_estimate ] ) , and using it in eqns .",
    "( [ eq : like ] ) &  ( [ eq : chisq ] ) . first , as pointed out by @xcite , we must correct for the offset nature of the inverse wishart distribution by correcting the inverse covariance matrix by the factor given in eq .",
    "( [ eq : psi ] ) .",
    "second , we need to correct for the additional contribution of the error in the covariance matrix to the final error on a derived parameter .",
    "three different corrections to create unbiased error estimates exist in different situations :    1 .",
    "if the variance of a measurement is estimated from the distribution of best - fit values recovered from data that are independent of that used to estimate the covariance matrix , the variance on the result is given in eq .",
    "( [ eq : var_idist ] ) @xcite .",
    "this variance corresponds to the true error on measurement from data ( which are independent from the mocks used to calculate the covariance matrix ) .",
    "2 .   if the variance is measured from a likelihood , calculated from fitting to a set of data ( be it from independent mocks , the same mocks used to estimate the covariance matrix , or the actual data ) , we derive a biased estimate of the variance , which is different from the expression given by eq .",
    "( [ eq : var_idist ] ) . to correct back to this variance",
    ", we must apply the correction @xmath53 , given in eq .",
    "( [ eq : m_1 ] ) to the derived estimate .",
    "3 .   if the variance is derived from the distribution of best - fit values recovered from the same data also used to estimate the covariance matrix , we also obtain a biased result , and must now apply the factor @xmath59 given in eq .",
    "( [ eq : m_2 ] ) to the estimate .",
    "we have considered how the mocks used to determine the covariances for boss affect parameter inferences , and have shown how they must be carefully analysed in order to take into account how they were produced , in particular the overlap between ngc and sgc components .",
    "having done this , we have not only included the extra errors in our final measurement errors given in companion papers @xcite , but also used the derivation to understand the effect of bin size on the final errors .",
    "we have derived optimal binning strategies for bao fits to the monopole correlation function and isotropically - averaged power spectrum , and anisotropic bao fits and rsd fits to the monopole and quadrupole moments of the correlation function .",
    "these best - fit strategies are dependent on the level of precision achieved within the covariance matrix . if more mocks were used , or higher precision could be achieved in some other way , then fits using more bins would become more desirable . however , after applying all corrections , the isotropically averaged bao distance scale error recovered from the mocks is quite independent of bin size over a broad range of bin widths .",
    "this suggests that our best strategy will not change significantly even with better precision for the covariance matrix .",
    "the lack of sensitivity to bin size is good to see , as one would hope that the analysis method does not have a strong effect on the final measurements .",
    "the ability to recover the bao scale without significant loss of accuracy using large bin sizes up to @xmath124 for @xmath125 , is perhaps more surprising , although we note that the bao feature is quite broad .",
    "our analysis on bin sizes demonstrates that , on average , after correction , the recovered errors derived in multiple ways are a better match to each other than before correction .",
    "however , we caution that this match depends on the actual noise in the covariance matrix , which might be expected to be of the same order as the difference between correction factors .",
    "this match also relies on the model adopted being a good fit to the data . for the fit to bao positions",
    "it is clear that a poor model can yield incorrect likelihood errors , while leaving the distribution of best - fit values relatively unaffected . the damping term in eq .",
    "[ eq : mod_pk ] is critical here - for any fit to data , if the model is over - damped , the likelihood maximum will be reduced as the model has more freedom to move , although the best - fit location for each mock will generally not change by the same amount . for an under - damped model , the likelihood maximum",
    "will be increased , although the data themselves do not support such an apparent improvement in errors , as evidenced by the recovered distribution of best - fit values .",
    "further investigation is required , but is outside of the remit of this paper .    the comparison of bao measurement errors as a function of bin size raises the interesting question of why the corrected error increases for increasing numbers of bins .",
    "the covariance matrix for large numbers of bins obviously still contains all the information used with a smaller number of bins , so theoretically you should be able to extract the same information from it and the data . as discussed in section  [ sec : intro ] , the correct approach is to construct a joint likelihood of the data and mocks given the cosmological model to be tested . in the standard gaussian assumption on the distributions of mocks and data ,",
    "this is the same as that given by eq .",
    "( [ eq : like_full ] ) .",
    "marginalising over the true covariance matrix would then yield the final likelihood for the parameters given the mocks - in essence this should be the same for any bin choice for smooth models , where the binning results in minimal loss of information .",
    "the problem is that we re not performing this optimal likelihood approach if we assume that the estimated covariance matrix is `` correct '' and use the standard likelihood equation ( eq .  [ eq : like ] ) . in this approach ,",
    "the effect of the covariance matrix , and the error it introduces through eq .",
    "( [ eq : like ] ) , changes with bin size : this dependence is given in eqns .",
    "( [ eq : dpsi ] ) &  ( [ eq : ab ] ) , and is propagated through to the final error on the recovered parameters .",
    "thus , the optimal bin size is actually only an optimal bin size if you want to retain eq .",
    "( [ eq : like ] ) as the likelihood equation - in this case the error does depend on bin size , and the error increases with increasing number of bins .",
    "the increase to large bin sizes can be more easily understood - here we are simply losing information as the averaging being performed increases in importance , leading to increasing errors .",
    "the minimum in the recovered error balances these two effects .",
    "in sections  [ sec : bao2d ] &  [ sec : rsd ] , we saw that the corrections required to the combined fits to both the monopole and quadrupole are quite large for both bao and rsd measurements .",
    "this result suggests that there are significant gains to be obtained either creating more accurate covariance matrices , or by reworking the likelihood calculation to include covariance matrix errors . for future surveys",
    ", this effect will become increasingly important , and having too few mocks , or too poor a model for the covariance matrix will have a serious impact on the measurements made .",
    "wjp acknowledges support from the uk science & technology facilities council ( stfc ) through the consolidated grant st / k0090x/1 , and from the european research council through the `` starting independent research '' grant 202686 , mdepugs .",
    "ags acknowledges support from the trans - regional collaborative research centre tr33 ` the dark universe ' of the german research foundation ( dfg ) .",
    "power spectrum calculations and fits made use of the cosmos / universe super - computer , a uk - dirac facility supported by hefce and stfc in cooperation with cgi / intel .",
    "funding for sdss - iii has been provided by the alfred p. sloan foundation , the participating institutions , the national science foundation , and the u.s .",
    "department of energy office of science .",
    "the sdss - iii web site is http://www.sdss3.org/.    sdss - iii is managed by the astrophysical research consortium for the participating institutions of the sdss - iii collaboration including the university of arizona , the brazilian participation group , brookhaven national laboratory , university of cambridge , carnegie mellon university , university of florida , the french participation group , the german participation group , harvard university , the instituto de astrofisica de canarias , the michigan state / notre dame / jina participation group , johns hopkins university , lawrence berkeley national laboratory , max planck institute for astrophysics , max planck institute for extraterrestrial physics , new mexico state university , new york university , ohio state university , pennsylvania state university , university of portsmouth , princeton university , the spanish participation group , university of tokyo , university of utah , vanderbilt university , university of virginia , university of washington , and yale university ."
  ],
  "abstract_text": [
    "<S> we present improved methodology for including covariance matrices in the error budget of baryon oscillation spectroscopic survey ( boss ) galaxy clustering measurements , revisiting data release 9 ( dr9 ) analyses , and describing a method that is used in dr10/11 analyses presented in companion papers . </S>",
    "<S> the precise analysis method adopted is becoming increasingly important , due to the precision that boss can now reach : even using as many as 600 mock catalogues to estimate covariance of 2-point clustering measurements can still lead to an increase in the errors of @xmath020% , depending on how the cosmological parameters of interest are measured . in this paper </S>",
    "<S> we extend previous work on this contribution to the error budget , deriving formulae for errors measured by integrating over the likelihood , and to the distribution of recovered best - fit parameters fitting the simulations also used to estimate the covariance matrix . </S>",
    "<S> both are situations that previous analyses of boss have considered . </S>",
    "<S> we apply the formulae derived to baryon acoustic oscillation ( bao ) and redshift - space distortion ( rsd ) measurements from boss in our companion papers . to further aid these analyses , </S>",
    "<S> we consider the optimum number of bins to use for 2-point measurements using the monopole power spectrum or correlation function for bao , and the monopole and quadrupole moments of the correlation function for anisotropic - bao and rsd measurements .    </S>",
    "<S> [ firstpage ]    cosmology : observations , distance scale , large - scale structure </S>"
  ]
}