{
  "article_text": [
    "consider the canonical non - parametric regression setup @xmath0 where @xmath1 is an unknown function in @xmath2 $ ] , @xmath3 and the @xmath4 s are i.i.d .",
    "standard gaussian random variables .",
    "we assume the noise level @xmath5 is known and , without loss of generality , set @xmath6 throughout .",
    "based on observing @xmath7 , estimating @xmath1 or various functionals of @xmath1 has been the central problem in non - parametric function estimation .",
    "the  asymptotic optimality of estimators is usually associated with the optimal rate of convergence in terms of minimax risk . a  huge body of literature has been devoted to the evaluation of minimax risks under @xmath8 loss over certain function spaces ; see , for example , pinsker @xcite , ibragimov and hasminskii @xcite , golubev and nussbaum @xcite , efroimovich @xcite , belitser and levit @xcite and goldenshluger and tsybakov @xcite .",
    "an excellent survey of the literature in this area can be found in efromovich @xcite .    sometimes , instead of estimating @xmath1 itself , one is interested in making statistical inference about future observations from the same process that generated @xmath9 .",
    "a  predictive distribution function assigns probabilities to all possible outcomes of a random variable .",
    "it thus provides a complete description of the uncertainty associated with a prediction .",
    "the  minimaxity of predictive density estimators has been studied for finite - dimensional parametric models ; see , for example , liang and barron @xcite , george , liang and xu @xcite , aslan @xcite and george and xu @xcite .",
    "however , so far , few results have been obtained on predictive density estimation for non - parametric models .",
    "the  major thrust of this paper is to establish the asymptotic minimax risk for predictive density estimation under kullback ",
    "leibler loss in the context of non - parametric regression .",
    "our result closely parallels the well - known work by pinsker @xcite for non - parametric function estimation under @xmath8 loss and provides a benchmark for studying the optimality of density estimates for non - parametric regression .",
    "let @xmath10 denote a vector of future observations from model ( [ eq : x ] ) at locations @xmath11 .",
    "to evaluate the performance of density prediction across the whole curve , we assume that the @xmath12 s are equally spaced dense ( that is , @xmath13 ) grids in @xmath14 $ ] . given @xmath1 , the conditional density @xmath15 is a product of @xmath16 , where @xmath17 denotes a univariate gaussian density function with mean @xmath18 and unit variance .",
    "based on observing @xmath19 , we estimate @xmath15 by a predictive density @xmath20 , a non - negative function of @xmath21 that integrates to 1 with respect to  @xmath21 .",
    "common approaches to constructing @xmath20 includes the `` plug - in '' rule that simply substitutes an estimate @xmath22 for @xmath1 in @xmath15 , @xmath23 and the bayes rule that integrates @xmath1 with respect to a prior @xmath24 to obtain @xmath25 we measure the discrepancy between @xmath15 and @xmath20 by the average kullback ",
    "leibler ( kl ) divergence @xmath26 assuming that @xmath1 belongs to a function space @xmath27 , such as a sobolev space , we are interested in the minimax risk @xmath28    it is worth observing that in this framework , the densities of future observations @xmath29 are estimated simultaneously by @xmath20 .",
    "an alternative approach is to estimate the densities individually by @xmath30 with risk @xmath31 when the @xmath12 s are equally spaced and @xmath32 goes to infinity , the risk above converges to @xmath33 which can be interpreted as the integrated kl risk of prediction at a random location @xmath34 in @xmath14 $ ] .",
    "this individual prediction problem can be studied in our simultaneous prediction framework with @xmath20 restricted to a product form , that is , @xmath35 .",
    "for example , the plug - in estimator ( [ hp : plug - in ] ) has such a product form and it is easy to check that its individual estimation risk ( [ risk : individual ] ) is the same as its simultaneous estimation risk ( [ risk : simultaneous ] ) .",
    "in general , simultaneous prediction considers a broader class of @xmath36 than the one considered by individual prediction .",
    "therefore , simultaneous prediction is more efficient since the corresponding minimax risk ( [ minimax : f ] ) is less than or equal to the one with individual prediction .",
    "this is distinct from estimating @xmath1 itself under @xmath8 loss where , due to the additivity of @xmath8 loss , simultaneous estimation and individual estimation are equivalent .",
    "this paper is organized as follows . in section [ sec2 ] , we show that the problem of predictive density estimation for a non - parametric regression model can be converted to the one for a gaussian sequence model with a constrained parameter space",
    ". direct evaluation of the minimax risk is difficult because of the constraint on the parameter space .",
    "therefore , in section [ sec3 ] , we first derive the minimax risk over a special class of @xmath36 that consists of predictive densities under gaussian priors on the unconstrained parameter space @xmath37 .",
    "then , in section [ sec4 ] , we show that this minimax risk is asymptotically equivalent to the overall minimax risk .",
    "finally , in section [ sec5 ] , we provide two explicit examples of minimax risks over @xmath8 balls and sobolev spaces .",
    "let @xmath38 be the orthonormal trigonometric basis of @xmath2 $ ] , that is , @xmath39 then , @xmath40 , where @xmath41 is the coefficient with respect to the @xmath42th basis element @xmath43 . a  function space",
    "@xmath27 corresponds to a constraint on the parameter space of @xmath44 . in this paper",
    ", we consider function spaces whose parameter spaces @xmath45 have ellipsoid constraints , that is , @xmath46 where @xmath47 and @xmath48 .",
    "we approximate @xmath1 by a finite summation @xmath49 .",
    "the  bias incurred by estimating @xmath50 instead of @xmath51 can be expressed as @xmath52 ^ 2   =   \\frac{1}{2 m } \\sum_{i = n+1}^{\\infty } \\theta_i^2.\\end{aligned}\\ ] ] this bias is often negligible compared to the prediction risk ( [ risk : simultaneous ] ) ; for example , it is of order @xmath53 for sobolov ellipsoids @xmath54 , as defined in ( [ sobolev_space ] ) .",
    "therefore , from now on , we set @xmath55 .",
    "let @xmath56 , @xmath57 be a @xmath58 matrix whose @xmath59th entry equals @xmath60 and @xmath61 be a @xmath62 matrix whose @xmath59th entry equals @xmath63 . then",
    ", @xmath64 and @xmath65 are two independent gaussian vectors with @xmath66 and @xmath67 , where @xmath68 denotes the @xmath69 identity matrix .",
    "note that since the @xmath70 s and @xmath12 s are equally spaced , we have @xmath71 and @xmath72 . defining @xmath73",
    "it is then easy to check that @xmath74 and @xmath75 are independent and that@xmath76 where @xmath77 and @xmath78 .",
    "we refer to the model above as a _ gaussian sequence model _ since its number of parameters is increasing at the same rate as the number of data points .",
    "consider the problem of predictive density estimation for the gaussian sequence model ( [ model : normal ] ) .",
    "let @xmath79 denote a predictive density function of @xmath80 given @xmath81 .",
    "the  incurred kl risk is defined to be @xmath82 and the corresponding minimax risk is given by @xmath83 the  following theorem states that the two minimax risks , the one associated with @xmath84 from a non - parametric regression model and the one associated with @xmath85 from a normal sequence model , are equivalent .",
    "[ teo2.1 ] @xmath86 , where @xmath87 is defined in and @xmath88 in .",
    "see the .",
    "the  idea of reducing a non - parametric regression model to a gaussian sequence model via an orthonormal function basis has been widely used for non - parametric function estimation .",
    "early references include ibraginov and hasminskii @xcite , efromovich and pinsker @xcite and references therein",
    ". for recent developments , see brown and low @xcite , nussbaum @xcite and johnstone @xcite .",
    "our proof of theorem [ teo2.1 ] , given in the , implies that _ simultaneous _ estimation of predictive densities in these two models are equivalent .",
    "however , this equivalence does not hold for the _ individual _ estimation approach described in section [ sec1 ] because the product form of the density estimators , that is , @xmath89 , is not retained under the transformation .",
    "direct evaluation of the minimax risk ( [ def : r ] ) is difficult because the parameter space @xmath90 is constrained . in this section ,",
    "we first consider a subclass of density estimators that have simple forms and investigate the minimax risk over this subclass . in next section , we then show that the minimax risk over this subclass is asymptotically equivalent to the overall minimax risk  @xmath91 .",
    "such an approach was first used in pinsker @xcite to establish a minimax risk bound for the function estimation problem .",
    "it inspired a series of developments , including belitser and levit @xcite , tsybakov @xcite and goldenshluger and tsybakov @xcite .",
    "recall that in the problem of estimating the mean of a gaussian sequence model under @xmath8 loss , diagonal linear estimators of the form @xmath92 play an important role .",
    "indeed , pinsker @xcite showed that when the parameter space ( [ theta(c ) ] ) is an ellipsoid , the minimax risk among diagonal linear estimators is asymptotically minimax among all estimators .",
    "moreover , the results in diaconis and ylvisaker @xcite imply that if such a diagonal linear estimator is bayes , then the prior  @xmath24 must be a gaussian prior with a diagonal covariance matrix .",
    "similarly , in investigating the minimax risk of predictive density estimation , we first restrict our attention to a special class of  @xmath36 that are bayes rules under gaussian priors over the unconstrained parameter space @xmath37 . due to the above connection",
    ", we call these predictive densities _ linear _ predictive densities and call the minimax risk over this class the _ linear _ minimax risk , even though ` linear ' does not have any literal meaning in our setting .    under a gaussian prior @xmath93 , where @xmath94 and @xmath95 for @xmath96 , the linear predictive density @xmath97 is given by @xmath98 note that @xmath97 is not a bayes estimator for the problem described in section [ sec2 ] because the prior distribution @xmath99 is supported on @xmath37 instead of on the ellipsoidal space @xmath100 .",
    "nonetheless , @xmath97 is a valid predictive density function .",
    "the  following lemma provides an explicit form of the average kl risk of @xmath97 .",
    "[ lem3.1 ] the  average kullback  leibler risk ( [ risk : simultaneous ] ) of @xmath97 is given by @xmath101,\\ ] ] where @xmath102 .",
    "let @xmath103 denote the posterior predictive density under the uniform prior @xmath104 , namely , @xmath105 then , by @xcite , lemma 2 , the average kl risk of @xmath97 is given by @xmath106 where @xmath107 and @xmath108 denotes the marginal distribution of @xmath109 under the normal prior  @xmath110 .",
    "it is easy to check that @xmath111 and @xmath112 - \\frac{1}{2 m } \\sum_{i=1}^n \\frac{\\s2_{n+m } + \\th_i^2}{\\s2_{n+m } + s_i } , \\label{lemma1:part2 } \\\\",
    "e \\log m_s(x ; \\s2_n ) & = & -\\frac{n}{2 m } \\sum_{i=1}^n \\log [ 2 \\curpi ( \\s2_n + s_i ) ] - \\frac{1}{2 m } \\sum_{i=1}^n \\frac{\\s2_n + \\th_i^2}{\\s2_n + s_i}. \\label{lemma1:part3}\\end{aligned}\\ ] ] the  lemma then follows immediately by combining equations ( [ eq : marginal representation])([lemma1:part3 ] ) .",
    "we denote the linear minimax risk over all @xmath97 by @xmath113 , that is , @xmath114 this linear minimax risk is not directly tractable because the inside maximization is over a constrained space @xmath90 . in the following theorem",
    ", we first show that we can switch the order of @xmath115 and @xmath116 in equation ( [ r_l : def ] ) and then evaluate @xmath117 using the lagrange multiplier method",
    ".    the  following notation will be useful throughout .",
    "let @xmath118 denote a solution of the equation @xmath119_+ = 2 c,\\ ] ] where @xmath120_{+ } = \\sup(x , 0)$ ] , and let @xmath121 be @xmath122_+\\ ] ] for @xmath123 .",
    "[ teo3.2 ] suppose that the parameter space @xmath90 is an ellipsoid , as defined in ( [ theta(c ) ] ) .",
    "the  linear minimax risk is then given by @xmath124 where @xmath121 is defined as in ( [ def : tth ] ) .",
    "the  linear minimax estimator @xmath125 is the bayes predictive density under a gaussian prior @xmath126 namely , @xmath127 with @xmath128    we first prove equality ( [ eq : solution ] ) .",
    "it is easy to check that for any fixed @xmath44 , @xmath129 achieves its minimum at @xmath130 , and @xmath131 to calculate the maximum of the above quantity over @xmath132 , one needs to solve @xmath133 with the lagrangian @xmath134 simple calculation reveals that the maximum is attained at @xmath135 given by ( [ def : tth ] ) .",
    "next , we prove equality ( [ eq : exchange ] ) , that is , that the order of inf and sup can be exchanged .",
    "note that for any diagonal matrix @xmath136 , we have @xmath137 therefore , if there exists an @xmath136 such that @xmath138 then all of the inequalities in ( [ ineq : chain ] ) become equalities .    if we let @xmath139 , then @xmath140 where the second equality holds because @xmath141 and @xmath142 is a solution to @xmath143 since @xmath144 implies that @xmath145 , we have @xmath146 which completes the proof .    note that @xmath147 , so we have @xmath148 for @xmath149 , where",
    "@xmath150 this implies that the prior distribution corresponding to the linear minimax estimator , that is , @xmath151 , puts a point mass at zero for @xmath152 for all @xmath153 .",
    "in this section , we turn to establishing the asymptotic behavior of the minimax risk @xmath154 over all predictive density estimators . by definition ,",
    "we extend the approach in @xcite to show that the difference between @xmath154 and @xmath156 vanishes as the number of observations  @xmath157 goes to infinity .",
    "therefore , the overall minimax risk is asymptotically equivalent to the linear minimax risk .",
    "this also implies that the gaussian prior @xmath158 defined in ( [ pi_v ] ) is asymptotically least favorable .",
    "the  following lemma provides a lower bound for the overall minimax risk @xmath154 under some conditions .",
    "[ lem4.1 ] let @xmath159 be a sequence such that for some @xmath160 , @xmath161^{1/2 } \\le c.\\ ] ] then , as @xmath162 , the minimax risk @xmath88 has the following lower bound : @xmath163    see the .",
    "note that , as shown in the proof , for a posterior density with a gaussian prior @xmath164 , where @xmath94 , condition ( [ cond : s_i ] ) guarantees @xmath110 to have most of its mass inside @xmath45 , in the sense that @xmath165 for some @xmath166 .    with the lower bound in the above lemma",
    ", we are ready to prove the main result in this paper , which shows that the overall minimax risk @xmath88 is asymptotically equivalent to the linear minimax risk @xmath113 .",
    "[ teo4.2 ] suppose that @xmath100 is the ellipsoid defined in ( [ theta(c ) ] ) and @xmath167 is defined in ( [ def : tth ] ) .",
    "if @xmath168 and @xmath169 then @xmath170    by definition , @xmath171 .",
    "so , to prove this theorem , it suffices to show that as , @xmath172    for a fixed constant @xmath173 , let @xmath174^{{1}/{2}}$ ] and let @xmath175 for @xmath176 .",
    "it is easy to check that the sequence @xmath177 satisfies the condition ( [ cond : s_i ] ) .",
    "therefore , by theorem [ lem4.1 ] , @xmath178\\\\[-8pt ]              & =   & r_l(\\theta ) - \\frac{1}{2 m } \\sum_{i=1}^n \\log              \\frac{(\\s2_n + b_i^2)(\\s2_{n+m } + \\tilde{\\theta}_i^2)}{(\\s2_{n+m } +              b_i^2)(\\s2_n + \\tilde{\\theta}_i^2 ) } + \\mathrm{o}(\\s2_n^\\alpha)\\qquad \\mbox{as } \\s2_n \\rightarrow 0.\\nonumber\\end{aligned}\\ ] ] next , we will derive the convergence rate of @xmath113 and show that the other terms are of smaller order .",
    "using the fact that @xmath179 for @xmath149 ( see ( [ def : n ] ) ) , we can rewrite @xmath113 as @xmath180 when @xmath168 , we have @xmath181 and @xmath182 .",
    "therefore , by means of a taylor expansion , @xmath183    similarly , since @xmath184 for @xmath149 , the second term in ( [ ineq : lower_bound ] ) can be written as @xmath185 for every @xmath186 , we have @xmath187 ( \\s2_{n+m } +    \\tilde{\\theta}_i^2 ) } { [ ( 1 + \\gamma ) \\s2_{n+m } +      \\tilde{\\theta}_i^2 ] ( \\s2_n + \\tilde{\\theta}_i^2 ) } \\biggr)\\\\          &   = & \\log \\biggl ( 1 + \\gamma \\frac{(\\s2_n - \\s2_{n+m } )    \\tilde{\\theta}_i^2}{(\\s2_n + \\tilde{\\theta}_i^2)(\\s2_{n+m } +    \\tilde{\\theta}_i^2 ) + \\gamma \\s2_n ( \\s2_{n+m } + \\tilde{\\theta}_i^2 ) } \\biggr ) \\\\ & \\leq & \\log \\biggl ( 1 + \\gamma \\frac{(\\s2_n - \\s2_{n+m } )    \\tilde{\\theta}_i^2}{(\\s2_n + \\tilde{\\theta}_i^2)\\s2_{n+m } } \\biggr).\\end{aligned}\\ ] ] again using a taylor expansion , as well as the condition that @xmath188 , we obtain @xmath189    finally , since @xmath190 , by choosing @xmath173 , the last term in ( [ ineq : lower_bound ] ) satisfies @xmath191    combining ( [ ineq : lower_bound])([eq : term_3 ] ) , the theorem then follows .",
    "in this section , we apply theorems [ teo3.2 ] and [ teo4.2 ] to establish asymptotic behaviors of minimax risks over some constrained parameter spaces . in particular , we consider the asymptotics over @xmath8 balls and sobolev ellipsoids .",
    "[ ex1 ] suppose that @xmath192 and @xmath44 is restricted in an @xmath8 ball , @xmath193 the  @xmath8 ball can be considered as a variant of the ellipsoid ( [ theta(c ) ] ) with @xmath194 and @xmath195 .",
    "although the values of the @xmath196 s here depend on @xmath157 , the proofs of the above theorems are still valid .",
    "it is easy to see that @xmath197 defined in ( [ def : n ] ) is equal to @xmath157 and that @xmath198",
    ". therefore , @xmath199 by theorem [ teo4.2 ] , the minimax risk among all predictive density estimators is asymptotically equivalent to the minimax risk among _ linear _ density estimators .",
    "furthermore , by theorem [ teo3.2 ] , @xmath200    note that this minimax risk is strictly smaller than the minimax risk over the class of plug - in estimators since , for any plug - in density @xmath201 , @xmath202 = \\frac{1}{2 } e \\|\\hat\\theta - \\theta\\|^2\\end{aligned}\\ ] ] and by pinsker s theorem , the minimax risk of estimating @xmath44 under squared error loss is @xmath203 , which is larger than @xmath204 , by the fact that @xmath205 for any @xmath206 .",
    "[ ex2 ] suppose that @xmath192 and @xmath44 is restricted in a sobolev ellipsoid @xmath207 where @xmath208 for @xmath209 then , by ( [ def : n ] ) , we have @xmath210 as @xmath162 . substituting this relation into equation ( [ def : tl ] ) yields @xmath211 using the taylor expression @xmath212 and the asymptotic relation @xmath213",
    "we obtain @xmath214 where @xmath215^{{1}/{(2\\alpha+1)}}.\\ ] ] note that , by ( [ def : tth ] ) , @xmath216_+ \\bigl(1+\\mathrm{o}(1)\\bigr).\\end{aligned}\\ ] ] therefore , @xmath217 by theorem [ teo4.2 ] , the minimax risk among all predictive density estimators is asymptotically equivalent to the minimax risk among the _ linear _ density estimators",
    ". furthermore , by theorem  [ teo3.2 ] , @xmath218 it is difficult to calculate an explicit form of the optimal constant for the minimax risk due to the @xmath219 function , but we can get an accurate bound for it . by taylor expansion , there exists @xmath220 , such that @xmath221 moreover , @xmath222 where @xmath223 therefore , @xmath224 that is , the convergence rate is @xmath225 and the convergence constant is between @xmath226 and  @xmath227 .    as in example",
    "[ ex1 ] , we compare the asymptotics of this minimax risk with the one over the class of plug - in estimators , where the latter can be easily computed by ( [ plugin_risk ] ) and the results in @xcite .",
    "direct comparison reveals that the convergence rates of both minimax risks are @xmath228 and the convergence constants can both be written in the form @xmath229 , where @xmath230 is a function depending only on @xmath231 .",
    "although it is hard to obtain an explicit representation for the convergence constant for the overall minimax risk , our simulation result in figure [ fig1 ] shows that it is strictly smaller than that over the class of plug - in estimators .     and @xmath232 .",
    "]    [ append ]",
    "in this appendix , we provide the proofs of theorem [ teo2.1 ] and lemma [ lem4.1 ] .",
    "proof of theorem [ teo2.1 ] let @xmath233 be an @xmath234 matrix whose @xmath235th entry equals @xmath236 . since the @xmath237 s form an orthogonal basis for @xmath8 and the @xmath238 s are equally spaced , we have @xmath239 .",
    "consider the transformation @xmath240 .",
    "since the first @xmath157 columns of @xmath233 are @xmath61 , the first @xmath157 elements of the transformed vector are just @xmath75 , defined in ( [ eq : transform ] ) , and we denote the remaining @xmath241 elements by @xmath242 . it is easy to check that @xmath243 and @xmath244 are independent multivariate gaussian variables , and the target density function @xmath15 satisfies @xmath245 where @xmath246 is the jacobian for this transformation .",
    "similarly , any predictor density estimator @xmath20 can be rewritten as @xmath247 where @xmath74 is a transformation of @xmath248 defined in ( [ eq : transform ] ) .",
    "note that the two predictive density functions on the left and right sides of the above equation may have different functional forms ; however , to simplify the notation , we use the same symbol @xmath36 to represent them when the context is clear .",
    "now , the average kl risk can be represented as @xmath249\\\\[-8pt ] & = & { e}_{x , \\x , \\z | \\th } \\log \\frac{p(\\x , \\z { \\vert } \\th)}{\\hp(\\x , \\z { \\vert } x)},\\nonumber\\end{aligned}\\ ] ] where the second equality follows from ( [ trans_1 ] ) and ( [ trans_2 ] ) . since @xmath75 and @xmath242 are independent , we can split @xmath250 as @xmath251 where @xmath252 has a known distribution @xmath253 moreover , to evaluate the minimax risk , it suffices to consider predictive density estimators in the form @xmath254 because any predictive density @xmath255 can be written as @xmath256 , and if @xmath257 is equal to @xmath252 , then this density estimator is dominated by @xmath258 , due to the non - negativity of kl divergence .    combining ( [ teo2.1:eq1])([teo2.1:eq3 ] ) , we have @xmath259 consequently , the minimax risk in the non - parametric regression model is equal to the minimax risk in the gaussian sequence model .",
    "proof of lemma [ lem4.1 ] let @xmath260 be the collection of all ( generalized ) bayes predictive densities . then , by @xcite , theorem 5 , @xmath260 is a complete class for the problem of predictive density estimation under kl loss .",
    "therefore , the minimax risk among all possible density estimators is equivalent to the minimax risk among ( generalized ) bayes estimators , namely , @xmath261    consider a gaussian distribution @xmath262 , where @xmath263 and the @xmath264 s satisfy condition ( [ cond : s_i ] ) .",
    "then , @xmath265    the  first term of ( [ ineq : bound_1 ] ) is the bayes risk under @xmath110 over the unconstrained parameter space @xmath37 .",
    "it is achieved by the linear predictive density @xmath97 ; see @xcite .",
    "therefore , @xmath266\\\\[-8pt ]                                                       & = & \\frac{n}{2 m } \\log \\frac{\\s2_n}{\\s2_{n+m } } + \\frac{1}{2m}\\sum_{i=1}^n \\log \\frac{\\s2_{n+m } + s_i^2}{\\s2_n +                                                       s_i^2}.\\nonumber\\end{aligned}\\ ] ]    to bound the second term of ( [ ineq : bound_1 ] ) , note that for any bayes predictive density @xmath267 , @xmath268 where ( [ ineq : risk : bayes:1 ] ) is due to jensen s inequality , ( [ ineq : risk : bayes:2 ] ) is due to @xmath269 and ( [ ineq : risk : bayes:3 ] ) is due to @xmath270 therefore , @xmath271,\\ ] ] where @xmath272 . using the cauchy ",
    "schwarz inequality , we can further bound the right - hand side of ( [ eq : bound : term2 ] ) as follows : @xmath273   \\\\ & & \\quad \\le \\frac{1}{m v_m } \\biggl [ \\sum_{i=1}^n \\biggl ( \\int_{\\th^c } \\th_i^4 \\pi_s(\\th ) \\,\\mathrm{d } \\th   \\biggr)^{1/2 } \\sqrt{\\pi_s(\\th^c ) } + \\frac{c}{a_1 ^ 2 } \\pi_s(\\th^c ) \\biggr ]   \\\\                                                                          & & \\quad   =   \\frac{1}{m v_m } \\biggl [ \\sqrt{3 } \\sqrt{\\pi_s(\\th^c ) } \\sum_{i=1}^n s_i^2 +   \\frac{c}{a_1 ^ 2 } \\pi_s(\\th^c ) \\biggr ] \\\\                                                                          & & \\quad \\le \\frac{1}{m v_m } \\biggl [ \\sqrt{3}\\frac{c}{a_1 } \\sqrt{\\pi_s(\\th^c ) } +                                                                          \\frac{c}{a_1 } \\pi_s(\\th^c ) \\biggr].\\end{aligned}\\ ] ] then , by @xcite , proposition 2 , which states that if @xmath274 are independent gaussian random variables with @xmath275 and @xmath276 , then @xmath277 we have @xmath278^{1/2 } \\le \\s2_n^ { \\alpha},\\end{aligned}\\ ] ] due to condition ( [ cond : s_i ] ) .    combining ( [ ineq : bound_1 ] ) , ( [ ineq : term_1 ] ) , ( [ eq : bound : term2 ] ) and ( [ order ] ) , the theorem then follows immediately .",
    "the  authors would like to thank edward i. george for helpful discussions and the associate editor for generous insights and suggestions .",
    "this work was supported in part by the national science foundation under award numbers dms-07 - 32276 and dms-09 - 07070 .",
    "any opinions , findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the national science foundation ."
  ],
  "abstract_text": [
    "<S> we consider the problem of estimating the predictive density of future observations from a non - parametric regression model . </S>",
    "<S> the  density estimators are evaluated under kullback  </S>",
    "<S> leibler divergence and our focus is on establishing the exact asymptotics of minimax risk in the case of gaussian errors . </S>",
    "<S> we derive the convergence rate and constant for minimax risk among bayesian predictive densities under gaussian priors and we show that this minimax risk is asymptotically equivalent to that among all density estimators . </S>"
  ]
}