{
  "article_text": [
    "the integration of computer technology into science and daily life has enabled the collection of massive volumes of data . however , this information can not be practically analyzed on a single commodity computer because the data is too large to fit in memory .",
    "examples of such massive - scale data include website transaction logs , credit card records , high - throughput biological assay data , sensor readings , gps locations of cell phones , etc .",
    "analyzing massive data requires either a ) subsampling the data down to a size small enough to be processed on a workstation ; b ) restricting analysis to streaming methods that sequentially analyze fixed - sized subsets ; or c ) distributing the data across multiple computers that perform the analyses in parallel .",
    "while subsampling is a simple solution , the models it produces are often less accurate than those learned from all available data  @xcite .",
    "streaming methods benefit from seeing all data but typically run on a single computer , which makes processing large datasets time consuming .",
    "distributed approaches are attractive because they can exploit multiple processors to construct models faster .    in this paper",
    "we propose to learn _ quickly _ from massive volumes of existing data using parallel computing and a divide - and - conquer approach .",
    "the data records are evenly partitioned across multiple compute nodes in a cluster , and each node _ independently _ constructs an ensemble classifier from its data partition .",
    "the resulting ensembles ( from all nodes ) form a mega - ensemble that votes to determine classifications .",
    "the complexities of data distribution , parallel computation , and resource scheduling are managed by the mapreduce framework  @xcite .",
    "in contrast to many previous uses of mapreduce to scale up machine learning that require multiple passes over the data  @xcite , this approach requires only a _",
    "single pass _",
    "( single mapreduce step ) to construct the entire ensemble .",
    "this minimizes disk i / o and the overhead of setting up and shutting down mapreduce jobs .",
    "our approach is called comet ( short for * * c**loud * * o**f * * m**assive * * e**nsemble * * t**rees ) , which leverages proven learning algorithms in a novel combination .",
    "each compute node constructs a random forest  @xcite based on its local data .",
    "comet employs ivoting ( importance - sampled voting )  @xcite instead of the usual bagging  @xcite to generate the training subsets used for learning the decision trees in the random forest .",
    "chawla et al .",
    "@xcite showed that ivoting produces more accurate ensembles than bagging in distributed settings .",
    "ivoting random forests combine the advantages of random forests ( good accuracy on many problems  @xcite and efficient learning with many features  @xcite ) with ivoting s ability to focus on more difficult examples .",
    "the local ensembles are combined into a mega - ensemble containing thousands of classifers in total .",
    "using such a large ensemble is computationally expensive and overkill for data points that are easy to classify .",
    "thus , we employ a _ lazy ensemble evaluation _ scheme that only uses as many ensemble members as are needed to make a confident prediction .",
    "we propose a new gaussian - based approach for lazy ensemble evaluation ( glee ) that is easier to implement and more scalable than previously proposed approaches .",
    "our main contributions are as follows :    * we present comet , a novel mapreduce - based framework for distributed random forest ensemble learning . *",
    "our method uses a divide - and - conquer approach for learning on massive data and requires only a single mapreduce pass for training , unlike recent work using mapreduce to learn decision tree ensembles @xcite .",
    "we also use a sampling approach called ivoting rather than the usual bagging technique .",
    "* we develop a new approach for lazy ensemble evaluation based on a gaussian confidence interval , called glee .",
    "* glee is easier to implement and asymptotically faster to compute than the bayesian approach proposed by hernndez - lobato  et  al .",
    "simulation experiments show that glee is as accurate as the bayesian approach .    * applying comet to two publicly available datasets ( the larger of which contains 200 m examples )",
    ", we demonstrate that using more data produces more accurate models than learning from a subsample on a single computational node .",
    "* our results also confirm that the ivoting sampling strategy significantly outperforms bagging in the distributed context .",
    "comet is a recipe for large - scale distributed ensemble learning and efficient ensemble evaluation .",
    "the recipe has three components :    * mapreduce : * we write our distributed learning algorithm using mapreduce to easily parallelize the learning task .",
    "the mapper tasks build classifiers on local data partitions ( `` blocks '' in mapreduce nomenclature ) , and one or more reducers can combine together and output the classifiers .",
    "the learning phase only takes a single mapreduce job .",
    "if the learned ensemble is large and/or the number of data points to be evaluated is large , evaluation can also be parallelized using at most two mapreduce jobs .",
    "* ivoting random forest : * each mapper builds an ensemble based on its local block of data ( assigned by mapreduce ) .",
    "the mapper runs a variant of random forests that replaces bagging with ivoting ( described in [ sec : ivoting ] ) .",
    "ivoting has the advantage that it gives more weight to difficult examples . unlike boosting @xcite , however , each model in the ensemble votes with equal weight , allowing us to trivially merge the ensembles from all mappers into a single large ensemble .",
    "* lazy ensemble evaluation : * many inputs are `` easy '' and the vast majority of the ensemble members agree on the classification . for these cases , querying a small sample of the members is sufficient to determine the ensemble s prediction with high confidence .",
    "lazy ensemble evaluation significantly lowers the prediction time for ensembles .",
    "the rest of this section describes these three components in more detail .",
    "we take a coarse - grained approach to distributed learning that minimizes communication and coordination between compute nodes .",
    "we assume that the training data is partitioned randomly into blocks in such a way that class distributions are roughly the same across all blocks .",
    "such shuffling can be accomplished in a simple pre - processing step that maps each data item to a random block .    in the learning phase",
    ", each mapper independently learns a predictive model from an assigned data block .",
    "the learned models are aggregated together into a final ensemble model by the reducer .",
    "this is the only step that requires internode communication , and only the final models are transmitted ( not the data ) .",
    "thus , _ we only require a single mapreduce pass for training_.    we implement the above strategy in the mapreduce framework @xcite because the framework s abstractions match our needs , although other parallel computing frameworks ( e.g. , mpi ) could also be used . to use mapreduce , one loads the input data into the framework s distributed file system and defines map and reduce functions to process key - value pair data during map and reduce stages , respectively .",
    "mappers execute a map function on an assigned data block ( usually read from the node s local file system ) .",
    "the map function produces zero or more key - value pairs for each input ; in our case , the values correspond to learned trees ( with random keys ) . during the reduce stage ,",
    "all the pairs emitted during the map stage are grouped by key and passed to reducer nodes that run the reduce function .",
    "the reduce function receives one key and all the associated values produced by the map stage . like the map function",
    ", the reduce function can emit any number of key - value pairs .",
    "resulting pairs are written to the distributed file system .",
    "the mapreduce framework manages data partitioning , task scheduling , data replication , and restarting from failures .",
    "the reducer(s ) write the learned trees to one or more output files",
    ".    the map and reduce functions for distributed ensemble learning are straightforward .",
    "the map function trains an ensemble on its local data block and then emits the learned trees .",
    "each tree is emitted with a random key to automatically partition the ensemble across the reducers .",
    "each mapper in comet builds an ensemble from the local data partition using ivoting . ivoting ( importance - sampled voting ) @xcite builds an ensemble by repeatedly applying the base learning algorithm ( e.g. , decision tree induction @xcite ) to small samples called _",
    "bites_. unlike bagging @xcite , examples are sampled with non - uniform probability .",
    "suppose that @xmath0 ivoting iterations have been run , producing ensemble @xmath1 comprised of @xmath0 base classifiers .",
    "to form the @xmath2 bite , training examples @xmath3 are drawn randomly .",
    "if @xmath1 incorrectly classifies @xmath4 , @xmath3 is added to training set @xmath5 .",
    "otherwise @xmath3 is added to @xmath5 with probability @xmath6 , where @xmath7 is the error rate of @xmath1 .",
    "this process is repeated until @xmath8 reaches the specified bite size @xmath9 ; @xmath9 is typically smaller than the size of the full data . out - of - bag ( oob )",
    "@xcite predictions are used to get unbiased estimates of @xmath7 and @xmath1 s accuracy on sampled points @xmath4 . the oob prediction for @xmath4",
    "is made by voting only the ensemble members that did not see @xmath4 during training , i.e. , @xmath4 was outside the base models training sets .",
    "ivoting s sequential and weighted sampling is reminiscent of boosting @xcite and is similar to boosting in terms of accuracy @xcite .",
    "ivoting differs from boosting in that each base model receives equal weight for deciding the ensemble s prediction .",
    "this property simplifies merging the multiple ensembles produced by independent ivoting runs .",
    "breiman @xcite showed that ivoting sampling generates bites containing roughly half correct and half incorrect examples .",
    "our implementation ( algorithm  [ alg : ivote ] ) draws , with replacement , 50% of the bite from the examples @xmath1 correctly classifies and 50% from the examples @xmath1 incorrectly classifies ( based on oob predictions ) .",
    "this implementation avoids the possibility of drawing and rejecting large numbers of correct examples for ensembles with very high accuracy .",
    "initialize @xmath10 , @xmath11 , @xmath12 = 0 $ ] , @xmath13    any classification learning algorithm could be used for the base learner in ivoting .",
    "our experiments use decision trees @xcite because they generally form accurate ensembles @xcite .",
    "the trees are grown to full size ( i.e. , each leaf is pure or contains fewer than ten training examples ) using information gain as the splitting criterion .",
    "we use full - sized trees because they generally yield slightly more accurate ensembles @xcite . to increase the diversity of trees and reduce training time for data sets with large numbers of features , only a random subset of features",
    "are considered when choosing the test predicate for each tree node .",
    "this attribute subsampling is used in random forests and has been shown to improve performance and decrease training time @xcite .",
    "we employ the random forest heuristic for choosing the attribute sample size where @xmath14 is the total number of attributes .      a major drawback to large ensembles",
    "is the cost of querying all ensemble members for their predictions . in practice , many data points are easy to classify : the vast majority of the ensemble members agree on the classification .",
    "for these cases , querying a small sample of the members is sufficient to determine the ensemble s prediction with high confidence .",
    "we exploit this phenomena via lazy ensemble evaluation .",
    "_ lazy ensemble evaluation _ is the strategy of only evaluating as many ensemble members as needed to make a good prediction on _ a case by case basis for each data point_. ensemble voting is stopped when the `` lazy '' prediction has high probability of being the same as the prediction from the entire ensemble .",
    "the risk that lazy evaluation stops voting too early ( i.e. , the probability that the early prediction is different from what the full ensemble prediction would have been ) is bounded by a user - specified parameter @xmath15 . algorithm  [ alg : lazy ] lists the lazy ensemble evaluation procedure .",
    "let @xmath4 be a data point to classify using ensemble @xmath16 , with @xmath16 containing @xmath17 base models .",
    "initially all @xmath17 models are in the unqueried set @xmath18 . in each step , a model @xmath19 is randomly chosen and removed from @xmath18 to vote on @xmath4 ; the vote is added to the running tallies of how many votes each class has received . based on the accumulated tallies and how many ensemble members have not yet voted , the stopping criterion decides if it is safe to stop and return the classification receiving the most votes . if it is not safe , a new ensemble member is drawn , and the process is repeated until it is safe to stop or all @xmath17 ensemble members have been queried .",
    "note that lazy evaluation is agnostic to whether the base models are correlated .",
    "its goal is to approximate the ( unmeasured ) vote distribution from a sample of votes , and the details of the process generating the votes are irrelevant .",
    "set @xmath20 , @xmath21 $ ] , @xmath22    in binary categorization , the vote of each base model can be modeled as a bernoulli random variable . accordingly , the distribution of votes for the full ensemble is a binomial distribution with proportion parameter @xmath23 .",
    "provided that the number of members queried is sufficiently large , we can invoke the central limit theorem and approximate the binomial distribution with a gaussian distribution .",
    "we propose gaussian lazy ensemble evaluation ( glee ) , which uses the gaussian distribution to infer a @xmath24 confidence interval around the observed mean @xmath25 .",
    "the interval is used to test the hypothesis that the unobserved proportion of positive votes @xmath23 falls on the same side of 0.5 as @xmath25 ( and consequently , that the current estimated classification agrees with the full ensemble s classification ) .",
    "if 0.5 falls outside the interval , glee rejects the null hypothesis that @xmath23 and @xmath25 are on different sides of 0.5 and terminates voting early .",
    "formally , denote the interval bounds as @xmath26 , where @xmath27 and @xmath28 the critical value @xmath29 is the usual value from the standard normal distribution . the finite population correction ( fpc )",
    "@xmath30 accounts for the fact that base models are drawn from a finite ensemble .",
    "intuitively , uncertainty about @xmath23 shrinks as the set @xmath18 becomes small . to ensure the gaussian approximation is reasonable , glee only stops evaluation only once some minimum number of models have voted . using simulation experiments we found 15 , 30 , and 45 reasonable for @xmath31 , @xmath32 , and @xmath33 , respectively .",
    "( simulation methodology is described in section  [ sec : evt - comparison ] . )    the above hypothesis test only requires the lower bound ( if @xmath34 ) or the upper bound ( if @xmath35 ) .",
    "consequently we can improve glee s statistical power by computing a one - sided interval ; i.e. , use @xmath36 instead of @xmath29 .",
    "when the glee stopping criteria is invoked , the _ leading class _ ( the class with the most votes so far ) is treated as class 1 , and the _ runner - up class _ is treated as class 0 . glee stops evaluation early if the lower bound @xmath37 is greater than 0.5 .",
    "show that the relative error of the lazy prediction is bounded by @xmath15 despite this bias . ]",
    "hernndez - lobato et al .",
    "@xcite present another way of deciding when to stop early using bayesian inference .",
    "we compare to this method in section  [ sec : evt - comparison ] and refer to it as madrid lazy ensemble evaluation ( mlee ) . in mlee , the distribution of vote frequencies for different classes is modeled as a multinomial distribution with a uniform dirichlet prior .",
    "the posterior distribution of the class vote proportions is updated at each evaluation step to reflect the observed base model prediction .",
    "mlee computes the probability that the final ensemble predicts class @xmath38 by combinatorially enumerating the possible prediction sequences for the as - yet unqueried ensemble members , based on the current posterior distribution . like glee ,",
    "ensemble evaluation stops when the probability of some class exceeds the specified confidence level or when all base models have voted .",
    "mlee is exponential in the number of classes but is @xmath39 for binary classification ( @xmath17 ensemble members ) , and approximations exist to make it tractable for some multi - class problems @xcite .",
    "large ensembles ( too large to fit into memory ) can make predictions on massive data ( also too large to fit into memory ) using _ lazy committee evaluation_. each input is first evaluated by a sub - committee  a random subset of the ensemble small enough to fit in memory  using a lazy evaluation rule . in most cases",
    ", the sub - committee will be able to determine the ensemble s output with high confidence and output a decision . in the rare cases where the sub - committee can not confidently classify the input , the input is sent to the full ensemble for evaluation .",
    "lazy committee evaluation requires two mapreduce jobs . in the first job each mapper randomly chooses and reads one of the @xmath23 ensemble partitions to be the local sub - committee and",
    "lazily evaluates the sub - committee on the mapped test data it receives ; only one test input needs to be in memory at a time .",
    "if the sub - committee reaches a decision , the input s identifier and label are written directly to the file system .",
    "otherwise , a copy of the input is written to each of @xmath23 reducers by using keys @xmath40 . in the reduce stage",
    ", each reducer reads a different ensemble partition so that every base model votes exactly once .",
    "reducers output the vote tallies for each input they read , keyed on the input identifier .",
    "the second job performs an identity map with reducers that sum the vote tallies for each input ; reducers output the class with the most votes keyed to the input s identifier . combining the two sets of label outputs , from the first map and second",
    "reduce , provides a label for every input .",
    "this section explores the efficacy of the glee rule across a wide range of ensemble sizes and for varying confidence levels .",
    "we simulate votes from large ensembles to explore the rule s behavior and to compare it to the mlee rule .    the stopping thresholds for both methods are pre - computed and stored in a table that is indexed by the number of votes received by the leading class .",
    "one table is needed per ensemble size @xmath17 . pre - computing and caching the thresholds is necessary to make mlee practical for large ensembles .",
    "once the thresholds are computed , evaluating them requires an array lookup and comparison .",
    "computing the large factorials in mlee requires care to avoid numerical overflow .",
    "martnez - muoz et al .",
    "@xcite suggest representing numbers in their prime factor decomposition to avoid overflow ; this approach requires @xmath41 time to compute the table .",
    "prime numbers less than @xmath17 .",
    "thus , operations on numbers represented by prime factors take @xmath42 time .",
    "these operations are inside two nested @xmath43 loops .",
    "] we instead compute the factorials for mlee in log - space which produces the same results and requires @xmath39 total complexity . in comparison , computing the threshold table for glee takes @xmath43 time .",
    "this difference is significant for very large ensembles ( table  [ tab : cache - time - summary ] ) .",
    ".time to pre - compute stopping thresholds [ cols=\"<,^,^,^,^,^,^ \" , ]      + * ebird bite size was 70k ( approx .",
    "data partition size ) .    in glee , the straightforward way to sample models ( without replacement ) from the ensemble is to generate a new random number for each ensemble member that is evaluated .",
    "if the cost of generating a random number is relatively expensive , lazy evaluation may not provide enough of a speed - up and may even slow down ensemble evaluation . to avoid this , our glee implementation permutes the ensemble order once at load time .",
    "each ensemble evaluation is started from a different random index in this order .",
    "thus , only a single random number is generated per ensemble prediction .",
    "we first compare comet to subsampling ( i.e. , ivoting random forests run serially on a single block of data ) to measure the benefits of learning from all data .",
    "accuracies are computed using full ensemble evaluation ( i.e. , glee is not used ) .    for the clueweb09 data ( figure  [ fig : clueweb - results ] ) , the serial code trains on a single block ( 1 m examples ) using 9 different ensemble sizes : 100 , 250 , 500 , 750 , 1000 , 1250 , 1500 , 1750 , 2000 .",
    "the accuracy ranges from 91.8% ( 100 ensemble members ) up to 93.8% ( 2000 members ) .",
    "the training time ranges from 12min to 5hr .",
    "comet trains on 200 blocks ( 200 m examples ) , varying across 13 different values for the local ensemble size : 1 , 5 , 10 , 25 , 50 , 75 , 100 , 200 , 300 , 400 , 500 , 750 , 1000 .",
    "the total ensemble size is 200 times the local ensemble size ; thus , the largest total ensemble has 200k members .",
    "the accuracy ranges from 89.5% ( corresponding to a local ensemble size of 1 and a total ensemble size of 200 ) to 94.2% ( corresponding to a local ensemble size of 1000 and a total ensemble size of 200k ) with time varying from less than 1min to 3hr , respectively . as a point of comparison",
    ", the distributed comet model achieves an accuracy of 93.8% ( the same as the best serial model ) in only 60min , corresponding to a total ensemble size of 60k ( 300 trees per block ) .",
    "thus , we achieve a 5x speed - up in training time using 200x more data without sacrificing any accuracy .    on the ebird data ( figure  [ fig : ebird - results ] ) , serial ivoting trains from a single block containing 70k examples and uses the same 9 ensemble sizes as for the clueweb09 data .",
    "the accuracy ranges from 76.4% ( for the smallest ensemble ) up to 77.6% ( for the largest ensemble time ) , and training time ranges from 120min .",
    "comet trains on 14 blocks ( 1 m examples ) , varying across 8 different values for the local ensemble size : 25 , 50 , 75 , 100 , 150 , 250 , 500 , 750 .",
    "the total ensemble size is 14 times the local ensemble size ; thus , the largest total ensemble has 10,500 members .",
    "the accuracy ranges from 77.7% ( better than the best serial accuracy ) to 78.9% with time ranging from 2min to 9min .",
    "the best accuracy achieved by the serial version is 77.5% with a total ensemble size of 2000 and a training time of 21min ; the distributed version improves on this with an accuracy of 77.8% for a total ensemble size of only 350 ( local size of 25 ) and a training time of 2min .",
    "thus , we see a 10x speed - up in training time while using 14x more data .      +   [",
    "fig : glee - perf ]    figures  [ fig : clueweb - data ] and  [ fig : ebird - data ] vary the number of data blocks used in the training . for clueweb ,",
    "all parameters are the same as above except for the following .",
    "the number of blocks is varied from 1 to 200 ( with 1 m examples per block ) , and the local ensemble size is varied from 1 to 1000 .",
    "we clearly see a flattening out as the number of blocks increases , essentially flat - lining at 40 .",
    "likewise , the gain for increasing the ensemble size becomes small ( invisible in this graph ) for a local ensemble size of more than 250 .",
    "for ebird , all parameters are the same as above except that we fix the local ensemble size at 200 and vary the number of blocks between 1 and 14",
    ". the accuracy increases almost monotonically with the number of blocks used .",
    "figures  [ fig : clueweb - results ] and  [ fig : ebird - results ] also show the performance of comet using bagging instead of ivoting at local nodes ( comet - b ; tree construction is still randomized ) . while comet - b is faster than comet , it is less accurate than both serial and distributed ivoting ( i.e. , standard comet ) .",
    "the second set of experiments measures the evaluation savings and relative error incurred by using glee for ensembles of different sizes on the clueweb and ebird data ( figure  [ fig : glee - perf ] ) . ) is _ not _ used here .",
    "the 200k full - size clueweb trees exceeded a single node s memory , so an ensemble of 200k trees trained to maximum depth of 6 was used for figure  [ fig : glee - perf ] instead .",
    "] as expected , the results show that decreasing @xmath15 increases the average number of votes ( figures [ fig : glee - clueweb - votes ] , [ fig : glee - ebird - votes ] ) and decreases the relative error for any size ensemble ( figures [ fig : glee - clueweb - err ] , [ fig : glee - ebird - err ] ) .",
    "for all ensemble sizes and @xmath15 values evaluated , using glee provides a significant speed - up over evaluating the entire ensemble .",
    "this speed - up increases with ensemble size , even for small values of @xmath15 . for the clueweb data ( top row ) ,",
    "relative error is less than 1% for @xmath44 . for an ensemble of size 1k , fewer than 8% of the ensemble needs to be evaluated , on average , and for an ensemble of size 100k , that drops to less than 1% .",
    "similar results hold for ebird ( bottom row ) .",
    "thus , the cost of evaluating a large ensemble can be largely mitigated via glee .",
    "finally , figure  [ fig : clueweb - evr - stopping - point ] shows a histogram of the number of evaluations needed by glee with @xmath44 on a log - log scale for clueweb , providing insight into why the stopping method works  the vast majority of instances require evaluating only a small proportion of the ensemble .",
    "e.g. , 75% of instances require 100 or fewer base model evaluations .",
    "ensemble learning has long been used for large - scale distributed machine learning . instead of converting a learning algorithm to be natively parallel , run the ( unchanged ) algorithm multiple times , in parallel , on each data partition @xcite .",
    "an aggregation strategy combines the set of learned models into an ensemble that is usually as accurate , if not more accurate , than a single model trained from all data would have been .",
    "for example , chan and stolfo @xcite study different ways to aggregate decision tree classifiers trained from disjoint partitions .",
    "they find that voting the trees in an ensemble is sufficient if the partition size is big enough to produce accurate trees .",
    "they propose arbiter trees to intelligently combine and boost weaker trees to form accurate ensembles in spite of small partitions .",
    "domingos @xcite similarly learns sets of decision rules from partitioned data , but combines them using a simpler weighted vote .",
    "yan et al .",
    "@xcite train many randomized support vector machines with a mapreduce job ; a second job runs forward stepwise selection to choose a subset with good performance .",
    "the final ensemble aggregates predictions through a simple vote . in this work",
    "we use simple voting as our aggregation strategy because our data partitions are relatively large .",
    "our distributed learning strategy is inspired by chawla et al.s work on distributed ivoting  @xcite .",
    "they empirically compare ivoting applied to all training data to distributed ivoting ( divoting ) in which ivoting is run independently on disjoint data partitions to create sub - ensembles that are merged to make the final ensemble .",
    "their results show that divoting achieves comparable classification accuracy to ( serial ) ivoting with a faster running time , and better accuracy than distributed bagging that used the same sample sizes .",
    "compared to divoting , comet benefits from using mapreduce instead of mpi ( for an easier implementation , scaling to data larger than the memory of all nodes , and ability to handle node failures ) and incorporates lazy ensemble evaluation for efficient predictions from large ensembles .",
    "lazy evaluation is particularly important when learning from large data sets with many data partitions .",
    "the work of wu et al .",
    "@xcite is also closely related to ours .",
    "they also train a decision tree ensemble using mapreduce in a single pass , but only train one decision tree per partition , do not use lazy ensemble evaluation , and evaluate the ensemble on a single small data set with only 699 records .",
    "like comet and divoting , distributed boosting  @xcite trains local ensembles from disjoint data partitions and combines them in a global ensemble . worker nodes train boosted trees from local data but need to share learned models with each other every iteration to update the sampling weights .",
    "the resulting ensembles are at least as accurate as boosted ensembles trained serially and can be trained much faster  @xcite .",
    "svore and burges  @xcite experiment with a variant of distributed boosting in which only one tree is selected to add to the ensemble at each iteration . as",
    "a result the boosted ensemble grows slowly but is not as accurate as serial boosting .",
    "chawla et al .",
    "@xcite showed that divoting gives similar accuracy as distributed boosting without the communication overhead of sharing models .    the bagboo algorithm  @xcite creates a bagged ensemble of boosted trees with each boosted sub - ensemble trained independently from data subsamples . like comet",
    ", bagboo is implemented on mapreduce and creates mega - ensembles when applied to massive datasets ( e.g. , 1.125 million trees ) .",
    "the ensembles are at least as accurate as boosted ensembles . unlike comet , sub - ensembles are small ( 1020 models ) to mitigate the risk of boosting overfitting , and the non - uniform weights of trees in the ensemble precludes lazy ensemble evaluation .",
    "because each sub - ensemble is trained from a sub - sample , a data point can appear in multiple bags ( unlike comet s partitions ) ; it is unclear from the algorithm description what communication cost this incurs .",
    "since ivoting and adaboost yield similar accuracies  @xcite , we expect that comet and bagboo would as well .",
    "a different strategy is to distribute the computation for building a single tree ; this subroutine is used to build an ensemble in which every model benefits from all training data .",
    "this approach involves multiple iterations of compute nodes calculating and sending split statistics for their local data to a controller node that chooses the best split .",
    "most such algorithms use mpi because of the frequent communications  @xcite .",
    "one exception is planet  @xcite which constructs decision trees from all data via multiple mapreduce passes .",
    "planet constructs individual trees by treating the construction of each node in the tree as a task involving the partially constructed tree .",
    "the mappers look at the examples that fall into the unexpanded tree node , collect sufficient statistics about each feature and potential split for the node , and send this information to the reducers .",
    "the reducers evaluate the best split point for each feature on a node .",
    "the controller chooses the final split point for the node based on the reducer output .",
    "because many mapreduce jobs will be involved in building a single tree , planet includes many optimizations to reduce overhead , including 1 ) batching together node construction tasks so that each level in the tree is a single job ; 2 ) finishing subtrees with a small number of items in - memory in the reducer ; and 3 ) using a custom job control system to reduce job setup and teardown costs .      whereas much research has studied removing unnecessary models from an ensemble ( called ensemble pruning ) @xcite , only a few studies have used lazy ensemble evaluation to dynamically speed up prediction time in proportion to the ease or difficulty of each data point",
    ". fan et al .",
    "@xcite use a gaussian confidence interval to decide if ensemble evaluation can stop early for a test point .",
    "their method differs from the one described in section  [ sec : glee ] in that a ) ensemble members are always evaluated from most to least accurate , and b ) confidence intervals are based on where evaluation could have reliably stopped on validation data .",
    "a fixed ordering is not necessary in our work because the base models have equal voting weight and similar accuracy ; this leads to a simpler gaussian lazy ensemble evaluation rule .",
    "markatopoulou et al .",
    "@xcite propose a more complicated runtime ensemble pruning , where the choice of which base models to evaluate is decided by a meta - model trained to choose the most reliable models for different regions of the input data space .",
    "their method can achieve better accuracy than using the entire ensemble , but generally will not lead to faster ensemble predictions .",
    "comet is a single - pass mapreduce algorithm for learning on large - scale data .",
    "it builds multiple ensembles on distributed blocks of data and merges them into a mega - ensemble .",
    "this approach is appropriate when learning from massive - scale data that is too large to fit on a single machine .",
    "it compares favorably ( in both accuracy and training time ) to learning on a subsample of data using a serial algorithm .",
    "our experiments showed that it is important to use a sequential ensemble method ( ivoting in our case ) when building the local ensembles to get the best accuracy .",
    "the combined mega - ensemble can be efficiently evaluated using lazy ensemble evaluation ; depending on the ensemble size , the savings in evaluation cost can be 100x or better .",
    "two options are available for lazy evaluation : our glee rule and the bayesian mlee rule  @xcite .",
    "glee is easy to implement , is asymptotically faster to compute than mlee , and provides the same evaluation savings and approximation quality as mlee .",
    "if one desires to further speed up evaluation or reduce the model s storage requirements , ensemble pruning  @xcite could be applied to remove extraneous base models , or model compression  @xcite could be used to compile the ensemble into an easily deployable neural network .",
    "ultimately the appropriateness of sacrificing some small accuracy ( and how much accuracy ) for faster evaluations will depend on the application domain .    in future work , it will be interesting to contrast comet to planet  @xcite , which builds trees using all available data via multiple mapreduce passes . as there is no open - source version of planet currently available and",
    "this procedure is highly time - consuming without special modifications to mapreduce  @xcite , we are unable to provide direct comparisons at this time .",
    "however , we imagine that there will be some trade - off between accuracy ( using all data for every tree ) and time ( since comet uses only a single mapreduce pass ) .",
    "the authors thank david gleich , todd plantenga , and greg bayer for many helpful discussions .",
    "we are particularly grateful to david for suggesting the learning task with the clueweb data .",
    "sandia national laboratories is a multi - program laboratory managed and operated by sandia corporation , a wholly owned subsidiary of lockheed martin corporation , for the u.s .",
    "department of energy s national nuclear security administration under contract de - ac04 - 94al85000 .",
    "we performed several preprocessing steps on the ebird data .",
    "first , we removed records for checklists covering @xmath45 miles since most attributes are based on a checklist s location and the location information for checklists covering large distances is less reliable .",
    "second , count types p34 and p35 are rare in the data , so we grouped them with the semantically equivalent count types p22 and p23 , respectively , by recoding p34 as p22 and p35 as p23 .",
    "third , we rederived the attributes caus_temp_avg , caus_temp_min , caus_temp_max , caus_prec , and caus_snow using the month attribute and the appropriate monthly climate features ( e.g. , caus_temp_avg01 ) to remove thousands of spurious missing values .",
    "( the ebird data providers plan to correct this processing mistake in future versions . )",
    "fourth , missing values for categorical attributes were replaced with the special token ` mv ' .",
    "similarly , missing and na values for numerical attributes were recoded as -9999 , a value outside the observed range of the data . with these missing value encodings",
    ", the decision tree learning algorithm can handle records with missing values as a special case if that leads to better accuracy .",
    "fifth , we converted the categorical features ( count_type , bcr , bailey_ecoregion , omernik_l3_ecoregion ) into multiple binary features with one feature per valid feature value because our decision tree implementation does not yet handle categorical features ."
  ],
  "abstract_text": [
    "<S> comet is a single - pass mapreduce algorithm for learning on large - scale data . </S>",
    "<S> it builds multiple random forest ensembles on distributed blocks of data and merges them into a mega - ensemble . </S>",
    "<S> this approach is appropriate when learning from massive - scale data that is too large to fit on a single machine . to get the best accuracy , ivoting </S>",
    "<S> should be used instead of bagging to generate the training subset for each decision tree in the random forest . </S>",
    "<S> experiments with two large datasets ( 5 gb and 50 gb compressed ) show that comet compares favorably ( in both accuracy and training time ) to learning on a subsample of data using a serial algorithm . </S>",
    "<S> finally , we propose a new gaussian approach for lazy ensemble evaluation which dynamically decides how many ensemble members to evaluate per data point ; this can reduce evaluation cost by 100x or more .    </S>",
    "<S> mapreduce ; decision tree ensembles ; lazy ensemble evaluation ; massive data </S>"
  ]
}