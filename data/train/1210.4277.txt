{
  "article_text": [
    "compressive sensing ( cs ) signal acquisition paradigm asserts that one can successfully recover certain signals sampled far below their nyquist frequencies given they are sparse in some dictionary @xcite .",
    "the fourier dictionary for frequency sparse signals is an example of this .",
    "encouraged by this assertion , the usual sample and then compress setup can be combined into a single efficient step .",
    "signals acquired in this fashion do , however , have to be reconstructed which , in the noiseless case , entails a non - convex optimisation problem of the form :    @xmath2    where @xmath3 is the reconstructed signal , @xmath4 is a known measurement matrix , and @xmath5 is the measured signal with @xmath6 . in the cs context",
    ", @xmath7 is the number of samples sensed while @xmath8 is the number of samples in the original signal .",
    "we take @xmath9 to denote the @xmath0 pseudo norm from @xcite , i.e. the number of non - zero entries in @xmath10 .",
    "solving the combinatorial problem in ( [ eq : compressive_sensing - general_problem ] ) by an exhaustive search is generally infeasible .",
    "one feasible approach in reconstructing the signal is to relax the problem in ( [ eq : compressive_sensing - general_problem ] ) by substituting the @xmath1 norm for the @xmath0 making the problem a linear program ( lp ) @xcite .",
    "another feasible approach is taken by the family of so - called iterative greedy algorithms . in these , the problem in ( [ eq : compressive_sensing - general_problem ] ) is reversed by minimising the residual of the energy of @xmath11 subject to some sparsity enforcing constraint .",
    "abstractly , the greedy algorithms can be separated into two classes @xcite : 1 ) simple one stage algorithms which use a single greedy step in each iteration . examples are matching pursuit ( mp ) @xcite and iterative hard thresholding ( iht ) @xcite .",
    "2 ) composite two stage algorithms which combine a greedy step with a refinement step in each iteration .",
    "examples are orthogonal matching pursuit ( omp ) @xcite and cosamp @xcite .",
    "the main advantage of the greedy algorithms over the @xmath1 approach is that they are computationally less complex @xcite and require less computation time than state - of - the - art lp solvers @xcite .",
    "in addition to the computation time , a measure of the reconstruction quality must be considered .",
    "recently , the measure of phase transition @xcite has become a standard way to specify reconstruction capabilities , see e.g. @xcite , @xcite , @xcite , @xcite .",
    "phase transitions evaluate the probability of successful reconstruction versus the indeterminacy of the constraints @xmath12 and the true sparsity of @xmath10 . in general",
    ", the main advantage of the @xmath1 approach over the greedy algorithms is that it is superior in terms of phase transition @xcite .    in search of a fast algorithm with a phase transition similar to that of the @xmath1 approach",
    ", it has been proposed to solve ( [ eq : compressive_sensing - general_problem ] ) by approximating the @xmath0 norm with a continuous function @xcite .",
    "the resulting smoothed @xmath0 norm ( sl0 ) algorithm has a better phase transition than the greedy algorithms while requiring considerably less computation time than the state - of - the - art lp solvers . in this paper , we show that a few key parameters must be carefully selected and knowledge of the indeterminacy exploited to fully unleash the potential of sl0 .",
    "we provide a set of empirically determined recommended parameters for a modified sl0 algorithm that may dramatically improve its phase transition . through extensive simulations , the claim of superiority of the recommended parameters is supported .",
    "finally , we discuss implementation strategies that speed up the algorithm by exploiting knowledge of the indeterminacy .",
    "the paper is organised as follows . in section [ sec :",
    "sl0 ] , we restate the sl0 algorithm and present the proposed algorithm .",
    "implementations of sl0 that yield reduced computation time are discussed in section [ sec : fast_implementations ] .",
    "section [ sec : simulation_framework ] describes the setup used for simulations while section [ sec : results ] provides the simulation results .",
    "a discussion of the results is given in section [ sec : discussion ] .",
    "finally , conclusions are stated in section [ sec : conclusions ] .",
    "sl0 attempts to solve the problem in ( [ eq : compressive_sensing - general_problem ] ) by approximating the @xmath0 norm with a continuous function . consider the continuous gaussian function @xmath14 with the parameter @xmath15 :    @xmath16    the parameter @xmath15 may be used to control the accuracy with which @xmath14 approximates the kronecker delta . in mathematical terms",
    ", we have @xcite :    @xmath17    define the continuous multivariate function @xmath18 as :    @xmath19    since the number of entries in @xmath10 is @xmath8 and the function @xmath18 is an indicator of the number of zero - entries in @xmath10 , the @xmath0 norm of the reconstructed vector @xmath10 is approximated by :    @xmath20    substituting this approximation into ( [ eq : compressive_sensing - general_problem ] ) yields the problem :    @xmath21    the approach is then to solve the problem in ( [ eq : compressive_sensing - sl0_problem ] ) for a decreasing sequence of @xmath15 s .",
    "the underlying thought is to select a @xmath15 which ensures that the initial solution is in the subset of @xmath22 over which the approximation is convex and gradually increase the accuracy of the approximation . by careful selection of the sequence of @xmath15 s , ( hopefully ) non - convexity and thereby local minima are avoided . in the sl0 algorithm stated below ,",
    "we let @xmath23 denote the moore - penrose pseudo - inverse of the matrix @xmath24 and let @xmath25 denote the hadamard product ( entry wise multiplication ) of the vectors @xmath26 and @xmath27 . furthermore",
    ", we let @xmath28^t$ ] and @xmath29 .    * sl0 algorithm : *    * initialise : * @xmath30 , @xmath31 , @xmath32 , @xmath33 @xmath34 @xmath35 @xmath36 [ alg : sl0-sl0_basic_algorithm - d ] @xmath37 [ alg : sl0-sl0_basic_algorithm - x1 ] @xmath38 [ alg : sl0-sl0_basic_algorithm - x2 ] @xmath39    for each @xmath15 , the problem in ( [ eq : compressive_sensing - sl0_problem ] ) is solved by repeatedly taking an unconstrained gradient descent step in line [ alg : sl0-sl0_basic_algorithm - x1 ] ( @xmath40 is a normalised gradient of @xmath18 ) and projecting @xmath10 back onto the feasible set in line [ alg : sl0-sl0_basic_algorithm - x2 ] .",
    "substituting line [ alg : sl0-sl0_basic_algorithm - x1 ] into line [ alg : sl0-sl0_basic_algorithm - x2 ] results in the expression :    @xmath41    where @xmath42 is the identity matrix .",
    "the matrix @xmath43 is a projector onto the null space of @xmath24 .",
    "consequently , as pointed out in @xcite , the unconstrained gradient descent step followed by projection back onto the feasible set is equivalent to a direct gradient descent step on the feasible set .",
    "the reason is that the projector @xmath43 restricts the gradient descent step to be on the feasible set .",
    "the parameter @xmath44 is the constant ( typically chosen in the interval @xmath45 ) in the geometric sequence of @xmath15 s while @xmath46 relates to the final value used for @xmath15 and hence this controls the quality of the reconstruction .",
    "the optimal choice of @xmath44 and @xmath46 is problem dependent while an initial @xmath47 as well as @xmath32 and @xmath33 are problem independent recommendations given in @xcite .",
    "a comprehensive proof of the convergence of the sl0 algorithm exists for a specific set of parameters provided that an asymmetric resticted isometry property ( arip ) constraint is satisfied @xcite .",
    "the authors of the proof conclude that though theoretically satisfactory , the arip constraint leads to an unnecessarily pessimistic choice of parameter values . motivated by this conclusion ,",
    "we have carried out an extensive empirical analysis with the objective of finding the optimal parameter values .",
    "this analysis has shown that the standard step - size @xmath32 and iteration number @xmath33 provided in @xcite result in a suboptimal phase transition . instead , we suggest a modification to the sl0 algorithm which may greatly improve the phase transition .",
    "our initial observation is that the sl0 algorithm presented here is outperformed by the greedy iht algorithm described in @xcite in terms of phase transition for indeterminacy @xmath48 . by introducing an update of the @xmath49 parameter",
    ", the phase transition of the sl0 algorithm can be improved to lie between that of the greedy iht algorithm and that of the @xmath1 approach .",
    "specifically , we obtained this improvement by multiplying @xmath49 with a factor @xmath50 after each update of @xmath15 . by carefully selecting sequences of @xmath51 s and @xmath15 s",
    ", the phase transition of the sl0 algorithm can be even further improved to consistently lie on or above that of the @xmath1 approach . in brief , we chose the step - size in the order of @xmath52 for the first few @xmath15 s and in the order of @xmath53 for the last @xmath15 s .",
    "also , we chose the initial @xmath15 based on knowledge of the indeterminacy @xmath54 to improve the phase transition across all @xmath54 .    based on our experimental results ,",
    "we propose the following strategy .",
    "we choose a step - size of @xmath55 for the first three @xmath15 s and a step - size of @xmath56 for the last @xmath15 s .",
    "we use a sequence of @xmath57{^{\\mathrm{t}}}$ ] where the number of entries equals the number of @xmath15 s used .",
    "furthermore , an inversely proportional relation between @xmath54 and the initial value of @xmath15 yielded the most promising phase transition .",
    "specifically , we choose an initial @xmath58 and combine this choice with a @xmath59 . finally , a gradually increasing @xmath49 for decreasing @xmath15 still provides an improvement for the updated parameter choices . here",
    ", we choose a geometric sequence starting with @xmath60 and increasing by a factor of @xmath61 for each update of @xmath15 .    with an increased value of @xmath44 and gradually increasing values of @xmath49 , the computation time is bound to increase .",
    "this effect can , however , be counteracted by introducing a stopping criterion in the inner loop of the sl0 algorithm .",
    "therefore , we choose to terminate the inner loop when the relative change @xmath62 falls below @xmath63 where @xmath64 .",
    "generally , this measure has proved to be a good indicator of convergence and significantly reduced the average number of iterations taken in the inner loop .",
    "we now propose the smoothed @xmath0 norm algorithm with modified step - size ( sl0 mss ) which incorporates all of the above findings .",
    "* sl0 mss algorithm : *    * initialise : * @xmath59 , @xmath31 , @xmath65 , + @xmath61 , @xmath66 , @xmath67 @xmath34 [ alg : sl0_mod - ls_init ] @xmath68{^{\\mathrm{t}}}$ ] @xmath69 @xmath70 @xmath71 [ alg : sl0_mod - inner_loop ] @xmath72 @xmath36 @xmath73 [ alg : sl0_mod - x_update ]",
    "@xmath74 @xmath75 @xmath76 @xmath77",
    "through experiments , we have found the most time consuming parts of the sl0 mss algorithm to be the computation of @xmath23 and the matrix - vector products in the gradient descent step .",
    "we now provide a strategy for exploiting the known indeterminacy @xmath78 to reduce the number of required computations .",
    "consider the full qr decomposition of @xmath79 @xcite :    @xmath80 \\begin{bmatrix } { \\mathbf{r } } \\\\",
    "{ \\mathbf{0 } } \\end{bmatrix } \\end{aligned}\\ ] ]    where @xmath81 and @xmath82 form bases for the row space and null space of @xmath24 , respectively and @xmath83 is upper triangular . note how the dimensions of @xmath84 , @xmath85 , and @xmath86 change with @xmath54 . from the qr decomposition , the pseudo - inverse @xmath23 can be found :    @xmath87    three equivalent expressions for the projector that projects onto the null space of @xmath24 are then :    @xmath88    we now propose the following scheme for an implementation of the sl0 mss algorithm . when @xmath89 , use the projector @xmath23 and split the update in line [ alg : sl0_mod - x_update ] in an infeasible gradient descent step followed by a projection onto the feasible set :    @xmath90    when @xmath91 , use the projector @xmath92 and split the update in line [ alg : sl0_mod - x_update ] in two steps :    @xmath93    to appreciate the split procedure , note that fewer arithmetic operations are needed in computing the matrix - vector product @xmath94 this way when :    @xmath95    where @xmath96 denotes the number of floating point operations in the computation of the expression @xmath97 .",
    "the split procedure does not require the explicit forming of @xmath23 since the initial least squares ( least norm ) solution @xmath98 in line [ alg : sl0_mod - ls_init ] may be efficiently computed by solving the following system of equations by substitution ( since @xmath86 is triangular ) :    @xmath99    followed by computing :    @xmath100    avoiding the explicit forming of @xmath23 eliminates the need for computing the inverse of @xmath101 which becomes progressively more computationally expensive as @xmath102 .",
    "on the other hand , only the reduced qr decomposition @xmath103 is needed when @xmath23 is explicitly formed .",
    "the reduced qr decomposition becomes progressively less computationally expensive than the full qr decomposition as @xmath104 .",
    "these two observations motivate the change of method at @xmath105 .",
    "we evaluate the reconstruction capabilities of an algorithm by use of the phase transition measure @xcite which provides the following framework .",
    "let @xmath106 denote the number of non - zero entries in the true signal @xmath26 .",
    "define the measure of indeterminacy @xmath78 and the generalised measure of sparsity ( density ) @xmath107 . given a success criterion , the success rate is then evaluated on the phase space @xmath108 ^ 2 $ ] . in general , reconstruction is easier for larger @xmath54 and smaller @xmath109 and becomes increasingly difficult when decreasing @xmath54 and increasing @xmath109 .",
    "somewhere in - between , the phase transition curve separates the phase space into a phase where reconstruction is likely and a phase where it is unlikely .",
    "this phase transition curve is continuous in @xmath54 for fixed @xmath8 . obviously , it is desirable to have a phase transition curve which is as close to @xmath110 as possible .",
    "different suites of problems , i.e. different combinations of ensembles of @xmath24 and @xmath26 generally result in different phase transitions .",
    "choosing @xmath24 from the uniform spherical ensemble ( use ) and the non - zero entries in @xmath26 from the rademacher distributed generally yields the most difficult problem suite in terms of obtaining good phase transitions @xcite . in the simulations , we consider this problem suite along with a the problem suite where @xmath24 is chosen from the use and the non - zero entries in @xmath26 are chosen from the zero mean , unit variance gaussian ensemble . in @xcite , it is shown that the probability of reconstruction versus @xmath109 for fixed @xmath8 and @xmath54 can be modelled accurately by logistic regression for a specific set of algorithms .",
    "logistic regression is used more generally in @xcite to determine the location of the phase transition curve .",
    "we adopt the logistic regression approach to estimate the location of the phase transition curve and fix @xmath111 as proposed in @xcite .",
    "we then attempt reconstruction on a uniform grid @xmath112 in the phase space specified by :    @xmath113    for each point in the grid , we do 10 monte carlo simulations where each simulation features a new draw of @xmath24 and @xmath26 . from @xcite we adopt that an attempted reconstruction is considered successful when :    @xmath114    where @xmath10 and @xmath26 are the reconstructed and true signal , respectively . if the criterion is not met , the attempted reconstruction is considered unsuccessful , i.e. , the attempted reconstruction can not be considered indeterminate .    to evaluate the required computation time for the different algorithms",
    ", we measure the absolute time spent on reconstruction when it succeeds .",
    "the problem suite formed by choosing @xmath24 from the use combined with rademacher distributed non - zero entries in @xmath26 is used in this test . a uniform grid @xmath112 in the phase space is formed by :    @xmath115    an algorithm is tested on all points in the grid that are at least 0.025 below its empirically determined phase transition ( measured on the @xmath109-axis ) . the time spent on reconstruction for the successful part of 10 monte carlo simulations in each point",
    "is then averaged .",
    "considered problem sizes are :    @xmath116    the simulations have been conducted on an intel core i7 970 6-core based pc with ddr3 ram .",
    "the os used is 64-bit ubuntu 12.04 lts linux and the enthought python distribution ( epd ) 7.2 - 2 ( 64-bit ) .",
    "all simulations are carried out in double precision .    to validate the results obtained from our simulation framework",
    ", we have simulated the iterative hard thresholding algorithm presented in @xcite .",
    "the phase transition obtained in our simulation framework has then been compared with the phase transition obtained in the simulation framework of @xcite . due to the non - deterministic nature of the monte carlo simulations used in both simulation frameworks , the two phase transitions will inevitably differ slightly .",
    "however , we have observed that they are almost identical and therefore concluded , that our simulation framework works as intended .",
    "four algorithms have been simulated : 1 ) sl0 std which is the sl0 algorithm presented in section [ sec : sl0 ] .",
    "2 ) sl0 min which is the same algorithm except it is modified such that @xmath49 is multiplied by @xmath117 each time @xmath15 is decreased .",
    "3 ) sl0 mss which is the algorithm presented in section [ sec : improving_phase_transition ] .",
    "4 ) iht which is the iterative hard thresholding algorithm described in @xcite . in the case of the sl0 mss algorithm ,",
    "two implementations have been simulated : the sl0 mssslowromancap1@ implementation based on ( [ eq : sl0_mod_mssi_1 ] ) and ( [ eq : sl0_mod_mssi_2 ] ) and the sl0 mssslowromancap2@ implementation based on ( [ eq : qr_methods - split1 ] ) and ( [ eq : qr_methods - split2 ] ) .",
    "the experimental results are presented in figure [ fig : results - rademacher - phase ] , [ fig : results - gaussian - phase ] , [ fig : results - computation_times ] , and [ fig : results - scaling ] .",
    "figure [ fig : results - rademacher - phase ] shows the phase transitions for rademacher distributed non - zero entries in @xmath26 while figure [ fig : results - gaussian - phase ] shows the phase transitions for zero mean , unit variance gaussian non - zero entries in @xmath26 . in both figures , the theoretical @xmath1 curve from @xcite is included for reference .",
    "figure [ fig : results - computation_times ] shows the measured average computation times versus indeterminacy @xmath78 and figure [ fig : results - scaling ] shows the measured average computation times versus problem size @xmath8 . note the abrupt ending of the sl0 std curve in figure [ fig : results - computation_times ] which is due to failure of reconstruction in the tested grid for @xmath118 .",
    "also , note that the measured computation times of the sl0 min implementation have been divided by 20 .    in summary ,",
    "sl0 mss shows the best phase transition among the tested algorithms for both rademacher and gaussian non - zero entries in @xmath26 . in large portions of the phase space",
    "it even surpasses the theoretical @xmath1 curve .",
    "the only exception is in the gaussian case for @xmath119 where iht shows better phase transition . regarding computation time , iht is faster than the sl0 approaches among which sl0 min is consistently more than 20 times slower than sl0 mss",
    ". furthermore , iht scales slightly better with problem size @xmath8 than the sl0 approaches .",
    "for rademacher non - zero entries in @xmath26 , figure [ fig : results - rademacher - phase ] reveals that sl0 std , sl0 min , and iht are by far outperformed by sl0 mss in terms of phase transistion .",
    "even the theoretical @xmath1 curve is surpassed by sl0 mss at around @xmath120 .",
    "for @xmath121 , sl0 mss shows the same phase transition as the theoretical @xmath1 curve . the curve for sl0 min is a clear example of the improvement in phase transition obtainable using more iterations in the inner loop of the sl0 algorithm . however , sl0 mss further improves on this , especially for @xmath122 and @xmath120 .    the results in figure [ fig : results - computation_times ]",
    "settle that sl0 does indeed require more computation time than iht .",
    "iht is around two to four times faster ( depending on @xmath54 ) than the fastest sl0 implementation for a problem size of @xmath123 .",
    "the important thing to note though , is that sl0 provides a trade - off between phase transition and computation time .",
    "the price paid in computation time for using a lot of iterations to get better phase transition is clear from the sl0 min curve .",
    "this is , however , not the case for sl0 mss , which requires less than or about the same computation time as sl0 std depending on @xmath54 .",
    "thus , a much better phase transition is obtained using largely the same computation time in going from sl0 std to sl0 mss .",
    "to obtain such a result , it is necessary to switch from sl0 mssslowromancap1@ to sl0 mssslowromancap2@ at around @xmath105 .",
    "an assessment of the scaling of average computation time with problem size @xmath8 reveals that all three sl0 algorithms seem to scale in an equivalent way .",
    "iht scales better than sl0 and hence requires relatively less computation time as the problem size @xmath8 increases .",
    "the scaling depicted in figure [ fig : results - scaling ] is for @xmath105 which provides a rough average computation time across all values of @xmath54 as can be seen from figure [ fig : results - computation_times ] .",
    "the parameters for sl0 mss stated in section [ sec : improving_phase_transition ] are ( locally ) optimal in terms of phase transition for rademacher non - zero entries in @xmath26 .",
    "gaussian non - zero entries in @xmath26 are known to be in favour of greedy algorithms @xcite , which is also the case for iht in our simulations , especially for @xmath119 where the iht curve surpasses the theoretical @xmath1 curve . for",
    "@xmath124 , sl0 min and sl0 mss demonstrate the best phase transition among the shown algorithms .",
    "min and sl0 mss phase transitions are about the same , though .",
    "comparing our results for sl0 mss in figure [ fig : results - gaussian - phase ] with the ones given for sl0 in figure 6 in @xcite shows about the same phase transition .",
    "the slightly better phase transition for @xmath119 in @xcite may be due to the sl0 mss parameters not necessarily being optimal for gaussian non - zero entries in @xmath26 .",
    "although the above simulations are quite encouraging , they are based on an empirically tuned algorithm . thus , to reach a final verdict of the success of sl0 mss , the validity of the simulation results must be exhaustively studied for a broader set of problem suites .",
    "alternatively , more sound mathematical proofs must be presented .",
    "we have proposed a new compressive sensing reconstruction algorithm named sl0 mss based on the smoothed @xmath0 norm .",
    "it turns out that sl0 phase transitions heavily depend on parameter selection .",
    "sl0 mss attempts to improve on phase transition by exploiting the known indeterminacy @xmath125 combined with carefully selected parameters .",
    "a trade - off between phase transition and computation time is provided by sl0 .",
    "improved phase transition has been measured for sl0 mss compared to standard sl0 while maintaining the same computation time .",
    "10 [ 1]#1 url@samestyle [ 2]#2 [ 2 ] l@#1=l@#1#2 e.  j. cands and m.  b. wakin , `` an introduction to compressive sampling , '' _ ieee signal processing magazine _ , vol .",
    "25 , no .  2 , pp .",
    "2130 , mar .",
    "d.  l. donoho , `` compressed sensing , '' _ ieee transactions on signal processing _ ,",
    "52 , no .  4 , pp .",
    "12891306 , apr .",
    " , `` for most large underdetermined systems of linear equations the minimal 1-norm solution is also the sparsest solution , '' department of statistics , stanford university , tech . rep .",
    "2004 - 9 , sep . 2004 .",
    "[ online ] .",
    "available : http://statistics.stanford.edu/~ckirby/techreports/gen/2004/2004-09.pdf a.  maleki and d.  l. donoho , `` optimally tuned iterative reconstruction algorithms for compressed sensing , '' _ ieee journal of selected topics in signal processing _ , vol .  4 , no .  2 , pp .",
    "330341 , apr .",
    "s.  g. mallat and z.  zhang , `` matching pursuits with time - frequency dictionaries , '' _ ieee transactions on signal processing _ , vol .",
    "41 , no .  12 , pp . 33973415 , mar .",
    "t.  blumensath and m.  e. davies , `` iterative hard thresholding for compressed sensing , '' _ applied and computational harmonic analysis _",
    "27 , no .  3 , pp . 265274 , 2009 . j.  a. tropp and a.  c. gilbert , `` signal recovery from random measurements via orthogonal matching pursuit , '' _ ieee transactions on information theory _ , vol .",
    "53 , no .  12 , pp .",
    "46554666 , dec . 2007 .",
    "d.  needell and j.  tropp , `` cosamp : iterative signal recovery from incomplete and inaccurate samples , '' _ applied and computational harmonic analysis _ ,",
    "26 , no .  3 , pp . 301321 , 2009 . w.  dai and o.  milenkovic , `` subspace pursuit for compressive sensing signal reconstruction , '' _ ieee transactions on information theory _",
    "55 , no .  5 , pp .",
    "22302249 , may 2009 .",
    "t.  blumensath and m.  e. davies , `` normalized iterative hard thresholding : guaranteed stability and performance , '' _ ieee journal of selected topics in signal processing _ , vol .  4 , no .  2 , pp .",
    "298309 , apr .",
    "d.  l. donoho and j.  tanner , `` precise undersampling theorems , '' _ proceedings of the ieee _ , vol .",
    "98 , no .  6 , pp . 913924 , jun .",
    "d.  l. donoho , a.  maleki , and a.  montanari , `` message passing algorithms for compressed sensing : ii .",
    "analysis and validation , '' in _ ieee information theory workshop ( itw ) _ , cairo , egypt , jan . 68 , 2010 , pp . 15 .",
    "p.  jain , a.  tewari , and i.  s. dhillon , `` orthogonal matching pursuit with replacement , '' in _ twenty - fifth annual conference on neural information processing systems _ , granada , spain , dec .",
    "1215 , 2011 , pp . 16721680",
    ". b.  l. sturm , m.  g. christensen , and r.  gribonval , `` cyclic pure greedy algorithms for recovering compressively sampled sparse signals , '' in _",
    "45th ieee asilomar conference on signals , systems , and computers _ , pacific grove ( ca ) , usa , nov . 69 , 2011 , pp . 11431147 .",
    "g.  h. mohimani , m.  babaie - zadeh , and c.  jutten , `` a fast approach for overcomplete sparse decomposition based on smoothed l0 norm , '' _ ieee transactions on signal processing _ ,",
    "57 , no .  1 ,",
    "pp . 289301 , jan .",
    "z.  cui , h.  zhang , and w.  lu , `` an improved smoothed l0-norm algorithm based on multiparameter approximation function , '' in _",
    "12th ieee international conference on communication technology ( icct ) _ , nanjing , china , nov . 1114 , 2010 , pp .",
    ". h.  mohimani , m.  babaie - zadeh , i.  gorodnitsky , and c.  jutten , `` sparse recovery using smoothed l0 ( sl0 ) : convergence analysis , '' _ arxiv _ , 2010 , submitted ( on 24 january 2010 ) to ieee transactions on information theory .",
    "[ online ] .",
    "available : http://arxiv.org/abs/1001.5073 s.  boyd and l.  vandenberghe , _",
    "convex optimization_.1em plus 0.5em minus 0.4emcambridge university press , 2004 , 9th printing .",
    "d.  donoho and j.  tanner , `` observed universality of phase transitions in high - dimensional geometry , with implications for modern data analysis and signal processing , '' _ phil .",
    "a _ , vol .",
    "1906 , pp . 42734293 ,",
    "j.  tanner .",
    "phase transitions of the regular polytopes and cone : tabulated values .",
    "the university of edinburgh . accessed : 22 - 05 - 2012 .",
    "[ online ] .",
    "available : http://ecos.maths.ed.ac.uk/polytopes.shtml"
  ],
  "abstract_text": [
    "<S> signal reconstruction in compressive sensing involves finding a sparse solution that satisfies a set of linear constraints . </S>",
    "<S> several approaches to this problem have been considered in existing reconstruction algorithms . </S>",
    "<S> they each provide a trade - off between reconstruction capabilities and required computation time . in an attempt to push the limits for this trade - off </S>",
    "<S> , we consider a smoothed @xmath0 norm ( sl0 ) algorithm in a noiseless setup . </S>",
    "<S> we argue that using a set of carefully chosen parameters in our proposed adaptive sl0 algorithm may result in significantly better reconstruction capabilities in terms of phase transition while retaining the same required computation time as existing sl0 algorithms . a large set of simulations further support this claim . </S>",
    "<S> simulations even reveal that the theoretical @xmath1 curve may be surpassed in major parts of the phase space . </S>"
  ]
}