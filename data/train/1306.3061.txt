{
  "article_text": [
    "physicists have long hoped that the functional behavior of large , highly interconnected neural networks could be described by statistical mechanics @xcite .",
    "the goal of this effort has been not to simulate the details of particular networks , but to understand how interesting functions can emerge , collectively , from large populations of neurons .",
    "the hope , inspired by our quantitative understanding of collective behavior in systems near thermal equilibrium , is that such emergent phenomena will have some degree of universality , and hence that one can make progress without knowing all of the microscopic details of each system .",
    "a classic example of work in this spirit is the hopfield model of associative or content  addressable memory @xcite , which is able to recover the correct memory from any of its subparts of sufficient size . because the computational substrate of neural states in these models were binary `` spins , '' and the memories were realized as locally stable states of the network dynamics , methods of statistical physics",
    "could be brought to bear on theoretically challenging issues such as the storage capacity of the network or its reliability in the presence of noise @xcite . on the other hand , precisely because of these abstractions , it has not always been clear how to bring the predictions of the models into contact with experiment .",
    "recently it has been suggested that the analogy between statistical physics models and neural networks can be turned into a precise mapping , and connected to experimental data , using the maximum entropy framework @xcite . in a sense",
    ", the maximum entropy approach is the opposite of what we usually do in making models or theories .",
    "the conventional approach is to hypothesize some dynamics for the network we are studying , and then calculate the consequences of these assumptions ; inevitably , the assumptions we make will be wrong in detail . in the maximum entropy method , however , we are trying to strip away all our assumptions , and find models of the system that have _ as little structure as possible _ while still reproducing some set of experimental observations .",
    "the starting point of the maximum entropy method for neural networks is that the network could , if we do nt know anything about its function , wander at random among all possible states . we then take measured , average properties of the network activity as constraints , and each constraint defines some minimal level of structure .",
    "thus , in a completely random system neurons would generate action potentials ( spikes ) or remain silent with equal probability , but once we measure the mean spike rate for each neuron we know that there must be some departure from such complete randomness .",
    "similarly , absent any data beyond the mean spike rates , the maximum entropy model of the network is one in which each neuron spikes independently of all the others , but once we measure the correlations in spiking between pairs of neurons , an additional layer of structure is required to account for these data .",
    "the central idea of the maximum entropy method is that , for each experimental observation that we want to reproduce , we add only the minimum amount of structure required .",
    "an important feature of the maximum entropy approach is that the mathematical form of a maximum entropy model is exactly equivalent to a problem in statistical mechanics .",
    "that is , the maximum entropy construction defines an `` effective energy '' for every possible state of the network , and the probability that the system will be found in a particular state is given by the boltzmann distribution in this energy landscape .",
    "further , the energy function is built out of terms that are related to the experimental observables that we are trying to reproduce . thus , for example , if we try to reproduce the correlations among spiking in pairs of neurons , the energy function will have terms describing effective interactions among pairs of neurons . as explained in more detail below ,",
    "these connections are not analogies or metaphors , but precise mathematical equivalencies .",
    "minimally structured models are attractive , both because of the connection to statistical mechanics and because they represent the absence of modeling assumptions .",
    "of course , these features do not guarantee that such models will provide an accurate description of a real system .",
    "interest in maximum entropy approaches to networks of real neurons was triggered by the observation that , for groups of @xmath0 ganglion cells in the vertebrate retina , maximum entropy models based on the mean spike probabilities of individual neurons and correlations between pairs of cells indeed generate successful predictions for the probabilities of all the combinatorial patterns of spiking and silence in the network as it responds to naturalistic sensory inputs @xcite .",
    "in particular , the maximum entropy approach made clear that genuinely collective behavior in the network can be consistent with relatively weak correlations among pairs of neurons , so long as these correlations are widespread , shared among most pairs of cells in the system .",
    "this approach has now been used to analyze the activity in a variety of neural systems @xcite , the statistics of natural visual scenes @xcite , the structure and activity of biochemical and genetic networks @xcite , the statistics of amino acid substitutions in protein families @xcite , the rules of spelling in english words @xcite , and the directional ordering in flocks of birds @xcite .    one of the lessons of statistical mechanics is that systems with many degrees of freedom can behave in qualitatively different ways from systems with just a few degrees of freedom .",
    "if we can study only a handful of neurons ( e.g. , @xmath1 as in ref @xcite ) , we can try to extrapolate based on the hypothesis that the group of neurons that we analyze is typical of a larger population .",
    "these extrapolations can be made more convincing by looking at a population of @xmath2 neurons , and within such larger groups one can also try to test more explicitly whether the hypothesis of homogeneity or typicality is reliable @xcite .",
    "all these analyses suggest that , in the salamander retina , the roughly 200 interconnected neurons that represent a small patch of the visual world should exhibit dramatically collective behavior .",
    "in particular , the states of these large networks should cluster around local minima of the energy landscape , much as for the attractors in the hopfield model of associative memory @xcite .",
    "further , this collective behavior means that responses will be substantially redundant , with the behavior of one neuron largely predictable from the state of other neurons in the network ; stated more positively , this collective response allows for pattern completion and error correction .",
    "finally , the collective behavior suggested by these extrapolations is a very special one , in which the probability of particular network states , or equivalently the degree to which we should be surprised by the occurrence of any particular state , has an anomalously large dynamic range @xcite .",
    "if correct , these predictions would have a substantial impact on how we think about coding in the retina , and about neural network function more generally .",
    "correspondingly , there is some controversy about all these issues @xcite .",
    "time bins , where @xmath3 represents a silence ( absence of spike ) of neuron @xmath4 , and @xmath5 represents a spike .",
    "[ f1 ] ]    here we return to the salamander retina , in experiments that exploit a new generation of multi ",
    "electrode arrays and associated spike  sorting algorithms @xcite . as schematized in fig  [ f1",
    "] , these methods make it possible to record from @xmath6 ganglion cells in the relevant densely interconnected patch , while projecting natural movies onto the retina .",
    "access to these large populations poses new problems for the inference of maximum entropy models , both in principle and in practice .",
    "what we find is that , with extensions of algorithms developed previously @xcite , it is possible to infer maximum entropy models for more than one hundred neurons , and that with nearly two hours of data there are no signs of `` overfitting . ''",
    "we have built models that match the mean probability of spiking for individual neurons , the correlations between spiking in pairs of neurons , and the distribution of summed activity in the network ( i.e. , the probability that @xmath7 out of the @xmath8 neurons spike in the same small window of time @xcite ) .",
    "we will see that models which satisfy all these experimental constraints provide a strikingly accurate description of the states taken on by the network as a whole , that these states _ are _ collective , and that the collective behavior predicted by our models has implications for how the retina encodes visual information .",
    "the idea of maximizing entropy has its origin in thermodynamics and statistical mechanics .",
    "the idea that we can use this principle to build models of systems that are not in thermal equilibrium is more recent , but still more than fifty years old @xcite .",
    "here we provide a description of this approach which we hope makes the ideas accessible to a broad audience .",
    "we imagine a neural system exposed to a stationary stimulus ensemble , in which simultaneous recordings of @xmath8 neurons can be made . in small windows of time , as we see in fig  [ f1 ] , a single neuron @xmath4 either does ( @xmath5 ) or does not ( @xmath3 ) generate an action potential or spike @xcite",
    "; the state of the entire network in that time bin is therefore described by a `` binary word '' @xmath9 .",
    "as the system responds to its inputs , it visits each of these states with some probability @xmath10 .",
    "even before we ask what the different states mean , for example as codewords in a representation of the sensory world , specifying this distribution requires us to determine the probability of each of @xmath11 possible states .",
    "once @xmath8 increases beyond @xmath12 , brute force sampling from data is no longer a general strategy for `` measuring '' the underlying distribution .",
    "even when there are many , many possible states of the network , experiments of reasonable size can be sufficient to estimate the averages or expectation values of various functions of the state of the system , @xmath13 , where the averages are taken across data collected over the course of the experiment .",
    "the goal of the maximum entropy construction is to search for the probability distribution @xmath14 that matches these experimental measurements but otherwise is as unstructured as possible .",
    "minimizing structure means maximizing entropy @xcite , and for any set of moments or statistics that we want to match , the form of the maximum entropy distribution can be found analytically : @xmath15 where @xmath16 is the effective `` energy '' function or the hamiltonian of the system , and the partition function @xmath17 ensures that the distribution is normalized .",
    "the couplings @xmath18 must be set such that the expectation values of all constraint functions @xmath19 , @xmath20 , over the distribution @xmath21 match those measured in the experiment : @xmath22 these equations might be hard to solve , but they are guaranteed to have exactly one solution for the couplings @xmath18 given any set of measured expectation values @xcite .",
    "why should we study the neural vocabulary , @xmath23 , at all ?",
    "in much previous work on neural coding , the focus has been on constructing models for a `` codebook '' which can predict the response of the neurons to arbitrary stimuli , @xmath24 @xcite , or on building a `` dictionary '' that describes the stimuli consistent with particular patterns of activity , @xmath25 @xcite . in a natural setting , stimuli are drawn from a space of very high dimensionality , so constructing these `` encoding '' and `` decoding '' mappings between the stimuli and responses is very challenging and often involves making strong assumptions about how stimuli drive neural spiking ( e.g. through linear filtering of the stimulus ) @xcite . by trying to understand directly the total distribution of responses , @xmath23 , rather than the conditional distribution , @xmath24",
    ", we take a very different approach .",
    "already when we study the smallest possible network , i.e. a pair of interacting neurons , the usual approach is to measure the correlation between spikes generated in the two cells , and to dissect this correlation into contributions which are intrinsic to the network and those which are ascribed to common , stimulus driven inputs .",
    "the idea of decomposing correlations dates back to a time when it was hoped that correlations among spikes could be used to map the synaptic connections between neurons @xcite .",
    "in fact , in a highly interconnected system , the dominant source of correlations between two neurons  even if they are entirely intrinsic to the network  will always be through the multitude of indirect paths involving other neurons @xcite . regardless of the source of these correlations , however , the question of whether they are driven by the stimulus or are intrinsic to the network is not a question that the brain can answer . we , as external observers , can repeat the stimulus exactly , and search for correlations conditional on the stimulus , but this is not accessible to the organism .",
    "the brain has access only to the output of the retina : the patterns of activity which are drawn from the distribution @xmath23 .",
    "if the responses @xmath9 are codewords for the visual stimulus , then the entropy of this distribution sets the capacity of the code to carry information . word by word , @xmath26 determines how surprised the brain should be by each particular pattern of response , including the possibility that the response was corrupted by noise in the retinal circuit and thus should be corrected or ignored @xcite . in a very real sense , what the brain `` sees '' are sequences of states drawn from @xmath23 .",
    "in the same spirit that many groups have studied the statistical structures of natural scenes @xcite , we would like to understand the statistical structure of the codewords that represent these scenes .",
    "the maximum entropy method is not a model for network activity .",
    "rather it is a framework for building models , and to implement this framework we have to choose which functions of the network state @xmath27 we think are interesting .",
    "the hope is that while there are @xmath11 states of the system as a whole , there is a much smaller number of measurements , @xmath28 , with @xmath29 and @xmath30 , which will be sufficient to capture the essential structure of the collective behavior in the system .",
    "we emphasize that this is a hypothesis , and must be tested .",
    "how should we choose the functions @xmath31 ?",
    "in this work we consider three classes of possibilities :    \\(a ) we expect that networks have very different behaviors depending on the overall probability that neurons generate spikes as opposed to remaining silent .",
    "thus , our first choice of functions to constrain in our models is the set of mean spike probabilities or firing rates , which is equivalent to constraining @xmath32 , for each neuron @xmath33 .",
    "these constraints contribute a term to the energy function @xmath34 note that @xmath35 , where @xmath36 is the mean spike rate of neuron @xmath33 , and @xmath37 is the size of the time slices that we use in our analysis , as in fig  [ f1 ] .",
    "_ maximum entropy models that constrain only the firing rates of all the neurons ( i.e. @xmath38 ) are called `` independent models '' ; we denote their distribution functions by @xmath39 . _",
    "\\(b ) as a second constraint we take the correlations between neurons , two by two .",
    "this corresponds to measuring @xmath40 for every pair of cells @xmath41 .",
    "these constraints contribute a term to the energy function @xmath42 it is more conventional to think about correlations between two neurons in terms of their spike trains .",
    "if we define @xmath43 where neuron @xmath33 spikes at times @xmath44 , then the spike  spike correlation function is @xcite @xmath45 and we also have the average spike rates @xmath46 .",
    "the correlations among the discrete spike / silence variables @xmath47 then can be written as @xmath48 _ maximum entropy models that constrain average firing rates and correlations ( i.e. @xmath49 ) are called `` pairwise models '' ; we denote their distribution functions by @xmath50 . _",
    "\\(c ) firing rates and pairwise correlations focus on the properties of particular neurons . as an alternative",
    ", we can consider quantities that refer to the network as a whole , independent of the identity of the individual neurons .",
    "a simple example is the `` distribution of synchrony '' ( also called `` population firing rate '' ) , that is the probability @xmath51 that @xmath7 out of the @xmath8 neurons spike in the same small slice of time .",
    "we can count the number of neurons that spike by summing all of the @xmath52 , remembering that we have @xmath53 for spikes and @xmath3 for silences .",
    "then @xmath54 where @xmath55 if we know the distribution @xmath51 , then we know all its moments , and hence we can think of the functions @xmath31 that we are constraining as being @xmath56 and so on . because there are only @xmath8 neurons , there are only @xmath57 possible values of @xmath7 , and hence only @xmath8 unique moments .",
    "constraining all of these moments contributes a term to the energy function @xmath58 where @xmath59 is an effective potential @xcite .",
    "_ maximum entropy models that constrain average firing rates , correlations , and the distribution of synchrony ( i.e. @xmath60 ) are called `` k - pairwise models '' ; we denote their distribution functions by @xmath61 .",
    "_    it is important that the mapping between maximum entropy models and a boltzmann distribution with some effective energy function is _ not _ an analogy , but rather a mathematical equivalence . in using the maximum entropy approach",
    "we are _ not _ assuming that the system of interest is in some thermal equilibrium state ( note that there is no explicit temperature in eq  ( [ boltz ] ) ) , nor are we assuming that there is some mysterious force which drives the system to a state of maximum entropy .",
    "we are also not assuming that the temporal dynamics of the network is described by newton s laws or brownian motion on the energy landscape .",
    "what we are doing is making models that are consistent with certain measured quantities , but otherwise have as little structure as possible .",
    "as noted above , this is the opposite of what we usually do in building models or theories  rather than trying to impose some hypothesized structure on the world , we are trying to remove all structures that are not strictly required by the data .",
    "the mapping to a boltzmann distribution is not an analogy , but if we take the energy function more literally we are making use of analogies .",
    "thus , the term @xmath62 that emerges from constraining the mean spike probabilities of every neuron is analogous to a magnetic field being applied to each spin , where spin `` up '' ( @xmath63 ) marks a spike and spin `` down '' ( @xmath3 ) denotes silence .",
    "similarly , the term @xmath64 that emerges from constraining the pairwise correlations among neurons corresponds to a `` spin  spin '' interaction which tends to favor neurons firing together ( @xmath65 ) or not ( @xmath66 ) . finally , the constraint on the overall distribution of activity generates a term @xmath67 which we can interpret as resulting from the interaction between all the spins / neurons in the system and one other , hidden degree of freedom , such as an inhibitory interneuron .",
    "these analogies can be useful , but need not be taken literally .",
    "we have applied the maximum entropy framework to the analysis of one large experimental data set on the responses of ganglion cells in the salamander retina to a repeated , naturalistic movie .",
    "these data are collected using a new generation of multi ",
    "electrode arrays that allow us to record from a large fraction of the neurons in a @xmath68 patch , which contains a total of @xmath69 ganglion cells @xcite , as in fig  [ f1 ] . in the present data set ,",
    "we have selected 160 neurons that pass standard tests for the stability of spike waveforms , the lack of refractory period violations , and the stability of firing across the duration of the experiment ( see methods and ref @xcite ) .",
    "the visual stimulus is a greyscale movie of swimming fish and swaying water plants in a tank ; the analyzed chunk of movie is @xmath70 long , and the recording was stable through 297 repeats , for a total of more than @xmath71 of data .",
    "as has been found in previous experiments in the retinas of multiple species @xcite , we found that correlations among neurons are most prominent on the @xmath72 time scale , and so we chose to discretize the spike train into @xmath73 bins .",
    "( top ) , firing rates ( equivalent to @xmath32 ; middle ) , and the distribution of correlation coefficients @xmath74 ( bottom ) .",
    "the red distribution is the distribution of differences between two halves of the experiment , and the small red error bar marks the standard deviation of correlation coefficients in fully shuffled data ( @xmath75 ) . at right , the parameters of a pairwise maximum entropy model [ @xmath76 from eq  ( [ hpair ] ) ] that reproduces these data : coupling constants @xmath77 ( top ) , fields @xmath78 ( middle ) , and the distribution of couplings in this group of neurons . [ f2 ] ]    .",
    "maximum entropy models have a simple form [ eq  ( [ boltz ] ) ] that connects precisely with statistical physics . but to complete the construction of a maximum entropy model , we need to impose the condition that averages in the maximum entropy distribution match the experimental measurements , as in eq  ( [ con1 ] )",
    ". this amounts to finding all the coupling constants @xmath79 in eq  ( [ ham ] ) .",
    "this is , in general , a hard problem .",
    "we need not only to solve this problem , but also convince ourselves that our solution is meaningful , and that it does not reflect overfitting to the limited set of data at our disposal .",
    "a detailed account of the numerical solution to this inverse problem is given in appendix  [ app_entest ] .",
    "in fig  [ f2 ] we show an example of @xmath80 neurons from a small patch of the salamander retina , responding to naturalistic movies .",
    "we notice that correlations are weak , but widespread , as in previous experiments on smaller groups of neurons @xcite .",
    "because the data set is very large , the threshold for reliable detection of correlations is very low ; if we shuffle the data completely by permuting time and repeat indices independently for each neuron , the standard deviation of correlation coefficients , @xmath81 is @xmath82 , as shown in fig  [ f2]c , vastly smaller than the typical correlations that we observe ( median @xmath83 , 90% of values between @xmath84 and @xmath85 ) . more subtly , this means that only a few percent of the correlation coefficients are within error bars of zero , and there is no sign that there is a finite fraction of pairs that have truly zero correlation ",
    "the distribution of correlations across the population seems continuous .",
    "note that , as customary , we report normalized correlation coefficients ( @xmath74 , between -1 and 1 ) , while maximum entropy formally constrains an equivalent set of unnormalized second order moments , @xmath86 [ eq ( [ def_cij ] ) ] .     covariance matrix elements , normalized by the estimated error bar in the data ; red overlay is a gaussian with zero mean and unit variance .",
    "the distribution has nearly gaussian shape with a width of @xmath87 , showing that the learning algorithm reconstructs the covariance statistics to within measurement precision .",
    "[ f3 ] ]    we began by constructing maximum entropy models that match the mean spike rates and pairwise correlations , i.e. `` pairwise models , '' whose distribution is , from eqs  ( [ ha],[hb ] ) , @xmath88\\nonumber\\\\ { \\mathcal h } & = & - \\sum_{{\\rm i}=1}^n h_{\\rm i } \\sigma_{\\rm i } - { 1\\over 2 } \\sum_{{\\rm i } , { \\rm j } = 1}^n j_{\\rm ij } \\sigma_{\\rm i } \\sigma_{\\rm j } .",
    "\\label{hpair}\\end{aligned}\\ ] ] when we reconstruct the coupling constants of the maximum entropy model , we see that the `` interactions '' @xmath77 among neurons are widespread , and almost symmetrically divided between positive and negative values ; for more details see appendix  [ app_entest ] .",
    "figure  [ f3 ] shows that the model we construct really does satisfy the constraints , so that the differences , for example , between the measured and predicted correlations among pairs of neurons are within the experimental errors in the measurements .",
    "with @xmath80 neurons , measuring the mean spike probabilities and all the pairwise correlations means that we estimate @xmath89 separate quantities .",
    "this is a large number , and it is not clear that we are safe in taking all these measurements at face value .",
    "it is possible , for example , that with a finite data set the errors in the different elements of the correlation matrix @xmath86 are sufficiently strongly correlated that we do nt really know the matrix as a whole with high precision , even though the individual elements are measured very accurately .",
    "this is a question about overfitting : is it possible that the parameters @xmath90 are being finely tuned to match even the statistical errors in our data ?    ) under the pairwise model of eq  ( [ hpair ] ) , computed on the training repeats ( black dots ) and on the testing repeats ( red dots ) , for the same group of @xmath80 neurons shown in fig  [ f2 ] and [ f3 ] . here",
    "the repeats have been reordered so that the training repeats precede testing repeats ; in fact , the choice of test repeats is random . *",
    "( b ) * the ratio of the log - likelihoods on test vs training data , shown as a function of the network size @xmath8 .",
    "error bars are the standard deviation across 30 subgroups at each value of @xmath8 .",
    "[ f4 ] ]    to test for overfitting ( fig [ f4 ] ) , we exploit the fact that the stimuli consist of a short movie repeated many times . we can choose a random @xmath91 of these repeats from which to learn the parameters of the maximum entropy model , and then check that the probability of the data in the other @xmath92 of the experiment is predicted to be the same , within errors .",
    "we see in fig  [ f4 ] that this is true , and that it remains true as we expand from @xmath93 neurons ( for which we surely have enough data ) out to @xmath94 , where we might have started to worry .",
    "taken together , figs  [ f2 ] , [ f3 ] , and [ f4 ] suggest strongly that our data and algorithms are sufficient to construct maximum entropy models , reliably , for networks of more than one hundred neurons .",
    "how well do our maximum entropy models describe the behavior of large networks of neurons ?",
    "the models predict the probability of occurrence for all possible combinations of spiking and silence in the network , and it seems natural to use this huge predictive power to test the models . in small networks , this is a useful approach . indeed ,",
    "much of the interest in the maximum entropy approach derives from the success of models based on mean spike rates and pairwise correlations , as in eq  ( [ hpair ] ) , in reproducing the probability distribution over states in networks of size @xmath0 @xcite . with @xmath93",
    ", there are @xmath95 possible combinations of spiking and silence , and reasonable experiments are sufficiently long to estimate the probabilities of all of these individual states . but with @xmath80",
    ", there are @xmath96 possible states , and so it is not possible to `` just measure '' all the probabilities .",
    "thus , we need another strategy for testing our models .    striking ( and model  independent ) evidence for nontrivial collective behavior in these networks",
    "is obtained by asking for the probability that @xmath7 out of the @xmath8 neurons generate a spike in the same small window of time , as shown in fig  [ f5 ] .",
    "this distribution , @xmath51 , should become gaussian at large @xmath8 if the neurons are independent , or nearly so , and we have noted that the correlations between pairs of cells are weak . thus @xmath97 is very well approximated by an independent model , with fractional errors on the order of the correlation coefficients , typically less than @xmath98 .",
    "but , even in groups of @xmath93 cells , there are substantial departures from the predictions of an independent model ( fig [ f5]a ) . in groups of @xmath2 cells ,",
    "we see @xmath99 cells spiking synchronously with probability @xmath100 times larger than expected from an independent model ( fig [ f5]b ) , and the departure from independence is even larger at @xmath80 ( fig  [ f5]c ) .     simultaneous spikes ( spike synchrony ) .",
    "* * ( a - c ) * @xmath51 for subnetworks of size @xmath101 ; error bars are s.d . across random halves of the duration of the experiment .",
    "for @xmath93 we already see large deviations from an independent model , but these are captured by the pairwise model . at @xmath2 ( b ) , the pairwise models miss the tail of the distribution , where @xmath102 . at @xmath80 ( c ) ,",
    "the deviations between the pairwise model and the data are more substantial . *",
    "( d ) * the probability of silence in the network , as a function of population size ; error bars are s.d . across 30 subgroups of a given size @xmath8 . throughout",
    ", red shows the data , grey the independent model , and black the pairwise model.[f5 ] ]    maximum entropy models that match the mean spike rate and pairwise correlations in a network make an unambiguous , quantitative prediction for @xmath51 , with no adjustable parameters . in smaller groups of neurons , certainly for @xmath93",
    ", this prediction is quite accurate , and accounts for most of the difference between the data and the expectations from an independent model , as shown in fig  [ f5 ] . but",
    "even at @xmath2 we see small deviations between the data and the predictions of the pairwise model .",
    "because the silent state is highly probable , we can measure @xmath103 very accurately , and the pairwise models make errors of nearly a factor of three at @xmath80 .",
    "these errors are negligible when compared to the many orders of magnitude differences from an independent model , but they are highly significant .",
    "the pattern of errors also is important , since in the real networks silence persists as being highly probable even at @xmath80 ( and beyond ) , which is surprising @xcite , and the pairwise model does nt quite capture this .    if a model based on pairwise correlations does nt quite account for the data , it is tempting to try and include correlations among triplets of neurons .",
    "but at @xmath80 there are @xmath104 of these triplets , so a model that includes these correlations is much more complex than one that stops with pairs .",
    "an alternative is to use @xmath51 itself as a constraint on our models , as explained above in relation to eq  ( [ hc ] ) .",
    "this defines the `` k - pairwise model , '' @xmath105\\label{hkpair } \\\\",
    "{ \\mathcal h } ( \\{\\sigma_{\\rm i}\\})&= & - \\sum_{{\\rm i}=1}^n h_{\\rm i } \\sigma_{\\rm i } - { 1\\over 2 } \\sum_{{\\rm i } , { \\rm j } = 1}^n j_{\\rm ij } \\sigma_{\\rm i } \\sigma_{\\rm j}- v \\left(\\sum_{{\\rm i}=1}^n \\sigma_{\\rm i } \\right ) ,   \\nonumber \\ ] ] where the `` potential '' @xmath59 is chosen to match the observed distribution @xmath51 .",
    "as noted above , we can think of this potential as providing a global regulation of the network activity , such as might be implemented by inhibitory interneurons with ( near ) global connectivity . whatever the mechanistic interpretation of this model , it is important that it is _ not _ much more complex than the pairwise model : matching @xmath51 adds only @xmath106 parameters to our model , while the pairwise model already has @xmath107 parameters .",
    "all of the tests given in the previous section can be redone in this case , and again we find that we can learn the k - pairwise models from the available data with no signs of overfitting .",
    "figure  [ f6 ] shows the parameters of the k - pairwise model for the same group of @xmath80 neurons shown in fig  [ f2 ] .",
    "notice that the pairwise interaction terms @xmath77 remain roughly the same ; the local fields @xmath78 are also similar but have a shift towards more negative values .",
    "cells shown in fig  [ f1 ] . *",
    "the neurons are again sorted in the order of decreasing firing rates . *",
    "( a ) * pairwise interactions , @xmath77 , and the comparison with the interactions of the pairwise model , * ( b)*. * ( c ) * single - neuron fields , @xmath78 , and the comparison with the fields of the pairwise model , * ( d)*. * ( e ) * the global potential , @xmath108 , where @xmath7 is the number of synchronous spikes . [ f6 ] ]    since we did nt make explicit use of the triplet correlations in constructing the k - pairwise model , we can test the model by predicting these correlations . in fig",
    "[ f7]a we show @xmath109 as computed from the real data and from the models , for a single group of @xmath80 neurons . we see that pairwise models capture the rankings of the different triplets , so that more strongly correlated triplets are predicted to be more strongly correlated , but these models miss quantitatively , overestimating the positive correlations and failing to predict significantly negative correlations .",
    "these systematic errors are largely corrected in the k - pairwise model , despite the fact that adding a constraint on @xmath51 does nt add any information about the identity of the neurons in the different triplets .",
    "it is also interesting that this improvement in our predictions ( as well as that in fig  [ f8 ] below ) occurs even though the numerical value of the effective potential @xmath110 is quite small , as shown in fig  [ f6]e .",
    "fixing the distribution of global activity thus seems to capture something about the network that individual spike probabilities and pairwise correlations have missed .     from eq ( [ 3pt ] ) . * * ( a ) * measured @xmath111 ( x - axis ) vs predicted by the model ( y - axis ) , shown for an example 100 neuron subnetwork .",
    "the @xmath112 triplets are binned into 1000 equally populated bins ; error bars in x are s.d .",
    "across the bin .",
    "the corresponding values for the predictions are grouped together , yielding the mean and the s.d . of the prediction ( y - axis ) .",
    "inset shows a zoom - in of the central region , for the k - pairwise model . *",
    "( b ) * error in predicted three - point correlation functions as a function of subnetwork size @xmath8 .",
    "shown are mean absolute deviations of the model prediction from the data , for pairwise ( black ) and k - pairwise ( red ) models ; error bars are s.d . across 30 subnetworks at each @xmath8 , and the dashed line shows the mean absolute difference between two halves of the experiment .",
    "inset shows the distribution of three  point correlations ( grey filled region ) and the distribution of differences between two halves of the experiment ( dashed line ) ; note the logarithmic scale .",
    "[ f7 ] ]    an interesting effect is shown in fig  [ f7]b , where we look at the average absolute deviation between predicted and measured @xmath111 , as a function of the group size @xmath8 . with increasing @xmath8 the ratio between the total number of ( predicted ) three - point correlations and ( fitted ) model parameters is increasing ( from @xmath113 at @xmath93 to @xmath114 for @xmath94 ) , leading us to believe that predictions will grow progressively worse .",
    "nevertheless , the average error in three - point prediction stays constant with network size , for both pairwise and k - pairwise models .",
    "an attractive explanation is that , as @xmath8 increases , the models encompass larger and larger fractions of the interacting neural patch and thus decrease the effects of `` hidden '' units , neurons that are present but not included in the model ; such unobserved units , even if they only interacted with other units in a pairwise fashion , could introduce effective higher - order interactions between observed units , thereby causing three - point correlation predictions to deviate from those of the pairwise model @xcite .",
    "the accuracy of the k - pairwise predictions is not quite as good as the errors in our measurements ( dashed line in fig  [ f7]b ) , but still very good , improving by a factor of @xmath115 relative to the pairwise model to well below @xmath116 .    .",
    "* * ( a ) * the cumulative distribution of energies , @xmath117 from eq  ( [ cume ] ) , for the k - pairwise models ( red ) and the data ( black ) , in a population of 120 neurons .",
    "inset shows the high energy tails of the distribution , @xmath118 from eq  ( [ cumge ] ) ; dashed line denotes the energy that corresponds to the probability of seeing the pattern once in an experiment .",
    "* ( b ) * relative difference in the first two moments ( mean , @xmath119 , dashed ; standard deviation , @xmath120 , solid ) of the distribution of energies evaluated over real data and a sample from the corresponding model ( black = pairwise ; red = k - pairwise ) .",
    "error bars are s.d .",
    "over 30 subnetworks at a given size @xmath8.[f8 ] ]    maximum entropy models assign an effective energy to every possible combination of spiking and silence in the network , @xmath121 from eq  ( [ hkpair ] ) .",
    "learning the model means specifying all the parameters in this expression , so that the mapping from states to energies is completely determined .",
    "the energy determines the probability of the state , and while we ca nt estimate the probabilities of all possible states , we can ask whether the distribution of energies that we see in the data agrees with the predictions of the model .",
    "thus , if we have a set of states drawn out of a distribution @xmath122 , we can count the number of states that have energies lower than @xmath123 , @xmath124 , \\label{cume}\\ ] ] where @xmath125 is the heaviside step function , @xmath126 similarly , we can count the number of states that have energy larger than @xmath123 , @xmath127 , \\label{cumge}\\ ] ] now we can take the distribution @xmath122 to be the distribution of states that we actually see in the experiment , or we can take it to be the distribution predicted by the model , and if the model is accurate we should find that the cumulative distributions are similar in these two cases .",
    "results are shown in fig  [ f8 ] .",
    "we see that the distribution of energies in the data and the model are very similar .",
    "there is an excellent match in the `` low energy '' ( high probability ) region , and then as we look at the high energy tail ( @xmath118 ) we see that theory and experiment match out to probabilities of better than @xmath128 . thus the distribution of energies , which is an essential construct of the model , seems to match the data across @xmath129 of the states that we see .",
    "the successful prediction of the cumulative distribution @xmath118 is especially striking because it extends to @xmath130 . at these energies ,",
    "the probability of any single state is predicted to be @xmath131 , which means that these states should occur roughly once per fifty years ( ! ) .",
    "this seems ridiculous  what are such rare states doing in our analysis , much less as part of the claim that theory and experiment are in quantitative agreement ?",
    "the key is that there are many , many of these rare states ",
    "so many , in fact , that the theory is predicting that @xmath98 of the all the states we observe will be ( at least ) this rare : individually surprising events are , as a group , quite common . in fact , of the @xmath132 combinations of spiking and silence ( @xmath133 distinct ones ) that we see in subnetworks of @xmath94 neurons , @xmath134 of these occur only once , which means we really do nt know anything about their probability of occurrence .",
    "we ca nt say that the probability of any one of these rare states is being predicted correctly by the model , since we ca nt measure it , but we can say that the distribution of ( log ) probabilities  that is , the distribution of energies  across the set of observed states is correct , down to the @xmath98 level .",
    "the model thus is predicting things far beyond what can be inferred directly from commonly observed patterns of activity .     neurons . * given any configuration of @xmath135 neurons , the k - pairwise model predicts the probability of firing of the @xmath8-th neuron by eqs  ( [ heff],[tanh ] ) ; the effective field @xmath136 is fully determined by the parameters of the maximum entropy model and the state of the network . for each activity pattern in recorded data we computed the effective field , and binned these values ( shown on x - axis ) .",
    "for every bin we estimated from data the probability that the @xmath8-th neuron spiked ( black circles ; error bars are s.d . across 120 cells ) .",
    "this is compared with a parameter - free prediction ( red line ) from eq  ( [ tanh ] ) .",
    "the gray shaded region shows the distribution of the values of @xmath136 over all 120 neurons and all patterns in the data .",
    "[ f9 ] ]    finally , the structure of the models we are considering is that the state of each neuron  an ising spin  experiences an `` effective field '' from all the other spins , determining the probability of spiking vs. silence .",
    "this effective field consists of an intrinsic bias for each neuron , plus the effects of interactions with all the other neurons : @xmath137 if the model is correct , then the probability of spiking is simply related to the effective field , @xmath138 to test this relationship , we can choose one neuron , compute the effective field from the states of all the other neurons , at every moment in time , then collect all those moments when @xmath136 is in some narrow range , and see how often the neuron spikes .",
    "we can then repeat this for every neuron , in turn .",
    "if the model is correct , spiking probability should depend on the effective field according to eq  ( [ tanh ] ) .",
    "we emphasize that there are no new parameters to be fit , but rather a parameter ",
    "free relationship to be tested .",
    "the results are shown in fig  [ f9 ] .",
    "we see that , throughout the range of fields that are well sampled in the experiment , there is good agreement between the data and eq  ( [ tanh ] ) .",
    "as we go into the tails of the distribution , we see some deviations , but error bars also are ( much ) larger .",
    "we have seen that it is possible to construct maximum entropy models which match the mean spike probabilities of each cell , the pairwise correlations , and the distribution of summed activity in the network , and that our data are sufficient to insure that all the parameters of these models are well determined , even when we consider groups of @xmath80 neurons or more .",
    "figures  [ f7 ] through [ f9 ] indicate that these models give a fairly accurate description of the distribution of states  the myriad combinations of spiking and silence  taken on by the network as a whole . in effect we have constructed a statistical mechanics for these networks , not by analogy or metaphor but in quantitative detail .",
    "we now have to ask what we can learn about neural function from this description .      in the hopfield model",
    ", dynamics of the neural network corresponds to motion on an energy surface .",
    "simple learning rules can sculpt the energy surface to generate multiple local minima , or attractors , into which the system can settle .",
    "these local minima can represent stored memories , or the solutions to various computational problems @xcite .",
    "if we imagine monitoring a hopfield network over a long time , the distribution of states that it visits will be dominated by the local minima of the energy function .",
    "thus , even if we ca nt take the details of the dynamical model seriously , it still should be true that the energy landscape determines the probability distribution over states in a boltzmann  like fashion , with multiple energy minima translating into multiple peaks of the distribution .    in our maximum entropy models ,",
    "we find a range of @xmath77 values encompassing both signs ( figs  [ f2]d and f ) , as in spin glasses @xcite .",
    "the presence of such competing interactions generates `` frustration , '' where ( for example ) triplets of neurons can not find a combination of spiking and silence that simultaneously minimizes all the terms in the energy function @xcite . in the simplest model of spin glasses , these frustration effects , distributed throughout the system , give rise to a very complex energy landscape , with a proliferation of local minima @xcite .",
    "our models are not precisely hopfield models , nor are they instances of the standard ( more random ) spin glass models .",
    "nonetheless , by looking at the pairwise @xmath77 terms in the energy function of our models , @xmath139 of all interacting triplets of neurons are frustrated across different subnetworks of various sizes ( @xmath140 ) , and it is reasonable to expect that we will find many local minima in the energy function of the network .    , for k - pairwise models ( black line ) .",
    "gray lines show the subsets of those basins that are encountered multiple times in the recording ( more than 10 times , dark gray ; more than 100 times , light gray ) .",
    "error bars are s.d .",
    "over 30 subnetworks at every @xmath8 .",
    "note the logarithmic scale for the number of ms states . ]         out of a total population of 160 neurons ; patterns have been clustered into 12 clusters ( colors ) . *",
    "( b ) * the probability ( across stimulus repeats ) that the population is in a particular basin of attraction at any given time .",
    "each line corresponds to one pattern from ( a ) ; patterns belonging to the same cluster are depicted in the same color .",
    "inset shows the detailed structure of several transitions out of the all - silent state ; overlapping lines of the same color show that the same transition is identified robustly across different subnetwork choices of 120 neurons out of 160 . *",
    "( c ) * on about half of the time bins , the population is in the all - silent basin ; on the remaining time bins , the coherence ( the probability of being in the dominant basin divided by the probability of being in every possible non - silent basin ) is high . *",
    "( d ) * the average autocorrelation function of traces in ( b ) , showing the typical time the population stays within a basin ( dashed red line is best exponential fit with @xmath141 , or about 2.5 time bins ) . ]    to search for local minima of the energy landscape , we take every combination of spiking and silence observed in the data and move `` downhill '' on the function @xmath142 from eq ( [ hkpair ] ) ( see appendix  [ app_descent ] ) .",
    "when we can no longer move downhill , we have identified a locally stable pattern of activity , or a `` metastable state , '' @xmath143 , such that a flip of any single spin ",
    "switching the state of any one neuron from spiking to silent , or vice versa  increases the energy or decreases the probability of the new state .",
    "this procedure also partitions the space of all @xmath11 possible patterns into domains , or basins of attraction , centered on the metastable states , and compresses the microscopic description of the retinal state to a number @xmath144 identifying the basin to which that state belongs .",
    "figure  [ f10 ] shows how the number of metastable states that we identify in the data grows with the size @xmath8 of the network . at",
    "very small @xmath8 , the only stable configuration is the all - silent state , but for @xmath145 the metastable states start to proliferate .",
    "indeed , we see no sign that the number of metastable states is saturating , and the growth is certainly faster than linear in the number of neurons .",
    "moreover , the total numbers of possible metastable states in the models energy landscapes could be substantially higher than shown , because we only count those states that are accessible by descending from patterns _ observed in the experiment_. it thus is possible that these real networks exceed the `` capacity '' of model networks @xcite .",
    "figure [ f11]a provides a more detailed view of the most prominent metastable states , and the `` energy valleys '' that surround them .",
    "the structure of the energy valleys can be thought of as clustering the patterns of neural activity , although in contrast to the usual formulation of clustering we do nt need to make an arbitrary choice of metric for similarity among patterns . nonetheless , we can measure the overlap @xmath146 between all pairs of patterns @xmath147 and @xmath148 that we see in the experiment , @xmath149 and we find that patterns which fall into the same valley are much more correlated with one another than they are with patterns that fall into other valleys ( fig [ f11]b ) .",
    "if we start at one of the metastable states and take a random `` uphill '' walk in the energy landscape ( appendix  [ app_descent ] ) , we eventually reach a transition state where there is a downhill path into other metastable states , and a selection of these trajectories is shown in fig [ f11]c .",
    "importantly , the transition states are at energies quite high relative to the metastable states ( fig [ f11]d ) , so the peaks of the probability distribution are well resolved from one another . in many cases",
    "it takes a large number of steps to find the transition state , so that the metastable states are substantially separated in hamming distance .",
    "individual neurons in the retina are known to generate rather reproducible responses to naturalistic stimuli @xcite , but even a small amount of noise in the response of single cells is enough to ensure that groups of @xmath80 neurons almost never generate the same response to two repetitions of the same visual stimulus .",
    "it is striking , then , that when we show the same movie again , the retina revisits the same basin of attraction with very high probability , as shown in fig [ f12 ] . the same metastable states and corresponding valleys are identifiable from different subsets of the full population , providing a measure of redundancy that we explore more fully below .",
    "further , the transitions into and out of these valleys are very rapid , with a time scale of just @xmath150 . in summary",
    ", the neural code for visual signals seems to respect the structure inferred from the energy landscape , despite the fact that the energy landscape is constructed without reference to the visual stimuli .",
    "central to our understanding of neural coding is the entropy of the responses @xcite .",
    "conceptually , the entropy measures the size of the neural vocabulary : with @xmath8 neurons there are @xmath11 _ possible _ configurations of spiking and silence , but since not all of these have equal probabilities  some , like the all - silent pattern , may occur orders of magnitude more frequently than others , such as the all - spikes pattern  the _ effective _ number of configurations is reduced to @xmath151 , where @xmath152 is the entropy of the vocabulary for the network of @xmath8 neurons . furthermore ,",
    "if the patterns of spiking and silence really are codewords for the stimulus , then the mutual information between the stimulus and response , @xmath153 , can be at most the entropy of the codewords , @xmath154 $ ] .",
    "thus , the entropy of the system s output bounds the information transmission .",
    "this is true even if the output words are correlated in time ; temporal correlations imply that the entropy of state sequences is smaller than expected from the entropy of single snapshots , as studied here , and hence the limits on information transmission are even more stringent @xcite .",
    "we can not sample the distribution  and thus estimate the entropy directly  for large sets of neurons , but we know that maximum entropy models with constraints @xmath155 put an upper bound to the true entropy , @xmath154 \\leq s[p^{(\\{f_\\mu\\})}(\\{\\sigma_{\\rm i}\\})]$ ] . unfortunately , even computing the entropy of our model distribution is not simple .",
    "naively , we could draw samples out of the model via monte carlo , and since simulations can run longer than experiments , we could hope to accumulate enough samples to make a direct estimate of the entropy , perhaps using more sophisticated methods for dealing with sample size dependences @xcite . but",
    "this is terribly inefficient ( see appendix  [ app : ent_comp ] ) .",
    "an alternative is to make more thorough use of the mathematical equivalence between maximum entropy models and statistical mechanics .",
    "the first approach to entropy estimation involves extending our maximum entropy models of eq ( [ ham ] ) by introducing a parameter analogous to the temperature @xmath156 in statistical physics : @xmath157 thus , for @xmath158 , the distribution in eq ( [ hamb ] ) is exactly equal to the maximum entropy model with parameters @xmath79 , but by varying @xmath156 and keeping the @xmath79 constant , we access a one - parameter family of distributions . unlike in statistical physics , @xmath156 here is purely a mathematical device , and we do not consider the distributions at @xmath159 as describing any real network of neurons .",
    "one can nevertheless compute , for each of these distributions at temperature @xmath156 , the heat capacity @xmath160 , and then thermodynamics teaches us that @xmath161 ; we could thus invert this relation to compute the entropy : @xmath162=s(t=1 ) = \\int_0 ^ 1 \\frac{c(t)}{t}\\,dt . \\label{sint}\\ ] ] the heat capacity might seem irrelevant since there is no `` heat '' in our problem , but this quantity is directly related to the variance of energy , @xmath163 , with @xmath164 as in fig  [ f8 ] .",
    "the energy , in turn , is related to the logarithm of the probability , and hence the heat capacity is the variance in how surprised we should be by any state drawn out of the distribution . in practice , we can draw sample states from a monte carlo simulation , compute the energy of each such state , and estimate the variance over a long simulation .",
    "importantly , it is well known that such estimates stabilize long before we have collected enough samples to visit every state of the system @xcite .",
    "thus , we start with the inferred maximum entropy model , generate a dense family of distributions at different @xmath156 spanning the values from 0 to 1 , and , from each distribution , generate enough samples to estimate the variance of energy and thus @xmath160 ; finally , we do the integral in eq ( [ sint ] ) .",
    "interestingly , the mapping to statistical physics gives us other , independent ways of estimating the entropy .",
    "the most likely state of the network , in all the cases we have explored , is complete silence .",
    "further , in the k - pairwise models , this probability is reproduced exactly , since it is just @xmath103 .",
    "mathematically , this probability is given by @xmath165,\\ ] ] where the energy of the silent state is easily computed from the model just by plugging in to the hamiltonian in eq ( [ hkpair ] ) ; in fact we could choose our units so that the silent state has precisely zero energy , making this even easier .",
    "but then we see that , in this model , estimating the probability of silence ( which we can do directly from the data ) is the same as estimating the partition function @xmath166 , which usually is very difficult since it involves summing over all possible states .",
    "once we have @xmath166 , we know from statistical mechanics that @xmath167 and we can estimate the average energy from a single monte carlo simulation of the model at the `` real '' @xmath158 ( c.f .",
    "fig  [ f8 ] ) .    finally , there are more sophisticated monte carlo resampling methods that generate an estimate of the `` density of states '' @xcite , related to the cumulative distributions @xmath117 and @xmath118 discussed above , and from this density we can compute the partition function directly .",
    "as explained in appendix [ app : ent_comp ] , the three different methods of entropy estimation agree to better than @xmath168 on groups of @xmath94 neurons .    , in black , and the entropy of the k - pairwise models per neuron , @xmath169 , in red , as a function of @xmath8 .",
    "dashed lines are fits from ( b ) . *",
    "( b ) * independent entropy scales linearly with @xmath8 ( black dashed line ) .",
    "multi - information @xmath170 of the k - pairwise models is shown in dark red .",
    "dashed red line is a best quadratic fit for dependence of @xmath171 on @xmath172 ; this can be rewritten as @xmath173 , where @xmath174 ( shown in inset ) is the effective scaling of multi - information with system size @xmath8 . in both panels , error bars are s.d . over 30 subnetworks at each size @xmath8 .",
    "[ f13 ] ]    figure  [ f13]a shows the entropy per neuron of the k - pairwise model as a function of network size , @xmath8 . for comparison , we also plot the independent entropy , i.e. the entropy of the non - interacting maximum entropy model that matches the mean firing rate of every neuron defined in eq ( [ ha ] ) .",
    "it is worth noting that despite the diversity of firing rates for individual neurons , and the broad distribution of correlations in pairs of neurons , the entropy per neuron varies hardly at all as we look at different groups of neurons chosen out of the larger group from which we can record .",
    "this suggests that collective , `` thermodynamic '' properties of the network may be robust to some details of the neural population , as discussed in the introduction . these entropy differences between the independent and correlated models may not seem large , but losing @xmath175 of entropy per neuron means that for @xmath176 neurons the vocabulary of neural responses is restricted @xmath177fold .",
    "the difference between the real entropy of the system and the independent entropy , also known as the _ multi  information _ , @xmath178-s[p^{(1,2,k)}(\\{\\sigma_{\\rm i}\\})],\\ ] ] measures the amount of statistical structure between @xmath8 neurons due to pairwise interactions and the k - spike constraint .",
    "as we see in fig  [ f13]b , the multi - information initially grows quadratically ( @xmath179 ) as a function of @xmath8 . while this growth is slowing as @xmath8 increases , it is still faster than linear ( @xmath180 ) , and correspondingly the entropy per neuron keeps decreasing , so that even with @xmath94 neurons we have not yet reached the extensive scaling regime where the entropy per neuron would be constant .",
    "these results are consistent with suggestions in ref @xcite based on much smaller groups of cells ; in particular the changeover towards extensive entropy growth could happen at a scale of @xmath181 , which corresponds to the total numbers of neuron within a `` correlated patch '' of this retina .",
    "usually we expect that , as the number of elements @xmath8 in a system becomes large , the entropy @xmath152 becomes proportional to @xmath8 and the distribution becomes nearly uniform over @xmath182 states .",
    "this is the concept of `` typicality '' in information theory @xcite and the `` equivalence of ensembles '' in statistical physics @xcite . at @xmath94 , we have @xmath183 bits , so that @xmath184 , and for the full @xmath185 neurons in our experiment the number of states is even larger . in a uniform distribution",
    ", if we pick two states at random then the probability that these states are the same is given by @xmath186 .",
    "on the hypothesis of uniformity , this probability is sufficiently small that large groups of neurons should _ never _ visit the same state twice during the course of a one hour experiment .",
    "in fact , if we choose two moments in time at random from the experiment , the probability that even the full 160neuron state that we observe will be the same is @xmath187 .",
    "dependence of @xmath188 to large @xmath8 .",
    "[ pc ] ]    we can make these considerations a bit more precise by exploring the dependence of coincidence probabilities on @xmath8 . we expect that the negative logarithm of the coincidence probability , like the entropy itself , will grow linearly with @xmath8 ; equivalently we should see an exponential decay of coincidence probability as we increase the size of the system .",
    "this is exactly true if the neurons are independent , even if different cells have different probabilities of spiking , provided that we average over possible choices of @xmath8 neurons out of the population .",
    "but the real networks are far from this prediction , as we can see in fig [ pc]a .",
    "larger and larger groups of neurons do seem to approach a `` thermodynamic limit '' in which @xmath189 ( fig [ pc]b ) , but the limiting ratio @xmath190 is an order of magnitude smaller than our estimates of the entropy per neuron ( fig  [ f13]b ) .",
    "thus , the correlations among neurons make the recurrence of combinatorial patterns thousands of times more likely than would be expected from independent neurons , and this effect is even larger than simply the reduction in entropy .",
    "this suggests that the true distribution over states is extremely inhomogeneous , not just because total silence is anomalously probable but because the dynamic range of probabilities for the different active states also is very large .",
    "importantly , as seen in fig [ pc ] , this effect is captured with very high precision by our maximum entropy model .      in the retina",
    "we usually think of neurons as responding to the visual stimulus , and so it is natural to summarize their response as spike rate vs. time in a ( repeated ) movie , the post ",
    "stimulus time histogram ( psth ) .",
    "we can do this for each of the cells in the population that we study ; one example is in the top row of fig [ f15]a .",
    "this example illustrates common features of neural responses to naturalistic sensory inputs  long epochs of near zero spike probability , interrupted by brief transients containing a small number of spikes @xcite .",
    "can our models predict this behavior , despite the fact that they make no explicit reference to the visual input ?",
    "cell group , as described in the text .",
    "solid lines are the mean prediction across all trials , and thin lines are the envelope @xmath191 one standard deviation . *",
    "( b ) * cross  correlation ( cc ) between predicted and observed spike rates vs. time , for each neuron in the @xmath94 group .",
    "green points are averages of cc computed from every trial , whereas blue points are the cc computed from average predictions . * ( c ) * dependence of cc on the population size @xmath8 .",
    "thin blue lines follow single neurons as predictions are based on increasing population sizes ; red line is the cell illustrated in ( a ) , and the line with error bars shows mean @xmath191 s.d . across all cells .",
    "green line shows the equivalent mean behavior computed for the green points in ( b ) .",
    "[ f15 ] ]    the maximum entropy models that we have constructed predict the distribution of states taken on by the network as a whole , @xmath23 . from this",
    "we can construct the conditional distribution , @xmath192 , which tells us the probability of spiking in one cell given the current state of all the other cells , and hence we have a prediction for the spike probability in one neuron at each moment in time .",
    "further , we can repeat this construction using not all the neurons in the network , but only a group of @xmath8 , with variable @xmath8 .    as the stimulus movie proceeds ,",
    "all of the cells in the network are spiking , dynamically , so that the state of the system varies . through the conditional distribution @xmath192 ,",
    "this varying state predicts a varying spike probability for the one cell in the network on which we are focusing , and we can plot this predicted probability vs. time in the same way that we would plot a conventional psth . on each repeat of the movie ,",
    "the states of the network are slightly different , and hence the predicted psth is slightly different . what we see in fig [ f15]a is that , as we use more and more neurons in the network to make the prediction , the psth based on collective effects alone , trial by trial , starts to look more and more like the real psth obtained by averaging over trials .",
    "in particular , the predicted psth has near zero spike probability over most of the time , the short epochs of spiking are at the correct moments , and these epochs have the sharp onsets observed experimentally .",
    "these are features of the data which are very difficult to reproduce in models that , for example , start by linearly filtering the visual stimulus through a receptive field @xcite .",
    "in contrast , the predictions in fig  [ f15 ] make no reference to the visual stimulus , only to the outputs of other neurons in the network .",
    "we can evaluate the predictions of spike probability vs. time by computing the correlation coefficient between our predicted psth and the experimental psth , as has been done in many other contexts @xcite .",
    "since we generate a prediction for the psth on every presentation of the movie , we can compute the correlation from these raw predictions , and then average , or average the predictions and then compute the correlation ; results are shown in figs [ f15]b and c. we see that correlation coefficients can reach @xmath193 , on average , or even higher for particular cells .",
    "predictions seem of more variable quality for cells with lower average spike rate , but this is a small effect .",
    "the quality of average predictions , as well as the quality of single trial predictions , still seem to grow gradually as we include more neurons even at @xmath194 , so it may be that we have not seen the best possible performance yet .    our ability to predict the state of individual neurons by reference to the network , but not the visual input , means that the representation of the sensory input in this population is substantially redundant .",
    "stated more positively , the full information carried by this population of neurons  indeed , the full information available to the brain about this small patch of the visual world  is accessible to downstream cells and areas that receive inputs from only a fraction of the neurons .",
    "it is widely agreed that neural activity in the brain is more than the sum of its parts  coherent percepts , thoughts , and actions require the coordinated activity of many neurons in a network , not the independent activity of many individual neurons .",
    "it is not so clear , however , how to build bridges between this intuition about collective behavior and the activity of individual neurons .",
    "one set of ideas is that the activity of the network as a whole may be confined to some very low dimensional trajectory , such as a global , coherent oscillation .",
    "such oscillatory activity is observable in the summed electrical activity of large numbers of neurons  the eeg  and should be reflected as oscillations in the ( auto)correlation functions of spike trains from individual neurons . on a more refined level",
    ", dimensionality reduction techniques like pca allow the activity patterns of a neural network to be viewed on a low - dimensional manifold , facilitating visualization and intuition @xcite .",
    "a very different idea is provided by the hopfield model , in which collective behavior is expressed in the stabilization of many discrete patterns of activity , combinations of spiking and silence across the entire network @xcite . taken together , these many patterns can span a large fraction of the full space of possibilities , so that there need be no dramatic dimensionality reduction in the usual sense of this term .",
    "the claim that a network of neurons exhibits collective behavior is really the claim that the distribution of states taken on by the network has some nontrivial structure that can not be factorized into contributions from individual cells or perhaps even smaller subnetworks .",
    "our goal in this work has been to build a model of this distribution , and to explore the structure of that model .",
    "we emphasize that building a model is , in this view , the first step rather than the last step . but building a model is challenging , because the space of states is very large and data are limited .",
    "an essential step in searching for collective behavior has been to develop experimental techniques that allow us to record not just from a large number of neurons , but from a large fraction of the neurons in a densely interconnected region of the retina @xcite . in large networks , even measuring the correlations among pairs of neurons can become problematic : individual elements of the correlation matrix might be well determined from small data sets , but much larger data sets are required to be confident that the matrix as a whole is well determined . thus , long",
    ", stable recordings are even more crucial than usual .",
    "to use the maximum entropy approach , we have to be sure that we can actually find the models that reproduce the observed expectation values ( fig  [ f2 ] ,  [ f3 ] ) and that we have not , in the process , fit to spurious correlations that arise from the finite size of our data set ( fig  [ f4 ] ) .",
    "once these tests are passed , we can start to assess the accuracy of the model as a description of the network as a whole . in particular",
    ", we found that the pairwise model began to break down at a network size @xmath140 ( fig  [ f5 ] ) .",
    "however , by adding the constraint that reproduces the probability of @xmath7 out of @xmath8 neurons spiking synchronously ( fig  [ f6 ] ) , which is a statistic that is well - sampled and does not greatly increase the model s complexity , we could again recover good performance ( figs  [ f7]-[f9 ] ) .",
    "perhaps the most useful global test of our models is to ask about the distribution of state probabilities : how often should we see combinations of spiking and silence that occur with probability @xmath21 ?",
    "this has the same flavor as asking for the probability of every state , but does not suffer from the curse of dimensionality .",
    "since maximum entropy models are mathematically identical to the boltzmann distribution in statistical mechanics , this question about the frequency of states with probability @xmath21 is the same as asking how many states have a given energy @xmath123 ; we can avoid binning along the @xmath123 axis by asking for the number of models with energies smaller ( higher probability ) or larger ( lower probability ) than @xmath123 .",
    "figure [ f8 ] shows that these cumulative distributions computed from the model agree with experiment far into the tail of low probability states .",
    "these states are so rare that , individually , they almost never occur , but there are so many of these rare states that , in aggregate , they make a measurable contribution to the distribution of energies . indeed , most of the states that we see in the data are rare in this sense , and their statistical weight is correctly predicted by the model .",
    "the maximum entropy models that we construct from the data do not appear to simplify along any conventional axes .",
    "the matrix of correlations among spikes in different cells ( fig [ f1]a ) is of full rank , so that principal component analysis does not yield significant dimensionality reduction .",
    "the matrix of `` interactions '' in the model ( fig [ f1]d ) is neither very sparse nor of low rank , perhaps because we are considering a group of neurons all located ( approximately ) within the radius of the typical dendritic arbor , so that all cells have a chance to interact with one another . most importantly ,",
    "the interactions that we find are not weak ( fig [ f1]f ) , and together with being widespread this means that their impact is strong .",
    "technically , we can not capture the impact within low orders of perturbation theory ( appendix [ app : perturb ] ) , but qualitatively this means that the behavior of the network as a whole is not in any sense `` close '' to the behavior of non  interacting neurons .",
    "thus , if our models work , it is not simply because correlations are weak , as had been suggested @xcite .",
    "having convinced ourselves that we can build models which give an accurate description of the probability distribution over the states of spiking and silence in the network , we can ask what these models teach us about function . as emphasized in ref @xcite ,",
    "one corollary of collective behavior is the possibility of error correction or pattern completion  we can predict the spiking or silence of one neuron by knowing the activity of all the other neurons . with a population of @xmath80 cells ,",
    "the quality of these predictions becomes quite high ( fig [ f15 ] ) .",
    "the natural way of testing these predictions is to look at the probability of spiking vs. time in the stimulus movie .",
    "although we make no reference to the stimulus , we reproduce the sharp peaks of activity and extended silences that are so characteristic of the response to naturalistic inputs , and so difficult to capture in conventional models where each individual neuron responds to the visual stimulus as seen through its receptive field @xcite",
    ".    one of the dominant concepts in thinking about the retina has been the idea that the structure of receptive fields serves to reduce the redundancy of natural images and enhance the efficiency of information transmission to the brain @xcite ( but see @xcite ) . while one could argue that the observed redundancy among neurons is less than expected from the structure of natural images or movies , none of what we have described here would happen if the retina truly `` decorrelated '' its inputs .",
    "far from being almost independent , the combinations of spiking and silence in different cells cluster into basins of attraction defined by the local minima of energy in our models , and the spiking of each neuron is highly predictable from the activity of other neurons in the network , even without reference to the visual stimulus ( fig [ f15 ] ) .    with @xmath94 neurons ,",
    "our best estimate of the entropy corresponds to significant occupancy of roughly one million distinct combinations of spiking and silence .",
    "each state could occur with a different probability , and ( aside from normalization ) there are no constraints  each of these probabilities could be seen as a separate parameter describing the network activity .",
    "it is appealing to think that there must be some simplification , that we wo nt need a million parameters , but it is not obvious that any particular simplification strategy will work .",
    "indeed , there has been the explicit claim that the approach we have taken here will not work for large networks @xcite .",
    "thus , it may seem surprising that we can write down a relatively simple model , with all parameters determined from experiment , and have this model predict so much of the structure in the distribution of states . surprising or not , it certainly is important that , as the community contemplates monitoring the activity of ever larger number of neurons @xcite , we can identify theoretical approaches that have the potential to tame the complexity of these large systems .",
    "some cautionary remarks about the interpretation of our models seem in order .",
    "using the maximum entropy method does not mean there is some hidden force maximizing the entropy of neural activity , or that we are describing neural activity as being in something like thermal equilibrium ; all we are doing is building maximally agnostic models of the probability distribution over states . even in the context of statistical mechanics ,",
    "there are infinitely many models for the dynamics of the system that will be consistent with the equilibrium distribution , so we should not take the success of our models to mean that the dynamics of the network corresponds to something like monte carlo dynamics on the energy landscape .",
    "it is tempting to look at the couplings @xmath77 between different neurons as reflecting genuine , mechanistic interactions , but even in the context of statistical physics we know that this interpretation need not be so precise : we can achieve a very accurate description of the collective behavior in large systems even if we do not capture every microscopic detail , and the interactions that we do describe in the most successful of models often are effective interactions mediated by degrees of freedom that we need not treat explicitly .",
    "finally , the fact that a maximum entropy model which matches a particular set of experimental observations is successful does not mean that this choice of observables ( e.g. , pairwise correlations ) is unique or minimal .",
    "for all these reasons , we do not think about our models in terms of their parameters , but rather as a description of the probability distribution @xmath23 itself , which encodes the collective behavior of the system .",
    "the striking feature of the distribution over states is its extreme inhomogeneity .",
    "the entropy of the distribution is not that much smaller than it would be if the neurons made independent decisions to spike or be silent , but the shape of the distribution is very different ; the network builds considerable structure into the space of states , without sacrificing much capacity .",
    "the probability of the same state repeating is many orders of magnitude larger than expected for independent neurons , and this really is quite startling ( fig [ pc ] ) .",
    "if we extrapolate to the full population of @xmath195 neurons in this correlated , interconnected patch of the retina , the probability that two randomly chosen states of the system are the same is roughly one percent .",
    "thus , some combination of spiking and silence across this huge population should repeat exactly every few seconds .",
    "this is true despite the fact that we are looking at the entire visual representation of a small patch of the world , and the visual stimuli are fully naturalistic .",
    "although complete silence repeats more frequently , a wide range of other states also recur , so that many different combinations of spikes and silence occur often enough that we ( or the brain ) can simply count them to estimate their probability . this would be absolutely impossible in a population of nearly independent neurons , and it has been suggested that these repeated patterns provide an anchor for learning @xcite .",
    "it is also possible that the detailed structure of the distribution , including its inhomogeneity , is matched to the statistical structure of visual inputs in a way that goes beyond the idea of redundancy reduction , occupying a regime in which strongly correlated activity is an optimal code @xcite .    building a precise model of activity patterns required us to match the statistics of global activity ( the probability that @xmath7 out of @xmath8 neurons spike in the same small window of time ) .",
    "elsewhere we have explored a very simple model in which we ignore the identity of the neurons and match only this global behavior @xcite .",
    "this model already has a lot of structure , including the extreme inhomogeneity that we have emphasized here . in the simpler model we can exploit the equivalence between maximum entropy models and statistical mechanics to argue that this inhomogeneity is equivalent to the statement that the population of neurons is poised near a critical surface in its parameter space , and we have seen hints of this from analyses of smaller populations as well @xcite",
    "the idea that biological networks might organize themselves to critical points has a long history , and several different notions of criticality have been suggested @xcite .",
    "a sharp question , then , is whether the full probability distributions that we have described here correspond to a critical system in the sense of statistical physics , and whether we can find more direct evidence for criticality in the data , perhaps without the models as intermediaries .",
    "finally , we note that the our approach to building models for the activity of the retinal ganglion cell population is entirely unsupervised : we are making use only of structure in the spike trains themselves , with no reference to the visual stimulus . in this sense , the structures that we discover here are structures that could be discovered by the brain , which has no access to the visual stimulus beyond that provided by these neurons . while there are more structures that we could use  notably , the correlations across time  we find it remarkable that so much is learnable from just an afternoon s worth of data . as it becomes more routine to record the activity of such ( nearly ) complete sensory representations , it will be interesting to take the organism s point of view @xcite more fully , and try to extract meaning from the spike trains in an unsupervised fashion .",
    "we thank a cavagna , i giardina , t mora , se palmer , gj stephens , and a walczak for many helpful discussions .",
    "this work was supported in part by nsf grants iis0613435 , phy0957573 , and ccf0939370 , and by nih grants r01 ey14196 and p50 gm071508 .",
    "additional support was provided by the fannie and john hertz foundation , the swartz foundation , and the wm keck foundation .",
    "* electrophysiology . * we analyzed the recordings from the tiger salamander ( _ ambystoma tigrinum _ ) retinal ganglion cells responding to naturalistic movie clips , as in the experiments of ref .",
    "@xcite . in brief",
    ", animals were euthanized according to institutional animal care standards .",
    "the retina was isolated from the eye under dim illumination and transferred as quickly as possible into oxygenated ringer s medium , in order to optimize the long - term stability of recordings .",
    "tissue was flatted and attached to a dialysis membrane using polylysine .",
    "the retina was then lowered with the ganglion cell side against a multi - electrode array .",
    "arrays were first fabricated in university cleanroom facilities @xcite .",
    "subsequently , production was contracted out to a commercial mems foundry for higher volume production ( innovative micro technologies , santa barbara , ca ) .",
    "raw voltage traces were digitized and stored for off - line analysis using a 252-channel preamplifier ( multichannel systems , germany ) .",
    "the recordings were sorted using custom spike sorting software developed specifically for the new dense array @xcite .",
    "234 neurons passed the standard tests for the waveform stability and the lack of refractory period violations .",
    "of those , 160 cells whose firing rates were most stable across stimulus repeats were selected for further analysis . within this group , the mean fraction of interspike intervals ( isi ) shorter than @xmath196 ( i.e. , possible refractory violations ) was @xmath197",
    ".    * stimulus display . * the stimulus consisted of a short ( @xmath198 ) grayscale movie clip of swimming fish and water plants in a fish tank , which was repeated 297 times .",
    "the stimulus was presented using standard optics , at a rate of 30 frames per second , and gamma corrected for the display",
    ".    * data preparation .",
    "* we randomly selected 30 subgroups of @xmath199 cells for analysis from the total of 160 sorted cells . in sum , we analyzed @xmath200 groups of neurons , which we denote by @xmath201 , where @xmath8 denotes the subgroup size , and @xmath202 indexes the chosen subgroup of that size",
    ". time was discretized into @xmath203 time bins , as in our previous work @xcite .",
    "the state of the retina was represented by @xmath204 if the neuron i spiked at least once ( was silent ) in a given time bin @xmath205 .",
    "this binary description is incomplete only in @xmath206 of the time bins that contain more than one spike ; we treat these bins as @xmath63 . across the entire experiment ,",
    "the mean probability of non - silence ( that is , @xmath63 ) is @xmath207 .",
    "time discretization resulted in 953 time bins per stimulus repeat ; @xmath208 presented repeats yielded a total of @xmath209 @xmath8-bit binary samples during the course of the experiment for each subgroup .",
    "we used a modified version of our previously published learning procedure to compute the maximum entropy models given measured constraints @xcite ; the proof of convergence for the core of this l1-regularized maximum entropy algorithm is given in ref .",
    "our new algorithm can use as constraints arbitrary functions , not only single and pairwise marginals as before .",
    "parameters of the hamiltonian are learned sequentially in an order which greedily optimizes a bound on the log likelihood , and we use a variant of histogram monte carlo to estimate the values of constrained statistics during learning steps @xcite .",
    "monte carlo induces sampling errors on our estimates of these statistics , which provide an implicit regularization for the parameters of the hamiltonian @xcite .",
    "we verified the correctness of the algorithm explicitly for groups of 10 and 20 neurons where exact numerical solutions are feasible .",
    "we also verified that our mc sampling had a long enough `` burn - in '' time to equilibrate , even for groups of maximal size ( @xmath94 ) , by starting the sampling repeatedly from same vs random different initial conditions ( 100 runs each ) and comparing the constrained statistics , as well as the average and variance of the energy and magnetization , across these runs ; all statistics were not significantly dependent on the initial state ( two  sample kolmogorov - smirnov test at significance level 0.05 ) .",
    ", in pairwise models of eq  ( [ hpair ] ) , for different network sizes ( @xmath8 )",
    ". the distribution is pooled over 30 networks at each @xmath8 . *",
    "( b ) * the mean ( solid ) and s.d .",
    "( dashed ) of the distributions in ( a ) as a function of network size ( black ) ; the mean and s.d . of the corresponding distributions for k - pairwise models as a function of network size ( red ) . ]",
    "figure [ sf1 ] provides a summary of the models we have learned for populations of different sizes . in small networks",
    "there is a systematic bias to the distribution of @xmath77 parameters , but as we look to larger networks this vanishes and the distribution of @xmath77 becomes symmetric .",
    "importantly , the distribution remains quite broad , with the standard deviation of @xmath77 across all pairs declining only slightly .",
    "in particular , the typical coupling does not decline as @xmath210 , as would be expected in conventional spin glass models @xcite .",
    "this implies , as emphasized previously @xcite , that the `` thermodynamic limit '' ( very large @xmath8 ) for these systems will be different from what we might expect based on traditional physics examples .",
    "we withheld a random selection of 20 stimulus repeats ( test set ) for model validation , while training the model on the remaining 277 repeats . on training data , we computed the constrained statistics ( mean firing rates , covariances , and the k - spike distribution ) , and used bootstrapping to estimate the error bars on each of these quantities ; the constraints were the only input to the learning algorithm .",
    "figure  [ f1 ] shows an example reconstruction for a pairwise model for @xmath80 neurons ; the precision of the learning algorithm is shown in fig .",
    "[ f2 ] .",
    "the dataset consists of a total of @xmath211 binary pattern samples , but due to the structure of the stimulus , the number of statistically independent samples must be smaller : while the repeats are statistically independent , the samples within each repeat are not , because they are driven by the stimulus .",
    "the variance for a binary variable given its mean , @xmath32 , is @xmath212 ; with @xmath213 independent repeats , the error on the estimate in the average rate should decrease as @xmath214 . by repeatedly estimating the statistical errors with different subsets of repeats and comparing the expected scaling of the error in the original data set and in the data set where we shuffle time bins randomly , thereby destroying the repeat structure , we can estimate the effective number of independent samples ; we find this to be @xmath215 , about @xmath216 of the total number of samples , @xmath156 .",
    "we note that our largest models have @xmath217 constrained statistics that are estimated from at least @xmath218 as many statistically independent samples",
    ". moreover , the vast majority of these statistics are pairwise correlation coefficients that can be estimated extremely tightly from the data , often with relative errors below @xmath168 , so we do not expect overfitting on general grounds .",
    "nevertheless , we explicitly checked that there is no overfitting by comparing the log likelihood of the data under the learned maximum entropy model , for each of the 360 subgroups @xmath201 , on the training and testing set , as shown in fig .",
    "to find the metastable ( ms ) states , we start with a pattern @xmath9 that appears in the data , and attempt to flip spins @xmath219 from their current state into @xmath220 , in order of increasing @xmath33 . a flip is retained if the energy of the new configuration is smaller than before the flip .",
    "when none of the spins can be flipped , the resulting pattern is recorded as the ms state .",
    "the set of ms states found can depend on the manner in which descent is performed , in particular when some of the states visited during descent are on the `` ridges '' between multiple basins of attraction . note that whether a pattern is a ms state or not is independent of the descent method ;",
    "what depends on the method is which ms states are found by starting from the data patterns . to explore the structure of the energy landscape in fig  [ f11 ]",
    ", we started 1000 metropolis mc simulations repeatedly in each of the 10 most common ms states of the model ; after each attempted spin - flip , we checked whether the resulting state is still in the basin of attraction of the starting ms state ( by invoking the descent method above ) , or whether it has crossed the energy barrier into another basin .",
    "we histogrammed the transition probabilities into other ms basins of attraction and , for particular transitions , we tracked the transition paths to extract the number of spin - flip attempts and the energy barriers .",
    "`` basin size '' of a given ms state is the number of patterns _ in the recorded data _ from which the given ms state is reached by descending on the energy landscape .",
    "the results presented in fig  [ f11 ] are typical of the transitions we observe across multiple subnetworks of 120 neurons .",
    ") versus entropy estimation using the wang - landau sampling method ( y - axis ) @xcite .",
    "each plot symbol is one subnetwork of either @xmath80 or @xmath94 neurons ( circles = pairwise models , crosses = k - pairwise models ) .",
    "the two sampling methods yield results that agree to within @xmath221 . *",
    "( b ) * fractional difference between the heat capacity method and the entropy determined from the all - silent pattern .",
    "the histogram is over 30 networks at @xmath80 and 30 at @xmath94 , for the k - pairwise model . *",
    "( c ) * fractional difference between the wang - landau sampling method and the entropy determined from the all - silent pattern .",
    "same convention as in ( b ) . ]",
    "entropy estimation is a challenging problem .",
    "as explained in the text , the usual approach of counting samples and identifying frequencies with probabilities will fail catastrophically in all the cases of interest here , even if we are free to draw samples from out model rather than from real data . within the framework of maximum entropy models",
    ", however , the equivalence to statistical mechanics gives us several tools .",
    "here we summarize the evidence that these multiple tools lead to consistent answers , so that we can be confident in our estimates .",
    "our first try at entropy estimation is based on the heat capacity integration in eq .",
    "( [ sint ] ) .",
    "to begin , with @xmath222 neurons , we _ can _ enumerate all @xmath11 states of the network and hence we can find the maximum entropy distributions exactly ( with no monte carlo sampling ) . from these distributions",
    "we can also compute the entropy exactly , and it agrees with the results of the heat capacity integration .",
    "indeed , there is good agreement for the entire distribution , with jensen - shannon divergence between exact maximum entropy solutions and solutions using our reconstruction procedure at @xmath223 . as a second check , now useable for all @xmath8",
    ", we note that the entropy is zero at @xmath224 , but @xmath225 at @xmath226 .",
    "thus we can do the heat capacity integration from @xmath158 to @xmath226 instead of @xmath224 to @xmath158 , and we get essentially the same result for the entropy ( mean relative difference of @xmath227 across 30 networks at @xmath80 and @xmath94 ) .    leaning further on the mapping to statistical physics , we realize that the heat capacity is a summary statistic for the density of states .",
    "there are monte carlo sampling methods , due to wang and landau @xcite ( wl ) , that aim specifically at estimating this density , and those allow us to compute the entropy from a single simulation run .",
    "the results , in fig  [ sf3]a , are in excellent agreement with the heat capacity integration .",
    "k - pairwise models have the attractive feature that , by construction , they match exactly the probability of the all - silent pattern , @xmath228 , seen in the data .",
    "as explained in the main text , this means that we can `` measure '' the partition function , @xmath166 , of our model directly from the probability of silence .",
    "then we can compute the average energy @xmath229 from a single mc sampling run , and find the entropy for each network . as shown in figs  [ sf3]b and c ,",
    "the results agree both with the heat capacity integration and with the wang  landau method , to an accuracy of better than @xmath168 .    finally , there are methods that allow us to estimate entropy by counting samples even in cases where the number of samples is much smaller than the number of states @xcite ( nsb ) .",
    "the nsb method is not guaranteed to work in all cases , but the comparison with the entropy estimates from heat capacity integration ( fig [ sf2]a ) suggests that so long as @xmath230 , nsb estimates are reliable ( see also @xcite ) .",
    "figure  [ sf2]b shows that the nsb estimate of the entropy does not depend on the sample size for @xmath230 ; if we draw from our models a number of samples equal to the number found in the data , and then ten times more , we see that the estimated entropy changes by just a few percent , within the error bars .",
    "this is another signature of the accuracy of the nsb estimator for @xmath230 .",
    "as @xmath8 increases , these direct estimates of entropy become significantly dependent on the sample size , and start to disagree with the heat capacity integration .",
    "the magnitude of these systematic errors depends on the structure of the underlying distribution , and it is thus interesting that nsb estimates of the entropy from our model and from the real data agree with one another up to @xmath94 , as shown in fig [ sf2]c .",
    "samples from the model ( same size as the experimental data set ) on y - axis ; the true entropy ( using heat capacity integration ) method on x - axis .",
    "each dot represents one subnetwork of a particular size ( @xmath8 , different colors ) . for small networks ( @xmath231 )",
    "the bias is negligible , but estimation from samples significantly underestimates the entropy for larger networks . *",
    "( b ) * the fractional bias of the estimator as a function of @xmath8 ( black dots = data from ( a ) , gray dots = using 10 fold more samples ) .",
    "red line shows the mean @xmath191 s.d . over 30 subnetworks at each size . *",
    "( c ) * the nsb estimation of entropy from samples drawn from the model ( x - axis ) vs the samples from real experiment ( y - axis ) ; each dot is a subnetwork of a given size ( color as in ( a ) ) .",
    "the data entropy estimate is slightly smaller than that of the model , as is expected for true entropy ; for estimates from finite data this would only be expected if the biases on data vs mc samples were the same . ]",
    "the pairwise correlations between neurons in this system are quite weak . thus , if we make a model for the activity of just two neurons , treating them as independent is a very good approximation .",
    "it might seem that this statement is invariant to the number of neurons that we consider  either correlations are weak , or they are strong . but",
    "this misses the fact that weak but widespread correlations can have a non  perturbative effect on the structure of the probability distribution .",
    "nonetheless , it has been suggested that maximum entropy methods are successful only because correlations are weak , and hence that we ca nt really capture non  trivial collective behaviors with this approach @xcite .     for a group of @xmath232 neurons , computed using the exact maximum entropy reconstruction algorithm , with the lowest order perturbation theory result , @xmath233 , where @xmath234 and @xmath235 @xcite . in the case of larger networks ,",
    "the perturbative @xmath77 deviate more and more from equality ( black line ) .",
    "inset : the average absolute difference between the true and perturbative coupling , normalized by the average true coupling . * ( b ) * the exact pairwise model , eq  ( [ hpair ] ) , can be compared to the distribution @xmath236 , sampled from data ; the olive line shows the jensen - shannon divergence ( corrected for finite sample size ) between the two distributions , for four example networks of size @xmath232 .",
    "the blue line shows the same comparison in which the pairwise model parameters , @xmath237 , were calculated perturbatively .",
    "the black line shows the @xmath238 between two halves of the data for the four selected networks .",
    "[ sf4 ] ]    while independent models fail to explain the behavior of even small groups of neurons @xcite , it is possible that groups of neurons might be in a weak perturbative regime , where the contribution of pairwise interactions could be treated as a small perturbation to the independent hamiltonian , if the expansion was carried out in the correct representation @xcite .",
    "of course , with finite @xmath8 , all quantities must be analytic functions of the coupling constants , and so we expect that , if carried to sufficiently high order , any perturbative scheme will converge  although this convergence may become much slower at larger @xmath8 , signaling genuinely collective behavior in large networks .    to make the question of whether correlations are weak or strong precise",
    ", we ask whether we can approximate the maximum entropy distribution with the leading orders of perturbation theory .",
    "there are a number of reasons to think that this wo nt work @xcite , but in light of the suggestion from ref @xcite we wanted to explore this explicitly .",
    "if correlations are weak , there is a simple relationship between the correlations @xmath86 and the corresponding interactions @xmath77 @xcite .",
    "we see in fig [ sf4]a that this relationship is violated , and the consequence is that models built by assuming this perturbative relationship are easily distinguishable from the data even at @xmath239 ( fig [ sf4]b ) .",
    "we conclude that treating correlations as a small perturbation is inconsistent with the data .",
    "indeed , if we try to compute the entropy itself , it can be shown that even going out to fourth order in perturbation theory is not enough once @xmath240 @xcite .",
    "jj hopfield ( 1982 ) neural networks and physical systems with emergent collective computational abilities .",
    "_ proc natl acad sci ( usa ) _ * 79 : * 25548 .",
    "dj amit ( 1989 ) _ modeling brain function : the world of attractor neural networks _ ( cambridge university press , cambridge ) .",
    "j hertz , a krogh & rg palmer ( 1991 ) _ introduction to the theory of neural computation _ ( addison wesley , redwood city ) .",
    "e schneidman , mj berry ii , r segev & w bialek ( 2006 ) weak pairwise correlations imply strongly correlated network states in a neural population .",
    "_ nature _ * 440 : * 1007 - 1012 .",
    "g tkaik , e schneidman , mj berry ii & w bialek ( 2006 ) ising models for networks of real neurons . _ arxiv.org : _ q - bio/0611072 .",
    "s yu , d huang , w singer & d nikolic ( 2008 ) a small world of neuronal synchrony .",
    "_ cereb cortex _ * 18 : * 28912901 .",
    "a tang , d jackson , j hobbs , w chen , jl smith , h patel , a prieto , d petruscam mi grivich , a sher , p hottowy , w dabrowski , am litke & jm beggs ( 2008 ) a maximum entropy model applied to spatial and temporal correlations from cortical networks in vitro .",
    "_ j neurosci _ * 28 : * 505518 .",
    "g tkaik , e schneidman , mj berry ii & w bialek ( 2009 ) spin  glass models for a network of real neurons . _",
    "arxiv.org : _ 0912.5409 .",
    "j shlens , gd field , jl gaulthier , mi grivich , d petrusca , a sher , am litke & ej chichilnisky ( 2006 ) the structure of multi - neuron firing patterns in primate retina . _ j neurosci _ * 26 : * 82548266 .",
    "j shlens , gd field , jl gaulthier , m greschner , a sher , am litke & ej chichilnisky ( 2009 ) the structure of large  scale synchronized firing in primate retina .",
    "_ j neurosci _ * 29 : * 50225031 .",
    "ie ohiorhenuan , f mechler , kp purpura , am schmid , q hu & jd victor ( 2010 ) sparse coding and higher  order correlations in fine ",
    "scale cortical networks .",
    "_ nature _ * 466 : * 617621 .",
    "e ganmor , r segev & e schniedman ( 2011 ) sparse low  order interaction network underlies a highly correlated and learnable neural population code .",
    "_ proc natl acad sci ( usa ) _ * 108 : * 96799684 .",
    "jc vasquez , o marre , ag palacios , mj berry ii & b cessac ( 2012 ) gibbs distribution analysis of temporal correlations structure in retina ganglion cells .",
    "_ j physiol paris _ * 34 : * 120127 .",
    "e granot - atedgi , g tkaik , r segev & e schneidman ( 2013 ) stimulus - dependent maximum entropy models of neural population codes .",
    "_ plos comput biol _ * 9 : * e1002922 .",
    "g tkaik , js prentice , jd victor & v balasubramanian ( 2010 ) local statistics in natural scenes predict the saliency of synthetic textures .",
    "_ proc natl acad sci ( usa ) _ * 107 : * 1814918154 .",
    "gj stephens , t mora , g tkaik & w bialek ( 2013 ) statistical thermodynamics of natural images . _",
    "phys rev lett _ * 110 : * 018701 .",
    "s saremi & tj sejnowski ( 2013 ) hierarchical model of natural images and the origin of scale invariance .",
    "_ proc natl acad sci ( usa ) _ * 110 : * 30713076 .",
    "tr lezon , jr banavar , m cieplak , a maritan & nv federoff ( 2006 ) using the principle of entropy maximization to infer genetic interaction networks from gene expression patterns .",
    "_ proc natl acad sci ( usa ) _ * 103 : * 1903319038 .",
    "g tkaik ( 2007 ) _ information flow in biological networks _",
    "( dissertation , princeton university ) .",
    "w bialek & r ranganathan ( 2007 ) rediscovering the power of pairwise interactions . _",
    "arxiv.org : _ 0712.4397 .",
    "f seno , a trovato , jr banavar & a maritan ( 2008 ) maximum entropy approach for deducing amino acid interactions in proteins .",
    "_ phys rev lett _ * 100 : * 078102 .",
    "m weigt , ra white , h szurmant , ja hoch & t hwa ( 2009 ) identification of direct residue contacts in protein  protein interaction by message passing .",
    "_ proc natl acad sci ( usa ) _ * 106 : * 6772 .",
    "n halabi , o rivoire , s leibler & r ranganathan ( 2009 ) protein sectors : evolutionary units of three  dimensional structure .",
    "_ cell _ * 138 : * 774786 .",
    "t mora , am walczak , w bialek & cg callan jr ( 2010 ) maximum entropy models for antibody diversity .",
    "_ proc natl acad sci ( usa ) _ * 107 : * 54055410 .",
    "ds marks , lj colwell , r sheridan , ta hopf , a pagnani , r zecchina & c sander ( 2011 ) protein 3d structure computed from evolutionary sequence variation .",
    "_ plos one _ * 6 : * e28766 .",
    "ji sulkowska , f morocos , m weigt , t hwa & jn onuchic ( 2012 ) genomics  aided structure prediction .",
    "_ proc natl acad sci ( usa ) _ * 109 : * 1034010345 .",
    "gj stephens & w bialek ( 2010 ) statistical mechanics of letters in words .",
    "_ phys rev e _ * 81 : * 066119 .",
    "w bialek , a cavagna , i giardina , t mora , e silvestri , m viale & a walczak ( 2012 ) statistical mechanics for natural flocks of birds .",
    "_ proc natl acad sci ( usa ) _ * 109 : * 47864791 .",
    "t mora & w bialek ( 2010 ) are biological systems poised at criticality ? _ j stat phys _ * 144 : * 268302 .",
    "s nirenberg & j victor ( 2007 ) analyzing the activity of large populations of neurons : how tractable is the problem._curr opin neurobiol _ * 17 : * 397400 .",
    "y roudi , e aurell & ja hertz ( 2009 ) statistical physics of pairwise probability models .",
    "_ front comput neurosci _ * 3 : * 22 .",
    "y roudi , s nirenberg & pe latham ( 2009 ) pairwise maximum entropy models for studying large biological systems : when they can work and when they ca nt .",
    "_ plos comput biol _ * 5 : * e1000380 .",
    "y roudi , j trycha & j hertz ( 2009 ) the ising model for neural data : model quality and approximate methods for extracting functional connectivity .",
    "_ phys rev e _ * 79 : * 051915 .",
    "o marre , d amodei , k sadeghi , f soo , te holy , & mj berry ii ( 2012 ) recording from a complete population in the retina .",
    "_ j neurosci _ * 32 : * 1485914873 .",
    "t broderick , m dudik , g tkaik , re schapire & w bialek ( 2007 ) faster solutions of the inverse pairwise ising problem .",
    "_ arxiv.org : _ 0712.2437 .",
    "a treves , s panzeri , et rolls , m booth & ea wakeman ( 1999 ) firing rate distributions and efficiency of information transmission in inferior temporal cortex neurons to natural visual stimuli . _",
    "neural comput _ * 11 : * 601631 .",
    "g tkaik , o marre , t mora , d amodei , mj berry ii & w bialek ( 2013 ) the simplest maximum entropy model for collective behavior in a neural network .",
    "_ j stat mech _ p03011 .",
    "m okun , p yger , sl marguet , f gerard - mercier , a benucci , s katzner , l busse , m carandini & kd harris ( 2012 ) population rate dynamics and multi neuron firing patterns in sensory cortex .",
    "_ j neurosci _ * 32 : * 1710817119 .",
    "et jaynes ( 1957 ) information theory and statistical mechanics .",
    "_ phys rev _ * 106 : * 620630 .",
    "f rieke , d warland , rr de ruyter van steveninck & w bialek ( 1997 ) _ spikes : exploring the neural code _",
    "( mit press , cambridge ) . dn mastronarde ( 1983 ) interactions between ganglion cells in cat retina .",
    "_ j neurophysiol _ * 49 : * 350365 .",
    "sh devries ( 1999 ) correlated firing in rabbit retinal ganglion cells .",
    "_ j neurophysiol _ * 81 : * 908920 .",
    "ih brivanlou , dk warland & m meister ( 1998 ) mechanisms of concerted firing among retinal ganglion cells .",
    "_ neuron _ * 20 : * 527539 . pk trong & f rieke ( 2008 ) origin of correlated activity between parasol retinal ganglion cells .",
    "_ nat neurosci _ * 11 : * 13431351 .",
    "jl puchalla , e schneidman , ra harris & mj berry ii ( 2005 ) redundancy in the population code of the retina .",
    "_ neuron _ * 46 : * 493504 .",
    "r segev , j puchalla & mj berry ii ( 2006 ) functional organization of retinal ganglion cells in the salamander retina .",
    "_ j neurophysiol _ * 95 : * 22772292 .",
    "ms bazaraa , hd sherali & cm shetty ( 2005 ) _ nonlinear programming : theory and algorithms _ ( wiley & sons , hoboken nj , usa )",
    ". j keat , p reinagel , rc reid & m meister ( 2001 ) predicting every spike : a model for the responses of visual neurons .",
    "_ neuron _ * 30 : * 803817 .",
    "al fairhall , ca burlingame , r narasimhan , ra harris , jl puchalla & mj berry ii ( 2006 ) selectivity for multiple stimulus features in retinal ganglion cells .",
    "_ j neurophysiol _ * 96 : * 27242738 . jw pillow , j shlens , l paninski , a sher , am litke , ej chichilnisky & ep simoncelli ( 2008 ) spatio - temporal correlations and visual signalling in a complete neuronal population .",
    "_ nature _ * 454 : * 995 - 999 . ks sadeghi ( 2009 )",
    "_ progress on deciphering the retinal code .",
    "_ ( dissertation , princeton university ) .",
    "d perkel & t bullock ( 1968 ) neural coding . _ neurosci res program bull _ * 6 : * 221343 .",
    "i ginzburg & h sompolinsky ( 1994 ) theory of correlations in stochastic neural networks .",
    "_ phys rev e _ * 50 : * 31713191 .",
    "jj hopfield ( 2008 ) searching for memories , sudoku , implicit check bits , and the iterative use of not ",
    "always  correct rapid neural computation .",
    "_ neural comp _ * 20 : * 11191164 .",
    "d field ( 1987 ) relations between the statistics of natural images and the response properties of cortical cells .",
    "_ j opt soc a _ * 4 : * 23792394 .",
    "dl ruderman & w bialek ( 1994 ) statistics of natural images : scaling in the woods .",
    "_ phys rev lett _ * 73 : * 814817 .",
    "dw dong & jj atick ( 1995 ) statistics of natural time - varying images .",
    "_ network _ * 6 : * 345358 . o schwartz & ep simoncelli ( 2001 ) natural signal statistics and sensory gain control .",
    "_ nat neurosci _ * 4 : * 819825 ( 2001 ) .",
    "gj stephens , t mora , g tkaik & w bialek ( 2013 ) statistical thermodynamics of natural images .",
    "_ phys rev lett _ * 110 : * 018701 .",
    "m bethge & p berens ( 2008 ) near - maximum entropy models for binary neural representations of natural images . in _",
    "adv neural info proc sys _ * 20 : * 97104 , j platt et al eds ( cambridge , ma , mit press ) .",
    "e schneidman , s still , mj berry ii & w bialek ( 2003 ) network information and connected correlations .",
    "_ phys rev lett _ * 91 : * 238701 .",
    "s cocco , s leibler & r monasson ( 2009 ) neuronal couplings between retinal ganglion cells inferred by efficient inverse statistical physics methods .",
    "_ proc natl acad sci ( usa ) _ * 106 : * 1405814062 .",
    "s cocco & r monasson ( 2011 ) adaptive cluster expansion for inferring boltzmann machines with noisy data .",
    "_ phys rev lett _ * 106 : * 090601 .",
    "m mezard & t mora ( 2009 ) constraint satisfaction problems and neural networks : a statistical physics perspective .",
    "_ j physiol paris _ * 103 : * 107113 .",
    "b cessac , h rostro , jc vasques & t vieville ( 2009 ) how gibbs distributions may naturally arise from synaptic adaptation mechanisms .",
    "_ j stat phys _ * 136 : * 565602 .",
    "o marre , se boustani , y fregnac & a destexhe ( 2009 ) prediction of spatio  temporal patterns of neural activity from pairwise correlations .",
    "_ phys rev lett _ * 102 : * 138101 .",
    "r segev , j goodhouse , jl puchalla & mj berry ii ( 2004 ) recording spikes from a large fraction of the ganglion cells in a retinal patch .",
    "_ nat neurosci _",
    "* 7 : * 11551162 .",
    "d gunning , c adams , w cunningham , k mathieson , v oshea , km smith & ej chichilnisky ( 2005 ) 30@xmath241 m spacing 519-electrode arrays for in vitro retinal studies . _",
    "nuc inst methods a _ * 546 : * 148153 .",
    "d amodei , g schwartz & mj berry ii ( 2008 ) correlations and the structure of the population code in a dense patch of the retina .",
    "_ proceedings mea meeting _ , pp .",
    "197 , stett a ed .",
    ", biopro baden - wrttemberg , 2008 .",
    "d amodei ( 2011 ) _ network - scale electrophysiology : measuring and understanding the collective behavior of neural circuits .",
    "_ ( dissertation , princeton university ) . s nirenberg , sm carcieri , al jacobs & pe latham ( 2001 ) retinal ganglion cells act largely as independent encoders .",
    "_ nature _ * 411 : * 698701 .",
    "dp landau & k binder ( 2000 ) _ monte carlo simulations in statistical physics _",
    "( cambridge university press , cambridge , uk ) .",
    "tm cover & ja thomas ( 1991 ) _ elements of information theory _ ( wiley , new york ) .",
    "oe lanford iii ( 1973 ) entropy and equilibrium states in classical statistical mechanics .",
    "_ statistical mechanics and mathematical problems , _ a lenard ed . , pp . 1113 .",
    "springer verlag ( berlin ) .",
    "ld landau and em lifshitz ( 1977 ) _ statistical physics _",
    "( pergamon , oxford ) .",
    "m dudik , sj phillips & re schapire ( 2004 ) performance guarantees for regularized maximum entropy density estimation .",
    "_ proceedings 17th annual conference on learning theory_. am ferrenberg & rh swendsen ( 1988 ) new monte carlo technique for studying phase transitions . _ phys rev lett _ * 61 : * 26352638 .",
    "j lin ( 1991 ) divergence measures based on the shannon entropy .",
    "_ ieee trans inf theory _ * 37 : * 145151 .",
    "v sessak & r monasson ( 2009 ) small - correlation expansions for the inverse ising problem .",
    "_ j phys a _ * 42 : * 055001 .",
    "f azhar ( 2008 ) _ an information theoretic study of neural populations _",
    "( dissertation , university of california at santa barbara ) .",
    "f azhar & w bialek ( 2010 ) when are correlations strong ? _ arxiv.org : _ 1012.5987 .",
    "i nemenman , f shafee & w bialek ( 2001 ) entropy and inference , revisited .",
    "_ adv neural info proc syst _ * 14 : * 471478 .",
    "f wang & dp landau ( 2001 ) efficient , multiple - range random walk algorithm to calculate the density of states .",
    "_ phys rev lett _ * 86 : * 20502053 .",
    "mj berry ii , dk warland & m meister ( 1997 ) the structure and precision of retinal spike train .",
    "_ proc natl acad sci ( usa ) _ * 94 : * 54115416 .",
    "jh van hateren , l rttiger , h sun & bb lee ( 2002 ) processing of natural temporal stimuli by macaque retinal ganglion cells . _ j neurosci _ * 22 : * 99459960 .",
    "rm shapley & jd victor ( 1978 ) the effect of contrast on the transfer properties of cat retinal ganglion cells .",
    "_ j physiol _ * 285 : * 275298 .",
    "jd victor & rm shapley ( 1979 ) receptive field mechanisms of cat x and y retinal ganglion cells .",
    "_ j gen physiol _ * 74 : * 275298 .",
    "v mante , v bonin & m carandini ( 2008 ) functional mechanisms shaping lateral geniculate responses to artificial and natural stimuli . _",
    "neuron _ * 58 : * 625638 .",
    "ey chen , o marre , c fisher , g schwartz , j levy , ra da silveira & mj berry ii ( 2013 ) alert response to motion onset in the retina .",
    "_ j neurosci _ * 33 : * 120132 .",
    "y dan , jj atick & rc reid ( 1996 ) efficient coding of natural scenes in lateral geniculate nucleus : experimental test of a computational theory .",
    "_ j neurosci _ * 16 : * 33513362 .",
    "fe theunissen , k sen , and aj doupe ( 2000 spectral - temporal receptive fields of nonlinear auditory neurons obtained using natural sounds .",
    "_ j neurosci _ * 20 : * 21352331 .",
    "m stopfer , v jayaraman & g laurent ( 2003 ) intensity versus identity coding in an olfactory system .",
    "_ neuron _ * 39 : * 9911004 .",
    "ck machens , r romo & cd brody ( 2010 ) functional , but not anatomical , separation of `` what '' and `` when '' in prefrontal cortex .",
    "_ j neurosci _ * 30 : * 350360 .",
    "mm churchland , jp cunningham , mt kaufman , si ryu & kv shenoy ( 2010 ) cortical preparatory activity : representation of movement or first cog in a dynamical machine ?",
    "_ neuron _ * 68 : * 387400 .",
    "u rutishauser , a kotowicz & g laurent ( 2013 ) a method for closed - loop presentation of sensory stimuli conditional on the internal brain - state of awake animals .",
    "_ j neurosci methods _ * 215 : * 139155 .",
    "f attneave ( 1954 ) some informational aspects of visual perception .",
    "_ psychol rev _ * 61 : * 183193 .",
    "hb barlow ( 1961 ) possible principles underlying the transformation of sensory messages . in : rosenblith w , ed . _ sensory communication , _",
    "pp 217234 . mit press ( cambridge , usa ) .",
    "jj atick & an redlich ( 1990 ) towards a theory of early visual processing .",
    "_ neural comput _ * 2 : * 308320 .",
    "jh van hateren ( 1992 ) real and optimal neural images in early vision",
    ". _ nature _ * 360 : * 6870 .",
    "h barlow ( 2001 ) redundancy reduction revisited .",
    "_ network _ * 12 : * 241253 .",
    "ap alivisatos , m chun , gm church , rj greenspan , ml roukes & r yuste , the brain activity map project and the challenge of functional connectomics .",
    "_ neuron _ * 74 , * 970974 ( 2012 ) .",
    "m mezard , g parisi & ma virasoro ( 1987 ) _ spin glass theory and beyond _",
    "( world scientific , singapore ) .",
    "g tkaik , js prentice , v balasubramanian & e schneidman ( 2010 ) optimal population coding by noisy spiking neurons .",
    "_ proc natl acad sci ( usa ) _ * 107 : * 1441914424 .",
    "jj hopfield & dw tank ( 1985 ) `` neural '' computation of decisions in optimization problems .",
    "_ biol cybern _ * 52 : * 141152 .",
    "jj hopfield & dw tank ( 1986 ) computing with neural circuits : a model .",
    "_ science _ * 233 : * 625633 .",
    "mj berry ii , g tkaik , j dubuis , o marre , r azerado da silveira ( 2013 ) a simple method for estimating the entropy of neural activity . _",
    "j stat mech _ p03015 ."
  ],
  "abstract_text": [
    "<S> maximum entropy models are the least structured probability distributions that exactly reproduce a chosen set of statistics measured in an interacting network . </S>",
    "<S> here we use this principle to construct probabilistic models which describe the correlated spiking activity of populations of up to 120 neurons in the salamander retina as it responds to natural movies . already in groups as small as 10 neurons , interactions between spikes can no longer be regarded as small perturbations in an otherwise independent system ; for 40 or more neurons pairwise interactions need to be supplemented by a global interaction that controls the distribution of synchrony in the population . here we show that such `` k - pairwise '' models  </S>",
    "<S> being systematic extensions of the previously used pairwise ising models  provide an excellent account of the data . </S>",
    "<S> we explore the properties of the neural vocabulary by : 1 ) estimating its entropy , which constrains the population s capacity to represent visual information ; 2 ) classifying activity patterns into a small set of metastable collective modes ; 3 ) showing that the neural codeword ensembles are extremely inhomogenous ; 4 ) demonstrating that the state of individual neurons is highly predictable from the rest of the population , allowing the capacity for error correction . </S>"
  ]
}