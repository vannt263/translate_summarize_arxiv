{
  "article_text": [
    "recently , discovered that word representations learned by a recursive neural net ( rnn ) as well as by related log - linear models @xcite can capture the linguistic regularities in language , which allows easy solutions to analogy questions of the form `` beijing : china as paris : '' using simple linear algebra . with this word analogy task",
    ", a flurry of subsequent work exhibited that similar linear structure can also be revealed from representations learned from other methods @xcite .",
    "besides word representation , document representation is also a fundamental and critical problem in natural language processing . over the past decades , various methods",
    "have been proposed to represent the document as a vector , including bag of words ( bow )  @xcite , latent semantic indexing ( lsi )  @xcite , non - negative matrix factorization ( nmf )  @xcite ) and latent dirichlet allocation ( lda )  @xcite .",
    "recently , there is a rising enthusiasm for applying the neural embedding methods to representing the documents  @xcite .",
    "it is , therefore , natural to ask whether there is also linear structure in these learned document representations to allow similar reasoning at document level .",
    "for example , given three articles talking about naive bayes , logistic regression , and hidden markov model , is it possible to find the article about conditional random fields as the solution to the document analogy question `` naive bayes : logistic regression as hidden markov model : '' ( _ i.e. _ , document pairs explaining generative - discriminative model relations ) ? obviously , such reasoning is much more complex in semantics and can not be achieved by simple retrieval or classification based on lexical information .",
    "representation with such linear structure would be useful for many semantic processing applications , _",
    "e.g. _ , it may help controversial search  @xcite by discovering document pairs talking about opposite facts on controversial topics with some seed pairs , or help non - local corpus navigation and paper recommendation together with word vectors  @xcite .    for this purpose",
    ", we introduce a new document analogy task for evaluating the semantic regularities in document representations .",
    "since it is non - trivial to directly label the analogy questions over documents , we leverage the existing word / phrase semantic analogy test set and map the words / phrases in these questions to wikipedia articles through title matching . in this way , we obtain a large labeled analogy test set over documents . the task is then to test whether different document representations over the wikipedia articles can find the right answers to these semantic analogy questions .",
    "based on this test set , we evaluate several existing state - of - the - art document representations and show that neural embedding based models can achieve better performance than conventional models .",
    "the major contributions of this paper include : 1 ) the introduction of a new document analogy task with benchmark dataset for evaluating document representations ; 2 ) empirical comparison among state - of - the - art models and preliminary explanations over the results .",
    "[ cols=\"<,>,<\",options=\"header \" , ]     [ tab : da ]      we propose to create a document analogy test set so that we can quantitatively evaluate how well different document representations capture semantic regularities .",
    "following the idea of word analogy task , we try to build a test set of analogy questions of the form `` @xmath0 is to @xmath1 as @xmath2 is to '' , where @xmath3 are the identities of the documents . however , it is not trivial to directly label the relations between two arbitrary documents due to the diversity in topics",
    ". fortunately , we found that each wikipedia page is a concise document describing one specific concept , and thus the relations between the documents can be explained by their corresponding concepts .",
    "therefore , we can convert the task of labeling between documents into that between concepts ( which are of words or phrases ) , where we already have a large labeled data set from .    based on the idea above",
    ", we build a document analogy test set using wikipedia and existing word and phrase analogy test set .",
    "specifically , we adopt the publicly available april 2010 dump of wikipedia  @xcite , which has been widely used in  @xcite .",
    "the corpus contains articles and about @xmath4 billion tokens .",
    "we then collect all the existing word and phrase analogy test sets and match the words / phrases in questions to wikipedia page titles .",
    "note here we do not take syntactic analogy questions of words into consideration because the relations between documents are usually semantic . by resolving the ambiguity in matching , we finally obtain analogy questions over wikipedia documents .",
    "table  [ tab : da ] shows the details of the test set .      in this work ,",
    "we adopt the same vector offset method  @xcite for analogy reasoning . to answer the questions like `` _ _ a _ _ is to _ b _ as _ c _ is to '' , we try to find a document with vector @xmath5 , which is the closest to @xmath6 according to the cosine similarity : @xmath7 where @xmath8 and @xmath5 are the normalized document vectors .",
    "the question is judged as correctly answered only if @xmath9 is exactly the answer document in the evaluation set .",
    "the evaluation metric for this task is the percentage of questions answered correctly .",
    "in this section , we briefly summarize the models used in this paper . before that , we first list the notations",
    ".    let @xmath10 denote a corpus of @xmath11 documents over the word vocabulary @xmath12 .",
    "let @xmath13 be a document - word matrix , where entry @xmath14 in @xmath15 denotes the weight of the @xmath16-th word @xmath17 in the @xmath18-th document @xmath19 .",
    "* bag of words ( bow ) * model treats a document as a bag ( multiset ) of its words .",
    "it represents a document @xmath20 as a vector @xmath21 , where @xmath14 denotes the weight of the @xmath16-th word @xmath17 in the @xmath18-th document @xmath19 .",
    "the most popular weighting scheme for @xmath14 is tf - idf  @xcite .",
    "however , the bow model suffers from the sparsity and curse of dimensionality due to treating individual word as distinct feature .",
    "* matrix factorization * methods attempt to tackle the limitation of bow model through learning a low - dimensional vector for document by factorizing the document - word matrix @xmath15",
    ".    applied truncated singular value decomposition ( svd ) to document - word matrix , namely latent semantic indexing ( lsi ) .",
    "lsi approximates @xmath15 by setting all but the largest @xmath22 singular values in @xmath23 to @xmath24 ( @xmath25 ) , as @xmath26 hence one might think of the rows of @xmath27 as representations for documents in the latent space .    an alternative way is factorizing @xmath15 into two non - negative matrices  @xcite , @xmath28 where the rows of @xmath29 can be seen as the representations of documents .    unlike lsi which may have negative entries , nmf has better interpretability with the non - negative constraint .",
    "* topic models * are also very popular in document representation fields because of their good interpretability , generalization ability and extensibility .",
    "the most representative work is the latent dirichlet allocation ( lda ) model introduced by .",
    "it represents the documents as distributions over latent topics , where each topic is characterized by a distribution over words .",
    "* neural embedding * models have attracted much attention in text representations due to its breakthrough in statistical language model  @xcite .    the paragraph vector models are first introduced in  @xcite for document representation .",
    "the distributed memory model of paragraph vectors ( pv - dm ) captures the representation of a document via inserting a document vector to the continuous bag - of - words ( cbow ) model  @xcite . a simpler model can be obtained by replacing the input word vector with document vector in skip gram ( sg ) model , which is called `` distributed bag of words version of paragraph vector '' ( pv - dbow ) .",
    "* bag of word embeddings ( bowe ) * model tries to represent the document as a linear combination of word vectors , where the word vectors @xmath30 can be obtained by tools like ` word2vec ` or ` glove ` .",
    "the low - dimensional representations of documents in bowe can be written as @xmath31 where @xmath15 denotes the bow representation of documents .",
    "l r r r r > [ .85]r > r r relation & bow & lsi & nmf & lda & pv - dm & pv - dbow & bowe + capital - common - countries & 0.0 & 23.12 & 9.29 & 23.72 & 60.87 & 54.15 & * 83.0 * + capital - world & 0.8 & 9.97 & 5.06 & 9.15 & 43.62 & 42.65 & *",
    "67.53 * + currency & 0.0 & 0.0 & 0.0 & 0.0 & 4.55 & 3.41 & * 14.77 * + city - in - state & 0.0 & 7.94 & 6.50 & 4.33 & 33.57 & 34.30 & * 51.26 * + family & 19.64 & 5.36 & 1.79 & 14.29 & * 21.43 * & * 21.43 * & 19.64 + newspapers & 5.0 & 25.0 & 10.0 & 10.0 & 5.0 & * 50.0 * & 40.0 + ice hockey & 0.0 & 3.68 & 2.16 & 0.0 & 12.12 & 20.13 & * 33.33 * + basketball & 0.0 & 4.25 & 1.63 & 0.0 & 10.13 & 14.71 & * 38.56 * + airlines & 11.76 & 9.15 & 2.94 & 2.61 & 12.42 & 20.26 & * 42.48 * + people - companies & 2.0 & 1.0 & 0.0 & 0.0 & 6.0 & * 12.0 * & 2.0 + total & 1.34 & 9.88 & 4.81 & 8.43 & 37.47 & 37.76 & * 60.42 * +    [ tab : d100 ]    in this section , we first describe our experimental settings including the corpus , hyper - parameter selections , and specifications for different document representation methods .",
    "then we compare these methods on document analogy task and discuss the results .",
    "the corpus used to learn document representations in this experiment is the same wikipedia april 2010 dump as described in section  [ sec : dataset ] . in preprocessing , we lowercase the corpus , remove pure digit words , non - english characters and the words occur less than 20 times .      the baseline methods used in this paper including bow with tf - idf weight , lsi , nmf , lda , pv - dm , pv - dbow , and bowe . for bow , lsi and lda",
    ", we use the popular python topic model library ` gensim ` . for nmf , we choose the python machine learning library ` scikit learn ` .",
    "we implement pv - dm and pv - dbow models in c++ due to have not released source codes of pv models . for word embeddings in bowe , we use cbow in the ` word2vec ` tool .",
    "the negative sampling method is adopted to take the place of the hierarchal softmax since we found the former always achieves better performance .",
    "the learning rate is linearly decayed to @xmath24 as described in  @xcite , where the initial learning rate of pv - dm and cbow model is 0.05 , and pv - dbow is 0.025 .",
    "we set context window size as 10 and use 10 negative samples .      in table",
    "[ tab : d100 ] , we compare the results of 100-dimensional document vectors from all the methods on different subtasks of document analogy .",
    "as we can see , among all the methods , bow is almost the worst .",
    "this demonstrates the weakness of simple vector space model on capturing semantic regularities .",
    "neural embedding models such as pv - dm and pv - dbow perform much better than conventional latent models such as lsi , nmf , and lda .",
    "this is quite amazing since pv models can also be viewed as implicit matrix factorization according to the explanations on ` word2vec `  @xcite .",
    "a major difference is that conventional latent models usually work on matrix with each entry standing for the frequency or tf - idf of a word in a document , while pv models factorize a shifted pointwise mutual information ( shifted - pmi ) matrix .",
    "as discussed in  @xcite , pmi is a key factor why ` word2vec ` can work well for word analogy task .",
    "we guess this might also be a major factor to explain the gap between pv models and other latent models .",
    "therefore , we conducted further experiments on lsi .",
    "as a result , we find that the total accuracy of lsi can achieve 15.53% with pmi matrix ( about 57% performance gain over lsi with tf - idf matrix ) .",
    "the results indicate that pmi plays an important role in revealing linear structure in document representations .",
    "a surprising result is that the simple bowe model performs significantly better than any other methods on almost all the subtasks ( @xmath32-value @xmath33 ) .",
    "there might be two possible reasons for the result .",
    "firstly , when learning word vectors alone with ` word2vec ` , one can achieve very high scores on word analogy tasks  @xcite .",
    "bowe thus benefits from the strong linear structures in word vectors by directly using word vectors as the representation of a document .",
    "secondly , the calculation of euclidean distance between documents under bowe is equivalent to using a relaxed word mover s distance  @xcite , which has been shown strong performance in measuring document distance .",
    "we also conduct the experiments on different dimensions as shown in table  [ tab : total ] .",
    "similar trending can be found as that in table  [ tab : d100 ] .",
    "l r r r r & + & 50 & 100 & 150 & 200 + bow & 1.34 & 1.34 & 1.34 & 1.34 + lsi & 4.19 & 9.88 & 17.0 & 21.81 + nmf & 1.59 & 4.81 & 8.75 & 11.85 + lda & 3.52 & 8.43 & 10.39 & 10.45 + pv - dm & 25.62 & 37.47 & 37.71 & 36.03 + pv - dbow & 25.33 & 37.76 & 40.61 & 39.09 + bowe & * 42.05 * & * 60.42 * & * 66.74 * & * 69.49 * +    [ tab : total ]",
    "in this paper , we introduce a new document analogy task for quantitatively evaluating how well different document representations capture semantic regularities .",
    "based on the introduced benchmark dataset , we conduct empirical comparisons among several state - of - the - art document representation methods .",
    "the results reveal that neural embedding based document representations work better on this analogy task .",
    "we provide some preliminary explanations on these observations , leaving the inherent differences of these models to be further investigated in the future . with this benchmark dataset",
    ", it would also be easier for us to develop new document representation models and to compare with existing methods .",
    "shiri dori - hacohen , elad yom - tov , and james allan .",
    "2015 . navigating controversy as a complex search task . in _ proceedings of the first international workshop on supporting complex search tasks co - located with the 37th european conference on information retrieval(ecir 2015)_. elsevier , march .",
    "eric  h. huang , richard socher , christopher  d. manning , and andrew  y. ng .",
    "2012 . improving word representations via global context and multiple word prototypes . in _ proceedings of the 50th annual meeting of the association for computational linguistics : long papers - volume 1 _ , acl 12 , pages 873882 , stroudsburg , pa , usa . association for computational linguistics .",
    "matt  j. kusner , yu  sun , nicholas  i. kolkin , and kilian  q. weinberger .",
    "2015 . from word embeddings to document distances . in _ proceedings of the 32th international conference on machine learning ( icml-15 )",
    "_ , icml 15 .",
    "acm , new york , ny , usa , july .",
    "quoc le and tomas mikolov .",
    "2014 . distributed representations of sentences and documents . in tony jebara and eric  p. xing , editors , _ proceedings of the 31st international conference on machine learning ( icml-14 ) _ , pages 11881196 .",
    "jmlr workshop and conference proceedings .",
    "omer levy and yoav goldberg . 2014a .",
    "neural word embedding as implicit matrix factorization . in _ advances in neural information processing systems 27 _ , pages 21772185 .",
    "curran associates , inc . ,",
    "montreal , quebec , canada .",
    "minh - thang luong , richard socher , and christopher  d. manning .",
    "better word representations with recursive neural networks for morphology . in _ proceedings of the seventeenth conference on computational natural language learning _ , pages 104113 .",
    "association for computational linguistics .",
    "tomas mikolov , ilya sutskever , kai chen , greg  s corrado , and jeff dean .",
    "2013b . distributed representations of words and phrases and their compositionality .",
    "burges , l.  bottou , m.  welling , z.  ghahramani , and k.q .",
    "weinberger , editors , _ advances in neural information processing systems 26 _ , pages 31113119 .",
    "curran associates , inc .",
    "tomas mikolov , wen tau yih , and geoffrey zweig .",
    "linguistic regularities in continuous space word representations . in _ proceedings of the 2013 conference of the north american chapter of the association for computational linguistics : human language technologies ( naacl - hlt-2013)_. association for computational linguistics , may .",
    "andriy mnih and koray kavukcuoglu .",
    "2013 . learning word embeddings efficiently with noise - contrastive estimation . in c.j.c .",
    "burges , l.  bottou , m.  welling , z.  ghahramani , and k.q .",
    "weinberger , editors , _ advances in neural information processing systems 26 _ , pages 22652273 .",
    "curran associates , inc .",
    "arvind neelakantan , jeevan shankar , alexandre passos , and andrew mccallum . 2014 .",
    "efficient non - parametric estimation of multiple embeddings per word in vector space . in _ proceedings of the 2014 conference on empirical methods in natural language processing ( emnlp )",
    "_ , pages 10591069 , doha , qatar , october .",
    "association for computational linguistics .",
    "jeffrey pennington , richard socher , and christopher  d. manning .",
    "glove : global vectors for word representation . in _ proceedings of the 2014 conference on empirical methods in natural language processing ,",
    "emnlp 2014 , october 25 - 29 , 2014 , doha ,",
    "qatar , a meeting of sigdat , a special interest group of the acl _ , pages 15321543 .",
    "nitish srivastava , ruslan salakhutdinov , and geoffrey  e. hinton .",
    "modeling documents with deep boltzmann machines . in _ proceedings of the twenty - ninth conference on uncertainty in artificial intelligence _ , pages 616625 , seattle , usa , august ."
  ],
  "abstract_text": [
    "<S> recent work exhibited that distributed word representations are good at capturing linguistic regularities in language . </S>",
    "<S> this allows vector - oriented reasoning based on simple linear algebra between words . since many different methods have been proposed for learning document representations , it is natural to ask whether there is also linear structure in these learned representations to allow similar reasoning at document level . to answer this question , we design a new document analogy task for testing the semantic regularities in document representations , and conduct empirical evaluations over several state - of - the - art document representation models . </S>",
    "<S> the results reveal that neural embedding based document representations work better on this analogy task than conventional methods , and we provide some preliminary explanations over these observations . </S>"
  ]
}