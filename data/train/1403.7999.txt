{
  "article_text": [
    "maximal monotone operators have been extensively studied since @xcite , largely because they have wide applications in pure and applied sciences @xcite , and because they provide a convenient framework for a unified treatment of equilibrium problems , variational inequalities , and convex optimization , see @xcite and references therein .",
    "more precisely , let @xmath0 be a real hilbert space and let @xmath1 be a set valued maximal monotone operator @xcite , a key problem is to find an element @xmath2 such that @xmath3 . in this paper , we consider the case where @xmath4 is the sum of two maximal monotone operators denoted by @xmath5 and @xmath6 , with @xmath6 single valued and cocoercive .",
    "the problem is then to find @xmath7 such that @xmath8 under the assumption that such a @xmath9 exists .",
    "problem includes for example fixed point problems and variational inequalities , which can be recovered choosing @xmath5 equal to the normal cone of a nonempty closed and convex set , but also composite minimization problems , where @xmath5 is the subdifferential of a proper lower semicontinuous convex functions and @xmath6 is the gradient of a smooth convex function @xcite .",
    "indeed , there is a vast literature on algorithmic schemes for solving , and in particular on approaches separating the contribution of @xmath5 and @xmath6 .",
    "well - known among such approaches is the forward - backward splitting algorithm @xcite , @xmath10 where @xmath110,+\\infty\\right[$ ] and @xmath12 is the resolvent operator of @xmath5 . since the seminal works in @xcite",
    ", forward - backward splitting methods have been considerably developed to be more flexible , to achieve better convergence properties , and to allow for numerical errors , see @xcite and references therein .    in this paper , we are interested in the practically relevant situation in which @xmath6 is known only through measurements subject to non vanishing random noise , or when the computation of a stochastic estimate of @xmath6 is cheaper than the evaluation of the operator itself . while there is a rich literature on stochastic proximal gradient splitting algorithms for convex minimization problems ( see e.g. @xcite ) , and various results for variational inequalities are available @xcite , we are not aware of previous study of stochastic splitting algorithms for solving monotone inclusions .",
    "the results in this paper fill this gap by proposing a stochastic forward backward splitting method for monotone inclusions and proving : 1 ) a non - asymptotic error analysis in expectation , and 2 ) strong almost sure convergence of the iterates . more specifically ,",
    "under strong monotonicity assumptions , we provide non asymptotic bounds for convergence in norm and in expectation , leveraging on a non asymptotic version of chung s lemma ( * ? ? ?",
    "* chapter 2 , lemma 5 ) .",
    "almost sure convergence is obtained under the weaker assumption of uniform monotonicity of @xmath6 using the concept of stochastic quasi - fejr sequences @xcite , which allows to originally combine deterministic and stochastic techniques .",
    "a few features of our approach are worth mentioning .",
    "first , from the modeling point of view , we consider weak assumptions on the stochastic estimate of @xmath6 with respect to the ones usually required in the stochastic optimization literature , see e.g. @xcite .",
    "in particular , the assumption on the stochastic estimate of @xmath6 is different from the one in @xcite , which assume a summability condition on the errors ( on the other hand , the fact that in our case the errors do not go to zero does not allow to consider a non vanishing step - size , as it is done in @xcite ) .",
    "second , our approach allows to avoid averaging the iterates .",
    "this aspect becomes crucial in situations where the problem structure is meant to enforce sparsity of the solution and an averaging process can be detrimental , see e.g. @xcite .",
    "the paper is organized as follows .",
    "section [ sec : pre ] collects some preliminaries proved in the appendix . in section [ sec : main ] we establish the main results of the paper : almost sure convergence of the iterates and a non - asymptotic analysis of stochastic forward - backward splitting algorithm , where the bounds depend explicitly on the parameters of the problem .",
    "section [ sec : sc ] focuses on special cases : variational inequalities , minimization problems , and minimization problems over orthonormal bases . for the case of variational inequalities",
    ", we obtain an additional convergence result without imposing stronger monotonicity properties on @xmath6 , which requires averaging of the iterates",
    ".    * notation . * throughout",
    ", @xmath13 is a probability space , @xmath14 , @xmath0 is a real hilbert space , and @xmath15 is its power set .",
    "we denote by @xmath16 and @xmath17 the scalar product and the associated norm of @xmath0 .",
    "the symbols @xmath18 and @xmath19 denote weak and strong convergence , respectively .",
    "we denote by @xmath20 the set of summable sequences in @xmath21 . the class of all lower semicontinuous convex functions @xmath22-\\infty,+\\infty\\right]$ ] such that @xmath23 is denoted by @xmath24 .",
    "we denote by @xmath25 the @xmath26-field generated by a random variable @xmath27 , where @xmath0 is endowed with the borel @xmath26-algebra .",
    "the expectation of a random variable @xmath28 is denoted by @xmath29 $ ] .",
    "the conditional expectation of @xmath28 given a sub sigma algebra @xmath30 is denoted by @xmath31 $ ] .",
    "the conditional expectation of @xmath28 given @xmath32 is denoted by @xmath33 $ ] . a sequence @xmath34 of sub sigma algebras of @xmath35 such that , for every @xmath36 , @xmath37 is called a filtration .",
    "let , for every @xmath38 , @xmath39 be an integrable random variable with @xmath40<+\\infty$ ] .",
    "the sequence @xmath41 is called a random process .",
    "before discussing our main contributions , we recall basic concepts and results we need in the following .    let @xmath42 be a set - valued operator .",
    "the domain and the graph of @xmath5 are respectively defined by @xmath43 and @xmath44 .",
    "we denote by @xmath45 the set of zeros of @xmath5 .",
    "the inverse of @xmath5 is @xmath46 .",
    "the following notion is central in the paper .",
    "let @xmath42 be a set - valued operator .",
    "@xmath5 is monotone if @xmath47 and maximally monotone if it is monotone and there exists no monotone operator @xmath48 such that @xmath49 properly contains @xmath50",
    ".    let @xmath5 be a monotone operator and let @xmath51 .",
    "the following concepts are pointwise variants of the well - known concepts of uniform and strong monotonicity .",
    "we say that @xmath5 is uniformly monotone at @xmath52 if there exists an increasing function @xmath53 $ ] vanishing only at @xmath54 such that @xmath55 in the case when @xmath56 , for some @xmath570,+\\infty\\right[$ ] , we say that @xmath5 is @xmath58-strongly monotone at @xmath52 .",
    "if @xmath59 is monotone , for some @xmath570,+\\infty\\right[$ ] , we say that @xmath5 is @xmath58-strongly monotone .",
    "we say that @xmath5 is strictly monotone at @xmath51 if @xmath60 let @xmath610,+\\infty\\right[$ ] . a single - valued operator @xmath62 is @xmath63-cocoercive if @xmath64 the resolvent of any maximally monotone operator @xmath5 is @xmath65 we recall that @xmath66 is well defined and single valued @xcite , and can therefore be identified with an operator @xmath67 . when @xmath68 for some @xmath69 , then @xmath66 coincides with the proximity operator of @xmath70 @xcite , which is defined as @xmath71    we next recall the concept of stochastic quasi fejr sequence , which was introduced and studied in the papers @xcite .",
    "this concept provides a unified approach to prove convergence of several algorithms in convex optimization ( see @xcite and references therein ) .",
    "@xcite let @xmath72 be a non - empty subset of @xmath0 .",
    "a random process @xmath73 in @xmath0 is stochastic quasi - fejr monotone with respect to the set @xmath72 if @xmath74 < + \\infty$ ] and there exists @xmath75 such that @xmath76 \\leq \\|w_n - w\\|^2 + \\varepsilon_n \\quad \\text{a.s.}\\ ] ]",
    "the following is the main problem studied in the paper .",
    "[ inc0 ] let @xmath77 be a maximally monotone operator , let @xmath780,+\\infty[$ ] and let @xmath79 be a @xmath63-cocoercive operator .",
    "assume that @xmath80 .",
    "the goal is to find @xmath7 such that @xmath81      we propose the following stochastic forward - backward splitting algorithm for solving problem [ inc0 ] .",
    "the key difference with respect to the classical setting is that we assume to have access only to a stochastic estimate of @xmath6 .",
    "[ a : maininc ] let @xmath82 be a sequence in @xmath830,+\\infty[$ ] , let @xmath84 be a sequence in @xmath85 $ ] , and let @xmath86 be a @xmath0-valued random process such that @xmath87   < + \\infty$ ] .",
    "let @xmath88 be a random variable such that @xmath74<+\\infty$ ] and set @xmath89 \\end{array}\\ ] ]    we will consider the following conditions for the filtration @xmath90 , @xmath91 .    1 .   for every @xmath92 , @xmath93 = bw_n$ ] .",
    "2 .   there exist @xmath94 in @xmath950,+\\infty\\right[$ ] and @xmath960,+\\infty\\right[}}$ ] such that , for every @xmath92 , @xmath97",
    "\\leq \\sigma^2(1 + \\alpha_n\\|bw_n\\|^2 ) $ ] .",
    "3 .   there exists @xmath980,+\\infty [ $ ] such that @xmath99 @xmath100 .",
    "4 .   let @xmath9 be a solution of problem [ inc0 ] and let @xmath101 then the following hold : @xmath102       1 .",
    "if , for every @xmath92 , @xmath103 , algorithm [ a : maininc ] reduces to the well known forward ",
    "backward splitting in ( * ? ? ?",
    "* section 6 ) . however , under assumptions ( a1)-(a2)-(a3)-(a4 ) , weak convergence of @xmath73 is not guaranteed since the conditions @xmath104 and @xmath105 imply @xmath106 , while to apply the classic theory we need @xmath107 to be strictly greater than 0 . assuming additionally that @xmath108 , under our assumptions only ergodic convergence of @xmath109 has been proved in the deterministic case in @xcite .",
    "a stochastic forward - backward splitting algorithm for monotone inclusions has been recently analyzed in @xcite , under rather different assumptions .",
    "indeed , they consider a fixed stepsize and a summability condition on @xmath97 $ ] . in the case @xmath110 and @xmath111 , for some @xmath70 and @xmath112 such that @xmath113 is differentiable with @xmath114-lipschitz continuous gradient , algorithm [ a : maininc ] reduces to the stochastic proximal forward - backward splitting which is a variant of the algorithm in @xcite , also studied in @xcite .",
    "3 .   condition ( a2 ) can be seen as a relative error criterion , and has been considered in @xcite for the case of constrained minimization problems on infinite dimensional spaces .",
    "this is a more general condition than the one usually assumed in the context of stochastic optimization , where @xmath115 .",
    "4 .   if @xmath116 , then @xmath117 for every solution of problem [ inc0 ] . in this case , @xmath118 in and condition @xmath119 becomes @xmath105 and @xmath120 .",
    "the latter are the usual conditions required on the stepsize in the study of stochastic gradient descent algorithms ( see e.g. @xcite ) .",
    "these conditions guarantee a sufficient , but not too fast , decrease of the stepsize length . moreover , in this case , @xmath121 is not necessarily bounded , therefore condition ( a2 ) is a very weak requirement .",
    "[ ex : afm ] let @xmath122 be a probability space , let @xmath123 be a measurable function such that @xmath124 , and suppose that @xmath6 satisfies @xmath125 then there are several ways to find a stochastic estimate of @xmath126",
    ". in particular , if an independent and identically distributed sequence @xmath127 of realizations of the random vector @xmath52 is available , then one can take @xmath128 . if in addition @xmath6 is a gradient operator and @xmath129 is finite dimensional , we are in the classical setting of stochastic optimization @xcite .      in this section",
    "we describe our main results about almost sure convergence of the iterates of algorithm [ a : maininc ] .",
    "all the proofs are postponed to section [ sec : pf ] .",
    "we first collect basic properties satisfied by the sequences in algorithm [ a : maininc ] .",
    "[ p:1 ] suppose that ( a1 ) , ( a2 ) , ( a3 ) , and ( a4 ) are satisfied .",
    "let @xmath130 be the sequence generated by algorithm [ a : maininc ] and let @xmath9 be a solution of problem [ inc0 ] . then the following hold :    1 .",
    "[ p:1i ] the sequence @xmath131)_{n\\in{\\ensuremath{\\mathbb n}}^{*}}$ ] converges to a finite value .",
    "[ p:1iii ] @xmath132 < { \\ensuremath{+\\infty}}$ ] .",
    "consequently , @xmath133 = 0 \\quad \\text{and } \\quad   \\varliminf_{n\\to\\infty}{\\ensuremath{\\mathbb{e}}}[\\|bw_n - b\\overline{w}\\|^2 ] = 0.\\ ] ] 3 .",
    "[ p:1ii ] @xmath134\\ ! < \\ ! { \\ensuremath{+\\infty}}$ ] and @xmath135\\ ! < \\ !",
    "+ \\infty$ ] .",
    "proposition [ p:1 ] states similar properties to those stated for the forward - backward splitting algorithm in @xcite .",
    "these properties are key to prove almost sure convergence , which is stated in the next theorem .",
    "depending on the monotonicity properties of the operator @xmath6 , we get different convergence results .",
    "[ t:2nv ] suppose that conditions ( a1 ) , ( a2 ) , ( a3 ) and ( a4 ) are satisfied .",
    "let @xmath130 be the sequence generated by algorithm [ a : maininc ] and let @xmath9 be a solution of problem [ inc0 ] . then the following hold :    1 .",
    "[ t:2nvi- ] @xmath73 is stochastic quasi - fejr monotone with respect to @xmath136 .",
    "[ t:2nvi ] there exists an integrable random variable @xmath137 such that @xmath138 a.s .",
    "[ t:2nvii ] if @xmath6 is uniformly monotone at @xmath9 , then @xmath139 a.s .",
    "[ t:2nviii ] if @xmath140 is strictly monotone at @xmath9 and weakly continuous , then there exists @xmath141 such that @xmath142 , and , for every @xmath143 , there exists a subsequence @xmath144 such that @xmath145 .",
    "this kind of convergence of the iterates is the one traditionally studied in the stochastic optimization literature .",
    "however , most papers focus on the finite dimensional setting , and require boundedness of the variance of the stochastic estimate of the gradients or subgradients ( namely , @xmath115 in assumption ( a2 ) ) .",
    "one exception is @xcite , dealing with a stochastic projected subgradient algorithm on a hilbert space .",
    "weak almost sure convergence of the iterates generated by the stochastic forward - backward splitting algorithm can be derived from the general results in @xcite , without additional monotonicity assumptions on @xmath5 or @xmath6 . the assumptions in (",
    "* proposition 5.7 ) allows for a nonvanishing stepsize but requires summability of the stochastic errors . even in the minimization case",
    ", we are not aware of any paper proving convergence of the stochastic forward - backward algorithm with constant step - size without assuming that the variance of the stochastic estimate goes to zero .",
    "[ t:2nviv ] under the same assumptions as in theorem [ t:2nv ] , suppose in addition that @xmath6 is strictly monotone at @xmath9 .",
    "the assumptions of theorem [ t:2nv][t:2nviii ] are satisfied in the following two cases :    1 .",
    "@xmath0 is finite dimensional . indeed in this case",
    "the weak and strong topology coincide , and therefore @xmath6 is weakly continuous .",
    "@xmath6 is bounded and linear , since in this case @xmath6 is weakly continuous . for instance , this covers the case of regularized quadratic minimization on hilbert spaces .",
    "in this section we focus on convergence in expectation .",
    "we provide results for the case when either @xmath5 or @xmath6 is strongly monotone .",
    "we derive a nonasymptotic bound for @xmath146 $ ] similarly to what has been done for the stochastic gradient algorithm for the case of minimization of a smooth function in the finite dimensional case ( * ? ? ?",
    "* theorem 1 ) . in the next theorem",
    "we will consider the following assumption .",
    "[ ass : strcon ] let @xmath9 be a solution of problem [ inc0 ] . furthermore ,",
    "suppose that @xmath5 is @xmath147-strongly monotone and @xmath6 is @xmath58-strongly monotone at @xmath9 , for some @xmath148 such that @xmath149 .    to state the results more concisely , for every @xmath150",
    ", we define the function @xmath1510,{\\ensuremath{+\\infty}}\\right[\\to { \\ensuremath{\\mathbb r}}\\colon t\\mapsto   \\begin{cases } ( t^{c}-1)/c & \\text{if $ c \\not=0$};\\\\",
    "\\log t & \\text{if $ c = 0$}. \\end{cases}\\ ] ]    [ t:2 ] let @xmath1520,+\\infty\\right[}}^2 $ ] and let @xmath109 be the sequence generated by algorithm [ a : maininc ] .",
    "assume that conditions @xmath153 , @xmath154 , and assumption [ ass : strcon ] are satisfied and suppose that @xmath155 , @xmath156 , and that @xmath157 for some @xmath1580,1\\right]$ ] and for some @xmath1590,+\\infty[$ ] .",
    "set @xmath160 let @xmath161 be the smallest integer such that for every integer @xmath162 , it holds @xmath163 define @xmath164.\\ ] ] then , for every @xmath165 , the following hold :    1 .",
    "[ t:2i ] suppose that @xmath1660,1\\right[$ ] . then @xmath167 2 .",
    "[ t:2ii ] suppose that @xmath168 .",
    "then @xmath169 3 .",
    "[ t:2iii ] the sequence @xmath170 satisfies @xmath1710,1\\right[\\\\   o(n^{-c } ) & \\text{if } \\theta=1,\\ , c<1\\\\ o(n^{-1}\\log n )   & \\text{if } \\theta=1,\\,c=1\\\\ o(n^{-1 } )   & \\text{if } \\theta=1,\\,c>1 .",
    "\\end{cases}\\ ] ]    theorem [ t:2 ] implies that , even without assuming @xmath119 , in the strongly monotone case there is convergence in quadratic mean for every @xmath1660,1\\right]$ ] .",
    "regardless of the choice of @xmath172 , the constants in and depend on @xmath173 and thus on the monotonicity constant of @xmath174 . by",
    ", it follows that the best rate is obtained with @xmath168 , for a choice of @xmath175 ensuring @xmath176 .",
    "the resulting rate of convergence for the iterates is @xmath177 , which , in the special case of minimization , coincides with the one that can be obtained applying recent accelerated variants of proximal gradient methods .",
    "we start with a result characterizing the asymptotic behavior of stochastic quasi - fejr monotone sequences .",
    "the following statement is given in ( * ? ? ?",
    "* lemma 2.3 ) without a proof .",
    "a version of proposition [ p : fejer ] in the finite dimensional setting can also be found in @xcite .",
    "the concept of stochastic fejr sequences has been revisited and extended in a hilbert space setting in the recent preprint @xcite .",
    "for the sake of completeness we provide a proof .",
    "[ p : fejer ] let @xmath72 be a non - empty closed subset of @xmath0 , and let @xmath130 be stochastic quasi - fejr monotone with respect to @xmath72 . then the following hold .    1 .",
    "[ p : fejeri ] let @xmath178 . then , there exist @xmath179 and an integrable random vector @xmath180 such that @xmath181\\to\\zeta_w$ ] and @xmath182 almost surely .",
    "[ p : fejerii ] @xmath130 is bounded a.s .",
    "[ p : fejeriii ] the set of weak subsequential limits of @xmath130 is non - empty a.s .",
    "it follows from that @xmath183   \\leq { \\ensuremath{\\mathbb{e}}}[\\|w_n - w\\|^2 ] + \\varepsilon_n \\,.\\ ] ]    [ p : fejeri ] : since the sequence @xmath184 is summable and @xmath185 $ ] is finite , we derive from that @xmath186)_{n\\in{\\ensuremath{\\mathbb n}}^*}$ ] is a real positive quasi - fejr sequence ( * ? ? ?",
    "* definition 1.1(2 ) ) , and therefore it converges to some @xmath187 by ( * ? ? ?",
    "* lemma 3.1 ) .",
    "set @xmath188 then , it follows from that @xmath189 & =   { \\ensuremath{\\mathbb{e}}}[\\|w_{n+1}-w\\|^2   |\\mathcal{f}_n ] + \\sum_{k = n+1}^{\\infty}\\varepsilon_n\\notag\\\\ & \\leq \\|w_n - w\\|^2 +   \\sum_{k = n}^{\\infty}\\varepsilon_n\\notag\\\\ & = r_n.\\end{aligned}\\ ] ] therefore @xmath190 is a real supermartingale .",
    "since @xmath191=0 < + \\infty$ ] by , @xmath192 converges a.s to an integrable random variable ( * ? ? ?",
    "* theorem 9.4 ) , that we denote by @xmath193 .",
    "[ p : fejerii]&[p : fejeriii ] : follow directly by [ p : fejeri ] .    for the convenience of the reader , we recall the following well - known property of the resolvent of a maximal monotone operator .",
    "* proposition 23.7 ) [ l : firm ] let @xmath42 be maximally monotone . then , the resolvent of @xmath5 is firmly - nonexpansive , i.e. , @xmath194    we next prove proposition  [ p:1 ] .",
    "let @xmath92 .",
    "since @xmath9 is a solution of problem [ inc0 ] we have @xmath195 it follows from and the convexity of @xmath196 that @xmath197 since @xmath198 is firmly non - expansive by lemma [ l : firm ] , setting @xmath199 we have @xmath200 since @xmath201<+\\infty$ ] by assumption , we derive that @xmath202<+\\infty$ ] . on the other hand , by induction",
    "we get that @xmath146<+\\infty$ ] and hence @xmath203<+\\infty$ ] and therefore @xmath204 < + \\infty$ ] , so that @xmath205 $ ] is well - defined .",
    "assumption ( a1 ) yields @xmath206   & = { \\ensuremath{\\mathbb{e}}}[{\\ensuremath{\\mathbb{e}}}[{\\left\\langle{w_n-\\overline{w}}\\mid { { \\ensuremath{{\\mathfrak b}}}_n - b\\overline{w } } \\right\\rangle}|\\mathcal{f}_n ] \\notag\\\\ & = { \\ensuremath{\\mathbb{e}}}[{\\left\\langle{w_n-\\overline{w}}\\mid { { \\ensuremath{\\mathbb{e } } } [ { \\ensuremath{{\\mathfrak b}}}_n - b\\overline{w}|\\mathcal{f}_n ] } \\right\\rangle}]\\notag\\\\ & = { \\ensuremath{\\mathbb{e}}}[{\\left\\langle{w_n-\\overline{w}}\\mid { bw_n - b\\overline{w } } \\right\\rangle}].\\end{aligned}\\ ] ] moreover , using assumption ( a2 ) and the cocoercivity of @xmath6 , we have @xmath207=\\notag\\\\   &   = { \\ensuremath{\\mathbb{e}}}[\\|bw_n - b\\overline{w } \\|^2 ] + { \\ensuremath{\\mathbb{e}}}[\\|{\\ensuremath{{\\mathfrak b}}}_n - bw_n\\|^2]+2{\\ensuremath{\\mathbb{e}}}[\\langle bw_n - b\\overline{w } , { \\ensuremath{{\\mathfrak b}}}_n - bw_n \\rangle ] \\notag\\\\ & = { \\ensuremath{\\mathbb{e}}}[\\|bw_n - b\\overline{w } \\|^2 ] + { \\ensuremath{\\mathbb{e}}}\\left[{\\ensuremath{\\mathbb{e}}}[\\|{\\ensuremath{{\\mathfrak b}}}_n - bw_n\\|^2|\\mathcal{f}_n]\\right]+2{\\ensuremath{\\mathbb{e}}}\\left[{\\ensuremath{\\mathbb{e}}}[\\langle bw_n - b\\overline{w } , { \\ensuremath{{\\mathfrak b}}}_n - bw_n \\rangle|\\mathcal{f}_n]\\right ] \\notag\\\\ & \\leq { \\ensuremath{\\mathbb{e}}}[\\|bw_n - b\\overline{w } \\|^2 ] + \\sigma^2(1+\\alpha_n{\\ensuremath{\\mathbb{e}}}[\\|bw_n\\|^2])+2{\\ensuremath{\\mathbb{e}}}\\left[\\langle bw_n - b\\overline{w},{\\ensuremath{\\mathbb{e}}}[{\\ensuremath{{\\mathfrak b}}}_n - bw_n |\\mathcal{f}_n\\rangle]\\right]\\notag\\\\ & \\leq ( 1 + 2\\sigma^2\\alpha_n){\\ensuremath{\\mathbb{e}}}[\\|bw_n - b\\overline{w } \\|^2 ] + \\sigma^2(1 + 2\\alpha_n\\|b\\overline{w}\\|^2)\\notag\\\\ & \\leq    \\frac { 1 + 2\\sigma^2\\alpha_n}{\\beta}{\\ensuremath{\\mathbb{e}}}[{\\left\\langle{w_n-\\overline{w}}\\mid { bw_n - b\\overline{w } } \\right\\rangle } ]   + 2\\sigma^2(1 + 2\\alpha_n\\|b\\overline{w}\\|^2 ) .",
    "\\end{aligned}\\ ] ] recalling the definition of @xmath208 , from , , , and we get that @xmath209   & \\leq ( 1-\\lambda_n ) { \\ensuremath{\\mathbb{e}}}[\\|w_n-\\overline{w}\\|^2 ] + \\lambda_n { \\ensuremath{\\mathbb{e}}}[\\|y_n-\\overline{w}\\|^2 ] \\notag\\\\ & \\leq { \\ensuremath{\\mathbb{e}}}[\\|w_n-\\overline{w}\\|^2 ]   - \\gamma_n\\lambda_n\\bigg(2-\\frac{\\gamma_n ( 1 + 2\\sigma^2\\alpha_n)}{\\beta}\\bigg)\\cdot\\notag\\\\ & \\quad \\cdot{\\ensuremath{\\mathbb{e}}}[{\\left\\langle{w_n-\\overline{w}}\\mid { bw_n - b\\overline{w } } \\right\\rangle } ] + 2\\sigma^2\\chi^{2}_n- \\lambda_n { \\ensuremath{\\mathbb{e}}}[\\|u_n\\|^2]\\notag\\\\ & \\leq { \\ensuremath{\\mathbb{e}}}[\\|w_n-\\overline{w}\\|^2 ]   - \\varepsilon \\gamma_n\\lambda_n { \\ensuremath{\\mathbb{e}}}[{\\left\\langle{w_n-\\overline{w}}\\mid { bw_n - b\\overline{w } } \\right\\rangle } ]    + 2\\sigma^2\\chi^{2}_n -\\lambda_n { \\ensuremath{\\mathbb{e}}}[\\|u_n\\|^2].\\end{aligned}\\ ] ]    [ p:1i ] : since the sequence @xmath210 is summable by assumption @xmath119 , we derive from that @xmath211)_{n\\in{\\ensuremath{\\mathbb n}}^{*}}$ ] converges to a finite value .",
    "[ p:1iii ] : it follows from that @xmath212 < + \\infty.\\ ] ] since @xmath213 by @xmath119 , we obtain , @xmath133 = 0\\ ] ] which implies , by cocoercivity , @xmath214 = 0 $ ] .",
    "since @xmath6 is cocoercive , it is lipschitzian .",
    "therefore , by [ p:1i ] , there exists @xmath2150,+\\infty\\right[}}$ ] such that @xmath216 \\leq \\beta^{-1 }   { \\ensuremath{\\mathbb{e}}}[\\|w_n-\\overline{w}\\|^2 ] \\leq m .\\end{aligned}\\ ] ] hence , we derive from ( a4 ) and that @xmath217 < + \\infty.\\ ] ]    [ p:1ii ] it follows from that @xmath218 < + \\infty$ ] .",
    "finally , by we obtain @xmath219 \\leq 2\\sum_{n\\in{\\ensuremath{\\mathbb n } } } \\lambda_n { \\ensuremath{\\mathbb{e}}}[\\|u_n\\|^2 ] + 2\\sum_{n\\in{\\ensuremath{\\mathbb n}}^ { * } } \\lambda_n\\gamma^{2}_n   { \\ensuremath{\\mathbb{e}}}[\\|{\\ensuremath{{\\mathfrak b}}}_n - b\\overline{w}\\|^2 ] < + \\infty.\\ ] ] therefore , [ p:1ii ] is proved .    next we prove theorem  [ t:2nv ] , which is based on propositions [ p : fejer ] and [ p:1 ] .",
    "[ t:2nvi- ] let @xmath92 .",
    "reasoning as in the proof of proposition [ p:1 ] , we have @xmath220 where @xmath221 is defined as in .",
    "we next estimate the conditional expectation with respect to @xmath222 of each term in the right hand side of .",
    "since @xmath223 is @xmath222-measurable , we have @xmath224 = \\|w_n-\\overline{w}\\|^2,\\ ] ] and using condition ( a1 ) , @xmath225   & = { \\left\\langle{w_n-\\overline{w}}\\mid { { \\ensuremath{\\mathbb{e } } } [ { \\ensuremath{{\\mathfrak b}}}_n - b\\overline{w}| \\mathcal{f}_n } \\right\\rangle}\\notag\\\\ & = { \\left\\langle{w_n-\\overline{w}}\\mid { bw_n - b\\overline{w } } \\right\\rangle}. \\end{aligned}\\ ] ] noting that @xmath226 is @xmath222-measurable since @xmath223 is @xmath222-measurable and @xmath6 is continuous , and using condition ( a2 ) , we derive @xmath227= { \\ensuremath{\\mathbb{e}}}[\\|{\\ensuremath{{\\mathfrak b}}}_n - bw_n\\|^2 |\\mathcal{f}_n   ] + { \\ensuremath{\\mathbb{e}}}[\\|bw_n - b\\overline{w } \\|^2| \\mathcal{f}_n]+ { \\ensuremath{\\mathbb{e}}}[\\langle bw_n - b\\overline{w } , { \\ensuremath{{\\mathfrak b}}}_n - bw_n\\rangle |\\mathcal{f}_n ] \\notag\\\\ & \\leq \\sigma^2(1+\\alpha_n\\|bw_n\\|^2)+\\|bw_n - b\\overline{w } \\|^2 \\notag\\\\ & \\leq   \\|bw_n - b\\overline{w } \\|^2 + \\sigma^2(1 + 2\\alpha_n   \\|bw_n - b\\overline{w } \\|^2 + 2\\alpha_n\\|b\\overline{w}\\|^2 ) \\notag\\\\ & \\leq \\frac{(1 + 2\\sigma^2\\alpha_n ) } { \\beta } { \\left\\langle{w_n-\\overline{w}}\\mid { bw_n - b\\overline{w } } \\right\\rangle }   + \\sigma^2(1 + 2\\alpha_n\\|b\\overline{w}\\|^2 ) ,   \\end{aligned}\\ ] ] where the last inequality follows from the cocoercivity of @xmath6 .",
    "now , note that by convexity we have @xmath228 taking the conditional expectation and invoking , , , we obtain , @xmath229   \\leq ( 1-\\lambda_n ) \\|w_n-\\overline{w}\\|^2 + \\lambda_n { \\ensuremath{\\mathbb{e}}}[\\|y_n-\\overline{w}\\|^2|\\mathcal{f}_n ] \\notag\\\\ & \\leq \\|w_n-\\overline{w}\\|^2 - \\gamma_n\\lambda_n\\bigg(2-\\frac{\\gamma_n(1 + 2\\sigma^2\\alpha_n)}{\\beta}\\bigg){\\left\\langle{bw_n - b\\overline{w}}\\mid { w_n-\\overline{w } } \\right\\rangle } + 2\\sigma^2\\chi^{2}_n-\\lambda_n{\\ensuremath{\\mathbb{e } } } [ \\|u_n\\|^2|\\mathcal{f}_n]\\notag\\\\ & \\leq \\|w_n-\\overline{w}\\|^2 - \\varepsilon \\gamma_n\\lambda_n { \\left\\langle{bw_n - b\\overline{w}}\\mid { w_n-\\overline{w } } \\right\\rangle } +   2\\sigma^2\\chi^{2}_n-\\lambda_n{\\ensuremath{\\mathbb{e } } } [ \\|u_n\\|^2|\\mathcal{f}_n].\\end{aligned}\\ ] ] hence @xmath130 is stochastic quasi - fejr monotone with respect to the set @xmath136 , which is nonempty , closed , and convex .    [ t:2nvi ] : it follows from proposition [ p : fejer][p : fejeri ] that @xmath230 converges a.s to some integrable random variable @xmath231 .",
    "[ t:2nvii ] since @xmath6 is uniformly monotone at @xmath9 , there exists an increasing function @xmath232 vanishing only at @xmath54 such that @xmath233 and thus @xmath9 is the unique solution of problem [ inc0 ] .",
    "we derive from proposition [ p:1 ] [ p:1iii ] and that @xmath234   < \\infty,\\ ] ] and hence @xmath235 since @xmath236 is not summable by ( a4 ) , we have @xmath237 a.s",
    ". consequently , taking into account [ t:2nvi ] , there exist @xmath238 and an integrable random variable @xmath231 in @xmath0 such that @xmath239 , and , for every @xmath143 , @xmath240 and @xmath241 .",
    "let @xmath143 .",
    "then , there exists a subsequence @xmath242 such that @xmath243 , which implies that @xmath244 , and therefore @xmath245 .",
    "since @xmath246 is arbitrary in @xmath247 , the statement follows .",
    "[ t:2nviii ] : by proposition [ p:1][p:1i ] , @xmath248= 0 $ ] , and hence there exists a subsequence @xmath242 such that @xmath249= 0.\\ ] ] therefore , there exists a subsequence @xmath250 of @xmath242 such that @xmath251 thus , it follows from [ t:2nvi ] and proposition [ p : fejer][p : fejeriii ] that there exists @xmath141 such that @xmath252 and , for every @xmath143 , @xmath253 has weak cluster points and @xmath254 .",
    "fix @xmath143 and let @xmath255 be a weak cluster point of @xmath256 , then there exists a subsequence @xmath257 such that @xmath258 . since @xmath6 is weakly continuous , @xmath259 .",
    "therefore , @xmath260 , and hence @xmath261 .",
    "since @xmath6 is strictly monotone at @xmath9 , we obtain , @xmath262 .",
    "this shows that @xmath263 . defining @xmath264 by setting , for every @xmath92 ,",
    "@xmath265 the statement follows .",
    "the following lemma establishes a non asymptotic bound for numerical sequences satisfying a given recursive inequality .",
    "this is a non asymptotic version of chung s lemma ( * ? ? ?",
    "* chapter 2 , lemma 5 ) ( see also @xcite ) .",
    "[ l : ocs ] let @xmath266 be in @xmath950,1\\right]$ ] , and let @xmath173 and @xmath267 be in @xmath830,+\\infty[$ ] , let @xmath268 be the sequence defined by @xmath269 .",
    "let @xmath270 be such that @xmath271 let @xmath161 be the smallest integer such that @xmath272 , set @xmath273 , and define @xmath274 and @xmath275 as in .",
    "then , for every @xmath165 , if @xmath2760,1\\right[$ ] , @xmath277 and if @xmath278 @xmath279    note that , for every @xmath92 and for every integer @xmath280 : @xmath281 where @xmath282 is defined by",
    ". since all terms in are positive for @xmath283 , by applying the recursion @xmath284 times we have @xmath285 let us estimate the first term in the right hand side of . since @xmath286 for every @xmath287 , from , we derive @xmath288 s_{n_0 } \\exp\\big(\\dfrac{c}{1-\\alpha}(n_{0}^{1-\\alpha } - ( n+1)^{1-\\alpha } ) \\big ) & \\text{if $ 0 < \\alpha < 1$. }     \\end{cases}\\end{aligned}\\ ] ] to estimate the second term on the right hand side of , let us first consider the case @xmath289 , and let @xmath290 such that @xmath291 .",
    "we have @xmath292 hence , combining and , for @xmath2760,1\\right[$ ] we get @xmath293 we next estimate the second term in the right hand side of in the case @xmath294 .",
    "we have @xmath295 therefore , for @xmath294 , we obtain , @xmath296 which completes the proof .",
    "we are now ready to prove theorem [ t:2 ] .",
    "( theorem [ t:2 ] )    since @xmath297 , then @xmath174 is strongly monotone at @xmath9 .",
    "hence , problem has a unique solution , i.e , @xmath298 .",
    "let @xmath92 . since @xmath299 is @xmath300-strongly monotone , by ( * ? ? ?",
    "* proposition 23.11 ) @xmath198 is @xmath301-cocoercive , and then @xmath302 next , proceeding as in the proof of proposition [ p:1 ] and recalling - , we obtain @xmath303 \\leq & \\frac{1}{(1+\\gamma_n\\nu)^2 } \\left({\\ensuremath{\\mathbb{e}}}[\\|w_n-\\overline{w}\\|^2]-\\gamma_n\\left(2-\\gamma_n\\frac{1 + 2\\sigma^2\\alpha_n}{\\beta}\\right)\\cdot \\right.\\\\   \\label{eq : ymenw } & \\cdot{\\ensuremath{\\mathbb{e}}}[{\\left\\langle{w_n-\\overline{w}}\\mid { bw_n - b\\overline{w } } \\right\\rangle } ] + 2\\gamma_n^2\\sigma^2 ( 1+\\alpha_n\\|b\\overline{w}\\|^2)\\bigg).\\end{aligned}\\ ] ] since @xmath6 is strongly monotone of parameter @xmath58 at @xmath9 , @xmath304 therefore , from , recalling the definition of @xmath208 in @xmath154 , we get @xmath305 & \\leq    \\frac{\\lambda_n}{(1+\\gamma_n\\nu)^2}\\bigg((1-\\gamma_n\\mu\\epsilon){\\ensuremath{\\mathbb{e}}}[\\|w_n-\\overline{w}\\|^2 ] + 2\\sigma^2\\chi_n^2\\bigg).\\end{aligned}\\ ] ] hence , by definition of @xmath306 , @xmath307    & \\leq \\bigg(1-\\frac{\\lambda_n\\gamma_n(2\\nu +   \\gamma_{n}\\nu^2+\\mu\\epsilon)}{(1+\\gamma_n\\nu)^2}\\bigg){\\ensuremath{\\mathbb{e}}}[\\|w_n-\\overline{w}\\|^2]+    \\frac { 2\\sigma^2\\chi^{2}_n}{(1+\\gamma_n\\nu)^2}.\\end{aligned}\\ ] ] now , suppose that @xmath283 . since @xmath308 , we have @xmath309 on the other hand , @xmath310 then , putting together , , and , we get @xmath311    & \\leq ( 1-\\eta_n){\\ensuremath{\\mathbb{e}}}[\\|w_n-\\overline{w}\\|^2]+ \\tau\\eta_n^2,\\end{aligned}\\ ] ] with @xmath312 and @xmath313 .    [ t:2i]&[t:2ii ] : inequalities and follow from by applying lemma [ l : ocs ] .    [ t:2iii ]",
    "let @xmath1660,1\\right[$ ] . then yields @xmath314=o(n^{-\\theta})$ ]",
    ". then implies @xmath314=o(n^{-c})+o(n^{-\\theta}\\varphi_{c-1}(n))$ ] .",
    "if @xmath315 , it follows from that @xmath316 , and in this case @xmath314=o(n^{-c})+o(n^{-1})$ ] . if @xmath317 , then it follows again from that @xmath314=o(n^{-1})+o(n^{-1}\\log n)$ ]",
    "in this section , we study two special instances of problem [ inc0 ] , namely variational inequalities and minimization problems .",
    "moreover , for variational inequalities , we prove an additional result , showing that a suitably defined merit function @xcite goes to zero when evaluated on the iterates of the stochastic forward - backward algorithm . this merit function has been used to quantify the inaccuracy of an approximation of the solution in @xcite .",
    "[ varine ] let @xmath79 be a @xmath63-cocoercive operator , for some @xmath3190,+\\infty\\right[$ ] , let @xmath320 be a function in @xmath24 .",
    "the problem is to solve the following variational inequality @xcite @xmath321 under the assumption that has at least one solution .",
    "there is a line of research studying stochastic algorithms for variational inequalities on finite dimensional spaces .",
    "the sample average approximation has been studied e.g. in @xcite ( see also references therein ) , and a mirror proximal stochastic approximation algorithm to solve variational inequalities corresponding to a maximal monotone operator has been proposed in @xcite . a stochastic iterative proximal method has been proposed in @xcite , and almost sure convergence properties of a stochastic forward - backward splitting algorithm for solving strongly monotone variational inequalities has been studied in @xcite .    in the setting of problem [ varine ] ,",
    "let @xmath322 be a random process taking values in @xmath0 such that @xmath87   < + \\infty$ ] , and @xmath323 @xmath324 for some @xmath1580,1\\right]$ ] and for some @xmath1590,+\\infty[$ ] .",
    "let @xmath84 be a sequence in @xmath950,1\\right]$ ] , and let @xmath88 be a random variable such that @xmath74<+\\infty$ ]",
    ". set @xmath325 \\end{array}\\ ] ]      1 .",
    "[ c:2nvii ] if @xmath6 is uniformly monotone at @xmath9 , then @xmath139 a.s .",
    "[ c:2nviii ] if @xmath140 is strictly monotone at @xmath9 and weakly continuous , then there exists a subsequence @xmath327 such that @xmath328 a.s .",
    "[ c:2nviv ] if @xmath6 is strictly monotone , and either @xmath0 is finite dimensional space or @xmath6 is a bounded and linear , then there exists @xmath141 such that @xmath329 such that , for almost every @xmath143 , there exists a subsequence @xmath327 such that @xmath145 .",
    "[ c:1 ] let @xmath3300,1\\right]$ ] and let @xmath331 .",
    "assume that conditions ( a1 ) , ( a2 ) , ( a3 ) and assumption [ ass : strcon2 ] are satisfied , with @xmath156 and @xmath332 .",
    "let @xmath73 be the sequence defined by with @xmath333 for some @xmath3340,+\\infty\\right[$ ] and @xmath3350,1\\right]$ ] .",
    "set @xmath336 , @xmath337 , @xmath338 and let @xmath161 be the smallest integer such that @xmath339 then , by setting @xmath340 $ ] , the following holds        when @xmath320 is the indicator function of a non - empty , closed , convex subset @xmath342 of @xmath0 , problem [ varine ] reduces to the problem of solving a classic variational inequality @xcite , namely to find @xmath9 such that @xmath343 proximal algorithms are often used to solve this problem , see ( * ? ? ?",
    "* chapter 25 ) and references therein .",
    "when @xmath6 is accessible only through a stochastic oracle , the available methods are limited . in @xcite a smoothing sample average approximation method is analyzed when @xmath6 can be written as an expectation .",
    "an iterative tikhonov regularization method , based on an iterative projected scheme is studied in @xcite .",
    "recently , a stochastic mirror - prox algorithm has been proposed in @xcite for the case when @xmath342 is a nonempty , compact , convex subset of @xmath344 , and @xmath6 is only lipschitz continuous .",
    "we also remark that in @xcite almost sure convergence of a forward - backward splitting algorithm with respect to a noneuclidean metric is studied in a finite dimensional space , when @xmath6 is given as an expectation .",
    "note that , by ( * ? ? ?",
    "* lemma 1 ) , since cocoercivity of @xmath6 implies lipschitz continuity , @xmath9 is a solution of if and only if @xmath345 as it has been done in @xcite , it is therefore natural to quantify the inaccuracy of a candidate solution @xmath346 by the merit function @xmath347 in particular , note that @xmath348 @xmath349 and @xmath350 if and only @xmath351 is a solution of .",
    "we will consider convergence properties of the following iteration , which differs from the one in algorithm [ a : maininc ] only by the averaging step .",
    "[ a : varine ] let @xmath342 be a nonempty bounded closed convex subset of @xmath0 .",
    "let @xmath352 be a sequence in @xmath830,+\\infty[$ ] .",
    "let @xmath353 be a sequence in @xmath85 $ ] , and let @xmath354 be a @xmath0-valued random process such that @xmath87   < + \\infty$ ] .",
    "let @xmath355 be a random variable such that @xmath74<+\\infty$ ] and set @xmath356 \\overline{w}_n = \\big(\\sum_{t=1}^{n}\\gamma_t\\lambda_t w_t\\big)/\\sum_{t=1}^{n}(\\gamma_t\\lambda_t ) .",
    "\\end{array } \\end{array } \\right.\\\\[2 mm ] \\end{array}\\ ] ]      ( ergodic convergence ) [ t:7 * ] in the setting of problem , assume that @xmath359 for some nonempty bounded closed convex set @xmath342 in @xmath0 .",
    "let @xmath360 be the sequence generated by algorithm [ a : varine ] and suppose that conditions @xmath361 , @xmath362 , and @xmath154 hold .",
    "set @xmath363 \\",
    "\\text{and}\\     \\theta_{1,n } = \\frac 1 2    \\sum_{t=1}^n\\big(\\lambda_t\\gamma_{t}^2(1 + \\sigma^2\\alpha_t ) { \\ensuremath{\\mathbb{e}}}[\\|bw_t \\|^2 ]   + \\sigma^2 \\lambda_t\\gamma_{t}^2\\big),\\ ] ]",
    "then @xmath364 ) \\leq ( \\theta_0+\\theta_{1,n } ) \\bigg(\\sum_{t=1}^n \\lambda_t\\gamma_t\\bigg)^{-1}.\\ ] ] moreover , suppose that the condition @xmath119 is also satisfied .",
    "then , @xmath365)=0.\\ ] ] in particular , if @xmath366 @xmath367 and @xmath368 for some @xmath3691/2,1[$ ] , we get @xmath370 ) = o(n^{\\theta-1}).\\ ] ]    since @xmath342 is a non - empty closed convex set , @xmath371 is non - expansive , and for every @xmath372 , @xmath373 .",
    "hence , from the convexity of @xmath196 @xmath374 we derive from conditions @xmath361 that @xmath375 =     { \\left\\langle{w_t - u } \\mid { bw_t } \\right\\rangle}\\ ] ] and from ( a3 ) that @xmath376   & \\leq { \\ensuremath{\\mathbb{e}}}[\\|{\\ensuremath{{\\mathfrak b}}}_t - bw_t \\|^2| \\mathcal{f}_t ] +   { \\ensuremath{\\mathbb{e}}}[\\|bw_t\\|^2 |\\mathcal{f}_t   ] + 2{\\ensuremath{\\mathbb{e}}}[{\\left\\langle{{\\ensuremath{{\\mathfrak b}}}_t - bw_t}\\mid { bw_t } \\right\\rangle}|\\mathcal{f}_t ] \\notag\\\\ & \\leq \\|bw_t \\|^2 + \\sigma^2(1+\\alpha_t\\|bw_t\\|^2 ) .",
    "\\end{aligned}\\ ] ] therefore , and the monotonicity of @xmath6 yield @xmath377   \\hfill + \\lambda_t\\gamma_{t}^2(1 + \\sigma^2\\alpha_t)\\|bw_t\\|^2 ) + \\sigma^2\\lambda_t\\gamma_{t}^2,\\end{aligned}\\ ] ] which implies that @xmath378 & \\leq \\bigg(\\sum_{t=1}^n \\lambda_t\\gamma_t\\bigg)^{-1 }   \\sum_{t=1}^n \\bigg({\\ensuremath{\\mathbb{e}}}[\\|w_{t } -u \\|^{2 } ]   - { \\ensuremath{\\mathbb{e}}}[\\| w_{t+1 } -u \\|^{2 } ]   \\notag\\\\ & \\quad+\\hfil \\lambda_t\\gamma_{t}^2(1 + \\sigma^2\\alpha_t ) { \\ensuremath{\\mathbb{e}}}[\\|bw_t \\|^2 ]   + \\sigma^2\\lambda_t\\gamma_{t}^2 )   \\bigg)\\notag\\\\ & \\leq   \\bigg(\\sum_{t=1}^n\\lambda_t\\gamma_t\\bigg)^{-1}\\bigg ( { \\ensuremath{\\mathbb{e}}}[\\|w_{1 } -u \\|^{2}]+    \\sum_{t=1}^n\\bigg ( \\lambda_t\\gamma_{t}^2(1 + \\sigma^2\\lambda_t\\alpha_t ) { \\ensuremath{\\mathbb{e}}}[\\|bw_t \\|^2 ]   + \\sigma^2\\lambda_t\\gamma_{t}^2 ) \\bigg ) \\bigg).\\end{aligned}\\ ] ] therefore , @xmath379 \\leq ( \\theta_0+\\theta_{1,n } ) \\bigg(\\sum_{t=1}^n \\lambda_t\\gamma_t\\bigg)^{-1},\\ ] ] which proves . finally , since @xmath342 is bounded , @xmath380 .",
    "now , additionally assume that @xmath119 is satisfied .",
    "then @xmath381 , therefore , to get , it is enough to prove that @xmath382 is bounded . since we derive from @xmath119 that @xmath383 and @xmath384 , we are left to prove that @xmath385)_{t\\in{\\ensuremath{\\mathbb n}}^*}$ ] is bounded .",
    "this directly follows from proposition [ p:1][p:1i ] .",
    "the last assertion of the statement follows from when evaluated for @xmath386 and @xmath367 .    as we mentioned before , when @xmath387 is finite",
    ", @xmath342 is a non - empty convex , compact subset of @xmath0 , and @xmath6 is bounded , an alternative method to solve problem [ varine ] can be found in @xcite , where @xmath115 and the assumption of cocoercivity of @xmath6 is replaced by the weaker lipschitz continuity assumption .",
    "note that , with respect to forward - backward , the mirror - prox algorithm proposed in @xcite , requires two projections per iteration , rather than one . with such procedure , in @xcite",
    "it is proved that @xmath388 $ ] goes to zero .",
    "note that in general @xmath389)\\leq { \\ensuremath{\\mathbb{e}}}[v(\\overline{w}_n)]$ ] .      in this section ,",
    "we specialize the results in section [ sec : main ] to minimization problems . in the special case of composite minimization ,",
    "stochastic implementations of forward - backward splitting algorithms , and more generally of first order methods , received much attention and have been recently studied in several papers @xcite for the ease of implement and the low memory requirement of each iteration . in particular , @xcite proposes an accelerated method and derives a rate of convergence for the objective function values which is optimal both with respect to the smooth component and the non - smooth term .",
    "similar accelerated proximal gradient algorithms have been also studied in the machine learning community , see @xcite , and also @xcite .",
    "[ proa2 ] let @xmath780,+\\infty[$ ] , let @xmath390 , and let @xmath391 be a convex differentiable function , with a @xmath114-lipschitz continuous gradient .",
    "the problem is to @xmath392 under the assumption that the set of solution to is non - empty .    in the setting of problem [ proa2 ] ,",
    "let @xmath323 @xmath324 for some @xmath1580,1\\right]$ ] and for some @xmath1590,+\\infty[$ ] .",
    "let @xmath84 be a sequence in @xmath950,1\\right]$ ] .",
    "let @xmath322 a random process taking values in @xmath0 such that @xmath87",
    "< + \\infty$ ] , and let @xmath393 a random variable in @xmath0 such that @xmath74<+\\infty$ ] .",
    "define @xmath394 \\end{array}\\ ] ]      [ cor:1 ] let @xmath3300,1\\right]$ ] and @xmath3960,+\\infty\\right[$ ] .",
    "suppose that conditions ( a1 ) , ( a2 ) and ( a3 ) are satisfied with @xmath111 and @xmath156 .",
    "let @xmath9 be a solution of problem [ proa2 ] .",
    "suppose that @xmath320 is @xmath147-strongly convex and @xmath113 is @xmath58-strongly convex at @xmath9 for some @xmath148 such that @xmath149 .",
    "set @xmath397 , @xmath398 , @xmath399 and let @xmath161 be the smallest integer such that @xmath339 let @xmath73 be the sequence generated by algorithm [ e : ex1 ] and define , for very @xmath92 , @xmath340 $ ] .",
    "then ,        in the case when @xmath70 is the indicator function of a non empty closed convex set and @xmath404 , a similar result on the rate of convergence of @xmath146 $ ] has been obtained in @xcite , under similar assumptions to @xmath405 for the case where @xmath113 is strongly convex , under the additional assumption of boundedness of the conditional expectations of @xmath406 .",
    "corollary [ cor:1 ] is the extension to the nonsmooth case of ( * ? ? ?",
    "* theorem 1 ) , in particular , when @xmath407 , we obtain the same convergence rate .",
    "note however that the assumptions on the stochastic approximations of the gradient of the smooth part are slightly different .",
    "algorithm  [ e : ex1 ] is closely related to the fobos algorithm studied in @xcite and the stochastic proximal gradient algorithm in @xcite .",
    "the main difference is that these papers consider convergence of the average of the iterates .",
    "also uniform boundedness of the iterations and the subdifferentials are required .",
    "our convergence results consider convergence of the iterates with no averaging , without boundedness assumptions .",
    "this is relevant for sparsity based regularization , where averaging can have a detrimental effect .",
    "the asymptotic rate @xmath408 which we obtain for the iterates improves the @xmath409 rate derived from ( * ? ? ?",
    "* corollary 10 ) for the average of the iterates and it coincides with the one that can be derived by applying optimal methods @xcite .      1",
    ".   [ c2:2nvii ] if @xmath113 is uniformly convex at @xmath9 , then @xmath139 a.s .",
    "[ c2:2nviii ] if @xmath410 is strictly convex at @xmath9 and @xmath411 is weakly continuous , then there exists a subsequence @xmath327 such that @xmath328 a.s .",
    "[ c2:2nviv ] if @xmath113 is strictly convex at @xmath9 , and either @xmath0 is finite dimensional space or @xmath113 is a bounded and linear , then there exists @xmath141 with @xmath252 such that , for every @xmath143 , there exists a subsequence @xmath327 such that @xmath145 .      in the optimization setting , the study of almost sure convergence has a long history , see e.g. @xcite and references therein .",
    "recent results on almost sure convergence of projected stochastic gradient algorithm can be found in @xcite , under rather technical assumptions .",
    "our results are generalizations of the analysis of the stochastic projected subgradient algorithm in @xcite .",
    "[ ex:1 ] let @xmath63 be in @xmath950,+\\infty\\right[$ ] , let @xmath147 be in @xmath412 $ ] , assume that @xmath0 is separable , and let @xmath413 be an orthonormal base of @xmath0 .",
    "let @xmath414 be a sequence of functions in @xmath415 such that @xmath416 and set @xmath417\\colon w\\mapsto \\sum_{k\\in{\\ensuremath{\\mathbb n}}}\\big ( \\phi_k({\\left\\langle{w}\\mid { e_k } \\right\\rangle } ) + \\frac{\\nu}{2 } |{\\left\\langle{w}\\mid { e_k } \\right\\rangle}|^2 \\big).\\ ] ] let @xmath113 be in @xmath24 such that @xmath113 is differentiable , @xmath58 strongly convex for some @xmath418 , with a @xmath114-lipschitz continuous gradient .",
    "the problem is to @xmath419 in the case when @xmath420 and @xmath113 is non - strongly convex , we assume that the set of solutions to is non - empty .",
    "let @xmath86 be a @xmath0-valued random process such that , for every @xmath92 ,",
    "@xmath421   < + \\infty$ ] , let @xmath422 be a random variable such that @xmath74<+\\infty$ ] , and set @xmath423 \\end{array}\\ ] ] suppose that conditions ( a1 ) , ( a2 ) , and ( a3 ) are satisfied . then the following hold .    1 .   under the assumptions of corollary [ cor:1 ]",
    ", @xmath9 is unique . in addition , setting , for every @xmath92 , @xmath340 $ ] , if @xmath4000,1[$ ] , then holds , and if @xmath401 then holds .",
    "moreover , holds .",
    "2 .   assume that @xmath113 is uniformly convex and ( a4 ) is satisfied",
    ". then @xmath139 a.s .",
    "3 .   assume that @xmath113 is strictly convex and @xmath411 is weakly continuous , and ( a4 ) is satisfied .",
    "then there exists a subsequence @xmath424 such that @xmath328 a.s .",
    "@xmath320 is @xmath147-strongly convex by parserval s identity , and its proximity operator is computable @xcite .",
    "more precisely , it follows from ( * ? ? ?",
    "* proposition 23.29(i ) and proposition 23.34 ) that the iteration reduces to .",
    "therefore , the statement follows from corollaries [ c:1mezzo ] and [ c:1 ] .",
    "bauschke , h.h . , combettes , p.l . :",
    "convex analysis and monotone operator theory in hilbert spaces .",
    "cms books in mathematics / ouvrages de mathmatiques de la smc .",
    "springer , new york ( 2011 ) . with a foreword by hdy attouch        benveniste , a. , mtivier , m. , priouret , p. : adaptive algorithms and stochastic approximations , _ applications of mathematics ( new york ) _ , vol .",
    "springer - verlag , berlin ( 1990 ) . translated from the french by stephen s. wilson        brzis , h. : oprateurs maximaux monotones",
    "et semi - groupes de contractions dans les espaces de hilbert .",
    "north - holland publishing co. , amsterdam - london ; american elsevier publishing co. , inc .",
    ", new york ( 1973 ) .",
    "north - holland mathematics studies , no .",
    "5 . notas de matemtica ( 50 )          combettes , p.l . :",
    "quasi - fejrian analysis of some optimization algorithms . in : inherently parallel algorithms in feasibility and optimization and their applications ( haifa , 2000 ) ,",
    "_ , vol .  8 , pp . 115152 .",
    "north - holland , amsterdam ( 2001 )                                                                  robbins , h. , siegmund , d. : a convergence theorem for non negative almost supermartingales and some applications . in : optimizing methods in statistics ( proc .",
    "ohio state univ . , columbus , ohio , 1971 ) , pp .",
    "233257 . academic press , new york ( 1971 )    rockafellar , r.t . : monotone operators associated with saddle - functions and minimax problems . in : nonlinear functional analysis ( proc .",
    "pure math .",
    "xviii , part 1 , chicago , ill . , 1968 ) ,",
    "soc . , providence , r.i ."
  ],
  "abstract_text": [
    "<S> we propose and analyze the convergence of a novel stochastic forward - backward splitting algorithm for solving monotone inclusions given by the sum of a maximal monotone operator and a single - valued maximal monotone cocoercive operator . </S>",
    "<S> this latter framework has a number of interesting special cases , including variational inequalities and convex minimization problems , while stochastic approaches are practically relevant to account for perturbations in the data . </S>",
    "<S> the algorithm we propose is a stochastic extension of the classical deterministic forward - backward method , and is obtained considering the composition of the resolvent of the maximal monotone operator with a forward step based on a stochastic estimate of the single - valued operator . </S>",
    "<S> our study provides a non - asymptotic error analysis in expectation for the strongly monotone case , as well as almost sure convergence under weaker assumptions . </S>",
    "<S> the approach we consider allows to avoid averaging , a feature critical when considering methods based on sparsity , and , for minimization problems , it allows to obtain convergence rates matching those obtained by stochastic extensions of so called accelerated methods . </S>",
    "<S> stochastic quasi fejer s sequences are a key technical tool to prove almost sure convergence .     </S>",
    "<S> stochastic first order methods forward - backward splitting algorithm monotone inclusions stochastic fejr sequences +   47h05 90c15 65k10 90c25 </S>"
  ]
}