{
  "article_text": [
    "consider the problem of inferring the association between the response @xmath0 and a @xmath1-dimensional vector of covariates @xmath2 .",
    "most statistical methods perform well with a moderate size of @xmath1 in comparison with the sample size .",
    "unfortunately , we have trouble in dealing with the problem when @xmath1 gets large , which is usually the case in the biological sciences nowadays . to improve statistical analysis , a preprocess",
    "is implemented first to reduce the number of covariates and then the subsequent statistical analysis is made based on those extracted covariates .",
    "sufficient dimension reduction aims to reduce the number of covariates while preserving necessary information . specifically , it searches for a matrix @xmath3 such that @xmath4{.03em}{.67em } \\hspace{-.25em } \\rule[0em]{.65em}{.03em } \\hspace{-.25em } \\rule[0em]{.03em}{.67em}\\;\\,}x \\mid \\gamma^tx , \\label{dr}\\end{aligned}\\ ] ] where @xmath5{.03em}{.67em } \\hspace{-.25em } \\rule[0em]{.65em}{.03em } \\hspace{-.25em } \\rule[0em]{.03em}{.67em}\\;\\,}$ ] stands for statistical independence and @xmath6 .",
    "an equivalent statement is that the conditional distribution of @xmath7 and @xmath8 are the same . in other words",
    ", all the information contained in @xmath2 regarding @xmath0 can be obtained through the lower - dimensional linear transformation @xmath9 .",
    "model ( [ dr ] ) is very general without any extra specification for the conditional distribution of @xmath0 given @xmath2 .",
    "it trivially holds when @xmath10 is set to be the identity matrix and , hence , is useful only when @xmath11 is adequately small . obviously , it is @xmath12 that is of interest to us , which is called the dimension reduction subspace ( cook , 1994 ; li , 1991 ) for the regression of @xmath0 on @xmath2 . under very general conditions ,",
    "the intersection of all such dimension reduction subspaces , denoted by @xmath13 , is still a dimension reduction subspace ( cook , 1994 ) and is called the central subspace ( cs ) .",
    "we thus assume in the sequel the existence of @xmath14 with structural dimension @xmath15 .",
    "there have been many methodologies proposed to estimate @xmath13 , beginning with the development of sliced inverse regression ( sir ) of li ( 1991 ) , including sliced average variance estimation ( save ) of cook and weisberg ( 1991 ) , third - moment estimation of yin and cook ( 2003 ) , inverse regression ( ir ) of cook and ni ( 2005 ) , directional regression ( dr ) of li and wang ( 2007 ) , discretization - expectation estimation of zhu et al .",
    "( 2010 ) , among others .",
    "oftentimes , researchers are interested in the induced response @xmath16 for a known function @xmath17 instead of the original one .",
    "for example , the original response @xmath0 in the cardiac arrhythmia study is a categorical random variable with value 1 referring to normal heart rhythm and values 2 - 16 for different types of arrhythmia . in the phase of population screening , however , one would merely like to distinguish patients with arrhythmia ( @xmath18 ) from those without it ( @xmath19 ) . in this case , @xmath20 is of major interest , where @xmath21 is the indicator function . taking the angiography cohort study as another example ,",
    "researchers aim to predict a patient s 10-year vital status . in this study ,",
    "coronary artery disease ( cad)-related death time @xmath0 is the original response , and the induced response of interest is @xmath22 . a far more complicated form of @xmath17 may , instead , be of interest , depending on the nature of the study .    similar to ( [ dr ] )",
    ", there must exist for every @xmath17 a @xmath23 such that @xmath24{.03em}{.67em } \\hspace{-.25em } \\rule[0em]{.65em}{.03em } \\hspace{-.25em } \\rule[0em]{.03em}{.67em}\\;\\,}x \\mid \\gamma_g^tx , \\label{dr.c}\\end{aligned}\\ ] ] and one has the central subspace @xmath25 for the regression of @xmath26 on @xmath2 with the structural dimension @xmath27 .",
    "we must have @xmath28 since @xmath26 is a function of @xmath0 , but a more complicated inclusion structure could exist .",
    "the following three examples demonstrate various relationships between @xmath29 and @xmath30 with @xmath31 .",
    "* example  1 . *",
    "assume the conditional distribution of @xmath0 given @xmath2 is @xmath32 which satisfies ( [ dr ] ) with @xmath33 .",
    "it is easy to show that ( [ dr.c ] ) also holds with @xmath34 . in this case ,",
    "@xmath35 for every @xmath36 .",
    "* example  2 . *",
    "assume the conditional distribution of @xmath0 given @xmath2 is @xmath37 which satisfies ( [ dr ] ) with @xmath38 $ ] .",
    "provided @xmath39 , @xmath40 is a function of @xmath41 , which satisfies ( [ dr.c ] ) with @xmath42 . in this case ,",
    "@xmath43 and the direction of @xmath44 changes as @xmath36 varies .",
    "* example  3 .",
    "* let the conditional hazard function of @xmath0 given @xmath2 be of the form @xmath45 which satisfies ( [ dr ] ) with @xmath46 $ ] .",
    "moreover , @xmath40 is a function of @xmath41 , which satisfies ( [ dr.c ] ) with @xmath47 $ ] . in this case",
    ", @xmath48 and @xmath44 expands up to @xmath13 as @xmath36 increases ( i.e. , the dimension also changes ) .",
    "these examples highlight the importance of @xmath49 , because both the dimension and direction of the cs of @xmath26 may be different from the original cs , i.e. , @xmath13 may contain redundant directions if we are interested in @xmath26 only .",
    "if we simply treat @xmath50 as the observed data , any dimension reduction method can be directly applied to estimate @xmath44 . from a statistical point of view",
    ", however , @xmath0 must contain more information than @xmath26 does , therefore this direct method may suffer the problem of inefficiency .",
    "we use model  ( [ ex.1 ] ) to demonstrate the potential drawback of the direct method . set @xmath51 and generate @xmath2 from @xmath52 , where @xmath53 represents the @xmath54 identity matrix , @xmath55 and @xmath56 are @xmath57 vectors of ones and zeroes .",
    "since @xmath58 , sir is implemented to estimate @xmath59 based on @xmath60 and @xmath50 separately with @xmath61 , where @xmath62 satisfies @xmath63 .",
    "the first element of the estimates is always forced to be one since only the direction is relevant .",
    "simulation results with sample size 300 and 500 replications performed give the means and standard errors of the estimates as @xmath64 under @xmath50 , and @xmath65 under @xmath60 .",
    "although both methods can accurately estimate the true direction @xmath66 , the standard errors for sir based on @xmath50 are larger .",
    "we detect even larger biases and errors for other choices of @xmath36 , especially for @xmath36 near the boundaries .",
    "the main theme of this paper is thus to propose a more efficient estimation procedure for @xmath44 based on @xmath60 .",
    "some notation is introduced first . for a square matrix @xmath67 ,",
    "let @xmath68 be the function which maps @xmath67 into its @xmath69 leading eigenvectors . the observed data @xmath70 is a random copy of @xmath60 .",
    "following the setting of cook and ni ( 2005 ) , we may assume @xmath0 has a finite support @xmath71 .",
    "in the case of a continuous response , it can be categorized as suggested by li ( 1991 ) .",
    "let @xmath72 be the standardized version of @xmath2 , where @xmath73 $ ] and @xmath74 .",
    "owing to @xmath75 and @xmath76 , there is no difference in considering the dimension reduction problem under @xmath77-scale . in this section",
    ", we will consider the estimation of @xmath78 and @xmath79 , the basis of @xmath80 and @xmath81 , respectively , and transform back to the original scale via @xmath82 and @xmath83 . in practice , @xmath77 is replaced with @xmath84 by plugging in the usual moment estimators @xmath85 and @xmath86 .",
    "the structural dimensions @xmath11 and @xmath87 are assumed to be already known .",
    "the selection of @xmath88 will be discussed later .",
    "we start by reviewing a general estimation procedure for @xmath80 .",
    "most dimension reduction methods aim to construct a symmetric kernel matrix @xmath89 ( if @xmath89 is not symmetric , @xmath90 is used instead ) based on @xmath60 satisfying the property @xmath91 a basis of @xmath80 is then given by @xmath92 . at the sampling level , @xmath78 is estimated by @xmath93 , where @xmath94 is a sample analogue of @xmath89 .",
    "for example , sir considers @xmath95)=({{m}}-\\mu{{1}}_h^t ) d_{{f}}({{m}}-\\mu{{1}}_h^t)^t,\\label{ker.sir}\\end{aligned}\\ ] ] where @xmath96 $ ] with @xmath97 $ ] , @xmath98 with @xmath99 , and @xmath100 .",
    "a sample analogue @xmath101 is obtained by plugging the moment estimators @xmath102 , @xmath103 , @xmath85 , and @xmath86 into @xmath104 .",
    "it should be noted that property ( [ ker.m ] ) does not hold without any cost .",
    "depending on the choice of @xmath89 , different conditions are imposed to ensure its validity .",
    "inverse regression methods , such as sir , commonly assume the linearity condition ( * ( a1 ) * : @xmath105 $ ]  is a linear function of @xmath77 for any matrix @xmath67 ) , which is equivalent to assuming the ellipticity of @xmath2 ( eaton , 1986 ) .",
    "turning to the estimation of @xmath81 for any given @xmath17 , parallel to ( [ ker.m ] ) , based on @xmath50 we find the symmetric kernel matrix @xmath106 satisfying @xmath107 and the basis of @xmath81 which is of major interest is defined to be @xmath108 .",
    "the direct estimation method then substitutes an estimator @xmath109 for @xmath106 , and estimates @xmath79 by @xmath110 .",
    "similar to ( [ ker.sir ] ) , @xmath106 of sir is given by @xmath111)=({{m}}_g-\\mu{{1}}_s^t ) d_{{{f}}_g } ( { { m}}_g-\\mu{{1}}_s^t)^t,\\label{kerg.sir}\\end{aligned}\\ ] ] where @xmath112 $ ] , @xmath113 $ ] , @xmath114 , @xmath115 , and @xmath116 is the number of categories of @xmath26 .",
    "note that @xmath117 since @xmath26 is a function of @xmath0 .",
    "the sample analogue @xmath118 can be obtained by plugging the moment estimators @xmath119 , @xmath120 , @xmath85 , and @xmath86 into @xmath121 .",
    "we have seen in the end of section  [ sec.intro ] that direct estimation based on @xmath50 may lose information , and we attempt to propose a more efficient estimation procedure . first observe that under the validity of ( [ ker.m ] ) and ( [ ker.mg ] ) , we must have @xmath122 where @xmath123 is the orthogonal projection matrix onto @xmath124 . although ( [ relationship ] ) is straightforward",
    ", it motivates us to estimate @xmath106 by @xmath125 , where @xmath126 is an estimate of @xmath127 .",
    "it is the projection @xmath128 that utilizes the extra information in @xmath60 , and results in an expected gain in efficiency .",
    "details of the procedure are listed below :    * based on @xmath60 , apply a dimension reduction method to obtain @xmath94 and , hence , @xmath128 . *",
    "based on @xmath50 , apply a dimension reduction method to obtain @xmath109 .",
    "* estimate @xmath79 by @xmath129 .",
    "with @xmath130 obtained , we then estimate a basis of @xmath30 , say @xmath131 , by @xmath132 .",
    "the @xmath133-consistency of @xmath134 is a direct consequence provided @xmath94 and @xmath135 are also @xmath133-consistent .",
    "we call the two - stage estimation procedure `` a - b '' hereafter , if method a is used in step  1 and method b in step  2 .",
    "as sir is the most widely applied dimension reduction method , the following theorem , which guarantees that sir - sir is more efficient than sir , highlights the desirability of using our two - stage estimation procedure .",
    "we use `` acov '' to denote the asymptotic covariance , and @xmath136 to indicate @xmath67 is positive semi - definite . the proof is deferred to the appendix",
    ".    [ thm.1 ] let @xmath134 be obtained from sir - sir , and let @xmath137 be the direct estimate of @xmath131 from sir .",
    "in addition to the linearity condition * ( a1 ) * above , assume the validity of : @xmath138 is non - random for any @xmath139 .",
    "then , @xmath140 the equality holds if and only if @xmath141 , where @xmath104 and @xmath121 are defined in ( [ ker.sir ] ) and ( [ kerg.sir ] ) .    in the establishment of theorem  [ thm.1 ] , in addition to the linearity condition we require @xmath138 to be non - random for any @xmath142 in the complement of @xmath143 .",
    "these conditions are not that restrictive and can be generally satisfied . as argued by li and wang ( 2007 ) ,",
    "* ( a1)-(a2 ) * are shown to approximately hold when @xmath1 is large . moreover , * ( a2 ) * is valid when @xmath2 is normally distributed . although normality is a stronger condition , it can be approximated by making a power transformation of @xmath2 . one implication of theorem  [ thm.1 ] is that the total asymptotic variance of @xmath144 is strictly larger than that of @xmath134 provided @xmath145 .",
    "the only possibility of no efficiency gain ( i.e. , @xmath146 ) is when @xmath147 and @xmath148 have no common element except the zero point .",
    "this is reasonable since , under this situation , all the information about @xmath149 contained in @xmath104 resides in @xmath121 and knowing the `` residual '' @xmath150 contributes nothing to the construction of @xmath149 .",
    "hence , we will gain nothing from sir - sir .",
    "a formal test for this condition is beyond the scope of this article and will be investigated in a future study . in summary , sir - sir is expected to perform well in most of the situations except the rather restrictive special case .",
    "this fact is also demonstrated by our simulation studies in section  4 , where the efficiency gain of the two - stage method is obviously detected .    the structural dimensions @xmath11 and @xmath87",
    "should be determined before practical implementation . to estimate @xmath11 , most methods rely on a sequence of hypothesis tests ( li , 1991 ; cook and lee , 1999 , cook and yin , 2001 ) .",
    "these methods , however , may not be readily applicable for the selection of @xmath87 . to simplify the estimation procedure , we alternatively suggest two approaches to select @xmath88 .",
    "one is to adopt the maximal eigenvalue ratio criterion ( merc ) proposed by luo , wang , and tsai ( 2009 ) .",
    "let @xmath151 be the eigenvalue of @xmath94 and define @xmath152 for @xmath153 .",
    "it is proposed to select @xmath11 by @xmath154 , where @xmath155 is a pre - specified constant .",
    "the authors suggest using @xmath156 in practice .",
    "once @xmath157 is obtained , we can estimate @xmath87 by a similar procedure .",
    "let @xmath158 be the eigenvalue of @xmath125 and define @xmath159 for @xmath160 .",
    "then @xmath87 is determined by @xmath161 . as to the second method , note that the purpose of dimension reduction is to improve regression or classification .",
    "thus , it is natural to select @xmath88 so that a measure of classification accuracy is maximized . in section  [ sec.data ] below",
    ", the classification accuracy obtained from cross - validation is used in the cardiac arrhythmia study , while the auc ( area under the receiver operating characteristic ( roc ) curve ) is considered in the angiography cohort study to select @xmath88 .",
    "[ rmk.save ] in our two motivating examples , @xmath31 is binary and , hence , due to its nature , sir can capture at most one direction of @xmath81 .",
    "alternatively , we can adopt save in step  2 . cook and lee ( 1999 )",
    "showed that for a binary response , save is more comprehensive than sir .",
    "the kernel matrix of save is @xmath162\\end{aligned}\\ ] ] with @xmath163 $ ] and @xmath164 , @xmath165 .",
    "its sample analogue @xmath166 is obtained by plugging moment estimators @xmath167 , @xmath168 , @xmath169 , @xmath170 , and @xmath171 into ( [ save ] ) .",
    "dimension reduction is usually applied in the field of life science when the response of interest @xmath0 represents the survival time of a subject . an important issue in survival analysis",
    "is that the response may be censored . the exact survival time @xmath0 ( and hence @xmath26 ) may not always be observed and we can only observe @xmath172 instead , where @xmath173 is the last observed time , @xmath174 is the censoring status , and @xmath175 is the censoring time . motivated from two data examples in section  1 , our aim here is to modify sir - save to estimate @xmath44 with the specific choice @xmath31 under the validity of totally independent censorship @xmath176{.03em}{.67em } \\hspace{-.25em } \\rule[0em]{.65em}{.03em } \\hspace{-.25em } \\rule[0em]{.03em}{.67em}\\;\\,}(y , x)$ ] .",
    "the modified sir - sir will also be illustrated .",
    "we note that totally independent censorship is satisfied in the angiography cohort study , since most of the patients are subject to type - i censoring .",
    "both sir and save in steps  1 - 2 should therefore be modified .",
    "for sir in step  1 , observe that @xmath177 , where the first inclusion property holds since @xmath178 is a function of @xmath179 , and the last equality is true by the totally independent censorship assumption . thus , we suggest using the modified kernel matrix @xmath180)=({{m}}^*-\\mu{{1}}_{h_0+h_1}^t ) d_{{{f}}^ * } ( { { m}}^*-\\mu{{1}}_{h_0+h_1}^t)^t , \\label{c.ker.sir}\\end{aligned}\\ ] ] where @xmath181 $ ] with @xmath182 $ ] , @xmath183 with @xmath184 , and @xmath185 and @xmath186 denote the number of categories of @xmath187 when @xmath188 and @xmath189 . here the slice means , the @xmath190 s ,",
    "are formed within those patients with @xmath188 and @xmath189 separately . by plugging in moment estimators @xmath191 , @xmath192 , @xmath85 , and @xmath171 ,",
    "the sample analogue @xmath193 is obtained .",
    "this double slicing procedure was originally proposed by li , wang , and chen ( 1999 ) , and our point is to emphasize its validity under totally independent censorship .    with regard to implementing save in step  2 , we can still use the kernel matrix @xmath194 in ( [ save ] ) provided it can be estimated based on @xmath172 .",
    "first observe that @xmath195=-\\frac{\\int u^{\\otimes i } ds_{xy}(u , t)}{s_{xy}(-\\infty , t)},~i=1,2 , \\label{mo.0}\\ ] ] @xmath196=-\\frac{\\int u^{\\otimes i } d\\{s_{xy}(u,-\\infty)-s_{xy}(u , t)\\}}{1-s_{xy}(-\\infty , t)},~i=1,2 , \\label{mo.1}\\ ] ] where @xmath197 and @xmath198 for a vector @xmath69 , and @xmath199 . here",
    "`` @xmath200 '' is interpreted as component - wise for a vector .",
    "it implies the @xmath201 s and @xmath202 s in ( [ save ] ) are functionals of @xmath203 .",
    "campbell ( 1981 ) and burke ( 1988 ) have separately proposed two different estimators of @xmath203 , denoted by @xmath204 and @xmath205 . by plugging @xmath206 into ( [ mo.0 ] ) and",
    "@xmath207 into ( [ mo.1 ] ) , we can estimate @xmath201 s and @xmath202 s by @xmath208 where @xmath209 and @xmath210 are kaplan - meier estimators of @xmath211 and @xmath212 .",
    "finally , a modified estimator of @xmath194 is given by @xmath213.\\end{aligned}\\ ] ] the modified sir - save is then proposed by using @xmath193 and @xmath214 in steps  1 - 2 .",
    "for binary @xmath26 , cook and lee ( 1999 ) showed that the population kernel matrix of sir can be expressed as @xmath215 .",
    "the modified sir - sir is then proposed by using @xmath216 in step  2 .",
    "we use models  ( [ ex.2])-([ex.3 ] ) to evaluate the performance of our two - stage estimation procedure under different combinations of sample sizes @xmath217 , number of covariates @xmath218 , and censoring rates ( @xmath219 ) . with censored data ,",
    "the modified procedure is implemented instead . to measure the closeness of two spaces with basis @xmath67 and @xmath220",
    ", we adopt the frobenius norm @xmath221 , where @xmath222 is the orthogonal projection matrix onto @xmath223 .",
    "simulations are repeated 500 times .    for model  ( [ ex.2 ] ) , set @xmath224 and @xmath225 .",
    "we independently generate @xmath226 and @xmath227 from @xmath228 and beta@xmath229 , and define @xmath230 with @xmath231 and @xmath232 .",
    "this ensures the ellipticity of @xmath2 . for the censored case ,",
    "@xmath175 is generated from gamma@xmath233 so that cr@xmath234 .",
    "both sir - sir and sir are implemented at @xmath235 , @xmath236 , and @xmath237 .",
    "as for the case of model  ( [ ex.3 ] ) , we set @xmath238 , @xmath239 , @xmath240 , and @xmath241 , generate @xmath2 from @xmath242 with @xmath243 , and generate @xmath175 from gamma(1,8 ) to produce cr@xmath234 .",
    "we implement sir - save and save at @xmath244 , @xmath245 , and @xmath246 so that @xmath247 , 2 , and 3 .",
    "various choices of the slicing number were examined and produced a similar result .",
    "we thus use @xmath248 for sir - sir and sir - save , and @xmath249 for the modified methods .",
    "simulation results are provided in table  1 .",
    "compared with the standard setting @xmath250 , an overall observation is that sir - sir and sir - save outperform sir and save , even for the cases of smaller sample size @xmath251 , of more `` noise '' covariates @xmath252 , and of censored response ( cr@xmath234 ) .",
    "the magnitude of efficiency gain from sir - sir is roughly the same for every @xmath36 in model  ( [ ex.2 ] ) .",
    "interestingly , the efficiency gain from sir - save in model  ( [ ex.3 ] ) becomes greater for larger @xmath36 .",
    "one reason is that the structural dimension of @xmath30 also increases as @xmath36 does . with more directions needing to be estimated ,",
    "more information is required to recover @xmath30 , and we gain more from the two - stage estimation procedure . it has been found empirically that save is less efficient than sir .",
    "li and zhu ( 2007 ) showed that save will not attain @xmath133-consistency in general , while sir will , even if the number of samples in each slice is only 2 . by combining sir and",
    "save , we expect an efficiency gain from sir - save as shown in this simulation .",
    "detailed description of the data can be found in lee et al .",
    "( 2006 ) . briefly speaking , for each of 1050 traceable patients ,",
    "four biomarkers ( crp , saa , il-6 , and thcy ) and the cad - related time of death were recorded with the aim of using the combined biomarkers to accurately predict a patient s @xmath36-year vital status , and thus the induced response of interest is @xmath31 .",
    "hung and chiang ( 2010 ) analyzed this data , combining biomarkers via the extended generalized linear model ( eglm ) : @xmath253 , where @xmath254 is a @xmath255 time - varying coefficient vector and @xmath256 is an unknown link function which is monotone increasing in its two arguments . under eglm , @xmath257 is promised to be optimal in distinguishing @xmath258 from @xmath259 , in the sense that the time - dependent roc curve ( heagerty , lumley , and pepe , 2000 ) is the highest among all functions of @xmath2 .",
    "the eglm also satisfies ( [ dr.c ] ) with @xmath31 , @xmath260 , and @xmath247 .",
    "thus , @xmath41 is also the optimal biomarker since any monotone transformation of @xmath257 will have the same time - dependent roc curve . given that a censoring mechanism is involved in this study , the modified sir - sir is applied to obtain @xmath261 in order to combine the biomarkers .",
    "we enter the transformed biomarker @xmath262 to perform our analysis .",
    "the analysis results with @xmath263 and @xmath264 are found in table  2 .",
    "we remind the reader that the choice of these tuning parameters attains the maximum of the time - dependent auc as mentioned in section  [ sec.complete ] .",
    "the absolute coefficient of crp is smallest at the beginning and increases as time goes by .",
    "saa has a totally different behavior , where it has a larger effect initially but seems to be diminishing at 3500 days .",
    "both il-6 and thcy are found to play important roles in predicting patient s vital status over time .",
    "interestingly , crp has a reverse effect as compared with the other three biomarkers .",
    "table  2 provides the time - dependent auc of the composite biomarkers @xmath265 at day @xmath36 , denoted by @xmath266 ( see equation ( 8) of chiang and hung , 2010 ) .",
    "the larger the @xmath266 values , the higher prediction power @xmath265 has .",
    "one can see that most of the @xmath266 values are greater than 0.7 , especially at the beginning of the study .",
    "we also calculated @xmath267 values , the maximal time - dependent auc of the method developed in hung and chiang ( 2010 ) , and a similar pattern to that of the @xmath266 values was detected ( note that @xmath268 will always hold for every @xmath36 ) . in summary ,",
    "sir - sir is easy to implement and achieves acceptable auc values .",
    "the study consisted of 452 patients , each with 279 covariates .",
    "the response @xmath269 is a categorical random variable , where 1 refers to `` normal '' and 2 - 16 refer to different classes of arrhythmia .",
    "see gvenir et al .",
    "( 1997 ) for details .",
    "to keep matters simple , we consider continuous predictors only and use their first 100 principal components in our analysis .",
    "we are interested in distinguishing normal patients @xmath270 from abnormal ones @xmath271 , i.e. , @xmath272 .",
    "the scatterplots of the extracted predictors ( denoted by ss1,@xmath273,ss5 ) from sir - save with @xmath274 are provided in figure  [ fg ] .",
    "again , the selection of @xmath88 is such that the averaged classification accuracy from cross - validation is maximized .",
    "it can be seen that ss1-ss3 demonstrate their ability to separate two groups via variation , while ss4-ss5 attempt to separate two groups via location . in every subplot",
    ", the normal group seems to have smaller variation and locates in the center of a relatively large data cloud of the abnormal group .",
    "the bottom - left 10 subplots of figure  [ fg ] are scatterplots of those extracted predictors taken from save directly .",
    "it can be seen that there is only a separation pattern of variation between the two groups , but no obvious location difference . to further evaluate the performance of those extracted predictors , we randomly separate the data into a training set ( @xmath275 ) and a test set ( @xmath276 ) , and then implement quadratic discriminant analysis based on those extracted predictors .",
    "the procedure with 200 replications gives sir - save the averaged classification accuracy of @xmath277 , while it is a mere @xmath278 for save .",
    "although we have considered univariate responses only , there is nothing different about carrying out the procedure with multivariate responses , except that the kernel matrices @xmath94 and @xmath135 are constructed for multivariate responses @xmath0 and @xmath279 .",
    "a multivariate response version of theorem  [ thm.1 ] can be derived with a proof analogous to the proof of the univariate case .",
    "we refer to li , wen , and zhu ( 2008 ) for some recent developments in dimension reduction with multivariate responses .",
    "we note that the proposed two - stage estimation procedure is a general framework , and is not limited to any specific method . depending on the purpose of a given study",
    ", we may adopt any dimension reduction technique in either steps  1 or 2 of the procedure .",
    "besides sir - sir and sir - save , we also tested various combinations of sir , save , ir , and dr .",
    "simulation results ( not shown here ) all convey the same message that an efficiency gain is significantly detected , which provides evidence that the superiority of the two - stage procedure comes mainly from using @xmath125 , and is not limited to any specific choice of dimension reduction method .",
    ": :    anderson , w. n. , jr . and duffin , r. j. ( 1969 ) .",
    "series and parallel    addition of matrices .",
    "_ j. math .",
    "* 26 * , 576 - 594 .",
    ": :    burke , m. d. ( 1988 ) .",
    "estimation of a bivariate distribution function    under random censorship .",
    "_ biometrika _ * 75 * , 379 - 382 . : :    campbell , g. ( 1981 ) .",
    "nonparametric bivariate estimation with randomly    censored data",
    ". _ biometrika _ * 68 * , 417 - 423 .",
    ": :    chiang , c. t. and hung , h. ( 2010 ) .",
    "nonparametric estimation for    time - dependent auc",
    ". _ j. stat . plan .",
    "_ * 140 * , 1162 - 1174 . : :    cook , r. d. and weisberg , s. ( 1991 ) .",
    "discussion of `` sliced inverse    regression for dimension reduction '' .",
    "assoc . _ * 86 * ,    328 - 332 .",
    ": :    cook , r. d. ( 1994 ) . on the interpretation of regression plots .",
    "_ j. am .",
    "* 89 * , 177 - 189 .",
    ": :    cook , r. d. and lee , h. ( 1999 ) .",
    "dimension reduction in binary response    regression .",
    "assoc . _ * 94 * , 1187 - 1200 . : :    cook , r. d. yin , x. ( 2001 ) .",
    "dimension reduction and visualization in    discriminant analysis ( with discussion ) .",
    "j. stat . _",
    "* 43 * ,    147 - 199 .",
    ": :    cook , r. d. and ni , l. ( 2005 ) .",
    "sufficient dimension reduction via    inverse regression : a minimum discrepancy approach .",
    "_ j. am . stat .",
    "assoc . _ * 100 * , 410 - 427 .",
    ": :    eaton , m. l. ( 1986 ) . a characterization of spherical distributions .    _",
    "j. multivariate anal . _ * 20 * , 272 - 276 .",
    ": :    gvenir , h. a. , acar , b. , demirz , g , and ekin , a. ( 1997 ) .",
    "a    supervised machine learning algorithm for arrhythmian analysis .    _ computers in cardiology _ * 24 * , 433 - 436 .",
    ": :    heagerty , p. j. , lumley , t. and pepe , m. ( 2000 ) . time - dependent roc    curves for censored survival data and a diagnostic marker .",
    "_ biometrics _ * 56 * , 337 - 344 .",
    ": :    hung , h. and chiang , c. t. ( 2010 ) . optimal composite markers for    time - dependent receiver operating characteristic curves with censored    survival data .",
    "j. stat . _",
    "* 37 * , 664 - 679 .",
    ": :    lee , k. w. j. , hill , j. s. , walley , k. r. , and frohlich , j. j. ( 2006 ) .",
    "relative value of multiple plasma biomarkers as risk factors for    coronary artery disease and death in an angiography cohort .",
    "_ canadian    medical association journal _ * 174 * , 461 - 466 .",
    ": :    li , b. and wang , s. ( 2007 ) . on directional regression for dimension    reduction .",
    "assoc . _ * 102 * , 997 - 1008 .",
    ": :    li , b. , wen , s. and zhu , l. ( 2008 ) . on a projective resampling method    for dimension reduction with multivariate responses .",
    "assoc . _ * 103 * , 1177 - 1186 .",
    ": :    li , k. c. ( 1991 ) . sliced inverse regression for dimension reduction    ( with discussion ) .",
    "_ * 86 * , 316 - 342 . : :    li , k. c. , wang , j. l. , and chen , c. h. ( 1999 ) .",
    "dimension reduction    for censored regression data .",
    "* 27 * , 1 - 23 .",
    ": :    li , y. x. and zhu , l. x. ( 2007 ) .",
    "asymptotics for sliced average    variance estimation .",
    "_ * 35 * , 41 - 69 .",
    ": :    luo , r. , wang , h. , and tsai , c. l. ( 2009 ) .",
    "contour projected dimension    reduction . _",
    "stat . _ * 37 * , 3743 - 3778 .",
    ": :    saracco , j. ( 1997 ) .",
    "an asymptotic theory for sliced inverse    regression .",
    "m. _ * 26 * , 2141 - 2171 .",
    ": :    tyler , d. e. ( 1981 ) .",
    "asymptotic inference for eigenvectors .",
    "_ * 9 * , 725 - 736 . : :    yin , x. and cook , r. d. ( 2003 ) . estimating the central subspaces via    inverse third moments .",
    "_ biometrika _ * 90 * , 113 - 125 .",
    ": :    zhu , l. , wang , t. , zhu , l. , and ferr , l. ( 2010 ) .",
    "sufficient dimension    reduction through discretization - expectation estimation .",
    "_ biometrika _    * 97 * , 295 - 304 .",
    "let @xmath280 $ ] , @xmath281 $ ] , @xmath282 with @xmath283 , @xmath284 with @xmath285 , @xmath286={{f}}$ ] , and @xmath287={{f}}_g$ ] .",
    "there must exist a code matrix @xmath288 $ ] with @xmath289 containing only zeros and ones such that @xmath290 .",
    "we may assume @xmath291 without loss of generality and , hence , @xmath292 and @xmath293 .",
    "from the definitions of @xmath104 and @xmath121 , we have @xmath294 and @xmath295 .",
    "similarly , @xmath296 , @xmath297 , and @xmath298 , where @xmath299 is an estimator of @xmath300 which is the projection matrix onto @xmath12 relative to the @xmath301-inner product .    by @xmath302 and delta method",
    ", it suffices to show @xmath303 where @xmath304 and @xmath305 .",
    "we first derive the weak convergence of @xmath306 .",
    "let @xmath307 .",
    "one has @xmath308)=[h_1,h_2,h_3]$ ] by lemma  4.1 of tyler ( 1981 ) , where @xmath309 , @xmath310 , @xmath311 , @xmath312 is the kronecker product , @xmath313 is the commutation matrix with @xmath314 being a @xmath315 matrix with a one in the @xmath316 position and zeroes elsewhere , @xmath317 is the moore - penrose inverse of @xmath318 , and @xmath319 . from lemma  1 below and delta method , @xmath320 converges weakly to @xmath321 , where @xmath322 is defined in lemma  1 . as to the weak convergence of @xmath323 ,",
    "define @xmath324 and its differential with respect to @xmath325 $ ] is calculated to be @xmath326 $ ] with @xmath327 .",
    "a similar technique gives @xmath328 which converges weakly to @xmath329 .",
    "the difference of the asymptotic covariance matrices is @xmath330 with @xmath331 , @xmath332 , and @xmath333 .",
    "it is shown in lemma  2 that @xmath334 .",
    "moreover , lemma  3 implies @xmath335 .",
    "hence , @xmath336 and we are left to show @xmath337 . by lemma  3 and @xmath338 , @xmath339 since @xmath340 and is not a zero matrix , it remains to show @xmath341 .",
    "let @xmath342 . since @xmath26 is a function of @xmath0 , @xmath343={{\\rm e}}\\{{{\\rm e}}[x\\mid y]\\mid y_g\\}$ ] and , hence , @xmath344\\mid y_g)]\\geq 0 $ ] .",
    "it further implies @xmath345 . by lemma  4 of anderson and duffin ( 1969 )",
    ", we have @xmath346 which proves @xmath347 .",
    "the equality holds if and only if @xmath348 , if and only if @xmath349 by lemma  3 of anderson and duffin ( 1969 ) , if and only if @xmath141 .    * lemma  1 . * as @xmath350 goes to infinity , @xmath351-[m , m_g,\\sigma])\\stackrel{d}{\\rightarrow}n(0,w)\\label{lm.1}$ ] , where the asymptotic covariance matrix @xmath352 $ ] , @xmath353 , is defined in the proof .",
    "the limiting distributions of sample covariance matrix are the same no matter we know the true mean @xmath291 or not .",
    "thus , we consider @xmath354 and adopt a similar strategy of saracco ( 1997 ) to complete the proof .",
    "let @xmath355 , @xmath356=\\mu_{{u}}$ ] , @xmath357 , and @xmath358 with @xmath359 s being random copies of @xmath360 . by the central limit theorem",
    "we have @xmath361 .",
    "consider @xmath362 which maps @xmath363 to @xmath364)$ ] for @xmath365 , @xmath366 , @xmath367 , @xmath368 , @xmath369 , and @xmath370 . by delta method",
    ", we deduce that @xmath371-[m , m_g,\\sigma])=n^{1/2}(f_0(\\bar{{u}})-f_0(\\mu_{{u}}))$ ] converges weakly to @xmath372 with @xmath373 , where @xmath374 is the differential of @xmath362 at @xmath375 .",
    "a direct calculation then gives @xmath376 , @xmath377 , @xmath378 , @xmath379 , @xmath380 $ ] , and @xmath381)$ ] , where @xmath382 $ ] , @xmath383 , @xmath384 , @xmath385 , @xmath386 $ ] , @xmath387 , @xmath388 $ ] , and @xmath389 .      from @xmath338 and @xmath390 , we have @xmath391 with @xmath392 and it suffices to show @xmath393 . from @xmath394 and * ( a1 ) * , we have @xmath395(p\\otimes i_p)={{\\rm e}}\\{(x^tp)\\otimes{{\\rm cov}}(q^tx\\mid \\gamma^tx)\\mid y = i\\}=(m_i^{t}p ) \\otimes ( q^t\\sigma q)$ ] by lemma  4 .",
    "it further implies @xmath396 . substituting this into @xmath397 and using @xmath398 to conclude @xmath393 .          from * ( a1 ) * , @xmath404 for some positive function @xmath405 .",
    "also , @xmath406+{{\\rm",
    "cov}}({{\\rm e}}[x\\mid \\gamma^tx])$ ] implies @xmath407 $ ] .",
    "these two facts gives @xmath408=1 $ ] .",
    "note that @xmath390 implies @xmath409 and , hence , @xmath410 is non - random by * ( a2)*. hence , we must have @xmath411 which completes the proof .            the scatter plot matrix of extracted predictors from sir - save ( upper triangular panel ) and save ( lower triangular panel ) with @xmath274 .",
    "the green pluses and black dots indicate the normal and abnormal patients.,height=476 ]"
  ],
  "abstract_text": [
    "<S> researchers in the biological sciences nowadays often encounter the curse of high - dimensionality , which many previously developed statistical models fail to overcome . to tackle this problem , sufficient dimension reduction aims to estimate the central subspace ( cs ) , in which all the necessary information supplied by the covariates regarding the response of interest is contained . </S>",
    "<S> subsequent statistical analysis can then be made in a lower - dimensional space while preserving relevant information . </S>",
    "<S> oftentimes studies are interested in a certain transformation of the response ( the induced response ) , instead of the original one , whose corresponding cs may vary . </S>",
    "<S> when estimating the cs of the induced response , existing dimension reduction methods may , however , suffer the problem of inefficiency . in this article , we propose a more efficient two - stage estimation procedure to estimate the cs of an induced response . </S>",
    "<S> this approach is further extended to the case of censored responses . </S>",
    "<S> an application for combining multiple biomarkers is also illustrated . </S>",
    "<S> simulation studies and two data examples provide further evidence of the usefulness of the proposed method . </S>",
    "<S> + key words : asymptotic efficiency , censoring , central subspace , classification , composite biomarker , sufficient dimension reduction , save , sir , survival .    </S>",
    "<S> = 0.3377 in </S>"
  ]
}