{
  "article_text": [
    "robust estimation of multivariate location and scatter for a distribution @xmath4 on @xmath5 is a recurring topic in statistics . for instance",
    ", different estimators of multivariate scatter are an important ingredient for independent component analysis ( ica ) or invariant coordinate selection ( ics ) , see nordhausen et al .",
    "@xcite and tyler et al .",
    "@xcite and the references therein . of particular interest",
    "are @xmath0-estimators and their symmetrized versions as defined in sections  [ subsec : scatteronly ] and [ subsec : symmetrizedm ] , respectively , because they offer a good compromise between robustness and computational feasibility .",
    "the most popular algorithm to compute @xmath0-estimators of multivariate scatter is to iterate a fixed - point equation , see huber  @xcite ( section 8.11 ) , tyler  @xcite and kent and tyler  @xcite .",
    "this algorithm has nice properties such as guaranteed convergence for any starting point .",
    "however , as discussed later , it can be rather slow for high dimensions and large data sets .",
    "we introduce two alternative methods , a gradient descent method with approximately optimal stepsize and a partial newton - raphson method , which turn out to be substantially faster .",
    "computation time becomes a major issue in connection with symmetrized @xmath0-estimators .",
    "these estimators are important because of a desirable `` block independence property '' as explained in section  [ subsec : symmetrizedm ] ; see also dmbgen  @xcite and sirki et al .",
    "if applied to a sample of @xmath6 observations @xmath7 , symmetrized @xmath0-estimators utilize the empirical distribution of all @xmath8 differences @xmath9 , @xmath10 .",
    "in section  [ sec : m - functional ] we describe briefly the various @xmath0-estimators we are interested in .",
    "then we introduce a general target functional on the space of symmetric and positive definite matrices in @xmath11 which has to be minimized .",
    "section  [ sec : analysis ] presents some analytical properties of the latter functional which are essential to understand existing algorithms and to devise new ones .",
    "these parts follow closely a recent survey of multivariate @xmath0-functionals by dmbgen et al .  @xcite . in section",
    "[ sec : algorithms ] we discuss the aforementioned fixed - point algorithm of kent and tyler  @xcite and explain rigorously why it is suboptimal .",
    "then we introduce two alternative methods , a gradient descent method with approximately optimal stepsize and a partial newton - raphson method .",
    "numerical experiments in section  [ sec : examples ] show that the new algorithms are substantially faster than the fixed - point algorithms or the algorithms by arslan et al .",
    "proofs are deferred to section  [ sec : proofs ] .",
    "[ [ some - notation . ] ] some notation .",
    "+ + + + + + + + + + + + + +    the space of symmetric matrices in @xmath11 is denoted by @xmath12 , and @xmath13 stands for its subset of positive definite matrices .",
    "the identity matrix in @xmath14 is written as @xmath15 .",
    "the euclidean norm of a vector @xmath16 is denoted by @xmath17 . for matrices",
    "@xmath18 with identical dimensions we write @xmath19 so @xmath20 is the frobenius norm of @xmath0 .",
    "let @xmath21 be independent random vectors with unknown distribution @xmath4 on @xmath5 .",
    "our task is to define and then estimate a certain center @xmath22 and scatter matrix @xmath23 .",
    "let us start with the assumption that @xmath24 . to define and estimate a scatter functional @xmath25 we consider a simple working model consisting of elliptically symmetric probability densities @xmath26 on @xmath5 depending on a parameter @xmath27 : @xmath28 where @xmath29 is a given function such that @xmath30 is finite .",
    "assuming temporarily that this working model is correct , one could estimate the true underlying matrix parameter by a maximizer of the corresponding log - likelihood function for this model , @xmath31 with the empirical distribution @xmath32 of the data @xmath21 , the log - likelihood at @xmath33 may be written as @xmath34 .",
    "thus maximization of the log - likelihood function over @xmath13 is equivalent to minimization of @xmath35 , where @xmath36 \\ , q(dx )          + \\log \\det(\\sigma)\\end{aligned}\\ ] ] for a generic distribution @xmath37 on @xmath5 .",
    "we include @xmath38 and @xmath39 , respectively , because often this increases the range of distributions @xmath37 such that @xmath40 is well - defined in @xmath41 .",
    "if @xmath42 has a unique maximizer over @xmath13 , we denote it with @xmath43 .",
    "the resulting mapping @xmath44 is called an @xmath0-functional of scatter . in particular",
    ", @xmath45 serves as an estimator of the scatter parameter @xmath25 , assuming that both exist . if @xmath4 happens to have a density @xmath46 in our working model , then @xmath47 .",
    "if @xmath4 is merely elliptically symmetric with center @xmath48 and scatter matrix @xmath49 , for instance , if it has a density @xmath50 of the form @xmath51 with @xmath52 , then at least @xmath53 for some @xmath54 .    an important example are multivariate @xmath55 distributions with @xmath56 degress of freedom . here",
    "@xmath57 with @xmath58 note that @xmath59 equals @xmath60 , a bounded and smooth function of @xmath61 .",
    "now our working model consists of probability densities @xmath62 on @xmath5 with parameters @xmath63 and @xmath27 , namely , @xmath64 here @xmath65 is defined as the minimizer of @xmath66 , where @xmath67 stands for @xmath4 or @xmath68 .",
    "but now we utilize a trick of kent and tyler  @xcite to get back to a scatter - only problem : with @xmath69 we may write @xmath70 and @xmath71 hence @xmath66 equals @xmath72          \\ , q(dy )          + \\log\\det(\\gamma)\\ ] ] with @xmath73 , where @xmath74 . consequently ,",
    "if @xmath75 minimizes @xmath42 under the constraint @xmath76 then we may write @xmath77 and @xmath65 solves the original minimization problem . the mappings @xmath78 and @xmath79 are called @xmath0-functional of location and @xmath0-functional of scatter , respectively .    in the special case of @xmath57 with @xmath80",
    "we have the identity @xmath81 where we define @xmath82 in case of @xmath83 one can show that any minimizer @xmath84 of @xmath42 does satisfy the equation @xmath85 , see @xcite and @xcite . in case of @xmath86 , which corresponds to multivariate cauchy distributions , any minimizer @xmath84 of @xmath42 may be rescaled such that @xmath85 .",
    "thus in connection with multivariate @xmath55 distributions with @xmath80 degrees of freedom , the location - scatter problem can be reduced to a scatter - only problem .    if @xmath4 has a density @xmath87 in our working model , then @xmath88 . if @xmath4 is just elliptically symmetric with center @xmath89 and scatter matrix @xmath49 , for instance , if it has a density @xmath50 of the form @xmath90 with @xmath52 , then @xmath91 and @xmath53 for some @xmath54 .",
    "suppose that @xmath4 is ( approximately ) elliptically symmetric with unknown center @xmath89 and unknown scatter matrix @xmath49 .",
    "in many situations one is only interested in the `` shape matrix '' @xmath92 , i.e.  a positive multiple of @xmath49 with determinant @xmath93 .",
    "examples are principal components , regression and correlation measures , where multiplying @xmath49 with a positive scalar has no impact",
    ". then we may get rid of the nuisance location parameter @xmath89 by replacing @xmath4 with its symmetrization @xmath94 indeed , @xmath95 is ( approximately ) elliptically symmetric with center @xmath48 and the same shape matrix @xmath92",
    ". we may estimate @xmath95 by the measure - valued @xmath1-statistic @xmath96 then , if we define @xmath43 to be the minimizer of @xmath97 \\ ,",
    "q(dx )          + \\log \\det(\\sigma)\\ ] ] with respect to @xmath33 , then the shape matrix of @xmath98 is a plausible estimator of the true shape matrix @xmath92 .",
    "the mapping @xmath99 is called a symmetrized @xmath0-functional of scatter .",
    "this symmetrization has a second , even more important advantage : consider an arbitrary distribution @xmath4 , i.e.  it may fail to be ( approximately ) elliptically symmetric . but suppose that a random vector @xmath100 may be written as @xmath101^\\top$ ] with independent subvectors @xmath102 .",
    "then @xmath25 is block - diagonal in the sense that @xmath103 with symmetric matrices @xmath104 . for a further discussion on the use of symmetrized scatter matrices in multivariate statistics",
    "see also nordhausen and tyler  @xcite .",
    "let @xmath37 be a probability distribution on @xmath5 .",
    "now we seek to minimize a certain target functional @xmath42 on the space @xmath13 of symmetric and positive definite matrices in @xmath14 , where @xmath105 and @xmath37 have to satisfy certain conditions :    * setting 0 .",
    "* we assume that @xmath106 , and for @xmath27 we define @xmath107 moreover , we assume that @xmath108 for any linear subspace @xmath109 of @xmath5 with @xmath110 .    * setting 1 .",
    "*  let @xmath29 be twice continuously differentiable such that @xmath111 .",
    "further we assume that @xmath112 satisfies the following two properties : @xmath113 and @xmath114 .",
    "for @xmath27 we define @xmath115 \\ , q(dx )          + \\log \\det(\\sigma ) .\\ ] ] moreover , we assume that @xmath116 for any linear subspace @xmath109 of @xmath5 with @xmath117 .",
    "note that for @xmath56 , @xmath57 satisfies the conditions of setting  1 with @xmath118 .",
    "hence @xmath119 , and @xmath37 has to satisfy @xmath120 for proper linear subspaces @xmath109 of @xmath5 .",
    "note also that setting  0 is similar to setting  1 if we define @xmath121 as in .",
    "the main difference to setting  1 is that @xmath122 for arbitrary @xmath27 and @xmath123 . in what follows",
    "we often write @xmath40 for @xmath124 or @xmath125 .",
    "the assumptions on @xmath126 and @xmath37 imply that the functional @xmath42 has essentially a unique minimizer ( see @xcite , @xcite or @xcite ) :    [ thm : existence ] in setting  0 there exists a unique matrix @xmath127 such that @xmath128    in setting  1 there exists a unique matrix @xmath129 such that @xmath130    coming back to the specific situation with independent random variables @xmath21 with distribution @xmath4 on @xmath5 , the scatter estimators in sections  [ subsec : scatteronly ] , [ subsec : locationscatter ] and [ subsec : symmetrizedm ] correspond to the following choices of @xmath37 :    *  @xmath131 ( section  [ subsec : scatteronly ] ) ; *  @xmath132 with dimension @xmath133 in place of @xmath134 ( section  [ subsec : locationscatter ] ) ; *  @xmath135 ( section  [ subsec : symmetrizedm ] ) .",
    "as shown in dmbgen et al .",
    "@xcite , the functionals @xmath136 and @xmath137 are smooth , strictly convex and coercive in a certain sense . to make this precise , we utilize the matrix - valued exponential function :",
    "for @xmath138 let @xmath139 in case of @xmath140 we may write @xmath141 with an orthogonal matrix @xmath142 and some vector @xmath143",
    ". then @xmath144 with @xmath145 .",
    "moreover , @xmath146 if @xmath147 , i.e.  @xmath148 , then @xmath149 with @xmath150 and @xmath151 .    by means of the matrix - valued exponential function and logarithm , we can describe the behavior of @xmath42 in a neighborhood of any matrix @xmath27 quite elegantly . instead of considering additive perturbations @xmath152 with @xmath153 , we write @xmath154 for some nonsingular matrix @xmath155 , for instance @xmath156 , and consider multiplicative perturbations @xmath157 .",
    "note that @xmath158 in case of @xmath159 , @xmath160 here is a basic expansion of @xmath161 around @xmath48 :    [ thm : convexity ] for a nonsingular matrix @xmath155 define @xmath162 with @xmath163 . then for @xmath153 , @xmath164 as @xmath165 , where @xmath166 and @xmath167 moreover , @xmath168 is continuous in @xmath169 , and @xmath170    the taylor expansion in theorem  [ thm : convexity ] implies that @xmath171 as @xmath165 , where @xmath172 hence the matrix @xmath173 is the gradient of the function @xmath174 at @xmath175 .",
    "note also that @xmath176 is positive definite , because otherwise @xmath37 would be concentrated on a proper linear subspace of @xmath5 .",
    "note that @xmath177 is constant in @xmath123 for any @xmath27 .",
    "in other words , for any nonsingular @xmath155 , @xmath178 is constant in @xmath179 . applying theorem  [ thm : convexity ] to @xmath180 yields that @xmath181 and @xmath182 in setting  0 .",
    "this explains the constraint @xmath183 for @xmath184 .",
    "the second derivative of the function @xmath185 at @xmath175 corresponds to the quadratic form @xmath186 with the self - adjoint linear operator @xmath187 given by @xmath188 theorem  [ thm : convexity ] implies that this operator is positive definite in setting  1 . in setting  0 , @xmath189 and one easily verifies that @xmath190 and @xmath191 for any @xmath153 .",
    "hence in both settings one may view @xmath192 as a self - adjoint and positive definite linear operator from the set @xmath193 onto itself .",
    "in particular , @xmath194 stands for the corresponding inverse mapping .",
    "an important consequence of theorem  [ thm : convexity ] is a convexity property of @xmath42 :    [ cor : convexity ] for any nonsingular @xmath155 and @xmath153 , the mapping @xmath195 is twice continuously differentiable and convex on @xmath41 . in setting  0 it is strictly convex if @xmath183 . in setting  1",
    "it is strictly convex if @xmath196 .",
    "this corollary implies that @xmath154 minimizes @xmath42 if , and only if , the gradient @xmath173 equals @xmath48 , i.e. @xmath197 this is equivalent to the fixed - point equation @xmath198",
    "the fixed - point equation gives rise to a fixed - point algorithm which has been proposed and used repeatedly , see for instance huber  @xcite ( section  8.11 ) , tyler  @xcite and kent and tyler  @xcite .",
    "the latter two references provide a rigorous proof of convergence for empirical distributions @xmath37 , the general case is covered by dudley et al .",
    "@xcite . a basic step works as follows :",
    "if @xmath27 is our current candidate for a minimizer of @xmath42 , then we replace it with @xmath199 when implementing this method it is more convenient to utilize the formulation directly : if @xmath154 for some nonsingular matrix @xmath155 , then @xmath200 now we use some factorization @xmath201 with nonsingular @xmath202 and replace @xmath169 with @xmath203 . replacing @xmath33 with @xmath204 yields always an improvement , because @xmath205 see @xcite .",
    "here is a description of the fixed - point algorithm :    [ [ algorithm - fp . ] ] algorithm fp .",
    "+ + + + + + + + + + + + +    choose an arbitrary matrix @xmath206 with nonsingular @xmath207 , and let @xmath208 .",
    "suppose that after @xmath209 steps we have determined a nonsingular matrix @xmath210 , corresponding to the candidate @xmath211 for @xmath212 .",
    "writing @xmath213 , we compute @xmath214 then we write @xmath215 for some nonsingular @xmath216 and define @xmath217 this corresponds to the new candidate @xmath218 .",
    "this description is similar to the one of huber  @xcite ( section  8.11 ) , the main difference being that we do nt restrict ourselves to the cholesky factorization of @xmath219 . indeed in our implementation",
    "we use @xmath215 with @xmath220 , where @xmath221 contains the eigenvalues of @xmath219 and @xmath222 is an orthogonal matrix of corresponding eigenvectors .",
    "our starting point is typically @xmath223 our stopping criterion for algorithm  fp is that @xmath224 for some given small number @xmath225 , where @xmath226 .",
    "an important fact is that under the conditions of theorem  [ thm : existence ] the sequence @xmath227 converges to a minimizer of @xmath42 , no matter which starting point @xmath228 has been chosen ; see also theorem  [ thm : convergence ] later .",
    "one may view the fixed - point algorithm as an approximate gradient method with constant stepsize one : note that with the gradient @xmath229 of @xmath230 at @xmath175 , @xmath231 in the present context an exact gradient method with constant step size one would mean to replace @xmath232 with @xmath233 .",
    "[ [ suboptimality - of - algorithm - fp . ] ] suboptimality of algorithm fp .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    as shown later , the steps performed in algorithm  fp are clearly suboptimal , at least when @xmath232 is already close to the limit @xmath43 . to understand this thoroughly and to devise improvements",
    "we first provide a corollary to theorem  [ thm : convexity ] :    [ cor : expansion ] let @xmath154 for a nonsingular matrix @xmath155 . further let @xmath234 . if we write @xmath235 with an orthogonal matrix @xmath236 , then for any @xmath153 , @xmath237 where @xmath238 moreover , @xmath239    now let us apply this corollary to algorithm  fp .",
    "we write @xmath240 for some orthogonal matrix @xmath241 .",
    "if we fix an arbitrary constant @xmath242 , then uniformly in @xmath153 with @xmath243 , @xmath244 in particular , if we choose @xmath245 with a bounded sequence @xmath246 in @xmath41 , @xmath247 consequently , an approximately optimal choice of @xmath248 would be a minimizer of the right hand side without the term @xmath249 , i.e. @xmath250 .\\end{aligned}\\ ] ] the upper bound involves the minimal eigenvalue of the symmetric operator @xmath251 .",
    "the lower bound follows from @xmath252 and is typically strictly larger than @xmath93 , for instance if @xmath57 as defined in or .",
    "hence the steps performed during the fixed - point algorithm tend to be too short !    [ [ algorithm - g . ] ] algorithm g. + + + + + + + + + + + +    one could easily fix this deficiency as follows : as a proxy for @xmath253 , which involves the unknown quadratic form @xmath254 , we compute in the @xmath255-th iteration the number @xmath256 the latter equality follows from corollary  [ cor : expansion ] .",
    "indeed , the latter corollary implies that we obtain @xmath257 .",
    "thus we check whether @xmath258 if yes , we replace @xmath259 with @xmath260 , where @xmath261 .",
    "otherwise we perform a usual fixed - point step as described before .",
    "the number @xmath262 in could be replaced with any number @xmath263 .    implementing this gradient method",
    "yielded already a substantial reduction of computation time .",
    "this approach of improving a fixed - point algorithm by means of variable step lengths is also used by redner and walker  @xcite in the context of maximum - likelihood estimation for mixture models .",
    "but in view of theorem  [ thm : convexity ] it is certainly tempting to try a newton - raphson procedure .",
    "suppose that our current candidate for @xmath43 is @xmath154 .",
    "in view of corollary  [ cor : expansion ] we should replace @xmath33 with @xmath264 because @xmath265 is the unique minimizer of @xmath266 a problem with this promising update @xmath267 is that the computation of the inverse operator @xmath194 may be too computer- or memory - intensive .",
    "indeed , we implemented a full newton - raphson algorithm , and it required only very few iterations , as expected .",
    "but the running time was even longer than with algorithm  fp , because the computation and inversion of @xmath192 , which may be represented by a symmetric matrix in @xmath268 , was too time - consuming . note that @xmath269 equals @xmath270 in setting  0 and @xmath271 in setting  1 .",
    "these difficulties with a full newton - raphson procedure have been noticed already by huber  @xcite ( section  8.11 ) .",
    "some authors have tried alternative approaches such as conjugate gradient methods or quasi newton methods in which the operator @xmath192 is replaced with a surrogate which is easier to compute and invert ; see for instance huber  @xcite . according to @xcite ,",
    "none of these attempts was overall convincing .",
    "a partial newton - raphson approach turned out to be quite successful .",
    "this means that instead of considering arbitrary multiplicative perturbations @xmath157 of a current candidate @xmath154 , we restrict @xmath272 to a particular @xmath134-dimensional subspace of @xmath12 depending on @xmath169 .",
    "precisely , consider the matrix @xmath273 and its spectral decomposition , @xmath274 with an orthogonal matrix @xmath142 whose columns are eigenvectors of @xmath176 and a vector @xmath275 containing the corresponding eigenvalues .",
    "now we consider only perturbations @xmath276 with @xmath277 , @xmath278 .",
    "since @xmath279 , this leads to the functional @xmath280 now the taylor expansion in theorem  [ thm : convexity ] may be rewritten as follows : @xmath281 where @xmath282 with @xmath283 and @xmath284 in setting  1 , @xmath285 is a positive definite matrix , and @xmath286 in setting  0 , the matrix @xmath285 satisfies @xmath287 and @xmath288 whenever @xmath289 , @xmath290 .",
    "moreover , @xmath291 .",
    "thus we may write @xmath292 for any constant @xmath293 .",
    "[ [ algorithm - pn . ] ] algorithm pn .",
    "+ + + + + + + + + + + + +    choose an arbitrary matrix @xmath206 with nonsingular @xmath207 , and let @xmath208 .",
    "+ suppose that for some integer @xmath209 we have already determined a nonsingular matrix @xmath210 .",
    "writing @xmath213 , we compute @xmath214 then we write @xmath294 with an orthogonal matrix @xmath295 and a vector @xmath221 . next we define @xmath296 and @xmath297 we expect that replacing @xmath259 with @xmath298 results in a change of @xmath42 of about @xmath299 .",
    "now we check whether @xmath300 if yes , we define @xmath301 which corresponds to the new candidate @xmath302 .",
    "if is violated we just perform a step of the fixed - point algorithm and set @xmath303 , i.e.  our new candidate is @xmath304 . again",
    ", the number @xmath262 in could be replaced by any number @xmath263 .",
    "the new algorithm  pn is also guaranteed to converge to a minimizer of @xmath42 :    [ thm : convergence ] for any starting point @xmath305 and in both settings  0 and 1 , algorithm  fp as well as algorithm  pn yield a sequence @xmath306 converging to a minimizer @xmath307 of @xmath42 .    for general distributions @xmath37",
    "it is difficult to compare algorithms  fp and pn explicitly .",
    "recall that in algorithm  pn we restrict our attention to a particular subspace of @xmath13 .",
    "the following lemma implies that at least in case of an ( approximately ) elliptically symmetric distribution @xmath37 this subspace is ( almost ) the right one to look in for better candidates .",
    "[ lem : pn.is.right ] suppose that @xmath37 is elliptically symmetric with center @xmath48 and scatter matrix @xmath308 .",
    "then @xmath309 for some @xmath310 .",
    "moreover , for any @xmath154 with nonsingular @xmath155 and any spectral decomposition @xmath311 , @xmath312 for a vector @xmath278 containing the log - eigenvalues of @xmath313 .    at this point we should mention that for `` well - behaved '' distributions @xmath37 in high dimension @xmath134 , algorithm fp can be rather efficient , because the standardized distribution @xmath314 satisfies @xmath315 for @xmath316 .",
    "for instance in setting  0 , if @xmath317 is spherically symmetric around @xmath48 , @xmath318 for all @xmath316 .",
    "hence , if @xmath154 is already close to @xmath212 , the newton step would be to replace @xmath33 with @xmath319 and for high dimension @xmath134 this is similar to @xmath320 .",
    "indeed our numerical experiments show that algorithm  pn is particularly useful in situations where @xmath37 is `` problematic '' , e.g.  an empirical distribution of a sample with strong outliers .",
    "[ [ standard - m - estimators . ] ] standard @xmath0-estimators .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    suppose that @xmath321 with a certain weight vector @xmath322 in @xmath323 such that @xmath324 and a data matrix @xmath325^\\top \\in { \\mathbb{r}}^{n\\times q}$ ] .",
    "then our algorithm  pn may be formulated as in table  [ tab : pn ] .",
    "@xmath326          b \\leftarrow \\bigl ( \\sum_{i=1}^n w_i^ { } x_i^{}x_i^\\top \\bigr)_{}^{1/2}\\\\      { \\boldsymbol}{y } \\leftarrow { \\boldsymbol}{x } b^{-1 }",
    "\\\\      \\psi \\leftarrow \\sum_{i=1}^n w_i \\rho'(\\|y_i\\|^2 ) \\ ,",
    "y_i^{}y_i^\\top\\\\      ( u,\\phi ) \\leftarrow \\mathrm{eigen}(\\psi)\\\\[1ex ]          \\mathbf{while } \\",
    "\\|1_q - \\phi\\| > \\delta \\",
    "\\mathbf{do}\\\\      \\strut\\qquad          b \\leftarrow b u\\\\      \\strut\\qquad          { \\boldsymbol}{y } \\leftarrow { \\boldsymbol}{y } u\\\\      \\strut\\qquad          \\tilde{h } \\leftarrow { \\mathop{\\mathrm{diag}}\\nolimits}(\\phi )              + \\sum_{i=1}^n w_i \\rho''(\\|y_i\\|^2 ) s(y_i ) s(y_i)^\\top              \\ ( + \\ c \\",
    ", 1_q^{}1_q^\\top \\",
    "\\text{in setting~0})\\\\      \\strut\\qquad          a \\leftarrow \\tilde{h}^{-1 } ( \\phi - 1_q)\\\\      \\strut\\qquad          { \\boldsymbol}{z } \\leftarrow { \\boldsymbol}{y } \\exp(- { \\mathop{\\mathrm{diag}}\\nolimits}(a)/2)\\\\      \\strut\\qquad          dl \\leftarrow \\sum_{i=1}^n w_i \\bigl ( \\rho(\\|z_i\\|^2 ) - \\rho(\\|y_i\\|^2 ) \\bigr )              + \\sum_{j=1}^q a_j\\\\      \\strut\\qquad          dl_0 \\leftarrow a^\\top ( 1_q - \\phi)/4\\\\      \\strut\\qquad          \\mathbf{if } \\ dl \\le dl_0 \\ \\mathbf{then}\\\\      \\strut\\qquad\\qquad              b \\leftarrow b \\exp({\\mathop{\\mathrm{diag}}\\nolimits}(a)/2)\\\\      \\strut\\qquad\\qquad              { \\boldsymbol}{y }",
    "\\leftarrow { \\boldsymbol}{z}\\\\      \\strut\\qquad          \\mathbf{else}\\\\      \\strut\\qquad\\qquad              b \\leftarrow b { \\mathop{\\mathrm{diag}}\\nolimits}(\\phi)^{1/2}\\\\      \\strut\\qquad\\qquad              { \\boldsymbol}{y } \\leftarrow { \\boldsymbol}{y } { \\mathop{\\mathrm{diag}}\\nolimits}(\\phi)^{-1/2}\\\\      \\strut\\qquad          \\mathbf{end~if}\\\\      \\strut\\qquad          \\psi \\leftarrow \\sum_{i=1}^n w_i \\rho'(\\|y_i\\|^2 ) \\",
    ", y_i^{}y_i^\\top\\\\      \\strut\\qquad          ( u,\\phi ) \\leftarrow \\mathrm{eigen}(\\psi)\\\\      \\mathbf{end~while}\\\\[1ex ]          \\sigma \\leftarrow bb^\\top\\\\      \\mathbf{return } \\",
    "\\sigma\\\\      \\hline        \\end{array}\\ ] ]    [ [ symmetrized - m - estimators . ] ] symmetrized @xmath0-estimators .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    suppose that @xmath327 for a certain data matrix @xmath325^\\top \\in { \\mathbb{r}}^{n\\times q}$ ] .",
    "in principle one could utilize the algorithm just described with @xmath328 in place of @xmath6 and @xmath329 replaced by a data matrix @xmath330 containing all @xmath331 differences @xmath332 . for large @xmath6 , however , this may require too much computer memory , and one should avoid the explicit storage of such a large data matrix @xmath330 .",
    "it turned out that the computation time can be reduced substantially if we first compute the @xmath0-estimator @xmath333 for the surrogate distribution @xmath334 with a randomly chosen permutation @xmath335 of @xmath336 and @xmath337 .",
    "then we use this estimator @xmath333 as a starting parameter @xmath228 in algorithm  pn .",
    "table  [ tab : pn_symm ] contains pseudo - code for the computation of the symmetrized @xmath0-estimator without using a large data matrix @xmath330 .",
    "instead it utilizes auxiliary programs to compute the following objects : @xmath338          + \\sum_{k=1}^q a_k .\\end{aligned}\\ ] ]    @xmath339          \\pi \\leftarrow \\mathrm{rpermute}(n)\\\\      { \\boldsymbol}{x}^0 \\leftarrow [ x_{\\pi(1 ) } - x_{\\pi(2 ) } , x_{\\pi(2 ) } - x_{\\pi(3 ) } ,          \\ldots , x_{\\pi(n ) } - x_{\\pi(1)}]_{}^\\top\\\\      b \\leftarrow \\mathbf{algorithmpn}({\\boldsymbol}{x}^0,(1/n)_{i=1}^n,\\delta)_{}^{1/2}\\\\      { \\boldsymbol}{y } \\leftarrow { \\boldsymbol}{x } b^{-1 } \\\\      \\psi \\leftarrow \\mathrm{psi}({\\boldsymbol}{y})\\\\      ( u,\\phi ) \\leftarrow \\mathrm{eigen}(\\psi)\\\\[1ex ]          \\mathbf{while } \\",
    "\\|1_q - \\phi\\| > \\delta \\ \\mathbf{do}\\\\      \\strut\\qquad          b \\leftarrow b u\\\\      \\strut\\qquad          { \\boldsymbol}{y }",
    "\\leftarrow { \\boldsymbol}{y } u\\\\      \\strut\\qquad          \\tilde{h } \\leftarrow \\mathrm{h}(\\phi,{\\boldsymbol}{y } ) \\ \\          ( + \\ c \\",
    ", 1_q^{}1_q^\\top \\ \\text{in setting~0 } ) \\\\      \\strut\\qquad          a \\leftarrow \\tilde{h}^{-1 } ( \\phi - 1_q)\\\\      \\strut\\qquad          { \\boldsymbol}{z } \\leftarrow { \\boldsymbol}{y } \\exp(- { \\mathop{\\mathrm{diag}}\\nolimits}(a)/2)\\\\      \\strut\\qquad          dl \\leftarrow",
    "\\mathrm{dl}({\\boldsymbol}{y},{\\boldsymbol}{z},a)\\\\      \\strut\\qquad          dl_0 \\leftarrow a^\\top ( 1_q - \\phi)/4\\\\      \\strut\\qquad          \\mathbf{if } \\ dl \\le dl_0 \\",
    "\\mathbf{then}\\\\      \\strut\\qquad\\qquad              b \\leftarrow b \\exp({\\mathop{\\mathrm{diag}}\\nolimits}(a)/2)\\\\      \\strut\\qquad\\qquad              { \\boldsymbol}{y } \\leftarrow { \\boldsymbol}{z}\\\\      \\strut\\qquad          \\mathbf{else}\\\\      \\strut\\qquad\\qquad              b \\leftarrow b { \\mathop{\\mathrm{diag}}\\nolimits}(\\phi)^{1/2}\\\\      \\strut\\qquad\\qquad              { \\boldsymbol}{y } \\leftarrow { \\boldsymbol}{y } { \\mathop{\\mathrm{diag}}\\nolimits}(\\phi)^{-1/2}\\\\      \\strut\\qquad          \\mathbf{end~if}\\\\      \\strut\\qquad          \\psi \\leftarrow \\mathrm{psi}({\\boldsymbol}{y})\\\\      \\strut\\qquad          ( u,\\phi ) \\leftarrow \\mathrm{eigen}(\\psi)\\\\      \\mathbf{end~while}\\\\[1ex ]          \\sigma \\leftarrow bb^\\top\\\\      \\mathbf{return } \\ \\sigma\\\\      \\hline        \\end{array}\\ ] ]",
    "in most of our simulation experiments we simulated data matrices @xmath340^\\top$ ] with independent rows @xmath341 having either standard gaussian or standard cauchy distribution on @xmath5 . in the latter case , @xmath342 is distributed as @xmath343 with independent random variables @xmath344 . in all experiments ,",
    "iterations were stopped when the gradient @xmath345 of our target function satisfies @xmath346 , and the number of monte carlo simulations for each setting was @xmath347 .",
    "the first three experiments were run on a macbook pro ( 2ghz intel(r ) core i7 , 16 gb ) , the fourth experiment on a windows server ( two intel(r ) xeon(r ) cpu r5 2440 with 2.40ghz and 64 gb ) .",
    "we used r 3.1.2 @xcite .",
    "[ [ comparisons - in - scatter - only - settings . ] ] comparisons in scatter - only settings .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to compare the three algorithms fp , g and pn , we first implemented them in pure r code .",
    "table  [ tab : comparisons1 ] contains the mean number of iterations and the mean computing times for the scatter estimator @xmath45 with @xmath348 based on a data matrix @xmath349 , @xmath350 .",
    "the table entries are the mean iteration numbers and mean computations times in milliseconds [ ms ] . in brackets",
    "the corresponding inter quartile ranges are recorded as well .",
    "the relative efficiencies are the ratios of the mean computation times .",
    "algorithm  g is already more efficient than algorithm  fp , but obviously algorithm  pn is substantially faster than the other two , and this advantage grows with the dimension @xmath134 .",
    "note also that computation costs are higher for cauchy data than for gaussian data .    @xmath351      \\cline{1 - 1 }      { \\boldsymbol}{q = 5 } \\\\      \\hline      \\text{iterations }      &   83.9 \\ ( 2 ) & 31.2 \\   ( 4 ) & 5.1 \\ ( 0 )      & 116.4 \\ ( 3 ) & 45.5 \\ ( 14 ) & 8.5 \\ ( 1 ) \\\\      \\hline      \\text{time [ ms ] }      & 13.5 \\ ( 0.5 ) & 11.4 \\ ( 1.8 ) & 1.8 \\ ( 0.3 )      & 18.5 \\ ( 1.0 ) & 16.8 \\ ( 5.3 ) & 2.8 \\ ( 0.5 ) \\\\      \\hline      \\text{relative }      & \\text{fp }             & 1.18 & { \\boldsymbol}{7.71 }      & \\text{fp }             & 1.10 & { \\boldsymbol}{6.53 } \\\\      \\text{efficiency }      &       & \\text{g }                    & { \\boldsymbol}{6.51 }      &       & \\text{g }                    & { \\boldsymbol}{5.95 } \\\\      \\hline          \\multicolumn{7}{c } { } \\\\[-1.5ex ]      \\cline{1 - 1 }      { \\boldsymbol}{q = 10 } \\\\      \\hline      \\text{iterations }      & 141.6 \\ ( 1 ) & 46.0 \\   ( 6 ) & 6.0 \\ ( 0 )      & 189.4 \\ ( 3 ) & 69.4 \\ ( 30 ) & 9.3 \\ ( 1 ) \\\\      \\hline      \\text{time [ ms ] }      & 41.9 \\ ( 1.0 ) & 25.0 \\   ( 2.8 ) & 3.1 \\ ( 0.3 )      & 56.2 \\ ( 2.3 ) & 37.1 \\ ( 16.0 ) & 5.0 \\ ( 1.0 ) \\\\      \\hline      \\text{relative }      & \\text{fp }             & 1.68 & { \\boldsymbol}{13.37 }      & \\text{fp }             & 1.51 & { \\boldsymbol}{11.19 } \\\\      \\text{efficiency }      &       & \\text{g }                    & { \\boldsymbol}{7.97 }      &       & \\text{g }                    & { \\boldsymbol}{7.40 } \\\\      \\hline          \\multicolumn{7}{c } { } \\\\[-1.5ex ]      \\cline{1 - 1 }      { \\boldsymbol}{q = 20 } \\\\      \\hline      \\text{iterations }      & 252.2 \\ ( 2 ) & 119.2 \\   ( 6 ) &   6.0 \\ ( 0 )      & 332.2 \\ ( 4 ) & 103.7 \\ ( 43 ) & 10.6 \\ ( 1 ) \\\\      \\hline      \\text{time [ ms ] }      & 176.2 \\ ( 4.8 ) & 120.2 \\   ( 7.8 ) &   6.9 \\ ( 0.3 )      & 230.1 \\ ( 4.8 ) & 104.4 \\ ( 43.4 ) & 12.4 \\ ( 1.3 ) \\\\      \\hline      \\text{relative }      & \\text{fp }             & 1.47 & { \\boldsymbol}{25.65 }      & \\text{fp }             & 2.20 & { \\boldsymbol}{18.54 } \\\\      \\text{efficiency }      &       & \\text{g }                    & { \\boldsymbol}{17.49 }      &       & \\text{g }                    & { \\boldsymbol}{8.41 } \\\\      \\hline      \\end{array}\\ ] ]    [ [ comparisons - in - location - scatter - settings . ] ] comparisons in location - scatter settings .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    now we consider the empirical distribution @xmath68 of the rows of @xmath329 and for given @xmath80 the minimizer @xmath352 of @xmath353 over all @xmath354 . here",
    "@xmath355 is defined as in , @xmath356 stands for the empirical distribution of the augmented data points @xmath357^\\top \\in { \\mathbb{r}}^{q+1}$ ] , @xmath358 , and @xmath359 \\ , { \\widehat}{q}(dy ) + \\log\\det(\\gamma)\\ ] ] for arbitrary @xmath360 .    in principle , we may apply any of the three algorithms fp , g and pn to the empirical distribution @xmath356 to compute a minimizer @xmath361 of @xmath362 . in case of @xmath83",
    "this minimizer satisfies automatically @xmath363 , so @xmath364 . in case of @xmath86",
    ", @xmath361 equals @xmath365 times @xmath366 .",
    "in addition we implemented a variant fp@xmath367 of fp proposed by arslan et al .",
    "suppose that @xmath368 with nonsingular @xmath210 is a current candidate for @xmath352 .",
    "let @xmath369 denote the empirical distribution of the standardized data points @xmath370 , @xmath358 , augmented by an additional component @xmath93 , and define @xmath371 recall that @xmath368 equals @xmath352 if , and only if , @xmath372 .",
    "now we write @xmath373 for some @xmath374 , @xmath375 and a nonsingular matrix @xmath216 .",
    "then the next candidate for @xmath352 equals @xmath376 with @xmath377 to provide a fair comparison , we used the same stopping criterion as for the other algorithms , that means , we considered the norm of @xmath378 .    for @xmath379 and @xmath380 we simulated data matrices @xmath381 with independent entries @xmath382 where @xmath383 is a certain parameter quantifying the outlyingness of the @xmath384 first data vectors .",
    "the left and right half of table  [ tab : comparisons2 ] show the resulting computation costs and times for @xmath385 when @xmath86 and @xmath386 , respectively . for @xmath86 ,",
    "algorithm fp is more efficient than fp@xmath367 .",
    "indeed one can easily verify that the two algorithms are essentially equivalent , the only difference being how they factorize matrices such as @xmath219 .",
    "for @xmath387 , algorithm fp ( @xmath86 ) and algorithm fp@xmath367 ( @xmath386 ) are remarkably efficient and even outperform algorithm pn . but for larger values of @xmath388 , leading to heterogeneous data sets , pn is clearly the fastest method .",
    "@xmath389      \\cline{1 - 1 }      { \\boldsymbol}{\\delta = 0 } \\\\",
    "\\hline      \\text{iterations }      &   15.1 \\ ( 0 ) & 15.1 \\ ( 0 ) & 9.6 \\ ( 1 )      & 152.0 \\ ( 3 ) & 13.8 \\ ( 1 ) & 8.9 \\ ( 0 ) \\\\",
    "\\hline      \\text{time [ ms ] }      &   2.3 \\ ( 0.2 ) & 2.7 \\ ( 0.2 ) & 3.0 \\ ( 0.2 )      & 21.8 \\ ( 0.6 ) & 2.8 \\ ( 0.3 ) & 2.9 \\ ( 0.1 ) \\\\      \\hline      \\text{relative }      & \\text{fp }             & 0.87 & { \\boldsymbol}{0.77 }      & \\text{fp }             & 7.81 & { \\boldsymbol}{7.62 } \\\\",
    "\\text{efficiency }      &       & \\text{fp$_3 $ }                    & { \\boldsymbol}{0.88 }      &       & \\text{fp$_3 $ }                    & { \\boldsymbol}{0.98 } \\\\      \\hline          \\multicolumn{7}{c } { } \\\\[-1.5ex ]      \\cline{1 - 1 }      { \\boldsymbol}{\\delta = 10 } \\\\",
    "\\hline      \\text{iterations }      &   27.4 \\ ( 4 ) & 27.4 \\ ( 4 ) & 12.3 \\ ( 1 )      & 157.3 \\ ( 3 ) & 25.8 \\ ( 3 ) & 11.6 \\ ( 1 ) \\\\",
    "\\hline      \\text{time [ ms ] }      &   4.0 \\ ( 0.6 ) & 4.7 \\ ( 0.7 ) & 3.7 \\ ( 0.3 )      & 22.3 \\ ( 0.6 ) & 4.9 \\ ( 0.6 ) & 3.7 \\ ( 0.3 ) \\\\      \\hline      \\text{relative }      & \\text{fp }             & 0.85 & { \\boldsymbol}{1.09 }      & \\text{fp }             & 4.60 & { \\boldsymbol}{6.11 } \\\\      \\text{efficiency }      &       & \\text{fp$_3 $ }                    & { \\boldsymbol}{1.28 }      &       & \\text{fp$_3 $ }                    & { \\boldsymbol}{1.33 } \\\\      \\hline          \\multicolumn{7}{c } { } \\\\[-1.5ex ]      \\cline{1 - 1 }      { \\boldsymbol}{\\delta = 20 } \\\\",
    "\\hline      \\text{iterations }      &   47.2 \\ ( 6 ) & 47.2 \\ ( 6 ) & 17.2 \\ ( 2 )      & 161.4 \\ ( 3 ) & 42.0 \\ ( 4 ) & 15.6 \\ ( 1 ) \\\\",
    "\\hline      \\text{time [ ms ] }      &   6.6 \\ ( 0.9 ) & 7.8 \\ ( 1.0 ) & 5.0 \\ ( 0.5 )      & 23.0 \\ ( 0.6 ) & 7.9 \\ ( 1.0 ) & 4.9 \\ ( 0.4 ) \\\\",
    "\\hline      \\text{relative }      & \\text{fp }             & 0.84 & { \\boldsymbol}{1.31 }      & \\text{fp }             & 2.93 & { \\boldsymbol}{4.66 } \\\\      \\text{efficiency }      &       & \\text{fp$_3 $ }                    & { \\boldsymbol}{1.56 }      &       & \\text{fp$_3 $ }                    & { \\boldsymbol}{1.59 } \\\\      \\hline      \\end{array}\\ ] ]    [ [ comparisons - for - symmetrized - scatter - estimators - i . ] ] comparisons for symmetrized scatter estimators , i. + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    as mentioned in the introduction , computation time becomes a major issue when computing symmetrized scatter estimators . in the simulation experiments described below we simulated data matrices @xmath390 with independent rows following a multivariate standard gaussian or standard cauchy distribution on @xmath5 .    our first simulation experiment concerns @xmath391 different variants of algorithm  pn for symmetrized estimators with @xmath392 : on the one hand we compared storing all @xmath393 pairwise differences of data vectors in a big matrix and running the algorithm in table  [ tab : pn ] ( `` pn - all '' ) with a less memory - intensive version where all statistics are computed sequentially as in table  [ tab : pn_symm ] ( `` pn - seq '' ) . in both cases",
    "we first prewhitened the data by means of a scatter estimator based on @xmath6 randomly chosen pairs of observations , see the first four lines of pseudo - code in table  [ tab : pn_symm ] .",
    "on the other hand we investigated the benefits of the latter prewhitening step and implemented versions without it ( `` pn - all.0 '' and `` pn - seq.0 '' ) .",
    "figures  [ fig : pn.symm.100 ] and [ fig : pn.symm.500 ] show box plots of the computation times ( using pure r code ) for dimension @xmath380 and sample sizes @xmath379 and @xmath394 , respectively .",
    "one sees clearly that for small to moderate sample sizes version `` pn - all '' is faster than `` pn - seq '' .",
    "but for larger sample sizes `` pn - seq '' becomes clearly preferable .",
    "comparing `` pn - all.0 '' with `` pn - all '' and `` pn - seq.0 '' with `` pn - seq '' shows that prewhitening is particularly beneficial for the heavy - tailed distribution and larger sample sizes .",
    "note that all computation times for the symmetrized scatter estimators are in seconds [ s ] rather than milliseconds [ ms ] as before .     of four variants of algorithmpn.symm ( @xmath380 , @xmath379 , @xmath86).,scaledwidth=90.0% ]     of four variants of algorithmpn.symm ( @xmath380 , @xmath395 , @xmath86).,scaledwidth=90.0% ]    [ [ more - efficient - code .",
    "] ] more efficient code .",
    "+ + + + + + + + + + + + + + + + + + + +    the new algorithms described in this paper are implemented in the r package _ fastm _ ( dmbgen et al .",
    "@xcite ) which is publicly available on cran .",
    "this includes implementations with c++ code which are even more efficient .",
    "we did substantial simulation experiments to compare our package with other implementations of @xmath0-estimators , namely ( i ) the function _",
    "cov.trob _ in the package _ mass _ ( venables and ripley  @xcite ) and ( ii ) the function _ tm _ in the package _ ics _",
    "( nordhausen et al .",
    "@xcite ) . both functions are essentially fix - point approaches .",
    "in particular , _ tm _ is based on a maximum - likelihood and em interpretation of the fixed point equation and uses algorithm fp@xmath367 by arslan et al .",
    "@xcite mentioned before .",
    "all in all our new algorithms were always comparable , often faster and in some settings even substantially faster than the other methods .",
    "a fair comparison is difficult , though , because the established algorithms use different stopping criteria . both _",
    "cov.trob _ and _ tm _ update the location and scatter parameters separately and do not treat it as our algorithms do , as a scatter - only problem .",
    "for the symmetrized estimator with @xmath396 , there is the function duembgen.shape available in the r package _ icsnp _ ( nordhausen et al .",
    "@xcite ) , which is essentially algorithm  fp and utilizes r and c code .    [",
    "[ comparisons - for - symmetrized - scatter - estimators - ii . ] ] comparisons for symmetrized scatter estimators , ii . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    finally , tables  [ tab : comparisonsduembgenn500 ] and [ tab : comparisonsduembgenn2000 ] compare the performance of the symmetrized estimator as implemented in _ fastm _ with pure r code and with c++ code , where @xmath57 , @xmath397 .",
    "the results show that algorithm  pn with c++ code is substantially faster than its pure r version .",
    "@xmath398 } & \\text{time [ s ] } & \\!\\text{rel.\\ eff.}\\ !      & \\text{iter .",
    "} & \\text{time [ s ] } & \\text{time [ s ] } & \\!\\text{rel.\\ eff.}\\ ! \\\\",
    "\\multicolumn{1}{l| } { }      &               & \\text{r }         & \\text{c++ }       &      &               & \\text{r }         & \\text{c++ }       & \\\\      \\cline{2 - 9 }      \\multicolumn{9}{c } { } \\\\[-1.5ex ]        \\cline{1 - 1 }      \\multicolumn{1}{|l|}{{\\boldsymbol}{\\nu = 0 } } \\\\      \\hline      q=5      & 4.0 \\ ( 0 ) & 1.2 \\ ( 0.3 ) & 0.2 \\ ( 0.1 ) & { \\boldsymbol}{6.81 }      & 5.1 \\ ( 0 ) & 1.3 \\ ( 0.2 ) & 0.2 \\ ( 0.1 ) & { \\boldsymbol}{5.67 } \\\\      \\hline      q=10      & 5.0 \\ ( 0 ) & 1.7 \\ ( 0.4 ) & 0.4 \\ ( 0.2 ) & { \\boldsymbol}{3.91 }      & 6.0 \\ ( 0 ) & 2.1 \\ ( 0.4 ) & 0.5 \\ ( 0.3 ) & { \\boldsymbol}{4.04 } \\\\      \\hline      q=20      & 5.0 \\ ( 0 ) & 2.9 \\ ( 0.7 ) & 0.9 \\ ( 0.3 ) & { \\boldsymbol}{3.13 }      & 6.9 \\ ( 0 ) & 3.7 \\ ( 1.0 ) & 1.2 \\ ( 0.3 ) & { \\boldsymbol}{3.15 }",
    "\\\\      \\hline        \\multicolumn{9}{c } { } \\\\[-1.5ex ]        \\cline{1 - 1 }      \\multicolumn{1}{|l|}{{\\boldsymbol}{\\nu = 1 } } \\\\      \\hline      q=5      & 4.0 \\ ( 0 ) & 1.2 \\ ( 0.3 ) & 0.2 \\ ( 0.1 ) & { \\boldsymbol}{6.40 }      & 5.1 \\ ( 0 ) & 1.3 \\ ( 0.2 ) & 0.2 \\ ( 0.2 ) & { \\boldsymbol}{5.44 } \\\\      \\hline      q=10      & 5.0 \\ ( 0 ) & 1.7 \\ ( 0.4 ) & 0.4 \\ ( 0.2 ) & { \\boldsymbol}{3.96 }      & 6.0 \\ ( 0 ) & 2.0 \\ ( 0.4 ) & 0.5 \\ ( 0.3 ) & { \\boldsymbol}{3.97 } \\\\      \\hline      q=20      & 5.0 \\ ( 0 ) & 2.9 \\ ( 0.8 ) & 0.9 \\ ( 0.3 ) & { \\boldsymbol}{3.11 }      & 6.9 \\ ( 0 ) & 3.7 \\ ( 1.0 ) & 1.2 \\ ( 0.4 ) & { \\boldsymbol}{3.11 } \\\\      \\hline      \\end{array}\\ ] ]    @xmath398 } & \\text{time [ s ] } & \\!\\text{rel.\\ eff.}\\ !      & \\text{iter . } & \\text{time [ s ] } & \\text{time [ s ] } & \\!\\text{rel.\\ eff.}\\ ! \\\\      \\multicolumn{1}{l| } { }      &               & \\text{r }         & \\text{c++ }       &      &               & \\text{r }         & \\text{c++ }       & \\\\      \\cline{2 - 9 }      \\multicolumn{9}{c } { } \\\\[-1.5ex ]        \\cline{1 - 1 }      \\multicolumn{1}{|l|}{{\\boldsymbol}{\\nu = 0 } } \\\\      \\hline      q=5      & 3.2 \\ ( 0 ) &   7.9 \\ ( 1.6 ) &   1.9 \\ ( 0.5 ) & { \\boldsymbol}{4.03 }      & 4.0 \\ ( 0 ) &   9.5 \\ ( 1.4 ) &   2.3 \\ ( 0.5 ) & { \\boldsymbol}{4.06 } \\\\      \\hline      q=10      & 4.0 \\ ( 0 ) & 14.3 \\ ( 2.6 ) &   4.3 \\ ( 0.5 ) & { \\boldsymbol}{3.30 }      & 4.6 \\ ( 1 ) & 16.0 \\ ( 3.5 ) &   4.9 \\ ( 1.0 ) & { \\boldsymbol}{3.27 } \\\\      \\hline      q=20      & 4.0 \\ ( 0 ) & 33.1 \\ ( 7.9 ) & 10.1 \\ ( 0.2 ) & { \\boldsymbol}{3.28 }      & 5.0 \\ ( 0 ) & 40.7 \\ ( 8.3 ) & 12.2 \\ ( 0.2 ) & { \\boldsymbol}{3.33 } \\\\      \\hline        \\multicolumn{9}{c } { } \\\\[-1.5ex ]        \\cline{1 - 1 }      \\multicolumn{1}{|l|}{{\\boldsymbol}{\\nu = 1 } } \\\\      \\hline      q=5      & 3.2 \\ ( 0 ) &   7.7 \\ ( 1.4 ) &   1.9 \\ ( 0.4 ) & { \\boldsymbol}{3.99 }      & 4.0 \\ ( 0 ) &   9.5 \\ ( 1.4 ) &   2.4 \\ ( 0.5 ) & { \\boldsymbol}{4.00 } \\\\      \\hline      q=10      & 4.0 \\ ( 0 ) & 14.3 \\ ( 2.8 ) &   4.4 \\ ( 0.6 ) & { \\boldsymbol}{3.24 }      & 4.7 \\ ( 1 ) & 16.2 \\ ( 3.4 ) &   5.0 \\ ( 0.5 ) & { \\boldsymbol}{3.25 } \\\\",
    "\\hline      q=20      & 4.0 \\ ( 0 ) & 33.1 \\ ( 7.7 ) & 10.1 \\ ( 0.2 ) & { \\boldsymbol}{3.27 }      & 5.0 \\ ( 0 ) & 40.8 \\ ( 7.9 ) & 12.3 \\ ( 0.2 ) & { \\boldsymbol}{3.32 } \\\\      \\hline      \\end{array}\\ ] ]",
    "for @xmath399 define @xmath400 and @xmath401 . note that @xmath402 is nonsingular with @xmath403 . for @xmath404 , @xmath405 as @xmath406 .",
    "since both @xmath407 and @xmath408 are continuous in @xmath399 , this expansion shows that @xmath409 is twice continuously differentiable with @xmath410 and @xmath411 in particular , @xmath409 is convex .",
    "it is even strictly convex unless @xmath412    to verify corollary  [ cor : expansion ] , we utilize the same auxiliary function @xmath413 and write @xmath414 as @xmath415 now let @xmath235 with an orthogonal matrix @xmath236 , and define @xmath416 then @xmath417 so @xmath418 is no larger than @xmath419 times the supremum of @xmath420 over all @xmath421 with @xmath422 , @xmath423 and all orthogonal matrices @xmath424 .",
    "but this converges to zero as @xmath425 and @xmath165 , because then @xmath426 finally , because @xmath427 , we may write @xmath428 +    dropping the index @xmath255 for the moment , suppose that @xmath154 is our current candidate parameter . then",
    "one step of algorithm  fp replaces @xmath33 with @xmath429 hence @xmath40 changes by @xmath430 and the inequality is strict unless @xmath33 minimizes @xmath42 already , see . note also that @xmath431 is a continuous function of @xmath33 .",
    "algorithm  pn is slightly more difficult to quantify , because the eigenmatrix @xmath1 in the representation @xmath311 is not unique .",
    "however , @xmath432 in the last step we utilized that fact that @xmath433 for some orthogonal matrix @xmath434 , and that @xmath435 , @xmath436 .",
    "consequently , the change of @xmath40 with algorithm  pn is at least @xmath437 again a continuous function of @xmath33 , and the inequality is strict unless @xmath33 minimizes @xmath42 .    in setting  1 , the minimizer @xmath438 is unique , and we may utilize the following standard arguments : suppose that @xmath306 does not converge to @xmath438 .",
    "we know that @xmath439 is decreasing in @xmath209 , and all @xmath232 belong to the compact set @xmath440 . hence there would exist a subsequence @xmath441 with limit @xmath442 .",
    "but then continuity of @xmath42 and @xmath443 would imply that @xmath444    in setting  0 , note first that @xmath40 , @xmath176 and @xmath192 remain unchanged if we replace @xmath445 with @xmath446 for some number @xmath123 .",
    "hence , with the same arguments as in setting  1 , we may conclude that @xmath447 as @xmath448 , where @xmath449 .",
    "now in case of algorithm  fp an elementary calculation shows that the matrices @xmath450 satisfy the equation @xmath451 together with the equation @xmath452 this implies that @xmath453 hence the sequence @xmath454 converges to a multiple of the identity matrix . in other words",
    ", @xmath306 converges to a multiple of @xmath455 .",
    "the definition of algorithm  pn implies that for sufficiently large @xmath255 , the new candidate @xmath456 is given by @xmath457 with @xmath458 satisfying @xmath459 .",
    "hence @xmath460 for sufficiently large @xmath255 .",
    "consequently @xmath306 converges to a multiple of @xmath455 .",
    "the fact that @xmath43 is a positive multiple of @xmath49 follows from simple equivariance considerations as outlined in @xcite .",
    "now let @xmath461 with nonsingular @xmath202 , and let @xmath462 with @xmath163 .",
    "the random vector @xmath463 has a spherically symmetric distribution around @xmath48 in the sense that for any orthogonal matrix @xmath236 , the distributions of @xmath464 and @xmath463 coincide .",
    "we may write @xmath465 \\\\      & = \\ b^{-1}c          { \\mathop{\\mathrm{i\\!e}}\\nolimits}\\bigl [ \\rho'(z^\\top c^\\top \\sigma^{-1 } c z ) zz^\\top \\bigr ]          c^\\top b^{-\\top } .\\end{aligned}\\ ] ] next let @xmath466 with an orthogonal matrix @xmath236 and a vector @xmath467 containing the eigenvalues of @xmath468 , i.e.  the eigenvalues of @xmath313 .",
    "then @xmath469 for another orthogonal matrix @xmath470 , so @xmath471              v { \\mathop{\\mathrm{diag}}\\nolimits}(\\gamma)^{1/2 } \\tilde{u}^\\top \\\\      & = \\",
    "\\tilde{u } { \\mathop{\\mathrm{diag}}\\nolimits}(\\gamma)^{1/2 }          { \\mathop{\\mathrm{i\\!e}}\\nolimits}\\bigl [ \\rho'((v^\\top z)^\\top { \\mathop{\\mathrm{diag}}\\nolimits}(\\gamma ) ( v^\\top z ) ) ( v^\\top z ) ( v^\\top z)^\\top \\bigr ]              { \\mathop{\\mathrm{diag}}\\nolimits}(\\gamma)^{1/2 } \\tilde{u}^\\top \\\\      & = \\",
    "\\tilde{u } { \\mathop{\\mathrm{diag}}\\nolimits}(\\gamma)^{1/2 }          { \\mathop{\\mathrm{i\\!e}}\\nolimits}\\bigl [ \\rho'(z^\\top { \\mathop{\\mathrm{diag}}\\nolimits}(\\gamma ) z ) zz^\\top \\bigr ]              { \\mathop{\\mathrm{diag}}\\nolimits}(\\gamma)^{1/2 } \\tilde{u}^\\top \\\\      & = \\",
    "\\tilde{u } { \\mathop{\\mathrm{diag}}\\nolimits}(\\gamma)^{1/2 }          { \\mathop{\\mathrm{i\\!e}}\\nolimits}\\bigl [ \\rho ' \\bigl ( \\sum_{i=1}^q \\gamma_i z_i^2 \\bigr ) ( z_jz_k)_{j , k=1}^q \\bigr ]              { \\mathop{\\mathrm{diag}}\\nolimits}(\\gamma)^{1/2 } \\tilde{u}^\\top \\\\      & = \\",
    "\\tilde{u } { \\mathop{\\mathrm{diag}}\\nolimits}(\\gamma)^{1/2 }          { \\mathop{\\mathrm{i\\!e}}\\nolimits}\\bigl [ \\rho ' \\bigl ( \\sum_{i=1}^q \\gamma_i z_i^2 \\bigr )              { \\mathop{\\mathrm{diag}}\\nolimits}\\bigl ( ( z_j^2)_{j=1}^q \\bigr ) \\bigr ]              { \\mathop{\\mathrm{diag}}\\nolimits}(\\gamma)^{1/2 } \\tilde{u}^\\top \\\\      & = \\ \\tilde{u }          { \\mathop{\\mathrm{i\\!e}}\\nolimits}\\bigl [ \\rho ' \\bigl ( \\sum_{i=1}^q \\gamma_i z_i^2 \\bigr )              { \\mathop{\\mathrm{diag}}\\nolimits}\\bigl ( ( \\gamma_j z_j^2)_{j=1}^q \\bigr ) \\bigr ]              \\tilde{u}^\\top , \\end{aligned}\\ ] ] by spherical symmetry of the distribution of @xmath463 .",
    "hence @xmath472 with @xmath275 given by @xmath473 moreover , since @xmath474 and the distribution of @xmath475 is invariant under permuting the components of @xmath463 , @xmath476    one may also say that @xmath477 is the unique vector of eigenvalues of @xmath176 , and the columns @xmath478 of @xmath470 are corresponding eigenvectors . if we consider another spectral decomposition @xmath311 with @xmath1 having orthonormal columns @xmath479 , then @xmath480 for any vector @xmath278 such that @xmath481 whenever @xmath482 .",
    "in particular , if we choose @xmath483 , then @xmath484 +"
  ],
  "abstract_text": [
    "<S> we present new algorithms for @xmath0-estimators of multivariate scatter and location and for symmetrized @xmath0-estimators of multivariate scatter . </S>",
    "<S> the new algorithms are considerably faster than currently used fixed - point and other algorithms . </S>",
    "<S> the main idea is to utilize a taylor expansion of second order of the target functional and devise a partial newton - raphson procedure . in connection with symmetrized @xmath0-estimators we work with incomplete @xmath1-statistics to accelerate our procedures initially .    </S>",
    "<S> @xmath2work supported by swiss national science foundation . </S>",
    "<S> + @xmath3work supported by academy of finland ( grant 268703 ) .    [ </S>",
    "<S> [ ams - subject - classifications ] ] ams subject classifications : + + + + + + + + + + + + + + + + + + + + + + + + + + + +    62h12 , 65c60 .    </S>",
    "<S> [ [ key - words ] ] key words : + + + + + + + + + +    fixed - point algorithm , matrix exponential function , newton - raphson algorithm , taylor expansion .    [ </S>",
    "<S> [ corresponding - author ] ] corresponding author : + + + + + + + + + + + + + + + + + + + + +    lutz dmbgen , e - mail : duembgen@stat.unibe.ch </S>"
  ]
}