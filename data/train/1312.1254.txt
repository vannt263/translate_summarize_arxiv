{
  "article_text": [
    "_ tensor _ is a generalization of _ vector _ and _ matrix_. a vector is a first - order ( also called one - way or one - mode ) tensor , and a matrix is a second - order tensor .",
    "higher - order tensor arises in many applications such as 3d image reconstruction @xcite , video inpainting @xcite , hyperspectral data recovery @xcite , higher - order web link analysis @xcite , personalized web search @xcite , and seismic data reconstruction @xcite . in this paper",
    ", we focus on the recovery of higher - order tensors that are ( exactly or approximately ) low - rank and have missing entries .",
    "we dub the problem as _ low - rank tensor completion _ ( lrtc ) . the introduced model and algorithm",
    "can be extended in a rather straightforward way to recovering low - rank tensors from their linear measurements .",
    "lrtc can be regarded as an extension of low - rank matrix completion @xcite . to recover a low - rank tensor from its partially observed entries ,",
    "one can unfold it into a matrix and apply a low - rank matrix completion algorithm such as fpca @xcite , apgl @xcite , lmafit @xcite , the alternating direction method @xcite , the @xmath0 minimization method @xcite , and so on .",
    "however , this kind of method utilizes only one mode low - rankness of the underlying tensor .",
    "we are motivated and convinced by the results @xcite that utilizing all mode low - ranknesses of the tensor gives much better performance .",
    "existing methods for lrtc in @xcite employ matrix nuclear - norm minimization and use the singular value decomposition ( svd ) in their algorithms , which become very slow or even not applicable for large - scale problems . to tackle this difficulty , we apply _ low - rank matrix factorization _ to each mode unfolding of the tensor in order to enforce low - rankness and update the matrix factors alternatively , which is computationally much cheaper than svd .",
    "our approach is non - convex , and the sizes of the matrix factors must be specified in the algorithm .",
    "non - convexity makes it difficult for us to predict the performance of our approach in a theoretical way , and in general , the performance can vary to the choices of algorithm and starting point .",
    "we found cyclic updates of the unknown variables in the model to perform well enough .",
    "the sizes of the matrix factors dictate the rank of the recovered tensor .",
    "if they are fixed to values significantly different from the true rank , the recovery can overfit or underfit . on the other hand ,",
    "during the run time of our algorithms , there are simple ways to adaptively adjust the factor sizes . in short , the all - mode matricizations ,",
    "cyclic block minimization , and adaptive adjustment are the building blocks of our approach . before introducing our model and algorithm , we review some notation and tensor operations .",
    "following @xcite , we use bold lower - case letters @xmath1 for vectors , bold upper - case letters @xmath2 for matrices , and bold caligraphic letters @xmath3 for tensors .",
    "the @xmath4-th component of an @xmath5-way tensor @xmath6 is denoted as @xmath7 . for @xmath8",
    ", we define their inner product in the same way as that for matrices , i.e. , @xmath9 the frobenius norm of @xmath6 is defined as @xmath10    a _ fiber _ of @xmath6 is a vector obtained by fixing all indices of @xmath6 except one , and a _ slice _ of @xmath6 is a matrix by fixing all indices of @xmath6 except two .",
    "for example , if @xmath11 has two frontal slices ( with the third index fixed ) @xmath12 then @xmath13^\\top$ ] is a mode-1 fiber ( with all but the first indices fixed ) , @xmath14^\\top$ ] is a mode-2 fiber ( with all but the second indices fixed ) , and @xmath15^\\top$ ] is a mode-3 fiber ( with all but the third indices fixed ) .",
    "its two horizontal ( with the first index fixed ) and two lateral slices ( with the second index fixed ) are respectively @xmath16    the mode-@xmath17 _ matricization _ ( also called _ unfolding _ ) of @xmath18 is denoted as @xmath19 , which is a matrix with columns being the mode-@xmath17 fibers of @xmath6 in the lexicographical order .",
    "take the tensor in for example .",
    "its mode-1 and mode-3 matricizations are respectively @xmath20 relating to the matricization process , we define @xmath21 and @xmath22 to reverse the process , i.e. , @xmath23 .",
    "the @xmath17-rank of an @xmath5-way tensor @xmath6 , denoted as @xmath24 , is the rank of @xmath25 , and we define the rank of @xmath6 as an array : @xmath26 .",
    "we say @xmath6 is ( approximately ) low - rank if @xmath25 is ( approximately ) low - rank for all @xmath17 .",
    "we aim at recovering an ( approximately ) low - rank tensor @xmath27 from partial observations @xmath28 , where @xmath29 is the index set of observed entries , and @xmath30 keeps the entries in @xmath29 and zeros out others .",
    "we apply low - rank matrix factorization to each mode unfolding of @xmath31 by finding matrices @xmath32 such that @xmath33 for @xmath34 , where @xmath35 is the estimated rank , either fixed or adaptively updated .",
    "introducing one common variable @xmath36 to relate these matrix factorizations , we solve the following model to recover @xmath31 @xmath37 where @xmath38 and @xmath39 . in the model , @xmath40 , @xmath34 ,",
    "are weights and satisfy @xmath41 .",
    "the constraint @xmath42 enforces consistency with the observations and can be replaced with @xmath43 if @xmath44 is contaminated by noise with a known frobenius norm equal to @xmath45 . in this paper",
    ", we do not assume the knowledge of @xmath45 and thus use for both noiseless and noisy cases .",
    "the ranks @xmath46 in must be specified , yet we do not assume the knowledge of their true values . to address this issue , we dynamically adjust the rank estimates in two schemes .",
    "one scheme starts from overestimated ranks and then decreases them by checking the singular values of the factor matrices in each mode .",
    "when a large gap between the @xmath47th and @xmath48th singular values of the factors is found , @xmath35 is reduced to @xmath47 .",
    "the other scheme starts from underestimated ranks and then gradually increases them if the algorithm detects slow progress .",
    "we try to solve by cyclically updating @xmath49 , @xmath50 , and @xmath36 .",
    "although a global solution is not guaranteed , we demonstrate by numerical experiments that our algorithm can reliably recover a wide variety of low - rank tensors .",
    "in addition , we show that any limit point of the iterates satisfies the kkt conditions .",
    "the details will be given in section [ sec : alg ] .",
    "our model can be regarded as an extension of the following model @xcite from matrix completion to tensor completion @xmath51 where @xmath52 contains partially observed entries of the underlying ( approximately ) low - rank matrix @xmath53 . if @xmath54 in , i.e. , the underlying tensor @xmath31 is two - way , then it is easy to see that reduces to by noting @xmath55 . the problem is solved in @xcite by a successive over - relaxation ( sor ) method , named as lmafit . although is non - convex , extensive experiments on both synthetic and real - world data demonstrate that solved by lmafit performs significantly better than nuclear norm based convex models such as @xmath56 where @xmath57 denotes the nuclear norm of @xmath58 , defined as the sum of its singular values .",
    "the work @xcite generalizes to the tensor case , and to recover the ( approximately ) low - rank tensor @xmath31 , it proposes to solve @xmath59 where @xmath60 are preselected weights satisfying @xmath41 .",
    "different from our model , the problem is convex , and in @xcite , various methods are applied to solve it such as block coordinate descent method , proximal gradient method , and alternating direction method of multiplier ( admm ) .",
    "the model utilizes low - rankness of all mode unfoldings of the tensor , and as demonstrated in @xcite , it can significantly improve the solution quality over that obtained by solving , where the matrix @xmath58 corresponds to some mode unfolding of the tensor .",
    "the recent work @xcite proposes a more `` square '' convex model for recovering @xmath31 as follows : @xmath61}\\| _ * , { \\text { subject to } } { \\mathcal{p}}_\\omega({\\boldsymbol{{\\mathcal{z}}}})={\\boldsymbol{{\\mathcal{b}}}},\\ ] ] where @xmath62 is a tensor by relabeling mode @xmath63 of @xmath36 to mode @xmath17 for @xmath34 , @xmath64}=\\text{reshape}\\left({\\hat{{\\mathbf{z}}}_{(1)},\\prod_{n\\le j}i_{i_n},\\prod_{n > j}i_{i_n}}\\right),\\ ] ] and @xmath65 and the permutation @xmath4 are chosen to make @xmath66 as close as to @xmath67 .",
    "the idea of reshaping a tensor into a `` square '' matrix has also appeared in @xcite for tensor principal component analysis . as the order of @xmath31 is",
    "no more than three , is the same as with @xmath58 corresponding to some mode unfolding of the tensor , and it may not perform as well as .",
    "however , for a low - rank tensor of more than three orders , it is shown in @xcite that can exactly recover the tensor from far fewer observed entries than those required by .",
    "there are some other models proposed recently for lrtc .",
    "for example , the one in @xcite uses , as a regularization term , a tight convex relaxation of the average rank function @xmath68 and applies the admm method to solve the problem .",
    "the work @xcite directly constrains the solution in some low - rank manifold and employs the riemannian optimization to solve the problem .",
    "different from the above discussed models that use tensor @xmath17-rank , the model in @xcite employs the so - called _ tubal - rank _ based on the recently proposed tensor singular value decomposition ( t - svd ) @xcite . for details about these models ,",
    "we refer the readers to the papers where they are proposed .",
    "the rest of the paper is organized as follows .",
    "section [ sec : phase ] shows the phase transition of our proposed method and some existing ones .",
    "section [ sec : alg ] gives our algorithm with two different rank - adjusting strategies , and the convergence result of the algorithm is given in section [ sec : convg ] . in section [ sec : numerical ] , we compare the proposed method with some state - of - the - art methods for tensor completion on both synthetic and real - world data .",
    "section [ sec : conclusion ] conludes the paper , and finally , section [ sec : fig - table ] shows all figures and tables of our numerical results .",
    "a phase transition plot uses greyscale colors to depict how likely a certain kind of low - rank tensors can be recovered by an algorithm for a range of different ranks and sample ratios .",
    "phase transition plots are important means to compare the performance of different tensor recovery methods .",
    "we compare our method ( called tmac ) to the following three methods on random tensors of different kinds . in section [ sec : numerical ] , we compare them on the real - world data including 3d images and videos .    *",
    "matrix completion method for recovering low - rank tensors : we unfold the underlying @xmath5-way tensor @xmath31 along its @xmath5th mode and apply lmafit @xcite to , where @xmath58 corresponds to @xmath69",
    ". if the output is @xmath70 , then we use @xmath71 to estimate @xmath31 . *",
    "nuclear norm minimization method for tensor completion : we apply falrtc @xcite to and use the output @xmath36 to estimate @xmath31 . * square deal method : we apply fpca @xcite to and use the output @xmath36 to estimate @xmath31 .",
    "we call the above three methods as matcomp , falrtc , and squaredeal , respectively .",
    "we chose these methods due to their popularity and code availability .",
    "lmafit has been demonstrated superior over many other matrix completion solvers such as apgl @xcite , svt @xcite , and fpca @xcite ; appears to be the first convex model for tensor completion , and falrtc is the first efficient and also reliable for falrtc was set small , falrtc could also produce solutions of high accuracy . ] solver of ; the work @xcite about squaredeal appears the first work to give theoretical guarantee for low - rank higher - order tensor completion .",
    "we set the stopping tolerance to @xmath72 for all algorithms except fpca that uses @xmath73 since @xmath72 appears too loose for fpca .",
    "note that the tolerances used here are tighter than those in section [ sec : numerical ] because we care more about the models recoverability instead of algorithms efficiency .    if the relative error @xmath74 the recovery was regarded as successful , where @xmath75 denotes the recovered tensor .",
    "two gaussian random datasets were tested .",
    "each tensor in the first dataset was 3-way and had the form @xmath76 , where @xmath77 was generated by matlab command ` randn(r , r , r ) ` and @xmath78 by ` randn(50,r ) ` for @xmath79 .",
    "we generated @xmath29 uniformly at random .",
    "the rank @xmath80 varies from 5 to 35 with increment 3 and the sample ratio @xmath81 from 10% to 90% with increment 5% . in the second dataset ,",
    "each tensor was 4-way and had the form @xmath82 , where @xmath77 was generated by matlab command ` randn(r , r , r , r ) ` and @xmath78 by ` randn(20,r ) ` for @xmath83 .",
    "the rank @xmath80 varies from 4 to 13 with increment 1 and sr from 10% to 90% with increment 5% . for each setting",
    ", 50 independent trials were run .",
    "figure [ fig : rate-3 g ] depicts the phase transition plots of tmac , matcomp , and falrtc for the 3-way dataset , and figure [ fig : rate-4 g ] depicts the phase transition plots of tmac and squaredeal for the 4-way dataset .",
    "since lmafit usually works better than fpca for matrix completion , we also show the result by applying lmafit to @xmath84}\\|_f^2 , { \\text { subject to } } { \\mathcal{p}}_\\omega({\\boldsymbol{{\\mathcal{z}}}})={\\boldsymbol{{\\mathcal{b}}}},\\ ] ] where @xmath85}$ ] is the same as that in . from the figures ,",
    "we see that tmac performed much better than all the other compared methods .",
    "note that our figure ( also in figures [ fig:4way - rand ] and [ fig:4way - plaw ] ) for squaredeal looks different from that shown in @xcite , because we fixed the dimension of @xmath31 and varied the rank @xmath80 while @xcite fixes @xmath86 to an array of very small values and varies the dimension of @xmath31 .",
    "the solver and the stopping tolerance also affect the results .",
    "for example , the `` square '' model solved by lmafit gives much better results but still worse than those given by tmac .",
    "in addition , figure [ fig : diff - mode ] depicts the phase transition plots of tmac utilizing 1 , 2 , 3 , and 4 modes of matricization on the 4-way dataset .",
    "we see that tmac can recover more tensors as it uses more modes .",
    "this section tests the recoverability of tmac , matcomp , falrtc , and squaredeal on two datasets in which the tensor factors have uniformly random entries . in the first dataset , each tensor had the form @xmath76 , where @xmath77 was generated by matlab command ` rand(r , r , r)-0.5 ` and @xmath78 by ` rand(50,r)-0.5 ` .",
    "each tensor in the second dataset had the form @xmath82 , where @xmath77 was generated by matlab command ` rand(r , r , r , r)-0.5 ` and @xmath78 by ` rand(20,r)-0.5 ` .",
    "figure [ fig:3way - rand ] shows the recoverability of each method on the first dataset and figure [ fig:4way - rand ] on the second dataset .",
    "we see that tmac with both rank - fixing and rank - increasing strategies performs significantly better than the other compared methods .",
    "this section tests the recoverability of tmac , matcomp , falrtc , and squaredeal on two more difficult synthetic datasets . in the first dataset , each tensor had the form @xmath76 , where @xmath77 was generated by matlab command ` rand(r , r , r ) ` and @xmath78 by ` orth(randn(50,r))*diag([1:r].^(-0.5 ) ) ` .",
    "note that the core tensor @xmath77 has nonzero - mean entries and each factor matrix has power - law decaying singular values .",
    "this kind of low - rank tensor appears more difficult to recover compared to the previous random low - rank tensors .",
    "each tensor in the second dataset had the form @xmath82 , where @xmath77 was generated by matlab command ` rand(r , r , r , r ) ` and @xmath78 by ` orth(randn(20,r))*diag([1:r].^(-0.5 ) ) ` .",
    "for these two datasets , tmac with rank - decreasing strategy can never decrease @xmath35 to the true rank and thus performs badly .",
    "figure [ fig:3way - plaw ] shows the recoverability of each method on the first dataset and figure [ fig:4way - plaw ] on the second dataset .",
    "again , we see that tmac with both rank - fixing and rank - increasing strategies performs significantly better than the other compared methods .",
    "we apply the alternating least squares method to . since the model needs an estimate of @xmath86 , we provide two strategies to dynamically adjust the rank estimates .",
    "the model is convex with respect to each block of the variables @xmath87 and @xmath36 while the other two are fixed .",
    "hence , we cyclically update @xmath87 and @xmath36 one at a time .",
    "let @xmath88 be the objective of .",
    "we perform the updates as    [ eq : update ] @xmath89    note that both and can be decomposed into @xmath5 independent least squares problems , which can be solved in parallel .",
    "the updates in can be explicitly written as    [ eq : exupdate ] @xmath90    where @xmath91 denotes the moore - penrose pseudo - inverse of @xmath92 , @xmath93 is the complement of @xmath29 , and we have used the fact that @xmath94 in .    no matter how @xmath95 is computed , only the products @xmath96 , affect @xmath36 and thus the recovery @xmath31 .",
    "hence , we shall update @xmath49 in the following more efficient way @xmath97 which together with gives the same products @xmath98 , as those by and according to the following lemma , which is similar to lemma 2.1 in @xcite .",
    "we give a proof here for completeness .",
    "[ lem : equiv ] for any two matrices @xmath99 , it holds that @xmath100    let @xmath101 be the compact svd of @xmath102 , i.e. , @xmath103 , and @xmath104 is a diagonal matrix with all positive singular values on its diagonal .",
    "it is not difficult to verify that @xmath105 . then @xmath106 where we have used @xmath107 for any @xmath108 of appropriate sizes in the second equality . on the other hand , @xmath109 hence , we have the desired result .",
    "the problem requires one to specify the ranks @xmath46 . if they are fixed , then a good estimate is important for to perform well .",
    "too small @xmath35 s can cause underfitting and a large recovery error whereas too large @xmath35 s can cause overfitting and large deviation to the underlying tensor @xmath31 . since we do not assume the knowledge of @xmath86 , we provide two schemes to dynamically adjust the rank estimates @xmath46 . in our algorithm , we use parameter @xmath110 to determine which one of the two scheme to apply . if @xmath111 , the rank - decreasing scheme is applied to @xmath35 ; if @xmath112 , the rank - increasing scheme is applied to @xmath35 ; otherwise , @xmath35 is fixed to its initial value .",
    "this scheme starts from an input overestimated rank , i.e. , @xmath113 .",
    "following @xcite , we calculate the eigenvalues of @xmath114 after each iteration , which are assumed to be ordered as @xmath115 .",
    "then we compute the quotients @xmath116 .",
    "suppose @xmath117 if @xmath118 which means a `` big '' gap between @xmath119 and @xmath120 , then we reduce @xmath35 to @xmath47 .",
    "assume that the svd of @xmath121 is @xmath122 .",
    "then we update @xmath95 to @xmath123 and @xmath124 to @xmath125 , where @xmath126 is a submatrix of @xmath127 containing @xmath47 columns corresponding to the largest @xmath47 singular values , and @xmath128 and @xmath129 are obtained accordingly .",
    "we observe in our numerical experiments that this rank - adjusting scheme generally works well for exactly low - rank tensors .",
    "because , for these tensors , @xmath130 can be very large and easy to identify , the true rank is typically obtained after just one rank adjustment . for approximately low - rank tensors , however , a large gap may or may not exist , and when it does not , the rank overestimates will not decrease . for these tensors ,",
    "the rank - increasing scheme below works better .",
    "this scheme starts an underestimated rank , i.e. , @xmath131 .",
    "following @xcite , we increase @xmath35 to @xmath132 at iteration @xmath133 if @xmath134 which means `` slow '' progress in the @xmath35 dimensional space along the @xmath17-th mode . here",
    ", @xmath135 is a positive integer , and @xmath136 is the maximal rank estimate . let the economy qr factorization of @xmath137 be @xmath138 .",
    "we augment @xmath139 $ ] where @xmath140 has @xmath135 randomly generated columns and then orthonormalize @xmath141 .",
    "next , we update @xmath142 to @xmath143 and @xmath144 $ ] , where @xmath145 is an @xmath146 _ zero _ matrix , appending any matrix of appropriate size after @xmath95 does not make any difference . ] .",
    "numerically , this scheme works well not only for exactly low - rank tensors but also for approximately low - rank ones .",
    "however , for exactly low - rank tensors , this scheme causes the algorithm to run longer than the rank - decreasing scheme .",
    "figure [ fig : dec - inc ] shows the performance of our algorithm equipped with the two rank - adjusting schemes . as in section [ sec : randtest ] , we randomly generated @xmath31 of size @xmath147 with @xmath148 , and we started tmac with 25% overestimated ranks for rank - decreasing scheme and 25% underestimated ranks for rank - increasing scheme . from figure",
    "[ fig : dec - inc ] , we see that tmac with the rank - decreasing scheme is better when @xmath80 is small while tmac with the rank - increasing scheme becomes better when @xmath80 is large .",
    "we can also see from the figure that after @xmath35 is adjusted to match @xmath149 , our algorithm converges linearly .",
    "in general , tmac with the rank - increasing scheme works no worse than it does with the rank - decreasing scheme in terms of solution quality no matter the underlying tensor is exactly low - rank or not . numerically , we observed that if the rank is relatively low , tmac with the rank - decreasing scheme can adjust the estimated rank to the true one within just a few iterations and converges fast , while tmac with the rank - increasing scheme may need more time to get a comparable solution .",
    "however , if the underlying tensor is approximately low - rank , or it is exactly low - rank but the user concerns more on solution quality , tmac with the rank - increasing scheme is always preferred , and small @xmath135 s usually give better solutions at the cost of more running time .",
    "the above discussions are distilled in algorithm [ alg : als ] .",
    "after the algorithm terminates with output @xmath150 , we use @xmath151 to estimate the tensor @xmath31 , which is usually better than @xmath36 when the underlying @xmath31 is only approximately low - rank or the observations are contaminated by noise , or both .",
    "@xmath29 , @xmath28 , and @xmath152 with @xmath153 .",
    "@xmath155 with @xmath156 .    in algorithm",
    "[ alg : als ] , we can have different rank - adjusting schemes , i.e. , different @xmath110 , for different modes .",
    "for simplicity , we set @xmath111 or @xmath112 uniformly for all @xmath17 in our experiments .",
    "introducing lagrangian multiplier @xmath157 for the constraint @xmath42 , we write the lagrangian function of @xmath158 letting @xmath159 and @xmath160 , we have the kkt conditions    [ eq : kkt ] @xmath161    our main result is summarized in the following theorem .",
    "[ thm : main ] suppose @xmath162 is a sequence generated by algorithm [ alg : als ] with fixed @xmath35 s and fixed positive @xmath40 s .",
    "let @xmath163 .",
    "then any limit point of @xmath164 satisfies the kkt conditions in .",
    "our proof of theorem [ thm : main ] mainly follows @xcite .",
    "the literature has work that analyzes the convergence of alternating minimization method for non - convex problems such as @xcite .",
    "however , to the best of our knowledge , none of them implies our convergence result .    before giving the proof",
    ", we establish some important lemmas that are used to establish our main result .",
    "we begin with the following lemma , which is similar to lemma 3.1 of @xcite .",
    "[ lem : diff1 ] for any matrices @xmath165 and @xmath166 of appropriate sizes , letting @xmath167 then we have @xmath168    from lemma [ lem : equiv ] , it suffices to prove by letting @xmath169 .",
    "assume the compact svds of @xmath170 and @xmath102 are @xmath171 and @xmath172 .",
    "noting @xmath173 and @xmath174 , we have @xmath175 similarly , noting @xmath176 and @xmath177 , we have @xmath178\\nonumber\\\\ = & { \\mathbf{u}}_{\\tilde{a}}{\\mathbf{u}}_{\\tilde{a}}^\\top({\\mathbf{c}}-{\\mathbf{a}}{\\mathbf{b}})({\\mathbf{i}}-{\\mathbf{v}}_b{\\mathbf{v}}_b^\\top)\\label{eq2}\\end{aligned}\\ ] ] where the third equality is from . summing and gives @xmath179 since @xmath180 is orthogonal to @xmath181 , we have @xmath182 in addition , note @xmath183 and @xmath184 hence , @xmath185 and thus @xmath186 then can be shown by noting @xmath187 this completes the proof .",
    "we also need the following lemma .",
    "[ lem : range ] for any two matrices @xmath102 and @xmath166 of appropriate sizes , it holds that    @xmath188^\\dagger ( { \\mathbf{c}}{\\mathbf{b}}^\\top)^\\top{\\mathbf{c}}\\right)\\cr = & \\ { \\mathcal{r}}_r\\left(\\big[({\\mathbf{c}}{\\mathbf{b}}^\\top({\\mathbf{b}}{\\mathbf{b}}^\\top)^\\dagger)^\\top ( { \\mathbf{c}}{\\mathbf{b}}^\\top({\\mathbf{b}}{\\mathbf{b}}^\\top)^\\dagger)\\big]^\\dagger \\big[{\\mathbf{c}}{\\mathbf{b}}^\\top({\\mathbf{b}}{\\mathbf{b}}^\\top)^\\dagger\\big]^\\top{\\mathbf{c}}\\right),\\label{range - row}\\end{aligned}\\ ] ]    where @xmath189 and @xmath190 denote the column and row space of @xmath92 , respectively .    following the proof of lemma [ lem : equiv ] , we assume the compact svd of @xmath102 to be @xmath101 .",
    "then @xmath191 for any vector @xmath192 of appropriate size , there must be another vector @xmath193 such that @xmath194 or equivalently @xmath195 .",
    "hence @xmath196 , which indicates @xmath197 . in the same way",
    ", one can show the reverse inclusion , and hence @xmath198 .",
    "the result can be shown similarly by noting that @xmath199^\\dagger ( { \\mathbf{c}}{\\mathbf{b}}^\\top)^\\top{\\mathbf{c}}&={\\mathbf{u}}{\\boldsymbol{\\sigma}}{\\mathbf{v}}^\\top({\\mathbf{c}}^\\top{\\mathbf{c}})^\\dagger{\\mathbf{v}}{\\mathbf{v}}^\\top{\\mathbf{c}}^\\top{\\mathbf{c}},\\\\ \\big[({\\mathbf{c}}{\\mathbf{b}}^\\top({\\mathbf{b}}{\\mathbf{b}}^\\top)^\\dagger)^\\top ( { \\mathbf{c}}{\\mathbf{b}}^\\top({\\mathbf{b}}{\\mathbf{b}}^\\top)^\\dagger)\\big]^\\dagger \\big[{\\mathbf{c}}{\\mathbf{b}}^\\top({\\mathbf{b}}{\\mathbf{b}}^\\top)^\\dagger\\big]^\\top{\\mathbf{c}}&={\\mathbf{u}}{\\boldsymbol{\\sigma}}^{-1}{\\mathbf{v}}^\\top({\\mathbf{c}}^\\top{\\mathbf{c}})^\\dagger{\\mathbf{v}}{\\mathbf{v}}^\\top{\\mathbf{c}}^\\top{\\mathbf{c}}.\\end{aligned}\\ ] ] this completes the proof .    according to lemma [ lem : range ] ,",
    "it is not difficult to get the following corollary .",
    "[ cor : diff ] for any matrices @xmath165 and @xmath166 of appropriate sizes , let @xmath200 if the compact svds of @xmath170 and @xmath102 are @xmath171 and @xmath172 , then we have .",
    "we are now ready to prove theorem [ thm : main ] .",
    "let @xmath201 be a limit point of @xmath164 , and thus there is a subsequence @xmath202 converging to @xmath203 . according to , we have @xmath204 , and @xmath205 .",
    "hence , and hold at @xmath206 for all @xmath207 and thus at @xmath203 .    from lemma [ lem : diff1 ] , it follows that @xmath208 in addition , it is not difficult to verify @xmath209 summing up and and observing that @xmath210 is lower bounded by _ zero _ , we have @xmath211 and thus @xmath212    for each @xmath17 and @xmath207 , let the compact svds of @xmath213 and @xmath214 be @xmath215 and @xmath216 . letting @xmath217 , and @xmath218 in , we have @xmath219 which together with gives    @xmath220    since @xmath221 and @xmath222 is bounded , then right multiplying @xmath223 for @xmath224 on both sides of yields",
    "@xmath225 which indicates that is satisfied at @xmath203 . from and , we have @xmath226 which together with the boundedness of @xmath227 and @xmath228 gives @xmath229 and thus is satisfied at @xmath203 .",
    "this completes the proof .",
    "this section tests algorithm [ alg : als ] , tmac , for solving . to demonstrate its effectiveness , we compared it with matcomp and falrtc ( see section [ sec : phase ] ) on real - world data .",
    "the parameters @xmath230 in were uniformly set to @xmath231 at the beginning of tmac . during the iterations",
    ", we either fixed them or dynamically updated them according to the fitting error @xmath232 the smaller @xmath233 is , the larger @xmath40 should be . specifically ,",
    "if the current iterate is @xmath234 , we set @xmath235^{-1}}{\\sum_{i=1}^n\\big[{\\mathbf{fit}}_i({\\mathbf{x}}_i^k{\\mathbf{y}}_i^k)\\big]^{-1}},\\ n=1,\\ldots , n.\\ ] ] as demonstrated below , dynamic updating @xmath40 s can improve the recovery quality for tensors that have better low - rankness in one mode than others .",
    "tmac was terminated if one of the following conditions was satisfied for some @xmath207 @xmath236 where @xmath237 is a small positive value specified below .",
    "the condition checks the relative change of the overall fitting , and is satisfied if the weighted fitting is good enough .",
    "this section compares tmac , matcomp , and falrtc on a @xmath238 brain mri data , which has been used in @xcite .",
    "the data is approximately low - rank : for its three mode unfodings , the numbers of singular values larger than 0.1% of the largest one are 28 , 33 , and 29 , respectively .",
    "one slice of the data is shown in figure [ fig : mri ] .",
    "we tested all three methods on both noiseless and noisy data .",
    "specifically , we added scaled gaussian noise to the original data to have @xmath239 and made noisy observations @xmath240 , where @xmath241 denotes the maximum absolute value of @xmath31 , and the entries of @xmath242 follow idendically independent standard gaussian distribution .",
    "we ran all the algorithms to maximum 1000 iterations .",
    "the stopping tolerance was set to @xmath243 for tmac and lmafit .",
    "for falrtc , @xmath244 was set since we found @xmath245 was too loose .",
    "both tmac and lmafit used the rank - increasing strategy .",
    "for tmac , we initialized @xmath246 and set @xmath247 , and for lmafit , we set initial rank @xmath248 , increment @xmath249 , and maximal rank @xmath250 .",
    "we tested tmac with fixed parameters @xmath251 and also dynamically updated ones by starting from @xmath252 .",
    "the smoothing parameter for falrtc was set to its default value @xmath253 and weight parameters set to @xmath254 table [ table : mri ] shows the average relative errors and running times of five independent trials for each setting of @xmath255 and sr .",
    "figure [ fig : mri ] shows one noisy masked slice and the corresponding recovered slices by different methods with the setting of @xmath256 and @xmath257 . from the results , we see that tmac consistently reached lower relative errors than those by falrtc and cost less time .",
    "matcomp used the least time and could achieve low relative error as sr is large .",
    "however , for low sr s ( e.g. , sr=10% ) , it performed extremely bad , and even we ran it to more iterations , say 5000 , it still performed much worse than tmac and falrtc .",
    "in addition , tmac using fixed @xmath40 s worked similarly well as that using dynamically updated ones , which should be because the data has similar low - rankness along each mode .",
    "this section compares tmac , matcomp , and falrtc on a @xmath258 hyperspectral data , one slice of which is shown in figure [ fig : hyp ] .",
    "this data is also approximately low - rank : for its three mode unfoldings , the numbers of singular values larger than 1% of the largest one are 19,19 , and 4 , respectively .",
    "however , its low - rank property is not as good as that of the above mri data .",
    "its numbers of singular values larger than 0.1% of the largest one are 198 , 210 , and 18 .",
    "hence , its mode-3 unfolding has better low - rankness , and we assigned larger weight to the third mode . for falrtc , we set @xmath259 . for tmac , we tested it with fixed weights @xmath259 and also with dynamically updated ones by starting from @xmath260 .",
    "all other parameters of the three methods were set as the same as those used in the previous test .    for each setting of @xmath255 and sr",
    ", we made 5 independent runs .",
    "table [ table : hyp ] reports the average results of each tested method , and figure [ fig : hyp ] shows one noisy slice with 90% missing values and 5% gaussian noise , and the corresponding recovered slices by the compared methods . from the results",
    ", we see again that tmac outperformed falrtc in both solution quality and running time , and matcomp gave the largest relative errors at the cost of least time .",
    "in addition , tmac with dynamically updated @xmath40 s worked better than that with fixed @xmath40 s as sr was relatively large ( e.g. , sr=30% , 50% ) , and this should be because the data has much better low - rankness along the third mode than the other two .",
    "as sr was low ( e.g. , sr=10% ) , tmac with dynamically updated @xmath40 s performed even worse .",
    "this could be explained by the case that all slices might have missing values at common locations ( i.e. , some mode-3 fibers were entirely missing ) as sr was low , and in this case , the third mode unfolding had some entire columns missing . in general , it is impossible for any matrix completion solver to recover an entire missing column or row of a matrix , and thus putting more weight on the third mode could worsen the recovery . that also explains why matcomp gave much larger relative errors than those by falrtc but the slice recovered by matcomp looks better than that by falrtc in figure [ fig : hyp ] .",
    "note that there are lots of black points on the slice given by matcomp , and these black points correspond to missing columns of the third mode unfolding .",
    "therefore , we do not recommend to dynamically update @xmath40 s in tmac when sr is low or some fibers are entirely missing .      in this section ,",
    "we compared tmac , matcomp , and falrtc on both grayscale and color videos .",
    "the grayscale videosrbecker / escalator_data.mat ] has 200 frames with each one of size @xmath261 , and the color video has 150 frames with each one of size @xmath262 .",
    "we treated the grayscale video as a @xmath263 tensor and the color video as three @xmath264 tensors , one for each channel .",
    "for the grayscale video , the numbers of singular values larger than 1% of the largest one for each mode unfolding are 79 , 84 , and 35 .",
    "hence , its rank is not low , and it is relatively difficult to recover this video .",
    "the color video has lower rank . for each of its three channel tensors ,",
    "the numbers of singular values larger than 1% of the largest one are about , @xmath265 , and @xmath266 respectively for three channel tensors . ]",
    "50 , 50 , and 24 . during each run , the three channel tensors had the same index set of observed entries , which is the case in practice , and we recovered each channel independently .",
    "we set @xmath267 for falrtc and tmac , while the latter was tested with both fixed @xmath40 s and dynamically updated one by .",
    "all the other parameters of the test methods were set as the same as those in the previous test .",
    "the average results of 5 independent runs were reported in table [ table : grayvideo ] for the grayscale video and table [ table : colorvideo ] for the color video .",
    "figure [ fig : grayvideo ] shows one frame of recovered grayscale video by each method and figure [ fig : colorvideo ] one frame of recovered color video . from the tables",
    ", we see that the comparisons are similar to those for the previous hyperspectral data recovery .",
    "we have proposed a new method for low - rank tensor completion .",
    "our model utilizes low - rank matrix factorizations to all - mode unfoldings of the tensor .",
    "synthetic data tests demonstrate that our model can recover significantly more low - rank tensors than two nuclear norm based models and one model that performs low - rank matrix factorization to only one mode unfolding .",
    "in addition , numerical results on 3d images and videos show that our method consistently produces the best solutions among all compared methods and outperforms the nuclear norm minimization method in both solution quality and running time .",
    "numerically , we have observed that our algorithm converges fast ( e.g. , linear convergence in figure [ fig : dec - inc ] ) .",
    "papers @xcite demonstrate that the sor technique can significantly accelerate the progress of alternating least squares .",
    "however , we did not observe any acceleration applying the same technique to our algorithm . in the future",
    ", we will explore the reason and try to develop other techniques to accelerate our algorithm .",
    "we also plan to incorporate the objective term in to enrich , if the underlying low - rank tensor has more than three orders .",
    "we give all figures and tables of our numerical results in this section .     +     +     +     +     +     +     +    [ cols=\"^,^ \" , ]",
    "the authors thank two anonymous referees and the associate editor for their very valuable comments .",
    "y. xu is supported by nsf grant eccs-1028790 .",
    "r. hao and z. su are supported by nsfc grants 61173103 and u0935004 .",
    "r. hao s visit to ucla is supported by china scholarship council .",
    "w. yin is partially supported by nsf grants dms-0748839 and dms-1317602 , and aro / arl muri grant fa9550 - 10 - 1 - 0567 .                                                            , _ a block coordinate descent method for regularized multi - convex optimization with applications to nonnegative tensor factorization and completion _",
    ", siam journal on imaging sciences , 6 ( 2013 ) , pp ."
  ],
  "abstract_text": [
    "<S> higher - order low - rank tensors naturally arise in many applications including hyperspectral data recovery , video inpainting , seismic data reconstruction , and so on . we propose a new model to recover a low - rank tensor by simultaneously performing low - rank matrix factorizations to the all - mode matricizations of the underlying tensor . </S>",
    "<S> an alternating minimization algorithm is applied to solve the model , along with two adaptive rank - adjusting strategies when the exact rank is not known . </S>",
    "<S> phase transition plots reveal that our algorithm can recover a variety of synthetic low - rank tensors from significantly fewer samples than the compared methods , which include a matrix completion method applied to tensor recovery and two state - of - the - art tensor completion methods . </S>",
    "<S> further tests on real - world data show similar advantages . </S>",
    "<S> although our model is non - convex , our algorithm performs consistently throughout the tests and gives better results than the compared methods , some of which are based on convex models . in addition , subsequence convergence of our algorithm can be established in the sense that any limit point of the iterates satisfies the kkt condtions .    </S>",
    "<S> yangyang xu    ruru hao    wotao yin    zhixun su </S>"
  ]
}