{
  "article_text": [
    "we seek to solve large nonsymmetric systems of linear equations and eigenvalue problems using variants of the gmres  @xcite and arnoldi  @xcite methods that change the inner product each time the method is restarted .    to solve @xmath0 for the unknown @xmath1 , gmres computes the approximate solution @xmath2 from the krylov subspace @xmath3 that minimizes the norm of the residual @xmath4 . in practice this residual is usually optimized in the ( euclidean ) 2-norm , but the algorithm can be implemented in any norm induced by an inner product .",
    "this generality has been studied in theoretical settings ( e.g. ,  @xcite ) and occasionally invoked in practical computations .",
    "the inner product does not affect the spectrum of @xmath5 , nor the approximation subspace @xmath6 ( assuming the method is not restarted ) , but it can significantly alter the departure of @xmath5 from normality and thus the conditioning of the eigenvalues .",
    "numerous authors have proposed inner products in which certain preconditioned saddle point systems are self - adjoint , thus permitting the use of short - recurrence krylov methods in place of the long - recurrences required in the 2-norm case  @xcite .",
    "recently pestana and wathen have used non - standard inner products to inform the design of preconditioners  @xcite .    for any inner product @xmath7 on @xmath8 ,",
    "there exists a matrix @xmath9 for which @xmath10 for all @xmath11 , and @xmath12 is hermitian positive definite in the euclidean inner product . with the @xmath12-inner product",
    "is associated the norm @xmath13 .",
    "gmres can be implemented in the @xmath12-inner product by simply replacing all norm and inner product computations in the standard gmres algorithm .",
    "any advantage gained by optimizing in this new geometry must be balanced against the extra expense of computing the inner products : essentially one matrix - vector product with @xmath12 for each inner product and norm evaluation .    in 1998",
    "essai proposed an intriguing way to utilize the inner product in restarted gmres  @xcite . in his algorithm",
    "the inner product is very simple ( @xmath12 is diagonal ) and _ the inner product changes each time gmres is restarted_. his computational experiments show that this `` weighting '' can significantly improve performance for restarted gmres , especially for difficult problems and frequent restarts .",
    "subsequent work on weighted gmres  @xcite and the related arnolid method  @xcite has focused on how the algorithms perform on large - scale examples . in this work",
    "we offer a new perspective on why weighting helps gmres ( or not ) , justified by some simple analysis and illustrated by careful computational experiments",
    ".    describes essai s weighted gmres algorithm . in [ sec : analysis ] , we provide some basic analysis for weighted gmres and illustrative experiments , arguing that the algorithm performs well when the eigenvectors are localized . in [ sec : fft ] we propose a new variant that combines weighting with the discrete cosine transform , which localizes eigenvectors in many common situations .   a weighted version of gmres with deflated restarting ( gmres - dr  @xcite ) is developed in [ sec : drwgmres ] .",
    "this algorithm both solves linear equations and computes eigenvalues and eigenvectors .",
    "the eigenvectors improve convergence of the linear equation solver , and a weighted inner product can improve convergence further .",
    "finally , in [ sec : arnoldi ] , we apply weighted inner products to improve convergence of the thick - restart arnoldi method for computing eigenvalues and eigenvectors .",
    "the standard restarted gmres algorithm  @xcite computes the residual - minimizing approximation from a degree-@xmath14 krylov subspace , then uses the residual associated with this solution estimate to generate a new krylov subspace .",
    "we call this algorithm _",
    "gmres(@xmath14 ) _ , and every set of @xmath14 iterations after a restart a _",
    "cycle_. essai s variant  @xcite , which we call _ w - gmres(@xmath14 ) _ , changes the inner product at the beginning of each cycle of restarted gmres , with weights determined by the residual vector obtained from the last cycle .",
    "let @xmath15 denote the gmres(@xmath14 ) residual after @xmath16  cycles .",
    "then the weight matrix defining the inner product for the next cycle is @xmath17 , where @xmath18 where @xmath19 denotes the @xmath20th entry of @xmath21 .",
    "essai scales these entries differently , but , as cao and yu  @xcite note , scaling the inner product by a constant does not affect the residuals produced by the algorithm . ]",
    "the weighted inner product is then @xmath10 and the associated norm is @xmath22 . at the end of a cycle ,",
    "the residual vector @xmath21 is computed and from it the inner product for the next cycle is built .",
    "we have found it helpful to restrict the diagonal entries in  ( [ eq : essai ] ) to be at least @xmath23 , thus limiting the condition number of @xmath12 to @xmath24 .",
    "finite precision arithmetic influences gmres in nontrivial ways ; see , e.g. , ( * ? ? ?",
    "weighted inner products can further complicate this behavior ; a careful study of this interesting phenomenon is beyond the scope of the present investigation , but we note that rozlonk et al .",
    "@xcite have studied orthogonalization algorithms in non - standard inner products .",
    "( although reorthogonalization is generally not needed for gmres  @xcite , all tests here use one step of full reothogonalization for extra stability . running the experiments in [ sec : wgmres ] without this step yields qualitatively similar results , but the iteration counts can differ .    since @xmath12 is diagonal , the inner product requires @xmath25 operations ( additions and multiplications ) , instead of @xmath26 for the euclidean inner product , increasing the expense of the gram ",
    "schmidt orthogonalization in gmres by roughly @xmath27 .",
    "for very sparse @xmath5 this can be significant , but if the matrix - vector product is expensive , the relative cost of weighting is negligible .",
    "how can weighting help ?",
    "suppose we restart gmres after every @xmath14 iterations .",
    "gmres minimizes a residual of the form @xmath28 , where @xmath29 is a polynomial of degree no greater than @xmath14 , normalized so that @xmath30 .",
    "when @xmath5 is nearly a normal matrix , one learns much from the magnitude of @xmath29 on the spectrum of  @xmath5 .  loosely speaking ,",
    "since gmres must minimize the norm of the residual , the optimal @xmath29 can not target isolated eigenvalues near the origin , since @xmath29 would then be large at other eigenvalues .",
    "this constraint often causes uniformly slow convergence of restarted gmres from cycle to cycle ; the restarted algorithm fails to match the `` superlinear '' convergence enabled by higher degree polynomials in full gmres  @xcite .",
    "( these higher degree polynomials can target a few critical eigenvalues with a few roots , while having sufficiently many roots remaining to be uniformly small on the rest of the spectrum . )",
    "weighting can potentially skew the geometry to favor troublesome eigenvalues that otherwise impede convergence , leaving easy components to be eliminated quickly in subsequent cycles . in aggregate",
    ", weighting can produce a string of iterates , each sub - optimal in the 2-norm , that collectively give faster convergence than standard restarted gmres , with its locally optimal ( but globally sub - optimal ) iterates .",
    "essai s strategy for the inner product relies on the intuition that one should emphasize those components of the residual vector that have the largest magnitude . intuitively speaking , these components",
    "have thus far been neglected by the method , and may benefit from some preferential treatment . by analyzing the spectral properties of the weighted gmres algorithm in [ sec : analysis ] , we explain when this intuition is valid , show how it can go wrong , and offer a possible remedy .",
    "essai showed that his weighting can give significantly faster convergence for some problems , but did not provide guidance on matrix properties for which this was the case .",
    "later , saberi najafi and zareamoghaddam  @xcite showed that taking @xmath12 to be diagonal with entries from a _ random _ uniform distribution on @xmath31 $ ] can sometimes be effective , too .",
    "weighting appears to be particularly useful when the gmres(@xmath14 ) restart parameter @xmath14 is small , though it can help for larger @xmath14 when the problem is difficult .",
    "we seek insight into this behavior through experiment and analysis .",
    "[ ex : add20 ] we first test w - gmres on an example used by essai  @xcite , the matrix add20 from the matrix market collection  @xcite .",
    "( essai apparently uses the term `` iterations '' for what we call `` cycles '' here . ) this nonsymmetric matrix has dimension @xmath32 ; for the right - hand side @xmath33 we use a random normal(0,1 ) vector .",
    "compares gmres(@xmath14 ) and w - gmres(@xmath14 ) for @xmath34 and @xmath35 . in terms of matrix - vector products",
    "( with @xmath5 ) , w - gmres(6 ) converges about  10 times faster than gmres(6 ) ; the relative improvement for @xmath35 is less extreme , but w - gmres is still better by more than a factor of two .",
    "( full gmres converges to @xmath36 in about 509  iterations . )",
    "note that the @xmath37 convergence curves for w - gmres do not decrease monotonically , since the method generates residuals that are only minimized in the weighted norm , not the 2-norm .",
    "convergence of full gmres , gmres(@xmath14 ) , and w - gmres(@xmath14 ) with @xmath34 and @xmath35 for the add20 matrix . in this and",
    "all subsequent illustrations , the residual norm is measured in the standard 2-norm for all algorithms .",
    "full gmres converges to this tolerance in about 509  iterations . ]",
    "is weighting typically this effective ?",
    "essai shows several other tests where weighting helps , though not as much as for the add20 problem .",
    "cao and yu  @xcite give evidence that suggests weighting is not as effective for preconditioned problems , though it can still help .",
    "in their recent study of harmonic ritz values for w - gmres , gttel and pestana arrive at a similar conclusion after conducting extensive numerical experiments on 109 ilu - preconditioned test matrices : `` weighted gmres may outperform unweighted gmres for some problems , but more often this method is not competitive with other krylov subspace methods  ''",
    "@xcite and `` we believe that wgmres should not be used in combination with preconditioners , although we are aware that for some examples it may perform satisfactorily ''  @xcite .",
    "yet weighting is fairly inexpensive and can improve convergence for some difficult problems . when weighting works , what makes it work ?",
    "analyzing w - gmres(@xmath14 ) must be at least as challenging as understanding standard restarted gmres , an algorithm for which results are quite limited .",
    "( one knows conditions under which gmres(@xmath14 ) converges for all initial residuals  @xcite , and about cyclic behavior for gmres(@xmath38 ) for normal @xmath5  @xcite .",
    "few other rigorous results are available . )",
    "restarted gmres is a nonlinear dynamical system involving the entries of @xmath5 and the initial residual ; experiments suggest its convergence can depend sensitively on the initial residual  @xcite . with these challenges in mind ,",
    "we seek to gain some insight into the performance of w - gmres by first studying a setting in which the algorithm is ideally suited .",
    "let @xmath39 denote a positive definite matrix that induces the inner product @xmath40 on @xmath41 , with induced norm @xmath42 given the initial guess @xmath43 and residual @xmath44 , at the @xmath16th step gmres in the @xmath12-inner product computes the residual @xmath45 that satisfies @xmath46              & = & \\mingmres \\|\\poly(sas^{-1 } ) sr_0\\|_2.\\end{aligned}\\ ] ] thus , before any restarts are performed , w - gmres applied to @xmath47 is equivalent to 2-norm gmres applied to @xmath48 .",
    "( from this observation gttel and pestana propose an alternative implementation of w - gmres , their algorithm  2  @xcite . )",
    "this connection between w - gmres and standard gmres via the similarity transformation @xmath49 was first noted in an abstract by gutknecht and loher  @xcite , who considered `` preconditioning by similarity transformation ; '' further details have not been published . since @xmath5 and @xmath49 have a common spectrum , one expects full gmres in the 2-norm and @xmath50-norm to have similar _ asymptotic _ convergence behavior .",
    "however , the choice of @xmath51 can significantly affect the departure of @xmath5 from normality .",
    "for example , if @xmath5 is diagonalizable with @xmath52 , then taking @xmath53 with @xmath54 renders @xmath55 a diagonal ( and hence normal ) matrix .",
    "the @xmath12-norms of the residuals produced by a cycle of w - gmres will decrease monotonically , but this need not be true of the 2-norms , as seen in [ fig : gwf1 ] .",
    "in fact , @xmath56 and similarly @xmath57",
    "so @xmath58 moreover , a result of pestana and wathen  ( * ? ? ?",
    "4 ) concerning gmres in general inner products implies that the same bound holds when the residual in the middle of this inequality is replaced by the optimal 2-norm gmres residual .",
    "that is , if @xmath59 and @xmath60 denote the @xmath16th residuals from gmres in the @xmath50-norm and 2-norm ( both before any restart is performed , @xmath61 ) , then @xmath62 thus for @xmath50-norm gmres to depart significantly from 2-norm gmres _ over the course of one cycle _ , @xmath51 must be ( at least somewhat ) ill - conditioned .",
    "however , restarting with a new inner product can change the dynamics across cycles significantly .      for diagonal @xmath12 , as in  @xcite",
    ", let @xmath63 when the coefficient matrix @xmath5 is diagonal , @xmath64 essai s weighting is well motivated and its effects can be readily understood . , and the analysis here can help explain their experimental results . ] in this setting , the eigenvectors are columns of the identity matrix , @xmath65 , and @xmath66 .",
    "consider one cycle of w - gmres(@xmath14 ) , which we presume starts with the initial residual @xmath67^t$ ] .",
    "then for all @xmath68    [ eq : diag ] @xmath69              & = & \\mingmres \\|\\poly(sas^{-1})sb\\|_2 ^ 2 \\\\[-.5em ]              & = & \\mingmres \\|\\poly(a)sb\\|_2 ^ 2               = \\mingmres \\sum_{j=1}^n   |@\\poly(\\lambda_j)|^2 |s_j b_j|^2 , \\label{eq : diag3}\\\\ \\nonumber\\end{aligned}\\ ] ]    using the commutativity of diagonal @xmath51 and @xmath5 .",
    "this last equation reveals that the weight @xmath70 affects gmres in precisely the same way as the component @xmath71 of the right - hand side ( which is just the component of @xmath33 in the @xmath20th eigenvector direction for this @xmath5 ) , and thus the weights tune how much gmres will emphasize any given eigenvalue .",
    "up to scaling , essai s proposal amounts to @xmath72 , so @xmath73 this approach favors large components in @xmath33 , while underemphasizing smaller ones .",
    "consider this intuition : small entries in @xmath33 ( reduced at a previous cycle of restarted gmres ) were `` easy '' for gmres to reduce because , for example , they correspond to eigenvalues far from the origin .",
    "large entries in @xmath33 have been neglected by previous cycles , most likely because they correspond to eigenvalues close to the origin .",
    "a low - degree polynomial that is small at these points is likely to be large at large magnitude eigenvalues , and so will not be picked as the optimal polynomial by the restarted gmres process .",
    "weighting tips the scales to favor of these neglected eigenvalues near the origin for one cycle ; this preference will usually _ increase _ the residual in `` easy '' components , but such an increase can be quickly remedied at the next cycle .",
    "consider , for example , how standard gmres(1 ) handles the scenario @xmath74 putting the root @xmath75 of the residual polynomial @xmath76 near the cluster of eigenvalues @xmath77 will significantly diminish the residual vector in components @xmath78 , while not increasing the first component , since @xmath79",
    ". on the other hand , placing @xmath75 near @xmath80 would give @xmath81 for @xmath82 , significantly increasing the overall residual norm ",
    "but in a manner that could be remedied at the next cycle , if only gmres(1 ) had the foresight to take a locally sub - optimal cycle to accelerate the overall convergence .",
    "essai s weighting gives gmres the ability to target that small magnitude eigenvalue for one cycle , potentially at an increase to the 2-norm of the residual that can easily be corrected at the next step without undoing the reduced component associated with @xmath80 .",
    "we illustrate this idea with a detailed example .",
    "[ ex:10diag ] consider the diagonal matrix @xmath83 the right - hand side is a normal(0,1 ) vector ( matlab s randn with rng(1319 ) ) , normalized so that @xmath84 .",
    "compares w - gmres(@xmath14 ) to gmres(@xmath14 ) for @xmath85 and @xmath34 : in both cases , weighting gives a big improvement .",
    "investigates this behavior further , comparing convergence of gmres(3 ) and w - gmres(3 ) for 5000  normal(0,1 ) right - hand side vectors ( also normalized so @xmath84 ) . as in , w - gmres(3 )",
    "usually substantially improves convergence , but it also adds considerable variation in performance .",
    "gmres(@xmath14 ) and w - gmres(@xmath14 ) with @xmath86 for the diagonal test case , [ ex:10diag ] . ]",
    "histograms of gmres(3 ) and w - gmres(3 ) convergence for the matrix in [ ex:10diag ] with 5000 random unit vectors @xmath87 ( the same for both methods ) .",
    "for each iteration , these plots show the distribution of the 2-norm of the residual ; while w - gmres(3 ) converges more rapidly in nearly all cases , the convergence is more variable.,title=\"fig : \" ]   histograms of gmres(3 ) and w - gmres(3 ) convergence for the matrix in [ ex:10diag ] with 5000 random unit vectors @xmath87 ( the same for both methods ) . for each iteration",
    ", these plots show the distribution of the 2-norm of the residual ; while w - gmres(3 ) converges more rapidly in nearly all cases , the convergence is more variable.,title=\"fig : \" ]    we will focus on the case of @xmath85 .",
    "gmres(3 ) converges quite slowly : the small eigenvalues of @xmath5 make this problem difficult .",
    "shows the residual components in each eigenvector after cycles  2 , 3 and  4 : gmres(3 ) makes little progress in the smallest eigencomponent .",
    "contrast this with w - gmres(3 ) : after cycles  2 and  3 all eigencomponents have been reduced _ except the first one , corresponding to @xmath88_. thus cycle  4 places a significant emphasis on this previously neglected eigenvalue , reducing the corresponding eigencomponent in the residual by two orders of magnitude ( while increasing some of the other eigencomponents ) . without weighting ,",
    "gmres(3 ) does not target this eigenvalue .",
    "eigencomponents of the residual at the end of cycles @xmath89 , @xmath90 , and @xmath91 for standard gmres(3 ) ( top ) and w - gmres(3 ) ( bottom ) applied to the matrix in example  2 .",
    "cycle  4 of w - gmres(3 ) reduces the eigencomponent corresponding to @xmath92 by two orders of magnitude . ]",
    "( 0,0 ) ( -203,50)(0,1)20 ( -216,40)@xmath88     eigencomponents of the residual at the end of cycles @xmath89 , @xmath90 , and @xmath91 for standard gmres(3 ) ( top ) and w - gmres(3 ) ( bottom ) applied to the matrix in example  2 .",
    "cycle  4 of w - gmres(3 ) reduces the eigencomponent corresponding to @xmath92 by two orders of magnitude . ]",
    "( 0,0 ) ( -203,50)(0,1)20 ( -216,40)@xmath88    examines the gmres residual polynomials @xmath93 that advance gmres(@xmath14 ) and w - gmres(@xmath14 ) from cycle  @xmath94 to cycle  @xmath95 ( i.e. , @xmath96 with @xmath97 ) for @xmath98 .",
    "gmres(3 ) shows a repeating pattern , with every other polynomial very similar ; such behavior has been noted before  @xcite .",
    "none of these four polynomials is small at the eigenvalue @xmath88 , as can be seen in the zoomed - in image on the right of [ fig : ex2poly ] .",
    "( at cycle  4 , @xmath99 . )",
    "the w - gmres(3 ) polynomials at cycles  2 , 3 , and  5 are much like those for gmres(3 ) ; however , the polynomial for cycle  4 breaks the pattern , causing a strong reduction in the first eigencomponent .",
    "specifically , @xmath100 . note that @xmath101 is also small at the eigenvalues @xmath102 and @xmath103 .",
    "these are the other eigencomponents that are reduced at this cycle ; the others all increase , as seen in [ fig : gwf2bc ] .",
    "while the weighted norm decreases from  0.0274 to  0.0018 during cycle  4 , the 2-norm shows a more modest reduction , from  0.0913 to  0.0734 .",
    "indeed , experiments with different initial residuals often produce similar results , though the cycle that targets @xmath88 can vary , and the 2-norm of the w - gmres residual can increase at that cycle .",
    "residual polynomials for cycles 25 for standard gmres(3 ) ( top ) and w - gmres(3 ) ( bottom ) for [ ex:10diag ] .",
    "the solid gray vertical lines denote the eigenvalues of @xmath5 . at cycle  4 ,",
    "the weighted inner product allows w - gmres(3 ) to make progress on the first eigencomponent , at the expense of increasing other eigencomponents .",
    ", title=\"fig : \" ]   residual polynomials for cycles 25 for standard gmres(3 ) ( top ) and w - gmres(3 ) ( bottom ) for [ ex:10diag ] .",
    "the solid gray vertical lines denote the eigenvalues of @xmath5 . at cycle  4 ,",
    "the weighted inner product allows w - gmres(3 ) to make progress on the first eigencomponent , at the expense of increasing other eigencomponents .",
    ", title=\"fig : \" ]     residual polynomials for cycles 25 for standard gmres(3 ) ( top ) and w - gmres(3 ) ( bottom ) for [ ex:10diag ] .",
    "the solid gray vertical lines denote the eigenvalues of @xmath5 . at cycle  4 ,",
    "the weighted inner product allows w - gmres(3 ) to make progress on the first eigencomponent , at the expense of increasing other eigencomponents .",
    ", title=\"fig : \" ]   residual polynomials for cycles 25 for standard gmres(3 ) ( top ) and w - gmres(3 ) ( bottom ) for [ ex:10diag ] .",
    "the solid gray vertical lines denote the eigenvalues of @xmath5 . at cycle  4 ,",
    "the weighted inner product allows w - gmres(3 ) to make progress on the first eigencomponent , at the expense of increasing other eigencomponents .",
    ", title=\"fig : \" ]    the next result makes such observations precise for @xmath104 matrices .",
    "[ thm : g1roots ] let @xmath105 with real @xmath106 , and suppose the residual vector at the start of a cycle of gmres is @xmath107^t$ ] for @xmath108 with @xmath109 .",
    "define @xmath110 .",
    "then the gmres(1 ) polynomial has the root @xmath111 while the w - gmres(1 ) polynomial has the root @xmath112    the proof is a straightforward calculation . to appreciate this theorem ,",
    "take @xmath113 .",
    "then the roots are  0.1818 for gmres(1 ) and  0.1089 for w - gmres(1 ) .",
    "these polynomials cut the residual component corresponding to the small eigenvalue by a factor  0.450 for gmres(1 ) and by  0.0818 for w - gmres(1 ) , the latter at the cost that w - gmres(1 ) increases the magnitude of the other eigencomponent by a factor of  8.18 .",
    "this increase is worthwhile , given the significant progress in the tough small component ; later cycles will handle the easier component .",
    "for another example , take @xmath114 : gmres(1 ) reduces the small eigencomponent by  0.495 , while w - gmres(1 ) reduces it by  0.0098 , two orders of magnitude .      beyond the potential to make progress on small magnitude eigenvalues ,",
    "w - gmres has the added advantage of breaking the cyclic pattern into which gmres(@xmath14 ) residual polynomials often fall .",
    "baker , jessup and manteuffel  ( * ? ? ?",
    "2 ) prove that if @xmath5 is an @xmath115 symmetric ( or skew - symmetric ) matrix , then gmres(@xmath38 ) produces residual vectors that exactly alternate in direction , i.e. , the residual vector at the end of a cycle is an exact multiple of the residual vector two cycles before ; thus the gmres residual polynomial at the end of each cycle repeats the same polynomial found two cycles before .",
    "baker et al .",
    "observe the same qualitative behavior for smaller restart parameters , and suggest that disrupting this cyclic pattern can accelerate gmres convergence .",
    "( longer cyclic patterns can emerge for nonnormal matrices  @xcite . ) to break the pattern , baker et al .",
    "propose changing the restart parameter between cycles of standard restarted gmres .",
    "changing inner products can have a similar effect , with the added advantage of targeting difficult eigenvalues if the corresponding eigenvectors are well - disposed .",
    "while essai s residual - based weighting scheme can break the cyclic pattern in restarted gmres , other schemes ( such as the random weighting scheme used in the experiments of  @xcite ; see [ sec : randw ] ) can have a similar effect ; however , such arbitrary inner products take no advantage of eigenvector structure .",
    "the next example cleanly illustrates how w - gmres can break patterns .",
    "[ ex3 ] let @xmath116 , and apply gmres(1 ) with starting vector @xmath117^t$ ] .",
    "the roots of the ( linear ) gmres(1 ) residual polynomials alternate between @xmath118 and @xmath119 ( exactly ) , values given by  [ thm : g1roots ] .",
    "therefore the gmres(1 ) residual polynomials for the cycles alternate between @xmath120 and @xmath121 .",
    "gmres(1 ) converges ( @xmath122 ) in 17  iterations .",
    "in contrast , w - gmres(1 ) requires only  7 iterations .",
    "as [ fig : g1roots ] shows , weighting breaks the cycle of polynomial roots , giving 1.667 , 1.200 , 1.941 , 1.0039 , 1.999985 , 1.0000000002 , and @xmath123 : the roots move out toward the eigenvalues , alternating which eigenvalue they favor and reducing the residual much faster in the process .",
    "roots of the gmres(1 ) and w - gmres(1 ) residual polynomials for @xmath116 with @xmath124^t$ ] , as a function of the restarted gmres cycle , @xmath16 .",
    "the gmres(1 ) roots occur in a repeating pair , and the method takes 17  iterations to converge to @xmath122 .",
    "in contrast , the w - gmres(1 ) roots are quickly attracted to the eigenvalues , giving convergence in just 7  steps.,title=\"fig : \" ]   roots of the gmres(1 ) and w - gmres(1 ) residual polynomials for @xmath116 with @xmath124^t$ ] , as a function of the restarted gmres cycle , @xmath16 .",
    "the gmres(1 ) roots occur in a repeating pair , and the method takes 17  iterations to converge to @xmath122 .",
    "in contrast , the w - gmres(1 ) roots are quickly attracted to the eigenvalues , giving convergence in just 7  steps.,title=\"fig : \" ]      the justification for w - gmres in  ( [ eq : diag3 ] ) relied on the fact that @xmath5 was diagonal , i.e. , all its eigenvectors were columns of the identity matrix .",
    "when this is not the case , the motivation for w - gmres becomes more tenuous , as explored in the next subsection .",
    "( this is true even if the eigenvectors are orthogonal , i.e. , @xmath5 is normal but not diagonal . ) yet in many important applications @xmath5 is not close to diagonal , but still has eigenvectors that are _ localized _ , meaning that only a few entries in the eigenvector are large in magnitude . in this case",
    "computational evidence suggests that w - gmres can still be very effective , particularly when the eigenvectors associated with the small magnitude eigenvalues are localized .",
    "indeed , one such example is the add20 matrix in [ ex : add20 ] .    to inform the discussion in the next subsection",
    ", we propose to roughly gauge the localization of the @xmath125 smallest magnitude eigenvectors , measuring how much these eigenvectors are concentrated in their @xmath125 largest magnitude entries . in particular , for diagonalizable @xmath5 label the eigenvalues in increasing magnitude , @xmath126 , with associated unit 2-norm eigenvectors @xmath127 . for any vector @xmath128 ,",
    "let @xmath129 denote the subvector containing the @xmath125 largest magnitude entries of @xmath130 ( in any order ) .",
    "then @xmath131     the localization measures @xmath132 for @xmath133 for four matrices that are used in our w - gmres experiments . if @xmath132 is close to  1 , the eigenvectors associated with smallest magnitude eigenvalues are highly localized . ]",
    "notice that @xmath134 $ ] , and @xmath135 .",
    "if @xmath136 for some @xmath137 , the eigenvectors associated with the @xmath125 smallest magnitude eigenvalues are localized within @xmath125 positions .",
    "this measure is imperfect , for it does not reflect the specific location of the smallest magnitude eigenvalues , nor does it reflect potential eigenvector alignment ( i.e. , ill - conditioned eigenvector matrix ) for nonnormal @xmath5 .",
    "nevertheless , we find this measure to be a helpful instrument for assessing localization of eigenvectors .",
    "shows @xmath132 for @xmath133 for four matrices that are used in experiments in this section .",
    "include only one eigenvector for the derogatory eigenvalues , specified by the initial residual vectors used in the experiments here . ]",
    "the add20 matrix has strongly localized eigenvectors ; the discretization of the 2d  laplacian has eigenvectors that are far from being localized , i.e. , they are _",
    "global_. the  and sherman5 matrices have eigenvectors with an intermediate degree of localization .",
    "we shall refer back to these measures in our future experiments .",
    "essai proposed w - gmres for general nonsingular matrices .",
    "when @xmath5 is not diagonal , the justification for the algorithm in equation  ( [ eq : diag3 ] ) is lost , and the iteration becomes more subtle and less compelling .",
    "the weights can still break cyclic patterns in restarted gmres , but the interplay between the weights and the eigenvectors is more difficult to understand .",
    "suppose @xmath5 is diagonalizable , @xmath52 , again with weight matrix @xmath138 .",
    "unlike the diagonal case , @xmath51 will not generally commute with @xmath139 and @xmath140 , so the analysis in  [ eq : diag ] becomes    [ eq : nondiag ] @xmath141 \\nonumber\\end{aligned}\\ ] ]    the matrix @xmath51 _ transforms the eigenvectors of @xmath5 _  @xcite .",
    "suppose , impractically , that @xmath142 were known , allowing the _ nondiagonal _ weight @xmath143 in this case  ( [ eq : nondiag2 ] ) reduces to @xmath144 a perfect analogue of the diagonal case  ( [ eq : diag3 ] ) that could appropriately target the small magnitude eigenvalues that delay convergence .",
    "one might imagine _ approximating _ this eigenvector weighting by using , as a proxy for @xmath142 , some estimate of the left eigenvectors of @xmath5 associated with the smallest magnitude eigenvectors .",
    "( recall that the rows of @xmath142 are the left eigenvectors of @xmath5 , since @xmath145 . )",
    "we do not pursue this idea here , instead preferring to incorporate eigenvector information in the deflated restarting technique described in [ sec : drwgmres ] .",
    "the conventional w - gmres(@xmath14 ) algorithm instead uses diagonal @xmath12 ( and @xmath51 ) . in this case",
    ", @xmath146 scales the _ rows _ of @xmath139 , effectively emphasizing certain entries of the eigenvectors at the expense of others .",
    "write out the right and left eigenvectors , @xmath147 \\in \\c^{n\\times n } , \\qquad   v^{-1 } = \\left[\\begin{array}{c } \\wh{v}_1^ * \\\\ \\wh{v}_2^ * \\\\ \\vdots \\\\ \\wh{v}_n^ * \\end{array}\\right]\\in\\c^{n\\times n}.\\ ] ] let @xmath148 denote the expansion coefficients for @xmath33 in the eigenvectors of @xmath5 : @xmath149",
    ". then one can also render  ( [ eq : nondiag2 ] ) in the form @xmath150 a different analogue of  ( [ eq : diag3 ] ) .",
    "denote the @xmath94th entry of @xmath151 by @xmath152 . in most cases",
    "@xmath139 will be a dense matrix .",
    "suppose that only the @xmath153th entry of @xmath45 is large , and that @xmath154 for all @xmath155",
    ". then essai s weighting strategy will make @xmath156 for all @xmath157 : all the vectors @xmath158 form a small angle with @xmath159 , the @xmath153th column of the identity , making @xmath146 ill - conditioned .",
    "thus the transformed coefficient matrix @xmath49 will have a large departure from normality , often a troublesome case for gmres ; see , e.g. ,  @xcite .",
    "it is not evident how such weighting could cause w - gmres(@xmath14 ) to focus on small magnitude eigenvalues , as it does for diagonal  @xmath5 , suggesting why w - gmres(@xmath14 ) has shown mixed results in past experiments , e.g. , @xcite .",
    "we show that by transforming the eigenvectors , diagonal weighting can even prevent w - gmres(@xmath14 ) from converging at all .",
    "faber et al .",
    "@xcite prove that gmres(@xmath14 ) converges for all initial residuals provided there exists no @xmath160 such that @xmath161 a condition sufficient to guarantee convergence of gmres(@xmath14 ) for all @xmath162 is that the field of values of @xmath5 , @xmath163 does not contain the origin .",
    "( often some aspect of the motivating problem ensures this condition holds , e.g. , coercivity in finite element methods . )",
    "weighting effectively applies gmres to the transformed matrix @xmath49 , and now it is possible that @xmath164 even though @xmath165 . in extreme cases",
    "this means that w - gmres(@xmath14 ) can stagnate , even when gmres(@xmath14 ) converges , as shown by the next example .",
    "( in contrast , for @xmath5 and @xmath51 diagonal , @xmath166 , so @xmath167 . )     the fields of values @xmath168 ( gray region ) and @xmath169 ( boundary is a black line ) in @xmath170 for @xmath5 and @xmath171 in  [ eq : stag2 ] ; the cross ( @xmath172 ) marks the origin . since @xmath173 , gmres(@xmath14 ) converges for any @xmath171 ; however , @xmath174 and @xmath175 , so w - gmres(1 ) completely stagnates . ]",
    "[ ex : stagnate ]   + consider the matrix and initial residual @xmath176 , \\qquad     r_0 = \\left[\\begin{array}{c } 1 \\\\ { 1\\over 10}(5+\\sqrt{5})\\end{array}\\right ]       = \\left[\\begin{array}{c } 1 \\\\ 0.72360\\ldots\\end{array}\\right].\\ ] ] shows @xmath168 , an ellipse in the complex plane . from the extreme eigenvalues of the hermitian and skew - hermitian parts of @xmath5 , one can bound @xmath168 within a rectangle in @xmath170 that does not contain the origin , @xmath177 \\approx [ 0.17157,5.82843 ] ,     \\qquad     { \\rm im}(f(a ) ) = [ -2@{\\rm",
    "i } , 2@{\\rm i}@],\\ ] ] and hence gmres(1 ) converges for all initial residuals .",
    "now consider the specific @xmath171 given in  [ eq : stag2 ] .",
    "using the scaling @xmath178 at the first step gives the w - gmres(1 ) polynomial ( see , e.g. , ( * ? ? ?",
    "* eq .  ( 2.2 ) ) ) @xmath179 this example has been engineered so that @xmath174 ; indeed , @xmath180 \\approx [ -0.086724,6.08672],\\ ] ] as seen in [ fig : stag2fv ] .",
    "worse still , this @xmath171 gives @xmath181 , so @xmath182 : w - gmres(1 ) makes no progress , and the same weight is chosen for the next cycle .",
    "the weighted algorithm completely stagnates , even though gmres(1 ) converges for any @xmath171 .",
    "gmres(6 ) and w - gmres(6 ) for [ ex:10lap ] .",
    "applying gmres(6 ) to @xmath183 and @xmath184 produces identical residual 2-norms .",
    "when applied to @xmath183 , w - gmres(6 ) converges faster than gmres(6 ) ( seen previously in [ fig : gwf2a ] ) ; when applied to @xmath185 the convergence is much slower ( @xmath5 having non - localized eigenvectors ) . ]",
    "this last example was designed to accentuate the difference between gmres(@xmath14 ) and w - gmres(@xmath14 ) when @xmath5 is not diagonal .",
    "next we show that global eigenvectors associated with small magnitude eigenvalues can also contribute to poor w - gmres(@xmath14 ) convergence .",
    "+ [ ex:10lap ] we revisit [ ex:10diag ] , again with dimension @xmath186 and eigenvalues @xmath187 but now building the eigenvector matrix @xmath139 from the orthonormal eigenvectors of the symmetric tridiagonal matrix @xmath188 , ordering the eigenvalues from smallest to largest .",
    "( the @xmath20th eigenvector has entries @xmath189 ; see , e.g. , @xcite . )",
    "the resulting @xmath190 is a symmetric matrix , a unitary similarity transformation of @xmath140 .",
    "let @xmath33 be the same normal(0,1 ) vector used in [ ex:10diag ] .",
    "since @xmath139 is unitary , gmres(@xmath14 ) applied to @xmath183 produces the same residual norms as gmres(@xmath14 ) applied to @xmath191 .",
    "however , weighted gmres can behave very differently for the two problems : when applied to @xmath140 ( as in [ ex:10diag ] ) , w - gmres(6 ) significantly outperforms gmres(6 ) ; when applied to @xmath5 with its non - localized eigenvectors , w - gmres(6 ) converges much more slowly , as seen in [ fig : lapev ] .",
    "the next example shows similar behavior for a larger matrix : eigenvalues on both sides of the origin with global eigenvectors cause poor w - gmres(@xmath14 ) performance ; convergence improves for the same spectrum with localized eigenvectors .",
    "gmres(@xmath14 ) and w - gmres(@xmath14 ) for the nonsymmetric sherman5 matrix . in the top plot ,",
    "the methods are applied to @xmath192 , and weighting slows convergence . in the bottom plot , the same methods are applied to @xmath193 , where @xmath140 is diagonal with the same eigenvalues of @xmath5 ; now w - gmres converges more rapidly .",
    "( the gmres(100 ) curve is magenta ; the w - gmres(30 ) curve is blue . ) for both @xmath192 and @xmath193 , taking  @xmath194 steps with each weighted inner product leads to significant growth in the 2-norm of the residual between restarts . ]",
    "gmres(@xmath14 ) and w - gmres(@xmath14 ) for the nonsymmetric sherman5 matrix . in the top plot ,",
    "the methods are applied to @xmath192 , and weighting slows convergence . in the bottom plot ,",
    "the same methods are applied to @xmath193 , where @xmath140 is diagonal with the same eigenvalues of @xmath5 ; now w - gmres converges more rapidly .",
    "( the gmres(100 ) curve is magenta ; the w - gmres(30 ) curve is blue . ) for both @xmath192 and @xmath193 , taking  @xmath194 steps with each weighted inner product leads to significant growth in the 2-norm of the residual between restarts . ]",
    "+ [ ex : sherman ] we present one final example showing how weighting can slow convergence . the sherman5 matrix from matrix market  @xcite has dimension @xmath195 .",
    "shows that weighting slows down restarted gmres , particularly for the larger restart value of @xmath194 .",
    "( the initial residual is a normal(0,1 ) vector . )",
    "shows that the eigenvectors are poorly localized .",
    "for example , in the eigenvector @xmath196 corresponding to the smallest eigenvalue , the 100  largest components range only from 0.051 down to 0.041 ( with @xmath197 ) .",
    "moreover , this matrix has eigenvalues to the left and right of the origin , and the eigenvectors are not orthogonal .",
    "is an eigenvalue of multiplicity 1674 for sherman5 , and the corresponding eigenvectors are columns of the identity matrix .",
    "this high multiplicity eigenvalue has little effect on gmres convergence .",
    "the eigenvalues closest to the origin are well - conditioned , while some eigenvalues far from the origin are relatively ill - conditioned .",
    "( for the diagonalization computed by matlab , @xmath198 . )",
    "the pseudospectra of this matrix are well - behaved near the origin ; the effect of the eigenvalue conditioning on gmres convergence is mild  @xcite . ]",
    "the second plot in [ fig : sherex ] shows how w - gmres performance improves when @xmath5 is replaced by a diagonal matrix with the same spectrum .",
    "the past three examples suggest that w - gmres performs poorly for matrices whose eigenvectors are not localized , but the algorithm is more nuanced than that .",
    "the next example is characteristic of behavior we have often observed in our experiments .",
    "we attribute the improvement over standard restarted gmres to the tendency of weighting to break cyclic patterns in the gmres residual polynomials .",
    "[ ex : lap2d ]   + consider the standard finite difference discretization of the laplacian on the unit square in two dimensions with dirichlet boundary conditions .",
    "the uniform grid spacing @xmath199 in both directions gives a matrix of order @xmath200 ; the initial residual is a random normal(0,1 ) vector .",
    "the small @xmath132 values shown in [ fig : comploc ] confirm that the eigenvectors are not localized ; in fact , they are less localized than for the sherman5 matrix in the last example . despite this , [ fig : lap2d ]",
    "shows that w - gmres(@xmath14 ) outperforms gmres(@xmath14 ) for @xmath201 and @xmath35 , though not as much as for the add20 matrix in [ ex : add20 ] . in this case , all the eigenvalues are positive , and none is particularly close to the origin .",
    "gmres(@xmath14 ) and w - gmres(@xmath14 ) for a discretization of the dirichlet laplacian on the unit square .",
    "the eigenvectors are not localized , but weighting still accelerates convergence . ]      if weighting often helps gmres convergence , particularly when the smallest eigenvalues of @xmath5 have localized eigenvectors , might convergence be further improved if the weights exaggerate the differences in the residual vector entries , thus accentuating the small - magnitude eigenvalues ?",
    "recall from equation  [ eq : diag ] that for diagonal @xmath5 , @xmath202 essai s standard weighting scheme uses @xmath203 .",
    "suppose one replaces this weight with @xmath204 for some @xmath205 ( where @xmath206 is gmres(@xmath14 ) , @xmath207 is w - gmres(@xmath14 ) , and @xmath208 gives extra weighting ) . for large @xmath125",
    "we find it particularly important to limit the minimal weight , as in  [ eq : essai ] ; we require @xmath209 $ ] .    for most cases",
    "we have studied the extra weighting makes little qualitative difference in convergence . for diagonal @xmath5 , where the motivation from  [ eq : diagextra ] holds",
    ", @xmath208 can notably accelerate convergence .",
    "when @xmath5 is not diagonal but has localized eigenvectors , extra weighting sometimes helps .",
    "it does little for the add20 matrix from [ ex : add20 ] , but has a greater effect for the  matrix studied next .",
    "[ ex : extra ]   + the  matrix from matrix market  @xcite is a nonsymmetric matrix of dimension @xmath210 .",
    "shows the eigenvectors to be fairly well localized ; all the eigenvalues are real and negative . reports the matrix - vectors products ( with @xmath5 ) required by w - gmres(@xmath14 ) with different weighting powers  @xmath125 to satisfy the convergence criterion @xmath122 .",
    "( here @xmath171 is a random normal(0,1 ) vector . )",
    "when @xmath14 is small ( e.g. , @xmath211 and @xmath212 ) , extra weighting improves convergence significantly .",
    "for example , when @xmath201 , the extreme value @xmath213 gives the best result among the reported  @xmath125 . for larger @xmath14 , extra weighting has less impact .    .[tbl",
    ": extra_orsirr ] w - gmres(@xmath14 ) performance for the  matrix , with extra weighting of the residual entries by the power @xmath125 .",
    "the table entries show the number of matrix - vector products with @xmath5 required to reduce @xmath214 by a factor of @xmath215 . [",
    "cols=\">,>,>,>,>,>,>,>,>,>,>\",options=\"header \" , ]     local support seems less important for eigenvalues than for linear equations , but we suggest another possible reason why weighting can help , a reason that applies for eigenvalue computations but not solving linear equations . increasing nonnormality is generally not helpful for linear equations , but can assist eigenvalue computations . if several eigenvectors are skewed toward the same direction , approximating",
    "any of them helps convergence toward the others ; see the related discussion in  @xcite and  ( * ? ? ?",
    "an example showing this effect is next .    here",
    "we demonstrate that weighting can skew eigenvectors in a way that assists convergence .",
    "while this example shows how this effect can be helpful , we do not claim it is the only mechanism responsible for the improved convergence of w - arnoldi .",
    "consider the matrix and initial vector @xmath216 , \\qquad     v_1 = \\left[\\begin{array}{c } 1.165 \\\\ 0.631\\\\ 0.075\\end{array}\\right].\\ ] ] for solving linear equations , gmres(1 ) and w - gmres(1 ) converge similarly .",
    "however for finding one eigenvalue , w - arnoldi(2,1 ) is much faster than arnoldi(2,1 ) , converging in 7  cycles instead of  25 .",
    "w - arnoldi(2,1 ) makes no progress in the first four cycles , but drops three orders of magnitude in cycle  5 .",
    "after cycle  4 , the approximate eigenvector is @xmath217^t$ ] , so the last component is accurate , but the second is too large .",
    "the residual vector is @xmath218^t$ ] , and the diagonal of the weighting matrix @xmath138 is a scaled version of this .",
    "the matrix @xmath219 ( discussed in [ sec : analysis ] ) and its eigenvectors are @xmath220 ,     \\quad     \\left[\\begin{array}{c } 1 \\\\",
    "0\\\\ 0\\end{array}\\right ] ,     \\left[\\begin{array}{c } 0.7692 \\\\",
    "0.6391\\\\ 0\\end{array}\\right ] ,     \\left[\\begin{array}{c } 0.6700 \\\\ 0.7423\\\\ 0.0082\\end{array}\\right].\\ ] ] this @xmath221 has a much larger departure from normality than @xmath5 , and the eigenvectors are skewed to be large in only the first two components .",
    "this leads to needed improvement in the second component of the approximate eigenvector .",
    "the ritz values for cycle  5 are @xmath222 and @xmath223 , so the method is able to focus on the first two components and remove much of the second component from the desired approximate eigenvector .",
    "the added departure from normality improves convergence .",
    "since its introduction by essai  @xcite , the weighted gmres algorithm has been an intriguing method whose adoption has been impeded by an incomplete understanding of its convergence properties . in this manuscript",
    "we have argued , through some simple analysis and numerous experiments , that its advantage is due to ( 1 )  its ability to break repetitive cycles in restarted gmres , and ( 2 )  its ability to target small - magnitude eigenvalues that slow gmres convergence , _ provided the corresponding eigenvectors are localized_. in cases where those vectors are not localized the proposed w - gmres - dct algorithm , which combines residual weighting with a fast transform to sparsify eigenvectors , can significantly improve convergence .",
    "weighted inner products can also be applied to deflated gmres methods , such as gmres - dr .",
    "while the improvement appears to be less significant than for standard gmres , weighting can sometimes reduce the number of matrix - vector products even further than with the deflation alone or weighting alone .",
    "we also show how weighting can be incorporated into a thick - restart arnoldi method for eigenvalue computations , where residual - based weighting can improve all eigenvector estimates simultaneously . here",
    "it appears that in some situations the tendency of weighting to increase nonnormality can improve convergence .",
    "many topics related to weighted inner products merit further investigation , such as additional sparsifying transformations or more general non - diagonal weighting matrices , and the potential of using weighting to avoid near - breakdown cases in the nonsymmetric lanczos  @xcite method .",
    "we thank jrg liesen , jennifer pestana and andy wathen for helpful conversations about this work . me is grateful for the support of the einstein stiftung berlin , which allowed him to visit the technical university of berlin while working on this project .",
    "rbm appreciates the support of the baylor university sabbatical program ."
  ],
  "abstract_text": [
    "<S> the convergence of the restarted gmres method can be significantly improved , for some problems , by using a weighted inner product that changes at each restart . </S>",
    "<S> how does this weighting affect convergence , and when is it useful ? we show that weighted inner products can help in two distinct ways : when the coefficient matrix has localized eigenvectors , weighting can allow restarted gmres to focus on eigenvalues that otherwise slow convergence ; for general problems , weighting can break the cyclic convergence pattern into which restarted gmres often settles . </S>",
    "<S> the eigenvectors of matrices derived from differential equations are often not localized , thus limiting the impact of weighting . for such problems , incorporating the discrete cosine transform into the inner product can significantly improve gmres convergence , giving a method we call w - gmres - dct .   integrating weighting with eigenvalue deflation via gmres - dr </S>",
    "<S> also can give effective solutions . </S>",
    "<S> similarly , weighted inner products can be combined with the restarted arnoldi algorithm for eigenvalue computations : weighting can enhance convergence of all eigenvalues simultaneously .    </S>",
    "<S> linear equations , eigenvalues , restarted gmres , restarted arnoldi , weighted inner product , deflation , localized eigenvectors    65f10 , 15a06 </S>"
  ]
}