{
  "article_text": [
    "homogeneity testing is an important problem in statistics and machine learning .",
    "it tests whether two samples are drawn from different distributions .",
    "this is relevant for many applications , for instance , schema matching in databases @xcite , and speaker identification  @xcite .",
    "popular two - sample tests like kolmogorov - smirnov  @xcite and cramer - von - mises  @xcite are not capable of capturing statistical information of densities with high frequency features .",
    "non - parametric kernel - based statistical tests such as maximum mean discrepancy ( mmd )  @xcite enable one to obtain greater power than such density based methods .",
    "mmd is applicable not only to euclidean spaces @xmath0 , but also to groups and semigroups  @xcite , and to structures such as strings or graphs in bioinformatics , and robotics problems , etc .",
    "@xcite . here",
    "we consider a regularized version of mmd to address hypothesis testing . + with more than two distributions to be compared simultaneously , we face the multiple comparisons setting , for which statistical methods exist to deal with the issue of multiple test correction  @xcite .",
    "given a prescribed global significance threshold @xmath1 ( type  slowromancap1@ error ) for the set of all comparisons , however , the corresponding threshold per comparison becomes small , which greatly reduces the power of the test . in situations where one wants to retain the null hypothesis , tests with small @xmath1 are not conservative .",
    "our main contribution is the definition of a regularized mmd ( rmmd ) method .",
    "+ the regularization term in rmmd allows to control the power of the test statistic .",
    "the regularizer is set * provably optimal * for maximal power ; there is no need for fine - tuning by the user .",
    "rmmd improves on mmd through higher power , especially for small sample sizes , while preserving the advantages of mmd .",
    "* power control * enables us to look for true sets of null distributions among the significant ones in challenging multiple comparison tasks .",
    "+ we provide experimental evidence of good performance on a challenging electroencephalography ( eeg ) dataset , artificially generated periodic and gaussian data , and the mnist and covertype datasets .",
    "we also assess power control with the asymptotic relative efficiency ( are ) test . + the paper is organized as follows . in section  2",
    ", we elaborate on hypothesis testing and define maximum mean discrepancy ( mmd ) as a metric .",
    "we describe how to use mmd for homogeneity testing , and how to extend it to multiple comparisons . in section  3 ,",
    "we define rmmd for hypothesis testing and compare it to mmd and kernel fisher discriminant analysis ( kfda ) , and assess power control through are . additional empirical justification of our test on various datasets is presented in section  4 .",
    "a statistical hypothesis test is a method which , based on experimental data , aims to decide whether a hypothesis ( called null or @xmath2 ) is true or false , against an alternative hypothesis ( @xmath3 ) .",
    "the level of significance @xmath1 of the test represents the probability of rejecting @xmath2 under the assumption that @xmath2 is true ( type  slowromancap1@ error ) .",
    "a type  slowromancap2@ error ( @xmath4 ) occurs when we reject @xmath3 although it holds .",
    "+ the * power * of the statistical test is usually defined as @xmath5 .",
    "a desirable property of a statistical test is that for a prescribed global significance level @xmath1 the power equals one in the population limit .",
    "we divide the discussion of hypothesis testing into two topics : homogeneity testing and multiple comparisons .",
    "embedding probability distributions into reproducing kernel hilbert spaces ( rkhss ) yields a linear method that takes information of higher order statistics into account  @xcite .",
    "characteristic kernels  @xcite injectively map the probability distribution onto its mean element in the corresponding rkhss .",
    "the distance between the * mean elements ( @xmath6 ) * in the rkhs is known as mmd  @xcite .",
    "the definition of mmd  @xcite is given in the following theorem : + * theorem 1 .",
    "* _ let ( @xmath7 ) be a metric space , and let @xmath8 , @xmath9 be two borel probability measures defined on @xmath10 .",
    "the kernel function @xmath11 embeds the points @xmath12 into the corresponding reproducing kernel hilbert space @xmath13",
    ". then @xmath14 if and only if @xmath15 _ , where @xmath16-e_{q}[k(y,.)]\\|   _ { \\mathcal{h}}\\nonumber\\\\ & = ( e_{x , x'\\sim p}[k(x , x')]+e_{y , y'\\sim q}[k(y , y')]\\nonumber\\\\ & -2e_{x\\sim p , y\\sim q}[k(x , y)])^{\\frac{1}{2}}.\\\\end{aligned}\\ ] ]      a two - sample test investigates whether two samples are generated by the same distribution . to do testing",
    ", mmd can be used to measure the distance between embedded probability distributions in rkhs .",
    "besides calculating the distance measure , we need to check whether this distance is significantly different from zero . for this",
    ", the asymptotic distribution of this distance measure is used to obtain a threshold on mmd values , and to extract the statistically significant cases .",
    "we perform a hypothesis test with null hypothesis @xmath17 and alternative @xmath18 on samples drawn from two distributions @xmath8 and @xmath9 .",
    "if the result of mmd is close enough to zero , we accept @xmath2 , which indicates that the distributions @xmath8 and @xmath9 coincide ; otherwise the alternative is assumed to hold . with @xmath1 as a threshold on the asymptotic distribution of the empirical mmd ( when @xmath19 ) , the ( @xmath20)-quantile of this distribution is statistically significant .",
    "our mmd test determines it by means of a bootstrap procedure",
    ".      statistical analysis of a data set typically needs testing many hypotheses .",
    "the multiple comparisons or multiple testing problem arises when we evaluate several statistical hypotheses simultaneously . let @xmath1 be the overall type  slowromancap1@ error , and let @xmath21 denote the type  slowromancap1@ error of a single comparison in the multiple testing scenario . maintaining the prescribed significance level of @xmath1 in multiple comparisons yields @xmath21 to be more stringent than @xmath1 .",
    "nevertheless , in many studies @xmath22 is used without correction .",
    "several statistical techniques have been developed to control @xmath1  @xcite .",
    "we use the dunn - idk method : for @xmath23 independent comparisons in multiple testing , the significance level @xmath1 is obtained by : @xmath24 .",
    "as @xmath1 decreases , the probability of type  slowromancap2@ error ( @xmath4 ) increases and the power of the test decreases .",
    "this requires to control @xmath4 while correcting @xmath1 . to tackle this problem , and to control @xmath4 ,",
    "we define a new hypothesis test based on rmmd , which has higher power than the mmd - based test , in the next section . to compare the distributions in the multiple testing problem we use two approaches : one - vs - all and pairwise comparisons . in the one - vs - all case",
    "each distribution is compared to all other distributions in the family , thus @xmath25 distributions require @xmath26 comparisons . in the pairwise case",
    "each pair of distributions is compared at the cost of @xmath27 comparisons .",
    "the main contribution of this paper is a novel regularization of mmd measure called rmmd .",
    "this regularization aims to provide a test statistics with greater power ( power closer to 1 with a prescribed type  slowromancap1@ error  @xmath1 ) .",
    "erdogmus and principe  @xcite showed that @xmath28 is the parzen window estimation of the renyi entropy  @xcite .",
    "with rmmd we obtain a statistical test with greater power by penalizing the term @xmath29 .",
    "we formulate @xmath30 and its empirical estimator as follows : @xmath31    where @xmath32 , and @xmath33 are non - negative regularization constants . for simplicity",
    "we consider @xmath34 in many application , however , we can introduce prior knowledge about the complexity of distributions by choosing @xmath35 . the modified jensen - shanon divergence ( js )  @xcite corresponding to rmmd is defined as : @xmath36 where @xmath37 denotes the ( cross ) entropy . since @xmath38 is positive , the absolute value of second term on the right - hand side of eq .",
    "( 4 ) increases , leading to a higher weight for the mutual information than for the entropy ( vice versa if @xmath38 would be lower than -1 ) .",
    "+ here we summarize the notation needed in the next section . given samples @xmath39 and @xmath40 drawn from distributions",
    "@xmath8 and @xmath9 , respectively , the mean element , the cross - covariance operator and the covariance operator are defined as follows  @xcite : @xmath41 , @xmath42 , and @xmath43 , where @xmath44 for @xmath45 is defined for all @xmath46 as @xmath47 . the quantities @xmath48 and @xmath49 are defined analogously for the second sample @xmath50",
    ". the population counterparts , i.e. , the population mean element and the population covariance operator are defined for any probability measure @xmath8 as @xmath51 $ ] for all @xmath46 , and @xmath52 $ ] for @xmath53 . from now on",
    "we call @xmath54 the _ between - distribution covariance_. the pooled covariance operator ( which we call also the _ within - distribution covariance _ ) is denoted by : @xmath55 .",
    "now we derive the distribution of the test statistics under the null hypothesis of homogeneity @xmath56 ( theorem 2 ) , which implies @xmath57 and @xmath58 .",
    "consistency of the test is guaranteed by the form of the distribution under @xmath59 ( theorem 2 ) .",
    "assume that @xmath60 and @xmath50 are independent samples from p and q , respectively ( a priori they are not equally distributed ) .",
    "let @xmath61 , @xmath62 , and @xmath63 , and @xmath64 denotes convergence in distribution . without loss of generality",
    "we assume @xmath65 , and @xmath34 .",
    "the proofs hold even when @xmath66 .",
    "based on hoeffding  @xcite , theorem a ( p. 192 ) and theorem b ( p. 193 ) by serfling  @xcite , we can prove the following theorem : + * theorem 2 .",
    "* _ if @xmath67<\\infty$ ] , under @xmath3 , @xmath68 is asymptotically normally distributed @xmath69 with variance @xmath70-e_{z , z'}^2[h(z , z')])$ ] , uniformly at rate @xmath71 . under @xmath2 ,",
    "the same convergence holds with @xmath72~]-$ ] @xmath73)>0 $ ] .",
    "_ + * to increase the power * of our rmmd - based test we need * to decrease the variance under @xmath3 * in theorem 2 .",
    "the following theorem can be used to obtain maximal power by setting @xmath74 .",
    "this will give us a fixed hyper - parameter  no need for user tuning .",
    "the optimal value of @xmath38 decreases both the variance of @xmath3 and @xmath2 simultaneously and the fixed @xmath1 is defined over the changed variance of @xmath2 . +",
    "* theorem 3 . *",
    "_ the * highest power * of rmmd is obtained for @xmath75 . _ + * proof .",
    "* let denote @xmath76 and @xmath77 . based on theorem 2 ,",
    "the variance under @xmath3 is obtained by : @xmath78-e_{z , z'}^2[h(z , z')])\\nonumber\\\\ & = 4(e[((1-\\kappa ) a - b)^2]-(e^2[(1-\\kappa ) a - b]))\\nonumber\\\\ & = 4((1-\\kappa ) ^2 ( e[a^2]-e^2[a])+ e[b^2]-e^2[b])\\nonumber\\\\ & = 4((1-\\kappa ) ^2 \\operatorname{var}(a ) + \\operatorname{var}(b)),\\end{aligned}\\ ] ] where @xmath79 , and @xmath80 denote the variances .",
    "to get maximal power , we set @xmath81 which yields @xmath74 .      according to theorem  8 by gretton et al .",
    "@xcite , under the null hypothesis the test statistics of mmd degenerates .",
    "this corresponds to @xmath82 in our theorem  2 . for large sample sizes the null distribution of mmd approaches in distribution as an infinite weighted sum of independent @xmath83 random variables , with weights equal to the eigenvalues of the within - distribution covariance operator @xmath84 .",
    "if we denote the test statistics based on mmd by @xmath85 , then @xmath86 , where @xmath87 are i.i.d .",
    "random variables , and @xmath88 is a scaling factor .",
    "harchaoui et al .",
    "@xcite introduced kernel fisher discriminant analysis ( kfda ) as a homogeneity test by regularizing mmd with the within - distribution covariance operator .",
    "the maximum fisher discriminant ratio defines this test statistic .",
    "the empirical kfda test statistic is denoted as @xmath89 . to analyze the asymptotic behaviour of this statistics under the null hypothesis , harchaoui et al .",
    "@xcite consider two situations regarding the regularization parameter @xmath90 : 1 ) one where @xmath90 is held fixed , obtaining the limit distribution similar to mmd under @xmath2 ; 2 ) one where @xmath90 tends to zero slower than @xmath91 .",
    "in the first situation the test statistic converges to @xmath92 .",
    "thus , the test statistics based on kfda normalizes the weights of @xmath83 random variables by using the covariance operator as the regularizer . in comparison mmd",
    "is more sensitive to the information of higher order moments because of their bigger weights ( larger eigenvalues of the covariance operator ) . in the second situation ( applicable in practice only for very large sample sizes )",
    "the test statistics converges to @xmath93 , where @xmath88 is a constant .",
    "+ the asymptotic convergence of the test statistic based on rmmd is @xmath94 , where @xmath95 is the variance of the function @xmath96 in theorem 2 .",
    "the precise analytical normal distribution obtains higher power in rmmd .",
    "because of the divergence ( @xmath97 in the asymptotic distribution ) for mmd and kfda , they use an estimation of the distribution under the null hypothesis which looses the accuracy and affect the power .",
    "in contrast to mmd and kfda , rmmd is consistent since the divergence under the null hypothesis does not happen any more .",
    "rmmd is the generalized form of the test statistics based on mmd , which we obtain for @xmath98 .",
    "moreover , by minimizing the variance of the normal distribution , we obtain the best power for @xmath75 and thus the hyper - parameter @xmath38 is fixed without requiring tuning by the user . + in comparison to kfda , rmmd does not require restrictive constraints to obtain high power .",
    "it also results in higher power than mmd and kfda in cases with small sample size .",
    "the speed of power convergence in kfda is @xmath99 , which is slower than @xmath100 in rmmd when @xmath101 .",
    "+ regarding the computational complexity , for mmd a parametric model with lower order moments of the test statistics is used to estimate the value of mmd which degenerates under @xmath2 , and which has no consistency or accuracy guarantee . in comparison ,",
    "the bootstrap resampling and the eigen - spectrum of the gram matrix are more consistent estimates with computational cost of @xmath102 , where @xmath23 is the number of samples  @xcite . for rmmd ,",
    "the convergence of the test statistic to a normal distribution enables a fast , consistent and straightforward estimation of the null distribution within @xmath102 time without the need of using an estimation method .",
    "the results of power comparison between these tests are reported in section  4 .      to assess the power control we use the asymptotic relative efficiency .",
    "this criterion shows that rmmd is a better test statistic and obtains higher power rather than kfda and mmd with smaller sample size .",
    "relative efficiency enables one to select the most effective statistical test quantitatively  @xcite .",
    "let @xmath103 and @xmath104 be test statistics to be compared .",
    "the necessary sample size for the test statistics t to achieve the power @xmath5 with the significance level @xmath1 is denoted by @xmath105 .",
    "the relative efficiency of the statistical test @xmath103 with respect to the statistical test @xmath104 is given by : @xmath106 since calculating @xmath105 is hard even for the simplest test statistics , the limit value @xmath107 , as @xmath108 , is used .",
    "the limiting value is called the bahadur asymptotic relative efficiency ( are ) denoted by @xmath109 . @xmath110 the test statistic @xmath104 is considered better than @xmath103 , if @xmath111 is smaller than 1 , because it means that @xmath104 needs a lower sample size to obtain a power of @xmath5 , for the given @xmath1 . in @xcite , authors assessed the power control by means of analysis of local alternatives which work when we have very large sample size or when n tends to infinity . in this article , we focus our attention on the small sample size case , which is more challenging . in section 4 ,",
    "we compute @xmath112 , @xmath113 , and @xmath114 using artificial datasets and two types of kernels , and we obtain smaller are for rmmd rather than kfda and mmd .",
    "this means rmmd gives higher power with much smaller sample size .",
    "results for different data sets are reported in table  2 , figure  2 , and figure  3 .",
    "mmd  @xcite was experimentally shown to outperform many traditional two - sample tests such as the generalized wald - wolfowitz test , the generalized kolmogorov - smirnov ( ks ) test  @xcite , the hall - tajvidi ( hall ) test  @xcite , and the biau - gyrf test .",
    "it was shown  @xcite that kfda outperforms the hall - tajvidi test .",
    "we select ks and hall as traditional baseline methods , on top of which we compare rmmd , kfda , and mmd . to experimentally evaluate the utility of the proposed hypothesis testing method , we present results on various artificial and real - world benchmark datasets .",
    "our proposed method can be used for testing the homogeneity of structured data , which is an advantage over traditional two - sample tests .",
    "we artificially generated distributions from locally compact abelian groups ( periodic data ) and applied our rmmd - test to decide whether the samples come from the same distributions or not .",
    "suppose the first sample is drawn from a uniform distribution @xmath8 on the unit interval .",
    "the other sample is drawn from a perturbed uniform distribution @xmath115 with density @xmath116 . for higher perturbation frequencies",
    "@xmath117 it becomes harder to discriminate @xmath115 from @xmath8 .",
    "since the distributions have a periodic nature , we use a characteristic kernel tailored to the periodic domain , @xmath118 . for 200 samples from each distribution , the type  slowromancap2@ error is computed by comparing the prediction to the ground truth over 1000 repetition .",
    "we average the results over 10 runs .",
    "the significance level is set to @xmath119 .",
    "we perform the same experiment with mmd , kfda , ks and hall .",
    "the powers of the homogeneity test for comparing @xmath8 and @xmath120 with the above mentioned methods are reported in table 1 as periodic1 .",
    "the best power is achieved by rmmd , and as expected , the results of kernel methods are better than traditional ones . +",
    "since the selection of the kernel is a critical choice in kernel - based methods , we also investigated the usage of a different kernel and replaced the previous kernel with @xmath121 , where @xmath122 is a hyperparameter .",
    "we report the best results achieved by @xmath123 as periodic2 in table 1 .",
    "the reader is referred to @xcite for a detailed study on these kernels .",
    "+ we also report the results on the toy problem of comparing two @xmath124-dimensional gaussian distributions with @xmath125 samples , both with zero mean vector but with covariance matrix @xmath126 and @xmath127 , respectively .",
    "this dataset is referred as gaussian in table  1 .",
    "[ cols=\"<,^,^,^,^,^ , > \" , ]                  moving from synthetic data to standard benchmarks , we tested our method on three datasets : 1 ) the mnist dataset of handwritten digits ( libsvm library : 10 classes , 5000 data points , and 784 dimensions ) ; 2 ) the covertype dataset of forest cover types ( libsvm library : 7 classes , 1400 instances , and 54 dimensions ) ; 3 ) the flare - solar dataset ( mldata.org : 2 classes , 100 instances , 10 dimensions ) .",
    "we compare the performance of rmmd with @xmath75 , kfda with @xmath128 and mmd , using the pairwise approach and testing for differences between the distributions of the classes , see table  1 .",
    "we average the results over 10 runs .",
    "the family wide level is set to @xmath129 ( resulting in @xmath130 , @xmath131 and @xmath132 for each individual comparison for mnist , covertype and flare - solar datasets , respectively ) .",
    "the rmmd - based test achieves higher power than the other methods ( see table  1 ) .",
    "we recorded eeg from four subjects performing a visual task .",
    "a checkerboard was presented in the subject s left visual field .",
    "we refer to @xcite for details on data collection and preprocessing . in our learning task ,",
    "for each subject we have 64 signal distributions assigned to 64 electrodes .",
    "the data contain 360 instances of a 200 dimensional feature vector for each distribution .",
    "the goal of hypothesis testing is to disambiguate signals recorded from electrodes corresponding to early visual cortex from the rest .",
    "this is difficult because of the low signal - to - noise ratio and the similarity of the patterns of all electrodes .",
    "moreover , the high number of electrodes makes this experiment a good candidate to assess the multiple comparison part of our method . in the one - vs - all approach the normalized distribution of each electrode",
    "is compared to the normalized combined distribution of the other 63 electrodes .",
    "rmmd with @xmath74 with gaussian kernel is used as our hypothesis test .",
    "the parameter @xmath133 of the gaussian kernel is set to the median distance of data points .",
    "the results of our hypothesis test reject the null hypothesis and confirm the dissimilarity of distributions in 63 electrodes .",
    "the results of the pairwise approach with rmmd and mmd are depicted in figure  6 .",
    "+            neuroscientists usually subjectively assess the results obtained from imaging techniques and inferred from machine learning .",
    "for instance , in the current experiment the expectation is that electrodes in region a1 are categorized together by means of eeg imaging techniques and multiple comparisons .",
    "but electrodes of other area ( such as @xmath134 and @xmath135 , see figure  7 ) can be confused as belonging to @xmath136 due to the high noise .",
    "figure  7 describes the categorization of the electrodes .",
    "we assess our results quantitatively by means of false discovery rates ( fdr ) , using the following fdrs to compare the results of rmmd to those of mmd : + @xmath137 ,    @xmath138    @xmath139 , + where @xmath140 is the total number of electrodes categorized for the task .",
    "the results are depicted in figure  7 .",
    "rmmd obtained more robust and better results than mmd with smaller fdrs .",
    "our novel regularized maximum mean discrepancy ( rmmd ) is a kernel - based test statistic generalizing the mmd test .",
    "we proved that rmmd overpowers mmd and kfda ; power consistency is obtained with higher rate .",
    "power control makes rmmd a good hypothesis test for multiple comparisons , especially for the crucial case of small sample sizes .",
    "in contrast to kfda and mmd , the convergence of rmmd - based test statistics to the normal distribution under null and alternative hypotheses yields fast and straightforward rmmd estimates .",
    "experiments with goldstandard benchmarks ( mnist , covertype and flare - solar dataset ) and with eeg data yield state of the art results .",
    "this work was partially funded by the nano - resolved multi - scale investigations of human tactile sensations and tissue engineered nanobio - sensors ( sinergia : crsiko_122697/1 ) grant , and the state representation in reward based learning from spiking neuron models to psychophysics(nanobio touch:228844 ) grant .",
    "borgwardt ,  k , gretton ,  a. , rash ,  m. , kriegel ,  h.  p. , schlkopf ,  b , smola ,  a.  j. : integrating structured biological data by kernel maximum mean discrepancy .",
    "bioinformatics , vol.22(14 ) , pp .",
    "49 - 57 , ( 2006 )        danafar ,  s. , gretton ,  a , schmidhuber ,  j. : characteristic kernels on structured domains excel in robotics and human action recognition .",
    "ecml : machine learning and knowledge discovery in databases , pp .",
    "264 - 279 , lcns , springer , ( 2006 )          fukumizu ,  k. , sriperumbudur ,  b. , gretton ,  a. schlkopf ,  b. : characteristic kernels on groups and semigroups . in advances in neural information processing systems .",
    "koller , d. , d. schuurmans , y. bengio , l. bottou ( eds . ) , vol .",
    "473 - 480 , curran , red hook , ny , usa , ( 2009 )    gretton ,  a , borgwadt ,  b. ,  k. , rasch ,  m. , schlkopf ,  b. , smola ,  a. : a kernel method for the two - sample - problem . in advances in neural information processing systems .",
    "schlkopf , b. , j. platt , t. hofmann ( eds . ) , vol.19 , pp .",
    "513 - 520 , mit press , cambridge , ma , usa , ( 2007 )    gretton ,  a. , fukumizu ,  k. , teo ,  c.  h. , song ,  l. , schlkopf ,  b , smola ,  a. : a kernel statistical test of independence . in advances in neural information processing system .",
    "platt , j. c.,d .",
    "koller , y. singer , s. roweis ( eds . ) , vol.20 , pp .",
    "585 - 592 , mit press , cambridge , ma , usa , ( 2008 )                      smola ,  a. ,  j. , gretton ,  a. , song ,  l , schlkopf ,  b. : a hilbert space embedding for distributions . in algorithmic learning theory .",
    "hutter , m. , r. a. servedio , e. takimoto ( eds . ) , vol.18 , pp .",
    "13 - 31 , springer , berlin , germany , ( 2007 )    sriperumbudur ,  b.  k. , gretton ,  a. , fukumizu ,  k.,lanckeriet , g . , schlkopf ,  b. : injective hilbert space embeddings of probability measures . in proceedings of the 21st annual conference on learning theory .",
    "servedio , r. a. , t. zhang ( eds . ) , pp .",
    "111 - 122 , omnipress , madison , wi , usa , ( 2008 )    sriperumbudur ,  b. ,  k. , fukumizu ,  k. , gretton ,  a. , lanckeriet ,  g. , schlkopf ,  b. : kernel choice and classifiability for rkhs embeddings of probability distributions . in advances in neural information processing systems 22 , (",
    "2009 )      wchter ,  a. , biegler ,  l. ,  t. : on the implementation of a primal - dual interior point filter line search algorithm for large - scale nonlinear programming . mathematical programming , vol.106(1 ) , pp .",
    "25 - 27 , ( 2006 )    whittingstall ,  k. , dough ,  w. , matthias ,  s. , gerhard ,  s. : correspondence of visual evoked potentials with fmri signals in human visual cortex .",
    "brain topogr , servedio , r. a. , t. zhang ( eds . ) , vol.21 , pp .",
    "86 - 92 , omnipress , madison , wi , usa , ( 2008 )"
  ],
  "abstract_text": [
    "<S> do two data samples come from different distributions ? recent studies of this fundamental problem focused on embedding probability distributions into sufficiently rich characteristic reproducing kernel hilbert spaces ( rkhss ) , to compare distributions by the distance between their embeddings . </S>",
    "<S> we show that regularized maximum mean discrepancy ( rmmd ) , our novel measure for kernel - based hypothesis testing , yields substantial improvements even when sample sizes are small , and excels at hypothesis tests involving multiple comparisons with power control . </S>",
    "<S> we derive asymptotic distributions under the null and alternative hypotheses , and assess power control . </S>",
    "<S> outstanding results are obtained on : challenging eeg data , mnist , the berkley covertype , and the flare - solar dataset . </S>"
  ]
}