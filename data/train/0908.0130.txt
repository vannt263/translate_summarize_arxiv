{
  "article_text": [
    "there are several cases in high - energy physics in which we are interested into measuring an efficiency , for example when dealing with the trigger or offline event selection with the aim of measuring a cross section . with our selection",
    ", we reject a subset of the input data set and we look at the ratio between the number of surviving events and the initial number .",
    "the selection efficiency is interpreted as the probability that any single event passes the selection .",
    "this statement is independent from the actual definition of probability ( provided that it satisfies all required properties ) , but in this paper we interpret it as the degree of belief that some statement is true .",
    "this interpretation is quite natural and is needed for the bayes theorem to be able to tell us about the probability distribution of the true but unknown efficiency , which we want to estimate .",
    "when speaking about a selection process , we count the initial number of events and the final number of selected events .",
    "usually , one also knows the distribution ( actually , the histogram ) of some control parameters before and after the selection , and wants to determine the efficiency as function of such parameters . in this paper ,",
    "the aim is to show how to estimate the efficiency ( possibly as function of the parameters ) and its uncertainty within the framework of bayesian statistics . the bayesian approach is also illustrated with several examples which can serve as guidance for most problems daily encountered by the experimenters .",
    "the frequentist approach , often preferred in high - energy physics data analysis , is not addressed here  the reader is kindly invited to find the recent review by @xcite and the short comparison between the two approaches in @xcite  for the following reasons .",
    "the experimenters most often ask about the probability that , given the data , the true value assumes some value .",
    "this question is answered by the bayesian approach but is ill - defined in the frequentist approach , because the true quantity ( the efficiency ) is viewed as a fixed unknown parameter which does not `` fluctuate '' , so that a probability distribution can not be defined . on the other hand , in the bayesian framework",
    "the probability statements always refer to some state of knowledge , so that it is possible ( and indeed it is desired ) to define a probability distribution for the true quantity , interpreted as a description of our degree of belief about its unknown value .",
    "the frequentist solution is often expressed as a `` confidence interval '' with some degree of `` coverage '' .",
    "the two concepts are closely related to the ideal repetition of the same experiment , exactly in the same conditions , for a large number of times : for example , a 95% confidence interval is expected to contain the true ( unknown ) value of the parameter for ( not less than ) 95% of the repeated experiments , while ( at most ) 5% of them will miss it . however , in practice repeated experiments with identical conditions are only possible in computer simulations . in real cases ,",
    "this long sequence of measurements is either ( 1 ) impossible  because the experiment is too complex and/or too expensive , or because perfect stability in time is impossible to achieve  or ( 2 ) differently interpreted as a single and longer experiment  this is done every day in high - energy physics , when people collect together several runs to get higher statistics rather than interpreting them as repeated measurements . hence , in this paper the interval coverage is not considered a fundamental property , though it will be mentioned again later on ( in section  [ sec - using - error ] ) .    in the bayesian approach , as well as in the frequentist one ,",
    "the likelihood ( i.e.  the probability that an event occurs , given the model ) plays a central rle .",
    "however , expecially when the input number of events is very low , one must also pay attention to the `` prior '' probability density function ( pdf ) , which represents our degree of belief about the possible values of the unknown efficiency _ before _ the experiment is actually carried out .",
    "indeed , the full bayesian solution is provided by the `` posterior '' pdf , which is proportional to the product of the likelihood with the prior ( bayes theorem ) . unless the prior is pathologic ( i.e.  null or negligible in the region where the true value is ) , the likelihood will `` attract '' the posterior more and more , as the number of input events increases .",
    "when the full posterior pdf can not be used , it can be summarized by providing its mean and some measure of its dispersion ( usually the standard deviation ) , which can be used in the computation of the desired physical quantities in the usual way .",
    "the only caveat is that working with the standard deviation implies assuming that the underlying pdf is somewhat symmetric about the mean , which in some case might be a bad approximation ( see section  [ sec - binomial - approx ] ) . in general",
    ", it is recommended to work with the full posterior whenever it is possible .",
    "note that a confidence interval found with a frequentist approach can not be used together with the best estimate ( usually , the maximum - likelihood estimate or mle ) as a `` two - sigma '' range , because it can not be interpreted as the width of a probability distribution .",
    "hence , when the efficiency is needed as intermediate step in the computation of some physical quantity , the frequentist solution can not be used to compute the uncertainty on such quantity with the usual `` error propagation '' .",
    "in contrast , the bayesian posterior mean and variance can be used to compute the final uncertainty in the usual way , and this is a further reason to consider the bayesian approach .    in the rest of the paper , the problem of estimating the selection efficiency and its uncertainty ( a concept which only makes sense in the bayesian framework ) is addressed .",
    "the choice of the prior requires some care , so that recommendations are made in section  [ sec - priors ] to deal with the usual practical cases of informative and non - informative priors .",
    "few cases in which the assumptions of independent selections with unit weight are not both satisfied are addressed in section  [ sec - patologic ] .",
    "in addition , the problem of fitting parametric models is addressed in section  [ sec - comparison ] .",
    "finally , useful mathematical relations and more complicate developments are shown in the appendix .",
    "being a probability , the efficiency can not be directly measured . instead",
    ", we must estimate it with the available data : we can only count events , i.e.  measure _ relative frequencies _",
    ", that are rational numbers . by virtue of the weak law of large numbers ( or bernoulli s theorem ) , the relative frequency will converge in probability to the true efficiency in the limit of an infinite number of measurements .",
    "convergence in probability implies that `` unusual '' outcomes become less and less likely as the sequence @xmath0 of random variables progresses , and is formally expressed by saying that , for any small positive number @xmath1 , the probability that the distance between @xmath2 and the true value @xmath3 exceeds @xmath4 tends to zero : @xmath5    it is important to emphasize that the tendency of the measured frequency to the probability has _ not _ the same behavior of a mathematical limit , for which it never happens , after some point , that the distance to the limit exceeds a given small quantity : convergence in probability is weaker .",
    "indeed , it is always possible to find , even after a large number of trials , a frequency that is not very near to the probability , although the _ probability _ for this to happen decreases with increasing number of trials ( bernoulli s theorem ) .",
    "this makes measuring probabilities conceptually different from measuring physical quantities like e.g. the electric field in some point , that is given by the mathematical limit of the measured force divided by the test charge , when the latter goes to zero .",
    "however , in practice this difference is not very important because of the fluctuations of stochastic nature which affect the measurement process , and this might explain why many good books on statistical methods of data analysis ( e.g.  @xcite ) do not make this distinction .    usually , we create and fill histograms , assuming that they converge to the true distributions in the limit of infinite statistics and zero bin width as the partial sums converge to the riemann s integral . hence , we usually approximate the pdf @xmath6 describing the efficiency of the selection @xmath7 as function of the parameter(s ) @xmath8 , with the step function representing the observed relative frequencies @xmath9 : @xmath10 where @xmath11 and @xmath12 count the entries in the @xmath13-th bin before and after the selection @xmath7 .",
    "a histogram of a quantity @xmath8 obtained with a series of repeated measurements of @xmath8 is a collection of pairs @xmath14 representing the number @xmath11 of times the measured value of @xmath8 has been found in the @xmath13-th bin .",
    "the integer values @xmath11 have been observed , hence they have no uncertainty . however , if we consider the histogram as an estimate of the true distribution of @xmath8 , then the @xmath11 s are estimates of the integral of the true distribution in each bin . in the assumption that the populations of all bins are statistically independent ,",
    "the uncertainty @xmath15 of the estimate @xmath11 for the true population @xmath16 of bin @xmath13 is given by the poisson distribution : @xmath17 .",
    "this assumption is usually justified , but in our case the entries @xmath11 and @xmath18 of bin @xmath13 before and after the selection are _ not _ statistically independent , hence one can not compute the variance of @xmath19 with the usual rules of the `` propagation of errors . ''",
    "rather , the application of a selection on each bin can be considered a binomial process , with probability of `` success '' @xmath20 , the true ( but unknown ) efficiency : the probability to obtain @xmath18 events passing the selection when the efficiency is @xmath20 and the sample size is @xmath11 is : @xmath21 with mean @xmath22 and variance @xmath23 . however",
    ", this does not solve our problem , because @xmath20 is still unknown ( instead , we measured @xmath11 and @xmath18 ) .      in practice , often people compute @xmath24 from ( [ eq - binomial ] )  we drop the index @xmath13 for a while  and find the approximate variance for the relative frequency @xmath25 by dividing by @xmath26 , which has no uncertainty because it has been measured .",
    "finally , the result is approximated by the substitution @xmath27 : @xmath28 though this is a good approximation when both @xmath29 are large and the observed frequency is not too similar to zero or one , rigorously speaking the formula ( [ eq - gauss - approx ] ) is incorrect because the binomial distribution ( [ eq - binomial ] ) is a function of @xmath18 with parameters @xmath20 and @xmath11 whereas in our case @xmath11 and @xmath18 are both known and we want to find @xmath20 : we should look instead for a function of @xmath20 , with parameters @xmath11 and @xmath18 , as it will be shown in section  [ sec - true - eff ] .",
    "in addition , this approximation suffers from the following problem : the formula ( [ eq - gauss - approx ] ) gives zero uncertainty for the two limiting cases @xmath30 and @xmath31 , independently from the actual value of @xmath32 .",
    "this means that , if we have a single event ( @xmath33 ) and this survives the cut , we get the very same result ( zero uncertainty ) as the case @xmath34 , whereas one would expect the latter estimate to be ( roughly 10 times ) more precise .      in order to find a function of @xmath20 , given @xmath11 and @xmath18",
    ", we use the bayes theorem : @xmath35 . we know",
    "@xmath18 and @xmath11 and that the process is binomial , so that the likelihood is @xmath36 from equation ( [ eq - binomial ] ) .",
    "in addition , the prior should not depend on the sample size , @xmath37 , hence @xmath38 ( where @xmath39 is a normalization constant ) is the probability that the true efficiency in bin @xmath13 is between @xmath20 and @xmath40 .",
    "the prior encodes our state of knowledge before the measurement is carried out . often , either we precisely know the prior pdf or we want to model a state of `` perfect ignorance '' .",
    "the first case happens for example when we already know the efficiency of some device from a recent calibration procedure , while the second case could be the attempt of providing the most `` objective '' estimate of the efficiency .",
    "intermediate cases in which we have some vague prior expectation in mind are also possible .      if we have some knowledge before we measure the relative frequency , it should be encoded into a specific form for the prior pdf .",
    "one does not need to be very precise , because the bayes theorem assures that the final result ( the posterior probability ) will be driven by the data , provided that we have enough events and that the prior is not null or negligible in the region containing the true value .    in our case ,",
    "the recommended family for the prior pdf is the beta family , whose properties are listed in appendix  [ sec - beta ] , because its natural conjungate property for binomial sampling of @xmath41 successes among @xmath32 trials brings a beta prior with parameters @xmath42 into a beta posterior with parameters @xmath43 and @xmath44 .",
    "simple formulae exist to express the mean , mode and variance of the resulting posterior distribution , as shown in appendix  [ sec - beta ] , so that we can use the `` method of moments '' to find the beta parameters which best match our prior knowledge of the value and uncertainty of the true efficiency .",
    "we can assume that such values are the mean @xmath45 and variance @xmath46 of the prior pdf and find the beta parameters @xmath42 as @xmath47    \\label{eq - a - mom }   \\\\    b & = & ( 1-e ) \\left [ \\frac{e ( 1-e)}{v } -1 \\right ]    \\label{eq - b - mom}\\end{aligned}\\ ] ] to model the prior as a beta distribution .",
    "examples of the use of the method of moments to define an approximate prior pdf are shown in the following sections .",
    "the choice of a uniform prior ( a beta distribution with @xmath48 ) could seem `` non - informative '' ( or better the least informative one ) and appropriate if we have no prior knowledge of @xmath49 and we do not have any reason to prefer any particular range for the true efficiency @xcite .",
    "however , one may also think in terms of the logarithm ( or some other function ) of the efficiency and want a non - informative prior also for such parametrization .",
    "unfortunately , the uniform prior is not invariant under reparametrization , so that the prior written as function of @xmath50 , for example , is no more uniform ( it is multiplied by the jacobian determinant ) . in summary ,",
    "the uniform prior is a legitimate _ informative _ prior when we have good reasons to use it , but is not a correct choice when one wants to model the situation of minimal prior information .",
    "the goal of the bayesian `` reference analysis '' @xcite is to study the impact of the choice of the prior on the posterior pdf , with respect to the case of the minimal possible prior knowledge .",
    "the _ reference posterior _ is the bayesian result which minimally depends on the prior knowledge ( equivalently , it maximally depends on the likelihood ) , and the corresponding _ reference prior _",
    "@xcite is used in the bayes theorem to model the minimal information on the system before carrying out the experiment .",
    "one may view the reference posterior as the `` most objective '' bayesian solution , and consider using it whenever a result should be published in a way which minimally depends on the prior experimenter knowledge , or when it is desired to assess the dependency of the solution on the choice of the prior  the reader may find all details about the use of reference analysis as a tool for objective bayesian inference in @xcite .",
    "there are good reasons to choose the least informative prior .",
    "often the minimal prior information is indeed the best model , but when this is not the case one certainly wants to assess the dependence of the solution ( the posterior pdf ) on the choice of the prior .",
    "one of the requirements for the reference analysis is the invariance under reparametrization . for the binomial case",
    ", it comes out that the reference prior coincides with the jeffreys prior ( a beta distribution with @xmath51 , proportional to @xmath52 ) , which was indeed found by requiring invariance under reparameterization @xcite .",
    "the jeffreys prior is the recommended choice to model the least informative prior in our efficiency study , and the resulting posterior pdf , @xmath53 , is the reference posterior for the binomial case .",
    "figure  [ fig - pdf - jeffreys - prior ] shows the reference posterior for a sample size of @xmath54 and @xmath55 .            from the relations ( [ eq - beta - dist - properties ] ) of appendix  [ sec - beta ] we immediately get the following values for the mean , mode , variance and skewness of the reference posterior @xmath53 : @xmath56 ( the mode is only defined for @xmath57 , which is also an obvious requirement for the actual measurement ) .",
    "the result is that both the mean and the mode are biased ( but robust ) estimators of the efficiency . in general , the distribution is asymmetric ( @xmath58 ) and the mode is different from the mean , apart from the case @xmath59 .",
    "note that for @xmath60 the variance is not null @xmath61 and @xmath62 when @xmath63 , as expected .",
    "both @xcite and @xcite considered as a reasonable choice for the prior @xmath64 a uniform distribution in @xmath65 $ ] , though this is questionable because it does _ not _ represent a `` complete ignorance '' , as mentioned above ( section  [ sec - non - inf - prior ] ) . on the other hand ,",
    "the choice of the uniform prior is interesting at least for two reasons .",
    "first , the posterior pdf @xmath66 , shown in figure  [ fig - pdf - unif - prior ] with 10 and 100 initial events , is proportional to the likelihood , so that the mle ( i.e.  the frequentist solution ) coincides with the mode of the posterior pdf .",
    "second , credible intervals obtained with such posterior have been implemented in root , being the only available bayesian credible regions in such framework .",
    "the mean of @xmath66 is @xmath67 and is a bit more biased estimator than the reference mean ( [ eq - mean - with - uniform - prior ] ) .",
    "the mode , i.e.  the value at which @xmath68 , is @xmath69 and indeed it coincides with the mle @xmath25 .",
    "again , because of the shape asymmetry , this does not coincide with the expectation value , apart for the case @xmath70 .",
    "finally , the variance is : @xmath71    incidentally , figure  [ fig - pdf - unif - prior ] is also useful to check by eye if the symmetric binomial approximation of section  [ sec - binomial - approx ] is well justified , because the only difference between the posterior @xmath66 and the binomial likelihood is just the normalization ( the likelihood is not normalized ) ,    the reference posterior ( obtained using the jeffreys prior ) is not much different from the posterior obtained with the uniform prior , unless @xmath32 is relatively small .",
    "the difference with respect to the frequentist mle is usually larger , and is most apparent when the measured relative frequency is one or zero ( which are never the best estimate in the bayesian approach ) , though is less significant for the reference posterior .",
    ".comparison between the posterior means and standard deviations obtained with the uniform and jeffreys priors .",
    "the frequentistic mle and the standard deviation in the binomial approximation are also shown . [",
    "cols=\">,>,^,^,^,^,^,^,^,^ \" , ]     table  [ tab - unif - jeffreys ] shows the mean ( or mle ) and square root of the variance for all cases , together with the ratio between the posterior mean and variance obtained with the uniform prior and the corresponding quantity computed with the reference posterior .",
    "clearly , the biggest discrepancy between the results obtained with the uniform and jeffreys priors is obtained when @xmath30 and increases for higher @xmath32 , because the expected value using the uniform prior is @xmath72 and the reference posterior mean is @xmath73 .",
    "the ratio between the standard deviations is larger than one at very small and large @xmath41 values and smaller when the efficiency is intermediate , whereas the uniform mean is larger than the reference posterior mean for small relative frequencies and smaller for large relative frequencies ( the means are equal only when @xmath59 ) .",
    "given that the reference posterior is `` more objective '' and that its mean is a less biased estimator of the true efficiency , the recommendation is to use the jeffreys prior unless there are good reasons to use the ( informative ) uniform prior .",
    "if we have two independent efficiency measurements for the same process and we want to use all available information , the correct approach to combine them is to merge the samples before and after the selection and use the results to make the final estimate ( this is also true for the frequentist approach ) . in the bayesian approach , the very same result is also obtained if we use the first posterior pdf to model the prior for the second estimate .",
    "indeed , the bayes theorem can be interpreted as a model for our learning process : it makes use of all information available at any given time , a very desirable property .",
    "to make an example , let us consider the histograms @xmath74 and @xmath75 , filled with all events in the first sample and with the subset obtained after the selection , and the analogous histograms @xmath76 and @xmath77 filled before and after the cut with the second sample .",
    "here we assume that all histograms are accessible , however the result is also valid if one of them is missing and we are given the posterior pdf which summarizes the unavailable measurement .",
    "we take a single bin ( omitting its index ) and assume that the prior pdf for the first measurement is a beta density with parameters @xmath42 ( in case of no prior knowledge we recommend using the jeffreys prior with @xmath78 ) , so that the posterior of the first measurement is the beta density @xmath79 .",
    "this then is used as the prior for the second estimate , whose posterior pdf is the beta density @xmath80 . the very same posterior is obtained if we start from the initial prior @xmath81 and consider the joint sample of size @xmath82 from which only @xmath83 events survived .",
    "with the same method , we can also use simulated data to provide the prior distribution for the true efficiency , to be used in conjunction with real data in the bayesian approach .",
    "this is mostly useful in case we need to test different kinds of `` systematic '' effects on the outcome of a real experiment .    given that using densities belonging to the beta family allows to summarize all available information in a rather easy way , the recommended way of communicating efficiencies is to provide the values of the 4 parameters of the beta posterior and of the beta prior .",
    "this is equivalent to the knowledge of the original samples ( before and after the selection ) and can be used to make a combined estimate of the efficiency without the original data : it is sufficient to use the corresponding beta distribution as prior for the new measurement . in addition",
    ", the users will be able to test the sensitivity of the result to the choice of a different prior .",
    "when the efficiency is needed to convert the measured quantities into the true ones ( to get e.g.  the cross section ) , its best estimate is needed together with its uncertainty ( whenever using the full distribution is not practical ) .",
    "the mean and variance of the posterior pdf can be used in computing quantities following the usual recipe of the `` error propagation '' .",
    "the only caveat is that , in general , the posterior is asymmetric so that the trivial recipe of taking `` @xmath84 '' intervals around the expected result might produce intervals which extend into an unphysical region .",
    "the asymmetry is more pronounced when the efficiency is very near to one or zero , and is negligible when the binomial approximation of section  [ sec - binomial - approx ] is good ( i.e.  when both @xmath85 ) . in general , it is recommended to work with the full posterior whenever it is possible , expecially when the ( symmetric ) binomial approximation is poor .    because physicists are used to take the interval @xmath86 , with @xmath87 as the gaussian credible interval with 68.3% probability",
    ", this feature is often desired : @xcite recommends using the smallest interval @xmath88\\subset[0,1]$ ] that contains the probability @xmath89 , i.e.  the shortest credible interval with posterior probability @xmath90 ( known in the statistics literature as the `` highest posterior density '' or hpd region ) , arguing that in practical applications it will behave more or less as the `` @xmath91 '' interval defined with a gaussian standard deviation .",
    "he also provided code to compute such interval when using the uniform prior , which has been adopted by root .",
    "we emphasize that this prescription is not invariant under reparametrization ( such credible interval is no more the hpd region after a general change of variable ) , which is a very desirable feature . it is recommended instead to show an invariante region like the 95% `` reference credible intervals '' @xcite in the efficiency graph ( see section  [ sec - invariant - cred - reg ] below ) .",
    "showing asymmetric errors is the recommended style for efficiency graphs , because the posterior pdf in general is not symmetric .",
    "the root method http://root.cern.ch/root/htmldoc/tgraphasymmerrors.html#tgraphasymmerrors:bayesdivide[tgraphasymmerrors::bayesdivide ( ) ] can be used to plot efficiency graphs showing paterno s intervals ( obtained with the uniform prior ) as asymmetric errors on the relative frequencies .",
    "unfortunately , it does not support non - integer input ( root 5.26/00 is considered here ) , so that it can not be used to plot intervals corresponding to any possible choice for the prior are integers , one can use http://root.cern.ch/root/htmldoc/tgraphasymmerrors.html#tgraphasymmerrors:bayesdivide[tgraphasymmerrors::bayesdivide ( ) ] to plot 68.3% credible regions by passing it modified histograms filled with @xmath92 and @xmath93 . ]",
    "( most notably , it can not be used with the jeffreys prior ) .",
    "in addition , paterno s intervals are not invariant under reparametrization .",
    "so far , there is no available root method for plotting reference credible histograms , though http://root.cern.ch/root/htmldoc/roostats__bayesiancalculator.html[roostats::bayesiancalculator ] could be a starting point to find central credible intervals .",
    "the latter contain the same probability on the left and on the right of the expected value and are invariant .      in this section",
    "we summarize the treatment by @xcite .",
    "a possible choice for a credible interval is the _ lowest posterior loss _ or lpl credible region , which depends on the definition of the _ loss function _ which specifies the loss to be suffered if a particular value for the efficiency is used in place of the true value . to obtain a lpl credible region which is invariant under reparametrization ,",
    "the loss function should depend on the full pdf , not on the value of a parameter .",
    "for example , the common choice of the quadratic distance @xmath94 does not lead to an invariant solution .",
    "let us consider a model with probability distribution @xmath95 for the observations @xmath96 , @xmath97 , dependent on the parameters @xmath98 , @xmath99 .",
    "an _ intrinsic loss function _ is a symmetric non - negative function @xmath100 which is zero if and only if @xmath101 almost everywhere in @xmath102 .",
    "an intrinsic loss function is invariant under reparametrization but not , in general , under a one - to - one transformation of the observations @xmath8 . because this is a very desirable property , we restrict ourselves to the intrinsic loss functions which are also invariant under one - to - one transformations of @xmath8 .",
    "an example from this class is the the @xmath103 norm , that is the integral of the absolute value of the difference between two distributions , computed at the same point @xmath8 , over the whole support @xmath102 .",
    "when applied to the reference posterior for the binomial case , the @xmath103 norm gives the invariant expected loss @xmath104 independent from one - to - one transformations of @xmath105 .",
    "one can now build a lpl @xmath106-credible region by finding the interval @xmath107 \\subset [ 0,1]$ ] which minimizes ( [ eq - l1-norm ] ) under the constraint @xmath108 .",
    "the behaviour of many important limiting processes in probability theory and statistical inference is better described in terms of another measure of divergence , related to the information theory , the _ intrinsic discrepancy _ @xmath109 , defined in terms of the kullback - leibler _ directed divergence _ @xmath110 between two pdfs @xmath111 : @xmath112 the intrinsic discrepancy is symmetric , non - negative , defined for strictly nested supports , invariant under one - to - one transformations , and additive for independent observations .",
    "it may be viewed as the minimum expected log - likelihood ratio in favour of the model which generates the data ( the `` true '' model , which is assumed to be described either by @xmath113 or @xmath114 ) and can be used to defined the _ intrinsic discrepancy loss _",
    "@xmath115 where @xmath116 is the parameter in which we are interested . for the binomial model ,",
    "the intrinsic discrepancy loss is @xmath117 where @xmath118 is the intrinsic discrepancy between bernoulli random variables with parameters @xmath119 and @xmath105 .    finally , _",
    "intrinsic credible regions _ are defined as the lowest posterior loss credible regions which correspond to the use of the intrinsic discrepancy loss function together with the reference posterior . the reference posterior expected loss from using @xmath119 rather than @xmath105 in the binomial model is @xmath120 and the intrinsic @xmath106-credible interval is the interval @xmath107 \\subset [ 0,1]$ ] which minimizes the loss  ( [ eq - ref - post - exp - loss ] ) under the constraint @xmath121 .",
    "approximate expressions for the intrinsic credible intervals are based on the asymptotic expressions obtained in theorem  4.1 of @xcite . in particular , they are built upon the _ reference parametrization _",
    "@xmath122 of the parameters @xmath116 of interest , defined as the one for which the reference prior is uniform . for the binomial model",
    "there is a single parameter @xmath105 and @xmath123 , i.e.  @xmath124 . using a shorter notation for the reference posterior mean @xmath125 and variance @xmath126 of the parameter of interest , the variance of the reference parametrization is @xmath127 ^ 2     = \\frac{1}{n+2}\\ ] ]",
    "while its mean is @xmath128 where @xmath129 and @xmath130 denote the first and second derivative with respect to @xmath105 . finally , the asymptotic intrinsic @xmath106-credible interval in the reference parametrization is @xmath131 $ ] where @xmath132 where @xmath133 is the @xmath134 quantile of the normal distribution .",
    "the intrinsic @xmath106-credible interval for the efficiency is obtained by transforming back to @xmath135 .",
    "the intrinsic @xmath106-credible intervals have also approximate @xmath106 coverage when interpreted in the frequentist way .",
    "however please note that , depending on the actual values of the parameters @xmath29 , the reference credible intervals can overcover or undercover the true value in repeated experiments , while the best practice in the frequentist approach is to choose intervals which never undercover ( for more details , see @xcite ; a frequentist choice for the asymmetric confidence intervals is also reported on chapter 32 of @xcite ) .",
    "so far , we assumed that all entries of the initial histogram had unit weight and had been selected by an independent binomial process .",
    "this may not be true in all cases , as it happens sometimes in high - energy physics .",
    "for example :    1 .",
    "the initial histogram @xmath76 was obtained by scaling the simulated data sample to normalize it to some different value of the cross section .",
    "the histogram _ should not be used _ to make efficiency studies !",
    "rather , the efficiency should be estimated by using the _ original _ histogram ( filled with unit weights ) , in order to have a binomial process ; 2 .",
    "the initial histogram @xmath76 was obtained as the weighted average of several contributions ( for example , by combining simulated samples corresponding to the same integrated luminosity but having very different cross sections ) .",
    "as above , the histogram _ should not be used _ to make efficiency studies , which require the use of the original histogram ( filled with unit weights ) ; 3 .",
    "the initial histogram @xmath76 has been filled using weights @xmath136 , summing up terms which may give a positive or negative contribution to the final production probability .",
    "this is the case of the output from mc@nlo , and is addressed in section  [ sec - weights ] ; 4 .",
    "the numbers of events before and after the cut have been obtained with a different procedure than simply counting events .",
    "for example , they could come from fits as in section  [ sec - eff - from - fits ] ; 5 .",
    "the initial sample was selected by a non independent process .",
    "this can be important when measuring the trigger efficiency and is addressed in section  [ sec - trigger ] .      in high - energy physics simulations",
    ", it might happen to work with samples filled with positive and negative unit weights , as it happens for example in the output of mc@nlo @xcite .",
    "each individual event is independently simulated , and knows nothing about its weight .",
    "hence we can separately consider the samples with positive and negative unit weights , with @xmath137 initial numbers of events and @xmath138 entries after the selection .",
    "for each sample , the efficiencies @xmath139 and @xmath140 can be computed individually following the methods already seen in previous sections : using the jeffreys prior , their posteriors are beta distributions with parameters @xmath141 and @xmath142 , with @xmath143 .",
    "however , we are interested in the overall efficiency , after subtraction of the two samples . for mc@nlo , its authors say that the efficiency should be estimated as @xmath144 when @xmath145 or zero otherwise , and they suggest to use the usual `` propagation of errors '' to estimate its variance whenever the numbers are high enough that the binomial approximation holds , or to run many mc samples through the cuts and look at the dispersion in the result if the data sample is too small .",
    "here we use instead the method of moments to find the parameters of the posterior beta distribution that matches the approximate mean @xmath146 and its approximate variance .",
    "one may write @xmath147 , which is our estimate for the weighted sum @xmath148 .",
    "the latter has variance @xmath149 where @xmath150 are computed from the posterior pdfs of the individual samples with homogeneous weights .",
    "putting @xmath151 and @xmath152 into equations ( [ eq - a - mom ] ) and ( [ eq - b - mom ] ) one can finally find the approximated posterior beta density which gives the desired result .",
    "this is the recommended approach , given that the other method suggested below is much more complex .",
    "please note that the method above can be easily generalized to be used in case we are told to consider @xmath153 as the best estimate of the efficiency from a mixture of samples with weights @xmath154 , provided that we know all pairs @xmath155 of events before and after the selection for each homogeneous sample . in such case , we can apply the method of moments with @xmath156 and @xmath157 / ( \\sum_i w_i)^2 $ ] to find the approximated posterior pdf .",
    "another possibility is to use the general results obtained by by @xcite , who found a ( rather complex ) analytical expressions for the case in which one makes the difference between two independent random variables , each one following a beta distribution .",
    "the relevant properties of their `` beta - difference '' distribution are summarized in appendix  [ sec - pdf - diff ] but have the following disadvantages with respect to the approximate solution explained above .",
    "first , in general the difference of two binomial parameters has domain in @xmath158 $ ] , so that in our case the posterior needs to be set to zero for negative values and renormalized .",
    "a numerical approach needs to be used to handle the resulting posterior , equation  ( [ eq - pdf - diff ] ) of appendix  [ sec - pdf - diff ] .",
    "second , @xcite adopted the uniform prior , arguing that it can be considered non - informative .",
    "we have already explained in section  [ sec - non - inf - prior ] that this is not a proper choice , so that such solution is appropriate only when the uniform prior is justified as an informative prior pdf .",
    "let us consider the case in which we have a distribution of @xmath159 events which is a mixture of `` signal '' ( s ) and `` background '' ( b ) events , and we want to know the effect of some cut on the two components . we obtain separate estimates @xmath160 for the initial numbers of s and b events by fitting the distribution with a function which is a mixture of two pdfs with relative weights @xmath161 and @xmath162 , so that @xmath163 and @xmath164 may be non - integer positive numbers with the constraint @xmath165 .",
    "later , we apply the cut under study and fit the resulting distribution , containing @xmath166 events , to obtain the estimates @xmath167 of the final numbers of s and b events .",
    "again , the fits provides the relative weight @xmath168 for the s component , so that @xmath169 and @xmath170 , with @xmath171 .",
    "we are interested into the efficiency of the cut for signal events , which would be naively expected to be @xmath172 , though this is not the correct guess , as shown below .",
    "a possible way of approaching the problem is to consider the following distinct phases :    1 .",
    "the first fit separates s from b events from the initial distribution and it is assumed to provide the best estimates of @xmath173 and its rms @xmath174 .",
    "this can be considered a binomial process in which we select @xmath163 signal candidates out of @xmath159 , so that the result can be cast with the method of moments in the approximate form of a beta density whose mean is @xmath161 and whose variance @xmath175 is also given by the fit : @xmath176 .",
    "it is sufficient to use equations ( [ eq - a - mom ] ) and ( [ eq - b - mom ] ) with @xmath177 and @xmath178 .",
    "this means that we consider the weight @xmath161 returned by the fit as the posterior estimate of the efficiency of the initial signal selection .",
    "the obtained beta density @xmath179 will be used as prior pdf for the next step .",
    "incidentally , we note that the non - integer nature of @xmath173 and @xmath180 is not a problem here , because the posterior beta density is not required to have integer parameters",
    "we are interested into the cut efficiency for signal candidates , that is the probability that an event both passes the cut and is assigned to the s population by the second fit .",
    "we can either consider this a unique selection process or split it into @xmath181 .",
    "the latter case implies one more iteration of the bayes theorem , but here we consider the unique selection because it is simpler and we are not interested into the details of the second fit .    from the discussion above , we know that the posterior pdf describing the efficiency of our cut for signal candidates is a beta density with parameters @xmath182 and @xmath183 , where @xmath42 have been determined with the method of moments from the first fit .",
    "hence , the expected efficiency for our selection is given by equation ( [ eq - beta - dist - properties ] ) from appendix  [ sec - beta ] : @xmath184 , which is different from the naive value @xmath172 , in that it explicitely takes into account the effects of the fitting procedure , i.e.  of the classification in `` signal '' and `` background '' events .    because in general @xmath185 and @xmath186 , the final beta density is different from what is obtained from simple event counting using jeffreys prior .",
    "depending on the fit properties ( expecially on the first fit ) , the final pdf may be wider than the reference posterior , most notably when the number of events is small .",
    "the fitting procedure has some cost : intuitively , a fit which better discriminates between signal and background events will provide a narrower density , whereas a poor fit will end up into a wider one .",
    "the good point of the bayesian treatment is that it accounts for all the available information , not that it produces narrower distributions .",
    "the case in which the initial histogram @xmath76 does not represent a statistically independent sample is expecially important in trigger efficiency measurements , when there is no other trigger selection which is statistically uncorrelated with respect to the signature @xmath7 under study .",
    "figure  [ fig - samples ] shows a situation in which one wants to study the systematic effect of a previous trigger selection @xmath187 on @xmath7 starting with the true distribution , on mc data .",
    "as we have seen , the best estimate of the efficiency of @xmath7 as function of some quantity @xmath8 is given by the histogram ratio between the distribution of @xmath8 after the selection ( the histogram filled with sample ) and its distribution before ( sample ) .",
    "real data can only be taken with trigger @xmath187 ( which in practice is chosen to be the least correlated as possible to @xmath7 ) , obtaining sample . later ,",
    "condition @xmath7 can be required on  obtaining sample , which has been selected by requiring _ both _ @xmath187 and @xmath7 , and the histogram division /  estimates the probability @xmath188 to select one event with @xmath7 , given that it was already selected by @xmath187 .",
    "@xmath189    in order to find the desired true efficiency @xmath190 , we make use of the relation defining the conditional probability , @xmath191 , obtaining @xmath192 $ ] , where the fraction in brackets can not be determined with real data alone . the true efficiency of @xmath187 alone is found with simulated data by requiring condition @xmath187 on sample , whereas the relative efficiency @xmath193 of @xmath187 with respect to @xmath7 is obtained by requiring condition @xmath187 on sample .",
    "the conclusion is that , without some statistically independent trigger , one can not estimate the trigger efficiency using real data only . rather , a simulation is required to measure the impact of the non - independent trigger @xmath187 on the selection @xmath7 under study .",
    "if the approximation in which @xmath7 and @xmath187 are independent is good enough ( mc data can be used to check this ) , the value of the fraction can be considered equal to one in all bins , and the bin - wise ratio between  and  gives a good estimate of the true @xmath7 efficiency .",
    "such approximation is justified if the ( systematic ) effect of @xmath194 is small compared to the statistical uncertainty on the ratio between  and  ( which might not be true in all bins ) , i.e.  when the square root of the variance is significantly larger than @xmath195 .",
    "otherwise , the recommended approach is to model the knowledge about such ratio with a beta density in each bin , and obtain the beta posterior for the efficiency of the selection @xmath7 as explained before , using the result from the mc encoded in the form of the prior beta density .",
    "here we consider a `` real life '' example : a simulation of a common experimental setup , the measurement of the selection efficiency versus the threshold on a scalar quantity , and the fit of the resulting `` turn - on '' plot in root . because no bayesian fitting technique is yet available in root , a pragmatic approach is to test the different fitting options available in the release ( 5.26/00 ) used for this work .",
    "the model describes the energy lost by minimum ionizing particles ( mips ) while crossing a thin slab of active material ( e.g.  a scintillator ) as a landau distribution .",
    "it is assumed that the read - out electronics ( e.g.  a photomultiplier tube read by a charge integrator ) is tuned to have a dynamic range large enough that the peak of the mip energy distribution is not very distant from the pedestal ( figure  [ fig - model ] , left panel ; the energy is in arbitrary units , e.g.  adc counts ) .",
    "the experimental setup is triggered by a comparator whose threshold is somewhere in between the two peaks . due to the electronic jitter",
    ", the comparator does not apply a sharp cut on the distribution .",
    "rather , a smooth `` turn - on '' function ( figure  [ fig - model ] , right panel ) is obtained , due to the random fluctuations on the difference between the measured energy and the threshold .",
    "the ideal ( and quite common ) case of gaussian fluctuations has been simulated , so that the turn - on curve is an `` error function '' ( the gaussian integral from minus infinity to the considered value ) .",
    "a total of 1 million events has been simulated , with energy following the distribution shown in the left plot of figure  [ fig - model ] .",
    "each event has been `` rejected '' accordingly to the true threshold function shown in the right plot of the same figure .",
    "later , the best estimate of the efficiency has been obtained by taking the bin - wise ratio of the energy histograms filled for the events passing the threshold and for all events .",
    "figure  [ fig - full - dist ] shows the full sample , before and after the cut , and four different fits performed with the following function : @xmath196\\ ] ] where @xmath197 is the best estimate of the threshold position , @xmath198 the threshold width ( gaussian standard deviation ) , @xmath199 is the plateau value reached at high energy ( left fixed at 1 in the fits ) and @xmath200 is the lowest efficiency ( i.e.  the `` offset '' of the `` turn - on '' curve , left fixed at zero ) .",
    "one of the fits is performed using the frequentist approach , implemented in the root class http://root.cern.ch/root/html/tbinomialefficiencyfitter.html[tbinomialefficiencyfitter ] , which fits the experimental points with a theoretical model using the maximum likelihood method ( the input histograms must be filled with weights of 1 ) .",
    "the best values are found by maximizing the sum of the binomial log - likelihoods defined for each bin in the input histograms ( which must have the same binning ) and , from the bayesian point of view , it gives the correct answer when the binomial approximation of section  [ sec - binomial - approx ] is acceptable , i.e.  when both @xmath201 are sufficiently large , a quite usual case .",
    "as expected with so many events , the fit results using the binomial approximation ( root option ` b ` of th1::divide ( ) ) , the standard deviations computed with the jeffreys prior , the asymmetric credible intervals recommended by paterno , or tbinomialefficiencyfitter agree with each other .",
    "the latter method underestimates the threshold jitter obtaining a result which is not compatible with the true value ( 0.2 a.u . )  within the quoted uncertainty , whereas the values from the other fits are at about 2 standard deviations from the true model .",
    "apart from tbinomialefficiencyfitter , which makes use of the two histograms filled before and after the cut , the other fits use the value of the ratio in each bin and the uncertainty which we have assigned following different methods .",
    "such fits have been made with the option ` me ` of th1::fit ( ) to select error estimation using the minos technique and the improved fit results by tminuit .",
    "another possible approach is to use the option ` ll ` to use the log - likelihood method instead of the chi - square method , when the bin contents are not integer values  such option is not supported by tgraph::fit ( ) hence was not used when fitting the output of tgraphasymmerrors::bayesdivide ( ) . in this case , the estimated errors are much larger than with ` me ` ( also , the reported quality of the fit is worse ) but again the results are compatible with the true model .    next , the sample has been divided into 100 subsets of 100 , 1000 , and 10000 events each , and repeated turn - on fits have been attempted with all methods .",
    "the values of the threshold and width from all fits have been histogrammed ( only when the fit was successful ) as shown in the figures [ fig - gauss ] and [ fig - bayes - sigma ] , in which the fit options ` me ` and ` ll ` are compared for the `` binomial errors '' ( option ` b ` for the histogram division ) and the reference standard deviations , and in figure  [ fig - likelihood - hpd ] , showing the results by tbinomialefficiencyfitter and by the chi - square fit of the graph showing paterno s hpd credible intervals .",
    "these figures show that the log - likelihood fit ( option ` ll ` ) obtains results which are more closely clustered around the true values when using the binomial approximation for the errors ( figure  [ fig - gauss ] ) . however , as noticed above , this approximation is very bad when one has most points at zero or full efficiency , because it assigns zero uncertainties to such cases .",
    "this problem is visible also with the full sample : in figure  [ fig - full - dist ] the fit with binomial errors has a very low number of degrees of freedom , compared to the other three cases ( bins with zero uncertainty are not counted as degrees of freedom ) .",
    "when using the option ` ll ` the number of degrees of freedom increases to 148 , so that this is preferrable when using the approximation of binomial errors .    in our example",
    ", the turn - on is so sharp that , expecially for small samples , is is very likely that there is no single bin at intermediate efficiency , which makes the fit fail in most cases with the option ` me ` , though in less cases with the option ` ll ` . even with larger samples ,",
    "the passage from zero to full efficiency happens in a couple of bins only , so that this is admittedly a quite pathological situation .",
    "having no bin at intermediate efficiencies is a problem for all fitting techniques , though tbinomialefficiencyfitter seems to suffer less about it .",
    "its clustering around the true values is acceptable and similar to the fit done with paterno s hpd credible intervals ( figure  [ fig - likelihood - hpd ] ) , but the latter fails many more times .",
    "when using the standard deviations computed with the jeffreys prior , maximizing the log - likelihood instead of minimizing the chi - square helps reducing the number of failing fits too , though the clustering around the true values does not improve significantly .",
    "in conclusion , http://root.cern.ch/root/html/tbinomialefficiencyfitter.html[tbinomialefficiencyfitter ] is the most robust way of fitting the efficiency implemented in root , though in our test it appears to underestimate the width of the turn - on curve . on the other hand ,",
    "the binomial approximation is by far the worst approach .",
    "if the samples are large enough that it is safe to use the binomial approximation for fitting , then one should always use the option ` ll ` , which in our test improves the clustering around the true values .",
    "even with such option , the performance is similar to what is obtained by the fits based on the reference standard deviations ( but the latter are to be certainly preferred if the default chi - square fitting method is used ) .",
    "finally , though it is possible to directly fit the graph showing paterno s hpd credible intervals , there is no advantage in doing so .",
    "the best approach is to fit the data using another method and plot the resulting function on top of the graph showing the hpd credible intervals , which are the only asymmetric efficiency intervals already available in root , though one could implement other things .",
    "estimating the selection efficiency is a fundamental task in most data analyses , based on simulated and/or real data .",
    "the measured relative frequency provides the best estimate of the true efficiency in the frequentist approach and coincides with the posterior mode obtained in the bayesian treatment with uniform prior .",
    "however , such prior can not be considered non - informative . instead , if we are completely uncertain about the efficiency before making the experiment , the use of the jeffreys prior is recommended .    in general ,",
    "if some prior knowledge is available , it is recommended to encode it into a function belonging to the family of beta distributions , whose parameters can be determined with the method of moments if an approximation is needed .",
    "this ensures that the posterior also belongs to the same family , so that all properties summarized in appendix  [ sec - beta ] are immediately available .",
    "an important example of the use of informative priors is the combination of independent samples , which is also the correct way for including prior knowledge coming from simulations to model systematic effects .",
    "the knowledge of the uncertainty on the efficiency is needed when scaling observed quantities to estimate their original values ( e.g. the true rate ) . in this case , the recommended approach is to use the mean and variance of the posterior pdf in the computation , whenever the use of the full posterior is not practical .",
    "the usual variance algebra holds , with the caveat that the square root of the final variance might not be good to define a symmetric credible interval , because of the inherent asymmetry of the posterior in the general case .",
    "though in many applications the posterior will be significantly peaked around the true value , so that the binomial ( symmetric ) approximation holds , care needs to be taken when handling very low or very high efficiencies , and when the number of events is relatively small , because such approximation behaves poorly in such cases ( using the full posterior is always better , if possible ) .",
    "when communicating the result of an efficiency measurement , the recommended approach is to provide the @xmath202 beta parameters corresponding to the posterior and prior pdfs , so that the user will be able to test the effects of different priors and to combine the posterior with other independent measurements . in the plots ,",
    "the observed frequency should be accompanied by asymmetric error bars .",
    "paterno s hpd credible intervals are already available in root via http://root.cern.ch/root/htmldoc/tgraphasymmerrors.html#tgraphasymmerrors:bayesdivide[tgraphasymmerrors::bayesdivide ( ) ] but are not necessarily the best choice : reference credible intervals would be a better option , as shown in section  [ sec - invariant - cred - reg ] .    when fitting the efficiency with a theoretical function , few different methods are available in root .",
    "as it is shown in section  [ sec - comparison ] , it is important to avoid performing a chi - square fit using binomial errors ( which unfortunately seems to be the easiest choice , though it is the worst solution ) .",
    "the tbinomialefficiencyfitter method is the most robust way of fitting turn - on plots in root , though a better parameter estimation is obtained by performing a fit with the reference standard deviations ( it is suggested to try using the option ` me ` first , and switch to ` ll ` in case of failure ) .",
    "finally , special care must be used when handling samples that do not have unit weights or are not independent .",
    "few recipes to deal with the most common use cases in particle physics have been sketched in section  [ sec - patologic ] .",
    "this appendix summarizes mathematical definitions and properties that are useful to deal with binomial processes .",
    "they can be found in standard books like @xcite .            for @xmath211 $ ] , the _ beta distribution _ has pdf @xmath212 and cumulative distribution function @xmath213 where @xmath214 is the _",
    "regularized incomplete beta function_. the mean @xmath45 , mode @xmath215 , variance @xmath46 and skewness @xmath216 of the beta distribution ( [ eq - beta - dist ] ) are @xmath217        here we consider two random variables that correspond to the selection efficiencies @xmath139 and @xmath140 for the samples with positive and negative weights considered in section  [ sec - weights ] .",
    "we want to find the posterior for the difference @xmath220 using the general result found by @xcite .",
    "their expression is valid for the general difference of two beta - distributed random variables , with domain ranging from @xmath221 to @xmath222 .",
    "however , we know that the physical efficiency can not be negative , hence we restrict the posterior to @xmath65 $ ] ( the normalization needs to be recomputed ) .",
    "they chose to use a uniform prior so that their posteriors , having counted @xmath223 and @xmath224 initial events and @xmath225 and @xmath226 entries after the selection , are beta distributions with parameters @xmath227 , @xmath228 and @xmath229 , @xmath230 .",
    "the correspondence between their and our notation , when dealing with the posterior under the assumption of uniform priors for @xmath139 and @xmath140 , is : @xmath231 , @xmath232 , @xmath233 , @xmath234 .",
    "their result ( equations ( 2a ) and ( 2c ) in @xcite ) can be rewritten in our case ( being @xmath235 ) in a more compact form , which makes use of the third appell hypergeometric function @xmath236 obtaining an expression valid for @xmath237 ( to be renormalized ) : @xmath238      r.d .",
    "cousins , k.e .",
    "hymes , j. tucker , `` frequentist evaluation of intervals estimated for a binomial parameter and for the ratio of poisson means '' , nim a 612 ( 2010 ) 388398 , http://arxiv.org/abs/0905.3831[arxiv:0905.3831 ] ."
  ],
  "abstract_text": [
    "<S> the measurement of the efficiency of some event selection is always an important part of the analysis of experimental data . </S>",
    "<S> the statistical techniques based on the use of the bayes theorem which are needed to determine the efficiency and its uncertainty are reviewed . </S>",
    "<S> the problem of choosing a meaningful prior is addressed , and different priors are considered in real - life use cases . </S>",
    "<S> the use of the uncertainties in practical cases is also considered , together with the problem of combining different samples . </S>",
    "<S> `` pathological '' cases are also addressed , in which non - unit weights or non - independent selections have been used to fill the histograms . </S>",
    "<S> the use of the family of beta distributions is illustrated in the examples , showing how its conjungate property for binomial sampling makes it the most convenient choice for defining priors . </S>",
    "<S> finally , several recommendations are made about the choice of the prior and about using and communicating the results .    </S>",
    "<S> efficiency , bayesian approach , reference analysis , non uniform priors    02.70.rr , 06.20.dk , 07.05.kf , 29.85.fj </S>"
  ]
}