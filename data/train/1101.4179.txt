{
  "article_text": [
    "clustering with fast algorithms large samples of high dimensional data is an important challenge in computational statistics and machine learning , with applications in various domains such as image analysis , biology or computer vision .",
    "there is a vast literature on clustering techniques and recent discussions and reviews may be found in @xcite or @xcite .",
    "moreover , as argued in @xcite , the development of fast algorithms is even more crucial when the computation time is limited and the sample is potentially very large , since fast procedures will be able to deal with a larger number of observations and will finally provide better estimates than slower ones .",
    "we focus here on partitioning techniques which are able to deal with large samples of data , assuming the number @xmath0 of clusters is fixed in advance .",
    "the most popular clustering methods are probably the non sequential ( @xcite ) and the sequential ( @xcite ) versions of the @xmath0-means algorithms .",
    "they are very fast and only require @xmath1 operations , where @xmath2 is the sample size",
    ". they aim at finding local minima of a quadratic criterion and the cluster centers are given by the barycenters of the elements belonging to each cluster .",
    "a major drawback of the @xmath0-means algorithms is that they are based on mean values and , consequently , are very sensitive to outliers . such atypical values , which may not be uncommon in large samples , can deteriorate significantly the performances of these algorithms , even if they only represent a small fraction of the data as explained in @xcite or @xcite . the @xmath0-medians approach is a first attempt to get more robust clustering algorithms ; it was suggested by @xcite and developed by @xcite .",
    "it consists in considering criteria based on least norms instead of least squared norms , so that the cluster centers are the spatial medians , also called geometric or @xmath3-medians ( see @xcite ) , of the elements belonging to each cluster .",
    "note that it has been proved in @xcite that under general assumptions , the minimum of the objective function is unique .",
    "many algorithms have been proposed in the literature to find this minimum .",
    "the most popular one is certainly the pam ( partitioning around medoids ) algorithm which has been developed by @xcite in order to search for local minima among the elements of the sample .",
    "its computation time is @xmath4 and as a consequence , it is not very well adapted for large sample sizes .",
    "many strategies have been suggested in the literature to reduce the computation time of this algorithm .",
    "for example subsampling ( see _ e.g _ the algorithm clara in @xcite and the algorithm clarans in @xcite ) , local distances computation ( @xcite ) or the use of weighted distances during the iteration steps ( @xcite ) , allow one to reduce significantly the computation time without deteriorating the accuracy of the estimated partition .",
    "trimmed @xmath0-means ( see @xcite and references therein ) is also a popular modification of the @xmath0-means algorithm that is more robust ( see @xcite ) in the sense that it has a strictly positive breakdown point , which is not the case for the @xmath0-medians .",
    "note however that the breakdown point is a pessimistic indicator of robustness since it is based on the worst possible scenario . for a small fraction of outliers",
    "whose distance is moderate to the cluster centers , @xmath0-medians remain still competitive compared to trimmed @xmath0-means as seen in the simulation study .",
    "furthermore , from a computational point of view , performing trimmed @xmath0-means needs to sort the data and this step requires @xmath5 operations , in the worst cases , at each iteration so that its execution time can get large when one has to deal with large samples .    borrowing ideas from @xcite and @xcite who have first proposed sequential clustering algorithms and @xcite who have studied the properties of stochastic gradient algorithms that can give efficient recursive estimators of the geometric median in high dimensional spaces , we propose in this paper a recursive strategy that is able to estimate the cluster centers by minimizing a @xmath0-medians type criterion .",
    "one of the main advantages of our approach , compared to previous ones , is that it can be computed in only @xmath1 operations so that it can deal with very large datasets and is more robust than the @xmath0-means .",
    "note also that by its recursive nature , another important feature is that it allows automatic update and does not need to store all the data .",
    "a key tuning parameter in our algorithm is the descent step value .",
    "we found empirically that reasonable values are given by the empirical @xmath3 loss function .",
    "we thus also consider an automatic two steps procedure in which one first runs the sequential version of the @xmath0-means in order to approximate the value of the @xmath3 loss function and then run our stochastic @xmath0-medians with an appropriate descent step .",
    "the paper is organized as follows .",
    "we first fix notations and present our algorithm . in the third section , we state the almost sure consistency of the stochastic gradient @xmath0-medians to a stationary point of the underlying objective function .",
    "the proof heavily relies on @xcite . in section 4",
    ", we compare on simulations the performance of our technique with the sequential @xmath0-means , the pam algorithm and the trimmed @xmath0-means when the data are contaminated by a small fraction of outliers .",
    "we note that applying averaging techniques ( see @xcite ) to our estimator , with a small number of different initializations points , is a very competitive approach even for moderate sample sizes with computation times that are much smaller . in section 5",
    ", we illustrate our new clustering algorithm on a large sample , of about 5000 individuals , in order to determine profiles of television audience .",
    "a major difference with pam is that our algorithm searches for a solution in all the space whereas pam , and its refinements clara and clarans , only look for a solution among the elements of the sample .",
    "consequently , approaches such as pam are not adapted to deal with temporal data presented in section 5 since the data mainly consist of 0 and 1 indicating that the television is switched on or switched off during each minute of the day .",
    "proofs are gathered in the appendix .",
    "let @xmath6 be a probability space .",
    "suppose we have a sequence of independent copies @xmath7 of a random vector @xmath8 taking values in @xmath9 the aim is to partition @xmath10 into a finite number @xmath0 of clusters @xmath11 .",
    "each cluster @xmath12 is represented by its center , which is an element of @xmath13 denoted by @xmath14 . from a population point of view , the @xmath0-means and @xmath0-medians algorithms aim at finding local minima of the function @xmath15 mapping @xmath16 to @xmath17 and defined as follows , for @xmath18 with for all @xmath19 , @xmath20 , @xmath21 where @xmath22 is a real , positive , continuous and non - decreasing function and the norm @xmath23 in @xmath13 takes account of the dimension @xmath24 of the data , for @xmath25 , @xmath26 .",
    "the particular case @xmath27 leads to the classical @xmath0-means algorithm , whereas @xmath28 leads to the k - medians .    before presenting our new recursive algorithm ,",
    "let us introduce now some notations and recall the recursive @xmath0-means algorithm developed by @xcite .",
    "let us denote by @xmath29 the indicator function , @xmath30 which is equal to one when @xmath31 is the nearest point to @xmath32 among the set of points @xmath33",
    "@xmath34 the @xmath0-means recursive algorithm proposed by @xcite starts with @xmath0 arbitrary groups , each containing only one point , @xmath35 then , at each iteration , the cluster centers are updated as follows , @xmath36 where for @xmath37 , @xmath38 and @xmath39 is just the number of elements allocated to cluster @xmath40 until iteration @xmath41 . for @xmath42 ,",
    "let @xmath43 .",
    "this also means that @xmath44 is simply the barycenter of the elements allocated to cluster @xmath40 until iteration @xmath45 @xmath46 the interesting point is that this recursive algorithm is very fast and can be seen as a robbins - monro procedure .      assuming @xmath8 has an absolutely continuous distribution , we have @xmath47 then , the @xmath0-medians approach relies on looking for minima , that may be local , of the function @xmath15 which can also be written as follows , for any @xmath48 such that @xmath49 when @xmath50 , @xmath51 .",
    "\\label{def : fnobj}\\ ] ] in order to get an explicit robbins - monro algorithm representation , it remains to exhibit the gradient of @xmath15 .",
    "let us write @xmath15 in integral form .",
    "denoting by @xmath52 the density of the random variable @xmath53 we have , @xmath54 for @xmath55 , it can be checked easily that @xmath56 and since @xmath57 the partial derivatives satisfy , @xmath58 we define , for @xmath59 @xmath60.\\ ] ]    we can now present our stochastic gradient @xmath0-medians algorithm .",
    "given a set of @xmath0 distinct initialization points in @xmath61 @xmath62 the set of @xmath0 cluster centers is updated at each iteration as follows . for @xmath63 and @xmath64 @xmath65 with @xmath66 and @xmath67,\\ ] ] @xmath68 the steps @xmath69 also called gains , are supposed to be @xmath70-measurable .",
    "we denote by @xmath71 the gradient of @xmath15 and define @xmath72 .",
    "let @xmath73 be the diagonal matrix of size @xmath74 @xmath75 each @xmath76 being repeated @xmath24 times .",
    "then , the @xmath0-medians algorithm can be written in a matrix way , @xmath77 which is a classical stochastic gradient descent .",
    "the behavior of algorithm ( [ def : algosto1 ] ) depends on the sequence of steps @xmath69 @xmath78 and the vector of initialization @xmath79 these two sets of tuning parameters play distinct roles and we mainly focus on the choice of the step values , noting that , as for the @xmath0-means , the estimation results must be compared for different sets of initialization points in order to get a better estimation of the cluster centers .",
    "assume we have a sample of @xmath2 realizations @xmath7 of @xmath8 and a set of initialization points of the algorithm , the selected estimate of the cluster centers is the one minimizing the following empirical risk , @xmath80    let us denote by @xmath81 the number of updating steps for cluster @xmath82 until iteration @xmath41 , for @xmath83 a classical form of the descent steps @xmath76 can be given by @xmath84 where @xmath85 @xmath86 and @xmath87 control the gain .",
    "adopting an asymptotic point of view , one could believe that @xmath88 should be set to @xmath89 with suitable constants @xmath90 and @xmath91 , which are unknown in practice , in order to attain the optimal parametric rates of convergence of robbins monro algorithms ( see _ e.g. _ @xcite , th .",
    "our experimental results on simulated data have shown that the convergence of algorithm ( [ def : algosto1 ] ) with descent steps defined in ( [ def : anr ] ) is then very sensitive to the values of the parameters @xmath92 and @xmath86 which have to be chosen very carefully .",
    "a simulation study performed in the particular case @xmath93 by @xcite showed that the direct approach could lead to inaccurate results and is nearly always less effective than the averaged algorithm presented below , even for well chosen descent step values . from an asymptotic point of view , it has been proved in @xcite that the averaged stochastic gradient estimator of the geometric median , corresponding to @xmath94 is asymptotically efficient under classical assumptions . intuitively , when the algorithm is not too far from the solution , averaging allows to decrease substantially the variability of the initial algorithm which oscillates around the true solution and",
    "thus improves greatly its performances .",
    "consequently , we prefer to introduce an averaging step ( see for instance @xcite or @xcite ) , which does not slow down the algorithm and provides an estimator which is much more effective .",
    "our averaged estimator of the cluster centers , which remains recursive , is defined as follows , for @xmath78 , @xmath95 , and for the value @xmath44 obtained by combining ( [ def : algosto1 ] ) and ( [ def : anr ] ) , @xmath96 with starting points @xmath97 @xmath98 then standard choices ( see _ e.g. _ @xcite and references therein ) for @xmath88 and @xmath86 are @xmath99 and @xmath100 so that one only needs to select values for @xmath101",
    "the following theorem is the main theoretical result of this paper .",
    "it states that the recursive algorithm defined in ( [ algo - vec ] ) converges almost surely to the set of stationary points of the objective function defined in ( [ def : fnobj ] ) , under the following assumptions .    *",
    "\\a ) the random vector @xmath8 is absolutely continuous with respect to lebesgue measure .",
    "+ b ) @xmath8 is bounded : @xmath102 : @xmath103 a.s .",
    "+ c ) @xmath104 : @xmath105 such that @xmath106 , @xmath107<c$ ] . * \\a ) @xmath108 , @xmath109 .",
    "+ b ) @xmath110 a.s .",
    "+ c ) @xmath111 a.s .",
    "+ d ) @xmath112 * @xmath113 * @xmath114<\\infty .$ ]    [ thm - principal ] assume that @xmath115 is absolutely continuous and that @xmath116 , for @xmath117 . then under assumptions ( h1a , c ) , ( h2a , b ) , ( h3 ) or ( h3 ) , @xmath118 and @xmath119 converge almost surely . + moreover , if the hypotheses ( h1b ) and ( h2c , d ) are also fulfilled then @xmath120 and the distance between @xmath121 and the set of stationary points of @xmath15 converge almost surely to zero .",
    "a direct consequence of theorem 1 is that if the set of stationary points of @xmath15 is finite , then the sequence @xmath122 necessarily converges almost surely towards one of these stationary points because @xmath123 converges almost surely towards zero . by cesaro means arguments",
    ", the averaged sequence @xmath124 also converges almost surely towards the same stationary point .",
    "note first that if the data do not arrive online and @xmath115 is chosen randomly among the sample units then @xmath115 is absolutely continuous and @xmath116 , for @xmath117 under ( h1a , b ) .",
    "moreover , the absolute continuity of @xmath8 is a technical assumption that is required to get decomposition ( [ def : fnobj ] ) of the @xmath3 error .",
    "proving the convergence in the presence of atoms would require to decompose this error in order to take into account the points which could have a non - null probability to be at the same distance .",
    "such a study is clearly beyond the scope of the paper .",
    "note however that in the simple case @xmath94 it has been established in @xcite that the stochastic algorithm for the functional median is convergent provided that the distribution , which can be a mixture of a continuous and a discrete distribution , does not charge the median .",
    "hypothesis ( h1c ) is a stronger version of a more classical hypothesis needed to get consistent estimators of the spatial median ( see @xcite ) .",
    "as noted in @xcite , it is closely related to small ball properties of @xmath8 and is fulfilled when @xmath125 for a constant @xmath126 that does not depend on @xmath48 and @xmath127 small enough .",
    "this implies in particular that hypothesis ( h1c ) can be satisfied only when the dimension @xmath24 of the data satisfies @xmath128 .",
    "hypotheses ( h2 ) and ( h3 ) or ( h3 ) deal with the stepsizes . considering the general form of gains @xmath76 given in ( [ def : anr ] ) , they are fulfilled when the sizes @xmath129 of all the clusters grow to infinity at the same rate and @xmath1301/2 , 1]$ ] .",
    "we first perform a simulation study to compare our recursive @xmath0-medians algorithm with the following well known clustering algorithms : recursive version of the @xmath0-means ( function ` kmeans ` in ) , trimmed @xmath0-means ( function ` tkmeans ` in the  package ` tclust ` , with a trimming coefficient @xmath88 set to default , @xmath131 ) and pam ( function ` pam ` in the  package ` cluster ` ) .",
    "our  codes are available on request .",
    "comparisons are first made according to the value of the empirical @xmath3 error ( [ def : emprisk ] ) which must be as small as possible .",
    "we note that the results of our averaged recursive procedure defined by ( [ def : algosto1 ] ) , ( [ def : anr ] ) and ( [ def : avekmed ] ) are very stable when the value of the tuning parameter @xmath92 is not too far from the minimum value of the @xmath3 error , with @xmath99 and @xmath132 this leads us to propose , in section  [ sec : sensitivity ] , an automatic clustering algorithm which consists in first approximating the @xmath3 error with a recursive @xmath0-means and then performing our recursive @xmath0-medians with the selected value of @xmath85 denoted by @xmath133 in the following .",
    "we have no mathematical justification for such an automatic choice of the tuning parameter @xmath133 but it always worked well on all our simulated experiments .",
    "this important point of our algorithm deserves further investigations that are beyond the scope of the paper .",
    "note however that this intuitive approach will certainly be ineffective when the dispersion is very different from one group to another .",
    "it would then be possible to consider refinements of the previous algorithm which would consist in considering different values of tuning parameter @xmath133 for the different clusters .",
    "we only present here a few simulation experiments which highlight both the strengths and the drawbacks of our recursive @xmath0-medians algorithm .",
    "we first consider a very simple case and draw independent realizations of variable @xmath53 @xmath135 which is a mixture , with weights @xmath136 of three bivariate random gaussian vectors @xmath137 , @xmath138 and @xmath139 with mean vectors @xmath140 @xmath141 and @xmath142 and covariance matrices @xmath143 @xmath144 and @xmath145 + point @xmath146 is an outlier and parameter @xmath127 controls the level of the contamination . a sample of @xmath147 realizations of @xmath8 is drawn in figure  [ fig1 ]",
    ".     realizations of @xmath8 .",
    "an outlier is located at position ( -14,14).,width=415,height=415 ]       realizations of @xmath8 with @xmath148 .",
    "the mean values @xmath149 @xmath150 and @xmath151 of the three natural clusters are drawn in bold lines.,width=415,height=415 ]    we also performed a simulation experiment , with a mixture of three gaussian random variables as in ( [ def : melange ] ) , but in higher dimension spaces with correlation levels that vary from one cluster to another .",
    "now , @xmath152 @xmath138 and @xmath139 are independent multivariate normal distributions in @xmath153 with means @xmath154 @xmath155 and @xmath156 for @xmath157 the covariance functions @xmath158 for @xmath159 and @xmath160 are controlled by a correlation parameter @xmath161 with @xmath162 , @xmath163 and @xmath164 note that this covariance structure corresponds to autoregressive processes of order one with autocorrelation @xmath165 as before , @xmath166 plays the role of an outlying point . a sample of @xmath167 independent realizations of @xmath53 without outliers ,",
    "is drawn in figure [ figsim201 ] for a dimension @xmath168      as noted in @xcite , comparing directly the distance of the estimates from the cluster centers @xmath149 @xmath150 and @xmath151 may not be appropriate to evaluate a clustering method .",
    "our comparison is thus first made in terms of the value of the empirical @xmath3 error ( [ def : emprisk ] ) which should be as small as possible .",
    "for all methods , we considered that there were @xmath169 clusters .     and",
    "@xmath170 mean empirical @xmath3 error ( over 50 replications ) for the pam algorithm ( dashed line ) , the @xmath0-means ( @xmath171 ) and the stochastic @xmath0-medians ( bold line ) , for @xmath1720,10].$],width=415,height=415 ]    we first study the simple case of simulation 1 .",
    "the empirical mean @xmath3 error of the pam algorithm , the @xmath0-means and the averaged @xmath0-medians , for 50 replications of samples with sizes @xmath173 and a contamination level @xmath174 is presented in figure  [ figsim1l1 ] .",
    "the number of initialization points equals 10 for both the @xmath0-means and the @xmath0-medians .",
    "when the descent parameter @xmath133 equals 0 , the initialization point is given by the estimated centers by the @xmath0-means , so that the empirical @xmath3 error corresponds in that case to the @xmath0-means error , which is sightly above 2.31 .",
    "we first note that this @xmath3 error is always larger , even if the contamination level is small , than the pam and the @xmath0-medians errors , for @xmath1720,10].$ ] secondly , it appears that for @xmath175,$ ] the @xmath0-medians @xmath3 error is nearly constant and is clearly smaller than the @xmath3 error of the pam algorithm .",
    "this means that , even if the sample size is relatively small ( @xmath173 ) , the recursive @xmath0-medians can perform well for values of @xmath133 which are of the same order of the @xmath3 error .",
    "@xmath176 and @xmath177 the mean empirical @xmath3 error ( over 50 replications ) is represented for the pam algorithm ( dashed line ) , the macqueen version of the @xmath0-means ( @xmath171 ) and the recursive @xmath0-medians estimator ( bold line ) , for @xmath1720,7].$],width=415,height=415 ]    we now consider 50 replications of samples drawn from the distribution described in simulation 2 , with @xmath178 @xmath148 and @xmath177 the number of initialization points for the @xmath0-means and the @xmath0-medians is now equal to 25 and the empirical mean @xmath3 error is presented in figure  [ figsim2l1p50 ] .",
    "we first note that the performances of the pam algorithm clearly decrease with the dimension .",
    "the @xmath0-means performs better even if there are 5% of outliers and if it is not designed to minimize an @xmath3 error criterion .",
    "this can be explained by the fact that pam , as well as clara and clarans , look for a solution among the elements of the sample .",
    "thus these approaches can hardly explore all the dimensions of the data when @xmath24 is large and @xmath2 is not large enough . on the other hand , the @xmath0-medians and the @xmath0-means look for a solution in all @xmath179 and are not restricted to the observed data and thus provide better results in terms of @xmath3 error .",
    "as before , we can also remark that the minimum error , which is around 1.36 , is attained for @xmath133 in the interval @xmath180.$ ]     @xmath181 @xmath182 and @xmath8 multiplied by a factor 10 . the mean @xmath3 loss function ( over 50 replications )",
    "is represented for the pam algorithm ( dashed line ) , the macqueen version of the @xmath0-means ( @xmath171 ) and our recursive @xmath0-medians estimator ( bold line ) , for @xmath1720,40].$],width=453,height=453 ]    we finally present results from simulation 2 in which we consider samples with size @xmath183 of variable @xmath184 with @xmath185 the contamination level is @xmath174 and 50 initialization points were considered for the @xmath0-means and @xmath0-medians algorithms . since @xmath8 has been multiplied by a factor 10 , the minimum of the @xmath3 error is now around 13.6 .",
    "we remark , as before , that because of the dimension of the data , @xmath181 pam is outperformed by the @xmath0-means and the @xmath0-medians even in the presence of a small fraction of outliers ( @xmath174 ) .",
    "the minimum of the @xmath3 error for the @xmath0-medians estimator is again very stable for @xmath186 $ ] with smaller values than the @xmath3 error of the @xmath0-means clustering .    as a first conclusion",
    ", it appears that for large dimensions the @xmath0-medians can give results which are much better than pam in terms of empirical @xmath3 error .",
    "we can also note that the averaged recursive @xmath0-medians is not very sensitive to the choice of parameter @xmath133 provided its value is not too far from the minimum value of the @xmath3 error .",
    "thus we only consider , in the following subsection , the data - driven version of our averaged algorithm described in section  [ ssec : stepsizes ] in which the value of @xmath133 is chosen automatically , its value being the empirical @xmath3 error of the recursive @xmath0-means .",
    "this data - driven @xmath0-medians algorithm can be summarized as follows    _ _    1 .",
    "run the @xmath0-means algorithm and get the estimated centers .",
    "2 .   set @xmath133 as the value of the @xmath3 error of the @xmath0-means , evaluated with formula  ( [ def : emprisk ] ) .",
    "3 .   run the averaged @xmath0-medians defined by ( [ def : algosto1 ] ) , ( [ def : anr ] ) and ( [ def : avekmed ] ) , with @xmath133 computed in step 2 and @xmath132      we now make comparisons in terms of classification error measured by the classification error rate ( cer ) introduced by @xcite and defined as follows .",
    "for a given partition @xmath187 of the sample , let @xmath188 be an indicator for whether partition @xmath187 places observations @xmath19 and @xmath189 in the same group .",
    "consider a partition @xmath190 with the true class labels , the cer for partition @xmath187 is defined as @xmath191 the cer equals 0 if the partitions @xmath187 and @xmath190 agree perfectly whereas a high value indicates disagreement . since pam , the @xmath0-means and our algorithm are not designed to detect outliers automatically , we only evaluate the cer on the non - outlying pairs of elements @xmath19 and @xmath189 of the sample .     and",
    "@xmath192 on the left , the @xmath3 empirical error .",
    "on the right , cer for the @xmath0-means , pam , the data - driven recursive @xmath0-medians algorithm ( kmed ) and the trimmed @xmath0-means ( tkm).,width=529,height=377 ]    we present in figure  [ figsim1l1n500 ] , results for 500 replications of simulation 1 , with a sample size @xmath193 and no outliers ( @xmath194 ) .",
    "we note , in this small dimension context with no contamination , that the @xmath3 errors are comparable . nevertheless , in terms of cer , the pam , the @xmath0-means and the data - driven @xmath0-medians algorithms have approximately the same performances . for the trimmed @xmath0-means ,",
    "the results are not as effective , since this algorithm automatically classifies 5% of the elements of the sample as outliers .     and @xmath192 on the left , the @xmath3 empirical error . on the right , cer for the @xmath0-means , pam , the data - driven recursive @xmath0-medians algorithm ( kmed ) and the trimmed @xmath0-means ( tkm).,width=529,height=377 ]    we then consider the same experiment as before , the only difference being that there is now a fraction of @xmath174 of outliers .",
    "the results are presented in figure  [ figsim1l1n500d05 ] .",
    "the @xmath0-means algorithm is clearly affected by the presence of outliers and both its @xmath3 error and its cer are now much larger than for the other algorithms .",
    "pam and the recursive @xmath0-medians have similar performances , even if pam is slightly better .",
    "the trimmed @xmath0-means now detects the outliers and also has good performances .",
    "if the contamination level increases to @xmath195 , as presented in figure  [ figsim1l1n1000d1 ] , then pam and the trimmed @xmath0-means ( with a trimming coefficient @xmath131 ) are strongly affected in terms of cer and do not recover the true groups .",
    "the @xmath0-medians algorithm is less affected by this larger level of contamination .",
    "its median cer is nearly unchanged , meaning that for at least 50 % of the samples , it gives a correct partition .     and",
    "@xmath196 on the left , the @xmath3 empirical error . on the right ,",
    "cer for the @xmath0-means , pam , the data - driven recursive @xmath0-medians algorithm ( kmed ) and the trimmed @xmath0-means ( tkm).,width=529,height=377 ]    we now consider simulation 2 , with a dimension @xmath148 and a fraction @xmath174 of outliers .",
    "the @xmath3 empirical errors and the cer , for sample sizes @xmath178 are given in figure  [ figsim2l1d05n500 ] .",
    "it clearly appears that pam has the largest @xmath3 errors and the trimmed @xmath0-means and the data - driven @xmath0-medians the smallest ones .",
    "intermediate @xmath3 errors are obtained for the @xmath0-means . in terms of cer ,",
    "the partitions obtained by the @xmath0-means and pam are not effective and do not recover well the true partition in the majority of the samples .",
    "the trimmed @xmath0-means and our algorithm always perform well and have similar low values in terms of cer .",
    "@xmath193 and @xmath168 on the left , the @xmath3 empirical error . on the right ,",
    "cer for the @xmath0-means , pam , the data - driven recursive @xmath0-medians algorithm ( kmed ) and the trimmed @xmath0-means ( tkm).,width=529,height=377 ]      the  codes of all the considered estimation procedures call ` c ` routines and provide the same output .",
    "mean computation times , for 100 runs , various sample sizes and numbers of clusters are reported in table [ table : temps ] .",
    "they are based on one initialization point . from a computational point of view , the recursive @xmath0-means based on the macqueen algorithm as well as the averaged stochastic @xmath0-medians algorithm are always faster than the other algorithms and the gain increases as the sample size gets larger .",
    "for example , when @xmath197 and @xmath198 the stochastic @xmath0-medians is approximately 30 times faster than the trimmed @xmath0-means and 350 times faster than the pam algorithm .",
    "the data - driven recursive @xmath0-medians has a computation time of approximately the sum of the computation time of the recursive @xmath0-means and the stochastic @xmath0-medians .",
    "this also means that when the allocated computation time is fixed and the dataset is very large , the data - driven recursive @xmath0-medians can deal with sample sizes that are 15 times larger than the trimmed @xmath0-means and 175 times larger than the pam algorithm .",
    ".comparison of the mean computation time in seconds , for 100 runs , of the different estimators for various sample sizes and number of clusters @xmath199 the computation time are given for one initialization point . [ cols=\"^,^,^,^,^,^,^,^,^,^ \" , ]     [ table : temps ]    when the sample size or the dimension increases , the computation time is even more critical .",
    "for instance , when @xmath200 and @xmath201 as in the example of section  [ sec : mediametrie ] , our data - driven recursive @xmath0-medians estimation procedure is at least 500 times faster than the trimmed @xmath0-means .",
    "it takes 5.5 seconds for the sequential @xmath0-means to converge and then about 3.0 seconds for the averaged @xmath0-medians , whereas it takes more than 5700 seconds for the trimmed @xmath0-means .",
    "the mdiamtrie company provides every day the official estimations of television audience in france .",
    "television consumption can be measured both in terms of how long people watch each channel and when they watch television .",
    "mdiamtrie has a panel of about 9000 individuals equipped at home with sensors that are able to record and send the audience of the different television channels . among this panel , a sample of around 7000 people is drawn every day and the television consumption of the people belonging to this sample is sent to mdiamtrie at night , between 3 and 5 am .",
    "online clustering techniques are then interesting to determine automatically , the number of clusters being fixed in advance , the main profiles of viewers and then relate these profiles to socio - economic variables . in these samples , mdiamtrie has noted the presence of some atypical behaviors so that robust techniques may be helpful .",
    "we are interested in building profiles of the evolution along time of the total audience for people who watched television at least one minute on the 6th september 2010 .",
    "about 1600 people , among the initial sample whose size is around 7000 , did not watch television at all this day , so that we finally get a sample of @xmath201 individual audiences , aggregated along all television channels and measured every minute over a period of 24 hours .",
    "an observation @xmath202 is a vector belonging to @xmath203^d,$ ] with @xmath204 each component giving the fraction of time spent watching television during the corresponding minute of the day .",
    "a sample of 5 individual temporal profiles is drawn in figure [ fig3 ] . clustering techniques based on medoids and representative elements of the sample ( _ e.g. _",
    "pam , clara and clarans ) are not really interesting in this context since they will return centers of the form of the profiles drawn in figure [ fig3 ] which are , in great majority , constituted of 0 and 1 and are consequently difficult to interpret .",
    "furthermore , the dimension being very large , @xmath204 these algorithms which do not consider all the dimensions of the data , as seen in the simulation study , will lead to a minimum value of the empirical @xmath3 error ( [ def : emprisk ] ) that will be substantially larger than for the @xmath0-means and our recursive @xmath0-medians .",
    "indeed , at the optimum , the value of the @xmath3 empirical error is 0.2455 for the @xmath0-medians , 0.2471 for the @xmath0-means and 0.2692 for pam .",
    "the cluster centers , estimated with our averaged algorithm for @xmath205 with a parameter value selected automatically , @xmath206 and 100 different starting points , are drawn in figure [ fig4 ] .",
    "they have been ordered in decreasing order according to the sizes of the partitions and labelled cl.1 to cl.5 .",
    "cluster 1 ( cl.1 ) is thus the largest cluster and it contains about 35% of the elements of the sample .",
    "it corresponds to individuals that do not watch television much during the day , with a cumulative audience of about 42 minutes . at the opposite ,",
    "cluster 5 , which represents about 12% of the sample , is associated to high audience rates during nearly all the day with a cumulative audience of about 592 minutes .",
    "clusters 2 , 3 and 4 correspond to intermediate consumption levels and can be distinguished according to whether the audience occurs during the evening or at night .",
    "for example cluster 4 , which represents 16% of the sample , is related to people watching television late at night , with a cumulative audience of about 310 minutes .",
    "the proof of theorem  [ thm - principal ] relies on the following light version of the main theorem in @xcite , section 2.1 .",
    "the proof of theorem  [ thm - principal ] consists in checking that all the conditions of the following theorem are satisfied .",
    "* @xmath15 is a non negative function ; * there exists a constant @xmath207 such that , for all @xmath208 , @xmath209 * the sequence @xmath210 is almost surely bounded and @xmath211 is continuous almost everywhere on the compact set containing @xmath210 ; * there exists four sequences of random variables @xmath212 , @xmath213,@xmath214 and @xmath215 in @xmath216 adapted to the sequence @xmath217 such that a.s .",
    ": * @xmath218 \\right\\|}^2\\leq b_n g(x_n)+c_n$ ] and @xmath219 ; * @xmath220\\leq d_n g(x_n)+e_n$ ] and @xmath221 ; * @xmath222 a.s . , @xmath111 a.s . and",
    "@xmath223 then the distance of @xmath121 to the set of stationary points of @xmath15 converges almost surely to zero .",
    "* step 1 : proof of @xmath224 * + let @xmath225 and @xmath226 . since @xmath121 is absolutely continuous with respect to lebesgue measure , @xmath227 a.s .",
    "and one gets @xmath228={\\mbox{$\\mathbb{e}$}}\\left[\\sum_{r=1}^ki_r(z;a)\\min_j { \\left\\| z - b^j \\right\\|}\\right],\\ ] ] and it comes @xmath229,\\ ] ] which yields @xmath230.\\ ] ] the application @xmath231 is a continuous function whose gradient @xmath232 is also continuous for @xmath233",
    ". then almost surely for @xmath234 , there exists @xmath235 , @xmath236 , such that @xmath237 consequently for all @xmath234 , @xmath238,\\ ] ] so that @xmath239\\\\ & + & \\sum_{r=1}^k{\\mbox{$\\mathbb{e}$}}\\left[i_r(z;a)\\langle b^r - a^r,\\nabla_r{\\left\\| z - a^r \\right\\|}\\rangle\\right ] { \\stackrel{\\mbox{\\upshape\\tiny def}}{=}}(1)+(2 ) \\end{aligned}\\ ] ] on the one hand @xmath240 and on the other hand @xmath241,\\ ] ] hence since @xmath242 one gets , with ( h1c ) @xmath243 \\leq 2c\\sum_{r=1}^k{\\left\\| b^r - a^r \\right\\|}^2=2c { \\left\\| b - a \\right\\|}^2.\\ ] ] consequently , we have @xmath244    * step 2 : proof of the assertion : @xmath108 , for all @xmath245 , @xmath246 * + let us prove by induction on @xmath2 that for all @xmath247 , for all @xmath117 , @xmath248 .",
    "this inequality is trivial for the case @xmath42 : @xmath116 .",
    "let @xmath247 such that @xmath248 , @xmath249 .",
    "let @xmath250 .",
    "first we assume that @xmath251 .",
    "then it comes @xmath252 now in the case when @xmath253 , one gets @xmath254 and then @xmath255 since for @xmath256 , @xmath257 , it remains to deal with the unique index @xmath40 such that @xmath258 . in that case ,",
    "@xmath259 by ( h1b ) and from the inequalities @xmath260 and @xmath261 , we have , @xmath262 which leads to @xmath263 and concludes the proof by induction .",
    "+        * step 5 : proof of @xmath271 * + @xmath272&=&\\sum_{r=1}^k{\\mbox{$\\mathbb{e}$}}\\left[\\left(a_n^r\\right)^2{\\left\\| v_n^r \\right\\|}^2|{\\mathcal{f}}_n\\right]\\\\ & \\leq & \\sum_{r=1}^k\\left(a_n^r\\right)^2{\\mbox{$\\mathbb{e}$}}\\left[i_r(z_n;x_n)\\frac{{\\left\\| x_n^r - z_n \\right\\|}^2}{{\\left\\| x_n^r",
    "- z_n \\right\\|}^2}\\big|{\\mathcal{f}}_n\\right]\\\\ & \\leq & \\sum_{r=1}^k(a_n^r)^2.\\end{aligned}\\ ] ] hence assuming ( h3 ) , one gets @xmath273\\right ] < \\infty.\\ ] ] in the case when ( h3 ) holds instead of ( h3 ) , one has @xmath273\\right]\\leq \\sum_{n=1}^{\\infty}\\sum_{r=1}^k{\\mbox{$\\mathbb{e}$}}\\left[(a_n^r)^2i_r(z_n;x_n)\\right]<\\infty.\\ ] ] consequently , @xmath274<\\infty \\quad \\mbox{a.s},\\ ] ] which concludes the proof .",
    "* acknowledgements . *",
    "we thank the anonymous referees for their valuable suggestions .",
    "we also thank the mdiamtrie company for allowing us to illustrate our sequential clustering technique with their data .",
    "cardot , h. , cnac , p. , chaouch , m. , 2010 .",
    "stochastic approximation to the multivariate and the functional median . in : lechevallier , y. , saporta , g. ( eds . ) , compstat 2010 .",
    "physica verlag , springer . , pp .",
    "421428 .",
    "macqueen , j. , 1967 .",
    "some methods for classification and analysis of multivariate observations . in : proc .",
    "fifth berkeley sympos .",
    "math . statist . and probability ( berkeley , calif . , 1965/66 ) .",
    "california press , berkeley , calif . , pp .",
    "i : statistics , pp ."
  ],
  "abstract_text": [
    "<S> clustering with fast algorithms large samples of high dimensional data is an important challenge in computational statistics . </S>",
    "<S> borrowing ideas from @xcite who introduced a sequential version of the @xmath0-means algorithm , a new class of recursive stochastic gradient algorithms designed for the @xmath0-medians loss criterion is proposed . by their recursive nature , these algorithms are very fast and are well adapted to deal with large samples of data that are allowed to arrive sequentially . </S>",
    "<S> it is proved that the stochastic gradient algorithm converges almost surely to the set of stationary points of the underlying loss criterion . </S>",
    "<S> a particular attention is paid to the averaged versions , which are known to have better performances , and a data - driven procedure that allows automatic selection of the value of the descent step is proposed . </S>",
    "<S> the performance of the averaged sequential estimator is compared on a simulation study , both in terms of computation speed and accuracy of the estimations , with more classical partitioning techniques such as @xmath0-means , trimmed @xmath0-means and pam ( partitioning around medoids ) . </S>",
    "<S> finally , this new online clustering technique is illustrated on determining television audience profiles with a sample of more than 5000 individual television audiences measured every minute over a period of 24 hours .    </S>",
    "<S> _ keywords _ : averaging , high dimensional data , @xmath0-medoids , online clustering , partitioning around medoids , recursive estimators , robbins monro , stochastic approximation , stochastic gradient . </S>"
  ]
}