{
  "article_text": [
    "_ sparse modeling _ calls for constructing a succinct representation of some data as a combination of a few typical patterns ( _ atoms _ ) learned from the data itself .",
    "significant contributions to the theory and practice of learning such collections of atoms ( usually called _ dictionaries _ or _ codebooks _ ) , e.g. , @xcite , and of representing the actual data in terms of them , e.g. , @xcite , have been developed in recent years , leading to state - of - the - art results in many signal and image processing tasks @xcite .",
    "we refer the reader for example to @xcite for a recent review on the subject",
    ".    a critical component of sparse modeling is the actual sparsity of the representation , which is controlled by a regularization term ( _ regularizer _ for short ) and its associated parameters .",
    "the choice of the functional form of the regularizer and its parameters is a challenging task .",
    "several solutions to this problem have been proposed in the literature , ranging from the automatic tuning of the parameters  @xcite to bayesian models , where these parameters are themselves considered as random variables @xcite . in this work we adopt the interpretation of sparse coding as a codelength minimization problem .",
    "this is a natural and objective method for assessing the quality of a statistical model for describing given data , and which is based on the minimum description length ( mdl ) principle @xcite . in this framework ,",
    "the regularization term in the sparse coding formulation is interpreted as the cost in bits of describing the sparse linear coefficients used to reconstruct the data .",
    "several works on image coding using this approach were developed in the 1990 s under the name of `` complexity - based '' or `` compression - based '' coding , following the popularization of mdl as a powerful statistical modeling tool @xcite .",
    "the focus on these early works was in denoising using wavelet basis , using either generic asymptotic results from mdl or fixed probability models , in order to compute the description length of the coefficients . a later ,",
    "major breakthrough in mdl theory was the adoption of _ universal coding _ tools to compute optimal codelengths . in this work ,",
    "we improve and extend on previous results in this line of work by designing regularization terms based on such universal codes for image coefficients , meaning that the codelengths obtained when encoding the coefficients of any ( natural ) image with such codes will be close to the shortest codelengths that can be obtained with any model fitted specifically for that particular instance of coefficients .",
    "the resulting framework not only formalizes sparse coding from the mdl and universal coding perspectives but also leads to a family of _ universal regularizers _ which we show to consistently improve results in image processing tasks such as denoising and classification .",
    "these models also enjoy several desirable theoretical and practical properties such as statistical consistency ( in certain cases ) , improved robustness to outliers in the data , and improved sparse signal recovery ( e.g. , decoding of sparse signals from a compressive sensing point of view @xcite ) when compared with the traditional @xmath2 and @xmath3-based techniques in practice .",
    "these models also yield to the use of a simple and efficient optimization technique for solving the corresponding sparse coding problems as a series of weighted @xmath3 subproblems , which in turn , can be solved with off - the - shelf algorithms such as lars @xcite or ist @xcite .",
    "details are given in the sequel .",
    "finally , we apply our universal regularizers not only for coding using fixed dictionaries , but also for learning the dictionaries themselves , leading to further improvements in all the aforementioned tasks .",
    "the remainder of this paper is organized as follows : in section  [ sec : sparse - modeling ] we introduce the standard framework of sparse modeling .",
    "section  [ sec : universal - sparse - coding ] is dedicated to the derivation of our proposed universal sparse modeling framework , while section  [ sec : implementation ] deals with its implementation .",
    "section  [ sec : results ] presents experimental results showing the practical benefits of the proposed framework in image denoising , zooming and classification tasks .",
    "concluding remarks are given in section  [ sec : conclusions ] .",
    "let @xmath4 be a set of @xmath5 column data samples @xmath6 , @xmath7 a dictionary of @xmath8 column atoms @xmath9 , and @xmath10 , a set of reconstruction coefficients such that @xmath11 .",
    "we use @xmath12 to denote the @xmath13-th row of @xmath14 , the coefficients associated to the @xmath13-th atom in @xmath15 . for each @xmath16",
    "we define the _ active set _ of @xmath17 as @xmath18 , and @xmath19 as its cardinality .",
    "the goal of sparse modeling is to design a dictionary @xmath15 such that for all or most data samples @xmath20 , there exists a coefficients vector @xmath17 such that @xmath21 and @xmath22 is small ( usually below some threshold @xmath23 ) .",
    "formally , we would like to solve the following problem@xmath24 where @xmath25 is a regularization term which induces sparsity in the columns of the solution @xmath14 .",
    "usually the constraint @xmath26 , is added , since otherwise we can always decrease the cost function arbitrarily by multiplying @xmath15 by a large constant and dividing @xmath14 by the same constant .",
    "when @xmath15 is fixed , the problem of finding a sparse @xmath17 for each sample @xmath20 is called sparse coding , @xmath27    among possible choices of @xmath25 are the @xmath2 pseudo - norm , @xmath28 , and the @xmath3 norm .",
    "the former tries to solve directly for the sparsest @xmath17 , but since it is non - convex , it is commonly replaced by the @xmath3 norm , which is its closest convex approximation . furthermore , under certain conditions on ( fixed ) and the sparsity of @xmath17 , the solutions to the @xmath2 and @xmath3-based sparse coding problems coincide ( see for example @xcite ) .",
    "the problem ( [ eq : sparse - modeling ] ) is also usually formulated in lagrangian form , @xmath29 along with its respective sparse coding problem when @xmath15 is fixed , @xmath30 even when the regularizer @xmath25 is convex , the sparse modeling problem , in any of its forms , is jointly non - convex in @xmath31 .",
    "therefore , the standard approach to find an approximate solution is to use alternate minimization : starting with an initial dictionary @xmath32 , we minimize ( [ eq : sparse - modeling - lagrangian ] ) alternatively in @xmath14 via ( [ eq : sparse - coding ] ) or ( [ eq : sparse - coding - lagrangian ] ) ( sparse coding step ) , and @xmath15 ( dictionary update step ) .",
    "the sparse coding step can be solved efficiently when @xmath33 using for example ist @xcite or lars  @xcite , or with omp  @xcite when @xmath28 .",
    "the dictionary update step can be done using for example mod  @xcite or k - svd  @xcite .",
    "we now turn our attention to the sparse coding problem : given a fixed dictionary @xmath15 , for each sample vector @xmath20 , compute the sparsest vector of coefficients @xmath17 that yields a good approximation of @xmath20 .",
    "the sparse coding problem admits several interpretations .",
    "what follows is a summary of these interpretations and the insights that they provide into the properties of the sparse models that are relevant to our derivation .      using the @xmath2 norm as @xmath25 in ( [ eq : sparse - coding - lagrangian ] )",
    "is known in the statistics community as the akaike s information criterion ( aic ) when @xmath34 , or the bayes information criterion ( bic ) when @xmath35 , two popular forms of model selection ( see ( * ? ? ?",
    "* chapter 7 ) ) . in this context , the @xmath3 regularizer was introduced in @xcite , again as a convex approximation of the above model selection methods , and is commonly known ( either in its constrained or lagrangian forms ) as the _",
    "lasso_. note however that , in the regression interpretation of ( [ eq : sparse - coding - lagrangian ] ) , the role of @xmath15 and @xmath36 is very different .",
    "another interpretation of ( [ eq : sparse - coding - lagrangian ] ) is that of a maximum a posteriori ( map ) estimation of @xmath17 in the logarithmic scale , that is @xmath37 where the observed samples @xmath20 are assumed to be contaminated with additive , zero mean , iid gaussian noise with variance @xmath38 , @xmath39 and a _ prior probability model _ on @xmath40 with the form @xmath41 is considered .",
    "the energy term in equation ( [ eq : sparse - coding - lagrangian ] ) follows by plugging the previous two probability models into ( [ eq : map - gen ] ) and factorizing @xmath42 into @xmath43 .",
    "according to ( [ eq : map - gen ] ) , the @xmath3 regularizer corresponds to an iid laplacian prior with mean @xmath44 and inverse - scale parameter @xmath45 , @xmath46 , which has a special meaning in signal processing tasks such as image or audio compression .",
    "this is due to the widely accepted fact that representation coefficients derived from predictive coding of continuous - valued signals , and , more generally , responses from zero - mean filters , are well modeled using laplacian distributions .",
    "for example , for the special case of dct coefficients of image patches , an analytical study of this phenomenon is provided in @xcite , along with further references on the subject .",
    "sparse coding , in all its forms , has yet another important interpretation .",
    "suppose that we have a fixed dictionary @xmath15 and that we want to use it to compress an image , either losslessly by encoding the reconstruction coefficients @xmath14 and the residual @xmath47 , or in a lossy manner , by obtaining a good approximation @xmath48 and encoding only @xmath14 .",
    "consider for example the latter case .",
    "most modern compression schemes consist of two parts : a probability assignment stage , where the data , in this case @xmath14 , is assigned a probability @xmath49 , and an encoding stage , where a code @xmath50 of length @xmath51 bits is assigned to the data given its probability , so that @xmath51 is as short as possible .",
    "the techniques known as arithmetic and huffman coding provide the best possible solution for the encoding step , which is to approximate the shannon ideal codelength @xmath52 ( * ? ? ?",
    "* chapter  5 ) .",
    "therefore , modern compression theory deals with finding the coefficients @xmath14 that maximize @xmath49 , or , equivalently , that minimize @xmath53 .",
    "now , to encode @xmath36 lossily , we obtain coefficients @xmath14 such that each data sample @xmath20 is approximated up to a certain @xmath54 distortion @xmath55 , @xmath56 .",
    "therefore , given a model @xmath57 for a vector of reconstruction coefficients , and assuming that we encode each sample independently , the optimum vector of coefficients @xmath17 for each sample @xmath20 will be the solution to the optimization problem @xmath58 which , for the choice @xmath59 coincides with the error constrained sparse coding problem ( [ eq : sparse - coding ] ) .",
    "suppose now that we want lossless compression .",
    "in this case we also need to encode the reconstruction residual @xmath60 .",
    "since @xmath61 , the combined codelength will be @xmath62 therefore , obtaining the best coefficients @xmath17 amounts to solving @xmath63 , which is precisely the map formulation of ( [ eq : map - gen ] ) , which in turn , for proper choices of @xmath64 and @xmath57 , leads to the lagrangian form of sparse coding ( [ eq : sparse - coding - lagrangian ] ) . , characterized by continuous probability density functions , @xmath65 , @xmath66 .",
    "if the reconstruction coefficients are considered real numbers , under any of these distributions , any instance of @xmath67 will have measure @xmath44 , that is , @xmath68 . in order to use such distributions as our models for the data",
    ", we assume that the coefficients in @xmath14 are quantized to a precision @xmath69 , small enough for the density function @xmath70 to be approximately constant in any interval @xmath71,\\,\\coef \\in { \\ensuremath{\\mathbb{r}}}$ ] , so that we can approximate @xmath72 . under these assumptions , @xmath73 , and",
    "the effect of @xmath69 on the codelength produced by any model is the same .",
    "therefore , we will omit @xmath69 in the sequel , and treat density functions and probability distributions interchangeably as @xmath74 .",
    "of course , in real compression applications , @xmath69 needs to be tuned . ]    as one can see , the codelength interpretation of sparse coding is able to unify and interpret both the constrained and unconstrained formulations into one consistent framework .",
    "furthermore , this framework offers a natural and objective measure for comparing the quality of different models @xmath64 and @xmath57 in terms of the codelengths obtained .      as mentioned in the introduction , the codelength interpretation of signal coding",
    "was already studied in the context of orthogonal wavelet - based denoising .",
    "an early example of this line of work considers a regularization term which uses the shannon entropy function @xmath75 to give a measure of the sparsity of the solution @xcite .",
    "however , the entropy function is not used as measure of the ideal codelength for describing the coefficients , but as a measure of the sparsity ( actually , group sparsity ) of the solution .",
    "the mdl principle was applied to the signal estimation problem in @xcite . in this case",
    ", the codelength term includes the description of both the location and the magnitude of the nonzero coefficients .",
    "although a pioneering effort , the model assumed in @xcite for the coefficient magnitude is a uniform distribution on @xmath76 $ ] , which does not exploit a priori knowledge of image coefficient statistics , and the description of the support is slightly wasteful .",
    "furthermore , the codelength expression used is an asymptotic result , actually equivalent to bic ( see section  [ sec : sparse - coding : interpretation : statistics ] ) which can be misleading when working with small sample sizes , such as when encoding small image patches , as in current state of the art image processing applications .",
    "the uniform distribution was later replaced by the _",
    "universal code for integers _ @xcite in @xcite .",
    "however , as in @xcite , the model is so general that it does not perform well for the specific case of coefficients arising from image decompositions , leading to poor results .",
    "in contrast , our models are derived following a careful analysis of image coefficient statistics .",
    "finally , probability models suitable to image coefficient statistics of the form @xmath77 ( known as generalized gaussians ) were applied to the mdl - based signal coding and estimation framework in @xcite .",
    "the justification for such models is based on the empirical observation that sparse coefficients statistics exhibit `` heavy tails '' ( see next section ) . however , the choice is ad hoc and no optimality criterion is available to compare it with other possibilities .",
    "moreover , there is no closed form solution for performing parameter estimation on such family of models , requiring numerical optimization techniques . in section  [ sec : universal - sparse - coding ] , we derive a number of probability models for which parameter estimation can be computed efficiently in closed form , and which are guaranteed to optimally describe image coefficients .",
    "as explained in the previous subsection , the use of the @xmath3 regularizer implies that all the coefficients in @xmath14 share the same laplacian parameter @xmath45 .",
    "however , as noted in @xcite and references therein , the empirical variance of coefficients associated to different atoms , that is , of the different rows @xmath12 of @xmath14 , varies greatly with @xmath78 .",
    "this is clearly seen in figures  [ fig : many - laplacians](a - c ) , which show the empirical distribution of dct coefficients of @xmath79 patches . as the variance of a laplacian is @xmath80 ,",
    "different variances indicate different underlying @xmath45 .",
    "the histogram of the set @xmath81 of estimated laplacian parameters for each row @xmath13 , figure  [ fig : many - laplacians](d ) , shows that this is indeed the case , with significant occurrences of values of @xmath82 in a range of @xmath83 to @xmath84 .",
    "the straightforward modification suggested by this phenomenon is to use a model where each row of @xmath14 has its own weight associated to it , leading to a weighted @xmath3 regularizer .",
    "however , from a modeling perspective , this results in @xmath8 parameters to be adjusted instead of just one , which often results in poor generalization properties .",
    "for example , in the cases studied in section  [ sec : results ] , even with thousands of images for learning these parameters , the results of applying the learned model to new images were always significantly worse ( over 1db in estimation problems ) when compared to those obtained using simpler models such as an unweighted @xmath3 .",
    "regularizers , using other types of weighting strategies , are known to improve over @xmath3-based ones for certain applications ( see e.g. @xcite ) .",
    "] one reason for this failure may be that real images , as well as other types of signals such as audio samples , are far from stationary . in this case , even if each atom @xmath13 is associated to its own @xmath85 ( @xmath86 ) , the optimal value of @xmath85 can have significant local variations at different positions or times .",
    "this effect is shown in figure  [ fig : many - laplacians](e ) , where , for each @xmath13 , each @xmath85 was re - estimated several times using samples from different regions of an image , and the histogram of the different estimated values of @xmath87 was computed . here",
    "again we used the dct basis as the dictionary @xmath15 .",
    "the need for a flexible model which at the same time has a small number of parameters leads naturally to bayesian formulations where the different possible @xmath86 are `` marginalized out '' by imposing an hyper - prior distribution on @xmath88 , sampling @xmath88 using its posterior distribution , and then averaging the estimates obtained with the sampled sparse - coding problems .",
    "examples of this recent line of work , and the closely related bayesian compressive sensing , are developed for example in @xcite . despite of its promising results , the bayesian approach is often criticized due to the potentially expensive sampling process ( something which can be reduced for certain choices of the priors involved @xcite ) , arbitrariness in the choice of the priors , and lack of proper theoretical justification for the proposed models @xcite .    in this work",
    "we pursue the same goal of deriving a more flexible and accurate sparse model than the traditional ones , while avoiding an increase in the number of parameters and the burden of possibly solving several sampled instances of the sparse coding problem . for this",
    ", we deploy tools from the very successful information - theoretic field of universal coding , which is an extension of the compression scenario summarized above in section  [ sec : sparse - coding : interpretation ] , when the probability model for the data to be described is itself unknown and has to be described as well .",
    "following the discussion in the preceding section , we now have several possible scenarios to deal with .",
    "first , we may still want to consider a single value of @xmath45 to work well for all the coefficients in @xmath14 , and try to design a sparse coding scheme that does not depend on prior knowledge on the value of @xmath45 .",
    "secondly , we can consider an independent ( but not identically distributed ) laplacian model where the underlying parameter @xmath45 can be different for each atom @xmath89 , @xmath90 . in the most extreme scenario",
    ", we can consider each single coefficient @xmath91 in @xmath14 to have its own unknown underlying @xmath92 and yet , we would like to encode each of these coefficients ( almost ) as if we knew its hidden parameter .",
    "the first two scenarios are the ones which fit the original purpose of universal coding theory @xcite , which is the design of optimal codes for data whose probability models are unknown , and the models themselves are to be encoded as well in the compressed representation .",
    "now we develop the basic ideas and techniques of universal coding applied to the first scenario , where the problem is to describe @xmath14 as an iid laplacian with unknown parameter @xmath45 . assuming a known parametric form for the prior , with unknown parameter @xmath45 , leads to the concept of a _ model class_. in our case",
    ", we consider the class @xmath93 of all iid laplacian models over @xmath94 , where @xmath95 and @xmath96 .",
    "the goal of universal coding is to find a probability model @xmath97 which can fit @xmath14 as well as the model in @xmath98 that best fits @xmath14 after having observed it .",
    "a model @xmath97 with this property is called _ universal _ ( with respect to the model @xmath98 ) .    for simplicity ,",
    "in the following discussion we consider the coefficient matrix @xmath14 to be arranged as a single long column vector of length @xmath99 , @xmath100 .",
    "we also use the letter @xmath101 without sub - index to denote the value of a random variable representing coefficient values .",
    "first we need to define a criterion for comparing the fitting quality of different models . in universal coding theory",
    "this is done in terms of the codelengths @xmath102 required by each model to describe @xmath40 .",
    "if the model consists of a single probability distribution @xmath74 , we know from section  [ sec : sparse - coding : interpretation : codelength ] that the optimum codelength corresponds to @xmath103 .",
    "moreover , this relationship defines a one - to - one correspondence between distributions and codelengths , so that for any coding scheme @xmath104 , @xmath105",
    ". now suppose that we are restricted to a class of models @xmath106 , and that we need choose the model @xmath107 that assigns the shortest codelength to a particular instance of @xmath40 .",
    "we then have that @xmath108 is the model in @xmath106 that assigns the maximum probability to @xmath40 . for a class @xmath106 parametrized by @xmath45 ,",
    "this corresponds to @xmath109 , where @xmath110 is the maximum likelihood estimator ( mle ) of the model class parameter @xmath45 given @xmath40 ( we will usually omit the argument and just write @xmath82 ) .",
    "unfortunately , we also need to include the value of @xmath82 in the description of @xmath40 for the decoder to be able to reconstruct it from the code @xmath111 .",
    "thus , we have that any model @xmath112 inducing valid codelengths @xmath104 will have @xmath113 .",
    "the overhead of @xmath104 with respect to @xmath114 is known as the _ codelength regret_,@xmath115a model @xmath112 ( or , more precisely , a sequence of models , one for each data length @xmath116 ) is called _ universal _",
    "if @xmath117 grows sublinearly in @xmath116 for all possible realizations of @xmath40 , that is @xmath118 so that the codelength regret with respect to the mle becomes asymptotically negligible .",
    "there are a number of ways to construct universal probability models .",
    "the simplest one is the so called _ two - part code _ , where the data is described in two parts .",
    "the first part describes the optimal parameter @xmath110 and the second part describes the data according to the model with the value of the estimated parameter @xmath82 , @xmath119 . for uncountable parameter spaces",
    "@xmath120 , such as a compact subset of @xmath121 , the value of @xmath82 has to be quantized in order to be described with a finite number of bits @xmath122 .",
    "we call the quantized parameter @xmath123 .",
    "the regret for this model is thus @xmath124 the key for this model to be universal is in the choice of the quantization step for the parameter @xmath82 , so that both its description @xmath125 , and the difference @xmath126 , grow sublinearly .",
    "this can be achieved by letting the quantization step shrink as @xmath127 @xcite , thus requiring @xmath128 bits to describe each dimension of @xmath123 .",
    "this gives us a total regret for two - part codes which grows as @xmath129 , where @xmath130 is the dimension of the parameter space @xmath120 .",
    "another important universal code is the so called _ normalized maximum likelihood _ ( nml ) @xcite . in this case",
    "the universal model @xmath131 corresponds to the model that minimizes the worst case regret , @xmath132    which can be written in closed form as @xmath133 , where the normalization constant @xmath134 is the value of the minimax regret and depends only on @xmath106 and the length of the data @xmath116",
    ". derives from the fact that it defines a complete uniquely decodable code for all data @xmath40 of length @xmath116 , that is , it satisfies the kraft inequality with equality.@xmath135 since every uniquely decodable code with lengths @xmath136 must satisfy the kraft inequality ( see ( * ? ? ?",
    "* chapter  5 ) ) , if there exists a value of @xmath40 such that @xmath137 ( that is @xmath138 ) , then there exists a vector @xmath139 for which @xmath140 for the kraft inequality to hold .",
    "therefore the regret of @xmath141 for @xmath139 is necessarily greater than @xmath142 , which shows that @xmath143 is minimax optimal . ]",
    "note that the nml model requires @xmath142 to be finite , something which is often not the case .",
    "the two previous examples are good for assigning a probability to coefficients @xmath40 that have already been computed , but they can not be used as a model for computing the coefficients themselves since they depend on having observed them in the first place . for this and other reasons that will become clearer later , we concentrate our work on a third important family of universal codes derived from the so called _ mixture models _ ( also called _ bayesian mixtures _ ) . in a mixture model , @xmath112 is a convex mixture of all the models @xmath144 in , indexed by the model parameter @xmath45 , @xmath145 , where @xmath146 specifies the weight of each model .",
    "being a convex mixture implies that @xmath147 and @xmath148 , thus @xmath146 is itself a probability measure over @xmath120 .",
    "we will restrict ourselves to the particular case when @xmath40 is considered a sequence of independent random variables , are out of the scope of this work . ]",
    "@xmath149    where the mixing function @xmath150 can be different for each sample @xmath151 .",
    "an important particular case of this scheme is the so called _ sequential bayes _",
    "code , in which @xmath150 is computed sequentially as a posterior distribution based on previously observed samples , that is @xmath152 ( * ? ? ?",
    "* chapter  6 ) . in this work , for simplicity",
    ", we restrict ourselves to the case where @xmath153 is the same for all @xmath151 .",
    "the result is an iid model where the probability of each sample @xmath154 is a mixture of some probability measure over @xmath121 , @xmath155    a well known result for iid mixture ( bayesian ) codes states that their asymptotic regret is @xmath156 , thus stating their universality , as long as the weighting function @xmath146 is positive , continuous and unimodal over @xmath120 ( see for example ( * ? ? ?",
    "* theorem  8.1),@xcite ) .",
    "this gives us great flexibility on the choice of a weighting function @xmath146 that guarantees universality .",
    "of course , the results are asymptotic and the @xmath157 terms can be large , so that the choice of @xmath146 can have practical impact for small sample sizes .    in the following discussion",
    "we derive several iid mixture models for the laplacian model class . for this purpose",
    ", it will be convenient to consider the corresponding one - sided counterpart of the laplacian , which is the exponential distribution over the absolute value of the coefficients , @xmath158 , and then symmetrize back to obtain the final distribution over the signed coefficients @xmath159 .      in general , ( [ eq : scalar - mixture ] )",
    "can be computed in closed form if @xmath146 is the conjugate prior of @xmath160 .",
    "when @xmath160 is an exponential ( one - sided laplacian ) , the conjugate prior is the gamma distribution , @xmath161 where @xmath162 and @xmath163 are its _ shape _ and _ scale _ parameters respectively . plugging this in ( [ eq : scalar - mixture ] )",
    "we obtain the _ mixture of exponentials _ model ( moe ) , which has the following form ( see appendix [ sec : moe - details ] for the full derivation ) , @xmath164 with some abuse of notation , we will also denote the symmetric distribution on @xmath159 as moe , @xmath165    although the resulting prior has two parameters to deal with instead of one , we know from universal coding theory that , in principle , any choice of @xmath162 and @xmath163 will give us a model whose codelength regret is asymptotically small .    furthermore , being iid models , each coefficient of @xmath40 itself is modeled as a mixture of exponentials , which makes the resulting model over @xmath40 very well suited to the most flexible scenario where the `` underlying '' @xmath45 can be different for each @xmath154 . in section  [ sec",
    ": results : fitting ] we will show that a single moedistribution can fit each of the @xmath8 rows of @xmath14 better than @xmath8 separate laplacian distributions fine - tuned to these rows , with a total of @xmath8 parameters to be estimated . thus , not only we can deal with one single unknown @xmath45 , but we can actually achieve maximum flexibility with only two parameters ( @xmath162 and @xmath163 ) .",
    "this property is particular of the mixture models , and does not apply to the other universal models presented .    finally , if desired , both @xmath162 and @xmath163 can be easily estimated using the method of moments ( see appendix  [ sec : moe - details ] ) . given sample estimates of the first and second non - central moments , @xmath166 and @xmath167 , we have that @xmath168 when the moeprior is plugged into ( [ eq : map - gen ] ) instead of the standard laplacian , the following new sparse coding formulation is obtained , @xmath169 where @xmath170 . an example of the moeregularizer , and the thresholding function it induces , is shown in figure  [ fig : regularizers ] ( center column ) for @xmath171 .",
    "smooth , differentiable non - convex regularizers such as the one in in ( [ eq : moe - sparse - coding ] ) have become a mainstream robust alternative to the @xmath3 norm in statistics  @xcite .",
    "furthermore , it has been shown that the use of such regularizers in regression leads to consistent estimators which are able to identify the relevant variables in a regression model ( oracle property ) @xcite .",
    "this is not always the case for the @xmath3 regularizer , as was proved in @xcite .",
    "the moe  regularizer has also been recently proposed in the context of compressive sensing @xcite , where it is conjectured to be better than the @xmath3-term at recovering sparse signals in compressive sensing applications .",
    "pseudo - norm as an @xmath3-normalized element - wise sum , without the insight and theoretical foundation here reported . ]",
    "this conjecture was partially confirmed recently for non - convex regularizers of the form @xmath172 with @xmath173 in @xcite , and for a more general family of non - convex regularizers including the one in ( [ eq : moe - sparse - coding ] ) in @xcite . in all cases",
    ", it was shown that the conditions on the sensing matrix ( here @xmath15 ) can be significantly relaxed to guarantee exact recovery if non - convex regularizers are used instead of the @xmath3 norm , provided that the exact solution to the non - convex optimization problem can be computed . in practice , this regularizer is being used with success in a number of applications here and in @xcite . our experimental results in section  [ sec : results ] provide further evidence on the benefits of the use of non - convex regularizers , leading to a much improved recovery accuracy of sparse coefficients compared to @xmath3 and @xmath2 .",
    "we also show in section  [ sec : results ] that the moeprior is much more accurate than the standard laplacian to model the distribution of reconstruction coefficients drawn from a large database of image patches .",
    "we also show in section  [ sec : results ] how these improvements lead to better results in applications such as image estimation and classification .",
    "the jeffreys prior for a parametric model class @xmath174 , is defined as @xmath175    where @xmath176 is the determinant of the _ fisher information matrix _ @xmath177\\right\\}\\right|_{\\tilde\\theta=\\theta}. \\label{eq : fisher - information}\\ ] ]    the jeffreys prior is well known in bayesian theory due to three important properties : it virtually eliminates the hyper - parameters of the model , it is invariant to the original parametrization of the distribution , and it is a `` non - informative prior , '' meaning that it represents well the lack of prior information on the unknown parameter @xmath45 @xcite .",
    "it turns out that , for quite different reasons , the jeffreys prior is also of paramount importance in the theory of universal coding .",
    "for instance , it has been shown in @xcite that the worst case regret of the mixture code obtained using the jeffreys prior approaches that of the nml as the number of samples @xmath178 grows .",
    "thus , by using jeffreys , one can attain the minimum worst case regret asymptotically , while retaining the advantages of a mixture ( not needing hindsight of @xmath40 ) , which in our case means to be able to use it as a model for computing @xmath40 via sparse coding .    for the exponential distribution we have that @xmath179 .",
    "clearly , if we let @xmath180 , the integral in ( [ eq : jeffreys - prior ] ) evaluates to @xmath181 .",
    "therefore , in order to obtain a proper integral , we need to exclude @xmath44 and @xmath181 from @xmath120 ( note that this was not needed for the conjugate prior ) .",
    "we choose to define @xmath182 $ ] , @xmath183 , leading to @xmath184 .",
    "$ ]    the resulting mixture , after being symmetrized around @xmath44 , has the following form ( see appendix [ sec : joe - details ] ) : @xmath185 we refer to this prior as a _",
    "jeffreys mixture of exponentials _ ( joe ) , and again overload this acronym to refer to the symmetric case as well . note that although @xmath186 is not defined for @xmath187 , its limit when @xmath188 is finite and evaluates to @xmath189 .",
    "thus , by defining @xmath190 , we obtain a prior that is well defined and continuous for all @xmath191 . when plugged into ( [ eq : map - gen ] ) , we get the joe - based sparse coding formulation , @xmath192    where , according to the convention just defined for @xmath193 , we define @xmath194 .",
    "according to the map interpretation we have that @xmath195 , coming from the gaussian assumption on the approximation error as explained in section  [ sec : sparse - coding : interpretation ] .    as with moe ,",
    "the joe - based regularizer , @xmath196 , is continuous and differentiable in @xmath197 , and its derivative converges to a finite value at zero , @xmath198 . as we will see later in section",
    "[ sec : implementation ] , these properties are important to guarantee the convergence of sparse coding algorithms using non - convex priors .",
    "note from ( [ eq : sparse - coding - joe ] ) that we can rewrite the joeregularizer as @xmath199 so that for sufficiently large @xmath200 , @xmath201 , @xmath202 , and we have that @xmath203 .",
    "thus , for large @xmath200 , the joeregularizer behaves like @xmath3 with @xmath204 . in terms of the probability model",
    ", this means that the tails of the joemixture behave like a laplacian with @xmath205 , with the region where this happens determined by the value of @xmath206 .",
    "the fact that the non - convex region of @xmath207 is confined to a neighborhood around @xmath44 could help to avoid falling in bad local minima during the optimization ( see section  [ sec : implementation ] for more details on the optimization aspects ) .",
    "finally , although having laplacian tails means that the estimated @xmath40 will be biased @xcite , the sharper peak at @xmath44 allows us to perform a more aggressive thresholding of small values , without excessively clipping large coefficients , which leads to the typical over - smoothing of signals recovered using an @xmath3 regularizer .",
    "see figure  [ fig : regularizers ] ( rightmost column ) for an example regularizer based on joewith parameters @xmath208 , and the thresholding function it induces .",
    "the joeregularizer has two hyper - parameters @xmath209 which define @xmath120 and that , in principle , need to be tuned .",
    "one possibility is to choose @xmath210 and @xmath211 based on the physical properties of the data to be modeled , so that the possible values of @xmath45 never fall outside of the range @xmath212 $ ] .",
    "for example , in modeling patches from grayscale images with a limited dynamic range of @xmath213 $ ] in a dct basis , the maximum variance of the coefficients can never exceed @xmath214 .",
    "the same is true for the minimum variance , which is defined by the quantization noise .",
    "having said this , in practice it is advantageous to adjust @xmath212 $ ] to the data at hand . in this case , although no closed form solutions exist for estimating @xmath212 $ ] using mle or the method of moments , standard optimization techniques can be easily applied to obtain them .",
    "see appendix  [ sec : joe - details ] for details .      a recent approach to deal with the case",
    "when the integral over @xmath120 in the jeffreys prior is improper , is the _",
    "conditional jeffreys _",
    "* chapter 11 ) .",
    "the idea is to construct a proper prior , based on the improper jeffreys prior and the first few @xmath215 samples of @xmath40 , @xmath216 , and then use it for the remaining data .",
    "the key observation is that although the normalizing integral @xmath217 in the jeffreys prior is improper , the unnormalized prior @xmath218 can be used as a measure to weight @xmath219 , @xmath220    it turns out that the integral in ( [ eq : cmoe - gen ] ) usually becomes proper for small @xmath215 in the order of @xmath221 . in our case",
    "we have that for any @xmath222 , the resulting prior is a @xmath223 distribution with @xmath224 and @xmath225 ( see appendix  [ sec : cmoe - details ] for details ) .",
    "therefore , using the conditional jeffreys prior in the mixture leads to a particular instance of moe , which we denote by cmoe ( although the functional form is identical to moe ) , where the gamma parameters @xmath162 and @xmath163 are automatically selected from the data .",
    "this may explain in part why the gamma prior performs so well in practice , as we will see in section  [ sec : results ] .",
    "furthermore , we observe that the value of @xmath163 obtained with this approach ( @xmath226 ) coincides with the one estimated using the method of moments for moe if the @xmath162 in moeis fixed to @xmath227 . indeed , if computed from @xmath215 samples , the method of moments for moegives @xmath228 , with @xmath229 , which gives us @xmath230 .",
    "it turns out in practice that the value of @xmath162 estimated using the method of moments gives a value between @xmath231 and @xmath232 for the type of data that we deal with ( see section  [ sec : results ] ) , which is just above the minimum acceptable value for the cmoeprior to be defined , which is @xmath233 .",
    "this justifies our choice of @xmath234 when applying cmoein practice .    as @xmath215 becomes large , so does @xmath235 , and the gamma prior @xmath146 obtained with this method converges to a kronecker delta at the mean value of the gamma distribution , @xmath236 .",
    "consequently , when @xmath237 , the mixture @xmath238 will be close to @xmath239 .",
    "moreover , from the definition of @xmath240 and @xmath226 we have that @xmath241 is exactly the mle of @xmath45 for the laplacian distribution .",
    "thus , for large @xmath215 , the conditional jeffreys method approaches the mle laplacian model .",
    "although from a universal coding point of view this is not a problem , for large @xmath215 the conditional jeffreys model will loose its flexibility to deal with the case when different coefficients in @xmath14 have different underlying @xmath45 . on the other hand",
    ", a small @xmath215 can lead to a prior @xmath146 that is overfitted to the local properties of the first samples , which for non - stationary data such as image patches , can be problematic .",
    "ultimately , @xmath215 defines a trade - off between the degree of flexibility and the accuracy of the resulting model .",
    "all of the mixture models discussed so far yield non - convex regularizers , rendering the sparse coding problem non - convex in @xmath40 .",
    "it turns out however that these regularizers satisfy certain conditions which make the resulting sparse coding optimization well suited to be approximated using a sequence of successive convex sparse coding problems , a technique known as _ local linear approximation _ ( lla )",
    "@xcite ( see also @xcite for alternative optimization techniques for such non - convex sparse coding problems ) . in a nutshell ,",
    "suppose we need to obtain an approximate solution to @xmath242 where @xmath25 is a non - convex function over @xmath243 .",
    "at each lla iteration , we compute @xmath244 by doing a first order expansion of @xmath25 around the @xmath8 elements of the current estimate @xmath245 , @xmath246    and solving the convex weighted @xmath3 problem that results after discarding the constant terms @xmath247 , @xmath248 where we have defined @xmath249 .",
    "if @xmath250 is continuous in @xmath251 , and right - continuous and finite at @xmath44 , then the lla algorithm converges to a stationary point of ( [ eq : sparse - coding - lagrangian - bis ] ) @xcite .",
    "these conditions are met for both the moeand joeregularizers .",
    "although , for the joe  prior , the derivative @xmath250 is not defined at @xmath44 , it converges to the limit @xmath252 when @xmath253 , which is well defined for @xmath254 . if @xmath255 , the joe  mixing function is a kronecker delta and the prior becomes a laplacian with parameter @xmath256 .",
    "therefore we have that for all of the mixture models studied , the lla method converges to a stationary point . in practice ,",
    "we have observed that @xmath83 iterations are enough to converge .",
    "thus , the cost of sparse coding , with the proposed non - convex regularizers , is at most @xmath83 times that of a single @xmath3 sparse coding , and could be less in practice if warm restarts are used to begin each iteration .",
    "of course we need a starting point @xmath257 , and , being a non - convex problem , this choice will influence the approximation that we obtain .",
    "one reasonable choice , used in this work , is to define @xmath258 , where @xmath259 is a scalar so that @xmath260 $ ] , that is , so that the first sparse coding corresponds to a laplacian regularizer whose parameter is the average value of @xmath45 as given by the mixing prior @xmath146 .    finally , note that although the discussion here has revolved around the lagrangian formulation to sparse coding of  ( [ eq : sparse - coding - lagrangian ] ) , this technique is also applicable to the constrained formulation of sparse - coding given by equation ( [ eq : sparse - modeling ] ) for a fixed dictionary @xmath15 .    *",
    "expected approximation error * : since we are solving a convex approximation to the actual target optimization problem , it is of interest to know how good this approximation is in terms of the original cost function .",
    "to give an idea of this , after an approximate solution @xmath40 is obtained , we compute the expected value of the difference between the true and approximate regularization term values .",
    "the expectation is taken , naturally , in terms of the assumed distribution of the coefficients in @xmath40 .",
    "since the regularizers are separable , we can compute the error in a separable way as an expectation over each @xmath13-th coefficient , @xmath261 $ ] , where @xmath262 is the approximation of @xmath263 around the final estimate of @xmath264 . for the case of @xmath265 ,",
    "the expression obtained is ( see appendix ) @xmath266 =     \\log(\\coef_k+\\beta ) + \\frac{1}{\\coef_k+\\beta}\\left[\\coef_k +    \\frac{\\beta}{\\kappa-1}\\right ] - \\log \\beta - \\frac{1}{\\kappa}.\\ ] ] in the moecase , for @xmath162 and @xmath163 fixed , the minimum of @xmath267 occurs when @xmath268 .",
    "we also have @xmath269 .",
    "the function @xmath270 can be evaluated on each coefficient of @xmath14 to give an idea of its quality .",
    "for example , in the experiments from section  [ sec : results ] , we obtained an average value of @xmath271 , which lies between @xmath272 and @xmath273 .",
    "depending on the experiment , this represents 6% to 7% of the total sparse coding cost function value , showing the efficiency of the proposed optimization .    * comments on parameter estimation:*[sec : universal : parameter - estimation ] all the universal models presented so far , with the exception of the conditional jeffreys ,",
    "depend on hyper - parameters which in principle should be tuned for optimal performance ( remember that they do not influence the universality of the model ) .",
    "if tuning is needed , it is important to remember that the proposed universal models are intended for reconstruction coefficients of _ clean data _ , and thus their hyper - parameters should be computed from statistics of clean data , or either by compensating the distortion in the statistics caused by noise ( see for example @xcite ) .",
    "finally , note that when @xmath15 is linearly dependent and @xmath274 , the coefficients matrix @xmath14 resulting from an exact reconstruction of @xmath36 will have many zeroes which are not properly explained by any continuous distribution such as a laplacian .",
    "we sidestep this issue by computing the statistics only from the non - zero coefficients in @xmath14 .",
    "dealing properly with the case @xmath275 is beyond the scope of this work .",
    "in the following experiments , the testing data @xmath36 are @xmath79 patches drawn from the pascal voc2006 _ testing _ subset , which are high quality @xmath276 rgb images with 8 bits per channel . for the experiments",
    ", we converted the 2600 images to grayscale by averaging the channels , and scaled the dynamic range to lie in the @xmath76 $ ] interval .",
    "similar results to those shown here are also obtained for other patch sizes .      for the experiments that follow ,",
    "unless otherwise stated , we use a `` global '' overcomplete dictionary @xmath15 with @xmath277 atoms trained on the full voc2006 _ training _ subset using the method described in @xcite , which seeks to minimize the following cost during training , @xmath278 where @xmath279 denotes frobenius norm .",
    "the additional term , @xmath280 , encourages _ incoherence _ in the learned dictionary , that is , it forces the atoms to be as orthogonal as possible .",
    "dictionaries with lower coherence are well known to have several theoretical advantages such as improved ability to recover sparse signals @xcite , and faster and better convergence to the solution of the sparse coding problems ( [ eq : sparse - modeling ] ) and ( [ eq : sparse - modeling - lagrangian ] ) @xcite .",
    "furthermore , in @xcite it was shown that adding incoherence leads to improvements in a variety of sparse modeling applications , including the ones discussed below .",
    "we used moeas the regularizer in ( [ eq : incoherent - sparse - modeling ] ) , with @xmath281 and @xmath282 , both chosen empirically . see  @xcite for details on the optimization of ( [ eq : sparse - modeling - lagrangian ] ) and ( [ eq : incoherent - sparse - modeling ] ) .",
    "we begin by comparing the performance of the laplacian and moepriors for fitting a single global distribution to the whole matrix @xmath14 .",
    "we compute @xmath14 using ( [ eq : sparse - modeling ] ) with @xmath283 and then , following the discussion in section  [ sec : universal : parameter - estimation ] , restrict our study to the nonzero elements of @xmath14 .",
    "the empirical distribution of @xmath14 is plotted in figure  [ fig : fitting - and - denoising](a ) , along with the best fitting laplacian , moe , joe , and a particularly good example of the conditional jeffreys ( cmoe ) distributions .",
    "uniformly in steps of @xmath284 , which for the amount of data available , gives us enough detail and at the same time reliable statistics for all the quantized values . ]",
    "the mlefor the laplacian fit is @xmath285 ( here @xmath286 is the number of nonzero elements in @xmath14 ) . for moe ,",
    "using ( [ eq : moe - parameter - estimation ] ) , we obtained @xmath287 and @xmath288 .",
    "for joe , @xmath289 and @xmath290 .",
    "according to the discussion in section  [ sec : universal : cond - jeffreys ] , we used the value @xmath287 obtained using the method of moments for moeas a hint for choosing @xmath234 ( @xmath291 ) , yielding @xmath292 , which coincides with the @xmath163 obtained using the method of moments .",
    "as observed in figure  [ fig : fitting - and - denoising](a ) , in all cases the proposed mixture models fit the data better , significantly better for both gamma - based mixtures , moe  and cmoe , and slightly better for joe .",
    "this is further confirmed by the kullback - leibler divergence ( kld ) obtained in each case .",
    "note that joe  fails to significantly improve on the laplacian mode due to the excessively large estimated range @xmath212 $ ] . in this sense , it is clear that the joe  model is very sensitive to its hyper - parameters , and a better and more robust estimation would be needed for it to be useful in practice .    given these results , hereafter we concentrate on the best case which is the moe  prior ( which , as detailed above , can be derived from the conditional jeffreys as well , thus representing both approaches ) .    from figure",
    "[ fig : many - laplacians](e ) we know that the optimal @xmath82 varies locally across different regions , thus , we expect the mixture models to perform well also on a per - atom basis .",
    "this is confirmed in figure  [ fig : fitting - and - denoising](b ) , where we show , for each row @xmath293 , the difference in kld between the globally fitted moe  distribution and the best per - atom fitted moe , the globally fitted laplacian , and the per - atom fitted laplacians respectively . as can be observed ,",
    "the kld obtained with the _ global _",
    "moe  is significantly smaller than the global laplacian in all cases , and even the _ per - atom _ laplacians in most of the cases .",
    "this shows that moe , with only two parameters ( which can be easily estimated , as detailed in the text ) , is a much better model than @xmath8 laplacians ( requiring @xmath8 critical parameters ) fitted specifically to the coefficients associated to each atom .",
    "whether these modeling improvements have a practical impact is explored in the next experiments .",
    "here we compare the active set recovery properties of the moeprior , with those of the @xmath1-based one , on data for which the sparsity assumption @xmath294 holds exactly for all @xmath151 .",
    "to this end , we obtain sparse approximations to each sample @xmath20 using the @xmath2-based orthogonal matching pursuit algorithm ( omp ) on @xmath15 @xcite , and record the resulting active sets @xmath295 as ground truth .",
    "the data is then contaminated with additive gaussian noise of variance @xmath296 and the recovery is performed by solving ( [ eq : sparse - modeling ] ) for @xmath14 with @xmath297 and either the @xmath3 or the moe - based regularizer for @xmath25 .",
    "we use @xmath298 , which is a standard value in denoising applications ( see for example @xcite ) .    for each sample @xmath151 , we measure the error of each method in recovering the active set as the hamming distance between the true and estimated support of the corresponding reconstruction coefficients . the accuracy of the method is then given as the percentage of the samples for which this error falls below a certain threshold @xmath299 .",
    "results are shown in figure  [ fig : fitting - and - denoising](c ) for @xmath300 and @xmath301 respectively , for various values of @xmath296 .",
    "note the very significant improvement obtained with the proposed model .",
    "given the estimated active set @xmath295 , the estimated clean patch is obtained by projecting @xmath20 onto the subspace defined by the atoms that are active according to @xmath295 , using least squares ( which is the standard procedure for denoising once the active set is determined ) .",
    "we then measure the psnrof the estimated patches with respect to the true ones .",
    "the results are shown in figure  [ fig : fitting - and - denoising](d ) , again for various values of @xmath296 . as can be observed ,",
    "the moe - based recovery is significantly better , specially in the high snr range .",
    "notoriously , the more accurate active set recovery of moedoes not seem to improve the denoising performance in this case .",
    "however , as we will see next , it does make a difference when denoising real life signals , as well as for classification tasks .",
    "this experiment is an analogue to the previous one , when the data are the original natural image patches ( without forcing exact sparsity ) . since for this case",
    "the sparsity assumption is only approximate , and no ground truth is available for the active sets , we compare the different methods in terms of their denoising performance .    a critical strategy in image denoising is the use of overlapping patches , where for each pixel in the image a patch is extracted with that pixel as its center .",
    "the patches are denoised independently as @xmath302-dimensional signals and then recombined into the final denoised images by simple averaging .",
    "although this consistently improves the final result in all cases , the improvement is very different depending on the method used to denoise the individual patches .",
    "therefore , we now compare the denoising performance of each method at two levels : individual patches and final image .    to denoise each image , the global dictionary described in section  [ sec : results : learning ]",
    "is further adapted to the noisy image patches using ( [ eq : incoherent - sparse - modeling ] ) for a few iterations , and used to encode the noisy patches via ( [ eq : sparse - coding ] ) with @xmath297 .",
    "we repeated the experiment for two learning variants ( @xmath1 and moeregularizers ) , and two coding variants ( ( [ eq : sparse - coding ] ) with the regularizer used for learning , and @xmath2 via omp .",
    "the four variants were applied to the standard images barbara , boats , lena , man and peppers , and the results summarized in table  [ tab : denoising ] .",
    "we show sample results in figure  [ fig : denoising - sample ] .",
    "although the quantitative improvements seen in table  [ tab : denoising ] are small compared to @xmath3 , there is a significant improvement at the visual level , as can be seen in figure  [ fig : denoising - sample ] . in all cases the",
    "psnr obtained coincides or surpasses the ones reported in @xcite .",
    "are already better without this extra step . ]",
    "+      as an example of signal recovery in the absence of noise , we took the previous set of images , plus a particularly challenging one ( tools ) , and subsampled them to half each side .",
    "we then simulated a zooming effect by upsampling them and estimating each of the 75% missing pixels ( see e.g. , @xcite and references therein ) .",
    "we use a technique similar to the one used in @xcite . the image is first interpolated and then deconvoluted using a wiener filter . the deconvoluted image has artifacts that we treat as noise in the reconstruction .",
    "however , since there is no real noise , we do not perform averaging of the patches , using only the center pixel of @xmath303 to fill in the missing pixel at @xmath151 .",
    "the results are summarized in figure  [ fig : zooming ] , where we again observe that using moeinstead of @xmath2 and @xmath3 improves the results .      in this section",
    "we apply our proposed universal models to a classification problem where each sample @xmath20 is to be assigned a class label @xmath304 , which serves as an index to the set of possible classes , @xmath305 .",
    "we follow the procedure of @xcite , where the classifier assigns each sample @xmath20 by means of the maximum a posteriori criterion ( [ eq : map - gen ] ) with the term @xmath306 corresponding to the assumed prior , and the dictionaries representing each class are learned from training samples using ( [ eq : incoherent - sparse - modeling ] ) with the corresponding regularizer @xmath307 .",
    "each experiment is repeated for the baseline laplacian model , implied in the @xmath3 regularizer , and the universal model moe , and the results are then compared .",
    "in this case we expect that the more accurate prior model for the coefficients will result in an improved likelihood estimation , which in turn should improve the accuracy of the system .",
    "we begin with a classic texture classification problem , where patches have to be identified as belonging to one out of a number of possible textures . in this case",
    "we experimented with samples of @xmath308 and @xmath309 textures drawn at random from the brodatz database , , the ones actually used shown in figure  [ fig : brodatz - subsample ] . in each case",
    "the experiment was repeated @xmath310 times . in each repetition ,",
    "a dictionary of @xmath311 atoms was learned from all @xmath312 patches of the leftmost halves of each sample texture .",
    "we then classified the patches from the rightmost halves of the texture samples .",
    "for the @xmath308 we obtained an average error rate of @xmath313 using @xmath3 against @xmath314 when using moe , which represents a reduction of @xmath315 in classification error . for @xmath309",
    "the average error rate obtained was @xmath316 using @xmath3 and @xmath317 using moe , which is @xmath318 lower .",
    "thus , using the universal model instead of @xmath3 yields a significant improvement in this case ( see for example @xcite for other results in classification of brodatz textures ) .",
    "+     the second sample problem presented is the graz02 bike detection problem , where each pixel of each testing image has to be classified as either background or as part of a bike . in the graz02 dataset",
    ", each of the pixels can belong to one of two classes : bike or background . on each of the training images ( which by convention are the first 150 even - numbered images )",
    ", we are given a mask that tells us whether each pixel belongs to a bike or to the background .",
    "we then train a dictionary for bike patches and another for background patches .",
    "patches that contain pixels from both classes are assigned to the class corresponding to the majority of their pixels .    in figure",
    "[ fig : classification ] we show the _ precision vs. recall curves _ obtained with the detection framework when either the @xmath3 or the moeregularizers were used in the system . as can be seen , the moe - based model outperforms the @xmath3 in this classification task as well , giving a better precision for all recall values . in the above experiments ,",
    "the parameters for the @xmath1 prior ( @xmath88 ) , the moemodel ( @xmath319 ) and the incoherence term ( @xmath320 ) were all adjusted by cross validation .",
    "the only exception is the moeparameter @xmath163 , which was chosen based on the fitting experiment as @xmath288 .",
    "a framework for designing sparse modeling priors was introduced in this work , using tools from universal coding , which formalizes sparse coding and modeling from a mdl perspective .",
    "the priors obtained lead to models with both theoretical and practical advantages over the traditional @xmath2 and @xmath3-based ones . in all derived cases ,",
    "the designed non - convex problems are suitable to be efficiently ( approximately ) solved via a few iterations of ( weighted ) @xmath3 subproblems .",
    "we also showed that these priors are able to fit the empirical distribution of sparse codes of image patches significantly better than the traditional iid laplacian model , and even the non - identically distributed independent laplacian model where a different laplacian parameter is adjusted to the coefficients associated to each atom , thus showing the flexibility and accuracy of these proposed models .",
    "the additional flexibility , furthermore , comes at a small cost of only @xmath231 parameters that can be easily and efficiently tuned ( either @xmath321 in the moemodel , or @xmath209 in the joemodel ) , instead of @xmath8 ( dictionary size ) , as in weighted @xmath3 models .",
    "the additional accuracy of the proposed models was shown to have significant practical impact in active set recovery of sparse signals , image denoising , and classification applications .",
    "compared to the bayesian approach , we avoid the potential burden of solving several sampled sparse problems , or being forced to use a conjugate prior for computational reasons ( although in our case , _ a fortiori _ , the conjugate prior does provide us with a good model ) .",
    "overall , as demonstrated in this paper , the introduction of information theory tools can lead to formally addressing critical aspects of sparse modeling .",
    "future work in this direction includes the design of priors that take into account the nonzero mass at @xmath187 that appears in overcomplete models , and online learning of the model parameters from noisy data , following for example the technique in @xcite .",
    "work partially supported by nga , onr , aro , nsf , nsseff , and fundaciba - antel .",
    "we wish to thank julien mairal for providing us with his fast sparse modeling toolbox , spams .",
    "we also thank federico lecumberry for his participation on the incoherent dictionary learning method , and helpful comments .",
    "in this case we have @xmath322 and @xmath323 which , when plugged into ( [ eq : scalar - mixture ] ) , gives @xmath324 after the change of variables @xmath325 ( @xmath326 , @xmath327 ) , the integral can be written as @xmath328 obtaining @xmath329 since the integral on the second line is precisely the definition of @xmath330 .",
    "the symmetrization is obtained by substituting @xmath159 by @xmath331 and dividing the normalization constant by two , @xmath332    the mean of the moe distribution ( which is defined only for @xmath333 ) can be easily computed using integration by parts , @xmath334                      = \\frac{\\beta}{\\kappa-1}.\\end{aligned}\\ ] ] in the same way , it is easy to see that the non - central moments of order @xmath335 are @xmath336    the mle estimates of @xmath162 and @xmath163 can be obtained using any nonlinear optimization technique such as newton method , using for example the estimates obtained with the method of moments as a starting point . in practice , however , we have not observed any significant improvement in using the mle estimates over the moments - based ones .      as mentioned in the optimization section ,",
    "the lla approximates the moeregularizer as a weighted @xmath3 .",
    "here we develop an expression for the expected error between the true function and the approximate convex one , where the expectation is taken ( naturally ) with respect to the moedistribution . given the value of the current iterate @xmath337 , ( assumed positive , since the function and its approximation are symmetric ) , the approximated regularizer is @xmath338 .",
    "we have @xmath339 & = & \\int_{0}^{\\infty}{\\frac{\\kappa\\beta^\\kappa}{(\\coef+\\kappa)^{\\kappa+1 } } \\left [ \\log(|\\coef_0+\\beta ) + \\frac{1}{\\coef_0+\\beta}(\\coef-\\coef_0 ) - \\log(\\coef + \\beta)\\right ] d\\coef } \\\\ \\!\\!\\!\\ ! & = & \\log(\\coef_0+\\beta ) +       \\frac{\\coef_0}{\\coef_0+\\beta } +       \\frac{\\kappa\\beta^\\kappa}{\\coef_0 + \\beta}\\int_{0}^{\\infty}{\\frac{\\coef}{(\\coef+\\beta)^{\\kappa+1}}d\\coef}-      \\kappa\\beta^\\kappa \\int_0^\\infty{\\frac{\\log(\\coef+\\beta)}{(\\coef+\\beta)^{\\kappa+1}}d\\coef } \\\\ \\!\\!\\!\\ ! & = & \\log(\\coef_0+\\beta ) +       \\frac{\\coef_0}{\\coef_0+\\beta } +       \\frac{\\beta}{(\\coef_0+\\beta)(\\kappa-1 ) } - \\log \\beta - \\frac{1}{\\kappa}.\\end{aligned}\\ ] ]",
    "in the case of the exponential distribution , the fisher information matrix in ( [ eq : fisher - information ] ) evaluates to @xmath340\\right\\}\\right|_{\\tilde\\theta=\\theta }   \\;=\\ ; \\left.\\left\\ { e_{p(\\cdot|\\tilde\\theta ) } \\left[\\frac{1}{\\tilde\\theta^2 } \\right]\\right\\}\\right|_{\\tilde\\theta=\\theta } = \\frac{1}{\\theta^2}.\\end{aligned}\\ ] ]    by plugging this result into ( [ eq : jeffreys - prior ] ) with @xmath182 $ ] , @xmath183 we obtain @xmath341 we now derive the ( one - sided ) joe probability density function by plugging this @xmath146 in ( [ eq : scalar - mixture ] ) , @xmath342 although @xmath343 can not be evaluated at @xmath187 , the limit for @xmath344 exists and is finite , so we can just define @xmath345 as this limit , which is @xmath346   = \\frac{\\tmax-\\tmin}{\\ln(\\tmax/\\tmin)}.\\end{aligned}\\ ] ]    again , if desired , parameter estimation can be done for example using maximum likelihood ( via nonlinear optimization ) , or using the method of moments . however , in this case , the method of moments does not provide a closed form solution for @xmath209 .",
    "the non - central moments of order @xmath335 are @xmath347d\\coef }   \\!=\\ !",
    "\\frac{1}{\\ln(\\tmax/\\tmin)}\\left\\ { \\int_{0}^{+\\infty}{\\!\\coef^{i-1}e^{-\\tmin\\coef}d\\coef } - \\int_{0}^{+\\infty}{\\!\\coef^{i-1}e^{-\\tmax\\coef}d\\coef}\\right\\}. \\label{eq : joe - moments}\\ ] ] for @xmath348 , both integrals in ( [ eq : joe - moments ] ) are trivially evaluated , yielding @xmath349 . for @xmath350",
    ", these integrals can be solved using integration by parts : @xmath351 where the first term in the right hand side of both equations evaluates to @xmath44 for @xmath350 .",
    "therefore , for @xmath350 we obtain the recursions @xmath352 which , combined with the result for @xmath348 , give the final expression for all the moments of order @xmath353 @xmath354 in particular , for @xmath348 and @xmath355 we have @xmath356 which , when combined , give us @xmath357    one possibility is to solve the nonlinear equation @xmath358 for @xmath359 by finding the roots of the nonlinear equation @xmath360 and choosing one of them based on some side information .",
    "another possibility is to simply fix the ratio @xmath361 beforehand and solve for @xmath210 and @xmath211 using ( [ eq : joe - method - of - moments ] ) .",
    "the conditional jeffreys method defines a proper prior @xmath146 by assuming that @xmath215 samples from the data to be modeled @xmath40 were already observed . plugging the fisher information for the exponential distribution , @xmath362 , into ( [ eq : cmoe - gen ] )",
    "we obtain @xmath363    denoting @xmath364 and performing the change of variables @xmath365 we obtain @xmath366 where the last equation derives from the definition of the gamma function , @xmath367 .",
    "we see that the resulting prior @xmath146 is a gamma distribution gamma@xmath368 with @xmath235 and @xmath369 .",
    "g.  motta , e.  ordentlich , i.  ramirez , g.  seroussi , and m.  weinberger .",
    "the dude framework for grayscale image denoising . technical report , hp laboratories , 2009 . http://www.hpl.hp.com/techreports/2009/hpl-2009-252.html .                      n.  saito . simultaneous noise suppression and signal compression using a library of orthonormal bases and the mdl criterion . in e.",
    "foufoula - georgiou and p.  kumar , editors , _ wavelets in geophysics _ , pages 299324 .",
    "new york : academic , 1994 ."
  ],
  "abstract_text": [
    "<S> sparse data models , where data is assumed to be well represented as a linear combination of a few elements from a dictionary , have gained considerable attention in recent years , and their use has led to state - of - the - art results in many signal and image processing tasks . </S>",
    "<S> it is now well understood that the choice of the sparsity regularization term is critical in the success of such models . </S>",
    "<S> based on a codelength minimization interpretation of sparse coding , and using tools from universal coding theory , we propose a framework for designing sparsity regularization terms which have theoretical and practical advantages when compared to the more standard @xmath0 or @xmath1 ones . </S>",
    "<S> the presentation of the framework and theoretical foundations is complemented with examples that show its practical advantages in image denoising , zooming and classification . </S>"
  ]
}