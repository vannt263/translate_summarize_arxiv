{
  "article_text": [
    "we consider the ill - posed inverse problems of the form @xmath0 where @xmath1 is a bounded linear operator between two hilbert spaces @xmath2 and @xmath3 whose inner products and the induced norms are denoted as @xmath4 and @xmath5 respectively which should be clear from the context .",
    "here the ill - posedness of ( [ 1.1 ] ) refers to the fact that the solution of ( [ 1.1 ] ) does not depend continuously on the data which is a characteristic property of inverse problems . in practical applications ,",
    "one never has exact data , instead only noisy data are available due to errors in the measurements . even if the deviation is very small , algorithms developed for well - posed problems may fail , since noise could be amplified by an arbitrarily large factor .",
    "therefore , regularization methods should be used in order to obtain a stable numerical solution .",
    "one can refer to @xcite for many useful regularization methods for solving ( [ 1.1 ] ) ; these methods , however , have the tendency to over - smooth solutions and hence are not quite successful to capture special features .    in case a priori information on the feature of the solution of ( [ 1.1 ] ) is available , we may introduce a proper , lower semi - continuous , convex function @xmath6 $ ] such that the sought solution of ( [ 1.1 ] ) is in @xmath7 . by taking @xmath8 and @xmath9 , the solution of ( [ 1.1 ] ) with the desired feature",
    "can be determined by solving the constrained minimization problem @xmath10 where @xmath11 denotes the bregman distance induced by @xmath12 at @xmath13 in the direction @xmath14 , i.e. @xmath15 when only a noisy data @xmath16 is available , an approximate solution can be constructed by the tikhonov - type method @xmath17 when the regularization parameter @xmath18 is given , many efficient solvers were developed to compute @xmath19 when @xmath12 is the @xmath20 or the total variation function .",
    "unfortunately almost all these methods do not address the choice of @xmath18 which , however , is important for practical applications . in order to use these solvers",
    ", one has to perform the trial - and - error procedure to find a reasonable @xmath18 which is time consuming . on the other hand , some iterative methods , equipping with proper termination criteria , were developed to find approximate solutions of ( [ 10.14.1 ] ) , see @xcite and references therein .",
    "these iterative methods have the advantage of avoiding the difficulty for choosing the regularization parameter .",
    "however in each iteration step one has to solve a minimization problem similar to ( [ tik ] ) , and overall it may take long time .    in this paper",
    "we will propose a fast iterative regularization methods for solving ( [ 10.14.1 ] ) by splitting @xmath21 and @xmath12 into different steps .",
    "our idea is to exploit the hilbert space structure of the underlying problem to build each iterate by first applying one step of a well - established classical regularization method and then penalizing the resultant by the convex function @xmath12 . to motivate the method",
    ", we consider the exact data case .",
    "we take an invertible bounded linear operator @xmath22 which can be viewed as a preconditioner and rewrite ( [ 10.14.1 ] ) into the equivalent form @xmath23 the corresponding lagrangian is @xmath24 where @xmath25 represents the dual variable",
    ". then a desired solution of ( [ 10.14.1 ] ) can be found by determining a saddle point of @xmath26 if exists .",
    "let @xmath27 be a current guess of a saddle point of @xmath26 , we may update it to get a new guess @xmath28 as follows : we first update @xmath29 by solving the proximal maximization problem @xmath30 with a suitable step length @xmath31 .",
    "we then update @xmath32 by solving the minimization problem @xmath33 by straightforward calculation and simplification it follows @xmath34 which is the one step result of the uzawa algorithm @xcite or the dual subgradient method @xcite , where @xmath35 and @xmath36 denote the adjoint operators of @xmath21 and @xmath37 respectively . by setting @xmath38 and @xmath39",
    ", the above equation can be transformed into the form @xmath40    now we may apply the updating scheme ( [ 10.13.2 ] ) to @xmath41 iteratively but with dynamic preconditioning operator @xmath42 and variable step size @xmath43 .",
    "this gives rise to the following iterative methods @xmath44 the performance of the method ( [ method0 ] ) depends on the choices of @xmath45 .",
    "if we take @xmath46 for all @xmath47 , ( [ method0 ] ) becomes the method that has been studied in @xcite which is the generalization of the classical landweber iteration and is known to be a slowly convergent method .",
    "in this paper we will consider ( [ method ] ) with @xmath48 for all @xmath47 , where @xmath49 is a decreasing sequence of positive numbers .",
    "this yields the nonstationary iterative method @xmath50 observing that when @xmath51 and @xmath52 for all @xmath47 , ( [ method ] ) reduces to the nonstationary iterated tikhonov regularization @xmath53 whose convergence has been studied in @xcite and it has been shown to be a fast convergent method when @xmath49 is a geometric decreasing sequence .",
    "this strongly suggests that our method ( [ method ] ) may also exhibit fast convergence property if @xmath49 and @xmath54 are chosen properly .",
    "we will confirm this in the present paper .",
    "it is worthy to point out that each iteration in ( [ method ] ) consists of two steps : the first step involves only the operator @xmath21 and the second step involves only the convex function @xmath12 .",
    "this splitting character can make the computation much easier .",
    "this paper is organized as follows . in section 2 , we start with some preliminary facts from convex analysis , and then give the convergence analysis of the method ( [ method ] ) when the data is given exactly . in case",
    "the data is corrupted by noise , we propose a stopping rule to terminate the iteration and establish the regularization property .",
    "we also give a possible extension of our method to solve nonlinear inverse problems in hilbert spaces . in section 3",
    "we test the performance of our method by reporting various numerical simulations , including the image deblurring , the determination of source term in poisson equation and the de - autoconvolution problem .",
    "in this section we first give the convergence analysis of ( [ method ] ) with suitable chosen @xmath55 when @xmath6 $ ] is a proper , lower semi - continuous function that is strongly convex in the sense that there is a constant @xmath56 such that @xmath57 for all @xmath58 and @xmath59 .",
    "we then consider the method when the data contains noise and propose a stopping rule to render it into a regularization method .",
    "our analysis is based on some important results from convex analysis which will be recalled in the following subsection .      given a convex function @xmath6 $ ]",
    ", we will use @xmath60 to denote its effective domain .",
    "it is called proper if @xmath61 .",
    "given @xmath62 , the set @xmath63 is called the subdifferential of @xmath12 at @xmath64 and each element @xmath65 is called a subgradient .    our convergence analysis of ( [ method ] )",
    "will not be carried out directly under the norm of @xmath2 .",
    "instead we will use the bregman distance ( @xcite ) induced by @xmath12 .",
    "given @xmath65 , the quantity @xmath66 is called the bregman distance induced by @xmath12 at @xmath64 in the direction @xmath67 .",
    "it is clear that @xmath68 .",
    "however , bregman distance is not a metric distance since it does not satisfy the symmetry and the triangle inequality in general . nevertheless , when @xmath12 is strongly convex in the sense of ( [ sc ] ) , there holds ( @xcite ) @xmath69 which means that the bregman distance can be used to detect information under the norm of @xmath2 .",
    "although @xmath12 could be non - smooth , its fenchel conjugate can have enough regularity provided @xmath12 has enough convexity .",
    "the fenchel conjugate of @xmath12 is defined by @xmath70 for a proper , lower semi - continuous , convex function @xmath12 , there always holds @xmath71 consequently , the bregman distance can be equivalently written as @xmath72 if in addition @xmath12 is strongly convex in the sense of ( [ sc ] ) , then @xmath73 , @xmath74 is frchet differentiable , and its gradient @xmath75 satisfies @xmath76 i.e. @xmath75 is lipschitz continuous .",
    "these facts are crucial in the forthcoming convergence analysis and their proofs can be found in many standard textbooks , cf .",
    "@xcite .",
    "we consider the convergence of the method ( [ method ] ) under the condition that @xmath12 is proper , lower semi - continuous , and strongly convex in the sense of ( [ sc ] ) .",
    "we will always assume that ( [ 1.1 ] ) has a solution in @xmath77 . by taking @xmath78 and define @xmath79 as an initial guess , we define @xmath80 to be the solution of ( [ 1.1 ] ) in @xmath77 satisfying @xmath81 it is easy to show that such @xmath80 is uniquely defined .",
    "our aim is to show that the sequence @xmath82 produced by ( [ method ] ) eventually converges to @xmath80 if @xmath55 is chosen properly .    to this end",
    ", we first consider the monotonicity of the bregman distance @xmath83 with respect to @xmath47 for any solution @xmath84 of ( [ 1.1 ] ) in @xmath77 . by the subdifferential calculus and the definition of @xmath85 , it is easy to see that @xmath86 and hence @xmath87",
    ". therefore , in view of ( [ 10.13.3 ] ) and ( [ 10.13.4 ] ) we have @xmath88 using the definition of @xmath89 in ( [ method ] ) and @xmath90 we obtain @xmath91 if @xmath92 , we may choose @xmath55 such that @xmath93 with @xmath94 , then it yields @xmath95 when @xmath96 , the inequality ( [ eq:11.22 ] ) obviously holds for any @xmath97 .",
    "we observe that the @xmath55 chosen by ( [ 10.13.5 ] ) could be very large when @xmath98 is small .",
    "using such a choice of @xmath55 it could make the method numerically unstable , in particular when the data contains noise . to avoid this",
    ", we take a preassigned number @xmath99 and then set @xmath100 the above argument then shows the following monotonicity result .",
    "[ lem10.13.1 ] if @xmath55 is chosen by ( [ 10.13.7 ] ) with @xmath94 and @xmath99 , then @xmath101 and @xmath102 for any solution @xmath84 of ( [ 1.1 ] ) in @xmath77 , where @xmath103 .",
    "we will use lemma [ lem10.13.1 ] to derive the convergence of the method ( [ method ] ) . for the step size @xmath55 defined by ( [ 10.13.7 ] ) , it is easy to see that @xmath104 where we used the inequality @xmath105 to derive the left inequality .",
    "this together with ( [ 10.3.4 ] ) implies @xmath106 since @xmath107 , we can further derive that @xmath108    the following main result shows that the method ( [ method ] ) is indeed convergent if @xmath55 is chosen by ( [ 10.13.7 ] ) .",
    "[ thm1 ] let @xmath6 $ ] be a a proper , lower semi - continuous function that is strongly convex in the sense of ( [ sc ] ) .",
    "if @xmath49 is a decreasing sequence of positive numbers and if @xmath55 is chosen by ( [ 10.13.7 ] ) with @xmath94 and @xmath99 , then for the method ( [ method ] ) there hold @xmath109    the proof is based on the following useful result .",
    "[ general ] consider the equation ( [ 1.1 ] ) .",
    "let @xmath110 $ ] be a proper , lower semi - continuous and strong convex function .",
    "let @xmath111 and @xmath112 be such that    1 .",
    "@xmath113 for all @xmath47 ; 2 .   for any solution @xmath84 of (",
    "[ 1.1 ] ) in @xmath77 the sequence @xmath114 is monotonically decreasing ; 3 .",
    "4 .   there is a subsequence @xmath116 with @xmath117 such that for any solution @xmath84 of ( [ 1.1 ] ) in @xmath77 there holds @xmath118    then there exists a solution @xmath119 of ( [ 1.1 ] ) in @xmath77 such that @xmath120 if , in addition , @xmath121 for all @xmath47 , then @xmath122",
    ".    * proof .",
    "* this is a slight modification of ( * ? ? ?",
    "* proposition 3.6 ) , we include here the proof for completeness .",
    "we first show the convergence of @xmath123 .",
    "for any @xmath124 we have from the definition of bregman distance that @xmath125 by the monotonicity of @xmath114 and ( [ 10.3.2 ] ) we obtain that @xmath126 as @xmath127 . in view of the strong convexity of @xmath12",
    ", it follows that @xmath123 is a cauchy sequence in @xmath2 .",
    "thus @xmath128 for some @xmath129 .",
    "since @xmath130 , we have @xmath131 .    in order to show @xmath132",
    ", we use @xmath133 to obtain @xmath134 in view of ( [ 10.3.2 ] ) and @xmath128 as @xmath135 , there is a constant @xmath136 such that @xmath137 therefore @xmath138 for all @xmath139 . by using the lower semi - continuity of @xmath12 we obtain @xmath140 this implies that @xmath132 .",
    "next we derive the convergence in bregman distance .",
    "since @xmath141 is monotonically decreasing , the limit @xmath142 exists . by taking @xmath135 in ( [ 10.3.1 ] ) with @xmath143 and using the lower semi - continuous of @xmath12",
    ", we obtain @xmath144 which is true for all @xmath145 .",
    "letting @xmath146 and using ( [ 10.3.2 ] ) gives @xmath147 .",
    "thus @xmath148 , i.e. @xmath149 .    finally we show that @xmath122 .",
    "we use ( [ 5.5.3.1 ] ) with @xmath150 replaced by @xmath80 to obtain @xmath151 because of ( [ 10.3.2 ] ) , for any @xmath152 we can find @xmath153 such that @xmath154 we next consider @xmath155 .",
    "since @xmath121 , we can find @xmath156 such that @xmath157 .",
    "consequently @xmath158 since @xmath159 as @xmath160 , we can find @xmath161 such that @xmath162 therefore @xmath163 for all @xmath164 .",
    "since @xmath152 is arbitrary , we obtain @xmath165 . by taking @xmath135 in ( [ 5.5.7 ] ) and using the lower semi - continuity of @xmath12 we obtain @xmath166 . according to the definition of @xmath80",
    "we must have @xmath167 . by uniqueness it follows @xmath122 .",
    "+    * proof of theorem [ thm1 ] .",
    "* we will use proposition [ general ] to complete the proof . by the definition of @xmath168 we always have @xmath169 .",
    "it remains to verify the four conditions in proposition [ general ] . by the definition of @xmath85 we have @xmath86 which implies ( i ) in proposition [ general ] .",
    "moreover , lemma [ lem10.13.1 ] and ( [ 10.13.9 ] ) confirm ( ii ) and ( iii ) in proposition [ general ] respectively .",
    "it remains only to verify ( iv ) in proposition [ general ] . to this end , we consider @xmath170 in view of ( [ 10.13.8 ] ) , we have @xmath171 . moreover , by the definition of the method ( [ method ] ) , if @xmath172 for some @xmath47 , then @xmath173 for all @xmath174 .",
    "consequently , we may choose a strictly increasing subsequence @xmath116 of integers such that @xmath175 and @xmath176 , for each @xmath177 , is the first integer satisfying @xmath178 for this chosen @xmath116 it is easy to see that @xmath179 inddeed , for @xmath180 , we can find @xmath181 such that @xmath182 and thus , by the definition of @xmath183 , we have @xmath184 . with the above chosen @xmath116 , we will show that ( [ 10.3.2 ] ) holds for any solution @xmath84 of ( [ 1.1 ] ) in @xmath77 . by the definition of @xmath185 we have for @xmath124 that @xmath186 therefore @xmath187 by using the monotonicity of @xmath49 and ( [ 9.29.1 ] )",
    ", we have for @xmath188 that @xmath189 consequently , it follows from ( [ 10.3.4 ] ) that @xmath190 which , together with the monotonicity of @xmath114 , implies ( [ 10.3.2 ] ) .",
    "the proof is therefore complete .",
    "we next consider the situation that the data contains noise .",
    "thus , instead of @xmath192 , we only have noisy data @xmath16 satisfying @xmath193 with a small known noise level @xmath194 .",
    "the corresponding method takes the form @xmath195 with suitably chosen step length @xmath196 , where @xmath197 and @xmath198 . in order to terminate the method , we need some stopping criterion .",
    "it seems that a natural one is the discrepancy principle @xmath199 for some number @xmath200 .",
    "unfortunately , we can not prove the regularization property for the method terminated by the discrepancy principle ; furthermore , numerical simulations indicate that the discrepancy principle might not always produce satisfactory reconstruction result . therefore , the discrepancy principle might not be a natural rule to terminate ( [ method_noise ] ) . recall that",
    "when we motivate our method , we consider the preconditioned equation @xmath201 instead of @xmath41 .",
    "this indicates that it might be natural to stop the iteration as long as @xmath202 is satisfied for the first time . the stopping rule ( [ pmd ] )",
    "can be viewed as the discrepancy principle applied to the preconditioned equation . since",
    "the right hand side of ( [ pmd ] ) involves @xmath192 which is not available , it can not be used in practical applications .",
    "considering @xmath203 , we may replace the right hand side of ( [ pmd ] ) by @xmath204 which leads to the following stopping rule .",
    "[ rule1 ] let @xmath200 be a given number .",
    "we define @xmath205 to be the first integer such that @xmath206    in the context of tikhonov regularization for linear ill - posed inverse problems , a similar rule was proposed in @xcite to choose the regularization parameter .",
    "the rule was then generalized and analyzed in @xcite for nonlinear tikhonov regularization and was further extended in @xcite as a stopping rule for the iteratively regularized gauss - newton method for solving nonlinear inverse problems in hilbert spaces .",
    "combining rule [ rule1 ] with ( [ method_noise ] ) and using suitable choice of the step length @xmath207 it yields the following algorithm .",
    "[ nonstationary iterative method with convex penalty ] [ alg1 ]    1 .",
    "take @xmath200 , @xmath208 , @xmath99 and a decreasing sequence @xmath49 of positive numbers ; 2 .",
    "take @xmath78 and define @xmath209 as an initial guess ; 3 .",
    "for each @xmath210 define @xmath211 and @xmath212 by ( [ method_noise ] ) , where @xmath213 4 .",
    "let @xmath205 be the integer determined by rule [ rule1 ] and use @xmath214 as an approximate solution .",
    "the following lemma shows that algorithm [ alg1 ] is well defined and certain monotonicity result holds along the iteration if @xmath208 is suitably small .",
    "[ lem1 ] let @xmath6 $ ] be a a proper , lower semi - continuous function that is strongly convex in the sense of ( [ sc ] ) .",
    "if @xmath49 is a decreasing sequence of positive numbers and @xmath207 is chosen by ( [ 10.3.6 ] ) with @xmath215 and @xmath99 , then rule [ rule1 ] defines a finite integer @xmath205 . moreover , if @xmath216 , then for the sequences @xmath217 and @xmath218 defined by ( [ method_noise ] ) there holds @xmath219 for any solution @xmath84 of ( [ 1.1 ] ) in @xmath77 .",
    "* let @xmath220 . by using the similar argument in the proof of lemma [ lem10.13.1 ]",
    "we can obtain @xmath221 in view of @xmath222 and the choice of @xmath207 , it follows that @xmath223 by the definition of @xmath205 and @xmath224 we have @xmath225 therefore , we have with @xmath226 that @xmath227 this shows the monotonicity result ( [ monotone ] ) and @xmath228 for all @xmath220 .",
    "we may sum the above inequality over @xmath47 from @xmath229 to @xmath230 for any @xmath231 to get @xmath232 by the choice of @xmath207 it is easy to check that @xmath233 .",
    "therefore , in view of ( [ 10.3.5 ] ) , we have @xmath234 for all @xmath231 . since @xmath235 for all @xmath47",
    ", it follows from ( [ 10.6.2 ] ) that @xmath236 .",
    "the proof is therefore complete .",
    "+    by taking @xmath237 in ( [ 10.6.2 ] ) , the integer @xmath205 defined by rule [ rule1 ] can be estimated by @xmath238 where @xmath239 . in case @xmath49",
    "is chosen such that @xmath240 for all @xmath47 for some constant @xmath241 , then @xmath242 it then follows from ( [ 10.6.1 ] ) that @xmath243 which implies that @xmath244 . therefore , with such a chosen @xmath49 , algorithm [ alg1 ] exhibits the fast convergence property .    in order to use the results given in lemma [ lem1 ] and theorem [ thm1 ] to prove the convergence of the method ( [ method_noise ] ) , we need the following stability result .    [ lem10.9.1 ]",
    "let @xmath168 and @xmath82 be defined by ( [ method ] ) with @xmath54 chosen by ( [ 10.13.7 ] ) , and let @xmath217 and @xmath218 be defined by ( [ method_noise ] ) with @xmath245 chosen by ( [ 10.3.6 ] ) .",
    "then for each fixed integer @xmath47 there hold @xmath246    * proof . *",
    "we prove the result by induction on @xmath47 .",
    "it is trivial when @xmath229 because @xmath247 and @xmath248 .",
    "assume next that the result is true for some @xmath249 .",
    "we will show that @xmath250 and @xmath251 as @xmath252 .",
    "we consider two cases :    _ case 1 : @xmath253 .",
    "_ in this case we must have @xmath254 since otherwise @xmath255 therefore , by the induction hypothesis it is straightforward to see that @xmath256 as @xmath257 . according to the definition of @xmath211 and the induction hypothesis , we then obtain @xmath258 .",
    "recall that @xmath259 we then obtain @xmath260 by the continuity of @xmath75 .",
    "_ case 2 : @xmath261 .",
    "_ in this case we have @xmath262 .",
    "therefore @xmath263 consequently , by the induction hypothesis , we have @xmath264 by using again the continuity of @xmath75 , we obtain @xmath260 .",
    "+    we are now in a position to give the main result concerning the regularization property of the method ( [ method_noise ] ) with noisy data when it is terminated by rule [ rule1 ] .",
    "[ thm2 ] let @xmath6 $ ] be proper , lower semi - continuous and strong convex in the sense of ( [ sc ] ) .",
    "let @xmath49 be a decreasing sequence of positive numbers and let @xmath245 be chosen by ( [ 10.3.6 ] ) with @xmath215 and @xmath99 .",
    "let @xmath205 be the finite integer defined by rule [ rule1 ] .",
    "then for the method ( [ method_noise ] ) there hold @xmath265    * proof . * due to the strong convexity of @xmath12 , it suffices to show that @xmath266 . by the subsequence - subsequence argument",
    ", we may complete the proof by considering two cases .",
    "assume first that @xmath267 is a sequence satisfying @xmath268 with @xmath269 such that @xmath270 as @xmath135 for some finite integer @xmath271 .",
    "we may assume @xmath272 for all @xmath139 .",
    "from the definition of @xmath273 we have @xmath274 by taking @xmath135 and using lemma [ lem10.9.1 ] , we can obtain @xmath275 . in view of the definition of @xmath168 and @xmath82",
    ", this implies that @xmath276 and @xmath277 for all @xmath278 .",
    "since theorem [ thm1 ] implies that @xmath279 as @xmath160 , we must have @xmath280 . moreover , by lemma [ lem10.9.1 ] , @xmath281 as @xmath135 . therefore , by the continuity of @xmath74 we can obtain @xmath282    assume next that @xmath267 is a sequence satisfying @xmath268 with @xmath269 such that @xmath283 as @xmath135 .",
    "let @xmath47 be any fixed integer .",
    "then @xmath284 for large @xmath139 .",
    "it then follows from ( [ monotone ] ) in lemma [ lem1 ] that @xmath285 by using lemma [ lem10.9.1 ] and the continuity of @xmath74 we obtain @xmath286 since @xmath47 can be arbitrary and since theorem [ thm1 ] implies that @xmath287 as @xmath160 , we therefore have @xmath288 .",
    "+    in certain applications , the solution of ( [ 1.1 ] ) may have some physical constraints .",
    "thus , instead of ( [ 1.1 ] ) , we need to consider the constrained problem @xmath289 where @xmath290 is a closed convex subset in @xmath2 .",
    "correspondingly , ( [ method_noise ] ) can be modified into the form @xmath291 which can be analyzed by the above framework by introducing @xmath292 , where @xmath293 denotes the indicator function of @xmath290 , i.e. @xmath294 when @xmath207 is chosen by ( [ 10.3.6 ] ) and ( [ 10.16.1 ] ) is terminated by rule [ rule1 ] , we still have @xmath295 and @xmath296 as @xmath257",
    ". however , @xmath297 may not converge to @xmath298 because @xmath299 is not necessarily in @xmath300 .    in order to implement algorithm [ alg1 ] , a key ingredient is to solve the minimization problem @xmath301 for any given @xmath302 .",
    "for some choices of @xmath12 , this minimization problem can be efficiently solved numerically . when @xmath303 , where @xmath304 is a bounded lipschitz domain in euclidean space , there are at least two important choices of @xmath12 that are crucial for sparsity recovery and discontinuity detection .",
    "the first one is @xmath305 with @xmath306 , the minimizer of ( [ eq4.16.1 ] ) can be given explicitly by @xmath307 the second one is @xmath308 with @xmath306 , where @xmath309 denotes the total variation of @xmath64 , i.e. @xmath310 then the minimization problem ( [ eq4.16.1 ] ) can be equivalently formulated as @xmath311 which is the total variation denoising problem ( @xcite ) .",
    "although there is no explicit formula for the minimizer of ( [ eq4.16.1 ] ) , there are many efficient numerical solvers developed in recent years , see @xcite . for the numerical simulations involving total variation presented in section 3",
    ", we always use the denoising algorithm ` fista ` from @xcite . indeed , when solving ( [ eq4.16.1 ] ) with @xmath12 given by ( [ eq : tv ] ) , `",
    "fista ` is used to solve its dual problem whose solution determines the solution of the primal problem ( [ eq4.16.1 ] ) directly ; one may refer to the algorithm on page 2462 in @xcite and its monotone version    after acceptance of this paper , we found that our method can be significantly accelerated if we use pdhg ( an application of uzawa algorithm ) to solve the tv denoising problem .",
    "the matlab code of pdhg can be found at http://pages.cs.wisc.edu/@xmath312swright/gpureconstruction/    .",
    "another key ingredient in implementing algorithm [ alg1 ] is to determine @xmath313 for @xmath314 , where @xmath315 .",
    "this amounts to solving the linear equation @xmath316 for which many efficient solvers from numerical linear algebra can be applied .",
    "when @xmath21 has special structure , this equation can even be solved very fast .",
    "for instance , if @xmath21 is a convolution operator in @xmath317 , say @xmath318 with the kernel @xmath139 decaying sufficiently fast at infinity , then @xmath319 can be determined as @xmath320 where @xmath321 and @xmath322 denote the fourier transform and the inverse fourier transform respectively .",
    "therefore @xmath319 can be calculated efficiently by the fast fourier transform .",
    "our method can be extended for solving nonlinear inverse problems in hilbert spaces that can be formulated as the equation @xmath323 where @xmath324 is a nonlinear continuous operator between two hilbert spaces @xmath2 and @xmath3 with closed convex domain @xmath325 .",
    "we assume that for each @xmath326 there is a bounded linear operator @xmath327 such that @xmath328 in case @xmath329 is frchet differentiable at @xmath64 , @xmath330 is exactly the frchet derivative of @xmath329 at that point .    in order to find the solution of ( [ n1.1 ] ) with special feature , as before we introduce a penalty function @xmath6 $ ] which is proper , convex and lower semi - continuous . let @xmath16 be the only available noisy data satisfying @xmath331 with a small known noise level @xmath194",
    ". then an obvious extension of algorithm [ alg1 ] for solving ( [ n1.1 ] ) takes the following form .",
    "[ alg2 ]    1 .",
    "take @xmath200 , @xmath208 , @xmath99 and a decreasing sequence @xmath49 of positive numbers ; 2 .",
    "take @xmath78 and define @xmath332 as an initial guess ; 3 .   for each @xmath210 define @xmath333 where @xmath334 4 .",
    "let @xmath205 be the first integer such that @xmath335 and use @xmath336 to approximate the solution of ( [ n1.1 ] ) .",
    "we remark that when @xmath337 , algorithm [ alg2 ] reduces to a method which is similar to the regularized levenberg - marquardt method in @xcite for which convergence is proved under certain conditions on @xmath329 . for general convex penalty function @xmath12 , however , we do not have convergence theory on algorithm [ alg2 ] yet .",
    "nevertheless , we will give numerical simulations to indicate that it indeed performs very well .",
    "in this section we will provide various numerical simulations on our method .",
    "the choice of the sequence @xmath49 plays a crucial role for the performance : if @xmath49 decays faster , only fewer iterations are required but the reconstruction result is less accurate ; on the other hand , if @xmath49 decays slower , more iterations are required but the reconstruction result is more accurate . in order to solve this dilemma",
    ", we choose fast decaying @xmath49 at the beginning , and then choose slow decaying @xmath49 when the method tends to stop .",
    "more precisely , we choose @xmath49 according to the following rule .",
    "[ alpha ] let @xmath338 and @xmath339 be preassigned numbers . we take some number @xmath340 and for @xmath249",
    "define @xmath341 if @xmath342 we set @xmath343 ; otherwise we set @xmath344 .",
    "all the computation results in this section are based on @xmath49 chosen by this rule with @xmath345 , @xmath346 and @xmath347 .",
    "our tests were done by using matlab r2012a on an lenovo laptop with intel(r ) core(tm ) i5 cpu 2.30 ghz and 6 gb memory .",
    "we first consider the integral equation of the form @xmath348,\\ ] ] where @xmath349 it is easy to see , that @xmath21 is a compact linear operator from @xmath350 $ ] to @xmath350 $ ] . our goal is to find the solution of ( [ int ] ) using noisy data @xmath16 satisfying @xmath351}=\\d$ ] for some specified noise level @xmath352 . in our numerical simulations , we divide @xmath353 $ ] into @xmath354 subintervals of equal length and approximate any integrals by the trapezoidal rule .",
    "in figure [ fig1 ] we report the numerical performance of algorithm [ alg1 ] .",
    "the sequence @xmath49 is selected by rule [ alpha ] with @xmath355 , @xmath356 , @xmath357 and @xmath358 .",
    "the first row gives the reconstruction results using noisy data with various noise levels when the sought solution is sparse ; we use the penalty function @xmath12 given in ( [ eq : l1 ] ) with @xmath359 .",
    "the second row reports the reconstruction results for various noise levels when the sought solution is piecewise constant ; we use the penalty function @xmath12 given in ( [ eq : tv ] ) with @xmath360 .",
    "when the 1d tv - denoising algorithm ` fista ` in @xcite is used to solve the minimization problems associated with this @xmath12 , it is terminated as long as the number of iterations exceeds @xmath361 or the error between two successive iterates is smaller than @xmath362 . during these computations ,",
    "we use @xmath363 and the parameters @xmath364 , @xmath365 and @xmath366 in algorithm [ alg1 ] .",
    "the computational times for the first row are @xmath367 , @xmath368 and @xmath369 seconds respectively , and the computation times for the second row are @xmath370 , @xmath371 and @xmath372 seconds respectively .",
    "this shows that algorithm [ alg1 ] indeed is a fast method with the capability of capturing special features of solutions .",
    "let @xmath373\\times [ 0,1]$ ] .",
    "we consider the problem of determining the source term @xmath374 in the poisson equation @xmath375 from an @xmath376 measurement @xmath377 of @xmath378 with @xmath379 .",
    "this problem takes the form ( [ 1.1 ] ) if we define @xmath380 , where @xmath381 is an isomorphism .",
    "the information on @xmath21 can be obtained by solving the equation .    in order to solve the poisson equation numerically ,",
    "we take @xmath382 grid points @xmath383 on @xmath304 , and write @xmath384 for @xmath385 and @xmath386 for @xmath387 . by the finite difference representation of @xmath388 , the poisson equation has the discrete form @xmath389 where @xmath390 . since @xmath391 on @xmath392 , the discrete sine transform can be used to solve ( [ 10.29.1 ] ) .",
    "consequently @xmath384 can be determined by the inverse discrete sine transform ( @xcite ) @xmath393 for @xmath394 , where @xmath395 and @xmath396 is determined by the discrete sine transform @xmath397 let @xmath398 .",
    "then @xmath399 can be determined by solving the equation @xmath400 . when applying algorithm [ alg1 ] , we need to determine @xmath401 for various @xmath314 and vectors @xmath402 .",
    "this can be computed as @xmath403 where , for any vector @xmath404 , @xmath405 and @xmath406 can be implemented by the fast sine and inverse sine transforms respectively , while @xmath407_{i , j}= { \\bf w}_{i , j}/(\\a + h^4/(4 - 2\\cos(ih\\pi)-2\\cos(jh\\pi))^2)\\ ] ] therefore @xmath408 can be computed efficiently .",
    "we apply algorithm [ alg1 ] to reconstruct the source term which is assumed to be piecewise constant . in our computation",
    "we use a noisy data with noise level @xmath409 .",
    "the left plot in figure [ fig4 ] is the exact solution . the right plot in figure [ fig4 ]",
    "is the reconstruction result by algorithm [ alg1 ] using initial guess @xmath410 and the penalty function @xmath411 where @xmath412 is the frobenius norm of @xmath399 and @xmath413 denotes the discrete isotropic tv defined by ( @xcite ) @xmath414 in each step of algorithm [ alg1 ] , the minimization problem associate with @xmath12 is solved by performing 400 iterations of the 2d tv - denoising algorithm ` fista ` in @xcite . in our computation , we use @xmath415 , and for those parameters in algorithm [ alg1 ] , we take @xmath364 , @xmath416 , @xmath417 and @xmath418 . when using rule [ alpha ] to choose @xmath49 we take @xmath419 , @xmath420 , @xmath421 and @xmath422 .",
    "the reconstruction result indicates that our method succeeds in capturing the feature of the solution .",
    "moreover , the computation terminates after @xmath423 iterations and takes 11.7092 seconds .      blurring in images",
    "can arise from many sources , such as limitations of the optical system , camera and object motion , astigmatism , and environmental effects ( @xcite ) .",
    "image deblurring is the process of making a blurry image clearer to better represent the true scene .",
    "we consider grayscale digital images which can be represented as rectangular matrices of size @xmath424 .",
    "let @xmath425 and @xmath426 denote the true image and the blurred image respectively .",
    "the blurring process can be described by an operator @xmath427 such that @xmath428 .",
    "we consider the case that the model is shift - invariant and @xmath429 is linear . by stacking the columns of @xmath425 and @xmath426",
    "we can get two long column vectors @xmath430 and @xmath431 of length @xmath432 . then there is a large matrix @xmath433 such that @xmath434 . considering the appearance of unavoidable random noise",
    ", one in fact has @xmath435 where @xmath436 denotes the noise . the blurring matrix @xmath437 is determined by the point spread function ( psf ) @xmath438the function that describes the blurring and the resulting image of the single bright pixel ( i.e. point source ) .    throughout this subsection ,",
    "periodic boundary conditions are assumed on all images .",
    "then @xmath437 is a matrix which is block circulant with circulant blocks ; each block is built from @xmath438 .",
    "it turns out that @xmath437 has the spectral decomposition @xmath439 where * f * is the two - dimensional unitary discrete fourier transform matrix and @xmath440 is the diagonal matrix whose diagonal entries are eigenvalues of @xmath437 .",
    "the diagonal matrix @xmath440 is easily determined by the smaller matrix @xmath438 , and the action of @xmath441 and @xmath442 can be realized by ` fft ` and ` ifft ` .",
    "therefore , for any @xmath443 and @xmath314 , @xmath444 is easily computable by the fast fourier transform .    in the following we perform some numerical experiments by applying algorithm [ alg1 ] to deblur various corrupted images . in our simulations the exact data @xmath431",
    "are contaminated by random noise vectors @xmath436 whose entries are normally distributed with zero mean .",
    "we use @xmath445 to denote the relative noise level . when applying algorithm [ alg1 ] , we use @xmath446 and the following two convex functions @xmath447 for those parameters in the algorithm we take @xmath448 , @xmath449 and @xmath418 . in each step of the algorithm , the minimization problem associate with @xmath12 is solved by performing 200 iterations of the algorithm ` fista ` in @xcite . when using rule [ alpha ] to choose @xmath49 we take @xmath450 , @xmath420 , @xmath451 and @xmath452 .",
    "in order to compare the quality of the restoration @xmath453 , we evaluate the peak signal - to - noise ratio ( psnr ) value defined by @xmath454 where @xmath455 denotes the maximum possible pixel value of the true image @xmath425 .",
    "in figure [ fig5 ] we plot the restoration results of the shepp - logan phantom of size @xmath456 which is blurred by a @xmath457 gaussian psf with standard derivation @xmath458 and is contaminated by gaussian white noise with relative noise level @xmath459 .",
    "the original and blurred images are plotted in ( a ) and ( b ) of figure [ fig5 ] respectively . in figure [ fig5 ] ( c )",
    "we plot the restoration result by algorithm [ alg1 ] with @xmath460 . with such chosen @xmath12 ,",
    "the method in algorithm [ alg1 ] reduces to the classical nonstationary iterated tikhonov regularization ( [ nsit ] ) which has the tendency to over - smooth solutions .",
    "the plot clearly indicates this drawback because of the appearance of the ringing artifacts .",
    "the corresponding psnr value is @xmath461 . in figure [ fig5 ] ( d )",
    "we plot the restoration result by algorithm [ alg1 ] with @xmath462 . due to the appearance of the total variation term in @xmath463 ,",
    "the artifacts are significantly removed .",
    "in fact the corresponding psnr value is @xmath464 ; the computation terminates after @xmath465 iterations and takes @xmath466 seconds .    in figure [ fig6 ]",
    "we plot the restoration results of the @xmath467 cameraman image corrupted by a @xmath468 linear motion kernel generated by ` fspecial(motion,30,40 ) ` and a gaussian white noise with relative noise level @xmath469 .",
    "the original and blurred images are plotted in ( a ) and ( b ) of figure [ fig6 ] respectively . in ( c ) and ( d ) of figure [ fig6 ] we plot the restoration results by algorithm [ alg1 ] with @xmath460 and @xmath470 respectively .",
    "the plot in ( c ) contains artifacts that degrade the visuality , the plot in ( d ) removes the artifacts significantly .",
    "in fact the psnr values corresponding to ( c ) and ( d ) are @xmath471 and @xmath472 respectively . the computation for ( d ) terminates after @xmath473 iterations and takes @xmath474 seconds .",
    "we finally present some numerical simulations for nonlinear inverse problems by solving the autoconvolution equation @xmath475 defined on the interval @xmath476 $ ] .",
    "the properties of the autoconvolution operator @xmath477(t ) : = \\int_0^t x(t - s ) x(s ) ds$ ] have been discussed in @xcite . in particular , as an operator from @xmath350 $ ] to @xmath350 $ ] , @xmath329 is frchet differentiable ; its frchet derivative and the adjoint are given respectively by @xmath478(t )   = 2 \\int_0^t x(t - s ) v(s ) ds , \\quad v \\in l^2[0,1 ] , \\\\ \\left[f'(x)^ * w\\right](s ) = 2 \\int_s^1 w(t ) x(t - s ) dt , \\quad",
    "w\\in l^2[0,1].\\end{aligned}\\ ] ]    we assume that ( [ autoconv ] ) has a piecewise constant solution and use a noisy data @xmath16 satisfying @xmath479}=\\d$ ] to reconstruct the solution . in figure [ fig7 ]",
    "we report the reconstruction results by algorithm [ alg2 ] using @xmath480 and the @xmath12 given in ( [ eq : tv ] ) with @xmath481 .",
    "all integrals involved are approximated by the trapezoidal rule by dividing @xmath353 $ ] into @xmath354 subintervals of equal length . for those parameters involved in the algorithm ,",
    "we take @xmath482 , @xmath483 and @xmath484 .",
    "we also take the constant function @xmath485 as an initial guess .",
    "the sequence @xmath49 is selected by rule [ alpha ] with @xmath21 replaced by @xmath486 in which @xmath487 , @xmath420 , @xmath357 and @xmath488 . when the 1d - denoising algorithm ` fista ` in @xcite is used to solve the minimization problems associated with @xmath12 , it is terminated as long as the number of iterations exceeds @xmath489 or the error between two successive iterates is smaller than @xmath490 .",
    "we indicate in figure [ fig7 ] the number of iterations and the computational time for various noise levels @xmath491 ; the results show that algorithm [ alg2 ] is indeed a fast method for this problem .",
    "we proposed a nonstationary iterated method with convex penalty term for solving inverse problems in hilbert spaces .",
    "the main feature of our method is its splitting character , i.e. each iteration consists of two steps : the first step involves only the operator from the underlying problem so that the hilbert space structure can be exploited , while the second step involves merely the penalty term so that only a relatively simple strong convex optimization problem needs to be solved .",
    "this feature makes the computation much efficient . when the underlying problem is linear , we proved the convergence of our method in the case of exact data ; in case only noisy data are available , we introduced a stopping rule to terminate the iteration and proved the regularization property of the method .",
    "we reported various numerical results which indicate the good performance of our method .",
    "q. jin is partially supported by the decra grant de120101707 of australian research council and x. lu is partially supported by national science foundation of china ( no . 11101316 and no .",
    "91230108 ) .",
    "999                    h. gfrerer , _ an a posteriori parameter choice for ordinary and iterated tikhonov regularization of ill - posed problems leading to optimal convergence rates _ , math . comp . , 49(180 ) : 507522 , s5s12 , 1987 ."
  ],
  "abstract_text": [
    "<S> in this paper we consider the computation of approximate solutions for inverse problems in hilbert spaces . in order to capture the special feature of solutions , </S>",
    "<S> non - smooth convex functions are introduced as penalty terms . by exploiting the hilbert space structure of the underlying problems , </S>",
    "<S> we propose a fast iterative regularization method which reduces to the classical nonstationary iterated tikhonov regularization when the penalty term is chosen to be the square of norm . </S>",
    "<S> each iteration of the method consists of two steps : the first step involves only the operator from the problem while the second step involves only the penalty term . </S>",
    "<S> this splitting character has the advantage of making the computation efficient . in case </S>",
    "<S> the data is corrupted by noise , a stopping rule is proposed to terminate the method and the corresponding regularization property is established . finally , we test the performance of the method by reporting various numerical simulations , including the image deblurring , the determination of source term in poisson equation , and the de - autoconvolution problem . </S>"
  ]
}