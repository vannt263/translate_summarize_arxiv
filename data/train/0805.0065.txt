{
  "article_text": [
    "what is the intrinsic connection between correlated random variables ? how much interaction is necessary to create correlation ?",
    "many fruitful efforts have been made to quantify correlation between two random variables .",
    "each quantity is justified by the operational questions that it answers .",
    "covariance dictates the mean squared error in linear estimation .",
    "shannon s mutual information is the descriptive savings from side information in lossless source coding and the additional growth rate of wealth due to side information in investing .",
    "gcs and krner s common information @xcite is the number of common random bits that can be extracted from correlated random variables .",
    "it is less than mutual information .",
    "wyner s common information @xcite is the number of common random bits needed to generate correlated random variables and is greater than mutual information .",
    "this work provides a fresh look at two of these quantities  mutual information and wyner s common information ( herein simply `` common information '' ) .",
    "both are extreme points of the channel simulation problem , introduced as follows : an observer ( _ encoder _ ) of an i.i.d .",
    "source @xmath0 describes the sequence to a distant random number generator ( _ decoder _ ) that produces @xmath1 ( see figure [ figure channel simulation ] ) .",
    "what is the minimum rate of description needed to achieve a joint distribution that is statistically indistinguishable ( as measured by total variation ) from the distribution induced by putting the source through a memoryless channel ?",
    "[ ] [ ] [ 0.8]@xmath2 [ ] [ ] [ 0.8]@xmath3 [ ] [ ] [ 0.8]@xmath4 bits [ ] [ ] [ 0.8]@xmath5 [ ] [ ] [ 0.8]@xmath6 [ ] [ ] [ 1.0]channel simulator : @xmath7   and @xmath8 .",
    "the first processor , @xmath9 , observes @xmath10 and the second processor , @xmath8 , generates @xmath11 after receiving a message at rate @xmath12 from @xmath9 .",
    "the minimum rate needed is the common entropy of @xmath10 and @xmath11.,title=\"fig:\",width=288 ]    channel simulation is a form of random number generation . the variables @xmath2 come from an external source and @xmath6 are generated to be correlated with @xmath2 .",
    "the channel simulation is successful if the total variation between the resulting distribution of @xmath13 and the i.i.d .",
    "distribution that would result from passing @xmath2 through a memoryless channel is small .",
    "this is a strong requirement .",
    "it s stricter than the requirement that @xmath13 be jointly typical as in the coordinated action work of cover and permuter @xcite .",
    "this total variation requirement means that any hypothesis test that a statistician comes up with to determine whether @xmath2 was passed through a real memoryless channel or the channel simulator will be virtually useless .",
    "wyner s result implies that in order to generate @xmath2 and @xmath6 separately as an i.i.d .",
    "source pair they must share bits at a rate of at least the common information @xmath14 of the joint distribution . in the channel simulation problem",
    "these shared bits come in the form of the description of @xmath2 .",
    "however , the `` reverse shannon theorem '' of bennett and shor @xcite suggests that a description rate of the mutual information @xmath15 of the joint distribution is all that is needed to successfully simulate a channel . how can we resolve this apparent contradiction",
    "the work of bennett and shor assumes that common random bits , or _",
    "common randomness _",
    ", independent of the source @xmath2 are available to the encoder and decoder . in that setting ,",
    "the common randomness provides a second connection between the source @xmath2 and output @xmath6 , in addition to the description of @xmath2 . remarkably , even though it is independent from the source @xmath2 , the common randomness assists in generating correlated random numbers and allows for description rates smaller than the common information @xmath14 .    in this work ,",
    "we characterize the tradeoff between the rate of available common randomness and the required description rate for simulating a discrete memoryless channel for a fixed input distribution , as in figure [ figure channel simulation with common randomness ] .",
    "indeed , the tradeoff region of section [ section main result ] confirms the two extreme cases .",
    "if the encoder and decoder are provided with enough common randomness , sending @xmath15 bits per symbol suffices . on the other hand , in the absence of common randomness one must spend @xmath14 bits per symbol .",
    "[ ] [ ] [ 0.8]@xmath2 [ ] [ ] [ 0.8]@xmath3 [ ] [ ] [ 0.8]@xmath16 bits [ ] [ ] [ 0.8]@xmath5 [ ] [ ] [ 0.8]@xmath6 [ ] [ ] [ 1.0]channel simulator : @xmath7 [ ] [ ] [ 0.8]@xmath17 bits   and @xmath8 .",
    "the first processor , @xmath9 , observes @xmath10 and common randomness independent of @xmath10 at rate @xmath18 .",
    "the second processor , @xmath8 , generates @xmath11 based on the common randomness and a message at rate @xmath19 from @xmath9.,title=\"fig:\",width=288 ]    this result has implications in cooperative game theory , reminiscent of the framework investigated in @xcite .",
    "suppose a team shares the same payoff in a repeated game setting .",
    "an opponent tries to anticipate and exploit patterns in the team s combined actions , but a secure line of communication is available to help them coordinate .",
    "of course , each player could communicate his randomized actions to the other players , but this is an excessive use of communication .",
    "a memoryless channel is a useful way to coordinate their random actions .",
    "thus , common information is found in section [ section game theory ] to be the significant quantity in this situation .",
    "we represent random variables as capital letters , @xmath10 , and their alphabets are written in script , @xmath20 .",
    "sequences , @xmath21 are indicated with a superscript @xmath2 .",
    "distribution functions , @xmath22 , are usually abbreviated as @xmath23 when there is no confusion .",
    "accented variables , @xmath24 , indicate different variables for each accent , but their alphabets are all the same , @xmath20 .",
    "similarly , distribution functions written with an accent or different letter , such as @xmath23 versus @xmath25 , represent different distributions .",
    "markov chains , satisfying @xmath26 , are represented with dashes , @xmath27 .",
    "( wyner s ) common information : @xmath28 conditional common information : @xmath29 total variation distance : @xmath30      a source @xmath2 is distributed i.i.d . according to @xmath31 . a description of the source at rate @xmath19 is represented by @xmath32 .",
    "a random variable @xmath33 , uniformly distributed on @xmath34 and independent of @xmath2 , represents the common random bits at rate @xmath18 known at both the encoder and decoder .",
    "the decoder generates a channel output @xmath6 based only on @xmath35 and @xmath33 .",
    "the channel being simulated has a the conditional distribution @xmath7 , thus the _ desired joint distribution _ is @xmath36 .",
    "a @xmath37 _ channel simulation code _ consists of a randomized encoding function , @xmath38 and a randomized decoding function , @xmath39 the description @xmath35 equals @xmath40 , and the channel output @xmath6 equals @xmath41 .    since randomized functions are specified by conditional probability distributions , it is equivalent to say that a @xmath37 channel simulation code consists of a conditional probability mass function @xmath42 with the properties that @xmath43 , @xmath44 , and @xmath45 .",
    "the _ induced joint distribution _ of a @xmath37 channel simulation code is the joint distribution on the quadruple @xmath46 . in other words , it is the probability mass function , @xmath47 where @xmath48 by construction .",
    "a sequence of @xmath37 channel simulation codes for @xmath49 is said to _ achieve _",
    "@xmath7 if the induced joint distributions have marginal distributions @xmath50 that satisfy @xmath51    a rate pair @xmath52 is said to be _ achievable _ if there exists a sequence of @xmath37 channel simulation codes that achieves @xmath7 .",
    "the _ simulation rate region _ is the closure of achievable rate pairs @xmath53 .",
    "[ theorem main result ] for an i.i.d .",
    "source with distribution @xmath31 and a desired memoryless channel with conditional distribution @xmath7 , the simulation rate region is the set , @xmath54 where @xmath55",
    "two extreme points of the simulation rate region @xmath56 fall directly from its definition . if @xmath57 , the second inequality in ( [ definition s ] ) dominates .",
    "thus , the minimum rate @xmath19 is the common information @xmath14 .",
    "this coincides with the intuition provided by wyner s result in @xcite . at the other extreme , using the data processing inequality on the first inequality of ( [ definition s ] ) yields @xmath58 no matter how much common randomness is available , and this is achieved when @xmath59 .",
    "does nt necessary have to be as large as @xmath60 for @xmath61 to be in the simulation rate region .",
    "] source coding results and the coordinated action work of cover and permuter in @xcite illustrate that with a description rate of @xmath15 we can create a codebook of output sequences in such a way that we ll likely be able to find a jointly typical output sequence for each input sequence from the source .",
    "consequently , we can then randomize the codebook using common randomness to actually simulate the channel , as bennett and shor proved in @xcite .      for a bernoulli - half source @xmath10 ,",
    "let us demonstrate the simulation rate region for the binary erasure channel .",
    "@xmath11 is an erasure with probability @xmath62 and is equal to @xmath10 otherwise .",
    "the distributions in @xmath63 that produce the boundary of the simulation rate region are formed by cascading two binary erasure channels as shown in figure [ figure erasure test channel ] , where @xmath64 , \\nonumber \\\\ p_1 & = & 1 - \\frac{1-p_e}{1-p_2}. \\nonumber\\end{aligned}\\ ] ] the mutual information terms in ( [ definition s ] ) become @xmath65 where @xmath66 is the binary entropy function .",
    "figure [ figure bec 075 tradeoff ] shows the boundary of the simulation rate region for erasure probability @xmath67 .",
    "the required description rate @xmath19 varies from @xmath68 bits to @xmath69 bits as the rate of common randomness runs between @xmath70 and @xmath71 bits .",
    "[ ] [ ] [ 1.0]@xmath10 [ ] [ ] [ 1.0]@xmath72 [ ] [ ] [ 1.0]@xmath11 [ ] [ ] [ 0.8]@xmath70 [ ] [ ] [ 0.8]@xmath73 [ ] [ ] [ 0.8]@xmath74 [ ] [ ] [ 0.8]@xmath74 [ ] [ ] [ 0.8]@xmath75 [ ] [ ] [ 0.8]@xmath75 [ ] [ ] [ 0.8]@xmath70 [ ] [ ] [ 0.8]@xmath76 [ ] [ ] [ 0.8]@xmath73    that give the boundary of the simulation rate region for the binary erasure channel with a bernoulli - half input are formed by cascading two erasure channels.,title=\"fig:\",width=240 ]    [ ] [ ] [ 0.9]bec simulation rate region , @xmath67 [ ] [ ] [ 0.8]@xmath15 [ ] [ ] [ 0.8]@xmath14    and a bernoulli - half input , where @xmath19 is the description rate and @xmath18 is the rate of common randomness . without common randomness ,",
    "a description rate of @xmath14 is required to simulate the channel . with unlimited common randomness , a description rate of @xmath15 suffices.,title=\"fig:\",width=288 ]",
    "let @xmath53 be an achievable rate pair .",
    "then for each @xmath77 there exists a @xmath37 channel simulation code with an induced joint distribution @xmath78 such that @xmath79 let the random variable @xmath80 be uniformly distributed over the set @xmath81 .",
    "the variable @xmath80 will serve as a random time index .",
    "the joint distribution of the sequences @xmath13 is close in total variation to an i.i.d .",
    "distribution , so we can extend lemma 2.7 of @xcite to obtain two bounds : @xmath82 where @xmath83 notice that @xmath84 .",
    "define an epsilon rate region , @xmath85 where @xmath86    [ lemma epsilon rate region ]",
    "@xmath87    we use familiar information theoretic inequalities , and the fact that @xmath2 and @xmath33 are independent , to bound @xmath19 and the sum rate @xmath88 .",
    "@xmath89 we then lower bound the r.h.s . of ( [ equation ixij ] ) and ( [ equation ixyij ] ) using similar steps . here",
    "we proceed from ( [ equation ixyij ] ) .",
    "@xmath90 the second inequality comes from ( [ equation entropy bound ] ) , and the last inequality comes from ( [ equation information bound ] ) .    the joint distribution of the pair @xmath91 can be shown to satisfy the total variation constraint in ( [ definition d epsilon ] ) . finally , we acknowledge the markovity of the triple @xmath92 to complete the proof of the lemma .",
    "( the cardinality bound of @xmath72 in ( [ definition d epsilon ] ) is shown to be satisfiable via a generalized caratheodory theorem . )",
    "the epsilon rate regions decrease to the simulation rate region as epsilon decreases to zero .",
    "[ lemma lower semi continuity ] @xmath93",
    "one key tool for the achievability proof is summarized in lemma [ lemma quantized input ] .",
    "this lemma is implied by the resolvability work of han and verd in @xcite , but the concept was first introduced by wyner in theorem 6.3 of @xcite .",
    "[ lemma quantized input ] for any discrete distribution @xmath94 and each @xmath95 , let @xmath96 be a `` codebook '' of sequences each independently drawn according to @xmath97 .    for a fixed codebook ,",
    "define the distribution @xmath98    then if @xmath99 , @xmath100 where the expectation is with respect to the randomly constructed codebooks @xmath101 .",
    "assume that @xmath52 is in the interior of @xmath56 .",
    "then there exists a distribution @xmath102 such that @xmath103 and @xmath104 .    for each @xmath95 , let @xmath105 be uniformly distributed on @xmath106 .",
    "we apply lemma [ lemma quantized input ] twice , once with @xmath107 and again with @xmath108 , to assert that there exists a sequence of `` codebooks '' @xmath109 , @xmath110 with the properties @xmath111 where @xmath112 and @xmath113 are marginal distributions derived from the joint distribution @xmath114    in an indirect way , we ve constructed a sequence of joint distributions @xmath115 from which we can derive channel simulation codes that achieve @xmath7 .",
    "the markovity of @xmath116 implies the markov property @xmath117 .",
    "let @xmath118 considering ( [ equation close joint distribution ] ) and ( [ equation close to independent ] ) with the properties of total variation and @xmath116 in mind , it can be shown that @xmath119 is a sequence of channel simulation codes that achieves @xmath7 .",
    "this channel simulation scheme requires randomization at both the encoder and decoder .",
    "in essence , a codebook of independently drawn @xmath120 sequences is overpopulated so that the encoder can choose one randomly from many that are jointly typical with @xmath2 .",
    "the decoder then randomly generates @xmath6 conditioned on @xmath120 .",
    "our framework finds motivation in a game theoretic setting .",
    "consider a zero - sum repeated game between two teams .",
    "team a consists of two players who on the @xmath121th iteration take actions @xmath122 and @xmath123 .",
    "the opponents on team b take combined action @xmath124 .",
    "all action spaces @xmath125 , and @xmath126 are finite .",
    "the payoff for team a at each iteration is a time - invariant finite function @xmath127 and is the loss for team b. each team wishes to maximize its time - averaged expected payoff .",
    "assume that team a plays conservatively , attempting to maximize the expected payoff for the worst - case actions of team b. then the payoff at the @xmath121th iteration is @xmath128.\\end{aligned}\\ ] ] clearly , ( [ loss function ] ) could be maximized by finding an optimal mixed strategy @xmath129 that maximizes @xmath130 $ ] and choosing independent actions each iteration .",
    "this would correspond to the minimax strategy .",
    "however , now we introduce a new constraint : the players on team a have a limited secure channel of communication .",
    "player 1 , who chooses the actions @xmath2 , communicates at rate @xmath12 to player 2 , who chooses @xmath6 .",
    "let @xmath72 be the message passed from player 1 to player 2 .",
    "we say a rate @xmath12 is achievable for payoff @xmath131 if there exists a sequence of random variable triples @xmath132 that each form markov chains for all @xmath133 . ]",
    "@xmath134 and such that @xmath135 and @xmath136 & > & \\theta.\\end{aligned}\\ ] ]    let @xmath137 be the infimum of achievable rates for payoff @xmath131 .",
    "we claim that @xmath137 is the least average common information of all combinations of strategies that achieve average payoff @xmath131 .",
    "define , @xmath138 \\right ] \\geq \\theta.\\end{aligned}\\ ] ]    [ theorem game theory ] @xmath139    _ converse sketch : _ + the important elements of the converse are the inequalities @xmath140 for all @xmath141 , where @xmath80 is uniformly distributed on @xmath81 .",
    "now identify the tuple @xmath142 as the auxiliary random variable @xmath143 .",
    "_ achievability comment : _ + the random variable @xmath143 serves as a time sharing variable to combine strategies of high and low correlation .",
    "the author would like to thank his advisor , tom cover , for encouraging the study of coordination via communication , and young - han kim for his suggestions and encouragement .",
    "this work is supported by the national science foundation through grants ccf-0515303 and ccf-0635318 .",
    "p. gcs and j. krner , `` common information is far less than mutual information , '' _ problems of control and info .",
    "149 - 162 , 1973 .",
    "a. wyner , `` the common information of two dependent random variables , '' _ ieee trans .",
    "info . theory _ ,",
    "it-21 , no .",
    "2 , march 1975 .",
    "t. cover and h. permuter , `` capacity of coordinated actions , '' isit 2007 , nice , france . c. h. bennett and p. w. shor , `` entanglement - assisted capacity of a quantum channel and the reverse shannon theorem , '' _ ieee trans .",
    "info . theory _ ,",
    "v. anantharam and v. borkar , `` common randomness and distributed control : a counterexample , '' _ systems and control letters _",
    "7 - 8 , july 2007 .",
    "i. csiszr and j. krner , _ information theory : coding theorems for discrete memoryless systems .",
    "_ new york : academic , 1981 .",
    "t. s. han and s. verd , `` approximation theory of output statistics , '' _ ieee trans .",
    "info . theory _ ,",
    "3 , may 1993 ."
  ],
  "abstract_text": [
    "<S> two familiar notions of correlation are rediscovered as extreme operating points for simulating a discrete memoryless channel , in which a channel output is generated based only on a description of the channel input . </S>",
    "<S> wyner s `` common information '' coincides with the minimum description rate needed . </S>",
    "<S> however , when common randomness independent of the input is available , the necessary description rate reduces to shannon s mutual information . </S>",
    "<S> this work characterizes the optimal tradeoff between the amount of common randomness used and the required rate of description . </S>"
  ]
}