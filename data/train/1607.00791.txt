{
  "article_text": [
    "recent years witnessed huge progress in deep learning architecture driven mostly by abundance of training data and huge performance of parallel gpu processing units @xcite@xcite .",
    "this sets a new path in a development of intelligent systems and ultimately puts us on a track to general artificial intelligence solutions .",
    "it is worth noting that in addition to well - established convolutional neural networks ( cnn ) architectures there is a set of biologically inspired solutions such as hierarchical temporal memories @xcite@xcite@xcite .",
    "those architectures as well as cnns suffer from lack of well - defined mathematical formulation of rules for their efficient hardware implementation .",
    "large range of heuristics and rules of thumb are used instead .",
    "this was not very harmful except for a long training time when most of the algorithm were executed on cpus without hardware acceleration .",
    "however , nowadays most of biologically inspired are ported to hardware for a sake of performance efficiency @xcite@xcite .",
    "this in turn requires a profound consideration and analysis of resources consumption to be able to predict both the capacity of the platform and the ultimate performance of the system .",
    "consequently , the authors of the papers analyzed htm design constrains on the mathematical ground and formulated a set of directives for building hardware modules .",
    "the rest of the paper is organized as follows .",
    "section [ section : htm ] provides the background and related works .",
    "section [ section : mathematical_formalism ] describes mathematical formalism of spatial pooler .",
    "finally , we present our conclusions in section [ section : conclusions ] .",
    "hierarchical temporal memory ( htm ) replicates the structural and algorithmic properties of the neocortex @xcite .",
    "it can be regarded as a memory system which is not programmed and it is trained through exposing them to data i.e. text .",
    "htm is organized in the hierarchy which reflects the nature of the world and performs modeling by updating the hierarchy .",
    "the structure is hierarchical in both space and time , which is the key in natural language modeling since words and sentences come in sequences which describe cause and effect relationships between the latent objects .",
    "htms may be considered similar to bayesian networks , hmm and recurrent neural networks , but they are different in the way hierarchy , model of neuron and time is organized .    at any moment in time ,",
    "based on current and past input , an htm will assign a likelihood that given concepts are present in the examined stream .",
    "the htm s output constitutes a set of probabilities for each of the learned causes .",
    "this moment - to - moment distribution of possible concepts ( causes ) is denoted as a belief .",
    "if the htm covers a certain number of concepts it will have the same number of variables representing those concepts .",
    "typically htms learn about many causes and create a structure of them which reflects their relationships .    even for human beings ,",
    "discovering causes is considered to be a core of perception and creativity , and people through course of their life learn how to find causes underlying objects in the world . in this sense htms",
    "mimic human cognitive abilities and with a long enough training , proper design and implementation , they should be able to discover causes humans can find difficult or are unable to detect .",
    "htm infers concepts of new stream elements and the result is a distribution of beliefs across all the learned causes . if the concept ( e.g. one of the categories occurring in the examined stream ) is unambiguous , the belief distribution will be peaked otherwise it will be flat . in htms it is possible to disable learning after training and still do inference .",
    "the role of an encoder within htm processing flow is critical .",
    "it maps data from various representations to sparse distributed representation ( sdr ) which is an internal form of holding data within htm network .",
    "quality of a transformation process directly affect the performance of the implemented system .",
    "the conversion involves mapping to strongly semantic - oriented way of representing data in which a single bit holds a meaning of tens or hundreds of original representation bits .",
    "there are various kinds of encoders for different input data @xcite .",
    "sp operates in the spatial domain and acts as an advanced encoder which translates from binary representation to the sparse distributed binary representation of approx .",
    "3% density .",
    "sp is also an interface between the encoder and the remaining part of the htm module fig .",
    "[ fig : sp_architecure ] .",
    "the detailed implementation of the algorithm is as follows :    * each column is connected by a fixed number of inputs to randomly selected node inputs .",
    "based on the input pattern , some columns will receive more active input values , * inputs of columns ( synapses ) have values ( floating point between 0 and 1 called permanence value ) which represents possibility of activating the synapse ( if the value is greater than 0.5 and corresponding input is 1 the synapse is active ) , * columns which have more active connected synapses than given threshold ( minoverlap ) and its overlap parameter ( number of active synapses ) is better than k - th overlap of set of columns in spectrum of inhibition radius , * during learning process columns gather information about their history of activity , overlap values ( if overlap was greater than minoverlap or not ) , compute minimum and maximum overlap duty cycle and then decides according to the combinations of these parameters if their permanence values or inhibition radius should be changed .",
    "there are two main parameters of spatial pooler which mimic the behaviour of the mammalian brain , i.e. permvalue and inhibitionradius .",
    "the first one reflects the sensitivity of the brain to external stimuli , and the latter one may be considered a focus . in the process of adjusting those parameters , sp adapts to the profile of the input data .",
    "generally , spatial pooling selects a relatively constant number of the most active columns and inactivates ( inhibits ) other columns in the vicinity of the active ones .",
    "similar input patterns tend to activate a stable set of columns .",
    "the classifier module based on spatial pooler is realized by overlap and activation computing on incoming input values .",
    "the rest of the parameters are set during learning process .",
    "the functionality of spatial pooler is similar to lvq or self organizing maps neural models .",
    "[ alg:1 ]    col.overlap @xmath0 0 ; col.overlap @xmath0 col.overlap * col.boost ;    [ alg:2 ]    col.active @xmath0 1 ; col.active @xmath0 0 ;    [ alg:3 ]    syn.perm_value @xmath0 min(1 , syn.perm_value + syn.perm_inc ) ; syn.perm_value @xmath0 max(0 , syn.perm_value - syn.perm_dec ) ;    [ alg:4 ]    syn.perm_value @xmath0 min(1 , syn.perm_value + 0.1 * min_perm_value ) ;    sp.update_inhibition_radius ( )",
    "this section covers properties and important features of sdr ( sparse distributed representation ) vector space .",
    "@xmath1    where @xmath2 and @xmath3 are sdr vector space and its dimension respectively .    depending on the context",
    ", the vectors are called points , patterns or words .",
    "this section covers htm encoder formalism .",
    "it is mostly based on @xcite .    * @xmath4 - an arbitrary input set , * @xmath5 - element of the input space @xmath4 , * @xmath6 - sdr ( sparse distributed representation ) of length @xmath3 and @xmath7 bits on ( egual @xmath8 ) , * @xmath9 - element of sdr space @xmath2 , * @xmath3 - total number of bits in a vector of sdr ( @xmath9 ) , * @xmath7 - number of active bits in a vector of sdr ( @xmath9 ) , * @xmath10 - number of buckets ( ranges ) to which @xmath11 is mapped @xmath2 , * @xmath12 - min value of the input space range , * @xmath13 - max value of the input space range .",
    "both input space and sdr space are decent metric spaces which means that they meet metric space postulates @xcite .",
    "* @xmath14 - input space * @xmath15 - input space metric * @xmath16 - sdr space * @xmath17 - sdr space metric      @xmath18    @xmath19    @xmath20    @xmath21    @xmath22      @xmath23    @xmath24    @xmath25    @xmath26    @xmath27      input and sdr spaces are correlated ( eq.[eq : input_and_sdr_relationship ] ) .",
    "@xmath28      there are several important aspects which must be considered in a process of encoding data :    * @xmath29 , * @xmath30 , * @xmath31 * @xmath32      the following notation prerequisites are adopted :    @xmath33    the number of buckets is given by eq .",
    "[ eq : number_of_bucket ] .",
    "@xmath34    the function mapping from the input space to sdr is expressed by eq .",
    "[ eq : mapping_function ] .",
    "@xmath35    input space is limited by the range is given by eq .",
    "[ eq : input_space_range ] .",
    "@xmath36    formula for a simple numbers encoder is given by eq .",
    "[ eq : number_encoder ] and [ eq : number_encoder_ranges ] .",
    "@xmath37    @xmath38 = 0\\\\ \\bigwedge_{i \\in \\langle 0 , k + w\\rangle } ,    & s[i ] = 1\\\\ \\end{array}\\right .",
    "\\label{eq : number_encoder_ranges}\\ ] ]      this section will concentrate on mathematical formalism of spatial pooler .",
    "the functionality of spatial pooler can be described in a vector and matrix representation , this format of data can improve the efficiency of the operations . the previous section covered encoder formalism definition which is critical since input data for the spatial pooler are generated at this stage .",
    "the quality of the data directly affects performance of sp . in article vectors",
    "are defined by lowercase names with an arrow hat ( the transpose of the vector will produce a column vector ) .",
    "all matrices will be uppercase .",
    "subscripts on vectors and matrices are presented as right bottom indexes to denote internal elements ( e.g. @xmath39 refers to element in @xmath40 row and @xmath41 column ) .",
    "element wise operations are defined by @xmath42 and @xmath43 operators .",
    "the i(k ) function is indicator function that returns 1 if event k given as a parameter is true and 0 otherwise .",
    "the input of this function can be matrix or vector , then the output is matrix or vector , respectively .",
    "the user - defined input parameters are defined in ( table i ) .",
    "these are parameters that must be defined before the initialization of the algorithm .",
    ".sp symbols . [",
    "cols=\"^,^\",options=\"header \" , ]     the terms @xmath9 , @xmath44 , @xmath40 , @xmath41 and @xmath45 are integer indices used in article .",
    "theirs vaues are bounded as follows : @xmath46 .",
    "+      competitive learning networks have typically each node fully connected to each input .",
    "the other architectures and techniques like self organizing maps , stochastic gradient networks have single input connected to single node . in spatial pooler",
    "the inputs connecting to a particular column are determined randomly .",
    "the density of inputs visible by spatial pooler can be computed by using input parameters which defines sp architecture .",
    "these rules and dependencies formulas will be described in this section .",
    "let @xmath47 be the vector of columns indices .",
    "the @xmath48 where @xmath49 is the column s index at @xmath40 .",
    "let @xmath50 be the set of input patterns , such that @xmath51 indicates @xmath9 index of @xmath44 pattern .",
    "+ let column synapse index @xmath52 be the source column indices for each proximal synapse on each column , such that @xmath53 is the source column s @xmath48 index of proximal synapse at index @xmath45 ( @xmath53 refers to specific index in @xmath54 ) .",
    "+ for hardware implementation purpose and its memory limitations the modifications to formal description were done .",
    "the following changes parameters were defined :    * @xmath55 - central index of input to which column @xmath40 is assign * radius - spectrum from @xmath55 in which the indexes of synapses inputs are randomly selected * @xmath56    given @xmath57 and @xmath58 , the probability of a single input @xmath51 , connecting to a column is calculated by using [ eq : prob ] . in [ eq : prob ] , the probability of an input not connecting is first determined .",
    "that probability is independent for each input .",
    "the total probability of a connection not being formed is simply the product of those probabilities .",
    "the probability of a connection forming is therefore the complement of the probability of a connection not forming .",
    "+ @xmath59    the number of columns that are connected to single input ( [ eq : n ] ) : @xmath60    expected number of columns to which given input is connected ( [ eq : enc ] ) : @xmath61=m*p(i_{r}c_{i } ) \\label{eq : enc}\\ ] ]    the version of above equations for hardware implementation are as follows ( equations [ eq : probh ] , [ eq : nch ] and [ eq : nchol ] ) : + @xmath62\\\\0   & \\text{if }   r \\in \\\\ [ 0 , ccon_{i}-radius ] \\cup [ ccon_{i}+radius , p ] \\end{cases } \\label{eq : probh } \\end{split}\\ ] ]    @xmath63    @xmath61=m*p(i_{r}c_{i } ) \\label{eq : nchol}\\ ] ]    it is also possible to calculate the probability of an input never connecting [ eq : nvc ] .",
    "@xmath64    the probabilities of connecting or not connecting input to different columns are independent , it reduces to the product of the probability of a input not connecting to a column , taken over all columns ( [ eq : ncc ] and [ eq : encc ] ) .",
    "@xmath65    @xmath66=p*p(ncol_{r}==0 )    \\label{eq : encc}\\ ] ]    using equation [ eq : prob ] and equation [ eq : nvc ] , it is possible to obtain a lower bound for input parameters @xmath67 and @xmath57 , by choosing those parameters such way that a certain percentage of input visibility is obtained . to guarantee observance of all inputs , equation",
    "[ eq : ncc ] must be zero .",
    "the desired number of times an input is observed may be determined by using equation eq : enc .    after connecting columns to input",
    ", the permanences of synapses must be initialized .",
    "permanences were defined to be initialized with a random value close to @xmath68 .",
    "permanences should be randomly initialized , with approximately half of the permanences creating connected proximal synapses and the remaining permanences creating potential ( unconnected ) proximal synapses .",
    "let @xmath69 be defined as the set of permanences for each column , @xmath70 is set of permanences for proximal synapses for column @xmath48 .",
    "@xmath71 is initialized by formula [ eq : phi ] .",
    "expected permanence value is equal to @xmath68 .",
    "therefore @xmath72 synapses will be connected .",
    "the parameter @xmath73 should be less to give chance each column to be activated at the beginning of learning process .",
    "@xmath74    as initial parameters are set the activation process can be described by mathematical formulas .",
    "let @xmath75 is the set of inputs for each column , @xmath76 set of inputs for column @xmath48 .",
    "let @xmath77 be the random variable of number of active inputs on column @xmath40 .",
    "the average number of active inputs on a column is defined by : @xmath78 .",
    "the @xmath79 is defined as the probability of the input connected to column @xmath40 via proximal synapses .",
    "therefore expected number of active proximal synapses can be computed as follows [ eq : eac ] :    @xmath80=q*p(x_{i } ) \\label{eq : eac}\\ ] ]    let @xmath81 defines the event that proximal synapse @xmath45 is active and connected to column @xmath40 .",
    "random variable of number of active and connected synapses for column @xmath40 is define by [ eq : acon ] :    @xmath82    the probability that synapse is active and connected : @xmath83 .",
    "expected number of active and connected synapses for single column is defined as [ eq : eacon ] :    @xmath84=q*p(actcon_{i } )    \\label{eq : eacon}\\ ] ]    @xmath85 is the probability mass function of a binomial distribution ( @xmath45 number of successes , @xmath3 number of trials , @xmath58 success probability in each trial ) .",
    "number of columns with more active inputs than threshold [ eq : actc ] :    @xmath86    number of columns with more active and connected proximal synapses than threshold [ eq : actcol ] :    @xmath87    let @xmath88 be the mean of p(x ) and @xmath89 the mean of @xmath90 then by [ eq : pactcol ] and [ eq : eactc ] , the summation computes the probability of having less than @xmath73 active connected and active proximal synapses , where the individual probabilities within the summation follow the pmf of a binomial distribution . to obtain the desired probability ,",
    "the complement of that probability is taken .",
    "+   + @xmath91=m*[1-\\sum_{t=0}^{\\rho_{d}-1}bin(t , q,\\pi_{x } ) ]    \\label{eq : eactc}\\ ] ]    @xmath92=[1-\\sum_{t=0}^{\\rho_{d}-1}bin(t , q,\\pi_{ac } ) ]   \\label{eq : pactcol}\\ ] ]    @xmath93=m*[1-\\sum_{t=0}^{\\rho_{d}-1}bin(t , q,\\pi_{ac } ) ]   \\label{eq : eactcol}\\ ] ]      let @xmath94 and @xmath95 be the bit mask for proximal synapses connectivity where @xmath96 ( equation [ eq : consyn ] ) .",
    "+   + @xmath97 + let @xmath98 be the vector of boost values for each column ( @xmath99 is the boost value for @xmath100 column ) .",
    "the equation [ eq : ovlp ] computes @xmath101 , which is the number of synapses in connected state whose input is activated ( line 4 in [ alg:1 ] ) : +   + @xmath102 + then real overlap @xmath103 value based on boost factor can be computed for each column ( equation [ eq : rovlp ] ) .",
    "the value @xmath104 is greater then zero if @xmath105 is ( line 6 - 9 in [ alg:1 ] ) .",
    "+   + @xmath106 +      after computing overlap values for each column , the process of activating them is based on set of neighbours of the specified column ( defined by inhibition radius ) . therefore neighborhood mask matrix is performed as @xmath107 , where @xmath108 is the neighborhood of the @xmath100 column .",
    "each element in a matrix is indicator of event that column belongs to neighborhood or not ( 1 or 0 value ) . in case of further optimizations",
    "the matrix can be reduced to @xmath109 .",
    "let @xmath110 be the k - th largest value from set s and @xmath111 be the largest value from vector @xmath112 .",
    "the set of active columns can be represented by vector @xmath113 .",
    "the vector can be obtained by computing following formula ( line 2 in [ alg:2 ] , equation [ eq : gam ] ) : +   + @xmath114 + then ( line 3 - 7 in [ alg:2 ] , equation [ eq : ractcol ] ) : +   + @xmath115      learning phase consists of updating permanence values , inhibition radius and boosting factors updating and duty cycle parameters coputing .",
    "the permanence values of synapses are updated only when column is activated",
    ". therefore update of synapse can be defined as element wise multiplication of transposed vector of column activations and matrix of values of inputs connected to columns synapses .",
    "if inputs are active then permanences are increased by value @xmath116 otherwise decreased by @xmath117 ( line 6 and 8 in [ alg:3 ] , equations [ eq : dphi ] and [ eq : dphik ] ) .",
    "+   + @xmath118 + @xmath119 +   + the permanence values must been in [ 0,1 ] range . the following equation is a rule of updating final permanence values ( line 6 and 8 in [ alg:3 ] , equation [ eq : phiu ] ) : +   + @xmath120 + the clip function clips the permanence values in [ 0,1 ] range ( [ eq : clip ] ) .",
    "+   + @xmath121 + each column in learning phase updates activedutycycle parameter - @xmath122 .",
    "the set of these paramters is represented by vector @xmath123 .",
    "it is worth noticed that history of activation of the columns activation should be stored in a additional structure to remember and update duty cycle parameter in each cycle - @xmath124 , ( only set number of steps before should be remember , history parameter is sliding window width ) .",
    "the activedutycycle is computed as follows ( equation [ eq : acdc ] ) : +   + @xmath125 + the procedure of @xmath126 in each cycle can be done by organizing above matrice as cycle list . in each cycle",
    "the whole single column is updated .",
    "then the index to the column which will be update in next cycle is incremented by one .",
    "if the index will be greater then matrice width the index is set to 0 .",
    "the minimum active duty cycle @xmath127 is computed for boosting purposes by the following equation ( [ eq : mdc ] ) : +   + @xmath128 + the maximal active duty cycle of columns in neighbourhood is scaled by @xmath129 factor .",
    "+ the boost factor computation is base on @xmath130 parameters .",
    "the boost function should be used when @xmath131 .",
    "it should be monotonically decreasing due to @xmath132 ( equations [ eq : boost ] and [ eq : beta ] ) .",
    "+   + @xmath133 + @xmath134 + the next parameter @xmath135 is @xmath136 .",
    "it is computed by the same manner like @xmath137 . apart from activation indicators the overlap are used .",
    "the similar matrice of overlap history is used - @xmath138 .",
    "the permanence boosting definition is based on comparing @xmath135 < @xmath127 .",
    "if it is true than all input synapses permanence are increased by constant factor ( line 8 in [ alg:4 ] , equation [ eq : clipb ] ) .",
    "+   + @xmath139 + the original inhibition radius presented by numenta is based on distances between columns and active connected synapses ( equation [ eq : dist ] ) .",
    "equation [ eq : inh ] presents how inhibition is computed ( sum of distances divided by sum of connected synapses ) .",
    "the inhibition radius can be constant during learning phase or can be changed .",
    "it depends of sp mode . both modes",
    "what will be described later should converge to the stable value of inhibition radius or to value with minimal variance .",
    "+ @xmath140 @xmath141    it should be noticed that the spectrum of inhbition radius in case of hardware implementation can be shifted or decreased in some situations . in gpu",
    "when columns are processed by thread blocks , boundary threads compare theirs @xmath142 and @xmath137 in spectrum of reduced inhibition radius to avoid device memory synchronization @xcite . during initialization process",
    "the mean distance and inhibition is defined by equations [ eq : mdist ] and [ eq : inh ] :    @xmath143    @xmath144    the initial probability of column activation based on inhibition radius is defined by equation [ eq : racol ] .",
    "@xmath145    the probability of boosting at initial stage can be computed by equation [ eq : pboost ] .",
    "@xmath146      the spatial pooler output representation is in sparse format so number of active columns is @xmath147 ( equations [ eq : spc ] and [ eq : maxsp ] ) .",
    "+   + @xmath148 @xmath149 + as sp pattern is learnt we can estimate what input give the same output and its probability .",
    "the probability is equals to product of probabilities that for active columns for given pattern the overlap is greater or equal to @xmath150 ( equation [ eq : sim ] ) .",
    "+   + @xmath151 + the single probability can be computed by following equation ( [ eq : sprob ] ) : + @xmath152    the number of unique patterns that can be represented on @xmath3 bits with @xmath7 bits on is defined by ( equation [ eq : choose ] ) : +   + @xmath153 + then we can define the number of codings that can give similar output as learnt pattern by sp ( equation [ eq : probact ] is a number of input codings for actie columns and [ eq : probnonact ] is number of input codings for non active columns ) .",
    "+   + @xmath154    @xmath155     + the @xmath156 is the number of codings on input to synapses that are learnt zero bit pattern ( @xmath157 ) .",
    "the @xmath158 is number of codings that input has more bits on than minoverlap on synapses learnt for receiving bits with value 1 ( @xmath159 ) .      in this section",
    "the convergence of sp learning process will be described .",
    "we divided the process of learning sp to two different modes .",
    "the first one consists of learning each pattern seperately . in this case for each column @xmath160 the final state of sp after learning process should be as follows ( for @xmath161 ) : +    * @xmath162 + @xmath163 for @xmath164 + @xmath165 for @xmath166 + * @xmath167@xmath168 @xmath163 +    there are three possible starting states at the beginning of learning , the possible transisions from state to other state are as follows ( indicated by @xmath169 ) :    * not overlap @xmath169 ( @xmath161 ) permanence boosting @xmath169 ( overlap @xmath170 minoverlap ) if @xmath171 * overlap @xmath170 minoverlap & no activation @xmath172 overlap boosting ( activedutycycle value ) @xmath172 activation @xmath172 permanence updating * overlap @xmath170 minoverlap & activation @xmath172 permanence updating    it can be noticed that if there are more columns than @xmath45 parameter with overlap greater or equal than @xmath150 value in spectrum of constant inhibition radius than columns are in priority queue ( priority is activedutycycle ) in which they are will be activated in cyclic way ( because of overlap boosting ) .",
    "+ process of single pattern learning can be run further for next pattern . before this process",
    "learnt columns ( columns activated by learnt pattern ) should be blocked from permanence boosting ( avoid boosting of learnt synapses ) .",
    "the columns activated ( learnt ) by previous pattern can ba activated by new pattern only when overlap between inputs of patterns to this column is greater or equal @xmath150 .",
    "overlap function is defined as follows ( equation [ eq : overlap ] ) : + @xmath173 where : x and y are binary vectors .    in case of sp learning process of multiple patterns simultaneously there can exist some other situation which can speed up or slow down process of convergence .",
    "these all situation are mentioned below :    * detraction of 1 on single synapse when multiple patterns activate the same column with opposite input value on single synapse @xmath174 @xmath175 @xmath176 * p(detraction of 0)=(p(act=1)*p(synapse=1 ) + p(boost ) ) * attraction of 1 on single synapse when multiple patterns activate the same column with the same input value on single synapse @xmath174 @xmath175 @xmath177 * attraction of 0 on single synapse when multiple patterns activate the same column with the same input value on single synapse @xmath178 @xmath175 @xmath179    there are three possible situations during learning process ( multiple pattern learning with constant inhibition radius ) :    * permanence boosting of inputs of columns activated by different patterns , harmful effect but if duty cycle big enough ( almost bigger than number of patterns ) , inputs will be learned * attraction ( equations above ) - speeding up learning * detraction ( equations above ) - slowing down learning    in both presented situations ( single and multi pattern learning ) constant inhibition radius is used .",
    "according to the original numenta algorithm the fluctuations of inhibition radius should decrease during learning process @xcite , but there is possibility that inhibition radius never converge to constant value . in our approach",
    "the constant inhibition radius allows to show convergence of learning process .",
    "this situation can be achieved by stopping the inhibition radius changing after some learning steps or to change algorithm by the one with radius convergence to stable value .",
    "it should be noticed that values of inhibition radius and @xmath45 should guarantee sparse output at the end of learning .",
    "the presented htm model is a new architecture in deep learning domain inspired by human brain .",
    "initial results show @xcite that it can be at least efficient like other machine and deep learning models .",
    "additionally , our earlier research @xcite showed that it can be significantly speed up by hardware accelerators .",
    "the presented formalism is one of the first article with full mathematical description of htm spatial pooler .",
    "the formal description will help to parameterized the model . according to given encoder and its input distribution characteristic it is possible by formal model to estimate number of learning cycles , probability of patterns attraction , detraction etc .",
    "further work the should concentrate on extending the formalism by accurate proofs of convergence of learning process",
    ". then formal description of temporal pooler should be added .",
    "k. woodbeck , g. roth and huiqiong chen , `` visual cortex on the gpu : biologically inspired classifier and feature descriptor for rapid recognition , '' computer vision and pattern recognition workshops , 2008 .",
    "cvprw 08 .",
    "ieee computer society conference on , anchorage , ak , 2008 , pp",
    ". 1 - 8 .",
    "d. thomas and w. luk , `` fpga accelerated simulation of biologically plausible spiking neural networks , '' field programmable custom computing machines , 2009 .",
    "17th ieee symposium on , napa , ca , 2009 , pp .",
    "45 - 52 .",
    "x. chen , w. wang and w. li ,  an overview of hierarchical temporal memory : a new neocortex algorithm ",
    ", _ proceedings of international conference on modelling , identification & control ( icmic ) _ , wuhan , hubei , china , 2012 , pp . 1004 - 1010 .",
    "m. pietron , m. wielgosz , k. wiatr , parallel implementation of spatial pooler in hierarchical temporal memory.in proceedings of the 8th international conference on agents and artificial intelligence , pp ."
  ],
  "abstract_text": [
    "<S> this paper introduces mathematical formalism for spatial ( sp ) of hierarchical temporal memory ( htm ) with a spacial consideration for its hardware implementation . </S>",
    "<S> performance of htm network and its ability to learn and adjust to a problem at hand is governed by a large set of parameters . </S>",
    "<S> most of parameters are codependent which makes creating efficient htm - based solutions challenging . </S>",
    "<S> it requires profound knowledge of the settings and their impact on the performance of system . </S>",
    "<S> consequently , this paper introduced a set of formulas which are to facilitate the design process by enhancing tedious trial - and - error method with a tool for choosing initial parameters which enable quick learning convergence . </S>",
    "<S> this is especially important in hardware implementations which are constrained by the limited resources of a platform .    </S>",
    "<S> the authors focused especially on a formalism of spatial pooler and derive at the formulas for quality and convergence of the model . </S>",
    "<S> this may be considered as recipes for designing efficient htm models for given input patterns . </S>"
  ]
}