{
  "article_text": [
    "the statistical inference for diffusion processes described by stochastic differential equations ( sdes ) is currently a subject of intensive researches .",
    "a basic difficulty of this statistical problem is that , except for a few simple examples , the joint distribution of the discrete - time observations of the process has unknown closed - form .",
    "in addition , if only some components of the diffusion process contaminated with noise are observed , then an extra complication arises . typically , in this situation , the statistical problem under consideration is reformulated in the framework of continuous - discrete state space models , where the sde to be estimated defines the continuous state equation and",
    "the given observations are described in terms of an discrete observation equation .",
    "for such class of models , a number of estimators based on analytical and simulated approximations have been developed in the last four decades .",
    "see , for instance , nielsen et .",
    "al ( 2000a ) and jimenez et al . (",
    "2006 ) for a review .    in particular , the present paper deals with the class of innovation estimators for the parameter estimation of sdes given a time series of partial and noisy observations .",
    "these are the estimators obtained by maximizing a normal log - likelihood function of the discrete - time innovations associated with the underlying continuous - discrete state space model .",
    "approximations to this class of estimators have been derived by approximating the discrete - time innovations by means of inexact filters . with this purpose ,",
    "approximate continuous - discrete filters like the local linearization ( ozaki 1994 , shoji 1998 , jimenez & ozaki 2006 ) , the extended kalman ( nielsen & madsen 2001 , singer 2002 ) , and the second order ( nielsen et al .",
    "2000b , singer 2002 ) filters have been used , as well as , discrete - discrete filters after the discretization of the sde by means of a numerical scheme ( ozaki & iino 2001 , and peng et al .",
    "the approximate innovation estimators obtained in this way have been useful for the identification , from actual data , of a variety of neurophysiological , financial and molecular models among others ( see , e.g. , calderon 2009 , chiarella et al .",
    "2009 , jimenez et al .",
    "2006 , riera et al .",
    "2004 , valdes et al .",
    "however , a common feature of the approximate innovation estimators mentioned above is that , once the observations are given , the error between the approximate and the exact innovations is fixed and completely determined by the distance between observations . clearly , this fixes the bias of the approximate estimators for finite samples and obstructs its asymptotic correction when the number of observations increases .    in this paper ,",
    "an alternative approximation to the innovation estimator for diffusion processes is introduced , which is oriented to reduce and control the estimation bias .",
    "this is based on a recursive approximation of the first two conditional moments of the innovation process through approximate filters that converge to the linear one of minimum variance .",
    "it is shown that , for finite samples , the resulting approximate estimators converge to the exact one when the error of the approximate filters decreases . for an increasing number of observations ,",
    "they are asymptotically normal distributed and their bias decreases when the above mentioned error does it . as a particular instance , the approximate innovation estimators designed with the order-@xmath0 local linearization filters are presented . their convergence , practical algorithms and performance in simulations",
    "are also considered in detail .",
    "the simulations show that , with respect to the conventional approximations to the innovation estimators , the new approximate estimators significantly enhance the parameter estimation of the test equations given a reduced number of partial and noisy observations distant in time , which is a typical situation in many practical inference problems .",
    "the paper is organized as follows . in section 2 ,",
    "basic notations and definitions are presented . in section 3 ,",
    "the new approximate estimators are defined and some of their properties studied .",
    "as a particular instance , the order-@xmath0 innovation estimator based on convergent local linearization filters is presented in section 4 , as well as algorithms for its practical implementation . in the last section ,",
    "the performance of the new estimators is illustrated with various examples .",
    "let @xmath1 be the underlying complete probability space and @xmath2 @xmath3 be an increasing right continuous family of complete sub @xmath4-algebras of @xmath5 , and @xmath6 be a @xmath7-dimensional diffusion process defined by the stochastic differential equation @xmath8for @xmath9 , where @xmath10 and @xmath11 are differentiable functions , @xmath12 is an @xmath13-dimensional @xmath14-adapted standard wiener process , @xmath15 is a vector of parameters , and @xmath16 is a compact set .",
    "linear growth , uniform lipschitz and smoothness conditions on the functions @xmath10 and @xmath11 that ensure the existence and uniqueness of a strong solution of ( [ ss1 ] ) with bounded moments are assumed for all @xmath17 .",
    "let us consider the state space model defined by the continuous state equation ( [ ss1 ] ) and the discrete observation equation @xmath18where @xmath19 @xmath20 is a sequence of @xmath21-dimensional i.i.d .",
    "gaussian random vectors independent of @xmath22 , @xmath23 an @xmath24 positive semi - definite matrix , and @xmath25 an @xmath26 matrix .",
    "here , it is assumed that the @xmath27 time instants @xmath28 define an increasing sequence @xmath29 , @xmath30 .",
    "suppose that , through ( [ ss2 ] ) , @xmath27 partial and noisy observations of the diffusion process @xmath6 defined by ( [ ss1 ] ) with @xmath31 are given on @xmath32 .",
    "in particular , denote by @xmath33 the sequence of these observations , where @xmath34 denotes the observation at @xmath28 for all @xmath35 .    the inference problem",
    "to be consider here is the estimation of the parameter @xmath36 of the sde ( [ ss1 ] ) given the time series @xmath37 . specifically",
    ", let us consider the innovation estimator defined as follows .",
    "( ozaki , 1994 ) given @xmath27 observations @xmath37 of the continuous - discrete state space model ( ss1)-([ss2 ] ) with @xmath31 on @xmath32 , @xmath38defines the innovation estimator of @xmath36 , where @xmath39@xmath40 is the discrete innovations of the model ( [ ss1])-([ss2 ] ) and @xmath41 the innovation variance for all @xmath35 .    in the above definition ,",
    "@xmath42where @xmath43 and @xmath44 denote the conditional mean and variance of the diffusion process @xmath6 at  @xmath28 given the observations @xmath45 for all @xmath46 and @xmath47 . here",
    ", the predictions @xmath48 and @xmath49 are recursively computed through the linear minimum variance filter for the model ( [ ss1])-(ss2 ) . because the first two conditional moments of @xmath6 are correctly specified , theorem 1 in ljung & caines ( 1979 ) for prediction error estimators",
    "implies the consistent and the asymptotic normality of the innovation estimator ( [ innovation estimator ] ) under conventional regularity conditions ( ozaki 1994 , nolsoe et al .",
    "2000 ) .    in general , since the conditional mean and variance of equation ( [ ss1 ] ) have not explicit formulas , approximations to them are needed .",
    "if @xmath50 and @xmath51 are approximations to @xmath52 and @xmath53 , then the estimator @xmath54with@xmath55provides an approximation to the innovation estimator ( [ innovation estimator ] ) , where @xmath56are approximations to @xmath40 and @xmath57 .",
    "approximate estimators of this type have early been considered in a number of papers .",
    "approximate continuous - discrete filters like local linearization filters ( ozaki 1994 , shoji 1998 , jimenez & ozaki 2006 ) , extended kalman filter ( nielsen & madsen 2001 , singer 2002 ) , and second order filters ( nielsen et al . 2000b , singer 2002 ) have been used for approximating the values of @xmath40 and @xmath41 . on the other hand , in ozaki & iino ( 2001 ) and peng et .",
    "al ( 2002 ) , discrete - discrete filters have also been used after the discretization of the equation ( [ ss1 ] ) by means of a numerical scheme . in all these approximations ,",
    "once the data @xmath37 are given ( and so the time partition @xmath58 is specified ) , the error between @xmath59 and @xmath40 is completely settled by @xmath60 and can not be reduced . in this way",
    ", the difference between the approximate innovation estimator @xmath61 and the exact one @xmath62 can not be reduced neither .",
    "clearly , this is a important limitation of these approximate estimators . nevertheless , in a number of practical situations ( see jimenez & ozaki 2006 , jimenez et al .",
    "( 2006 ) , and references therein ) the bias the approximate innovation estimators is negligible .",
    "therefore , these estimators has been useful for the identification , from actual data , of a variety of neurophysiological , financial and molecular models among others as it was mentioned above .",
    "further , in a simulation study with the cox - ingersoll - ross model of short - term interest rate , approximate innovation methods have provided similar or better results than those obtained by prediction - based estimating functions but with much lower computational cost ( nolsoe et al . , 2000 ) .",
    "similar results have been reported in a comparative study with the approximate likelihood via simulation method ( singer , 2002 ) .",
    "denote by @xmath63 the space of @xmath64 time continuously differentiable functions @xmath65 for which @xmath66 and all its partial derivatives up to order @xmath64 have polynomial growth .",
    "let @xmath68 @xmath69 be a time discretization of @xmath70 $ ] such that @xmath71 , and @xmath72 be the approximate value of @xmath73 obtained from a discretization of the equation ( [ ss1 ] ) for all @xmath74 .",
    "let us consider the continuous time approximation @xmath75 @xmath76:\\mathbf{y}(\\tau _",
    "{ n})=\\mathbf{y}_{n}$ ] for all @xmath77 of @xmath6 with initial conditions@xmath78satisfying the bound condition @xmath79for all @xmath76 $ ] ; and the weak convergence criteria @xmath80for all @xmath81 and @xmath82 , where @xmath83 , @xmath84 and @xmath85 are positive constants , @xmath86 , and @xmath87 . the process @xmath88 defined in this way is typically called order-@xmath89 approximation to @xmath6 in weak sense ( kloeden & platen , 1999 ) . the second conditional moment of @xmath88 is also assumed to be positive definite and continuous for all @xmath90 .",
    "in addition , let us consider the following approximation to the linear minimum variance ( lmv ) filter of the model ( [ ss1])-([ss2 ] ) .",
    "( jimenez 2012b ) given a time discretization @xmath91 , the order-@xmath92 linear minimum variance filter for the state space model ( [ ss1])-([ss2 ] ) is defined , between observations , by @xmath93@xmath94for all @xmath95 , and by @xmath96@xmath97for each observation at @xmath98 , with filter gain@xmath99for all @xmath81 , where @xmath88 is an order-@xmath92 approximation to the solution of ( [ ss1 ] ) in weak sense , and @xmath100 @xmath101 are given observations of ( [ ss1])-([ss2 ] ) until the time instant @xmath102 .",
    "the predictions @xmath103 and @xmath104 , with initial conditions @xmath105 and @xmath106 , are defined for all @xmath107 $ ] and @xmath108",
    ".    once an order-@xmath92 approximation to the solution of equation ( [ ss1 ] ) is chosen , and so an order-@xmath92 lmv filter is specified , the following approximate innovation estimator can naturally be defined .",
    "[ definition order - b inn estimator ] given @xmath27 observations @xmath37 of the continuous - discrete state space model ( [ ss1])-([ss2 ] ) with @xmath109 on @xmath32 , the order-@xmath92 innovation estimator for the parameters of ( [ ss1 ] ) is defined by @xmath110where @xmath111 with @xmath112 and @xmath113 , being @xmath114 and @xmath115 the prediction mean and variance of an order-@xmath92 lmv filter for the model ( [ ss1])-([ss2 ] ) , and @xmath116 the maximum stepsize of the time discretization @xmath117 associated to the filter .    in principle , according to the above definitions , any kind of approximation @xmath118 converging to @xmath6 in a weak sense can be used to construct an approximate order-@xmath92 lmv filter and so an approximate order-@xmath92 innovation estimator . in this way , the euler - maruyama , the local linearization and any high order numerical scheme for sdes as those considered in kloeden & platen ( 1999 ) might be used as well .",
    "however , the approximations @xmath119 and @xmath120 to @xmath40 and @xmath41 in ( [ innovation estimator ] ) at each @xmath28 will be now derived from the predictions of approximate lmv filter after various iterations with stepsizes lower than @xmath121 .",
    "note that , when @xmath122 , an order-@xmath92 lmv filter might reduce to some one of the conventional approximation to the exact lmv filter .",
    "in this situation , the corresponding order-@xmath92 innovation estimator reduces to some one of the approximate innovation estimator mentioned in section [ inn method section ] . in particular , to those considered in ozaki ( 1994 ) , shoji ( 1998 ) , jimenez & ozaki ( 2006 ) when local linearization schemes are used to define order-@xmath123 lmv filters .",
    "note that the goodness of the approximation @xmath88 to @xmath6 is measured ( in weak sense ) by the left hand side of ( [ inn lmvf7 ] ) .",
    "thus , the inequality ( [ inn lmvf7 ] ) gives a bound for the errors of the approximation @xmath124 to @xmath6 , for all @xmath125 $ ] and all pair of consecutive observations @xmath81 .",
    "moreover , this inequality states the convergence ( in weak sense and with rate @xmath92 ) of the approximation @xmath88 to @xmath6 as the maximum stepsize @xmath116 of the time discretization @xmath126 goes to zero .",
    "clearly this includes , as particular case , the convergence of the first two conditional moments of @xmath88 to those of @xmath6 , which implies the convergence of order-@xmath0 lmv filter ( [ lmvf12 app])-([lmvf5 app ] ) to the exact lmv filter stated by theorem 5 in jimenez ( 2012b ) .",
    "since the approximate innovation estimator ( [ order - b innovation estimator ] ) is designed in terms of the order-@xmath0 lmv filter ( [ lmvf12 app])-([lmvf5 app ] ) , the weak convergence of @xmath88 to @xmath6 should then imply the convergence of the approximate innovation estimator ( [ order - b innovation estimator ] ) to the exact one ( [ innovation estimator ] ) and the similarity of their asymptotic properties , as @xmath116 goes to zero .",
    "next results deal with these matters .      for a finite sample @xmath37 of @xmath27 observation of the state space model ( ss1)-([ss2 ] ) ,",
    "theorem 5 in jimenez ( 2012b ) states the convergence of the order-@xmath92 lmv filters to the exact lmv one when @xmath116 decreases .",
    "therefore , the convergence of the order-@xmath92 innovation estimator to the exact innovation estimator is predictable when @xmath116 goes to zero .",
    "[ innovation convergence theorem]let @xmath37 be a time series of @xmath27 observations of the state space model ( [ ss1])-([ss2 ] ) with @xmath127 on the time partition @xmath32 .",
    "let @xmath128 and @xmath129 be , respectively , the innovation and an order-@xmath92 innovation estimator for the parameters of ( [ ss1 ] ) given @xmath37 .",
    "then @xmath130as @xmath131 .",
    "moreover , @xmath132as @xmath131 , where the expectation is with respect to the measure on the underlying probability space generating the realizations of the model ( [ ss1])-([ss2 ] ) with @xmath133 .",
    "defining @xmath134 , it follows that @xmath135and@xmath136by using these two identities and the identity @xmath137with @xmath138 and @xmath139 , it is obtained that @xmath140where @xmath141 and @xmath142 are defined in ( [ innovation estimator ] ) and ( [ order - b innovation estimator ] ) , respectively , and @xmath143with @xmath144 .",
    "theorem 5 in jimenez ( 2012b ) deals with the convergence of the order-@xmath89 filters to the exact lmv one .",
    "in particular , for the predictions , it states that @xmath145for all @xmath81 , where @xmath146 is a positive constant . here , we recall that @xmath52 and @xmath147 are the predictions of the exact lmv filter for the model ( [ ss1])-([ss2 ] ) , whereas @xmath114 and @xmath148 are the predictions of the order-@xmath92 filter . from this and taking into account that @xmath149 and @xmath150  @xmath151 follows that@xmath152as @xmath131 for all @xmath17 and @xmath153 .",
    "this and the finite bound for the first two conditional moments of @xmath6 and @xmath88 imply that @xmath154 as well with @xmath116 . from this and ( [ plk ind 4 ] ) , @xmath155as @xmath131 , which implies the first assertion of the theorem .    on the other hand , since the constant @xmath146 in ( [ inn lmvf10 ] ) does not depend of a specific realization of the model ( [ ss1])-([ss2 ] ) , from these inequalities follows that @xmath156where the new expectation here is with respect to the measure on the underlying probability space generating the realizations of the model ( [ ss1])-([ss2 ] ) with @xmath127 . from this and ( [ inn lmvf11 ] )",
    "follows that @xmath157 as @xmath131 , which concludes the proof .",
    "the first assertion of this theorem states that , for each given data @xmath37 , the order-@xmath92 innovation estimator @xmath129 converges to the exact one @xmath62 as @xmath116 goes to zero . because @xmath116 controls the weak convergence criteria ( [ inn lmvf7 ] ) is then clear that the order-@xmath92 innovation estimator ( [ order - b innovation estimator ] ) converges to the exact one ( [ innovation estimator ] ) when the error ( in weak sense ) of the order-@xmath92 approximation @xmath88 to @xmath6 decreases or , equivalently , when the error between the order-@xmath92 filter and the exact lmv filter decreases .",
    "on the other hand , the second assertion implies that the average of the errors @xmath158 corresponding to different realizations of the model ( [ ss1])-([ss2 ] ) decreases when @xmath116 does .",
    "next theorem deals with error between the averages of the estimators @xmath159 and @xmath160 computed for different realizations of the state space model .",
    "[ inn week convergence]let @xmath37 be a time series of @xmath27 observations of the state space model ( [ ss1])-([ss2 ] ) with @xmath127 on the time partition @xmath32 .",
    "let @xmath128 and @xmath161 be , respectively , the innovation and an order-@xmath92 innovation estimator for the parameters of ( [ ss1 ] ) given @xmath37 . then , @xmath162as @xmath131 , where the expectation is with respect to the measure on the underlying probability space generating the realizations of the model ( [ ss1])-([ss2 ] ) with @xmath133 .",
    "trivially , @xmath163where the expectation here is taken with respect to the measure on the underlying probability space generating the realizations of the model ( [ ss1])-([ss2 ] ) with @xmath127 . from this and the second assertion of theorem [ innovation convergence theorem ]",
    ", the proof is completed .    here",
    ", it is worth to remak that the conventional approximate innovation estimators mentioned in section [ inn method section ] do not have the desired convergence properties stated in the theorems above for the order-@xmath92 innovation estimator .",
    "further note that , either in definition [ definition order - b inn estimator ] nor in theorems [ innovation convergence theorem ] and [ inn week convergence ] some restriction on the time partition @xmath32 for the data has been assumed .",
    "thus , there are not specific constraints about the time distance between two consecutive observations , which allows the application of the order-@xmath92 innovation estimator in a variety of practical problems with a reduced number of not close observations in time , with sequential random measurements , or with multiple missing data .",
    "neither there are restrictions on the time discretization @xmath164 @xmath165 on which the order-@xmath92 innovation estimator is defined .",
    "thus , @xmath164 can be set by the user by taking into account some specifications or previous knowledge on the inference problem under consideration , or automatically designed by an adaptive strategy as it will be shown in the section concerning the numerical simulations .      in this section",
    ", asymptotic properties of the approximate innovation estimator @xmath129 will be studied by using a general result obtained in ljung and caines ( 1979 ) for prediction error estimators . according to that ,",
    "the relation between the estimator @xmath166 and the global minimum @xmath167 of the function @xmath168should be considered , where @xmath141 is defined in ( [ innovation estimator ] ) and the expectation is taken with respect to the measure on the underlying probability space generating the realizations of the state space model ( [ ss1])-([ss2 ] ) . here",
    ", it is worth to remark that @xmath169 is not an estimator of @xmath170 since the function @xmath171 does not depend of a given data @xmath37 .",
    "in fact , @xmath169 indexes the best predictor , in the sense that the average prediction error loss function @xmath171 is minimized at this parameter ( ljung & caines , 1979 ) .    in what follows , regularity conditions for the unique identifiability of the state space model ( [ ss1])-([ss2 ] )",
    "are assumed , which are typically satisfied by stationary and ergodic diffusion processes ( see , e.g. , ljung & caines , 1979 ) .",
    "[ innovation lemma]if @xmath41 is positive definite for all @xmath172 , then the function @xmath173 defined in ( [ plkw ] ) has an unique minimum and @xmath174    since @xmath41 is positive definite for all @xmath172 , lemma a.2 in bollerslev & wooldridge * *  * * ( 1992 ) ensures that @xmath175 is the unique minimum of the function@xmath176on @xmath177 for all @xmath178 , where @xmath179  and @xmath180 . consequently and under the assumed unique identifiability of the model ( ss1)-([ss2 ] ) , @xmath36 is then the unique minimum of @xmath181on @xmath182    denote by @xmath183 the derivative of @xmath142 with respect to @xmath184 , and by @xmath185 the second derivative of @xmath171 with respect to @xmath170 .",
    "[ innovation main theorem]let @xmath37 be a time series of @xmath27 observations of the state space model ( [ ss1])-([ss2 ] ) with @xmath186 on the time partition @xmath32 .",
    "let @xmath187 be an order-@xmath92 innovation estimator for the parameters of ( [ ss1 ] ) given @xmath37 .",
    "then @xmath188w.p.1 as @xmath189 , where @xmath190 as @xmath131 . moreover , if for some @xmath191 there exists @xmath192 such that@xmath193for all @xmath194 and @xmath17 , then@xmath195as @xmath189 , where @xmath196 with @xmath197 as @xmath198 .    let @xmath199 and @xmath200 , where @xmath142 is defined in ( [ order - b innovation estimator ] ) .",
    "for a @xmath116 fixed , theorem 1 in ljung & caines ( 1979 ) implies that@xmath201w.p.1 as @xmath189 ; and @xmath202as @xmath189 , where @xmath203with @xmath204 .    by using the identities ( [ plk ind 1])-([plk ind 3 ] )",
    ", the function @xmath205with @xmath206 , can be written as @xmath207where @xmath171 is defined in ( [ plkw ] ) and@xmath208with @xmath144 , @xmath209 and @xmath210 .",
    "denote by @xmath211 and @xmath212 the second derivative of @xmath213 and @xmath214 with respect to @xmath170 .    taking into account that @xmath215with @xmath216it is obtained that@xmath217where@xmath218    theorem 5 in jimenez ( 2012b ) deals with the convergence of the order-@xmath89 filters to the exact lmv one .",
    "in particular , for the predictions , it states that @xmath219for all @xmath81 , where @xmath146 is a positive constant . here , we recall that @xmath52 and @xmath147 are the predictions of the exact lmv filter for the model ( [ ss1])-([ss2 ] ) , whereas @xmath114 and @xmath148 are the predictions of the order-@xmath92 filter . from this and taking into account that @xmath149 and @xmath150  @xmath151 follows that@xmath220as @xmath131 for all @xmath17 and @xmath153 .",
    "this and the finite bound for the first two conditional moments of @xmath6 and @xmath88 imply that @xmath221 and @xmath222 as well with @xmath116 . from this and ( [ plk3 ] )",
    ", it is obtained that @xmath223 in addition , left ( [ plk9 ] ) and lemma [ innovation lemma ] imply that @xmath224whereas from right ( [ plk9 ] ) follows that@xmath225    finally , ( [ plk11])-([plk10 ] ) together ( [ plk7 ] ) , ( [ plk8 ] ) and ( [ plk12 ] ) imply that ( [ plk1 ] ) and ( [ plk2 ] ) hold , which completes the proof .    theorem [ innovation main theorem ] states that , for an increasing number of observations , the order-@xmath92 innovation estimator @xmath226 is asymptotically normal distributed and its bias decreases when @xmath116 goes to zeros .",
    "this is a predictable result due to the asymptotic properties of the exact innovation estimator @xmath227 derived from theorem 1 in ljung & caines ( 1979 ) and the convergence of the approximate estimator @xmath129 to @xmath62 given by theorem [ innovation convergence theorem ] when @xmath116 goes to zero .",
    "further note that , when @xmath228 , the theorem [ innovation main theorem ] reduces to theorem 1 in ljung & caines ( 1979 ) for the exact innovation estimator @xmath229 .",
    "this is other expected result since the order-@xmath92 innovation estimator @xmath129 reduces to the exact one @xmath230 when @xmath228 .",
    "further note that , neither in theorem [ innovation main theorem ] there are restrictions on the time partition @xmath32 for the data or on the time discretization @xmath164 @xmath231 on which the approximate estimator is defined .",
    "therefore , the comments about them at the end of the previous subsection are valid here as well .      previous definitions and results have been stated for models with linear observation equation . however , by following the procedure proposed in jimenez and ozaki ( 2006 ) , they can be easily applied as well to state space models with nonlinear observation equation .    for illustrate this ,",
    "let us consider the state space model defined by the continuous state equation ( [ ss1 ] ) and the discrete observation equation    @xmath232    where @xmath233 is defined as in ( [ ss2 ] ) and @xmath234 @xmath235 is a twice differentiable function . by using the ito formula , @xmath236with @xmath237 .",
    "hence , the state space model ( [ ss1])+([ss3 ] ) is transformed to the following higher - dimensional state space model with linear observation @xmath238@xmath239where @xmath240 , \\text { } \\mathbf{a}=\\left [ \\begin{array}{l } \\mathbf{f } \\\\ \\mathbf{\\rho } % \\end{array}% \\right ] , \\text { } \\mathbf{b}_{i}=\\left [ \\begin{array}{l } \\mathbf{g}_{i } \\\\ \\mathbf{\\sigma } _ { i}% \\end{array}% \\right]\\]]and the matrix @xmath25 is such that @xmath241 .    in this way",
    ", the state space model ( [ ss1])+([ss3 ] ) is transformed to the form of the model ( [ ss1])-([ss2 ] ) , and so the previous definition and results related to the order-@xmath92 innovation estimator can be applied .",
    "further , note that if the nonlinear function @xmath116 depends of unknown parameters , they can be estimated as well by the approximate innovation method .",
    "this section deals with the particular case that the observation noise is zero and all components of the diffusion process defined in ( [ ss1 ] ) are discretely observed .",
    "that is , when @xmath242 and @xmath243 in ( [ ss2 ] ) for all @xmath178 , where @xmath244 denotes the @xmath7-dimensional identity matrix .",
    "hence , the inference problem under consideration in this paper reduces then to the well known problem of parameter estimation of diffusion processes from complete observations . in this situation ,",
    "it is easy to realize that the innovation estimator ( [ innovation estimator ] ) reduces to the well known quasi - maximum likelihood ( qml ) estimator for sdes , and that the approximate order-@xmath92 innovation estimator ( [ order - b innovation estimator ] ) reduces to the approximate order-@xmath92 qml estimator introduced in jimenez ( 2012c ) for the estimation of sdes from complete observations . for the same reason , theorems [ innovation convergence theorem ] , [ inn week convergence ] and [ innovation main theorem ] reduce to those corresponding in jimenez ( 2012c ) concerning the convergence and asymptotic properties of the approximate order-@xmath92 qml estimator",
    "since , in principle , any approximate filter converging to lmv filter of the model ( [ ss1])-([ss2 ] ) can be used to construct an order-@xmath92 innovation estimator , some additional criterions could be considered for the selection of one of them .",
    "for instance , high order of convergence , efficiency of the algorithm from computational viewpoint , and so on . in this paper",
    ", we elected the order-@xmath92 local linearization ( ll ) filters proposed in jimenez ( 2012b ) for the following reasons : 1 ) their predictions have simple explicit formulas that can be computed by means of efficient algorithm ( including high dimensional equations ) ; 2 ) their predictions are exact for linear sdes in all the possible variants ( with additive and/or multiplicative noise , autonomous or not ) ; 3 ) they have an adequate order @xmath245 of convergence ; and 4 ) the better performance of the approximate innovation estimators based on conventional ll filters ( see , e.g. , ozaki , 1994 ; shoji , 1998 ; singer , 2002 ) .    according to jimenez ( 2012b ) ,",
    "the order-@xmath92 ll filter is defined on @xmath71 in terms of the order-@xmath92 local linear approximation @xmath88 that satisfies the conditions ( inn lmvf5)-([inn lmvf7 ] ) .",
    "denote by @xmath246 and @xmath247 the first two conditional moment of @xmath88 at @xmath248 given the observations @xmath249 , for all @xmath250 @xmath251 @xmath252\\}$ ] and @xmath253 .    starting with the initial filter values @xmath254 and @xmath255",
    ", the ll filter algorithm performs the recursive computation of :    1 .   the predictions @xmath256and @xmath257 for all @xmath250 @xmath258 @xmath259\\}$ ] by means of the recursive formulas ( [ allf8])-([allf9 ] ) given in the appendix , and the prediction variance @xmath260 2 .",
    "the filters @xmath261with filter gain@xmath262    for each @xmath178 , with @xmath263 .    under general conditions , the convergence of the order-@xmath92 ll filter to exact lmv filter when @xmath116 goes to zero has been stated by theorem 10 in jimenez ( 2012b ) .",
    "hence , theorem [ innovation convergence theorem ] implies that the ll - based innovation estimator @xmath264with@xmath265converges to the exact one ( [ innovation estimator ] ) as @xmath116 goes to zero for all given @xmath37 , where @xmath266 .",
    "for the same reason , this order-@xmath92 innovation estimator has the asymptotic properties stated in theorem [ innovation main theorem ] , and the average of their values for different realizations of the state space model ( [ ss1])-([ss2 ] ) satisfies the convergence property of theorem [ inn week convergence ] .",
    "note that , when @xmath122 , the order-@xmath92 ll filter reduces to the conventional ll filter .",
    "in this situation , the order-@xmath92 innovation estimator ( [ ll - based innovation estimator ] ) reduces to the conventional innovation estimators of ozaki ( 1994 ) or shoji ( 1998 ) for sdes with additive noise , and to that of jimenez and ozaki ( 2006 ) for sdes with multiplicative noise .",
    "it is worth to emphasize here that , for each data @xmath267 , the formulas ( [ allf8])-([allf9 ] ) for the predictions are recursively evaluated at all the time instants @xmath250 @xmath258 @xmath268\\}$ ] for the order-@xmath0 estimator , whereas they are evaluated only at @xmath269 @xmath258 @xmath270\\}$ ] for the conventional ones .",
    "in addition , since the predictions of the order-@xmath92 ll filter are exact for linear sdes , the order-@xmath92 innovation estimator ( [ ll - based innovation estimator ] ) reduces to the maximum likelihood estimator of schweppe ( 1965 ) for linear equations with additive noise .    in practical situations ,",
    "it is convenient to write a code that automatically determines the time discretization @xmath271 for achieving a prescribed absolute ( @xmath272 and relative ( @xmath273 error tolerance in the computation of @xmath274 and @xmath275 . with this purpose",
    "the adaptive strategy proposed in jimenez ( 2012b ) is useful .",
    "in this section , the performance of the new approximate estimators is illustrated , by means of simulations , with four test sdes .",
    "to do so , four types of innovation estimators are computed and compared : 1 ) the exact one ( [ innovation estimator ] ) , when it is possible ; 2 ) the conventional one based on the ll filter . that is , the estimator defined by ( [ ll - based innovation estimator ] ) with @xmath276 and @xmath277 ; 3 ) the order-@xmath278 innovation estimator ( [ ll - based innovation estimator ] ) with various uniform time discretizations @xmath279 ; and 4 ) the adaptive order-@xmath280 innovation estimator ( [ ll - based innovation estimator ] ) with the adaptive selection of time discretizations @xmath281 proposed in jimenez ( 2012b ) . for each example ,",
    "histograms and confidence limits for the estimators are computed from various sets of discrete and noisy observations taken with different time distances ( sampling periods ) on time intervals with distinct lengths .",
    "* example 1 .",
    "* state equation with multiplicative noise@xmath282and observation equation @xmath283with @xmath284 , @xmath285 and observation noise variance @xmath286 . for this state equation",
    ", the predictions for the first two conditional moments are @xmath287where the filters @xmath288 and @xmath289 are obtained from the well - known formulas of the exact lmv filter for all @xmath290 , with initial values @xmath291 and @xmath292 at @xmath293 .",
    "* example 2 .",
    "* state equation with two additive noise@xmath294and observation equation @xmath295with @xmath296 , @xmath297 , @xmath298 and observation noise variance @xmath286 . for this state equation ,",
    "the predictions for the first two conditional moments are@xmath299and @xmath300where the filters @xmath288 and @xmath289 are obtained from the formulas of the exact lmv filter for all @xmath290 , with initial values @xmath301 and @xmath302 at @xmath303 .",
    "* example 3 .",
    "* van der pool oscillator with random input ( gitterman , 2005)@xmath304and observation equation @xmath305where @xmath306 and @xmath307 are the intensity and the variance of the random input , respectively .",
    "in addition , @xmath308 is the observation noise variance , and @xmath309 @xmath310 $ ] and @xmath311 are the initial filter values at @xmath312 .",
    "* example 4 .",
    "* van der pool oscillator with random frequency ( gitterman , 2005)@xmath313and observation equation @xmath314where @xmath315 and @xmath316 are the frequency mean value and variance , respectively .",
    "@xmath308 is the observation noise variance , and @xmath317 @xmath318 $ ] and @xmath319 are the initial filter values at @xmath312 .    in these examples , autonomous or non autonomous , linear or nonlinear ,",
    "one or two dimensional sdes with additive or multiplicative noise are considered for the estimation of two or three parameters .",
    "note that , since the first two conditional moments of the sdes in examples 1 and 2 have explicit expressions , the exact innovation estimator ( [ innovation estimator ] ) can be computed .",
    "these four state space models have previously been used in jimenez ( 2012b ) to illustrate the convergence of the order-@xmath92 ll filter by means of simulations .",
    "tables with the errors between the approximate moments and the exact ones as a function of @xmath116 were given for the examples 1 and 2 .",
    "tables with the estimated rate of convergence were provided for the fours examples .",
    "for the first two examples , @xmath320 realizations of the state equation solution were computed by means of the euler ( kloeden & platen , 1999 ) or the local linearization scheme ( jimenez et al . , 1999 ) for the equation with multiplicative or additive noise , respectively .",
    "for each example , the realizations where computed over the thin time partition @xmath321 to guarantee a precise simulation of the stochastic solutions on the time interval @xmath322 $ ] .",
    "twelve subsamples of each realization at the time instants @xmath323 @xmath20 were taken for evaluating the corresponding observation equation with various values of @xmath27 and @xmath324 .",
    "in particular , the values @xmath325 and @xmath326 with @xmath327 were used . in this way , twelve sets of @xmath320 time series @xmath328 @xmath329 , with @xmath330 , of @xmath27 observations @xmath331 each one were finally available for both state space models to make inference .",
    "this will allow us to explore and compare the performance of each estimator from observations taken with different sampling periods @xmath332  on time intervals with distinct lengths @xmath324 .",
    "figure 1 shows the histograms and the confidence limits for both , the exact ( @xmath333 ) and the conventional ( @xmath334 ) innovation estimators of @xmath335 computed from the twelve sets of @xmath320 time series @xmath336 available for the example 1 .",
    "figure 2 shows the same but , for the exact ( @xmath337 ) and the conventional ( @xmath338 ) innovation estimators of @xmath4 .",
    "as it was expected , for the samples @xmath336 with largest sampling periods , the parameter estimation is distorted by the well - known lowpass filter effect of signals sampling ( see , e.g. , oppenheim & schafer , 2010 ) .",
    "this is the reason of the under estimation of the variance @xmath339 from the samples @xmath340 , with @xmath341 and @xmath325 , when the parameter @xmath335 in the drift coefficient of ( [ se ej1 ] ) is better estimated by @xmath333 .",
    "contrarily , from these samples , the conventional innovation estimators @xmath342 can not provided a good approximation to @xmath335 , and so the whole unexplained component of the drift coefficient of ( [ se ej1 ] ) included in the samples is interpreted as noise by the conventional estimators .",
    "for this reason , @xmath338 over estimates the value of the parameter @xmath4 .",
    "further , note that when the sampling period @xmath332 decreases , the difference between the exact ( @xmath343 ) and the conventional ( @xmath344 ) innovation estimators decreases , as well as the bias of both estimators .",
    "this is also other expected result . here",
    ", the bias is estimated by the difference between the parameter value and the estimator average , whereas the difference between estimators refers to the histogram shape and confidence limits .    for the data of ( [ se ej1 ] ) with largest sampling period @xmath345 , the order-@xmath278 innovation estimators ( @xmath346 ) and ( @xmath347 ) on uniform @xmath348 @xmath349 and adaptive @xmath350 time discretizations ,",
    "respectively , were computed with @xmath351 and tolerances @xmath352 and @xmath353 , @xmath354 . for each data @xmath336 , with @xmath355 ,",
    "the errors @xmath356between the exact ( @xmath357 ) and the approximate ( @xmath358 ) innovation estimators were computed .",
    "average and standard deviation of these @xmath320 errors were calculated for each set of values @xmath359 specified above , which are summarized in table i. note as , for fixed @xmath324 , the average of the errors decreases as @xmath116 does it .",
    "this clearly illustrates the convergence of the order-@xmath278 innovation estimators to the exact one stated in theorem [ innovation convergence theorem ] when @xmath116 goes to zero .",
    "in addition , figure 3 shows the histograms and the confidence limits for the order-@xmath278 innovation estimators ( @xmath360 ) and ( @xmath361 ) for each set of values @xmath359 . by comparing the results of this figure with the corresponding in the previous ones ,",
    "the decreasing difference between the order-@xmath278 innovation estimators ( @xmath362 ) and the exact one ( @xmath363 ) is observed as @xmath116 decreases , which is consistent with the convergence results of table i. these findings are more precisely summarized in table ii , which shows the difference between the averages of the exact and the approximate innovation estimators .",
    "further , note the small difference between the adaptive estimators ( @xmath364 ) and the exact ones ( @xmath365 ) , which illustrates the usefulness of the adaptive strategy for improving the innovation  parameter estimation for finite samples with large sampling periods .",
    "the number of accepted and fail steps of the adaptive innovation estimators at each @xmath366 are shown in figure 4 .",
    "further , note that the results of table ii illustrate the convergence findings of theorem [ inn week convergence ] .",
    "@xmath367 & @xmath368 & @xmath369 +    [ cols= \" < , < \" , ]      +    table vi : bias of the approximate innovation estimators for the equation ( [ sea ej4])-([seb ej4 ] ) .",
    "@xmath370 , for the conventional ; @xmath371  for the order-@xmath278 on @xmath372 ; and @xmath373for the adaptive order-@xmath278 on @xmath374 .      in section [ section on complete observations ]",
    ", the connection among the innovation and quasi - maximum likelihood estimators was early mentioned for the identification of models with noise free complete observations . in this situation , it is easy to verify that the ll - based innovation estimator ( ll - based innovation estimator ) reduces to the ll - based quasi - maximum likelihood estimator introduced in jimenez ( 2012b ) . in that paper ,",
    "the state equations of the four models considered in section [ section test models ] were also used as test examples in simulations .",
    "the reader interested in this identification problem is encouraged to consider these simulations .",
    "an alternative approximation to the innovation method was introduced for the parameter estimation of diffusion processes given a time series of partial and noisy observations .",
    "this is based on a convergent approximation to the first two conditional moments of the innovation process through approximate continuous - discrete filters of minimum variance . for finite samples ,",
    "the convergence of the approximate innovation estimators to the exact one was proved when the error between the approximate and the exact linear minimum variance filters decreases .",
    "it was also demonstrated that , for an increasing number of observations , the approximate estimators are asymptotically normal distributed and their bias decreases when the above mentioned error does it . as particular instance , the order-@xmath92 innovation estimators based on local linearization filters were proposed . for them ,",
    "practical algorithms were also provided and their performance in simulation illustrated with various examples .",
    "simulations shown that : 1 ) with thin time discretizations between observations , the order-@xmath278 innovation estimator provides satisfactory approximations to the exact innovation estimator ; 2 ) the convergence of the order-@xmath278 innovation estimator to the exact one when the maximum stepsize of the time discretization between observations decreases ; 3 ) with respect to the conventional innovation estimator , the order-@xmath278 innovation estimator gives much better approximation to the exact innovation estimator , and has less bias and higher efficiency ; 4 ) with an adequate tolerance , the adaptive order-@xmath278 innovation estimator provides an automatic , suitable and computational efficient approximation to the exact innovation estimator ; and 5 ) the effectiveness of the order-@xmath278 innovation estimator for the identification of sdes from a reduced number of partial and noisy observations distant in time .",
    "further note that new estimators can also be easily applied to a variety of practical problems with sequential random measurements or with multiple missing data",
    ".    * acknowledgement .",
    "* the numerical simulations of this paper were concluded on july 2012 within the framework of the associateship scheme of the abdus salam international centre for theoretical physics ( ictp ) , trieste , italy .",
    "the author thanks to the ictp for the partial support to this work .",
    "according  to jimenez ( 2012b ) , given the filters values @xmath375 and @xmath376 , the predictions @xmath377 and @xmath378 of the order-@xmath92 ll filter are computed by the recursive formulas@xmath379and @xmath380for all @xmath107 $ ] and @xmath81 , where @xmath381and the vector @xmath382 and the matrices @xmath383 , @xmath384 , @xmath385  are defined as @xmath386 \\text { , \\ \\ } \\mathbf{u}_{\\tau , t_{k}}=\\left [ \\begin{array}{c } vec(\\mathbf{p}_{\\tau /t_{k } } ) \\\\",
    "\\mathbf{0 } \\\\",
    "\\mathbf{r } \\\\ 0 \\\\ 0 \\\\",
    "1% \\end{array}% \\right ] \\in % tcimacro{\\u{211d } } % % beginexpansion \\mathbb{r } % endexpansion ^{(d^{2}+2d+7)}\\]]and@xmath387 \\text { , \\ \\ \\ \\ \\ \\ \\ \\ \\ } \\mathbf{l}_{2}=\\left [ \\begin{array}{ccc } \\mathbf{0}_{d\\times ( d^{2}+d+2 ) } & \\mathbf{i}_{d } & \\mathbf{0}_{d\\times 5}% \\end{array}% \\right]\\]]in terms of the matrices and vectors @xmath388@xmath389 \\in \\mathbb{r}^{(d+2)\\times ( d+2)},\\]]@xmath390\\]]@xmath391 , @xmath392 , @xmath393 , @xmath394 and @xmath395 with @xmath396@xmath397 $ ] , and the @xmath7-dimensional identity matrix @xmath398 . here , @xmath399are matrices , and the vectors @xmath400 , @xmath401 , @xmath402 and @xmath403 satisfy the expressions@xmath404for all @xmath125 $ ] and @xmath405 , where @xmath406^{j , l}\\text { } \\frac{\\partial ^{2}\\mathbf{f}(\\tau , \\mathbf{y}_{\\tau /t_{k}})}{\\partial \\mathbf{y}^{j}\\partial \\mathbf{y}^{l}}(t-\\tau ) & \\text{% for } \\mathbb{\\beta } = 2% \\end{array}% \\right.\\]]and@xmath407^{j , l}\\text { } \\frac{\\partial ^{2}\\mathbf{g}_{i}(\\tau , % \\mathbf{y}(\\tau ) ) } { \\partial \\mathbf{y}^{j}\\partial \\mathbf{y}^{l}}(t-\\tau ) & \\text{for } \\mathbb{\\beta } = 2% \\end{array}% \\right.\\]]are functions associated to the order-@xmath92 ito - taylor expansions for the drift and diffusion coefficients of ( [ ss1 ] ) in the neighborhood of @xmath408 , respectively , and @xmath409 $ ] is an @xmath410 matrix function .",
    "the symbols @xmath411 , @xmath412 and @xmath413 denote the vectorization operator , the kronecker sum and product , respectively .    from computational viewpoint ,",
    "each evaluation of  the formulas ( [ allf8])-([allf9 ] ) at @xmath248 requires the computation of just one exponential matrix whose matrix depends of the drift and diffusion coefficients of ( [ ss1 ] ) at @xmath414 .",
    "this exponential matrix can the efficiently computed through the well known pad method ( moler & van loan , 2003 ) or , alternatively , by means of the krylov subspace method ( moler & van loan , 2003 ) in the case of high dimensional sdes .",
    "even more , low order pad and krylov methods as suggested in jimenez & de la cruz ( 2012 ) can be used as well for reducing the computation cost , but preserving the order-@xmath92 of the approximate moments .",
    "alternatively , simplified formulas for the moments can be used when the equation to be estimate is autonomous or has additive noise ( see jimenez , 2012a ) .",
    "all this makes simple and efficient the evaluation of the approximate moments @xmath415 and @xmath416 required by the innovation estimator ( [ ll - based innovation estimator ] ) .",
    "* bollerslev t. and wooldridge j.m . * ( 1992 ) quasi - maximun likelihood estimation and inference in dynamic models with time - varying covariances .",
    ", 11 , 143 - 172 .                                    * nolsoe k. , nielsen j.n . and madsen h. * ( 2000 ) prediction - based estimating function for diffusion processes with measurement noise .",
    "technical reports 2000 , no.10 , informatics and mathematical modelling , technical university of denmark .",
    "* ozaki t. * ( 1994 ) the local linearization filter with application to nonlinear system identification . in bozdogan h. ( ed . )",
    "proceedings of the first us / japan conference on the frontiers of statistical modeling : an informational approach , 217 - 240 .",
    "kluwer academic publishers .",
    "* peng h. , ozaki t. and jimenez j.c . * ( 2002 ) modeling and control for foreign exchange based on a continuous time stochastic microstructure model , in proceedings of the 41st ieee conference on decision and control , las vegas , nevada usa , december 2002 , 4440 - 4445 .    * riera j.j .",
    ", watanabe j. , iwata k. , miura n. , aubert e. , ozaki t. and kawashima r. * ( 2004 ) a state - space model of the hemodynamic approach : nonlinear filtering of bold signals .",
    "neuroimage , 21 , 547 - 567 .",
    "* singer h. * ( 2002 ) parameter estimation of nonlinear stochastic differential equations : simulated maximum likelihood versus extended kalman filter and ito - taylor expansion , j. comput .",
    "graphical statist.11 , 972 - 995 ."
  ],
  "abstract_text": [
    "<S> in this paper , an alternative approximation to the innovation method is introduced for the parameter estimation of diffusion processes from partial and noisy observations . </S>",
    "<S> this is based on a convergent approximation to the first two conditional moments of the innovation process through approximate continuous - discrete filters of minimum variance . </S>",
    "<S> it is shown that , for finite samples , the resulting approximate estimators converge to the exact one when the error of the approximate filters decreases . for an increasing number of observations , </S>",
    "<S> the estimators are asymptotically normal distributed and their bias decreases when the above mentioned error does it . a simulation study is provided to illustrate the performance of the new estimators . </S>",
    "<S> the results show that , with respect to the conventional approximate estimators , the new ones significantly enhance the parameter estimation of the test equations . </S>",
    "<S> the proposed estimators are intended for the recurrent practical situation where a nonlinear stochastic system should be identified from a reduced number of partial and noisy observations distant in time . </S>"
  ]
}