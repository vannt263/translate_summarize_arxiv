{
  "article_text": [
    "let us consider a stationary random process @xmath0 with zero mean @xmath1 and the background trend @xmath2 ( some known mean function @xmath3 with unknown regression parameters @xmath4 , where @xmath5 ) then @xmath6 and @xmath7 where @xmath8 ( @xmath9 ) is a given vector and @xmath10 ( @xmath11 ) is a given matrix .    the unbiasedness constraint on the estimation statistics @xmath12 @xmath13 produces the system of @xmath14 equations in the @xmath15 unknowns @xmath16 @xmath17    for white noise @xmath18",
    "^ 2\\ } = \\sigma^2+\\sigma^2 \\omega^i_j \\rho_{ii } \\omega^i_j        \\quad \\left (   e\\{[\\hat{\\epsilon}-\\epsilon]^2\\}=\\sigma^2+\\sigma^2 \\omega ' \\lambda \\omega \\right ) \\ , \\ ] ] where @xmath19 ( @xmath20 ) is the identity auto - correlation matrix , the minimization constraint @xmath21 ^ 2\\ } } { \\partial \\omega^i_j }   = 2\\sigma^2\\rho_{ii}\\omega^i_j   + 2\\sigma^2 f_{ik}\\mu^k_j = 0 \\ , \\ ] ] where @xmath18 ^ 2\\ } = \\sigma^2 + \\sigma^2 \\omega^i_j \\rho_{ii } \\omega^i_j    + 2\\sigma^2\\underbrace{(\\omega^i_j f_{ik } - f_{jk})}_0 \\mu^k_j \\ , \\ ] ] let us add the @xmath15 equations in the @xmath14 unknowns @xmath22 to the system @xmath23 equivalent to @xmath24 substituting this term into the unbiased system @xmath25 we get @xmath26 and the kriging weights @xmath27 now , we can write the kriging estimator @xmath28 where the least - squares estimator @xmath29 is the best linear unbiased estimator for @xmath30 @xmath31 ^ 2\\ } =   \\lim_{n \\rightarrow \\infty } e\\{[\\hat{v}_j - f_{jk}\\beta^k]^2\\ } = 0 \\ , \\ ] ] where @xmath32 ^ 2\\ } = \\sigma^2 \\omega^i_j \\rho_{ii } \\omega^i_j   = - \\sigma^2 \\omega^i_j f_{ik } \\mu^k_j     = - \\sigma^2 f_{jk}\\mu^k_j   = \\sigma^2 f_{jk } ( f_{ki } \\rho^{ii } f_{ik})^{-1 } f_{kj } \\ .   % ~~ % \\left(e\\{[\\hat{v}- f'\\beta]^2\\ } % = % \\sigma^2 f ' ( f ' \\lambda^{-1 } f)^{-1 } f \\right ) \\ .\\ ] ]",
    "since for constant bias - noise mean ( @xmath33 ) @xmath34 and @xmath35 the precession of the estimation statistics can not be compared to zero value @xmath36 ^ 2\\ } = \\frac{\\sigma^2}{n}\\ ] ] let us introduce the bias - noise mean with non - zero slope ( @xmath37 ) @xmath38 and @xmath39 to find the best linear unbiased estimator for any @xmath40 @xmath41 ^ 2\\ } =   e\\{[\\hat{v}_j - f_{jk}\\beta^k]^2\\ } = 0\\ ] ] we have to fulfill @xmath42 \\left [ \\begin{array}{cc } n & n\\overline{i } \\\\    &             \\\\",
    "n\\overline{i } & n\\overline{i^2 } \\\\",
    "\\end{array }      \\right]^{-1 } \\left [ \\begin{array}{c } 1 \\\\ j \\\\",
    "\\end{array }      \\right ] = \\frac { j^2 - 2m_nj+m_{sn } } { n\\sigma^2_n } = 0\\ ] ] at @xmath43 in @xmath44 where : @xmath45 , @xmath46 , @xmath47 , @xmath48 ; and we get simple mean @xmath49 and variance @xmath50 where @xmath51 charged by the imaginary error",
    ".      000 f. tulajter , _ mean squared errors of prediction by kriging in linear models with ar(1 ) errors _ , acta math . univ .",
    "comenianae vol .",
    "lxiii , 2(1994 ) 247254 .",
    "t. suso , _ modern statistics by kriging _ , arxiv : cs.na/0609079 ."
  ],
  "abstract_text": [
    "<S> the aim of the paper is to derive the complex - valued least - squares estimator for bias - noise mean and variance . </S>"
  ]
}