{
  "article_text": [
    "the stochastic resonance ( sr ) constitutes a cooperative phenomenon wherein the addition of noise to the information carrying signal can improve in a paradoxical manner the detection and transduction of signals in nonlinear systems ( see , e.g. , @xcite for an introductory overview and @xcite for a comprehensive survey and references ) .",
    "clearly , this effect could play a prominent role for the function of sensory biology . as such ,",
    "the beneficial role of ambient and external noises has been addressed not only theoretically ( see , e.g. , @xcite ) , but also has been manifested experimentally on different levels of biological organization ",
    "e.g. , in the human visual perception @xcite and tactile sensation @xcite , in the cricket cercal sensory systems @xcite , or also in the mammalian neuronal networks @xcite and  even earlier  for the mechanoreceptive system in crayfish @xcite .",
    "presumably , the molecular mechanisms of the biological sr have their roots in stochastic properties of the ion channel arrays of the receptor cell membranes @xcite .",
    "this stimulates the interest to study sr in biological ion channels .",
    "one of the outstanding challenges in sr - research therefore is the quest to answer whether  and how  sr occurs in single and/or coupled ion channels .",
    "these channels are the evolution s solution of enabling membranes made out of fat to participate in electrical signaling .",
    "they are formed of special membrane proteins @xcite . in spite of the great diversity , these natural occurring nanotubes",
    "share some common features .",
    "most importantly , the channels are functionally bistable , i.e. they are either _ open _ , allowing specific ions to cross the membrane , or are _ closed_@xcite .",
    "the regulation of the ion flow is achieved by means of the so - called gating dynamics , i.e. , those intrinsic stochastic transitions occurring inside the ion channel that regulate the dynamics of open and closed states .",
    "the key feature of gating dynamics is that the opening - closing transition rates depend strongly on external factors such as the membrane potential ( voltage - gated ion channels ) , membrane tension ( mechanosensitive ion channels ) , or presence of chemical ligands ( ligand - gated ion channels ) .",
    "this sensitivity allows one to look upon the corresponding ion channels as a kind of single - molecular sensors which transmits an input information to the signal - modulated ion current response .",
    "recently , it has been demonstrated experimentally by bezrukov and vodyanoy @xcite that a parallel ensemble of independent , although _ artificial _ ( alamethicin ) voltage - gated ion channels does exhibit sr behavior , when the information - carrying voltage signal is perturbed by a noisy component .",
    "these authors have put forward the so - called _ non - dynamical model _ of sr .",
    "it is based on the statistical analysis of the `` doubly stochastic '' , periodically driven poisson process with corresponding voltage - dependent spiking rate @xcite .",
    "conceptually , such a model can be adequate to those situations only where the channel is closed on average with openings constituting relatively rare events .",
    "an experimental challenge is to verify whether the sr effect persists for _ single _ natural biological ion channels under realistic conditions .",
    "moreover , a second challenge is to extend the theoretical description in @xcite to account properly for the distribution of dwell times spent by the channel in the conducting state .",
    "the previous research on sr in ion channels has exclusively been restricted to the case of conventional sr , i.e. , sr with a periodic input signal . in a more general situation , however , input aperiodic signals can be drawn from some statistical distribution .",
    "this case of the so - termed _ aperiodic _ sr has recently been put forward for neuronal systems @xcite .",
    "note that the important assumption of dealing with a signal realization that is taken from a stationary process has been made in all previous studies . in practice",
    ", however , one frequently meets a situation where this stationarity assumption is not rigorously valid , because the signal has a finite duration on the time - scale set by observation . in this",
    "_ nonstationary _ situation , both the spectral and the cross - correlation sr measures are inadequate .",
    "a preferable approach is then to look for sr from the perspective of statistical information transduction @xcite .",
    "as is elucidated with this work , information theory @xcite can indeed provide a _ unified _ framework to address different types of sr , including _",
    "nonstationary _ sr .",
    "it is the main purpose of this work to investigate the possibility to enhance the transmission of information in a _",
    "ion channel in presence of a dose of noise .",
    "this task will be accomplished within a simplistic two - state markovian model for the ion channel conductance @xcite .",
    "already within such an idealization , our analysis in terms of information theory measures turns out to be rather involved .",
    "in principle , the microscopic description of the gating dynamics should be based upon the detailed understanding of the structure of channel s `` gating dynamics '' .",
    "present state of the art assumes that the voltage - sensitive gates are represented by mobile charged @xmath1  -  helix fragments of the channel protein which can dynamically block the ion conducting pathway .",
    "therefore , the gating dynamics can be described by diffusive motion of gating `` particles '' in an effective potential .",
    "then , kramers diffusion theory @xcite and its extension to the realm of _ fluctuating barriers _ ( see , e.g. , @xcite for a review and further references ) can be utilized to describe the gating dynamics .",
    "such a type of procedure , however , is still in its infancy @xcite . for our purpose , it suffices to follow a well - established phenomenological road provided by a discrete phenomenological modeling @xcite .    the simplest two - state model of this kind reflects the functional bistability of ion channels . the dichotomous fluctuations between the conducting and nonconducting conformations of _ single _ ion channels are clearly seen in the patch clamp experiments @xcite . the statistical distributions of sojourn times of the open channel state and the closed channel state ,",
    "respectively , are generically not exponentially distributed @xcite .",
    "however , one can characterize these time distributions by its average , @xmath2 , to dwell the open ( o ) state , and by its corresponding average , @xmath3 , to stay in the closed ( c ) state .",
    "these two averages depend on the transmembrane voltage @xmath4 .",
    "then , the actual multistate gating dynamics can be approximately mapped onto the effective two - state dynamics described by the simple kinetic scheme @xmath5 \\longrightarrow \\\\[-1.5em ] \\longleftarrow \\\\[-1.9em ] k_c(v ) \\end{array } o \\\\\\end{aligned}\\ ] ] with corresponding voltage - dependent effective transition rates @xmath6 and @xmath7 , respectively .",
    "although such a two - state markov description presents a rather crude approximation , it captures the main features of gating dynamics of the voltage - sensitive ion channels  the dichotomous nature and the voltage - dependence of transition rates .",
    "moreover , this model yields by construction the correct mean open ( closed ) dwell times , and the stationary probability for the channel to stay open , i.e. @xmath8 .",
    "an example for the experimental dependence of the transition rates on voltage @xmath4 can be found for a @xmath9 channel in ref .",
    "@xcite and is depicted in fig .",
    "we note that in contrast to the closing rate the _ effective _ opening rate has _ no exponential _ dependence on the voltage .",
    "thus , these two rates are not symmetric ( with respect to dependence on @xmath4 , cf .",
    "1 ) . the reason being that the two - state description results as the _ reduction _ of an intrinsic multistate ( or multi - well ) gating dynamics and thus presents only a shadow of real behavior . in this sense , the markovian approximation models the true non - markovian dynamics on a coarse grained time scale .    to proceed",
    ", one has to generalize this working model to the case with time - dependent voltages @xmath10 .",
    "here we distinguish among three components of the voltage : ( i ) the constant bias voltage @xmath11 , ( ii ) some time - dependent , unbiased signal @xmath12 , and ( iii ) a noisy component voltage @xmath13 .",
    "the noisy voltage @xmath13 is assumed to be a stationary gaussian markovian noise with zero average and root mean squared ( r.m.s . )",
    "amplitude @xmath14 . moreover , it possesses a frequency bandwidth @xmath15 .",
    "let us restrict our treatment to the situation where _ both _ the signal and the external noise are slowly varying on the time - scale set by diffusive motions occurring within the open ( or closed ) conformation .",
    "this time - scale @xmath16 typically lies in the @xmath17 range as manifested experimentally by the fast events in channel activation @xcite .",
    "we thus can apply the _ fluctuating rate _",
    "model @xcite assuming that the transition rates @xmath18 $ ] follows _ adiabatically _ the voltage @xmath19 .",
    "furthermore , we assume that the applied gaussian voltage @xmath13 presents effectively  white - noise \" on the time scale set by the decay of autocorrelations of the ion current fluctuations .",
    "the autocorrelation time @xmath20 $ ] is typically of the order of milliseconds @xcite .",
    "then , the choice of a noise bandwidth @xmath15 satisfying @xmath21 , i.e. , @xmath22 , presents a consistent specification for the fluctuating rate description .",
    "the role of external noise is thus reduced within the same two - state approximation merely to forming new , noise - dressed time - dependent transition rates @xmath23\\rangle_n$ ] .",
    "they result by taking the stochastic average of the fluctuating rates over the _",
    "external _ noise .",
    "these effective rates depend now on the noise r.m.s .",
    "amplitude @xmath14 , the static voltage @xmath11 and the time - dependent signal @xmath12 .",
    "it turns out that within in the given approximation the averaged transition rates do not depend on the noise bandwidth @xmath15 , see also appendix a.    our model for the channel dynamics thus reads    @xmath24    where @xmath25 , and @xmath26 denote the time - dependent probabilities for a single ion channel to be open or closed , respectively . the stochastic process described by eq .",
    "( [ balance ] ) is a _",
    "nonstationary _ random telegraph noise ( rtn ) with time - dependent transition rates .",
    "this model has extensively been studied in the literature , for example , to model conventional sr @xcite . moreover ,",
    "this model has been studied in ref .",
    "@xcite from the perspective of input - output cross  correlations as a simple model for _ aperiodic _ sr .",
    "however , to the best of our knowledge the detailed analysis of this cornerstone model from the perspective to use information theory @xcite to specify the information transduction process is still lacking .",
    "how can we estimate the amount of information transmitted from the input voltage signal @xmath12 to the output ion current @xmath27 ?",
    "a comparative statistical analysis of the ion current fluctuations performed in the absence and in the presence of signal allows one to answer this question .    when the channel is open a large number of ions cross the channel ; thus creating a finite current @xmath28 .",
    "this current obeys the ohmic law , @xmath29 $ ] , where @xmath30 is the conductivity of the open channel and @xmath31 is the `` reversal '' potential ( nernst potential ) for @xmath9 ion flow . when the channel is closed , the ion flow is negligible and the current is zero .",
    "we recall that the current passing through the open channel is generally time - dependent in accordance with the externally applied signal @xmath12 .",
    "however , we will assume that the information about signal is encoded in the switching events of current between zero and @xmath28 , and _ not _ in the additional modulation of @xmath28 .",
    "in other words , the information is assumed to be encoded in the signal - modulated _ conductance _ fluctuations between @xmath30 and zero @xcite .",
    "moreover , one can describe the resulting current fluctuations in terms of conductance fluctuations , i.e. , @xmath32\\ ] ] wherein @xmath33 as a two - state random point process @xcite .",
    "the sample space of @xmath33 within the time interval @xmath34 $ ] consists of stochastic trajectories which flip between zero and @xmath35 at randomly distributed switch - time points @xmath36 , @xmath37 = 1,2 , ... ; i.e. , @xmath38 this defines a continuous time point process @xmath39 , @xmath40 .",
    "next , we divide the sample space into two subspaces : ( i ) the subspace `` o '' contains all trajectories which finish in the open state at the end point @xmath41 of the considered time interval , and ( ii ) the subspace `` c '' which contains all trajectories which end in the closed state , respectively .",
    "furthermore , within each subspace the trajectories are divided into the subclasses described by the number @xmath42 which enumerates the number of intermediate flips that occurred between open and closed states in order to arrive at the final state .",
    "the probability distribution on this space is given by a sequence of joint multi - time probability densities @xmath43 for switches to occur at time @xmath44 and to end up at time @xmath41 in either the open state @xmath45 or closed state @xmath46 , respectively .",
    "this probability distribution is normalized ; i.e. , @xmath47=1\\;.\\end{aligned}\\ ] ] the probability densities @xmath43 are readily constructed by taking into account the facts that the process @xmath33 is ( semi-)markovian for any given realization of the voltage signal @xmath12 with the switching time points @xmath36 being drawn alternatingly from two different _ time - dependent _ poisson distributions @xcite . in particular ,",
    "the probability to stay in the closed conformation until time @xmath41 , given that this conformation has been occupied initially with the probability @xmath48 , is @xmath49 to obtain the remaining probability densities , we introduce the conditional probability density @xmath50 for leaving the state @xmath51 in the time interval @xmath52 $ ] given that this state was occupied with probability one at @xmath53 .",
    "analogous expressions , with indices changed from @xmath51 to @xmath54 , hold obviously also for the complementary quantities @xmath55 and @xmath56 .",
    "then , the multi - time probability densities emerge as @xmath57 for a given even number of flips , and @xmath58 for odd number of flips , respectively .",
    "the probability densities for the other subspace ending in the open state ( labeled with @xmath54 ) can be written down by use of a simple interchange of the indices @xmath51 and @xmath54 in eqs .",
    "( [ q0 ] ) - ( [ qodd ] ) .",
    "the above reasoning yields a _",
    "complete _ probabilistic description of the stochastic switching process that is related to the conductance fluctuations @xmath33 . in terms of the stochastic path description , the probability that the channel is open at the instant time @xmath41",
    "is therefore given by @xmath59 an analogous expression holds also for the probability of the closed conformation @xmath26 . upon differentiating @xmath25 and @xmath26 with respect to time @xmath41",
    "one can check that these time - dependent probabilities indeed satisfy the kinetic equations ( [ balance ] ) .",
    "in the following we derive the general theory for various information measures that can be used to quantify the information gain obtained from an input signal @xmath12 being transduced by the ion channel current realizations @xmath60 when @xmath12 is switched on , versus the case with @xmath12 being switched off .",
    "intuitively , this information describes the difference in uncertainty about the current realizations in the absence and in the presence of the signal @xmath12 .",
    "we start out by reviewing the necessary background .",
    "let us first consider a _ discrete random variable _ @xmath61 . as demonstrated by k. shannon in 1948 @xcite ( his expression was discovered independently by n. wiener ) , the information entropy    @xmath62    provides a measure for the uncertainty about a particular realization @xmath63 of @xmath61 @xcite . in eq .",
    "( [ sh1 ] ) , the set @xmath64 denotes the normalized probabilities for the realizations @xmath63 to occur , @xmath65 .",
    "the positive constant @xmath66 in ( [ sh1 ] ) defines the unit used in measurement .",
    "if the information entropy is measured in binary units , then @xmath67 , natural units yield @xmath68 , and digits give @xmath69 .",
    "this measure attains a minimum ( being zero ) if and only if one @xmath70 for a particular value of @xmath37 , and all others satisfying @xmath71 .",
    "it reaches a maximum if @xmath72 . the information entropy for a probability distribution is therefore a measure of how strongly it is peaked about a given alternative .",
    "uncertainty _ is consequently large for spread out distributions and small for concentrated ones .",
    "the application of an external signal ( perturbation ) results in a change of probabilities @xmath73 and consequently in entropy @xmath74 .",
    "the gained information @xmath75 is then defined by the corresponding change in entropy , i.e. @xmath76 .    the generalization of the information concept onto the case of continuous variable @xmath77 presents no principal difficulties . in this case",
    "a proper definition of entropy reads @xmath78dx \\nonumber \\\\   & \\equiv & -\\kappa\\int p(x)\\ln [ p(x)]dx-\\kappa\\ln \\delta x;\\end{aligned}\\ ] ] wherein @xmath79 is the probability density and @xmath80 denotes the precision with which the variable @xmath77 can be measured ( coarse graining of cell size ) . as is clearly seen from eq .",
    "( [ sh2 ] ) , the _ absolute _ entropy of a continuous variable is not well defined since it diverges in the limit @xmath81",
    ". nevertheless , the _ entropy difference : = information _ is well - defined and _ does not _ depend on the precision @xmath80 .",
    "the generalization of information theory onto the case of stochastic processes is not trivial . in our case",
    ", the proper definition of entropy of the switch - point process @xmath82 , considered on the time interval @xmath83 $ ] , is  by analogy with eq .",
    "( [ sh2 ] ) ",
    "@xmath84   & & \\equiv    -\\kappa\\sum_{\\alpha= o , c}\\big \\{q_0^{\\alpha}(t ) \\ln q_0^{\\alpha}(t ) \\nonumber \\\\   & & +   \\sum_{s=1}^{\\infty } \\int_{0}^{t}d\\tau_s   \\int_{0}^{\\tau_s}d\\tau_{s-1 } ... \\int^{\\tau_2}_{0}d\\tau_1 q^{\\alpha}_s(t,\\tau_s, .. ,\\tau_1)\\nonumber \\\\ & & \\times   \\ln [ q_s^{\\alpha}(t,\\tau_s, .. ,\\tau_1 ) ( \\delta\\tau)^s ] \\big\\}\\;,\\end{aligned}\\ ] ] where @xmath85 denotes the precision of time measurement , and the symbol @xmath86 indicates that the entropy is defined in presence of the signal @xmath12 .",
    "the presence of the time resolution @xmath85 in ( [ tau - s ] ) gives the name `` @xmath0-entropy '' to this quantity @xcite .",
    "it is very important that in the contrast to the case of a continuous variable , the contribution of the finite time resolution @xmath85 to the @xmath0-entropy can not be recasted in a form like @xmath87 , cf .",
    "( [ sh2 ] ) .",
    "we note that its contribution _ depends on the statistics of the random process _ , being different in the presence and in the absence of signal .",
    "this is why not only the _ absolute _ entropy , but also the _ difference _ of entropies become not well defined for continuous time point random processes . as a result ,",
    "the definition of information in this manner becomes rather ambiguous .    for a sufficiently large time interval @xmath88 the averaged information transferred per unit time from the input voltage signal @xmath12 to the output current signal @xmath27",
    "can be defined as follows@xcite @xmath89 this information measure can be termed @xmath0-information per unit time to underline its dependence on the time resolution @xmath85 . upon taking the derivative of @xmath90 $ ] in eq .",
    "( [ tau - s ] ) with respect to time @xmath41 we obtain after some involved algebra ( cf .",
    "appendix b ) the result @xmath91}{dt}=-\\kappa\\sum_{\\alpha = o , c}\\bar k_{\\alpha}(t)\\ln \\big(\\bar k_{\\alpha}(t)\\delta\\tau / e \\big)p_{\\bar\\alpha}(t)\\;,\\end{aligned}\\ ] ] where @xmath92 , if @xmath93 and _ vice versa_. together with eq . ( [ balance ] ) and the definition ( [ t - inf ] ) the prominent result in eq .",
    "( [ result1 ] ) allows one to express the @xmath0-information for arbitrary signal @xmath12 through straightforward quadratures .",
    "the @xmath0-information concept has been used in fact to analyze the information transfer in neuronal systems in ref .",
    "@xcite . however , the strong dependence of @xmath0-information on the time precision @xmath85 @xcite presents surely an undesirable _ subjective _ feature . in search for _ objective _",
    "information measures we consider the information transfer in terms of the mutual information measure .      to introduce the reader to the mutual information concept",
    ", we follow the reasoning of shannon @xcite : the signals @xmath12 are drawn from some statistical distribution characterized by the probability density functional @xmath94 $ ] . noting that the probability densities @xmath95 in eqs .",
    "( [ q0 ] ) , ( [ qeven ] ) and ( [ qodd ] ) are in fact _ conditional _ with respect to the given realization of @xmath12 , one can define the joint probability densities , @xmath96 $ ] for the corresponding stochastic processes @xmath12 and @xmath27 .",
    "moreover , one can define the averaged probability densities @xmath97 for the process @xmath27 _ in the presence of the process _",
    "@xmath12 , where the path integral @xmath98 $ ] denotes stochastic averaging over the signal realizations .",
    "the mutual information between the stochastic process @xmath12 and @xmath27 can then be defined as the entropy difference @xmath99 where @xmath100 is the @xmath0-entropy of the averaged process defined similarly to ( [ tau - s ] ) , but with the _ averaged _ probability densities @xmath101 .",
    "note that making use of the bayes rules one can transform the definition ( [ mi ] ) into a form which makes transparent the fact that the mutual information @xmath102 is a symmetric functional of the processes @xmath12 and @xmath27 and provides a _ nonlinear _ cross - correlation measure between them @xcite .",
    "we , however , will take advantage of an equivalent form ; it is obtained from eq .",
    "( [ mi ] ) by using eq .",
    "( [ tau - s ] ) , yielding @xmath103 as is clearly deduced from eq .",
    "( [ mi ] ) , shannon s mutual information _ does not _ depend  due to its skillful definition in eq .",
    "( [ mi ] )  on the time resolution @xmath85 .",
    "this underpins its advantage over the information measure in eq .",
    "( [ t - inf ] ) .",
    "moreover , the functional form ( [ mi2 ] ) inherits important connections between the mutual information and another prominent information measure  the ( relative ) kullback entropy or termed also the _",
    "information gain_.      the information gain @xcite is given in terms of the relative entropy of the given statistical distribution with respect to some reference distribution . in our case , the reference distribution corresponds to the stationary ion current fluctuations in the absence of the voltage signal @xmath12 . for a given signal @xmath12",
    "the information gain reads @xmath104",
    "\\equiv   \\kappa\\sum_{\\alpha = o , c}\\big\\{q_0^{\\alpha}(t ) \\ln\\frac{q_0^{\\alpha}(t)}{q_0^{(0)\\alpha}(t ) } \\nonumber \\\\ & + & \\sum_{s=1}^{\\infty } \\int_{0}^{t}d\\tau_s\\int_{0}^{\\tau_s}d\\tau_{s-1 } ... \\int^{\\tau_2}_{0}d\\tau_1 q^{\\alpha}_s(t,\\tau_s, .. ,\\tau_1)\\nonumber \\\\ & & \\times \\ln\\frac{q_s^{\\alpha}(t,\\tau_s, .. ,\\tau_1 ) } { q_s^{(0)\\alpha}(t,\\tau_s, .. ,\\tau_1 ) } \\big\\}\\;,\\end{aligned}\\ ] ] where the index @xmath105 in @xmath106 refers to the case when no voltage signal is applied .",
    "the relative entropy can be regarded as the signal - induced deviation of entropy of the random point process @xmath39 from its stationary value obtained in the absence of signal .",
    "although the _ absolute _ entropy of such a switch - time point process @xmath39 depends strongly on the time resolution @xmath85 and thus is not well - defined , the deviation of entropy from the steady - state value can be defined _ independently _ of @xmath85 via eq .",
    "( [ kul ] ) . for stochastic processes",
    "this relative entropy plays the role similar to the entropy difference ; thus characterizing an information measure .",
    "this justifies its given name  the information gain .",
    "in contrast to mutual information this measure can be defined for _ deterministic _ signals as well .",
    "consequently , the information gain can be used as an information measure both for conventional and for aperiodic sr .",
    "moreover , this measure is also well - defined for _ nonstationary _ signals and therefore can be used to quantify _ nonstationary _ sr as well .",
    "in contrast to the information gain the mutual information is more difficult to handle analytically .",
    "this is rooted in the fact that the _ averaged _ point process @xmath39 is non - markovian process with corresponding joint probabilities not factorizing into products of conditional probabilities .",
    "the following important inequality can be deduced @xmath107 \\big\\rangle_{signal}-{\\cal k}_t[\\langle i\\rangle_{signal}]\\nonumber \\\\ & &   \\leq \\big\\langle   { \\cal k}_t[i|v_s ] \\big\\rangle_{signal}\\;.\\end{aligned}\\ ] ] in eq .",
    "( [ ineq ] ) , @xmath108\\geq 0 $ ] is the relative entropy of the _ averaged _ process @xmath33 defined similarly to eq .",
    "( [ kul ] ) , but with the averaged multi - time probability densities @xmath109 .",
    "the averaged information gain provides thus an upper bound for the mutual information .",
    "moreover , applying a weak gaussian signal in the limit @xmath110 one can show that the difference between the mutual information and the averaged information gain in ( [ ineq ] ) is of order @xmath111 , where @xmath112 denotes the r.m.s .",
    "amplitude of signals @xmath113 . on the other hand",
    ", it is shown below that the averaged information gain per unit time is of the order @xmath114 and does not depend , within the given lowest order approximation , on other statistical parameters of signal .",
    "thus , the upper bound for mutual information in eq .",
    "( [ ineq ] ) can indeed be achieved with an accuracy of @xmath114 .",
    "this fact opens a way to calculate the informational capacity for weak signals @xcite .",
    "the information gain can be evaluated from eq .",
    "( [ kul ] ) without further problems . by differentiating @xmath115",
    "$ ] with respect to @xmath88 we find following to the reasoning detailed in appendix b the remarkable simple , _ main _ result for the _ rate of information gain _ , i.e. @xmath116}{dt}=&&\\kappa\\sum_{\\alpha = o , c}[\\bar k_{\\alpha}(t)\\ln \\big(\\frac{\\bar k_{\\alpha}(t)}{\\bar k_{\\alpha}(v_0 ) } \\big)\\nonumber \\\\ & &   -\\bar k_{\\alpha}(t ) + \\bar k_{\\alpha}(v_0)]p_{\\bar\\alpha}(t)\\;,\\end{aligned}\\ ] ] wherein @xmath117 denote the stationary transition rates in the absence of signal . together with eq .",
    "( [ balance ] ) this equation _",
    "completely _ determines the information gain within the considered two - state model for any applied signal @xmath12 . for the case of a periodic signal @xmath12 ( conventional sr ) , or a stochastic stationary signal ( aperiodic sr ) , one should average additionally eq .",
    "( [ eq1 ] ) over the signal fluctuations and to take the limit @xmath118 . in doing so , eq . ( [ eq1 ] ) yields the stationary rate of information gain . for weak stochastic signals this quantity also defines the informational capacity @xcite    @xmath119 \\big\\rangle_{signal}/t \\;.\\ ] ]",
    "if the signal is deterministic and has a finite duration , one obtains the _ total _ information gain @xmath120 by integrating eq .",
    "( [ eq1 ] ) in the range from @xmath121 to @xmath122 .",
    "in the following we apply our developed information theory concepts to investigate sr in a k@xmath123 ion channel .",
    "we restrict our treatment to the case of weak signals with a time duration which strongly exceeds the autocorrelation time of current fluctuations @xmath124",
    ". then , eqs .",
    "( [ eq1 ] ) and ( [ balance ] ) yield after some elementary calculations in the lowest order of @xmath12 , @xmath125}{dt}=r(v_0,\\sigma ) v_s^2(t)\\end{aligned}\\ ] ] where the form factor @xmath126,\\end{aligned}\\ ] ] depends  via the rates @xmath127  on the static voltage @xmath11 and on the r.m.s .",
    "noise amplitude @xmath14 . in eq.([res2 ] ) , @xmath128,\\ ; \\alpha = o , c$ ] , and the noise averaged rates @xmath129 are given in the appendix a for a k@xmath123 channel in eqs .",
    "( [ r1 ] ) and ( [ r2 ] ) . in the case of stationary stochastic signals or for a periodic driving , eq .",
    "( [ res1 ] ) provides after stochastic averaging , or averaging over the driving period of applied voltage @xmath12 , respectively , the stationary rate of information gain . for signals of finite duration",
    "the total information gain is directly proportional to the total intensity of signal @xmath130 , @xmath131 as a result we find that weak signals of the the same intensity @xmath132 produce equal information gains .",
    "the occurrence of three different kinds of sr behavior , i.e. , periodic , aperiodic , and nonstationary sr clearly depends on the behavior of the form function @xmath133 _ vs. _ the r.m.s .",
    "noise amplitude @xmath14 .",
    "we recall that the static voltage ( membrane potential ) @xmath11 controls whether the ion channel is on average open or closed , cf .",
    "fig.[fig1 ] . in fig .",
    "[ fig2 ] , we depict the behavior of the function @xmath133 _ vs. _ the r.m.s . noise amplitude for different values of the applied static voltage .",
    "if the k@xmath123 ion channel is closed on average we observe that the information gain becomes strongly be amplified by noise , and even can pass through a maximum , i.e. sr occurs , cf .",
    "in contrast , when the stationary probability for an open channel @xmath134 becomes appreciably large , the addition of an additional dose of noise can only deteriorate the detection of signal . as a result ,",
    "the information gain decreases monotonically with increasing noise amplitude , cf .",
    "2b . this _",
    "no_-sr behavior occurs already at a static bias of @xmath135 mv yielding @xmath136 .",
    "note also , if the channel is predominantly open , the information gain becomes practically insensitive to the external noise , cf .",
    "the bottom curve in fig . 2b .",
    "although the information gain can slightly be increased versus the noisy intensity in this case ( @xmath137 ) , this effect is hardly of importance because the overall information gain diminishes drastically with increasing the static voltage @xmath11 ( see fig .",
    "the occurrence of sr in the considered single ion channel thus requires that the channel is predominantly resting in its closed state .",
    "let us now summarize the main results of this work .",
    "we have studied an illustrative two - state model for a single ion channel gating dynamics from an information theoretic point of view .",
    "the channel serves as an information channel transducing information from the applied time - dependent voltage signal to the ion current fluctuations .",
    "three different information theory measures have been developed to characterize stochastic resonance . from our viewpoint",
    "it is advantageous to use an information measure which is independent of time resolution @xmath85 .",
    "we argued that the rate of information gain constitutes a unified characteristic measure for periodic ( conventional ) , aperiodic and nonstationary stochastic resonance . for conventional ( periodic ) sr and aperiodic sr",
    "this measure yields the averaged information gain per unit time .",
    "moreover , for weak stochastic signals it gives also the informational capacity , i.e. , the maximal mutual information which can be transferred per unit time for random signals with a fixed r.m.s .",
    "the concept of information gain can also be applied to the case of _ nonstationary _ deterministic signals with finite duration , i.e. , nonstationary sr , cf .",
    "( [ res2 ] ) , ( [ res3 ] ) .",
    "our main result is the closed formula for the rate of information gain in ( [ eq1 ] ) : it can be evaluated in a straightforward manner by using the corresponding probabilities of the two - state gating dynamics in ( [ balance ] ) .",
    "the information gain itself follows upon a time integration . in presence of weak driving",
    "we derived handy analytical results given in eqs .",
    "( [ res1 ] ) , ( [ res2 ] ) , and ( [ res3 ] ) . for voltage input signals referring to a stationary process the averaged rate of the information gain is determined by the r.m.s .",
    "amplitude of the signal input and by the form factor @xmath133 . in the case of a nonstationary signal of finite duration , the total information gain is the product of this very form function and the integrated signal intensity   @xmath132 .",
    "the experimental procedure of determining the rate of information gain can be formulated along the lines used for the @xmath0-entropy in ref .",
    "first , one finds the corresponding probability histograms in the presence and in the absence of signal and then evaluates the information gain for the related binary stochastic chains .",
    "naturally , this so obtained information gain will still depend on the time resolution @xmath85 .",
    "however , in contrast to the @xmath0-information , this experimentally determined information gain should exhibit a much weaker dependence on the time resolution @xmath85 . by using increasingly smaller time grids @xmath85",
    ", the experimentally obtained rate of information gain will approach a definite value .",
    "our theoretical results have been applied to investigate the phenomenon of stochastic resonance in a potassium - selective _ shaker ir _",
    "ion channel @xcite , as depicted with in fig .",
    "interestingly enough , we find that periodic , aperiodic or nonstationary sr for this sort of ion channel , as quantified by the rate of information gain , is exhibited only for a situation in which the channel resides on average in the closed state .",
    "this type of behavior is rooted in the asymmetry of two rates @xmath138 and @xmath139 ; with @xmath138 depicting a characteristic steep , threshold - like behavior , cf .",
    "fig . 1 .",
    "our sr - feature is similar to the study of parallel sr in an array of alamethicin channels @xcite , although the two situations are , however , not directly comparable .",
    "we note that the amount of transmitted information crucially depends on the membrane potential @xmath11 . for the studied model ,",
    "the information transfer is optimized at zero noise level near @xmath140 mv when the opening probability becomes appreciable ( note the upper curve in fig .",
    "however , under such optimal conditions the addition of external noise has the effect of only further deteriorating the rate of information transfer ( fig .",
    "2b ) . upon further increasing the static bias @xmath11 the ion channel probability to stay open increases .",
    "the rate of information transfer then diminishes and becomes practically insensitive to the input noise level .",
    "these results hopefully will motivate researchers to measure the predicted sr behavior in single potassium ion channels . ever since the discovery of the sr phenomenon , the quest to use noise to optimize and control the transduction and relay of biological information has been one of the holy grails of sr research .",
    "given this challenge , such and related experiments are much needed in order to settle the issue in question .",
    "the authors gratefully acknowledge the support of this work by the german - israel - foundation g.i.f . g-411 - 018.05/95 , as well as by the deutsche forschungsgemeinschaft ( sfb 486 and ha1517/13 - 2 ) .",
    "the opening and closing rates for the effective two - state model can be found from the voltage - dependent average dwell times .",
    "the latter can be determined from the experimental recordings .",
    "the experimental dependence of the effective transition rates on voltage @xmath11 for the potassium - selective channel _ shaker ir _ embedded in the membrane of a _ xenopus _ oocyte at _ fixed _ temperature @xmath141 have been fitted @xcite by a hodgkin - huxley type of data parameterization @xcite .",
    "this corresponding fitting procedure yields @xmath142 which are depicted in fig .",
    "[ fig1 ] . note that we replace the original fit of the closing rate @xmath139 in @xcite by a new expression in eq .",
    "( a1 ) . unlike to @xcite ,",
    "our fit of experimental data in @xcite is valid now also for positive voltages @xmath4 .",
    "one should emphasize , that the two rates in eq .",
    "( a1 ) are strongly asymmetric with respect to their dependence on voltage . in particular , the opening rate @xmath143 depicts a steep , threshold - like behavior , see fig .",
    "1 . in this work we explicitly use these experimental findings in our calculations . the rates in eq .",
    "( [ rates ] ) are measured in @xmath144 and the voltage in mv . according to our model study",
    ", the input voltage reads @xmath145 when no additional signal is applied .",
    "these eqs .",
    "( [ rates ] ) must be averaged over the realizations of @xmath13 to obtain the noise averaged rates @xmath146 and @xmath147 . for a gaussian voltage noise @xmath13",
    "this averaging of the exponential in the first equation in ( [ rates ] ) is governed by the second cumulant , yielding @xmath148 where @xmath149 .",
    "the averaged opening rate @xmath150 unfortunately can not be analytically simplified further . however",
    ", this rate along with its derivative @xmath151 can readily be evaluated numerically from eq .",
    "( [ r2 ] ) .",
    "the purpose of this appendix to provide the readers with some details of calculation of the entropic measures for the continuous time random point two - state process considered in this paper .",
    "first , we note two useful properties of the multi - time probability densities which can be established from eqs .",
    "( [ qeven ] ) , ( [ qodd ] ) .",
    "namely , @xmath152 and @xmath153 the index @xmath1 in eqs . ( [ a1 ] ) , ( [ a2 ] ) takes the values @xmath154 ; and the index @xmath155 takes the value @xmath92 , if @xmath93 , and _",
    "vice versa_. using eqs .",
    "( [ a1 ] ) , ( [ a2 ] ) one can check that @xmath156 given in eq .",
    "( [ solution ] ) do satisfy eq .",
    "( [ balance ] ) .",
    "furthermore , let us consider the @xmath0-entropy in eq .",
    "( [ tau - s ] ) as a sum of two contributions , @xmath90=\\kappa\\sum_{\\alpha = o , c}s_{\\alpha}(t)$ ] , with @xmath157 defined from the corresponding partitioning in eq .",
    "( [ tau - s ] ) .",
    "then , using repeatedly the relationships ( [ a1 ] ) and ( [ a2 ] ) we obtain after some straightforward , but lengthy calculations @xmath158 and @xmath159 the addition of eq .",
    "( [ a3 ] ) and eq .",
    "( [ a4 ] ) then yields eq .",
    "( [ result1 ] ) . likewise ,",
    "splitting the information gain @xmath160 $ ] in eq .",
    "( [ kul ] ) into the sum of two contributions , @xmath160= \\kappa\\sum_{\\alpha = o , c } k_{\\alpha}(t)$ ] , and invoking the properties ( [ a1 ] ) and ( [ a2 ] ) we obtain after some algebra @xmath161p_o(t)\\nonumber \\\\ & &   + \\bar k_o(t)\\ln\\big ( \\frac{\\bar k_o(t ) } { \\bar k_o(v_0)}\\big)p_c(t)\\end{aligned}\\ ] ] and @xmath162p_c(t)\\nonumber \\\\ & &   + \\bar k_c(t)\\ln\\big ( \\frac{\\bar k_c(t ) } { \\bar k_c(v_0)}\\big)p_o(t)\\;.\\end{aligned}\\ ] ] adding eq .",
    "( [ a4 ] ) and eq .",
    "( [ a5 ] ) results after multiplying with @xmath66 in our main result in eq .",
    "( [ eq1 ] ) .",
    "k. wiesenfeld and f. jaramillo , chaos * 8 * , 539 ( 1998 ) .",
    "l. gammaitoni , p. hnggi , p. jung , and f. marchesoni , rev .",
    "* 70 * , 223 ( 1998 ) .",
    "a. longtin , a. bulsara , and f. moss , phys .",
    "lett . * 67 * , 656 ( 1991 ) .",
    "e. simonotto , _ et al .",
    "lett . * 78 * , 1186 ( 1997 ) .",
    "collins , t.t .",
    "imhoff , and p. grigg , nature ( london ) * 383 * , 770 ( 1996 ) .",
    "j. e. levin , j. p. miller , nature ( london ) * 380 * , 165 ( 1996 ) .",
    "gluckman , p.",
    "netoff , m.l .",
    "spano , and s.j .",
    "schiff , chaos * 8 * , 588 ( 1998 ) .",
    "douglass , l.wilkens , e. pantazelou , and f. moss , nature ( london ) * 365 * , 337 ( 1993 ) .",
    "b. hille , _ ionic channels of excitable membranes _",
    "( sinauer associates , sunderland , ma , 1992 ) .",
    "s.m . bezrukov and i. vodyanoy , nature ( london ) * 378 * , 362 ( 1995 ) .",
    "s.m . bezrukov and i. vodyanoy , nature ( london ) * 385 * , 319 ( 1997 ) ; s.m .",
    "bezrukov , phys .",
    "a * 248 * , 29 ( 1998 ) . j. j. collins , c. c. chow , and t. t. imhoff , nature ( london ) * 376 * , 236 ( 1995 ) ; j. j. collins , c. c. chow , and t. t. imhoff , phys .",
    "e * 52 * r3321 ( 1995 ) ; j. j. collins , c. c. chow , a. c. capela and t. t. imhoff , phys .",
    "e * 54 * 5575 ( 1996 ) ; c. heneghan , c. c. chow , j. j. collins , t. t. imhoff , s. b. lowen , and m. c. teich , phys",
    "e * 54 * , r2228 ( 1996 ) .",
    "m. stemmler , network * 7*,687 ( 1996 ) .",
    "a. d. bulsara and a. zador , phys .",
    "e * 54 * , r2185 ( 1996 ) .",
    "k. shannon , bell system technical journal * 27 * , pp .",
    "379 - 423 , 623 - 656 ( 1948 ) .",
    "h. a. kramers , physica * 7 * , 284 ( 1940 ) .",
    "p. hnggi , p. talkner , and m. borkovec , rev .",
    "* 62 * , 251 ( 1990 ) .",
    "p. reimann and p. hnggi , in : springer - series _ lecture notes in physics _",
    ", ed . by l.",
    "schimansky - geier and t. pschel ( springer , berlin , 1997 ) , vol .",
    "127 - 139 .",
    "d. sigg , h. qian , and f. bezanilla , biophys .",
    "j. * 76 * , 782 ( 1999 ) .",
    "b. sakmann and e. neher ( eds . ) , _ single - channel recording _ , 2nd ed .",
    "( plenum , new york , 1995 ) . h. salman , y. soen , and e. braun , phys . rev . lett . * 21*,4458 .",
    "( 1996 ) ; h. salman and e. braun , phys .",
    "e * 56 * , 852 ( 1997 ) .",
    "s. marom , h. salman , v. lyakhov , and e. braun , j. membrane biol .",
    "* 154 * , 267 ( 1996 ) .",
    "b. mcnamara and k. wiesenfeld , phys .",
    "a * 39 * , 4854 ( 1989 ) .",
    "a. neiman , l. schimansky - geier , and f. moss , phys .",
    "e * 56 * , r9 ( 1997 ) .",
    "the problem of extracting these conductance fluctuations from the current recordings in the presence of a time - dependent ( e.g. , periodic ) driving is explained in : d. petracchi _",
    "_ , j. stat . phys . * 70 * , 393 ( 1993 ) .",
    "van kampen,_stochastic processes in physics and chemistry , 2-d , enlarged and extended edition _ ( north - holland , amsterdam , 1992 ) .",
    "r. l. stratonovich , _ topics in the theory of random noise _ ,",
    "i ( gordon and breach , new york , 1963 ) .",
    "it is remarkable that the permutation invariance of @xmath163 with respect to the set of probabilities @xmath64 and the property of additivity , i.e. @xmath164  in case that the probabilities factorize in the composed state space , or subadditivity , in case of statistical dependence of two composed ( i.e. cartesian product ) measure spaces  characterize the shannon entropy @xmath165 almost uniquely : any functional satisfying these requirements is a linear combination of the shannon entropy and the hartley entropy ( @xmath166 , with @xmath167 being the number of @xmath168 that are different from zero )",
    ". the additional requirements of ( i ) @xmath169 being a continuous function of @xmath170 , @xmath171 , and ( ii ) @xmath172p_1,p_2, ... ,p_n ) = s(p_1, .... ,p_n )   + p_1 s(t,1-t)$ ] , with @xmath173 ( a. feinstein , _ foundations of information theory _ , ( mc graw hill , new york , 1958 ) ) determine then the shannon entropy uniquely .",
    "p. gaspard and x .- j .",
    "wang , phys .",
    "rep . * 235 * , 292 ( 1993 ) .",
    "f. rieke , d. warland , r. de ruyter van steveninck , and w. bialek , _ spikes : exploring the neural code _",
    "( mit press , cambridge , ma , 1997 ) .",
    "strong , r. koberle , r. r. de ruyter van steveninck , and w. bialek , phys .",
    "lett . * 80 * , 197 ( 1997 ) .",
    "the many facets of entropy are beautifully outlined in : a. wherl , rep .",
    "phys . * 30*,119 ( 1991 ) ; see also : c. beck and f. schlgl , _ thermodynamics of chaotic systems : an introduction _ , ( cambridge university press , cambridge , 1993 ) . the informational capacity of an informational channel is defined as the maximal rate of mutual information obtained for all possible statistical distributions of input signals with",
    "amplitude @xcite .",
    "a. l. hodgkin and a. f. huxley , j. physiol .",
    "* 117 * , 500 ( 1952 ) ."
  ],
  "abstract_text": [
    "<S> we identify a unifying measure for stochastic resonance ( sr ) in voltage dependent ion channels which comprises periodic ( conventional ) , aperiodic and nonstationary sr . within a simplest setting , </S>",
    "<S> the gating dynamics is governed by two - state conductance fluctuations , which switch at random time points between two values . </S>",
    "<S> the corresponding continuous time point process is analyzed by virtue of information theory . in pursuing this goal </S>",
    "<S> we evaluate for our dynamics the @xmath0 -information , the mutual information and the rate of information gain . as a main result we find an analytical formula for the rate of information gain that solely involves the probability of the two channel states and their noise averaged rates . for small voltage </S>",
    "<S> signals it simplifies to a handy expression . </S>",
    "<S> our findings are applied to study sr in a potassium channel . </S>",
    "<S> we find that sr occurs only when the closed state is predominantly dwelled . upon increasing the probability for the open channel state </S>",
    "<S> the application of an extra dose of noise monotonically deteriorates the rate of information gain , i.e. , no sr behavior occurs . +    2 </S>"
  ]
}