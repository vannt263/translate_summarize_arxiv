{
  "article_text": [
    "generalized linear mixed models ( glmms ) extend generalized linear models ( glms ) by introducing random effects to account for within - subject association and have wide applications .",
    "estimation of glmms using maximum likelihood is , however , challenging as the integrals over random effects are intractable and have to be approximated using computationally intensive methods such as numerical quadrature or markov chain monte carlo ( mcmc ) .",
    "various approximate methods for fitting glmms have been proposed , such as penalized quasi - likelihood @xcite , laplace approximation and its extensions @xcite , gaussian variational approximation @xcite and integrated nested laplace approximations @xcite .",
    "stochastic approximation has also been used in conjunction with mcmc @xcite and the expectation maximization ( em ) algorithm @xcite to fit glmms .",
    "recently , @xcite demonstrated how glmms can be fitted using variational bayes ( vb , * ? ? ?",
    "* ) via an algorithm called nonconjugate variational message passing @xcite",
    ". a popular method of approximation , vb is deterministic and requires much less computation time than mcmc methods . in vb ,",
    "the intractable true posterior is approximated by a factorized distribution , which is optimized to be close to the true posterior in terms of kullback - leibler divergence .",
    "variational message passing @xcite is an algorithmic implementation of vb for conjugate - exponential models @xcite .",
    "@xcite extended variational message passing to nonconjugate models by assuming that the factors in vb belong to some exponential family .",
    "the nonconjugate variational message passing algorithm for glmms @xcite has to update local variational parameters associated with every unit before re - estimating the global variational parameters at each iteration .",
    "this algorithm is inefficient for large data sets and is unsuitable for streaming data as it can never complete one iteration . to address these issues , @xcite proposed optimizing the vb objective function using stochastic gradient approximation @xcite , where gradients computed on small random subsets of data are used to approximate the true gradient over the whole data set .",
    "this approach reduces the computational cost for large data sets significantly @xcite .",
    "@xcite focused on developing stochastic variational inference for conjugate - exponential models .    in this article , we extend stochastic variational inference to nonconjugate models and develop a stochastic nonconjugate variational message passing algorithm for fitting glmms that is scalable to large data sets .",
    "a strong motivation for developing stochastic gradient optimization algorithms is their efficiency in terms of memory .",
    "as data are processed in mini - batches , analysis of data sets too large to fit into memory can still be contemplated .",
    "we focus on poisson and logistic glmms , and applications in longitudinal data analysis .",
    "our paper makes three contributions .",
    "first , we show how updates in nonconjugate variational message passing can be used in stochastic natural gradient optimization of the variational lower bound .",
    "second , we show that variational message passing facilitates an automatic computation of diagnostics for prior - likelihood conflict ( useful for bayesian model criticism ) and provides an attractive alternative to simulation - based mcmc methods .",
    "third , we demonstrate that for moderate - sized data sets , convergence can be accelerated by using the stochastic version of nonconjugate variational message passing in the initial stage of optimization before switching to the standard version .",
    "recently , there is increasing interest in developing vb algorithms capable of handling large data sets or streaming data ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "stochastic optimization is an important tool in parameter estimation for large data sets ( e.g. * ? ?",
    "* ; * ? ? ?",
    "* ) and has been considered in the context of vb .",
    "for example , the online vb algorithms for latent dirichlet allocation @xcite and the hierarchical dirichlet process @xcite are based on stochastic natural gradient optimization of the vb objective function , with data processed one at a time or in mini - batches .",
    "@xcite generalized these methods to derive stochastic variational inference for conjugate - exponential family models .",
    "stochastic approximation methods have also been considered by @xcite , @xcite and @xcite for optimization of vb objective functions containing intractable integrals .",
    "@xcite proposed a stochastic approximation algorithm that does not require analytic evaluation of integrals and allows fixed - form vb to be applied to any posterior available in closed form up to the proportionality constant .",
    "@xcite considers the approach of @xcite for fitting glmms , and analyzes large data sets by combining variational approximations learned in parallel on smaller partitions .",
    "random effects in each partition were treated as a single block . in this paper , we consider a different approach of fitting glmms to large data sets by using nonconjugate variational message passing within stochastic variational inference .",
    "variational posteriors of random effects from different clusters are assumed to be independent and partial noncentering @xcite is used to improve posterior approximation .",
    "global variational parameters are then updated using stochastic gradient approximation based on mini - batches of optimized local variational parameters .",
    "model checking is an important part of statistical analyses . in the bayesian approach",
    ", assumptions are made about the sampling model and prior , and prior - likelihood conflict arises when the observed data are very unlikely under the prior model .",
    "@xcite discuss how to assess whether there is prior - data conflict and @xcite proposed a graphical diagnostic , the local critique plot , for identifying influential statistical modelling choices at the node level .",
    "see also @xcite for a review of other methods in bayesian model criticism .",
    "@xcite proposed a diagnostic test for identifying divergent units in hierarchical models , based on measuring the conflict between the likelihood of a parameter and its predictive prior given the remaining data .",
    "a simulation - based approach was adopted and diagnostic tests were carried out using mcmc .",
    "we show that the approach of @xcite can be approximated in the variational message passing framework .",
    "section 2 introduces some notation .",
    "section 3 specifies the model and motivates partial noncentering for glmms . the stochastic nonconjugate variational message passing algorithm",
    "is developed in section 4 .",
    "section 5 describes how variational message passing facilitates computation of prior - likelihood conflict diagnostics .",
    "section 6 considers examples including real and simulated data and section 7 concludes .",
    "we use @xmath0 to denote the @xmath1 column vector with all entries equal to 1 and @xmath2 to denote the @xmath3 identity matrix .",
    "scalar functions such as @xmath4 applied to vector arguments are evaluated element by element .",
    "we use @xmath5 to denote element by element multiplication of two vectors .",
    "if @xmath6 is a @xmath1 vector , we use @xmath7 to denote the @xmath3 diagonal matrix with diagonal entries given by @xmath6 .",
    "on the other hand , if @xmath8 is a @xmath3 square matrix , we use @xmath9 to denote the @xmath1 vector containing the diagonal entries of @xmath8 .",
    "we consider one - parameter exponential family models which are specified as follows .",
    "let @xmath10 denote the @xmath11th response in cluster @xmath12 , @xmath13 , @xmath14 .",
    "conditional on a vector of length @xmath15 of random effects @xmath16 , independently distributed as @xmath17 , @xmath10 is independently distributed as @xmath18 where @xmath19 is the canonical parameter and @xmath20 and @xmath21 are functions specific to the exponential family .",
    "the link function @xmath22 relates the conditional mean of @xmath10 , @xmath23 to the linear predictor @xmath24 as @xmath25 . here ,",
    "@xmath26 and @xmath27 are @xmath28 and @xmath29 vectors of covariates and @xmath30 is a @xmath28 vector of unknown fixed regression parameters .",
    "we consider responses from the bernoulli and poisson families . if @xmath31 , then @xmath32 , @xmath33 and @xmath34 . for poisson responses ,",
    "we allow for an offset @xmath35 . if @xmath36 , then @xmath37 , @xmath38 and @xmath39 . for the @xmath12th cluster ,",
    "let @xmath40 we assume that the first column of @xmath41 is @xmath42 if @xmath41 is not a zero matrix and that the columns of @xmath41 are a subset of the columns of @xmath43 .    for bayesian inference , we specify a diffuse prior @xmath44 on @xmath30 where @xmath45 is large and an independent inverse - wishart prior , @xmath46 on @xmath47 .",
    "we use the default conjugate prior proposed in @xcite , which is based on a prior guess for @xmath47 determined from first - stage data variability . for this default",
    "prior , @xmath48 and @xmath49 where @xmath50 here , @xmath51 denotes the @xmath52 diagonal glm weight matrix with diagonal elements @xmath53^{-1}$ ] , where @xmath54 is the variance function and @xmath55 is the link function .",
    "we let @xmath56 where @xmath57 is set as @xmath58 for all @xmath12 and @xmath59 is an estimate of the regression coefficients from the glm obtained by pooling all data and setting @xmath60 for all @xmath12 .",
    "the constant @xmath61 is an inflation factor representing the amount in which within - cluster variability can be increased .",
    "we use @xmath62 in all examples . some heuristic justifications for @xmath63",
    "is given in @xcite .",
    "a similar prior was used in @xcite .",
    "alternatively , one may consider marginally noninformative priors for covariance matrices @xcite .",
    "methods in this paper can be extended to these priors easily .",
    "reparametrization techniques such as centering , noncentering and partial noncentering have been used in hierarchical models to boost efficiency in mcmc and em algorithms ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "? * ; * ? ? ?",
    "recently , @xcite introduced a partially noncentered parametrization for glmms and studied its performance in the context of vb .",
    "we introduce the idea of partial noncentering by considering the following linear mixed model ( see also * ? ? ?",
    "suppose @xmath64 and @xmath65 , @xmath43 , @xmath41 , @xmath16 and @xmath30 are as defined previously .",
    "let us specify a constant prior on @xmath30 and assume that @xmath66 and @xmath47 are known .",
    "suppose @xmath67 . in this case , we can introduce @xmath68 so that @xmath69 is `` centered '' about @xmath30 .",
    "we can also obtain a partially noncentered parametrization by introducing @xmath70 , where @xmath71 is an @xmath72 tuning matrix to be specified .",
    "the proportion of @xmath30 subtracted from @xmath73 is allowed to vary with @xmath12 as each @xmath65 carries different amount of information about the underlying @xmath74 .",
    "the centered ( @xmath75 ) and noncentered ( @xmath76 ) parametrizations are special cases of the partially noncentered parametrization . rewriting as @xmath77",
    "we can apply vb to the reparametrized model and assume that @xmath78 .",
    "@xcite showed that the resulting vb algorithm converges in one iteration when @xmath79 where @xmath80 .",
    "this result implies that partial noncentering can yield more rapid convergence than centering or noncentering .",
    "more importantly , the true posteriors are recovered in but not in the centered or noncentered parametrizations . even though assumption of a factorized posterior in vb tends to result in underestimation of posterior variance , partial noncentering was ( in this case ) able to capture dependence between fixed and random effects via tuning parameters @xmath71 so that the true posterior can be recovered .",
    "the above result is particularly useful in the context of stochastic variational inference for glmms . to implement stochastic variational inference , we need to assume that variational posteriors of random effects associated with each unit are independent of each other and of the global variables @xmath30 and @xmath47 .",
    "however , correlation between fixed and random effects is often strong and partial noncentering allows some of this dependence to be captured via the tuning matrices @xmath71 .",
    "this leads to more accurate posterior approximations of the fixed and random effects .",
    "in particular , estimation of the posterior variance of fixed effects which can be centered is improved greatly .",
    "partial noncentering can also give more rapid convergence than centering or noncentering .",
    "this is desirable in the analysis of large data sets and is particularly useful when the convergence of one of the centered or noncentered parametrizations is especially slow .",
    "we emphasize that it is not easy to tell beforehand which of centering or noncentering will perform better , and partial noncentering automatically chooses a parametrization close to optimal .    we adopt the partially noncentered parametrization introduced by @xcite for the glmm , which is explained below .",
    "first , we partition @xmath43 as @xmath81 $ ] and @xmath30 as @xmath82^t$ ] accordingly , where @xmath83 is a @xmath84 matrix consisting of `` subject specific '' covariates and @xmath85 is a @xmath86 matrix consisting of `` general '' covariates ( i.e. not subject specific ) .",
    "all the rows of @xmath83 are thus the same and equal to say @xmath87 .",
    "we have @xmath88 \\text{and}\\ ; \\beta_c=\\begin{bmatrix } \\beta_z \\\\ \\beta_s \\end{bmatrix}.\\end{aligned}\\ ] ] note that @xmath89 is an @xmath90 matrix",
    ". we introduce @xmath91 where @xmath71 is an @xmath72 tuning matrix .",
    "@xmath75 corresponds to the centered and @xmath76 to the noncentered parametrization . letting @xmath92 $ ]",
    "be an @xmath93 matrix , @xmath94 .",
    "the partially noncentered parametrization is thus @xmath95 where @xmath96 $ ] is a @xmath97 matrix .",
    "following @xcite , @xmath71 can be specified as in with @xmath98 for logistic glmms and @xmath99 for poisson glmms .",
    "let @xmath100^t$ ] and @xmath101^t$ ] .",
    "the set of unknown parameters in the glmm is @xmath102 and @xmath103 the fixed effects @xmath30 and random effects covariance @xmath47 can be regarded as `` global '' variables which are common across clusters , while the partially noncentered random effects @xmath104 can be thought of as `` local '' variables associated only with the individual units .",
    "in this section , we derive and present the stochastic nonconjugate variational message passing algorithm for fitting glmms , which is scalable to large data sets .",
    "we start with a brief introduction to variational approximation methods ( see , e.g. * ? ? ?",
    "* ) and review of nonconjugate variational message passing @xcite .    in variational approximation , the true posterior @xmath105 is approximated by a more tractable distribution @xmath106 , where @xmath107 denotes the set of parameters of @xmath108 .",
    "we attempt to make @xmath106 a good approximation to @xmath105 by minimizing the kullback - leibler divergence between @xmath106 and @xmath105 .",
    "this is given by @xmath109 where @xmath110 is the marginal likelihood .",
    "as the kullback - leibler divergence is nonnegative , we have @xmath111 where @xmath112 denotes expectation with respect to @xmath106 and @xmath113 is a lower bound on the log marginal likelihood .",
    "maximization of @xmath113 is thus equivalent to minimization of the kullback - leibler divergence between @xmath106 and @xmath105 . in some cases , @xmath113 is used as an approximation to the log marginal likelihood for performing model selection .",
    "see @xcite for an illustration of how @xmath113 can be used for model selection in glmms .      in vb , @xmath106 is assumed to factorize into @xmath114 for some partition @xmath115 of @xmath116 and @xmath117 denotes variational parameters associated with each factor .",
    "optimization of @xmath113 with respect to @xmath118 leads to optimal densities satisfying @xmath119,\\ ] ] where @xmath120 denotes expectation with respect to @xmath121 .",
    "when conjugate priors are used , the optimal densities have the same form as the priors and it suffices to update the parameters of @xmath122 .",
    "however , for non - conjugate priors , the optimal densities may not belong to recognizable density families . to address this issue",
    ", @xcite imposed a further restriction that each @xmath122 must belong to some exponential family .",
    "let @xmath123 where @xmath117 is the vector of natural parameters and @xmath124 are the sufficient statistics",
    ". updates in nonconjugate variational message passing can be derived by maximizing @xmath113 with respect to each @xmath117 and setting @xmath125 .",
    "let @xmath126 denote the covariance matrix of @xmath127 .",
    "it can be shown that @xmath128 updates in nonconjugate variational message passing are thus given by @xmath129 for @xmath130 .",
    "as nonconjugate variational message passing is a type of fixed - point iterations algorithm , the lower bound is not guaranteed to increase after each update .",
    "sometimes , convergence issues may be encountered which may require damping to fix ( see * ? ? ?",
    "* ) . for conjugate factors ,",
    "the update in can be simplified and details are given in appendix a.    the nonconjugate variational message passing algorithm for glmms @xcite considers a variational approximation of the form @xmath131 where @xmath132 is @xmath133 , @xmath134 is @xmath135 , @xmath136 is @xmath137 and @xmath138 , @xmath139 , @xmath140 are the respective natural parameter vectors . for bernoulli or poisson responses ,",
    "@xmath141 is nonconjugate with respect to the priors over @xmath30 and @xmath104 .",
    "applying nonconjugate variational message passing and approximating the posteriors of @xmath30 and @xmath104 by gaussian distributions , parameter updates for @xmath142 and @xmath143 can be derived using .",
    "the variational posterior for @xmath47 is optimal under and parameter updates can be derived using .",
    "the main steps are given in algorithm [ alg 1 ] below .",
    "algorithm [ alg 1 ] iterates repeatedly between updating local variational parameters for each unit @xmath144 , and re - estimating the global variational parameters .",
    "this procedure is inefficient for large data sets and impossible to accomplish for streaming data or data sets too massive to fit into memory . using ideas in stochastic variational inference @xcite",
    ", we develop a stochastic nonconjugate variational message passing algorithm for fitting glmms that is more efficient at handling large data .      in stochastic variational inference , the global variational parameters are optimized using stochastic gradient ascent",
    ". updates of the form @xmath145 are considered , where @xmath146 denotes a small step taken in the direction of steepest ascent at the @xmath147th iteration . under the euclidean metric",
    ", the direction of steepest ascent is given by the regular gradient @xmath148 .",
    "in stochastic gradient ascent , a noisy estimate of @xmath148 is used in its place . @xcite",
    "propose using natural gradients instead of regular gradients in this optimization step .",
    "their motivation is that the euclidean distance between two parameter settings @xmath107 and @xmath149 is often a poor measure of how dissimilar two distributions @xmath106 and @xmath150 are . a more intuitive measure of dissimilarity between two probability distributions is given by the symmetrized kullback - leibler divergence , which is invariant to parameter transformations .",
    "under this measure , @xcite showed that the direction of steepest ascent is given by the natural gradient @xcite .",
    "previously , @xcite also showed that replacing regular gradients with natural gradients in the conjugate gradient algorithm can speed up variational learning .",
    "following @xcite , we use natural gradients instead of regular gradients in the stochastic approximation . to obtain the natural gradient of @xmath113 with respect to @xmath117 ,",
    "we premultiply @xmath151 with the inverse of the fisher information matrix of @xmath152 ( see , e.g. * ? ? ?",
    "* ) . in nonconjugate variational message passing ,",
    "the fisher information matrix is given by @xmath153 & = e_q\\left [ \\left\\ {   t_l(\\theta_l)-\\nabla_{\\lambda_l } h_l(\\lambda_l ) \\right\\ } \\left\\ { t_l(\\theta_l)-\\nabla_{\\lambda_l}h_l(\\lambda_l ) \\right\\ } ^t\\right ]   \\\\ & = \\mathcal{v}_l(\\lambda_l).\\end{aligned}\\ ] ] from , the natural gradient denoted by @xmath154 is thus given by @xmath155      in this section , we review the key ideas in stochastic variational inference @xcite and discuss how they can be extended to nonconjugate models via nonconjugate variational message passing .",
    "the following steps are carried out in each iteration of stochastic variational inference until convergence is reached .    1 .",
    "randomly select a mini - batch @xmath156 of of @xmath157 units from the whole data set .",
    "2 .   optimize local variational parameters of units in mini - batch @xmath156 ( as a function of the global variational parameters at their current setting ) .",
    "3 .   update global variational parameters using stochastic natural gradient ascent .",
    "noisy gradients are computed based on optimized local variational parameters of units in mini - batch @xmath156 .",
    "the main difficulty in extending stochastic variational inference to nonconjugate models lies in step 2 . for conjugate models",
    ", the local variational parameters can be optimized as a function of the global variational parameters in a single update [ see ] but the same is not true for nonconjugate models . in nonconjugate variational message passing , the update equation for the local variational parameters",
    "is recursive ( they depend on the current setting of the local variational parameters ) and has to be applied repeatedly until convergence is reached [ see ] .",
    "this incurs a higher computational cost .",
    "we have tried performing the update for local variational parameters only once but this further slowed down convergence of the global variational parameters .",
    "we have also tried using a loose criterion for assessing convergence .",
    "this approach yielded much better results . choosing a good initialization is also important as convergence problems can be encountered in recursive updates if the starting point is poor .",
    "the other main difference is that for conjugate models , the update equations and natural gradients are easier to compute as the fisher information matrix @xmath158 does not have to be evaluated [ see and ] .",
    "fortunately , nonconjugate variational message passing updates can be simplified considerably when the variational posteriors are multivariate gaussian ( see * ? ? ?",
    "* ) and the fisher information matrix does not have to be computed explicitly as well .",
    "the extension of stochastic variational inference to nonconjugate models greatly widens the scope of models to which stochastic variational inference can be applied .",
    "we think that nonconjugate variational message passing is an important tool in facilitating this extension as it allows for efficient closed - form updates in some cases ( e.g. poisson glmms ) and there is a lot of flexibility in the evaluation of expectations ( using bounds or quadrature ) .",
    "while convergence issues remain in fixed - point iterations algorithms , these can usually be mitigated by good initializations .",
    "we later show that nonconjugate variational message passing , like vb , is a type of natural gradient method ( see * ? ? ?",
    "with this interpretation , some convergence issues might be resolved by taking adaptive steps in the direction of the natural gradient .",
    "let @xmath159 and @xmath160 denote the global and local variational parameters respectively .",
    "the lower bound @xmath113 is a function of @xmath161 , i.e. @xmath162 .",
    "@xcite showed that to find a setting of @xmath159 that maximizes @xmath113 using stochastic natural gradient ascent , we can first optimize @xmath160 as a function of @xmath159 so that @xmath163 for some function @xmath164 . in nonconjugate variational message passing ,",
    "this is done by computing the update in repeatedly until convergence , starting with some current setting of @xmath160 and keeping @xmath159 fixed .",
    "this implies that @xmath165 since @xmath166 is a local optimum of the local variational parameters .",
    "the current value of the lower bound is @xmath167 which is a function of @xmath159 only .",
    "let us define @xmath168 .",
    "to optimize @xmath169 with respect to @xmath159 , we have @xmath170 therefore , @xmath171 can be computed by finding the optimized local variational parameters @xmath166 and then computing the gradient of @xmath172 with respect to @xmath159 by keeping @xmath166 fixed .",
    "the corresponding natural gradient can be obtained as discussed in section [ sec_nat_grad ] .    in stochastic variational inference ,",
    "noisy estimates of the natural gradients are used in stochastic optimization of the global variational parameters .",
    "the idea is to approximate true gradients over the whole data with gradients computed on mini - batches of data . for large data sets",
    ", this can lead to significant reductions in computation time . for the glmm , @xmath173 and @xmath174 .",
    "as @xmath30 and @xmath47 are independent in the variational posterior , stochastic gradient ascent for @xmath138 and @xmath139 can be done separately . from and",
    ", the natural gradient of @xmath113 with respect to @xmath138 , @xmath175 is given by @xmath176 where @xmath177 denotes @xmath178 optimized as a function of the global variational parameters .",
    "if @xmath156 is a mini - batch of @xmath179 units randomly sampled from the whole data set ( with or without replacement ) , then an unbiased estimate of @xmath180 is @xmath181 , where @xmath182 note that each of the @xmath183 units in the whole data set has a probability @xmath184 of being selected and hence the expectation of @xmath185 is equal to ( * ? ? ?",
    "18  19 ) .",
    "similarly , an unbiased estimate of @xmath186 is @xmath187 , where @xmath188 when @xmath156 is the whole data set , @xmath189 and @xmath190 are respectively the updates of @xmath138 and @xmath139 in nonconjugate variational message passing .    with these unbiased estimates of the natural gradients , @xmath138 and @xmath139",
    "can be updated using stochastic gradient approximation @xcite . at the @xmath147th iteration , @xmath191 where",
    "@xmath189 and @xmath190 are evaluated using the current settings of @xmath138 and @xmath139 . under certain regularity conditions",
    "( see * ? ? ?",
    "* ) , the iterates will converge to a local maximum of the lower bound .",
    "the gain sequence @xmath146 , @xmath192 should satisfy @xmath193 the condition @xmath194 ensures that the step size goes to zero sufficiently fast so that iterates will converge while @xmath195 ensures that the rate at which step sizes approach zero is slow enough to avoid false convergence . @xcite",
    "recommends @xmath196 where @xmath197 , @xmath198 is a stability constant that helps to avoid unstable behaviour in the early iterations and @xmath6 keeps step sizes nonnegligible in later iterations .",
    "note that updates in can be rewritten as @xmath199 the @xmath147th iterate is thus a weighted average of the previous iterate and the nonconjugate variational message passing update estimated using mini - batch @xmath156 . when @xmath200 and @xmath156 is the whole data set , @xmath201 is precisely the update in nonconjugate variational message passing .",
    "this shows that nonconjugate variational message passing is a type of natural gradient method with step size 1 and other schedules are equivalent to damping .",
    "the stochastic nonconjugate variational message passing algorithm for fitting poisson and logistic glmms is presented in algorithm [ alg 2 ] .",
    "derivation of updates and definitions of @xmath202 and @xmath203 ( appearing in algorithm [ alg 2 ] and which differ according to whether logistic or poisson glmms are fitted ) are given in appendix b. algorithm [ alg 2 ] reduces to algorithm [ alg 1 ] when mini - batch @xmath156 is the entire data set , @xmath200 , and updates for local variational parameters are performed only once .    to initialize algorithm 2 , we recommend using the fit from penalized quasi - likelihood , which can be implemented in r via the function glmmpql in the package mass @xcite . alternatively , for large data sets where penalized quasi - likelihood converges too slowly",
    ", we can use the fit from the glm ( obtained by pooling all data and setting random effects as zero ) for initialization .",
    "for instance , we can set @xmath204 and @xmath205 respectively as estimates of the regression coefficients and their covariances from the glm , @xmath206 where @xmath207 , @xmath208 and @xmath209 .",
    "@xcite gave a justification of @xmath63 [ defined in ] being a reasonable guess for @xmath47 in the absence of any other prior knowledge .",
    "the tuning parameters @xmath71 can be initialized by setting @xmath210 and @xmath211 ( for logistic glmms ) .    in step 1 , mini - batches may be selected with or without replacement from the whole data set . here , we consider sampling randomly at each iteration without replacement .",
    "suppose the data set consist of @xmath183 clusters and we randomly select @xmath179 clusters at the first iteration . at the second iteration , we will randomly sample @xmath179 clusters from the remaining @xmath212 clusters and so on . algorithm 2 is considered to have made a sweep through the data when all clusters have been used once .",
    "this process is then repeated .",
    "mini - batches in each sweep are sampled randomly and do not depend on those in previous sweeps .",
    "we allow mini - batches in each sweep to differ in size by one when @xmath183 is not divisible by @xmath179 .",
    "the advantage of sampling without replacement is that this scheme ensures all clusters ( and local variational parameters ) have been used or updated once in each sweep .    in step 2",
    ", we consider a loose criterion for assessing convergence to reduce computational overhead .",
    "suppose mini - batch @xmath156 consist of units @xmath213 .",
    "we define @xmath214^t$ ] and terminate repetitions in step 2 when @xmath215 , where @xmath216 represents the euclidean norm .",
    "typically 37 repetitions are required for each mini - batch in the first sweep .",
    "the number of repetitions reduces steadily with the number of sweeps and usually just a single update is required by the third sweep .    for the examples in this paper",
    ", we did not update the tuning parameters @xmath71 beyond initialization when the partially noncentered parametrization was used . while updating tuning parameters ( at the end of each cycle in algorithm [ alg 1 ] or at the end of each sweep in algorithm [ alg 2 ] ) can lead to further improvements , more computation is required and this can be time - consuming for large data sets .",
    "a good initialization of the tuning parameters based on say penalized quasi - likelihood usually suffices .",
    "the choice of step sizes @xmath146 can strongly influence the performance of a stochastic approximation algorithm @xcite .",
    "we discuss the choice of a gain sequence for algorithm [ alg 2 ] in the next section .",
    "determining an appropriate stopping criterion for a stochastic approximation algorithm can be challenging .",
    "some commonly used criteria include stopping when the relative change in parameter values or objective function is sufficiently small or when the gradient of the objective function is sufficiently close to zero @xcite .",
    "such criteria do not provide any guarantees of the terminal iterate being close to the optimum however , and may be satisfied by random chance .",
    "@xcite recommend applying such rules for several consecutive iterations to minimize chances of a premature stop .",
    "however , @xcite gave an illustrative example to show that even this may not be enough of a safeguard .",
    "moreover , stochastic approximation can become excruciatingly slow in later iterations due to small step sizes .",
    "our experimentations with moderate - sized data sets indicate that gains made by algorithm [ alg 2 ] are usually largest in the first few sweeps .",
    "however , beyond a certain point , it can become slower than algorithm [ alg 1 ] if step sizes are too small or iterates simply bounce around if step sizes are still too big .",
    "an example is shown in figure [ global ] where global variational parameters @xmath204 and @xmath217 are plotted against iterations @xmath147 ( or number of sweeps ) . here",
    "algorithm 2 is applied to a simulated data set of size @xmath218 ( details in example [ poly ] ) and mini - batches of size @xmath219 are used with step size @xmath220 .",
    "blue trajectories correspond to @xmath221 and black to @xmath222 .",
    "red dotted lines represent values obtained using algorithm 1 .",
    "figure [ global ] shows that the blue and black trajectories converge towards the red dotted lines quickly in the first few sweeps .",
    "however , full convergence takes much longer .",
    "a larger step size ( @xmath221 ) implies faster convergence at first but the iterates bounce around the optimum eventually if step sizes are still too large . a possible remedy to this is iterate averaging @xcite .",
    "polypharmacy simulated data ( @xmath218 ) .",
    "global variational parameters @xmath204 and @xmath217 fitted using algorithm 2 plotted against number of sweeps .",
    "mini - batches @xmath219 and step size @xmath220 .",
    "blue trajectories correspond to @xmath221 and black to @xmath222 .",
    "red dotted line denotes values obtained using algorithm 1.,scaledwidth=90.0% ]    we suggest switching to algorithm [ alg 1 ] when algorithm [ alg 2 ] shows signs of slowing down . using the lower bound both as a switching and stopping criterion , we propose switching from stochastic to standard nonconjugate variational message passing when the relative increase in the lower bound after a sweep is less than @xmath223 , and terminating algorithm [ alg 1 ] when the relative increase in the lower bound is less than @xmath224 . for large datasets or streaming data",
    ", it might be more practical to terminate algorithm [ alg 2 ] beyond a certain period of available runtime . to switch from algorithm [ alg 2 ] to [ alg 1 ]",
    ", the final setting of local and global variational parameters computed using algorithm [ alg 2 ] is used as initialization of algorithm [ alg 1 ] .",
    "let @xmath225 denote the number of iterations required to make a sweep through the data set .",
    "following @xcite , we consider step sizes of the form @xmath226 setting @xmath227 and @xmath228 in .",
    "we let @xmath229 where @xmath230 denotes the number of mini - batches that has been analysed at the @xmath231th sweep .",
    "this specification slows down the rate of decrease in step size within each sweep and the larger step sizes help iterates move faster towards the optimum .",
    "we investigate performance of different stability constants @xmath8 for various mini - batch sizes .",
    "smaller values of @xmath232 correspond to a slower decrease in step size and are desirable in some cases as they provide bigger step sizes in later iterations . for our proposed strategy",
    ", we observed that smaller mini - batch sizes generally performed better . since smaller step sizes are preferred for smaller mini - batch sizes ( see * ? ? ?",
    "* ) , we set @xmath228 for simplicity and report results only for this case .",
    "recently , @xcite developed an adaptive learning rate for stochastic variational inference , which is designed to minimize the expected distance between stochastic and optimal updates of the global variational parameters .",
    "they showed that adaptive step sizes led to improved convergence for the latent dirichlet allocation model in topic modelling .",
    "it might be possible to extend this adaptive learning rate to nonconjugate models and we are working on this area .",
    "a wide variety of approaches have been developed to enhance the rate of convergence of stochastic approximation algorithms , and examples include iterate averaging @xcite , momentum method @xcite and gradient averaging @xcite .",
    "see @xcite for the stochastic average gradient method as well as a review of other approaches .",
    "in this section , we consider diagnostic tests for identifying divergent units in glmms . such diagnostics are useful for detecting institutions ( e.g. hospitals , trusts or schools ) which deviate from the rest in a certain outcome . in healthcare for instance , it may be of interest to identify hospitals which are divergent in terms of quality of care provided or choice of surgical procedure for treating a cancer @xcite .",
    "we demonstrate how prior - likelihood conflict diagnostics for identifying divergent units can be obtained as a by - product of nonconjugate variational message passing .",
    "the intuitive idea is that messages coming from above and below a node in a hierarchical model can be separated and  mixed messages \" indicate conflict . our  mixed messages \" diagnostics can be shown to approximate the conflict diagnostics of @xcite .",
    "we start with a review of the simulation - based diagnostic test @xcite , which is based on measuring the conflict between likelihood of a parameter and its predictive prior given the remaining data .",
    "subsequently , we show how their approach can be approximated in the variational message passing framework .      for glmms with a partially noncentered parametrization ,",
    "the linear predictor is @xmath234 to identify units that do not appear to be drawn from the assumed random effects distributions , @xcite suggest comparing replicates of @xmath104 from its likelihood and predictive prior .",
    "a predictive prior replicate @xmath235 is first generated from @xmath236 where @xmath237 denotes observed data @xmath238 with unit @xmath12 left out .",
    "this replicate can be obtained by generating @xmath239 and @xmath240 from @xmath241 using mcmc , followed by simulation of @xmath242 .",
    "a likelihood replicate @xmath243 is then generated using only the unit @xmath65 being tested and a non - informative prior @xmath244 for @xmath104 . @xcite",
    "recommend using the jeffreys s prior as a noninformative prior for @xmath104 ( see * ? ? ?",
    "these prior and likelihood replications represent independent sources of evidence about @xmath104 and conflict between them suggests discrepancies in the model .",
    "the discussion above ignores nuisance parameters . for glmms",
    ", we need to regard @xmath30 as a nuisance parameter . as @xmath245 and @xmath30",
    "is not estimable from individual unit @xmath12 , @xcite[pg .",
    "420 ] recommend generating @xmath246 from @xmath247 note that the two replications @xmath235 and @xmath246 are no longer entirely independent as @xmath237 will slightly influence @xmath246 through @xmath30 .",
    "to compare prior and likelihood replicates , @xcite considered @xmath248 and calculated a conflict @xmath233-value , @xmath249 as the proportion of times simulated values of @xmath250 are less than or equal to zero for scalar @xmath104 . depending on the context , the upper tail area @xmath251 or two - sided @xmath233-value @xmath252 may be of interest instead .",
    "if @xmath250 is not a scalar , @xmath253 can be used as a standardized discrepancy measure . if we further assume a multivariate normal distribution for @xmath250 , then a conflict @xmath233-value for testing @xmath254 can be calculated as @xmath255 , where @xmath256 denotes a chi - square random variable with @xmath15 degrees of freedom .",
    "further discussion on @xmath233-values in multivariate case can be found in @xcite .    as mcmc methods are not well - suited to cross - validation approaches",
    ", @xcite proposed an alternative simulation - based full - data approach .",
    "the procedure is the same as before except that @xmath242 is simulated using @xmath239 , @xmath240 generated from @xmath257 , without leaving out @xmath65 .",
    "mild conservatism is introduced as @xmath65 will influence @xmath235 slightly through @xmath30 and @xmath47 .",
    "next , we show how approximate conflict @xmath233-values can be calculated within nonconjugate variational message passing . from , the update for @xmath140 is @xmath258 the first term can be considered as a message from the prior @xmath259 and the second term a message from the likelihood @xmath260 of unit @xmath65 .",
    "we argue below that the first message from the prior can be interpreted as natural parameter of a gaussian approximation say @xmath261 to @xmath262 . on the other hand ,",
    "the second message from the likelihood can be interpreted as natural parameter of a gaussian approximation say @xmath263 to @xmath264 .",
    "this implies that @xmath265 and @xmath266 .",
    "if we further assume @xmath235 and @xmath246 are independent , then @xmath267",
    ". even though @xmath235 and @xmath246 are not entirely independent , the dependence between @xmath235 and @xmath246 will be increasingly weak as the number of clusters increases .",
    "since these messages are computed in the nonconjugate variational message passing algorithm , conflict @xmath233-values can be calculated easily at convergence for identifying divergent units .",
    "the arguments presented below are by no means rigorous . however",
    ", they lend some insight into how conflict @xmath233-values can be approximated from nonconjugate variational message passing and experimental results suggest the approximations work well in practice . for large data sets , automatic computation of diagnostics for prior - likelihood conflict can be an attractive alternative to simulation - based mcmc approaches .",
    "they are also useful generally as initial screening tools and clusters flagged as divergent can be studied more closely and possibly conflict @xmath233-values recomputed by monte carlo .",
    "first , consider the message from the prior .",
    "if we treat the message as natural parameter of a normal distribution , we get @xmath268 and @xmath269 . for large data sets",
    ", @xmath241 is close to @xmath257 and we approximate @xmath241 in by the variational posterior @xmath270 .",
    "this combined with jensen s inequality gives @xmath271 . \\end{aligned}\\ ] ] while @xmath272 $ ] is only a lower bound to @xmath262 , we find that by using it as an approximation to @xmath262 , we get @xmath273 $ ] .",
    "this gives @xmath274 , which is what we would get if we interpret the first message as being the natural parameter of a gaussian approximation to @xmath262 .",
    "next , consider the second message from the likelihood .",
    "if we treat the message as the natural parameter of a normal distribution , it can be shown that @xmath275 and @xmath276 .",
    "now consider the sum of the two messages .",
    "this gives us the natural parameter of @xmath136 which is an approximation of @xmath277 .",
    "note that @xmath278 if we think of @xmath279 as the ` prior ' to be updated when @xmath65 becomes available , we have @xmath280 interpreting the first message as a gaussian approximation to @xmath279 and the sum of the two messages as a gaussian approximation to @xmath277 , the ratio of these two normal distributions gives an approximation ( up to a proportionality constant ) of @xmath281 . as a function of @xmath104 , the ratio of the two normal distributions is proportional to @xmath282 which gives a normal distribution with mean @xmath283 and covariance @xmath284 , precisely that given by the second message .",
    "as @xmath285 and @xmath286 is close to @xmath287 when the number of clusters is large ( in the sense that dependence of @xmath30 on @xmath104 is reduced ) , the second message can be considered as the natural parameter of a gaussian approximation to @xmath264 if we assume a uniform prior for @xmath244 .",
    "the arguments above generalize to detecting conflict for other parameters of the model as well .",
    "while the discussion here uses the partially noncentered parametrization , conclusions hold for the centered and noncentered parametrizations as well .",
    "we observed small differences in conflict @xmath233-values computed using different parametrizations , which is due likely to varying accuracy of approximations to the true posterior . to compare the accuracy of different approaches",
    ", we first transform the conflict @xmath233-values to @xmath288-scores to reflect the importance of good agreement at the extremes @xcite . using the cross - validatory conflict @xmath233-values as a  gold - standard \"",
    ", we use the mean absolute difference in @xmath288-scores , @xmath289 as a measure of the degree of agreement between the cross - validatory conflict @xmath233-values ( @xmath290 ) and conflict @xmath233-values computed from the method we are trying to assess ( @xmath291 ) .    to compute conflict-@xmath233 values for large data sets",
    ", one needs to ensure that local variational parameters for every unit are optimized . as algorithm 2",
    "focuses on optimization of global variational parameters using stochastic approximation , not all local variational parameters may have been fully optimized when the global variational parameters have converged . this can be resolved by performing an additional step of optimizing local variational parameters for every unit as a function of the converged global variational parameters .",
    "alternatively , our proposed strategy of switching from algorithm 2 to 1 also ensures that local variational parameters for every unit are optimized .",
    "however , due to the difficulty in computing conflict @xmath233-values for large data sets using cross - validatory or even full - data approaches with mcmc , we focus on comparisons with nonconjugate variational message passing using only small data problems in the examples .",
    "in sections [ bristol ] and [ epilepsy ] , we use the bristol inquiry data and epilepsy data to compare conflict @xmath233-values computed using nonconjugate variational message passing with those obtained using the simulation - based cross - validatory approach of @xcite .",
    "an additional example on madras schizophrenia data can be found in appendix [ schizo ] .",
    "these data sets are relatively small and we only use algorithm [ alg 1 ] for fitting .    in sections [ poly ] and [ skincancer ] , we use moderately large simulated data sets to illustrate the improvements in efficiency that can be obtained by using stochastic nonconjugate variational message passing in the initial stage of optimization .",
    "we compare performances of algorithms [ alg 1 ] and [ alg 2 ] for the simulated data sets using only the partially noncentered parametrization .",
    "algorithms [ alg 1 ] and [ alg 2 ] were initialized using penalized quasi - likelihood in all examples except for the large simulated data set in section [ skincancer ] , where penalized quasi - likelihood converges too slowly . the glm fit was used instead for initialization .    in all examples , fitting via mcmc was performed in openbugs @xcite through r by using r2openbugs as an interface .",
    "r2openbugs was adapted by neal thomas from r2winbugs @xcite .",
    "the mcmc algorithm was initialized using penalized quasi - likelihood and the same priors were used in mcmc and nonconjugate variational message passing .",
    "we consider a vague @xmath292 prior for @xmath30 in each case .",
    "all code was written in r and run on a dual processor windows pc 3.30 ghz workstation .",
    "computation times reported are in seconds ( s ) .    in some examples below , the variational posterior approximations are biased as compared to results from mcmc .",
    "this is due to the assumption of a factorized variational posterior and the impact of this restriction depends on how strong posterior dependence is among the factored variables . in vb ,",
    "the posterior variance tends to be underestimated and this issue has been noted by @xcite and @xcite .",
    "recently , @xcite proposed some diagnostics for assessing how well vb approximates the true posterior as well as correction measures that can be undertaken when the approximation error is large .",
    "@xcite developed stochastic approximation methods for hierarchical approximations that allow independence assumptions in vb to be relaxed .      in 1998",
    ", a public inquiry was set up to look into the management of children receiving complex cardiac surgical services at the bristol royal infirmary .",
    "the outcomes of surgical services at bristol , uk , relative to other specialist centres was a key issue .",
    "we consider a subset of the data recorded by hospital episode statistics on mortality rates in open surgeries for 12 hospitals including bristol ( hospital 1 ) , for children under 1 year old , from 1991 to 1995 ( see * ? ? ?",
    "* table 1 ) .",
    "although the number of clusters is small in this example whereas our methodology is motivated by applications to large data sets , this example is interesting as a benchmark data set in the literature for computing conflict diagnostics using nonconjugate variational message passing .",
    "let @xmath293 where @xmath294 if patient @xmath11 at hospital @xmath12 died and 0 otherwise .",
    "we use @xmath295 to denote the number of deaths at hospital @xmath12 , @xmath296 .",
    "let @xmath297 in the cross - validatory approach , each hospital @xmath12 was removed in turn from the analysis , and @xmath298 were generated using mcmc followed by a simulated @xmath299 . assuming a jeffreys s prior for @xmath300 , a @xmath301 was simulated from @xmath302 .",
    "excess mortality is of concern and the upper - tail area is used as a 1-sided @xmath233-value so that @xmath303 . for each fitting via mcmc ,",
    "two chains were run simultaneously to assess convergence , each with 51,000 iterations , and the first 1000 iterations were discarded in each chain as burn - in .",
    "cross - validatory conflict @xmath233-values were calculated based on the remaining 100,000 simulations . the total time taken for model updating in openbugs",
    "is 5 s @xmath304 12 = 60 s for the cross - validatory approach .    the variational lower bounds and cpu times taken for model fitting and computation of conflict @xmath233-values by algorithm [ alg 1 ] ( via different parametrizations ) and mcmc ( full - data approach ) are shown in table [ bristoltable ] .",
    "figure [ bristolplot ] shows the marginal posteriors of @xmath30 and @xmath47 estimated using mcmc and algorithm [ alg 1 ] .",
    "the partially noncentered parametrization attained the highest lower bound , was quick to converge and produced posterior approximations very close to that of mcmc .",
    "bristol data .",
    "marginal posteriors estimated by mcmc ( black ) and algorithm [ alg 1 ] using the centered ( green ) , noncentered ( blue ) and partially noncentered ( red ) parametrizations.,scaledwidth=55.0% ]     bristol data .",
    "cross - validatory conflict @xmath233-values ( @xmath290 ) and conflict @xmath233-values from nonconjugate variational message passing ( @xmath305 ) using a partially noncentered parametrization.,width=283 ]    figure [ bristolfig ] compares conflict @xmath233-values computed using the cross - validatory approach and nonconjugate variational message passing using the partially noncentered parametrization .",
    "the plot indicates very good agreement between the two sets of @xmath233-values .",
    "both approaches suggest hospital 1 ( bristol ) is discrepant .",
    "the mean absolute difference in @xmath288-scores for nonconjugate variational message passing and the simulation - based full - data approach relative to the cross - validatory approach are given in table [ bristoltable ] .",
    "nonconjugate variational message passing does better than the simulation - based full - data approach both in terms of @xmath288-scores and computation time .",
    "the difference in conflict @xmath233-values computed using different parametrizations is small .    for this example ,",
    "nonconjugate variational message passing is of an order of magnitude faster than the cross - validatory approach .",
    "we will see in the next two examples that the reduction in computation time is even greater for larger data sets .",
    "there are some difficulties in comparing nonconjugate variational message passing and mcmc in this way as the time taken for the variational algorithm to converge depends on the initialization , stopping rule and the rate of convergence is problem - dependent .",
    "the updating time for mcmc is also problem - dependent and depends on the length of burn - in and number of sampling iterations .",
    "it is clear , however , that for large data sets , the variational approach is attractive as an alternative to mcmc methods for obtaining prior - likelihood conflict diagnostics or as an initial screening tool .",
    "the epilepsy data set of @xcite contains records from a clinical trial of 59 patients with epilepsy . each patient",
    "was randomly administered a new anti - epileptic drug , progabide , ( trt=1 ) or a placebo ( trt=0 ) and the number of seizures during the two weeks before each of four successive clinic visits ( visit , coded as @xmath306 , @xmath307 , @xmath308 and @xmath309 ) was recorded . the number of seizures during the 8-week period prior to randomization was also noted .",
    "we consider the logarithm of @xmath310 the number of baseline seizures ( base ) and the logarithm of the age of patient ( age ) as covariates .",
    "we center the covariate age at its mean to improve mixing in mcmc methods .",
    "@xcite considered a poisson random intercept and slope model : @xmath311 for @xmath312 , @xmath313 and @xmath314\\sim n\\left(0,\\left[\\begin{smallmatrix } \\sigma_{11}^2 & \\sigma_{12}\\\\\\sigma_{21 } & \\sigma_{22}^2 \\end{smallmatrix}\\right]\\right)$ ] .",
    "we compare conflict @xmath233-values computed using the cross - validatory approach and nonconjugate variational message passing for two models .",
    "model i is a random intercept model where the random slope is dropped from .",
    "model ii is the random intercept and slope model in .",
    "we examine the suitability of the assumed random effects distribution and report two - sided conflict @xmath233-values for both models .    for simulation - based approaches ,",
    "it is easier to work with the centered parametrization as handling of nuisance parameters is minimized ( see details in appendix c ) . under this parametrization , there are no nuisance parameters in model ii and only @xmath315 needs to be regarded as a nuisance parameter in model i. each patient was removed in turn from the analysis in the cross - validatory approach . for each model",
    "fitting via mcmc , two chains were run simultaneously to assess convergence , each with 26,000 iterations , and the first 1000 iterations were discarded in each chain as burn - in .",
    "cross - validatory conflict @xmath233-values were calculated based on the remaining 50,000 simulations .",
    "the total time taken for model updating in openbugs is 61 s @xmath304 59 = 3599 s for model i and 54 s @xmath304 59 = 3186 s for model ii .",
    "simulation of prior and likelihood replicates of the centered random effects @xmath73 was performed in r. to simulate likelihood replicates , we assume jeffreys s prior for @xmath73 and use adaptive rejection metropolis sampling via the arms function in the hi package @xcite .",
    "epilepsy data model i. marginal posteriors estimated by mcmc ( black ) and algorithm [ alg 1 ] using the centered ( green ) , noncentered ( blue ) and partially noncentered ( red ) parametrizations.,scaledwidth=90.0% ]    variational lower bounds and cpu times taken for model fitting and computation of conflict @xmath233-values by algorithm [ alg 1 ] ( via different parametrizations ) and mcmc ( full - data approach ) are given in table [ epilepsytable ] .",
    "marginal posteriors of parameters in model i estimated using mcmc and algorithm [ alg 1 ] are given in figure [ epilmar ] .",
    "comparison of parameter estimates for model ii can be found in @xcite .",
    "the partially noncentered parametrization performed very well in posterior approximations and was quick to converge .",
    "cross - validatory conflict @xmath233-values are plotted against conflict @xmath233-values from nonconjugate variational message passing using the partially noncentered parametrization in figure [ epilpvplot ] , for model i ( left ) and model ii ( right ) .",
    "the mean absolute difference in @xmath288-scores for nonconjugate variational message passing and the simulation - based full - data approach relative to the cross - validatory approach are given in table [ epilepsytable ] .",
    "figure [ epilpvplot ] shows good agreement between cross - validatory conflict @xmath233-values and conflict @xmath233-values computed using nonconjugate variational message passing . the agreement is better in model ii and",
    "this is reflected in the @xmath288-scores in table [ epilepsytable ] .",
    "nonconjugate variational message passing compares well with the simulation - based full - data approach in terms of @xmath288-scores and is faster than both simulation - based approaches by an order of magnitude .     epilepsy data .",
    "cross - validatory conflict @xmath233-values plotted against conflict @xmath233-values from nonconjugate variational message passing using a partially noncentered parametrization , for model i ( left ) and model ii ( right).,scaledwidth=75.0% ]    * 3c + patient & @xmath290 & @xmath305 + 10 & 0.047 & 0.056 + 25 & 0.048 & 0.062 + 35 & 0.038 & 0.044 + 56 & 0.023 & 0.028 + 58 & 0.002 & 0.006 +    * 3c + patient & @xmath290 & @xmath305 + 10 & 0.001 & 0.005 + 25 & 0.024 & 0.049 + 56 & 0.038 & 0.051 +    at the 0.05 level , outliers identified by the cross - validatory approach are patients 10 , 25 , 35 , 56 and 58 for model i and patients 10 , 25 and 56 for model ii .",
    "table [ epilepsytable2 ] shows the cross - validatory conflict @xmath233-values for these patients .",
    "the corresponding conflict @xmath233-values computed using nonconjugate variational message passing with a partially noncentered parametrization are shown for comparison .",
    "while @xmath233-values from the two approaches are close , some of the outliers identified by the cross - validatory approach are not detected using nonconjugate variational message passing .",
    "one way to resolve this issue is to flag all patients with conflict @xmath233-values @xmath316 say as possible outliers and recompute conflict @xmath233-values for this smaller group using cross - validatory approach . in this way , nonconjugate",
    "variational message passing can be regarded as a screening tool which will be very useful for large data sets .",
    "the polypharmacy data set @xcite contains data on 500 subjects studied over a period of seven years ( available at http://www.umass.edu/statdata/statdata/stat-logistic.html ) .",
    "the outcome of interest is whether the subject is taking drugs from 3 or more different groups .",
    "the number of outpatient mental health visits ( mhv ) and inpatient mental health visits made by each subject were recorded each year .",
    "we consider the dummy variables mhv1=1 if @xmath317 and 0 otherwise , mhv2=1 if if @xmath318 and mhv3=1 if @xmath319 and 0 otherwise .",
    "let inptmhv = 0 if there were no inpatient mental health visits and 1 otherwise .",
    "other covariates include age , @xmath320 if male and 0 if female and @xmath321 if subject is white and 1 otherwise .",
    "following @xcite , we consider a logistic random intercept model of the form @xmath322 where @xmath323 for @xmath324 , @xmath325 .",
    "this model was fitted using algorithm [ alg 1 ] and mcmc .",
    "variational lower bounds and cpu times for model fitting are shown in table [ polytable ] .",
    "for mcmc , two chains were run simultaneously to assess convergence , each with 11,000 iterations , and the first 1000 iterations were discarded in each chain as burn - in .",
    "algorithm 1 is of an order of magnitude faster than mcmc .",
    "figure [ polyplot ] shows the marginal posterior distributions of parameters estimated using mcmc and algorithm 1 .",
    "the partially noncentered parametrization attained the highest lower bound and took much less time to converge than the noncentered parametrization .",
    "posterior approximations for @xmath326 , @xmath327 and @xmath328 from partial noncentering were better than that of centering and noncentering . while posterior variance of @xmath329 , @xmath315 and @xmath330 were underestimated by partial noncentering ,",
    "the estimated posterior means were close to that of mcmc .",
    "as this data set is relatively small , using algorithm [ alg 2 ] in the initial stage of optimization did not lead to significant reductions in computation times .",
    "polypharmacy data .",
    "marginal posteriors estimated by mcmc ( black ) and algorithm [ alg 1 ] using the centered ( green ) , noncentered ( blue ) and partially noncentered ( red ) parametrizations.,scaledwidth=96.0% ]    to illustrate the improvements in efficiency that can be obtained from stochastic nonconjugate variational message passing , we simulated a larger data set comprising of @xmath331 subjects from the model fitted by algorithm [ alg 1 ] ( using the partially noncentered parametrization ) .",
    "the design matrices for each cluster were replicated 20 times and responses were generated from the model in , using as parameters variational posterior means from the fitted model . for this simulated data , algorithm [ alg 1 ] using the partially noncentered parametrization took 656.6 s to converge .    for algorithm [ alg 2 ] , we considered mini - batch sizes @xmath332 ( which correspond to 0.05% , 1% , 2% and 4% of @xmath333 ) and stability constants @xmath334 .",
    "larger stability constants were used for smaller mini - batch sizes . for each mini - batch size and stability constant @xmath8",
    ", we performed ten runs of algorithm [ alg 2 ] switching to algorithm [ alg 1 ] when the relative increment in the lower bound after a sweep is less than @xmath223 .",
    "computation times for the four mini - batch sizes corresponding to different stability constants are displayed in boxplots in figure [ polyplot2 ] .",
    "the shortest average time to convergence for the different mini - batch sizes are given in table [ polytable2 ] together with the corresponding stability constant @xmath8 . from figure [ polyplot2 ] ,",
    "computation times were reduced by a factor of close to 2 or more across different mini - batch sizes and stability constants considered .",
    "table [ polytable2 ] showed that larger stability constants @xmath8 are preferred for smaller mini - batch sizes .",
    "the shortest average time to convergence of 236.7 s was achieved by mini - batches of size 100 with @xmath335 .",
    "this represents a reduction in computation time from algorithm [ alg 1 ] by a factor of 2.8 .",
    "polypharmacy simulated data .",
    "computation times ( s ) for mini - batch sizes 50 , 100 , 200 and 400 ( from left to right ) corresponding to different stability constants displayed in boxplots.,scaledwidth=95.0% ]    polypharmacy simulated data .",
    "plot of average lower bound against number of sweeps for different batch sizes , with stability constants @xmath8 given in table [ polytable2].,scaledwidth=57.0% ]    figure [ polyplot3 ] tracks the average lower bound attained at the end of each sweep for different mini - batch sizes , with stability constants @xmath8 given in table [ polytable2 ] .",
    "only the first seven sweeps are shown .",
    "figure [ polyplot3 ] shows that with appropriate step sizes , stochastic nonconjugate variational message passing is able to make much bigger gains than the standard version , particularly in the first few sweeps .",
    "thus , for moderate - sized data sets , gains in computation times can be obtained by using algorithm [ alg 2 ] in the initial stage of optimization .      in a clinical trial to test the effectiveness of beta - carotene in preventing non - melanoma skin cancer @xcite , 1805 high risk patients were randomly assigned to receive either a placebo or 50 mg of beta - carotene per day for five years .",
    "the response @xmath10 is a count of the number of new skin cancers in year @xmath11 for the @xmath12th subject .",
    "covariate information for the @xmath12th subject include @xmath336 , the age in years at the beginning of the study , @xmath337 if male and 0 if female , @xmath338 if skin has burns and 0 otherwise , @xmath339 , a count of the number of previous skin cancers , and @xmath340 , the year of follow - up .",
    "the treatment effect has been shown to be insignificant in previous analyses .",
    "we consider @xmath341 subjects with complete covariate information ( data set available at http://www.biostat.harvard.edu/~fitzmaur/ala2e/ ) .",
    "following @xcite , we consider the random intercept and slope model @xmath342 where @xmath314\\sim n\\left(0,\\left[\\begin{smallmatrix } \\sigma_{11}^2 & \\sigma_{12}\\\\\\sigma_{21 } & \\sigma_{22}^2 \\end{smallmatrix}\\right]\\right)$ ] for @xmath343 , @xmath344 .",
    "the covariates year , age and skin were standardized to have mean 0 and variance 1 .",
    "fitting this model using algorithm [ alg 1 ] and mcmc , the estimated marginal posterior distributions of model parameters are shown in figure [ skinplot ] and computation times and variational lower bounds are given in table [ skintable ] .",
    "for mcmc , two chains were run simultaneously to assess convergence , each with 11,000 iterations , and the first 1000 iterations were discarded in each chain as burn - in .",
    "partial noncentering performed very well as compared to centering and noncentering , producing posterior approximations that were closest to that of mcmc and converging in the shortest time .",
    "skin cancer data .",
    "marginal posteriors estimated by mcmc ( black ) and algorithm [ alg 1 ] using the centered ( green ) , noncentered ( blue ) and partially noncentered ( red ) parametrizations.,scaledwidth=85.0% ]    to investigate the performance of stochastic nonconjugate variational message passing , we simulated a much larger data set ( comprising of @xmath345 subjects ) from the model fitted by algorithm [ alg 1 ] ( using the partially noncentered parametrization ) .",
    "the design matrices for each cluster were replicated by 15 times and responses were generated from model using as parameters variational posterior means from the fitted model . for large data sets ,",
    "penalized quasi - likelihood may not be feasible for use as initialization as they converge too slowly ( e.g. penalized quasi - likelihhood took more than 9 mins to converge for this simulated data set ) . using the fit from glm as initialization , algorithm [ alg 1 ] ( using the partially noncentered parametrization ) took 1230.9 s to converge .",
    "skin cancer simulated data .",
    "computation times ( s ) for mini - batch sizes 63 , 126 , 252 and 504 ( from left to right ) corresponding to different stability constants displayed in boxplots.,scaledwidth=90.0% ]    we consider mini - batch sizes @xmath346 ( corresponding to 0.025% , 0.05% , 1% , and 2% of @xmath347 ) and stability constants @xmath348 .",
    "larger stability constants were used for smaller mini - batch sizes . for each mini - batch size and stability",
    "constant , we performed ten runs of algorithm [ alg 2 ] , switching to algorithm [ alg 1 ] when the relative increment in the lower bound after a sweep is less than @xmath223 .",
    "computation times for the four mini - batch sizes corresponding to different stability constants are displayed in boxplots in figure [ skinplot2 ] .",
    "the shortest average time to convergence for different mini - batch sizes are given in table [ skintable2 ] together with the corresponding stability constant @xmath8 . from figure [ skinplot2 ] , computation times",
    "were reduced by a factor of 2 or more across the different mini - batch sizes and stability constants considered . as in the previous example , table [ skintable2 ] showed that larger stability constants @xmath8 are preferred for smaller mini - batch sizes . the shortest average time to convergence of 200.8 s",
    "was achieved by mini - batches of size 504 with @xmath349 .",
    "this represents a reduction in computation time from algorithm [ alg 1 ] by a factor of 6 .",
    "similar results can be achieved by smaller mini - batch sizes with appropriately chosen step sizes .",
    "skin cancer simulated data .",
    "plot of @xmath350 against time for the mini - batch of size 504 ( @xmath349 ) fitted using algorithm [ alg 2 ] in the initial stage followed by algorithm [ alg 1 ] and the whole data set fitted using algorithm [ alg 1].,scaledwidth=65.0% ]    figure [ skinlb ] compares the rate of convergence of standard and stochastic nonconjugate variational message passing for one of the runs where @xmath351 and @xmath349 .",
    "the variational lower bound @xmath113 is @xmath352 at convergence and @xmath350 is plotted against time .",
    "stochastic nonconjugate variational message passing took just 8 sweeps to converge in 208.0 s while standard nonconjugate variational message passing took 62 sweeps and converged in 1230.9 seconds .",
    "this represents a reduction in computation time by a factor of close to 6 .",
    "in this paper , we have extended stochastic variational inference to nonconjugate models and derived a stochastic nonconjugate variational message passing algorithm that is scalable to large data sets .",
    "the data sets that we have considered in this paper were only of moderate size .",
    "nevertheless , we show that computation times can be reduced by applying stochastic nonconjugate variational message passing in the initial stage of optimization .",
    "the stochastic version seems computationally preferable once the number of clusters is of the order of ten thousand and above .",
    "we imagine the gain to be bigger for larger data sets and more work remains to be done in that aspect .",
    "experimentation with various settings of stability constants @xmath8 suggest that larger @xmath8 is preferred for smaller mini - batch sizes . to avoid hand - tuning of step sizes",
    ", it will be useful to develop adaptive step sizes for stochastic nonconjugate variational message passing and we are currently working on extending the work of @xcite to nonconjuagte models . we have also shown that conflict diagnostics for identifying divergent units can be obtained as a by - product of nonconjugate variational message passing .",
    "our diagnostics approximate the approach of @xcite and experiments suggest relatively good agreement between the two methods . for large data sets , computation of conflict @xmath233-values using simulation - based approaches",
    "is very computationally intensive and nonconjugate variational message passing is attractive as an alternative for obtaining prior - likelihood diagnostics or for use as an initial screening tool .",
    "linda tan was partially supported as part of the singapore - delft water alliance s tropical reservoir research programme .",
    "we thank matt wand for making available to us his preliminary work on fully simplified multivariate normal non - conjugate variational message passing updates .",
    "references    amari , s. ( 1998 ) natural gradient works efficiently in learning . _ neural computation _ , 10 , 251276 .",
    "attias , h. ( 1999 ) inferring parameters and structure of latent variable models by variational bayes . in _ proceedings of the 15th conference on uncertainty in artificial intelligence _",
    "k. laskey , h. prade ) , 2130 .",
    "morgan kaufmann , san francisco , ca .    bishop , c. m. ( 2006 ) _ pattern recognition and machine learning_. springer , new york .    booth , j. g. and hobert , j. p. ( 1999 ) maximizing generalized linear mixed model likelihoods with an automated monte carlo em algorithm . _",
    "journal of the royal statistical society : series b _ , 61 , 265285 .",
    "bottou , l. and cun , y. l. ( 2005 ) on - line learning for very large data sets .",
    "_ applied stochastic models in business and industry _ , 21 , 137151 .",
    "bottou , l. and bousquet , o. ( 2008 ) the trade - offs of large scale learning . in _ advances in neural information processing systems 20",
    "_ ( eds . j.c .",
    "platt , d. koller , y. singer and s. roweis ) , 161168 .",
    "neural information processing systems , la jolla , ca .",
    "box , g. e. p. and tiao , g. c. ( 1973 ) _ bayesian inference in statistical analysis_. addison - wesley , ma .",
    "breslow , n. e. and clayton , d. g. ( 1993 ) approximate inference in generalized linear mixed models .",
    "_ journal of the american statistical association _ , 88 , 925 .",
    "broderick , t. , boyd , n. , wibisono , a. , wilson , a. c. and jordan , m. i. ( 2013 ) streaming variational bayes .",
    "_ advances in neural information processing systems 26 _ , 17271735 .",
    "diggle , p. j. , heagerty , p. , liang , k. and zeger , s. l. ( 2002 ) _ analysis of longitudinal data _",
    "( 2nd ed . ) .",
    "oxford university press , uk .",
    "donohue , m. c. , overholser , r. , xu , r. and vaida , f. ( 2011 ) conditional akaike information under generalized linear and proportional hazards mixed models .",
    "_ biometrika _ , 98 , 685-700 .",
    "evans , m. and moshonov , h. ( 2006 ) checking for prior - data conflict .",
    "_ bayesian analysis _ , 4 , 893914 .",
    "farrell , p. j. , groshen , s. , macgibbon , b. and tomberlin , t. j. ( 2010 ) outlier detection for a hierarchical bayes model in a study of hospital variation in surgical procedures . _ statistical methods in medical research _ , 19 , 601619 .",
    "fitzmaurice , g. m. , laird , n. m. and ware , j. h. ( 2004 ) _ applied longitudinal analysis_. wiley , new jersey .",
    "fong , y. , rue , h. and wakefield , j. ( 2010 ) bayesian inference for generalised linear mixed models .",
    "_ biostatistics _ , 11 , 397412 .",
    "gelfand , a. e. , sahu , s. k. and carlin , b. p. ( 1995 )",
    "efficient parametrisations for normal linear mixed models .",
    "_ biometrika _ , 82 , 479488 .    ",
    "( 1996 ) efficient parametrizations for generalized linear mixed models . in _ bayesian statistics 5",
    "j. m. bernardo , j. o. berger , a. p. dawid , and a. f. m. smith ) , 165180 .",
    "clarendon press , oxford .",
    "ghahramani , z. and beal , m. j. ( 2001 ) propagation algorithms for variational bayesian learning . in _ advances in neural information processing systems 13 _ ( eds .",
    "t. k. leen , t. g. dietterich and v. tresp ) , 507513 . mit press , cambridge ,",
    "greenberg , e. r. , baron , j. a. , stevens , m. m. , stukel , t. a. , mandel , j. s. , spencer , s. k. , elias , p. m. , lowe , n. , nierenberg , d. n. , bayrd g. and vance , j. c. ( 1989 ) the skin cancer prevention study : design of a clinical trial of beta - carotene among persons at high risk for nonmelanoma skin cancer .",
    "_ controlled clinical trials _",
    ", 10 , 153166 .",
    "hoffman , m. d. , blei , d. m. and bach , f. ( 2010 ) online learning for latent dirichlet allocation . in _ advances in neural information processing systems 23",
    "j. lafferty , c. williams , j. shawe - taylor , r. zemel and a. culotta ) , 856864 .",
    "neural information processing systems , la jolla , ca .",
    "hoffman , m. d. , blei , d. m. , wang , c. and paisley , j. ( 2013 ) stochastic variational inference .",
    "_ journal of machine learning research _ , 14 , 13031347 .",
    "honkela , a. , tornio , m. , raiko , t. and karhunen , j. ( 2008 ) natural conjugate gradient in variational inference . in _ neural information processing _",
    "( eds . m. ishikawa , k. doya , h. miyamoto and t. yamakawa ) , 305314 .",
    "springer - verlag , berlin .",
    "hosmer , d. w. , lemeshow , s. and sturdivant , r. x. ( 2013 ) _ applied logistic regression ( 3rd ed.)_. john wiley & sons inc . ,",
    "hoboken , new jersey .",
    "huang , a. and wand , m. p. ( 2013 ) simple marginally noninformative prior distributions for covariance matrices .",
    "_ bayesian analysis _ , 8 , 439452 .",
    "ibrahim , j. g. and laud , p. w. ( 1991 ) on bayesian analysis of generalized linear models using jeffrey s prior .",
    "_ journal of the american statistical association _ , 86 , 981986 .",
    "jank , w. ( 2006 ) implementing and diagnosing the stochastic approximation em algorithm . _ journal of computational and graphical statistics _",
    ", 15 , 803829 .",
    "ji , c. , shen , h. and west , m. ( 2010 ) bounded approximations for marginal likelihoods .",
    "available at http://ftp.stat.duke.edu/workingpapers/10-05.pdf .",
    "kass , r. e. and natarajan , r. ( 2006 ) a default conjugate prior for variance components in generalized linear mixed models ( comment on article by browne and draper ) .",
    "_ bayesian analysis _ , 1 , 535542 .",
    "knowles , d. a. , minka , t. p. ( 2011 ) non - conjugate variational message passing for multinomial and binary regression . in _ advances in neural information processing systems 24 _ ( eds",
    ". j. shawe - taylor , r. s. zemel , p. bartlett , f. pereira and k. q. weinberger ) , 17011709 .",
    "neural information processing systems , la jolla , ca .",
    "liang , f. , cheng , y. , song , q. , park , j. and yang , p. ( 2013 ) a resampling - based stochastic approximation method for analysis of large geostatistical data .",
    "_ journal of the american statistical association _",
    ", 108 , 325339 .",
    "liu , q. and pierce , d. a. ( 1994 ) a note on gauss - hermite quadrature .",
    "_ biometrika _ , 81 , 624629 .",
    "lunn , d. , spiegelhalter , d. , thomas , a. and best , n. ( 2009 ) the bugs project : evolution , critique and future directions _ statistics in medicine _ , 28 , 30493067 .",
    "luts , j. , broderick , t. and wand , m. p. ( 2013 ) real - time semiparametric regression available at arxiv : 1209.3550 .",
    "magnus , j. r. and neudecker , h. ( 1988 ) matrix differential calculus with applications in statistics and econometrics .",
    "wiley , chichester , uk .    marshall , e. c. and spiegelhalter , d. j. ( 2007 ) identifying outliers in bayesian hierarchical models : a simulation - based approach .",
    "_ bayesian analysis _ , 2 , 409 - 444 .",
    "nott , d. j. , tan , s. l. , villani , m. and kohn , r. ( 2012 ) regression density estimation with variational methods and stochastic approximation .",
    "_ journal of computational and graphical statistics _ , 21 , 797820 .",
    "nott , d. j. , tran , m .-",
    "n . , kuk , a. y. c. , kohn , r. ( 2013 ) efficient variational inference for generalized linear mixed models with large datasets .",
    "available at arxiv : 1307.7963 .",
    "ormerod , j. t. and wand , m. p. ( 2010 ) explaining variational approximations .",
    "_ the american statistician _ , 64 , 140153 .    ",
    "( 2012 ) gaussian variational approximate inference for generalized linear mixed models .",
    "_ journal of computational and graphical statistics _ , 21 , 217 .",
    "overstall , a. m. and forster , j. j. ( 2010 ) default bayesian model determination methods for generalised linear mixed models .",
    "_ computational statistics and data analysis _",
    ", 54 , 32693288 .",
    "paisley , j. , blei , d. m. and jordan , m. i. ( 2012 ) variational bayesian inference with stochastic search . in _ proceedings of the 29th international conference on machine learning _",
    "j. langford and j. pineau ) , 13671374 .",
    "omnipress , madison , wi .",
    "papaspiliopoulos , o. , roberts , g. o. and skld , m. ( 2003 ) non - centered parametrizations for hierarchical models and data augmentation . _ bayesian statistics 7 _ ( eds .",
    "j. m. bernardo , m. j. bayarri , j. o. berger , a. p. dawid , d. heckerman , a. f. m. smith , m. west ) , 307326 .",
    "oxford university press , new york .    ",
    "( 2007 ) a general framework for the parametrization of hierarchical models . _ statistical science _",
    ", 22 , 5973 .",
    "petris , g. and tardella , l. ( 2003 ) a geometric approach to transdimensional markov chain monte carlo . _ the canadian journal of statistics _ , 31 , 469482 .",
    "polyak , b. t. and juditsky , a. b. ( 1992 ) acceleration of stochastic approximation by averaging .",
    "_ siam journal on control and optimization _",
    ", 30 , 838855 .",
    "presanis , a. m. , ohlssen , d. , spiegelhalter , d. j. and de angelis , d. ( 2013 ) conflict diagnostics in directed acyclic graphs , with applications in bayesian evidence synthesis .",
    "_ statistical science _ , 28 , 376397",
    "ranganath , r. , wang , c. , blei , d. m. and xing , e. p. ( 2013 ) an adaptive learning rate for stochastic variational inference .",
    "_ jmlr w&cp : proceedings of the 30th international conference on machine learning _ , 28 , 298306    raudenbush , s.w . , yang , m.l . and yosef , m. ( 2000 ) maximum likelihood for generalized linear models with nested random effects via high - order , multivariate laplace approximation .",
    "_ journal of computational and graphical statistics _ , 9 , 141157 .",
    "robbins , h. and monro , s. ( 1951 ) a stochastic approximation method . _",
    "annals of mathematical statistics _ 22 , 400407 .",
    "roux , n. l. , schmidt , m. and bach , f. ( 2012 ) a stochastic gradient method with an exponential convergence rate for finite training sets .",
    "_ advances in neural information processing systems 25 _ ( eds .",
    "p. bartlett , f.c.n .",
    "pereira , c.j.c .",
    "burges , l. bottou and k.q .",
    "weinberger ) .",
    "available at http://books.nips.cc/papers/files/nips25/nips2012_1246.pdf    salimans , t. and knowles , d. a. ( 2013 ) fixed - form variational posterior approximation through stochastic linear regression .",
    "_ bayesian analysis _ , 4 , 837882 .",
    "sato , m. ( 2001 ) online model selection based on the variational bayes .",
    "_ neural computation _",
    ", 13 , 16491681 .",
    "scheel , i. , green , p. j. and rougier , j. c. ( 2011 ) a graphical diagnostic for identifying influential model choices in bayesian hierarchical models .",
    "_ scandinavian journal of statistics _ , 38 , 529550 .",
    "spall , j. c. ( 2003 ) introduction to stochastic search and optimization : estimation , simulation and control .",
    "wiley , new jersey .",
    "sturtz , s. , ligges , u. , and gelman , a. ( 2005 ) r2winbugs : a package for running winbugs from r. _ journal of statistical software _",
    ", 12 , 116 .",
    "tan , l. s. l. and nott , d. j. ( 2013 ) variational inference for generalized linear mixed models using partially non - centered parametrizations .",
    "_ statistical science _ , 28 , 168188 .",
    "thall , p. f. and vail , s. c. ( 1990 ) some covariance models for longitudinal count data with overdispersion .",
    "_ biometrics _ , 46 , 657671 .",
    "thara , r. , henrietta , m. , joseph , a. , rajkumar , s. and eaton , w. ( 1994 ) ten year course of schizophrenia - the madras longitudinal study .",
    "_ acta psychiatrica scandinavica _ , 90 , 329336 .",
    "tseng , p. ( 1998 ) an incremental gradient(-projection ) method with momentum term and adaptive stepsize rule .",
    "_ siam journal on optimization _",
    ", 8 , 506531 .",
    "venables , w. n. and ripley , b. d. ( 2002 ) _ modern applied statistics with s _ , 4th ed .",
    "springer , new york .",
    "wand , m. p. ( 2013 ) fully simplified multivariate normal updates in non - conjugate variational message passing .",
    "available at http://www.uow.edu.au/~mwand/fsupap.pdf .",
    "wang , c. , paisley , j. and blei , d. m. ( 2011 ) online variational inference for the hierarchical dirichlet process . _",
    "journal of machine learning research - proceedings track _ ( eds .",
    "g. gordon , d. dunson and m. dudk ) , 15 , 752760 .",
    "wang , b. and titterington , d. m. ( 2005 ) inadequacy of interval estimates corresponding to variational bayesian approximations . in _ proceedings of the 10th international workshop on artificial intelligence and statistics _ ( eds .",
    "r. g. cowell and z , ghahramani ) , 373380 .",
    "society for artificial intelligence and statistics .",
    "winn , j. and bishop , c. m. ( 2005 ) variational message passing .",
    "_ journal of machine learning research _",
    ", 6 , 661694 .",
    "xiao , l. ( 2010 ) dual averaging methods for regularized stochastic learning and online optimization . _ journal of machine learning research _ , 11 , 25432596 .",
    "zhao , h. and marriott , p. ( 2013",
    ") diagnostics for variational bayes approximations .",
    "available at arxiv:1309.5117 .",
    "zhu , h. t. and lee , s. y. ( 2002 ) analysis of generalized linear mixed models via a stochastic approximation algorithm with markov chain monte carlo method . _ statistics and computing _ , 12 , 175183 .",
    "let @xmath353 denote the neighbourhood of @xmath354 in the factor graph of @xmath355 ( see * ? ? ?",
    "suppose @xmath356 and each factor @xmath357 in @xmath353 is conjugate to @xmath358 , say @xmath359 where @xmath360",
    ". then @xmath361\\ ] ] and the nonconjugate variational message passing update in reduces to @xmath362 note that @xmath363 does not depend on @xmath364 .",
    "the natural gradient in can also be simplified as @xmath365",
    "for poisson responses , @xmath366 for @xmath144 . for bernoulli responses , @xmath367 for @xmath144 , where @xmath368 and @xmath369 .",
    "we have @xmath370 where @xmath32 and @xmath371 denotes the @xmath15th derivative of @xmath20 with respect to @xmath372 . if @xmath373 and @xmath374 are vectors , say @xmath375 $ ] and @xmath376 $ ] , then @xmath377 $ ] .",
    "the terms , @xmath378 , @xmath379 may be evaluated efficiently using adaptive gauss - hermite quadrature @xcite .",
    "more details can be found in @xcite .",
    "the updates in step 2 of algorithm [ alg 2 ] are taken directly from the nonconjugate variational message passing algorithm for glmms in @xcite . to derive the updates in step 3 ,",
    "let us first introduce the following notation for specification of the natural parameter vectors @xmath138 and @xmath139 .",
    "for a @xmath3 square matrix @xmath8 , let @xmath380 denote the @xmath381 vector obtained by stacking the columns of @xmath8 under each other , from left to right in order and @xmath382 denotes the @xmath383 vector obtained from @xmath380 by eliminating all supradiagonal elements of @xmath8 .",
    "the matrix @xmath384 is a unique @xmath385 matrix that transforms @xmath382 into @xmath380 if @xmath8 is symmetric , that is , @xmath386 .",
    "see @xcite for more details .",
    "we have @xmath387 from , @xmath388 where @xmath389^{-1 } \\\\",
    "\\hat{\\mu}_{q(\\beta ) } & = \\mu_{q(\\beta)}^{(t ) } + \\hat{\\sigma}_{q(\\beta)}\\left [   \\frac{n}{|b|}\\sum_{i\\in b } \\big\\ { \\nu_{q(d ) } { \\tilde{w}_i}^ts_{q(d)}^{-1}(\\mu_{q(\\tilde{\\alpha}_i)}-\\tilde{w}_i\\mu_{q(\\beta)}^{(t ) } ) + v_i^t(y_i - g_i ) \\big\\}-\\sigma_\\beta^{-1}\\mu_{q(\\beta)}^{(t ) } \\right].\\end{aligned}\\ ] ] expressions for @xmath390 and @xmath391 can be deduced from algorithm 3 of @xcite . the first line in gives @xmath392 which is the update for @xmath205 in algorithm 2",
    ". the second line in gives @xmath393,\\end{aligned}\\ ] ] which is the update for @xmath204 in algorithm 2 .",
    "similarly , from , we have @xmath394 where @xmath395 and @xmath396 can be deduced from @xcite as @xmath397 and @xmath398 .",
    "the updates for @xmath217 and @xmath399 in algorithm 2 can be obtained by simplifying .",
    "in the centered parametrization , @xmath400 for @xmath401 . to generate likelihood replicates @xmath402 from @xmath403 in the cross - validatory approach , we consider jeffreys s prior for the centered random effects @xmath73 .",
    "jeffreys s prior is defined as @xmath404 , where @xmath405 is the fisher information matrix of @xmath73 . for poisson and logistic glmms",
    ", it can be shown that @xmath406 , where @xmath407 is a @xmath52 diagonal matrix ( see , e.g. * ? ? ?",
    "definitions of @xmath407 are given in section [ sec_pncp ] .",
    "in general , we will need to consider @xmath408 as a nuisance parameter .",
    "following the discussion in section [ cv ] , we generate a @xmath408 from @xmath409 and simulate @xmath402 from @xmath410 where @xmath411 is jeffreys s prior . for poisson glmms ,",
    "@xmath412 for logistic glmms , @xmath413 |z_i^t q_i z_i|^{\\frac{1}{2}}.\\ ] ]",
    "the madras schizophrenia study @xcite contains records of the psychiatric symptoms of 86 patients in the first year after initial hospitalization .",
    "this data set has been analyzed by @xcite and is available at http://faculty.washington.edu/heagerty/books/analysislongitudinal/datasets.html .",
    "the reponse @xmath10 is 1 if the symptom `` thought disorder '' is present and 0 otherwise .",
    "we consider the covariates , age at onset of disease ( age = 1 if patient is at least 20 years old and 0 otherwise ) , sex of patient ( gender = 1 if female and 0 otherwise ) and number of months since hospitalization when symptom was recorded ( @xmath147 ) .",
    "we consider the logistic random effects model : @xmath414 where @xmath415 for @xmath416 , @xmath417 .",
    "we report both one - sided ( upper - tail ) and two - sided conflict @xmath233-values for this example .",
    "the upper - tail one - sided conflict @xmath233-values are useful for identifying patients with unusually large number of  thought disorders \" while the two - sided conflict @xmath233-values can be used to detect patients with either more or less than the expected number of  thought disorders \" .    in the simulation - based approaches , @xmath418 , @xmath329 and @xmath419",
    "have to be regarded as nuisance parameters under the centered parametrization ( see appendix c ) . for each model",
    "fitting via mcmc , two chains were run simultaneously to assess convergence , each with 26,000 iterations , and the first 1000 iterations were discarded in each chain as burn - in .",
    "simulation - based conflict @xmath233-values were calculated based on the remaining 50,000 simulations . for the cross - validatory approach ,",
    "model refitting took a total of 372 s @xmath304 86 ( more than 8 hours ) to complete in openbugs .",
    "simulation of prior and likelihood replicates of the centered random effects @xmath73 was performed in r. assuming jeffreys s prior for @xmath73 , likelihood replicates were simulated using adaptive rejection metropolis sampling .",
    "variational lower bounds and cpu times taken for model fitting and computing conflict @xmath233-values by algorithm [ alg 1 ] ( different parametrizations ) and mcmc ( full - data approach ) are given in table [ madrastable ] .",
    "figure [ madrasplot ] shows the marginal posteriors of parameters estimated using mcmc and algorithm [ alg 1 ] .",
    "the partially noncentered parametrization took the shortest time to converge and attained the highest lower bound . from figure [ madrasplot ] , partial noncentering produced better posterior approximations for @xmath326 , @xmath327 and @xmath328 than both centering and noncentering . for @xmath418 , @xmath329 , @xmath315 , partial centering performed better than centering but did not do as well as noncentering .",
    "cross - validatory conflict @xmath233-values are plotted against conflict @xmath233-values from nonconjugate variational message passing using the partially noncentered parametrization in figure [ madrascv ] .",
    "the left plot shows the upper - tail one - sided @xmath233-values while the right plot shows the two - sided @xmath233-values . the mean absolute difference in @xmath288-scores for nonconjugate variational message passing and the simulation - based full - data approach relative to the cross - validatory approach",
    "are given in table [ madrastable ] .",
    "figure [ madrascv ] shows that the agreement between the cross - validatory approach and nonconjugate variational message passing is better for the one - sided @xmath233-values than in the two - sided case .",
    "this is expected as any discrepancy between the two sets of @xmath233-values will be doubled in the two - sided case .",
    "however , we note that agreement at the extremes is still relatively good . for this example , the simulation - based full - data approach performed better in terms of @xmath288-scores than nonconjugate variational message passing .",
    "this is likely due to the fact that in this case , the variational posterior does not provide as good an approximation to the true posterior as in examples 6.1 and 6.2 .",
    "however , nonconjugate variational message passing remains useful as a screening tool as the computation time required to compute conflict @xmath233-values even in the simulation - based full - data approach is quite significant .",
    "finally , outliers ( at the 0.05 level ) identified by the cross - validatory approach and nonconjugate variational message passing using the partially noncentered parametrization are identical in this example .",
    "conflict @xmath233-values for these outliers are shown in table [ madrastable2 ] .",
    "madras data .",
    "cross - validatory conflict @xmath233-values plotted against conflict @xmath233-values from nonconjugate variational message passing with a partially noncentered parametrization.,scaledwidth=80.0% ]"
  ],
  "abstract_text": [
    "<S> in stochastic variational inference , the variational bayes objective function is optimized using stochastic gradient approximation , where gradients computed on small random subsets of data are used to approximate the true gradient over the whole data set . </S>",
    "<S> this enables complex models to be fit to large data sets as data can be processed in mini - batches . in this article </S>",
    "<S> , we extend stochastic variational inference for conjugate - exponential models to nonconjugate models and present a stochastic nonconjugate variational message passing algorithm for fitting generalized linear mixed models that is scalable to large data sets . </S>",
    "<S> in addition , we show that diagnostics for prior - likelihood conflict , which are useful for bayesian model criticism , can be obtained from nonconjugate variational message passing automatically , as an alternative to simulation - based markov chain monte carlo methods . </S>",
    "<S> finally , we demonstrate that for moderate - sized data sets , convergence can be accelerated by using the stochastic version of nonconjugate variational message passing in the initial stage of optimization before switching to the standard version .    a stochastic variational framework for fitting and + diagnosing generalized linear mixed models    linda s. l. tan and david j. nott    _ keywords _ : variational bayes , stochastic approximation , nonconjugate variational message passing , conflict diagnostics , hierarchical models , identifying divergent units . </S>"
  ]
}