{
  "article_text": [
    "much of our intuition about the world around us involves the idea of clustering : many different acoustic waveforms correspond to the same syllable , many different images correspond to the same object , and so on .",
    "it is plausible that a mathematically precise notion of clustering in the space of sense data may approximate the problems solved by our brains .",
    "clustering methods also are used in many different scientific domains as a practical tool to evaluate structure in complex data .",
    "interest in clustering has increased recently because of new areas of application , such as data mining , image and speech  processing and bioinformatics . in particular ,",
    "many groups have used clustering methods to analyze the results of genome  wide expression experiments , hoping to discover genes with related functions as members of the same cluster ; see , for example , eisen , spellman , brown and botstein ( 1998 ) .",
    "a central issue in these and other applications of clustering is how many clusters provide an appropriate description of the data .",
    "the estimation of the true number of classes has been recognized as `` one of the most difficult problems in cluster analysis '' by bock ( 1996 ) , who gives a review of some methods that address the issue . +   + the goal of clustering is to group data in a meaningful way .",
    "this is achieved by optimization of a so called `` clustering criterion '' ( an objective function ) , and a large variety of intuitively reasonable criteria have been used in the literature ( a summary is given in gordon , 1999 ) .",
    "clustering methods include agglomerative clustering procedures such as described by ward ( 1963 ) and iterative re - allocation methods , such as the commonly used k  means algorithm ( lloyd , 1957 ; macqueen , 1967 ) , which reduces the sum of squares criterion , the average of the within cluster squared distances .",
    "more recently , algorithms with physically inspired criteria were introduced ( blatt , wiseman and domany 1996 ; horn and gottlieb , 2002 ) .",
    "all these clustering methods have in common that the number of clusters has to be found by another criterion .",
    "often , a two - step procedure is performed : the optimal partition is found for a given data set , according to the defined objective function , and then a separate criterion is applied to test the robustness of the results against noise due to finite sample size .",
    "such procedures include the definition of an intuitively reasonable criterion for the goodness of the classification , as in tibshirani , walther and hastie ( 2001 ) , or performing cross  validation ( stone , 1974 ) and related methods in order to estimate the prediction error and to find the number of clusters that minimizes this error ( e.g. smyth , 2000 ) .",
    "roth , lange , braun and buhmann ( 2002 ) quantify the goodness of the clustering via a resampling approach .",
    "+   + it would be attractive if these two steps could be combined in a single principle . in a sense",
    "this is achieved in the probabilistic mixture model approach , but at the cost of assuming that the data can be described by a mixture of @xmath0 multivariate distributions with some parameters that determine their shape .",
    "now the problem of finding the number of clusters is a statistical model selection problem .",
    "there is a trading between complexity of the model and goodness of fit .",
    "one approach to model selection is to compute the total probability that models with @xmath0 clusters can give rise to the data , and then one finds that phase space factors associated with the integration over model parameters serve to discriminate against more complex models ( balasubramanian , 1997 ) .",
    "this bayesian approach has been used to determine the number of clusters ( fraley and raftery , 2002 ) .",
    "+   + from an information theoretic point of view , clustering is most fundamentally a strategy for lossy data compression : the data are partitioned into groups such that the data could be described in the most efficient way ( in terms of bit cost ) by appointing a representative to each group .",
    "the clustering of the data is achieved by compressing the original data into representatives , throwing away information that is not relevant to the analysis . while most classical approaches in statistics give an explicit definition of a similarity measure , in rate ",
    "distortion theory we arrive at a notion of similarity through a fidelity criterion implemented by a distortion function ( shannon 1948 ) .",
    "the choice of the distortion function provides an implicit distinction between relevant and irrelevant information in the raw data .",
    "the notion of relevance was made explicit by tishby , pereira and bialek ( 1999 ) who defined relevant information as the information which the data provide about an auxiliary variable and performed lossy compression , keeping as much relevant information as possible .",
    "this formulation , termed `` information bottleneck method '' ( ib ) , is attractive , because the objective function follows only from information theoretical principles .",
    "in particular , this formulation does not require an explicit definition of a measure for similarity or distortion .",
    "the trade  off between the complexity of the model on one hand and the amount of relevant information it captures on the other hand is regulated by a trade - off parameter . for a given problem , the complete range of this trade ",
    "off is meaningful , and the structure of the trade  of characterizes the `` clusterability '' of the data .",
    "however , for a _ finite _ data set , there should be a maximal value for this trade  off after which we start to `` overfit , '' and this issue has not yet been addressed in the context of the ib . in related",
    "work , buhmann and held ( 2000 ) derived for a particular class of histogram clustering models a lower bound on the annealing temperature from a bound on the probability of a large deviation between the error made on the training data and the expected error .",
    "+   + in this article , we follow the intuition that if a model  which , in this context , is a ( probabilistic ) partition of the data set  captures information ( or structure ) in the data , then we should be able to quantify this structure in a way that corrects automatically for the overfitting of finite data sets .",
    "attempts to capture only this `` corrected information '' will , by definition , not be sensitive to noise . put another way ,",
    "if we would separate at the outset real structure from spurious coincidences due to undersampling , then we could fit only the real structure . in the context of information",
    "estimation from finite samples there is a significant literature on the problem and we argue here that the known finite sample correction to information estimates is ( in some limits ) sufficient to achieve the `` one - step '' compression and clustering in the sense described above , leading us naturally to a principled method of finding the best clustering that is consistent with a finite data set . +   + we should point out that in general we are not looking for the `` true '' number of clusters , but rather for the maximum number of clusters which can be resolved from a finite data set .",
    "this number equals the true number only if there exists a true number of classes and if the data set is also large enough to allow us to resolve them .",
    "if data @xmath1 are chosen from a probability distribution @xmath2 , then a complete description of a single data point requires an average code length equal to the entropy of the distribution , @xmath3 $ ] bits . on the other hand ,",
    "if we assign points to clusters @xmath4 , then we need at most @xmath5 bits .",
    "for @xmath6 we have @xmath7 , and our intuition is that many problems will allow substantial compression at little cost if we assign each @xmath8 to a cluster @xmath9 and approximate @xmath8 by a representative @xmath10 .    [ [ rate - distortion - theory ] ]",
    "rate distortion theory + + + + + + + + + + + + + + + + + + + + + +    formalizes the cost of approximating the signal @xmath8 by @xmath10 as the expected value of some distortion function , @xmath11 ( shannon 1948 ) .",
    "this distortion measure can , but need not be a metric .",
    "lossy compression is achieved by assigning the data to clusters such that the mutual information @xmath12 \\label{mi}\\ ] ] is minimized .",
    "the minimization is constrained by fixing the expected distortion @xmath13 this leads to the variational problem @xmath14.\\ ] ] the ( formal ) solution is a boltzmann distribution , because the information is measured in bits in eq .",
    "( [ mi ] ) .",
    "@xmath15 is calculated as @xmath16 . ]",
    "@xmath17 , \\label{boltz1}\\ ] ] with the distortion playing the role of energy , and the normalization @xmath18\\ ] ] playing the role of a partition function ( rose , gurewitz and fox , 1990 ) .",
    "the representatives , @xmath10 , often simply called cluster centers , are determined by the condition that all of the `` forces '' within each cluster balance for a test point located at the cluster center , , we find : @xmath19 = 0 \\leftrightarrow   \\frac{\\partial}{\\partial x_c}\\langle d(x , x_c)\\rangle = p(c ) \\sum_x p(x|c ) \\frac{\\partial}{\\partial x_c } d(x , x_c ) = 0 ; \\ ; \\forall c \\rightarrow$ ] eq .",
    "( [ centers ] ) ] @xmath20 recall that if the distortion measure is the squared distance , @xmath21 , then eq .",
    "( [ centers ] ) becomes @xmath22 ; the cluster center is in fact the center of mass of the points which are assigned to the cluster .",
    "+   + the lagrange parameter @xmath23 regulates the trade  off between the detail we keep and the bit cost we are willing to pay ; in analogy with statistical mechanics , @xmath23 often is referred to as the temperature ( rose , gurewitz and fox , 1990 ) .",
    "@xmath23 measures the softness of the cluster membership .",
    "the deterministic limit ( @xmath24 ) is the limit of hard clustering solutions .",
    "as we lower @xmath23 there are phase transitions among solutions with different numbers of distinct clusters , and if we follow these transitions we can trace out a curve of @xmath25 vs. @xmath26 , both evaluated at the minimum .",
    "this is the rate ",
    "distortion curve and is analogous to plotting energy vs. ( negative ) entropy with temperature varying parametrically along the curve .",
    "crucially , there is no optimal temperature which provides the unique best clustering , and thus there is no optimal number of clusters : more clusters always provide a more detailed description of the original data and hence allow us to achieve smaller average values of the distortion @xmath11 , while the cost of the encoding increases .",
    "+    [ [ information - bottleneck - method ] ] information bottleneck method + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the distortion function _ implicitly _ selects the features that are relevant for the compression .",
    "however , for many problems , we know _ explicitly _ what it is that we want to keep information about while compressing the data , but one can not always construct the distortion function that selects for these relevant features . in the information bottleneck method ( tishby , pereira and bialek , 1999 )",
    "the relevant information in the data is defined as information about another variable , @xmath27 .",
    "both @xmath8 and @xmath28 are random variables and we assume that we know the distribution of co - occurrences , @xmath29 .",
    "we wish to compress @xmath8 into clusters @xmath9 , such that the relevant information , i.e. the information about @xmath28 , is maximally preserved .",
    "this leads directly to the optimization problem @xmath30 . \\label{ibn_vari}\\ ] ] one obtains a solution similar to eq .",
    "( [ boltz1 ] ) @xmath31\\right ] \\label{ibn_assign}\\ ] ] in which the kullback ",
    "leibler divergence , @xmath32 = \\sum_v p(v|x ) \\log_2 \\left [ \\frac{p(v|x)}{p(v|c ) } \\right],\\ ] ] emerges in the place of the distortion function ( tishby , pereira and bialek , 1999 ) , providing a notion of similarity between the distributions @xmath33 and @xmath34 , where @xmath34 is given by @xmath35 when we plot @xmath36 as a function of @xmath26 , both evaluated at the optimum , we obtain a curve similar to the rate distortion curve , the slope of which is given by the trade  off between compression and preservation of relevant information : @xmath37",
    "the formulation above assumes that we know the probability distribution underlying the data , but in practice we have access only to a finite number of samples , and so there are errors in the estimation of the distribution .",
    "these random errors produce a systematic error in the computation of the cost function .",
    "the idea here is to compute the error perturbatively and subtract it from the objective function .",
    "optimization with respect to the assignment rule is now by definition insensitive to noise and we should ( for the ib ) find a value for the trade  off parameter @xmath38 at which the relevant information is kept maximally .",
    "+   + the compression problem expressed in eq . ( [ ibn_vari ] ) gives us the right answer if we evaluate the functional ( [ ibn_vari ] ) at the true distribution @xmath29 .",
    "but in practice we do not know @xmath29 , instead we have to use an estimate @xmath39 based on a finite data set .",
    "we use perturbation theory to compute the systematic error in the cost function that results from the uncertainty in the estimate . +   + we first consider the case that @xmath2 is known and we have to estimate only the distribution @xmath33 .",
    "this is the case in many practical clustering problems , where @xmath8 is merely an index to the identity of samples , and hence @xmath2 is constant , and the real challenge is to estimate @xmath33 . in section [ uncertain_px ] , we discuss the error that comes from uncertainty in @xmath2 and also what happens when we apply this approach to rate  distortion theory .",
    "+   + viewed as a functional of @xmath40 , @xmath26 can have errors arising only from uncertainty in estimating @xmath2 . therefore ,",
    "if @xmath2 is known , then there is no bias in @xmath26 .",
    "we assume for simplicity that @xmath28 is discrete .",
    "let @xmath41 be the total number of observations of @xmath8 and @xmath28 .",
    "for a given @xmath8 , the ( average ) number of observations of @xmath28 is then @xmath42 .",
    "we assume that the estimate @xmath43 converges to the true distribution in the limit of large data set size @xmath44 .",
    "however , for finite @xmath41 , the estimated distribution will differ from the true distribution and there is a regime in which @xmath41 is large enough such that we can approximate ( compare treves and panzeri , 1995 ) @xmath45 where we assume that @xmath46 is some small perturbation and its average over all possible realizations of the data is zero @xmath47 taylor expansion of @xmath48 around @xmath33 leads to a systematic error @xmath49 : @xmath50 where the error @xmath51 with @xmath52\\end{aligned}\\ ] ] is given by @xmath53 note that the terms with @xmath54 vanish , because of eq .",
    "( [ pert.ave.zero ] ) and that the second term in the sum is constant with respect to @xmath40 .",
    "+   + our idea is to subtract this error from the objective function ( [ ibn_vari ] ) and to recompute the distribution that maximizes the corrected objective function . @xmath55.\\ ] ] the last constraint ensures normalization , and the optimal assignment rule @xmath40 is now given by @xmath56 + \\sum_v   \\sum_{n=2}^{\\infty } \\frac{(-1)^n}{\\ln(2 ) } \\right .",
    "\\nonumber   \\\\ & & \\left .",
    "\\times \\left [ p(v|x ) \\frac{\\langle ( \\delta p(v|c))^{n}\\rangle}{n ( p(v|c))^{n } } - \\frac{\\langle \\delta p(v|x)(\\delta p(v|c))^{n-1}\\rangle}{(n-1 ) ( p(v|c))^{n-1}}\\right ] \\bigg ) \\right ] \\label{full_pcx}\\end{aligned}\\ ] ] which has to be solved self consistently together with eq .",
    "( [ markov ] ) and @xmath57 + the error @xmath49 is calculated in eq .",
    "( [ theerror_1 ] ) as an asymptotic expansion and we are assuming that @xmath41 is large enough to ensure that @xmath46 is small @xmath58 .",
    "let us thus concentrate on the term of leading order in @xmath46 which is given by ( disregarding the term which does not depend on @xmath40 ) ) by calculating the first leading order term ( @xmath59 ) in the sum of eq .",
    "( [ theerror_1 ] ) : @xmath60 making use of approximation ( [ approx ] ) and summing over @xmath61 , which leads to @xmath62 ^ 2 p(v|x ) p(x)}{p(c|v ) p(v)},\\ ] ] and then substituting @xmath63 and @xmath64 ( compare eq .",
    "( [ markov ] ) ) . ] @xmath65 ^ 2 p(x|v)}{\\sum_x p(c|x ) p(x|v ) } , \\label{lead_error}\\ ] ] where we have made use of eq .",
    "( [ markov ] ) and the approximation ( for counting statistics ) @xmath66 + can we say something about the shape of the resulting `` corrected '' optimal information curve by analyzing the leading order error term ( eq . [ lead_error ] ) ?",
    "this term is bounded from above by the value it assumes in the _ deterministic limit _",
    "( @xmath24 ) , in which assignments @xmath40 are either 1 or 0 and thus @xmath67 ^ 2 = p(c|x)$],^2 = p(c|x)$ ] into eq .",
    "( [ lead_error ] ) gives @xmath68 . ]",
    "@xmath69 @xmath70 is the number of bins we have used to obtain our estimate @xmath43 .",
    "note that if we had adopted a continuous , rather than a discrete , treatment , then the volume of the ( finite ) @xmath71-space would arise instead of @xmath70 .",
    ", as a function of the data set size @xmath41 , we refer to the large body of literature on this problem , for example hall and hannan ( 1988 ) . ] if one does not make the approximation ( [ approx ] ) and , in addition , also keeps the terms constant in @xmath40 , then one obtains for the upper bound ( @xmath24 limit ) of the @xmath59 term from expression ( [ theerror_1 ] ) : @xmath72 which is the leading correction to the bias as derived in ( treves and panzeri , 1995 ; eq .",
    "( 2.11 ) ; term @xmath73 ) .",
    "similar to what these authors found when they computed higher order corrections we also found in numerical experiments that the leading term is a surprisingly good estimate of the total bias and we therefore feel confident to approximate the error by ( [ lead_error ] ) , although we can not guarantee convergence of the series in ( [ theerror_1 ] ) . ]",
    "where @xmath74 , the @xmath75 are positive integers , and the sum @xmath76 runs over all partitions of @xmath77 , i.e. , values of @xmath78 , ... , @xmath79 such that @xmath80 .",
    "there is a growing number of contributions to the sum at each order @xmath81 , some of which can be larger than the smallest terms of the expression at order @xmath82 . if there are enough measurements such that @xmath42 is large , the binomial distribution approaches a normal distribution with @xmath83 , and @xmath84 ( @xmath85 ) .",
    "substituting this into ( [ theerror_1 ] ) , and considering only terms with @xmath86 , we get @xmath87^k$ ] , which is not guaranteed to converge . ]",
    "the lower bound of the leading order error ( eq . [ lead_error ] ) is given by } \\geq 2^{i(c;x)}$ ] ] @xmath88 and hence the `` corrected information curve '' , which we define as @xmath89 is ( to leading order ) bounded from above by @xmath90 the slope of this upper bound is @xmath91 ( using eq .",
    "( [ ibn_slope ] ) ) , and there is a maximum at @xmath92 +    * if the hard clustering solution assigns equal numbers of data to each cluster * , then the upper bound on the error , eq .",
    "( [ error.ub ] ) , can be rewritten as @xmath93 and therefore , the lower bound on the information curve , @xmath94 has a maximum at @xmath95 since both upper and lower bound coincide at the endpoint of the curve , where @xmath24 ( see sketch in fig . [ sketchfig ] ) , the actual corrected information curve must have a maximum at @xmath96 where @xmath97 .",
    "+   + * in general * , for deterministic assignments , the information we gain by adding another cluster saturates for large @xmath0 , and it is reasonable to assume that this information grows sub - linearly in the number of clusters .",
    "that means that the lower bound on @xmath98 has a maximum ( or at least a plateau ) .",
    "this ensures us that @xmath98 must have a maximum ( or plateau ) , and hence that an optimal temperature exists . +   + in the context of the ib , asking for the number of clusters that are consistent with the uncertainty in our estimation of @xmath33 makes sense only for deterministic assignments . from the above discussion",
    ", we know the leading order error term in the deterministic limit , and we define the `` corrected relevant information '' in the limit @xmath24 as : @xmath99 where @xmath100 is calculated by fixing the number of clusters and cooling the temperature down to obtain a hard clustering solution . while @xmath100 increases monotonically with @xmath0 , we expect @xmath101 to have a maximum ( or at least a plateau ) at @xmath102 , as we have argued above .",
    "@xmath102 , is then the optimal number of clusters in the sense that using more clusters , we would not capture more meaningful structure ( or in other words would `` overfit '' the data ) , and although in principle we could always use fewer clusters , this comes at the cost of keeping less relevant information @xmath36 .",
    "we test our method for finding @xmath103 on data that we understand well and where we know what the answer _ should _ be .",
    "we thus created synthetic data drawn from normal distributions with zero mean and 5 different variances ( for figs .",
    "[ res_5clus_1 ] and [ res_5clus_2 ] ) . and @xmath104 where @xmath105 , and @xmath106 , with @xmath107 ; and @xmath108 .",
    "@xmath109 is the number of `` objects '' we are clustering . ]",
    "we emphasize that we chose an example with gaussian distributions _ not _ because any of our analysis makes use of gaussian assumptions , but rather because in the gaussian case we have a clear intuition about the similarity of different distributions and hence about the difficulty of the clustering task .",
    "this will become important later , when we make the discrimination task harder ( see fig .",
    "[ planegauss_space_n_2_5_10 ] ) .",
    "we use @xmath110 bins to estimate @xmath43 . in figs .",
    "[ res_5clus_1 ] and [ res_5clus_2 ] , we compare how @xmath111 and @xmath101 behave as a function of the number of clusters .",
    "the number of observations of @xmath28 , given @xmath8 , is @xmath112 . for a large range of `` resolutions '' ( average number of observations per bin ) , @xmath113",
    ", @xmath101 has a maximum at @xmath114 ( the true number of clusters ) .",
    "when we have too little data ( @xmath115 ) , we can resolve only 4 clusters .",
    "the curves in figs .",
    "[ res_5clus_1 ] and [ res_5clus_2 ] are calculated as the mean of the curves obtained from 5 different realizations of the data , , we start at 100 different , randomly chosen initial conditions to increase the probability of finding the global maximum of the objective functional . ] and the error bars are @xmath116 1 standard deviation .",
    "the optimal number of clusters @xmath102 , as determined by our method , is always the same , for each of these individual curves , so that there are no error bars on the optimal number of clusters as a function of the data set size . as @xmath113 becomes very large , @xmath101 approaches @xmath111 , as expected . +   + the curves in figs .",
    "[ res_5clus_1 ] and [ res_5clus_2 ] differ in the average number of examples per bin , @xmath113 .",
    "the classification problem becomes harder as we see less data .",
    "however , it also becomes harder when the true distributions are more like each other . to separate the two effects , we create synthetic data drawn from gaussian distributions with unit variance and @xmath117 different , equidistant means @xmath118 , which are @xmath119 apart . , @xmath105 , @xmath120 . ]",
    "@xmath117 is the true number of clusters .",
    "this problem becomes intrinsically harder as @xmath119 becomes smaller .",
    "examples are shown in figs .",
    "[ dataeasy ] and [ data ] .",
    "the problem becomes easier as we are allowed to look at more data , which corresponds to an increase in @xmath113 .",
    "we are interested in the regime in the space spanned by @xmath113 and @xmath119 in which our method retrieves the correct number of clusters . +   + in fig .",
    "[ planegauss_space_n_2_5_10 ] , points mark those values of @xmath119 and @xmath113 ( evaluated on the shown grid ) at which we find the true number of clusters .",
    "the different shapes of the points summarize results for 2 , 5 and 10 clusters . a missing point on the grid",
    "indicates a value of @xmath119 and @xmath113 at which we did not find the correct number of clusters .",
    "all these missing points lie in a regime which is characterized by a strong overlap of the true distributions combined with scarce data . in that regime",
    ", our method always tells us that we can resolve _ fewer _ clusters than the true number of clusters . for small sample sizes ,",
    "the correct number of clusters is resolved only if the clusters are well separated , but as we accumulate more data , we can recover the correct number of classes for more and more overlapping clusters . to illustrate the performance of the method , we show in fig .",
    "[ data ] the distribution @xmath29 in which @xmath121 has 5 different values which occur with equal probability , @xmath122 and which differ by @xmath123 . for this separation ,",
    "our method still retrieves 5 as the optimal number of clusters when we have @xmath124 observations per example @xmath8 . bins to estimate @xmath33.the distribution of examples is simply @xmath125 with @xmath126 . ] +   + our method detects when only one cluster is present , a case in which many methods fail ( gordon , 1999 ) .",
    "we verified this for data drawn from one gaussian distribution and for data drawn from the uniform distribution .",
    "we consider data drawn from a radial normal distribution , according to @xmath127 , with @xmath128 , @xmath129 , and @xmath130 , as shown in fig .",
    "[ input_ringdist ] .",
    "the empirical information curves ( fig .",
    "[ infocurve_emp_ringdist ] ) and corrected information curves ( fig .",
    "[ infocurve_corr_ringdist ] ) are computed as the mean of 7 different realizations of the data for different sample sizes . , we start at 20 different , randomly chosen initial conditions to increase the probability of finding the global maximum of the objective functional .",
    "increasing the number of initial conditions would decrease the error bars at the cost of computational time . ] the corrected curves peak at @xmath102 , which is shown as a function of @xmath41 in fig .",
    "[ nccurve_ringdist ] . for less than a few thousand samples ,",
    "the optimal number of clusters goes roughly as @xmath131 , but there is a saturation around @xmath132 .",
    "this number corresponds to half of the number of x - bins ( and therefore half of the number of `` objects '' we are trying to cluster ) which makes sense given the symmetry of the problem .",
    "in the most general case , @xmath8 can be a continuous variable drawn from an unknown distribution @xmath2 .",
    "we then have to estimate the full distribution @xmath29 and if we want to follow the same treatment as above , we have to assume that our estimate approximates the true distribution @xmath133 where @xmath134 is some small perturbation and its average over all possible realizations of the data is zero @xmath135 now , this estimate induces an error not only in @xmath136 , but also in @xmath137 .",
    "taylor expansion of these two terms gives @xmath138 this results in a correction to the objective function ( @xmath139 ) , given by : @xmath140 where @xmath141 is constant in @xmath40 and therefore not important .",
    "at critical temperature @xmath142 , the error due to uncertainty in @xmath2 made in calculating @xmath136 cancels that made in computing @xmath137 . for small @xmath23 , the largest contribution to the error",
    "is given by the first term in the sum of eq .",
    "( [ errorf ] ) , since @xmath143 , @xmath144 .",
    "therefore , the procedure that we have suggested for finding the optimal number of clusters in the deterministic limit ( @xmath24 ) remains unchanged , even if @xmath2 is unknown .",
    "let us consider , as before , the leading order term of the error ( using the approximation in eq .",
    "( [ approx ] ) ) @xmath145 in the @xmath24 limit this term becomes @xmath146 , and we find , @xmath147 which is insignificantly different from eq .",
    "( [ corr_info ] ) in the regime @xmath148 .",
    "+   + only for very large temperatures @xmath149 ( i.e. at the onset of the annealing process ) could the error that results from uncertainty in @xmath2 make a significant difference .",
    "+   + the corrected objective function is now given by @xmath150 and the optimal assignment rule is given by @xmath151 \\nonumber \\\\ & & + \\frac{1}{\\ln(2 ) p(x ) } \\sum_v \\sum_{n=2}^{\\infty } ( -1)^n   \\bigg [ \\frac{1}{n ( p(c))^{n } } \\left ( \\frac{1}{(p(v|c))^n } + t-1 \\right ) \\nonumber \\\\ & & \\times \\left\\langle ( \\delta p(v , c))^{n } \\right\\rangle - \\frac{1}{(n-1 ) ( p(c))^{n-1 } } \\left ( \\frac{1}{(p(v|c))^{n-1 } } + t-1\\right ) \\nonumber \\\\ & & \\times \\left\\langle \\delta p(x , v)(\\delta p(v , c))^{n-1 } \\right\\rangle \\bigg ] \\bigg ) \\bigg ] ,   \\end{aligned}\\ ] ] which has to be solved self consistently together with eq .",
    "( [ markov ] ) and @xmath152    [ [ ratedistortion - theory . ] ] rate  distortion theory .",
    "+ + + + + + + + + + + + + + + + + + + + + + +    let us assume that we estimate the distribution @xmath2 by @xmath153 , with @xmath154 , as before .",
    "while there is no systematic error in the computation of @xmath25 , this uncertainty in @xmath2 does produce a systematic _ under _ estimation of the information cost @xmath26 : @xmath155 when we correct the cost functional for this error ( with @xmath156 ) , @xmath157 we obtain for the optimal assignment rule ( with @xmath158 ) , @xmath159 .",
    "\\end{aligned}\\ ] ] let us consider the leading order term of the error made in calculating the information cost , @xmath160 for counting statistics , we can approximate , as before , @xmath161 the information cost is therefore underestimated by at least @xmath162 bits . }",
    "\\geq 2^{i(c;x)}$ ] . ] the corrected rate  distortion curve with @xmath163 is then bounded from below by @xmath164 and this bound has a rescaled slope given by @xmath165 but no extremum .",
    "since there is no optimal trade  off , it is not possible to use the same arguments as we have used before to determine an optimal number of clusters in the hard clustering limit . to do this",
    ", we have to carry the results obtained from the treatment of the finite sample size effects in the ib over to rate  distortion theory .",
    "this is possible , with insights we have gained in still , bialek and bottou ( 2004 ) about how to use the ib for data that are given with some measure of distance ( or distortion ) .",
    "clustering , as a form of lossy data compression , is a trade  off between the quality and complexity of representations . in general , a data set ( or clustering problem ) is characterized by the whole structure of this trade  off  the rate ",
    "distortion curve or the information curve in the ib method  which quantifies our intuition that some data are more clusterable than others . in this sense",
    "there is never a single `` best '' clustering of the data , just a family of solutions evolving as a function of temperature .",
    "+   + as we solve the clustering problem at lower temperatures , we find solutions that reveal more and more detailed structure and hence have more distinct clusters . if we have only finite data sets , however , we expect that there is an end to the _ meaningful _ structure that can be resolved  at some point separating clusters into smaller groups just corresponds to fitting the sampling noise .",
    "the traditional approach to this issue is to solve the clustering problem in full , and then to test for significance or validity of the results by some additional statistical criteria .",
    "what we have presented in this work is , we believe , a new approach : because clustering is formulated as an optimization problem , we can try to take account of the sampling errors and biases directly in the objective functional . in particular , for the information bottleneck method all terms in the objective functional are mutual informations , and there is a large literature on the systematic biases in information estimation .",
    "there is a perturbative regime in which these biases have a universal form and can be corrected . applying these corrections , we find that at fixed sample size the trade  off between complexity and quality really does have an endpoint beyond which lowering the temperature or increasing the number of clusters does not resolve more relevant information .",
    "we have seen numerically that in model problems this strategy is sufficient to set the maximum number of resolvable clusters at the correct value .",
    "we thank n. tishby for helpful comments on an earlier draft .",
    "s. still wishes to thank m. berciu and l. bottou for helpful discussions and acknowledges support from the german research foundation ( dfg ) , grant no .",
    "sti197 .      * v. balasubramanian , statistical inference , occam s razor , and statistical mechanics on the space of probability distributions . neural comp . 9 ( 1997 ) 349 . * m. blatt , s. wiseman and e. domany , superparamagnetic clustering of data .",
    "76 ( 1996 ) 3251 - 3254 , cond - mat/9702072 * h .- h .",
    "bock , probability models and hypotheses testing in partitioning cluster analysis .",
    "in _ clustering and classification _ eds . : p. arabie , l.j .",
    "hubert and g. de soete ( 1996 ) world scientific , pp .",
    "378 - 453 .",
    "* j. m. buhmann and m. held , model selection in clustering by uniform convergence bounds . in _ adv .",
    "neural inf .",
    "( nips ) 12 _ eds . :",
    "s. a. solla , t. k. leen and k .- r .",
    "mller ( 2000 ) mit press , cambridge , ma * m. eisen , p. t. spellman , p. o. brown and d. botstein , cluster analysis and display of genome - wide expression patterns .",
    "( usa ) 95 ( 1998 ) 14863 . * c. fraley and a. raftery , model - based clustering , discriminant analysis , and density estimation . j. am .",
    "97 ( 2002 ) 611 .",
    "* a. d. gordon , _ classification _ , ( 1999 ) chapmann and hall / crc press , london * p. hall and e. j. hannan , on stochastic complexity and nonparametric density estimation .",
    "biometrika 75 ( 1988 ) no .",
    "4 , pp.705 - 714 .",
    "* d. horn and a. gottlieb , algorithm for data clustering in pattern recognition problems based on quantum mechanics .",
    "88 ( 2002 ) 018702 , extended version : physics/0107063 * s. lloyd , least squares quantization in pcm . technical report ( 1957 ) bell laboratories .",
    "also in : ieee trans .",
    "it-28 ( 1982 ) 129 . * j. macqueen , some methods for classification and analysis of multivariate observations . in _ proc .",
    "5th berkeley symp .",
    "math . statistics and",
    "probability _ eds .",
    ": l.m.l cam and j. neyman ( 1967 ) university of california press , pp . 281 - 297 ( vol .",
    "i ) * k. rose , e. gurewitz and g. c. fox , statistical mechanics and phase transitions in clustering .",
    "( 1990 ) 945 . *",
    "v. roth , t. lange , m. braun and j. m. buhmann , a resampling approach to cluster validation . in _ proceedings in computational statistics : 15th symposium , berlin , germany 2002 ( compstat2002 ) _ , eds .",
    ": w. hrdle , bernd rnz ( 2002 ) physica - verlag , heidelberg , pp.123 - 128 . * c. e. shannon , a mathematical theory of communication .",
    "bell system tech .",
    "j. 27 , ( 1948 ) .",
    "379423 , 623656 .",
    "see also : c. shannon and w. weaver , _ the mathematical theory of communication _ ( 1963 ) university of illinois press * p. smyth , model selection for probabilistic clustering using cross - validated likelihood .",
    "statistics and computing 10 ( 2000 ) 63 . *",
    "s. still , w. bialek and l. bottou , geometric clustering using the information bottleneck method .",
    "_ advances in neural information processing systems 16 _ eds . :",
    "s. thrun , l. saul , and b. schlkopf ( 2004 ) mit press , cambridge , ma . * m. stone , cross - validatory choice and assessment of statistical predictions .",
    "j. r. stat .",
    "( 1974 ) 111 .",
    "* r. tibshirani , g. walther and t. hastie , estimating the number of clusters in a dataset via the gap statistic .",
    "j. r. stat .",
    "* b * 63 ( 2001 ) 411 .",
    "* n. tishby , f. pereira and w. bialek , the information bottleneck method . in _ proc .",
    "37th annual allerton conf .",
    "_ eds . : b. hajek and r. s. sreenivas ( 1999 ) university of illinois , physics/0004057 * a. treves and s. panzeri , the upward bias in measures of information derived from limited data samples . neural comp .",
    "7 ( 1995 ) 399 .",
    "* j. h. ward , hierarchical groupings to optimize an objective function . j.",
    "58 ( 1963 ) 236 ."
  ],
  "abstract_text": [
    "<S> clustering provides a common means of identifying structure in complex data , and there is renewed interest in clustering as a tool for the analysis of large data sets in many fields . a natural question is how many clusters are appropriate for the description of a given system . </S>",
    "<S> traditional approaches to this problem are based either on a framework in which clusters of a particular shape are assumed as a model of the system or on a two - step procedure in which a clustering criterion determines the optimal assignments for a given number of clusters and a separate criterion measures the goodness of the classification to determine the number of clusters . in a statistical mechanics </S>",
    "<S> approach , clustering can be seen as a trade  off between energy and entropy  like terms , with lower temperature driving the proliferation of clusters to provide a more detailed description of the data . for finite data sets </S>",
    "<S> , we expect that there is a limit to the meaningful structure that can be resolved and therefore a minimum temperature beyond which we will capture sampling noise . </S>",
    "<S> this suggests that correcting the clustering criterion for the bias which arises due to sampling errors will allow us to find a clustering solution at a temperature which is optimal in the sense that we capture maximal meaningful structure  without having to define an external criterion for the goodness or stability of the clustering . </S>",
    "<S> we show that , in a general information theoretic framework , the finite size of a data set determines an optimal temperature , and we introduce a method for finding the maximal number of clusters which can be resolved from the data in the hard clustering limit . </S>"
  ]
}