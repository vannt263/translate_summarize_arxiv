{
  "article_text": [
    "recommendation services , event detection , clustering and categorization of video data , and retrieval algorithms for large video databases depend on efficient and reliable similarity identification amongst video segments @xcite . in a nutshell , given a query video , we wish to find all similar video segments within a large video database in the most reliable and efficient way .",
    "the state - of - the - art in similarity identification hinges on video fingerprinting algorithms @xcite .",
    "the aim of such algorithms is to provide for distinguishable representations that remain robust under visual distortions , such as , rotation , compression , blur , resizing , flicker , etc .",
    "such distortions are expected to be present within large video collections , or when dealing with content _ in the wild _",
    "@xcite .    in a broad sense ,",
    "video similarity identification can be seen as a spatio - temporal matching problem via an appropriate feature space or descriptor .",
    "recent results have shown that similarity identification algorithms based on local descriptors , such as the scale invariant feature transform ( sift ) @xcite or dense sift @xcite , tend to significantly outperform previous approaches based on histogram methods @xcite or fingerprinting algorithms @xcite , especially under the presence of distortions in the video data .",
    "therefore , the state - of - the - art in this area is based on vectors of locally aggregated descriptors ( vlad ) @xcite , or bag - of - words ( bow ) methods @xcite , which merge feature descriptors in video frame regions .",
    "more recently , hyper - pooling approaches have been proposed @xcite , which perform two consecutive vlad stages in order to compact entire video sequences into a unique aggregated descriptor vector .    in this paper , we focus on vlad - based algorithms and examine the problem of creating compact representations that are suitable for efficient and accurate similarity identification of segments of videos within a large video collection .",
    "the paper makes the following contributions :    * instead of creating holistic hyper - pooling approaches for entire video sequences , we concentrate on groups of frames ( gofs ) within a video sequence in order to allow for video segment search . * instead of directly compacting feature descriptors",
    ", we follow a two - stage clustering approach : we first cluster features to obtain local - feature - centers ( lfcs ) and then encode the latter with respect to a given set of centers of local - feature - centers ( clfcs ) , computed from a training set . *",
    "similar to vlad , we encode the lfcs by aggregating their differences with respect to their corresponding clfcs , thereby creating _ vectors of locally aggregated centers _",
    "* experiments using a 100-minute training set and a 1000-minute test set from the open video project reveal that , for the same compaction factor , our proposal is outperforming the state - of - the - art vlad method @xcite by more than 15% in terms of mean average precision ( map ) .",
    "the remainder of the paper is as follows .",
    "section [ sec : background ] summarizes the operation of vlad and hyper - pooling that constitute the state - of - the - art and form the basis of the proposed compaction algorithm .",
    "section [ sec : vlac ] presents the proposed vlac approach .",
    "section [ sec : evaluation ] presents experimental results , while section [ sec : conclusion ] draws concluding remarks .",
    "current solutions make use of image descriptors to represent individual frames within a video @xcite . after extracting the local feature descriptors of a given set of frames using an algorithm such as sift @xcite or dense sift @xcite , these descriptors",
    "are then accumulated to produce a compact frame representation .",
    "recent work advocated the use of pooling strategies instead of simple averaging methods , in order to minimize information loss . a common way to achieve",
    "this is by using bow methods @xcite or vlad @xcite . in this paper , we focus on the latter as it has been shown to achieve state - of - the - art results in terms of map in medium and large - scale sets of image and video content .",
    "vlad @xcite is a vector aggregation algorithm that produces a fixed - size compact description of a set comprising a variable number of data points .",
    "vlad was proposed as a novel approach aimed to optimize : _",
    "( i ) _ the representation of aggregated feature descriptors ; _ ( ii ) _ the dimensionality reduction ; _ ( iii ) _ the indexing of the output vectors .",
    "these aspects are interrelated  for example , dimensionality reduction directly affects the way we index the output vectors .",
    "while high dimensional vectors produce more accurate search results , low dimensional vectors are easier to index and require less operations and storage .",
    "consider a set of @xmath0 video frames to be used for training purposes .",
    "for the @xmath1th training frame , @xmath2 , a visual feature detector and descriptor ( e.g. , the sift detector and descriptor @xcite ) is calculated , thereby producing @xmath3 feature vectors @xmath4 , @xmath5 , each with dimension @xmath6 .",
    "the ensemble of these features comprises the @xmath1th training frame s set of visual features @xmath7 .",
    "the concatenation of all these sets for all @xmath0 training frames , given by @xmath8 , undergoes a clustering approach , such as k - means @xcite , thereby grouping all vectors in @xmath9 into @xmath10 clusters , with centers denoted by set @xmath11 .",
    "vlad then encodes the set of visual features , @xmath12 , of the @xmath1th frame as the group of @xmath13-dimensional vectors @xmath14 ( @xmath15 ) given by @xmath16 where @xmath17 is the quantization function that determines which cluster @xmath4 belongs to .",
    "then , the vlad of the @xmath1th frame is given by the vector of aggregated local differences @xmath18 , with dimension @xmath19 .",
    "all these vectors are concatenated into the @xmath20-dimensional matrix @xmath21 , which comprises the vlad encoding of the training set . in order to allow for further dimensionality reduction ( thereby accelerating the matching process ) , principal component analysis ( pca )",
    "is applied to @xmath22 , and the @xmath23 most dominant eigenvectors are maintained in the @xmath24 matrix @xmath25 in order to be used in the test set .",
    "when considering a test video frame , once its set of visual features @xmath26 is produced by the sift descriptor ( assuming @xmath27 points were detected ) , vlad performs the following step : _ ( i ) _ calculation of @xmath28 ( @xmath15 ) via with the precalculated center set @xmath29 ; _ ( ii ) _ aggregation of these into a @xmath30 composite vector and application of dimensionality reduction via the retained pca coefficients in @xmath25 : @xmath31 where @xmath32 denotes the @xmath33 vlad of the test video frame after compaction with pca .",
    "the similarity between two vlad vectors of two test video frames @xmath34 and @xmath35 is simply measured via @xmath36 . thresholding the set of similarity ( i.e. , inner product )  results between a test video frame and the entire test set of video frames provides the list of similar frames retrieved under the selected threshold value .      a recent method proposed by douze _",
    "et al . _",
    "@xcite makes use of hyper - pooling ( hp ) strategies on the video description level .",
    "hyper - pooling works by using a second layer of data clustering and encoding a set of frame vlad descriptors into a single vector .",
    "hyper - pooling utilizes an enhanced hashing scheme by exploiting the temporal variance properties of vlad vectors @xcite that have been produced per frame . after performing pca ,",
    "the temporal variance of vlad vectors is most prominent in the components associated with low eigenvalues .",
    "hence , hyper - pooling postulates that we can get a more stable set of centers by applying a clustering algorithm ( such as k - means ) on the set of components relating to the highest eigenvalues .",
    "indeed , hashing the components that vary less with time has been shown to provide better results in terms of stability and robustness to noise @xcite .      from the previous description ,",
    "it is evident that the crucial aspects of vlad and hyper - pooling are the clustering and the pca process performed on the training set . ideally , for a given set of video frames",
    ", we would like to produce principal component vectors for compaction of vlads that do not change substantially when the video frames undergo real - world visual distortions .",
    "for example , consider two ensembles of training video frame sets , @xmath37 and @xmath38 , with the latter produced by distorting the video frames in @xmath37 via blurring , compression artifacts , rotation , gamma changes , etc . during the training stage , applying pca on the vectors of local differences ( obtained per frame ) will produce @xmath23 dominant eigenvectors forming the @xmath24 matrices @xmath39 and @xmath40 . in case of hyper - pooling the aforementioned matrices will have a dimension of @xmath41 , where @xmath42 is the number of dimensions retained after the first vlad stage .",
    "ideally , the vectors in @xmath39 and @xmath40 should be reasonably - well aligned , which is an indication that the compaction process is robust to noise .",
    "this can be tested by computing the sum - of - inner - products between the @xmath23 dominant eigenvectors of both cases : for both vlad and hyper - pooling , we obtain @xmath43p_{\\text{train , noisy}}\\left[i , j\\right]\\end{aligned}\\ ] ] where @xmath44 $ ] denotes the @xmath45 element of @xmath46 .",
    "we carried out such an indicative test in a set of @xmath47 video frames taken from 10 video clips of 10-minute duration each .",
    "each video underwent seven different visual distortions , as tabulated in table [ tab : distortions ] and detailed in section [ sec : evaluation ] . using @xmath48 clusters for vlad and @xmath49 for dense sift , we obtain @xmath50 and @xmath51 . however , utilizing the sift vectors directly , performing pca decomposition to produce the two @xmath52 matrices @xmath53 and @xmath54 , and computing @xmath55p_{\\text{sift , train , noisy}}\\left[i , j\\right]\\ ] ] we get @xmath56 .",
    "the significant difference between @xmath57 and @xmath58 and @xmath59 represents the reduction in tolerance to distortions incurred when the vectors are projected to their principal components , which is performed in order to gain the benefit of compaction .    in this paper ,",
    "our aim is to design a method leading to the same compaction factor as vlad , albeit having increased tolerance to distortion in the video frames , which will allow for high recall rates even when dealing with distorted versions of the input video content .",
    "a secondary aim is to design our approach in a way that directly deals with video segments rather that individual video frames , thus allowing for video segment similarity detection .",
    "these two aspects are elaborated in the next section .",
    "the similarity between two videos can be estimated by obtaining the vlad inner products per frame and averaging .",
    "we consider this approach as the baseline for video similarity detection .",
    "this direct application of vlad to video achieves good results in terms of retrieval accuracy , albeit at the expense of high complexity and storage requirements , even when the video is sampled at a substantially lower frame - rate .",
    "all the solutions proposed are designed to approach the performance of this baseline   as much as possible while requiring a fraction of its computational complexity and storage , or , alternatively , significantly - exceed the vlad  performance while incurring the same complexity and storage .",
    "video description algorithms such as hyper - pooling @xcite were designed for holistic video description , namely , the derived vector describes the entire video information _ as a whole_. temporal coherency is lost when using such holistic description methods , thereby making the detection of video segments within longer videos impossible .",
    "this problem can be solved by modifying holistic solutions to work on _ groups of frames _ ( gofs ) within each given video .",
    "gofs can be viewed as fixed - size temporal windows , each of which is then compacted into a single vlad , hyper - pooling or vlac descriptor ( referred to as vlad - gof , hp - gof and vlac - gof , respectively ) .",
    "a video segment can then be matched by finding maximum inner product between its vlad - gof , hp - gof , or vlac - gof descriptor and the corresponding descriptor from the a gof in the video .",
    "evidently , the length of the gof controls the accuracy of the detection of video segments within longer videos .",
    "in addition , gofs can also be overlapping to allow for better temporal resolution within the matching process .",
    "instead of clustering the local descriptors found within each gof , we propose to cluster the centers of clusters of local descriptors .",
    "the aim is to produce results that are increasingly robust to distortions that may be found in a typical large video database .",
    "encoding centers is expected to be more robust to such visual distortions since , compared to local feature descriptors , the centers of local feature descriptors will vary less when artifacts from processing are incurred on video frames .",
    "consider @xmath60 training gofs stemming from a set of training videos . from the frames of each @xmath61th gof ( @xmath62 $ ] ) ,",
    "we extract a set of @xmath63 dense sift feature vectors @xmath64 , each having @xmath13 dimensions . from each @xmath65 , we calculate @xmath66 local feature centers ( lfcs ) @xmath67 . by concatenating the lfcs for each @xmath61 , we acquire the training set of lfcs @xmath68 .",
    "we then apply a second stage of clustering on @xmath69 to generate a set of @xmath70 centers of lfcs ( clfcs ) @xmath71 , where each clfc has @xmath13 dimensions .",
    "we now consider a test video query @xmath72 that contains @xmath73 gofs .",
    "for every @xmath74 $ ] , we extract @xmath75 local features to obtain @xmath76 .",
    "then , for every @xmath77 , we obtain a set of @xmath66 local feature centers @xmath78 . using vlad",
    "we encode each set of centers @xmath79 with the set of trained centers @xmath80 to generate a vector of locally aggregated centers ( vlac ) .",
    "particularly , we first obtain the @xmath13-dimensional vector @xmath81 for each center @xmath82 in @xmath80 by applying @xmath83 the vlac for @xmath84 is then obtained by concatenating @xmath81 for all @xmath85 $ ] into a single @xmath86-dimensional vector @xmath87 .",
    "we observe that @xmath66 does not affect the dimension of vlac , but serves as a control variable for the _ coarseness _ of the description . after calculating @xmath88 for all",
    "@xmath89 $ ] , we project them on a trained set of @xmath23 principal eigenvectors to perform dimensionality reduction .",
    "we then concatenate these vectors to generate a compact @xmath90-dimensional vector @xmath91 for video @xmath72 .",
    "the similarity between two videos @xmath92 and @xmath93 is given by calculating @xmath94 .",
    "a threshold is then applied on @xmath95 to determine whether the videos are similar .",
    "if two videos contain a different number of gofs ( e.g. , @xmath96 and @xmath97 gofs with @xmath98 ) , @xmath95 is calculated for all possible alignments @xmath99 of the vectors @xmath100 and @xmath101 . finally , the maximum over @xmath99 is taken to be the similarity score .",
    "this can be expressed as @xmath102    examining the performance of vlac  under the experiment of section [ sec : motivation ] , we obtain @xmath103 , which is more than 13 times higher than @xmath104 .",
    "we therefore expect the proposed method to be significantly more robust than vlad and hyper - pooling when assessing video similarity under noisy conditions .",
    "however , in order to be suitable for video retrieval , it must also be _ discriminative _ , i.e. , be able to differentiate between _ dissimilar _ videos that would inherently lead to different features .",
    "this is assessed experimentally in the following section .",
    "we selected 100 random videos from the open video project ( ovp ) , comprising 1000 minutes of video .",
    "seven types of distortions ( table [ tab : distortions ] ) were applied to this footage to examine the performance of vlad , hyper - pooling ( hp ) and vlac under noise .",
    "training for vlad , vlac and hp  centers was done on different ovp videos from the utilized test material .    to generate the queries ,",
    "one - minute video segments were extracted from each original videos .",
    "then , the dataset and query videos were sampled at a rate of @xmath105 frames - per - second ( fps ) .",
    "the sampling of the query videos , however , is shifted by 0.25 seconds with respect to the sampling of the videos in the dataset . in this way",
    ", sampling misalignments were also taken into account .",
    "first , we evaluate the similarity detection of the proposed vlac versus the state - of - the - art vlad when both are extracted from each sampled frame in the sequence ( that is , @xmath106 ) . for vlad",
    ", we set @xmath48 , while for vlac we use @xmath107 and @xmath108 .",
    "this provides an upper bound on the detection accuracy and assesses the performance of the proposed method versus the standard per - frame vlad .",
    "next , the proposed vlac - gof is compared against vlad - gof  and hp - gof , where one descriptor per gof of 5 frames is derived and the overlap is set to one frame . concerning the parameters for each method , we use @xmath48 for vlad - gof , @xmath109 and @xmath108 for vlac - gof . for hp - gof ,",
    "the number of centers used to encode the first stage vlad is @xmath110 and for the second stage @xmath111 , where we keep @xmath112 dimensions from the first stage vlad .    [ cols=\"^,^\",options=\"header \" , ]",
    "we proposed a novel compact video representation method based on aggregating local feature centers .",
    "our results show that encoding local feature centers yields significantly better results than simply encoding the features , which are less tolerant to visual distortions commonly found in video databases .",
    "the proposed approach is therefore suitable for video similarity detection with robustness to visual distortions .",
    "the recall - precision results were improved without incurring extra complexity in the signature matching process .",
    "future work will assess the performance of the proposed approach under uncontrolled distortion conditions and even larger datasets .",
    "j.  revaud , m.  douze , c.  schmid , and h.  jgou , `` event retrieval in large video collections with circulant temporal encoding , '' in _ ieee int .",
    "conf . on computer vision and pattern recognition ( cvpr ) _ , 2013 , pp .",
    "24592466 .",
    "j.  yang , y .- g .",
    "jiang , a.  g. hauptmann , and c .- w .",
    "ngo , `` evaluating bag - of - visual - words representations in scene classification , '' in _ int .",
    "workshop on multimedia information retrieval_. acm , 2007 , pp .",
    "197206 .",
    "m.  r naphade , m.  m yeung , and b .-",
    "yeo , `` novel scheme for fast and efficent video sequence matching using compact signatures , '' in _ electronic imaging_. international society for optics and photonics , 1999 , pp .",
    "564572 .",
    "j.  lu , `` video fingerprinting for copy identification : from research to industry applications , '' in _ is&t / spie electronic imaging_. international society for optics and photonics , 2009 , pp . 725402725402 ."
  ],
  "abstract_text": [
    "<S> we propose a novel vector aggregation technique for compact video representation , with application in accurate similarity detection within large video datasets . </S>",
    "<S> the current state - of - the - art in visual search is formed by the vector of locally aggregated descriptors ( vlad ) of jegou _ et al . </S>",
    "<S> _ vlad generates compact video representations based on scale - invariant feature transform ( sift ) vectors ( extracted per frame ) and local feature centers computed over a training set . with the aim to increase robustness to visual distortions , </S>",
    "<S> we propose a new approach that operates at a coarser level in the feature representation . </S>",
    "<S> we create _ vectors of locally aggregated centers _ ( vlac ) by first clustering sift features to obtain _ local feature centers _ ( lfcs ) and then encoding the latter with respect to given centers of local feature centers ( clfcs ) , extracted from a training set . the sum - of - differences between the lfcs and the clfcs </S>",
    "<S> are aggregated to generate an extremely - compact video description used for accurate video segment similarity detection . </S>",
    "<S> experimentation using a video dataset , comprising more than 1000 minutes of content from the open video project , shows that vlac obtains substantial gains in terms of mean average precision ( map ) against vlad and the hyper - pooling method of douze _ et al . </S>",
    "<S> _ , under the same compaction factor and the same set of distortions .    </S>",
    "<S> video similarity , vector of locally aggregated descriptors , scale - invariant feature transform </S>"
  ]
}