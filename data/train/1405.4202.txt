{
  "article_text": [
    "parametric uncertainty is among the most challenging problems in control system design due to its np - hardness .",
    "albeit , being able to provide solutions to this fundamental problem is a must for any practical design tool worthy of this attribute .",
    "not surprisingly , therefore , parametric uncertainty has remained high up on the agenda of unsolved problems in control for the past three decades .",
    "it is of avail to distinguish between analysis and synthesis techniques for parametric robustness .",
    "analysis refers to assessing robustness of a closed - loop system when the controller is already given .",
    "if the question whether this given controller renders the closed loop parametrically robustly stable is solved exhaustively , then it is already an np - hard problem @xcite .",
    "parametric robust synthesis , that is , computing a controller which is robust against uncertain parameters , is even harder , because it essentially involves an iterative procedure where at every step an analysis problem is solved .",
    "roughly , we could say that in parametric robust synthesis we have to optimize a criterion , a single evaluation of which is already np - hard .    for the analysis of parametric robustness ,",
    "theoretical and practical tools with only mild conservatism and acceptable cpus have been proposed over the years @xcite .",
    "in contrast , no tools with comparable merits in terms of quality and cpu are currently available for synthesis .",
    "it is fair to say that the parametric robust synthesis problem has remained open .",
    "the best currently available techniques for synthesis are the @xmath2 tools going back to @xcite , made available to designers through the matlab robust control toolbox .",
    "these rely on upper bound relaxations of @xmath2 and follow a heuristic which alternates between analysis and synthesis steps .",
    "when it works , it gives performance and stability certificates , but the approach may turn out conservative , and the computed controllers are often too complicated for practice .",
    "the principal obstruction to efficient robust synthesis is the inherent nonconvexity and nonsmoothness of the mathematical program underlying the design .",
    "these obstacles have to some extent been overcome by the invention of the nonsmooth optimization techniques for control @xcite , which we have applied successfully during recent years to multi - model structured control design @xcite .",
    "these have become available to designers through synthesis tools like hinfstruct or systune . here",
    "we initiate a new line of investigation , which addresses the substantially harder parametric robust synthesis problem .    in order to understand our approach",
    ", it is helpful to distinguish between inner and outer approximations of the robust control problem on a set @xmath3 of uncertain parameters .",
    "outer approximations relax the problem over @xmath3 by choosing a larger , but more convenient , set @xmath4 , the idea being that the problem on @xmath5 becomes accessible to computations .",
    "if solved successfully on @xmath5 , this provides performance and robustness certificates for @xmath6 .",
    "typical tools in this class are the upper bound approximation @xmath7 of the structured singular value @xmath2 developed in @xcite , the dk - iteration function dksyn of @xcite , or lmi - based approaches like @xcite .",
    "the principal drawback of outer approximations is the inherent conservatism , which increases significantly with the number of uncertainties and their repetitions , and the fact that failures occur more often .",
    "inner approximations are preferred in practice and relax the problem by solving it on a smaller typically finite subset @xmath8 .",
    "this avoids conservatism and leads to acceptable cpus , but has the disadvantage that no immediate stability or performance certificate for @xmath3 is obtained .",
    "our principal contribution here is to show a way how this shortcoming can be avoided or reduced .",
    "we present an efficient technique to compute an inner approximation with structured controllers with a local optimality certificate in such a way that robust stability and performance are achieved over @xmath3 in the majority of cases .",
    "we then also show how this can be certified a posteriori over @xmath3 , when combined with outer approximation for analysis .",
    "the new method we propose is termed _ dynamic inner approximation _ , as it generates the inner approximating set @xmath9 dynamically .",
    "the idea of using inner approximations , and thus multiple models , to solve robust synthesis problems is not new and was employed in different contexts , see e.g. @xcite .    to address the parametric robust synthesis problem we use a nonsmooth optimization method tailored to minimizing a cost function , which is itself a semi - infinite minimum of smooth functions",
    "this is in contrast with previously discussed nonsmooth optimization problems , where a semi - infinite maximum of smooth functions is minimized , and which have been dealt with successfully in @xcite . at the core of",
    "our new approach is therefore understanding the principled difference between a min - max and a min - min problem , and the algorithmic strategies required to solve them successfully . along with the new synthesis approach ,",
    "our key contributions are    * an in - depth and rigorous analysis of worst - case stability and worst - case performance problems over a compact parameter range . * the description of a new resolution algorithm for worst - case programs along with a proof of convergence in the general nonsmooth case .",
    "note that convergence to local minima from an arbitrary , even remote , starting point is proved , as convergence to global minima is not algorithmically feasible due to the np - hardness of the problems .",
    "the paper is organized as follows .",
    "section [ sec : robust ] states the problem formally , and subsection [ sec : dynamic ] presents our novel dynamic inner approximation technique and the elements needed to carry it out .",
    "section [ sec : highlight ] highlights the principal differences between nonsmooth min - min and min - max problems .",
    "sections [ sec : hinf ] and [ sec : alpha ] examine the criteria which arise in the optimization programs , the @xmath0-norm , and the spectral abscissa .",
    "section [ sec : algorithm ] presents the optimization method we designed for min - min problems and the subsections [ conv : hinf ] , [ conv : alpha ] are dedicated to convergence analysis .",
    "section [ sec : testing ] presents an assessment and a comparison of our algorithm on a bench of test examples .",
    "section [ sec : missile ] gives a more refined study of a challenging missile control problem .",
    "for complex matrices @xmath10 denotes conjugate transpose . for hermitian matrices ,",
    "@xmath11 means positive definite , @xmath12 positive semi - definite .",
    "we use concepts from nonsmooth analysis covered by @xcite . for a locally lipschitz function @xmath13",
    ", @xmath14 denotes its ( compact and convex ) clarke subdifferential at @xmath15 .",
    "the clarke directional derivative at @xmath16 in direction @xmath17 can be computed as @xmath18    the symbols @xmath19 , @xmath20 denote lower and upper linear fractional transformations ( lft ) @xcite . for partitioned @xmath21 block matrices , @xmath22 stands for the redheffer star product @xcite .",
    "we consider an lft plant in fig .",
    "[ fig - lft ] with real parametric uncertainties @xmath23 where @xmath24 and @xmath25 is the state , @xmath26 the control , @xmath27 the vector of exogenous inputs , @xmath28 the output , and @xmath29 the regulated output .",
    "the uncertainty channel is defined as @xmath30 where the uncertain matrix @xmath31 is without loss assumed to have the block - diagonal form @xmath32\\ ] ] with @xmath33 representing real uncertain parameters , and @xmath34 giving the number of repetitions of @xmath35 .",
    "we assume without loss that @xmath36 represents the nominal parameter value .",
    "moreover , we consider @xmath37 in one - to - one correspondence with the matrix @xmath31 in ( [ matrix ] ) .    given a compact convex set @xmath38 containing @xmath39 ,",
    "the parametric robust structured @xmath0 control problem consists in computing a structured output - feedback controller @xmath40 with the following properties :    1 .   * robust stability*. @xmath41 stabilizes @xmath42 internally for every @xmath43 .",
    "2 .   * robust performance*. given any other robustly stabilizing controller @xmath44 with the same structure , the optimal controller satisfies @xmath45    here @xmath46 denotes the closed - loop transfer function of the performance channel @xmath47 of ( [ plant ] ) when the control loop with controller @xmath44 and the uncertainty loop with uncertainty @xmath31 are closed .",
    "we recall that according to @xcite a controller @xmath48 in state - space form is called _ structured _ if @xmath49 depend smoothly on a design parameter @xmath50 varying in a design space @xmath51 or in some constrained subset of @xmath51 .",
    "typical examples of structure include pids , reduced - order controllers , observer - based controllers , or complex control architectures combining controller blocks such as set - point filters , feedforward , washout or notch filters , and much else @xcite .",
    "in contrast , full - order controllers are state - space representations with the same order as @xmath52 without particular structure , and are sometimes referred to as _ unstructured _ , or as _ black - box controllers_.    parametric robust control is among the most challenging problems in linear feedback control . the structured singular value @xmath2 developed in @xcite is the principled theoretical tool to describe problem ( i ) , ( ii ) formally . in the same vein , based on the spectral abscissa @xmath53 of a square matrix @xmath54 , criterion ( i ) may be written as @xmath55 where @xmath56 is the a - matrix of the closed - loop transfer function @xmath57 .    if the uncertain parameter set is a cube @xmath58^m$ ] , which is general enough for applications , then the same information is obtained from the distance to instability in the maximum - norm @xmath59 because criterion ( i ) is now equivalent to @xmath60 .",
    "it is known that the computation of any of these elements , @xmath2 , ( [ rob_alpha ] ) , or ( [ distance ] ) is np - complete , so that their practical use is limited to analysis of small problems , or to the synthesis of tiny ones .",
    "practical approaches have to rely on intelligent relaxations , or _ heuristics _ , which use either inner or outer approximations .    in the next chapters",
    "we will develop our _ dynamic inner approximation _ method to address problem ( i ) , ( ii ) .",
    "we solve the problem on a relatively small set @xmath8 , which we construct iteratively .",
    "the following _ static _ inner approximation to ( i ) , ( ii ) is near at hand . after fixing a sufficiently fine approximating static grid @xmath61 ,",
    "one solves the multi - model @xmath0-problem @xmath62 this may be addressed with recent software tools like hinfstruct and systune , cf .",
    "@xcite , or hifoo @xcite , but has a high computational burden due to the large number of scenarios in @xmath63 , which makes it prone to failure .",
    "straightforward gridding becomes very quickly intractable for sizable dim@xmath64 .",
    "here we advocate a different strategy , which we call _ dynamic _ inner approximation , because it operates on a substantially smaller set @xmath65 generated dynamically , whose elements are called the _ active scenarios _ , which we update a couple of times by applying a search procedure locating problematic parameter scenarios in @xmath3 .",
    "this leads to a rapidly converging procedure , much less prone to failure than ( [ static ] ) .",
    "the method can be summarized as shown in algorithm [ algo1 ] .",
    "initialize the set of active scenarios as @xmath67 .",
    "[ syn ] given the current finite set @xmath8 of active scenarios , compute a structured multi - model @xmath0-controller by solving @xmath68 the solution is the structured @xmath0-controller @xmath41 . try to destabilize the closed - loop system @xmath69 by solving the destabilization problem @xmath70 if @xmath71 , then the solution @xmath72 destabilizes the loop .",
    "include @xmath73 in the active scenarios @xmath9 and go back to step [ syn ] . if no destabilizing @xmath74 was found then go to step [ perform ] . [",
    "perform ] try to degrade the robust @xmath0-performance by solving @xmath75 the solution is @xmath73 . if @xmath76 degradation of performance is only marginal",
    ". then exit , or optionally , go to step [ post - pro ] for post - processing",
    ". otherwise include @xmath73 among the active scenarios @xmath9 and go back to step [ syn ] .",
    "[ post - pro ] check robust stability ( i ) and performance ( ii ) of @xmath41 over @xmath3 by computing the distance @xmath77 to instability ( [ distance ] ) , and its analogue @xmath78 possibly use @xmath2-tools from @xcite to assess @xmath79 approximately .",
    "if all @xmath73 obtained satisfy @xmath80 , then terminate successfully .",
    "the principal elements of algorithm [ algo1 ] will be analyzed in the following sections .",
    "we will focus on the optimization programs @xmath81 in step 4 , @xmath82 in step 3 , and @xmath77 , @xmath83 in step 6 , which represent a relatively unexplored type of nonsmooth programs , with some common features which we shall put into evidence here .",
    "in contrast , program @xmath84 in step 2 is accessible to numerical methods through the work @xcite and can be addressed with tools like hinfstruct or systune available through @xcite , or hifoo available through @xcite .",
    "note that our approach is heuristic in so far as we have relaxed ( i ) and ( ii ) by computing locally optimal solutions , so that a global stability / performance certificate is only provided in the end as a result of step 6 .",
    "introducing the functions @xmath85 , the problem of step 3 can be equivalently written in the form @xmath86 for a matrix @xmath87 depending smoothly on the parameter @xmath88 . here the dependence of the matrix on controller @xmath41 is omitted for simplicity , as the latter is fixed in step 3 of the algorithm .",
    "similarly , if we introduce @xmath89 , with @xmath90 a transfer function depending smoothly on @xmath91 , then problem of step 4 has the abstract form @xmath92 where again controller @xmath41 is fixed in step 4 , and therefore suppressed in the notation .",
    "in contrast , the @xmath0-program @xmath84 in step 2 of algorithm [ algo1 ] has the form @xmath93 which is of the more familiar min - max type .",
    "here we use the well - known fact that the @xmath0-norm may be written as a semi - infinite maximum function @xmath94 } \\overline{\\sigma}\\left ( g(\\kappa , j\\omega ) \\right)$ ] .",
    "the maximum over the finitely many @xmath95 in step 2 complies with this structure and may in principle be condensed into the form ( [ h_plus ] ) , featuring only a single transfer @xmath96 . in practice",
    "this is treated as in @xcite .",
    "due to the minus sign , programs ( [ minus_alpha ] ) and ( [ minus_h ] ) , written in the minimization form , are now of the novel min - min type , which is given special attention here .",
    "this difference is made precise by the following    [ spingarn @xcite , rockafellar - wets @xcite ] a locally lipschitz function @xmath97 is lower-@xmath98 at @xmath99 if there exist a compact space @xmath100 , a neighborhood @xmath101 of @xmath102 , and a mapping @xmath103 such that @xmath104 for all @xmath105 , and @xmath106 and @xmath107 are jointly continuous .",
    "the function @xmath108 is said to be upper-@xmath98 if @xmath109 is lower-@xmath98 .",
    "@xmath110    we expect upper- and lower-@xmath98 functions to behave quite differently in descent algorithms .",
    "minimization of lower-@xmath98 functions , as required in ( [ h_plus ] ) , should lead to a genuinely nonsmooth problem , because iterates of a descent method move toward the points of nonsmoothness .",
    "in contrast , minimization of upper-@xmath98 functions as required in ( [ minus_alpha ] ) and ( [ minus_h ] ) is expected to be better behaved , because iterates move away from the nonsmoothness .",
    "accordingly , we will want to minimize upper-@xmath98 functions in ( [ minus_alpha ] ) and ( [ minus_h ] ) in much the same way as we optimize smooth functions in classical nonlinear programming , whereas the minimization of lower-@xmath98 functions in ( [ h_plus ] ) requires specific techniques like nonconvex bundle methods @xcite .",
    "[ fig - surface ] for an illustration .",
    "note that the computation of the distance to instability @xmath77 defined in ( [ distance ] ) for step @xmath111 of algorithm [ algo1 ] has also the features of a min - min optimization program .",
    "namely , when written in the form @xmath112 with variable @xmath113 , the lagrangian of ( [ distance ] ) is @xmath114 for lagrange multipliers @xmath115 and @xmath116 . in particular ,",
    "if @xmath117 is a karush - kuhn - tucker ( kkt ) point @xcite of ( [ newdist ] ) , then the local minimum @xmath118 we are looking for is also a critical point of the unconstrained program @xmath119 which features the function @xmath120 and is therefore of min - min type .",
    "therefore , in solving ( [ distance ] ) , we expect phenomena of min - min type to surface rather than those of a min - max program .",
    "a similar comment applies to the computation of @xmath83 in step @xmath111 of the algorithm .",
    "yet another aspect of algorithm [ algo1 ] is that in order to be robustly stable over the parameter set @xmath3 , the lfts must be well - posed in the sense that @xmath121 exists for every @xmath122 , where @xmath123 is the closed - loop d - matrix .",
    "questioning well - posedness could therefore be included in step 3 of the algorithm , or added as posterior testing in step 6 .",
    "it can be formulated as yet another min - min program @xmath124 where one would diagnose the solution @xmath73 to represent an ill - posed scenario as soon as it achieves a large negative value .",
    "program ( [ eq - wp ] ) exhibits the same properties as minimizing @xmath125 in section [ sec : hinf ] and is handled with the same techniques .    for programs @xmath81 in step 4 ,",
    "@xmath82 in step 3 , and @xmath77 , @xmath83 in step 6 of algorithm [ algo1 ] , well - posedness ( [ eq - wp ] ) is a prerequisite .",
    "however , we have observed that it may not be necessary to question well - posedness over @xmath3 at every step , since questioning stability over @xmath3 has a similar effect . since the posterior certificate in step 6 of the algorithm covers also well - posedness , this is theoretically justified .",
    "our notation makes it easy for the reader to distinguish between min - min and min - max programs .",
    "namely , minimizations over the controller variable @xmath50 turn out the min - max ones , while minimizations over the uncertain parameters @xmath74 lead to the min - min type .      in this section",
    "we look at the typical difficulties which surface in min - max and min - min programs .",
    "this is crucial for the understanding of our algorithmic approach .",
    "consider first a min - max program of the form @xmath126 where the @xmath127 are smooth .",
    "when the set @xmath128 is finite , we may simply dissolve this into a classical nonlinear programming ( nlp ) using one additional dummy variable @xmath129 : @xmath130 the situation becomes more complicated as soon as the set @xmath128 is infinite , as is for instance the case in program @xmath84 in step 2 of algorithm [ algo1 ] .",
    "the typical difficulty in min - max programs is to deal with this semi - infinite character , and one is beholden to use a tailored solution , as for instance developed in @xcite . altogether",
    "this type of difficulty is well - known and has been thoroughly studied .",
    "in contrast , a min - min program @xmath131 can not be converted into an nlp even when @xmath128 is finite .",
    "the problem has disjunctive character , and if solved to global optimality , min - min programs lead to combinatorial explosion .",
    "on the other hand , a min - min problem has some favorable features when it comes to solely finding a good local minimum .",
    "namely , when meeting a nonsmooth iterate @xmath132 , where several branches @xmath127 are active , we can simply pick one of those branches and continue optimization as if the objective function were smooth . in the subsequent sections we prove that this intuitive understanding is indeed correct .",
    "our experimental section will show that good results are obtained if a good heuristic is used .",
    "the above considerations lead us to introduce the notion of active indices and branches for functions @xmath133 defined by the inner @xmath134 and @xmath135 in ( [ eq - minmax ] ) and ( [ eq - maxmin ] ) .",
    "the set of active indices for @xmath108 at @xmath74 is defined as @xmath136 active branches of @xmath108 at @xmath74 are those corresponding to active indices , i.e , @xmath127 , @xmath137 .",
    "in this section we briefly discuss how the subgradient information needed to minimize @xmath125 and @xmath120 is computed .",
    "we start by investigating the case of the @xmath0-norm @xmath138 .",
    "we recall that function evaluation is based on the hamiltonian algorithm of @xcite and its further developments @xcite .",
    "computation of subgradients of @xmath125 in the sense of clarke can be adapted from @xcite , see also @xcite .",
    "we assume the controller is fixed in this section and investigate the properties of @xmath125 as a function of @xmath74 . to this aim",
    ", the controller loop is closed by substituting the structured controller ( [ controller ] ) in ( [ plant ] ) , and we obtain the transfer function @xmath139 .",
    "substantial simplification in clarke subdifferential computation is then obtained by defining the @xmath140-block transfer function    @xmath141:= \\left[\\begin{array}{cc}0 & i \\\\i & \\delta\\end{array}\\right ] \\star m \\,,\\ ] ]    where the dependence on @xmath50 has now been suppressed , as the controller will be fixed to @xmath142 after step 2 .",
    "it is readily seen that @xmath143 coincides with the closed - loop transfer function where both controller and uncertainty loops are closed .",
    "now consider the function @xmath144 , which is well defined on its domain @xmath145 .",
    "we have the following    [ prop1 ] the function @xmath125 is everywhere clarke subdifferentiable on @xmath146 .",
    "the clarke subdifferential at @xmath147 is the compact and convex set @xmath148 where the @xmath149-th entry of @xmath150 is @xmath151 with @xmath152 , and @xmath153 here @xmath154 is the set of active frequencies at @xmath74 , @xmath155 is a matrix whose columns are the left singular vectors associated with the maximum singular value of @xmath156 , @xmath157 is the corresponding matrix of right singular vectors , and @xmath158 is an hermitian matrix of appropriate size .",
    "computation of the clarke subdifferential of @xmath125 can be obtained from the general rule @xmath159 , and knowledge of @xmath160 , see @xcite .",
    "note that in that reference the clarke subdifferential is with respect to the controller and relies therefore on the redheffer star product @xmath161\\,.\\ ] ] here we apply this in the upper loop in @xmath31 , so we have to use the analogue expression ( [ eq - stard ] ) instead .    in the case where a single frequency @xmath162 is active at @xmath74 and the maximum singular value @xmath163 of @xmath164 has multiplicity 1 , @xmath125 is differentiable at @xmath74 and the gradient is @xmath165 where @xmath166 and @xmath167 are the unique right and left singular vectors of @xmath164 associated with @xmath168 .",
    "let @xmath169 .",
    "then @xmath170 is lower-@xmath98 on @xmath146 , so that @xmath171 is upper-@xmath98 there .",
    "recall that the maximum singular value has the variational representation @xmath172 now observe that @xmath173 , being convex , is lower-@xmath98 as a mapping @xmath174 , so we may write it as @xmath175 for @xmath176 jointly of class @xmath98 and @xmath177 compact",
    ". then @xmath178 where @xmath179 is homeomorphic with the @xmath180-sphere .",
    "this is the desired representation ( [ lower ] ) , where the compact space @xmath100 is obtained as @xmath181 , @xmath106 as @xmath182 and @xmath183 as @xmath184 .      for the spectral abscissa",
    "the situation is more complicated , as @xmath185 is not locally lipschitz everywhere . recall that an eigenvalue @xmath186 of @xmath87 is called active at @xmath74 if @xmath187 .",
    "we use @xmath188 to denote the indices of active eigenvalues .",
    "let us write the lft describing @xmath87 as @xmath189 , where dependence on controller parameters @xmath50 is again omitted and considered absorbed into the state - space data @xmath190 , @xmath191 , etc .",
    "[ prop4 ] suppose all active eigenvalues @xmath186 , @xmath192 of @xmath87 at @xmath74 are semi - simple .",
    "then @xmath193 is clarke subdifferentiable in a neighborhood of @xmath74 .",
    "the clarke subdifferential of @xmath120 at @xmath74 is @xmath194 , where the @xmath149-th entry of @xmath150 is @xmath195 with @xmath196 , and @xmath197 where @xmath198 is a column matrix of right eigenvectors , @xmath199 a row matrix of left eigenvectors of @xmath87 associated with the eigenvalue @xmath186 , and such that @xmath200    this follows from @xcite .",
    "see also @xcite .",
    "a very concise proof that semi - simple eigenvalue functions are locally lipschitz could also be found in @xcite .",
    "when every active eigenvalue is simple , @xmath201 reduces to a scalar @xmath202 and a fast implementation is possible .",
    "we use the lu - decomposition to solve for @xmath203 and @xmath204 in the linear systems @xmath205 given the particular structure ( [ matrix ] ) of @xmath31 , subgradients with respect to the @xmath206th entry are readily obtained as a sum over @xmath192 of inner products of the form @xmath207 , where @xmath208 is a selection of indices associated to the rows / columns of @xmath209 in @xmath210 .",
    "similar inner product forms apply to the computation of @xmath0 norm subgradients .",
    "it was observed in @xcite that @xmath185 may fail to be locally lipschitz at @xmath74 if @xmath87 has a derogatory active eigenvalue .",
    "[ prop5 ] suppose every active eigenvalue of @xmath87 is simple",
    ". then @xmath120 is upper-@xmath98 in a neighborhood of @xmath74 .    if active eigenvalues are simple , then @xmath211 is the maximum of @xmath98 functions in a neighborhood of @xmath74 .",
    "the result follows from @xmath212 .",
    "in this section we present our descent algorithm to solve programs ( [ minus_alpha ] ) and ( [ minus_h ] ) .",
    "we consider an abstract form of the min - min program with @xmath108 a general objective function of this type : @xmath213 where as before @xmath6 is a compact convex set with a convenient structure . as we already pointed out , the crucial point is that we want to stay as close as possible to a standard algorithm for smooth optimization , while assuring convergence under the specific form of upper nonsmoothness in these programs .",
    "@xmath214 , @xmath215 .",
    "put outer loop counter @xmath216 , choose initial guess @xmath217 , and fix memory step size @xmath218 .",
    "[ stopping ] if @xmath132 is a kkt point of ( [ min - min ] ) then exit , otherwise go to inner loop . at current iterate",
    "@xmath132 call the step finding subroutine ( subroutine [ algo - prox ] ) started with last memorized stepsize @xmath219 to find a step @xmath220 and a new serious iterate @xmath221 such that @xmath222 if @xmath223 then update memory stepsize as @xmath224 , otherwise update memory stepsize as @xmath225 . increase counter @xmath226 and go back to step [ stopping ] .    in order to understand algorithm [ algo - outer ] and its step finding subroutine ( subroutine [ algo - prox ] ) , we recall from @xcite that @xmath227 the standard model of @xmath108 at @xmath74 , where @xmath228 is the clarke directional derivative of @xmath108 at @xmath74 in direction @xmath229 @xcite .",
    "this model can be thought of as a substitute for a first - order taylor expansion at @xmath74 and can also be represented as @xmath230 where @xmath231 is the clarke subdifferential of @xmath108 at @xmath74 . in the subroutine",
    "we generate lower approximations @xmath232 of @xmath233 using finite subsets @xmath234 , putting @xmath235 we call @xmath232 the working model at inner loop counter @xmath206 .",
    "current serious iterate @xmath74 , last memorized stepsize @xmath236 . flag .",
    "next serious iterate @xmath237 .",
    "put linesearch counter @xmath238 , and initialize search at @xmath239 .",
    "choose subgradient @xmath240 .",
    "put @xmath241 .",
    "[ tangent_prog ] given @xmath242 , a finite set of clarke subgradients @xmath243 , and the corresponding working model @xmath244 , compute solution @xmath245 of the convex quadratic tangent program @xmath246 compute @xmath247 if @xmath248 then @xmath249 successfully to algorithm [ algo - outer ] .",
    "otherwise go to step [ model_update ] [ model_update ] pick a subgradient @xmath250 such that @xmath251 , or equivalently , @xmath252 .",
    "include @xmath253 into the new set @xmath254 for the next sweep .",
    "add the aggregate subgradient @xmath255 into the set @xmath254 to limit its size .",
    "[ manage ] compute the test quotient @xmath256 if @xmath257 then select @xmath258 $ ] , else keep @xmath259 . increase",
    "counter @xmath206 and go back to step [ tangent_prog ] .",
    "typical values are @xmath260 , @xmath261 , and @xmath262 . for backtracking we use @xmath263 and @xmath264 .",
    "the subroutine of the descent algorithm [ algo - outer ] looks complicated , but as we now argue , it reduces to a standard backtracking linesearch in the majority of cases . to begin with , if @xmath108 is certified upper-@xmath98 , then we completely dispense with step 4 and keep @xmath265 , which by force reduces the subroutine to a linesearch along a projected gradient direction .",
    "this is what we indicate by flag = upper in step 4 of the subroutine .",
    "if @xmath108 is only known to have a strict standard model @xmath233 in ( [ eq - clarkemodel ] ) , without being certified upper-@xmath98 , which corresponds to flag = strict , then step 4 of the subroutine is needed , as we shall see in section [ conv : alpha ] . however , even then we expect the subroutine to reduce to a standard linesearch .",
    "this is clearly the case when the clarke subdifferential @xmath266 at the current iterate @xmath74 is singleton , because @xmath267 is then again independent of @xmath206 , so @xmath248 reads @xmath268 which is the usual armijo test @xcite .",
    "moreover , @xmath269 is then a step along the projected gradient @xmath270 , which is easy to compute due to the simple structure of @xmath3 .",
    "more precisely , for @xmath271^m$ ] and stepsize @xmath220 , the solution @xmath272 of tangent program @xmath273 in step 2 can be computed coordinatewise as @xmath274 where @xmath275 .",
    "cutting plane and aggregate plane in step 4 become redundant , and the quotient @xmath276 in step 5 is also redundant as it is always equal to @xmath180 .",
    "step 4 is only fully executed if @xmath108 is _ not _ certified upper-@xmath98 and the subgradient @xmath240 in step 1 of subroutine [ algo - prox ] does _ not _ satisfy @xmath277 . in that event",
    "step 4 requires computation of a new subgradient @xmath278 which _ does _ satisfy @xmath279 . from here on the procedure changes .",
    "the sets @xmath254 may now grow , because we will add @xmath253 into @xmath254 .",
    "this corresponds to what happens in a bundle method .",
    "the tangent program @xmath273 has now to be solved numerically using a qp - solver , but since we may limit the number of elements of @xmath254 using the idea of the aggregate subgradient of kiwiel @xcite , see also @xcite , this is still very fast .",
    "for the spectral abscissa @xmath280 , which is not certified upper-@xmath98 , we use this cautious variant , where the computation of @xmath253 in step 4 may be required . for @xmath281",
    "this leads to a low - dimensional semidefinite program .",
    "the stopping test in step 2 of algorithm [ algo - outer ] can be delegated to subroutine [ algo - prox ] .",
    "namely , if @xmath132 is a karush - kuhn - tucker point of ( [ min - min ] ) , then @xmath282 is solution of the tangent program @xmath273 .",
    "this means we can use the following practical stopping tests : if the inner loop at iterate @xmath132 finds @xmath283 such that @xmath284 then we decide that @xmath221 is optimal and stop .",
    "that is , the @xmath285st inner loop is not started . on the other hand ,",
    "if the inner loop at @xmath132 has difficulties finding a new iterate and provides five consecutive unsuccessful backtracks @xmath269 such that @xmath286 or if a maximum @xmath287 of linesearch steps @xmath206 is exceeded , then we decide that @xmath132 was already optimal and stop . in our experiments we use @xmath288 , @xmath289 , @xmath290 .",
    "the term _ stepsize _ used for the parameter @xmath291 in the tangent program @xmath273 in step 2 of algorithm [ algo - prox ] is understood verbatim when @xmath292 consists of a single element @xmath293 and the minimum in @xmath273 is unconstrained , because then @xmath294 . however , even in those cases where step 4 of the subroutine is carried out in its full version , @xmath295 still acts like a stepsize in the sense that decreasing @xmath295 gives smaller steps ( in the inner loop ) , while increasing @xmath296 allows larger steps ( in the next inner loop ) .",
    "algorithm [ algo - outer ] was studied in much detail in @xcite , and we review the convergence result here , applying them directly to the functions @xmath120 and @xmath125 .",
    "the significance of the class of upper-@xmath98 functions for convergence lies in the following    suppose @xmath108 is upper-@xmath98 at @xmath297 .",
    "then its standard model @xmath233 is strict at @xmath297 in the following sense : for every @xmath298 there exists @xmath299 such that @xmath300 is satisfied for all @xmath301 .",
    "the following , even stronger property of upper-@xmath98 functions was proved in @xcite , see also @xcite .",
    "suppose @xmath302 and @xmath303 , and let @xmath304 arbitrary .",
    "then there exist @xmath305 such that @xmath306 is satisfied .",
    "remark [ note ] below shows that upper-@xmath98 , and thus ( [ stronger ] ) , are stronger than strictness ( [ strict ] ) of the standard model .",
    "[ theorem1 ] let @xmath307 be the sequence generated by algorithm [ algo - outer ] with standard linesearch for minimizing program ( [ minus_h ] ) .",
    "then the sequence @xmath132 converges to a karush - kuhn - tucker point @xmath73 of ( [ minus_h ] ) .",
    "the proof of ( * ? ? ?",
    "* theorem 2 ) shows that every accumulation point of the sequence @xmath132 is a critical point of ( [ minus_h ] ) , provided @xmath233 is strict .",
    "moreover , since the iterates are feasible , we obtain a kkt point .",
    "see clarke @xcite for a definition .",
    "however , it was observed in @xcite that estimate ( 26 ) in that proof can be replaced by ( [ stronger ] ) when the objective is upper-@xmath98 .",
    "since this is the case for @xmath125 on its domain @xmath146 , the step finding subroutine [ algo - prox ] can be reduced to a linesearch .",
    "reference @xcite gives also details on how to deal with the constraint set @xmath3 .",
    "note that hypotheses assuring boundedness of the sequence @xmath132 in @xcite are not needed , since @xmath3 is bounded .",
    "convergence to a single kkt point is now assured through ( * ? ? ? * cor .",
    "1 ) , because @xmath308 depends analytically on @xmath74 , so that @xmath125 is a subanalytic function , and satisfies therefore the ojasiewicz inequality @xcite .",
    "subanalyticity of @xmath125 can be derived from the following fact @xcite . if @xmath103 is subanalytic , and @xmath100 is subanalytic and compact , then @xmath309 is subanalytic .",
    "we apply this to the negative of ( [ 4sup ] ) .",
    "[ note ] the lightning function @xmath310 in @xcite is an example which has a strict standard model but is not upper @xmath98 .",
    "it is lipschitz with constant @xmath180 and has @xmath311 $ ] for every @xmath16 .",
    "the standard model of @xmath108 is strict , because for all @xmath312 there exists @xmath313 $ ] such that @xmath314 using the fact that sign@xmath315 . at the same time",
    "@xmath108 is certainly not upper-@xmath98 , because it is not semi - smooth in the sense of @xcite .",
    "this shows that the class of functions @xmath108 with a strict standard model offers a scope of its own , justifying the effort made in the step finding subroutine .      while we obtained an ironclad convergence certificate for the @xmath0-programs ( [ minus_h ] ) , and similarly , for ( [ eq - wp ] ) ,",
    "theory is more complicated with program ( [ minus_alpha ] ) . in our numerical testing @xmath316",
    "behaves consistently like an upper-@xmath98 function , and we expect this to be true at least if all active eigenvalues of @xmath317 are semi - simple .",
    "we now argue that we expect @xmath120 to have a strict standard model as a rule .",
    "since @xmath87 depends analytically on @xmath74 , the eigenvalues are roots of a characteristic polynomial @xmath318 with coefficients @xmath319 depending analytically on @xmath74 .",
    "for fixed @xmath320 , every eigenvalue @xmath321 of @xmath322 has therefore a newton - puiseux expansion of the form @xmath323 for certain @xmath324 , where the coefficients @xmath325 and leading exponent @xmath326 can be determined by the newton polygon @xcite . if all active eigenvalues of @xmath327 are semi - simple , then @xmath120 is lipschitz around @xmath73 by proposition [ prop4 ] , so that necessarily @xmath328 in ( [ puis ] ) .",
    "it then follows that either @xmath329 when @xmath330 for all active @xmath331 , or @xmath332 for the active @xmath333 if @xmath334 . in both cases @xmath120 satisfies the strictness estimate ( [ strict ] ) _ directionally _ , and we expect @xmath120 to have a strict standard model .",
    "indeed , for @xmath334 we have @xmath335 , while the case @xmath336 gives @xmath329 , hence @xmath337 , and so @xmath338 .",
    "as soon as these estimates hold uniformly over @xmath339 , @xmath120 has indeed a strict standard model , i.e. , we have the following    [ lem ] suppose every active eigenvalue of @xmath317 is semi - simple , and suppose the following two conditions are satisfied : @xmath340 then the standard model of @xmath120 is strict at @xmath73 . @xmath110    even though these conditions are not easy to check , they seem to be verified most of the time , so that the following result reflects what we observe in practice for the min - min program of the negative spectral abscissa @xmath120 .    [ theorem2 ]",
    "let @xmath341 be the sequence generated by algorithm [ algo - outer ] for program ( [ minus_alpha ] ) , where the step finding subroutine is carried out with step 4 activated .",
    "suppose every accumulation point @xmath73 of the sequence @xmath132 is simple or semi - simple and satisfies condition ( [ sups ] ) .",
    "then the sequence converges to a unique kkt point of program ( [ minus_alpha ] ) .",
    "we apply once again ( * ? ? ?",
    "* corollary 1 ) , using the fact that @xmath120 satisfies the ojasiewicz inequality at all accumulation points .",
    "convergence certificates for minimizing @xmath120 or @xmath211 seem to hinge on additional hypotheses which are hard to verify in practice . in @xcite",
    "the authors propose the gradient sampling algorithm to minimize @xmath211 , and their subsequent convergence analysis in @xcite needs at least local lipschitzness of @xmath211 , which is observed in practice but difficult to verify algorithmically .",
    "a similar comment applies to the hypotheses of theorem [ theorem2 ] , which appear to be satisfied in practice , but remain difficult to check directly .",
    "practical applications often feature several design requirements combining @xmath0 and @xmath342 performances with spectral constraints related to pole locations .",
    "the results in section [ conv : hinf ] easily extend to this case upon defining @xmath343 , where several performance channels @xmath344 are assessed against various requirements @xmath345 , as in @xcite .",
    "all results developed so far carry over to multiple requirements , because the worst - case multi - objective performance in step 4 of algorithm [ algo1 ] involves @xmath346 which has the same min - min structure as before .",
    "in this section our dynamic inner relaxation technique ( algorithm [ algo1 ] ) is tested on a bench of @xmath347 examples of various sizes and structures .",
    "all test cases have been taken and adapted from the literature and are described in table [ tab - list ] .",
    "some tests have been made more challenging by adding uncertain parameters in order to illustrate the potential of the technique for higher - dimensional parametric domains @xmath3 .",
    "the notation @xmath348 $ ] in the rightmost column of the table stands for the block sizes in @xmath349 $ ] .",
    "uncertain parameters have been normalized so that @xmath58^m$ ] , and the nominal value is @xmath36 .    the dynamic relaxation technique of algorithm [ algo1 ] is first compared to static relaxation ( [ static ] ) .",
    "that technique consists in choosing a dense enough static grid @xmath63 of the uncertainty box @xmath3 and to perform a multi - model synthesis for a large number card@xmath350 of models . in consequence , static relaxation",
    "can not be considered a practical approach .",
    "namely ,    * dense grids become quickly intractable for high - dimensional @xmath3 . *",
    "static relaxation may lead to overly optimistic answers in terms of worst - case performance if critical parametric configurations are missed by gridding .",
    "this is what is observed in table [ tab - comp ] , where we have used a @xmath351-point grid with @xmath352 the number of uncertain parameters .",
    "worst - case performance is missed in tests @xmath111 , @xmath353 , @xmath354 and @xmath347 , as we verified by algorithms @xmath355 .",
    "running times may rise to hours or even days for cases @xmath180 , @xmath355 , @xmath356 and @xmath357 . on the other hand ,",
    "when gridding turns out right , then algorithm [ algo1 ] and static relaxation are equivalent . in this respect",
    ", the dynamic relaxation of algorithm [ algo1 ] can be regarded as a cheap , and therefore very successful , way to cover the uncertainty box .",
    "the number of scenarios in @xmath9 rarely exceeds @xmath357 in our testing .",
    "computations were performed using matlab r2013b on os windows 7 home premium with cpu intel core i5 - 2410 m running at 2.30 ghz and 4 gb of ram .",
    "the results achieved by algorithm [ algo1 ] can be certified _ a posteriori _ through the mixed @xmath2 upper bound @xcite .",
    "this technique computes an overestimate @xmath358 of the worst - case performance on the unit cube @xmath3 .",
    "we introduce the ratio @xmath359 , where @xmath360 is the underestimate of @xmath2 predicted by our algorithm [ algo1 ] , given in column 4 of table [ tab - comp ] .",
    "clearly @xmath361 , or what is the same , @xmath362 , so that values @xmath363 certify the values predicted by algorithm [ algo1 ] .",
    "note that a value @xmath364 indicates failure to certify the value @xmath360 a posteriori , but such a failure could be due _ either _ to a sub - optimal result of algorithm [ algo1 ] , _ or _ to conservatism of the upper bound @xmath365 .",
    "this was _ not _ observed in our present testing , so that algorithm [ algo1 ] was certified in all cases .",
    "for instance , in row 2 of table [ tab - dksyn ] we have a guaranteed performance for parameters in @xmath366 .",
    "our last comparison is between algorithm [ algo1 ] and dksyn for complex and real @xmath2 synthesis , and the results are shown in table [ tab - dksyn ] .",
    "a value @xmath367 in column 6 of that table means worst - case performance of @xmath368 is guaranteed on the cube @xmath369^m$ ] .",
    "it turned out that no reasonable certificates were to be obtained with @xmath370 synthesis , since @xmath371 as a rule , so that @xmath372^m$ ] became too small to be of use , except for test cases @xmath373 , @xmath353 and @xmath374 . in this test bench ,",
    "algorithm [ algo1 ] achieved better worst - case performance on a larger uncertainty box with simpler controllers .",
    "it also proves competitive in terms of execution times .    !",
    "width 1ptc !",
    "width 1ptc | c | c | c ! width 1pt    n^o^ & benchmark name & ref . &",
    "states & uncertainty block structure + 1 & flexible beam & @xcite & 8 & [ 1 1 1 3 1 ] + 2 & mass - spring - dashpot & @xcite & 12 & [ 1 1 1 1 1 1 ] + 3 & dc motor & @xcite & 5 & [ 1 2 2 ] + 4 & dvd drive & @xcite & 5 & [ 1 3 3 3 1 3 ] + 5 & four disk & @xcite & 10 & [ 1 3 3 3 3 3 1 1 1 1 ] + 6 & four tank & @xcite & 6 & [ 1 1 1 1 ] + 7 & hard disk drive & @xcite & 18 & [ 1 1 1 2 2 2 2 1 1 1 1 ] + 8 & hydraulic servo & @xcite & 7 & [ 1 1 1 1 1 1 1 1 ] + 9 & mass - spring system & @xcite & 4 & [ 1 1 ] + 10 & tail fin controlled missile & @xcite & 23 & [ 1 1 1 6 6 6 ] + 11 & robust filter design 1 & @xcite & 4 & [ 1 ] + 12 & robust filter design 2 & @xcite & 2 & [ 1 1 ] + 13 & satellite & @xcite & 5 & [ 1 6 1 ] + 14 & mass - spring - damper & @xcite & 8 & [ 1 ] +    ! width 1ptc ! width 1ptr ! width 1ptr | r | r ! width 1ptr | r | r ! width 1pt & & & + & & & & & & & + 1 & 3 & 4 & 1.290 & 25.093 & 3125 & i & @xmath375 + 2 & 5 & 16 & 2.929 & 261.754 & 15625 & i & @xmath375 + 3 & pid & 2 & 0.500 & 6.256 & 125 & 0.500 & 127.952 + 4 & 5 & 1 & 45.455 & 2.012 & 15625 & 45.454 & 4908.805 + 5 & 6 & 6 & 0.672 & 68.768 & 9765625 & i & @xmath375 + 6 & 6 & 4 & 5.571 & 41.701 & 625 & 5.564 & 3871.898 + 7 & 4 & 4 & 0.026 & 34.647 & 48828125 & i & @xmath375 + 8 & pid & 3 & 0.701 & 10.140 & 390625",
    "& i & @xmath375 + 9 & 4 & 4 & 0.814 & 22.917 & 25 & 0.759 & 67.268 + 10 & 12 & 6 & 1.810 & 159.299 & 15625 & i & @xmath375 + 11 & 4 & 4 & 2.636 & 16.723 & 5 & 2.636 & 6.958 + 12 & 1 & 3 & 2.793 & 8.221 & 25 & 2.660 & 23.400 + 13 & 6 & 5 & 0.156 & 48.445 & 125 & 0.156 & 876.039 + 14 & 5 & 3 & 1.651 & 39.250 & 5 & 1.644 & 27.456 +",
    "! width 1ptc ! width 1ptr | r | r ! width 1ptr | r | r !",
    "width 1ptc !",
    "width 1pt & & & + & & & & & & & + 1 & 38 & 2.072 & 80.231 & 88 & 1.835 & 86.144 & 1.00 + 2 & 54 & 2.594 & 123.288 & 66 & 2.586 & 141.181 & 1.01 + 3 & 51 & 17.093 & 76.799 & 65 & 16.854 & 27.269 & 1.01 + 4 & 5 & 72.464 & 27.113 & 5 & 45.455 & 53.898 & 1.00 + 5 & 10 & 5.151 & 131.259 & 10 & 1.894 & 315.949 & 1.01 + 6 & 6 & 4.558 & 17.519 & 12 & 4.555 & 29.469 & 1.01 + 7 & 18 & 50.451 & 159.152 & f & f & f & 1.01 + 8 & 61 & 0.963 & 100.636 & 61 & 0.878 & 133.740 & 1.05 + 9 & 24 & 0.921 & 47.565 & 28 & 0.989 & 112.820 & 1.04 + 10 & 147 & 5.639 & 1412.402 & 337 & 2.834 & 7611.679 & 1.04 + 11 & 14 & 1.804 & 13.759 & 14 & 1.782 & 22.293 & 1.02 + 12 & 10 & 2.268 & 16.021 & 16 & 2.323 & 21.310 & 1.01 + 13 & 133 & 0.821 & 183.052 & 255 & 0.509 & 257.589 & 1.04 + 14 & 14 & 1.523 & 16.583 & 16 & 1.562 & 46.722 & 1.00 +      we now illustrate our robust synthesis technique in more depth for a tail fin controlled missile .",
    "this problem is adapted from ( * ? ? ? * chapter iv ) and has been made more challenging by adding parametric uncertainties in the most critical parameters .",
    "the linearized rigid body dynamics of the missile are @xmath376 where @xmath377 is the angle of attack , @xmath378 the pitch rate , @xmath272 the vertical acceleration and @xmath379 the fin deflection .",
    "both @xmath272 and @xmath378 are measured through appropriate devices as described below .",
    "a more realistic model also includes bending modes of the missile structure . in this application",
    ", we have @xmath380 bending modes whose contribution to @xmath272 and @xmath378 is additive and described as follows :",
    "@xmath381    it is also important to account for actuator and detector dynamics .",
    "the actuator is modeled as a 2nd - order transfer function with damping @xmath382 and natural frequency @xmath383 rad./sec .",
    "similarly , the accelerometer and pitch rate gyrometer are 2nd - order transfer functions with damping @xmath382 and natural frequencies @xmath384 rad./sec . and",
    "@xmath385 rad./sec . , respectively .",
    "uncertainties affect both rigid and flexible dynamics and the deviations from nominal are 30% for @xmath386 , 15% for @xmath387 , 30% for @xmath388 , and 10% for each @xmath389 .",
    "this leads to an uncertain model with uncertainty structure given as @xmath390 \\,,\\ ] ] which corresponds to @xmath391 and repetitions @xmath392 $ ] in the terminology of table [ tab - list ] .",
    "the controller structure includes both feed - forward @xmath393 and feedback @xmath394 actions @xmath395 where @xmath396 is the acceleration set - point and @xmath397 , @xmath398 are the detectors outputs .",
    "the total number of design parameters @xmath50 in @xmath399 is @xmath400 , as a tridiagonal state space representation of a @xmath354-th order controller was used .",
    "the missile autopilot is optimized over @xmath401 to meet the following requirements :    * the acceleration @xmath397 should track the reference input @xmath396 with a rise time of about @xmath402 seconds . in terms of the transfer function from @xmath396 to the tracking error @xmath403 this is expressed as @xmath404 , where the weighting function @xmath405 is @xmath406 * penalization of the high - frequency rate of variation of the control signal and roll - off are captured through the constraint @xmath407 , where @xmath408 is a high - pass weighting @xmath409 .",
    "* stability margins at the plant input are specified through the @xmath0 constraint @xmath410 , where @xmath411 is the input sensitivity function @xmath412 and with static weights @xmath413 .",
    "finally , stability and performance requirements must hold for the entire range of parametric uncertainties , where @xmath414 is the @xmath415-hyperbox with limits in percentage given above .",
    "the resulting nonsmooth program @xmath81 to be solved in step 2 of algorithm [ algo1 ] takes the form @xmath416 we have observed experimentally that controllers @xmath417 of order greater than @xmath354 do not improve much .",
    "the order of the augmented plant including flexible modes , detector and actuator dynamics , and weighting filters is @xmath418 .",
    "the evolution of the worst - case @xmath0 performance vs. iterations in algorithm [ algo - outer ] ( and its subroutine [ algo - prox ] ) is problem - dependent . for the missile example",
    ", a destabilizing uncertainty is found at the @xmath180st iteration .",
    "the algorithm then settles very quickly in @xmath356 iterations on a final set @xmath9 consisting of @xmath111 scenarios .",
    "the number of scenarios in the final @xmath9 coincides with the number of iterations in algorithm [ algo1 ] plus the nominal scenario , and can be seen in column 3 of table [ tab - comp ] .",
    "note that the evolution of the worst - case @xmath0 performance is not always monotonic .",
    "typically the curve may bounce back when a bad parametric configuration @xmath74 is discovered by the algorithm .",
    "this is the case e.g. for the mass - spring example .",
    "the achieved values of the @xmath0 norm and corresponding running times are given in table [ tab - comp ] .",
    "responses to a step reference input for @xmath419 models from the uncertainty set @xmath3 are shown in fig .",
    "[ fig - timesimu ] to validate the robust design .",
    "good tracking is obtained over the entire parameter range .",
    "the magnitude of the @xmath380 controller gains of @xmath417 are plotted in fig .",
    "[ fig - sigmak ] .",
    "robust roll - off and notching of flexible modes are clearly achieved .",
    "potential issues due to pole - zero cancellations are avoided as a consequence of allowing parameter variations in the model . finally , fig .",
    "[ fig - nichols ] displays the nichols plots for @xmath419 models sampled in the uncertainty set .",
    "we observe that good `` rigid '' margins as well as attenuation of the flexible modes over @xmath414 has been achieved .",
    "real @xmath2 synthesis turned out time - consuming , exceeding two hours in the missile example .",
    "the controller order inflates to @xmath420 and conservatism is still present as compared to dynamic relaxation via algorithm [ algo1 ] .",
    "a value @xmath421 reads as a worst - case @xmath0 performance of @xmath422 over the box @xmath423^m$ ] . to resort to interpreting uncertain parameters as complex",
    "can not be considered an acceptable workaround either . even when it delivers a result , this approach as a rule leads to high - order controllers ( @xmath424 states in the missile example ) .",
    "complex @xmath2 synthesis is also fairly conservative , as we expected .",
    "it appears that scaling- or multiplier - based approaches using outer relaxations @xcite encounter two typical difficulties :    * the number and repetitions of parametric uncertainties lead to conservatism .",
    "* repetitions of the parameters lead to high - order multipliers , which in turn produce high - order controllers .",
    "our approach is not affected by these issues .",
    "static relaxation remains intractable even for a coarse grid of @xmath356 points in each dimension .",
    "see table [ tab - comp ] .",
    "we have presented a novel algorithmic approach to parametric robust @xmath0 control with structured controllers . a new inner relaxation technique termed _ dynamic inner approximation _ , adapting a set of parameter scenarios @xmath425 iteratively , was developed and shown to work rapidly without introducing conservatism . global robustness and performance certificates",
    "are then best obtained _ a posteriori _ by applying analysis tools based on outer approximations . at the core our new method is leveraged by sophisticated nonsmooth optimization techniques tailored to the class of upper-@xmath98 stability and performance functions .",
    "the approach was tested on a bench of challenging examples , and within a case study .",
    "the results indicate that the proposed technique is a valid practical tool , capable of solving challenging design problems with parametric uncertainty ."
  ],
  "abstract_text": [
    "<S> we present a new approach to parametric robust controller design , where we compute controllers of arbitrary order and structure which minimize the worst - case @xmath0 norm over a pre - specified set of uncertain parameters . at the core of our method </S>",
    "<S> is a nonsmooth minimization method tailored to functions which are semi - infinite minima of smooth functions . </S>",
    "<S> a rich test bench and a more detailed example illustrate the potential of the technique , which can deal with complex problems involving multiple possibly repeated uncertain parameters </S>",
    "<S> .    * keywords . * real uncertain parameters @xmath1 structured @xmath0-synthesis @xmath1 parametric robust control @xmath1 nonsmooth optimization @xmath1 local optimality @xmath1 inner approximation     control system department , onera , toulouse , france ] </S>",
    "<S> institut de mathmatiques de toulouse , france ] hanoi national university of education , vietnam ] </S>"
  ]
}