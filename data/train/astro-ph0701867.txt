{
  "article_text": [
    "bayesian inference has become an invaluable tool in analysing cosmological datasets to place constraints on model parameters .",
    "however the more fundamental question of model selection , naturally incorporated in the bayesian framework by the evidence , has been under - utilised in cosmology .",
    "impeded until recently by the high computational cost of methods such as thermodynamic integration , progress has now been made with the advent of nested sampling @xcite , targeted solely at efficient calculation of the evidence but also capable of providing posterior inferences .",
    "this method has recently been applied to cosmological problems by @xcite allowing model selection on feasible timescales , typical of markov chain monte carlo ( mcmc ) parameter estimation .",
    "their algorithm uses an elliptical bound to restrict the prior around the maximum likelihood peak in the posterior and thus improves the acceptance ratio and efficiency .",
    "while this method can be used for the majority of cosmological applications , where posteriors are generally uni - modal , there are examples where this is not so . in this paper",
    "we describe a _ clustering _ nested sampler which is capable of detecting and isolating multiple peaks in the posterior , fitting separate ellipsoidal bounds around each and making considerable savings in computational load when compared to a single ellipse .",
    "in addition we have implemented an improved error calculation @xcite on the final evidence result which produces a mean and standard error in one calculation , eliminating the need for multiple runs .",
    "a bayesian analysis provides a coherent approach to the estimation of a set of model parameters @xmath3 and , crucially in determining which model , @xmath4 , best describes the data , @xmath5 .",
    "bayes theorem states that @xmath6 where @xmath7 is the posterior probability distribution , @xmath8 the likelihood , @xmath9 the prior , @xmath10 the bayesian evidence .",
    "typically , the posterior is generated via a metropolis - hastings algorithm with mcmc sampling , where at equilibrium the chain contains a set of samples from parameter space distributed with the posterior probability distribution . in the bayesian framework all of the inference",
    "is contained in the final multi - dimensional posterior , which can be marginalised over each parameter to obtain constraints . the bayesian evidence is represented by the overall normalisation of this posterior .",
    "the evidence is large in models which have a high likelihood over a large proportion of the prior parameter space so that we can consider the evidence to be the average of the likelihood divided by the prior .",
    "as such it forms the integral , over the @xmath0-dimensional parameter space @xmath11 thus a model containing a large number of parameters for which only a narrow region of the prior is likely will have a low evidence and vice versa , providing a natural mechanism to limit the complexity of cosmological models and elegantly incorporating ockham s razor .",
    "a standard scenario in bayesian model selection would require the computation of evidences for two models a and b. the difference of log - evidences @xmath12 , also called the bayes factor then quantifies how well a may fit the data when compared with model b. @xcite provides a scale on which we can make qualitative conclusions based on this difference : @xmath13 is not significant , @xmath14 significant , @xmath15 strong and @xmath16 decisive .    the prior dependence of the evidence requires that the entire parameter space is adequately sampled , the mcmc method described above moves rapidly through parameter space toward areas of high likelihood leaving the majority of the surrounding prior heavily under sampled .",
    "this is sufficient to generate parameter constraints where regions close to the peak of the posterior are most important , but it radically over estimates the evidence due to under sampling of regions of low likelihood . in order to sample more uniformly the method of thermodynamic integration has previously been implemented ( see e.g @xcite ; @xcite ) and used successfully by a number of authors ( @xcite , @xcite , @xcite , @xcite , @xcite , @xcite ) in computing the evidence .",
    "thermodynamic integration uses mcmc to draw samples not from the posterior directly but from @xmath17 where @xmath18 is the inverse temperature and is raised from @xmath19 to @xmath20 .",
    "for low values of @xmath18 peaks in the posterior are sufficiently flattened to allow improved mobility of the chains over the entire prior range .",
    "typically it is possible to obtain accuracies of within 0.5 units in log - evidence via this method however this does require of order @xmath21 samples per chain ( with around 10 chains required to determine a sampling error ) making it at least an order of magnitude more costly than the sampling needed for parameter estimation .",
    "nested sampling is computationally more efficient as it transforms the integral in eqn .",
    "[ equation : evidence ] to a single dimension by a suitable re - parameterisation in terms of the prior _ mass _ @xmath22 .",
    "this mass can be divided into elements @xmath23 which can be combined in any order to give say @xmath24 the prior mass covering all likelihoods above the iso - likelihood curve @xmath25 .",
    "we also require the function @xmath26 to be a singular decreasing function ( which is trivially satisfied for most posteriors ) so that using sampled points we can estimate the evidence via the integral : @xmath27 an example of a posterior in two dimensions and its associated function @xmath26 is shown in fig .",
    "[ figure1 ] .",
    "the transformed integral is dominated by a relatively small region of the prior containing the majority of the posterior mass . for efficient sampling",
    "the division of the prior elements should not be linear but geometric :    @xmath28    where we would desire a largely constant @xmath29 ; the procedure for nested sampling ensures just that .",
    "the method is then to draw an initial set of @xmath30 _ active _ samples uniformly from the full prior @xmath31 $ ] .",
    "the samples are ordered in terms of likelihood , the smallest of which , @xmath32 is removed from the active set .",
    "the prior is then reduced according to eqn .",
    "[ equation : recursion ] so that the region sampled becomes @xmath33 $ ] where @xmath34 , a point is then found to replenish the active set subject to the criterion that its likelihood @xmath35 .",
    "this sampling / replenishment cycle is then repeated until the entire prior has been traversed .",
    "the algorithm thus travels through nested shells of likelihood as the prior mass is reduced . at this point",
    "the evidence is approximated by a numerical integration routine such as the trapezoid rule : @xmath36 the method described above ensures that the @xmath29 have the distribution @xmath37 , allowing them to be statistically assigned .",
    "this does introduce an uncertainty , but it is possible to quantify this accurately and relate it to the final @xmath38 . if we could assign each @xmath39 exactly then the only error would be due to the discretisation of the integral which , given sufficient points would be expected to be negligible . for this reason the dominant source of error in the final @xmath38 arises from the incorrect assignment of each prior mass .",
    "fortuitously our knowledge of the distribution @xmath40 from which each @xmath29 is drawn allows us to assess the errors in any quantities we produce .",
    "given the probability of a vector @xmath41 as @xmath42 we can write the expectation value of some quantity @xmath43 as @xmath44 evaluating this integral is possible by monte carlo methods by sampling a given number of vectors @xmath41 and finding the average @xmath45 . by this method",
    "we can essentially determine the variance of the curve in @xmath46 space , and thus in the evidence integral @xmath47 .",
    "previous numerical evidence analyses have determined an associated error on the final evidence estimate by repeating identical calculations to determine the standard error , and for reliable results this method requires of order tens of estimates . as our results will show",
    "the above method is capable of estimating the expectation and variance of @xmath38 accurately when compared to sampling statistics so as to eliminate the need for any repetition .      evaluating the likelihood of a point in cosmological parameter space",
    "is computationally expensive .",
    "nested sampling reduces this overhead as it requires fewer likelihood evaluations .",
    "however further reductions are possible by restricting the prior region at successive sample / replacement cycles . drawing blindly from the prior probability distribution ,",
    "will invariably result in an increasing number of samples from unlikely regions of the prior .",
    "ellipsoidal sampling partially overcomes this by approximating the iso - likelihood of the point to be replaced by an @xmath0-dimensional ellipsoid formed from the covariance matrix of the current set of active points . in the limit as the ellipsoid approaches that of the likelihood bound",
    ", the acceptance rate tends to unity .",
    "ellipsoidal sampling provides an elegant solution , but there are some algorithmic points to note .",
    "firstly , it is imperative that the ellipse does not restrict the sampling region to lie within @xmath48 to avoid overestimating the evidence . to prevent this",
    ", previous authors ( such as @xcite ) enlarged the ellipse by a fixed enlargement factor @xmath49 , so that in most , but not all cases , the ellipse will encompass all active points at any stage .",
    "an obvious example where this may break down would be a rectangular posterior .",
    "we have chosen instead to define the ellipse to guarantee that at any stage all active points are enclosed .",
    "if we have determined the covariance matrix @xmath50 of samples at a given stage , the diagonal matrix @xmath51 is formed by transformation @xmath52 where @xmath53 is the matrix of eigenvectors of @xmath50 , where each eigenvalue is the squared length of the ellipse along each principle axis .",
    "thus the ellipsoidal bound is defined as @xmath54 where @xmath55 are the parameter vectors along each axis and where the constant scaling length @xmath56\\ ] ] with @xmath57 denoting the vectors of the active point set .",
    "this ensures that all active points are enclosed within the ellipse although there does , of course , still exist the possibility that some parts of the region @xmath58 lie outside the ellipse and so we still require an enlargement factor but one that is generally smaller than that required by mukherjee s method . in fig [ enlargement ]",
    "evidence estimates are shown calculated with a range of enlargement factors @xmath59 as determined for a relatively complex posterior of multiple gaussians where the analytic @xmath60 .",
    "the algorithm succesfully converges to the correct value with an enlargement factor of @xmath61 .",
    "computationally it is convenient to draw the random samples not from this ellipsoidal bound directly but from a unit sphere centred on the origin .",
    "thus , we construct a transformation matrix @xmath62 where @xmath63 which maps points from the unit sphere into the ellipse ( centred at the origin ) so a uniform deviate @xmath55 drawn from the sphere forms a point within the ellipse @xmath64 via @xmath65    generating deviates from a uniform @xmath0-sphere can be most easily achieved by drawing a set of @xmath0 gaussian random numbers @xmath66 ( via the box - muller algorithm ) , scaling to give a point uniformly distributed on the surface of the sphere @xmath67 where @xmath68 and finally scaling again by a deviate drawn from a quadratic distribution between 0 and 1 to generate a point uniformly within the sphere .",
    "it is worth pointing out that the number of active points @xmath30 must always be greater than the dimensionality of the parameter space in order for the ellipse to be defined unambiguously .",
    "the usefulness of ellipsoidal sampling is reduced for posteriors that are not uni - modal .",
    "our extension allows the formation of multiple ellipsoids centred on individual isolated peaks in the posterior which can greatly reduce the region of prior sampled and thus increases sampling efficiency .",
    "the idea is depicted in fig .",
    "[ figure2 ] in a toy posterior and equivalently in fig .",
    "[ figure3 ] in @xmath46 space , where upon identifying distinct _",
    "clusters _ they are separated and fit by individual ellipsoids . while this does violate the requirement that @xmath26 be a uniquely valued function we are rescued by the linear nature of the evidence .",
    "it is valid to consider each cluster individually and sum the contributions provided we correctly assign the prior masses to each distinct region . since our collection of @xmath30 active points is distributed evenly across the prior we can safely assume that the number of points within each clustered region is proportional to the prior mass contained therein .     +   +     space . ]    identifying clusters in the posterior , in the absence of any analytical form for the function , is performed by recursive application of the _ k - means _ algorithm with @xmath69 .",
    "this process is designed to place @xmath30 data points in an arbitrarily large number of dimensions into @xmath70 clusters .",
    "an iterative process the algorithm starts by dividing the set of samples into two clusters , the mean position of each cluster is then determined and each subsequent point is assigned to one or other of the two mean positions .",
    "the next iteration updates the mean positions and so on . in this way",
    "a set of sampled data can be searched by the alorithm for clustering centres . for",
    "a more detailed discussion of the @xmath70-means process see for example see @xcite . while our implementation does not require the user to know beforehand any details about the shape of the posterior ,",
    "the algorithm does contain a number of user definable parameters which can guide the clustering .",
    "these parameters define two conditions that must be met once two separate clusters have been identified via k - means .",
    "the first is a volume reduction factor set so that the total volume of the combined clustered ellipsoids is less than some fraction of the pre - clustered ellipsoid .",
    "the second parameter requires that the clusters are sufficiently separated by some distance to avoid overlapping regions .",
    "we have found reasonable values for the former to be @xmath71 while the latter seperation criteria requires a new ellipse to be at least @xmath72 times the existing ellipse major axis in each dimension .",
    "if these conditions are met clustering will occur and the algorithm will search independently within each cluster for further sub - clusters .",
    "this process continues recursively until some stopping criterion ( discussed in sec . [",
    "implementation : stopping ] . ) is met .",
    "so long as condition ( i ) is met we will be guaranteed a performance improvement as less of the prior needs to be sampled at each cycle .",
    "the division of prior mass between two clusters introduces a further source of error , and we can no longer simply assess the uncertainty in @xmath38 . to resolve this",
    "we must find the probability of the mass fraction in each cluster .",
    "if we have a fraction @xmath73 in cluster 1 , with a total number of points @xmath30 , the probability of @xmath74 points in cluster 1 is simply binomial : @xmath75 we can invert this using bayes theorem with a uniform prior to yield @xmath76 and draw samples via mcmc which can then be used , as in sec .",
    "[ section : nested ] to generate a variance for each cluster which can be summed to give the total error .",
    "we wish to stop the calculation on determining the evidence to some specified accuracy",
    ". one way would be to proceed until the evidence estimated at each replacement changes by less than a specified tolerance .",
    "however , this could for example underestimate the evidence in cases where the posterior contains any narrow peaks close to its maximum .",
    "@xcite provides an adequate and robust condition by determining an upper limit on the evidence that could be determined from the remaining set of current active points .",
    "by selecting the maximum likelihood @xmath77 in the set of active points one can then safely assume that the largest evidence contribution that can be made by the remaining portion of the posterior is @xmath78 i.e. the product of the remaining prior mass and maximum likelihood . we choose to stop when this quantity would no longer change the final evidence estimate by some user defined factor .",
    "to demonstrate the capabilities of clustered nested sampling we now take three specific examples ; the first two involve posteriors of known functional form so that an analytic evidence can be compared with that found through our nested sampling algorithm and the final example demonstrates a cosmological application involving a bi - modal posterior .",
    "our first example posterior consists of 3-gaussian peaks in a two dimensional parameter space .",
    "the clustering ( see fig . [ examplei:3gaussian ] ) algorithm first divides the space into cluster 1 and cluster 2 + 3 , runs to the required accuracy on cluster 1 then returns to the combination of clusters 2 + 3 , immediately clusters to separate 2 + 3 then running both successively to completion . table [",
    "examplei:3gaussian_table ] shows that with and without clustering we can determine the evidence to within 2 % of the analytic value . to obtain similar accuracy",
    ", however the clustered example required only 5 % of the number of likelihood evaluations .",
    "our calculated errors agree to within 10 % of the sampling errors based on 50 repetitions .    .",
    "the clusters found are depicted in colour at each peak . ]",
    ".summary of sampling statistics with and without clustering for the 3-gaussian posterior with a log - evidence , determined analytically of 2.813 . [ cols=\"^,^,^\",options=\"header \" , ]     [ exampleiii : bimodal_table ]",
    "we have demonstrated the applicability of our clustered nested sampling algorithm to multi - modal posterior distributions in both simulated ` toy ' models and a cosmological example based on the starobinsky primordial power spectrum in which a substantial performance increase was noted .",
    "although we have examined here a multi - modal cosmological example , our sampler is by no means restricted to such use .",
    "in fact we feel an important feature is the ability to determine the variance of the final evidence value , for any model , without calculating the sample variance over repetitive runs .",
    "this alone reduces the computational load by at least an order when compared with previous methods .",
    "this work was carried out largely on the cosmos uk national cosmology supercomputer at damtp , cambridge and we would like to thank s. rankin and v. treviso for their computational assistance . the authors would like to thank andrew liddle , david mackay and faran ferhoz for useful discussions .",
    "jrs was supported by stfc .",
    "mb was supported by a benefactors scholarship at st .",
    "john s college , cambridge and an isaac newton studentship .",
    "99 basset b.a . ,",
    "corasaniti p.s . ,",
    "kunz , m. , astrophys .",
    "j. lett . , 2004 , 617 , l1 beltran m. , garcia - bellido j. , lesgourgues j. , liddle a. , slosar a. , 2005 , phys .",
    "d , 71 , 063532 bennett c.l .",
    ", et al . , 2003 ,",
    "apjs , 148 , 1 bridges m. , lasenby a.n . , hobson , m.p . , 2006 , mnras , 369 , 1123 bridle s. , lewis a. , weller j. , efstathiou g. , 2003 , mnras , 342 , l72 efstathiou g. , 2003 , mnras , 343 l95 hobson m.p . , bridle s.l .",
    ", lahav o. , 2002 , mnras , 335 , 377    jeffreys h. , 1961 , _ theory of probability _ , 3rd ed . , oxford university press , oxford lasenby a.n . ,",
    "doran , c. , 2005 , phys.rev .",
    ", 063502 lewis a. and bridle s. , 2002 , phys . rev .",
    "d , 66 , 103511 mackay d.j.c .",
    ", 2003 , information theory , inference and learning algorithms .",
    "cambridge university press , cambridge mukherjee p. , parkinson d. , liddle a.r .",
    ", 2006 , apj , 638 , l51 niarchou a. , jaffe a. , pogosian l. , 2004 , phys.rev .",
    "d 69 063515 sinha r. , souradeep t. , 2006 , phys.rev .",
    "d74 043518 skilling j. , 2004 , ` nested sampling for general bayesian computation ' , http://www.inference.phy.cam.ac.uk/bayesys slosar a. et al . , 2003 , mnras , 341 , l29 shafieloo , a. , souradeep , t. , 2004 , phys .",
    "d , 70 , 043523 starobinsky a.a .",
    ", 1992 , jetp lett , 55 , 489 trotta , r.  2007 , mnras , 378 , 72"
  ],
  "abstract_text": [
    "<S> bayesian model selection provides the cosmologist with an exacting tool to distinguish between competing models based purely on the data , via the bayesian evidence . </S>",
    "<S> previous methods to calculate this quantity either lacked general applicability or were computationally demanding . </S>",
    "<S> however , nested sampling ( skilling 2004 ) , which was recently applied successfully to cosmology by muhkerjee et al . </S>",
    "<S> 2006 , overcomes both of these impediments . </S>",
    "<S> their implementation restricts the parameter space sampled , and thus improves the efficiency , using a shrinking ellipsoidal bound in the @xmath0-dimensional parameter space encompassing parameter samples above a decreasing likelihood value . </S>",
    "<S> however , if the likelihood function contains any multi - modality , separated over a significant portion of the parameter space then the ellipse is prevented from constraining the sampling region by less than the distance between the likelihood peaks . in this paper </S>",
    "<S> we introduce a method of clustered nested sampling whereby ellipsoidal clusters can form on any peaks identified thus improving the efficiency by a factor which is equal to the ratio of the volumes enclosed by the set of small clustered ellipsoids and the large single ellipse that would necessarily be required without clustering . </S>",
    "<S> in addition we have implemented a method for determining the expectation _ and _ variance of the final evidence value without the need to use sampling error from repetitions of the algorithm ; this further reduces the computational load by at least an order of magnitude . </S>",
    "<S> we have applied our algorithm to a pair of toy models and one cosmological example where we demonstrate that the number of likelihood evaluations required is @xmath1 4@xmath2 of that necessary for using previous algorithms . </S>",
    "<S> we have produced a fortran library containing our routines which can be called from any sampling code , in addition for convenience we have incorporated it into the popular cosmomc code as cosmoclust . </S>",
    "<S> both are available for download at www.mrao.cam.ac.uk/software/cosmoclust .    </S>",
    "<S> [ firstpage ]    methods : statistical  cosmological parameters </S>"
  ]
}