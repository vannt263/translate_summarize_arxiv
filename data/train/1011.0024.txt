{
  "article_text": [
    "lattice quantum chromodynamics ( lqcd ) is the lattice discretized theory of the strong nuclear force , the force that binds quarks together into particles such as the proton and neutron .",
    "high precision predictions from lqcd are required for testing the standard model of particle physics , a task with increased importance in the era of the large hadron collider ( lhc ) , where deviations between numerical lqcd predictions and experiment could be signs of new physics .",
    "lqcd also has a vital role to play in nuclear physics , where such calculations are used to compute and classify the excited states of protons , neutrons and other hadrons ; to study hadronic structure ; and to compute the forces and binding energies in light nuclei .",
    "lqcd is a grand challenge subject , with large - scale computations consuming a considerable fraction of publicly available supercomputing resources .",
    "the computations typically proceed in two phases : in the first phase , one generates thousands of _ configurations _ of the strong force fields ( gluons ) , colloquially referred to as _",
    "gauge fields_. this computation is a long - chain monte carlo process , requiring the focused power of leadership class computing facilities for extended periods . in the second phase ,",
    "these configurations are _ analyzed _ , a process that involves probing the interaction of quarks and gluons with each other on each configuration .",
    "the interactions are calculated by solving systems of linear equations with coefficients determined by elements of the gauge field . on each configuration",
    "the equations are solved for many right hand sides , and the solution vectors are used to compute the final observables of interest . this second phase can proceed independently on each configuration , and as a result , cluster partitions of modest size have proven to be highly cost - effective for this purpose . until a few years ago",
    ", the analysis phase would often account for a relatively small part of the cost of the overall calculation , with analysis corresponding to perhaps 10% of the cost of gauge field generation . in recent years , however , focus has turned to more challenging physical observables and new analysis techniques that demand solutions to the aforementioned linear equations for much larger numbers of right hand sides ( see , e.g. ,  @xcite ) . as a result ,",
    "the relative costs have shifted to the point where analysis often requires an equal or greater amount of computation than gauge field generation .",
    "the rapid growth of floating point power in graphics processing units ( gpus ) together with drastically improved tools and programmability has made gpus a very attractive platform for lqcd computations .",
    "the quda library  @xcite provides a package of optimized kernels for lqcd that take advantage of nvidia s compute unified device architecture ( cuda ) .",
    "once coupled to lqcd application software , e.g. , chroma  @xcite , this provides a powerful framework for lattice field theorists to exploit .",
    "the `` 9 g '' gpu cluster at jefferson laboratory features 192 nvidia gtx 285 gpus providing over 30 tflops of sustained performance in lqcd , when aggregated over single gpu jobs . for problems that can be accommodated by the limited gpu memory , the price / performance compared to typical clusters or massively parallel supercomputers ( e.g. , bluegene / p ) is improved by around a factor of five . however , for problem sizes that are too large , individual gpus have no benefit .    even for problems that do fit on a single gpu ,",
    "the economics of constructing a gpu cluster tend to motivate provisioning each cluster node with multiple gpus , since the incremental cost of an additional gpu is fairly small . in this scenario , it is possible to run multiple independent jobs on each node , but then the size of the host memory may prove to be the limiting constraint .",
    "the obvious recourse in both cases is therefore to parallelize a single problem over multiple gpus , which is the subject of our present work .",
    "the paper is organized as follows . in sections  [ sec :",
    "lqcd ] and  [ sec : gpus ] we review basic details of the lqcd application and of nvidia gpu hardware .",
    "we then briefly consider some related work in section  [ sec : related - work ] before turning to a general description of the quda library in section  [ sec : quda ] .",
    "our parallelization of the quark interaction matrix is described in [ sec : multi - gpu ] , and we present and discuss our performance data for the parallelized solver in section  [ sec : solver - perf ] .",
    "we finish with conclusions and a discussion of future work in section  [ sec : conclusions ] .",
    "the necessity for a lattice discretized formulation of qcd arises due to the failure of perturbative approaches commonly used for calculations in other quantum field theories , such as electrodynamics .",
    "quarks , the fundamental particles that are at the heart of qcd , are described by the dirac operator acting in the presence of a local su(3 ) symmetry . on the lattice ,",
    "the dirac operator becomes a large sparse matrix , @xmath0 , and the calculation of quark physics is essentially reduced to many solutions to systems of linear equations given by @xmath1 the form of @xmath0 on which we focus in this work is the sheikholeslami - wohlert @xcite ( colloquially known as _ wilson - clover _ ) form , which is a central difference discretization of the dirac operator .",
    "when acting in a vector space that is the tensor product of a 4-dimensional discretized euclidean spacetime , _ spin _ space , and _ color _ space it is given by @xmath2 here @xmath3 is the kronecker delta ; @xmath4 are @xmath5 matrix projectors in _ spin _",
    "space ; @xmath6 is the qcd gauge field which is a field of special unitary @xmath7 ( i.e. , su(3 ) ) matrices acting in _ color _ space that live between the spacetime sites ( and hence are referred to as link matrices ) ; @xmath8 is the @xmath9 clover matrix field acting in both spin and color space , corresponding to a first order discretization correction ; and @xmath10 is the quark mass parameter .",
    "the indices @xmath11 and @xmath12 are spacetime indices ( the spin and color indices have been suppressed for brevity ) .",
    "this matrix acts on a vector consisting of a complex - valued 12-component _ color - spinor _ ( or just _ spinor _ ) for each point in spacetime .",
    "we refer to the complete lattice vector as a spinor field .",
    "the nearest neighbor stencil part of the lattice dirac operator @xmath13 , as defined in ( [ eq : m ] ) , in the @xmath14 plane .",
    "the _ color - spinor _ fields are located on the sites .",
    "the su(3 ) color matrices @xmath15 are associated with the links .",
    "the nearest neighbor nature of the stencil suggests a natural even - odd ( red - black ) coloring for the sites.,width=240 ]    since @xmath0 is a large sparse matrix , an iterative krylov solver is typically used to obtain solutions to ( [ eq : linear ] ) , requiring many repeated evaluations of the sparse matrix - vector product .",
    "the matrix is non - hermitian , so either conjugate gradients @xcite on the normal equations ( cgne or cgnr ) is used , or more commonly , the system is solved directly using a non - symmetric method , e.g. , bicgstab @xcite .",
    "even - odd ( also known as red - black ) preconditioning is used to accelerate the solution finding process , where the nearest neighbor property of the @xmath16 matrix ( see fig .  [",
    "fig : dslash ] ) is exploited to solve the schur complement system  @xcite .",
    "this has no effect on the overall efficiency since the fields are reordered such that all components of a given parity are contiguous .",
    "the quark mass controls the condition number of the matrix , and hence the convergence of such iterative solvers . unfortunately",
    ", physical quark masses correspond to nearly indefinite matrices .",
    "given that current leading lattice volumes are @xmath17 , for @xmath18 degrees of freedom in total , this represents an extremely computationally demanding task .",
    "in the context of general - purpose computing , a gpu is effectively an independent parallel processor with its own locally - attached memory , herein referred to as _",
    "device memory_. the gpu relies on the host , however , to schedule blocks of code ( or _ kernels _ ) for execution , as well as for i / o .",
    "data is exchanged between the gpu and the host via explicit memory copies , which take place over the pci - express bus .",
    "the low - level details of the data transfers , as well as management of the execution environment , are handled by the gpu device driver and the runtime system .",
    "it follows that a gpu cluster embodies an inherently heterogeneous architecture .",
    "each node consists of one or more processors ( the cpu ) that is optimized for serial or moderately parallel code and attached to a relatively large amount of memory capable of tens of gb / s of sustained bandwidth . at the same time , each node incorporates one or more processors ( the gpu ) optimized for highly parallel code attached to a relatively small amount of very fast memory , capable of 150 gb / s or more of sustained bandwidth .",
    "the challenge we face is that these two powerful subsystems are connected by a narrow communications channel , the pci - e bus , which sustains at most 6 gb / s and often less . as a consequence , it is critical to avoid unnecessary transfers between the gpu and the host . for single - gpu code ,",
    "the natural solution is to carry out all needed operations on the gpu ; in the quda library , for example , the linear solvers are written such that the only transfers needed are the initial upload of the source vector to the gpu and the final download of the solution , aside from occasional small messages needed to complete global sums . a multi - gpu implementation",
    ", however , can not avoid frequent large data transfers , and so the challenge becomes to overlap the needed communication with useful work .",
    "this is exacerbated further if one wishes to take advantage of many gpus spread across multiple nodes , since the bandwidth provided the fastest available interconnect , qdr infiniband , is half again that provided by ( x16 ) pci - e .",
    ".[table : specs]specifications of representative nvidia graphics cards . [ cols=\"<,^,^,^,^,^\",options=\"header \" , ]     we turn now to the architecture of the gpu itself .",
    "our purpose is only to highlight those features that have directly influenced our implementation .",
    "we focus here on cards produced by nvidia and specifically on the gt200 generation , as typified by the tesla c1060 and the geforce gtx 285 , since the latter will serve as our test bed .",
    "the gt200 series is the second of the three extant generations of cuda - enabled cards , representative examples of which are listed in table  [ table : specs ] .",
    "the most recent generation , embodying nvidia s `` fermi '' architecture , is only now becoming available in mid-2010 .",
    "we note that while hardware features and performance differ between generations , these have relatively little impact on our multi - gpu strategy .",
    "likewise , most of the considerations we discuss would apply even to an opencl implementation targeting graphics cards produced by amd / ati .",
    "gpus support a single - program multiple - data ( spmd ) programming model with up to thousands of threads in flight at once .",
    "each thread executes the same kernel , using a unique thread index to determine the work that should be carried out .",
    "the gpu in the geforce gtx 285 card consists of 240 cores organized into 30 multiprocessors of 8 cores each .",
    "each core services multiple threads concurrently by alternating between them on successive clock cycles , so a group of 32 threads ( a _ warp _ in nvidia s parlance ) is executing on the multiprocessor at a given moment . at the same time , many additional threads ( ideally hundreds ) are typically resident on the multiprocessor and ready to execute .",
    "this allows the multiprocessor to swap in a new set of 32 threads when a given set stalls  while waiting for a memory access to complete , for example . in order to hide latency , it is desirable to have many threads resident at once , but each such thread requires a certain number of registers and quantity of shared memory , which limits the total . just as on a cpu , a _ register _",
    "is where a variable is stored while it is being operated on or written out .",
    "registers are not shared between threads .",
    "_ shared memory _",
    ", on the other hand , may be shared between threads executing on the same multiprocessor . strictly speaking",
    ", the threads must belong to the same _ thread block _",
    ", a group of threads whose size is specified by the programmer ; each thread block must consist of a multiple of 64 threads , and one or more thread blocks may be active on a multiprocessor at a time .",
    "the geforce gtx 285 provides 16,384 single - precision registers ( 8,192 in double precision ) and 16  kib of shared memory per multiprocessor .",
    "the cuda programming model treats the threads within a block as independent threads of execution , as though they were executing on cores that were true scalar processors ; threads may take independent code paths , read arbitrary locations in memory , and so on . in order to obtain optimal performance , however , it is better to treat the multiprocessor as a single 32-lane or 16-lane simd unit .",
    "this follows from two considerations .",
    "first , when threads within a set of 32 ( a warp ) take different paths at a branch , the various paths are serialized and executed one after another , a condition known as `` warp divergence . ''",
    "second , when accessing device memory , maximum bandwidth is achieved only when 16 threads access contiguous elements of memory , where each such element is a 4-byte , 8-byte , or 16-byte block .",
    "( the cuda c language defines various short vector types for this purpose , e.g. , _",
    "float2 , float4 , double2 , short4 , _ etc . )",
    "this allows the transfer to proceed as a single `` coalesced '' memory transaction . as described in section  [ sec : quda ] below , this consideration directly influences the layout of our data .",
    "an additional consideration has to do with the physical organization of the device memory . like many classic vector architectures but unlike commodity cpus",
    ", gpus are equipped with a very wide memory bus ( 512-bit on the gtx 285 ) with memory partitioned into multiple banks ( eight on the gtx 285 ) .",
    "successive 256-byte regions in device memory map to these partitions in a round - robin fashion .",
    "this organization is generally transparent to the programmer , but if memory is accessed with a stride that results in traffic to only a subset of the partitions , performance will be lower than if all partitions were stressed equally .",
    "such `` partition camping '' can result in an unexpected loss of performance for certain problem sizes  @xcite . as discussed in  @xcite and section  [ sec : quda ] below , this was found to be a problem for certain lattice volumes in our lqcd application , with the solution being to pad the relevant arrays to avoid the camping .    to summarize , the gpu memory hierarchy consists of globally - accessible device memory and local per - multiprocessor shared memory , often used as a manually - managed cache , as well as local registers .",
    "in addition , gpus such as the gtx 285 provide two special - purpose caches .",
    "the first is a read - only texture cache , which speeds up reads from device memory for certain kinds of access patterns .",
    "it also provides various addressing modes and rescaling capability .",
    "as described further in section  [ sec : quda ] , we take advantage of the latter in our half - precision implementation .",
    "finally , each multiprocessor provides a small _ constant cache _",
    "( 8  kib on the gtx 285 ) , which is useful for storing run - time parameters and other constants , accessible to all threads with very low latency .",
    "gpus were first used to perform lqcd calculations in @xcite .",
    "this pioneering study predated various programmability improvements , such as the c for cuda framework , and hence was implemented using the opengl graphics api .",
    "it targeted single gpu devices .",
    "the quda library  @xcite was discussed extensively in @xcite , where the primary techniques and algorithms for maximizing the efficient use of memory bandwidth were presented for a single gpu device .",
    "lqcd on gpus has also been explored in @xcite , which focused on questions of fine grained vs.  coarse grained parallelism on single gpu devices .",
    "in addition , there are several as yet unpublished efforts aimed at exploiting gpus for lqcd underway .",
    "lqcd has also been implemented on other heterogeneous devices , primarily on the cell broadband engine .",
    "efforts in this direction have been reported in @xcite as part of the `` qcd parallel computing on the cell broadband engine '' ( qpace ) project and elsewhere  @xcite .    outside the context of lqcd ,",
    "general challenges of implementing message passing on heterogeneous architectures have been considered for gpus in @xcite and for the roadrunner supercomputer in @xcite .",
    "an effort to provide a general message passing framework utilizing cuda , mpi , and posix threads is also underway at jefferson laboratory  @xcite .",
    "the quda library is a publicly available collection of optimized qcd kernels built on top of the cuda framework , with a simple c interface to allow for easy integration with lqcd application software .",
    "currently , quda provides highly optimized cg and bicgstab linear solvers for a variety of different discretizations of the dirac operator , as well as other time critical components .",
    "the power of gpus may only be brought to bear when a large degree of parallelism is available .",
    "lqcd is fortunate in this regard , since parallelism can easily be achieved by assigning one thread to each lattice site .",
    "the mapping from the linear thread index to the 4-dimensional spacetime index is easily obtained through integer division and modular arithmetic involving the lattice dimensions . these runtime parameters ( and others , such as boundary conditions ) are stored in the constant cache .    in applying the lattice dirac operator ,",
    "each thread is thus responsible for gathering its eight neighboring spinors ( 24 numbers apiece ) , applying the appropriate spin projector for each , multiplying by the color matrix connecting the sites ( 18 numbers ) , and accumulating the results together with the local spinor ( 24 numbers ) weighted by the mass term .",
    "the wilson - clover discretization also requires an extra multiplication by the clover matrix ( 72 numbers ) before the result ( 24 numbers ) is saved to memory . in total , the application of the wilson - clover matrix requires 3696 floating point operations for every 2976 bytes of memory traffic in single precision ( assuming kernel fusion to minimize memory traffic ) .",
    "the ordering typical on a cpu is to place the spacetime dimensions running slowest , with internal dimensions ( color , spin , and real / imaginary ) running fastest . however , since memory coalescing is only achieved if adjacent threads load consecutive blocks of 4 , 8 , or 16 bytes , the fields must be reordered to ensure this condition .",
    "this can be achieved if we abandon the naive ordering , @xmath19 in favor of the new mapping @xmath20 here @xmath21 is the spacetime volume ; @xmath11 is the linear spacetime index running from 0 through @xmath22 ; @xmath23 corresponds to the internal index running from 0 through @xmath24 , with @xmath25 24 , 12 , and 72 elements for the spinor , color ( see section  [ sec:12gauge ] ) , and clover fields respectively ; and @xmath26 is the length of the vector type used ( e.g. , @xmath27 for _ float , float2 , _ and _ float4 _ ) .",
    "we have found that using @xmath28 and @xmath29 is optimal in single and double precision , respectively , each corresponding to a length of 16 bytes .    the field ordering used in quda : @xmath30 numbers are broken up into @xmath31 blocks of @xmath21 short vectors ( @xmath32 numbers ) .",
    "successive threads thus read successive short vectors ensuring coalescing of the memory transfers . within a block the time index runs slowest , implying that the two faces on the temporal boundaries are each contiguous within the block ; each face is stored in @xmath33 vectors .",
    "the blocks are separated by a padding region to avoid partition camping . as an example , in single precision one would use the _ float4 _ vector type ( @xmath28 ) , and thus 6 blocks would be needed to store the @xmath34 numbers that make up a color - spinor .",
    "likewise , in 2-row storage , the gauge field would need 3 blocks to store the @xmath35 numbers needed for each direction @xmath36 . with 4 such directions ,",
    "altogether 12 blocks are needed to store all the link matrices . with the size of the padding chosen to be @xmath37 sites , the ghost zone of link matrices @xmath38 can be hidden entirely in the padding.,width=220 ]    quda follows the usual lattice site assignment for the color matrices .",
    "the color matrix connecting sites @xmath11 and @xmath39 is denoted by @xmath40 and stored at lattice site @xmath11 .",
    "it follows that @xmath41 , which is required for the gather from the backwards direction for site @xmath11 , is stored at site @xmath42 .",
    "( the matrix conjugation is performed at no cost through register relabeling in the kernel . )    as anticipated in section  [ sec : gpus ] , for certain problem sizes performance may be affected by partition camping .",
    "the simple solution quda takes to this problem is to pad the gauge , spinor , and clover fields by one spatial volume , @xmath43 , so that the linear indexing is given by @xmath44 here @xmath45 , @xmath46 , @xmath47 and the @xmath48 are the lengths of the respective spacetime dimensions , with @xmath49 .",
    "although not originally intended for this purpose , padding the fields by an extra spatial volume is also convenient for the parallelization process ( see section  [ sec : multi - gpu ] ) .",
    "we illustrate the field ordering in fig .",
    "[ fig : layout ] .      given the peak instruction and bandwidth throughputs of current gpus ( table [ table : specs ] ) ,",
    "evaluation of the wilson - clover matrix vector product is strongly bandwidth bound .",
    "the approach taken by quda is to minimize memory traffic , even at the expense of additional floating point operations , to accelerate performance using the following techniques :      only the first two rows of the color matrices are stored in device memory , and using unitarity , the third row is reconstructed in registers from the complex conjugate of the cross product of the first two rows .",
    "physically motivated similarity transformations are employed to increase the sparsity of the matrix . in particular",
    ", the spin projectors in the temporal dimension @xmath50 are diagonalized by changing from the conventional chiral basis to a `` non - relativistic '' basis , @xmath51 @xmath52 this has the benefit that only 12 real numbers need be loaded when gathering neighboring spinors in the temporal direction and also aids our parallelization approach ( see section  [ sec : multi - gpu ] ) .",
    "further acceleration is obtained through the use of 16-bit fixed point storage , from here on referred to as half precision .",
    "this is implemented by reading the gauge field and spinor field elements via the texture cache , using the read mode _",
    "cudareadmodenormalizedfloat_. when a texture reference is defined using this mode , a signed 16-bit ( or even 8-bit ) integer read in from device memory will be automatically converted to a 32-bit floating point number in the range @xmath53 $ ] .",
    "this format is immediately suitable for the color matrices since all of their elements lie exactly in this range , as a consequence of unitarity .",
    "the spinors require an extra normalization , which is shared between all elements of a single spinor .",
    "thus in half precision a spinor is stored as 6 _ short4 _ arrays and a single _ float _ normalization array .",
    "the use of mixed - precision iterative refinement for solving linear equations is fairly commonplace on gpus and other architectures where the use of double precision comes with a significant performance penalty .",
    "such an approach allows the bulk of the computation to be performed in fast low precision , with periodic updates in high precision to ensure accuracy of the final solution .",
    "even on architectures where there is parity between peak single and double precision performance , a factor of two difference in memory traffic is unavoidable , and so for bandwidth - bound problems such as our sparse matrix - vector product , the use of mixed precision remains advantageous .",
    "quda uses a variant of reliable updates  @xcite to implement mixed - precision iterative refinement .",
    "this approach has the advantage that a single krylov space is preserved throughout the solve , as opposed to the traditional approach of defect correction which explicitly restarts the krylov space with every correction , increasing the total number of solver iterations  @xcite .",
    "it was found that the best time to solution is typically obtained using either double - half or single - half approaches .",
    "quda provides the additional vector - vector linear algebra ( blas1-like ) kernels needed to implement the linear solvers .",
    "these additional routines take advantage of kernel fusion wherever possible to reduce memory traffic and hence improve performance of the complete solver .",
    "since each of these kernels and their various half , single , and double precision variants may have different optimal cuda parameters ( i.e. , sizes of the thread blocks and the number of blocks treated at once ) , an auto - tuning approach is taken to ensure maximum performance .",
    "all possible combinations of parameters are tested for each kernel , and the optimal values are written out to a header file for inclusion in production code after a recompilation of the library . due to the memory bandwidth intensity of these ( essentially streaming ) kernels , the complete solver typically runs 10 to 20% slower than would the matrix - vector product in isolation .",
    "in parallelizing across multiple gpus , we have taken the simplest approach by only dividing the time dimension , with the full extent of the spatial dimensions confined to a single gpu .",
    "this approach was motivated by the asymmetric nature of the lattice dimensions under study ( @xmath54 and @xmath17 ) , and in order to simplify this initial parallelization . in this form , since we are parallelizing over the slowest running spacetime index , the changes required to the single gpu kernel code were relatively minimal . if one were to attempt to scale to hundreds of gpus or more , multi - dimensional parallelization would clearly be needed to keep the local surface to volume ratio under control . given current lattice sizes , however , such extreme parallelization would imply small local volumes and require rethinking of the fundamental algorithms . work in this direction is underway .    for parallelizing across multiple gpus ,",
    "each gpu can either be controlled using a distinct cpu thread or with a distinct process .",
    "the potential advantage of the threaded approach is that it avoids unnecessary copies within a node ; however , this advantage has decreased on recent cpus that feature integrated memory controllers and much higher memory bandwidth ( compared to pre - nehalem xeons , for example ) , reducing the overhead of an additional local memory copy . to communicate between gpus on different nodes ,",
    "a message passing approach is necessary since the memory space is by definition separate .",
    "while mixed - mode programming is possible ( threads within a node , message passing between the nodes ) , we exclusively used a message passing approach since initial investigations suggested no improvement would be gained from the use of threads .",
    "in particular , we used qmp ( qcd message passing ) which is an api built on top of mpi that provides convenient functionality for lqcd computations  @xcite",
    ".    in parallelizing the action of the wilson - clover matrix onto a spinor field partitioned between @xmath55 distinct gpus , we slice the temporal dimension into @xmath55 equal sized volumes of size @xmath56 . referring to ( [ eq : m ] ) ,",
    "the only part of the matrix that connects different lattice sites is the action of @xmath16 , since the clover matrix @xmath8 is local to a given lattice site . when updating the sites on either end of the local temporal boundary , the adjacent spinors which are on the neighboring gpus are required , as well as the gauge field matrix connecting these sites .",
    "the link matrix @xmath40 connecting sites @xmath11 and @xmath39 is stored at site @xmath11 ; hence the required link matrix for the receive from the forward temporal direction for sites in the last spatial volume ( or timeslice ) will already be present locally , and only the adjacent spinor is required . for the receive from the backward temporal direction into the first timeslice , the required link matrix will be on the adjoining gpu and so must be transferred .",
    "since the link matrices are constant throughout the execution of the linear solver , we transfer the adjoining link matrices in the program initialization . compared to the original single gpu code , this posed the obvious question : where should the extra face ( the ghost zone ) of gauge field matrices be stored ?",
    "given that the fields were already padded by an extra spatial volume , a very natural location is within the padded region since this is exactly the correct size to store the additional gauge field slice ( see fig .",
    "[ fig : layout ] ) . altering the kernel for this change simply required that if the thread i d corresponded to the first timeslice ( local to the gpu ) then the gauge field array indices are set to the padded region .",
    "extra constants were introduced to describe the boundary conditions at the start and end of the local volume , since one of these boundaries might correspond to a global boundary and not just a local boundary .",
    "our initial strategy for storing the transferred faces was to put them in the padded regions of the destination gpu s spinor field . like the gauge field ghost zone",
    "this seems very natural , but it introduced complications into the reduction kernels used in the krylov solvers : these assume a contiguous memory buffer , and so without careful rewriting the ghost zones would be double counted .",
    "the approach we opted for instead was to oversize the spinor fields by the size of the two transferred faces . when doing reductions , this end zone can be simply excluded ensuring correctness . as described in section  [ sec : similar ] the spin projectors in the temporal direction are diagonalized , halving the amount of data that needs to be transferred in the temporal gathering , and so the extra total storage required is actually only @xmath57 components .",
    "the upper 12 spinor components which arise from the receive from the backward direction occupy the first half of the end zone , and the lower 12 spinor components arising from the receive from the forward direction occupy the second half . for half precision the extra normalization constant for each ( 12 component ) spinor is also required , and hence an end zone of size @xmath58 elements is added to the normalization field .",
    "we illustrate the spinor ghost zones and the basic communication requirements in fig .",
    "[ fig : comms ] .    with the ghost zone elements stored in the end zone , extra indexing logic was required to ensure that the correct spinors would be loaded by the threads updating the boundaries .",
    "fortunately , this extra logic introduced minimal overhead since warp divergence is avoided because the number of spatial sites @xmath33 is divisible by the warp size , a condition that is met by the lattice dimensions under consideration here ( and all production lqcd calculations that we aware of ) .      spinor ghost zones and communication steps : we show the source spinor on the sending device ( top ) assuming @xmath28 , corresponding to 6 blocks from fig .",
    "[ fig : layout ] .",
    "the grey buffers at the end correspond to the ghost zones .",
    "the top 3 blocks correspond to the @xmath59 projected components , while the lower 3 blocks nearer the ghost zone correspond to @xmath60 .",
    "data from the back faces ( green ) needs to be gathered into a communications buffer on the sending host and likewise for the forward face ( orange ) .",
    "the faces are then transferred to the receiving host via qmp / mpi .",
    "once transferred the faces are transferred to the ghost zones on the receiving device ( bottom of diagram ) , which then uses the data directly from the ghost zones , hence the corresponding faces have been greyed out.,width=288 ]      the first and simplest approach to parallelization is to perform all of the communications up front and then do the computation for the entire volume in a single kernel .",
    "the device - to - host transfers are achieved through the use of separate _ cudamemcpy _ calls ( one for each face block ) , with half precision requiring an extra _ cudamemcpy _ for the face of the normalization array . once on the host ,",
    "all of these blocks are contiguous in memory , allowing for a single message passing in each direction .",
    "the received faces are sent to the device using a single _ cudamemcpy _ for each face ( with an extra _ cudamemcpy _ required for each of the normalization faces in half precision ) and placed in the end zone of the spinor field . finally the wilson - clover kernel is executed .",
    "our second implementation aimed to overlap all of the communication with the computation of the internal volume .",
    "to do so , the cuda streaming api was used , which allows for a cuda kernel to execute asynchronously on the gpu at the same time that data is being transferred between the device and host using _",
    "additionally this makes use of non - blocking mpi communication possible : after the backward face has been transferred to the host , the mpi exchange of this face to its neighbor is overlapped with the transfer of the forward face from device to host . in turn , when the first face has been received , this can be sent to the device while the second face is being communicated .",
    "this approach requires three cuda streams : one to execute the kernel on the internal volume , one for the face send backward / receive forward , and one for the face send forward / receive backward .",
    "an additional required step is that the streams responsible for gathering the faces to the host must be synchronized , using _",
    "cudastreamsynchronize _ , before message",
    "passing can take place to ensure transfer completion . in principle , we could also overlap the host - to - device transfer of the second face and the computation involving the first face . this would yield a minimal speedup at best , since the time spent executing the face kernel is not the limiting factor , and it may actually reduce overall performance since the kernel would be updating half as many sites at a time , reducing parallelism and potentially decreasing kernel efficiency .      aside from the parallelization of the sparse matrix vector product , the",
    "only other required addition to the code was the insertion of mpi reductions for each of the linear algebra reduction kernels .",
    "our numerical experiments were carried out on the `` 9 g '' cluster at jefferson laboratory .",
    "this cluster is made up of 40 nodes containing 4 gpus each , as well as an additional 16 nodes containing 2 gpu devices each that are interconnected by qdr infiniband on a single switch . in this study",
    ", we focused our attention primarily on the partition made up of the 16 infiniband connected nodes , with one or two exceptions .",
    "the nodes themselves utilize the supermicro x8dtg - qf motherboard populated with two intel xeon e5530 ( nehalem ) quad - core processors running at 2.4 ghz , 48 gib of main memory , and two nvidia geforce gtx 285 cards with 2 gib of device memory each .",
    "the nodes run the centos 5.4 distribution of linux with version 190.29 of the nvidia driver .",
    "the quda library was compiled with cuda 2.3 and linked into the chroma software system using the red hat version 4.1.2 - 44 of the gcc / g++ toolchain .",
    "communications were performed using version 2.3.2 of the qcd message passing library ( qmp ) built over openmpi 1.3.2 . in all our tests we ran in a mode with one mpi process bound to each gpu .",
    "the numerical measurements were taken from running the chroma propagator code and performing 6 linear solves for each test ( one for each of the 3 color components of the upper 2 spin components ) , with the quoted performance results given by averages over these solves .",
    "statistical errors were also determined but are generally too small to be seen clearly in the figures .",
    "importantly , all performance results are quoted in terms of `` effective gflops '' that may be compared with implementations on traditional architectures . in particular",
    ", the operation count does not include the extra work done to reconstruct the third row of the link matrix .",
    "we carried out both strong and weak scaling measurements .",
    "the strong scaling measurements used lattice sizes of @xmath61 and @xmath62 sites respectively . both the lattice sizes and",
    "the wilson - clover matrix had their parameters chosen so as to correspond to those in current use by the _ anisotropic clover _",
    "analysis program of the hadron spectrum collaboration .",
    "the lattices used were _ weak field _ configurations .",
    "such configurations are made by starting with all link matrices set to the identity , mixing in a small amount of random noise , and re - unitarizing the links to bring the links back to the @xmath63 manifold .",
    "we emphasize that while these lattices were not physical , we have tested the code on actual production lattices on both the volumes mentioned for correctness .",
    "the concrete physical parameters do not affect the rate at which the code executes but control only the number of iterations to convergence in the solver .",
    "the weak scaling tests utilized local lattice sizes of @xmath64 and @xmath65 sites per gpu , respectively .    the solver we employed was the reliably updated bicgstab solver discussed in @xcite .",
    "we ran the solver in single precision and mixed single - half precision with and without overlapped communications in the linear operator .",
    "for the lattices with @xmath66 spatial sites , we also ran the solver in uniform double precision and in mixed double - half precision modes . when run in single or single - half mixed precision modes",
    "the target residuum was @xmath67 , whereas in the double precision and mixed double - half precision modes the residuum was @xmath68 .",
    "in addition , the delta parameter was set to @xmath69 in single , @xmath70 in mixed single - half , @xmath71 in double and @xmath72 in the mixed double - half modes of the solver respectively",
    ". the meanings of these parameters are explained fully in @xcite .",
    "+    our results for weak scaling are shown in fig .",
    "[ fig : weak - scale ] .",
    "we see near linear scaling on up to 32 gpus in all solver modes . in the case with @xmath64 sites per gpu",
    ", we were unable to fit the double precision and mixed double - half precision problems into device memory , and hence we show only the single and single - half data . in the case with local volume of @xmath73",
    "we show also double precision and mixed double - half precision data .",
    "it is gratifying to note that the mixed double - half precision performance of fig .  [ fig : weak - scale](b ) is nearly identical to that of the single - half precision case .",
    "both mixed precision solvers are substantially more performant than either the uniform single or the uniform double precision solver .",
    "we note that for lattices with @xmath74 sites per gpu we have reached a performance of 4.75 tflops .",
    "+   +    strong scaling results for the @xmath75 lattice in single precision , double precision , single - half mixed precision , and double - half mixed precision .",
    "we used the solver that did not overlap computations and communications for these results , since as shown in fig .",
    "[ fig : strong - scale ] it was faster than the overlapped solver for the @xmath75 lattice in single and mixed single - half precisions.,width=336 ]    fig .",
    "[ fig : strong - scale ] shows our strong scaling results . in fig .",
    "[ fig : strong - scale](a ) we show the data for the lattices with @xmath76 sites .",
    "we see a clear deviation from linear scaling as the number of gpus is increased and the local problem size per gpu is reduced .",
    "this is not unexpected , since as the number of gpus is increased the faces represent a larger fraction of the overall work .",
    "the improvement from overlapping communication with computation is increasingly apparent as the number of gpus increases .",
    "the benefits of mixed precision over uniform single precision can clearly be seen . however , we note that performing the mixed precision computation comes with a penalty in terms of memory usage : the mixed precision solver must store data for both the single and half precision solves , and this increase in memory footprint means that at least 8 gpus are needed to solve this system .",
    "the uniform single precision solver requires only the single precision data and can be solved ( at a performance cost ) already on 4 gpus .",
    "we highlight the fact that the 32 gpu system is made up of 16 cluster nodes , which themselves contain 128 nehalem cores .",
    "we have performed a solution of this system on the jefferson lab `` 9q '' cluster , which is identical in terms of cores and infiniband networking but does not contain gpus . on a 16-node partition of the `` 9q '' cluster we obtained 255 gflops in single precision using highly optimized sse routines , which corresponds to approximately 2 gflops per cpu core . in our parallel gpu computation , on 16 nodes and 32 gpus we sustained over 3 tflops which is over a factor of 10 faster than observed without the gpus .",
    "[ fig : strong - scale](b ) shows our strong scaling results for the lattice with @xmath61 sites .",
    "this lattice has half the time extent of the larger lattice , and thus we expect strong scaling effects to be noticeable at smaller gpu partitions than in the previous case .",
    "further , the spatial volume is a factor of @xmath77 smaller for the @xmath61 lattices than for the larger case .",
    "we were surprised that the trend in our results is different from that in fig .",
    "[ fig : strong - scale](a ) .",
    "notably , in this case we seem to gain little from overlapping communication and computation in the mixed precision solver .",
    "indeed , for more than 8 gpus the mixed precision performance reaches a plateau and is surpassed even by the purely single precision case .",
    "we believe this dropoff in the strong scaling is due to additional overheads incurred in overlapping communications with computations arising from system and driver issues .",
    "we will return to this point in section  [ sec : system ] , where we discuss latency microbenchmarks , but suffice it to say that using _",
    "cudamemcpyasync _ appears to have a higher latency than _",
    "cudamemcpy_. this may be a feature of our motherboard or the version of the nvidia driver we are using . in the case of the @xmath78 lattice , probably the volume in the body is large enough to hide this extra latency . in the case of the @xmath75 lattice ,",
    "our data suggests that the local volume may be sufficiently small that the overhead of setting up the asynchronous transfers dominates and that in this instance the lower latency of synchronous _ cudamemcpy _ calls can result in better performance .",
    "[ fig : strong - scale24 ] shows the strong scaling data for various precision combinations for the @xmath75 lattice , where we now include uniform double and mixed double - half precision results and do not overlap communication with computation .",
    "again we see that the mixed precision solvers employing half precision outperform both single and double uniform precision solvers .",
    "note that uniform double precision exhibits the best strong scaling of all because this kernel is less bandwidth bound due to the much lower double precision peak performance of the gtx 285 ( see table [ table : specs ] ) .",
    "the pci - e architecture in our supermicro nodes was such that the two gpu devices were each on a bus with a direct connection to a separate socket on the motherboard . in our tests we launched two mpi processes per node . in order to obtain maximum bandwidth on the buses , it was necessary to explicitly bind each mpi process to the correct socket .",
    "we accomplished this using the processor affinity feature of openmpi .    in fig .",
    "[ fig : strong - scale](a ) we show the performance a deliberately badly chosen numa configuration ( with maroon x - symbols ) .",
    "we bound each process to the opposite socket from the cuda device it was using .",
    "one can see that the performance is noticeably lower than the correctly bound case denoted by blue asterisks in fig .",
    "[ fig : strong - scale](a ) .    secondly ,",
    "as alluded to previously , we note that on these nodes the latencies of _ cudamemcpy _ ( used in the non - overlapped communication code ) and of _ cudamemcpyasync _ ( followed immediately by a _ cudasynchronizethread _ ) call are quite different .",
    "latency microbenchmark showing tranfer times from host to device or vice versa for messages of varying sizes .",
    "we show data for : device to host using _ cudamemcpy _ ( black ) , host to device using _ cudamemcpy _ ( red ) , device to host using _ cudamemcpyasync _ + _ cudasynchronizethreads _ ( green ) and host to device using _",
    "cudamemcpyasync_+_cudasynchronizethreads _ ( blue ) .",
    "the timings are taken over 500,000 message transfers , width=336 ]    as shown in fig .",
    "[ fig : latency ] , using _ cudamemcpyasync",
    "_ incurs a latency of just under 50 microseconds whereas a synchronous _ cudamemcpy _ has a much shorter latency of 11 microseconds .",
    "it can also be seen that once out of the latency limited region , the graphs show different gradients for the host - to - device and device - to - host transfers , indicating different host - to - device and device - to - host bandwidths .",
    "these features may depend somewhat on the version of the nvidia driver and motherboard bios used , but additional testing so far suggests that the main culprit is a hardware limitation in the early revision of the intel 5520 ( tylersburg ) chipset used in the nodes",
    ". therefore the decision on whether to overlap communication and computation or not may depend on the system under consideration , as well as the problem size .",
    "we have demonstrated what we believe is the first successful attempt to use multiple gpu units in parallel for lqcd computations .",
    "we have weak scaled our application to 4.75 tflops on 32 gpus and have strong scaled the application , on a problem size of scientific interest , to over 3 tflops . in this latter case , we have achieved over a factor of 10 increase in performance compared to not using gpus ( 255 gflops on a `` regular '' cluster partition containing the same number processors ) .",
    "we believe that the order of magnitude increase in computing power is an enabling technology for sophisticated modern analysis methods of great interest to particle and nuclear physics . indeed the solver we have described is now in use in production lqcd calculations of the spectrum of hadrons using the technique of _ distillation _ @xcite .",
    "current calculations use lattice configurations of the same size as described in section  [ sec : solver - perf ] which were generated on leadership computing platforms under doe incite and nsf teragrid allocations ( granted to the usqcd and hadron spectrum collaborations , respectively ) .",
    "the calculations involve 32768 calls to the solver for each configuration and benefit enormously from the speedup delivered by the gpu solver .",
    "prior to parallelizing the quda library , our larger volume dataset was not amenable to solution on gpus due to memory constraints .",
    "the use of multiple gpus allows the solution to proceed , realizing the large increases in cost effectiveness promised by gpus .",
    "a slightly more nuanced point is that the nodes containing 4 gpus ( and no infiniband ) may now be more efficiently utilized .",
    "prior to parallelization , one could solve the @xmath79 problem on a single gpu and analyze two configurations simultaneously on a single node .",
    "one could not analyze more , due to the limitations on the host ( primarily memory capacity ) .",
    "now one can analyze 2 configurations simultaneously using 2 gpus each , optimally utilizing all 4 gpus in the node .",
    "the exact optimization of a node configuration in terms of infiniband cards , gpus , and operating model is an interesting issue but is beyond the scope of this paper .",
    "there are many avenues for future exploration .",
    "currently only the solvers have been accelerated in the quda library .",
    "parallelization onto multiple gpus may make gauge generation on gpu clusters an interesting and desirable possibility .",
    "we are also interested in porting more modern algorithms to the gpus such as the adaptive multigrid solver discussed in @xcite to speed up computations even further .",
    "we follow the development of the opencl standard with interest with a view to potentially harness gpu devices from amd as well as nvidia , and we await future hardware and software improvements to allow better coexistence of gpus and message - passing ( such as sharing pinned memory regions between cuda and mpi ) . finally , we hope that the lessons learned from gpus will be usefully applicable on heterogeneous systems in general as we head towards the exascale .",
    "the authors would like to thank chip watson for funding an extremely productive week of coding , and for dedicated access to the jefferson lab 9 g cluster .",
    "enlightening discussions with jie chen , paulius micikevicius , and guochun shi are also gratefully acknowledged .",
    "this work was supported in part by u.s .",
    "nsf grants phy-0835713 and oci-0946441 and u.s .",
    "doe grant de - fc02 - 06er41440 .",
    "computations were carried out on facilities of the usqcd collaboration at jefferson laboratory , which are funded by the office of science of the u.s .",
    "department of energy . authored by jefferson science associates , llc under u.s .",
    "doe contract no .",
    "de - ac05 - 06or23177 .",
    "the u.s .",
    "government retains a non - exclusive , paid - up , irrevocable , world - wide license to publish or reproduce this manuscript for u.s .",
    "government purposes .",
    "r.  babich , r.  brower , m.  clark , g.  fleming , j.  osborn , and c.  rebbi , `` strange quark content of the nucleon , '' _ proc .",
    "science _ ( lattice2008 ) , 2008 , 160 [ arxiv:0901.4569 [ hep - lat ] ] . m.  peardon _ et al . _ [ hadron spectrum collaboration ] , `` a novel quark - field creation operator construction for hadronic physics in lattice qcd , '' _ phys .",
    "d _ , vol .",
    "80 , 2009 , p.  054506",
    "[ arxiv:0905.2160 [ hep - lat ] ] .",
    "m.  a.  clark , r.  babich , k.  barros , r.  c.  brower , and c.  rebbi , `` solving lattice qcd systems of equations using mixed precision solvers on gpus , '' _ comput .",
    "_ , vol .  181 , 2010 , p.  1517 .",
    "[ arxiv:0911.3191 [ hep - lat ] ] .",
    "r.  g.  edwards and b.  joo [ scidac collaboration and lhpc collaboration and ukqcd collaboration ] , `` the chroma software system for lattice",
    "qcd , '' _ nucl .",
    "_ , vol .  140 , 2005 , p.  832",
    "[ arxiv : hep - lat/0409003 ] . b.  sheikholeslami and r.  wohlert , `` improved continuum limit lattice action for qcd with wilson fermions , '' _ nucl .  phys .",
    "b _ , vol .  259 , 1985 , p.  572 .",
    "m.  r.  hestenes and e.  stiefel , `` methods of conjugate gradients for solving linear systems '' , _ j. of research of the national bureau of standards _ , vol .",
    "49 , no .  6 , 1952 , p.  409 .",
    "g.  i.  egri , z.  fodor , c.  hoelbling , s.  d.  katz , d.  nogradi , and k.  k.  szabo , `` lattice qcd as a video game , '' _ comput .  phys .",
    "_ , vol .  177 , 2007 , p.  631",
    "[ arxiv : hep - lat/0611022 ] .",
    "k.  barros , r.  babich , r.  brower , m.  a.  clark , and c.  rebbi , `` blasting through lattice calculations using cuda , '' _ proc .",
    "science _ ( lattice2008 ) , 2008 , 045 [ arxiv:0810.5365 [ hep - lat ] ] .",
    "k.  z.  ibrahim and f.  bodin , and o.  pene , `` fine - grained parallelization of lattice qcd kernel routine on gpu '' , _",
    "j.  of parallel and distributed computing _",
    "68 , no .  10 , 2008 , pp . 13501359 .",
    "f.  belletti _ et al .",
    "_ , `` qcd on the cell broadband engine , '' _ proc .  science _ ( lat2007 ) , 2007 , 039 [ arxiv:0710.2442 [ hep - lat ] ] . h.  baier _ et al .",
    "_ , `` qpace  a qcd parallel computer based on cell processors , '' _ proc .  science _ ( lat2009 ) , 2009 , 001 [ arxiv:0911.2174 [ hep - lat ] ] .",
    "g.  shi , v.  kindratenko , and s.  gottlieb , `` cell processor implementation of a milc lattice qcd application , '' _ proc .",
    "science _ ( lattice2008 ) , 2008 , 026 [ arxiv:0910.0262 [ hep - lat ] ] . j.  spray , j.  hill , and a.  trew , `` performance of a lattice quantum chromodynamics kernel on the cell processor , '' _ comput .",
    "_ , vol .  179 , 2008 , p.  642",
    "[ arxiv:0804.3654 [ hep - lat ] ] .",
    "j.  stuart and j.  d.  owens , `` message passing on data - parallel architectures , '' _ proc .  the 23rd ieee international parallel and distributed processing symposium , rome , italy _ , 2009 .",
    "j.  j.  dudek , r.  g.  edwards , m.  j.  peardon , d.  g.  richards , and c.  e.  thomas , `` toward the excited meson spectrum of dynamical qcd , '' 2010 [ arxiv:1004.4930 [ hep - ph ] ] .",
    "j.  brannick , r.  c.  brower , m.  a.  clark , j.  c.  osborn , and c.  rebbi , `` adaptive multigrid algorithm for lattice",
    "qcd , '' _ phys .",
    "_ , vol .  100 , 2008 , 041601 [ arxiv:0707.4018 [ hep - lat ] ]"
  ],
  "abstract_text": [
    "<S> graphics processing units ( gpus ) are having a transformational effect on numerical lattice quantum chromodynamics ( lqcd ) calculations of importance in nuclear and particle physics . </S>",
    "<S> the quda library provides a package of mixed precision sparse matrix linear solvers for lqcd applications , supporting single gpus based on nvidia s compute unified device architecture ( cuda ) . </S>",
    "<S> this library , interfaced to the qdp++/chroma framework for lqcd calculations , is currently in production use on the `` 9 g '' cluster at the jefferson laboratory , enabling unprecedented price / performance for a range of problems in lqcd . </S>",
    "<S> nevertheless , memory constraints on current gpu devices limit the problem sizes that can be tackled . in this contribution </S>",
    "<S> we describe the parallelization of the quda library onto multiple gpus using mpi , including strategies for the overlapping of communication and computation . </S>",
    "<S> we report on both weak and strong scaling for up to 32 gpus interconnected by infiniband , on which we sustain in excess of 4 tflops .    </S>",
    "<S> = 1 </S>"
  ]
}