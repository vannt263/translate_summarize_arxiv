{
  "article_text": [
    "this work was supported by nasa grant nnx10ah15 g and nsf award 0855407 ."
  ],
  "abstract_text": [
    "<S> theoretical studies in gravitational wave astronomy often require the calculation of fisher information matrices and likelihood functions , which in a direct approach entail the costly step of computing gravitational waveforms . here </S>",
    "<S> i describe an alternative technique that sidesteps the need to compute full waveforms , resulting in significant computational savings . </S>",
    "<S> i describe how related techniques can be used to speed up bayesian inference applied to real gravitational wave data .    over the past two decades </S>",
    "<S> there have been literally hundreds of papers written describing parameter estimation studies in gravitational wave astronomy ( we have to something to pass the time while waiting for the first detection ) . </S>",
    "<S> see refs .  </S>",
    "<S> @xcite for some important early examples . </S>",
    "<S> the set - up is as follows : a waveform family @xmath0 describing the radiation produced by some potential source of gravitational waves is chosen , and the detector response function is specified . from these inputs </S>",
    "<S> the waveform templates @xmath1 describing the signals produced in the detector for model parameters @xmath2 can be computed . </S>",
    "<S> the output of the detector - or a network of detectors , it makes no difference - can then be written as @xmath3 , where @xmath4 is the instrument noise . for theoretical studies </S>",
    "<S> it is usually assumed that the noise is gaussian with a colored spectrum @xmath5 , making it advantageous to shift the analysis to the fourier domain where the noise samples are uncorrelated . </S>",
    "<S> the question is then asked , how well can the parameters @xmath2 be constrained by the data ? the answer follows from considering how well waveforms @xmath6 with parameters @xmath7 are able to fit the data . </S>",
    "<S> the goodness of fit is found by taking the squared difference between the model and data , scaled by the noise level : @xmath8 where the brackets denote the standard noise weighted inner product @xmath9 the likelihood that the data would arise from a signal with parameters @xmath7 is then  @xcite @xmath10 where @xmath11 is a constant that does not depend on the signal . </S>",
    "<S> the posterior probability , @xmath12 , is simply the product of the prior probability , @xmath13 , and the likelihood , @xmath14 , divided by an overall normalization factor ( the evidence ) . </S>",
    "<S> notice that i never mentioned mr . </S>",
    "<S> wiener or matched filtering . </S>",
    "<S> gravitational wave data analysis is no different than any other model fitting procedure : you compute a goodness of fit and turn the crank ( but to confound astronomers in other fields we better keep on talking about matched filtering ) . </S>",
    "<S> whatever you do , do nt start referring to the analysis as homodyne detection or the radio astronomers will descend on our field like a plague of locusts .    </S>",
    "<S> the parameter recovery accuracy is computed by looking at contours of the posterior . for strong signals </S>",
    "<S> the central limit theorem says that the posterior distribution is well approximated by a multivariate gaussian distribution : @xmath15 where @xmath16 is the fisher information matrix and @xmath17 . </S>",
    "<S> it follows that the fisher matrix is given by the expectation value of the negative hessian of the log posterior density : @xmath18 for sufficiently large signal - to - noise ratios , the variance - covariance matrix @xmath19 is given by the inverse of the fisher information matrix . </S>",
    "<S> better error estimates can be found by directly estimating the posterior distribution function using markov chain monte carlo  @xcite or nested sampling  @xcite techniques . </S>",
    "<S> either way , these parameter estimation studies require that we compute a large number of noise weighted inner products , @xmath20 , which would seem to imply that we need to compute a large number of waveforms . </S>",
    "<S> but that turns out not to be the case .    </S>",
    "<S> suppose that we want to map out the posterior using a markov chain or nested sampling . </S>",
    "<S> the first thing to realize is that you do not need to add simulated noise to the data  @xcite . </S>",
    "<S> this will only push the recovered recovered parameters off from the injected parameters in a way that depends on the particular noise realization ( later on i will explain how to generalize the approach to handle data with noise ) . </S>",
    "<S> what we are really interested in is the allowed spread in recovered parameters , and that is set by the noise level @xmath5 in the noise weighted inner product . </S>",
    "<S> so the quantity we need to compute is @xmath21 where i m using the shorthand @xmath22 and @xmath23 . </S>",
    "<S> suppose for the moment that we happen to have stationary phase approximation waveforms for @xmath24 and @xmath25 , we ll deal with time domain waveforms in a moment . </S>",
    "<S> writing @xmath26 and @xmath27 we get @xmath28 where it is understood that all of the quantities are frequency dependent . </S>",
    "<S> the first two terms in the integrand are always slowly varying function of frequency , and these terms can be integrated using a small number of function calls . </S>",
    "<S> the oscillatory term that comes from @xmath29 deserves more attention . </S>",
    "<S> so long as we are near maximum likelihood , the phase difference @xmath30 will be a slowly vary function of @xmath31 , as will @xmath32 , and once again the integral can be computed to the desired accuracy with very few function calls . </S>",
    "<S> as we move away from maximum likelihood the phase difference grows and evaluating the integral takes more function calls . </S>",
    "<S> but we are not very interested in regions with low likelihood . </S>",
    "<S> it is simple to show that the variance of @xmath33 is equal to the dimension @xmath34 of the parameter space , and it follows that a markov chain will rarely accept moves to places with @xmath35 . </S>",
    "<S> even there the phase evolution is not large , and the likelihood calculation remains inexpensive . </S>",
    "<S> of course , moves will be proposed to locations with low likelihoods , and these locations do lead to very rapid oscillations in the @xmath29 terms . which is why we simply set @xmath36 whenever the phase change across the band exceeds some threshold ( say tens of radians ) . </S>",
    "<S> the results of a mcmc or nested sampling analysis done in this way is indistinguishable from what you get when computing the likelihood directly .    </S>",
    "<S> the same technique can be used to compute the fisher matrix if we write @xmath37 with ( no sum on @xmath38 ) @xmath39 expanded out , the numerical central differences in equation ( [ fish1 ] ) lead to @xmath40 inner products that have to be evaluated . </S>",
    "<S> since the @xmath41 are by definition small , the integrands are all slowly varying and can be computed to the necessary accuracy with a small number of function evaluations . </S>",
    "<S> the calculation is even simpler if we work directly with the derivatives of the amplitude and phase : @xmath42 again , all the quantities in the integrand are slowly varying functions of frequency , and the integrand can be computed at little cost . </S>",
    "<S> this method of evaluating fisher matrices has the added advantage that it is numerically far more stable than directly taking derivatives of the waveforms  @xcite .    for the methods i have described to be useful the signals must have a discrete collection of instantaneous frequency components . when this condition is met , it is usually possible to derive stationary phase approximation waveforms  @xcite . in some instances </S>",
    "<S> the orbital timescale of the detector is comparable in duration to the gravitational wave signal ( _ e.g. _ most lisa , decigo or einstein telescope sources , signals from deformed neutron stars for ligo / virgo ) , and it is necessary to compute a stationary phase approximation to the detector response as well  @xcite . </S>",
    "<S> these calculations are straightforward , and including finite arm length effects is only a minor complication .    </S>",
    "<S> we can avoid the additional step of computing stationary phase approximations to the signal by invoking parseval s theorem and working directly in the time domain : @xmath43 where the instantaneous frequency is given by the time derivative of the phase : @xmath44 . here </S>",
    "<S> i am assuming that the signal can be written in the form @xmath45 , where @xmath46 is a slowly varying amplitude ( in other words , the same condition that is required for the stationary phase approximation to work ) . using the product formula : @xmath47 , and dropping the rapidly oscillating term involving the sum of the phases </S>",
    "<S> , we have @xmath48 again , for small phase differences , all the terms in the integrand are slowly varying , and the integral can be computed with a small number of function calls . </S>",
    "<S> fisher matrix elements can be computed using the the time domain analog of equation ( [ fish ] ) .    in many cases </S>",
    "<S> the gravitational wave signals have power spread over multiple harmonics . for a fisher matrix analysis </S>",
    "<S> it is enough to simply add together the contributions from each harmonic , but for likelihood calculations there is the possibility that different harmonics might overlap  @xcite . to account for this possibility we need to compute @xmath49 where the @xmath50 label the harmonics . </S>",
    "<S> only those terms with small phase differences need to be computed .    </S>",
    "<S> the techniques described here are applicable to analytic or semi - analytic waveform models such as those derived using the post - newtonian  @xcite or effective one body  @xcite approaches . </S>",
    "<S> these waveforms may require the numerical integration of energy fluxes and/or spin precession equations . </S>",
    "<S> an effective strategy for handling these cases is to include the calculation of the noise weighted inner products as differential equations , @xmath51 , and to solve the couple set of differential equations using a high order adaptive solver . </S>",
    "<S> this approach works very well for spinning systems since the precession timescale sets the amplitude evolution timescale . because the orbital decay and spin precession timescales are generally much slower than the orbital timescale , there are substantial savings </S>",
    "<S> to be had from computing the likelihood and fisher matrix elements in this way . </S>",
    "<S> if the evolution equations happen to be `` stiff '' , and require small step sizes , it may be advantageous to compute the derivatives directly . </S>",
    "<S> for example , rather than integrating the equation @xmath52 for different values of @xmath2 to estimate the derivatives @xmath53 , it is usually faster and more accurate to integrate the equations @xmath54    there are many applications for these techniques outside of theoretical parameter estimation studies . </S>",
    "<S> while the techniques described ( so far ) are not directly applicable when instrument noise is present , they can play a role in setting up template grids or driving stochastic searches . </S>",
    "<S> for example , in mcmc style searches it is important to have well chosen proposal distributions . </S>",
    "<S> the best are those that closely approximate the posterior distribution . </S>",
    "<S> the fisher information matrix provides a local approximation to the posterior , and multi - variate normal distributions of the form ( [ multi ] ) have proven useful as proposal distributions  @xcite . </S>",
    "<S> however , if the fisher matrix computation uses the full waveforms , it costs @xmath55 times more to compute than the likelihood , and at that point you are better off using a cheaper proposal distribution and taking more steps in the chain . on the other hand , </S>",
    "<S> if the fisher matrix elements can be computed at a fraction of the cost of the full waveforms , it makes sense to use ( [ multi ] ) as a proposal distribution . </S>",
    "<S> now suppose that the search has found a mode of the posterior at @xmath2 , which may or may not be the primary mode . to fully explore all the modes it helps to have a crude map of the global structure of the posterior . </S>",
    "<S> we can generate such a map at moderate computational cost . </S>",
    "<S> the procedure is to run a new mcmc search using the noise - free chi - squared @xmath56 , which can be computed for a fraction of the cost of the true chi - squared . </S>",
    "<S> if @xmath1 happens to correspond to a secondary mode of the true posterior , one of the secondary modes of the approximate posterior will correspond to the primary mode of the true posterior . </S>",
    "<S> thus , once the original search finds any hint of the signal , be it a secondary or tertiary mode , we can develop a map that points us towards the primary mode . because noise will push the true posterior distribution away from the noise - free approximation , it is a good idea to temper the approximate posterior : @xmath57 using some `` temperature '' @xmath58 . </S>",
    "<S> increasing the temperature flattens the distribution , and we have to find a balance between making the approximate map uninformative @xmath59 , and being overly prescriptive @xmath60 . </S>",
    "<S> temperatures in the range @xmath61 $ ] have been found to work quite well .    </S>",
    "<S> the same techniques used to speed up the likelihood and fisher matrix calculations can also be used to speed up the computation of the template metrics  @xcite @xmath62 for a grid based search . </S>",
    "<S> this should prove especially useful in high dimensional spaces where random template placement algorithms are required  @xcite . </S>",
    "<S> in particular , fast metric calculations are needed for hybrid grid - mcmc searches  @xcite where the physical priors are replaced by @xmath63 . </S>",
    "<S> this hybrid search is equivalent to a random grid search when no signals are present , which ensures full coverage of the search space , and has the advantage of being significantly faster than a grid search when signals are present ( the saving grows with the signal strength ) .    </S>",
    "<S> in essence , the time saving techniques that i have described all amount to heterodyning of the data . </S>",
    "<S> that is , if we have signals @xmath1 and @xmath6 that differ by a small phase difference , their product yields a low frequency beat signal plus a high frequency signal that can be discarded without loss of information . </S>",
    "<S> the frequencies of the signals do not have to be constant for heterodyning to work . </S>",
    "<S> heterodyning has been used in the search for gravitational wave signals from known radio pulsars  @xcite , and to simulate lisa observations of white dwarf binaries  @xcite . </S>",
    "<S> what apparently has not been realized before is that heterodyning can be used to significantly speed up mcmc and nested sampling explorations of the posterior for signals embedded in noisey instrument data . </S>",
    "<S> suppose that the primary or a secondary mode of the posterior @xmath2 has been located by some search algorithm and you would now like to fully map out the posterior distribution . rather than work with the full signal @xmath64 , first fourier </S>",
    "<S> transform the data , whiten using the noise spectral density @xmath5 , and heterodyne using the carrier phase @xmath65 . </S>",
    "<S> the high frequency components of the data can now be thrown away , and the noise weighted inner products can be computed using templates that are heterodyned against the carrier phase . </S>",
    "<S> the bandwidth of the heterodyned signal that needs to be kept depends on the details of the analysis , but the data volume will typically be reduced by many orders of magnitude . to gain the full benefit from this approach </S>",
    "<S> the heterodyned templates must be computed directly at low cadence using the phase difference @xmath66 ( or the equivalent in the time domain ) . </S>",
    "<S> note that carrier phase that beats with the signal at the primary maximum will also beat with the signal at the secondary maxima . </S>",
    "<S> the likelihoods computed far from the maxima will not agree with those computed using the full signal , but if the heterodyned signal is given sufficient bandwidth , the differences will be small in regions with noticable posterior weight .     and @xmath67 . </S>",
    "<S> , title=\"fig:\",scaledwidth=50.0% ]   and @xmath67 . , title=\"fig:\",scaledwidth=50.0% ]    as a concrete example , consider a binary neutron star inspiral that might be observed by the ligo / virgo detectors . the gravitational wave signal in each detector can be written as @xmath68 the antenna patterns @xmath69 , @xmath70 depend on the sky location and polarization angle . </S>",
    "<S> the amplitudes @xmath71 and @xmath72 depend on the merger time , the orbital inclination , the masses and the distance to the binary . </S>",
    "<S> the phase @xmath73 depends on the sky location ( which determines the time delay between the signals seen at the different sites ) , merger time and the masses . </S>",
    "<S> here i am using 2-pn waveforms without spin . </S>",
    "<S> the heterodyning can be done in the time domain by multiplying the signal by @xmath74 and @xmath75 , where @xmath76 is the reference , or carrier phase corresponding to the best fit source parameters . for a @xmath77-@xmath78 inspiral , the original waveforms reached a maximum frequency of @xmath79 hz . </S>",
    "<S> the signal was whitened and heterodyned , then lowpass filtered to a maximum of 10 hz . </S>",
    "<S> low cadence templates for the sine and cosine streams can then be generated : @xmath80 where @xmath81 . </S>",
    "<S> figure  [ fig ] shows the whitened and heterodyned time domain signal and noise for a neutron star binary with network @xmath82 . </S>",
    "<S> we used the injected source parameters to generate the carrier phase @xmath73 . </S>",
    "<S> also shown is a template generated using ( [ template ] ) with the masses shifted by @xmath83 and @xmath67 . the noise weighted inner products are computed directly in the time domain : @xmath84 for the case shown , @xmath85 , @xmath86 , @xmath87 , , @xmath88 , and @xmath89 . the inner products computed using the full cadence data agree with those computed using the heterodyned data to @xmath90 . </S>",
    "<S> the only difference is that the heterodyned inner products can be computed one thousand times faster .    </S>",
    "<S> hopefully the techniques i have described will help reduce the high computational cost of bayesian approaches to gravitational wave data analysis , and allow their wider adoption . </S>"
  ]
}