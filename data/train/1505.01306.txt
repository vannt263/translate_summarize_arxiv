{
  "article_text": [
    "knowledge bases such as wikipedia , yago or wordnet are used in many applications as a source of knowledge  @xcite .",
    "one of these applications is query expansion , which is the process of expanding a query issued by a user , introducing new terms , called expansion features , in order to improve the quality of the retrieved results .",
    "query expansion is motivated by the assumption that the query introduced by the user is not the best to express its real intention .",
    "for example , _ vocabulary mismatch _ between queries and documents is one of the main causes of a poor precision in information retrieval systems  @xcite .",
    "poor results also arise from the _ topic inexperience _ of the users .",
    "users searching for information are often not familiar with the vocabulary of the topic in which they search , and hence , they may not use the most effective keywords .",
    "this leads to the loss of important results due to the lack of precision when choosing the query terms .",
    "thus , the challenge is to properly select the best expansion features ( terms added to the original query ) , that improve the most the quality of the results .",
    "note also that a bad choice of expansion features may be counterproductive .",
    "in this paper we focus on wikipedia as a knowledge base for query expansion .",
    "wikipedia is a popular encyclopedia which contains a large amount of rich data and thus , its topic coverage is extremely broad .",
    "the structure of wikipedia has been used for query expansion in different ways . in  @xcite",
    "the authors describe different information extraction strategies by using the individual links of each wikipedia article , without going deeper into further relationships . in  @xcite",
    "the authors borrow a social network community detection metric  @xcite to extract better expansion features from wikipedia , assuming that a structure as simple as a transitive relation is sufficient to capture good relationships among terms .",
    "however , they do not take into account the difference between a social network and a knowledge base .    to the best of our knowledge",
    ", this work presents the first analysis of the trends that appear in the structure of wikipe - dia that contribute to identify the best expansion features for a given query . for this task ,",
    "we support our analysis on an information retrieval benchmark borrowed from the imageclef 2011 track  @xcite , which consists of a set of documents and a set of queries . for each query",
    ", it also contains the set of documents that are correct results for that particular query , which from now on we will refer to as the result set .",
    "we use this information to build a ground truth that relates each query from the query set to a graph of wikipedia articles and categories that we called query graph . given a query",
    ", its query graph contains those articles that in case of being used as expansion features , allow us to retrieve the correct documents for that particular query . from the analysis of the structure of the query graphs we reveal that",
    ", within the maze of relations among articles and categories that group them , cycle - based structures contribute to find articles whose titles are good candidates to be used as expansion features .",
    "the main contributions of this paper can be summarized as follows :    1 .",
    "we create a ground truth consisting of those articles in wikipedia that provide good results for each of the queries of imageclef 2011 track , which we use as the baseline in our experiments .",
    "we analyze how the articles and categories of the ground truth are structured within the wikipedia graph .",
    "we identify cycles of articles and categories as an important structure and also , we identify some trends within them . we find that dense cycles with a minimum ratio of categories , around the 30% , are able to identify the best expansion features .",
    "we identify challenging and open problems for graph processing technologies when it comes to exploit structures of large graphs such as wikipedia .",
    "the remainder of this paper is organized as follows : in section  [ studydataset ] we describe in detail the process of building a ground truth out of imageclef 2011 . in section  [ sec : analysis ] we analyze the query graphs and finally , in section  [ challenge ] we conclude and propose some challenges for graph- based technologies .",
    "query expansion consists in reformulating an input query to improve its retrieval performance .",
    "the input query is expressed as a list of keywords , for example , ` graffiti street art ` has 3 keywords .",
    "query expansion combines the original query keywords with a set of expansion features that are identified by applying morphological transformations or by finding synonyms and semantical related concepts .    to find the expansion features , we rely on wikipedia .",
    "wikipedia has a rich schema and can be used as a knowledge base in several ways . in this paper",
    "we use that part of the schema depicted in figure  [ fig : wikipedia ] , which consists of two different types of entries : ` article ` and ` category ` .",
    "a wikipedia article describes a single topic , and has a title that , according to the wikipedia edition rules , must be * recognizable * , * natural * , * precise * , * concise * and * consistent*. each article represents an entity  something that exists in itself , actually or potentially , concretely or abstractly , physically or not . hence , titles are useful to identify the entities that are mentioned in the input query .",
    "in the example above , we identify 2 entities : ` graffiti ` and ` street art ` .",
    "articles can ` link ` to other articles and must belong to , at least , one ` category ` .",
    "articles can also be connected by another special kind of relation , called ` redirect ` , when two articles refer to the same topic but have different titles . in this case , the articles with the less used / common titles ( _ redirect articles _ ) points to the article with the most common title ( _ main article _ ) .",
    "each category can also be ` inside ` one or more general categories forming , according to wikipedia edition rules , a tree - like structure .",
    "this forms a graph with multiple nodes , articles and categories , and relations with semantics such as equivalence , hierarchical or associative .",
    "> m1.25cm||m6.75 cm notation&definition + @xmath0&@xmath1 is a set of documents .",
    "+ @xmath2&@xmath3 is a set of articles .",
    "+ @xmath4&wikipedia category .",
    "+ @xmath5&a list of keywords .",
    "+ @xmath6&a tuple @xmath7 such that @xmath8 , @xmath9 is a correct document for @xmath5 .",
    "+ @xmath10&the set of wikipedia articles mentioned in @xmath5 .",
    "+ @xmath11&the set of wikipedia articles mentioned within the text of document @xmath9 .",
    "+ @xmath12&@xmath13 .",
    "+ @xmath14&the set of wikipedia articles whose titles are the best expansion features for query @xmath6 .",
    "+ @xmath15&the query graph of @xmath6 .",
    "+    we are interested in knowing whether the graph structure of the articles and their categories encodes information that could potentially be used to identify expansion features . for that purpose ,",
    "we need a ground truth that relates a query to a graph of articles and categories , which we call the query graph .",
    "the articles of the query graph are those whose titles a ) identify the entities mentioned in the query and b ) are the best expansion features for the query . the categories of a query graph are the categories of the articles and help to understand better the structures .",
    "we first describe the process for building up the ground truth based on query graphs and later , we analyze their structure in order to identify trends that benefit the identification of good expansion features . before continuing , in table  [ tab : definitions ]",
    "we introduce some definitions and notation that will be used throughout this paper .        to build the ground truth we rely on the imageclef 2011 track  @xcite .",
    "it consists of a set of 237,434 images and their respective xml metadata files .",
    "also , the track provides a set of fifty queries consisting of a set of keywords @xmath5 and their results set @xmath0 , as explained in table  [ tab : definitions ] .",
    "these results sets contain only the xml metadata files describing the images . for each query @xmath6 of the imageclef 2011 query set",
    ", we build a query graph @xmath15 , whose construction can be summarized as follows :    1 .",
    "identify the sets of wikipedia articles @xmath16 and @xmath17 .",
    "2 .   find the set @xmath14 .",
    "3 .   assemble the query graph @xmath15 .      given a query @xmath6 , to identify those wikipedia articles that are mentioned within @xmath18 and @xmath19",
    ", we perform an entity linking process consisting in identifying the entities within the given text .",
    "as shown in table  [ tab : definitions ] , this process is denoted as @xmath20 . even though the entity linking process is essentially the same regardless of whether the input item is a set of keywords or a document , for the later an additional preprocessing step",
    "is performed where , the relevant text of the document to be linked is extracted .",
    "given @xmath9 , an xml metadata document as the one depicted in figure  [ fig : xmlfile ] , we extract the name of the file without the file extension , the information in the english section ( there are also sections in german and french ) and the description from the general comment field .",
    "these three items are then combined in a string , in which we do entity linking .    to perform the entity linking process",
    ", we require a knowledge base of entities such as wikipedia . in our case",
    ", we consider each article in wikipedia as an entity , whose title is used to perform the entity linking process against the input text .",
    "thus , this allows us to represent a given text as a set of articles of wikipedia .",
    "the entity linking process consists in identifying the set of the largest substrings in the input query that matches with the title of an article in wikipedia . in order to improve the accuracy of our entity linkage , we do not only search entities in the input text , but also in synonym phrases .",
    "we derive a synonym phrase by replacing at least one term of the input text by a synonymous term .",
    "synonymous terms are calculated using redirections of wikipedia . with more detail ,",
    "given a term @xmath21 , we retrieve ( if it exists ) the article @xmath22 from wikipedia whose title is equal to @xmath21 . then , the synonyms of @xmath21 are the titles of the redirects of @xmath22 .",
    "this simple strategy proved effective for our purposes .",
    "finally , for each query @xmath6 we compute @xmath16 and @xmath17 .          according to table  [",
    "tab : definitions ] , @xmath14 is the set of articles whose titles are the best expansion features for @xmath6 . to find @xmath14",
    ", we need a mechanism to evaluate how good are the titles of a set of articles @xmath2 when these are used as expansion features of a query @xmath6 .",
    "for that purpose we rely on the indri search engine  @xcite . given the articles in @xmath2 ,",
    "we use their titles to internally write a query in the indri query language , based on exact phrase matching .",
    "the returned results are then used to calculate the top-@xmath24 precision of the query .",
    "so , if @xmath25 is the top-@xmath24 results when the titles of articles in @xmath2 are used to write the query , then the top-@xmath24 precision over a set of expected result @xmath0 is computed as follows : @xmath26 then , the average of the top-1 , top-5 , top-10 and top-15 precision is computed as : @xmath27 where @xmath28 .",
    "note that @xmath16 and @xmath17 are the sets of articles that are mentioned in the query keywords ( @xmath18 ) and in the documents of the query result set ( @xmath19 ) respectively .",
    "since we want to analyze how the articles in @xmath17 help to improve the most the results obtained by @xmath16 , we define @xmath14 as : @xmath29    the naive way to compute @xmath14 is to compute the quality for all possible combinations of @xmath30 from articles in @xmath17 .",
    "however , the number of possible combinations is @xmath31which makes unfeasible to find the best solution using a brute force approach .",
    "therefore we propose the following procedure to find the best combination .",
    "the procedure starts with @xmath30 containing a random article of @xmath17 . from this moment on , it starts an iterative process that incrementally applies a single operation out of the following possible : ` add ` a new article to @xmath30 from @xmath17 , ` remove ` an article from @xmath30 , ` swap ` an article of @xmath30 by one of @xmath17 .",
    "operations are applied as long as they improve equation  [ eq : o ] , repeating the process until no further improvements can be found .",
    "note that if after removing an article the quality remains the same , the article is removed as we want the minimum set of articles with the maximum quality .",
    "this method for building @xmath14 as @xmath32 is capable of achieving good results in terms of precision for the different top-@xmath33 .",
    "table  [ tab : precision_stats ] shows , for each top-@xmath24 , the average of the precision obtained in all the queries for that particular .",
    ".statistics of precision of ground truth .",
    "[ cols= \" < , > , > , > , > , > \" , ]        [ fig : cyclelenghtcontribution ]     [ fig : numofcycles ]    0.45     0.45     in table  [ table : cycleresults ] we summarize the results achieved by using the titles of the articles within the cycles of a given length , and combinations of them , as expansion features . broadly speaking , the precisions achieved by the different configurations are comparable to the best results obtained in the imageclef 2011 conference  @xcite",
    ". however , the current results of the conference were achieved by using a hybrid and textual search engine and also using relevance feedback techniques .",
    "this supports the idea that wikipedia encodes relevant information in its structure ( and more concretely , in the form of cycles of articles and categories ) that can be used to solve queries from many different domains . also , we see that cycles of length 2 and 3 achieve better precision on the top-1 and the top-5 results , while cycles of length 4 and 5 achieve better results on the top-10 and top-15 .",
    "this is because shorter cycles contain articles whose title is semantically very close and similar to @xmath18 , which are those that better help to define the user needs . on the other hand ,",
    "larger cycles contain articles whose titles are capable of introducing new concepts  also semantically related to @xmath18  that widen the space of search , thus , those concepts may not be so exact but contribute to find more results , improving the top-10 and top-15 precision .",
    "we define the contribution of a cycle @xmath34 for a query @xmath6 as the percentual difference between @xmath35 and @xmath36 we only consider the articles in @xmath34 but ignore the categories . ] . in figure",
    "[ fig : cyclelenghtcontribution ] , we show the average contribution of cycles of different lengths .",
    "we observe that cycles of length 2 are able to achieve an average contribution of up to 50% , while those of in larger cycles contribute 32.74% at most .",
    "this suggests that cycles of length 2 contain significantly better articles than the rest of the cycles , therefore one could be tempted to deliberately add such cycles to expand queries . however , to better understand these results , we also count the average number of cycles of each length , which are shown in figure  [ fig : numofcycles ] .",
    "we observe that the amount of cycles of length 2 is significantly smaller than those larger .",
    "this could be caused either because a ) wikipedia does not contain a large amount of such cycles or b ) because the cycles of length 2 are not always reliable , as otherwise they would appear more frequently in the query graphs .",
    "however , according to our experiments , among all pairs of articles that are connected , 11.47% form a cycle of length 2 , meaning that this structure is not so infrequent . then , we must assume the hypothesis that the cycles of length 2 that contribute significantly to the quality of the results are scarce .",
    "we count the ratio between categories and the total number of nodes that forms the cycle to understand the importance of this type of nodes .",
    "note that , due the schema depicted in figure  [ fig : wikipedia ] , only cycles whose length is equal or larger to 3 can contain categories . in figure",
    "[ subfig : categoryratio ] , we see that among all analyzed cycles , the average ratio of categories grows very slowly the slope of the trend line is almost 0  when the length grows . more concretely ,",
    "the number of categories in cycles of length 3 is in general 1 ( @xmath37 ) , while the number of categories in cycles of length 5 is , in general , 2 ( @xmath38 ) .",
    "this suggests that categories play a significant role in connecting semantically related articles .",
    "this observation implies breaking away from the idea that , the shorter the cycle and larger the proportion of articles , the stronger the relation between them is .",
    "for example , even short cycles of length 3 that do not contain categories , as the one depicted in figure  [ fig : wrongcycles ] , may introduce semantically - distant terms as can be `` sheep '' from `` anthrax '' that are likely to diminish the retrieval performance of a query .",
    "another relevant characteristic is the characterization of the cycles based on the density of extra edges ( those extra edges beside those strictly necessary to form a cycle ) .",
    "the minimum amount of edges of a cycle of length @xmath39 is @xmath39 , thus we define the density of extra edges as the ratio between the extra edges and the maximum amount of extra edges a cycle can have .",
    "given the following functions : + - @xmath40 : returns the number of articles in the cycle , + - @xmath41 : returns the number of categories in the cycle and + - @xmath42 : returns the number of edges in the cycle .",
    "+ we calculate the maximum amount of edges cycles of length larger than 3 as follows :    @xmath43 and the density of extra edges is calculated as : @xmath44    in figure  [ fig : densityofextraedgestendency ] we show the trend line of the density of extra edges compared to the contribution of the cycle .",
    "we see that , the denser the cycle , the better its contribution .",
    "this assertion is also supported by the information depicted in figure  [ fig : cyclelenghtcontribution ] and in figure  [ subfig : densityofextraedges ] .",
    "in particular , we observe that there is a correlation between the cycles that are denser in figure  [ subfig : densityofextraedges ] and the cycles that contribute more , depicted in figure  [ fig : cyclelenghtcontribution ] .",
    "thus , cycles of length 4 are the densest and the ones that achieve the largest average contribution , and cycles of length 3 are the least dense and also the ones that achieve the smallest contribution .",
    "[ challenge ] in this paper , we have analyzed the use of knowledge bases as graphs to improve the query expansion problem .",
    "we have created a ground truth that relates a query with a graph of entries within wikipedia ( our knowledge base ) , and we have named this the query graph . in particular , we have used the imageclef query set and wikipedia , creating a graph of articles and categories for each query .",
    "later , we have analyzed the query graphs , aiming at revealing structures that help to extract information for the particular case of query expansion .",
    "in other words , given a query @xmath6 and its query graph @xmath15 , we analyze the structures of @xmath15 .",
    "this analysis allows us to identify the cycles as an important structure in order to find expansion features . according to our analysis , dense cycles",
    ", in which the ratio of categories stands around the 30% , are specially useful to identify new expansion features . among the cycles that fulfill those properties , small cycles help to describe better the user needs , expressed as @xmath18 , and improve the precision in the first results while , larger cycles introduce expansion features , that widen the search space and , thus , favor the precision in the top-10 and top-15 results .",
    "the structural analysis of the developed ground truth and its conclusions pose some interesting challenges for graph technologies .",
    "the computation of all the dense cycles of a given length without taking the edges direction into account is a complex problem and computationally expensive even for a high performance graph database . as an example , analyzing the query graphs , which have an average size of 208.22 nodes , took us an average time of 6 minutes per query .",
    "taking into account that query expansion techniques are expected to respond in real time , and that wikipedia has almost @xmath45 articles , there is still a lot to do in many fields such as high performance graph technology and algorithms .    in the particular scenario of using wikipedia as a knowledge base ,",
    "it is also interesting to study the convenience of using the redirect articles as expansion features , since they represent less common ways to refer a concept and may reveal as a way of introducing good expansion featured .",
    "however , due to the cycle analysis that we have done , redirects are never considered as an expansion feature since they can never close a cycle ( see figure  [ fig : wikipedia ] ) .",
    "we have not analysed how the frequency of a given article in the cycles and the goodness of its title as expansion feature are correlated , as cycles are considered individually",
    ". such correlation , if existing , could be exploited .",
    "last but not least , future research should include techniques aimed at taking advantage of the trends analyzed in this paper in real query expansion system , which are expected to respond in real time .",
    "the members of dama - upc thank the ministry of science and innovation of spain , generalitat de catalunya , for grant numbers tin2013 - 47008-r and sgr2014 - 890 respectively and also the eu fp7/2007 - 2013 for funding the ldbc project ( ict2011 - 8 - 317548 ) .",
    "also , thanks to oracle labs for the support to our research on graph technologies ."
  ],
  "abstract_text": [
    "<S> knowledge bases are very good sources for knowledge extraction , the ability to create knowledge from structured and unstructured sources and use it to improve automatic processes as query expansion . however , extracting knowledge from unstructured sources is still an open challenge  @xcite . in this respect , understanding the structure of knowledge bases can provide significant benefits for the effectiveness of such purpose . in particular , wikipedia has become a very popular knowledge base in the last years because it is a general encyclopedia that has a large amount of information and thus , covers a large amount of different topics . in this piece of work , </S>",
    "<S> we analyze how articles and categories of wikipedia relate to each other and how these relationships can support a query expansion technique . </S>",
    "<S> in particular , we show that the structures in the form of dense cycles with a minimum amount of categories tend to identify the most relevant information . </S>"
  ]
}