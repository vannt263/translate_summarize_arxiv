{
  "article_text": [
    "this paper considers a decision problem that involves gussing an invisible state @xmath0 after observing @xmath1 , which is correlated with @xmath0 . in decision theory  (",
    "* section 1.5.2 ) , an optimal decision rule , which minimizes the decision error probability , is called the _",
    "bayes decision rule_.    let @xmath0 and @xmath1 be random variables that take values in sets @xmath2 and @xmath3 , respectively , where we call @xmath0 a _ state of nature _ or a _",
    "parameter _ and @xmath1 an _",
    "observation_. let @xmath4 be the joint distribution of @xmath5 .",
    "let @xmath6 and @xmath7 be the marginal distributions of @xmath0 and @xmath1 , respectively .",
    "let @xmath8 be the conditional distribution of @xmath0 for a given @xmath1 .",
    "it is well known that an optimal strategy for guessing the state @xmath0 consists of finding @xmath9 , which maximizes the conditional probability @xmath10 depending on a given observation @xmath11 . formally , by taking an @xmath9 that maximizes @xmath10 for each @xmath12 , we can define the function @xmath13 as @xmath14 which is a bayes decision rule . it should be noted that the discussion throughout this paper does not depend on choosing states with the same maximum probability .",
    "when the cardinality @xmath15 of @xmath2 is small , operations ( [ eq : map ] ) and ( [ eq : ml ] ) are tractable by using a brute force search . however , with coding problems , these operations appears to be intractable because @xmath15 grows exponentially as the dimension of @xmath2 grows . in this paper",
    ", we assume a situation where operations ( [ eq : map ] ) and ( [ eq : ml ] ) are intractable .    in source coding",
    ", @xmath0 corresponds to a source output and @xmath1 corresponds to a codeword and side information . in channel coding",
    ", @xmath0 corresponds to a codeword and @xmath1 corresponds to a channel output , where the decoding with ( [ eq : map ] ) is called _ maximum a posteriori decoding_. on the other hand , the decoding method that maximizes the conditional probability @xmath16 of a channel is called _ maximum likelihood decoding _ ) as maximizing the likelihood @xmath17 . for this reason ,",
    "we might call @xmath18 a _ maximum - likelihood decision rule_. in a series of papers on the coding problem , we have called @xmath18 a maximum - likelihood decoding based on this idea . ] , which is equivalent to maximum a posteriori decoding when @xmath0 is generated subject to the uniform distribution . in this paper",
    ", we call the decision rule with @xmath18 the _ maximum a posteriori decision rule_.    in this paper , we consider a stochastic decision , where the decision is made randomly subject to a probability distribution .",
    "we investigate the relationship between the error probabilities of the stochastic and maximum a posteriori decisions .",
    "then , we introduce the construction of stochastic decoders for source / channel codes .",
    "for a stochastic decision , we use a random number generator to obtain @xmath19 after observing @xmath1 and let @xmath20 be a decision ( guess ) about the state @xmath0 . formally , we generate @xmath20 subject to the conditional distribution @xmath21 on @xmath2 depending on an observation @xmath1 and let an output be a decision of @xmath0 , where @xmath0 and @xmath20 are conditionally independent for a given @xmath1 , that is , @xmath22 forms a markov chain .",
    "the joint distribution @xmath23 of @xmath24 is given as @xmath25 let us call @xmath26 a _ stochastic decision rule_. as a special case , when @xmath26 is given by using a function @xmath27 and is defined as @xmath28 we call @xmath26 or @xmath29 a _ deterministic decision rule_. it should be noted that the maximum a posteriori decision rule is deterministic .    throughout this paper , we assume that @xmath2 and @xmath3 are countable sets . it should be noted that the results do not change when @xmath3 is an uncountable set , where the summation should be replaced with the integral . the following lemma guarantees that the right hand side of ( [ eq : map ] ) always exists for every @xmath12 .",
    "this fact , which is implicitly used in this paper , implies that it is enough to assume that @xmath2 is a countable set .",
    "let @xmath30 be a probability distribution on a countable set @xmath2 .",
    "then the maximum of @xmath30 on @xmath2 always exists , that is , there is @xmath31 such that @xmath32 for any @xmath33 .",
    "the lemma is trivial when @xmath2 is a finite set . in the following ,",
    "we assume that @xmath2 is a countable infinite set .",
    "since @xmath34 for all @xmath33 , then @xmath35 always exists , that is , @xmath36 for all @xmath37 , and for any @xmath38 there is a @xmath37 such that @xmath39 .",
    "we prove the lemma by contradiction .",
    "assume that there is no @xmath31 such that @xmath40 .",
    "since @xmath41 and @xmath42 for all @xmath33 , there is @xmath43 such that @xmath44 . from the definition of @xmath35 , there is a @xmath45 such that @xmath46 , where the second inequality comes from the assumption . by repeating this argument",
    "times so that @xmath47 . ]",
    ", we have a sequence @xmath48 such that @xmath49 this implies @xmath50 , which contradicts @xmath41 .",
    "when @xmath2 is a finite dimensional euclidian space , we can make the same discussion by quantizing uniformly from @xmath2 to a countable set , where the decision is interpreted as guessing @xmath0 with a finite precision .",
    "then we can apply the results to parameter estimation problems .",
    "let @xmath51 be a support function defined as @xmath52 then the error probability @xmath53 of a ( stochastic ) decision rule @xmath26 is given as @xmath54 .",
    "\\label{eq : error - random}\\end{aligned}\\ ] ] in the last equality , @xmath55 corresponds to the error probability of the decision rule @xmath26 after the observation @xmath12 , and @xmath53 corresponds to the average of this error probability .",
    "when @xmath26 is defined by using @xmath27 and ( [ eq : deterministic ] ) , the decision error probability @xmath56 of a deterministic decision rule @xmath29 is given as @xmath57 .",
    "\\label{eq : error - f}\\end{aligned}\\ ] ] it should be noted that the right hand side of the first equality can be derived directly from ( [ eq : error - random ] ) and the fact that @xmath58 that is , we have @xmath59 when @xmath29 and @xmath26 satisfy ( [ eq : deterministic ] ) .",
    "in this section , we discuss the optimality of the maximum a posteriori decision .",
    "we introduce the following well - known lemma .",
    "[ prop : bayes ] let @xmath5 be a pair consisting of a state @xmath0 and an observation @xmath1 and @xmath4 be the joint distribution of @xmath5 .",
    "when we make a stochastic decision with a distribution @xmath26 , an optimal decision rule minimizing the decision error probability satisfies @xmath60 for all @xmath61 such that @xmath62 and @xmath63 . in particular , the maximum a posteriori decision rule @xmath18 defined by ( [ eq : map ] ) minimizes the decision error probability , where @xmath26 is defined by @xmath64 and ( [ eq : deterministic ] ) .    from the definition of @xmath53 , we have    @xmath65   \\notag   \\\\   &   =   1-\\sum_y p_y(y)\\sum_x p_{x|y}(x|y)q_{{\\widehat{x}}|y}(x|y )   \\notag   \\\\   &   =   1   -\\sum_y p_y(y )   { \\left [ {    \\sum_{x\\neq { f_{\\mathrm{map}}}(y)}p_{x|y}(x|y)q_{{\\widehat{x}}|y}(x|y )    + p_{x|y}({f_{\\mathrm{map}}}(y)|y)q_{{\\widehat{x}}|y}({f_{\\mathrm{map}}}(y)|y )   } \\right ] }   \\notag   \\\\   &   =   1   -\\sum_y p_y(y )   { \\left [ {    \\sum_{x\\neq{f_{\\mathrm{map}}}(y ) }    p_{x|y}(x|y)q_{{\\widehat{x}}|y}(x|y )    +    p_{x|y}({f_{\\mathrm{map}}}(y)|y){\\left[{1-\\sum_{x\\neq { f_{\\mathrm{map}}}(y)}q_{{\\widehat{x}}|y}(x|y)}\\right ] }   } \\right ] }   \\notag",
    "\\\\   &   =   1   -\\sum_yp_y(y)p_{x|y}({f_{\\mathrm{map}}}(y)|y )   + \\sum_yp_y(y )   \\sum_{x\\neq{f_{\\mathrm{map}}}(y)}q_{{\\widehat{x}}|y}(x|y){\\left[{p_{x|y}({f_{\\mathrm{map}}}(y)|y)-p_{x|y}(x|y)}\\right]}.   \\label{eq : optimal}\\end{aligned}\\ ] ]    ( [ eq : optimal ] ) , which is on the top of the next page , where @xmath66 implies that @xmath53 is minimized only when @xmath60 for all @xmath61 such that @xmath62 and @xmath67 ( i.e. , @xmath68 ) .",
    "in this section , we consider the case where @xmath69 for all @xmath70 , that is , we make a stochastic decision with the conditional distribution @xmath8 of a state @xmath0 for a given observation @xmath1 .",
    "it should be noted that @xmath20 is independent of @xmath0 for a given @xmath1 , where the joint distribution @xmath23 of @xmath24 is given as @xmath71 in the following , we call this type of decision a _",
    "stochastic decision with the a posteriori distribution_. it should be noted that it may be unnecessary to know ( or compute ) the distribution @xmath8 to make this type of decision . to make this type of decision ,",
    "it is sufficient that we have a random number generator subject to the distribution @xmath72 with arbitrary input @xmath12 , where the generated random number is independent of @xmath0 for a given @xmath11 .",
    "we have the following theorem . in section  [ sec : decoder ] , we apply this theorem to an analysis of stochastic decoders of coding problems .",
    "[ thm : random - fmap ] let @xmath5 be a pair consisting of a state @xmath0 and an observation @xmath1 and @xmath4 be the joint distribution of @xmath5 . when we make a stochastic decision with @xmath8 ,",
    "the decision error probability of this rule is at most twice the decision error probability of the maximum a posteriori decision rule @xmath18 .",
    "that is , we have @xmath73    it should be noted that this theorem can be considerd as a special case of ( * ? ? ?",
    "* corollary 1 of theorem 1 ) ( * ? ? ?",
    "* eq .  ( 29 ) ) which assme a general loss function .",
    "although the idea of the proof in ( * ? ? ?",
    "* theorem 1 ) is quite simple by assuming that a loss function is symmetric and satisfies the triangle inequality , we show this lemma in a different way by assuming the loss function @xmath74 .",
    "we have @xmath75   \\notag   \\\\   & =   \\sum_{y}p_y(y )   { \\left[{1-\\sum_{x}p_{x|y}(x|y)^2}\\right ] }   \\notag   \\\\   & \\leq   \\sum_{y}p_y(y )   { \\left[{1-p_{x|y}({f_{\\mathrm{map}}}(y)|y)^2}\\right ] }   \\notag   \\\\   & =   \\sum_{y}p_y(y )   [ 1-p_{x|y}({f_{\\mathrm{map}}}(y)|y ) ]   [ 1+p_{x|y}({f_{\\mathrm{map}}}(y)|y ) ]   \\notag   \\\\   & \\leq   2\\sum_{y}p_y(y )   [ 1-p_{x|y}({f_{\\mathrm{map}}}(y)|y ) ]   \\notag   \\\\   & =   2{\\mathrm{error}}({f_{\\mathrm{map } } } ) ,   \\label{eq : proof - random - deterministic}\\end{aligned}\\ ] ] where the second inequality comes from the fact that @xmath76 and the fourth equality comes from ( [ eq : error - f ] ) .    here",
    ", we introduce the following inequalities , which come from lemma  [ prop : bayes ] and theorem  [ thm : random - fmap ] . in these inequalities , if either @xmath77 or @xmath78 vanishes as the dimension ( block length ) of @xmath0 goes to infinity , then the other one also vanishes .",
    "[ cor : bound ] @xmath79    here , we introduce another lemma that comes from lemma  [ prop : bayes ] and theorem  [ thm : random - fmap ] .",
    "[ cor : random ] let @xmath5 be a pair consisting of a state @xmath0 and an observation @xmath1 and @xmath4 be the joint distribution of @xmath5 .",
    "when we make a stochastic decision with @xmath8 , the decision error probability of this rule is at most twice the decision error probability of _ any _ decision rule @xmath26 .",
    "that is , we have @xmath80 for any @xmath26 .",
    "let us consider a situation where @xmath26 is unknown but @xmath53 can be estimated empirically .",
    "then the above corollary implies that the error probability of stochastic decision with the a posteriori distribution is upper bounded by @xmath81 .",
    "for example , when we know empirically that a human being can guess @xmath0 with small error probability explicitly .",
    "] , then the error probability of a stochastic decision with the a posteriori distribution is also small because it is at most twice the error probability of her / his decision rule .    inequality ( [ eq : random ] ) is tight in the sense that there is a pair consisting of @xmath8 and @xmath26 such that ( [ eq : random ] ) is satisfied with equality . in fact , by assuming that @xmath82 for all @xmath12 , we have @xmath83\\ ] ] and @xmath84   \\notag   \\\\ *   & \\quad   + 2\\sum_yp_y(y)p_{x|y}(1|y)[1-q_{{\\widehat{x}}|y}(1|y ) ]   \\notag   \\\\   & =   2\\sum_yp_y(y)p_{x|y}(0|y)[1-q_{{\\widehat{x}}|y}(0|y ) ]   \\notag   \\\\ *   & \\quad   + 2\\sum_yp_y(y)[1-p_{x|y}(0|y)]q_{{\\widehat{x}}|y}(0|y )   \\notag   \\\\   & =   2\\sum_yp_y(y)p_{x|y}(0|y )   \\notag   \\\\ *   & \\quad   + 2\\sum_yp_y(y)[1 - 2p_{x|y}(0|y)]q_{{\\widehat{x}}|y}(0|y )   \\notag   \\\\   & =   2\\sum_yp_y(y)p_{x|y}(0|y )   -2\\sum_yp_y(y)p_{x|y}(0|y)^2   \\notag   \\\\ *   & =   2\\sum_yp_y(y)p_{x|y}(0|y)[1-p_{x|y}(0|y)]\\end{aligned}\\ ] ] from ( [ eq : error - random ] ) .",
    "let @xmath85 be the variational distance of two probability distributions @xmath30 and @xmath86 on the same set as @xmath87 ( see  ( * ? ? ? * eq .  ( 11.137 ) ) ) .",
    "we have the following lemma .",
    "[ thm : random - approx ] let @xmath5 be a pair consisting of a state @xmath0 and an observation @xmath1 and @xmath4 be the joint distribution of @xmath5 .",
    "when we make decisions with two stochastic decision rules @xmath88 and @xmath89 for each @xmath12 , we have @xmath90 where @xmath91 and @xmath92 .",
    "the lemma is obtained immediately from ( [ eq : vd - max ] ) by considering the decision error event measured by using the joint probability distributions @xmath93 and @xmath94 .",
    "formally , we have    @xmath95    -    \\sum_{y}p_y(y )    \\sum_{x }    p_{x|y}(x|y )    [ 1-q'(x|y ) ]   } \\right| }   \\notag   \\\\   & =   { \\left| {    \\sum_{y}p_y(y )    \\sum_{x }    p_{x|y}(x|y )    q({\\mathcal{x}}\\setminus\\{x\\}|y )    -    \\sum_{y}p_y(y )    \\sum_{x }    p_{x|y}(x|y )    q'({\\mathcal{x}}\\setminus\\{x\\}|y )   } \\right| }   \\notag   \\\\   & \\leq   \\sum_{y}p_y(y )   \\sum_{x }   p_{x|y}(x|y )   { \\left| {    q({\\mathcal{x}}\\setminus\\{x\\}|y )    -    q'({\\mathcal{x}}\\setminus\\{x\\}|y )   } \\right| }   \\notag   \\\\   & \\leq   \\sum_{y}p_y(y )   \\sum_{x }   p_{x|y}(x|y )   d(q(\\cdot|y),q'(\\cdot|y ) )   \\notag   \\\\   & =   \\sum_{y}p_y(y )   d(q(\\cdot|y),q'(\\cdot|y ) )   \\notag   \\\\   & =   d(q\\times p_y , q'\\times p_y )   \\label{eq : proof - random - approx}\\end{aligned}\\ ] ]    ( [ eq : proof - random - approx ] ) , which appears on the top of the next page , where the second inequality comes from ( [ eq : vd - max ] ) and the last equality comes from ( [ eq : vd ] ) as @xmath96    applying lemma  [ thm : random - approx ] by letting @xmath97 and @xmath98 , we have the following theorem from theorem  [ thm : random - fmap ] .",
    "let @xmath5 be a pair consisting of a state @xmath0 and an observation @xmath1 and @xmath4 be the joint distribution of @xmath5 .",
    "when we make a stochastic decision with @xmath26 , the decision error probability @xmath53 is bounded as @xmath99",
    "in this section , we assume that a conditional probability @xmath8 is computable . we make a stochastic decision @xmath100 from a random sequence @xmath101 as @xmath102 a typical example of a random sequence @xmath101 is that generated by a markov chain monte carlo method .    here",
    ", we assume that @xmath0 and @xmath103 are conditionally independent for a given @xmath1 , that is , the joint distribution @xmath104 of @xmath105 is given as @xmath106 where @xmath107 is a joint probability distribution of @xmath103 for a given @xmath12 .",
    "then , we have a decision error probability @xmath108 as follows ; @xmath109 }   \\notag   \\\\   & =   e_{y{\\widehat{x}}^{{\\overline{t}}}}{\\left [ {    1-p_{x|y}({\\widehat{x}}_{t(y)}|y )   } \\right ] }   \\notag   \\\\   & =   e_{y{\\widehat{x}}^{{\\overline{t}}}}{\\left [ {    1-\\max_{t\\in\\{1,\\ldots,{\\overline{t}}\\}}p_{x|y}({\\widehat{x}}_t|y )   } \\right]}.   \\label{eq : seq - error}\\end{aligned}\\ ] ] we have the following theorem .    [ thm : seq - random ] let @xmath5 be a pair consisting of a state @xmath0 and an observation @xmath1 and @xmath4 be the joint distribution of @xmath5 .",
    "when we make a stochastic decision @xmath110 with a random sequence @xmath103 defined by ( [ eq : ty])([eq : sequence ] ) , the decision error probability @xmath108 defined by ( [ eq : seq - error ] ) satisfies @xmath111 where @xmath112 is a conditional marginal distribution given as @xmath113    from ( [ eq : seq - error ] ) , we have @xmath114 }   \\notag   \\\\   & \\leq   e_{y{\\widehat{x}}^{{\\overline{t}}}}{\\left [ {    1-p_{x|y}({\\widehat{x}}_t|y )   } \\right ] }   \\notag   \\\\   & =   \\sum_{y}p_y(y )   \\sum_{{\\widehat{x}}^{{\\overline{t } } } }   q_{{\\widehat{x}}^{{\\overline{t}}}|y}({\\widehat{x}}^{{\\overline{t}}}|y )   { \\left [ {    1-p_{x|y}({\\widehat{x}}_t|y )   } \\right ] }   \\notag   \\\\   & =   \\sum_{y}p_y(y )   \\sum_{{\\widehat{x}}_t}q_{{\\widehat{x}}_t|y}({\\widehat{x}}_t|y )   [ 1-p_{x|y}({\\widehat{x}}_t|y ) ]   \\notag   \\\\   & =   \\sum_{y}p_y(y )   \\sum_{{\\widehat{x}}_t }   p_{x|y}({\\widehat{x}}_t|y )   [ 1-q_{{\\widehat{x}}_t|y}({\\widehat{x}}_t|y ) ]   \\notag   \\\\   & =   { \\mathrm{error}}(q_{{\\widehat{x}}_t|y})\\end{aligned}\\ ] ] for any @xmath115 , where the fourth equality comes from the fact that @xmath8 and @xmath116 are probability distributions . from this inequality , we have ( [ eq : seq - random ] ) .",
    "applying lemma  [ thm : random - approx ] by letting @xmath117 and @xmath98 , we have the following theorem from theorem  [ thm : random - fmap ] .",
    "this theorem implies that if @xmath118 tends towards @xmath119 as @xmath120 ( e.g. ( [ eq : gibbs - approx ] ) in appendix ) then the upper bound of error probability @xmath108 is close to at most twice the error probability @xmath77 of the maximum a posteriori decision .",
    "[ thm : seq - random - approx ] let @xmath5 be a pair consisting of a state @xmath0 and an observation @xmath1 and @xmath4 be the joint distribution of @xmath5 . when we make a stochastic decision @xmath110 with a random sequence @xmath121 defined by ( [ eq : ty])([eq : sequence ] ) , the decision error probability is bounded as @xmath122    in the following , we assume that a random sequence @xmath103 is independent and identically distributed ( i.i.d . ) with a distribution @xmath26 for a given @xmath1 , that is , the conditional probability distribution @xmath123 is given as @xmath124 then we have the following theorem . from this theorem with a trivial inequality @xmath125",
    ", we have the fact that @xmath108 tends towards the error probability @xmath77 of the maximum a posteriori decision , where the difference @xmath126 is exponentially small as the length @xmath127 of a sequence increases .",
    "[ thm : seq - iid ] let @xmath5 be a pair consisting of a state @xmath0 and an observation @xmath1 and @xmath4 be the joint distribution of @xmath5 . when we make a stochastic decision @xmath110 with an i.i.d .",
    "random sequence @xmath103 defined by ( [ eq : ty])([eq : sequence ] ) and ( [ eq : iid ] ) , the decision error probability @xmath108 defined by ( [ eq : seq - error ] ) satisfies @xmath128^{{\\overline{t } } } ,   \\label{eq : seq - qiid - fmap}\\end{aligned}\\ ] ] where @xmath18 is given by ( [ eq : map ] ) .",
    "in particlular , when @xmath129 , we have @xmath130^{{\\overline{t } } }   \\label{eq : seq - iid - fmap }   \\\\   & \\leq   { \\mathrm{error}}({f_{\\mathrm{map}}})+{\\left[{1-\\inf_{y : p_y(y)>0}\\max_x p_{x|y}(x|y)}\\right]}^{{\\overline{t}}}.   \\label{eq : seq - iid - inffmap}\\end{aligned}\\ ] ]    @xmath131 }   \\notag   \\\\   & =   \\sum_{y}p_y(y)\\sum_{{\\widehat{x}}^{{\\overline{t}}}\\in{\\widehat{{\\mathcal{x}}}}^{{\\overline{t } } } }   { \\left [ {    1-\\max_{t\\in\\{1,\\ldots,{\\overline{t}}\\}}p_{x|y}({\\widehat{x}}_t|y )   } \\right ] }   \\prod_{t=1}^{{\\overline{t}}}q_{{\\widehat{x}}|y}({\\widehat{x}}_t|y )   +   \\sum_{y}p_y(y)\\sum_{{\\widehat{x}}^{{\\overline{t}}}\\notin{\\widehat{{\\mathcal{x}}}}^{{\\overline{t } } } }   { \\left [ {    1-\\max_{t\\in\\{1,\\ldots,{\\overline{t}}\\}}p_{x|y}({\\widehat{x}}_t|y )   } \\right ] }   \\prod_{t=1}^{{\\overline{t}}}q_{{\\widehat{x}}|y}({\\widehat{x}}_t|y )   \\notag   \\\\   & \\leq   \\sum_{y}p_y(y)\\sum_{{\\widehat{x}}^{{\\overline{t}}}\\in{\\widehat{{\\mathcal{x}}}}^{{\\overline{t } } } }   { \\left [ {    1-\\max_{{\\widehat{x}}}p_{x|y}({\\widehat{x}}|y )   } \\right ] }   \\prod_{t=1}^{{\\overline{t}}}q_{{\\widehat{x}}|y}({\\widehat{x}}_t|y )   +   \\sum_{y}p_y(y)\\sum _ {    { \\widehat{x}}^{{\\overline{t}}}:{\\widehat{x}}_t\\neq{f_{\\mathrm{map}}}(y)\\ \\text{for all}\\ t   }   \\prod_{t=1}^{{\\overline{t}}}q_{{\\widehat{x}}|y}({\\widehat{x}}_t|y )   \\notag   \\\\   & =   \\sum_{y}p_y(y )   { \\left [ {    1-p_{x|y}({f_{\\mathrm{map}}}(y)|y )   } \\right ] }   +   \\sum_{y}p_y(y )   \\prod_{t=1}^{{\\overline{t } } }   \\sum_{{\\widehat{x}}_t\\neq{f_{\\mathrm{map}}}(y ) }   q_{{\\widehat{x}}|y}({\\widehat{x}}_t|y )   \\notag   \\\\   & =   { \\mathrm{error}}({f_{\\mathrm{map } } } )   +   \\sum_{y}p_y(y )   { \\left [ {   1-q_{{\\widehat{x}}|y}({f_{\\mathrm{map}}}(y)|y )   } \\right]}^{{\\overline{t } } }   \\label{eq : proof - iid}\\end{aligned}\\ ] ]    for a given @xmath12 , let @xmath132 be defined as @xmath133 then ( [ eq : seq - qiid - fmap ] ) is shown as ( [ eq : proof - iid ] ) , which appears on the top of the next page , where the inequality comes from the fact that @xmath134 implies @xmath135 and @xmath136 for all @xmath115 .    inequality ( [ eq : seq - iid - fmap ] ) is obtained from ( [ eq : proof - iid ] ) by letting @xmath129 . inequalities ( [ eq : seq - iid - inffmap ] ) and ( [ eq : seq - iid - x ] ) are shown by the fact that @xmath137}^{{\\overline{t } } }   \\notag   \\\\ *   & \\leq   \\sum_{y}p_y(y )   \\sup_{y : p_y(y)>0 }   { \\left [ {    1-p_{x|y}({f_{\\mathrm{map}}}(y)|y )   } \\right]}^{{\\overline{t } } }   \\notag   \\\\   & =   \\sup_{y : p_y(y)>0 }   { \\left [ {    1-p_{x|y}({f_{\\mathrm{map}}}(y)|y )   } \\right]}^{{\\overline{t } } }   \\notag   \\\\   & =   { \\left [ {    1-\\inf_{y : p_y(y)>0}\\max_x p_{x|y}(x|y )   } \\right]}^{{\\overline{t}}}.\\end{aligned}\\ ] ]    when @xmath15 is finite , we have @xmath138}^{{\\overline{t } } }   \\notag   \\\\   & \\leq   { \\mathrm{error}}({f_{\\mathrm{map } } } )   + { \\left [ {    1-\\frac1{|{\\mathcal{x}}| }   } \\right]}^{{\\overline{t } } }   \\label{eq : seq - iid - x}\\end{aligned}\\ ] ] from ( [ eq : seq - iid - inffmap ] ) , where the last inequality comes from the fact that @xmath139 for all @xmath12 implies @xmath140 we have the same bound ( [ eq : seq - iid - x ] ) from ( [ eq : seq - qiid - fmap ] ) when @xmath141 is the uniform distribution on @xmath2 for every @xmath12 .",
    "this implies that the stochastic decision with an i.i.d .",
    "sequence subject to the a posteriori distribution is at least as good as that subject to the uniform distribution .",
    "it should be noted that , when @xmath15 increases exponentially as the dimension of @xmath2 increases , @xmath127 should also increase exponentially to ensure that the second term @xmath142^{{\\overline{t}}}$ ] tends towards zero .    by letting @xmath143 in ( [ eq : seq - qiid - fmap ] ) , we have the following corollary .",
    "it should be noted that this corollary includes theorem  [ thm : random - fmap ] as the case @xmath143 and @xmath129 .",
    "let @xmath5 be a pair consisting of a state @xmath0 and an observation @xmath1 and @xmath4 be the joint distribution of @xmath5 .",
    "when we make a stochastic decision with a distribution @xmath26 the decision error probability @xmath53 defined by ( [ eq : error - random ] ) satisfies @xmath144,\\end{aligned}\\ ] ] where @xmath18 is given by ( [ eq : map ] ) .",
    "this section introduces applications of the stochastic decision with the a posteriori distribution to some coding problems .    for simplicity",
    ", we assume that we can obtain an ideal random number subject to a given distribution . for",
    "a given function @xmath145 on @xmath146 , let @xmath147 .",
    "for a given @xmath148 , let @xmath149 .",
    "this section introduces a stochastic decoder for a fixed - length lossless compression with an arbitrary small decoding error probability .",
    "let @xmath150 be the probability distribution of @xmath151 and @xmath152 be an encoding map , where @xmath153 is the codeword of @xmath154 .",
    "then the joint distribution @xmath155 of a source @xmath156 and a codeword @xmath157 are given as @xmath158 the decoder receives a codeword @xmath159 . by using a stochastic decoder with the distribution @xmath160 we have the bound of error probability from theorem  [ thm : random - fmap ] .",
    "this implies that , when we use the encoding map @xmath145 such that the decoding error probability by using the maximum a posteriori decoder vanishes as @xmath161 , the decoding error probability by using the stochastic decoder with @xmath162 also vanishes as @xmath161 .",
    "in addition , for a special case of source coding with no side information at the decoder  @xcite , the fundamental limit @xmath163 is achievable with this code , where @xmath163 is the spectrum sup - entropy rate of @xmath164 ( see  @xcite ) .",
    "it should be noted that the right hand side of the above equality is the output distribution of the constrained - random - number generator introduced in  @xcite .    for a given linear code for an additive noise channel , we can use the constrained - random - number generator as the stochastic decoder by letting @xmath164 be a channel noise , @xmath145 be a parity check matrix , and @xmath165 be the syndrome of @xmath164 .",
    "a channel encoder encodes a message to a channel input @xmath166 .",
    "a channel decoder receives a channel output @xmath167 , reproduces a channel noise @xmath168 from the syndrome @xmath169=a{\\boldsymbol{x}}$ ] by using the above scheme , and reproduces a message corresponding to @xmath170 , where the decoding is successful when @xmath168 is reproduced correctly from @xmath171 . from the above discussion , the decoding error probability is at most twice the decoding error probability of the maximal a posteriori decoder .",
    "this section introduces a stochastic decoder for the fixed - length lossless compression of a source @xmath164 with an arbitrary small decoding error probability , where the decoder has access to the side information @xmath172 correlated with @xmath164 .",
    "let @xmath173 be the joint distribution of @xmath174 and @xmath152 be an encoding map , where @xmath153 is the codeword of @xmath154 .",
    "then the joint distribution @xmath175 of a source @xmath156 , side information source @xmath176 , and codeword @xmath157 is given as @xmath177    the decoder receives a codeword @xmath159 and side information @xmath167 . by using a stochastic decoder with the distribution @xmath178",
    "we obtain the bound of error probability from theorem  [ thm : random - fmap ] .",
    "this implies that , when we use the encoding map @xmath145 such that the error probability of the maximum a posteriori decoder vanishes as @xmath161 , the error probability of the stochastic decoder with @xmath179 also vanishes as @xmath161 .",
    "in addition , the fundamental limit @xmath180 is achievable with this code  @xcite , where @xmath180 is the spectrum sup - entropy rate of @xmath164 ( see  ( * ? ? ?",
    "* theorems 4 and 5 ) ) .",
    "it should be noted that the right hand side of ( [ eq : side - information - decoder ] ) is the output distribution of the constrained - random - number generator introduced in  @xcite .",
    "this section introduces a stochastic decoder for the channel code introduced in  @xcite .",
    "let @xmath156 and @xmath176 be random variables corresponding to a channel input and a channel output , respectively .",
    "let @xmath181 be the conditional probability of the channel and @xmath150 be the distribution of the channel input .",
    "we consider correlated sources @xmath174 with the joint distribution @xmath182 .",
    "let @xmath152 be the source code with the decoder side information introduced in the previous section .",
    "let @xmath175 be the joint distribution defined by ( [ eq : joint - side - information ] ) .",
    "the decoder of this source code obtains the reproduction by using the stochastic decoding with the distribution defined by ( [ eq : side - information - decoder ] ) .",
    "let @xmath183 be the error probability of this source code .",
    "we can assume that for all @xmath184 , @xmath185 and all sufficiently large @xmath186 there is a function @xmath145 such that @xmath187 where a maximum a posteriori decoder is not assumed for this code .    when constructing a channel code , we prepare a map @xmath188 and a vector @xmath148 .",
    "we use the stochastic encoder with the distribution @xmath189 for a message @xmath190 generated subject to the uniform distribution on @xmath191 .",
    "it should be noted that the right hand side of the above equality is the output distribution of the constrained - random - number generator introduced in  @xcite .",
    "the decoder reproduces @xmath154 satisfying @xmath192 by using the stochastic decoder with the distribution given by ( [ eq : side - information - decoder ] ) and reproduces a message @xmath193 by operating @xmath194 on @xmath195 .    in the above channel coding , let us assume that @xmath196 , @xmath197 and the balanced - coloring property  @xcite of an ensemble @xmath198 , where @xmath199 is the spectrum inf - entropy rate of @xmath164 and @xmath200 .",
    "then , from ( * ? ? ?",
    "* theorem 1 ) and ( [ eq : errora ] ) , we have the fact that for all @xmath185 and all sufficiently large @xmath186 there are @xmath201 and @xmath148 such that the error probability @xmath202 of this channel code is bounded as @xmath203 it should be noted that the channel capacity @xmath204,\\ ] ] which is derived in  ( * ? ? ?",
    "* lemma 1 ) , is achievable by letting @xmath161 , @xmath205 , @xmath206 , @xmath207 , and @xmath164 be the general source that attains the supremum .      here",
    ", we comment on the decoding with a random sequence introduced in section  [ sec : seq ] . for decoding",
    ", we can use random sequences generated by markov chains ( random walks ) that converge to the respective stationary distributions ( [ eq : source - decoder ] ) and ( [ eq : side - information - decoder ] ) . in this case",
    ", we can apply theorem  [ thm : seq - random - approx ] to guarantee that the decoding error probability is bounded by twice the error probability of the maximum a posteriori decoding as the sequence becomes longer .",
    "we can also use random sequences by repeating stochastic decisions independently with the respective distributions ( [ eq : source - decoder ] ) and ( [ eq : side - information - decoder ] ) , where we can use independent markov chains with independent initial states to generate an i.i.d .  random sequence .",
    "in this case , we can use theorem  [ thm : seq - iid ] to guarantee that the decoding error probability tends to the error probability of the maximum a posteriori decoding as the sequence becomes longer .",
    "when implementing ( [ eq : ty ] ) for ( [ eq : source - decoder ] ) and ( [ eq : side - information - decoder ] ) , it is sufficient to calculate the value of the numerator on the right hand side of these qualities because the denominator does not depend on @xmath195 .",
    "the numerator value is easy to calculate when the base probability distribution is memoryless .",
    "this paper investigated stochastic decision and stochastic decoding problems .",
    "it is shown that the error probability of a stochastic decision with the a posteriori distribution is at most twice the error probability of a maximum a posteriori decision .",
    "a stochastic decision with the a posteriori distribution may be sub - optimal but acceptable when the error probability of another decision rule ( e.g.  the maximum a posteriori decision rule ) is small .",
    "furthermore , by generating an i.i.d .",
    "random sequence subject to the a posteriori distribution and making a decision that maximizes the a posteriori probability over the sequence , the error probability approaches exponentially the error probability of the maximum a posteriori decision as the sequence becomes longer .",
    "when it is difficult to make the maximum a posteriori decision but the error probability of the decision is small , we may use the stochastic decision rule with the a posteriori distribution as an alternative .",
    "in particular , when the error probability of the maximum a posteriori decoding of source / channel coding tends towards zero as the block length goes to infinity , the error probability of the stochastic decoding with the a posteriori distribution also tends towards zero .",
    "the stochastic decoder with the a posteriori distribution can be considered to be the constrained - random - number generator  @xcite implemented by using the sum - product algorithm or the markov chain monte carlo method ( see appendix ) .",
    "however , the trade - off between the computational complexity and the precision of these algorithms is unclear .",
    "it remains a challenge for the future .",
    "in this section , we introduce the algorithms for the constrained - random - number generator  @xcite , which generates random numbers subject to a distribution @xmath208 for a given matrix @xmath145 and vectors @xmath209 , @xmath210 , where @xmath211 is assumed to be memoryless , that is , there is @xmath212 such that @xmath213 for all @xmath214 and @xmath215 .",
    "we review the algorithms introduced in  @xcite and  @xcite , which make use of the sum - product algorithm and the markov chain monte carlo method , respectively .",
    "let @xmath217 be a family of subsets of @xmath218 .",
    "for a set of local functions @xmath219\\}_{i=1}^l$ ] , the sum - product algorithm  @xcite@xcite calculates a real - valued global function @xmath220 on @xmath2 defined as @xmath221 approximately , where the summation @xmath222 is taken over all @xmath154 except for the variable @xmath223 , and the function @xmath224 depends only on the set of variables @xmath225 .",
    "it should be noted that the algorithm calculates the global function exactly when the corresponding factor graph has no loop .",
    "let @xmath226 and @xmath227 be messages calculated as @xmath228 the summation @xmath229 is taken over all @xmath230 , @xmath231 when there is no @xmath232 such that @xmath233 and @xmath234 when @xmath235 .",
    "the sum - product algorithm is performed by iterating the above operations for every message @xmath227 and @xmath226 satisfying @xmath236 and finally calculating the approximation of the global function as @xmath237 where we assign initial values to @xmath226 and @xmath227 when they appear on the right hand side of the above operations and are undefined .    in the following ,",
    "we describe an algorithm for a constrained - random - number generator .",
    "let @xmath238 be an @xmath239 ( sparse ) matrix with a maximum row weight @xmath240 , where the set @xmath241 satisfies @xmath242 for all @xmath243 .",
    "then there is a set @xmath244 such that @xmath245 where @xmath246 is a @xmath247-dimensional vector and @xmath248 denotes the inner product of vectors @xmath246 and @xmath249 .",
    "let @xmath250 , where @xmath251 is a null string if @xmath252 .",
    "let @xmath253 .",
    "calculate the conditional probability distribution @xmath255 defined as @xmath256 where @xmath257 is a support function defined by ( [ eq : support ] ) .",
    "it should be noted that the sum - product algorithm can be employed to obtain ( [ eq : sum - product - fk ] ) , where @xmath258 and @xmath259 are local functions , and we substitute the generated sequence @xmath260 for ( [ eq : sum - product - fk ] ) . if @xmath261 is a constant after the substitution of @xmath260 , we can record the constant in preparation for the future .",
    "this section reviews an algorithm that employs the gibbs sampling  @xcite , which is a kind of markov chain monte carlo method . in the following algorithm",
    ", it is assumed that @xmath145 is a systematic matrix illustrated as @xmath266 where the left part @xmath267 is an @xmath268 $ ] matrix and the right part of @xmath145 is the @xmath269 identity matrix .",
    "it should be noted that , when a matrix @xmath145 is not systematic , the elementary transformation and the elimination of redundant rows can be used to obtain an equivalent condition represented by a systematic matrix .",
    "let @xmath270 be the @xmath271-th column of @xmath145 and let @xmath272 .",
    "let @xmath273 be the number of iterations .",
    "in the following , we describe an algorithm for a constrained - random - number generator . it should be noted that steps 2 , 5 , and 7 realize the stochastic decision defined by ( [ eq : ty ] ) and ( [ eq : fy ] ) , which may be skipped by outputting @xmath195 instead of @xmath274 in step 9 .",
    "[ lem : gibbs ] for given @xmath293 , let @xmath294 be the probability of @xmath195 at step 7 in the above algorithm , where we can take an arbitrary initial sequence satisfying @xmath192 at step 1 .",
    "then @xmath295 for all @xmath293 .",
    "s.  m.  aji and r.  j.  mceliece , `` the generalized distributive law , '' _ ieee trans .",
    "theory _ , vol .",
    "46 , no .  2 , pp .  325343 ,",
    ". j.  o.  berger , _ statistical decision theory and bayesian analysis _ , springer - verlag new york inc . , 1985 . t.  m.  cover , `` estimation by the nearest neighbor rule , '' _ ieee trans .  inform .",
    "theory _ , vol .",
    "14 , no .  1 ,",
    "pp .  5055 , jan .",
    "t.  m.  cover and p.  e.  hart , `` nearest neighbor pattern classification , '' _ ieee trans .",
    "theory _ , vol .  13 , no .  1 ,",
    "pp .  2127 , jan .",
    "t.  m.  cover and j.  a.  thomas , _ elements of information theory 2nd . ed .",
    "_ , john wiley & sons , inc . , 2006 .",
    "han and s.  verd , `` approximation theory of output statistics , '' _ ieee trans .  inform .",
    "theory _ , vol .",
    "it-39 , no .  3 , pp .",
    "752772 , may 1993 . w.  k.  hastings , `` monte carlo sampling methods using markov chains and their applications , '' _ biometrica _ , vol .",
    "57 , no .  1 ,",
    "pp .  97109 , 1970 . f.  r.  kschischang , b.  j.  frey , and h.  a.  loeliger , `` factor graphs and the sum - product algorithm , '' _ ieee transactions on information theory _",
    "47 , no .  2 , pp .",
    "498519 , feb .",
    "j.  muramatsu , `` channel coding and lossy source coding using a generator of constrained random numbers , '' _ ieee trans .  inform .",
    "theory _ , vol .",
    "it-60 , no .  5 , pp .  26672686 , may 2014 . j.  muramatsu , `` variable - length lossy source code using a constrained - random - number generator , '' _ ieee trans .",
    "theory _ , vol .",
    "it-61 , no .  6 , pp .  35743592 , jun",
    "j.  muramatsu and s.  miyake , `` construction of a channel code from an arbitrary source code with decoder side information , '' _ proc .",
    "int . symp . on inform .",
    "theory and its applicat .",
    "_ , moterey , usa , 2016 , pp .",
    "extended version is available at arxiv:1601.05879[cs.it ] , 2016 .",
    "y.  steinberg and s.  verd , `` channel simulation and conding with side information , '' _ ieee trans .",
    "theory _ , vol .",
    "it-40 , no .  3 , pp .",
    "634646 , may 1994 ."
  ],
  "abstract_text": [
    "<S> this paper investigates the error probability of a stochastic decision and the way in which it differs from the error probability of an optimal decision , i.e. , the maximum a posteriori decision . </S>",
    "<S> it is shown that the error probability of a stochastic decision with the a posteriori distribution is at most twice the error probability of the maximum a posteriori decision . </S>",
    "<S> furthermore , it is shown that , by generating an independent identically distributed random sequence subject to the a posteriori distribution and making a decision that maximizes the a posteriori probability over the sequence , the error probability approaches exponentially the error probability of the maximum a posteriori decision as the sequence length increases . </S>",
    "<S> using these ideas as a basis , we can construct stochastic decoders for source / channel codes .    </S>",
    "<S> channel coding , decision theory , error probability , maximum a posteriori decision , source coding , source coding with decoder side information , stochastic decision , stochastic decoding </S>"
  ]
}