{
  "article_text": [
    "let @xmath21 be a mean zero gaussian random vector in a separable hilbert space @xmath1 with covariance operator @xmath22 and let @xmath23 be a sample of @xmath24 i.i.d .",
    "copies of @xmath25 the sample covariance operator @xmath26 is defined as follows : @xmath27 denote by @xmath8 the @xmath7-th eigenvalue of @xmath6 ( in a decreasing order ) and by @xmath5 the corresponding spectral projector of @xmath6 ( that is , the orthogonal projector on the eigenspace of eigenvalue @xmath8 ) .",
    "let @xmath9 denote properly defined empirical counterpart of @xmath5 ( see section [ sec : pert ] for a precise definition ) .",
    "the main goal of the paper is to obtain a tight bound on the accuracy of normal approximation of the distribution of the squared hilbert ",
    "schmidt norm error @xmath20 of the estimator @xmath28 another goal is to provide bounds on the risk @xmath18 of this estimator as well as non - asymptotic bounds on concentration of random variables @xmath20 around its expectation .",
    "these bounds will be expressed in terms of natural complexity parameters of the problem , the most important one being the so called _ effective rank _",
    "@xmath29 that has been recently used in the literature ( see @xcite , @xcite , @xcite ) .",
    "the following quantity @xmath30 will be called the effective rank of @xmath31    here @xmath15 denotes the trace of @xmath6 and @xmath16 denotes its operator norm .",
    "the above definition clearly implies that @xmath32 a recent result by koltchinskii and lounici , see @xcite , shows that , in the gaussian case , the size of the operator norm error @xmath33 of sample covariance @xmath34 is completely characterized by @xmath16 and @xmath35 this makes the effective rank @xmath29 the crucial complexity parameter of the problems of estimation of covariance and its spectral characteristics ( its principal components ) that allows one to study principal component analysis ( pca ) problems in a unified dimension - free framework that includes their high - dimensional and infinite - dimensional versions ( functional pca , kernel pca , etc ) . as in the preceding paper @xcite , our goal is to study the problem in a  high - complexity setting \" , where both the sample size @xmath24 and the effective rank @xmath29 are large , although our primary focus is on the case when @xmath36 which implies operator norm consistency of both @xmath34 and @xmath28 this setting is much closer to high - dimensional covariance estimation and pca problems than to standard results on pca in hilbert spaces with a fixed value of @xmath15 ( see , for instance , @xcite ) that are commonly used in the literature on functional pca and kernel pca .",
    "it includes , in particular , high - dimensional _ spiked covariance models _",
    "( see @xcite , @xcite , @xcite ) in which @xmath37 where @xmath38 is an orthonormal basis of @xmath39 @xmath40 are the variances of @xmath41 independent components of the  signal \" , @xmath42 is the variance of the noise components and @xmath43 is the orthogonal projector on the linear span of the vectors @xmath44 where @xmath45 this models the covariance of a gaussian signal with @xmath41 independent components observed in an independent gaussian white noise .",
    "it is usually assumed that the number of components @xmath41 and the variances @xmath46 are fixed , but the overall dimension of the problem @xmath47 as @xmath48 is large , implying that @xmath49 and @xmath50 estimation of the components of the  signal \" @xmath51 is viewed as pca for unknown covariance @xmath31 it is common to consider a sequence of high - dimensional problems in spaces @xmath52 ( rather than explicitly embed the spaces @xmath53 into an infinite dimensional hilbert space @xmath1 ) . to assess the performance of the pca , the loss function @xmath54 where @xmath55 are unit vectors , was used in @xcite .",
    "a closely related loss function is defined by @xmath56 see , for instance , @xcite . in the case of spiked covariance model with @xmath57 and @xmath58 as @xmath59 the following asymptotic representation of the risk holds , @xcite : @xmath60(1+o(1 ) ) , j=1,\\dots , m.\\ ] ] under the assumption @xmath61 as @xmath62 the classical pca is known to yield inconsistent estimators of the eigenvectors , see , e.g. , @xcite . in @xcite , a thresholding procedure in spirit of diagonal thresholding of johnstone and lu @xcite was proposed and it was proved that it achieves optimality in the minimax sense for the loss @xmath63 under sparsity conditions on the eigenvectors of @xmath6 .    in this paper",
    ", we are not making any structural assumptions on the covariance operator @xmath64 such as the spiked covariance model , sparsity , etc , but rather study the problem in terms of complexity parameter @xmath35 we derive representations of the hilbert ",
    "schmidt risk @xmath18 of empirical spectral projectors in the case when @xmath36 that imply representation ( [ birnbaumresult ] ) for spiked covariance model .",
    "specifically , we prove that @xmath65 where @xmath66 and the operator @xmath67 is defined as @xmath68 in addition , we show that @xmath69 where @xmath70 and derive concentration bounds for random variable @xmath20 around its expectation .",
    "one of the main results of the paper is the following bound on the accuracy of normal approximation of random variable @xmath20 that holds under rather mild assumptions : @xmath71,\\end{aligned}\\ ] ] where @xmath72 denotes the standard normal distribution function .",
    "this bound implies that the distribution of random variable @xmath73 is asymptotically standard normal as soon as @xmath74 @xmath75 and @xmath76 which , in particular , implies that @xmath36 .    throughout the paper , for @xmath77",
    "the notation @xmath78 means that there exists an absolute constant @xmath79 such that @xmath80 similarly , @xmath81 means that @xmath82 for an absolute constant @xmath79 and @xmath83 means that @xmath78 and @xmath84 in the cases when the constant @xmath85 in the above bounds might depend on some parameter(s ) , say , @xmath86 and we want to emphasize this dependence , we will write @xmath87 @xmath88 or @xmath89 also , throughout the paper ( as it was already done in the introduction ) , @xmath12 denotes the hilbert ",
    "schmidt norm and @xmath90 the operator norm of operators acting in @xmath91 with a minor abuse of notation , @xmath92 denotes both the inner product of @xmath1 and the hilbert ",
    "schmidt inner product .",
    "we will also use the sign @xmath93 to denote the tensor product .",
    "for instance , for @xmath94 @xmath95 is a linear operator in @xmath1 defined as follows : @xmath96    in what follows , we will frequently prove exponential bounds for certain random variables , say , @xmath97 of the following type : for some constant @xmath79 and for all @xmath98 with probability at least @xmath99 @xmath100 often , it will be proved instead that the inequality holds with probability , say , @xmath101 in such cases , it is easy to rewrite the probability bound in the initial form by changing the value of the constant @xmath102 for instance , replacing @xmath103 by @xmath104 allows one to claim that with probability @xmath99 @xmath105 that holds for all @xmath106 in such cases , it will be said without further explanation that probability bound @xmath107 can be replaced by @xmath108 by adjusting the constants .",
    "in this section , we discuss recent bounds on the operator norm @xmath109 obtained in @xcite and several well known results of perturbation theory used throughout the paper ( see also @xcite ) .      in @xcite , it was proved that , in the gaussian case , moment bounds and concentration inequalities for the operator norm @xmath33 are completely characterized by the operator norm @xmath16 and the effective rank @xmath35 more precisely , the following theorems hold .",
    "[ th_operator ] let @xmath111 be i.i.d .",
    "centered gaussian random vectors in @xmath1 with covariance @xmath112 then , for all @xmath113 @xmath114    [ spectrum_sharper ] let @xmath111 be i.i.d .",
    "centered gaussian random vectors in @xmath1 with covariance @xmath112 then , there exist a constant @xmath79 such that for all @xmath115 with probability at least @xmath99 @xmath116.\\end{aligned}\\ ] ] as a consequence of this bound and ( [ e1/p ] ) , with some constant @xmath79 and with the same probability @xmath117.\\end{aligned}\\ ] ]      several simple and well known facts on perturbations of linear operators ( see kato @xcite ) will be stated in a form suitable for our purposes .",
    "the proofs of some of these facts that seem not to be readily available in the literature were given in @xcite ( see also koltchinskii @xcite and kneip and utikal @xcite for some bounds in the same direction ) .",
    "let @xmath118 be a compact symmetric operator ( in our case , the covariance operator of a random vector @xmath21 in @xmath1 ) with the spectrum @xmath119 the following spectral representation is well known to hold with the series converging in the operator norm : @xmath120 where @xmath8 denotes distinct non - zero eigenvalues of @xmath6 arranged in decreasing order and @xmath5 the corresponding spectral projectors .",
    "denote by @xmath121 the eigenvalues of @xmath6 arranged in nonincreasing order and repeated with their respective multiplicities .",
    "let @xmath122 and let @xmath123 denote the multiplicity of @xmath8 .",
    "define @xmath124 let @xmath125 for @xmath126 and @xmath127 the quantity @xmath128 will be called _ the @xmath7-th spectral gap , or the spectral gap of eigenvalue @xmath8_.",
    "let now @xmath129 be another compact symmetric operator in @xmath1 with spectrum @xmath130 and eigenvalues @xmath131 ( arranged in nonincreasing order and repeated with their multiplicities ) , where @xmath132 is a perturbation of @xmath31 by lidskii s inequality , @xmath133 thus , for all @xmath134 @xmath135 and @xmath136 assuming that the perturbation @xmath132 is small in the sense that    @xmath137    it is easy to conclude that all the eigenvalues @xmath138 are covered by an interval @xmath139 and the rest of the eigenvalues of @xmath140 are outside of the interval @xmath141.\\ ] ] moreover , under the assumption @xmath142 the set @xmath143 of the largest eigenvalues of @xmath140 consists of @xmath7  clusters \" , the diameter of each cluster being strictly smaller than @xmath144 and the distance between any two clusters being larger than @xmath145 thus , it is possible to identify clusters of eigenvalues of @xmath140 corresponding to each of the @xmath7 largest distinct eigenvalues @xmath146 of @xmath31 let @xmath147 be the orthogonal projector on the direct sum of eigenspaces of @xmath140 corresponding to the eigenvalues @xmath138 ( to the @xmath7-th cluster of eigenvalues of @xmath140 ) .",
    "the following  partial resolvent \" operator will be frequently used throughout the paper : @xmath148    we will need a couple of lemmas proved in @xcite ( see lemmas 1 and 4 therein ) :    [ lem - pert - spectral ] the following bound holds : @xmath149 moreover , @xmath150 where @xmath151 and @xmath152    [ lipsr ] let @xmath153 and suppose that @xmath154 suppose also that @xmath155 then , there exists a constant @xmath156 such that @xmath157",
    "let @xmath9 be the orthogonal projector on the direct sum of eigenspaces of @xmath34 corresponding to the eigenvalues @xmath158 ( in other words , to the @xmath7-th cluster of eigenvalues of @xmath159 see section [ sec : pert ] ) .",
    "we will state simple bounds for the bias @xmath160 and the `` variance '' @xmath161 that immediately imply a representation of the risk @xmath162    denote @xmath163 it is easy to see that @xmath164 and @xmath165 which implies that @xmath166 ( assuming that @xmath16 and @xmath167 are bounded away both from @xmath168 and from @xmath169 @xmath128 is bounded away from @xmath168 and @xmath170 ) .",
    "[ risk_bd ] the following bounds hold :    1 .   @xmath171 + and @xmath172 2 .",
    "in addition , @xmath173 where @xmath174 3 .",
    "if @xmath175 the sequences @xmath176 and @xmath177 are both bounded away from @xmath168 and from @xmath169 @xmath178 is bounded away from @xmath179 and @xmath180 then the following representation holds : @xmath181    in the case of spiked covariance model ( [ spike ] ) for all @xmath182 @xmath183 assuming that @xmath184 are fixed , @xmath185 and @xmath186 as @xmath74 it is easy to check that ( [ risk_bou ] ) implies bound ( [ birnbaumresult ] ) obtained in @xcite .",
    "recall the following relationship ( see lemma [ lem - pert - spectral ] ) @xmath187 where @xmath188 @xmath189 and @xmath190 clearly , @xmath191 ( due to the orthogonality of @xmath5 and @xmath192 ) .",
    "also , @xmath193 and @xmath194 are independent random variables ( since , by the same orthogonality property , they are uncorrelated and @xmath21 is gaussian ) .    to prove claim 1 , note that , since @xmath195 we have @xmath196 therefore , by bound ( [ remainder_a ] ) of lemma [ lem - pert - spectral ] , we get @xmath197 bound ( [ bias_a ] ) now follows from theorem [ th_operator ] .",
    "bound ( [ bias_b ] ) is also obvious since @xmath198 are operators of rank @xmath199 @xmath200 is of rank at most @xmath201 and @xmath202 is of rank at most @xmath203 thus , @xmath204 and the result follows from the previous bounds .    to prove claim 2 , note that @xmath205 therefore , @xmath206 the following representations are obvious : @xmath207 note that , by ( [ cep ] ) , due to orthogonality of @xmath208 and due to independence of @xmath209 @xmath210    next , note that @xmath211 recall that @xmath212 is of rank @xmath213 and @xmath214 quite similarly to ( [ bou_sre ] ) , one can prove that @xmath215 therefore , by theorem [ th_operator ] , we get @xmath216 as a consequence of ( [ ga_ga ] ) and ( [ go_go ] )",
    ", it easily follows that @xmath217 ( [ var_var_a ] ) and ( [ var_var_b ] ) now follow from ( [ gu_gu ] ) , ( [ ga_ga ] ) , ( [ go_go ] ) and ( [ ge_ge ] ) .",
    "claim 3 is an easy consequence of the first two claims due to the `` bias - variance decomposition '' @xmath218 ( see also ( [ a_n_bou  ] ) ) .",
    "the main goal of this section is to derive a concentration bound for the squared hilbert ",
    "schmidt error @xmath20 around its expectation .",
    "denote @xmath219    [ hs - conc ] suppose that , for some @xmath220 @xmath221 moreover , let @xmath115 and suppose that @xmath222 then , for some constant @xmath223 with probability at least @xmath99 @xmath224.\\ ] ]    note that the first term @xmath225 in the right hand side of ( [ hs - conc ] ) is dominant if @xmath75 and @xmath226 in the next section , it will be shown that under the same assumptions the random variable @xmath227 is close in distribution to the standard normal and , in addition , @xmath228    the main ingredient in the proofs of these results is a concentration bounds for the random variables @xmath229 given below .",
    "[ technical_2 ]    suppose that , for some @xmath220 condition ( [ cond_gamma ] ) holds .",
    "then , there exists a constant @xmath230 such that for all @xmath115 the following bound holds with probability at least @xmath231 @xmath232    it easily follows from theorem [ th_operator ] that under assumption ( [ cond_gamma ] ) @xmath233 which implies that @xmath234 theorem [ spectrum_sharper ] implies that for some constant @xmath235 and for all @xmath115 with probability at least @xmath108 @xmath236 we will first assume that @xmath237 with a sufficiently large constant @xmath238 ( the proof of the concentration bound in the opposite case will be much easier ) .",
    "this assumption easily implies that @xmath239 and , if @xmath240 @xmath241 denote @xmath242 then @xmath243    as before , denote @xmath244 the main part of the proof is the derivation of a concentration inequality for the function @xmath245 where , for some @xmath220 @xmath246 is a lipschitz function on @xmath247 with constant @xmath248 @xmath249 @xmath250 @xmath251 and @xmath252 is such that @xmath253 with a high probability .",
    "this inequality will be then used with @xmath254 together with theorem [ spectrum_sharper ] , it will imply bound ( [ remaind+ ] ) under the assumption ( [ case_odin ] ) .",
    "our main tool is the following concentration inequality that easily follows from gaussian isoperimetric inequality .",
    "[ gaussian_concentration ] let @xmath23 be i.i.d",
    ". centered gaussian random variables in @xmath1 with covariance operator @xmath31 let @xmath255 be a function satisfying the following lipschitz condition with some @xmath256 @xmath257 suppose that , for a real number @xmath258 @xmath259 then , there exists a numerical constant @xmath260 such that for all @xmath98 @xmath261    we have to check now that the function @xmath262 satisfies the lipschitz condition ( with a minor abuse of notation we view @xmath23 here as non - random vectors in @xmath1 rather than random variables ) .",
    "[ lipschitz_constant_2 ] suppose that , for some @xmath263 @xmath264 then , there exists a numerical constant @xmath223 such that , for all @xmath265 @xmath266    observe that @xmath267 also , note that @xmath200 is an operator of rank at most @xmath201 and @xmath202 has rank at most @xmath268 ( under the assumption that @xmath269 implying that @xmath9 is of rank @xmath167 ) .",
    "this allows us to bound the hilbert  schmidt norms of such operators in terms of their operator norms : @xmath270 thus , we get @xmath271 since @xmath272 if @xmath273 claims ( [ linear_perturb ] ) , ( [ remainder_a ] ) of lemma [ lem - pert - spectral ] imply that , under assumption ( [ con_delta  ] ) @xmath274 depending only @xmath275 .",
    "we will denote @xmath276 and @xmath277 using now ( [ linear_perturb ] ) , ( [ remainder_a ] ) , ( [ bdf ] ) and the fact that @xmath246 is bounded by @xmath278 and lipschitz with constant @xmath248 which implies that the function @xmath279 is lipschitz with constant @xmath280 we easily get that , under the assumptions @xmath281 the following inequality holds : @xmath282    using the lipschitz bound of lemma [ lipsr ] and ( [ linear_perturb ] ) , ( [ remainder_a ] ) of lemma [ lem - pert - spectral ] ,    we easily get that @xmath283 where @xmath284 depends only on @xmath275 .",
    "a similar bound holds in the case when @xmath285 ( when both norms are larger than @xmath286 the function @xmath246 is equal to zero and the bound is trivial ) .",
    "indeed , first consider the case when @xmath287 then , in view of ( [ bdf ] ) , we have @xmath288 on the other hand , if @xmath289 we have that @xmath290 and , taking into account assumption ( [ con_delta  ] ) , we can repeat the argument in the case ( [ assumee ] ) ending up with the same bound as ( [ bdassee ] ) with a positive constant ( possibly different from @xmath291 but still depending only on @xmath275 ) in the right hand side .",
    "the following bound ( see lemma 5 in @xcite ) provides a control of @xmath292 @xmath293 now substitute the last bound in the right hand side of ( [ bdassee ] ) and observe that , in view of ( [ bdf ] ) , the left hand side of ( [ bdassee ] ) can be also upper bounded by @xmath294 therefore , we get that with some constant @xmath295 @xmath296 \\bigwedge   2c_\\gamma m_r\\frac{\\delta^3}{\\bar g_r^3 } \\\\ & \\nonumber \\leq l_{\\gamma}m_r\\frac{\\delta^2}{\\bar g_r^3}\\biggl [ \\frac{\\|\\sigma\\|_{\\infty}^{1/2}+\\sqrt{2\\delta}}{\\sqrt{n } } \\biggl(\\sum_{j=1}^n \\|x_j - x_j'\\|^2\\biggr)^{1/2 } \\bigvee \\biggl(\\frac{1}{n } \\sum_{j=1}^n \\|x_j - x_j'\\|^2\\bigwedge \\delta   \\biggr)\\biggr].\\end{aligned}\\ ] ] using an elementary inequality @xmath297 we get @xmath298 this allows us to drop the last term in the maximum in the right hand side of ( [ bdee  ] ) ( since a similar expression is a part of the first term ) . this yields bound ( [ lip_lip_cc ] ) .    getting back to the proof of theorem [ technical_2 ]",
    ", it will be convenient to prove first a version of its concentration bound with a median instead of the mean .",
    "denote by @xmath299 a median of a random variable @xmath300 and define @xmath301 let @xmath302 and suppose that @xmath303 ( by adjusting the constants , one can replace this condition by @xmath115 as it is done in the statement of the theorem ) .",
    "under conditions ( [ cond_gamma ] ) and ( [ case_odin ] ) , @xmath304 for some @xmath305 thus , the function @xmath306 satisfies the lipschitz condition ( [ lip_lip_cc ] ) with some constant @xmath307 also , we have @xmath308 note that on the event @xmath309 @xmath310 therefore , @xmath311 quite similarly , @xmath312 it follows from lemma [ gaussian_concentration ] that with probability at least @xmath108 @xmath313 with some constant @xmath314 using the bound @xmath315 that easily follows from the definition of @xmath316 and the bound of theorem [ th_operator ] , we get that with some @xmath230 and with the same probability @xmath317 since @xmath318 and @xmath319 when @xmath320 we can conclude that with probability at least @xmath321 @xmath322 adjusting the value of the constant @xmath323 one can replace the probability bound @xmath321 by @xmath324    we will now prove a similar bound in the case when condition ( [ case_odin ] ) does not hold .",
    "then , @xmath325 it follows from bound ( [ bd_1 ] ) and the definition of @xmath200 that , for some constant @xmath326 @xmath327 we can now use the bounds of theorems [ th_operator ] and [ spectrum_sharper ] to show that under condition ( [ cond_gamma ] ) for some @xmath79 with probability at least @xmath108 @xmath328 in view of condition ( [ case_dva ] ) , we get from the last bound that with some @xmath329 with probability at least @xmath108 @xmath330 this easily implies the following bound on the median @xmath331 @xmath332 therefore , for some @xmath230 and for all @xmath98 with probability at least @xmath108 @xmath333 and the last bound was proved in both cases ( [ case_odin ] ) and ( [ case_dva ] ) .",
    "it remains to integrate out the tails of exponential bound ( [ exp_med ] ) to get the inequality @xmath334 with some @xmath335 which , along with ( [ exp_med ] ) , implies concentration inequality ( [ remaind+ ] ) .",
    "we now turn to the proof of theorem [ hs - conc ] .    in view of theorem [ technical_2 ] ,",
    "it is sufficient to obtain a concentration bound for @xmath336 this could be done by rewriting @xmath337 in terms of @xmath338-statistics and using the corresponding exponential bounds .",
    "however , we will follow a different ( more elementary ) path that directly utilizes the gaussiness of random variables @xmath339 the key ingredient is the following simple representation lemma . in",
    "what follows , @xmath340 means that random variables @xmath341 and @xmath300 have the same distribution .",
    "[ repr_d ] the following representation holds : @xmath342 where @xmath343 are the eigenvalues of the random matrix @xmath344 and @xmath345 , @xmath346 are i.i.d .",
    "copies of @xmath21 independent of @xmath347    note that @xmath348 since the operators @xmath349 and @xmath350 are orthogonal with respect to the hilbert  schmidt inner product and",
    "@xmath351 we have @xmath352 also , note that @xmath353 therefore , @xmath354    define the following mapping @xmath355 it can be extended in a unique way by linearity and continuity to a bounded linear operator @xmath356    recall that @xmath357 and @xmath358 are centered gaussian random variables and they are uncorrelated ( see the proof of theorem [ risk_bd ] ) .",
    "therefore , they are also independent . conditionally on @xmath359 the distribution of random operator @xmath360 is centered gaussian with covariance @xmath361 note that @xmath362 can be viewed as a symmetric operator acting in the eigenspace of eigenvalue @xmath363 and it is nonnegatively definite . thus , it has spectral representation @xmath364 where @xmath365 are its eigenvalues and @xmath366 are its orthonormal eigenvectors ( that belong to the eigenspace of @xmath8 ) .",
    "it follows that @xmath367 let @xmath368 be independent copies of @xmath21 ( also independent of @xmath23 ) .",
    "denote @xmath369 it is now easy to check that @xmath370 implying that conditional distributions of @xmath338 and @xmath371 given @xmath357 are the same . as a consequence ,",
    "the distribution of @xmath372 coincides with the distribution of random variable @xmath373    note that @xmath374 where @xmath375 are i.i.d .",
    "standard normal random variables , @xmath376 being an orthonormal basis of the eigenspace corresponding to @xmath377 in view of representation ( [ repr_ll ] ) , we get @xmath378 and , since @xmath379 and @xmath380 are independent , @xmath381 therefore , @xmath382    in order to control the right hand side in the above display , the following elementary lemma will be used .",
    "[ lem : chi - dev ] let @xmath383 be i.i.d .",
    "standard normal random variables .",
    "there exists a numerical constant @xmath384 such that for all @xmath385 @xmath386    by a simple computation , @xmath387 for all @xmath388 such that @xmath389 since @xmath390 for @xmath391 , we easily get @xmath392 this implies that for all @xmath388 satisfying the condition @xmath393 the following bound holds : @xmath394 the bound on @xmath395 now follows by a standard application of markov s inequality and optimizing the resulting bound with respect to @xmath396    similarly , @xmath397 since @xmath398 we get @xmath399 implying the bound on the lower tail .",
    "applying the bound of the lemma to the first term in the right hand side of relationship ( [ relay_a ] ) _ conditionally on @xmath400 _ we get that with probability at least @xmath108 @xmath401 since @xmath402 @xmath403 and @xmath404 the last bound can be rewritten as @xmath405 as to the second term in the right hand of ( [ relay_a ] ) , the following bound is straightforward : @xmath406 theorems [ th_operator ] and [ spectrum_sharper ] easily imply that for all @xmath115 with probability at least @xmath108 @xmath407 under additional assumptions @xmath408 @xmath409 this bound could be simplified as @xmath410 and it implies that @xmath411    thus , representation ( [ relay_a ] ) and bounds ( [ term_1 ] ) , ( [ term_2 ] ) imply that with probability at least @xmath108 @xmath412    to complete the proof , it is enough to combine bound ( [ l_r_conc ] ) with concentration inequality of theorem [ technical_2 ] , to use bound ( [ a_n_bou ] ) to control @xmath413 and to take into account conditions ( [ siml_assump ] ) to simplify the resulting bound .",
    "the main result of this section is the following theorem :    [ th : normal_approx ] suppose that , for some constants @xmath414 @xmath415 and @xmath416 suppose also condition ( [ cond_gamma ] ) holds with some @xmath417 then , the following bounds hold with some constant @xmath79 depending only on @xmath418 @xmath419\\end{aligned}\\ ] ] and @xmath420,\\end{aligned}\\ ] ] where @xmath72 denotes the distribution function of standard normal random variable .",
    "this result essentially means that as soon as @xmath75 and @xmath76 as @xmath48 ( for @xmath421 ) , the sequence of random variables @xmath73 is asymptotically standard normal .",
    "we will first establish the following fact that would allow us to replace @xmath422 in bound ( [ normal_approx_a ] ) by a normalizing factor @xmath423 in bound ( [ normal_approx ] ) .",
    "[ th : varvar ] suppose condition ( [ cond_gamma ] ) holds for some @xmath417 then the following bound holds with some constant @xmath424 @xmath425    bound ( [ variance_bd ] ) shows that , under the assumptions @xmath426 and @xmath427 we have @xmath428    note that in the case of spiked covariance model ( [ spike ] ) , for @xmath182 @xmath429 which , under the assumption that the parameters @xmath184 are fixed , but @xmath47 as @xmath48 yields that @xmath430 note also that @xmath431 thus , the condition @xmath186 implies @xmath76 as @xmath432 therefore , theorem [ th : varvar ] yields that @xmath433 moreover , the bounds on the accuracy of normal approximation of theorem [ th : normal_approx ] are of the order @xmath434,\\ ] ] so , the asymptotic normality of @xmath20 holds if @xmath47 and @xmath186 as @xmath435    in view of relationships @xmath436 and ( [ repr_ll ] ) ( see the proof of lemma [ repr_d ] ) , we have @xmath437\\notag\\\\ & \\hspace{1 cm } + \\frac{4}{n^2}\\mathrm{var}\\left ( \\mathbb e \\left [ \\sum_{k\\in \\delta_r } \\gamma_k \\|c_rx^{(k)}\\|^2 \\big| p_rx_1,\\ldots , p_r x_n \\right]\\right).\\end{aligned}\\ ] ] recall that @xmath343 , @xmath438 depend only @xmath439 and that @xmath345 , @xmath346 are independent of @xmath440 .",
    "thus , we get @xmath441=\\notag\\\\ & = \\e\\left[\\sum_{k\\in \\delta_r } \\gamma_k^2 \\mathrm{var}\\left ( \\|c_rx^{(k)}\\|^2 \\right ) \\right]=",
    "\\sum_{k\\in \\delta_r } \\e\\left[\\gamma_k^2\\right ] \\mathrm{var}\\left ( \\|c_rx^{(k)}\\|^2 \\right)\\notag\\\\ & =    \\e\\left[\\|\\gamma_r\\|_2 ^ 2\\right ] \\mathrm{var}\\left ( \\|c_rx\\|^2 \\right ) .",
    "\\ ] ] by an easy computation , @xmath442 and , for i.i.d .",
    "standard normal random variables @xmath443 @xmath444 therefore , @xmath445 = \\frac{b_r^2(\\sigma)}{4}\\left(1+\\frac{m_r+1}{n}\\right).\\ ] ] similarly , we have @xmath446\\right)= \\mathrm{var}\\left ( \\sum_{k\\in \\delta_r } \\gamma_k \\mathbb e \\left [ \\|c_rx^{(k)}\\|^2 \\right]\\right ) \\notag\\\\ & \\hspace{2 cm } = \\mathrm{var}\\left ( \\mathrm{tr}(\\gamma_r)\\right ) \\left(\\mathbb e \\left [ \\|c_rx\\|^2 \\right]\\right)^2 \\ ] ] and @xmath447\\right)^2=\\frac{1}{4 m_r^2 \\mu_r^2}a_r^2(\\sigma),\\ ] ] implying that @xmath448\\right ) = \\frac{a_r^2(\\sigma)}{2m_r n}.\\ ] ] it follows from ( [ var - le-1 ] ) , ( [ var - le-10 ] ) and ( [ var - le-11 ] ) that @xmath449    denote now @xmath450 and @xmath451 , @xmath452 . combining concentration bound of theorem [ technical_2 ] with the identity @xmath453",
    "we obtain that latexmath:[\\ ] ] which proves bound ( [ normal_approx_a ] ) .    to complete the proof of bound ( [ normal_approx ] ) , it is enough to use theorem [ th : varvar ] to replace the normalization with @xmath483 by the normalization with the standard deviation of @xmath484 to this end , note that @xmath485 under the assumptions @xmath486 and @xmath464 we get from theorem [ th : varvar ] that @xmath487 without loss of generality , we can and do assume that @xmath488 for a small enough constant @xmath384 so that @xmath489 ( otherwise , the bound of the theorem is trivial )",
    ". then @xmath490 combining this with bound of theorem [ hs - conc ] , we get that with probability at least @xmath108 @xmath491 using the last bound with @xmath103 defined by ( [ t_def ] ) , we easily get that @xmath492.\\ ] ]    the result now follows from ( [ b - ess ] ) , ( [ golova ] ) and ( [ hvost ] ) by proving bounds on @xmath493 similar to ( [ b - ess-1 ] ) , ( [ b - ess-2 ] ) .",
    "we start this section with deducing from the non - asymptotic bound of theorem [ th : normal_approx ] an asymptotic normality result . to this end , consider a sequence of problems in which the data is sampled from gaussian distributions in @xmath1 with mean zero and covariance @xmath494 let @xmath495 be a centered gaussian random vector in @xmath1 with covariance operator @xmath421 and let @xmath496 be i.i.d .",
    "copies of @xmath497 the sample covariance based on @xmath498 is denoted by @xmath499 let @xmath500 be the spectrum of @xmath501 @xmath502 be distinct nonzero eigenvalues of @xmath503 arranged in decreasing order and @xmath504 be the corresponding spectral projectors",
    ". as before , denote @xmath505 and let @xmath506 be the orthogonal projector on the direct sum of eigenspaces corresponding to the eigenvalues @xmath507    suppose that the spectral projector of @xmath503 to be estimated is @xmath508 the corresponding eigenvalue is @xmath509 its multiplicity is @xmath510 and its spectral gap is @xmath511 denote @xmath512 the following assumption on @xmath503 will be needed :            * 2 . *",
    "neither normal approximation bounds of theorem [ th : normal_approx ] , nor the asymptotic normality result of corollary [ th_2_norm ] could be directly used to construct confidence regions for spectral projectors of covariance operators or to develop hypotheses tests .",
    "the reason is that , in these results , the squared hilbert ",
    "schmidt norm @xmath521 is centered with its expectation and normalized with its standard deviation ( or , alternatively , with @xmath483 ) that depend on unknown covariance operator @xmath31 it would be of interest to develop  data - driven \" versions of these results , but this problem seems to be challenging and goes beyond the scope of the current paper . at the moment , we have only a partial solution ( that is far from being perfect ) of this problem in the case when the target spectral projector @xmath522 is one - dimensional ( that is , the eigenvalue @xmath523 is of multiplicity one ) .",
    "we briefly outline such a result below .",
    "assume that we are given a sample of size @xmath524 of i.i.d .",
    "centered gaussian vectors @xmath525 with common covariance operator @xmath503 . for",
    "each of the three subsamples of size @xmath526 define its sample covariance operator : @xmath527 let @xmath528 be the orthogonal projector onto the eigenspace associated with the eigenvalue @xmath529 of @xmath530 ( which is of multiplicity one with a high probability ) .",
    "similarly , @xmath531 and @xmath532 are the orthogonal projectors onto the eigenspaces associated with the eigenvalue @xmath533 of @xmath534 and the eigenvalue @xmath535 of @xmath536 respectively .",
    "denote @xmath537 it turns out that the statistic @xmath538 can be used as an estimator of the expectation @xmath539 while the statistic @xmath540 can be used to estimate the standard deviation @xmath541 ( note that @xmath542 was introduced and studied in @xcite as an estimator of a  bias parameter \" of empirical spectral projectors and empirical eigenvectors ) .",
    "moreover , it can be proved that , under assumption [ ass_sigma_n ] , the sequence @xmath543 converges in distribution to a cauchy type random variable .    for the spiked covariance model",
    "( [ spike ] ) with @xmath544 being fixed and @xmath47 as @xmath545 it is easy to find a simpler version of data - driven normalization with the limit distribution being standard normal . for simplicity , assume that @xmath546 so , the goal is to estimate the first principal components @xmath547 recall that in this case @xmath548 ( see ( [ b_r_large_p ] ) ) .",
    "thus , the following estimator of @xmath549 could be used : @xmath550 where @xmath551 and @xmath552 are the largest and the second largest eigenvalues of @xmath553 respectively . in the case of such a spiked covariance model ,",
    "assumption [ ass_sigma_n ] is equivalent to @xmath47 and @xmath554 under these assumptions , it is easy to prove that @xmath555      * 3 . * to illustrate the asymptotic behavior of standard pca , we consider the following spiked covariance setting .",
    "let @xmath558 be @xmath524 i.i.d .",
    "random vectors in @xmath559 with covariance @xmath560 , @xmath561 , @xmath562 where @xmath563 is an arbitrary unit vector in @xmath559 . for selected values of @xmath564",
    "we computed the statistic @xmath565 , @xmath566 and the empirical bias estimators @xmath567 , @xmath568 as well as the statistics ( [ pure - data - driven - stat ] ) and @xmath569 we performed @xmath570 replications of this experiment .    in table",
    "[ table-1-mean - risk ] , we compare the sample mean of the statistic @xmath565 denoted by @xmath571 ( that provides an estimator of the risk @xmath572 based on the repeated samples of size @xmath24 ) to the estimated risk @xmath573 for each individual sample and the first order approximation of the theoretical risk derived in ( [ risk - mean ] ) which can be computed easily in this model since @xmath574 .",
    "more precisely , in the second row of the table the sample means of @xmath575 over @xmath570 replications of the experiment are presented .",
    "the results show that @xmath573 provides a somewhat better approximation of the risk @xmath572 than the first order approximation ( [ risk - mean ] ) for small sample size . for relatively large sample size",
    ", the first order approximation ( [ risk - mean ] ) becomes more precise than the estimator @xmath573 .       finally , we compute empirical densities of the statistics ( [ pure - data - driven - stat ] ) and ( [ theory - stat ] ) and compare them with their respective theoretical limiting distributions in figure [ normal convergence ] . for ( [ theory - stat ] )",
    ", we also provide the empirical mean and variance .",
    "v.  koltchinskii .",
    "asymptotics of spectral projections of some random matrices approximating integral operators . in",
    "_ high dimensional probability ( oberwolfach , 1996 ) _ , volume  43 of _ progress in probability _ , pages 191227 .",
    "birkhuser , basel , 1998 ."
  ],
  "abstract_text": [
    "<S> let @xmath0 be i.i.d . </S>",
    "<S> gaussian random variables in a separable hilbert space @xmath1 with zero mean and covariance operator @xmath2 and let @xmath3 be the sample ( empirical ) covariance operator based on @xmath4 denote by @xmath5 the spectral projector of @xmath6 corresponding to its @xmath7-th eigenvalue @xmath8 and by @xmath9 the empirical counterpart of @xmath10 the main goal of the paper is to obtain tight bounds on @xmath11 where @xmath12 denotes the hilbert  </S>",
    "<S> schmidt norm and @xmath13 is the standard normal distribution function . </S>",
    "<S> such accuracy of normal approximation of the distribution of squared hilbert  </S>",
    "<S> schmidt error is characterized in terms of so called effective rank of @xmath6 defined as @xmath14 where @xmath15 is the trace of @xmath6 and @xmath16 is its operator norm , as well as another parameter characterizing the size of @xmath17 other results include non - asymptotic bounds and asymptotic representations for the mean squared hilbert  </S>",
    "<S> schmidt norm error @xmath18 and the variance @xmath19 and concentration inequalities for @xmath20 around its expectation .    and </S>"
  ]
}