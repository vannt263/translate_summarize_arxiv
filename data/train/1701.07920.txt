{
  "article_text": [
    "the regression analysis is a statistical methodology for predicting values of response ( dependent ) variables from a set of explanatory ( independent ) variables by investigating the relationships among the variables .",
    "the regression analysis is used for forecasting and prediction in a variety of areas , from economics to biology . when the relationship among the variables is expressed as a linear equation and the set of explanatory variables has more than one variable ,",
    "it is termed _ multiple linear regression_.",
    "the multiple linear regression model is the most popular model among the various variants of regression analyses .",
    "given a fixed set of explanatory variables , the goal of the multiple linear regression is to find the coefficients for the explanatory variables that minimize the fitting error .",
    "when the fitting error is measured as the sum of squared errors ( @xmath0 ) , optimal coefficients can be calculated by a formula .",
    "however , when the fitting error is measured as the sum of absolute errors ( @xmath1 ) , often referred as the least absolute deviation ( lad ) regression , there is no explicit formula available .",
    "@xcite first pointed out that lad regression is essentially a linear program ( lp ) , and @xcite formulated the problem as an lp .",
    "@xcite proposed an alternative approach by iteratively reweighting the square error to build lad regression . for a detailed review of algorithms for lad regression ,",
    "the reader is referred to @xcite and @xcite .",
    "the subset selection problem , also referred as _",
    "variable selection _ or _ model selection _",
    ", for multiple linear regression is to choose a subset of the set of explanatory variables to build an efficient linear regression model . in detail , given a data set with @xmath2 observations and @xmath3 explanatory variables , we want to use only @xmath4 @xmath5 explanatory variables to build a linear regression model .",
    "the goal is to decrease @xmath4 as much as possible while maintaining error loss relative small .",
    "@xcite stated that regression subset selection consists of two steps : ( 1 ) determining an objective to measure the efficiency of the model and ( 2 ) developing an algorithm to solve the problem and optimize the determined objective function .",
    "depending on the goal of the regression analysis , an objective function is defined to measure the efficiency of the model .",
    "given a subset of the explanatory variables and the corresponding coefficients , the objective function is typically defined based on the number of explanatory variables used and the errors that the regression model produces .",
    "criteria such as the mean square error ( @xmath6 ) , mean absolute error ( @xmath7 ) , adjusted @xmath8 , mallow s @xmath9 , etc , are in this category .",
    "on the other hand , there also exist objective functions that additionally take the regression coefficients into account by penalizing large regression coefficients . among many variants in this category , ridge and lasso regressions are the most popular one proposed by @xcite and @xcite , respectively .",
    "note that the objective functions in the second category do not explicitly take the number of explanatory variables used into account and require penalty parameters . in this paper",
    ", we focus on @xmath7 and @xmath6 .    after an objective function",
    "is chosen , an algorithm is needed to optimize the objective function value over all possible regression models .",
    "algorithms for optimizing @xmath6 have already been studied . among them , stepwise - type algorithms are frequently used in practice due to their computational simplicity and efficiency .",
    "an exact algorithm is to enumerate all possible regression models , but the computational cost is excessive . to overcome this computational difficulty , @xcite proposed a branch - and - bound algorithm , called _ leaps - and - bound _ , to find the best subset for @xmath6 without enumerating all possible subsets . for subset selection of lad regression",
    ", @xcite presented a mixed integer program ( mip ) to optimize @xmath1 given fixed @xmath4 .",
    "recently , @xcite proposed an mip based algorithm for optimizing sse and sae given fixed @xmath4 .",
    "a discrete first order method is proposed and used to warmstart the mip formulation , which is formulated based on specially ordered sets , @xcite , to avoid use of big m. however , to the best of authors knowledge , there is no direct mathematical formulation to optimize @xmath7 or @xmath6 and @xmath4 directly . for a detailed review of algorithms for subset selection ,",
    "the reader is referred to @xcite and @xcite .",
    "the mip model presented in @xcite can be compared to the models studied in @xcite , @xcite , and @xcite , as all of them include cardinality constraints . in table",
    "[ table_model_comparison ] , we summarize the properties of the mathematical programming models for regression subset selection in the literature , including the models in this paper . the models in @xcite , @xcite , and @xcite",
    "assume fixed @xmath4 and the cardinality constraint is explicit in the models .",
    "@xcite optimizes @xmath1 by introducing binary variables , whereas @xcite optimizes @xmath0 without introducing binary variables .",
    "@xcite provides a polyhedral study for the cardinality constrained models studied in @xcite and @xcite .",
    "in contrast to the models in @xcite , @xcite , and @xcite , we optimize @xmath6 and @xmath7 without fixing @xmath4 . the models in sections [ reg_subsection_formulation_subset_mae ] and [ reg_subsection_formulation_subset_mse ] do not have explicit cardinality constraint since the objective functions implicitly penalize large cardinality . with fixed @xmath4 , the model in section [ reg_subsection_formulation_subset_mae ] reduces to the model in @xcite .",
    "the difference between the models in section [ reg_subsection_formulation_subset_mse ] with fixed @xmath4 and @xcite is the existence of binary variables .",
    "the difference between the models in section [ reg_subsection_formulation_subset_mse ] with fixed @xmath4 and @xcite is the use of big m or specially ordered sets .",
    ".comparison of mathematical programming models for regression subset selection [ cols=\"<,<,<,<\",options=\"header \" , ]     which completes the proof .",
    "using lemmas [ lemma_size_x ] and [ lemma_size_suba ] , it is trivial to see that holds .      * for * @xmath10 * for * @xmath11 pick explanatory variable @xmath12 and @xmath13 explanatory variables randomly and generate new instances with the selected @xmath14 columns and @xmath2 observations solve and set @xmath15 * end - for * @xmath16 , @xmath17 @xmath18-@xmath19 , @xmath20 * end - for *    instead of trying to get a valid value of @xmath21 , we use a statistical approach to get an approximated value of @xmath21 for @xmath22 . in , we estimate a valid value of @xmath21 for each @xmath12 . in steps 2 - 5",
    ", we obtain 30 i.i.d .",
    "sample values of @xmath21 when explanatory variable @xmath12 is included in the regression model .",
    "then , in step 6 , we obtain the upper tail of the confidence interval . with 95% confidence , the true valid value of @xmath21 is less than @xmath23 in step 6",
    ". hence , we set @xmath24 for @xmath25 in and for the fat case ( @xmath26 ) .",
    "similar to @xmath27 in , @xmath28 can be defined as @xmath29 where @xmath30 is the mean squared error of an optimal regression model when @xmath31 . next , similar to and , we define @xmath32 while remains the same .",
    "finally , we obtain @xmath33 for the @xmath28 objective .",
    "note that is mixed integer quadratically constrained program that has @xmath34 variables and @xmath35 constraints .    for the core set algorithm ,",
    "similar to , we have @xmath36",
    "1 .   we generate response variable @xmath37 for @xmath38 .",
    "2 .   next ,",
    "@xmath39 explanatory variables are generated , in which each variable is correlated to @xmath40 with correlation coefficient @xmath41 .",
    "each already generated explanatory variable , we generate four explanatory variables that are correlated to the already generated variable with correlation coefficient @xmath42 .",
    "most of the publicly available implementations of stepwise algorithm consider aic , bic , or criteria other than @xmath7 and @xmath6 .",
    "the r statistics package leaps , @xcite , supports the adjusted @xmath8 objective , which is equivalent to optimizing @xmath6 .",
    "however , leaps can not handle the fat case , and there is no publicly available packages for the @xmath7 objective .",
    "further , we consider new objectives @xmath27 and @xmath28 for computational experiments .",
    "hence , we implement a stepwise algorithm that works for all of the objective functions we are considering for both the thin and fat cases",
    ".    closely follows the standard stepwise selection procedure except that it considers @xmath43 , the maximum number of explanatory variables allowed , given selection criterion obj .",
    "it starts with empty set in step [ algo_stepwise_line1 ] .",
    "then in step [ algo_stepwise_line2 ] , it iteratively adds an explanatory variable based on a greedy strategy ( one that minimizes the objective function most ) until there is no improvement or we reach @xmath43 explanatory variables .",
    "next in step [ algo_stepwise_line3 ] , we consider both the forward and backward direction to check if a neighboring set of @xmath44 is better .",
    "step [ algo_stepwise_line3 ] continues until there is no improvement .",
    "finally , we call",
    "_ stepwise_(@xmath3,obj ) with obj @xmath45 for the thin case , as @xmath46 is guaranteed for this case . for the fat case ,",
    "stepwise_(@xmath43,obj ) with @xmath47 and obj @xmath45 .    @xmath43 ( maximum cardinality of subset ) , obj ( selection criteria ) @xmath44 ( subset of explanatory variables ) @xmath48 [ algo_stepwise_line1 ] forward selection based on obj and update @xmath44 , until ( 1 ) there is no improvement or ( 2 ) @xmath49[algo_stepwise_line2 ] forward selection and backward elimination based on obj , and update @xmath44 as long as @xmath50 , until there is no improvement [ algo_stepwise_line3 ]      leaps , r package by @xcite , supports exhaustive search based on the leaps - and - bound algorithm ( leaps b&b ) of @xcite . in this section ,",
    "we investigate the empirical time complexities of leaps b&b and by checking the execution time to get an optimal solution for the instances used in .",
    "if it takes more than 1 hour , we quit the algorithm and record 3600 seconds instead . in , we present the average computational time of leaps b&b and mip for @xmath6 . figure ( [ fig : result_comptime_1 ] ) is the full size plot and figure ( [ fig : result_comptime_2 ] ) is with a truncated @xmath51 axis .",
    "axes @xmath52 and @xmath51 are instance sets and time in seconds . the circles and rectangles represent the average computational time of the mip model and leaps b&b , respectively . the plain and dotted lines are fitted exponential curves for mip and leaps , respectively . observe that the computation time for both algorithms increases superlinearly , yet the computational time of leaps b&b grows much faster .",
    "recall that both _",
    "core - heuristic _ and _ core - rand _",
    "take @xmath53 as input to decide the core set cardinality in . to decide the best @xmath53 value for each core set algorithm",
    ", we compare the performance of the algorithms with several @xmath53 values for selected instance sets .      1 .   for @xmath28 , there is no big difference between the three @xmath53 values .",
    "this is because the algorithm did not update the best solution after the first iteration and thus terminates the algorithm immediately .",
    "for this reason , the shape of the lines are very similar to those of the thin case result in figure ( [ fig : result_mse ] ) .",
    "we conclude that @xmath55 is best for the _ core - heuristic _ , as it gives a slightly larger average improvement .",
    "2 .   for @xmath27 , on the other hand , the shape of the lines seems random although @xmath54 are generally increasing as instance size increases .",
    "we observe that @xmath56 is best for ( 100,40),(100,50),(100,60 ) instances sets , and @xmath57 is best for the other instance sets .",
    "hence , we conclude that @xmath58 is the best for instance sets satisfying @xmath59 or @xmath60 . for all other instances ,",
    "we conclude @xmath57 is the best .        for _ core - random _ , we tested 30 instances in @xmath61 , where each instance set contains 10 instances .",
    "the three instance sets are selected to represent varying instance size .",
    "we refer the instance sets @xmath62 as small , medium , large size , respectively .",
    "we only tested the algorithm with the @xmath27 objective , since the algorithm behaves similarly for the @xmath28 objective . although _ core - random _ is designed to iterate infinitely , we executed it only for one hour since we are interested in if _ core - random _ defeats the mip model with the same one hour time limit .",
    "we rank the @xmath53 values over time based on the best objective function value the algorithm gives with each @xmath53 .                in",
    ", the average ranking for each @xmath53 is plotted .",
    "note that the ranking is close to 1 if the algorithm with the corresponding @xmath53 gives the best objective function value out of the three @xmath53 values , and the ranking is close to 3 for the opposite case . observe that @xmath56 is best in general for the small and medium size instances .",
    "however , @xmath55 outperforms @xmath56 for the large size instances , especially after 40 minutes .",
    "this is because we have a larger number of iterations with @xmath55 than with @xmath56 in 1 hour , and @xmath55 starts to take advantage after some time .    in order to check the performance from a different view , in , we plot the area charts over time .",
    "each area represents the percentage of the top ranked for each @xmath53 .",
    "the figure shows that @xmath56 is best for the small and medium size instances , whereas @xmath55 starts to outperform as iterations increases for the large size instances .",
    "this is in line with the observation from .      1 .   with a 10 minute time limit , @xmath56 is best for all sizes .",
    "2 .   with a 1 hour time limit",
    ", @xmath55 is best for large instances .",
    "hence , with the one hour time limit , we use @xmath55 if @xmath63 and @xmath56 otherwise ."
  ],
  "abstract_text": [
    "<S> subset selection in multiple linear regression is to choose a subset of candidate explanatory variables that tradeoff error and the number of variables selected . </S>",
    "<S> we built mathematical programming models for subset selection and compare the performance of an lp - based branch - and - bound algorithm with tailored valid inequalities to known heuristics . </S>",
    "<S> we found that our models quickly find a quality solution while the rest of the time is spent to prove optimality . </S>",
    "<S> our models are also applicable with slight modifications to the case with more candidate explanatory variables than observations . for this case </S>",
    "<S> , we provide mathematical programming models , propose new criteria , and develop heuristic algorithms based on mathematical programming </S>",
    "<S> .    * keywords . * multiple linear regression , subset selection , high dimensional data , mathematical programming , linearization </S>"
  ]
}