{
  "article_text": [
    "we are given five bands of photometric data from the sloan digital sky survey data release 6 .",
    "associated with each magnitude measurement is an error measurement and an extinction measurement used to correct the effect of galactic dust .",
    "for some objects we have spectroscopic redshifts , hereon denoted @xmath0 for a particular object .",
    "we wish to estimate photometric redshifts for non - spectroscopic objects which are represented by the spectroscopic sample .",
    "we are currently interested only in galaxies with @xmath1 , and we exclude other objects which are likely to `` contaminate '' our sample .",
    "we further choose for the moment not to consider luminous red galaxies ( lrgs ) , leaving us with some @xmath2 objects with which to train and test .",
    "in our current work we use magnitudes available in the new ubercal table ( padmanabhan 2007 ) for each object .",
    "we subtract sequential extinction - corrected magnitudes to get color features , named @xmath3 , @xmath4 , @xmath5 , and @xmath6 .",
    "for instance , our color @xmath3 is actually the magnitude difference @xmath7 and so on .",
    "given photometric colors and spectroscopic redshifts @xmath8 , @xmath9 where @xmath10 is the number of objects for which we have spectroscopic redshifts , and assuming the existence of a function @xmath11 mapping tuples @xmath12 to unique @xmath13 values , estimating @xmath14 becomes a regression problem , and we can choose from among any number of common methods of regression .",
    "we choose random forest regression .",
    "* random forests * ( breiman 2001 ) are ensembles of classification and regression trees ( breiman 1984 ) trained on bootstrap samples ( efron 1994 ) .",
    "a random forest is composed of one or more regression trees .",
    "trees are grown _",
    "mostly _ `` the normal way '' , at each node making a binary partition at the `` best '' point along the `` best '' axis . in random forests , however , rather than choosing the best axis at each node from the input space , it is chosen from a random subspace of the input space ( ho 1998 ) .",
    "nodes are split until a user - specified minimum number of inputs ( commonly five ) is reached in a node and that node is declared terminal , its value being defined as the mean of the values of its constituent inputs in the regression case . after a tree is grown , a new input is classified left or right starting at the root of the tree , moving down until it reaches its associated terminal node , and that node s value becomes the tree estimate for that input . the ensemble regression estimate for that input is then generally taken to be the mean of the corresponding regression tree estimates .    *",
    "our procedure * is to select a training set of @xmath15 objects uniformly at random and without replacement from our full sample . with this",
    "we train a random forest of @xmath16 trees yielding for each redshift to be estimated a set of individual tree estimates , called @xmath17 , @xmath18 .",
    "we have several options for aggregating these into a single forest estimate , and we choose a trimmed mean computed as follows .",
    "for a given test input we compute @xmath19 and the rms estimated tree error ( called @xmath20 ) around @xmath21 over the @xmath17 , then compute the mean over those @xmath17 which are within @xmath22 of @xmath21 , giving @xmath23 , which is our final redshift estimate for that input .",
    "our primary measure of accuracy is then the rms of @xmath24 over all inputs .    *",
    "our error model * falls out of the ensemble nature of random forests . in the following we use",
    "the untrimmed mean for notational simplicity , but the case with the trimmed mean differs only trivially .",
    "first define the regression tree errors for a given input as @xmath25 , @xmath26 , @xmath27 equal the number of trees in the forest ( @xmath16 in our case ) . then define the random forest regression error as @xmath28 , and it s trivial to show that @xmath29 .",
    "one can view the tree errors as instances of identically distributed random variables .",
    "assuming independence between trees ( or at least a way to correct for dependence ) , one can model the tree errors as random variables @xmath30 , @xmath26 , with @xmath31 some unknown distribution with parameters @xmath32 and @xmath33 _ unique to each input_. then the mean of these random variables is itself a random variable , call it @xmath34 , and the central limit theorem gives us that @xmath35 , thus we have a distribution for the random forest regression error for that input and we have convenient plugin estimates for its parameters .",
    "we then can further say that @xmath36 in practice we wo nt have @xmath0 for most objects , however we do find that our estimation error is zero mean ( or , rather , insignificantly non - zero mean ) , so the best easy estimate we have for @xmath32 is zero . if , for a given input , we knew some better value for @xmath32 , it would mean our forest did nt utilize some information , and figure  [ levelandbars]b shows this not to be the case .",
    "our estimate tells us nothing about whether we over- or under - estimated the actual value ; either outcome is equally likely conditioned on our redshift estimate .",
    "but we do get bias as a function of @xmath0 , and correcting this bias is a problem to be dealt with in future work .",
    "we select @xmath37 test objects uniformly at random from our held - out data ( our main sample minus our training set ) and use our forest to produce redshift and error distribution parameter estimates for each test object .",
    "the resulting rms error between trimmed means and corresponding @xmath0 over these objects is @xmath38 .",
    "figure  [ levelandbars]a characterizes our estimates , which are generally good with some slight bias visible near the origin due to the local skewness of the underlying redshift distribution .",
    "figure  [ levelandbars]b shows the binned mean difference between @xmath39 and @xmath0 as a function of @xmath39 ( where @xmath40 for each test input ) , and it indicates that our random forest extracted nearly all the information contained in the training data .",
    "equation 1 above gives us a simple way to test our error distribution parameter estimates .",
    "for each test input we set @xmath41 , @xmath42 ( pardon the flagrant abuse of notation ) , @xmath43 , @xmath44^{1/2}$ ] , and @xmath45 .",
    "the results over all test inputs should be distributed as iid standard normal random variables . however , in our tests , it turns out that setting @xmath45 , the number of trees in our forest and the number of ostensibly iid tree error estimates for each test input , yields a vertical spike after standardizing .",
    "in fact we discover empirically that in our case setting @xmath46 yields almost a perfect standard normal over all test inputs as shown in figure  [ errdist]a .",
    "we interpret this to mean that we have strong dependence between tree error estimates , and correcting for this algorithmically will be another subject of future work .",
    "moving on for now with the parameter estimates resulting from our hand - chosen value of @xmath46 , we next seek to verify that for given confidence levels @xmath47 , we have the expected number of errors between the corresponding level-@xmath47 critical values over all test inputs , and the quantile - quantile plot shown in figure  [ errdist]b is pleasing in that regard .",
    "thus we have good reason to believe in our error distribution parameter estimates .",
    "our rms error is consistent with results from previous studies using similar datasets , though slightly lower rms errors from different methodologies have been reported .",
    "we may yet gain by expanding our training data set as we have more than @xmath48 training objects in reserve and the computational complexity of random forests is quite modest ( in preliminary testing we trained multiple forests with multiple training sets on a core 2 duo 2.0ghz macbook in less than one week ; regression on all of our held - out data took several days using the same machine ) .",
    "future work will include better accounting for dependence between trees , investigating more deeply the behavior of our error distributions as a function of @xmath0 , addressing @xmath0-dependent bias , and extending estimation to objects not represented by the training data .",
    "as it is , the quality of the estimates , the per - object error distributions , and the computational efficiency make this approach an attractive option for photometric redshift estimation .",
    "breiman , l. , friedman , j.h . ,",
    "olshen , r.a . , & stone , c.j .",
    "1984 , _ classification and regression trees_. ( wadsworth ) breiman , l. 2001 .",
    "`` random forests '' .",
    "machine learning , 45 ( 1 ) , 5 efron , b. , & tibshirani , r. 1994 , _ an introduction to the bootstrap_. ( chapman & hall ) ho , t.k .",
    "`` the random subspace method for constructing decision forests '' .",
    "ieee trans . on pattern analysis and machine intelligence , 20 ( 8) , 832 padmanabhan , n. , et .",
    "al . 2007 .",
    "`` an improved photometric calibration of the sloan digital sky survey imaging data '' .",
    "astro - ph/0703454 ."
  ],
  "abstract_text": [
    "<S> given multiband photometric data from the sdss dr6 , we estimate galaxy redshifts . we employ a random forest trained on color features and spectroscopic redshifts from 80,000 randomly chosen primary galaxies yielding a mapping from color to redshift such that the difference between the estimate and the spectroscopic redshift is small . </S>",
    "<S> our methodology results in tight rms scatter in the estimates limited by photometric errors . </S>",
    "<S> additionally , this approach yields an error distribution that is nearly gaussian with parameter estimates giving reliable confidence intervals unique to each galaxy photometric redshift . </S>"
  ]
}