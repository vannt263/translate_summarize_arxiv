{
  "article_text": [
    "data clustering or data partitioning has emerged as the workhorse of _ exploratory data analysis_. this unsupervised learning methodology comprises a set of data analysis techniques which group data into clusters by either optimizing a quality criterion or by directly employing a clustering algorithm .",
    "the zoo of models range from centroid based algorithms like @xmath0-means or @xmath0-medoids , spectral graph methods like normalized cut , average cut or pairwise clustering to linkage inspired grouping principles like single linkage , average linkage or path - based clustering .",
    "the various clustering methods and algorithms ask for a unifying meta - principle how to choose the `` right '' clustering method dependent on the data source .",
    "this paper advocates a shift of viewpoint away from the problem _ `` what is the ` right ' clustering model ? '' _ to the question _ `` how can we algorithmically validate clustering models?''_. this conceptual shift roots in the assumption that ultimately , the data should vote for their prefered model type and model complexity@xcite .",
    "therefore , algorithms which are endowed with the ability to validate clustering concepts can maneuver through the space of clustering models and , dependent on the training and validation data sets , they can select a model with maximal information content and optimal robustness .    in this paper",
    ", we propose an information theoretic model validation strategy to select clustering models .",
    "a clustering model is used to generate a code for communication over a noisy channel .",
    "`` good '' models are selected according to their robustness to noise .",
    "the approximation precision of clustering solutions is controlled by an algorithm called empirical risk approximation ( era ) @xcite which quantizes the hypothesis class of clusterings .",
    "era  employs an hypothetical communication framework where sets of approximate clustering solutions for the training and for the test data are used as a communication code .",
    "approximations of the empirical minimizer with model averaging over approximate solutions favors stability of clusterings .",
    "furthermore , it is well known that stability based model selection @xcite yields highly satisfactory results in applications although the theoretical foundation of this model selection strategy is still controversially debated @xcite .",
    "given are a * set of objects * @xmath1 and measurements @xmath2 to characterize these objects .",
    "@xmath3 denotes the object or measurement space , respectively .",
    "such measurements might be @xmath4-dimensional vectors @xmath5 or relations @xmath6 which describe the ( dis)-similarity between object @xmath7 and @xmath8 .",
    "more complicated data structures than vectors or relations , e.g. , three - way data or graphs , are used in various applications . in the following ,",
    "we use the generic notation @xmath9 for measurements .",
    "we have to distinguish between objects and measurements since repeated measurements might refer to the same object .",
    "data denote object - measurement relations @xmath10 , e.g. , vectorial data @xmath11 describe surjective relations between objects @xmath7 and measurements @xmath12 .",
    "the * hypotheses * for a clustering problem are the functions assigning data to groups , i.e. , @xmath13 the parameter @xmath14 denotes the number of objects . in cases where @xmath9 uniquely identifies the object set @xmath15 , i.e. , there exists a bijective function between objects and measurements , then we omit the first argument of @xmath16 to simplify notation . a clustering",
    "is then denoted by @xmath17 .",
    "the * hypothesis class * for a clustering problem is defined as the set of functions assigning data to groups , i.e. , @xmath18 . for @xmath19 objects we can distinguish @xmath20 such functions .",
    "specific clustering models might require additional parameters @xmath21 which characterize a cluster , e.g. , the centroids in @xmath0-means clustering .",
    "the hypothesis class is then the product space of possible assignments and possible parameter values .",
    "exploratory pattern analysis and model selection for grouping requires to assess the quality of clustering hypotheses .",
    "various criteria emphasize coherency of data or connectedness , e.g. , @xmath0-means clustering measures the average distance of data vectors to the nearest cluster centroid or prototype .",
    "for the subsequent discussion on information theoretic model validation , a cost or risk function @xmath22 is assumed to measure how well a particular clustering with assignments @xmath23 and cluster parameters @xmath21 groups the objects . to simplify the notation , cluster parameters @xmath21",
    "are not explicitly listed as arguments of clustering costs but are subsumed in the specification of the cost function @xmath24 .",
    "a suitable metric for the space of hypotheses might be chosen based on such a cost function @xmath24 .",
    "the clustering solution @xmath25 minimizes the empirical risk ( erm ) of data clustering given the measurements @xmath9 , i.e. , @xmath26 clustering solutions which are similar in costs to the erm solution @xmath25 define the set @xmath27 of empirical risk approximations for clustering , i.e. , @xmath28 the set @xmath27 reduces to the erm solution in the limit @xmath29 .    to validate clustering methods we have to define and estimate the generalization performance of partitionings .",
    "we adopt the two sample set scenario with training and test data which is widely used in statistics and statistical learning theory @xcite i.e. to bound the deviation of empirical risk from expected risk , but also for two - terminal systems in information theory @xcite .",
    "we assume for the subsequent discussion that training data and test data are described by respective object sets @xmath30 and measurements @xmath31 which are drawn i.i.d . from the same probability distribution @xmath32 .",
    "furthermore , @xmath33 uniquely identify the training and test object sets @xmath34 so that it is sufficient to list @xmath35 as references to object sets @xmath36 .",
    "statistical inference requires that clustering solutions have to generalize from training data to test data since noise in the data renders the erm solution @xmath37 unstable .",
    "how can we evaluate the generalization properties of clustering solutions ? before we can evaluate the clustering costs @xmath38 on test data of the erm clustering on training data @xmath39 we have to identify a clustering @xmath40 which corresponds to @xmath39 . a priori ,",
    "it is not clear how to compare clusterings @xmath41 for measurements @xmath42 with clusterings @xmath43 for measurements @xmath44 .",
    "therefore , we define the mapping @xmath45 which identifies a clustering hypothesis for training data @xmath46 with a clustering hypothesis for test data @xmath47 .",
    "the reader should note that such a mapping @xmath48 might change the object indices . in cases when the measurements are elements of an underlying metric space , then a natural choice for @xmath48 is the nearest neighbor mapping @xmath49 where we identify clustering @xmath41 with @xmath50 .    the mapping @xmath48 enables us to evaluate clustering costs on test data @xmath44 for clusterings @xmath41 selected on the basis of training data @xmath42 .",
    "consequently , we can determine how many @xmath51-optimal training solutions are also @xmath51-optimal on test data , i.e. , @xmath52 .",
    "a large overlap means that the training approximation set generalizes to the test data , whereas a small or empty intersection indicates the lack of generalization .",
    "essentially , @xmath51 parametrizes a coarsening of the hypothesis class such that sets of data partitionings become stable w.r.t measurement fluctuations .",
    "the tradeoff between stability and informativeness is controlled by minimizing @xmath51 under the constraint of large @xmath53 for given risk function @xmath54 .",
    "code problems for communication by e.g. permuting the object indices . ]    in the following , we describe a communication scenario with a sender @xmath55 , a receiver @xmath56 and a problem generator @xmath57 where the problem generator serves as a noisy channel between sender and receiver .",
    "communication takes place by approximately optimizing clustering cost functions , i.e. , by calculating approximation sets @xmath58 .",
    "this coding concept will be refered to as approximation set coding ( asc ) .",
    "the noisy channel is characterized by a clustering cost function @xmath22 which determines the channel capacity of the asc scenario .",
    "validation and selection of clustering models is then achieved by maximizing the channel capacity over a set of cost functions @xmath59 where @xmath21 indexes the various clustering models .",
    "sender @xmath55 and receiver @xmath56 agree on a clustering principle @xmath60 and on a mapping function @xmath48 .",
    "the following procedure is then employed to generate the code for the communication process :    1 .",
    "sender @xmath55 and receiver @xmath56 obtain a data set @xmath42 from the problem generator @xmath57 .",
    "2 .   @xmath55 and @xmath56 calculate the @xmath51-approximation set @xmath61 .",
    "@xmath55 generates a set of ( random ) permutations @xmath62 to rename the objects .",
    "the permutations define a set of optimization problems @xmath63 with associated approximation sets @xmath64 .",
    "@xmath55 sends the set of permutations @xmath65 to @xmath56 who determines the approximation sets @xmath66 .",
    "the rationale behind this procedure is the following : given the measurements @xmath42 the sender has randomly covered the set of clusterings @xmath67 by respective approximation sets @xmath68 .",
    "communication succeeds if the approximation sets are stable under the stochastic fluctuations of the measurements .",
    "the criterion for reliable communication is defined by the ability of the receiver to identify a specific permutation that has been selected by the sender .",
    "the approximation sets @xmath69 play the role of codebook vectors in shannon s theory of communication .",
    "after this setup procedure , both sender and receiver have a list of approximation sets available or can algorithmically determine membership of clusterings in one of the @xmath70 approximation sets .",
    "how is the communication between sender and receiver organized ? during communication , the following steps take place as depicted in fig .",
    "[ fig:2 ] :    1 .",
    "the sender @xmath55 selects a permutation @xmath71 as message and send it to the problem generator @xmath57 .",
    "2 .   @xmath57 generates a new data set @xmath44 and it applies the selected permutation to @xmath44 , yielding @xmath72 .",
    "3 .   @xmath57",
    "send @xmath73 to the receiver @xmath56 without revealing @xmath71 .",
    "4 .   @xmath56 calculates the approximation set @xmath74 5 .",
    "@xmath56 estimates the applied permutation @xmath71 by using the decoding rule @xmath75    this communication channel supports to communicate at most @xmath76 nats if two conditions hold : ( i ) the channel is noise free @xmath77 ; ( ii ) all clusters have the same number of objects assigned to .",
    "it is worth mentioning that asc is conceptually not restricted to clustering problems although we focus the discussion here to this problem domain .",
    ", ( 2 ) the problem generator draws @xmath78 and applies @xmath71 to it , and the receiver estimates @xmath79 based on @xmath80 . ]",
    "to determine the optimal approximation precision for an optimization problem @xmath54 we have to determine necessary and sufficient conditions which have to hold in order to reliably identify approximation sets .",
    "reliable identification of approximation sets enable us to define a communication protocol using the above described coding scheme .",
    "therefore , we analyse the error probability of _ approximation set coding _ and the channel capacity which is associated with a particular cost function @xmath54 .",
    "this channel capacity will be refered to as _ approximation capacity _ since it determines the approximation precision of the coding scheme .",
    "a communication error occurs if the sender selects @xmath71 and the receiver decodes @xmath81 . to estimate the probability of this event",
    ", we introduce the sets @xmath82 the set @xmath83 measures the intersection between the approximation set @xmath84 for @xmath85-permuted measurements and the approximation set which has been calculated by the receiver based on the test data @xmath73 .",
    "the probability of a communication error is given by a substantial overlap @xmath86 with @xmath87 , i.e. , @xmath88 \\nonumber\\end{aligned}\\ ] ] the notation @xmath89 and @xmath90 is used .",
    "the inequality in ( [ proberr ] ) is caused by the union bound . the confusion probability with message @xmath91 for given training data @xmath42 and",
    "test data @xmath44 conditioned on @xmath71 is defined by @xmath92 & = & \\frac{1}{\\vert\\{\\sigma_j\\}\\vert } \\sum_{\\{\\sigma_j\\ } }      { \\mathbb{i}_{\\ {          \\log\\vert { \\delta\\mathcal{c}}_j \\vert \\ge \\log\\vert { \\delta\\mathcal{c}}_s          \\vert      \\ } } } \\nonumber\\\\ & \\stackrel{(a)}{\\le } & \\sum_{\\{\\sigma_j\\ } }      \\frac {          \\exp\\left(\\log\\vert { \\delta\\mathcal{c}}_j \\vert - \\log\\vert { \\delta\\mathcal{c}}_s            \\vert\\right )      } {          \\vert\\{\\sigma_j\\}\\vert      } \\nonumber\\\\ & = & \\frac{1}{\\vert\\{\\sigma_j\\}\\vert } \\sum_{\\{\\sigma_j\\ } }      \\frac{\\vert { \\delta\\mathcal{c}}_j \\vert}{\\vert{\\delta\\mathcal{c}}_s\\vert } \\nonumber\\\\ & \\stackrel{(b)}{= } & \\frac {      \\vert{\\mathcal{c}_\\gamma}(\\mathbf{x}^{(1)})\\vert      \\vert{\\mathcal{c}_\\gamma}(\\mathbf{x}^{(2)})\\vert } {      \\vert\\{\\sigma_j\\}\\vert \\vert{\\delta\\mathcal{c}}_s\\vert } \\nonumber\\\\ & \\stackrel{(c)}{= } & \\exp\\left (      -n \\mathcal{i}_\\gamma(\\sigma_j,\\hat{\\sigma } ) \\right ) \\label{evsigmaj}\\end{aligned}\\ ] ] the expectation @xmath93 $ ] in derivation ( [ evsigmaj ] ) is conditioned on @xmath71 which has been omitted to increase the readability of the formulas .",
    "the summation @xmath94 is indexed by all possible realizations of the transformation @xmath85 that are uniformly selected .",
    "( a ) we have used the inequality @xmath95 ; ( b ) averaging over a random permutation @xmath85 of object indices breaks any statistical dependence between sender and receiver approximation sets which corresponds to the error case in jointly typical coding @xcite ; ( c ) we have introduced the mutual information between the uniform distribution of the sender message @xmath85 and the receiver message @xmath96 @xmath97 to compactify the formula , the following notation is introduced : @xmath98 .",
    "the interpretation of eq .",
    "( [ minfo ] ) is straightforward : the first logarithm measures the entropy of the number of transformations which can be resolved with an uncertainty of @xmath99 in the space of clusterings on the sender side .",
    "the logarithm @xmath100 calculates the entropy of the receiver clusterings which are quantized by @xmath101 .",
    "the third logarithm measures the joint entropy of @xmath102 which depends on the size of the intersection @xmath103 .    inserting ( [ evsigmaj ] ) into ( [ proberr ] )",
    "yields the upper bound for the error probability @xmath104 the communication rate @xmath105 is limited by the mutual information @xmath106 for asymptotically error - free communication .",
    "the analysis of the error probability suggests the following inference principle for model selection : the approximation precision is controlled by @xmath51 which has to be minimized to derive more expressive clusterings . for large @xmath51 the rate @xmath107 will be low since we resolve the space of clusterings in only a coarse grained fashion . for too small @xmath51",
    "the error probability does not vanish which indicates confusions between @xmath85 and @xmath71 .",
    "the optimal @xmath51-value is given by the smallest @xmath51 or , equivalently the highest approximation precision @xmath108    another choice to be made in modeling is to select a suitable cost function for clustering @xmath54 .",
    "let us assume that a number of cost functions @xmath109 are considered as candidates .",
    "the cost function to be selected is @xmath110 where both the random variables @xmath111 and @xmath96 depend on @xmath22 .",
    "the selection rule ( [ cv_rule ] ) prefers the model which is `` expressive '' enough to exhibit high information content ( e.g. , many clusters ) and , at the same time robustly resists to noise in the data set . the bits or nats which are measured in the asc communication setting are context sensitive since they refer to a hypothesis class @xmath112 , i.e. , how finely or coarsely functions can be resolved in @xmath113 .",
    "to estimate the mutual information @xmath114 computationally , we have to calculate the size of the sets @xmath115 .    the cardinality @xmath116 is determined by the type of the empirical minimizer @xmath25 , i.e. , the probabilities @xmath117 with @xmath118 where @xmath119 denotes the entropy of the type of @xmath39 , ( @xmath120 ) .",
    "the cardinality of the approximation sets can be estimated estimated using concepts from statistical physics .",
    "the approximation sets @xmath121 are known as microcanonical ensembles in statistical mechanics .",
    "estimating their size is achieved up to logarithmic corrections by calculating the partition function @xmath122 the scaling factor @xmath123 , also know as inverse computational temperature , is determined such that the average costs of the ensemble @xmath61 yields @xmath124 .",
    "the weights @xmath125 are known as boltzmann factors .",
    "the joint entropy in the mutual information , which is related to the intersection @xmath126 involves a product of boltzmann factors .",
    "the identification of approximation sets with microcanonical ensembles provides access to a rich source of computational and analytical methods from statistical physics to calculate the mutual information @xmath127 .",
    "this analogy is by no means accidental since information theory and statistical mechanics are both specializations of empirical process theory with large deviation analysis of many particle systems .",
    "the central role of entropy and free energy is reflected in asc coding where the logarithm of the partition function arises in the mutual information ( [ minfo ] ) twice .",
    "the cardinalities of the approximation sets can also be numerically estimated by sampling using markov chain monte carlo methods or by employing analytical techniques like deterministic annealing @xcite .",
    "there exists a long history of information theoretic approaches to model selection , which traces back at least to akaike s extension of the maximum likelihood principle .",
    "aic penalizes fitted models by twice the number of free parameters .",
    "the bayesian information criterion ( bic ) suggests a stronger penalty than aic , i.e. , number of model parameters times logarithm of the number of samples .",
    "rissanen s minimum description length principles is closely related to bic ( see e.g. @xcite for model selection penalties ) .",
    "tishby et al @xcite proposed to select the number of clusters according to a difference of mutual informations which is closely related to rate distortion theory with side information .",
    "all these information criteria regularize model estimation of the data source .",
    "approximation set coding pursues a different strategy for the following reason : quite often the measurement space @xmath128 has a much higher `` dimension '' than the solution space .",
    "consider for example the problem of spectral clustering with @xmath0 groups based on dissimilarities @xmath129 : the measurements are elements of @xmath130 for real valued , symmetric weights with vanishing self - dissimilarities , but we can at most distinguish @xmath20 different clusterings .",
    "any approach which relies on estimating the probability distribution @xmath32 of the data ultimately will fail since we require far too many observations than needed to identify one hypothesis or a set of hypotheses , i.e. , one clustering or a set of clusterings .",
    "using an information theoretic perspective , we might ask the question how the uncertainty in the measurements reduces the resolution in the hypothesis class .",
    "how similar can two hypotheses be so that they are still statistically distinguishable given a cost function @xmath22 ?",
    "this research program is based on the idea that approximation sets of clustering cost functions can be used as a reliable code .",
    "the capacity of such a coding scheme then answers the question how sensitive a particular cost function is to data noise .",
    "model selection and validation requires to estimate the generalization ability of models from training to test data .",
    "`` good '' models show a high expressiveness and they are robust w.r.t .",
    "noise in the data .",
    "this tradeoff between _ informativeness _ and _ robustness _ ranks different models when they are tested on new data and it quantitatively describes the underfitting / overfitting dilemma . in this paper",
    "we have explored the idea to use approximation sets of clustering solutions as a communication code . since clustering solutions with @xmath0 clusters",
    "can be represented as strings of @xmath19 symbols with a @xmath0-ary alphabet , the significant problem of model order selection in clustering can be naturally phrased as a communication problem .",
    "the _ approximation capacity _ of a cost function provides a selection criterion which renders various models comparable in terms of their respective bit rates . the number of reliably extractable bits of a clustering cost function @xmath54 define a `` task sensitive information measure '' since it only accounts for the fluctuations in the data @xmath9 which actually have an influence on identifying an individual clustering solution or a set of clustering solutions .",
    "the maximum entropy inference principle suggests that we should average over the statistically indistinguishible solutions in the optimal approximation set @xmath131 .",
    "such a model averaging strategy replaces the original cost function with the free energy and , thereby , it defines a continuation methods with maximal robustness .",
    "the urgent question in many data analysis applications , which regularization term should be used without introducing an unwanted bias , is naturally answered by the entropy .",
    "the second question , how the regularization parameter should be selected , in answered by asc : choose the parameter value which maximizes the approximation capacity !",
    "asc for model selection can be applied to all combinatorial or continuous optimization problems which depend on noisy data .",
    "the noise level is characterized by two samples @xmath33 .",
    "two samples provide by far too little information to estimate the probability density of the measurements but two large samples contain sufficient information to determine the uncertainty in the solution space .",
    "the equivalence of ensemble averages and time averages of ergodic systems is heavily exploited in statistical mechanics and it also enables us in this paper to derive a model selection strategy based on two samples .",
    "future work also includes the study of algorithmic complexity issues .",
    "the question how hard are properly regularized optimization problems hints at a relationship between computational complexity and statistical complexity .",
    "the author appreciated valuable and insightful discussions with s. ben - david , t. lange , f. pla , v. roth and n. tishby .",
    "this work has been partially supported by the dfg - snf research cluster for916 and by the fp7 eu project simbad .",
    "shai ben - david , ulrike von luxburg , and dvid pl . a sober look at clustering stability . in g.  lugosi and h.u .",
    "simon , editors , _ learning theory , proceedings of 19th annual conference on learning theory , colt 2006 , pittsburgh , pa , usa _ , volume 4005 of _ lnai _ , pages 519 .",
    "springer - verlag berlin heidelberg , 2006 .",
    "naftali tishby , fernando  c. pereira , and william bialek .",
    "the information bottleneck method . in _ proc . of the 37-th annual allerton conference on communication , control and computing _ , pages 368377 , 1999 ."
  ],
  "abstract_text": [
    "<S> model selection in clustering requires ( i ) to specify a suitable clustering principle and ( ii ) to control the model order complexity by choosing an appropriate number of clusters depending on the noise level in the data . </S>",
    "<S> we advocate an information theoretic perspective where the uncertainty in the measurements quantizes the set of data partitionings and , thereby , induces uncertainty in the solution space of clusterings . a clustering model , which can tolerate a higher level of fluctuations in the measurements than alternative models , is considered to be superior provided that the clustering solution is equally informative . </S>",
    "<S> this tradeoff between _ informativeness _ and _ robustness _ is used as a model selection criterion . the requirement that data partitionings should generalize from one data set to an equally probable second data set </S>",
    "<S> gives rise to a new notion of structure induced information . </S>"
  ]
}