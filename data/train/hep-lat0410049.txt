{
  "article_text": [
    "the simulation codes of lattice gauge theory require substantial computing resources in order to calculate various matrix elements with sufficient precision to test the standard model against emerging experimental measurements .",
    "historically , these codes have demanded the use of large supercomputers at significant cost .",
    "both general purpose commercial supercomputers and custom , or `` purpose - built '' , supercomputers have been employed .",
    "traditional supercomputers came with very high prices .",
    "the price of purpose - built supercomputer hardware was lower , but the design and construction of such machines required significant amounts of engineering and physicist manpower .    in the last half decade , the performance of commodity computing equipment has increased to the point that tightly coupled clusters of such machines can compete with traditional supercomputers in capacity ( lattice size ) and throughput ( mflop / sec ) , and with purpose - built supercomputers in price / performance .",
    "commodity systems have been so successful across a wide spectrum of applications in many academic fields , that more than half of the supercomputers listed on the `` top500 ''  @xcite supercomputer list are clusters .    in this paper ,",
    "i discuss the requirements placed on clusters by lattice qcd codes and the historical performance trends of commodity computing equipment for meeting those requirements . extrapolating from these trends , together with vendor roadmaps ,",
    "allows prediction of the performance and price / performance of reasonable cluster designs in the next few years .",
    "inversion of the dirac operator ( _ dslash _ ) is the most computationally intensive task of lattice codes .",
    "the improved staggered action ( _ asqtad _ ) will be used throughout this paper for quantitative examples . during each iteration of the inversion of the improved staggered _",
    "dslash _ , eight sets of su(3 ) matrix - vector multiplies occur using nearest and next - next - nearest neighbor spinors .",
    "when domain decomposition is used on a cluster , ideally these floating point operations overlap with the communication of the hyper - surfaces of the sub - lattices held on neighboring nodes . using global sums ,",
    "the results of these sweeps over the full lattice are accumulated and communicated to all nodes in order to modify the spinors for the next iteration .",
    "_ dslash _ inversion throughput depends upon the floating point performance of the processors , the bandwidth available for reading operands from memory , the throughput of the i / o bus of the cluster nodes , and the bandwidth and latency of the network fabric connecting the computers . on any cluster , one of these factors will be the limiting factor which dictates performance for a given problem size .",
    "minimization of price / performance requires designs which balance these factors .",
    "most floating point operations in lattice codes occur during su(3 ) matrix - vector multiplies . for operands in cache",
    ", the throughput of these multiplies is dictated by processor clock speed and the capabilities of the floating point unit .",
    "table  [ matvec ] shows the performance of matrix - vector kernels on four intel processors introduced since the year 2000 .",
    "the `` c '' language kernels used are from the milc  @xcite code .",
    "the use of simd instructions on intel - brand and compatible cpus , as suggested by csikor _",
    "et al . _",
    "@xcite for amd k6 - 2 cpus and implemented for the intel sse unit by lscher  @xcite , can give significant performance improvements .",
    "table  [ matvec ] lists the performance of two styles of sse implementation .",
    "the first , site wise , uses a conventional data layout scheme with the real and imaginary pieces of individual matrix and vector elements adjacent in memory .",
    "the second , fully vectorized , follows pochinsky s  @xcite practice of placing the real components of the operands belonging to four consecutive lattice sites consecutively in memory , followed by the four imaginary components .",
    "whereas site wise implementations require considerable shuffling of operands in the sse registers in order to perform complex multiplies , the fully vectorized form requires only loads , stores , multiplies , additions , and subtractions .",
    ".su(3 ) matrix - vector multiply performance .",
    "results are given in mflop / sec . [ cols=\"<,^,^,^\",options=\"header \" , ]     given the historical performance trends , along with vendor roadmaps , we can attempt predictions of future lattice qcd cluster price / performance .",
    "these predictions are based upon the following assumptions :    * intel ia32 processors will be available at 4.0 ghz and 1066 mhz fsb in 2005 .",
    "* processors will be available either singly at 5.0 ghz , or in dual core equivalence ( _ e.g. _ , dual core 4.0 ghz processors ) in 2006 .",
    "* equivalent memory bus speed will exceed 1066 mhz by 2006 through fully buffered dimm technology or other advances .",
    "* the cost of high performance networks such as infiniband will drop as these networks increase in sales volume and the network interfaces are embedded on motherboards .",
    "the predictions assume that several new technologies are delayed by one year from their first appearance on current vendor roadmaps .",
    "for example , vendor roadmaps predict that 1066 mhz memory buses will appear in 2004 , dual core processors in 2005 , and fully buffered dimm technology in 2005 . by year ,",
    "the details of the predicted values in fig .",
    "[ predict ] , also summarized in table  [ tbl_predict ] are as follows . in mid-2004 ,",
    "the latest fermilab cluster used 2.8 ghz p4e systems at $ 900/node .",
    "the measured sustained performance of this cluster varies from approximately 900 to 1100 mflop / node , depending upon lattice layout ( _ i.e. _ , the number of directions of communications ) .",
    "a myrinet fabric from an older cluster was reused ; this fabric has an estimated replacement cost of $ 900 per node . in late 2004",
    ", a cluster based on 3.4 ghz p4e processors with and infiniband would sustain 1.4 gflop / node , based on the faster processors and the improved communications . in late 2005 , a cluster based on 4.0 ghz processors with 1066 mhz fsb would sustain 1.9 gflop / node , based upon faster processors and higher memory bandwidth . in late 2006 , a cluster based on the equivalent of 5.0 ghz processors with memory bandwidth greater than 1066 mhz fsb would sustain 3.0 gflop / node .",
    "the network fabrics used on clusters limit both achievable performance and cost effectiveness . as discussed previously , the largest single high performance network switches currently available are 288-port infiniband switches . to build a larger cluster based on such a switched network , cascading of multiple switches is required . to preserve bisectional bandwidth through the fabric , switches in a two - layer cascaded fabric have as many connections to other switches as they do to compute nodes . cascading increases the switch costs of a fabric .",
    "toroidal gigabit ethernet mesh designs do not have this limitation .",
    "however , the use of ethernet requires custom communications software to replace the traditional tcp / ip communications protocol ; tcp / ip introduces too much latency for lattice qcd codes .",
    "in contrast , the communications software which is supplied with networks such as myrinet and infiniband not only is widely used and robust , but it also requires no modification for lattice qcd . in terms of reduced custom software development , significant benefits may be derived from using popular high performance switched networks , even though the hardware costs may be greater .    the term `` strong scaling '' refers to the decrease in time to solve a fixed size problem as additional nodes are employed . communications latencies limit strong scaling . as node",
    "counts increase , the size of the local lattice stored on each node decreases , and so the size of the messages used to communicate neighboring hyperplanes also decreases . because of the dispersion of communications bandwidth with message size caused by latency , the decreasing bandwidth available with shorter messages will eventually limit the performance as the number of nodes increases .",
    "the reliability of the nodes in a cluster will limit the length of the longest calculation .",
    "typical mtbf figures for commodity computers are of order @xmath0 to @xmath1 hours . for @xmath2 nodes ,",
    "an mtbf of @xmath0 hours will result in an average of one hardware failure every 100 hours .",
    "operating system stability may play a role as well , with `` mean time between reboots '' similarly dictating maximum job lengths .",
    "this problem can be addressed by checkpointing long calculations at regular intervals , so that they may be restored at an intermediate position after cluster repair .",
    "note that switched networks are very tolerant of node failure in that a given sublattice may be relocated to any available node in the cluster at the start of the next job .",
    "mesh networks , on the other hand , are generally limited to nearest computer neighbor communications unless a large latency penalty is incurred .",
    "the loss of a node within one of the dimensions of a mesh architecture requires rewiring to route around the failed computer .",
    "since 1999 , pc clusters have exhibited steadily improving price / performance for lattice qcd ; the measured price / performance halving time for improved staggered codes over this time period was 1.25 years .",
    "performance trends indicate that balanced designs will be achievable on large scale clusters in the future . with the advent of , i / o bus designs will have more than sufficient bandwidth to match the communications requirements of many future generations of processors .",
    "networks such as infiniband similarly have excess bandwidth today , and vendor roadmaps indicate performance growth which will pace or exceed processor requirements .",
    "improvements in memory designs should provide sufficient memory bandwidth to balance faster processors .",
    "to date , the largest clusters in the us specifically devoted to lattice qcd have been no larger than 256 processors and have been based on myrinet or gigabit mesh networks . based on performance and cost trends ,",
    "it is clear that significant clusters will be constructed in the coming years .",
    "a 512 processor cluster in 2005 should sustain 1.9 gflop / sec per node on the improved staggered action at less than $ 1/mflop price / performance . by 2006 ,",
    "a cluster with several thousand processors should sustain multiple tflop / sec per node for less than $ 0.50/mflop .",
    "leveraging the results of the wide spread use of commodity clusters , these facilities will require neither specialized designs nor operational procedures .",
    "f. csikor _",
    "et al . _ ,",
    "* 134 * ( 2001 ) 139 , [ hep - lat/9912059 ] .",
    "m. lscher , nucl .",
    "b ( proc . suppl . ) * 106 * ( 2002 ) 21 .",
    "a. pochinsky , these proceedings . .",
    "s. gottlieb , physics.indiana.edu/ sg / pcnets/. z. fodor , s.d .",
    "katz , g. papp , comput .",
    "* 152 * ( 2003 ) 121 , [ hep - lat/0202030 ] .",
    "w. watson , private communication . ."
  ],
  "abstract_text": [
    "<S> in the last several years , tightly coupled pc clusters have become widely applied , cost effective resources for lattice gauge computations . </S>",
    "<S> this paper discusses the practice of building such clusters , in particular balanced design requirements . </S>",
    "<S> i review and quantify the improvements over time of key performance parameters and overall price to performance ratio . </S>",
    "<S> applying these trends and technology forecasts given by computer equipment manufacturers , i predict the range of price to performance for lattice codes expected in the next several years . </S>"
  ]
}