{
  "article_text": [
    "the literature on the _ jeffreys  lindley paradox _ has been prolific since it was brought to the attention of objective bayesians by @xcite .",
    "many authors have discussed this so - called paradox from varying perspectives ; including not only statisticians , but philosophers too .",
    "our aim is to consider the problem using simple mathematics .",
    "@xcite shows that , for point null hypothesis testing , there may be a concern with the objective bayesian approach .",
    "in the specific example used , if the prior for the location parameter , in the alternative model to the parameter being zero , has infinite variance , then the bayesian will always select the null model , regardless of the observed data .",
    "this was first suggested as a warning against using improper priors , but the consequences have now become far reaching with a substantial amount of literature written about the observation .",
    "let us describe the mathematical setting of the problem .",
    "suppose we wish to test the hypothesis @xmath0 for the normal model @xmath1 .",
    "let @xmath2 be the prior probability assigned to the null hypothesis and let @xmath3 , for some @xmath4 , be the prior distribution for the unknown parameter @xmath5 under the alternative model .",
    "then the bayes factor for this problem is given by @xmath6 which represents the odds in favour of the null hypothesis with respect to the alternative .",
    "the decision on whether one rejects @xmath7 in favour of @xmath8 is based on the posterior probability , given by @xmath9^{-1}.\\ ] ] this is the extent of the mathematical foundations to the problem . as lindley noted , there are some combinations of @xmath10 yielding a @xmath11 which one would not wish to countenance .",
    "the natural objective choice for @xmath12 involves taking @xmath13 .",
    "however , rather than a direct plug in of this value , a more general setting has been suggested and considered by @xcite which is to let @xmath14 depend on @xmath15 , i.e. we have @xmath16 , so then it is possible to study @xmath11 as @xmath17 . in this case",
    "we can identify three scenarios for @xmath11 as @xmath17 , all of which have associated problems .",
    "that is , there is no setting , i.e. choice of @xmath16 , in which the choice @xmath13 as an objective choice can work .",
    "what we mean by this is explained in section 2 .",
    "the conclusion is that the objective idea of @xmath13 does not work and consequently the message is not to use it . on the other hand , we can set the pair @xmath18 objectively using ideas of type i error calculations and a novel approach to the selection of priors for models .",
    "the layout of the paper is as follows . in section [ sc_paradox ]",
    "we formalise the jeffreys  lindley paradox and discuss @xcite solution to it .",
    "section [ sc_our ] is dedicated to the our approach , and section [ sc_disc ] is reserved to conclusions and final comments .",
    "in order to discuss approaches to the jeffreys  lindley paradox , let us first formalise it and , at the same time , define the notation .",
    "the objective is to compare the two normal models , @xmath19 @xmath20 to apply the bayesian approach , as described in section [ sc_intro ] , we need to define both the priors ; i.e. the value of @xmath15 , for the unknown parameter @xmath5 , and the prior for the null hypothesis ; i.e. the value of @xmath14 . to be most general we will assume that @xmath14 can depend on @xmath15 and hence we write it as @xmath16 .    with this information we can compute the bayes factor representing the odds of the null hypothesis @xmath7 .",
    "that is @xmath21 so the posterior probability for the null hypothesis is given by @xmath22^{-1}=\\left[1+\\frac{1-\\rho_0}{\\rho_0}\\frac{e^{-\\half x^2/(\\sigma^2 + 1)}}{e^{-\\half x^2}}\\frac{1}{\\sqrt{\\sigma^2 + 1}}\\right]^{-1}.\\ ] ]    we note in ( [ eq_intro_2 ] ) that the quantity @xmath23 is the key term and opens the way to understanding the paradox .",
    "we now assume the decision maker wants to select @xmath13 in order to implement an objective bayesian approach . to adequately understand this procedure",
    "we argue the decision maker needs to specify @xmath24 as @xmath25 , and to this end we identify 3 important and exhaustive cases :    \\(i ) @xmath26 . under this scenario",
    "we have the undesirable result that @xmath27 converges to one regardless of the @xmath28 value .",
    "this is the so - called paradoxical result .",
    "in fact , @xmath29 whenever for large @xmath15 we have , for any @xmath30 , @xmath31 so if the prior on the null hypothesis is too large as @xmath25 ; i.e. @xmath32 , then the posterior probability on the null hypothesis will converge to 1 .",
    "\\(ii ) @xmath33 for some constant @xmath34 . under this scenario",
    "it is that , for large @xmath15 , @xmath35 in particular , @xcite presents an objective argument for @xmath36 however , this idea leads to an undesirable inconsistency in that @xmath37 yet @xmath11 is converging to a constant bounded away from 0 .",
    "thus , with @xmath13 , we have @xmath38 but @xmath39 , which are incoherent choices .",
    "\\(iii ) @xmath40 . under this scenario",
    "we have that @xmath41 .",
    "this at least now becomes consistent with the prior probability since @xmath37 in this case . yet undesirable in that with @xmath13 , @xmath42 .",
    "these considerations clearly exclude the choice @xmath13 .",
    "it simply does not work .",
    "thus a finite choice of @xmath15 is required . in the next section",
    "we will demonstrate how we can set @xmath18 objectively .",
    "given a value of @xmath15 we first , in section [ sc_rho ] , show how to obtain an objective choice for @xmath16",
    ". then , in section [ sc_sigma ] , we show how @xmath44 can be selected objectively .",
    "our approach consists in measuring the _ worth _ of the alternative hypothesis with respect to the null , as outlined in @xcite . in particular , we apply the well known asymptotic bayesian property that , if a model is misspecified , the posterior accumulates at the model which the nearest , in terms of kullback  leibler divergence , to the true model @xcite . as such , the divergence @xmath45 represents the loss we would incur if model @xmath46 is removed and it is true . since we do not know @xmath5 , but we have the prior @xmath12 , we can compute the expected loss as @xmath47 the model prior is determined by means of the _ self - information _ loss function @xcite , which represents the loss connected to a probability statement . for model @xmath48 ,",
    "the self - information loss is given by @xmath49 .",
    "therefore , by equating the self - information with the expected loss determined in ( [ eq_ourprior1 ] ) , we have that the prior on the alternative model is @xmath50 note that the prior for the null hypothesis is @xmath51 , and so we have @xmath52 this then fits into category ( iii ) for large @xmath15 , which implies that @xmath53 goes to zero as @xmath54 .",
    "thus there is coherence in this approach ; however , we are not advocating the choice of large @xmath15 .      in any classical test",
    "the type i error is of key importance .",
    "we can use this quantity to objectively set the value for @xmath15 ; if indeed the type i error is an objective quantity , but nevertheless it needs to be set , and a valid objective bayesian criterion is to match classical benchmarks and quantities .    to determine an appropriate value for @xmath15 based on the classical concept of type i error",
    ", we would select @xmath15 so that @xmath55 where @xmath56 and @xmath57 is the probability under the null hypothesis .",
    "regardless of the surroundings , all bayesian experimenters in this problem would need to assign an @xmath58 value for which one would reject @xmath7 if @xmath59 .",
    "to have @xmath60 we require @xmath61 therefore , if we write @xmath62 we have ( [ nedo ] ) as @xmath63=\\alpha.\\ ] ]    the key here is that @xmath64 is decreasing as @xmath15 increases , so there is a one - to - one correspondence between @xmath65 and @xmath15 satisfying ( [ eq_sigma_2 ] ) .",
    "figure [ fig1 ] shows the behaviour of @xmath66 , given @xmath67 . as it must be that @xmath68 , we compute @xmath69 up to @xmath70 , which is the value that ensures @xmath71 , therefore , a positive @xmath64 .    , with @xmath67 , for @xmath72 . by setting @xmath73",
    "we ensure that @xmath68 . ]",
    "expression ( [ eq_sigma_2 ] ) has to be solved numerically .",
    "so , for example , if @xmath74 , we would have @xmath75 .",
    "in other words , we can be objective about @xmath15 with a finite value .",
    "the notion therefore that an objective @xmath15 and @xmath13 is the only choice is wrong .",
    "an objective classical test requires an @xmath65 value and it is this which can be linked to the ( finite ) objective choice for @xmath15 .",
    "the findings of this paper can be summarised as follows .",
    "the posterior for the point null hypothesis is driven by the quantity @xmath24 ; in particular , if @xmath13 is desired as an objective criterion then the behaviour of @xmath24 as @xmath17 is the key .",
    "if the prior @xmath16 is fixed , e.g. is equal to @xmath76 , then the jeffreys  lindley paradox arises , since the posterior probability @xmath27 goes to one .",
    "@xcite proposed to solve the issue by having @xmath24 to converge to a positive constant .",
    "although the direct paradox is avoided , the approach gives an incoherent result as the posterior mass on @xmath7 is positive whereas the prior mass is zero .",
    "our approach gives a quantity @xmath24 which goes to infinity , for @xmath15 going to infinity , which both solves the paradox and yields zero posterior mass for @xmath7 when @xmath77 , implying the prior mass for @xmath7 is zero .",
    "it is clear that the three types of behaviour of @xmath24 for large @xmath15 rule out the possibility of having @xmath13 . as such",
    ", @xmath15 has to be determined to have a finite value .",
    "for @xmath16 , the choice can be either objective or subjective .",
    "our approach allows @xmath16 to be determined in an objective fashion by considering the loss in information if the true model is removed .",
    "@xcite , on the other hand , propose a prior for the null hypothesis that is subjective .",
    "@xcite focus on models for which the use of a multivariate normal prior is appropriate , such as linear regression models , generalised linear models and standard time series models .",
    "the idea is to set the multiplicative constant for the prior dispersion matrix , @xmath78 , which will indicate the level of prior uncertainty .",
    "the authors aim to reduce the sensitivity of the posterior model probabilities to the scale of the prior by suitably specifying the prior model probability .",
    "this is done by setting @xmath79 where @xmath80 is the dimension of the model @xmath48 and @xmath81 is a suitably determined base line prior model probability . @xcite",
    "recommend @xmath82 , although other choices are possible .",
    "we see that the core of the whole approach is to make a prior model probability dependent on the variance of the prior in the parameters , avoiding the jeffreys  lindley paradox .",
    "+ the conclusion is that it is not possible to be objective for @xmath12 by setting @xmath13 .",
    "this is not the sole case where objective bayes fails to deliver adoptable solutions .",
    "for example , jeffreys rule prior for multidimensional parameter spaces gives prior distribution with poor performance properties @xcite .",
    "it is common practice not to use jeffreys prior in these type of problems and opt for a different solution , such as reference priors .",
    "however , an objective and finite value of @xmath15 can be assigned by exploiting thinking behind classical tests and setting the type i error .",
    "that is , there is a one - to - one correspondence between @xmath15 and the type i error @xmath65 and it is this correspondence which permits the interpretation and assignment of @xmath15 .",
    "surprisingly , or not , there have been philosophical papers attempting to find some hidden profound explanation behind the paradox ; see , for example , the recent papers of @xcite and @xcite .",
    "we argue that it is not necessary to philosophize , as the mathematics of the problem are quite straightforward and a clear picture of what is happening can be understood solely by mathematical considerations .    to discuss some of the philosophy , @xcite says :  the question that generally arises is why the bayesian and the likelihoodist approaches give rise to the above conflicting and confusing results \" .",
    "however , we have @xmath83 , which is precisely the form of the classical test !",
    "the classical test is : reject @xmath7 if @xmath84 , where @xmath85 .",
    "we can then set @xmath86 to ensure a standard value for the type i error .",
    "consequently , bayes makes no contribution to this problem , since even a subjective bayesian approach will yield a classical test , but with perhaps a non - standard type i error .",
    "such an observation between bayesian and classical tests has been made by @xcite .    in short , both bayesian and classical tests",
    "reject @xmath7 if @xmath87 , and this is the obvious procedure for testing @xmath88 .",
    "how one determines @xmath89 makes the difference , either via a type i error , @xmath65 , or via a prior @xmath12 , i.e. @xmath15 , but nevertheless there is a one - to - one correspondence between the two ."
  ],
  "abstract_text": [
    "<S> this paper is concerned with the well known jeffreys  lindley paradox . in a bayesian set up , </S>",
    "<S> the so - called paradox arises when a point null hypothesis is tested and an objective prior is sought for the alternative hypothesis . in particular , the posterior for the null hypothesis tends to one when the uncertainty , i.e. the variance , for the parameter value goes to infinity . </S>",
    "<S> we argue that the appropriate way to deal with the paradox is to use simple mathematics , and that any philosophical argument is to be regarded as irrelevant .    _ some key words_:factor , bayesian hypothesis testing , kullback  leibler divergence , self - information loss </S>"
  ]
}