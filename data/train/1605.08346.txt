{
  "article_text": [
    "recurrent neural networks ( rnns ) have drawn interest from researchers because of their effectiveness at processing sequences of data  @xcite . while deep networks have shown remarkable performance improvements at task such as image classification , rnns",
    "have recently been successfully employed as layers in conventional deep neural networks to expand these tools into tasks with time - varying data  @xcite .",
    "this inclusion is becoming increasingly important as neural networks are being applied to a growing variety of inherently temporal high - dimensional data , such as video  @xcite , audio  @xcite , eeg data  @xcite , two - photon calcium imaging  @xcite . despite the growing use of both deep and recurrent networks , theory characterizing the properties of such networks remain relatively unexplored . for deep neural networks ,",
    "much of the computational power is often attributed to flexibility in learned representations  @xcite .",
    "the power of rnns , however , is tied to the ability of the recurrent network dynamics to act as a distributed memory substrate , preserving information about past inputs to leverage temporal dependencies for data processing tasks such as classification and prediction . to understand the success and limitations of rnns , it is critical that we advance our analysis of the fundamental memory properties of these network structures .",
    "there are many types of recurrent network structures that have been employed in machine learning applications , each with varying complexity in the network elements and the training procedures . in this paper",
    "we will focus on rnn structures known as echo state networks ( esns ) .",
    "these networks have discrete time continuous - valued nodes @xmath0}\\xspace\\in\\ensuremath{\\mathbb{r}}\\xspace^\\ensuremath{m}\\xspace$ ] that evolve at time @xmath1 in response to the inputs @xmath2}\\xspace\\in\\ensuremath{\\mathbb{r}}\\xspace^\\ensuremath{l}\\xspace$ ] according to the dynamics : @xmath3}\\xspace = f(\\ensuremath{\\bm{w}}\\xspace\\ensuremath{\\bm{{x}}[n]}\\xspace + \\bm{z}\\ensuremath{\\bm{{s}}[n]}\\xspace + \\ensuremath{\\bm{\\epsilon}[n]}\\xspace ) ,      \\label{eqn : gennndyn } \\end{gathered}\\ ] ] where @xmath4 is the connectivity matrix defining the recurrent dynamics , @xmath5 is the weight vector describing how the input drives the network , @xmath6 is an element - wise nonlinearity evaluated at each node and @xmath7}\\xspace\\in\\ensuremath{\\mathbb{r}}\\xspace^\\ensuremath{m}\\xspace$ ] represents the error due to potential system imperfections  @xcite . in an esn ,",
    "the connectivity matrix @xmath8 is random and untrained , while the simple individual nodes have a single state variable with no memory .",
    "this is in contrast to approaches such as long short - term memory units  @xcite which have individual nodes with complex memory properties . as with many other recent papers ,",
    "we will also focus on linear networks where @xmath9 is the identity function  @xcite .",
    "the memory capacity of these networks has been studied in both the machine learning and computational neuroscience literature . in the approach of interest , the short - term memory ( stm ) of a network",
    "is characterized by quantifying the relationship between the transient network activity and the recent history of the exogenous input stream driving the network  @xcite .",
    "note that this is in contrast to alternative approaches that characterize long - term memory in rnns through quantifying the number of distinct network attractors that can be used to stably remember input patterns with the asymptotic network state . in the vast majority of the existing theoretical analysis of stm ,",
    "the results conclude that networks with @xmath10nodes can only recover inputs of length @xmath11  @xcite when the inputs are unstructured .",
    "however , in any machine - learning problem of interest , the input statistical structure is precisely what we intend to exploit to accomplish meaningful tasks . for one example , many signals are well - known to admit a sparse representation in a transform dictionary  @xcite .",
    "in fact , some classes of deep neural networks have been designed to induce sparsity at higher layers that may serve as inputs into the recurrent layers  @xcite .",
    "for another example , a collection of time - varying input streams ( e.g. , pixels or image features in a video stream ) are often heavily correlated . in the specific case of single input streams ( @xmath12 ) with inputs that are @xmath13-sparse in a basis",
    ", recent work  @xcite has shown that the stm capacity can scale as favorably as @xmath14 , where @xmath15 is a constant .",
    "in other words , the memory capacity can scale linearly with the information rate in the signal and only logarithmically with the signal dimension , resulting in the potential for recovery of inputs of length @xmath16 .",
    "unfortunately , existing analyses  @xcite are generally specific to the restricted case of single time - series inputs ( @xmath12 ) or unstructured inputs  @xcite .",
    "conventional wisdom is that structured inputs should lead to much higher stm capacity , though this has never been addressed with strong analysis in the general case of esns with multidimensional input streams .",
    "the main contribution of this paper is to provide general results characterizing the stm capacity for linear randomly connected esns with multidimensional input streams when the inputs are either sparse in a basis or have significant statistical dependence ( with no sparsity assumption ) . in both cases ,",
    "we show that the number of nodes in the network must scale linearly with the information rate and poly - logarithmically with the total input dimension .",
    "the analysis relies on advanced applications of random matrix theory , and results in non - asymptotic analysis of explicit bounds on the recovery error .",
    "taken together , this analysis provides a significant step forward in our understanding of the stm properties in rnns . while this paper is primarily focused on network structures in the context of rnns in machine learning",
    ", these results also provide foundation for the theoretical understanding of recurrent network structures in biological neural networks , as well as the memory properties in other network structures with similar dynamics ( e.g. , opinion dynamics in social networks ) .",
    "many approaches have been used to analyze the stm of randomly connected networks , including nonlinear networks  @xcite and linear networks  @xcite with both discrete - time and continuous - time dynamics .",
    "these methods can be broadly be classified as either correlation - based methods  @xcite or uniqueness methods  @xcite .",
    "correlation methods focus on quantifying the correlation between the network state and recent network inputs . in these studies ,",
    "the stm is defined as the time of the oldest input where the correlation between the network state and that input remains above a given threshold  @xcite .",
    "these methods have mostly been applied to discrete - time systems , and have resulted in bounds on the stm that scale linearly with the number of nodes ( i.e. @xmath17 ) .",
    "in contrast , uniqueness methods instead aim to show that different network states correspond to unique input sequences ( i.e. the network dynamics are bijective ) . for uniqueness methods ,",
    "the stm is defined as the longest input length where this input - network state bijection still holds .",
    "these methods have been used under the term _",
    "separability property _ for continuous - time liquid state machines  @xcite and under the term _ echo - state property _ for discrete - time esns  @xcite .",
    "the echo - state property is the method most related to the approach we take here , and essentially derives the maximum length of the input signal such that the resulting network states remain unique .",
    "while this property guarantees a bijection between inputs and network states , it does not take into account input signal structure , does not capture the robustness of the mapping , and does not provide guarantees for stably recovering the input from the network activity .",
    "the compressed sensing literature and its recent extensions include many tools for studying the effects of random matrices applied to low - dimensional signals .",
    "specifically , in the basic compressed sensing problem we desire to recover the signal @xmath18 from @xmath10measurementsas the number of measurements in this section ] generated from a random linear measurement operator , @xmath19 where @xmath20 represents the potential measurement errors .",
    "typically , @xmath21is assumed to have low - dimensional structure and recovery is performed via a convex optimization program .",
    "the most common example is a sparsity model where @xmath21can be represented as @xmath22 where @xmath23 is a transform matrix and @xmath24 is the sparse coefficient representation of @xmath21with @xmath25 of its entries non - zero . under this sparsity assumption ,",
    "the coefficient representation is recoverable if the linear operator @xmath26satisfies the restricted isometry property ( rip ) that guarantees uniqueness of the compressed measurements .",
    "specifically , we say that @xmath26satisfies the rip(2@xmath27,@xmath28 ) if for every 2@xmath27-sparse signal @xmath21 , the following condition is satisfied : @xmath29 where @xmath30 and @xmath31 is a positive constant . when @xmath26satisfies the rip(2@xmath27,@xmath28 ) the sparse coefficients @xmath32can be recovered by solving an @xmath33-norm based optimization function @xmath34 up to a reconstruction error given by @xmath35 where @xmath36 and @xmath37 are constants  @xcite .",
    "the first term of this recovery error bound depends on the norm of the measurement error @xmath38 , while the second term depends on the @xmath33 difference between the true signal and the best @xmath27-sparse approximation of the true vector ( @xmath39 ) .",
    "this term essentially measures how closely the signal matches the sparsity model .",
    "the @xmath33 optimization program in   required for recovery can be solved by many efficient algorithms , including neurally plausible architectures  @xcite .    when the data of interest is a matrix @xmath40 , other low - dimensional models have also been explored .",
    "for example , as an alternative to a sparsity assumption , the successful low - rank model assumes that there are correlations between rows and columns such that @xmath41has rank @xmath42 .",
    "we can then write the decomposition of the matrix as @xmath43 where @xmath44 and @xmath45 . there is a rich and growing literature dedicated to establishing guarantees for recovering low - rank matrices from incomplete measurements . due to the difficulty of establishing a general matrix - rip property for observations of a matrix  @xcite , the guarantees in this literature",
    "more commonly use the optimality conditions for specific optimization procedures to show that the resulting solution has bounded error with high probability .",
    "the most common optimization program used for low - rank matrix recovery is the nuclear norm minimization , @xmath46 where the nuclear norm @xmath47 is defined as the sum of the singular values of @xmath41  @xcite .",
    "this optimization procedure is similar to the @xmath33-regularized optimization of equation  , however the nuclear - norm induces sparsity in the singular values rather than the matrix entries directly .",
    "the solution to equation   can be shown to satisfy performance guarantees via the dual - certificate approach  @xcite .",
    "this technique is a proof by construction and shows that a dual certificate ( i.e. , a vector whose projections into and out of the space spanned by the singular vectors of @xmath41are bounded ) exists .",
    "showing that such a certificate exists demonstrates that equation   converges to a valid solution and is key to deriving accuracy bounds  @xcite . specifically ,",
    "if the dual certificate exists , then the solution to equation   satisfies the recovery bound @xmath48 where the forbenius norm @xmath49 is defined as the sum of the squares of all the matrix entries .",
    "this bound demonstrates that perfect recovery is achievable in the case where there is no error ( @xmath50 ) .",
    "we note that alternate optimization programs with similar guarantees have been proposed in the literature for inferring low - rank matrices ( i.e.  @xcite ) , but we will focus on nuclear norm optimization approaches due to the extensive literature on nuclear - norm solvers and the connections to sparse vector inference .",
    "the ideas and tools from the compressed sensing literature have recently been used to show that _ a - priori _ knowledge of the input sparsity can lead to improvements recovery - based stm capacity results for esns . for a single input stream under a sparsity assumption",
    ", @xcite analyzed an annealed version of the network dynamics to show that the network memory capacity can be larger than the network size .",
    "building on this observation ,  @xcite provided an analysis of the exact network dynamics in an esn ( for the single input case of @xmath51 ) , yielding precise bounds on a network s stm capacity captured in the following theorem :    _ ( theorem 4.1.1 ,  @xcite ) _ [ thm : stmwithz ] suppose @xmath52 , @xmath53 , @xmath54 , notation to indicate that a variable is a finite constant . ] and",
    "let @xmath55 be any unitary matrix of eigenvectors ( containing complex conjugate pairs ) of the connectivity matrix @xmath56and for @xmath57 an even integer , denote the eigenvalues of @xmath56by @xmath58 .",
    "let the first @xmath59 eigenvalues ( @xmath60 ) be chosen uniformly at random on the complex unit circle ( i.e. , @xmath61 is uniformly distributed over @xmath62 ) and the other @xmath59 eigenvalues as the complex conjugates of these values .",
    "furthermore , let the entries of the input weights @xmath63be i.i.d .",
    "zero - mean gaussian random variables with variance @xmath64 . given rip conditioning @xmath28and failure probability @xmath65 , if @xmath66 then for a universal constant @xmath67 , with probability @xmath68 the mapping of length-@xmath69 input sequences into @xmath57 network state variables satisfies the rip(@xmath70 , @xmath71 ) .",
    "this theorem proves a rigorous and non - asymptotic bound on the length of the input that can be robustly extracted from the network nodes . by showing the rip property on the network dynamics ,",
    "the recovery bound given in equation   establishes the recovery performance for any @xmath72-length , @xmath27-sparse signal from the resulting network state at time @xmath72 .",
    "in short , the number of required nodes scales linearly with the information rate of the signal ( i.e. , the sparsity level ) and poly - logarithmically with the length of the input .",
    "the coherence factor @xmath73 , defined as @xmath74 } \\left|\\sum_{m = 0}^{n-1}\\ensuremath{\\bm{\\psi}}\\xspace_{m , n}e^{-jtm } \\right| , \\nonumber\\end{gathered}\\ ] ] expresses the _ types _ of sparsity that are efficiently stored in the network .",
    "essentially this coherence factor is large ( on the order of @xmath75 ) for inputs sparse in the fourier basis , and is very low ( essentially a small constant ) for inputs that are sparse in bases different from the fourier bases ( e.g. wavelet transforms ) . for the extreme case of fourier - sparse inputs , the number of nodes must again exceed the number of inputs .",
    "when this coherence is low and @xmath76 , this bound is a clear improvement over existing results as it allows for @xmath77 .",
    "however , this result is restricted to single input streams with one type of low - dimensional structure .",
    "the current paper addresses the much more general problem of multidimensional inputs and other types of low - dimensional structure .",
    "in this work we will use the tools of random matrix theory to establish stm capacity results for recurrent networks under the general conditions of multiple simultaneous input streams and a variety of low - dimensional models .",
    "the temporal evolution of the linear network with multiple inputs is similar to the previous esn definition , with the main difference being that the input at each time - step @xmath78\\in\\ensuremath{\\mathbb{r}}\\xspace^\\ensuremath{l}\\xspace$ ] is a length @xmath79vector that drives the network through a feed - forward matrix @xmath5 rather than a feed - forward vector , @xmath80}\\xspace & = & \\ensuremath{\\bm{w}}\\xspace\\ensuremath{\\bm{{x}}[\\ensuremath{n}\\xspace-1]}\\xspace + \\sum_{l=1}^{\\ensuremath{l}\\xspace}\\ensuremath{\\bm{{z}}}\\xspace_{l}s_{l}[n ] + \\widetilde{\\bm{\\epsilon}}[\\ensuremath{n}\\xspace ] \\nonumber \\\\          & = & \\ensuremath{\\bm{w}}\\xspace\\ensuremath{\\bm{{x}}[\\ensuremath{n}\\xspace-1]}\\xspace + \\bm{z}\\ensuremath{\\bm{{s}}}\\xspace[\\ensuremath{n}\\xspace ] + \\widetilde{\\bm{\\epsilon}}[\\ensuremath{n}\\xspace ] .",
    "\\label{eqn : sysdyn2}\\end{aligned}\\ ] ] we denote the columns of @xmath81 as @xmath82 to separately notate the vectors mapping each input stream .",
    "we can write the current network state as a linear function of the inputs by iterating equation  , @xmath83}\\xspace = \\sum_{k=1}^{n } { \\bm{w}^{n - k}\\bm{z}\\bm{s}[k ] } + \\bm{\\epsilon } , \\label{eqn : syssum1 } \\nonumber\\end{gathered}\\ ] ] where the error term @xmath84 $ ] is the accumulated error , and then rewriting sum as a matrix - vector multiply , @xmath85}\\xspace & = & \\left[\\bm{z } , \\ensuremath{\\bm{w}}\\xspace\\bm{z } , \\cdots , \\ensuremath{\\bm{w}}\\xspace^{\\ensuremath{n}\\xspace-1}\\bm{z } \\right]\\left[\\ensuremath{\\bm{{s}}}\\xspace^t[\\ensuremath{n}\\xspace ] , \\ensuremath{\\bm{{s}}}\\xspace^t[\\ensuremath{n}\\xspace-1 ] , \\cdots , \\ensuremath{\\bm{{s}}}\\xspace^t[1 ] \\right]^t + \\bm{\\epsilon } .\\end{aligned}\\ ] ] depending on the signal statistics in question , we will find it convenient in some cases to express the network dynamics in terms of a linear operator applied to an input matrix , i.e. @xmath83}\\xspace = \\ensuremath{\\mathcal{{a}}\\left ( \\ensuremath{\\bm{{s}}}\\xspace \\right)}\\xspace + \\bm{\\epsilon } , \\nonumber \\end{gathered}\\ ] ] where @xmath86 , \\ensuremath{\\bm{{s}}}\\xspace^t[\\ensuremath{n}\\xspace-1 ] , \\cdots , \\ensuremath{\\bm{{s}}}\\xspace^t[1 ] \\right]^t$ ] .",
    "in other cases , we find it more convenient to reorganize the columns into an effective measurement matrix applied to a vector of inputs . by defining the eigen - decomposition of @xmath87 , we can re - write the dynamics process as @xmath85}\\xspace & = & \\bm{u}\\left[\\bm{d}^0\\bm{u}^{-1}\\bm{z } , \\bm{d}\\bm{u}^{-1}\\bm{z } , \\cdots , \\bm{d}^{\\ensuremath{n}\\xspace-1}\\bm{u}^{-1}\\bm{z } \\right]\\left[\\ensuremath{\\bm{{s}}}\\xspace^t[\\ensuremath{n}\\xspace ] , \\ensuremath{\\bm{{s}}}\\xspace^t[\\ensuremath{n}\\xspace-1 ] , \\cdots , \\ensuremath{\\bm{{s}}}\\xspace^t[1 ] \\right]^t + \\bm{\\epsilon } .",
    "\\nonumber \\end{aligned}\\ ] ] to simplify this expression , we can reorganize the columns of the linear operator ( and the rows of the vector of inputs ) such that all the inputs corresponding to the @xmath88 input vector @xmath89 create a single block .",
    "the @xmath90 row of the @xmath88 block of out matrix is now represented by @xmath91 , which can be written as @xmath92 , where @xmath93 and @xmath94 is the vector of the diagonal elements of @xmath95 raised to the @xmath96 power .",
    "we can more concisely by defining the matrix @xmath97 consisting of the eigenvalues of @xmath56raised to different powers ( i.e. @xmath98 ) , resulting in the expression @xmath83}\\xspace = \\ensuremath{\\bm{u}}\\xspace\\left[\\ensuremath{\\widetilde{\\bm{z}}}\\xspace_1\\ensuremath{\\bm{f}}\\xspace , \\ensuremath{\\widetilde{\\bm{z}}}\\xspace_2\\ensuremath{\\bm{f}}\\xspace , \\cdots , \\ensuremath{\\widetilde{\\bm{z}}}\\xspace_{\\ensuremath{l}\\xspace}\\ensuremath{\\bm{f}}\\xspace \\right]\\left[\\ensuremath{\\bm{{s}}}\\xspace_1^t , \\ensuremath{\\bm{{s}}}\\xspace^t_2 , \\cdots , \\ensuremath{\\bm{{s}}}\\xspace^t_\\ensuremath{l}\\xspace \\right]^t + \\bm{\\epsilon } = \\bm{a}\\widetilde{\\ensuremath{\\bm{{s}}}\\xspace } + \\bm{\\epsilon}.    \\label{eqn : matvec2}\\end{gathered}\\ ] ] since the eigenvalues of @xmath56here are restricted to reside on the unit circle , we note that @xmath99is a vandermonde matrix whose rows are fourier basis vectors . from equation",
    "we see that the current state is simply the sum of @xmath79compressed input streams , where the compression for each block essentially performs the same compression as for a single stream , but modulated by the different feed - forward vectors @xmath82 .      to begin , we consider the direct extension of previous results based on sparsity models to the multi - input setting . in this",
    "setting we consider the model where the composite of all input signals is sparse in a basis @xmath100 so that @xmath101 .",
    "this means that each signal stream can be written as @xmath102 where @xmath103 is the @xmath104 @xmath105 block of @xmath106 .",
    "this signal model captures dependencies between input streams because a given coefficient can influence multiple channels . while in many application the basis",
    "@xmath107 is pre - specified ( i.e. wavelet decomposition in image processing ;  @xcite ) , these bases can also be learned from exemplar data via dictionary learning algorithms  @xcite .",
    "this sparsity model can be a useful model for signals of interest , such as video signals , where similar sparse decompositions have been used for action recognition  @xcite and video categorization  @xcite . with this model , we will use a generalized notion of the coherence parameter used in  @xcite : @xmath108 } \\frac{\\left|\\sum_{m = 0}^{n-1}\\ensuremath{\\bm{\\psi}}\\xspace^{l , k}_{m , n}e^{-jtm } \\right|}{\\|\\ensuremath{\\bm{\\psi}}\\xspace_m^{l , k } \\|_2}. \\label{eqn : multico}\\end{gathered}\\ ] ] in this case , each @xmath109 block must be different from the fourier basis to achieve high stm capacity .",
    "this restriction is reasonable , since if a single sub - block of @xmath110was coherent with the fourier basis , then at least one input stream could be sparse in a fourier - like basis and hence would be unrecoverable . using this network and signal model",
    ", we obtain the following theorem on the stability of the network representation :    [ thm : stmmulti ] suppose @xmath111 , @xmath53 and @xmath54 .",
    "let @xmath112 be any unitary matrix of eigenvectors ( containing complex conjugate pairs ) and the entries of @xmath81 be i.i.d .",
    "zero - mean gaussian random variables with variance @xmath64 . for @xmath57 an even integer ,",
    "denote the eigenvalues of @xmath56by @xmath58 .",
    "let the first @xmath59 eigenvalues ( @xmath60 ) be chosen uniformly at random on the complex unit circle ( i.e. , we chose @xmath61 uniformly at random from @xmath62 ) and the other @xmath59 eigenvalues as the complex conjugates of these values .",
    "for a given rip conditioning @xmath71 , failure probability @xmath65 , and coherence @xmath113 as defined as in equation  , if @xmath114 then @xmath115 satisfies rip-@xmath116 with probability exceeding @xmath68 for a universal constant @xmath67 .",
    "the proof of theorem  [ thm : stmmulti ] is provided in appendix  [ app : stmmulti ] .",
    "note that when @xmath79= 1 , theorem  [ thm : stmmulti ] reduces to theorem  [ thm : stmwithz ] . in this result",
    "we see that that the number of nodes relies only linearly on the underlying dimensionality ( @xmath27 ) and poly - logarithmically on the total size of the input @xmath117 .",
    "this means that under favorable coherence and sparsity conditions on the input , the network can again have stm capacities that are higher than the number of nodes in the network .",
    "specifically , showing that @xmath118satisfies the rip property , theorem  [ thm : stmmulti ] ensures that standard recovery guarantees from the sparse inference literature hold . in particular , any @xmath27-sparse input is recoverable from the network state at time @xmath72 up to the error bound of equation   by solving the @xmath33-regularized least - squares optimization of equation  .",
    "next we consider the case of a very different type of low - dimensional structure where the input signals are correlated but not necessarily sparse .",
    "specifically , in this setting we assume that the inputs arise from a process where @xmath119 prototypical signals combine linearly to form the various input streams .",
    "such a signal structure could arise , for instance , due to correlations between input streams at spatially neighboring locations .",
    "a number of interesting applications display such correlations , including important measurement modalities in neuroscience ( e.g. two - photon calcium imaging ;  @xcite and neural electrophysiological recordings ;  @xcite ) , and remote sensing applications ( e.g. hyperspectral imagery ;  @xcite ) .",
    "the applicability of rnns and machine learning methods to data well described by this low - rank model is also increasingly relevant as there is increasing interest in applying neural network techniques to such data , either for detection  @xcite , classification  @xcite , or as samplers via variational auto - encoders  @xcite . in this case",
    ", we can write out the input matrix in the reduced form @xmath120 , where @xmath45 is the matrix whose rows may represent environmental causes generating the data and @xmath44 represents the mixing matrix that defines the input stream",
    ". we will assume both @xmath121 and @xmath122 , meaning that @xmath123 is low - rank .",
    "with this model we use a definition of coherence given by : @xmath124 } \\ensuremath{\\left|\\left| { \\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}}\\xspace_{\\omega } } \\right|\\right|}\\xspace_2 ^ 2 .",
    "\\label{eqn : lowrankco}\\end{gathered}\\ ] ] where @xmath125^t$ ] is the fourier vector with frequency @xmath126 .",
    "this coherence parameter mirrors the coherence used for the sparse - input case . as @xmath127 measured the similarity between the measurement vectors and the sparsity basis @xmath110",
    ", @xmath128 measures the similarity between the measurements and the left singular vectors of the measured matrix .",
    "the intuition here is that measurements that align with the left singular vectors are unlikely to measure significant information about the @xmath41 .    to analyze the stm of the network dynamics with respect to low - rank signal statistics , we leverage the dual certificate approach  @xcite to derive the following theorem ,    [ thm : stmlowrank ] suppose @xmath111 , @xmath129 , @xmath54 and @xmath130 .",
    "let @xmath63be i.i.d .",
    "zero - mean gaussian random variables with variance @xmath64 . for @xmath57 an even integer ,",
    "denote the eigenvalues of @xmath56by @xmath58 .",
    "let the first @xmath59 eigenvalues ( @xmath60 ) be chosen uniformly at random on the complex unit circle ( i.e. , we chose @xmath61 uniformly at random from @xmath62 ) and the other @xmath59 eigenvalues as the complex conjugates of these values .",
    "for a given coherence @xmath128 as defined as in equation  , if @xmath131 then , with probability at least @xmath132 , the minimization in equation   recovers the rank-@xmath119 input matrix @xmath41up to the error bound in equation  .",
    "the proof of theorem  [ thm : stmlowrank ] is in appendix  [ app : lowrank ] and follows a golfing scheme to find an inexact dual certificate .",
    "in fact , we note that since our architecture is extremely similar mathematically to the architecture in  @xcite , our proof is also very similar .",
    "the main difference is that due to the unbounded nature of our distributions ( i.e. the feed - forward vectors @xmath81 are gaussian random variables ) and the fact that our fourier vectors are on the unit circle ( rather than gridded ) , we can consider our proof as a generalization of the proof in  @xcite .    theorem  [ thm : stmlowrank ] is qualitatively similar to theorem  [ thm : stmmulti ] in the way the stm capacity scales . in this case , the bound still scales linearly with the information rate as captured by the number of elements in the left and right matrices that compose @xmath41 : @xmath133 .",
    "interestingly , due to the left singular vectors interacting with the measurement operator first , the coherence term only affects the portion of the bound related to the number of elements in @xmath134 .",
    "additionally , as before , the number of total inputs @xmath135 only impacts the bound poly - logarithmically .",
    "to empirically verify that these theoretical stm scaling laws are representative of the empirical behavior , we generated a number of random networks and evaluated the recovery of ( sparse or low - rank ) input sequences in the presence of noise .",
    "for each simulation we generate a @xmath136 random orthogonal connectivity matrix @xmath56 and a @xmath137 random gaussian feed - forward matrix @xmath81 . in both cases",
    "we fixed the number of inputs to @xmath138 and the number of time - steps to @xmath139 while varying the network size @xmath10and underlying dimensionality of the input ( i.e. , the sparsity level or the input matrix rank ) .",
    "for the sparse input simulations , inputs were chosen with a uniformly random support pattern with random gaussian values on the support .",
    "for low - rank simulations , the right singular vectors were chosen to be gaussian random vectors , and the left singular values were chosen at random from a number of different basis sets .    in figure",
    "[ fig : spsims ] we show the relative mean - squared error of the input recovery as a function of the sparsity - to - network size ratio @xmath140 and the network size - to - input ratio @xmath141 .",
    "each pixel value represents the average recovery relative mean - squared error ( rmse ) , as calculated by @xmath142 over 20 randomly generated trials with a noise level of @xmath143 .",
    "we show results for recovery of three different types of sparse signals : signals sparse in the canonical basis , signals sparse in a haar wavelet basis , and signals sparse in a discrete cosine transform ( dct ) basis . as our theory predicts , for canonical- and haar wavelet - sparse signals the network has very favorable stm capacity results .",
    "the fact that the capacity achieves @xmath144 is demonstrated by the area left of the @xmath145 point ( @xmath146 ) where the signal is recovered with high accuracy .",
    "likewise , for the dct - sparse signals we find that the inputs are never recovered well for any @xmath144 .",
    "this behavior is also predicted by our theory because of the unfavorable coherence properties of the dct basis .    for the low - rank trials we see that recovery of low - rank inputs for a range of @xmath147 is possible as predicted by the theoretical results .",
    "as with the sparse input case we consider three types of low - rank inputs .",
    "instead of changing the sparsity basis , however , we change the right singular vectors @xmath148of the low - rank input matrix @xmath41 .",
    "we explore the cases where the elements of @xmath148are chosen from the canonical basis , the haar basis and the dct basis .",
    "these results are shown in figure  [ fig : lrsims ] with plots similar to those in figure  [ fig : spsims ] , but with only showing the range @xmath149 to reduce computational time .",
    "as our theory predicts , the recovery of inputs with canonical- and haar right singular vectors is more accurate for a larger range of @xmath150 pairs than the inputs with dct right singular vectors .",
    "determining the fundamental limits of memory in recurrent networks is critical to understanding their behavior in machine learning tasks . in this work",
    "we show that randomly connected echo - state networks can exploit the low - dimensional structure in multidimensional input streams to achieve very high short - term memory capacity .",
    "specifically , we show non - asymptotic bounds on recovery error for input sequences that have underlying low - dimensional statistics described by either joint sparsity or low - rank properties ( with no sparsity ) . for multiple sparse inputs , we find that the network size must be linearly proportional to the input sparsity and only logarithmically dependent on the total input size ( a combination of the input length and number of inputs ) . for inputs with low - rank structure",
    ", we find a similar dependency where the network size depends linearly on the underlying dimension of the inputs ( the product of the input rank with the input length and the number of inputs ) and logarithmically on the total input size .",
    "both results continue to demonstrate that esns can have stm capacities much larger than the network size .",
    "these results are a significant ( conceptual and technical ) generalization over previous work that provided theoretical guarantees in the case of a single sparse input  @xcite .",
    "while the linear esn structure is a simplified model , rigorous analysis of these networks has remained elusive due to the recurrence itself .",
    "these results isolate the properties of the transient dynamics due to the recurrent connections , and may provide one foundation for which to explore the analysis of other complex network attributes such as nonlinearities and spiking properties .",
    "we also note that knowledge of how well neural networks compresses structured signals could indicate methods to pick the size of recurrent network layers .",
    "specifically , if a task is thought to require a certain time - frame of an input signal , the overall sparsity ( or rank ) of the signal in that time - frame can be used in conjunction with the length of that time - frame to give a lower bound for the required number of nodes in the recurrent layer of the network .    while the current paper is restricted to orthogonal connectivity matrices",
    ", previous work  @xcite has shown that a number of network structures can satisfy these criteria , including some types of small - world network topologies . additionally , we explored here low - rank and sparse inputs separately .",
    "the methods we have used to prove theorem  [ thm : stmlowrank ] , however , have also been used to analyze recovery signals with other related structures ( e.g. matrices that can be decomposed into the sum of a sparse and low - rank matrix ) from compressive measurements  @xcite .",
    "our bounds presented here therefore could open up avenues for similar analysis of other low - dimensional signal classes .    while the context of this paper is focused on the role of recurrent networks as a tool in machine learning tasks",
    ", these results may also lead to a better understanding of the stm properties in other networked systems ( e.g. , social networks , biological networks , distributed computing , etc . ) .",
    "with respect to the literature relating recurrent esn and liquid - state machines to working memory in biological systems , the notion of sparsity has much of the same flavor as the concept of _ chunking _",
    "chunking is the concept of humans learning to remember items in highly correlated groups rather than remembering items individually as a way of artificially increasing their working memory .",
    "similarly , the use of sparsity bases allow rnns to ` chunk ' items according to the basis elements .",
    "thus , each basis counts only as one item ( the true underlying sparsity ) and the network needs only store these elements rather than storing every input separately .",
    "the authors are grateful to s. bahmani , a. ahmed and j. romberg for valuable discussions related to this work .",
    "this work was supported in part by onr grant n00014 - 15 - 1 - 2731 and nsf grants ccf-0830456 and ccf-1409422 .",
    "in this appendix we prove theorem  [ thm : stmmulti ] , showing that the matrix representing the network evolution with @xmath79inputs and _ i.i.d . _",
    "gaussian feed - forward vectors satisfies the rip .",
    "recall that we have @xmath151 = \\bm{a}\\widetilde{\\ensuremath{\\bm{{s}}}\\xspace}$ ] , where @xmath152 is derived in section  [ sec : stm ] and that @xmath153 ( the vectorization of @xmath154 ) is sparse with respect to the basis @xmath155 , meaning that there is a @xmath27-sparse signal @xmath156 such that @xmath157 .",
    "similar to  @xcite , this proof is based on showing conditions on @xmath10such that @xmath118satisfies the rip with respect to @xmath110 , i.e. @xmath158 holds with high probability for all @xmath27-sparse @xmath156 .",
    "this is equivalent to bounding the following probability of the event @xmath159 where the norm @xmath160 is defined as @xmath161",
    "first we let @xmath163\\ensuremath{\\bm{\\psi}}\\xspace , \\nonumber\\end{gathered}\\ ] ] since @xmath164 , then for any @xmath165 , @xmath166 .",
    "therefore we only need to prove that @xmath167 holds with high probability when @xmath10is large enough .",
    "let @xmath168 denote the @xmath169th row of @xmath170 , i.e. , @xmath171    let @xmath172 and @xmath173 ; it is easy to check that @xmath174 and @xmath175 are complex conjugates , and that @xmath176 .",
    "therefore @xmath177 , and we only need to show that @xmath178 with high probability when @xmath10is large enough .",
    "we can easily see that when @xmath179 , @xmath180 , @xmath181 and @xmath182 are independent .",
    "first we show that @xmath183}\\xspace$ ] is small with high probability when @xmath10is large enough .",
    "we then show that @xmath184 is concentrated around its mean with high probability when @xmath10is large enough . by lemma 6.7 in @xcite , for a rademacher sequence @xmath185 , @xmath186",
    ", we have @xmath187}\\xspace   = \\ensuremath{\\mathbb{e}\\left [ { \\ensuremath{\\left|\\left| { \\sum_{i=1}^{\\ensuremath{m}\\xspace/2}(v_iv_i^{h}-\\frac{1}{\\ensuremath{m}\\xspace}\\bm{i } ) } \\right|\\right|}\\xspace_\\ensuremath{k}\\xspace}\\right]}\\xspace \\le   2\\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\left|\\left| { \\sum_{i=1}^{\\ensuremath{m}\\xspace/2}\\epsilon_iv_iv_i^{h } } \\right|\\right|}\\xspace_\\ensuremath{k}\\xspace}\\right]}\\xspace . \\nonumber\\end{gathered}\\ ] ]    we can now apply lemma 8.2 from @xcite , giving us @xmath188}\\xspace & \\leq & 2\\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\left|\\left| { \\sum_{i=1}^{\\ensuremath{m}\\xspace/2}\\epsilon_iv_iv_i^{h } } \\right|\\right|}\\xspace_\\ensuremath{k}\\xspace}\\right]}\\xspace \\nonumber\\\\      & \\leq & 2\\mathbb{e}\\left[\\mathbb{e}\\left[c_0 v_{\\max } \\sqrt{k}\\log{(100k)}\\sqrt{\\log{(4nl)}\\ln{(5 m ) } } \\right.\\right.\\nonumber \\\\          & & \\qquad\\qquad\\qquad \\left.\\left.\\cdot\\sqrt{\\left|\\left| { \\sum_{i=1}^{m/2}v_iv_i^{h } } \\right|\\right|}_k \\",
    "| v_i , i=1, ...",
    ",m/2 \\right]\\right ] \\nonumber\\\\      & \\leq & \\sqrt{c_1\\ensuremath{k}\\xspace\\log^4(\\ensuremath{n}\\xspace\\ensuremath{l}\\xspace)}\\sqrt{\\ensuremath{\\mathbb{e}\\left[{v_{\\max}^2}\\right]}\\xspace\\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\left|\\left| { \\bm{b}_1 } \\right|\\right|}\\xspace_\\ensuremath{k}\\xspace+\\frac{1}{2}}\\right]}\\xspace } , \\label{eqn : bineq}\\end{aligned}\\ ] ] where the last inequality results due to @xmath189 , @xmath190 , the cauthy - schwarz inequality and the triangle inequality , and @xmath191 note that the @xmath192th element of @xmath193 can be written @xmath194 and since we know that @xmath195 and @xmath196 we can now use corollary  [ tail_exp_max ] . setting @xmath197 , @xmath198 in corollary [ tail_exp_max ] yields @xmath199}\\xspace\\le \\eta ,   \\label{v_max_tail}\\end{gathered}\\ ] ] and @xmath200}\\xspace   \\le \\frac{\\mu^2(\\ensuremath{\\bm{\\psi}}\\xspace)}{\\ensuremath{m}\\xspace}(\\log{\\frac{\\ensuremath{m}\\xspace\\ensuremath{n}\\xspace\\ensuremath{l}\\xspace}{2}}+1 ) \\le   c_2\\frac{\\mu^2(\\ensuremath{\\bm{\\psi}}\\xspace)}{\\ensuremath{m}\\xspace}\\log{(\\ensuremath{n}\\xspace\\ensuremath{l}\\xspace)}.\\label{v_max_exp}\\end{gathered}\\ ] ] returning to the inequality in equation  , considering ( [ v_max_exp ] ) , we have @xmath201}\\xspace\\le a\\sqrt{\\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\left|\\left| { \\bm{b}_1 } \\right|\\right|}\\xspace_\\ensuremath{k}\\xspace}\\right]}\\xspace+1},\\ ] ] where @xmath202",
    ". then @xmath183}\\xspace \\le \\frac{a^2}{2}+a\\sqrt{\\frac{1}{2}+\\frac{a^2}{4}}$ ] .",
    "when @xmath203 , we get @xmath183}\\xspace \\le a$ ] .",
    "let @xmath204 , and we can conclude that when @xmath205 then @xmath201}\\xspace \\le { \\ensuremath{\\delta}\\xspace}^{\\prime}.\\ ] ]      now we study the tail bound of @xmath184 . first we construct a second set of random variables @xmath206 , which are independent of @xmath207 and are identically distributed as @xmath207 . additionally , we let @xmath208 and then according to @xcite , there is @xmath209}\\xspace\\le 2\\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\left|\\left| { \\bm{b}_1 } \\right|\\right|}\\xspace_\\ensuremath{k}\\xspace}\\right]}\\xspace , \\label{exp_b_b_tilde}\\end{gathered}\\ ] ] @xmath210}\\xspace+u}\\right]}\\xspace \\le 2\\ensuremath{\\mathcal{p}\\left[{\\ensuremath{\\left|\\left| { \\widetilde{\\bm{b}}_1 } \\right|\\right|}\\xspace_\\ensuremath{k}\\xspace > u}\\right]}\\xspace .",
    "\\label{tail_b_b_tilde}\\end{gathered}\\ ] ] now since we have @xmath211 and @xmath212 then we know that @xmath213 and @xmath214 , where @xmath215 then by equation  , we obtain @xmath216}\\xspace\\le \\ensuremath{\\mathcal{p}\\left[{v_{\\max}^2>\\frac{\\mu^2(\\ensuremath{\\bm{\\psi}}\\xspace)}{\\ensuremath{m}\\xspace}\\log{\\frac{\\ensuremath{m}\\xspace\\ensuremath{n}\\xspace\\ensuremath{l}\\xspace}{2\\eta } } } \\right]}\\xspace\\le \\eta.\\ ] ]    since the probability theorems depend on bounded random variables , we define @xmath217 to denote the following event @xmath218 such that @xmath219}\\xspace \\le 2\\eta$ ] . furthermore",
    ", we define @xmath220 as the indicator function of @xmath217 , and let @xmath221 , where @xmath222 , @xmath223 is a rademacher sequence and independent of @xmath224 .",
    "the truncated variable @xmath225 has a symmetric distribution and @xmath226 is bounded by @xmath227 . by proposition 19 in @xcite , we have @xmath228}\\xspace + tb_{\\max})}\\right]}\\xspace\\le e^{-u^2}+e^{-t } , \\label{tail_hat}\\end{gathered}\\ ] ] for all @xmath229 . following  @xcite",
    ", we find that @xmath230}\\xspace\\le \\ensuremath{\\mathcal{p}\\left[{\\ensuremath{\\left|\\left| { \\widehat{\\bm{b}}_1 } \\right|\\right|}\\xspace_\\ensuremath{k}\\xspace > v}\\right]}\\xspace+\\ensuremath{\\mathcal{p}\\left[{{\\bf f}^c}\\right]}\\xspace,\\label{tail_hat_tilde}\\end{gathered}\\ ] ] and @xmath231}\\xspace \\le \\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\left|\\left| { \\widetilde{\\bm{b}}_1 } \\right|\\right|}\\xspace_\\ensuremath{k}\\xspace}\\right]}\\xspace . \\label{exp_hat_tilde}\\end{gathered}\\ ] ] by combining equations  , , and , we get @xmath232}\\xspace+tb_{\\max})}\\right]}\\xspace\\le e^{-u^2}+e^{-t}+2\\eta . \\nonumber\\end{gathered}\\ ] ] in equation  , let @xmath233 , @xmath234 and @xmath235 .",
    "these values yield @xmath236}\\xspace+(\\log{\\eta^{-1}})b_{\\max})}\\right]}\\xspace\\le 4\\eta .",
    "\\label{tail_tilde}\\end{gathered}\\ ] ] now we can combine equations  , , and to get @xmath210}\\xspace+2c_4\\sqrt{\\log{\\eta^{-1}}}\\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\left|\\left| { \\bm{b}_1 } \\right|\\right|}\\xspace_\\ensuremath{k}\\xspace}\\right]}\\xspace+c_4(\\log{\\eta^{-1}})b_{\\max}}\\right]}\\xspace \\le 8\\eta . \\nonumber\\end{gathered}\\ ] ] by choosing @xmath237 where @xmath238 , we obtain @xmath239}\\xspace\\le8\\eta.\\ ] ]    we observe now that @xmath240 indicating that when @xmath241 , i.e. , @xmath242 , then @xmath243 , for a constant @xmath244 .",
    "this inequality reduces our probability statement to @xmath245}\\xspace \\le 8\\eta.\\ ] ] for an arbitrary @xmath28with @xmath246 , we let @xmath247 resulting in @xmath248 @xmath249 @xmath250 then we can see that if @xmath251 then @xmath252}\\xspace<8\\eta.\\ ] ] now by plugging ( [ deltaprime ] ) into ( [ m_deltaprime ] ) , we know that when @xmath253 , there exists a constant @xmath67 such that when @xmath254 there is @xmath255 which completes the proof .",
    "[ tail_single ] suppose we have @xmath1 complex gaussian random variables , @xmath256 , @xmath257 , where @xmath258 and @xmath259 denote the real and imaginary parts of @xmath260 .",
    "let @xmath258 and @xmath259 i.i.d gaussian r.v.s with mean 0 and variance @xmath261 .",
    "@xmath262 r.v.s which are independent of @xmath260 for all @xmath169 and satisfy @xmath263 then for @xmath264 , @xmath265}\\xspace \\le e^{-\\frac{\\ensuremath{m}\\xspace u}{\\mu_0 ^ 2}}.\\ ] ]    we use @xmath266 and @xmath267 to denote the real and imaginary parts of @xmath268 , and @xmath269 and @xmath270 to denote the real and imaginary parts of @xmath271 .",
    "we have @xmath272 then conditioned on @xmath269 and @xmath270 , @xmath266 and @xmath267 have distribution @xmath273 .",
    "the next step is to prove the conditional independence of @xmath266 and @xmath267 . since @xmath274 when @xmath275 , @xmath258 , @xmath259 are independent of @xmath276 , @xmath277 , and @xmath266 and @xmath267 are conditionally independent .",
    "thus , conditioned on @xmath269 and @xmath270 , @xmath278 is @xmath279 distributed .",
    "we use @xmath280 to denote a two - degree @xmath279 distributed random variable . according to the results on @xmath279 distributions",
    ", we have @xmath281}\\xspace = \\ensuremath{\\mathcal{p}\\left[{\\chi_2>\\frac{2\\ensuremath{m}\\xspace u}{\\sum_{i=1}^n{(a_i^2+b_i^2 ) } } | a_i , b_i } \\right]}\\xspace   = e^{-\\frac{\\ensuremath{m}\\xspace u}{\\sum_{i=1}^n{(a_i^2+b_i^2 ) } } }   \\le e^{-\\frac{\\ensuremath{m}\\xspace u}{\\mu_0 ^ 2}}. \\label{chi_2_tail}\\end{gathered}\\ ] ] noticing that equation   holds for all possible values of @xmath269 and @xmath270 completes the proof .    [ tail_exp_max ] for @xmath282 r.v.s , @xmath283 , let @xmath284 and @xmath285 , where @xmath286 , @xmath287 , @xmath288 , @xmath289 are i.i.d .",
    "gaussian distributed with mean 0 and variance @xmath290 .",
    "suppose for any @xmath169 , there is @xmath291 and let @xmath292 , then for @xmath293 , we have @xmath294}\\xspace\\le \\eta,\\ ] ] and @xmath295}\\xspace \\le \\frac{\\mu_0",
    "^ 2}{\\ensuremath{m}\\xspace}(\\ln{q}+1).\\ ] ]      then we have @xmath298}\\xspace & = & \\int_0^\\infty\\ensuremath{\\mathcal{p}\\left[{w_{\\max}^2>u}\\right]}\\xspace du\\nonumber\\\\                              & \\le & \\int_0^{\\frac{\\mu_0 ^ 2}{\\ensuremath{m}\\xspace}\\ln{q}}1du+\\int_{\\frac{\\mu_0 ^ 2}{\\ensuremath{m}\\xspace}\\log{q}}^\\infty qe^{-\\frac{\\ensuremath{m}\\xspace u}{\\mu_0 ^ 2}}du\\nonumber\\\\    & = & \\frac{\\mu_0 ^ 2}{\\ensuremath{m}\\xspace}(\\log{q}+1).\\nonumber\\end{aligned}\\ ] ]      in this appendix we prove theorem  [ thm : stmlowrank ] where a low - rank input matrix @xmath41can be recovered from the network state @xmath299}\\xspace$ ] via nuclear norm optimization  @xcite . to prove this theorem we use the dual certificate approach used to prove similar results in  @xcite .",
    "in this methodology we seek a certificate @xmath300whose projections into and out of the space spanned by the singular vectors of @xmath41are bounded appropriately .",
    "specifically if we consider the singular value decomposition of @xmath41as @xmath301 and we consider the projection @xmath302which projects a matrix into the space @xmath303 spanned by the left and right singular vectors , @xmath304 the conditions for the dual certificate are that @xmath26is injective on @xmath303 and there exists a matrix @xmath300which satisfies @xmath305 where the projection @xmath306is the projection onto the perpendicular space to @xmath303 , @xmath307    the remainder of this proof will be devoted to demonstrating that there does exist a certificate @xmath300by iteratively devising @xmath300via a golfing scheme  @xcite .",
    "the golfing scheme essentially generates an iterative method which defined a series of certificate vectors @xmath308for @xmath309 $ ] which converge to a certificate @xmath310 which satisfies the necessary conditions . as in  @xcite , we can initialize the @xmath311 iterate to zero , and define the @xmath90 iterate in terms of the @xmath312 as @xmath313 where @xmath314 we can see that since every iterate has @xmath315 applied to it , every iteration is projected in to the range of @xmath316 , indicating that the final iteration @xmath300will also be in the range of @xmath316 . in  @xcite , asif and romberg",
    "define a simpler iteration @xmath317 which is expressed in terms of the modified certificate @xmath318    what remains now is to demonstrate that this iterative procedure converges , with high probability , to a certificate which satisfies the desired dual certificate conditions .",
    "we start by using lemma  [ lem : lemma1 ] and observing that the forbenious norm of the @xmath90 iterate is well bounded with probability @xmath319 by @xmath320 so long that @xmath321 . as in",
    "@xcite we observe that when we choose @xmath322 , the bound for the frobenious norm of @xmath323 is bounded by @xmath324 .    to show that the second condition on the certificate is also satisfies , we apply lemma  [ lem : lemma2 ] .",
    "we begin with writing the quantity we wish to bound in terms of the past golfing scheme iterate @xmath325 } \\ensuremath{\\left|\\left| { { \\kappa}\\ensuremath{\\mathcal{{a}}^{\\ast}}\\xspace_k\\ensuremath{\\mathcal{{a}}}\\xspace_k\\ensuremath{\\widetilde{\\bm{y}}}\\xspace_{k-1 } - \\ensuremath{\\widetilde{\\bm{y}}}\\xspace_{k-1 } } \\right|\\right|}\\xspace_f   \\nonumber \\\\      & \\leq & \\sum_{k=1}^{\\kappa } \\frac{1}{2}2^{-k } \\nonumber \\\\      & \\leq & \\frac{1}{2 }                                   \\nonumber\\end{aligned}\\ ] ] we use lemma  [ lem : lemma2 ] to bound the maximum spectral norm of @xmath326 with probability @xmath132 .",
    "taking @xmath327 shows that the final certificate @xmath310 satisfies all the desired properties , completing the proof .",
    "the lemmas required in our main result depend heavily on the matrix bernstein inequality  @xcite .",
    "this inequality uses the variance measure and oricz norm of a matrix to bound the largest singular value of the matrix .",
    "the matrix bernstein inequality is summarized as    [ thm : matbern ] let @xmath328 , @xmath329 $ ] be @xmath10random matrices such that @xmath330}\\xspace = 0 $ ] and @xmath331 for some @xmath332",
    ". then with probability @xmath333 , the spectral norm of the sum is bounded by @xmath334 for some constant @xmath67 and the variance parameter defined by @xmath335}\\xspace } \\right|\\right|}\\xspace^{1/2 } , \\ensuremath{\\left|\\left| { \\sum_{i=1}^\\ensuremath{m}\\xspace \\ensuremath{\\mathbb{e}\\left[{\\bm{x}_i^{\\ast}\\bm{x}_i}\\right]}\\xspace } \\right|\\right|}\\xspace^{1/2 } \\right\\}. \\nonumber      \\end{gathered}\\ ] ]      in particular we will use the matrix bernstein inequality with the orlicz-1 and orlicz-2 norms , since subgaussian and subexponential random variables have bounded orlicz-2 and -1 norms , respectively . to calculate these norms , we find the following lemmas from  @xcite useful :        lemma  [ lem : onorm1 ] relates the orlicz-1 and -2 norms for a random variable and it s square .",
    "lemma  [ lem : onorm2 ] allows us to factor an orlicz-1 norm of a sub - exponential random variable as the product of two subgaussian random variables .",
    "finally we find useful the following calculation for the orlicz-1 norm of the norm of a random gaussian vector @xmath345with _ i.i.d .",
    "_ zero - mean and variance @xmath346 entries :    @xmath347}\\xspace \\leq 2 \\right\\ } \\nonumber \\\\      & = & \\inf \\left\\{y : \\frac{1}{\\sqrt{2\\pi\\sigma } } \\int_\\ensuremath{\\mathbb{r}}\\xspace e^{-z_n^2(1/2\\sigma^2 - 1/y ) } dz_n \\leq 2^{\\frac{1}{\\ensuremath{m}\\xspace } } \\right\\ } \\nonumber \\\\      & = & \\frac{2\\sigma^2}{1 - 4^{-\\frac{1}{\\ensuremath{m}\\xspace}}}. \\label{eqn : onormzvec}\\end{aligned}\\ ] ]      [ lem : lemma1 ] let @xmath348 be defined as in equation   and @xmath349 be the restricted measurement operator as defined in equation  . then if the number of nodes scale as @xmath350 for a constant @xmath351 , then with probability greater then @xmath352 , we have @xmath353}\\ensuremath{\\left|\\left| { { \\kappa}\\ensuremath{\\mathcal{p}_t}\\xspace\\ensuremath{\\mathcal{{a}}^{\\ast}}\\xspace\\ensuremath{\\mathcal{{a}}}\\xspace\\ensuremath{\\mathcal{p}_t}\\xspace - \\ensuremath{\\mathcal{p}_t}\\xspace } \\right|\\right|}\\xspace \\leq \\frac{1}{2}. \\nonumber      \\end{gathered}\\ ] ]    lemma  [ lem : lemma1 ] bounds the operator norm @xmath354 since @xmath355}\\xspace = \\frac{1}{{\\kappa}}\\mathcal{i}$ ] , this norm is equivalent to @xmath356}\\xspace \\nonumber \\\\      & = & \\kappa \\sum_{n\\in\\gamma_k}\\left(\\ensuremath{\\mathcal{p}_t}\\xspace(\\ensuremath{\\bm{{a}}_n}\\xspace)\\otimes\\ensuremath{\\mathcal{p}_t}\\xspace(\\ensuremath{\\bm{{a}}_n}\\xspace ) - \\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\mathcal{p}_t}\\xspace(\\ensuremath{\\bm{{a}}_n}\\xspace)\\otimes\\ensuremath{\\mathcal{p}_t}\\xspace(\\ensuremath{\\bm{{a}}_n}\\xspace)}\\right]}\\xspace \\right ) .",
    "\\nonumber\\end{aligned}\\ ] ]    we can also define here @xmath357 which has @xmath358 which gives us @xmath359}\\xspace =   { \\kappa}\\sum_{n\\in\\gamma_k}(\\mathcal{l}_n - \\ensuremath{\\mathbb{e}\\left[{\\mathcal{l}_n}\\right]}\\xspace ) . \\nonumber \\end{gathered}\\ ] ]    to calculate the variance , we can use the symmetry of @xmath360 to only calculate @xmath361}\\xspace - \\ensuremath{\\mathbb{e}\\left[{\\mathcal{l}_n}\\right]}\\xspace^2 } \\right|\\right|}\\xspace \\leq { \\kappa}^2\\ensuremath{\\left|\\left| { \\sum_{n\\in\\gamma_k } \\ensuremath{\\mathbb{e}\\left[{\\mathcal{l}_n^2}\\right]}\\xspace } \\right|\\right|}\\xspace   =   { \\kappa}^2\\ensuremath{\\left|\\left| { \\ensuremath{\\mathbb{e}\\left[{\\sum_{n\\in\\gamma_k } \\|\\ensuremath{\\mathcal{p}_t}\\xspace(\\ensuremath{\\bm{{a}}_n}\\xspace)\\|_f^2\\mathcal{l}_n } \\right]}\\xspace } \\right|\\right|}\\xspace .",
    "\\nonumber\\end{gathered}\\ ] ] we now need to bound @xmath362 , which can be done by the following : @xmath363    using this calculation we can write @xmath364}\\xspace } \\right|\\right|}\\xspace & \\leq & \\ensuremath{\\left|\\left| { \\sum_{n\\in\\gamma_k}\\ensuremath{\\mathbb{e}\\left [ { \\left(n\\|\\ensuremath{\\bm{q}^{\\ast}}\\xspace\\ensuremath{\\bm{{z}}_n}\\xspace\\|_2 ^ 2 + \\|\\ensuremath{\\bm{{z}}_n}\\xspace\\|_2 ^ 2\\|\\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}_n}\\xspace\\|_2 ^ 2 \\right)\\mathcal{l}_n } \\right]}\\xspace } \\right|\\right|}\\xspace \\nonumber \\\\      & \\leq & n\\left|\\left| \\sum_{n\\in\\gamma_k}\\mathbb{e}\\left[\\|\\bm{q}^{\\ast}\\bm{{z}}_n\\|_2 ^ 2\\mathcal{l}_n\\right ] \\right|\\right| \\nonumber \\\\          & & \\qquad\\qquad\\qquad + \\sup\\|\\bm{v}^{\\ast}\\bm{{f}}_n\\|_{\\infty}\\left|\\left| \\sum_{n\\in\\gamma_k}\\mathbb{e}\\left [ { \\|\\bm{{z}}_n\\|_2 ^ 2\\mathcal{l}_n } \\right ] \\right|\\right| \\nonumber \\\\      & \\leq & \\ensuremath{n}\\xspace \\ensuremath{\\left|\\left| { \\sum_{n\\in\\gamma_k}\\ensuremath{\\mathbb{e}\\left[{\\|\\ensuremath{\\bm{q}^{\\ast}}\\xspace\\ensuremath{\\bm{{z}}_n}\\xspace\\|_2 ^ 2\\mathcal{l}_n}\\right]}\\xspace } \\right|\\right|}\\xspace + \\ensuremath{r}\\xspace\\mu_0 ^ 2\\ensuremath{\\left|\\left| { \\sum_{n\\in\\gamma_k}\\ensuremath{\\mathbb{e}\\left[{\\|\\ensuremath{\\bm{{z}}_n}\\xspace\\|_2 ^ 2\\mathcal{l}_n } \\right]}\\xspace } \\right|\\right|}\\xspace \\nonumber\\end{aligned}\\ ] ]    we now need to bound these two quantities .",
    "first we look to bound the first quantity @xmath365}\\xspace } \\right|\\right|}\\xspace & \\leq & \\|\\ensuremath{\\mathcal{p}_t}\\xspace\\|\\ensuremath{\\left|\\left| { \\ensuremath{\\mathbb{e}\\left[{\\|\\ensuremath{\\bm{q}^{\\ast}}\\xspace\\ensuremath{\\bm{{z}}_n}\\xspace\\|_2 ^ 2(\\ensuremath{\\bm{{a}}_n}\\xspace\\otimes\\ensuremath{\\bm{{a}}_n}\\xspace)}\\right]}\\xspace } \\right|\\right|}\\xspace\\|\\ensuremath{\\mathcal{p}_t}\\xspace\\| \\nonumber \\\\      & \\leq & \\ensuremath{\\left|\\left| { \\ensuremath{\\mathbb{e}\\left[{\\|\\ensuremath{\\bm{q}^{\\ast}}\\xspace\\ensuremath{\\bm{{z}}_n}\\xspace\\|_2 ^ 2\\{z_n[\\alpha]z^{\\ast}_n[\\beta]\\ensuremath{\\bm{{f}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace\\}_{\\alpha,\\beta } } \\right]}\\xspace } \\right|\\right|}\\xspace . \\nonumber\\end{aligned}\\ ] ] expanding , we have : @xmath366|^2 + 2\\sum_{l \\neq m}\\mbox{re}(\\langle\\bm{q}_l,\\bm{q}_m\\rangle z_n[l]z^{\\ast}_n[m])\\right ) z_n[\\alpha]z_n^{\\ast}[\\beta]\\bm{i}_\\ensuremath{n}\\xspace } \\right]}\\xspace_{\\alpha,\\beta } } \\right|\\right|}\\xspace \\nonumber \\\\      & = & \\ensuremath{\\left|\\left| { \\left\\{\\frac{1}{\\ensuremath{m}\\xspace^2}\\sum_{l=1}^l\\|\\bm{q}_l\\|_2 ^ 2\\bm{i}_\\ensuremath{n}\\xspace\\delta_{\\alpha=\\beta } + \\frac{2}{\\ensuremath{m}\\xspace^2}\\ensuremath{\\langle { \\bm{q}_\\alpha},{\\bm{q}_\\beta}\\rangle}\\xspace \\bm{i}_\\ensuremath{n}\\xspace\\delta_{\\alpha \\neq \\beta } \\right\\}_{\\alpha,\\beta } } \\right|\\right|}\\xspace \\nonumber \\\\      & = & \\frac{1}{\\ensuremath{m}\\xspace^2}\\left\\|\\left\\{\\|\\ensuremath{\\bm{q}}\\xspace\\|_f^2\\bm{i}_\\ensuremath{n}\\xspace\\delta_{\\alpha=\\beta } + 2\\ensuremath{\\langle { \\bm{q}_\\alpha},{\\bm{q}_\\beta}\\rangle}\\xspace \\bm{i}_\\ensuremath{n}\\xspace\\delta_{\\alpha \\neq \\beta } \\right\\}_{\\alpha,\\beta}\\right\\| , \\nonumber\\end{aligned}\\ ] ] giving us @xmath367}\\xspace } \\right|\\right|}\\xspace \\leq \\frac{1}{\\ensuremath{m}\\xspace{\\kappa } } \\|\\ensuremath{\\bm{q}}\\xspace\\|_f^2 + \\|\\ensuremath{\\bm{q}}\\xspace\\ensuremath{\\bm{q}^{\\ast}}\\xspace\\| \\leq \\frac{\\ensuremath{r}\\xspace+1}{\\ensuremath{m}\\xspace{\\kappa}}. \\nonumber\\end{gathered}\\ ] ]    similarly , for the second term we can take @xmath368 to get @xmath369}\\xspace } \\right|\\right|}\\xspace \\leq \\frac{1}{\\ensuremath{m}\\xspace{\\kappa } } \\|\\bm{i}\\|_f^2   = \\frac{\\ensuremath{l}\\xspace}{\\ensuremath{m}\\xspace{\\kappa}}.   \\nonumber \\end{gathered}\\ ] ]      to use the matrix bernstein inequality , it now remains to bound the following orlicz-1 norm : @xmath371}\\xspace } \\right|\\right|}\\xspace_{\\psi_1}$ ] . by the psd quality of @xmath360 and its expectation , @xmath372}\\xspace } \\right|\\right|}\\xspace_{\\psi_1 } \\leq \\max\\left\\{\\ensuremath{\\left|\\left| { \\mathcal{l}_n } \\right|\\right|}\\xspace_{\\psi_1 } - \\ensuremath{\\left|\\left| { \\ensuremath{\\mathbb{e}\\left[{\\mathcal{l}_n}\\right]}\\xspace } \\right|\\right|}\\xspace_{\\psi_1 } \\right\\}. \\nonumber\\end{gathered}\\ ] ] the norm of @xmath373}\\xspace } \\right|\\right|}\\xspace$ ]",
    "can be calculated via @xmath374}\\xspace } \\right|\\right|}\\xspace = \\ensuremath{\\left|\\left| { \\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\mathcal{p}_t\\left ( \\ensuremath{\\bm{{a}}_n}\\xspace \\right)}\\xspace}\\right]}\\xspace } \\right|\\right|}\\xspace_f^2 = \\ensuremath{\\mathbb{e}\\left[{\\sum_m|z_n[m]|^2|f_n[m]|^2}\\right]}\\xspace = \\frac{1}{\\ensuremath{m}\\xspace}\\|\\ensuremath{\\bm{{f}}_n}\\xspace\\|_2 ^ 2 = \\frac{\\ensuremath{n}\\xspace}{\\ensuremath{m}\\xspace } ,   \\nonumber\\end{gathered}\\ ] ] indicating that the second term is simply @xmath373}\\xspace } \\right|\\right|}\\xspace_{\\psi_1 } = \\ensuremath{n}\\xspace/(\\ensuremath{m}\\xspace\\log(2))$ ] .",
    "to calculate @xmath373}\\xspace } \\right|\\right|}\\xspace_{\\psi_1}$ ] , we use the definition of the orlitcz-1 norm in equation   to see that @xmath375 where the inequality stems form the fact that @xmath376 and @xmath377 . using the result in equation   with @xmath378 in the first term and in @xmath379 in the second term yields @xmath380}\\xspace }",
    "\\right|\\right|}\\xspace_{\\psi_1 } & \\leq & \\ensuremath{n}\\xspace\\ensuremath{\\left|\\left| { \\|\\ensuremath{\\bm{q}^{\\ast}}\\xspace\\ensuremath{\\bm{{z}}_n}\\xspace\\|_2 ^ 2 } \\right|\\right|}\\xspace_{\\psi_1 } + r\\ensuremath{n}\\xspace\\mu_0 ^ 2\\ensuremath{\\left|\\left| { \\|\\ensuremath{\\bm{{z}}_n}\\xspace\\|_2 ^ 2 } \\right|\\right|}\\xspace_{\\psi_1 } \\nonumber \\\\      & \\leq & \\frac{1}{\\ensuremath{m}\\xspace}\\left(\\frac{\\ensuremath{n}\\xspace}{\\ensuremath{m}\\xspace(1 - 4^{-\\frac{1}{\\ensuremath{r}\\xspace } } ) } + \\frac{\\ensuremath{r}\\xspace\\mu_0 ^ 2}{1 - 4^{-\\frac{1}{\\ensuremath{l}\\xspace}}}\\right ) \\nonumber \\\\        & \\leq & \\frac{2\\ensuremath{r}\\xspace\\left(\\ensuremath{n}\\xspace + \\ensuremath{l}\\xspace\\mu_0 ^ 2\\right)}{\\log(2)\\ensuremath{m}\\xspace}. \\nonumber \\end{aligned}\\ ] ]    we now have appropriate bounds on both the variance and orliscz norm , permitting a bound on the largest singular value via the matrix bernstein inequality . specifically , we can see that the first term in theorem  [ thm : matbern ] with @xmath381 is bounded as @xmath382          [ lem : lemma2 ] let @xmath349 be defined as in equation  , @xmath387 be the number of steps in the golfing scheme and assume that @xmath388 .",
    "then as long as @xmath389 where @xmath390 is the coherence term defined by @xmath391 } \\ensuremath{\\left|\\left| { \\widetilde{\\bm{y}}_k^{\\ast}\\ensuremath{\\bm{{f}}}\\xspace_{\\omega } } \\right|\\right|}\\xspace_2 ^ 2 , \\label{eqn : coherencek }      \\end{gathered}\\ ] ] then with probability at least @xmath392 , we have @xmath393    lemma  [ lem : lemma2 ] essentially bounds the operator norm of @xmath394 . in particular , to prove theorem 2 , the reduced version with @xmath395 is needed .",
    "lemma 2 essentially uses the matrix bernstein inequality to accomplish this task , taking @xmath396}\\xspace),\\ ] ] and we just need to control @xmath397}\\xspace } \\right|\\right|}\\xspace$ ] and @xmath398}\\xspace } \\right|\\right|}\\xspace$ ] . to bound the second of these , we can calculate @xmath399}\\xspace } \\right|\\right|}\\xspace & \\leq & { \\kappa}^2\\ensuremath{\\left|\\left| { \\sum_{n\\in\\gamma_k } \\ensuremath{\\mathbb{e}\\left[{|\\ensuremath{\\langle { \\bm{g}},{\\ensuremath{\\bm{{a}}_n}\\xspace}\\rangle}\\xspace|^2\\ensuremath{\\bm{{a}}_n}\\xspace\\ensuremath{\\bm{{a}}_n^{\\ast}}\\xspace}\\right]}\\xspace } \\right|\\right|}\\xspace   \\nonumber \\\\      & = & { \\kappa}^2\\ensuremath{\\left|\\left| { \\sum_{n\\in\\gamma_k }",
    "\\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\left|\\left| { \\ensuremath{\\bm{{f}}_n}\\xspace } \\right|\\right|}\\xspace_2 ^ 2|\\ensuremath{\\langle { \\bm{g}},{\\ensuremath{\\bm{{a}}_n}\\xspace}\\rangle}\\xspace|^2\\ensuremath{\\bm{{z}}_n}\\xspace\\ensuremath{\\bm{{z}}^{\\ast}_n}\\xspace}\\right]}\\xspace } \\right|\\right|}\\xspace \\nonumber\\\\      & \\leq & \\frac{3\\ensuremath{n}\\xspace{\\kappa}}{\\ensuremath{m}\\xspace}\\ensuremath{\\left|\\left| { \\bm{g } } \\right|\\right|}\\xspace_f^2 , \\nonumber \\end{aligned}\\ ] ] where the second inequality is due to lemma  [ lem : lemma3 ] and @xmath400 . for the other expectation @xmath401}\\xspace }",
    "\\right|\\right|}\\xspace & \\leq & { \\kappa}^2\\ensuremath{\\left|\\left| { \\sum_{n\\in\\gamma_k } \\ensuremath{\\mathbb{e}\\left[{|\\ensuremath{\\langle { \\bm{g}},{\\ensuremath{\\bm{{a}}_n}\\xspace}\\rangle}\\xspace|^2\\ensuremath{\\bm{{a}}_n^{\\ast}}\\xspace\\ensuremath{\\bm{{a}}_n}\\xspace}\\right]}\\xspace } \\right|\\right|}\\xspace   \\nonumber \\\\      & = & \\frac{\\ensuremath{l}\\xspace{\\kappa}^2}{\\ensuremath{m}\\xspace^2}\\ensuremath{\\left|\\left| { \\sum_{n\\in\\gamma_k } \\ensuremath{\\mathbb{e}\\left[{\\|\\bm{g}\\ensuremath{\\bm{{f}}_n}\\xspace\\|_2 ^ 2\\ensuremath{\\bm{{f}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace}\\right]}\\xspace } \\right|\\right|}\\xspace \\nonumber \\\\          & \\leq & \\frac{\\ensuremath{l}\\xspace{\\kappa}^2}{\\ensuremath{m}\\xspace^2}\\sup_{\\omega}(\\|\\bm{g}\\bm{f}_{\\omega}\\|_2 ^ 2 ) \\ensuremath{\\left|\\left| { \\sum_{n\\in\\gamma_k } \\bm{1}_\\ensuremath{n}\\xspace } \\right|\\right|}\\xspace \\nonumber \\\\          & \\leq & \\frac{\\ensuremath{l}\\xspace{\\kappa}}{\\ensuremath{m}\\xspace}\\sup_{\\omega}(\\|\\bm{g}\\bm{f}_\\omega\\|_2 ^ 2 ) \\nonumber \\\\      & = & \\frac{\\ensuremath{l}\\xspace{\\kappa}}{\\ensuremath{m}\\xspace}\\mu^2\\|\\bm{g}\\|_f^2 . \\nonumber \\end{aligned}\\ ] ]      we can now apply the matrix bernstein theorem with the calculated values of @xmath405 and @xmath406 . again using @xmath407 ,",
    "the first portion of the bound is @xmath408 and the second portion of the bound is @xmath409    we can now use lemma  [ lem : lemma4 ] to bound @xmath410 with probability @xmath411 and lemma  [ lem : lemma1 ] to bound @xmath412 , which gives us a bound of @xmath413 simplifying the bound using @xmath414 , @xmath415 taking @xmath416 proves the lemma . to simplify the bound on the probability",
    ", we note that lemma  [ lem : lemma4 ] holds with probability @xmath411 and this lemma holds with probability @xmath417 . since @xmath387 and assuming that @xmath388 , we can write that the result holds with probability @xmath418 . additionally , since lemma  [ lem : lemma4 ] holds when @xmath419 then both lemmas hold under the same condition .",
    "[ lem : lemma3 ] suppose @xmath421 be defined as the outer product of an _",
    "_ random gaussian vector @xmath422 with zero mean and variance @xmath423 and a random fourier vector @xmath424",
    ". then the operator @xmath425 satisfies @xmath426}\\xspace   \\preceq \\frac{3}{\\ensuremath{m}\\xspace^2}\\|\\bm{c}^{\\ast}\\ensuremath{\\bm{{f}}_n}\\xspace\\|_2 ^ 2\\bm{i}_\\ensuremath{m}\\xspace , \\nonumber      \\end{gathered}\\ ] ] and @xmath427}\\xspace   \\preceq \\frac{3}{\\ensuremath{m}\\xspace^2}\\|\\bm{c}\\|_f^2\\bm{i}_\\ensuremath{m}\\xspace .",
    "\\nonumber      \\end{gathered}\\ ] ]    to begin the proof , we look at the expectation of each element of the matrix . we first calculate the expectation with respect to @xmath422 , @xmath428\\bm{c}_l^{\\ast}\\ensuremath{\\bm{{f}}_n}\\xspace \\right|^2z_{n}[\\alpha]z^{\\ast}_n[\\beta]}\\right]}\\xspace \\nonumber   \\\\            & = &   \\ensuremath{\\mathbb{e}_{z}\\left [ { \\left(\\sum_{l=1}^lz_{n}[l]\\bm{c}_l^{\\ast}\\ensuremath{\\bm{{f}}_n}\\xspace\\right)^{\\ast } \\left(\\sum_{l=1}^lz_{n}[l]\\bm{c}_l^{\\ast}\\ensuremath{\\bm{{f}}_n}\\xspace \\right ) z_{n}[\\alpha]z^{\\ast}_n[\\beta ] } \\right]}\\xspace \\nonumber \\\\      & = & \\ensuremath{\\mathbb{e}_{z}\\left [ { \\sum_{l=1}^l|z_{n}[l]|^2|\\bm{c}_l^{\\ast}\\ensuremath{\\bm{{f}}_n}\\xspace|^2z_n[\\alpha]z_n^{\\ast}[\\beta ] + 2\\sum_{k\\neq l}\\mbox{re}\\left(z_n^{\\ast}[l]z_n[k ] \\ensuremath{\\langle { \\bm{c}_l^{\\ast}\\ensuremath{\\bm{{f}}_n}\\xspace},{\\bm{c}_k^{\\ast}\\ensuremath{\\bm{{f}}_n}\\xspace}\\rangle}\\xspace \\right ) z_{n}[\\alpha]z^{\\ast}_n[\\beta]}\\right]}\\xspace \\nonumber \\\\      & = & \\left(\\frac{3}{2\\ensuremath{m}\\xspace^2}|\\bm{c}_\\alpha^{\\ast}\\ensuremath{\\bm{{f}}_n}\\xspace|^2 + \\frac{1}{\\ensuremath{m}\\xspace^2}\\|\\bm{c}\\ensuremath{\\bm{{f}}_n}\\xspace\\|_2 ^ 2\\right)\\delta_{\\alpha = \\beta } + \\frac{2}{\\ensuremath{m}\\xspace^2 } \\ensuremath{\\langle { \\bm{c}_\\alpha^{\\ast}\\ensuremath{\\bm{{f}}_n}\\xspace},{\\bm{c}_\\beta^{\\ast}\\ensuremath{\\bm{{f}}_n}\\xspace}\\rangle}\\xspace\\delta_{\\alpha\\neq\\beta}. \\nonumber\\end{aligned}\\ ] ]    we can then use the matrix formulation @xmath429}\\xspace & = & \\frac{3}{2\\ensuremath{m}\\xspace^2}\\mbox{diag}(\\bm{c}\\ensuremath{\\bm{{f}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace\\bm{c}^{\\ast } ) + \\frac{1}{\\ensuremath{m}\\xspace^2}\\|\\bm{c}\\ensuremath{\\bm{{f}}_n}\\xspace\\|_2 ^ 2\\bm{i}_\\ensuremath{m}\\xspace \\nonumber \\\\           & & \\qquad \\qquad + \\frac{2}{\\ensuremath{m}\\xspace^2}\\bm{c}\\ensuremath{\\bm{{f}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace\\bm{c}^{\\ast } + \\frac{2}{\\ensuremath{m}\\xspace^2}\\mbox{diag}(\\bm{c}\\ensuremath{\\bm{{f}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace\\bm{c}^{\\ast } ) \\nonumber \\\\      & = & \\frac{1}{\\ensuremath{m}\\xspace^2}\\|\\bm{c}\\ensuremath{\\bm{{f}}_n}\\xspace\\|_2 ^ 2\\bm{i}_\\ensuremath{m}\\xspace + 2\\bm{c}\\ensuremath{\\bm{{f}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace\\bm{c}^{\\ast } - \\frac{1}{2}\\mbox{diag}(\\bm{c}\\ensuremath{\\bm{{f}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace\\bm{c}^{\\ast } ) \\nonumber \\\\      & \\preceq & \\frac{3}{\\ensuremath{m}\\xspace^2 } \\|\\bm{c}\\ensuremath{\\bm{{f}}_n}\\xspace\\|_2 ^ 2\\bm{i}_\\ensuremath{m}\\xspace ,   \\nonumber\\end{aligned}\\ ] ] where to obtain the result we first use the linearity of the expectation along with the with the positive - semidefinite property of diag(@xmath430 ) , proving the fist portion of the lemma . to prove the second portion we simply take an expectation with respect to @xmath424 : @xmath431}\\xspace   \\preceq   \\ensuremath{\\mathbb{e}_{f}\\left[{\\frac{3}{\\ensuremath{m}\\xspace^2}\\|\\bm{c}\\ensuremath{\\bm{{f}}_n}\\xspace\\|_2 ^ 2\\bm{i}_\\ensuremath{m}\\xspace } \\right]}\\xspace \\preceq \\frac{3}{\\ensuremath{m}\\xspace^2 } \\|\\bm{c}\\|_f^2\\bm{i},_\\ensuremath{m}\\xspace , \\nonumber\\end{gathered}\\ ] ] completing the proof .      [",
    "lem : lemma4 ] let @xmath390 be the coherence factor as defined in equation  , and additionally assume that @xmath432 and that @xmath433 .",
    "if @xmath434 then with probability at least @xmath435 , @xmath436 for all @xmath437 $ ] .    in lemma  [ lem : lemma4 ]",
    "we show that the coherence term reduces at each golfing iteration .",
    "observe that @xmath438 @xmath439}\\xspace \\right)^2 .",
    "\\nonumber\\end{aligned}\\ ] ] to bound this quantity we use the scalar bernstein inequality on each of the inner quantities @xmath440}\\xspace .",
    "\\nonumber\\end{gathered}\\ ] ] as in the matrix bernstein formulation , we require both the variance and orlicz norm .",
    "first we find the variance , @xmath441}\\xspace & = & { \\kappa}^2\\sum_{n\\in\\gamma_k } \\ensuremath{\\mathbb{e}\\left[{|\\ensuremath{\\langle { \\ensuremath{\\mathcal{p}_t\\left ( \\ensuremath{\\bm{{a}}_n}\\xspace \\right)}\\xspace},{\\bm{e}_l\\ensuremath{\\bm{{f}}^{\\ast}}\\xspace}\\rangle}\\xspace|^2|\\ensuremath{\\langle { \\ensuremath{\\widetilde{\\bm{y}}}\\xspace_{k-1}},{\\ensuremath{\\bm{{a}}_n}\\xspace}\\rangle}\\xspace|^2}\\right]}\\xspace \\nonumber \\\\          & & \\qquad\\qquad\\qquad   - |\\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\langle { \\ensuremath{\\mathcal{p}_t\\left ( \\ensuremath{\\bm{{a}}_n}\\xspace \\right)}\\xspace},{\\bm{e}_l\\ensuremath{\\bm{{f}}^{\\ast}}\\xspace}\\rangle}\\xspace\\ensuremath{\\langle { \\ensuremath{\\widetilde{\\bm{y}}}\\xspace_{k-1}},{\\ensuremath{\\bm{{a}}_n}\\xspace}\\rangle}\\xspace } \\right]}\\xspace|^2 \\nonumber \\\\          & \\leq & { \\kappa}^2\\sum_{n\\in\\gamma_k } \\mathbb{e}\\left[|\\ensuremath{\\langle { \\ensuremath{\\bm{q}}\\xspace\\ensuremath{\\bm{q}^{\\ast}}\\xspace\\ensuremath{\\bm{{z}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace},{\\bm{e}_l\\ensuremath{\\bm{{f}}^{\\ast}}\\xspace}\\rangle}\\xspace + \\ensuremath{\\langle { \\ensuremath{\\bm{{z}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace\\ensuremath{\\bm{v}}\\xspace\\ensuremath{\\bm{v}}\\xspace},{\\bm{e}_l\\ensuremath{\\bm{{f}}^{\\ast}}\\xspace}\\rangle}\\xspace\\right .",
    "\\nonumber \\\\          & & \\qquad\\qquad\\qquad \\left.+ \\ensuremath{\\langle { \\ensuremath{\\bm{q}}\\xspace\\ensuremath{\\bm{q}^{\\ast}}\\xspace\\ensuremath{\\bm{{z}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace\\ensuremath{\\bm{v}}\\xspace\\ensuremath{\\bm{v}^{\\ast}}\\xspace},{\\bm{e}_l\\ensuremath{\\bm{{f}}^{\\ast}}\\xspace}\\rangle}\\xspace|^2|\\ensuremath{\\langle { \\ensuremath{\\widetilde{\\bm{y}}}\\xspace_{k-1}},{\\ensuremath{\\bm{{a}}_n}\\xspace}\\rangle}\\xspace|^2\\right ] \\nonumber \\\\      & \\leq & { \\kappa}^2\\sum_{n\\in\\gamma_k } \\ensuremath{\\mathbb{e}}\\xspace \\left[\\left(|\\ensuremath{\\langle { \\ensuremath{\\bm{q}}\\xspace\\ensuremath{\\bm{q}^{\\ast}}\\xspace\\ensuremath{\\bm{{z}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace},{\\bm{e}_l\\ensuremath{\\bm{{f}}^{\\ast}}\\xspace}\\rangle}\\xspace|^2 + |\\ensuremath{\\langle { \\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace\\ensuremath{\\bm{v}}\\xspace\\ensuremath{\\bm{v}^{\\ast}}\\xspace},{\\ensuremath{\\bm{{f}}^{\\ast}}\\xspace}\\rangle}\\xspace z_n[l]|^2 \\right .",
    "\\nonumber \\\\ & &   \\qquad\\qquad\\qquad \\left .",
    "+ |\\ensuremath{\\langle { \\ensuremath{\\bm{q}}\\xspace\\ensuremath{\\bm{q}^{\\ast}}\\xspace\\ensuremath{\\bm{{z}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace\\ensuremath{\\bm{v}}\\xspace\\ensuremath{\\bm{v}^{\\ast}}\\xspace},{\\bm{e}_l\\ensuremath{\\bm{{f}}^{\\ast}}\\xspace}\\rangle}\\xspace|^2\\right)| \\ensuremath{\\langle { \\ensuremath{\\widetilde{\\bm{y}}}\\xspace_{k-1}},{\\ensuremath{\\bm{{a}}_n}\\xspace}\\rangle}\\xspace|^2 \\right ] .",
    "\\nonumber\\end{aligned}\\ ] ]    this sum consists of three terms .",
    "the first of which can be bounded using lemma  [ lem : lemma3 ] , @xmath442}\\xspace   \\nonumber \\\\          & & \\qquad\\qquad\\qquad =   \\sum_{n\\in\\gamma_k}\\ensuremath{\\mathbb{e}\\left[{|\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace\\ensuremath{\\bm{{f}}}\\xspace\\ensuremath{\\langle { \\bm{q}_l},{\\ensuremath{\\bm{q}^{\\ast}}\\xspace\\ensuremath{\\bm{{z}}_n}\\xspace}\\rangle}\\xspace|^2|\\ensuremath{\\langle { \\ensuremath{\\widetilde{\\bm{y}}}\\xspace_{k-1}},{\\ensuremath{\\bm{{a}}_n}\\xspace}\\rangle}\\xspace|^2}\\right]}\\xspace \\nonumber \\\\      & & \\qquad\\qquad\\qquad\\leq   3\\sum_{n\\in\\gamma_k}\\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\bm{{f}}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace\\ensuremath{\\bm{{f}}}\\xspace\\bm{q}_l^{\\ast}\\ensuremath{\\bm{q}^{\\ast}}\\xspace\\ensuremath{\\left|\\left| { \\ensuremath{\\widetilde{\\bm{y}}}\\xspace_{k-1}\\ensuremath{\\bm{{f}}_n}\\xspace } \\right|\\right|}\\xspace_2 ^ 2\\bm{i}_\\ensuremath{l}\\xspace\\ensuremath{\\bm{q}}\\xspace\\bm{q}_l}\\right]}\\xspace \\nonumber \\\\      & & \\qquad\\qquad \\qquad\\leq   \\frac{3\\ensuremath{r}\\xspace\\mu_{k-1}^2}{\\ensuremath{m}\\xspace^3}\\ensuremath{\\left|\\left| { \\bm{q}_l } \\right|\\right|}\\xspace_2 ^ 2\\sum_{n\\in\\gamma_k}\\ensuremath{\\bm{{f}}^{\\ast}}\\xspace\\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\bm{{f}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace}\\right]}\\xspace\\ensuremath{\\bm{{f}}}\\xspace \\nonumber \\\\      & & \\qquad\\qquad\\qquad = \\frac{3\\ensuremath{n}\\xspace\\ensuremath{r}\\xspace\\mu_{k-1}^2}{{\\kappa}\\ensuremath{m}\\xspace^2}\\ensuremath{\\left|\\left| { \\bm{q}_l } \\right|\\right|}\\xspace_2 ^ 2 .",
    "\\nonumber\\end{aligned}\\ ] ]    for the second term we have @xmath443|^2|\\ensuremath{\\langle { \\ensuremath{\\widetilde{\\bm{y}}}\\xspace_{k-1}},{\\ensuremath{\\bm{{a}}_n}\\xspace}\\rangle}\\xspace|^2}\\right]}\\xspace   \\nonumber \\\\          & & \\qquad\\qquad\\qquad = \\sum_{n\\in\\gamma_k}\\ensuremath{\\mathbb{e}\\left[{|\\ensuremath{\\langle { \\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}_n}\\xspace},{\\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}}\\xspace}\\rangle}\\xspace|^2|z_n[l]|^2|\\ensuremath{\\langle { \\ensuremath{\\widetilde{\\bm{y}}}\\xspace_{k-1}},{\\ensuremath{\\bm{{a}}_n}\\xspace}\\rangle}\\xspace|^2}\\right]}\\xspace \\nonumber \\\\      & & \\qquad\\qquad\\qquad = \\sum_{n\\in\\gamma_k}\\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\bm{{f}}^{\\ast}}\\xspace\\ensuremath{\\bm{v}}\\xspace\\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace\\ensuremath{\\bm{v}}\\xspace\\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}}\\xspace|\\ensuremath{\\langle { \\ensuremath{\\widetilde{\\bm{y}}}\\xspace_{k-1}},{z_n[l]\\ensuremath{\\bm{{z}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace}\\rangle}\\xspace|^2}\\right]}\\xspace .",
    "\\nonumber \\end{aligned}\\ ] ] using the fact that @xmath444|^2 = \\bm{e}_l^{\\ast}\\ensuremath{\\bm{{z}}_n}\\xspace\\ensuremath{\\bm{{z}}^{\\ast}_n}\\xspace\\bm{e}_l$ ] and lemma  [ lem : lemma3 ] , we obtain @xmath443|^2|\\ensuremath{\\langle { \\ensuremath{\\widetilde{\\bm{y}}}\\xspace_{k-1}},{\\ensuremath{\\bm{{a}}_n}\\xspace}\\rangle}\\xspace|^2}\\right]}\\xspace   \\nonumber \\\\          & & \\qquad\\qquad\\qquad \\leq \\frac{3}{\\ensuremath{m}\\xspace^2}\\sum_{n\\in\\gamma_k}\\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\bm{{f}}^{\\ast}}\\xspace\\ensuremath{\\bm{v}}\\xspace\\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace\\ensuremath{\\bm{v}}\\xspace\\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}}\\xspace\\bm{e}_l^{\\ast}\\ensuremath{\\left|\\left| { \\ensuremath{\\widetilde{\\bm{y}}}\\xspace_{k-1}\\ensuremath{\\bm{{f}}_n}\\xspace } \\right|\\right|}\\xspace_2 ^ 2\\bm{i}_\\ensuremath{l}\\xspace\\bm{e}_l}\\right]}\\xspace \\nonumber \\\\      & & \\qquad\\qquad\\qquad \\leq \\frac{3\\ensuremath{r}\\xspace\\mu_{k-1}^2}{\\ensuremath{m}\\xspace^2}\\sum_{n\\in\\gamma_k}\\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\bm{{f}}^{\\ast}}\\xspace\\ensuremath{\\bm{v}}\\xspace\\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace\\ensuremath{\\bm{v}}\\xspace\\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}}\\xspace}\\right]}\\xspace \\nonumber \\\\      & & \\qquad\\qquad\\qquad \\leq \\frac{3\\ensuremath{r}\\xspace\\mu_{k-1}^2}{\\ensuremath{m}\\xspace^2}|\\gamma_k|\\ensuremath{\\left|\\left| { \\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}}\\xspace } \\right|\\right|}\\xspace_2 ^ 2 \\nonumber \\\\      & & \\qquad\\qquad\\qquad \\leq \\frac{3\\ensuremath{r}\\xspace^2\\mu_{k-1}^2\\mu_0 ^ 2}{{\\kappa}\\ensuremath{m}\\xspace^1}. \\nonumber \\\\\\end{aligned}\\ ] ]    finally , for the third term , we have @xmath445}\\xspace   \\nonumber \\\\          & & \\qquad\\qquad\\qquad = \\sum_{n\\in\\gamma_k}\\ensuremath{\\mathbb{e}\\left[{|\\ensuremath{\\langle { \\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}_n}\\xspace},{\\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}}\\xspace}\\rangle}\\xspace|^2|\\ensuremath{\\langle { \\bm{q}_l},{\\ensuremath{\\bm{q}^{\\ast}}\\xspace\\ensuremath{\\bm{{z}}_n}\\xspace}\\rangle}\\xspace|^2|\\ensuremath{\\langle { \\ensuremath{\\widetilde{\\bm{y}}}\\xspace_{k-1}},{\\ensuremath{\\bm{{a}}_n}\\xspace}\\rangle}\\xspace|^2}\\right]}\\xspace \\nonumber \\\\      & & \\qquad\\qquad\\qquad = \\sum_{n\\in\\gamma_k}\\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\bm{{f}}^{\\ast}}\\xspace\\ensuremath{\\bm{v}}\\xspace\\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}_n}\\xspace\\ensuremath{\\bm{{f}}^{\\ast}_n}\\xspace\\ensuremath{\\bm{v}}\\xspace\\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}}\\xspace|\\ensuremath{\\langle { \\bm{q}_l},{\\ensuremath{\\bm{q}^{\\ast}}\\xspace\\ensuremath{\\bm{{z}}_n}\\xspace}\\rangle}\\xspace|^2|\\ensuremath{\\langle { \\ensuremath{\\widetilde{\\bm{y}}}\\xspace_{k-1}},{\\ensuremath{\\bm{{a}}_n}\\xspace}\\rangle}\\xspace|^2}\\right]}\\xspace \\nonumber \\\\      & & \\qquad\\qquad\\qquad \\leq \\sum_{n\\in\\gamma_k}\\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\left|\\left| { \\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}}\\xspace } \\right|\\right|}\\xspace_2 ^ 2\\bm{q}_l^{\\ast}\\ensuremath{\\bm{q}^{\\ast}}\\xspace\\ensuremath{\\bm{{z}}_n}\\xspace\\ensuremath{\\bm{{z}}^{\\ast}_n}\\xspace|\\ensuremath{\\langle { \\ensuremath{\\widetilde{\\bm{y}}}\\xspace_{k-1}},{\\ensuremath{\\bm{{a}}_n}\\xspace}\\rangle}\\xspace|^2\\ensuremath{\\bm{q}}\\xspace\\bm{q}_l   } \\right]}\\xspace \\nonumber \\\\      & & \\qquad\\qquad\\qquad \\leq \\frac{3}{\\ensuremath{m}\\xspace^2}\\ensuremath{\\left|\\left| { \\bm{q}_l } \\right|\\right|}\\xspace_2 ^ 2\\ensuremath{\\left|\\left| { \\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}}\\xspace } \\right|\\right|}\\xspace_2 ^ 2\\sum_{n\\in\\gamma_k}\\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\left|\\left| { \\ensuremath{\\widetilde{\\bm{y}}}\\xspace_{k-1}\\ensuremath{\\bm{{f}}_n}\\xspace } \\right|\\right|}\\xspace_2 ^ 2   } \\right]}\\xspace \\nonumber \\\\      & & \\qquad\\qquad\\qquad\\leq \\frac{3\\ensuremath{r}\\xspace^2\\mu_0 ^ 2\\mu_{k-1}^2}{{\\kappa}\\ensuremath{m}\\xspace}\\ensuremath{\\left|\\left| { \\bm{q}_l } \\right|\\right|}\\xspace_2 ^ 2 .",
    "\\nonumber\\end{aligned}\\ ] ]          for the second term we have @xmath451 } \\right|\\right|}\\xspace_{\\psi_2}^2 \\leq \\ensuremath{\\left|\\left| { \\ensuremath{\\left|\\left| { \\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}_n}\\xspace } \\right|\\right|}\\xspace_2\\ensuremath{\\left|\\left| { \\ensuremath{\\bm{v}^{\\ast}}\\xspace\\ensuremath{\\bm{{f}}}\\xspace } \\right|\\right|}\\xspace_2z_n[l ] } \\right|\\right|}\\xspace_{\\psi_2}^2 \\leq c\\frac{\\ensuremath{r}\\xspace^2}{\\ensuremath{m}\\xspace}\\mu_0 ^ 4 .",
    "\\nonumber \\end{gathered}\\ ] ] similarly , for the final term we have @xmath452      since we wish to bound the square of the sum of terms , we calculate the square values of the two terms in the bernstein inequality .",
    "the first term is bounded by @xmath454 and the second term is bounded by @xmath455 @xmath456 where the last step assumes @xmath432 and @xmath433 .",
    "each summand is then bounded by the maximum of these two quantities with probability @xmath457 , the @xmath458 term coming from the union bound over all terms in each inner sum .    using this bound on each summand",
    ", we obtain the total bound by taking a union bound , summing over @xmath459 $ ] , and dividing by @xmath119 , yielding a bound of the maximum of @xmath460 and @xmath461 with probability @xmath462 . to complete the proof",
    ", we note that if we have @xmath463 then both terms in this bound are less than @xmath464 .",
    "m.  aharon , m.  elad , a.  bruckstein , and y.  katz . : an algorithm for designing of overcomplete dictionaries for sparse representations .",
    "_ ieee transactions on signal processing _ , 540 ( 11):0 43114322 , 2006 .          n.  j. apthorpe , a.  j. riordan , r.  e. aguilar , j.  homann , y.  gu , d.  w. tank , and s.  h. seung .",
    "automatic neuron detection in calcium imaging data using convolutional networks .",
    "_ advances in neural information processing systems ( nips ) _ , pages 32703278 , dec . 2016 .",
    "a.  balavoine , j.  romberg , and c.  j. rozell . convergence and rate analysis of neural networks for sparse approximation .",
    "_ ieee transactions on neural networks and learning systems _ , 23:0 13771389 , sept . 2012 .",
    "a.  bernyi , z.  somogyvri , a.  j. nagy , l.  roux , j.  d. long , s.  fujisawa , e.  stark , a.  leonardo , t.  d. harris , and g.  buzski .",
    "large - scale , high - density ( up to 512 channels ) recording of local circuits in behaving animals .",
    "_ journal of neurophysiology _ , 1110 ( 5):0 11321149 , 2014 .",
    "e.  j. cands and y.  plan .",
    "tight oracle inequalities for low - rank matrix recovery from a minimal number of noisy random measurements . _ ieee transactions on information theory _ , 570 ( 4):0 23422359 , 2011 .",
    "e.  j. cands , j.  romberg , and tao t. robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information . _ ieee transactions on information theory _ , 520 ( 2):0 489509 , 2006 .        p.  chen and d.  suter . recovering the missing components in a large noisy low - rank matrix : application to sfm . _ ieee transactions on pattern analysis and machine intelligence _ ,",
    "260 ( 8):0 10511063 , 2004 .    y.  chen , z.  lin , x.  zhao , g.  wang , and y.  gu .",
    "deep learning - based classification of hyperspectral data .",
    "_ ieee journal of selected topics in applied earth observations and remote sensing _ , 70 ( 6):0 20942107 , 2014 .",
    "y.  chen , h.  jiang , c.  li , x.  jia , and p.  ghamisi . deep feature extraction and classification of hyperspectral images based on convolutional neural networks .",
    "_ ieee transactions on geoscience and remote sensing _ , 540 ( 10):0 62326251 , 2016 .",
    "chiang , c .- h .",
    "liu , c .- h .",
    "duan , and s .- h .",
    "lai . learning component - level sparse representation for image and video categorization .",
    "_ ieee transactions on image processing _ , 220 (",
    "12):0 47754787 , 2013 .",
    "j.  donahue , l.  a. hendricks , s.  guadarrama , m.  rohrbach , s.  venugopalan , k.  saenko , and t.  darrell .",
    "long - term recurrent convolutional networks for visual recognition and description . in _ ieee conference on computer vision and pattern recognition _ , pages 26252634 , 2015 .      o.  faugeras , j.  touboul , and b.  cessac . a constructive mean - field analysis of multi - population neural networks with random synaptic weights and stochastic inputs .",
    "_ frontiers in computational neuroscience _ , 30 ( 1):0 126 , feb . 2009 .",
    "y.  gao , e.  archer , l.  paninski , and j.  p. cunningham .",
    "linear dynamical neural population models through nonlinear embeddings .",
    "_ advances in neural information processing systems ( nips ) _ , pages 163171 , dec . 2016 .",
    "a.  graves , a .- r .",
    "mohamed , and g.  hinton .",
    "speech recognition with deep recurrent neural networks . in _",
    "ieee international conference on acoustics , speech and signal processing ( icassp ) _ , pages 66456649 , 2013 .",
    "x.  hinaut , m.  petit , g.  pointeau , and p.  f. dominey .",
    "exploring the acquisition and production of grammatical constructions through human - robot interaction with echo state networks . _",
    "frontiers in neurorobotics _ , 80 ( 16):0 117 , 2014 .",
    "k.  kavukcuoglu , p.  sermanet , y.  l. boureau , k.  gregor , m.  mathieu , and y.  lecun . learning convolutional feature hierarchies for visual recognition .",
    "_ advances in neural information processing systems ( nips ) _ , pages 10901098 , 2010 .",
    "z.  liu and l.  vandenberghe .",
    "interior - point method for nuclear norm approximation with application to system identification .",
    "_ siam journal on matrix analysis and applications _ , 310 ( 3):0 12351256 , 2009 .",
    "h.  sak , a.  senior , and f.  beaufays",
    ". long short - term memory recurrent neural network architectures for large scale acoustic modeling . _",
    "conference of international speech communication association ( interspeech ) _ , 2014 .",
    "m.  a. veganzones , m.  simoes , g.  licciardi , n.  yokoya , j.m .",
    "bioucas - dias , and j.  chanussot .",
    "hyperspectral super - resolution of locally low rank images from complementary multisource data .",
    "_ ieee transactions on image processing _ , 250 ( 1):0 274288 , 2016"
  ],
  "abstract_text": [
    "<S> recurrent neural networks ( rnns ) have drawn interest from machine learning researchers because of their effectiveness at preserving past inputs for time - varying data processing tasks . to understand the success and limitations of rnns , it is critical that we advance our analysis of their fundamental memory properties . </S>",
    "<S> we focus on echo state networks ( esns ) , which are rnns with simple memoryless nodes and random connectivity . in most existing analyses , the short - term memory ( stm ) capacity results </S>",
    "<S> conclude that the esn network size must scale linearly with the input size for unstructured inputs . </S>",
    "<S> the main contribution of this paper is to provide general results characterizing the stm capacity for linear esns with multidimensional input streams when the inputs have common low - dimensional structure : sparsity in a basis or significant statistical dependence between inputs . in both cases , </S>",
    "<S> we show that the number of nodes in the network must scale linearly with the information rate and poly - logarithmically with the input dimension . </S>",
    "<S> the analysis relies on advanced applications of random matrix theory and results in explicit non - asymptotic bounds on the recovery error . </S>",
    "<S> taken together , this analysis provides a significant step forward in our understanding of the stm properties in rnns .    </S>",
    "<S> short - term memory , recurrent neural networks , sparse signal recovery , low - rank recovery , restricted isometry property </S>"
  ]
}