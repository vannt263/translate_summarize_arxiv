{
  "article_text": [
    "survival analysis is a commonly - used method for the analysis of failure time such as biological death , mechanical failure , or credit default . within this context , death or failure",
    "is also referred to as an  event \" .",
    "survival analysis tries to model time - to - event data , which is usually subject to censoring due to the termination of study .",
    "the main goal is to study the dependence of the survival time @xmath0 on covariate variables @xmath1 , where @xmath2 denotes the dimensionality of the covariate space .",
    "one common way of achieving this goal is hazard regression , which studies how the conditional hazard function of @xmath0 depends on the covariate @xmath3 , which is defined as @xmath4 according to the definition , the conditional hazard function is nothing but the instantaneous rate of failure at time @xmath5 given a particular value @xmath6 for the covariate @xmath7 .",
    "the proportional hazards model is very popular , partially due to its simplicity and its convenience in dealing with censoring .",
    "the model assumes that @xmath8 in which @xmath9 is the baseline hazard function and @xmath10 is the covariate effect .",
    "note that this model is not uniquely determined in that @xmath11 and @xmath12 give the same model for any @xmath13 .",
    "thus one identifiability condition needs to be specified .",
    "when the identifiability condition @xmath14 is enforced , the function @xmath9 , the conditional hazard function of @xmath0 given @xmath15 , is called the baseline hazard function .    by taking the reparametrization @xmath16",
    ", @xcite introduced the proportional hazards model @xmath17 see @xcite and references therein for more detailed literature on cox s proportional hazards model .    here",
    "the baseline hazard function @xmath9 is typically completely unspecified and needs to be estimated nonparametrically .",
    "a linear model assumption , @xmath18 , may be made , as is done in this paper . here",
    "@xmath19 is the regression parameter vector . while conducting survival analysis",
    ", we not only need to estimate @xmath20 but also have to estimate the baseline hazard function @xmath9 nonparametrically .",
    "interested readers may consult @xcite for more details .",
    "recent technological advances have made it possible to collect a huge amount of covariate information such as microarray , proteomic and snp data via bioimaging technology while observing survival information on patients in clinical studies .",
    "however it is quite likely that not all available covariates are associated with the clinical outcome such as the survival time .",
    "in fact , typically a small fraction of covaraites are associated with the clinical time .",
    "this is the notion of sparsity and consequently calls for the identification of important risk factors and at the same time quantifying their risk contributions when we analyze time - to - event data with many predictors .",
    "mathematically , it means that we need to identify which @xmath21s are nonzero and also estimate these nonzero @xmath21s .",
    "most classical model selection techniques have been extended from linear regression to survival analysis .",
    "they include the best - subset selection , stepwise selection , bootstrap procedures @xcite , bayesian variable selection @xcite .",
    "please see references therein .",
    "similarly , other more modern penalization approaches have been extended as well .",
    "@xcite applied the lasso penalty to survival analysis .",
    "@xcite considered survival analysis with the scad and other folded concave penalties .",
    "@xcite proposed the adaptive lasso penalty while studying time - to - event data . among many other considerations",
    "is @xcite .",
    "available theory and empirical results show that these penalization approaches work well with a moderate number of covariates .",
    "recently we have seen a surge of interest in variable selection with an ultra - high dimensionality . by ultra - high",
    ", @xcite meant that the dimensionality grows exponentially in the sample size , i.e. , @xmath22 for some @xmath23 .",
    "for ultra - high dimensional linear regression , @xcite proposed sure independence screening ( sis ) based on marginal correlation ranking .",
    "asymptotic theory is proved to show that , with high probability , sis keeps all important predictor variables with vanishing false selection rate .",
    "an important extension , iterative sis ( isis ) , was also proposed to handle difficult cases such as when some important predictors are marginally uncorrelated with the response . in order to deal with more complex real data , @xcite extended sis and isis to more general loss based models such as generalized linear models , robust regression , and classification and improved some important steps of the original isis .",
    "in particular , they proposed the concept of conditional marginal regression and a new variant of the method based on splitting samples .",
    "a non - asymptotic theoretical result shows that the splitting based new variant can reduce false discovery rate .",
    "although the extension of @xcite covers a wide range of statistical models , it has not been explored whether the iterative sure independence screening method can be extended to hazard regression with censoring event time . in this work , we will focus on cox s proportional hazards model and extend sis and isis accordingly",
    ". other extensions of sis include @xcite and @xcite to generalized linear models and nonparametric additive models , in which new insights are provided via elegant mathematical results and carefully designed simulation studies .",
    "the rest of the article is organized as follows .",
    "section [ sec : cox ] details the cox s proportional hazards model .",
    "an overview of variable selection via penalized approach is given in section [ sec : variable - selection - cox ] for cox s proportional hazards model .",
    "we extend the sis and isis procedures to cox s model in section [ sec : sis - cox ] .",
    "simulation results in section [ sec : simulation ] and real data analysis in section [ sec : realdata ] demonstrate the effectiveness of the proposed sis and isis methods .",
    "let @xmath0 , @xmath24 , and @xmath7 denote the survival time , the censoring time , and their associated covariates , respectively .",
    "correspondingly , denote by @xmath25 the observed time and @xmath26 the censoring indicator . for simplicity",
    "we assume that @xmath0 and @xmath24 are conditionally independent given @xmath7 and that the censoring mechanism is noninformative . our observed data set @xmath27 is an independently and identically distributed random sample from a certain population @xmath28 .",
    "define @xmath29 and @xmath30 to be the censored and uncensored index sets , respectively .",
    "then the complete likelihood of the observed data set is given by @xmath31 where @xmath32 , @xmath33 , and @xmath34 are the conditional density function , the conditional survival function , and the conditional hazard function of @xmath0 given @xmath3 , respectively .",
    "let @xmath35 be the ordered distinct observed failure times .",
    "let @xmath36 index its associate covariates @xmath37 and @xmath38 be the risk set right before the time @xmath5 : @xmath39 .",
    "consider the proportional hazards model , @xmath40 where @xmath9 is the baseline hazard function . in this model , both @xmath9 and @xmath20 are unknown and have to be estimated . under model ,",
    "the likelihood becomes @xmath41 where @xmath42 is the corresponding cumulative baseline hazard function .",
    "following breslow s idea , consider the ",
    "least informative \" nonparametric modeling for @xmath43 , in which @xmath44 has a possible jump @xmath45 at the observed failure time @xmath46 , namely , @xmath47 .",
    "then @xmath48 consequently the log - likelihood becomes @xmath49 maximizer @xmath45 is given by @xmath50 putting this maximizer back to the log - likelihood , we get @xmath51,\\nonumber\\ ] ] which is equivalent to @xmath52 by using the censoring indicator @xmath53 .",
    "this is the partial likelihood due to @xcite .    maximizing @xmath54 in ( [ simlik ] ) with respect to @xmath20",
    ", we can get an estimate @xmath55 of the regression parameter .",
    "once @xmath55 is available , we may plug it into ( [ hj ] ) to get @xmath56 .",
    "these newly obtained @xmath57s can be plugged into ( [ leastinfo ] ) to obtain our nonparametric estimate of the baseline cumulative hazard function .",
    "in the estimation scheme presented in the previous section , none of the estimated regression coefficients is exactly zero , leaving all covariates in the final model .",
    "consequently it is incapable of selecting important variables and handling the case with @xmath58 . to achieve variable selection , classical techniques such as the best - subset selection , stepwise selection , and bootstrap procedures",
    "have been extended accordingly to handle cox s proportional hazards model .    in this section , we will focus on some more advanced techniques for variable selection via penalization .",
    "variable selection via penalization has received lots of attention recently .",
    "basically it uses some variable selection - capable penalty function to regularize the objective function while performing optimization .",
    "many variable selection - capable penalty functions have been proposed .",
    "a well known example is the @xmath59 penalty @xmath60 , which is also known as the lasso penalty @xcite . among many others",
    "are the scad penalty @xcite , elastic - net penalty @xcite , adaptive @xmath59 @xcite , and minimax concave penalty @xcite .",
    "denote a general penalty function by @xmath61 , where @xmath62 is a regularization parameter . from derivations in the last section ,",
    "penalized likelihood is equivalent to penalized partial likelihood : while maximizing @xmath54 in ( [ simlik ] ) , one may regularize it using @xmath63 .",
    "equivalently we solve @xmath64 by including a negative sign in front of @xmath54 . in the literature , @xcite , @xcite , and",
    "@xcite considered the @xmath59 , scad , and adaptive @xmath59 penalties while studying time - to - event data , respectively , among many others .    in this paper , we will use the scad penalty for our extensions of sis and isis whenever necessary .",
    "the scad function is a quadratic spline and symmetric around the origin .",
    "it can be defined in terms of its first order derivative @xmath65 for some @xmath66 and @xmath67 . here",
    "@xmath68 is a parameter and @xcite recommend to use @xmath69 based on a bayesian argument .",
    "the scad penalty is plotted in figure [ scadplot ] for @xmath69 and @xmath70 .",
    "the scad penalty is non - convex , leading to non - convex optimization . for the non - convex scad penalized optimization ,",
    "@xcite proposed the local quadratic approximation ; @xcite proposed the local linear approximation ; @xcite presented the difference convex algorithm . in this work ,",
    "whenever necessary we use the local linear approximation algorithm to solve the scad penalized optimization .",
    "the penalty based variable selection techniques work great with a moderate number of covariates .",
    "however their usefulness is limited while dealing with an ultra - high dimensionality as shown in @xcite . in the linear regression case",
    ", @xcite proposed to rank covariates according to the absolute value of their marginal correlation with the response variable and select the top ranked covariates .",
    "they provided theoretical result to guarantee that this simple correlation ranking retains all important covariates with high probability .",
    "thus they named their method sure independence screening ( sis ) . in order to handle difficult problems such as the one with some important covariates being marginally uncorrelated with the response , they proposed iterative sis ( isis )",
    ". isis begins with sis , then it regresses the response on covariates selected by sis and uses the regression residual as a  working \" response to recruit more covariates with sis .",
    "this process can be repeated until some convergence criterion has been met .",
    "empirical improvement over sis has been observed for isis . in order to increase the power of the sure independence screening technique ,",
    "@xcite has extended sis and isis to more general models such as generalized linear models , robust regression , and classification and made several important improvements .",
    "we now extend the key idea of sis and isis to handle cox s proportional hazards model .",
    "let @xmath71 be the index set of the true underlying sparse model , namely , @xmath72 , where @xmath73s are the true regression coefficients in the cox s proportional hazards model ( [ coxmodelwu ] ) .",
    "first let us review the definition of sure screening property .",
    "we say a model selection procedure satisfies sure screening property if the selected model @xmath74 with model size @xmath75 includes the true model @xmath71 with probability tending to one .    for each covariate @xmath76 ( @xmath77 ) , define its marginal utility as the maximum of the partial likelihood of the single covariate : @xmath78 here @xmath79 is the @xmath80-th element of @xmath81 , namely @xmath82 .",
    "intuitively speaking , the larger the marginal utility is , the more information the corresponding covariate contains the information about the survival outcome .",
    "once we have obtained all marginal utilities @xmath83 for @xmath84 , we rank all covariates according to their corresponding marginal utilities from the largest to the smallest and select the @xmath85 top ranked covariates .",
    "denote by @xmath86 the index set of these @xmath85 covariates that have been selected .",
    "the index set @xmath86 is expected to cover the true index set @xmath71 with a high probability , especially when we use a relative large @xmath85 .",
    "this is formally shown by @xcite for the linear model with gaussian noise and gaussian covariates and significantly expanded by @xcite to generalized linear models with non - gaussian covariates .",
    "the parameter @xmath85 is usually chosen large enough to ensure the sure screening property .",
    "however the estimated index set @xmath86 may also include a lot of unimportant covariates . to improve performance ,",
    "the penalization based variable selection approach can be applied to the selected subset of the variables @xmath87 to further delete unimportant variables .",
    "mathematically , we then solve the following penalized partial likelihood problem : @xmath88 where @xmath89 denotes a sub - vector of @xmath81 with indices in @xmath86 and similarly for @xmath90 .",
    "it will lead to sparse regression parameter estimate @xmath91 . denote the index set of nonzero components of @xmath92 by @xmath74 , which will serve as our final estimate of @xmath71 .",
    "@xcite pointed out that sis can fail badly for some challenging scenarios such as the case that there exist jointly related but marginally unrelated covariates or jointly uncorrelated covariates having higher marginal correlation with the response than some important predictors . to deal with such difficult scenarios , iterative sis ( isis ) has been proposed . comparing to sis which is based on marginal information only ,",
    "isis tries to make more use of joint covariates information .",
    "the iterative sis begins with using sis to select an index set @xmath93 , upon which a penalization based variable selection step is applied to get regression parameter estimate @xmath94 .",
    "a refined estimate of the true index set is obtained and denoted by @xmath95 , the index set corresponding to nonzero elements of @xmath94 .    as in @xcite",
    ", we next define the conditional utility of each covariate @xmath80 that is not in @xmath95 as follows : @xmath96 this conditional utility measures the additional contribution of the @xmath80th covariate given that all covariates with indices in @xmath95 have been included in the model .    once the conditional utilities have been defined for each covariate that is not in @xmath95 , we rank them from the largest to the smallest and select these covariates with top rankings .",
    "denote the index set of these selected covariates by @xmath97 . with @xmath97 having been identified ,",
    "we minimize @xmath98 with respect to @xmath99 to get sparse estimate @xmath100 . denote the index set corresponding to nonzero components of @xmath100 to be @xmath101 , which is our updated estimate of the true index set @xmath71 .",
    "note that this step can delete some variables @xmath102 that were previously selected .",
    "this idea was proposed in @xcite and is an improvement of the idea in @xcite .",
    "the above iteration can be repeated until some convergence criterion is reached .",
    "we adopt the criterion of either having identified @xmath85 covariates or @xmath103 for some @xmath104 .",
    "@xcite noted that the idea of sample spliting can also be used to reduce the false selection rate . without loss of generality",
    ", we assume that the sample size @xmath105 is even .",
    "we randomly split the sample into two halves .",
    "then apply sis or isis separately to the data in each partition to obtain two estimates @xmath106 and @xmath107 of the true index set @xmath108 .",
    "both these two estimates could have high fsrs because they are based on a simple and crude screening method .",
    "yet each of them should include all important covariates with high probabilities .",
    "namely , important covariates should appear in both sets with probability tending to one asymptotically .",
    "define a new estimate by intersection @xmath109 .",
    "the new estimate @xmath110 should include all important covaraites with high probability as well due to properties of each individual estimate .",
    "however by construction , the number of unimportant covariates in the new estimate @xmath110 is much smaller .",
    "the reason is that , in order for an unimportant covariate to appear in @xmath110 , it has to be included in both @xmath106 and @xmath107 randomly .",
    "for the new variant method based on random splitting , @xcite obtained some non - asymptotic probability bound for the event that @xmath111 unimportant covaraites are included in the intersection @xmath110 for any natural number @xmath111 under some exchangeability condition on all unimportant covariates .",
    "the probability bound is decreasing in the dimensionality , showing a  blessing of dimensionality \" .",
    "please consult @xcite for more details .",
    "we want to remark that their theoretical bound is applicable to our setting as well while studying time - to - event data because theoretical bound is based on splitting the sample into two halves and only requires the independence between these two halves .",
    "while defining new variants , we may use the same @xmath85 as used in the original sis and isis .",
    "however it will lead to a very aggressive screening .",
    "we call the corresponding variant the first variant of ( i)sis . alternatively , in each step we may choose larger @xmath106 and @xmath107 to ensure that their intersection @xmath112 has @xmath85 covariates , which is called the second variant .",
    "the second variant ensures that there are at least @xmath85 covariates included before applying penalization in each step and is thus much less aggressive .",
    "numerical examples will be used to explore their performance and prefer to the first variant .",
    "in this section , we conduct simulation studies to show the power of the ( i)sis and its variants by comparing them with lasso @xcite in the cox s proportional hazards model . here",
    "the regularization parameter for lasso is tuned via five fold cross validation .",
    "most of the settings are adapted from @xcite and @xcite .",
    "four different configurations are considered with @xmath113 and @xmath114 . and",
    "two of them are revisited with a different pair of sample size @xmath115 and dimensionality @xmath116 .",
    "covariates in different settings are generated as follows .",
    "* 1 : @xmath117 are independent and identically distributed @xmath118 random variables . * 2 : @xmath117 are multivariate gaussian , marginally @xmath118 , and with serial correlation @xmath119 if @xmath120 . here",
    "we take @xmath121 . * 3 : @xmath117 are multivariate gaussian , marginally @xmath118 , and with correlation structure @xmath122 for all @xmath123 and @xmath124 if @xmath125 and @xmath104 are distinct elements of @xmath126 . *",
    "4 : @xmath117 are multivariate gaussian , marginally @xmath118 , and with correlation structure @xmath127 for all @xmath128 , @xmath122 for all @xmath129 , and @xmath130 if @xmath125 and @xmath104 are distinct elements of @xmath131 .",
    "* 5 : same as case 2 except @xmath115 and @xmath116 . *",
    "6 : same as case 4 except @xmath115 and @xmath116 .    here ,",
    "case 1 with independent predictors is the most straightforward for variable selection . in cases 2 - 6 , however , we have serial correlation such that @xmath132 does not decay as @xmath133 increases .",
    "we will see later that for cases 3 , 4 and 6 , the true coefficients are specially chosen such that the response is marginally independent but jointly dependent of @xmath134 .",
    "we therefore expect variable selection in these situations to be much more challenging , especially for the non - iterated versions of sis .",
    "notice that in the asymptotic theory of sis in @xcite , this type of dependence is ruled out by their condition ( 4 ) .    in our implementation , we choose @xmath135 for both the vanilla version of sis ( van - sis ) and the second variant ( var2-sis ) .",
    "for the first variant ( var1-sis ) , however , we use @xmath136 ( note that since the selected variables for the first variant are in the intersection of two sets of size @xmath85 , we typically end up with far fewer than @xmath85 variables selected by this method ) . for any type of sis or isis",
    ", we apply scad with these selected predictors to get a final estimate of the regression coefficients at the end of the screening step . whenever necessary",
    ", the bic is used to select the best tuning parameter in the regularization framework .    in all setting , the censoring time is generated from exponential distribution with mean @xmath137 .",
    "this corresponds to choosing the baseline hazard function @xmath138 for @xmath139 . the true regression coefficients and censoring rate in each of the six cases are as follows :    * 1 : @xmath140 , and @xmath141 for @xmath142 .",
    "the corresponding censoring rate is 33% . * 2 : the coefficients are the same as case 1 .",
    "the corresponding censoring rate is 27% . *",
    "3 : @xmath143 , and @xmath141 for @xmath144 .",
    "the corresponding censoring rate is 30% . *",
    "4 : @xmath145 and @xmath141 for @xmath146 .",
    "the corresponding censoring rate is 31% . * 5 : @xmath147 , and @xmath141 for @xmath142 .",
    "the corresponding censoring rate is 23% . *",
    "6 : the coefficients are the same as case 4 .",
    "the corresponding censoring rate is 36% .    in cases 1 , 2 and 5 the coefficients",
    "were chosen randomly , and were generated as @xmath148 with @xmath149 and @xmath150 with probability 0.5 and -1 with probability 0.5 , independent of @xmath151 . for cases 3 , 4 , and 6 , the choices ensure that even though @xmath152 , we have that @xmath134 and @xmath153 are marginally independent . the fact that @xmath134 is marginally independent of the response is designed to make it difficult for the common independent learning to select this variable . in cases 4 and 6",
    ", we add another important variable @xmath154 with a small coefficient to make it even more difficult .",
    "we report our simulation results based on 100 monte carlo repetitions for each setting in tables 1 - 7 .",
    "to present our simulation results , we use several different performance measures . in the rows labeled @xmath155 and @xmath156",
    ", we report the median @xmath59 and squared @xmath157 estimation errors @xmath158 and @xmath159 , respectively , where the median is over the 100 repetitions . in the row with label @xmath160 ,",
    "we report the proportion of the 100 repetitions that the ( i)sis procedure under consideration includes all of the important variables in the model , while the row with label @xmath161 reports the corresponding proportion of times that the final variables selected , after further application of the scad penalty , include all of the important ones .",
    "we also report the median model size ( mms ) of the final model among 100 repetitions in the row labeled mms .",
    "c|c|c|c|c|c|c|c + & & & & & & & + @xmath155 & 0.79 & 0.57 & 0.73 & 0.61 & 0.76 & 0.62 & 4.23 + @xmath156 & 0.13 & 0.09 & 0.15 & 0.1 & 0.15 & 0.1 & 0.98 + @xmath160 & 1 & 1 & 0.99 & 1 & 0.99 & 1 &  + @xmath161 & 1 & 1 & 0.99 & 1 & 0.99 & 1 & 1 + mms & 7 & 6 & 6 & 6 & 6 & 6 & 68.5 +   + & & & & & & & + @xmath155 & 2.2 & 0.64 & 4.22 & 0.8 & 3.95 & 0.78 & 4.38 + @xmath156 & 1.74 & 0.11 & 4.71 & 0.29 & 4.07 & 0.28 & 0.98 + @xmath160 & 0.71 & 1 & 0.42 & 0.99 & 0.46 & 0.99 & ",
    "+ @xmath161 & 0.71 & 1 & 0.42 & 0.99 & 0.46 & 0.99 & 1 + mms & 7 & 6 & 6 & 6 & 7 & 6 & 57 +    [ case1 ]    we report results of cases 1 and 2 in table 1 .",
    "recall that the covariates in case 1 are all independent . in this case",
    ", the van - sis performs reasonably well .",
    "yet , it does not perform well for the dependent case , case 2 .",
    "note the only difference between case 1 and case 2 is the covariance structure of the covariates .",
    "for both cases , vanilla - isis and its second variant perform very well .",
    "it is worth noticing that the isis improves significantly over sis , when covariates are dependent , in terms of both the probability of including all the true variables and in reducing the estimation    error .",
    "this comparison indicates that the isis performs much better when there is serious correlation among covariates .",
    "while implementing the lasso penalized cox s proportional hazards model , we adapted the fortran source code in the r package  glmpath . \"",
    "recall that the objective function in the lasso penalized cox s proportional hazards model is convex and nonlinear .",
    "what the fortran code does is to call a minos subroutine to solve the corresponding nonlinear convex optimization problem . here",
    "minos is an optimization software developed by systems optimization laboratory at stanford university .",
    "this nonlinear convex optimization problem is much more complicated than a general quadratic programming problem .",
    "thus generally it takes much longer time to solve , especially so when the dimensionality is high as confirmed by table 3 .",
    "however the algorithm we used does converge as the objective function is strictly convex .",
    "table 1 shows that lasso has the sure screening property as the isis , however , the median model size is ten times as large as that of isis . as a consequence , it also has larger estimation errors in terms of @xmath155 and @xmath156 .",
    "the fact that the median absolute deviation error is much larger than the median square error indicates that the lasso selects many small nonzero coefficients for those unimportant variables .",
    "this is also verified by the fact that lasso has a very large median model size .",
    "the explanation is the bias issue noted by @xcite . in order for lasso to have a small bias for nonzero coefficients",
    ", a smaller @xmath162 should be chosen .",
    "yet , a small @xmath162 recruits many small coefficients for unimportant variables . for case 2",
    ", the lasso has a similar performance as in case 1 in that it includes all the important variables but has a much larger model size .",
    "c|c|c|c|c|c|c|c + & & & & & & & + @xmath155 & 20.1 & 1.03 & 20.01 & 0.99 & 20.09 & 1.08 & 20.53 + @xmath156 & 94.72 & 0.49 & 100.42 & 0.47 & 94.77 & 0.55 & 76.31 + @xmath160 & 0 & 1 & 0 & 1 & 0 & 1 & ",
    "+ @xmath161 & 0 & 1 & 0 & 1 & 0 & 1 & 0.06 + mms & 13 & 4 & 8 & 4 & 13 & 4 & 118.5 +   + & & & & & & & + @xmath155 & 20.87 & 1.15 & 20.95 & 1.4 & 20.96 & 1.41 & 21.04 + @xmath156 & 96.46 & 0.51 & 102.14 & 1.77 & 97.15 & 1.78 & 77.03 + @xmath160 & 0 & 1 & 0 & 0.99 & 0 & 0.99 & ",
    "+ @xmath161 & 0 & 1 & 0 & 0.99 & 0 & 0.99 & 0.02 + mms & 13 & 5 & 9 & 5 & 13 & 5 & 118 +    [ case3 ]    results of cases 3 and 4 are reported in table 2 .",
    "note that , in both cases , the design ensures that @xmath134 is marginally independent of but jointly dependent on @xmath153 .",
    "this special design disables the sis to include @xmath134 in the corresponding identified model as confirmed by our numerical results .",
    "however , by using isis , we are able to select @xmath134 for each repetition .",
    "surprisingly , lasso rarely includes @xmath134 even if it is not a marginal screening based method .",
    "case 5 is even more challenging .",
    "in addition to the same challenge as case 4 , the coefficient @xmath163 is 3 times smaller than the first four variables . through the correlation with the first 4 variables , unimportant variables @xmath164",
    "have a larger marginal utility with @xmath153 than @xmath154 .",
    "nevertheless , the isis works very well and demonstrates once more that it uses adequately the joint covariate information .",
    ".the average running time ( in seconds ) comparison for van - isis and lasso .",
    "[ cols=\"^,^,^,^,^\",options=\"header \" , ]     now we try to provide some understanding to the significance of these selected genes in predicting the survival information in comparison to other genes that are not selected .",
    "we first fitted the cox s proportional hazard model with all these eight genes .",
    "estimated coefficients are given in table [ tablenb ] , estimated baseline survival function is plotted in figure [ fignb ] , and the corresponding log-(partial)likelihood ( [ simlik ] ) is -129.3517 .",
    "the log - likelihood corresponding to the null model without any predictor is -215.4561 .",
    "a @xmath165 test shows the obvious significance of the model with the eight selected genes .",
    "table [ tablenb ] shows that there are two estimated coefficients that are statistically insignificant at @xmath166 ..    next for each one of these eight genes , we remove it , fit cox s proportional hazard model with the other seven genes , and get the corresponding log - likelihood .",
    "the eight log - likelihoods are -137.5785 , -135.1846 , -129.4621 , -142.4066 , -156.4644 , -158.3799 , -141.0432 , and -129.8390 .",
    "their average is -141.2948 , a decrease of log - likelihood by 11.9431 , which is very significant with reduction one gene ( the reduction of the degree of freedom by 1 ) . in comparison to the model with the eight selected genes ,",
    "@xmath165 tests shows significance for all selected genes except a_23_p31816 and hs150167.1 .",
    "this matches the p - values reported in table [ tablenb ] .",
    "finally we randomly select 2 genes out of the genes that are not selected , fit the cox s proportional hazard model with the above eight genes plus these two randomly selected genes , and record the corresponding log - likelihood .",
    "we repeat this process 20 times .",
    "we find that the average of these 20 new log - likelihoods is -128.3933 , an increase of the log - likelihood merely by 0.9584 with two extra variables included .",
    "comparing to the model with the eight selected genes , @xmath165 test shows no significance for the model corresponding to any of the 20 repetitions .",
    "the above experiments show that the selected 8 genes are very important .",
    "deleting one reduces a lot of log - likelihood , while adding two random genes do not increase very much the log - likelihood .",
    "we have developed a variable selection technique for the survival analysis with the dimensionality that can be much larger than sample size .",
    "the focus is on the iterative sure independence screening , which iteratively applies a large - scale screening that filters unimportant variables by using the conditional marginal utility , and a moderate - scale selection by using penalized partial likelihood method , which selects further the unfiltered variables .",
    "the methodological power of the vanilla isis has been demonstrated via carefully designed simulation studies .",
    "it has sure independence screening with very small false selection . comparing with the version of lasso we used , it is much more computationally efficient and far more specific in selection important variables . as a result",
    ", it has much smaller absolute deviation error and mean square error .",
    "oberthuer , a. , berthold , f. , warnat , p. , hero , b. , kahlert , y. , spitz , r. , ernestus , k. , knig , r. , haas , s. , eils , r. , schwab , m. , brors , b. , westermann , f. and fischer , m. ( 2006 ) .",
    "customized oligonucleotide microarray gene expressionbased classification of neuroblastoma patients outperforms current clinical risk stratification . _ journal of clinical oncology _ , * 24 * 50705078 ."
  ],
  "abstract_text": [
    "<S> variable selection in high dimensional space has challenged many contemporary statistical problems from many frontiers of scientific disciplines . </S>",
    "<S> recent technological advances have made it possible to collect a huge amount of covariate information such as microarray , proteomic and snp data via bioimaging technology while observing survival information on patients in clinical studies . </S>",
    "<S> thus , the same challenge applies in survival analysis in order to understand the association between genomics information and clinical information about the survival time . in this work </S>",
    "<S> , we extend the sure screening procedure @xcite to cox s proportional hazards model with an iterative version available . </S>",
    "<S> numerical simulation studies have shown encouraging performance of the proposed method in comparison with other techniques such as lasso . </S>",
    "<S> this demonstrates the utility and versatility of the iterative sure independence screening scheme .    , </S>"
  ]
}