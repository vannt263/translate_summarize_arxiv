{
  "article_text": [
    "the high - dimensional data is widely used in medical research , bioinformatics , econometrics etc .",
    "it has attracted a lot of interest recently .",
    "variable selection is fundamentally important for knowledge discovery with the high - dimensional data and it could greatly enhance the prediction performance of the fitted model .",
    "traditional model selection procedures follow best - subset selection and its step - wise variants . however , best - subset selection is computationally prohibitive when the number of predictors is large , and it is unstable .",
    "thus , the resulting model has poor prediction accuracy . to overcome these drawbacks of subset selection ,",
    "statisticians have recently proposed various penalization methods to perform simultaneous model selection and estimation . in particular , the lasso ( tibshirani@xcite ) and scad ( fan and li @xcite ) are two very popular methods due to their good computational and statistical properties .",
    "efron et al .",
    "@xcite proposed the lars algorithm for computing the entire lasso solution path .",
    "knight and fu @xcite studied the asymptotic properties of the lasso .",
    "fan and li @xcite showed that the scad enjoys the oracle property .",
    "but the oracle property does not hold for lasso . then , zou @xcite proposed the adaptive lasso ( alasso ) by utilizing the adaptively weighted @xmath2 penalty , which has the oracle property .",
    "correlated variables are very important in applications and theory .",
    "so it is interesting and important to estimate coefficients of the correlated variables .",
    "however , the methods mentioned above can not deal with the strongly correlated variables perfectly .",
    "zou and hastie @xcite introduced the elastic net procedure which can deal with the strongly correlated variables effectively .",
    "the essential strongly correlated variables tend to be into the model together for the group effect of the elastic net . furthermore , similar to lasso and ridge estimation",
    ", the elastic net procedure has some excellent properties .",
    "thus , it has already received a considerable amount of attention .",
    "zou and zhang @xcite proved the oracle property of the adaptive elastic net .",
    "chen et al .",
    "@xcite showed that the profiled adaptive elastic net for partially linear models also has the oracle property .",
    "its estimation identifies the right subset model and has the optimal estimation rate .",
    "but little work has been done on the highly correlated variables .",
    "so we will investigate whether the elastic net encourages the group effect in partially linear models in this paper .",
    "the paper is organized as follows . in section 2",
    ", we turn partially linear models into classical linear models by the kernel estimation .",
    "the elastic net procedure for partially linear models is presented in this section as well . in section 3",
    ", we discuss the group effect that is caused by the elastic net penalty for partially linear models .",
    "the simulation results comparing lasso , alasso , ridge and the elastic net are presented in section 4 .",
    "section 5 studies a real date example .",
    "partially linear models are a class of commonly - used semiparametric models , which are flexible enough and well interpretable , since they contains both parametric and nonparametric components .",
    "next , we consider the elastic net procedure for partially linear models and make a further study of its group effect .    consider the following partially linear model , @xmath3 where @xmath4",
    ", @xmath5 is sparse which means that only some components are nonzero , and @xmath6 is an unknown smooth function of the covariate @xmath7 , @xmath8 is random error with expectation 0 and the standard deviation @xmath9 , which is independent of @xmath10 . in this paper",
    ", we only consider univariate @xmath7 .",
    "from , we have @xmath11 then @xmath12    obviously , we can turn the partially linear model into the classical linear model if @xmath13 and @xmath14 are known .",
    "we estimate @xmath15 and @xmath16 by the kernel estimation .",
    "suppose a random sample of @xmath1 individuals is chosen .",
    "let @xmath17 be the design matrix , where @xmath18 , @xmath19 .",
    "similarly , we assume that @xmath20 , @xmath21 , and @xmath22 .",
    "moreover , denote the estimators of @xmath23 and @xmath24 by @xmath25 and @xmath26 , respectively .",
    "then , @xmath27 and @xmath28 where @xmath29 is a kernel function and @xmath30 is the bandwidth .",
    "let @xmath31 and @xmath32 .",
    "then , in matrix notation , can be rewritten as @xmath33 where @xmath34 and @xmath35 .",
    "so is a standard linear model , and we may adopt the procedure developed by zou and hastie @xcite to study variable selection for the partially linear model .    [ 2 - 2 ]  for fixed nonnegative parameters @xmath36 and @xmath37 , the elastic net procedure for the partially linear model is defined as follows : @xmath38 where @xmath39 and @xmath40 .",
    "define @xmath41    according to the definition 2.1 , the elastic net procedure becomes lasso when @xmath42 in . by a appropriate transformation",
    ", the solution of the elastic net procedure can be expressed analogously to the solution form of lasso ( zou and hastie @xcite ) .",
    "thus we can use the least angle regression algorithm ( lars ) ( efron et al.@xcite ) to solve it .",
    "one of the key issues is the choice of the parameters @xmath43 and @xmath30 .",
    "here we fix @xmath37 and choose the optimal values of @xmath36 by cross - validation ( verweij @xcite ) . for the selection of bandwidth , its best value is @xmath44 .",
    "so we find effective bandwidth @xmath30 for @xmath25 and @xmath26 with interpolation technique proposed by ruppert et al.@xcite .",
    "collinearity is a major obstacle in dealing with high - dimensional data .",
    "eliminating collinearity in the determination of the best linear model is a vital subject . in this section ,",
    "we investigate the group effect of the elastic net procedure .",
    "assume that the response @xmath45 is centred and the predictor @xmath46 is standardized .",
    "given the data @xmath47 and parameters @xmath48 , let @xmath49 be the elastic net estimation .",
    "assume that @xmath50 .",
    "define the group effect @xmath51 by @xmath52 then @xmath53 where @xmath54 and @xmath55 given by is the predicted residual .",
    "_ proof : _ since @xmath50 , @xmath56 where @xmath57 is the sign function .",
    "let @xmath58 .",
    "note that @xmath49 satisfies @xmath59 moreover ,",
    "we have l(_1,_2,)&&=-^2+_2 ^ 2+_1_1 + & & = _ i(-_j_j_ij)^2 + _ 2_j_j^2+_1_j|_j| + & & = _ i(_i^2 - 2_j_j_ij+ ( _ j_j_ij)^2)+_2_j_j^2+_1_j|_j| . therefore , [ 1 - 1 ] 2_i_ik+2_i_ik _ j_j(_1,_2)_ij+_1\\{_k(_1,_2 ) } + 2_2_k(_1,_2)=0 , and [ 1 - 2 ]  -2_i_il+2_i_il _ j_j(_1,_2)_ij+_1\\{_l(_1,_2 ) } + 2_2_l(_1,_2)=0 . by ( [ 1 - 1 ] ) and ( [ 1 - 2 ] ) , we have @xmath60 on the other hand , we have @xmath61 and @xmath62    therefore ,    & & |_k(_1,_2)-_l(_1,_2)| + & & = _ i|x_ik - x_il-(-)||_i| + & & _ i(|x_ik - x_il|+)|_i| + & & _ i(|x_ik - x_il|+|x_jk - x_jl|)|_i| , where [ a-1]_i=_i-_j_j(_1,_2)_ij .",
    "let @xmath63 .",
    "then we have  @xmath64    @xmath51 describes the difference between the coefficient paths of predictors @xmath65 and @xmath66 .",
    "@xmath67 means @xmath68 and @xmath69 are highly correlated .",
    "then the theorem 3.1 suggests that the difference between the coefficient paths of predictor @xmath65 and predictor @xmath66 is almost zero .",
    "if @xmath70 , we consider the @xmath71 .",
    "the upper bound in provides quantitative description for the group effect of the elastic net .",
    "it can be seen that the elastic net procedure has the ability to do group selection , but the lasso fails ( efron et al.@xcite ) .",
    "in this section we report a numerical simulation study to compare the elastic net procedure with lasso , alasso , and ridge .",
    "we have known that all the four methods can deal with collinearity problems well . however , the last three methods can only select one of the highly correlated predictors . as the statement in the theorem , all the necessary",
    "highly correlated variables can be selected into the model by the elastic net procedure . in the extreme situation where some variables are exactly identical ,",
    "the last three methods can only select one of the identical variables into the model .",
    "but all the identical variables can be selected into the model by the elastic net procedure .",
    "moreover , it can assign identical coefficients to the identical variables .",
    "we now demonstrate the above argument by the following numerical simulation .",
    "we generated data from the partially linear model : @xmath72 , where @xmath73 , @xmath74 , @xmath75 with @xmath76}$ ] , and @xmath77 .",
    "moreover , we assume that @xmath78 , @xmath79 , where @xmath80 and @xmath81 follow @xmath82 .",
    "the kernel function is k(y)=    , & |y|1 , + 0 , & .",
    "we did the simulations for @xmath83 and repeated 50 times by using the software @xmath84 .",
    "we considered the lasso , alasso , ridge and the elastic net procedure for the variable selection .",
    "we turned alasso and elastic net procedure into lasso and estimate coefficients by lars .",
    "we picked a value for @xmath37 , say @xmath85 .",
    "we chose the optimal values of the parameters @xmath36 by 10-fold cv .",
    "the best value of bandwidth is @xmath44 .",
    "so we found effective bandwidth @xmath30 for @xmath25 and @xmath26 with interpolation technique .",
    "the coefficients estimates are in table 1 .",
    "the mse ( mean squared error ) are in table 2 , where @xmath86    .the mean value of coefficient estimates based on 50 replications . [ cols=\"<,^,^,^,^,^,^,^,^\",options=\"header \" , ]     [ table3 ]    we stopped the lasso after 60 steps , alasso after 30 steps , and the elastic net after 150 steps .",
    "table [ table3 ] compares the elastic net with lasso and alasso .",
    "the elastic net gives the better classification , and it has an internal gene selection facility .",
    "figure [ fig1 ] displays the solution paths and the gene selection results .",
    "we get that the number of genes selected into the model by lasso is 26 at step 29 , while the alasso is 22 at step 25 .",
    "the zero components can be eliminated to the final model by alasso for its oracle property than lasso .",
    "the optimal elastic net model is given at step 60 with 51 selected genes .",
    "note that the size of the sample is 38 , so the lasso and alasso can at most select 38 genes .",
    "in contrast , the elastic net selects more than 38 genes , not limited by the sample size .",
    "the elastic net is particularly useful when the number @xmath0 of predictors is much bigger than the sample size @xmath1 .",
    "neither lasso nor alasso is a very satisfactory variable selection method in the case @xmath87 .",
    "collinearity between variables is a problem we usually encounter in high - dimensional data .",
    "if it can not be handled properly , the accuracy of models we get does not reach the standard required and it will affect the interpretability of the models seriously . in this paper , we have proposed a more effective selection method , elastic net procedure , to eliminate the collinearity and select all the strongly correlated variables .",
    "the elastic net procedure for partially linear models produces a sparse model with good prediction accuracy , while encourages a group effect .",
    "the simulations and empirical results demonstrate the good performance of the elastic net and its superiority over the other methods .",
    "* acknowledgments : *  the authors thank two anonymous referees for very detailed comments and suggestions .",
    "this work was supported by the natural science foundation of china ( no.11361007 ) , the guangxi natural science foundation ( nos.2012gxnsfba053010 and 2014gxnsfca118001 ) and the project for fostering distinguished youth scholars of shandong university of finance and economics .",
    "chen , b. , yu , y. , zou , h. , liang , h. efron , b. , hastie , t. , johnstone , i. , tibshirani , r. fan , j.q .",
    ", li , r. frank , i.e. , friedman , j.h .",
    "golub , t. , slonim , d. , tamayo , p. , huard , c. , et al .",
    "knight , k. , fu , w.j ."
  ],
  "abstract_text": [
    "<S> variable selection plays an important role in the high - dimensional data analysis . </S>",
    "<S> however the high - dimensional data often induces the strongly correlated variables problem . in this paper </S>",
    "<S> , we propose elastic net procedure for partially linear models and prove the group effect of its estimate . by a simulation study </S>",
    "<S> , we show that the strongly correlated variables problem can be better handled by the elastic net procedure than lasso , alasso and ridge . based on an empirical analysis </S>",
    "<S> , we can get that the elastic net procedure is particularly useful when the number of predictors @xmath0 is much bigger than the sample size @xmath1 .    </S>",
    "<S> * keywords : * elastic net , partially linear models , group effect , lasso </S>"
  ]
}