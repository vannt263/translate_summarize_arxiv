{
  "article_text": [
    "optimization is an important subject with many important application , and algorithms for optimization are diverse with a wide range of successful applications @xcite . among these optimization algorithms ,",
    "modern metaheuristics are becoming increasingly popular , leading to a new branch of optimization , called metaheuristic optimization .",
    "most metaheuristic algorithms are nature - inspired @xcite , from simulated annealing @xcite to ant colony optimization @xcite , and from particle swarm optimization @xcite to cuckoo search @xcite .",
    "since the appearance of swarm intelligence algorithms such as pso in the 1990s , more than a dozen new metaheuristic algorithms have been developed and these algorithms have been applied to almost all areas of optimization , design , scheduling and planning , data mining , machine intelligence , and many others .",
    "thousands of research papers and dozens of books have been published @xcite .    despite the rapid development of metaheuristics ,",
    "their mathematical analysis remains partly unsolved , and many open problems need urgent attention .",
    "this difficulty is largely due to the fact the interaction of various components in metaheuristic algorithms are highly nonlinear , complex , and stochastic .",
    "studies have attempted to carry out convergence analysis @xcite , and some important results concerning pso were obtained @xcite . however , for other metaheuristics such as firefly algorithms and ant colony optimization , it remains an active , challenging topic . on the other hand , even we have not proved or can not prove their convergence , we still can compare the performance of various algorithms .",
    "this has indeed formed a majority of current research in algorithm development in the research community of optimization and machine intelligence @xcite .    in combinatorial optimization",
    ", many important developments exist on complexity analysis , run time and convergence analysis @xcite . for continuous optimization , no - free - lunch - theorems do not hold @xcite . as a relatively young field",
    ", many open problems still remain in the field of randomized search heuristics @xcite . in practice , most assume that metaheuristic algorithms tend to be less complex for implementation , and in many cases , problem sizes are not directly linked with the algorithm complexity .",
    "however , metaheuristics can often solve very tough np - hard optimization , while our understanding of the efficiency and convergence of metaheuristics lacks far behind .",
    "apart from the complex interactions among multiple search agents ( making the mathematical analysis intractable ) , another important issue is the various randomization techniques used for modern metaheuristics , from simple randomization such as uniform distribution to random walks , and to more elaborate lvy flights @xcite .",
    "there is no unified approach to analyze these mathematically . in this paper , we intend to review the convergence of two metaheuristic algorithms including simulated annealing and pso , followed by the new convergence analysis of the firefly algorithm . then , we try to formulate a framework for algorithm analysis in terms of markov chain monte carlo .",
    "we also try to analyze the mathematical and statistical foundations for randomization techniques from simple random walks to lvy flights . finally , we will discuss some of important open questions as further research topics .",
    "the formulation and numerical studies of various metaheuristics have been the main focus of most research studies .",
    "many successful applications have demonstrated the efficiency of metaheuristics in various context , either through comparison with other algorithms and/or applications to well - known problems .",
    "in contrast , the mathematical analysis lacks behind , and convergence analysis has been carried out for only a minority few algorithms such as simulated annealing and particle swarm optimization @xcite .",
    "the main approach is often for very simplified systems using dynamical theory and other ad hoc approaches . here in this section ,",
    "we first review the simulated annealing and its convergence , and we move onto the population - based algorithms such as pso .",
    "we then take the recently developed firefly algorithm as a further example to carry out its convergence analysis .",
    "simulated annealing ( sa ) is one of the widely used metaheuristics , and is also one of the most studies in terms of convergence analysis @xcite .",
    "the essence of simulated annealing is a trajectory - based random walk of a single agent , starting from an initial guess @xmath0 .",
    "the next move only depends on the current state or location and the acceptance probability @xmath1 .",
    "this is essentially a markov chain whose transition probability from the current state to the next state is given by p= , where @xmath2 is boltzmann s constant , and @xmath3 is the temperature . here",
    "the energy change @xmath4 can be linked with the change of objective values .",
    "a few studies on the convergence of simulated annealing have paved the way for analysis for all simulated annealing - based algorithms @xcite .",
    "bertsimas and tsitsiklis provided an excellent review of the convergence of sa under various assumptions @xcite . by using the assumptions that sa forms an inhomogeneous markov chain with finite states , they proved a probabilistic convergence function @xmath5 , rather than almost sure convergence , that p , where @xmath6 is the optimal set , and @xmath7 and @xmath8 are positive constants @xcite .",
    "this is for the cooling schedule @xmath9 , where @xmath10 is the iteration counter or pseudo time .",
    "these studies largely used markov chains as the main tool .",
    "we will come back later to a more general framework of markov chain monte carlo ( mcmc ) in this paper @xcite .",
    "particle swarm optimization ( pso ) was developed by kennedy and eberhart in 1995 @xcite , based on the swarm behaviour such as fish and bird schooling in nature . since then",
    ", pso has generated much wider interests , and forms an exciting , ever - expanding research subject , called swarm intelligence .",
    "pso has been applied to almost every area in optimization , computational intelligence , and design / scheduling applications .",
    "the movement of a swarming particle consists of two major components : a stochastic component and a deterministic component .",
    "each particle is attracted toward the position of the current global best @xmath11 and its own best location @xmath12 in history , while at the same time it has a tendency to move randomly .",
    "let @xmath13 and @xmath14 be the position vector and velocity for particle @xmath15 , respectively .",
    "the new velocity and location updating formulas are determined by _ i^t+1= _",
    "i^t + _ 1 [ ^*-_i^t ] + _ 2 [ _ i^*-_i^t ] .",
    "[ pso - speed-100 ] _ i^t+1=_i^t + _ i^t+1 , [ pso - speed-140 ] where @xmath16 and @xmath17 are two random vectors , and each entry taking the values between 0 and 1",
    ". the parameters @xmath18 and @xmath19 are the learning parameters or acceleration constants , which can typically be taken as , say , @xmath20",
    ".    there are at least two dozen pso variants which extend the standard pso algorithm , and the most noticeable improvement is probably to use inertia function @xmath21 so that @xmath22 is replaced by @xmath23 where @xmath24 $ ] @xcite .",
    "this is equivalent to introducing a virtual mass to stabilize the motion of the particles , and thus the algorithm is expected to converge more quickly .",
    "the first convergence analysis of pso was carried out by clerc and kennedy in 2002 @xcite using the theory of dynamical systems .",
    "mathematically , if we ignore the random factors , we can view the system formed by ( [ pso - speed-100 ] ) and ( [ pso - speed-140 ] ) as a dynamical system .",
    "if we focus on a single particle @xmath15 and imagine that there is only one particle in this system , then the global best @xmath11 is the same as its current best @xmath12 . in this case",
    ", we have _ i^t+1 = _ i^t + ( ^*-_i^t ) , = + , and _",
    "i^t + _ i^t+1 . considering the 1d dynamical system for particle swarm optimization , we can replace @xmath11 by a parameter constant @xmath1 so that we can see if or not the particle of interest will converge towards @xmath1 . by setting @xmath25 and using the notations for dynamical systems , we have a simple dynamical system v_t+1=v_t + u_t , u_t+1 = -v_t + ( 1- ) u_t , or y_t+1 = a y_t , a=    (    cccc 1 & & + -1 & & 1-    )    , y_t=    (    ccccv_t + u_t    )    . the general solution of this dynamical system can be written as @xmath26 $ ] .",
    "the system behaviour can be characterized by the eigenvalues @xmath27 of @xmath7 , and we have @xmath28 .",
    "it can be seen clearly that @xmath29 leads to a bifurcation . following a straightforward analysis of this dynamical system , we can have three cases .",
    "for @xmath30 , cyclic and/or quasi - cyclic trajectories exist . in this case , when randomness is gradually reduced , some convergence can be observed . for @xmath31",
    ", non - cyclic behaviour can be expected and the distance from @xmath32 to the center @xmath33 is monotonically increasing with @xmath10 . in a special case @xmath29 , some convergence behaviour can be observed . for detailed analysis",
    ", please refer to clerc and kennedy @xcite .",
    "since @xmath1 is linked with the global best , as the iterations continue , it can be expected that all particles will aggregate towards the the global best .",
    "firefly algorithm ( fa ) was developed by yang @xcite , which was based on the flashing patterns and behaviour of fireflies .",
    "in essence , each firefly will be attracted to brighter ones , while at the same time , it explores and searches for prey randomly . in addition , the brightness of a firefly is determined by the landscape of the objective function .    the movement of a firefly @xmath15 is attracted to another more attractive ( brighter ) firefly @xmath34 is determined by _",
    "i^t+1 = _ i^t + _ 0 e^-r^2_ij ( _ j^t-_i^t ) + _ i^t , [ fa - equ-50 ] where the second term is due to the attraction .",
    "the third term is randomization with @xmath18 being the randomization parameter , and @xmath35 is a vector of random numbers drawn from a gaussian distribution or other distributions . obviously ,",
    "for a given firefly , there are often many more attractive fireflies , then we can either go through all of them via a loop or use the most attractive one . for multiple modal problems , using a loop while moving toward each brighter one is usually more effective , though this will lead to a slight increase of algorithm complexity .",
    "here is @xmath36 $ ] is the attractiveness at @xmath37 , and @xmath38 is the @xmath39-norm or cartesian distance . for other problems such as scheduling ,",
    "any measure that can effectively characterize the quantities of interest in the optimization problem can be used as the ` distance ' @xmath40 .    for most implementations , we can take @xmath41 , @xmath42 and @xmath43 .",
    "it is worth pointing out that ( [ fa - equ-50 ] ) is essentially a random walk biased towards the brighter fireflies .",
    "if @xmath44 , it becomes a simple random walk .",
    "furthermore , the randomization term can easily be extended to other distributions such as lvy flights @xcite .    ) in the firefly algorithm and the transition between from periodic / multiple states to chaos .",
    "[ fap - fig-100 ] , title=\"fig:\",width=216,height=192 ] ) in the firefly algorithm and the transition between from periodic / multiple states to chaos .",
    "[ fap - fig-100 ] , title=\"fig:\",width=240,height=192 ]    we now can carry out the convergence analysis for the firefly algorithm in a framework similar to clerc and kennedy s dynamical analysis . for simplicity , we start from the equation for firefly motion without the randomness term _ i^t+1=_i^t + _ 0 e^-r_ij^2 ( _ j^t - _ i^t ) . if we focus on a single agent , we can replace @xmath45 by the global best @xmath46 found so far , and we have _ i^t+1 = _",
    "i^t + _ 0 e^-r_i^2 ( g-_i^t ) , where the distance @xmath47 can be given by the @xmath39-norm @xmath48 .",
    "in an even simpler 1-d case , we can set @xmath49 , and we have y_t+1 = y_t - _ 0 e^-y_t^2 y_t .",
    "[ fa - dynamics-25 ] we can see that @xmath50 is a scaling parameter which only affects the scales / size of the firefly movement .",
    "in fact , we can let @xmath51 and we have u_t+1=u_t [ 1-_0 e^-u_t^2 ] .",
    "[ fa - dynamics-150 ] these equations can be analyzed easily using the same methodology for studying the well - known logistic map u_t+1=u_t ( 1-u_t ) .",
    "[ fa - chaos-55 ] the chaotic map is shown in fig .",
    "[ fap - fig-100 ] , and the focus on the transition from periodic multiple states to chaotic behaviour is shown in the same figure .    as we can see from fig .",
    "[ fap - fig-100 ] that convergence can be achieved for @xmath52 .",
    "there is a transition from periodic to chaos at @xmath53 .",
    "this may be surprising , as the aim of designing a metaheuristic algorithm is to try to find the optimal solution efficiently and accurately .",
    "however , chaotic behaviour is not necessarily a nuisance ; in fact , we can use it to the advantage of the firefly algorithm .",
    "simple chaotic characteristics from ( [ fa - chaos-55 ] ) can often be used as an efficient mixing technique for generating diverse solutions .",
    "statistically , the logistic mapping ( [ fa - chaos-55 ] ) with @xmath54 for the initial states in ( 0,1 ) corresponds a beta distribution b(u , p , q)= u^p-1 ( 1-u)^q-1 , when @xmath55 . here",
    "@xmath56 is the gamma function ( z ) = _",
    "0^ t^z-1 e^-t dt . in the case",
    "when @xmath57 is an integer , we have @xmath58 . in addition , @xmath59 . from the algorithm implementation point of view",
    ", we can use higher attractiveness @xmath60 during the early stage of iterations so that the fireflies can explore , even chaotically , the search space more effectively . as the search continues and convergence approaches , we can reduce the attractiveness @xmath60 gradually , which may increase the overall efficiency of the algorithm .",
    "obviously , more studies are highly needed to confirm this .      from the above convergence analysis ,",
    "we know that there is no mathematical framework in general to provide insights into the working mechanisms , the stability and convergence of a give algorithm . despite the increasing popularity of metaheuristics ,",
    "mathematical analysis remains fragmental , and many open problems need urgent attention .",
    "monte carlo methods have been applied in many applications @xcite , including almost all areas of sciences and engineering .",
    "for example , monte carlo methods are widely used in uncertainty and sensitivity analysis @xcite . from the statistical point of view",
    ", most metaheuristic algorithms can be viewed in the framework of markov chains @xcite .",
    "for example , simulated annealing @xcite is a markov chain , as the next state or new solution in sa only depends on the current state / solution and the transition probability . for a given markov chain with certain ergodicity ,",
    "a stability probability distribution and convergence can be achieved .",
    "now if look at the pso closely using the framework of markov chain monte carlo @xcite , each particle in pso essentially forms a markov chain , though this markov chain is biased towards to the current best , as the transition probability often leads to the acceptance of the move towards the current global best .",
    "other population - based algorithms can also be viewed in this framework .",
    "in essence , all metaheuristic algorithms with piecewise , interacting paths can be analyzed in the general framework of markov chain monte carlo .",
    "the main challenge is to realize this and to use the appropriate markov chain theory to study metaheuristic algorithms .",
    "more fruitful studies will surely emerge in the future .",
    "metaheuristics can be considered as an efficient way to produce acceptable solutions by trial and error to a complex problem in a reasonably practical time .",
    "the complexity of the problem of interest makes it impossible to search every possible solution or combination , the aim is to find good feasible solutions in an acceptable timescale .",
    "there is no guarantee that the best solutions can be found , and we even do not know whether an algorithm will work and why if it does work .",
    "the idea is to have an efficient but practical algorithm that will work most the time and is able to produce good quality solutions . among the found quality solutions",
    ", it is expected some of them are nearly optimal , though there is no guarantee for such optimality .",
    "the main components of any metaheuristic algorithms are : intensification and diversification , or exploitation and exploration @xcite .",
    "diversification means to generate diverse solutions so as to explore the search space on the global scale , while intensification means to focus on the search in a local region by exploiting the information that a current good solution is found in this region .",
    "this is in combination with with the selection of the best solutions .    as discussed earlier ,",
    "an important component in swarm intelligence and modern metaheuristics is randomization , which enables an algorithm to have the ability to jump out of any local optimum so as to search globally .",
    "randomization can also be used for local search around the current best if steps are limited to a local region .",
    "fine - tuning the randomness and balance of local search and global search is crucially important in controlling the performance of any metaheuristic algorithm .",
    "randomization techniques can be a very simple method using uniform distributions , or more complex methods as those used in monte carlo simulations @xcite .",
    "they can also be more elaborate , from brownian random walks to lvy flights .",
    "a random walk is a random process which consists of taking a series of consecutive random steps .",
    "mathematically speaking , let @xmath61 denotes the sum of each consecutive random step @xmath62 , then @xmath61 forms a random walk u_n=_i=1^n s_i = s_1 + ... + s_n = u_n-1 + s_n , [ walk - markov-50 ] where @xmath62 is a random step drawn from a random distribution .",
    "this suggests that the next state @xmath61 will only depend the current existing state @xmath63 and the motion or transition @xmath61 from the existing state to the next state . in theory , as the number of steps @xmath64 increases",
    ", the central limit theorem implies that the random walk ( [ walk - markov-50 ] ) should approaches a gaussian distribution .",
    "in addition , there is no reason why each step length should be fixed .",
    "in fact , the step size can also vary according to a known distribution .",
    "if the step length obeys the gaussian distribution , the random walk becomes the standard brownian motion @xcite .    from metaheuristic point of view",
    ", all paths of search agents form a random walk , including a particle s trajectory in simulated annealing , a zig - zag path of a particle in pso , or the piecewise path of a firefly in fa .",
    "the only difference is that transition probabilities are different , and change with time and locations .    under simplest assumptions ,",
    "we know that a gaussian distribution is stable . for a particle starts with an initial location @xmath0 , its final location @xmath65 after @xmath64 time steps is _",
    "n = _ 0 + _",
    "i=1^n _ i s_i , where @xmath66 is a parameters controlling the step sizes or scalings .",
    "if @xmath62 is drawn from a normal distribution @xmath67 , then the conditions of stable distributions lead to a combined gaussian distribution _",
    "n ~n ( _ * , _ * ^2 ) , _ * = _ i=1^n _ i _ i , _ * ^2=_i=1^n _ i [ _ i^2 + ( _ * -_i)^2 ] .",
    "we can see that that the mean location changes with @xmath64 and the variances increases as @xmath64 increases , this makes it possible to reach any areas in the search space if @xmath64 is large enough",
    ".    a diffusion process can be viewed as a series of brownian motion , and the motion obeys the gaussian distribution . for this reason , standard diffusion",
    "is often referred to as the gaussian diffusion .",
    "as the mean of particle locations is obviously zero if @xmath68 , their variance will increase linearly with @xmath10 .",
    "in general , in the @xmath69-dimensional space , the variance of brownian random walks can be written as ^2(t ) = |v_0|^2 t^2 + ( 2 d d ) t , where @xmath70 is the drift velocity of the system . here",
    "@xmath71 is the effective diffusion coefficient which is related to the step length @xmath72 over a short time interval @xmath73 during each jump .",
    "if the motion at each step is not gaussian , then the diffusion is called non - gaussian diffusion .",
    "if the step length obeys other distribution , we have to deal with more generalized random walks .",
    "a very special case is when the step length obeys the lvy distribution , such a random walk is called lvy flight or lvy walk .      in nature ,",
    "animals search for food in a random or quasi - random manner .",
    "in general , the foraging path of an animal is effectively a random walk because the next move is based on the current location / state and the transition probability to the next location .",
    "which direction it chooses depends implicitly on a probability which can be modelled mathematically @xcite .",
    "for example , various studies have shown that the flight behaviour of many animals and insects has demonstrated the typical characteristics of lvy flights @xcite .",
    "subsequently , such behaviour has been applied to optimization and optimal search , and preliminary results show its promising capability @xcite .    in general ,",
    "lvy distribution is stable , and can be defined in terms of a characteristic function or the following fourier transform f(k)= , 0 < 2 , where @xmath18 is a scale parameter .",
    "the inverse of this integral is not easy , as it does not have nay analytical form , except for a few special cases @xcite . for the case of @xmath74",
    ", we have @xmath75 $ ] , whose inverse fourier transform corresponds to a gaussian distribution .",
    "another special case is @xmath76 , which corresponds to a cauchy distribution    for the general case , the inverse integral l(s ) = _ 0^ ( k s ) dk , can be estimated only when @xmath72 is large .",
    "we have l(s ) , s .",
    "lvy flights are more efficient than brownian random walks in exploring unknown , large - scale search space .",
    "there are many reasons to explain this efficiency , and one of them is due to the fact that the variance of lvy flights takes the following form ^2(t ) ~t^3- , 1 2 , which increases much faster than the linear relationship ( i.e. , @xmath77 ) of brownian random walks .",
    "studies show that lvy flights can maximize the efficiency of resource searches in uncertain environments .",
    "in fact , lvy flights have been observed among foraging patterns of albatrosses and fruit flies @xcite .",
    "in addition , lvy flights have many applications . many physical phenomena such as the diffusion of fluorenscent molecules , cooling behavior and noise could show lvy - flight characteristics under the right conditions @xcite .",
    "it is no exaggeration to say that metahueristic algorithms have been a great success in solving various tough optimization problems . despite this huge success",
    ", there are many important questions which remain unanswered . we know how these heuristic algorithms work , and we also partly understand why these algorithms work . however , it is difficult to analyze mathematically why these algorithms are so successful , though significant progress has been made in the last few years @xcite .",
    "however , many open problems still remain .    for all population - based metaheuristics ,",
    "multiple search agents form multiple interacting markov chains . at the moment ,",
    "theoretical development in these areas are still at early stage .",
    "therefore , the mathematical analysis concerning of the rate of convergence is very difficult , if not impossible .",
    "apart from the mathematical analysis on a limited few metaheuristics , convergence of all other algorithms has not been proved mathematically , at least up to now .",
    "any mathematical analysis will thus provide important insight into these algorithms .",
    "it will also be valuable for providing new directions for further important modifications on these algorithms or even pointing out innovative ways of developing new algorithms .    for almost all metaheuristics including future new algorithms ,",
    "an important issue to be addresses is to provide a balanced trade - off between local intensification and global diversification @xcite . at present ,",
    "different algorithm uses different techniques and mechanism with various parameters to control this , they are far from optimal .",
    "important questions are : is there any optimal way to achieve this balance ?",
    "if yes , how ? if not , what is the best we can achieve ?",
    "furthermore , it is still only partly understood why different components of heuristics and metaheuristics interact in a coherent and balanced way so that they produce efficient algorithms which converge under the given conditions .",
    "for example , why does a balanced combination of randomization and a deterministic component lead to a much more efficient algorithm ( than a purely deterministic and/or a purely random algorithm ) ?",
    "how to measure or test if a balance is reached ? how to prove that the use of memory can significantly increase the search efficiency of an algorithm ?",
    "under what conditions ?",
    "in addition , from the well - known no - free - lunch theorems @xcite , we know that they have been proved for single objective optimization for finite search domains , but they do not hold for continuous infinite domains @xcite .",
    "in addition , they remain unproved for multiobjective optimization . if they are proved to be true ( or not ) for multiobjective optimization , what are the implications for algorithm development ?",
    "another important question is about the performance comparison . at the moment",
    ", there is no agreed measure for comparing performance of different algorithms , though the absolute objective value and the number of function evaluations are two widely used measures .",
    "however , a formal theoretical analysis is yet to be developed .",
    "nature provides almost unlimited ways for problem - solving .",
    "if we can observe carefully , we are surely inspired to develop more powerful and efficient new generation algorithms .",
    "intelligence is a product of biological evolution in nature .",
    "ultimately some intelligent algorithms ( or systems ) may appear in the future , so that they can evolve and optimally adapt to solve np - hard optimization problems efficiently and intelligently .",
    "finally , a current trend is to use simplified metaheuristic algorithms to deal with complex optimization problems .",
    "possibly , there is a need to develop more complex metaheuristic algorithms which can truly mimic the exact working mechanism of some natural or biological systems , leading to more powerful next - generation , self - regulating , self - evolving , and truly intelligent metaheuristics .",
    "l. steinhofel , a. albrecht and c. k. wong , convergence analysis of simulated annealing - based algorithms solving flow shop scheduling problems , lecture notes in computer science , * 1767 * , 277 - 290 ( 2000 ) ."
  ],
  "abstract_text": [
    "<S> metaheuristic algorithms are becoming an important part of modern optimization . </S>",
    "<S> a wide range of metaheuristic algorithms have emerged over the last two decades , and many metaheuristics such as particle swarm optimization are becoming increasingly popular . despite their popularity , mathematical analysis of these algorithms </S>",
    "<S> lacks behind . </S>",
    "<S> convergence analysis still remains unsolved for the majority of metaheuristic algorithms , while efficiency analysis is equally challenging . in this paper , we intend to provide an overview of convergence and efficiency studies of metaheuristics , and try to provide a framework for analyzing metaheuristics in terms of convergence and efficiency . </S>",
    "<S> this can form a basis for analyzing other algorithms . </S>",
    "<S> we also outline some open questions as further research topics . </S>",
    "<S> + * citation details * : yang , x. s. , ( 2011 ) . </S>",
    "<S> metaheuristic optimization : algorithm analysis and open problems , in : proceedings of 10th international symposium on experimental algorithms ( sea 2011 ) ( eds . </S>",
    "<S> p. m. pardalos and s. rebennack ) , kolimpari , chania , greece , may 5 - 7 ( 2011 ) , lecture notes in computer sciences , vol . </S>",
    "<S> 6630 , pp . </S>",
    "<S> 21 - 32 . </S>"
  ]
}