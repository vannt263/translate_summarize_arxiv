{
  "article_text": [
    "a central difficulty of any numerical propagation scheme is that , although the propagator seeks to model the evolution of an ideal wavefunction @xmath0 , it has access only to the representation of the wavefunction in some chosen basis , @xmath3 .",
    "the error of the representation is given by @xmath12 .",
    "as an alternative to direct exponentiation of the hamiltonian , a propagator may be constructed by minimizing the integral of the error over the time step @xmath13 writing the error as a two term taylor series , @xmath14 and recalling that @xmath15 for the true wavefunction , the second term in equation [ eq : integralequation ] can be written as @xmath16    from equations [ eq : integralequation ] and [ eq : ddeltadt ] , it is apparent that the global error may arise either from imperfectly representing the wavefunction in a particular basis ( terms containing @xmath17 ) or from imperfectly describing the evolution of the wavefunction in that basis ( terms containing @xmath3 ) .",
    "the representation error may be minimized by an appropriate choice of basis ; here the focus is on minimizing the error of propagation .",
    "the quantity @xmath18 found in equation [ eq : ddeltadt ] is the lagrangian density , and its integral over time gives the action accumulated by the wavefunction in a particular interval .",
    "however , unlike the true lagrangian density , here the action is calculated with respect to the representation of the wavefunction , rather than the wavefunction itself .",
    "the distinction is significant . for the true wavefunction ,",
    "minimizing the action is equivalent to setting @xmath19 for all @xmath20 and @xmath7 . for the action minimizing representation of the wavefunction",
    ", @xmath21 is dependent on the choice of spatial and temporal basis functions and is not guaranteed to be zero .    for a finite basis of spatial @xmath22 and temporal @xmath23 basis functions , a time dependent representation of the wavefunction can be written as @xmath24 in this basis , the global error of eq .",
    "[ eq : ddeltadt ] becomes a function of the coefficients @xmath25 and the matrix representations of the quantum mechanical operators .",
    "writing the hamiltonian as the sum of time independent and time dependent operators @xmath26 and defining the matrices @xmath27 @xmath28 @xmath29 @xmath30 @xmath31 the change in action resulting from @xmath32 is given by @xmath33\\epsilon^{*}_{jm}\\ ] ] and the condition to minimize the accumulated action is that either @xmath34 ( for the initial conditions ) or @xmath35\\epsilon^{*}_{jm}=0 \\label{eq : stationaryphase}\\ ] ] for all j , m . in these equations , @xmath36 appearing as a subscript is treated as an index , while @xmath36 multiplying @xmath37 is the square root of negative one .",
    "equation [ eq : stationaryphase ] is the main result of this paper . in order to construct a least action propagator ,",
    "it is necessary only to choose an appropriate temporal basis .",
    "while in principle this analysis applies equally well to any choice of basis , an obvious choice is for @xmath23 to be a set of linearly independent low - order polynomials in t.    lagrange interpolating polynomials provide a convenient set of temporal basis functions . for an evenly spaced grid @xmath38 for @xmath39 ,",
    "the interpolating polynomials are given by @xmath40 this yields a basis set of @xmath41 linearly independent @xmath41-order polynomials in t , with the property that @xmath42 .",
    "one advantage of this choice of basis is that for small propagation times , @xmath25 will have comparable amplitudes for all n , making the associated linear system easier to solve with high accuracy .",
    "having chosen a temporal basis , coefficients @xmath25 which satisfy equation [ eq : stationaryphase ] as well as the initial condition @xmath43 can be found using lagrange multipliers .",
    "if @xmath44 is the action accumulated in the time interval , let @xmath45 , where @xmath46 . the least action coefficients are found by minimizing @xmath47 with the constraint that @xmath48 for all @xmath36 .",
    "this yields a system of linear equations @xmath49+\\lambda_{j}=0 \\label{eq : lalinearsystem}\\ ] ] for all @xmath50 , and @xmath51 for all @xmath36 .",
    "the lagrange multipliers @xmath52 calculated in this procedure are not needed by the propagator and can be discarded after solving the linear system .",
    "the least action propagator derived in the previous section is the unique , variationally optimum propagator for a particular order in time . as such",
    ", it represents a formal improvement over all propagators approximating the wavefunction as a low order polynomial in time  forward and backwards euler , crank nicholson , second order differencing , etc .",
    "however , it is less clear how this formal improvement translates to a practical benefit , or how the least action propagator compares to methods which attempt to diagonalize the hamiltonian in a krylov subspace , such as the popular short iterative lanczos method @xcite .",
    "the lanczos method works by repeatedly multiplying the initial wavefunction by the hamiltonian matrix to create a krylov space of limited dimension in which the matrix exponential @xmath53 can be calculated exactly .",
    "it is considered to be both efficient and quickly converging@xcite .",
    "existing variational propagators have focused on the evolution of the wavefunction in the krylov subspace , yielding convergence properties similar to the lanczos propagator@xcite .",
    "the chebyshev propagator , which also uses repeated multiplication by the hamiltonian matrix to construct a krylov space , converges similarly to the lanczos method@xcite .    in the limit that the krylov space has dimension equal to the full hamiltonian",
    ", the lanczos method is equivalent to diagonalizing the hamiltonian , yielding @xmath54 at all times .",
    "this solution is the global minimum action solution , and can not be improved upon .",
    "however , for most applications , the krylov subspace is chosen to have a much smaller order  typically in the range 1 - 10 .",
    "because the krylov subspace is constructed through repeatedly multiplying an initial wavefunction by the hamiltonian matrix , later krylov basis vectors will tend to some overlap with those eigenvectors of @xmath55 with the largest eigenvalues . for problems with coulomb singularities or fine spatial bases ,",
    "it is not uncommon for these largest eigenvalues to be artifacts of the choice of basis , having no counterpart in the system being described .",
    "however , they may nonetheless serve to limit the stepsize which may be taken with high accuracy .",
    "if an ideal wavefunction ( ie , without reference to a basis ) @xmath0 can be expanded in terms of energy eigenfunctions over a short time interval @xmath56 and the evolution of the wavefunction s representation in the krylov subspace is given by @xmath57 where @xmath58 , then the error is given by @xmath59 and @xmath60    if the krylov subspace is now partitioned into a `` good '' subspace with @xmath61 and a `` bad '' subspace with @xmath62 , the error can be estimated by setting @xmath63 in the good subspace and @xmath64 , where @xmath65 reflects the largest eigenvalues of h , yielding @xmath66 the error of the lanczos method is thus minimized either when the projection into the bad subspace is small or when @xmath67 .",
    "in contrast to the lanczos method , the error of the least action propagator is bounded by the error of the taylor series of the true wavefunction .",
    "thus @xmath68 and the condition for the error to remain small is simply that @xmath69 . for a problem with @xmath70 , the least action propagator offers the possibility of much larger stepsizes at high accuracy than the lanczos propagator .",
    "the two propagators were tested numerically using a 1 dimensional coulomb potential @xmath71 for @xmath20 ranging from 0 to 10 .",
    "the region was separated into 100 finite element regions , with two quadratic finite elements per region .",
    "the wavefunction was restricted to have zero value at both endpoints .",
    "the largest eigenvalue of the resulting hamiltonian matrix was 499 hartree .",
    "the initial wavefunction was chosen to be a gaussian of unit width , centered at x=2 .",
    "the choice of initial wavefunction and potential were made to ensure that the wavefunction would be far from equilibrium and have a strong interaction with the coulomb potential , as for an electron wavepacket scattering from a positive ion .",
    "the accuracy of the lanczos- and least action propagator was calculated by propagating the initial wavefunction a single timestep and comparing the resulting wavefunction with the `` true '' wavefunction found by directly diagonalizing the full hamiltonian .",
    "yielding an error @xmath72 for the least action propagator , the order of propagation is one less than the degree of the polynomial basis functions . for the lanczos propagator",
    ", the order is given by the dimension of the krylov subspace , starting with 0 for the initial wavefunction .",
    "the error as a function of order and stepsize for the two methods is shown in figure [ fig : errvstimestep ] . also shown in the figure",
    "is the error vs time for the popular crank nicholson first order propagator @xcite .",
    "as opposed to the global error which was used in the derivation of the least action propagator , these figures show the point error after a single propagation step .",
    "of crank nicholson ( dashed line ) , lanczos ( dotted lines ) and least action ( solid lines ) propagators , as a function of step size . for large stepsizes ,",
    "the least action propagator is many times more accurate than the lanczos propagator of the same order.,scaledwidth=45.0% ]    these results show that the least action propagator offers the potential for large timesteps to be taken with high accuracy , with the greatest advantage coming from propagation at high order . at first order , the least action propagator gives approximately the same point error as the crank nicholson method , while higher orders rapidly decrease the error for a particular timestep , or alternatively increase the size of the timestep which can be taken for a particular desired error .",
    "as the order increases , the error begins to saturate as different order propagators converge on the same result . that this saturation does not result in zero error",
    "may result from numerical error in the diagonalization of the hamiltonian or the linear solver .    for all orders tested , the least action propagator was many times more accurate than the lanczos propagator for large stepsizes . for very small stepsizes ,",
    "the error of both methods was comparable , with the lanczos method more accurate .",
    "both methods became much more accurate for stepsizes of less than @xmath73 , which is interpreted to mean that the initial wavefunction had some projection onto very high energy eigenstates ; ie , the sample problem did not have @xmath70 .",
    "one weakness of the least action propagators which arises from the choice of polynomial basis functions is that the norm of the propagated wavefunction is not required to be a constant as a function of time .",
    "figure [ fig : normerrvstimestep ] shows the rate of growth / decay of the norm @xmath74 for different orders of propagation as a function of step size . here the largest deviation from zero growth rate is found for the combination of low order and large stepsize .",
    "higher order propagators show growth rates close to zero . for problems requiring repeated use of the propagator over many timesteps ,",
    "it is thus likely that the propagated wavefunction will need to be renormalized periodically . because the norm is still very close to 1 , such renormalization has a minimal effect on the point error shown in figure [ fig : errvstimestep ] .      from these figures ,",
    "it is apparent that the least action propagator works best at high orders , which offer the combination of large time steps , high accuracy and low rates of growth or decay of the norm .",
    "however , high order also increases the size of the linear system which must be solved at each step .",
    "for a basis consisting of @xmath75 spatial and @xmath76 temporal basis functions , the least action propagator requires @xmath77 variables to be solved for . while specific implementations are beyond the scope of this paper , the question of how best to solve this linear system will play a crucial role in applying the least action propagator to real world problems . to this end , a few features of the least action linear system are worth noting .",
    "first among these is the highly separable nature of the least action linear system defined in equations [ eq : lalinearsystem ] and [ eq : lainitialconditions ] . in equation [ eq : lalinearsystem ] , the variation of the action",
    "is given as the sum of three matrices , @xmath37 , @xmath78 , and @xmath79 .",
    "of these , the first two are separated into the product of spatial and temporal matrices .",
    "because of this , these two matrices inherit the sparsity and/or banded structure of the underlying spatial matrices .",
    "the case is similar for the nonseparable @xmath79 : the integral over time and space is nonzero only if the integral over space is nonzero . because of this , basis sets such as finite elements which are chosen for the structure of their hamiltonian and overlap matrices will retain these advantages in the least action equation . for a banded problem such as the 1d finite element problem treated in this paper ,",
    "the bandwidth of the linear system increases linearly with the order of the propagator , giving an overall @xmath80 scaling with the order .    for very large problems which lack such a simple structure ,",
    "it is likely that solution of the least action linear system will require use of an iterative solver , such as those available in the petsc @xcite or trilinos @xcite libraries .",
    "such solvers , require calculating a matrix vector product at every iteration . here",
    "the advantages of the least action equation s separable form are very apparent , particularly in the case of a static hamiltonian .",
    "a single matrix vector product of the linear system defined in equations [ eq : lalinearsystem ] and [ eq : lainitialconditions ] requires 1 ( very expensive ) matrix - vector multiplication by the unseparated matrix @xmath79 , @xmath81 independent ( expensive ) multiplications of the form @xmath82 , where @xmath83 or @xmath55 , followed by @xmath84 ( cheap ) independent multiplications of the form @xmath85 , where @xmath86 or @xmath87 .",
    "thus , although the linear system which must be solved is very large , it is well suited to iterative solution .",
    "the overall scaling of one iteration with respect to propagator order will be limited by the slowest of these steps , which may depend on specifics of the data structure and architecture of the system used .",
    "this paper has addressed the problem of propagating a wavefunction in time by minimizing he accumulated action . for a particular choice of spatial and temporal basis functions ,",
    "the problem is reduced to solution of a ( potentially very large ) system of linear equations .",
    "this linear system inherits the sparsity and/or banded structure of the spatial hamiltonian and overlap matrices , while its separable structure makes it amenable to solution by iterative solvers .",
    "the resulting propagator was shown to have improved convergence relative to the commonly used short iterative lanczos propagator , giving the potential for larger stepsizes at high accuracy .",
    "the derivation of the action as a measure of propagation error is a powerful result which offers many opportunities for the systematic improvement of propagation schemes . by monitoring the spacetime volumes where the most action is accumulated , spatial or temporal bases",
    "could be selectively refined to increase the total accuracy of a propagation step at minimal additional computational effort .",
    "for this reason , the full power of the action minimization principle may be yet to be unlocked ."
  ],
  "abstract_text": [
    "<S> this paper presents a new technique to calculate the evolution of a quantum wavefunction in a chosen spatial basis by minimizing the accumulated action . </S>",
    "<S> introduction of a finite temporal basis reduces the problem to a set of linear equations , while an appropriate choice of temporal basis set offers improved convergence relative to methods based on matrix exponentiation for a class of physically relevant problems .    </S>",
    "<S> calculating the time evolution of a quantum wavefunction is a longstanding problem in computational physics . </S>",
    "<S> the fundamental nature of the problem makes it resistant to simplification . </S>",
    "<S> problems of physical interest , such as the interaction of a molecule with a strong laser field , may lack symmetry or involve time dependent , nonperturbative fields . problems involving multiple dimensions or multiple interacting particles may quickly grow so large as to be unmanageable with all but the largest computational resources @xcite , a problem which is not easily outstripped by increases in computational power . </S>",
    "<S> an ideal propagator , then , must serve two masters  it must treat the physical side of the problem accurately , and the computational side of the problem efficiently .    </S>",
    "<S> the most common approach to the problem does not treat the evolving wavefunction directly . </S>",
    "<S> instead , the wavefunction @xmath0 is expanded in some chosen basis set @xmath1 , where @xmath2 . here </S>",
    "<S> @xmath0 is the true wavefunction and @xmath3 is its representation in the chosen basis . </S>",
    "<S> after this expansion has been made , the propagation scheme may operate only on the wavefunction s representation , rather than the wavefunction itself . within this general framework </S>",
    "<S> , there has been a great profusion of methods for calculating the evolution of the coefficients @xmath4 . </S>",
    "<S> popular methods include crank - nicholson@xcite , second order differencing@xcite , split operator@xcite , short iterative lanczos@xcite , and chebyshev propagation@xcite , as well as many others@xcite .    </S>",
    "<S> as may be inferred from the large number of competing methods , the practical question of which method works best is very difficult to answer , and usually requires problem - specific information . </S>",
    "<S> ironically , the time dependent schrdinger equation ( tdse ) , while challenging to solve , is not difficult to satisfy at a particular time . </S>",
    "<S> all of the above methods satisfy the tdse exactly or approximately at the initial time in the propagation interval . indeed , </S>",
    "<S> entire families of propagators may satisfy the tdse at the initial point : for the class of propagators @xmath5 , @xmath6 , the tdse at time @xmath7 is satisfied to first order for any value of @xmath8 . </S>",
    "<S> @xmath9 yields the forwards euler method , @xmath10 the backwards euler , @xmath11 the crank - nicholson . </S>",
    "<S> although such methods may show sharp differences in suitability to particular problems , the tdse alone gives little guidance . </S>",
    "<S> a full diagonalization of the hamiltonian matrix would satisfy the tdse at all times ; however , such a diagonalization would be prohibitively expensive for a large problem , and would not exist for a problem with a time dependent hamiltonian . </S>",
    "<S> comparison of different propagators has often involved numerical testing on simple problems@xcite or algorithmic scaling arguments@xcite . </S>",
    "<S> this paper addresses the problem of wavefunction propagation from the physical perspective of minimizing the action accumulated over the chosen time interval . minimizing this action </S>",
    "<S> is shown to be equivalent to minimizing the time integrated error of propagation . because the action is calculated over the entire time step rather than at a single point , the constraint that it be minimized is more strict than the tdse , allowing the construction of a unique , variationally optimum propagator for a particular order in time . </S>"
  ]
}