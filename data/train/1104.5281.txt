{
  "article_text": [
    "dimension reduction is a key step for high dimensional data analysis . principal component analysis ( pca )",
    "is probably the most commonly used method for dimension reduction . given @xmath0 observations on @xmath1 variables ,",
    "pca calculates the @xmath2 covariance matrix and solves the eigenvalue decomposition problem for the covariance matrix .",
    "the goal is to choose a smaller set of eigenvectors as a new coordinate system so that the newly transformed variables can retain the most data variation .",
    "this pca approach has been widely applied in many scientific fields for dimension reduction and compact data representation ( jolliffe , 2002 ) , where the collected data are organized in an @xmath3 design matrix with each row representing an observation and each column a variable .    when data are tensor objects , traditional analysis vectorizes each of the tensor objects into a long vector and arranges these vectorized objects in a design matrix form .",
    "subsequent analysis is followed in the usual way .",
    "nevertheless , this approach usually produces a large number of variables , where the available sample size is relatively small , and many existing statistical methods fail to apply . for a typical example , like the olivetti faces data set to be used in an experimental study later , there are 400 images each with @xmath4 pixels .",
    "vectorizing each image leads to a design matrix of size @xmath5 , which the variable dimension @xmath1 largely exceeds the sample size @xmath0 .",
    "one strategy to overcome this difficulty is to take advantage of the natural tensor structure of the data .",
    "singular value decomposition ( svd ) is an example .",
    "given a @xmath6 matrix @xmath7 which can be treated as an order - two tensor , svd can decompose two directional spaces simultaneously : @xmath8 , where @xmath9\\in \\mathfrak{r}^{p\\times p}$ ] and @xmath10\\in \\mathfrak{r}^{q\\times q}$ ] are , respectively , the left and right singular vectors , @xmath11 is a diagonal matrix of size @xmath6 with diagonal elements @xmath12 . the dimension can be reduced when the index @xmath13 is properly truncated",
    ". de lathauwer , de  moor and vandewalle ( 2000a ) then generalized the svd to high - order svd ( hosvd ) for a given @xmath14-order tensor object @xmath15 .",
    "further , they formulated the problem of `` best rank-@xmath16 approximation of higher - order tensors '' in the least - squares sense , and discussed many algorithms to achieve this task ( de lathauwer , de  moor and vandewalle , 2000b ) .",
    "later , yang et al . ( 2004 ) proposed two - dimensional pca ( 2dpca ) for analyzing image data , which are order - two tensors .",
    "an improved two - directional two - dimensional pca ( @xmath17pca ) was developed in zhang and zhou ( 2005 ) , which was shown to perform better than 2dpca through simulation studies .",
    "ye ( 2005 ) formulated the problem of generalized low rank approximation of matrices , which can be treated as a sample extension of the best rank-@xmath18 approximation for order - two tensors in de  lathauwer , de  moor and vandewalle ( 2000b ) .",
    "lu , plataniotis and venetsanopoulos  ( 2008 ) further generalized the work of ye ( 2005 ) and proposed multilinear pca ( mpca ) for tensor objects of arbitrary orders .",
    "there are other tensor decomposition methods for dimension reduction .",
    "for instance , kolda and bader ( 2009 ) provided a general overview of current development of tensor decomposition methods for unsupervised learning , their applications , and available softwares ; li , kim and altman .",
    "( 2010 ) considered the tensor decomposition methods for supervised learning such as regression and classification .",
    "similar to conventional pca , the goal of mpca is to look for low - dimensional multilinear projection for tensor objects that captures the most data variation . back to the example of olivetti faces , one eigenvector in conventional pca creates an image basis element that contains 4095 free parameters .",
    "by contrast , one image basis element in mpca or ( 2d)@xmath19pca , which involves the kronecker product of a column vector and a row vector , contains 126 free parameters . from the viewpoint of the number of parameters required to specify one basis element , mpca is expected to perform better than conventional pca , when the sample size is small to moderate , like this olivetti faces example .",
    "compared to ( 2d)@xmath19pca , mpca has the advantage of capturing more data variation by the chosen image basis , because of its specific criterion .",
    "mpca has been successfully applied in real data analysis and checked by simulations ( ye , 2005 ; lu et al . , 2008 ) . yet , to our best knowledge",
    ", there is neither statistical justification nor asymptotic study for mpca .    in this paper",
    ", we try to establish some relevant properties of order - two mpca from a statistical point of view .",
    "our study is based on the following model : @xmath20 where @xmath21 is the mean parameter of @xmath7 , @xmath22 and @xmath23 with @xmath24 and @xmath25 are non - random basis matrices , @xmath26 is a random coordinate matrix with @xmath27=0 $ ] and a strictly positive definite covariance matrix @xmath28 , where @xmath29 and @xmath30 is the operator that stacks the columns of a tensor into a long vector .",
    "the error term @xmath31 is a random matrix independent of @xmath32 and with @xmath33=0 $ ] and @xmath34 , where @xmath35 . under model  ( [ model ] ) which characterizes the tensor structure of @xmath7",
    ", we justify the validity of mpca .",
    "asymptotic properties of mpca are rigorously developed , including asymptotic distributions for principal components , associated projections and the explained variance .",
    "it is also shown that mpca is asymptotically more efficient than ( 2d)@xmath19pca in estimating the target dimension reduction subspace .",
    "furthermore , a test of dimensionality is developed , based on the derived asymptotic results .",
    "this paper is organized as follows .",
    "section 2 presents some properties of the estimation for the target subspace and a test for its dimensionality .",
    "the relations between mpca and both conventional pca and ( 2d)@xmath19pca are also discussed in this section . in section  3 ,",
    "the asymptotic theory of mpca is developed . in section 4 , the performance of mpca and its comparison with conventional pca is demonstrated by analyzing the olivetti faces data set .",
    "the paper ends with a brief discussion .",
    "technical proofs of main results are deferred to the appendix .",
    "mpca , as a dimension reduction algorithm , is originally designed to search basis matrices @xmath36 and coordinate matrices @xmath37 s that best approximate the observed data @xmath38 as @xmath39 for @xmath40 .",
    "although many simulation studies and real data analyses in literature support the usage of mpca and multilinear tensor decomposition ( ye , 2005 ; lu et al . , 2008 ; kolda and bader , 2009 ; li et al . , 2010 )",
    ", there is no theoretical study from the statistical point of view .",
    "let @xmath41 be the kronecker product .",
    "then , there is an equivalent formula for model  ( [ model ] ) @xmath42 by the fact that @xmath43 . without loss of generality",
    ", we may assume that @xmath44 and @xmath45 are orthogonal matrices , i.e. , @xmath46 and @xmath47 .",
    "model  ( [ model ] ) thus ensures that , without considering the error term @xmath48 , the columns and rows of @xmath49 belong to @xmath50 and @xmath51 , respectively , and @xmath52 belongs to the subspace @xmath53 .",
    "it is then reasonable to estimate @xmath54 for follow - up analysis such as data compression , pattern recognition , regression analysis , etc . in this section ,",
    "we show that , under model ( [ model ] ) , mpca actually attempts to extract a basis pair @xmath36 targeting the subspace @xmath54 .",
    "proposition  [ lrapprox ] below proves the existence of a solution pair @xmath36 .",
    "proposition  [ relationship ] summarizes that the inclusion relation between @xmath55 ( resp . , @xmath56 ) and the target dimension reduction subspace @xmath50 ( resp . , @xmath51 )",
    ", depends on the size comparison between the specified dimensionality @xmath57 ( resp . , @xmath58 ) and @xmath59 ( resp . , @xmath60 ) .",
    "recognizing the important roles of @xmath57 and @xmath58 , we construct a hypothesis test for choosing @xmath57 and @xmath58 .",
    "these works justify the usage of mpca in extracting the relevant basis for subsequent analysis , provided the data has a natural tensor structure .",
    "let @xmath61 be the collected data set which are assumed to be random copies of a random matrix @xmath62 .",
    "mpca aims to extract the basis pair that best approximate @xmath61 while preserving the tensor structure of them .",
    "in particular , for a pre - specified dimensionality @xmath63 , ye ( 2005 ) proposed a criterion to find @xmath64 , and @xmath65 that minimize @xmath66 where @xmath67 is the sample mean matrix , @xmath68 is the frobenius norm of a matrix , and @xmath69 is the collection of all orthogonal matrices @xmath70 of size @xmath71 such that @xmath72 .",
    "note that the objective function ( [ obj ] ) can be expressed as @xmath73 if we replace @xmath74 by @xmath75 in ( [ obj2 ] ) , the minimization problem then becomes the conventional pca . from this viewpoint ,",
    "mpca can be treated as a constrained pca with the tensor constraint @xmath76 , where @xmath77 and @xmath78 .",
    "the following theorem established in ye ( 2005 ) characterizes some useful properties of the solutions of the minimization problem ( [ obj ] ) . in the rest of discussion",
    ", @xmath79 denotes the orthogonal projection matrix onto @xmath80 and @xmath81 .",
    "[ thm.ye](ye , 2005 ) let @xmath82 constitutes a minimizer for ( [ obj ] ) under the dimensionality @xmath63 . then",
    ",    * @xmath83 .",
    "* @xmath84 is the maximizer of @xmath85 .",
    "* @xmath86 consists of the leading @xmath57 eigenvectors of @xmath87 , and @xmath88 consists of the leading @xmath58 eigenvectors of @xmath89 .",
    "similarly , we can define a population version of ( [ obj ] ) : @xmath90 , and the corresponding minimizer should follow theorem [ thm.ye ] such that the minimizer over @xmath77 and @xmath78 , is equivalent to the maximizer of the maximization problem : @xmath91 where @xmath92 .",
    "the following proposition gives the existence of the solution .",
    "[ lrapprox ] for a fixed but arbitrary positive semi - definite matrix @xmath93 of size @xmath94 , solution(s ) to the maximization problem ( [ max_prob ] ) exists .",
    "note that we do not need the model assumption  ( [ model ] ) for proposition  [ lrapprox ] .",
    "also note that proposition  [ lrapprox ] applies to problem ( [ obj ] ) as well by replacing @xmath93 with its sample estimate @xmath95 , the sample covariance matrix of @xmath96 , and by rephrasing the maximization problem into the equivalent minimization problem . with the existence of the maximizer in ( [ max_prob ] ) we can formally define the tensor principal components and the mpca subspace .    [ def ] for a pre - specified dimensionality @xmath63 ,",
    "let @xmath36 be the unique solution to the maximization problem  ( [ max_prob ] ) , where @xmath97 and @xmath98 can be expressed in their columns as @xmath99 $ ] and @xmath100 $ ] .",
    "we call @xmath101 the tensor principal components , and @xmath102 the mpca subspace of dimensionality @xmath63 .    using similar arguments as in theorem  [ thm.ye ]  ( c )",
    ", we have that @xmath97 and @xmath98 consist of the leading @xmath57 and @xmath58 eigenvectors of @xmath103 $ ] and @xmath104 $ ] , respectively . since @xmath105=\\sum_{j=1}^{\\qtilde}e[(x-\\mu ) ( b_jb_j^t ) ( x-\\mu)^t ] = \\sum_{j=1}^{\\qtilde}(b_j\\otimes i_p)^t\\sigma(b_j\\otimes i_p),\\label{def2.3_eq1}\\\\ e[(x-\\mu)^tp_a(x-\\mu)]=\\sum_{i=1}^{\\ptilde}e[(x-\\mu)^t ( a_ia_i^t ) ( x-\\mu)]=\\sum_{i=1}^{\\ptilde } ( a_i\\otimes i_p)^t\\sigma(a_i\\otimes i_p),\\label{def2.3_eq2}\\end{aligned}\\ ] ] equivalently , @xmath36 consist of the leading solutions of the system of stationary equations @xmath106 over @xmath107 and @xmath108 , where the ordering is determined by the corresponding eigenvalues @xmath109 and @xmath110 .    [ rmk.def ] obviously @xmath111 s , @xmath112 s , @xmath113 s and @xmath114 s depend on @xmath93 . besides such dependence",
    ", they also depend on the dimensionality @xmath63 . a more precise notation for them",
    "should be @xmath115 , @xmath116 , @xmath117 and @xmath118 .",
    "however , for notation simplicity , we use @xmath111 , @xmath112 , @xmath113 and @xmath114 , unless we want to emphasize on their dependence on @xmath119 .    from remark  [ rmk.def ] , for any fixed @xmath63 , we could define the sample analogues @xmath120",
    ", @xmath121 s , and @xmath122 s by replacing @xmath93 with the sample covariance matrix @xmath95 . in the rest of the discussion , with pre - specified dimensionality @xmath63",
    ", we denote the solution of ( [ max_prob ] ) by @xmath36 and the population tensor principal components by @xmath74 , and the corresponding sample analogues by @xmath123 and @xmath124 . finding principal components in conventional pca is equivalent to an eigenvalue - problem .",
    "however , there is no explicit solution of @xmath84 for mpca ; therefore , an algorithm was proposed .",
    "the glram algorithm of ye ( 2005 ) to obtain @xmath125 is summarized below .",
    "+ * glram ( ye , 2005 ) : * given a random initial @xmath126 . for @xmath127    * obtain the maximizer @xmath128 .",
    "* obtain the maximizer @xmath129 . * repeat steps  1 - 2 until there is no significant difference between @xmath130 and @xmath131 . output @xmath132 .    for any fixed @xmath133 or @xmath134 ,",
    "the optimization problems in steps  1 and 2 are the usual eigenvalue - problems of sizes @xmath135 and @xmath136 , respectively . hence , @xmath137 and @xmath134 can be easily obtained .",
    "moreover , the algorithm ensures the quantity @xmath130 to be monotonically increasing as @xmath138 increases and , hence , the solution must exist since @xmath139 is bounded above by @xmath140 . because glram can only find a local maximum ( depends on the chosen random initial @xmath141 ) , multiple random initials are suggested by ye ( 2005 ) to ensure the global maximum .",
    "in contrast to this suggestion , we propose to use the leading @xmath142 eigenvectors of @xmath143 as an initial of @xmath141 .",
    "we observe that hierarchical nesting structure may not exist for mpca . precisely ,",
    "if @xmath144 with the corresponding solution pairs @xmath145 and @xmath120 , respectively , there is no guarantee that @xmath146 , nor @xmath147 . in the population level",
    ", however , there certainly exist relationships between the target subspaces and the mpca subspaces prescribed by the optimization problem  ( [ max_prob ] ) .    [ relationship ]",
    "assume model ( [ model ] ) and let @xmath36 be the solution pair to the maximization problem ( [ max_prob ] ) under dimensionality @xmath148 .    * if @xmath149 and @xmath150 , then @xmath151 and @xmath152 .",
    "* if @xmath153 and @xmath154 , then @xmath155 and @xmath152 .",
    "* if @xmath156 and @xmath157 , then @xmath158 and @xmath159 .",
    "* if @xmath153 and @xmath157 , then @xmath155 and @xmath160 .",
    "even though there is no general hierarchical nesting structure for mpca subspaces , proposition  [ relationship ] ensures the existence of a specific nesting structure , which the extracted mpca subspace is a proper subspace of the target subspace if the dimension is under - specified , and contains the target subspace if the dimension is over - specified .",
    "it also implies that mpca indeed searches the true target subspace @xmath54 when @xmath161 is correctly specified . as a result , these arguments provide a justification of using @xmath162 in the sample level for subsequent statistical analysis .",
    "the @xmath17pca is another method to extract basis for tensor objects .",
    "for a given dimensionality @xmath63 , the population @xmath17pca components @xmath164 $ ] and @xmath165 $ ] are defined to be the leading @xmath57 and @xmath58 eigenvectors of @xmath166 $ ] and @xmath167 $ ] with the corresponding eigenvalues @xmath168 and @xmath169 .",
    "the sample analogues , denoted by @xmath170 , @xmath171 , @xmath172 , and @xmath173 are similarly defined to be the leading eigenvectors and eigenvalues of @xmath174 and @xmath175 .",
    "the following proposition states a connection between the @xmath17pca and mpca in the population level .",
    "[ thm.2d ] assume model ( [ model ] ) and that @xmath176 and @xmath177 are simple roots .",
    "* if @xmath178 , then mpca and ( 2d)@xmath19pca share the same leading @xmath179 eigenvectors , i.e. , @xmath180 for @xmath181 .",
    "moreover , @xmath182 for @xmath183 . *",
    "if @xmath184 , then mpca and ( 2d)@xmath19pca share the same leading @xmath185 eigenvectors , i.e. , @xmath186 for @xmath187 .",
    "moreover , @xmath188 for @xmath189 .",
    "when the dimension @xmath63 is adequate , proposition  [ thm.2d ] implies that @xmath17pca and mpca , in the population level , actually target the same subspace @xmath54 under model  ( [ model ] ) . however , there is no guarantee that the extracted bases @xmath190 of @xmath17pca also maximize the sample version of ( [ max_prob ] ) .",
    "though , under the setting of proposition  [ thm.2d ] , @xmath191 $ ] and @xmath166 $ ] have the same leading eigenvectors , we expect an efficiency gain in using @xmath191 $ ] , since it is less noise - contaminated than @xmath166 $ ] . a rigorous proof of efficiency gain is provided in section  3 .    from proposition  [ thm.2d",
    "] , it is suggested to select @xmath63 through @xmath17pca , since the dimension of @xmath44 and @xmath45 are not known before being estimated .",
    "a formal statistical test is provided in section 2.3 .",
    "there is also a connection between mpca and conventional pca . under model ( [ model ] ) , without considering the random noise @xmath48 , @xmath52 belongs to @xmath192 , which is the target subspace of mpca .",
    "observe also that @xmath193 where @xmath194 is the projection matrix onto the complement of @xmath54 .",
    "it should be noted that the matrix @xmath195 is not necessarily a diagonal matrix .",
    "hence , @xmath196 is not the same with the conventional pca components in general .",
    "if we further diagonalize @xmath197 with @xmath198 being a diagonal matrix of size @xmath199 , we have the following factorization : @xmath200 \\left[\\begin{array}{cc}d+\\sigma^2 i_{m_0}&0\\\\ 0&\\sigma^2 i_{m - m_0}\\end{array}\\right ] [ \\gamma,~\\gamma_\\bot]^t,\\end{aligned}\\ ] ] where @xmath201 and @xmath202 is an orthonormal basis for the orthogonal complement of @xmath203 . consequently , the conventional pca uses @xmath201 as coordinate system for a compressed representation for @xmath52 , while the mpca uses @xmath196 . notice that @xmath204 provided that @xmath195 in ( [ sigma_form ] ) is of full rank . in summary , mpca and conventional pca use the same subspace for compressed data representation .",
    "however , mpca requires less parameters ( see the following remark ) to specify the low - dimensional subspace than the conventional approach .",
    "[ correct_dim ] the number of free parameters required for mpca is @xmath205 , which is relatively small in contrast with the number of free parameters required for conventional pca : @xmath206 .",
    "it is the adoption of @xmath207 for the sake of parsimony , which is one of the purposes of using mpca .",
    "the following table gives the numbers of parameters needed to specify an orthonormal basis for a subspace of dimensionality @xmath208 within a space of dimensionality @xmath209 .",
    "we fix @xmath210 and let @xmath60 vary .",
    ".[t0 ] numbers of required free parameters at @xmath210 . [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     in figures  [ test.face]-[recon.conven ] , 40 test images are randomly chosen from the test set to show the visual performance of image reconstructions by these two pca schemes . in mpca ,",
    "28 row eigenvectors and 28 column eigenvectors , both with size 64 , are used to generate 784 basis images , of which the 100 leading ones are shown in figure  [ basis.tensor ] .",
    "we remind the reader that the selection @xmath211 produces an @xmath212 value @xmath213 .",
    "based on theorem  [ asymp.test ] , a one - sided @xmath214 confidence interval for @xmath215 is given by @xmath216 $ ] .",
    "we also show the variability pattern plots ( tu and huang , 2011 ) in figure [ v - plot ] .",
    "these plots present the average variations ( absolute values ) of the eigenvectors for the bootstrap re - sampled data , from those eigenvectors for the original data .",
    "the horizontal and vertical indices refer to the eigenvector indices for re - sampled and original data .",
    "the indices of eigenvectors are sorted by eigenvalues .",
    "the variations are presented by colors from dark blue for perfect matched , to dark red for extremely deviated .",
    "usually , eigenvectors with distinct eigenvalues show deep blue on the diagonal and deep red on the off - diagonal .",
    "eigenvectors with the same multiple root eigenvalue tend to be visualized by a cubic pattern on their correspondence indices .",
    "it can be seen that our choice of @xmath211 does not produce multiple roots , since the bootstrapped variability of the solutions at this selection is quite small .    in conventional pca , @xmath217 eigenvectors ( basis images ) with size 4096",
    "are used , of which the 100 leading ones are shown in figure  [ basis.conven ] . because of using 100 training images with average subtraction , there are at most 99 meaningful eigenvectors in the conventional pca .",
    "the rest are randomly orthogonal eigenvectors with zero eigenvalue from the remaining subspace . in figure",
    "[ basis.conven ] , from top to bottom , we can see the images with clear facial shape to vague ones and a random image on the @xmath218 one . on the other hand",
    ", mpca tends to distribute the image characteristics to more basis elements which may allow for more local modification on the images .    in figure",
    "[ increase_basis ] , one particular image among the 40 test images is chosen to demonstrate the performance of these two methods .",
    "the top row shows the image reconstruction process for mpca when more basis elements are added in , and the bottom shows for conventional pca .",
    "the mean face is put in the first column and the target image in the @xmath219 column as references .",
    "the right - most column shows the absolute values of projection scores on the leading 784 basis elements .",
    "it is clear that the conventional pca concentrates on no more than 99 basis elements while the mpca spreads out to much more basis elements . for mpca",
    ", the image turns its view when @xmath220 basis elements are used ; the pupil turns to left when @xmath221 basis elements are used ; the double eyelid and nostrils show up when @xmath222 basis elements are used ; the facial curves become clear when @xmath223 basis elements are used . while we can observe the reconstruction progress by adding more basis elements for mpca , we do not see much difference after 100 basis elements for conventional pca .",
    "it is clear that mpca performs better than conventional pca in reconstructing the test images from table  [ t1 ] and these figures .",
    "pca is a popular tool to reduce the dimensions for high dimensional data analysis ; mpca could be likely to serve the similar function for higher order tensor data sets . from this work ,",
    "the statistical properties of mpca become clear through the theoretical framework and the performance of mpca is predictable through the asymptotic results .",
    "most importantly , based on these asymptotic results , various hypothesis tests become feasible for subsequent analysis , including pattern recognition or classification . our work , though technically theoretical , may construct a platform to expand the application potentials of mpca .",
    "the advantages of mpca over conventional pca on tensor structure data are evident in the olivetti faces data example .",
    "therein , conventional pca suffers seriously from the large @xmath1 and small @xmath0 problem such that there can be at most @xmath224 meaningful eigenvectors .",
    "this makes it unavoidable that all the data noises are still carried by the chosen principal components .",
    "furthermore , too concentrated information in one component , which may not be good for pattern recognition or classification prediction . on the other hand",
    ", mpca distributes the information to more components which may allow local modification in the process of image reconstruction , with even fewer free parameters .",
    "the key point for the good performance of mpca is the data tensor structure . for practical purposes ,",
    "the robustness of mpca over model variety should be further investigated .",
    "anderson , t. w. ( 1963 ) .",
    "asymptotic theory for principal component analysis .",
    "_ annals of mathematical statistics _ , 34 , 122 - 148 .",
    "de lathauwer , l. , de moor , b. and vandewalle , j. ( 2000a ) . a multilinear singular value decomposition . _ siam j. matrix anal . appl . _ , 21 , 1253 - 1278 .",
    "de lathauwer , l. , de moor , b. and vandewalle , j. ( 2000b ) . on the best rank-1 and rank-@xmath225 approximation of higher - order tensors .",
    "_ siam j. matrix anal .",
    "_ , 21 , 1324 - 1342",
    ".    fine , j. ( 1987 ) . on the validity of the perturbation method in asymptotic theory .",
    "_ statistics _ , 18 , 401 - 414 .",
    "henderson , h. v. and searle , s. r. ( 1979 ) .",
    "vec and vech operators for matrices , with some uses in jacobians and multivariate statistics .",
    "_ canadian j. statistics _ , 7 , 65 - 81 .",
    "jolliffe , i.t .",
    "_ principal component analysis_. springer , new york .",
    "kolda , t.g . and bader , b.w .",
    "tensor decompositions and applications .",
    "_ siam review _ , 51(3 ) , 455 - 500 .",
    "li , b. , kim , m.k . and altman , n. ( 2010 ) . on dimension folding of matrix- or array - valued statistical objects .",
    "_ annals of statistics _ , 38 , 1094 - 1121 .",
    "lu , h. , plataniotis , k. n. and venetsanopoulos , a. n. ( 2008 ) .",
    "mpca : multilinear principal component analysis of tensor objects . _ ieee transactions on neural networks _ , 19 , 18 - 39 .",
    "magnus , j. r. and neudecker , h. ( 1979 ) . the commutation matrix : some properties and applications . _ annals of statistics _ , 7 , 381 - 394 .",
    "sibson , r. ( 1979 ) .",
    "studies in the robustness of multidimensional scaling : perturbational analysis of classical scaling .",
    "_ , 41 , 217 - 229 .",
    "tu , i. p. and huang , h. c. ( 2011 ) .",
    "an estimation on a covariance matrix when multiple roots exist",
    ". _ manuscript_.    tyler , d. e. ( 1981 ) .",
    "asymptotic inference for eigenvectors .",
    "_ annals of statistics _ , 9 , 725 - 736 .",
    "yang , j. , zhang , d. , frangi , a.f . and",
    "yang , j.y .",
    "two - dimensional pca : a new approach to appearance - based face representation and recognition .",
    "_ ieee transactions on pattern analysis and machine intelligence _ , 26 , 131 - 137 .",
    "ye , j. ( 2005 ) . generalized low rank approximations of matrices .",
    "_ machine learning _",
    ", 61 , 167 - 191 .",
    "zhang , d. and zhou , z. h. ( 2005 ) .",
    "@xmath17pca : two - directional two - dimensional pca for efficient face representation and recognition .",
    "_ neurocomputing _ , 69 , 224 - 231 .",
    "in the maximization problem ( [ max_prob ] ) , the objective function is continuous and the feasible region @xmath226 is compact .",
    "( both continuity and compactness are with respect to the topology induced by frobenius norm . )",
    "thus , solution(s ) exists .",
    "\\(a ) let @xmath227 $ ] be a @xmath228 orthonormal matrix .",
    "since @xmath229)$ ] , there exists @xmath230 and @xmath231 such that @xmath232 . as @xmath47 , we have @xmath233 .",
    "observe that @xmath234 where the equality in ( [ eq.8 ] ) holds if and only if @xmath235 , if and only if @xmath236 . thus , if @xmath150 , such an @xmath237 ( with rank @xmath60 ) exists to ensure the equality in ( [ eq.8 ] )",
    ". this implies @xmath238 and , hence , @xmath239 .",
    "similarly , @xmath240 which establishes ( a ) .    to show ( b ) , when @xmath150 , from ( a ) we have @xmath239 and @xmath241a)+\\ptilde\\qtilde\\sigma^2,\\end{aligned}\\ ] ] which is an eigenvalue - problem for the matrix @xmath242 $ ] . by diagonalizing @xmath243=\\gamma_u\\lambda_u\\gamma_u^t$ ] , then",
    ", @xmath242 $ ] has @xmath59 non - zero eigenvalues @xmath244 with the corresponding eigenvectors @xmath245 .",
    "when @xmath246 , the maximizer @xmath97 consists of the first @xmath57 columns of @xmath245 and , hence , @xmath247 .      to show ( d ) , observe that @xmath248 to maximize ( [ prop.3.4.d ] ) over @xmath249 with @xmath153 and @xmath157 ,",
    "the rank of @xmath250 and @xmath251 must be @xmath57 and @xmath58 , respectively , in order to attain the maximal value .",
    "this can happen only if @xmath155 and @xmath252 .",
    "we will only provide a proof for ( a ) , and ( b ) can be obtained in a similar way . if @xmath150 , from proposition  [ relationship ]  ( a ) we have @xmath253 , which further implies that @xmath105&=&e[a_0uu^ta_0^t ] + e[\\varepsilon p_b\\varepsilon^t ] \\label{eig_a},\\\\ e[(x-\\mu)(x-\\mu)^t ] & = & e[a_0uu^ta_0^t]+e[\\varepsilon\\varepsilon^t ] .",
    "\\label{eig_a2}\\end{aligned}\\ ] ] note that @xmath254=\\qtilde\\sigma^2i_p$ ] and @xmath255=q\\sigma^2i_p$ ] .",
    "hence , @xmath166 $ ] and @xmath103 $ ] have the same leading @xmath256 eigenvectors as @xmath242 $ ] has .",
    "moreover , we have @xmath257 and @xmath258 , where @xmath259 is the @xmath260 eigenvalue of @xmath242 $ ] .",
    "hence , @xmath261 for @xmath183 , which completes the proof .",
    "let @xmath262 be the function maps @xmath95 to its tensor principal components under @xmath63 , which gives @xmath263 and @xmath264 .",
    "note that @xmath265 and @xmath266 . from the weak convergence @xmath267 and an application of the delta method",
    ", we have , for @xmath268 , @xmath269 the explicit forms of @xmath270 and elements in @xmath271 are provided in lemma  [ lem.diff ] .    for a given pair @xmath63 with @xmath272 and @xmath273 , we have , from  ( [ def2.3_eq1 ] ) and  ( [ def2.3_eq2 ] ) , that @xmath97 and @xmath98 satisfy the following system of stationary equations @xmath274 where @xmath275 depend on @xmath119 .",
    "the indices @xmath276 in the above system of equations can go beyond @xmath57 and @xmath58 and up to @xmath135 and @xmath136 .",
    "but those @xmath112 and @xmath114 with @xmath277 and @xmath278 will not be included in the solution pair @xmath279 .",
    "note that we have the following identity , which is due to the definition of @xmath280 and the stationary equations : @xmath281    we will use the perturbation method ( sibson , lemma  2.1 , 1979 ; fine , 1987 ) to derive the derivatives @xmath270 , @xmath282 and @xmath283 .",
    "suppose that @xmath93 is perturbed to @xmath284 .",
    "denote the corresponding system of stationary equations with @xmath285 by @xmath286 let their first order expansions be denoted by @xmath287 following the same arguments as in lemma  2.1 of sibson ( 1979 ) and by equating the terms involving @xmath288 in ( [ eq.26 ] ) we have , for @xmath289 , @xmath290 where @xmath291+\\sum_{j=1}^{\\qtilde}(b_j\\otimes i_p)^t\\dot\\sigma(b_j\\otimes i_p ) \\label{d.sig.b}.\\end{aligned}\\ ] ] since @xmath292 , then @xmath293 $ ] must satisfy @xmath294 .",
    "\\(a ) for @xmath295 , the first term of @xmath296 can be expressed as @xmath297a_i&=&\\sum_{j=1}^{\\qtilde}\\left(\\dot b_j^te [ x^tp_ax]b_j+b_j^te[x^tp_ax]\\dot b_j\\right)\\nonumber\\end{aligned}\\ ] ] which vanishes by noting that @xmath114 is an eigenvector of @xmath298 $ ] and @xmath299 .",
    "this concludes that @xmath300 and , hence , @xmath301    \\(b ) assume now @xmath161 . to derive the form of @xmath302",
    ", we are going to show that the first term of @xmath303 is zero and conclude @xmath304 .",
    "this together with ( [ d.a ] ) gives @xmath305\\}^{+}\\right)^t\\nonumber\\\\ & = & \\left\\{a_i\\otimes \\vec(p_{b_0})\\otimes ( \\lambda_ii_p - e[xp_{b_0}x^t])^{+}\\right\\}^t(k_{p , q}\\otimes i_{pq}).\\end{aligned}\\ ] ] as desired , where the second equality follows from proposition  [ relationship ] that @xmath306 when @xmath307 . to complete the proof , first note that proposition  [ relationship ] ensures the existence of a nonsingular matrix @xmath308 such that @xmath309 . from @xmath310 ( remember @xmath311 ) and the independent structure of @xmath32 and @xmath48",
    ", we can represent the first term of @xmath312 as @xmath313&=&e[a_0ub_0^t(\\dot bb^t+b\\dot b^t)b_0u^ta_0^t]+\\sigma^2\\tr(b^t\\dot b+\\dot b^tb)i_p\\\\ & = & e[a_0u\\eta^t(b^t\\dot b+\\dot b^tb)\\eta u^ta_0^t]+\\sigma^2\\tr(b^t\\dot b+\\dot b^tb)i_p.\\end{aligned}\\ ] ] the proof is completed by noting that @xmath314 .",
    "the case of @xmath315 can be established in a similar way .",
    "consider the function @xmath316 with the corresponding differential @xmath317 . from theorem  [ thm.tpc ] ( b ) and delta method",
    ", we have @xmath318-\\left[\\begin{array}{c}\\vec(a)\\\\ \\vec(b ) \\end{array}\\right]\\right)+o(\\frac{1}{\\sqrt{n}})\\\\ & \\stackrel{d}{\\rightarrow}&d_{p_{b\\otimes a}}\\vec(n),\\end{aligned}\\ ] ] where @xmath319 . when @xmath161 , the expression of @xmath320 is obtained by a direct calculation together with lemma  [ lem.diff ] ( b ) and theorem  [ relationship ] ( a ) .",
    "consider the function @xmath321 with the corresponding differential @xmath322 . from theorem  [ thm.tpc ] ( a ) and delta method",
    ", we have @xmath323-\\left[\\begin{array}{c}\\phi(\\ptilde,\\qtilde)\\\\ \\phi(p , q ) \\end{array}\\right]\\right)+o(\\frac{1}{\\sqrt{n}})\\\\ & \\stackrel{d}{\\rightarrow}&d_{f(\\phi(\\ptilde,\\qtilde),\\phi(p , q))}\\left[\\begin{array}{c}d_{\\phi(\\ptilde,\\qtilde)}\\\\ \\vec(i_m)^t \\end{array}\\right ] \\vec(n).\\end{aligned}\\ ] ] a direct calculation gives the expression of the asymptotic variance @xmath324 .",
    "since @xmath161 , we have @xmath328 and @xmath329 from theorem  [ thm.2d ] , and @xmath330 and @xmath306 from theorem  [ relationship ]  ( a ) .",
    "let @xmath331 and @xmath332 be orthogonal bases of @xmath333 and @xmath334 , @xmath335 , @xmath336 and @xmath337 , @xmath338 . also define @xmath339^t$ ] and @xmath340^t$ ] , where @xmath341\\}^{+}$ ] and @xmath342\\}^{+}$ ] . by using these notations and from theorem  [ thm.tpc ]  ( b )",
    ", we have the limiting distribution of mpca @xmath343 where @xmath344 $ ] , @xmath345 $ ] . by lemma",
    "a.1 below , the limiting distribution of @xmath346pca is derived to be @xmath347 where @xmath348 $ ] , @xmath349 , and @xmath350 .",
    "note that @xmath328 and @xmath329 . to complete the proof , by an application of delta method",
    ", it thus suffices to show @xmath351 from ( [ asymp.tensor])-([asymp.2d ] ) we are left to show @xmath352 where @xmath353 under normality of @xmath354 .",
    "we are going to show @xmath355 .",
    "this together with the fact @xmath356 then establishes the desired result .",
    "observe that @xmath357\\nonumber.\\end{aligned}\\ ] ] from model  ( [ model ] ) , @xmath358 , where @xmath359 .",
    "this implies @xmath360 and @xmath361 and , hence , the diagonal elements of the above matrix vanish .",
    "for the off - diagonal elements , the same reasoning can be used to deduce that @xmath362 and @xmath363 , which establishes ( [ asymp.efficiency ] ) .",
    "a direct calculation further gives @xmath364,\\ ] ] which equals a zero matrix if and only if @xmath365 .",
    "* lemma a.1 . *",
    "_ assume model  ( [ model ] ) and assume that the leading eigenvalues @xmath366 and @xmath367 of ( 2d)@xmath19pca are simple roots .",
    "then , the differentials of ( 2d)@xmath19pca components with respect to @xmath93 under @xmath161 are given by _",
    "we only derive the differential of @xmath369 , where the case of @xmath370 is similarly obtained .",
    "remember that ( 2d)@xmath19pca components @xmath369 are leading eigenvectors of @xmath371 $ ] with eigenvalues @xmath372 .",
    "a standard argument ( sibson , 1979 ) then gives @xmath373 where the second equality follows from theorem  [ thm.2d ] with @xmath374 being defined in theorem  [ thm.efficiency ] .",
    "turning to the differential of @xmath375 with respect to @xmath93 .",
    "it is always true that @xmath376 where @xmath332 are defined in the beginning of theorem  [ thm.efficiency ] .",
    "thus , we have @xmath377 from ( [ diff.2d.1])-([diff.2d.2 ] ) and the chain rule , the proof is completed .",
    "the reconstructed images for the test face by adding more basis elements are compared for mpca ( top row ) and conventional pca ( bottom row ) .",
    "both mean face and the target face are put in this figure as references .",
    "plots in the right - most column are projection coefficients ( in absolute value ) onto pca subspace.,height=170 ]"
  ],
  "abstract_text": [
    "<S> principal component analysis ( pca ) is a commonly used tool for dimension reduction in analyzing high dimensional data ; multilinear principal component analysis ( mpca ) has the potential to serve the similar function for analyzing tensor structure data . </S>",
    "<S> mpca and other tensor decomposition methods have been proved effective to reduce the dimensions for both real data analyses and simulation studies ( ye , 2005 ; lu , plataniotis and venetsanopoulos , 2008 ; kolda and bader , 2009 ; li , kim and altman , 2010 ) . in this paper , we investigate mpca s statistical properties and provide explanations for its advantages . </S>",
    "<S> conventional pca , vectorizing the tensor data , may lead to inefficient and unstable prediction due to its extremely large dimensionality . </S>",
    "<S> on the other hand , mpca , trying to preserve the data structure , searches for low - dimensional multilinear projections and decreases the dimensionality efficiently . </S>",
    "<S> the asymptotic theories for order - two mpca , including asymptotic distributions for principal components , associated projections and the explained variance , are developed . </S>",
    "<S> finally , mpca is shown to improve conventional pca on analyzing the olivetti faces data set , by constructing more module oriented basis in reconstructing the test faces .    * keywords and phrases : * asymptotic theory , dimension reduction , image reconstruction , multilinear principal component analysis , principal component analysis , tensor . </S>"
  ]
}