{
  "article_text": [
    "concentration inequalities play a crucial role in statistical learning theory . generally speaking , the inequalities can help us estimate the generalization ability of learning models on the unseen data based on training samples . estimating such an ability of learning models is essentially to measure the deviations between empirical risk and expectation risk @xcite .",
    "it is noticeable that most of important learning theories such as vc dimension and rademacher complexity are closely related to employ concentration inequalities to study such deviations @xcite .",
    "the commonly used concentration inequalities in learning theory are hoeffding @xcite and mcdiarmid s inequalities @xcite . besides being used to analyze the algorithmic stability @xcite , hoeffding and mcdiarmid s inequalities , which are distribution - independent , are the powerful tools in the estimate of vc dimension and rademacher complexity @xcite .",
    "although becoming popular , these two inequalities 1 ) can not deal with unbounded functions , and 2 ) have weaker abilities for functions with a larger constant on a small exceptional set . in the later case ,",
    "the bound given by them will become less tight since the large constant dominates this bound .",
    "if we generalize these inequalities bounds to any distribution , consequently , it will raise a potential risk to loose the estimation of the deviations . to address these issues , kutin et al .",
    "proved two extensions of mcdiarmid s inequality to strongly and weakly difference - bounded functions and used them to study the generalization ability  @xcite .",
    "kontorovich introduced the definition of the subgaussian diameter and proved an extension of mcdiarmid s inequality with the subgaussian diameter @xcite .",
    "recently , combes proposed an extension of mcdiarmid s inequality for functions with bounded differences on a high probability set and the functions behavior outside of this set may be arbitrary @xcite .    however , the strong and weak bounded difference conditions proposed by kutin et al .",
    "are still too restrictive in practice .",
    "the reason is that the approach kontorovich employed requires to define an extra metric on the sample space and to bound the subgaussian diameter @xcite .",
    "meanwhile , the bound discussed by combes may be over - loose , which will be detailed in section  [ sec3 ] .",
    "after exploring the assumptions of hoeffding and mcdiarmid s inequalities , we propose some extensions to these two inequalities with the probabilistic boundedness and bounded differences . to some extent , our results improve the bound in @xcite and the bounds in the original inequalities , and can handle unbounded functions without introducing extra metrics .    * main results . *",
    "the main work of this paper include proving some distribution - dependent extensions of hoeffding and mcdiarmid s inequalities and tightening generalization bounds .",
    "in theorems 1 to 4 , we attain some extensional inequalities with the probabilistic boundedness and bounded differences .",
    "it is worth pointing out that these extensions are distribution - dependent , which means that better estimation of performance ( theorems 5 to 8) can be obtained when they are applied to particular examples in learning theory .    * motivation . * the unbounded functions often occur in the analysis of regression and classification @xcite .",
    "it thus becomes important in studying the concentration estimates of such functions .",
    "since hoeffding and mcdiarmid s inequalities provide bounds that hold for any distribution , we expect that our proposed distribution - dependent bounds will be tighter",
    ".    * outline .",
    "* in section  [ sec2 ] , we set up the basic notations and preliminary definitions which will be used in the later discussion of the paper . in section  [ sec3 ] , we analyze the limitations of hoeffding and mcdiarmid s inequalities , discussing some related work in this research . in section  [ sec4 ] , we show that the conditions in previous published inequalities are too restrictive through two examples .",
    "furthermore , we propose four assumptions about the probabilistic boundedness and bounded differences , and compare these differences with the previous bounded ones . in section  [ sec5 ] , we present the probabilistic extensions of hoeffding and mcdiarmid s inequalities . in section  [ sec6 ]",
    ", we discuss the potential applications of our results in learning theory .",
    "we conclude this paper in section  [ sec7 ] .",
    "in this section , we will introduce some notations and definitions used throughout the paper .",
    "@xmath0 let @xmath1 denote the indicator function .",
    "@xmath0 let @xmath2 be the set of natural numbers , @xmath3 be the set of real numbers .",
    "let @xmath4 .",
    "@xmath0 let @xmath5 be a probability space , that is , @xmath6 alone is called the sample space , @xmath7 is a @xmath8-algebra of sets in @xmath6 , and @xmath9 is a probability measure on @xmath6 .",
    "and @xmath6 has the structure @xmath10 , where @xmath11 and @xmath12 are the input space and output space respectively .",
    "the set @xmath13 denotes an empty set .",
    "@xmath0 let @xmath14 be the set of all measurable functions @xmath15 . and",
    "suppose that @xmath16 be a subset of @xmath14 , that is , @xmath17 , the set @xmath16 is called the hypothesis class .",
    "@xmath0 let the set @xmath18 be a finite set of labeled training samples , and assume that these samples are independent and identically distributed ( i.i.d . )",
    "according to @xmath19 .",
    "@xmath0 let @xmath20 be the loss function , @xmath21 $ ] , and the loss of @xmath22 on a sample point @xmath23 is defined by @xmath24 .",
    "we can see that the function @xmath25 is nonnegative , but not necessarily bounded .",
    "three representative examples often used in machine learning domain are the absolute loss @xmath26 , squared loss @xmath27 and @xmath28 loss @xmath29  @xcite . here",
    "@xmath30 and @xmath31 is a statistical model of conditional densities for @xmath32 .",
    "@xmath0 let the sign @xmath33 denote the growth function for a hypothesis class @xmath16 .",
    "@xmath0 let the sign @xmath34 be rademacher complexity , defined as @xmath35.\\ ] ] here the random variables @xmath36 are rademacher variables .",
    "in learning theory , the goal is to find a function @xmath37 in hypothesis space @xmath16 that best approximates the target function in @xmath14 , i.e. , minimizing the following generalization error @xmath38\\triangleq \\int_{\\omega}q(h , z)d{\\rm{p}}\\ ] ] generally speaking , the distribution @xmath19 in the equation ( [ equ1 ] ) is unknown . rather than minimizing the equation ( [ equ1 ] )",
    ", we generally minimize the training error @xmath39\\triangleq({1}/{n})\\sum_{i=1}^{n}q(h , z_{i})$ ] . by employing some criteria ,",
    "we can also obtain the approximation @xmath40 of the target function in @xmath14 . in this paper",
    ", we are interested in the uniform estimation of @xmath41-{\\rm{e}}_{n}[h]$ ] , and also denote @xmath41 $ ] and @xmath39 $ ] as @xmath42 and @xmath43 , respectively .    _",
    "* definition 1 ( uniformly difference - bounded @xcite ) * let @xmath44 be a function .",
    "we say that @xmath45 is uniformly difference - bounded by @xmath46 , if the following holds : _",
    "_ for any @xmath47 , if @xmath48 differ only in the @xmath49th coordinate , that is , there exists @xmath50 , s.t . @xmath51 and @xmath52",
    ", then we have @xmath53 . _",
    "_ * definition 2 ( strongly difference - bounded @xcite ) * let @xmath44 be a function .",
    "we say that @xmath45 is strongly difference - bounded by @xmath54 , if the following holds : _    _ there exists a badsubset @xmath55 , where @xmath56 .",
    "for any @xmath47 , if @xmath57 differ only in the @xmath49th coordinate and @xmath58 , then @xmath53 ; if @xmath57 differ only in the @xmath49th coordinate , then @xmath59 .",
    "_ * definition 3 ( weakly difference - bounded @xcite ) * let @xmath44 be a function .",
    "we say that @xmath45 is weakly difference - bounded by @xmath54 , if the following holds : _",
    "_ for any @xmath47 , we have @xmath60 where @xmath61 and @xmath62 for @xmath63 . _    _ for any @xmath64 and @xmath65 differing only in the @xmath49th coordinate , moreover , @xmath59 . _    _ * note 1 * equation ( [ equ7 ] ) means that if we construct @xmath66 by replacing the @xmath49th entry of @xmath67 with @xmath68 , then @xmath53 holds for all but a @xmath69 fraction of the choices . _    for the discussion of later sections and self - containness",
    ", here we give the original forms of hoeffding and mcdiarmid s inequalities as follows :    * hoeffding s inequality @xcite * _ let @xmath70 be independent random variables on a probability space @xmath5 , s.t .",
    "@xmath71 , i=1,2,\\ldots , n$ ] . set @xmath72 .",
    "then , for all @xmath73 we have @xmath74 _    * mcdiarmid s inequality @xcite * _ let @xmath70 be independent random variables on a probability space @xmath5 .",
    "then , for all @xmath73 we have @xmath75 where the function @xmath22 is a real - valued function of the sequence @xmath70 , s.t .",
    "@xmath76 , whenever @xmath77 and @xmath78 differ only in the @xmath79th coordinate , @xmath80 , that is , @xmath22 is uniformly difference - bounded . _",
    "although hoeffding and mcdiarmid s inequalities have achieved great success in learning theory , some researchers @xcite pointed out that these inequalities never consider the distribution , i.e. distribution - independent .",
    "so they hold uniformly for any distribution and can not provide generalization bounds for unbounded loss functions .",
    "this limits their extension to some applications .    to avoid these issues",
    ", researchers devoted to study much more general conditions to these concentration inequalities .",
    "specifically , bentkus gave an extension of hoeffding s inequality to unbounded random variables with bounded mathematical expectation @xcite .",
    "kutin et al .",
    "proved two extensions of mcdiarmid s inequality to strongly and weakly difference - bounded functions ( see definitions 2 and 3 in section  [ sec2 ] ) for the study of the generalization error . in kutin et al.s work , they assumed that there existed some constant vectors , e.g. , @xmath81 and @xmath82 , with @xmath83 for all @xmath84 , s.t .",
    "the function @xmath22 has @xmath81 bounded differences on a subset set @xmath85 of @xmath11 and @xmath82 bounded differences on the complement of the set @xmath85 .",
    "kontorovich noted that the strong and weak bounded differences conditions proposed by kutin et al . were still too restrictive in practice and the bounds of the inequalities are uninformative if the constant vector @xmath81 is infinite . in order to relax the difference - bounded conditions , kontorovich introduced the definition of the subgaussian diameter and proved an extension of mcdiarmid s inequality with the subgaussian diameter @xcite .",
    "nevertheless , the approach kontorovich proposed entails an extra metric on the sample space and requires that the subgaussian diameter is bounded .",
    "recently , combes developed the much more general difference - bounded conditions that the function @xmath22 with @xmath82 bounded differences on a high probability set @xmath86 and is arbitrary outside of @xmath85 , which is mainly described ( or controlled ) by a probability @xmath87 @xcite ( this is similar to assumption 3 in section  [ sec4 ] ) .",
    "finally , combes proposed an extension of mcdiarmid s inequality  @xcite : @xmath88 here , @xmath89 . obviously , the above bound in the equation ( [ comb ] ) discussed by combes is over - loose .",
    "the boundedness conditions we discuss in this paper do nt require 1 ) loss functions to be bounded as in @xcite , and 2 ) to define extra metrics as in @xcite .",
    "in contrast , our extensional conditions are classified into two cases :    @xmath0 being similar to the boundedness conditions given by combes ( see assumptions 1 and 3 in section  [ sec4 ] ) .",
    "@xmath0 refining the boundedness and bounded differences ( see assumptions 2 and 4 in section  [ sec4 ] ) .    and we will compare the inequalities ( [ hoe ] ) , ( [ mcd ] ) and ( [ comb ] ) with our results in section  [ sec5 ] .",
    "in this section , we will analyze the conditions of the previous published inequalities , showing that they may be too rigid to be widely used by some examples . at the end of this section",
    ", we will discuss several general assumptions for the boundedness conditions of concentration inequalities .      here",
    ", we will give two examples to show these limitations discussed above .",
    "_ * example 1 * let @xmath90 , set @xmath91\\\\ \\omega,&\\omega\\in(n_0,+\\infty ) \\end{array } \\right.\\ ] ] _    for all @xmath92 , if we set @xmath93 , then we have the identity @xmath94 . by eq .",
    "( [ equ16 ] ) , it follows that , for any @xmath95 $ ] , @xmath96 ; and for any @xmath97 , @xmath98 .",
    "_ * note 2 * in this example , the random variable @xmath99 is unbounded , and thus does not satisfy the condition of hoeffding s inequality .",
    "if the random variable @xmath99 is limited in a certain range ( for example , @xmath100 $ ] ) , however , the condition of hoeffding s inequality will be satisfied .",
    "_    _ * example 2 * let @xmath90 , set @xmath101 here , we set @xmath102 , @xmath103 is a constant , @xmath104 . _    for all @xmath105 , we assume that @xmath106 where @xmath107 presents the cardinality of @xmath108 . by the equation ( [ equ18 ] ) , it follows that for any @xmath109 $ ] , @xmath110 ; for any @xmath111 , @xmath112 .",
    "_ * note 3 * in this example , we know that @xmath22 is not uniformly differences - bounded , rebeling the condition of mcdiarmid s inequality .",
    "but if the sample space is limited in a certain range ( for example , @xmath109 $ ] ) , the condition of mcdiarmid s inequality is satisfied .",
    "in addition , we observe that @xmath22 is neither weakly differences - bounded nor strongly differences - bounded .",
    "_      the aforementioned examples 1 and 2 do not satisfy most of various existing definitions of the boundedness and bounded differences .",
    "a possible reason behind is that these existing definitions either are too restrictive or neglect the practicability of learning theory in some cases . therefore , four assumptions below are proposed to alleviate these issues .    _",
    "* assumption 1 ( @xmath113 bounded ) * let @xmath114 be the independent random variable on a probability @xmath115 , s.t",
    ". @xmath116 .",
    "if this is true , then we say that @xmath114 is @xmath113 bounded by the pair @xmath117 .",
    "_    _ * assumption 2 ( @xmath118 hierarchy - bounded ) * let @xmath114 be the independent random variable on a probability @xmath115 , s.t .",
    "there exists an integer @xmath119 , we have @xmath120 and @xmath121 .",
    "if this is true , then @xmath114 is @xmath118 hierarchy - bounded by the pair @xmath122 . _",
    "_ * assumption 3 ( @xmath87 difference - bounded ) * let @xmath123 be a function , s.t",
    ". for any @xmath124 , @xmath125 , for any @xmath126 and @xmath127 differ only in the @xmath128th coordinate , we have @xmath129 and @xmath130 .",
    "if this is true , then @xmath45 is @xmath87 difference - bounded by @xmath131 . _    _ * assumption 4 ( @xmath132 hierarchy - difference - bounded ) * let @xmath123 be a function , s.t . for any @xmath124 , there exists an integer @xmath133 , @xmath134 , @xmath135 and @xmath136 , for any @xmath137 and @xmath138 differ only in the @xmath128th coordinate , we have @xmath139 and @xmath140 . if this is true , then @xmath45 is @xmath132 hierarchy - difference - bounded by @xmath141 . _    under these assumptions",
    ", we are interested in two questions :    _ * question 1 * where does a random variable or multivariate random function concentrate on ?",
    "is it still concentrate on its expectation ? _    _ * question 2 * how can we bound the concentration of a random variable or multivariate random function ? or how fast is such a concentration ? _    actually , it is not difficult to see from example 2 that it does not always concentrate around its expectation .",
    "for instance , @xmath142 will be close to @xmath143 as @xmath144 tends to @xmath143 . in the next section",
    ", we will present the answers to these two questions through theorems 1 to 4 .    at the end of this section ,",
    "we compare four proposed bounded conditions with the previous three definitions in section  [ sec2 ] . since the sums of random variables can be regard as a special case of a multivariate random function , we will mainly discuss the relationships between the @xmath87 difference - bounded , @xmath132 hierarchy - difference - bounded , uniformly differences - bounded , strongly differences - bounded and weakly differences - bounded conditions .",
    "due to limited space , more strict comparisons of these conditions derived mathematically can be seen in appendix a in supplementary material for saving space .",
    "here we only show the pairwise comparisons as @xmath145 where the sign `` @xmath146 '' means that the item is strictly stronger on the left than on the right .",
    "in this section , we will show several extensions to hoeffding and mcdiarmid s inequalities , discussing their relationship with the questions mentioned above .",
    "essentially , hoeffding s inequality ( [ hoe ] ) in section  [ sec2 ] can be proved by combining the properties of convex functions , taylor expansion , the monotonicity of probability measures , the exponential markov inequality and the independence of random variables .",
    "meanwhile , mcdiarmid s inequality ( [ mcd ] ) in section  [ sec2 ] can be proved through constructing the martingale difference sequences in combination with a similar proof of hoeffding s inequality .",
    "note that the proofs of theorems 1 to 4 mainly depend on conditional mathematical expectation .",
    "we defer the proofs of these theorems to appendix b in supplementary material for saving space .    _ * theorem 1 * let @xmath114 be independent random variables on a probability space @xmath115 . assume that @xmath114 is @xmath147 bounded by the pair @xmath117 , that is , there exists an element @xmath148 , s.t .",
    "for any @xmath149 , i\\in\\mathbb{n}_{n}$ ] . set @xmath72 and @xmath150 .",
    "then , for any @xmath151 , we have @xmath152 _    _ * theorem 2 * let @xmath114 be independent random variables on a probability space @xmath115 . assume that @xmath114 is @xmath153 hierarchy - bounded by the pair @xmath122 , that is , there exists a subset @xmath154 , and @xmath155 is a partition of @xmath156 , s.t .",
    "for any @xmath157 .",
    "set @xmath72 and @xmath158 . then , for any @xmath151 , we have @xmath159 _    _ * theorem 3 * let @xmath114 be independent random variables on a probability space @xmath115 . let the function @xmath22 be a map from @xmath160 to @xmath3 .",
    "assume that @xmath22 is @xmath161 difference - bounded by @xmath162 , that is , there exists an element @xmath163 , and for any @xmath164 .",
    "@xmath165 , s.t .",
    "@xmath22 satisfies the inequality @xmath166 .",
    "set @xmath150 .",
    "then , for any @xmath151 , we have @xmath167 _    _ * theorem 4 * let @xmath114 be independent random variables on a probability space @xmath115 .",
    "let the function @xmath22 be a map from @xmath160 to @xmath3 .",
    "assume that @xmath22 is @xmath153 hierarchy - difference - bounded by @xmath168 , that is , there exists a subset @xmath169 , @xmath155 is a partition of @xmath6 .",
    "for any @xmath170 , @xmath171 , and @xmath172 , s.t .",
    "@xmath22 satisfies the inequality @xmath173 .",
    "set @xmath158 .",
    "then , for any @xmath151 , we have @xmath174 _    _ * note 4 * from theorems 1 to 4 , we can conclude that : _    _",
    "@xmath0 under the assumptions proposed by us , the random variable or multivariate random function does nt concentrate around its mathematical expectation .",
    "theorems 2 and 4 imply that the random variable or multivariate random function should concentrate around its conditional expectation . and to some extent",
    ", theorems 1 and 3 also imply such concentration in some cases : 1 ) @xmath175 is close to @xmath143 as @xmath144 tends to @xmath143 , or 2 ) @xmath176 .",
    "_    _ @xmath0 the original inequalities ( [ hoe ] ) and ( [ mcd ] ) can be viewed as special cases of the extensions ( [ exten1 ] ) and ( [ exten5 ] ) : if @xmath177 increases up to @xmath6 , then the inequality ( [ exten1 ] ) reduces to the inequality ( [ hoe ] ) in section  [ sec2 ] .",
    "if @xmath177 increases up to @xmath6 , then the inequality ( [ exten5 ] ) reduces to the inequality ( [ mcd ] ) in section  [ sec2 ] . here",
    "@xmath177 equals to @xmath178 , and @xmath6 equals to @xmath179 .",
    "in addition , the bounds of the extensions ( the equalities ( [ exten1 ] ) and ( [ exten5 ] ) ) improve the bounds of the equation ( [ comb ] ) given by combes @xcite : 1 ) the bound of the equation ( [ comb ] ) is trivial if the item @xmath87 in equation ( [ comb ] ) is larger than @xmath180 , whereas the @xmath181 in our extensions has no such limitations , 2 ) the factor of the item ` @xmath182 ' in equation ( [ comb ] ) is always @xmath183 , whereas our factor is a probability @xmath181 . _    _ @xmath0 the bounds of the extensions ( the equalities ( [ exten3 ] ) and ( [ exten7 ] ) ) are tighter than the equalities ( [ hoe ] ) and ( [ mcd ] ) : let @xmath184 where @xmath185",
    ". then we have @xmath186 _    _ according to the definitions of conditional expectation , @xmath187 implies @xmath188 . by substituting the inequality ( [ modifyadd1 ] ) into the inequality ( [ exten3 ] ) , we thus get @xmath189 the inequality ( [ add1 ] ) means that , if we take @xmath190 and @xmath191 in theorem 2 , then the inequality ( [ exten3 ] ) will reduce to the inequality ( [ hoe ] ) in section  [ sec2 ] .",
    "thus , it can be concluded that if the bound of the random variables is refined on a sample space @xmath6 , then a tighter bound by theorem 2 will be attained . _    _ similarly , if we take @xmath192 in theorem 4 , then the inequality ( [ exten7 ] ) will reduce to the inequality ( [ mcd ] ) in section  [ sec2 ] , leading to a conclusion that if the bounded differences of the function @xmath22 is refined on a sample space @xmath6 , then a tighter bound by theorem 4 will be reached . _",
    "in last section , we propose several extensions and compare them with the previous published bounds . now",
    ", we will discuss the applications of these extensions in learning theory .",
    "the notations used here can be seen in section  [ sec2 ] , and we refer the reader to @xcite for the background and much more detailed notations of vc dimension , rademacher complexity and supervised learning .",
    "the discussion of generalization bounds in learning theory are mostly under the boundedness conditions of loss functions , since there are lots of handy tools such as hoeffding and mcdiarmid s inequalities that are for bounded functions @xcite are available .",
    "however , it is no such a limitation of boundedness conditions of loss functions in practical environment .",
    "suppose that the loss function satisfies the @xmath113 bounded or the @xmath113 difference - bounded condition , then we apply our results in section  [ sec5 ] to vc dimension ( theorem 5 ) and rademacher complexity ( theorem 6 ) and estimate their generalization bounds .",
    "such conditions are often unrelated to the natural inherent characteristics of the problems discussed . here",
    ", we suppose that the loss function satisfies the @xmath113 bounded or the @xmath113 difference - bounded condition , then we apply our results in section  [ sec5 ] to vc dimension and rademacher complexity and analyze their generalization bounds .    _",
    "* theorem 5 * let @xmath16 be a family of functions . for each @xmath193 , assume that @xmath37 is @xmath161 bounded by the pair @xmath194 .",
    "then , for any @xmath195 , with probability at least @xmath196 , for any @xmath193 , @xmath197 _    _ * theorem 6 * let @xmath198 be a family of functions . for each @xmath199 , assume that @xmath45 is @xmath161 difference - bounded by @xmath183 .",
    "then , for any @xmath195 , with probability at least @xmath196 , the following inequality holds for all @xmath199 , @xmath200 _      note that the classical tools such as hoeffding and mcdiarmid s inequalities used to study generalization bounds are distribution - independent .",
    "we expect to achieve some tighter generalization bounds when we adopt our distribution - dependent inequalities in section  [ sec4 ] .",
    "suppose that the loss function satisfies the @xmath113 hierarchy - bounded or the @xmath113 hierarchy - difference - bounded condition , it is not difficult to employ our results in section  [ sec5 ] to study generalization bounds via vc dimension ( theorem 7 ) and rademacher complexity ( theorem 8) .    _ * theorem 7 * let @xmath16 be a family of functions . for each @xmath193",
    ", assume that @xmath37 is @xmath153 hierarchy - bounded by the pair @xmath122 . then , for any @xmath195 , with probability at least @xmath196 , for any @xmath193 , @xmath201 _    _ * theorem 8 * let @xmath198 be a family of functions . for each @xmath199",
    ", assume that @xmath45 is @xmath202 hierarchy - bounded by the pair @xmath168 . then , for any @xmath195 , with probability at least @xmath196 , the following inequality holds for all @xmath199 , @xmath203 _ here , @xmath204 is a partition of the sample space and @xmath205 .    _",
    "* note 5 * theorems 5 and 6 indicate that our extensions from theorems 1 and 3 can deal with unbounded loss functions .",
    "for example , theorems 7 and 8 imply the much tighter generalization bounds than the original results : _",
    "_ @xmath0 suppose that @xmath190 and @xmath191 .",
    "inserting @xmath206 and @xmath207 into the inequality ( [ vcapplicationineq1 ] ) , and by the definitions of conditional expectation , we have @xmath208 from the above inequality , it is obvious that the generalization bound is tighter by theorem 7 than the original bound obtained on the entire sample space . _    _ @xmath0 suppose that @xmath209 .",
    "plugging @xmath210 into the inequality ( [ vcapplicationineq2 ] ) , and by the definitions of conditional expectation , we have @xmath211 from the above inequality , it is obvious that the generalization bound is better by theorem 8 than the original bound obtained on the entire sample space . _",
    "the proofs and analyses of theorems 5 to 8 are shown in appendix c in supplementary material .",
    "in this paper , we review the conditions and limitations of hoeffding and mcdiarmid s inequalities . we propose four assumptions and compare them with the existing difference - bounded conditions . based on these assumptions , we attain several extensions of hoeffding and mcdiarmid s inequalities . we also discuss the potential applications of our extensional results in learning theory . finally , we achieve generalization bounds for ( difference- ) unbounded loss function , which have tighter generalization bounds than the existing bounds . in the future , we will study how to design effective machine learning algorithms based on the proposed extensions .",
    "this supplementary material will provide more details and detailed proofs related to our proposed generalization bounds .          on the one hand , in assumption 3 , we set @xmath212 , if for any @xmath213 and @xmath214 differ only in the @xmath128th coordinate , we assume that @xmath215 , for any @xmath216 and @xmath217 differ only in the @xmath128th coordinate , and for any @xmath218 , the equality @xmath219 holds",
    ", then we have that the @xmath132 hierarchy - difference - bounded condition does nt hold .",
    "so , the @xmath87 difference - bounded condition is not stronger than the @xmath132 hierarchy - difference - bounded condition .",
    "on the other hand , if the @xmath132 hierarchy - difference - bounded condition holds , then we have that there exits @xmath220 , for any @xmath221 and @xmath222 differ only in the @xmath128th coordinate , we have @xmath223 and @xmath224 .        on the one hand , if the @xmath87 difference - bounded condition is satisfied , then as the discussion in comparison 1 , we set @xmath212 , and we assume that there exists @xmath225 , for any @xmath216 and @xmath217 differ only in the @xmath128th coordinate .",
    "then , for any @xmath218 , the equality @xmath219 holds .",
    "so , the @xmath87 difference - bounded condition is not stronger than the weakly differences - bounded condition .              for any @xmath124 , @xmath226 , s.t . @xmath135 and @xmath136 .",
    "for any @xmath137 and @xmath138 differ only in the @xmath128th coordinate , we have @xmath139 and @xmath140 ; take @xmath227 , for any @xmath228 and @xmath229 differ only in the @xmath128th coordinate , we have @xmath129 .",
    "if we set @xmath230 in definition 2 , then we have the uniformly differences - bounded condition .",
    "therefore , it is obvious that the strongly differences - bounded condition is strictly weaker than the uniformly differences - bounded condition .",
    "now , we will prove theorems 1 to 4 in section 5 .",
    "the proofs here are almost identical with the original proofs of hoeffding and mcdiarmid s inequalities , the major change being the substitution of conditional mathematical expectation for mathematical expectation .",
    "now we have for any @xmath232 @xmath233 = & \\displaystyle \\int_{\\{s_{n}-{{\\rm{e}}(s_{n}|a)}\\geq t\\}\\cap a}\\lambda_{\\omega}{\\rm d}{\\rm p}\\\\[0.4 cm ] = & \\displaystyle \\int_{\\{{\\rm{exp}}\\left({s(s_{n}-{{\\rm{e}}(s_{n}|a)}})\\right)\\geq { \\rm{exp}}\\left({st}\\right)\\}\\cap a}\\lambda_{\\omega}{\\rm d}{\\rm p}\\\\[0.4 cm ] \\leq&\\displaystyle \\int_{\\{{\\rm{exp}}\\left({s(s_{n}-{{\\rm{e}}(s_{n}|a)})\\geq { \\rm{exp}}\\left({st}\\right)}\\right)\\}\\cap a}\\frac{{\\rm{exp}}\\left({s(s_{n}-{{\\rm{e}}(s_{n}|a)})}\\right)}{{\\rm{exp}}\\left({st}\\right)}{\\rm d}{\\rm p}\\\\[0.4 cm ] \\leq&\\displaystyle \\int_{a}\\frac{{\\rm{exp}}\\left({s(s_{n}-{{\\rm{e}}(s_{n}|a)})}\\right)}{{\\rm{exp}}\\left({st}\\right)}{\\rm d}{\\rm p}\\\\ = & \\displaystyle{\\rm{exp}}\\left({-st}\\right)\\displaystyle \\int_{\\prod\\limits_{i=1}^{n}a_{i}}{\\prod\\limits_{i=1}^{n}{\\rm{exp}}\\left({s(x_{i}-{{\\rm{e}}(x_{i}|a)})}\\right)}{\\rm d}{\\rm p}\\\\ = & \\displaystyle{\\rm{exp}}\\left({-st}\\right)\\prod_{i=1}^{n}\\int_{a_i}{{\\rm{exp}}\\left({s(x_{i}-{{\\rm{e}}(x_{i}|a)})}\\right)}{\\rm d}{\\rm p}_i\\\\ \\displaystyle\\leq & \\displaystyle{\\rm{exp}}\\left({-st}\\right)\\prod_{i=1}^{n}\\int_{a_i}\\left\\{{\\frac{b-(x_{i}-{{\\rm{e}}}(x_{i}|a))}{b - a}\\cdot{\\rm{exp}}\\left({s\\cdot a}\\right)+\\frac{x_{i}-{{\\rm{e}}}(x_{i}|a)-a}{b - a}\\cdot{\\rm{exp}}\\left({s\\cdot b}\\right)}\\right\\}{\\rm d}{\\rm p}_i\\\\ \\displaystyle\\leq&\\displaystyle { \\rm{exp}}\\left({-st}\\right)\\prod\\limits_{i=1}^{n}\\int_{a_i}\\left\\{{\\frac{b-(x_{i}-({\\int_{a}x_{i}{{\\rm d}{\\rm p}}})/{{\\rm{p}}(a)})}{b - a}\\cdot{\\rm{exp}}\\left({s\\cdot a}\\right)}\\right.\\\\ & \\displaystyle\\left.+{\\frac{x_{i}-({\\int_{a}x_{i}{{\\rm d}{\\rm p}}})/{{\\rm{p}}(a)}-a}{b - a}\\cdot{\\rm{exp}}\\left({s\\cdot b}\\right)}\\right\\}{\\rm d}{\\rm p}_i\\\\ = & \\displaystyle { \\rm{exp}}\\left({-st}\\right)\\prod_{i=1}^{n}\\int_{a_i}\\left\\{{\\frac{b}{b - a}\\cdot{\\rm{exp}}\\left({s\\cdot a}\\right)+\\frac{-a}{b - a}\\cdot{\\rm{exp}}\\left({s\\cdot b}\\right)}\\right\\}{\\rm d}{\\rm p}_i\\\\ \\displaystyle\\leq & \\displaystyle { \\rm{exp}}\\left({-st}\\right)\\cdot { \\rm{p}}(a)\\cdot \\prod_{i=1}^{n}\\cdot{\\rm{exp}}\\left({\\frac{s^2(b_{i}-a_{i})^2}{8}}\\right ) \\end{array}\\ ] ] where the minimizing value of @xmath234 is given by @xmath235",
    ". it the minimum value is @xmath236 .",
    "then , we have @xmath237 the proof of the other side @xmath238 follows in a similar manner .",
    "so , we have @xmath239 by substituting the inequality ( [ final3 ] ) into the inequality ( [ hold1 ] ) , which completes the proof of theorem 1 .",
    "take @xmath242 , the inequality ( [ hold3 ] ) gets the minimum value @xmath243 .",
    "finally , we have @xmath244 by substituting the inequality ( [ final4 ] ) into the inequality ( [ hold2 ] ) , the proof of theorem 2 is completed .                and @xmath257\\\\ = & \\displaystyle { \\rm{exp}}\\left({-st}\\right)\\cdot{{\\rm{e}}}{{\\rm{e}}}\\left\\{\\left[(\\prod\\limits_{i=1}^{n}{{\\rm{exp}}\\left({s \\widetilde{v}_{i}}\\right)})\\cdot\\lambda_{a}\\right]|\\widetilde{v}_{1},\\widetilde{v}_{2},\\ldots,\\widetilde{v}_{n-1}\\right\\}\\\\ = & \\displaystyle { \\rm{exp}}\\left({-st}\\right)\\cdot{{\\rm{e}}}\\left[(\\prod\\limits_{i=1}^{n-1}{{\\rm{exp}}\\left({s \\widetilde{v}_{i}}\\right)})\\cdot\\lambda_{a}\\right]\\cdot{{\\rm{e}}}\\left\\{\\left[{{\\rm{exp}}\\left({s \\widetilde{v}_{n}}\\right)}\\cdot\\lambda_{a}\\right]|\\widetilde{v}_{1},\\widetilde{v}_{2},\\ldots,\\widetilde{v}_{n-1}\\right\\}\\\\ = & \\displaystyle { \\rm{exp}}\\left({-st}\\right)\\cdot{{\\rm{e}}}\\left[(\\prod\\limits_{i=1}^{n-1}{{\\rm{exp}}\\left({s \\widetilde{v}_{i}}\\right)})\\cdot\\lambda_{a}\\right]\\cdot { { \\rm{e}}}(\\lambda_{a_n}|\\widetilde{v}_{1},\\widetilde{v}_{2},\\ldots,\\widetilde{v}_{n-1})\\cdot { \\rm{exp}}\\left({\\frac{s^2 c_n^2}{8}}\\right)\\\\ = & \\displaystyle { \\rm{exp}}\\left({-st}\\right)\\cdot{{\\rm{e}}}\\left[(\\prod\\limits_{i=1}^{k}{{\\rm{exp}}\\left({s \\widetilde{v}_{i}}\\right)})\\cdot\\lambda_{a}\\right]\\cdot\\prod_{\\ell = k}^{n}{{\\rm{e}}}(\\lambda_{a_l}|\\widetilde{v}_{1},\\widetilde{v}_{2},\\ldots,\\widetilde{v}_{\\ell-1})\\cdot { \\rm{exp}}\\left({\\frac{s^2 c_l^2}{8}}\\right)\\\\ & \\displaystyle ( k\\in\\mathbb{n}_{n})\\\\ = & \\displaystyle { \\rm{exp}}\\left({-st}\\right)\\cdot\\prod_{\\ell=1}^{n } { { \\rm{e}}}(\\lambda_{a_l}|\\widetilde{v}_{1},\\widetilde{v}_{2},\\ldots,\\widetilde{v}_{\\ell-1})\\cdot e^{\\frac{s^2 c_l^2}{8}}\\\\ = & \\displaystyle { \\rm{exp}}\\left({-st}\\right)\\cdot { \\rm{p}}(a)\\cdot\\prod\\limits_{i=1}^{n}{\\rm{exp}}\\left({\\frac{s^2 c_i^2}{8}}\\right)\\\\ \\end{array}\\ ] ]                    * lemma 1 * let @xmath198 be a family of functions mapping .",
    "for each @xmath199 , assume that @xmath45 is @xmath161 difference - bounded by @xmath183 .",
    "then , we have @xmath261 is @xmath161 difference - bounded by @xmath262 .    * lemma 2 * let @xmath198 be a family of functions . for each @xmath199 , assume that @xmath45 is @xmath202 hierarchy - bounded by the pair @xmath168 .",
    "then , we have @xmath263 is @xmath202 hierarchy - difference - bounded by @xmath264"
  ],
  "abstract_text": [
    "<S> we prove several distribution - dependent extensions of hoeffding and mcdiarmid s inequalities with ( difference- ) unbounded and hierarchically ( difference- ) bounded functions . for this purpose , </S>",
    "<S> several assumptions about the probabilistic boundedness and bounded differences are introduced . </S>",
    "<S> our approaches improve the previous concentration inequalities bounds , and achieve tight bounds in some exceptional cases where the original inequalities can not hold . </S>",
    "<S> furthermore , we discuss the potential applications of our extensions in vc dimension and rademacher complexity . then we obtain generalization bounds for ( difference- ) unbounded loss functions and tighten the existing generalization bounds . </S>"
  ]
}