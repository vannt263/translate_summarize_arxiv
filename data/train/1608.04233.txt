{
  "article_text": [
    "symmetric positive definite ( spd ) matrices are often encountered in a variety of areas . in medical imaging , they are commonly used in diffusion tensor magnetic resonance imaging @xcite . in visual recognition , due to the effectiveness of encoding data variations ,",
    "spd matrix - valued data have been shown to provide powerful statistical representations for images and videos .",
    "examples include region covariance matrices for pedestrian detection @xcite , joint covariance descriptors for action recognition @xcite and image set covariance matrices for face recognition @xcite .    as a consequence",
    ", there has been a growing need to carry out effective computations to interpolate , restore , and classify spd matrix representations .",
    "however , the computations on spd matrices often accompany with the challenge of their non - euclidean data structure which underlies a specific riemannian manifold @xcite . applying the euclidean geometry directly to spd matrices",
    "often results in poor performances and undesirable effects , such as the swelling of diffusion tensors in the case of spd matrices @xcite . to address this problem , @xcite introduced riemannian metrics , e.g. , affine - invariant metric @xcite and log - euclidean metric @xcite , to encode the riemannian geometry of spd manifolds properly .    by employing these well - studied riemannian metrics ,",
    "existing spd matrix learning approaches typically extend euclidean algorithms to work on the underlying riemannian manifold by either flattening it via tangent space approximation @xcite or mapping it into a high dimensional reproducing kernel hilbert space @xcite . to more faithfully respect the original riemannian geometry , recently proposed spd matrix discriminant",
    "learning methods @xcite pursue a manifold - to - manifold transformation mapping the original spd manifold to another discriminative spd manifold with the same geometry .",
    "although these spd matrix learning methods have reached some success , most of them adopt linear learning scheme on spd matrices , which however reside on non - linear manifolds rather than vector spaces with usual additive structure .",
    "therefore , this would inevitably lead to sub - optimal solutions for the problem of spd matrix learning .",
    "deep neural networks have greatly surpassed traditional shallow linear learning architectures in many contexts in artificial intelligence and visual recognition .",
    "the power of deep networks stems both from their ability to perform non - linear computations , and from the effectiveness of the gradient - descent training procedure based on backpropagation .",
    "this motivates us to devise a deep neural network architecture for spd matrix non - linear learning .",
    "differing from most existing deep networks ( e.g. , @xcite ) that deal with unstructured matrices ( usually handle their vector forms ) , our designed network architecture takes spd matrices directly as input , and yields new spd matrices as output .",
    "in other words , this new network deeply learns spd matrices on their underlying riemannian manifolds in an end - to - end learning architecture .",
    "accordingly , the key issue of the proposed riemannian network is how to generate valid spd matrices through multiple hierarchical layers in a deep architecture . in the light of the recent works @xcite that advocate geometry - aware spd matrix learning",
    ", we propose a convolution - like layer to transform the input spd matrices by a bilinear mapping strategy with requiring the transformation matrices to be orthogonal and thus reside on compact stiefel manifolds . in summary",
    ", this paper mainly brings the following three innovations :    * a novel riemannian network architecture is introduced to deeply learn more desirable spd matrices in a non - linear learning scheme .",
    "this opens a new direction of spd matrix non - linear learning on riemannian manifolds . *",
    "the proposed network offers a generalization of the traditional neural network paradigm to non - euclidean spd manifolds . in other words ,",
    "we provide an interesting paradigm of incorporating the well - established riemannian structures into deep network architectures for compressing both of the data space and the weight parameter space . * to optimize the weights performing transformations on spd matrices during training the proposed network ,",
    "a riemannian matrix backpropagation is proposed by exploiting a stochastic gradient descent optimization algorithm on stiefel manifolds .",
    "deep neural networks have exhibited their great powers in machine learning problems where the processed data own a euclidean data structure . in many contexts ,",
    "however , one may be faced with data defined over coordinates which adhere to non - euclidean geometries . to tackle manifold - like graph data other than regular euclidean data",
    ", @xcite presented a spectral formulation of convolutional networks by exploiting the notion of generalized ( non shift - invariant ) convolution that depends on the analogy between the classical fourier transform and the laplace - beltrami eigenbasis .",
    "following @xcite , a localized spectral network was proposed in @xcite to non - euclidean domains for the analysis of deformable shapes . in particular , @xcite generalized the windowed fourier transform to manifolds to extract the local behavior of some dense intrinsic descriptor .",
    "this is roughly acted as an analogy to patches in images .",
    "similarly , @xcite proposed a notion of geodesic convolution on non - euclidean domains based on local geodesic system of coordinates to extract local patches on the shape manifold .",
    "this approach used a natural way of generalizing euclidean networks to riemannian manifolds , where convolutions are performed by sliding a window over the manifold , and local geodesic coordinates are used in place of image patches .",
    "stochastic gradient descent ( sgd ) has been the workhorse for optimizing deep neural networks .",
    "one of the most well - known sgd algorithms uses euclidean gradients with a varying learning rate to optimize the weights of a deep network .",
    "backpropagation is an algorithm for efficiently computing the gradient of the loss with respect to the parameters .",
    "in addition to standard backpropagation algorithms working scalar inputs , the two works @xcite also developed backpropagation algorithms directly on matrices .",
    "for example , @xcite formulated matrix backpropagation as a generalized chain rule mechanism for computing derivatives of composed matrix functions with respect to matrix inputs by relying on the calculus of adjoint matrix variations . besides , the other family of network optimization algorithms exploits riemannian gradients to handle weight space symmetries in neural networks .",
    "optimizing over a riemannian manifold has been a topic of much research and provides guide to compute non - euclidean gradients for parameter computations @xcite .",
    "this kind of gradient pursuing methods commonly computes the steepest descent for weight update on a specific type of riemannian manifold .",
    "for instance , recent works @xcite developed several network optimization algorithms by building riemannian metrics on the activity and parameter space of neural networks , treated as riemannian manifolds .",
    "[ fig1 ]    the proposed spd network ( spdnet ) architecture performs deep learning on input spd matrices , and outputs more appropriate spd matrices .",
    "taking the well - known convolutional network ( convnet ) as an analogy , the proposed network also designs convolution - like layers and rectified linear units ( relu)-like layers , named bilinear mapping ( bimap ) layers and eigenvalue rectification ( reeig ) layers respectively .",
    "analogously to the convolution layer in the convnet , the proposed bimap layers perform transformations on input spd matrices to generate new spd matrices by adopting a bilinear mapping scheme . as the classical relu layers ,",
    "the designed reeig layers introduce the non - linearity to the spdnet by rectifying the resulting spd matrices with a non - linear function .",
    "since spd matrices reside on non - euclidean manifolds , directly applying regular euclidean output layers such as softmax layer on them would yield sub - optimal solutions . to address this issue",
    ", we devise eigenvalue logarithm ( logeig ) layers to carry out log - euclidean riemannian computing on the resulting spd matrices . by exploiting the riemannian computation ,",
    "the spd matrices are transformed into usual symmetric matrices , which lie in euclidean space and thus can be fed into any euclidean output layers .",
    "the proposed riemannian network is illustrated in fig.[fig1 ] .",
    "the most primary function of the spdnet is to generate more desirable spd matrices on riemannian manifolds . for this purpose , we design the bimap layer ( a convolution - like fully connected layer ) to transform the input spd matrices into new spd matrices by a bilinear mapping @xmath0 as @xmath1 where @xmath2 is the input spd matrix of the @xmath3-th layer , @xmath4 is the transformation matrix ( connection weights ) , @xmath5 is the resulting matrix . to ensure the matrix @xmath6 becomes a valid spd matrix residing on another spd manifold @xmath7 , the transformation matrix @xmath8 is basically required to be a row full - rank matrix . by applying the bimap layer ,",
    "the input matrices on the original spd manifold @xmath9 are transformed into lower - dimensional spd matrices , which also form a valid spd manifold @xmath7 . in other words ,",
    "the data space on each bimap layer corresponds to one spd manifold .    since the weight space @xmath10 of full - rank matrices on each bimap layer is a non - compact stiefel manifold where the distance function has no upper bound , directly optimizing on the manifold is infeasible . to handle this problem",
    ", we additionally assume the transformation matrix @xmath8 to be orthogonal so that they reside on a compact stiefel manifold @xmath11 .",
    "is the set of @xmath12-dimensional orthonormal matrices of the @xmath13 . ] as a result , optimizing over the compact stiefel manifolds can achieve the optimal solution of the transformation matrices .      in the context of convnets , @xcite have presented various rectified linear units ( relu ) ( including the @xmath14 non - linearity ) to improve discriminative performance .",
    "hence , exploiting relu - like layers to introduce the non - linearity to the domain of spdnet is also necessary .",
    "inspired by the idea of the @xmath14 non - linearity , we devise the reeig layer to rectify the spd matrices by tuning up their small positive eigenvalues .",
    "intuitively , this rectification would prevent the spd matrices from being non - positive as well .",
    "nevertheless , it is not originally designed for regularization because the input matrices for reeig layers are already non - singular after the operation of bimap layers .",
    "accordingly , we formulate the rectification function @xmath15 on a spd matrix in the @xmath3-th layer as @xmath16 where @xmath17 are the eigenvector and eigenvalue matrices achieved by singular value decomposition ( svd ) on the input spd matrix as @xmath18 , @xmath19 is the small rectification threshold , @xmath20 is the diagonal matrix of the max operations defined as @xmath21    besides , there also exist other feasible strategies to derive a rectifying non - linearity on the input spd matrices .",
    "for example , the noisy version of eqn.[eq2 ] can be applied by @xmath22 , where @xmath23 is gaussian noise with zero mean and variance of @xmath24 . due to the space limitation",
    ", we do not discuss this any further in this paper .",
    "the logeig layer is designed to perform riemannian computing on the resulting spd matrices for output layers with objective functions . as studied in @xcite ,",
    "the log - euclidean riemannian metric is able to endow the riemannian manifold of spd matrices with a lie group structure so that the manifold is reduced to a flat space with the matrix logarithm operation @xmath25 on the spd matrices . in the flat",
    ", classical euclidean computations can be applied to the domain of spd matrix logarithms .",
    "formally , we apply the log - euclidean riemannian computation @xcite on a spd matrix in the @xmath3-th layer with the function @xmath26 defined as @xmath27 where @xmath18 , @xmath28 is the diagonal matrix of the eigenvalue logarithms .    for spd manifolds ,",
    "the log - euclidean riemannian computation is particularly simple to use and avoids the high expense of other riemannian computations @xcite , while preserving favorable theoretical properties . as for other riemannian computations on spd manifolds ,",
    "please refer to @xcite for more detailed discussion on their properties .",
    "after applying the logeig layer , the outputs ( i.e. , spd matrix logarithms ) lie in euclidean space and thus can be converted into vector forms .",
    "therefore , on the top of the logeig layer , classical euclidean network layers can be applied .",
    "for example , the euclidean fully connected ( fc ) layer could be built after the logeig layer .",
    "the dimensionality of the filters in the fc layer can be set to @xmath29 , where @xmath12 and @xmath30 are the class number and the dimensionalities of the vector forms of the input symmetric matrices respectively .",
    "the final output layer for visual recognition tasks could be softmax layer or softmax log - loss layer used in the context of euclidean networks .",
    "in addition , the pooling layer and normalization layer are also important to improve euclidean convnets . for the spdnet ,",
    "the pooling on spd matrices can be first carried out on their matrix logarithms , and then transform them back to spd matrices by employing the matrix exponential map @xmath31 in the riemannian framework @xcite .",
    "similarly , the normalization procedure on spd matrices could be first to calculate the mean and variance of their matrix logarithms , and then normalize them with their mean and variance as done in @xcite .",
    "the normalized matrices are finally mapped back by using the operation @xmath31 to the spd manifold to get the normalized spd matrices .",
    "similar to traditional euclidean networks , the model of the proposed spdnet can be written as a series of successive function compositions @xmath32 with parameter tuple @xmath33 , where @xmath34 is the function for the @xmath3-th layer , @xmath8 is the weight parameter of the @xmath3-th layer and @xmath35 is the number of layers .",
    "the loss of the @xmath3-th layer could be denoted by a function as @xmath36 , where @xmath37 is the loss function for the final output layer .",
    "training deep networks often uses stochastic gradient descent ( sgd ) algorithms .",
    "the key operation of one classical sgd algorithm is to compute the gradient of the objective function , which is obtained by an application of the chain rule known as backpropagation ( backprop ) . for the @xmath3-th layer ,",
    "the gradients of the weight @xmath8 and the data @xmath38 can be respectively computed by backprop as @xmath39 where @xmath40 is the desired output , @xmath41 .",
    "eqn.[eq5 ] is the gradient for updating @xmath8 , while eqn.[eq6 ] is necessary for computing the gradients in the layers below to update the involved parameters .",
    "there exist two key issues for generalizing backprop to the context of the proposed riemannian network for spd matrices .",
    "the first one is updating the weights in the bimap layers .",
    "as we force the weights to be on the stiefel manifold , merely using eqn.[eq5 ] to compute their euclidean gradients rather than riemannian gradients in the procedure of backprop can not yield valid orthogonal weights . while the gradients of the spd matrices in the bimap layers can be calculated by eqn.[eq6 ] as usual , computing those with svd operations in the layers of reeig and logeig has not been well - solved by the traditional backprop .",
    "thus , it is the second key issue for training the proposed network .    to tackle the first issue",
    ", we propose a new way of updating the weights occurred in eqn.[eq1 ] for the bimap layers by exploiting a stochastic gradient descent setting on stiefel manifolds .",
    "the steepest descent direction for the corresponding loss function @xmath42 with respect to @xmath8 on the stiefel manifold is the riemannian gradient @xmath43 . to obtain it , the normal component of the euclidean gradient @xmath44",
    "is subtracted to generate the tangential component ( to the stiefel manifold ) .",
    "searching along the tangential direction takes the update in the tangent space of the stiefel manifold .",
    "then , such the update is mapped back to the stiefel manifold with a retraction operation . for more details about the geometry of stiefel manifolds and its retraction operation ,",
    "the readers are referred to the works @xcite and @xcite ( page 45 - 48 , 59 ) .",
    "consequently , an update of the weight @xmath8 on the stiefel manifold is of the following form @xmath45 where @xmath46 is the retraction operation , @xmath47 is the current weight , @xmath48 is the learning rate , @xmath49 is the normal component of the euclidean gradient @xmath44 , which is computed by using eqn.[eq5 ] as @xmath50    as for the second issue , we exploit the matrix generalization of backprop studied in @xcite to compute the gradients of the involved spd matrices in the reeig and logeig layers . in particular , let @xmath51 be a function describing the variations of the upper layer variables with respect to the lower layer variables , i.e. , @xmath52 . by defining the function @xmath51 , a new version of the chain rule eqn.[eq6 ] for the matrix backprop is defined as @xmath53 where @xmath54 is a non - linear adjoint operator of @xmath51 , i.e. , @xmath55 , the matrix inner product @xmath56 .",
    "actually , both of the two functions eqn.[eq2 ] and eqn.[eq4 ] for the reeig and logeig layers involve the svd operation @xmath57 .",
    "hence , we introduce a virtual spectral layer ( @xmath58 layer ) for the svd operation . applying the new chain rule eqn.[eq10 ]",
    ", we can achieve @xmath59 where the two variations @xmath60 and @xmath61 are derived by the variation of the svd operation @xmath62 as : @xmath63 where @xmath64 is the hadamard product , @xmath65 , @xmath66 is @xmath67 with all off - diagonal elements being 0 ( note that we also use these two denotations in the following ) , @xmath68 is calculated by operating on the eigenvalues @xmath69 in @xmath24 : @xmath70    for more details to derive eqn.[eq12 ] and eqn.[eq13 ] , the readers are referred to @xcite . plugging eqn.[eq12 ] and eqn.[eq13 ] into eqn.[eq11 ] gives the partial derivatives of the loss functions for the reeig and logeig layers : @xmath71 where @xmath72 and @xmath73 can be obtained by the same derivation strategy used in eqn.[eq11 ] . for the function eqn.[eq2 ] employed in the reeig layers ,",
    "its variation becomes @xmath74 , and these two partial derivatives are computed by @xmath75 where @xmath20 is defined in eqn.[eq3 ] , and @xmath76 is the gradient of @xmath20 : @xmath77    for the function eqn.[eq4 ] used in the logeig layers , its variation is @xmath78 .",
    "then we calculate the following two partial derivatives : @xmath79    by mainly employing eqn.[eq7]eqn.[eq9 ] and eqn.[eq15]eqn.[eq20 ] , the riemannian matrix backprop for training the spdnet can be realized .",
    "the convergence analysis of the used sgd algorithm on riemannian manifolds follows the developments in @xcite .",
    "as claimed before , building the spdnet structure is inspired by the geometry - aware spd matrix learning idea ( i.e. preserving spd property ) of two recent works @xcite and the paradigm of typical deep networks .",
    "thus , the goal of the spdnet , that mainly uses multiple bimap layers with a softmax layer , is to deeply learn lower - dimensional and more discriminative ( desirable ) spd matrices , which benefit the classification tasks .",
    "in addition , our spdnet has designed reeig layers to introduce the non - linearity during learning desirable spd matrices .",
    "consequently , the main advantages of the proposed spdnet over the two works @xcite lie in the multiple - layer learning and non - linear learning schemes , both of which has shown great power in the field of deep learning on euclidean data .",
    "the work @xcite proposed a deep network to introduce a logeig - like map for second - order pooling in image based visual tasks .",
    "however , @xcite merely plugs one layer of covariance log - euclidean map into convnets starting from images .",
    "in contrast , our spdnet exploits multiple layers for the operations on spd matrices , including bimap , reeig and logeig layers , to deeply learn the spd matrices in the context of riemannian manifolds . from another point of the view , the proposed work could be built on the top of the network @xcite for deeper spd matrix learning architecture that starts from images .",
    "the superiority of the proposed spdnet will be further studied in the following experiments . moreover",
    ", we must claim that our spdnet with bilinear maps is still useful while the work @xcite will totally break down when the processing data are not covariance matrices for images .",
    "we study the effectiveness of the proposed spd network ( spdnet ) by conducting evaluations for three popular visual classification tasks including emotion recognition , action recognition and face verification where spd matrix representations have achieved successes . for the evaluations ,",
    "the state - of - the - art spd matrix learning methods are compared .",
    "they are covariance discriminative learning ( cdl ) @xcite , cdl on approximate infinite - dimensional region covariance descriptors obtained by random fourier feature ( cdl - f ) @xcite , log - euclidean metric learning ( leml ) @xcite and spd manifold learning ( spdml ) @xcite that uses affine - invariant metric ( aim ) @xcite and stein divergence @xcite . the riemannian sparse representation ( rsr ) @xcite for spd matrices is also evaluated .",
    "in addition , we also evaluate the deep second - order pooling ( deepo2p ) network @xcite which merely plugs a covariance log - euclidean map layer into convnets starting from images . for all of them ,",
    "we use their source codes from authors with tuning their parameters according to the original works . for our spdnet",
    ", we build its architecture with 8 layers : @xmath80 , where @xmath81 indicate the bimap , reeig , logeig , euclidean fully connected and softmax log - loss layers respectively .",
    "the learning rate @xmath48 is fixed as @xmath82 , the batch size is set to 30 , the weights in bimap layers are initialized as random semi - orthogonal matrices , and the rectification threshold @xmath19 is set to @xmath83 . for training the spdnet",
    ", we just use an i7 - 2600k ( 3.40ghz ) pc without any gpus .",
    ".emotion recognition accuracies for the afew database . [ cols=\"<,^\",options=\"header \" , ]      for face verification , we employ the point - and - shoot challenge ( pasc ) database @xcite , which is a challenge for verifying faces in videos .",
    "it includes 1,401 videos taken by control cameras and 1,401 videos captured by handheld cameras for 265 people .",
    "besides , it also contains 280 videos for training .    on the pasc database",
    ", there are control - to - control and handheld - to - handheld face verification tasks , both of which are to verify a claimed in the query video by comparing with the associated target video .",
    "as done in @xcite , we also use the external training data ( cox ) with 900 videos .",
    "similar to the last two experiments , the whole training data is also augmented to 12,529 small video clips . for evaluation , we use the approach of @xcite to extract state - of - the - art deep face features from the normalized face images of size @xmath84 .",
    "then we employ pca to reduce the deep features to 400-dimensional features .",
    "following @xcite , we compute a spd matrix of size @xmath85 , which fuses the data covariance matrix and mean , for each video sequence of frames on this dataset .",
    "for the evaluation of face verification , we configure the sizes of the spdnet weights in bimap layers to @xmath86 respectively . the time for training the spdnet at each of 100 epoches is around 15 m .",
    "table.[tab3 ] compares the accuracies of the different methods including the state - of - the - art methods ( herml - delf @xcite and vggdeepface @xcite ) on the pasc .",
    "since the rsr method is designed for recognition tasks rather than verification tasks , we do not report its results .",
    "although the used sofmax output layer in our spdnet is not favorable for the verification tasks , we find that it still achieves state - of - the - art results . in the end , we can also obtain the same validations for the studies of without using logeig layers ( i.e. , spdnet-0-birelayer ) and different numbers of bimap / reeig layers ( i.e. , spdnet-1-birelayer and spdnet-2-birelayers ) as observed from table.[tab3 ] .",
    "we introduced a novel deep riemannian network for opening up a possibility of spd matrix non - linear learning . in the proposed deep network architecture",
    ", we mainly built bimap layers , reeig layers and logeig layers in the context of spd matrix deep learning on the underlying riemannian manifolds . to train the proposed spd network , we exploited a riemannian matrix backpropagation by generalizing the sgd optimization algorithm to stiefel manifolds .",
    "the evaluations on three typical visual classification tasks studied the effectiveness of the proposed network for spd matrix learning .",
    "as future work , it would be interesting to explore more effective layers , e.g. , bimap layers with multiple projections , pooling layer and normalization layer , to equip the new riemannian network .",
    "another interesting direction is to extend this work to a framework for a general riemannian manifold by adopting the constrained transformations and the proposed riemannian matrix backprop .",
    "in addition , we would also build the proposed spd network on the top of existing convolution networks such as @xcite for deeper architecture starting from images in an end - to - end deep learning manner .",
    "j.  beveridge , j.  phillips , d.  bolme , b.  draper , g.  givens , y.  lui , m.  teli , h.  zhang , w.  scruggs , k.  bowyer , p.  flynn , and s.  cheng .",
    "the challenge of face recognition from digital point - and - shoot cameras . in _ btas _ , 2013 .",
    "d.  boscaini , j.  masci , s.  melzi , m.  bronstein , u.  castellani , and p.  vandergheynst . learning class - specific descriptors for deformable shapes using localized spectral convolutional networks . in _",
    "computer graphics forum _ , volume  34 , pages 1323 .",
    "wiley online library , 2015 ."
  ],
  "abstract_text": [
    "<S> symmetric positive definite ( spd ) matrix learning methods have become popular in many image and video processing tasks , thanks to their ability to learn appropriate statistical representations while respecting the riemannian geometry of the underlying spd manifold . in this paper </S>",
    "<S> we build a riemannian network to open up a new direction of spd matrix non - linear learning in a deep architecture . </S>",
    "<S> the built network generalizes the euclidean network paradigm to non - euclidean spd manifolds . </S>",
    "<S> in particular , we devise bilinear mapping layers to transform input spd matrices into more desirable spd matrices , exploit eigenvalue rectification layers to introduce the non - linearity with a non - linear function on the new spd matrices , and design eigenvalue logarithm layers to perform log - euclidean riemannian computing on the resulting spd matrices for regular output layers . for training the deep network , we propose a riemannian matrix backpropagation by exploiting a variant of stochastic gradient descent on stiefel manifolds where the network weights reside on . </S>",
    "<S> we show through experiments that the proposed spd network can be simply trained and outperform existing spd matrix learning and state - of - the - art methods in three typical visual classification tasks . </S>"
  ]
}