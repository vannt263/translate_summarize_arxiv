{
  "article_text": [
    "raghavan and baum s reliability - output viterbi algorithm ( rova ) @xcite uses the sequence - estimation property of the viterbi algorithm to calculate the exact word - error probability of a received convolutional code sequence . in general",
    ", the rova can be used to compute the word - error probability for any finite - state markov process observed via memoryless channels ( i.e. , processes with a trellis structure ) . however , the rova is only valid for processes that terminate in a known state ( usually the all - zeros state ) . for codes with large constraint lengths",
    "@xmath0 , a significant rate penalty is incurred due to the @xmath1 additional symbols that must be transmitted in order to arrive at the termination state .",
    "tail - biting convolutional codes can start in any state , but must terminate in the same state .",
    "the starting / terminating state is unknown at the receiver .",
    "these codes do not suffer the rate loss of terminated codes , making them throughput - efficient ( see , e.g. , @xcite and ( * ? ? ?",
    "* ch . 12.7 ) ) .",
    "the tail - biting technique is commonly used for short - blocklength coding .      in this paper",
    ", we extend the rova to compute the word - error probability for tail - biting codes .",
    "first , we present a straightforward approach , which we call the tail - biting rova ( tb rova ) .",
    "tb rova invokes the original rova for each of the possible starting states @xmath2 .",
    "the complexity of this straightforward approach is large , proportional to @xmath3 for standard binary convolutional codes ( and @xmath4 for convolutional codes over galois field @xmath5 ) .",
    "we explore several approaches to reduce the complexity of tb rova .",
    "we first introduce a post - decoding algorithm that computes the reliability of codewords that have already been decoded by an existing tail - biting decoder , including possibly suboptimal decoders .",
    "we then propose a new tail - biting decoder that uses the posterior distribution of the starting states to identify the most probable starting state of the received sequence .",
    "finally , we discuss how to use fricke and hoeher s simplified ( approximate ) rova @xcite for each of the @xmath6 initial states , which reduces the complexity of the word - error probability computation .",
    "the reliability - output algorithms presented in this paper apply to both feedforward ( non - recursive ) and feedback ( recursive ) convolutional encoders .",
    "however , as pointed out by sthl et al .",
    "@xcite , it is not possible to have a one - to - one mapping from information words to codewords and still fulfill the tail - biting restriction for feedback encoders at certain tail - biting codeword lengths .",
    "sthl et al . @xcite",
    "provide conditions for when tail - biting will work for recursive encoders and also describe how to determine the starting state corresponding to an input sequence . in the cases that the tail - biting technique works for feedback encoders ,",
    "there is a one - to - one mapping between input sequences and codewords , and the reliability - output algorithms in this paper are valid .",
    "the remainder of this paper proceeds as follows : sec .",
    "[ sec : relatedliterature ] reviews the related literature and sec .",
    "[ sec : notation ] introduces notation .",
    "[ sec : rova ] reviews raghavan and baum s rova and discusses how to extend it to tail - biting codes .",
    "the simplified rova for tail - biting codes is discussed in sec .",
    "[ sec : simplified ] .",
    "[ sec : post - proc_rova ] presents the post - decoding reliability computation ( prc ) for tail - biting codes and sec .",
    "[ sec : state_estimation ] introduces the tail - biting state - estimation algorithm ( tb sea ) . sec .",
    "[ sec : tbbcjr ] discusses an alternative to tb sea using the tail - biting bcjr algorithm .",
    "[ sec : complexity ] evaluates the complexity of the proposed algorithms , and sec .",
    "[ sec : simulation_results ] shows numerical examples of the computed word - error probability and the actual word - error probability .",
    "[ sec : conc ] concludes the paper .",
    "there are a number of reliability - based decoders for terminated convolutional codes , most notably the yamamoto - itoh algorithm @xcite , which computes a reliability measure for the decoded word , but not the exact word - error probability . in @xcite ,",
    "fricke and hoeher use raghavan and baum s rova in a reliability - based type - i hybrid automatic repeat request ( arq ) scheme .",
    "hof et al .",
    "@xcite modify the viterbi algorithm to permit generalized decoding according to forney s generalized decoding rule @xcite .",
    "when the generalized decoding threshold is chosen for maximum likelihood ( ml ) decoding with erasures and the erasure threshold is chosen appropriately , this augmented viterbi decoder is equivalent to the rova .",
    "a type - ii hybrid arq scheme for incremental redundancy using punctured terminated convolutional codes is presented by williamson et al .",
    "@xcite . in @xcite ,",
    "additional coded symbols are requested when the word - error probability computed by the rova exceeds a target word - error probability .",
    "this word - error requirement facilitates comparisons with recent work in the information theory community @xcite .",
    "polyanskiy et al .",
    "@xcite investigate the maximum rate achievable at short blocklengths with variable - length feedback codes .",
    "while @xcite shows that terminated convolutional codes can deliver throughput above the random - coding lower bound of @xcite , the rate loss from termination is still significant at short blocklengths . to avoid the termination overhead , it is imperative to have a reliability - output decoding algorithm for tail - biting codes .",
    "in contrast to the decoding algorithms for terminated codes , anderson and hladik @xcite present a tail - biting maximum a posteriori ( map ) decoding algorithm .",
    "this extension of the bcjr algorithm @xcite can be applied to tail - biting codes with a priori unequal source data probabilities . as with the bcjr algorithm",
    ", @xcite computes the posterior probabilities of individual data symbols .",
    "in contrast , the rova @xcite and the tail - biting reliability - based decoders in this paper compute the posterior probabilities of the codeword .",
    "more importantly , the tail - biting bcjr of @xcite is only an approximate symbol - by - symbol map decoder , as pointed out in @xcite and @xcite .",
    "because the tail - biting restriction is not strictly enforced , non - tail - biting  pseudocodewords \" can cause bit errors , especially when the ratio of the tail - biting length @xmath7 to the memory length @xmath1 is small ( i.e. , @xmath8-@xmath9 ) . further comparisons with the tail - biting bcjr are given in sec .",
    "[ sec : tbbcjr ] . an exact symbol - by - symbol map decoder for tail - biting codes",
    "is given in ( * ? ? ?",
    "* ch . 7 ) .",
    "handlery et al .",
    "@xcite introduce a suboptimal , two - phase decoding scheme for tail - biting codes that computes the approximate posterior probabilities of each starting state and then uses the standard bcjr algorithm to compute the posterior probabilities of the source symbols .",
    "this approach is compared to the tail - biting bcjr of @xcite and exact map decoding in terms of bit - error - rate ( ber ) performance .",
    "both the two - phase approach of @xcite and the tail - biting bcjr of @xcite perform close to exact map decoding when @xmath10 is large , but suffer a ber performance loss when @xmath10 is small .",
    "although it does not compute the word - error probability , yu @xcite introduces a method of estimating the initial state of tail - biting codes , which consists of computing a pre - metric for each state based on the last @xmath1 observations of the received word . this pre - metric",
    "is then used to initialize the path metrics of the main tail - biting decoder ( e.g. , the circular viterbi decoder @xcite ) , instead of assuming that all states are equally likely at initialization .",
    "the state - estimation method of @xcite , which is not maximum - likelihood , is limited to systematic codes and a special configuration of nonsystematic codes that allows information symbols to be recovered from noisy observations of coded symbols .",
    "because tail - biting codes can be viewed as circular processes @xcite , decoding can start at any symbol .",
    "@xcite describe a reliability - based decoding method that compares the log likelihood - ratios of the received symbols in order to determine the most reliable starting - location for tail - biting decoders .",
    "selecting a reliability - based starting location reduces the error probability by minimizing the chance of choosing non - tail - biting paths early in the decoding process .",
    "@xcite apply this approach to existing suboptimal decoders , including the wrap - around viterbi algorithm of @xcite . as with @xcite",
    ", @xcite does not compute the word - error probability .",
    "pai et al .",
    "@xcite generalizes the yamamoto - itoh algorithm to handle tail - biting codes and uses the computed reliability measure as the retransmission criteria for hybrid arq . when there is a strict constraint on the word - error probability , however , this type of reliability measure is not sufficient to guarantee a particular undetected - error probability .",
    "providing such a guarantee motivates the word - error probability calculations in this paper ( instead of bit - error probability as in @xcite ) .",
    "we use the following notation in this paper : @xmath11 denotes the probability mass function ( p.m.f . ) of discrete - valued random variable @xmath12 at value @xmath13 , which we also write as @xmath14 . the probability density function",
    "( p.d.f . ) of a continuous - valued random variable @xmath15 at value @xmath16 is @xmath17 , sometimes written as @xmath18 . in general , capital letters denote random variables and lowercase letters denote their realizations .",
    "boldface letters with superscripts denote vectors , as in @xmath19 , while subscripts denote a particular element of a vector : @xmath20 is the @xmath21th element of @xmath22 .",
    "we use the hat symbol to denote the output of a decoder , e.g. , @xmath23 is the codeword chosen by the viterbi algorithm .",
    "raghavan and baum s reliability - output viterbi algorithm @xcite augments the canonical viterbi decoder with the computation of the word - error probability of the maximum - likelihood ( ml ) codeword . in this section",
    ", we provide an overview of the rova .    for rate-@xmath24 convolutional codes with @xmath7 trellis segments and input alphabet @xmath25",
    ", we denote the ml codeword as @xmath26 and the noisy received sequence as @xmath27 . the probability that the ml decision is correct given the received word",
    "is @xmath28 , and the word - error probability is @xmath29 .",
    "the probability of successfully decoding can be expressed as follows : @xmath30 where we have used @xmath31 to denote the conditional p.d.f . of the real - output channel ( e.g. , the binary - input awgn channel ) .",
    "this may be replaced by the conditional p.m.f .",
    "@xmath32 for discrete - output channels ( e.g. , the binary symmetric channel ) .",
    "the probability of correctly decoding can be further simplified if each of the codewords @xmath33 is a priori equally likely , i.e. , @xmath34 , which we assume for the remainder of the paper .",
    "this assumption yields @xmath35 in general , the denominator in may be computationally intractable when the message set cardinality is large",
    ". however , the rova @xcite takes advantage of the trellis structure of convolutional codes to compute @xmath36 exactly with complexity that is linear in the blocklength and exponential in the constraint length of the code ( i.e. , it has complexity on the same order as that of the original viterbi algorithm ) .",
    "this probability can also be computed approximately by the simplified ( approximate ) rova @xcite , which will be discussed further in sec .",
    "[ sec : simplified ] .",
    "the rova can compute the probability of word error for any finite - state markov process observed via memoryless channels ( e.g. , in maximum - likelihood sequence estimation for signal processing applications ) . in the remainder of this paper , we use the example of convolutional encoding and decoding , but the rova and our tail - biting trellis algorithms apply to any finite - state markov process .",
    "raghavan and baum s rova applies only to codes that begin and end at a known state .",
    "each of the probabilities @xmath37 in is implicitly conditioned on the event that the receiver knows the initial and final state of the convolutional encoder .    to be precise , rova beginning and ending at the same state @xmath2 , which we shall denote as rova@xmath38 , effectively computes the following : @xmath39 where the limit @xmath40 in the summation of the denominator denotes that the enumeration for the summation is over all codewords @xmath40 with starting state @xmath2 , and @xmath41 is shorthand for @xmath42 . in summary",
    ", rova@xmath38 computes the ml codeword @xmath43 corresponding to starting state @xmath2 , the posterior probability of that codeword given @xmath2 , @xmath44 , and the probability of the received sequence given @xmath2 , @xmath45 .",
    "the inputs and outputs of rova@xmath38 are illustrated in the block diagram of fig .",
    "[ fig : rova_block ] .    for tail - biting codes , we are interested in computing the quantity @xmath36 without conditioning on the unknown starting and ending state @xmath2 : @xmath46 the ml codeword @xmath47 has an associated initial state , @xmath48 .",
    "note that @xmath49 unless @xmath50 , since @xmath47 is not a possible codeword for any starting state other than @xmath48 .",
    "thus , we have : @xmath51 thus , the tail - biting rova ( tb rova ) must compute the probability @xmath36 of successful decoding in by weighting @xmath52 with @xmath53 .",
    "( for the original rova with a known starting state @xmath48 , @xmath54 and @xmath55 . )",
    "using the fact that each of the initial states @xmath2 is equally likely _ a priori _",
    "( i.e. , @xmath56 ) , we have : @xmath57 this finally yields @xmath58 where the summation in the denominator of is over all @xmath6 possible initial states .",
    "a straightforward ml approach to decoding tail - biting codes is to perform the viterbi algorithm va@xmath38 , for each possible starting state @xmath59 .",
    "the ml codeword @xmath47 is then chosen by determining the starting state with the greatest path metric ( i.e. , the greatest probability ) . as shown in fig .  [",
    "fig : tb_rova_block ] , this approach will work for the rova as well : perform rova@xmath38 for each possible @xmath2 and then pick @xmath47 and its starting state @xmath48 .",
    "the probability @xmath36 is then computed as in , using @xmath60 from the rova for the ml starting state and the @xmath61 terms produced by the rovas for all the states .",
    "this approach is illustrated in the block diagram of fig .",
    "[ fig : tb_rova_block ] .",
    "this section proposes replacing the exact word - error computations of sec .",
    "[ sec : straightforward_algo ] s straightforward tb rova with an estimated word - error probability , using fricke and hoeher s simplified ( approximate ) rova @xcite .",
    "this approach requires a viterbi decoder for each starting state to select the ml codeword for that state .",
    "fricke and hoeher s @xcite simplified rova for starting state @xmath2 , which we call approx rova@xmath38 , estimates the probability @xmath62 . substituting this estimate @xmath63 into , we have the following approximation : @xmath64 while @xmath61 is not computed directly by approx rova@xmath38 , we can approximate it with quantities available from approx rova@xmath38 as @xmath65 when all @xmath66 codewords with starting state @xmath2 are equally likely , where @xmath67 is the number of trellis segments before termination .",
    "note @xmath68 can be calculated by the viterbi algorithm for starting state @xmath2 .",
    "equations and lead to the following estimate of the word - correct probability : @xmath69 we refer to the overall computation of @xmath70 in as approx tb rova . sec .",
    "[ sec : complexity ] provides a discussion of its complexity and sec .",
    "[ sec : simulation_results ] presents simulation results .",
    "note that despite the approximations , the simplified rova chooses the ml codeword for terminated codes . for the tail - biting version approx tb rova ,",
    "as long as the winning path metric of each starting / ending state is used to determine the ml state @xmath48 , the decoded codeword will also be ml ( and the same as the codeword chosen by the exact tail - biting rova in sec .  [",
    "sec : straightforward_algo ] ) .",
    "however , if the approximate reliabilities @xmath71 are used instead of the path metrics to select the decoded word @xmath47 as @xmath72 , it is possible that the decoded word will not be ml ( if the channel is noisy enough ) .",
    "there are @xmath6 possible starting states that must be evaluated in the straightforward tb rova of sec .",
    "[ sec : straightforward_algo ] and fig .",
    "[ fig : tb_rova_block ] .",
    "thus it may be beneficial to instead use an existing reduced - complexity tail - biting decoder to find @xmath47 , and then compute the reliability separately .",
    "many reduced - complexity tail - biting decoders take advantage of the circular decoding property of tail - biting codes .",
    "some of these approaches are not maximum likelihood , such as the wrap - around viterbi algorithm or bidirectional viterbi algorithm ( bva ) , both discussed in @xcite .",
    "the a * algorithm @xcite is one ml alternative to the tail - biting decoding method described in sec .",
    "[ sec : straightforward_algo ] .",
    "its complexity depends on the snr .",
    "suppose that a decoder has already been used to determine @xmath47 and its starting state @xmath48 , and that we would like to determine @xmath36 .",
    "one operation of rova@xmath73 would compute the probability @xmath60 , but the probability @xmath74 required by would still be undetermined .",
    "must we perform rova@xmath38 for each @xmath75 in order to compute @xmath76 as in ?",
    "this section shows how to avoid this by combining the computations of the straightforward approach into a novel post - decoding reliability computation ( prc ) for tail - biting codes .",
    "[ fig : post_block ] shows a block diagram of prc . for a rate-@xmath24 tail - biting convolutional code with @xmath1 memory elements , prc takes the following inputs : a received sequence @xmath77 with @xmath7 trellis segments , a _ candidate codeword _",
    "@xmath78 corresponding to a _ candidate path _ in the trellis , and the starting / ending state @xmath48 of the candidate codeword . the goal is to compute the posterior probability of the candidate codeword , @xmath79 .",
    "the candidate codeword selected by the decoder may not be the ml codeword .",
    "prc computes its true reliability regardless .",
    "raghavan and baum s rova @xcite performs the traditional add - compare - select operations of the viterbi algorithm and then computes , for every state in each trellis segment , the posterior probability that the survivor path is correct and the posterior probability that one of the non - surviving paths at the state is correct . upon reaching the end of the trellis ( the @xmath7th segment ) , having",
    "selected survivor paths at each state , there will be one survivor path corresponding to the ml codeword .",
    "in contrast , with the candidate path already identified , prc processes the trellis without explicitly selecting survivors .",
    "prc computes the reliability of the candidate path and the overall reliability of all other paths .",
    "we define the following events at trellis stage @xmath80 ( @xmath81 ) :    * @xmath82the candidate path from its beginning at state @xmath48 to its arrival at state @xmath83 in segment @xmath80 is correct@xmath84 * @xmath85some path from its beginning at state @xmath2 to its arrival at state @xmath86 in segment @xmath80 is correct ( including possibly the candidate path if @xmath87 ) @xmath84 * @xmath88the branch from state @xmath21 to state @xmath83 at time @xmath80 is correct@xmath84    for @xmath89 memory elements , fig .",
    "[ fig : trellis_diagram ] gives some examples of the paths corresponding to each of these events .",
    "the black branches in fig .",
    "[ fig : trellis_diagram ] constitute all of the paths in the event @xmath90 .",
    "the red branches in fig .",
    "[ fig : trellis_diagram ] show the candidate path corresponding to the event @xmath91 .",
    "the posterior probability that the red candidate path starting at state @xmath48 is correct is @xmath92 .",
    "the posterior probability that any of the paths that started at state @xmath2 and arrive at state @xmath86 in segment @xmath93 are correct is @xmath94 .",
    "note that since some branch transitions are invalid in the trellis , @xmath95 and @xmath96 may be zero for invalid states and branches in segment @xmath80 .",
    "the path - correct probabilities @xmath95 and @xmath97 can be expressed recursively in terms of the probabilities of the previous trellis segments paths being correct . conditioned on the noisy channel observations @xmath19 , the path - correct probability for the candidate path ( which passes through state @xmath21 in segment @xmath98 ) is @xmath99 the decomposition in uses bayes rule and follows @xcite .",
    "[ fig : trellis_diagram ] identifies an example of states @xmath21 and @xmath83 used to compute the probability of @xmath100 .    by the markov property , @xmath101 , which is the conditional p.d.f . , related to the familiar viterbi algorithm branch metric .",
    "similarly , the second term is @xmath102 . with these simplifications ,",
    "becomes @xmath103 the denominator can be expressed as a sum over all branches in the trellis @xmath104 at time @xmath80 , where each branch from @xmath105 to @xmath86 is denoted by a pair @xmath106 : @xmath107 the derivation thus far has followed @xcite , which focused on terminated convolutional codes .    for tail - biting codes , we can further expand the term @xmath108 by summing over all the possible starting states @xmath109 as follows : @xmath110 where the last equality follows from the markov property @xmath111 .",
    "thus , becomes @xmath112    the term @xmath113 is the probability that the branch from state @xmath105 to state @xmath86 is correct , given that one of the paths that started at state @xmath109 and arrived at state @xmath105 at time @xmath114 is correct .",
    "recall that @xmath67 .",
    "@xmath115 for @xmath116 ( i.e. , all @xmath80 except for the last @xmath1 trellis segments ) .",
    "this is because there are @xmath117 equiprobable next states for these values of @xmath80 .    using the notation @xmath118 to indicate there is a valid path from state @xmath86 at time @xmath80 to state @xmath109 at time @xmath7 ,",
    "we define the following indicator function @xmath119 , which indicates that the trellis branch from state @xmath105 to state @xmath86 at trellis stage @xmath80 is a branch in a possible trellis path that terminates at @xmath109 : @xmath120 the branch - correct probabilities can now be written as @xmath121 we now define the following normalization term for the @xmath80th trellis segment using the above indicators : @xmath122 the @xmath123 normalization term includes most of but excludes the potential @xmath124 in @xmath113 because it cancels with @xmath125 in the numerator of .",
    "( either both have @xmath124 or both are @xmath126 , depending only on @xmath80 . ) substituting into , we have @xmath127 thus , for the @xmath80th trellis segment , expresses the candidate path - correct probability in terms of the candidate path - correct probability in the previous segment .",
    "the corresponding expression for the overall path probabilities @xmath128 involves more terms . instead of tracing a single candidate path through the trellis , we must add the probabilities of all the valid tail - biting paths incident on state @xmath86 in segment @xmath80 as follows : @xmath129 the summation above is over the @xmath117 incoming branches to state @xmath86 . in the special case of a rate-@xmath130 binary code ( @xmath25=@xmath9 ) , there are @xmath9 incoming branches , which we will label as @xmath131 and @xmath132 , so becomes @xmath133 .",
    "\\label{eqn : p_nonsurv_k1}\\end{aligned}\\ ] ] fig .",
    "[ fig : trellis_diagram ] illustrates how the paths from starting state @xmath2 merge into state @xmath86 at trellis segment @xmath134 .",
    "the path probabilities are initialized as follows :    * @xmath135 if @xmath136 , or @xmath137 otherwise .",
    "* @xmath138 if @xmath139 , or @xmath137 otherwise .    in each trellis -",
    "segment @xmath80 ( @xmath140 ) , do the following :    1 .   for each branch",
    "@xmath141 , compute the conditional p.d.f .",
    "2 .   for each branch",
    "@xmath141 and each starting state @xmath2 , compute the branch - valid indicator @xmath143 , as in . 3 .   using the above values , compute the normalization constant @xmath123 , as in . 4 .   for the current state @xmath83 of the candidate path , compute the candidate path - correct probability @xmath144 , as in .",
    "5 .   for each starting state @xmath2 and each state @xmath86 , compute the overall path - correct probabilities @xmath128 , as in .    after processing all @xmath7 stages of the trellis ,",
    "the following meaningful quantities emerge :    * the posterior probability that the tail - biting candidate path from @xmath48 to @xmath48 is correct is @xmath145 , which is the probability that the decoded word is correct , given the received sequence . * the posterior word - error probability is then @xmath146 . * the posterior probability that any of the tail - biting paths ( any of the codewords ) from @xmath2 to @xmath2 is correct is @xmath147 , which is the state reliability desired for .",
    "numerical results of prc are shown in fig .",
    "[ fig : tb_rova_awgn ] in sec .",
    "[ sec : simulation_results ] .",
    "the post - decoding reliability computation described above relies on a separate decoder to identify the candidate path .",
    "if , on the other hand , we would like to compute the word - error probability of a tail - biting code without first having determined a candidate path and starting state , we may use the following tail - biting state - estimation algorithm ( tb sea ) .",
    "tb sea computes the map starting state @xmath148 , along with its reliability @xmath149 .",
    "rova@xmath73 can then be used to determine the map codeword @xmath150 corresponding to starting state @xmath48 , as illustrated in fig .",
    "[ fig : tb_sea_block ] .    prc relied on tracing a single candidate path through the trellis and computing the candidate path - correct probability , as in .",
    "however , the overall path - correct probabilities in do not rely on the candidate path or its probability .",
    "the proposed tb sea aggregates all the previous - segment path - correct probabilities @xmath151 as in , without regard to a candidate path . as a result",
    ", tb sea replaces the traditional add - compare - select operations of the viterbi algorithm with the addition of all the path probabilities merging into a state that emanate from a particular origin state .",
    "once the entire trellis has been processed , the state reliabilities are compared and the map starting state is selected .",
    "the path probabilities are initialized as follows :    * @xmath138 if @xmath139 , or @xmath137 otherwise .    in each trellis -",
    "segment @xmath80 ( @xmath140 ) , do the following :    1 .   for each branch",
    "@xmath141 , compute the conditional p.d.f .",
    "2 .   for each branch",
    "@xmath141 and each starting state @xmath2 , compute the branch - valid indicator @xmath143 , as in . 3 .   using the above values , compute the normalization constant @xmath123 , as in . 4 .   for each starting state @xmath2 and each state @xmath86 ,",
    "compute the overall path - correct probabilities @xmath128 , as in .",
    "after processing all @xmath7 stages of the trellis , the following meaningful quantity emerges :    * the posterior probability that any of the tail - biting paths ( any of the codewords ) from @xmath2 to @xmath2 is correct is @xmath152 .    tb sea selects the starting state with the maximum value of @xmath153 ( the map choice of starting state ) , yielding @xmath48 and its reliability @xmath154 .",
    "thus , tb sea has selected the map starting state without explicitly evaluating all possible codewords ( i.e. , paths through the trellis ) .",
    "this result is not limited to error control coding ; it can be applied in any context to efficiently compute the map starting state of a tail - biting , finite - state markov process .      after finding the map starting state @xmath48 with tb sea , rova@xmath73",
    "may be used to compute the map codeword @xmath155 and @xmath156 .",
    "we have used the subscript @xmath48 to indicate that @xmath155 is the map codeword for the terminated code starting and ending in @xmath48 .",
    "the overall reliability @xmath157 can then be computed as in , which we have replicated below to show how the tb sea and rova@xmath73 provide the needed factors :    @xmath158      is it possible that the maximum a posteriori codeword @xmath47 corresponds to a starting state other than the map state @xmath48 ?",
    "the following theorem proves that the answer is no , given a suitable probability of error .    the map codeword @xmath47 for a tail - biting convolutional code begins and ends in the map state @xmath48 , as long as @xmath159 .",
    "[ thm : mlstates ]    consider a codeword @xmath47 with @xmath160 . by ,",
    "@xmath161 , where @xmath162 is the starting state of @xmath47 .",
    "this implies that @xmath163 .",
    "the map state is @xmath164 , which must be @xmath162 , since all other states @xmath109 must have @xmath165 .",
    "theorem  [ thm : mlstates ] shows that the application of tb sea followed by the viterbi algorithm ( or the rova ) will always yield the map codeword @xmath47 of the tail - biting code , not just the map codeword for the terminated code starting in @xmath48 ( as long as the probability of error is less than @xmath166 ) . in most practical scenarios , the word - error probability ( @xmath167 ) , even if unknown exactly , is much less than @xmath166 , so the theorem holds . as a result , in these cases",
    "tb sea selects the same codeword @xmath47 as would the tb rova of sec .",
    "[ sec : straightforward_algo ] , and computes the same reliability @xmath36 .",
    "l | c | c | c | c | c | c algorithm & path metrics & cand .",
    "& & & +   + va@xmath38 & @xmath6 & @xmath137 & @xmath137 & @xmath137 & @xmath168 & @xmath137 + rova@xmath38 @xcite & @xmath6 & @xmath6 & @xmath6 & @xmath169 & @xmath170 & @xmath171 + prc & @xmath137 & @xmath126 & @xmath172 & @xmath173 & @xmath174 & @xmath175 + & @xmath137 & @xmath137 & @xmath172 & @xmath173 & @xmath174 & @xmath172 + approx rova@xmath38 @xcite & @xmath6 & @xmath6 & @xmath137 & @xmath176 & @xmath177 & @xmath178 +   + & @xmath172 & @xmath172 & @xmath172 & @xmath179 & @xmath180 & @xmath181 + & @xmath6 & @xmath6 & @xmath182 & @xmath183 & @xmath184 & @xmath185 + & @xmath172 & @xmath172 & @xmath137 & @xmath186 & @xmath187 & @xmath172 +    [ tbl : complexity ]      while several related papers such as @xcite and @xcite have proposed ways to estimate the starting state of a tail - biting decoder , none computes exactly the posterior probability of the starting state , @xmath188 , as described for tb sea . upon a first inspection",
    ", the tail - biting bcjr ( tb bcjr ) of @xcite appears to provide a similar method of computing this probability . applying the forward recursion of the bcjr algorithm provides posterior probabilities that are denoted as @xmath189 in @xcite .",
    "thus , it would appear that the state - estimation algorithm of sec .",
    "[ sec : tb_sea ] can be replaced by a portion of the tb bcjr algorithm .",
    "this would yield a significant decrease in computational complexity , from roughly @xmath172 operations per trellis segment for tb sea to @xmath6 for the tb bcjr .",
    "however , as will be shown in sec .",
    "[ sec : simulation_results ] , the word - error performance of the tail - biting bcjr when used in this manner is significantly inferior to that of tb sea .",
    "as noted in @xcite and @xcite , the tail - biting bcjr algorithm is an approximate symbol - by - symbol map decoder .",
    "it is approximate in the sense that the forward recursion of the tb bcjr in @xcite does not strictly enforce the tail - biting restriction , allowing non - tail - biting `` pseudocodewords '' to appear and cause errors .",
    "@xcite requires the probability distributions of the starting and ending states to be the same , which is a weaker condition than requiring all codewords to start and end in the same state .",
    "@xcite and @xcite have shown that when the tail - biting length @xmath7 is large relative to the memory length @xmath1 , the suboptimality of the tb bcjr in terms of the bit - error rate is small .",
    "however , we are concerned with word - error performance in this paper .",
    "we find that when the tb bcjr is used to estimate the initial state @xmath48 and its probability @xmath154 , followed by rova@xmath73 for the most likely state @xmath48 , the impact on word error is severe ( fig .",
    "[ fig : tb_rova_awgn ] ) .",
    "frequent state - estimation errors prevent the viterbi algorithm in the second phase from decoding to the correct codeword .",
    "thus , the approximate tail - biting bcjr of @xcite is not effective as a replacement for tb sea when using the word - error criterion .",
    "in contrast , the exact symbol - by - symbol map decoder for tail - biting codes in ( * ? ? ? * ch .",
    "7 ) does enforce the tail - biting restriction , and has complexity on the same order as that of tb sea .",
    "however , because the symbol - by - symbol map decoder selects the most probable input symbols while tb sea + rova@xmath73 selects the most probable input sequence , tb sea + rova@xmath73 is recommended for use in retransmission schemes that depend on the word - error probability .",
    "table  [ tbl : complexity ] compares the complexity per trellis segment of each of the discussed algorithms , assuming that the conditional p.d.f .",
    "@xmath142 has already been computed for every branch in the trellis .",
    "the columns labeled ` path metrics ' , ` cand . prob . ' , and ` overall prob . '",
    "refer to the number of quantities that must be computed and stored in every trellis segment , for the path metrics of the viterbi algorithm , the candidate path probability of , and the overall path probability of , respectively .",
    "the number of operations per trellis segment required to compute these values is listed in the columns labeled ` additions ' , ` multiplications ' , and ` divisions ' .",
    "the rova(@xmath2 ) row of table  [ tbl : complexity ] corresponds to raghavan and baum s rova @xcite for a terminated code starting and ending in state @xmath2 .",
    "the operations listed include the multiplications required for the path metric computations of the viterbi algorithm for state @xmath2 , va@xmath38 .",
    "the tb rova row represents performing the rova for each of the @xmath6 possible starting states as described in sec .",
    "[ sec : straightforward_algo ] , so each of the quantities is multiplied by @xmath6 .",
    "the prc row corresponds to the proposed post - decoding reliability computation of sec .",
    "[ sec : post - proc_rova ] .",
    "the complexity incurred to determine the candidate path ( e.g. , by the bva or the a * algorithm ) is not included in this row and must also be accounted for , which is why no path metrics are listed for prc .",
    "compared to tb rova , due to combining computations into a single pass through the trellis , the complexity of prc is reduced by approximately a factor of 2 .",
    "this is because tb rova calculates a candidate path probability for each of the @xmath6 starting states ( due to decoding to the ml codeword each time ) , whereas the combined trellis - processing of prc involves only one candidate path .",
    "both algorithms compute @xmath6 overall path probabilities , so the ratio of complexity is roughly @xmath190 .    the reduction in complexity of tb sea compared to prc is modest , with slightly fewer multiplications and divisions required due to the absence of the candidate path calculations in tb sea .",
    "importantly , performing tb sea followed by rova@xmath73 for the ml state @xmath48 is shown to be an improvement over tb rova for moderately large @xmath1 .",
    "tb sea + rova@xmath73 requires approximately one half the additions , one third the multiplications , and one half the divisions of tb rova .",
    "tb sea s complexity reduction is partly due to the fact that it does not require the add - compare - select operations of the viterbi algorithm , which tb rova performs for each starting state .",
    "note also that the number of trellis segments processed in tb sea is constant ( @xmath7 segments ) , whereas the number of trellis segments processed by many tail - biting decoders ( e.g. , the bva ) depends on the snr .",
    "lastly , the computational costs of performing fricke and hoeher s simplified rova @xcite are listed in the approx rova@xmath38 row , along with the tail - biting approximate version of sec .",
    "[ sec : simplified ] ( approx tb rova ) . in both of these cases ,",
    "the word - error outputs are estimates . in contrast , tb rova and tb sea + rova@xmath73 compute the exact word - error probability of the received word .    for the special case of rate-@xmath191 binary convolutional codes with @xmath192 memory elements , fig .",
    "[ fig : complexity ] gives an example of the number of additions , multiplications , and divisions that must be performed per trellis segment for the three tail - biting decoders in table  [ tbl : complexity ] .",
    "tb sea + rova@xmath73 is competitive with approx tb rova in terms of the number of multiplications and divisions that must be performed , but approx tb rova requires fewer additions than tb sea + rova@xmath73 .",
    ".generator polynomials @xmath193 , @xmath194 , and @xmath195 corresponding to the simulated rate-@xmath126/@xmath196 tail - biting convolutional code .",
    "@xmath197 is the free distance , @xmath198 is the number of nearest neighbors with weight @xmath197 , and @xmath199 is the analytic traceback depth . [",
    "cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]     [ tbl : conv_codes ]    table [ tbl : conv_codes ] lists the rate-@xmath126/@xmath196 , binary convolutional encoder polynomials from lin and costello ( * ? ? ?",
    "* table 12.1 ) used in the simulations .",
    "the number of memory elements is @xmath1 , @xmath200 is the number of states , and @xmath201 are the generator polynomials in octal notation .",
    "the code selected has the optimum free distance @xmath197 , which is listed along with the analytic traceback depth @xmath199 @xcite .",
    "@xmath198 is the number of nearest neighbors with weight @xmath197 . the simulations in this section use a feedforward encoder realization of the generator polynomial .      for antipodal signaling ( i.e. , bpsk ) over the gaussian channel",
    ", the conditional density @xmath202 can be expressed as    @xmath203 ^ 2 } { 2 \\sigma^2 } \\right \\},\\end{aligned}\\ ] ]    where @xmath204 is the @xmath105th received bpsk symbol in trellis - segment @xmath80 , @xmath205 is the @xmath105th output symbol of the encoder branch from state @xmath21 to state @xmath83 in trellis segment @xmath80 , and @xmath206 is the noise variance . for a transmitter power constraint @xmath207 ,",
    "the encoder output symbols are @xmath208 and the energy per bit is @xmath209 .",
    "this yields an snr equal to @xmath210 when the noise variance is @xmath211 .",
    "this section provides a comparison of the average word - error probability computed by the tail - biting reliability - output algorithms for the awgn channel using the rate-@xmath212 , @xmath213-state tail - biting convolutional code listed in table  [ tbl : conv_codes ] .",
    "the simulations in fig .",
    "[ fig : tb_rova_awgn ] use @xmath214 input bits and @xmath215 output bits .",
    "the ` actual ' curves in the figures show the fraction of codewords that are decoded incorrectly , whereas the ` computed ' curves show the word - error probability computed by the receiver .",
    "` actual ' values are only plotted for simulations with more than 100 codewords in error .    fig .",
    "[ fig : tb_rova_awgn ] evaluates the performance of sec .  [",
    "sec : straightforward_algo ] s tb rova and sec .",
    "[ sec : post - proc_rova ] s prc . in the figure",
    ", prc is applied to the output of the bidirectional viterbi algorithm ( bva ) , a suboptimal tail - biting decoder .",
    "the ` actual ' word - error performance of the suboptimal ` bva ' is slightly worse than that of the ml ` exact tb rova ' , but the difference is not visible in fig .",
    "however , even though the bidirectional viterbi decoder may choose a codeword other than the ml codeword , the posterior probability @xmath216 computed by prc is exact .",
    "thus , prc provides reliability information about the decoded word that the receiver can use as retransmission criteria in a hybrid arq setting .",
    "[ fig : tb_rova_awgn ] also shows the performance of the combined tb sea + rova@xmath73 approach in comparison with tb rova . as shown in thm .",
    "[ thm : mlstates ] , the word - error probability calculated by the computationally efficient tb sea + rova@xmath38 is identical to that of tb rova , except when the probability of error is extremely high ( i.e. , when @xmath217 ) . even in the high - error regime",
    ", however , the difference is negligible .    the performance of the exact and approximate versions of tb rova is compared in fig .  [ fig : tb_rova_awgn ] .",
    "for each starting state @xmath2 , the ` exact tb rova ' uses raghavan and baum s rova @xcite and the ` approx .",
    "tb rova ' uses fricke and hoeher s simplified rova @xcite , as described in sec .",
    "[ sec : simplified ] .",
    "the approximate approach results in an estimated word - error probability that is very close to the exact word - error probability .",
    "both reliability computations invoke the same decoder , the tail - biting viterbi algorithm ( ` tb va ' ) , so the ` actual ' curves are identical .",
    "finally , fig .",
    "[ fig : tb_rova_awgn ] also shows that when the forward recursion of the ` tb bcjr ' of @xcite is used to estimate the starting / ending state , there is a severe word - error penalty for disregarding the tail - biting restriction , as discussed in sec .",
    "[ sec : tbbcjr ] .",
    "the ` tb bcjr ' simulations used one iteration through @xmath218 trellis segments .",
    "simulations with additional loops around the circular trellis did not improve the actual word - error probability , since the tail - biting condition was not enforced .",
    "care should be taken when estimating the starting - state probability @xmath219 based on observations of @xmath77 in multiple trellis - loops .",
    "[ fig : tb_rova_histogram ] provides a histogram of the word - error probabilities computed by the receiver for the rate-@xmath212 , @xmath213-state convolutional code listed in table  [ tbl : conv_codes ] , with @xmath220 input bits , @xmath221 output bits and snr 0 db ( @xmath222 db ) .",
    "[ fig : tb_rova_histogram ] illustrates that the exact and approximate tb rova approaches give very similar word - error probabilities , whereas the word - error probabilities computed by the tail - biting bcjr followed by rova@xmath73 differ significantly .",
    "the difference in the histogram for the tb bcjr is due to poorer decoder performance .",
    "frequent errors in the state - estimation portion of the tail - biting bcjr cause the word - error probability to be high .",
    "we have extended the reliability - output viterbi algorithm to accommodate tail - biting codes , providing several tail - biting reliability - output decoders .",
    "tb rova invokes raghavan and baum s rova for each possible starting state @xmath2 , and then computes the posterior probability of the ml starting state , @xmath74 , in order to compute the overall word - error probability .",
    "we then demonstrated an approximate version of tb rova using fricke and hoeher s simplified rova .",
    "we introduced the post - decoding reliability computation , which calculates the word - error probability of a decoded word , and the tail - biting state - estimation algorithm , which first computes the map starting state @xmath48 and then decodes based on that starting state with rova@xmath73 .",
    "a complexity analysis shows that tb sea followed by rova@xmath73 reduces the number of operations by approximately half compared to tb rova .",
    "importantly , theorem 1 proved that the word - error probability computed by tb sea + rova@xmath73 is the same as that computed by tb rova in snr ranges of practical interest . because of this , tb sea is a suitable tail - biting decoder to use in reliability - based retransmission schemes ( i.e. , hybrid arq ) , being an alternative to approx tb rova ."
  ],
  "abstract_text": [
    "<S> we present extensions to raghavan and baum s reliability - output viterbi algorithm ( rova ) to accommodate tail - biting convolutional codes . </S>",
    "<S> these tail - biting reliability - output algorithms compute the exact word - error probability of the decoded codeword after first calculating the posterior probability of the decoded tail - biting codeword s starting state . </S>",
    "<S> one approach employs a state - estimation algorithm that selects the maximum a posteriori state based on the posterior distribution of the starting states . </S>",
    "<S> another approach is an approximation to the exact tail - biting rova that estimates the word - error probability . </S>",
    "<S> a comparison of the computational complexity of each approach is discussed in detail . </S>",
    "<S> the presented reliability - output algorithms apply to both feedforward and feedback tail - biting convolutional encoders . </S>",
    "<S> these tail - biting reliability - output algorithms are suitable for use in reliability - based retransmission schemes with short blocklengths , in which terminated convolutional codes would introduce rate loss . </S>"
  ]
}