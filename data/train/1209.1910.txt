{
  "article_text": [
    "the eigenvalue decomposition of a symmetric matrix , i.e. , a decomposition into a product of matrices consisting of eigenvectors and eigenvalues , is one of the most important operations in linear algebra .",
    "it is used in vibrational analysis , image processing , data searches , etc .",
    "let us note that the eigenvalue decomposition of real symmetric matrices is reduced to that of real symmetric tri - diagonal matrices .",
    "owing to recent improvements in the performance of computers equipped with multicore processors , we have had more opportunities to perform computation on parallel computers . as a result",
    ", there has been an increase in demand for an eigenvalue decomposition algorithm that can be effectively parallelized .",
    "the inverse iteration algorithm is an algorithm for computing eigenvectors independently associated with mutually distinct eigenvalues .",
    "however , when we use this algorithm , we must reorthogonalize the eigenvectors if some eigenvalues are very close to each other . adding this reorthogonalization process increases the computational cost . for this reorthogonalization",
    ", we have generally used the mgs ( modified gram - schmidt ) algorithm . however , this algorithm is sequential and inefficient for parallel computing . as a result , we are unable to maximize the performance of parallel computers .",
    "hereinafter , we will refer to the inverse iteration algorithm with mgs as the classical inverse iteration .",
    "we can also orthogonalize vectors by using the householder transformation @xcite and we call this precess the householder orthogonalization algorithm . while the mgs algorithm is unstable in the sense that the orthogonality of the resulting vectors crucially depends on the condition number of the matrix @xcite , the householder algorithm is stable because its orthogonality does not depend on the condition number .",
    "the householder algorithm is also sequential and ineffective for parallel computing , and its computational cost is higher than that of mgs .    in 1989 ,",
    "the householder orthogonalization in terms of the compact wy representation was proposed by r. schreiber _ et al _ @xcite . by adopting this orthogonalization , stability and",
    "effective parallelization can be achieved .",
    "hereafter , we refer to this algorithm as the compact wy orthogonalization algorithm .",
    "@xcite reformulated this algorithm for an incremental orthogonalization .",
    "moreover , they showed that this algorithm achieves theoretically high accurate orthogonality and high scalability in parallel computing @xcite . here , the incremental orthogonalization is implemented on many numerical computation library .",
    "lapack(linear algebra package ) @xcite is one of the most popular libraries and all the code of lapack is implemented by using blas ( basic linear algebra subroutines ) operations .",
    "the compact wy orthogonalization algorithm can be implemented by using blas .    in @xcite",
    ", authors have implemented the compact wy orthogonalization to the reorthogonalization process of inverse iteration for computing eigenvectors of a tri - diagonal matrix .",
    "it is shown @xcite that , in parallel computing , the new inverse iteration algorithm is faster than the classical one .    in this paper , we present two implementations :",
    "one is a new implementation of the compact wy orthogonalization algorithm based on blas .",
    "we focus on a mathematical structure of this algorithm and reformulate this algorithm . therefore , using this new implementation , the computational cost of the compact wy orthogonalization can be reduced .",
    "the other is an implementation of the compact wy orthogonalization to the inverse iteration algorithm for a real symmetric tri - diagonal matrix .",
    "thereafter , we perform the numerical experiments by computing all the eigenvectors using the second implementation and evaluate its performance .",
    "we consider the problem of computing eigenvectors of a real symmetric tri - diagonal matrix @xmath0 .",
    "let @xmath1 be eigenvalues of @xmath2 such that @xmath3 .",
    "let @xmath4 be the eigenvector associated with @xmath5 .",
    "when @xmath6 , an approximate value of @xmath5 , and a starting vector @xmath7 are given , we can compute an eigenvectors of @xmath2 . to this end",
    ", we solve the following equation iteratively : @xmath8 here @xmath9 is the @xmath10-dimensional identity matrix .",
    "if the eigenvalues of @xmath2 are mutually well - separated , @xmath11 , the solution of eq . , generically converges to the eigenvector associated with @xmath5 as @xmath12 goes to @xmath13 .",
    "the above iteration method is the inverse iteration .",
    "the computational cost of this method is of @xmath14 when we compute @xmath15 eigenvectors . in the implementation",
    ", we have to normalize the vectors @xmath11 to avoid overflow .",
    "when some of the eigenvalues are close to each other or there are clusters of eigenvalues of @xmath2 , we have to reorthogonalize all the eigenvectors associated with such eigenvalues because they need to be orthogonal to each other . in the classical inverse iteration",
    ", we apply the mgs to this process and the computational cost of it is of @xmath16 . therefore , when we compute eigenvectors of the matrix that has many clustered eigenvalues , the total computational cost increases significantly .",
    "in addition , the classical inverse iteration is implemented the peters - wilkinson method @xcite . in this method , when the distance between the close eigenvalues is less than @xmath17 , we regard them as members of the same cluster of eigenvalues , and we orthogonalize all of the eigenvectors associated with these eigenvalues .",
    "the classical inverse iteration algorithm is shown by alg.[alg : ii ] , and @xmath18 denotes the index of the minimum eigenvalue of some cluster .",
    "this algorithm is implemented as dstein in lapack @xcite .",
    "generate @xmath7 from random numbers . @xmath19 .",
    "@xmath20 . normalize @xmath21 .",
    "solve @xmath22 ( eq . ) .",
    "@xmath23 , @xmath24 @xmath25 . some condition is met .",
    "normalize @xmath11 to @xmath26 .",
    "the inverse iteration is a prominent method for computing eigenvectors , because we can compute eigenvectors independently .",
    "when there are many clusters in the distribution of eigenvalues , the inverse iteration can be parallelized by assigning each cluster to each core .",
    "let us consider the peters - wilkinson method in the classical inverse iteration .",
    "when the dimension of @xmath2 is greater than 1000 , most of the eigenvalues are regarded as being in the same cluster @xcite . in this case , we have to parallelize the inverse iteration with respect to not the cluster but the loop described from lines 2 to 16 in alg.[alg : ii ] .",
    "this loop includes the iteration based on eq . and the orthogonalization of the eigenvectors .",
    "this orthogonalization process becomes a bottleneck of the classical inverse iteration with respect to the computational cost .",
    "the mgs algorithm is mainly based on a blas level-1 operation and it is a sequential algorithm . because of this , when we compute all the eigenvectors on parallel computers , the number of synchronizations is of @xmath27 . therefore , the mgs algorithm is ineffective in parallel computing .    in conclusion ,",
    "the classical inverse iteration is an ineffective algorithm for parallel computing because the mgs algorithm is used in its orthogonalization process .",
    "in this section , we introduce alternative orthogonalization algorithms instead of the mgs algorithm .",
    "now , we discuss the incremental orthogonalization of @xmath4 to @xmath28 ( @xmath29 , @xmath30 , @xmath15 , @xmath31 ) .",
    "the incremental orthogonalization arises in the reorthogonalization process on the inverse iteration and it is defined as follows : @xmath26 ( @xmath32 ) is not given in advance but is computed from @xmath33 , @xmath30 , @xmath34 .    in the following ,",
    "let us define a vector @xmath35 as the @xmath36-dimensional zero vector and matrices @xmath37 , @xmath38 as @xmath39 $ ] , @xmath40 $ ] .",
    "@xmath41 @xmath42 compute @xmath43 and @xmath44 by using @xmath45 @xmath46 @xmath47    the householder orthogonalization , based on the householder matrices , is one of the alternative orthogonalization methods .",
    "when vectors @xmath45 , @xmath48 ( @xmath29 , @xmath30 , @xmath15 ) satisfy @xmath49 , there exists the orthogonal matrices @xmath50 called the householder matrices satisfying @xmath51 , @xmath52 defined by @xmath53 , @xmath54 , @xmath55 .",
    "the transformation from @xmath45 to @xmath26 by @xmath50 is called the householder transformation . by using the householder transformations .",
    "this orthogonalization algorithm is shown in alg.[alg : house ] .",
    "the vector @xmath43 is the vector in which the elements from 1 to @xmath56 are the same as the elements of @xmath45 and the elements from @xmath57 to @xmath10 are zero .",
    "the vectors @xmath45 and @xmath58 are defined as follows : @xmath59 where @xmath60 ( @xmath61 , @xmath30 , @xmath10 ) is the @xmath36-th element of @xmath45 and @xmath62 here , @xmath43 and @xmath44 are computed as follows : @xmath63 the vector @xmath64 in alg.[alg : house ] is the @xmath65-th vector of an @xmath10-dimensional identity matrix .",
    "the orthogonality of the vectors @xmath66 generated by the householder orthogonalization does not depend on the condition number of @xmath37 .",
    "therefore , the householder orthogonalization is more stable than mgs . on the other hand ,",
    "being similar to mgs , it is a sequential algorithm , that is mainly based on a blas level-1 operation .",
    "its computational cost is about twice higher than that of mgs .",
    "thus the householder orthogonalization is an ineffective algorithm for parallel computing .      in 1989 ,",
    "the householder orthogonalization in terms of the compact wy representation was proposed by schreiber and van loan @xcite .",
    "yamamoto and hirota @xcite reformulated this algorithm for the incremental orthogonalization .",
    "this study suggests that the householder orthogonalization becomes capable of computation with a blas level-2 operation in terms of the compact wy representation .",
    "they also showed that this algorithm achieved theoretically high orthogonality and high scalability in parallel computing @xcite .",
    "now , we consider the householder orthogonalization in alg.[alg : house ] and we introduce the compact wy representation .",
    "first , we define @xmath67 \\in \\mathbb{r}^{n \\times 1}$ ] and @xmath68 \\in \\mathbb{r}^{1 \\times 1}$ ] .",
    "let us define matrices @xmath69 and upper triangular matrices @xmath70 recursively as follows : @xmath71 in this case , the following equation holds @xmath72 as shown in eq . , we can rewrite the product of the householder matrices @xmath73 in a simple block matrix form . here",
    "@xmath74 is called the compact wy representation of the product @xmath73 of the householder matrices .",
    "alg.[alg : cwy ] shows the compact wy orthogonalization algorithm .",
    "compute @xmath75 and @xmath76 by using @xmath77 @xmath78 $ ] , @xmath79 $ ] @xmath80 @xmath81 compute @xmath43 and @xmath44 by using @xmath45 @xmath82 , @xmath83 .",
    "@xmath84      in this subsection , we discuss the implementation of the compact wy orthogonalization algorithm using blas operations .",
    "in addition , we discuss a mathematical structure of this algorithm and present a new implementation of the compact wy orthogonalization for reducing the computational cost and the usage of memory .",
    "now we discuss the implementation of the compact wy orthogonalization based on line @xmath85 to @xmath86 in alg.[alg : cwy ] using blas operations .    for the adaptation of blas operations , we have to reformulate the formula of line @xmath85 as follows : @xmath87 now we can implement this formula by using blas as follows : @xmath88 where @xmath89 .",
    "we set the initial address of @xmath90 assigned on cpu memory to correspond to that of @xmath26 .",
    "dcopy denotes the copying operation of a vector @xmath91 to a vector @xmath92 : @xmath93 .",
    "dgemv means the matrix - vector operation : @xmath94 , where @xmath95 is a general rectangular matrix .",
    "dtrmv denotes the matrix - vector product : @xmath96 , where @xmath2 is a triangular matrix .",
    "next , on line 6 , we compute @xmath43 and @xmath44 based on eq .. these computations is mainly performed by using blas level-1 operations and its computational cost is relatively lower .",
    "we implement the computation of @xmath43 and @xmath44 as follows : @xmath97 where @xmath98 ( @xmath61 ,  , @xmath10 ) is the @xmath36-th column element of @xmath43 .",
    "dnrm2 denotes the computation of the @xmath99-norm of a vector .",
    "on line 7 , updating @xmath100 and @xmath44 can be done easily .",
    "now , let @xmath101 be @xmath102 .",
    "note that @xmath103 is implemented by using blas as follows : @xmath104    at last , on line 8 , we can reformulate as follows : @xmath105 here , the matrix - vector product @xmath106 can be simplified as follows : @xmath107 .",
    "this computation can be performed only by copying the @xmath65-th column of @xmath100 to some vector .",
    "therefore we can implement the formula of line 8 using blas as follows : @xmath108 where @xmath109 , @xmath28 .",
    "we set the initial address of @xmath110 , @xmath66 assigned on cpu memory to correspond to that of @xmath45 , @xmath26 , respectively .",
    "the computational cost of the above compact wy orthogonalization algorithm is almost @xmath111 . in the worst case ,",
    "i.e. , @xmath112 , the computational cost is @xmath113 .",
    "in addition , for this implementation , we have to use almost @xmath114 cpu memory because @xmath115 use @xmath116 and @xmath117 use @xmath118 domain .      in the above section",
    ", we discuss the ordinary implementation of the compact wy orthogonalization algorithm .",
    "now we focus on the mathematical structure of this algorithm and present the new implementation of the compact wy orthogonalization which has the less computational cost than the ordinary one has .    before the formula of line 5 in alg.[alg : cwy ]",
    ", let us consider the formula of line 6 . from eq .",
    ", we can strictly compute @xmath44 as follows : since @xmath119 we have @xmath120 hence , we have @xmath121 from this fact and the definition of @xmath43 and @xmath122 , we need not compute the elements from @xmath123 to @xmath56 of @xmath45 in actual . therefore we compute only the elements from @xmath65 to @xmath10 of @xmath45 so that the formula of line 5 is reduced as follows : @xmath124 where @xmath125 is @xmath126 .    here",
    ", we focus on the structure of @xmath43 . from eq .",
    ", @xmath43 ( @xmath127 , @xmath30 , @xmath15 ) can be represented as the block vector of the form : @xmath128 where @xmath129 is the vector of nonzero elements of @xmath43 . from this fact",
    ", @xmath100 can be represented as the following block matrix : @xmath130 where @xmath131 is a lower triangular matrix and @xmath132 is generally a dense rectangular matrix .",
    "in addition , let us consider @xmath26 as the block vector of the form : @xmath133 where @xmath134 , @xmath135 .    by using these block form of @xmath26 and @xmath100",
    ", we can reduce the computational cost of the matrix - vector product @xmath136 through @xmath137 therefore , the formula of @xmath138 can be simplified as follows : @xmath139 this formula can be implemented by using blas as follows : @xmath140    from the above discussion , the computation on line 6 is implemented by using blas as follows : @xmath141    on line 7 , we can also reduce the computational cost of @xmath103 through @xmath142 this formula can be implemented by using blas as follows : @xmath143    at last , on line 8 , even if the sign of the orthogonal vector @xmath66 is reversed , the orthogonality along with other vectors is not changed . therefore , we can reformulate @xmath66 as @xmath144 .",
    "in addition , let us consider @xmath66 as the following block vector : @xmath145 where @xmath146 , @xmath147 .",
    "these are reformulated as follows : @xmath148 where @xmath149 is the @xmath65-th vector of the @xmath65-dimensional identity matrix .",
    "therefore this formula can be implemented by using blas as follows : @xmath150 where @xmath151 is assigned on workspace memory .    when the above implementation is adapted , the highest order of the computational cost of the compact wy algorithm reduced to @xmath152 . in the worst case , i.e. , @xmath112 , the computational cost of the new implementation of the compact wy algorithm is almost @xmath153 .",
    "in addition , our implementation have not to be referred any zero elements of @xmath100 and @xmath154 . therefore ,",
    "if @xmath100 and @xmath154 are assigned on a cpu memory like alg.[figure : assignment ] , the use of memory can be reduced to almost @xmath155 ,     and @xmath154 ]      the compact wy orthogonalization has a stable orthogonality arising from the householder transformations , and its numerical computation is mainly performed by blas level-2 operations . as a result , this orthogonalization has a better stability and a sophisticated orthogonality , and it is more effective for parallel computing than mgs .",
    "table [ table : perform ] displays the differences in performance of the orthogonalization methods mentioned above . in this table , _ computation _ denotes the order of the computational cost . _ synchronization _ means the order of the number of synchronizations . _ orthogonality _ indicates the norm @xmath157 and @xmath158 denotes the machine epsilon and @xmath159 is the condition number of @xmath37 .",
    ".comparison of the orthogonalization methods @xcite @xcite [ cols=\">,^,^,^\",options=\"header \" , ]     table [ table:3]-[table:8 ] show the results of the experiments on computer 1 and 2 that are mentioned in the previous section , in tables , @xmath10 is the dimension of the experimental matrices , @xmath160 and @xmath161 are computation time by dstein and dstein - cwy , respectively .",
    "in addition , fig .",
    "[ graph:1]-[graph:3 ] illustrate the results in tables [ table:3 ] and [ table:4 ] , [ table:5 ] and [ table:6 ] , [ table:7 ] and [ table:8 ] through graphs , respectively . in fig .",
    "[ graph:1]-[graph:3 ] , the dotted line corresponds to @xmath160 and the straight line to @xmath161 .",
    "it is noted that dstein - cwy is faster than dstein for any cases of the all types matrices , without the cases of type 1 matrix for @xmath162 .",
    "we see that the change from mgs to the compact wy orthogonalization on the dstein code in parallel computing results in a significant reduction of computation time .",
    "we introduce a barometer @xmath163 of the reduction effect by using the program dstein - cwy which depends on @xmath10 , the dimension of the experimental matrix . on computer 1 , the maximum value of @xmath164 is @xmath165 for @xmath166 of type 1 , @xmath167 for @xmath168 of type 2 , and @xmath169 for @xmath170 of type 3 . on computer 2 ,",
    "@xmath171 for @xmath172 of type 1 , @xmath173 for @xmath174 of type 2 , and @xmath175 for @xmath176 of type 3 . considering these facts ,",
    "even if the dimension of the experimental matrices is larger than that in these examples , we can not expect that the computation time can be further shortened by using dstein - cwy .",
    "it is shown that dstein - cwy is faster than dstein for any dimension @xmath10 of the experimental matrix both on computers 1 and 2 . as mentioned earlier , according to the theoretical background in section 3.3 ,",
    "this result shows that the compact wy orthogonalization is an effective algorithm for parallel computing .",
    "the cause of this is related to the time required for floating - point arithmetic and for synchronization in parallel computing .",
    "the floating - point computation time increases with increasing the dimension @xmath10 of matrices . in comparison ,",
    "the synchronization cost does not change significantly even if @xmath10 becomes larger .",
    "therefore , in parallel computing , dstein , which contains mgs ( for which the number of synchronizations is large ) , creates a huge bottleneck for the synchronization cost when @xmath10 is small .",
    "this bottleneck gradually becomes less when @xmath10 is larger .",
    "however , dstein - cwy has a smaller bottleneck for the synchronization cost because the compact wy orthogonalization requires less synchronization , and the floating - point computation time becomes greater than that of dstein .",
    "this reduction effect can be seen in table [ table:3]-[table:8 ] .",
    "in this study , we present a new inverse iteration algorithm for computing all the eigenvectors of a real symmetric tri - diagonal matrix . the new algorithm is equipped with the new implementation of the compact wy orthogonalization algorithm , established in this paper , in the orthogonalization process .",
    "we have given numerical experiments for computing eigenvectors of certain real symmetric tri - diagonal matrices that have many clusters with several thousand dimensions by using two types of inverse iteration algorithms on parallel computers .",
    "the results show that the compact wy inverse iteration is more efficient than the classical one owing to the reduction in computation time because of the parallelization efficiency . as the number of cores of the cpu increases , the parallelization efficiency increases .",
    "it may be expected to apply the new inverse iteration algorithms to other types of matrix eigenvector problem , such as eigenvectors of a real symmetric band matrix , or singular vectors of a bidiagonal matrix .",
    "99 j. w. demmel , l. grigori , m. hoemmen and j. langou , _ communication - optimal parallel and sequential qr and lu factorizations _ , lapack working notes , no.204 , 2008 . j. w. demmel , o. a. marques , b. n. parlett , and c. vmel , _ performance and accuracy of lapack s symmetric tridiagonal eigensolvers _ , siam j. sci .",
    "3 , pp . 1508 - 1526 , 2008 .",
    "i. s. dhillon , _ a new o(@xmath177 ) algorithm for the symmetric tridiagonal eigenvalue / eigenvector problem _ , ph.d .",
    "thesis , computer science division , university of california , berkeley , california , uc berkeley technical report ucb//csd-97 - 971 , 1997 .",
    "i. s. dhillon , b. n. parlett , and c. vmel , _ glued matrices and the mrrr algorithm _",
    ", siam j. sci .",
    "496 - 510 , 2005 .",
    "gotoblas2 , + http://www.tacc.utexas.edu / tacc - projects / gotoblas2/. h. ishigami , k. kimura and y. nakamura , _ implementation and performance evaluation of new inverse iteration algorithm with householder transformation in terms of the compact wy representation _ , proc . of the 2011 international conference on parallel and distributed processing techniques and applications ( pdpta2011 ) , vol .",
    "ii , pp . 775 - 780 , 2011 .",
    "lapack , http://www.netlib.org / lapack/. g. peters and j. wilkinson , _ the calculation of specified eigenvectors by inverse iteration _ , contribution ii/18 , in linear algebra , handbook for automatic computation , vol .",
    "ii , springer - verlag , berlin , pp . 418 - 439 , 1971 .",
    "r. schreiber and c. van loan , _ a storage - efficient wy representation for products of householder transformations _",
    ", siam j. sci .",
    "53 - 57 , 1989 .",
    "h. walker , _ implementation of the gmres method using householder transformations _ , siam j. sci .",
    ", vol . 9 , no .",
    "152 - 163 , 1988 .",
    "y. yamamoto and y. hirota , _ a parallel algorithm for incremental orthogonalization based on the compact wy representation _",
    ", jsiam letters , vol .",
    "89 - 92 , 2011 ."
  ],
  "abstract_text": [
    "<S> a new inverse iteration algorithm that can be used to compute all the eigenvectors of a real symmetric tri - diagonal matrix on parallel computers is developed . </S>",
    "<S> the modified gram - schmidt orthogonalization is used in the classical inverse iteration . </S>",
    "<S> this algorithm is sequential and causes a bottleneck in parallel computing . in this paper , </S>",
    "<S> the use of the compact wy representation is proposed in the orthogonalization process of the inverse iteration with the householder transformation . </S>",
    "<S> this change results in drastically reduced synchronization cost in parallel computing . </S>",
    "<S> the new algorithm is evaluated on both an 8-core and a 32-core parallel computer , and it is shown that the new algorithm is greatly faster than the classical inverse iteration algorithm in computing all the eigenvectors of matrices with several thousand dimensions .    </S>",
    "<S> inverse iteration , orthogonalization , compact wy representation , eigenvalue problem , parallelization , householder transformation </S>"
  ]
}