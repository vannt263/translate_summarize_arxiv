{
  "article_text": [
    "deep neural networks ( dnn ) have been widely adopted in several applications such as object classification , pattern recognition and regression problems [ 1 ] .",
    "although dnns achieve high performance in many applications , this comes at the expense of a large number of arithmetic and memory access operations for both training and testing [ 2 ] .",
    "therefore , dnn accelerators are highly desired [ 3 ] .",
    "fpga - based dnn accelerators are favorable since fpga platforms support high performance , configurability , low power consumption and quick development process [ 3 ] . on the other hand ,",
    "implementing a dnn or a convolutional neural network ( cnn ) on an fpga is a challenging task since dnns and cnns require a large amount of resources [ 4 ] , [ 5 ] and [ 6 ] .",
    "dnns consist of a number of hidden layers that work in parallel , and each hidden layer has a number of artificial neurons ( an ) [ 1 ] .",
    "each neuron receives signals from other neurons and computes a weighted - sum of these inputs .",
    "then , an activation function of the an is applied on this weighted - sum .",
    "one of the main purposes of the activation function is to introduce non - linearity into the network .",
    "the hyperbolic tangent is one of the most popular non - linear activation functions in dnns [ 1 ] .",
    "realizing a precise implementation of the hyperbolic tangent activation function in hardware entails a large number of additions and multiplications [ 7 ] .",
    "this implementation would badly increase the overall resources required for implementing a single an and a fully parallel dnn .",
    "therefore , approximations with different precisions and amount of resources are generally employed [ 7 ] .",
    "we propose a new high - accuracy approximation using the discrete cosine transform interpolation filter ( dctif ) [ 8 ] .",
    "the proposed dctif approximation achieves higher accuracy than the existing approximations , and it needs fewer resources than other designs when a high precision approximation is required .",
    "we also study the effect of approximating the hyperbolic tangent activation function on the performance of training and testing dnns .",
    "the rest of the paper is organized as follows : different tanh approximations are reviewed in section 2 .",
    "the operation principle of the proposed dctif approximation is described in section 3 . in section 4 ,",
    "an implementation of the proposed dctif approximation is detailed .",
    "section 5 is dedicated to the experimental results and a comparison with other approximations and discussion .",
    "finally , section 6 concludes the paper .",
    "the hardware implementation of a dnn is always constrained by the available computational resources [ 9 ] .",
    "the required computational resources to implement a dnn can be reduced by limiting the precision of the data representation [ 9 ] . on the other hand ,",
    "using bitwise dnns is another way to reduce the computational resources of a dnn .",
    "bitwise dnn replaces floating or fixed - point arithmetic operations by efficient bitwise operations [ 10 ] .",
    "however , this comes at the expense of the training and testing performance of the dnn .",
    "another approach to meet the constraints of the available computational resources is to approximate the activation function of the dnn .",
    "the selection of the tanh approximation accuracy as an activation function is one of the aspects that define the training and testing performance of the dnns [ 11 ] .",
    "high accuracy approximations lead to high training and testing performance of the dnn , and low accuracy approximations lead to poor dnn performance [ 11 ] .",
    "there are several approaches for the hardware implementation of the hyperbolic tangent activation function based on piecewise linear ( pwl ) , piecewise non - linear , lookup table ( lut ) and hybrid methods .",
    "all of these approaches exploit that the hyperbolic tangent function , shown in figure 1 , is negatively symmetric about the y - axis .",
    "therefore , the function can be evaluated for negative inputs by negating the output values of the same corresponding positive values and vice versa .",
    "armato et al .",
    "[ 12 ] proposed to use pwl which divides the hyperbolic tangent function into segments and employs a linear approximation for each segment . on the other hand ,",
    "zhang and his colleagues [ 13 ] used a non - linear approximation for each segment .",
    "although both methods achieve precise approximations for the hyperbolic tangent function , this comes at the expense of the throughput of the hardware implementation .",
    "lut - based approximations divide the input range into sub - ranges where the output of each sub - range is stored in a lut .",
    "leboeuf et al .",
    "[ 14 ] proposed using a classical lut and a range addressable lut to approximate the function .",
    "lut - based implementations are fast but they require more resources than pwl approximations in order to achieve the same accuracy",
    ". therefore , most of the existing lut - based methods limit the approximation accuracy to the range [ 0.02 , 0.04 ] .",
    "several authors noticed that the hyperbolic tangent function can be divided into three regions a ) pass region , b ) processing region ( pr ) and c ) saturation region , as shown in figure 1 .",
    "the hyperbolic tangent function behaves almost like the identity function in the pass region , and its value is close to 1 in the saturation region .",
    "some hybrid methods that combine luts and computations were used to approximate the non - linear pr .",
    "namin and his colleagues [ 15 ] proposed to apply a pwl algorithm for the pr . on the other hand , meher et al . [ 16 ] proposed to divide the input range of the pr into sub - ranges , and they implemented a decoder that takes the input value and selects which value should appear on the output port .",
    "finally , zamanloony et al .",
    "[ 7 ] introduced a mathematical analysis that defines the boundaries of the pass , processing and saturation regions of the hyperbolic tangent function based on the desired maximum error of the approximation .",
    "generally , activation function approximations with high error badly affect the performance of dnns in terms of their training and testing accuracies .",
    "approximations with higher accuracies are favorable in order to maintain the same learning capabilities and testing results compared to the exact activation function . therefore , we propose a high precision approximation of the hyperbolic tangent activation function while using a small amount of computational resources .",
    "the dct - based interpolation filter ( dctif ) interpolates data points from a number of samples of a function [ 6 ] .",
    "it was firstly introduced for interpolating fractional pixels from integer pixels in the motion compensation process of the latest video coding standard h.265 [ 6 ] .",
    "dctif can be used to approximate several non - linear functions .",
    "it interpolates values with a desired accuracy by controlling the number of samples involved in the interpolation process and the number of interpolated points between two samples .",
    "we propose to use dctif in order to approximate the hyperbolic activation function in dnns .",
    "the dct transformation used to generate dctif coefficients is defined by equation 1 , where _ l~max~ _ and _ l~min~ _ define the range of the given sample points used in the interpolation process , _ size _ is defined as ( _ l~max~ _ - _ l~min~ _ + _ 1 _ ) and the center position of a given size is _ center _ = ( _ l~max~ _ + _",
    "l~min~_)/_2_. by substituting equation 1 into the inverse dct formula defined in equation 2 , we get the dctif co - efficients generation formula for position _",
    "i+r@xmath0 _ as in equation 3 .",
    "as shown in figure 2 , let s assume that \\{_p~2m~ _ } denotes a set of _ 2 m _ given sample points ( no . of dctif filter s tabs ) used to interpolate _",
    "p~i+r@xmath0~ _ at fractional position _ i+r@xmath0 _ between two adjacent samples at positions",
    "_ i _ and _ i+1 _ of the function _ x(n)_. the parameter _",
    "@xmath0 _ is a positive fractional number that is equal to ( 1/2^j^ ) where _ j _ is the number of interpolated points between two sample points .",
    "the parameter _ r _ is a positive integer that represents the position of the interpolated point between two sample points where it is @xmath1 [ 1 , 2^j^-1 ] .",
    "a fractional position value _ p~i+r@xmath0~ _ is interpolated using an even number of samples when _",
    "r@xmath0 _ is equal to 1/2 , which means that the interpolated point is exactly between two adjacent samples . otherwise , _",
    "p~i+r@xmath0~ _ is interpolated using an odd number of samples since the interpolated point is closer to one of the samples than the other .",
    "therefore , equation 3 is modified to generate the dctif co - efficients for even and odd numbers of tabs as in equations 4 and 5 , respectively .",
    "the dctif co - efficients can be smoothed using a smoothing window of size _ w _ [ 8 ] . for hardware implementation , the smoothed co",
    "- efficients are scaled by a factor of ( 2^s^ ) and rounded to integers , where _ s _ is a positive integer value .",
    "in addition , the scaled co - efficients should be normalized which means that their summation is equal to 2^s^. consequently , equation 6 defines the final dctif co - efficients .",
    "@xmath2    @xmath3    @xmath4    @xmath5    @xmath6    @xmath7    table 1 shows the generated dctif co - efficient values using different numbers of dctif tabs , _",
    "r@xmath0 _ values and scaling factors by substituting in equation 6 .",
    "the co - efficient values exihibit similarity among some _",
    "r@xmath0 _ positions .",
    "for example , the _ i+1/4 _ and _ i+3/4 _ positions have the same set of co - efficient values . moreover , at the _",
    "i+1/2 _ position , the set of co - efficients is symmetric about the center element .",
    "these properties can be exploited to reduce the implementation cost .",
    "a dctif approximation error analysis is presented in figure 3 .",
    "it can be seen that the dctif approximation error increases for small _ @xmath0 _ values .",
    "although a large _ @xmath0 _ value means that fewer points need to be interpolated , this comes at the expense of memory resources since more samples must be stored .",
    "a large value of _ s _ increases the accuracy of the approximation , but increases complexity as well because the interpolation coefficients take larger values , potentially expressed with more signed digits as shown in table 1 .",
    "moreover , using more dctif tabs comes at the expense of the computational resources as shown in table 2 .",
    "the proposed dctif approximation divides the input range of the hyperbolic tangent function into pass , processing and saturation regions as shown in figure 1 .",
    "the boundaries of these regions are computed based on the targeted maximum error of the approximation [ 7 ] .",
    "the output is equal to the input when the input is in the pass region .",
    "the proposed dctif approximation is utilized for the inputs in the processing region . in the saturation region , all the bits of the output port are set to one which represents the maximum value of the output signal .    _ value and the scaling parameter",
    "_ s _ ]        [ cols=\"^ \" , ]      + & 0.04 & 0.43279 & & 0.04 & 10.7 + & 0.02 & 0.78250 & & 0.02 & 16.4 + & 0.01 & 0.78976 & & 0.01 & 23.1 + & 0.001 & 0.84850 & & 0.001 & 31.1 + & 0.0001 & 0.87712 & & 0.0001 & 68.0 + & 0 & 0.90287 & & 0 & 68.1 + & 0.04 & 0.77945 & & 0.04 & 86.1 + & 0.02 & 0.80033 & & 0.02 & 86.9 + & 0.01 & 0.80068 & & 0.01 & 86.9 + & 0.001 & 0.84581 & & 0.001 & 86.9 + & 0.0001 & 0.85014 & & 0.0001 & 94 .. 1 + & 0 & 0.86097 & & 0 & 94.1 +",
    "the authors would like to thank ahmed el - sheikh , awny m. el - mohandes and hamza bendaoudi for their insightful comments on our work .",
    "d. hunter , h. yu , m. s. pukish , j. kolbusz , and b. m. wilamowski , `` selection of proper neural network sizes and architectures - a comparative study , '' in ieee transactions on industrial informatics , vol .",
    "228 - 240 , 2012 .",
    "s. himavathi , d. anitha , and a. muthuramalingam , `` feedforward neural network implementation in fpga using layer multiplexing for effective resource utilization , '' in ieee transactions on neural networks , vol .",
    "18 , no .3 , pp .",
    "880 - 888 , 2007 .",
    "j. qiu , j. wang , s. yao , k. guo , b. li , e. zhou , j. yu , t. tang , n. xu , s. song , and y. wang , `` going deeper with embedded fpga platform for convolutional neural network , '' in proceedings of the international symposium on field - programmable gate arrays .",
    "26 - 35 , 2016 . c. zhang , p. li , g. sun , y. guan , b. xiao , and j. cong , `` optimizing fpga - based accelerator design for deep convolutional neural networks , '' in proceedings of the international symposium on field - programmable gate arrays .",
    "161 - 170 , 2015 .",
    "b. zamanlooy , and m. mirhassani , `` efficient vlsi implementation of neural networks with hyperbolic tangent activation function , '' in ieee transactions on very large scale integration ( vlsi ) systems , vol .",
    "39 - 48 , 2014 .",
    "k. ugur , a. alshin , e. alshina , f. bossen , w. j. han , and j. h. park , `` motion compensated prediction and interpolation filter design in h. 265/hevc , '' in ieee journal of selected topics in signal processing , vol . 7 , no . 6 , pp .",
    "946 - 956 , 2013 .",
    "k. basterretxea , j. m. tarela , i. del campo , and g. bosque , `` an experimental study on nonlinear function computation for neural / fuzzy hardware design , '' in ieee transactions on neural networks , vol .",
    "266 - 283 , 2007 .",
    "a. armato , l. fanucci , e. p. scilingo , and d. de rossi , `` low - error digital hardware implementation of artificial neuron activation functions and their derivative , '' in microprocessors and microsystems , vol .",
    "557 - 567 , 2011 .",
    "k. leboeuf , a. h. namin , r. muscedere , h. wu , and m. ahmadi , `` high speed vlsi implementation of the hyperbolic tangent sigmoid function , '' in convergence and hybrid information technology international conference on .",
    "ieee , vol .",
    "1 , pp . 1070 - 1073 , 2008 .",
    "a. h. namin , k. leboeuf , r. muscedere , h. wu , and m. ahmadi , `` efficient hardware implementation of the hyperbolic tangent sigmoid function , '' in international symposium on circuits and systems .",
    "ieee , pp .",
    "2117 - 2120 , 2009 ."
  ],
  "abstract_text": [
    "<S> implementing an accurate and fast activation function with low cost is a crucial aspect to the implementation of deep neural networks ( dnns ) on fpgas . we propose a high - accuracy approximation approach for the hyperbolic tangent activation function of artificial neurons in dnns . </S>",
    "<S> it is based on the discrete cosine transform interpolation filter ( dctif ) . </S>",
    "<S> the proposed architecture combines simple arithmetic operations on stored samples of the hyperbolic tangent function and on input data . </S>",
    "<S> the proposed dctif implementation achieves two orders of magnitude greater precision than previous work while using the same or fewer computational resources . </S>",
    "<S> various combinations of dctif parameters can be chosen to tradeoff the accuracy and complexity of the hyperbolic tangent function . in one case , </S>",
    "<S> the proposed architecture approximates the hyperbolic tangent activation function with 10 ^ -5^ maximum error while requiring only 1.52 kbits memory and 57 luts of a virtex-7 fpga . </S>",
    "<S> we also discuss how the activation function accuracy affects the performance of dnns in terms of their training and testing accuracies . </S>",
    "<S> we show that a high accuracy approximation can be necessary in order to maintain the same dnn training and testing performances realized by the exact function . </S>"
  ]
}