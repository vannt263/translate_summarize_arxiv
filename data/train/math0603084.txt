{
  "article_text": [
    "functional data are more and more frequently involved in statistical problems . developping statistical methods in this special framework",
    "has been popularized during the last few years , particularly with the monograph by ramsay & silverman ( 2005 ) .",
    "more recently , new developments have been carried out in order to propose nonparametric statistical methods for dealing with such functional data ( see ferraty & vieu , 2006 , for large discussion and references ) .",
    "these methods are also called doubly infinite dimensional ( see ferraty & vieu , 2003 )",
    ". indeed these methods deal with infinite - dimensional ( i.e. functional ) data and with a statistical model which depends on an infinite - dimensional unknown object ( i.e. a nonparametric model ) .",
    "this double infinite framework motivates the appellation of nonparametric functional statistics for such kind of methods .",
    "our paper is centered on the functional regression model : @xmath0 where @xmath1 is a real random variable , @xmath2 is a functional random variable ( that is , @xmath2 takes values in some possibly infinite - dimensional space ) and where the statistical model assumes only smoothness restriction on the functional operator @xmath3 . at this point , it worth noting that the operator @xmath3 is not constrained to be linear .",
    "this is a functional nonparametric regression model ( see section [ notations ] for deeper presentation ) .",
    "the aim of this paper is to extend in several directions the current knowledges about functional nonparametric regression estimates presented in section [ notations ] . in section [ mse ]",
    "we give asymptotic mean squared expansions , while in section [ asnorm ] the limiting distribution is derived .",
    "the main novelty / difficuly along the statement of these results relies on the exact calculation of the leading terms in the asymptotic expressions .",
    "section [ exsbp ] points out how such results can be used when the functional variable belongs to standard families of continuous time process .",
    "the accuracy of our asymptotic results leads to interesting perspectives from a practical point of view : minimizing mean squared errors can govern automatic bandwidth selection procedure while the limiting distribution of the error is a useful tool for building confidence bands . to this end",
    ", we propose in section [ computfeatures ] a functional version of the wild bootstrap procedure , and we use it , both on simulated and on real functional datasets , to get some automatic rule for choosing the bandwidth .",
    "the concluding section [ conc ] contains some important open questions which emerge naturally from the theoretical results given in this paper , such as the theoretical study of the accuracy of the functional wild bootstrap procedure used in our applications .",
    "the model is defined in the following way .",
    "assume that @xmath4 is a sample of @xmath5 i.i.d .",
    "pairs of random variables . the random variables @xmath6 are real and the @xmath7 s are random elements with values in a functional space @xmath8 . in all the sequel",
    "we will take for @xmath8 a separable banach space endowed with a norm @xmath9 .",
    "this setting is quite general since it contains the space of continuous functions , @xmath10 spaces as well as more complicated spaces like sobolev or besov spaces .",
    "separability avoids measurability problems for the random variables @xmath7 s .",
    "the model is classically written : @xmath11 where @xmath3 is the regression function mapping @xmath8 onto @xmath12 and the @xmath13 s are such that for all @xmath14 , @xmath15 and @xmath16 .",
    "estimating @xmath3 is a crucial issue in particular for predicting the value of the response given a new explanatory functional variable @xmath17 .",
    "however , it is also a very delicate task because @xmath3 is a nonlinear operator ( from @xmath8 into @xmath12 ) for which functional linear statistical methods were not planned . to provide a consistent procedure to estimate the nonlinear regression operator @xmath3",
    ", we propose to adapt the classical finite dimensional nadaraya - watson estimate to our functional model .",
    "we set @xmath18    several asymptotic properties of this estimate were obtained recently .",
    "it turns out that the existing literature adresses either the statement of upper bounds of the rates of convergence without specification of the exact constants ( see chapter 6 in ferraty & vieu , 2006 ) , or abstract expressions of these constants which are unusable in practice ( as for instance in the recent work by masry , 2005 , which has been published during the reviewing process of this paper ) .",
    "our aim in this paper is to give bias , variance , means square errors and asymptotic distribution of the functional kernel regression estimate with exact computation of all the constants ( see section [ theorie ] ) .",
    "we will focus on practical purposes in section computfeatures .",
    "several assumptions will be made later on the kernel @xmath19 and on the bandwidth @xmath20 .",
    "remind that in a finite - dimensional setting pointwise mean squared error ( at @xmath21 ) of the estimate depends on the evaluation of the density ( at @xmath21 ) w.r.t .",
    "lebesgue s measure and on the derivatives of this density .",
    "we refer to schuster ( 1972 ) for an historical result about this topic . on infinite - dimensional spaces ,",
    "there is no measure universally accepted ( as the lebesgue one in the finite - dimensional case ) and there is need for developping a free - densityapproach . as discussed along section [ exsbp ] the problem of introducing a density for @xmath2 is shifted to considerations on the measure of small balls with respect to the probability of @xmath22 .",
    "only pointwise convergence will be considered in the forthcoming theoretical results . in",
    "all the following , @xmath21 is a fixed element of the functional space @xmath8 .",
    "let @xmath23 be the real valued function defined as @xmath24 , \\ ] ] and @xmath25 be the c.d.f",
    ". of the random variable @xmath26 : @xmath27 note that the crucial functions @xmath23 and @xmath25 depends implicitely on @xmath28 consequently we should rather note them by @xmath29 and @xmath30 but , as @xmath21 is fixed , we drop this index once and for all .",
    "similarly , we will use in the remaining the notation @xmath31 instead of @xmath32 .",
    "let us consider now the following assumptions .",
    "* h0 :* @xmath3 and @xmath33 are continuous in a neighborhood of @xmath21 , and @xmath34 .    * h1 * * :* @xmath35 exists .    * h2 :* the bandwidth @xmath20 satisfies @xmath36 and @xmath37 , while the kernel @xmath19 is supported on @xmath38 $ ] , has a continuous derivative on @xmath39 , @xmath40 and @xmath41",
    ".    assumptions * h0 * and * h2 * are clearly unrestrictive , since they are the same as those classically used in the finite - dimensional setting . much more should be said on assumption * h1*. note first that , obviously , @xmath42 .",
    "it is worth noting that , whereas we could expect assumptions on the local regularity of @xmath3 ( as in the finite - dimensional case ) , hypothesis * h1 * skips over that point and avoids to go into formal considerations on differential calculus on banach spaces . to fix the ideas ,",
    "if we assumed differentiability of @xmath3 , we would get by taylor s expansion that @xmath43 where @xmath44 , @xmath45 being the conjugate space of @xmath8 and @xmath46 being the duality bracket between @xmath8 and @xmath47 in this context , a non trivial link would appear between @xmath35 and @xmath48 through the following relation : @xmath49 indeed , even if the link between the existency of @xmath48 and of @xmath35 is strong , one can build counter - examples for their non equivalence ( these counter - examples are available on request but they are out of the main scope of this paper ) . in the perspective of estimating the constants given in theorem [ theoremmse ] , it will be easier to estimate @xmath35 ( for instance by using @xmath50 ) than the operator @xmath48 .",
    "therefore , we prefer to express computations by mean of @xmath35 instead of @xmath48 .",
    "this has the additional advantage to produce more readable writings .",
    "consequently , the differentiability of @xmath3 is not needed .",
    "let us now introduce the function @xmath51 defined for all @xmath52 $ ] as : @xmath53 for which the following assumption is made :    * h3 :* for all @xmath52 , $ ] @xmath54 as @xmath55 .",
    "note that the function @xmath51 is increasing for all @xmath56 the measurable ( as the pointwise limit of the sequence of measurable functions @xmath51 ) mapping @xmath57 is non decreasing .",
    "let us finally mention that this function @xmath57 will play a key role in our methodology , in particular when we will have to compute the exact constant terms involved in our asymptotic expansions .",
    "for the sake of clarity , the following proposition ( whose a short proof will be given in the appendix ) will explicit the function @xmath58 for various cases . by @xmath59 0,1\\right ] } ( \\cdot)$ ] we denote the indicator function on the set @xmath60 0,1\\right ] $ ] and @xmath61 stands for the dirac mass at @xmath62 .",
    "[ prop1 ]  i ) if @xmath63 for some @xmath64 then @xmath65.ii ) if @xmath66 with @xmath64 and @xmath67 then @xmath68 iii ) if @xmath69 for some @xmath70 and some @xmath64 then @xmath71 iv ) if @xmath72 then @xmath73 0,1\\right ] } ( s)$ ] .    a deeper discussion linking the above behavior of @xmath25 with small ball probabilities notions will be given in section [ exsbp ] .",
    "in both following subsections we will state some asymptotic properties ( respectively mean squared asymptotic evaluation and asymptotic normality ) for the functional kernel regression estimate @xmath74 .",
    "it is worth noting that all the results below can be seen as extensions to functional data of several ones already existing in the finite - dimensional case ( the literature is quite extensive in this field and the reader will find in sarda & vieu ( 2000 ) deep results as well as a large scope of references ) . with other words , our technique for proving both theorem theoremmse and theorem [ theoremasnorm ] is also adapted to the scalar or vector regression model since the abstract space @xmath8 can be of finite dimension ( even , of course , if our main goal is to treat infinite - dimensional cases ) .",
    "moreover , it turns out that the transposition to finite - dimensional situations of our key conditions ( see discussion in section [ exsbp ] below ) becomes ( in some sense ) less restrictive than what is usually assumed . with other words , the result of theorem theoremmse and theorem [ theoremasnorm ]",
    "can be directly applied to finite - dimensional settings , and will extend the results existing in this field ( see again sarda & vieu , 2000 ) to situation when the density of the corresponding scalar or multivariate variable does not exist or has all its successive derivatives vanishing at point @xmath21 ( see discussion in section [ dimfinie ] ) .    all along this section",
    "we assume that assumptions * h0-h3 * hold .",
    "let us first introduce the following notations :    @xmath75      the following result gives asymptotic evaluation of the mean squared errors of our estimate .",
    "the asymptotic mean squared errors have a standard convex shape , with large bias when the bandwidth @xmath20 increases and large variance when @xmath20 decays to zero .",
    "we refer to the appendix for the proof of theorem [ theoremmse ] .",
    "[ theoremmse ] when * h0-h3 * hold , we have the following asymptotic developments : @xmath76 and@xmath77      let us denote the leading bias term by : @xmath78 before giving the asymptotic normality , one has to be sure that the leading bias term does not vanish .",
    "this is the reason why we introduce the following additional assumption :    *  h4 :* @xmath79 and @xmath80.the first part of assumption * h4 * is very close to what is assumed in standard finite - dimensional literature .",
    "it forces the nonlinear operator @xmath3 not to be too smooth ( for instance , if @xmath3 is lipschitz of order @xmath81 , then @xmath82 ) .",
    "the second part of assumption * h4 * is specific to the infinite - dimensional setting , and the next proposition pr2 will show that this condition is general enough to be satisfied in some standard situations .",
    "this proposition will be proved in the appendix .",
    "[ pr2 ]  i ) if @xmath83 0,1\\right ] } ( s)$ ] and @xmath58 is continuously differentiable on @xmath84 , then @xmath80 for any kernel @xmath19 satisfying * h2*.ii ) if @xmath85 , then @xmath80 for any kernel @xmath19 satisfying * h2*.    to emphasize the interest of these results , they should be combined with those of proposition [ prop1 ] .",
    "note that the result @xmath86 includes the well - known family of processes for which @xmath65 ( see proposition [ prop1]-@xmath14 ) , that is those whose distributions admit fractal dimensions ( see section [ fractal ] ) . the second case when @xmath85 ( for which a particular case is given in proposition [ prop1]-@xmath87 ) corresponds to nonsmooth processes ( see section [ nonsmooth ] ) .",
    "these two cases cover a large number of situations . however , if a more general function @xmath58 has to be used , one can make additional hypotheses on the kernel @xmath19 .",
    "in particular , if @xmath19 is such that @xmath88 $ ] , @xmath89 and @xmath83 0,1\\right ] } ( s)$ ] , then @xmath80 , which covers the case of the uniform kernel .",
    "more complicated kernel functions @xmath19 would lead to more technical assumptions linking @xmath19 with @xmath58 .",
    "it is out of purpose to give these tedious details ( available on request ) but let us just note that the key restriction is the condition @xmath83 0,1\\right ] } ( s)$ ] ( else we have @xmath90 ) .",
    "moreover , since the rate of convergence depends on the function @xmath91 and for producing a reasonably usable asymptotic distribution it is worth having some estimate of this function .",
    "the most natural is its empirical counterpart : @xmath92    the pointwise asymptotic gaussian distribution for the functional nonparametric regression estimate is given in theorem [ theoremasnorm ] below which will be proved in the appendix .",
    "note that the symbol @xmath93 stands for  convergence in distribution  .",
    "[ theoremasnorm ] when * h0-h4 * hold , we have @xmath94    a simpler version of this result is stated in corollary [ coro1 ] below whose proof is obvious .",
    "the key - idea relies in introducing the following additional assumption :    * h5 :* @xmath95    which allows to cancel the bias term .    [ coro1 ] when * h0-h5 * hold , we have @xmath96    in practice , the constants involved in corollary [ coro1 ] need to be estimated . in order to compute explicitely both constants @xmath97 and @xmath98 , one may consider the simple uniform kernel and get easily the following result :    under assumptions of corrollary [ coro1 ] , if @xmath99}(.)$ ] and if @xmath100 is a consistent estimator of @xmath31 , then we have : @xmath101    there are many possibilities for constructing a consistent conditional variance estimate .",
    "one among all the possibilities consists in writing that @xmath102 and , by estimating each conditional expectation with the functional kernel regression technique .",
    "[ examples ] the distribution function @xmath25 plays a prominent role in our methodology .",
    "this appears clearly in our conditions ( through the function @xmath58 ) and in the rates of convergence of our estimate ( through the asymptotic behavior of the quantity @xmath103 ) .",
    "more precisely , the behaviour of @xmath25 around @xmath104 turns out to be of first importance . in other words , the small ball probabilities of the underlying functional variable @xmath2",
    "will be determining . in order to illustrate our ideas and to connect with existing probabilistic knowledges in this field ,",
    "let us now just discuss how @xmath25 ( and hence @xmath58 ) behave for different usual examples of processes @xmath2 valued in an infinite - dimensional space .",
    "calculation of the quantity @xmath105 for small  @xmath106 ( i.e. for @xmath106 tending to zero ) and for a fixed @xmath21 is known as a `` small ball problem '' in probability theory .",
    "this problem is unfortunately solved for very few random variables ( or processes ) @xmath107 even when @xmath108 in certain functional spaces , taking @xmath109 yield considerable difficulties that may not be overcome .",
    "authors usually focus on gaussian random elements .",
    "we refer to li & shao ( 2001 ) for a survey on the main results on small ball probability .",
    "if @xmath2 is a gaussian random element on the separable banach space @xmath8 and if @xmath21 belongs to the reproducing kernel hilbert space associated with @xmath2 , then the following well - known result holds : @xmath110 so , the small ball problem at any point @xmath21 may be shifted to a small ball problem at @xmath111 moreover , ( [ pb ] ) can be precised in a few situations .",
    "for instance , mayer - wolf & zeitouni ( 1993 ) investigate the case when @xmath2 is a one - dimensional diffusion process and @xmath21 satisfies some conditions ( see mayer - wolf & zeitouni , 1993 , p15 ) .",
    "they also briefly mention the non gaussian case ( see mayer - wolf & zeitouni , 1993 , remark 3 , p19 ) but many other authors have considered different settings ( see ferraty et _ al . _ , 2005 , for a large discussion and references therein ) . as far as we know , the results which are available in the literature are basically all of the form : @xmath112 where @xmath113 and @xmath114 are positive constants and @xmath115 may be a @xmath116 , a @xmath10 , a besov norm ... the next remark is a direct consequence of proposition [ prop1 ] .",
    "it proves that non - smooth processes may satisfy the assumptions needed to get the asymptotic expansions of previous sections .",
    "[ rem1 ] in the case of `` non - smooth '' processes defined by ( [ ex1 ] ) we have @xmath117 .",
    "in addition , condition @xmath118 ( in * h2 * ) is checked as soon as @xmath119 for @xmath120 large enough .",
    "another family of infinite dimensional processes is the class of fractal processes for which the small ball probabilities are of the form @xmath121 where @xmath122 and @xmath123 are once again positive constants .",
    "like above , it is elementary to get the following result from proposition [ prop1 ] .",
    "[ rem2 ] under ( [ ex2 ] ) , we have @xmath65 while the condition @xmath124 ( in * h2 * ) is satisfied as soon as @xmath125 for @xmath126 small enough .      finally , it is important to note that a special case of fractal processs is given by the usual multivariate case ( that is , by the case when @xmath127 ) .",
    "the following result is obvious for the uniform norm on @xmath128 and extends directly to any norm , since all of them are equivalent in finite dimension .",
    "[ rem3 ] if @xmath127 , then any random variable @xmath2 on @xmath128 which has a finite and non zero density function at point @xmath21 satisfies ( [ ex2 ] ) with @xmath129 .    from remarks [ rem2 ] and [ rem3 ] ,",
    "it is clear that all the results of section [ theorie ] apply in a finite dimensional setting . besides",
    ", the assumptions needed for theorems [ theoremmse ] and [ theoremasnorm ] are weaker than those described in remark [ rem3 ] since there is no need to assume the existence of a density for @xmath2 . in this sense , our results extend the standard multivariate literature ( see the discussion at the beginning of section [ theorie ] ) .",
    "the asymptotic results presented in the previous section [ theorie ] are particularly appealing because , in addition to the specification of the rates of convergence , the exact constants involved in the leading terms of each result are precised .",
    "this is particularly interesting in practice .",
    "let us focus now on the mean squared errors expansion given in section [ mse ] .",
    "in fact , theorem [ theoremmse ] could give clues for possible automatic bandwidth choice balancing the trade - off between variance and squared - bias effects .",
    "however , the constants are unknown in practice which could seem to be a serious drawback for practical purposes .",
    "this general problem is well - known in classical nonparametric statistics , but in our functional context this question gets even more intricate because of the rather complicated expression of @xmath130 , @xmath97 and @xmath98 . an appealing way to attack",
    "the problem is to use bootstrap ideas . in section [ funcwildboot ]",
    "we propose a track for building a functional version of the so - called wild bootstrap .",
    "we will show in section [ funcwildbootsim ] , through some simulated examples , how this functional wild bootstrap procedure works on finite sample sizes for choosing automatically an optimal bandwidth .",
    "a case study , based on spectrometric functional data coming from the food industry , will be shortly presented in section [ funcwildbootspect ] .    at this stage",
    "it is worth noting that we have no asymptotic support for this functional bootstrapping procedure",
    ". this open question will be one of the main point discussed in the concluding section [ conc ] .      basically , when using bootstrapping techniques one expects to approximate directly the distribution of the error of estimation without having to estimate the leading terms involved in some asymptotic expansion of this error . in standard finite - dimensional problems ( that is , when the variable @xmath2 is valued in @xmath128 ) ,",
    "a so - called wild bootstrap has been constructed for approximating the distribution of the error of estimation in kernel nonparametric regression .",
    "we refer to hrdle ( 1989 ) and hrdle & marron ( 1991 ) for a previous presentation of the wild bootstrap in nonparameric regression",
    ". a selected set of additional references would include mammen ( 2000 ) for the description of the state of art on nonparametric bootstrapping , mammen ( 1993 ) for a large study of wild bootstrap , and hrdle , huet & jolivet ( 1995 ) for specific advances on wild bootstrap in ( finite - dimensional ) nonparametric regression setting .",
    "the main interest of this kind of bootstrap relies on a resampling procedure of the residuals which makes it easily adaptable to our functional setting .",
    "precisely , an adaptation to our functional setting could be the following functional wild bootstrap procedure :    * given the estimate @xmath74 constructed with a bandwidth @xmath20 , compute the residuals @xmath131 and construct a sequence of bootstrapped residuals such that each @xmath132 is drawn from a distribution @xmath133 which is the sum of two dirac distributions : @xmath134 such a distribution ensures that the first three moments of the bootstrapped residuals are respectively @xmath104 , @xmath135 and @xmath136 ( see hrdle & marron , 1991 , for details ) . * given the bootstrapped residuals @xmath137 , and using a new kernel estimate @xmath138 which is defined as @xmath74 but by using another bandwidth @xmath139 , construct a bootstrapped sample @xmath140 by putting @xmath141 * given the bootstrapped sample @xmath140 , compute the kernel estimate @xmath142 which is defined as @xmath74 ( with the same bandwidth @xmath20 ) but using the bootstrapped sample @xmath140 instead of the previous sample @xmath143 .",
    "we suggest to repeat several times ( let say @xmath144 times ) this bootstrap procedure , and to use the empirical distribution of @xmath145 for bandwidth selection purpose .",
    "precisely , the bootstrapped bandwidth is defined as follows :    [ defbb ] given @xmath144 replications of the above described bootstrapping scheme , and given a fixed set h of bandwidths , the bootstrapped bandwidth @xmath146 is defined by : @xmath147    of course , this procedure has still to be validated theoretically ( see discussion in section [ conc ] ) , but we will see in the next sections [ funcwildbootsim ] and [ funcwildbootspect ] how it behaves both on simulated on and real data samples .      the aim of this section is to look at how the automatic bootstrapped bandwidth constructed in definition [ defbb ] behaves on simulated samples .",
    "we construct random curves in the following way : @xmath148 where @xmath149 and @xmath150 ( respectively @xmath151 ) are r.r.v . drawn from a uniform distribution on @xmath84 ( respectively on @xmath152 ) .",
    "some of these curves are presented in figure [ curves ] below .",
    "the real response is simulated according to the following regression relation : @xmath153 where @xmath154 and where @xmath155 is drawn from a @xmath156 distribution .",
    "our experience is based on the following lines . for each experience",
    ", we simulated two samples : a sample of size @xmath157 on which all the estimates are computed and a testing sample of size @xmath158 which is used to look at the behaviour of our method .",
    "also , for each experience , the number of bootstrap replications was taken to be @xmath159 .",
    "other values for @xmath160 and @xmath144 were also tried without changing the main conclusions . to improve the speed of our algorithm , the bandwidth @xmath20 is assumed to belong to some grid in terms of nearest neighbours , that is @xmath161 where @xmath162 is the radius of the ball of center @xmath21 and containing exactly @xmath163 among the curves data @xmath164 . concerning the other parameters of our study , the kernel function @xmath19 was chosen to be @xmath165 and the norm @xmath166 was taken to be the @xmath167 one between the first order derivatives of the curves .    we computed , for the @xmath168 different values of @xmath20 , the average ( over the @xmath21 s belonging to the second testing data sample ) of the true error @xmath169 and of its bootstrap approximation @xmath170 .",
    "finally , this monte carlo scheme was repeated @xmath171 times and the results are reported in figure compar - errors ( only @xmath172 among the @xmath173 curves are presented to make the plot clearer ) .",
    "it appears clearly that both the theoretical quadratic loss and its data - driven bootstrapped version have the same convex shape .",
    "this convex shape is directly linked with the asymptotic expansion given in theorem theoremmse before : large values of @xmath20 give high bias , while small values of @xmath20 lead to high variance .",
    "these results are quite promising in the sense that the similarity of the shapes of both sets of curves presented in figure [ compar - errors ] let us expect that the bootstrapped bandwidths will be closed from the optimal ones . to check that point",
    ", we computed the theroretical minimal quadratic loss ( that is , the error obtained by using the best bandwidth ) and we compared it with the error obtained by using the boostrapped bandwidth @xmath146 . this was done for each among the @xmath171 experiences , and the results are reported in figure [ bandwchoiceresults ] which gives mean , variance and density estimates of these two errors .",
    "undoubtedly , these results show the good behaviour ( at least on this example ) of the bootstrapping method as an automatic bandwidth selection procedure .",
    "of course , as discussed in section [ conc ] , theoretical support for this functional bootstrap bandwidth selection rule is still an open question .",
    "let us now quickly show how our procedure is working on real data .",
    "these data contain of @xmath174 spectra of light absorbance @xmath175 as functions of the wavelength , and observed on finely chopped pieces of meat .",
    "we present in figure [ spectrocurves ] the plots of the @xmath174 spectra .",
    "for each spectral cruve corresponds some real response @xmath6 which is the percentage of fatness , and our aim is to study the regression relation existing between the real variable @xmath1 and the functional one @xmath176 .",
    "these data have been widely studied and , inspired by previous studies ( see ferraty & vieu , 2006 ) we decide to apply the functional kernel methodology to the curves @xmath177 , and by taking as norm @xmath166 between curves the usual @xmath167 norm between the second derivatives of the spectra .",
    "the kernel function @xmath19 was chosen to be @xmath178 .",
    "along our study we splitted the data into two subsamples .",
    "a first subsample of size @xmath179 from which our estimates are computed , and a testing sample of size @xmath180 on which they are applied .    in a first attempt",
    ", we used the automatic bootstrapping bandwidth selection rule , where @xmath181 was defined as in ( [ hsim ] ) .",
    "we present in figure errorbootspeccurves the shape of the bootsrapped mean square error as a function of the number of neighbours ( and thus , as function of the bandwidth ) .    the same convex form as for the simulated data appears .",
    "this form matches the theoretical results obtained in section [ mse ] , with high bias for large values of @xmath20 and high variance for small bandwidths .",
    "these bootstrapped errors are , in this example , minimal for the value @xmath182 .",
    "that means that , for each new curve @xmath21 to be predicted , the data - driven bootstrapped bandwidth @xmath183 is such that there are exactly @xmath184 curves - data which are falling inside of the ball of radius @xmath183 .",
    "these bandwidths lead to completely automatic data - driven fat contents prediction .",
    "for instance , we present in figure [ spectropredictions ] the fat content predictions for the @xmath180 spectra in our testing sample . in order to highlight the nice behaviour of our prediction algorithm , figure spectropredictions plots the predicted values as functions of the true ones .",
    "the implementation of the method was performed by using the _ splus _ routine _",
    "funopare.kernel _ which is included in the package _",
    "npfda_. this package will go with the monograph by ferraty & vieu ( 2006 ) .",
    "this _ splus _ package ( as well as a similar _ r _ package ) and the spectrometric dataset ( as well as several other curves datasets ) will be put in free access on line in the next future . by that time ,",
    "programs and data are available on request .",
    "this paper completes the recent advances existing in kernel nonparametric regression for functional data , by giving not only the rates of convergence of the estimates but also the exact expressions of the constant terms involved in these rates .",
    "these theoretical results deal with mean squared errors evaluations and asymptotic normality results . as explained in section [ computfeatures ] , these new results open interesting perspectives for applications , such as for instance data - driven automatic bandwidth selection and confidence band construction .",
    "we support the idea that bootstrap methods offers interesting perspectives for the functional context .",
    "one of them is illustrated by our functional version of the wild bootstrap for selecting the smoothing parameter .",
    "we have observed nice results for bandwidth selection on some simulated and real datasets .",
    "it should be pointed out that theoretical support for this functional wild bootstrap bandwidth selection rule remains an open problem .",
    "our guess is that it should be possible to extend to functional variables some results stated in finite dimension ( for instance those in hrdle & bowman , 1987 ) , but this has still to be proved .",
    "another direct application of our result concerns the construction of confidence bands .",
    "once again , the problem of estimating the constants involved in the asymptotic normal distribution can be attacked by the wild bootstrap track described before .",
    "one possible way for that would be to try to extend standard finite - dimensional knowledge ( see for instance hrdle & marron , 1991 , or hrdle , huet & jolivet , 1995 ) to infinite - dimensional variables .",
    "in the following , given some @xmath12-valued random variable @xmath185 , @xmath186 will stand for the probability measure induced by @xmath187 to make its treatment easier , the kernel estimate @xmath74 will be decomposed as follows :            * * bias term : proof of ( [ b])*. let us write the following decomposition . @xmath191",
    "with@xmath192   \\label{a1}\\ ] ] and@xmath193 .",
    "\\label{a2}\\ ] ] + the first step of the proof consists in rewritting the first term in right hand side of the decomposition ( [ bias ] ) in the following way : + [ lemma - bias-1 ] we have : [ b2 ] @xmath194 where @xmath195 + in a second attempt the next lemma will provide the constant term involved in this bias expression and its limit . + [ b3]we have : @xmath196 + finally , to finish this proof it suffices to prove that both last terms at right hand side of ( [ bias ] ) are neglectible .",
    "this is done in next lemma .",
    "+ [ b7 ] we have : @xmath197 + so the proof of ( [ b ] ) is complete . *",
    "* variance term : proof of ( [ v])*. the starting point of the proof is the following decomposition .",
    "this decomposition has been obtained in earlier work by collomb ( 1976 ) ( see also sarda & vieu ( 2000 ) ) in the finite - dimensional case , but since the proof is only using analytic arguments about taylor expansion of the function @xmath198 around @xmath104 , it extends obviously to our functional setting : + @xmath199 + finally , the result ( [ v ] ) will follow directly from this decomposition together with both following lemmas .",
    "+ [ b4bis ] we have successively : @xmath200 + [ b5 ] we have successively : @xmath201      the following lemma states a preliminary pointwise limiting distribution result .",
    "this lemma stems from the bias and variance expressions obtained along theorem 1 ; it will be proved in the next subsection .          * * proof of lemma [ lemma - bias-1 ] : * to calculate @xmath204 } { ek\\left ( \\frac{\\left\\vert \\mathcal{x}-\\chi\\right\\vert } { h}\\right ) } , \\label{pro1}\\ ] ] note first that @xmath205 & = e\\left [ \\left ( r\\left (   \\mathcal{x}\\right ) -r\\left ( \\chi\\right )",
    "\\right ) k\\left ( \\frac{\\left\\vert   \\mathcal{x}-\\chi\\right\\vert } { h}\\right ) \\right ] \\\\ & = e\\left [ \\varphi\\left ( \\left\\vert \\mathcal{x}-\\chi\\right\\vert \\right ) k\\left ( \\frac{\\left\\vert \\mathcal{x}-\\chi\\right\\vert } { h}\\right ) \\right ] .\\end{aligned}\\ ] ] moreover , it comes : @xmath206 \\\\ & = \\int\\varphi\\left ( t\\right ) k\\left ( \\frac{t}{h}\\right ) dp^{\\left\\vert   \\mathcal{x}-\\chi\\right\\vert } \\left ( t\\right ) \\\\ & = \\int\\varphi\\left ( ht\\right ) k\\left ( t\\right ) dp^{\\left\\vert \\mathcal{x}-\\chi\\right\\vert /h}\\left",
    "( t\\right ) \\\\ & = h\\varphi^{\\prime}\\left ( 0\\right ) \\int tk\\left ( t\\right ) dp^{\\left\\vert   \\mathcal{x}-\\chi\\right\\vert /h}\\left ( t\\right ) + o\\left ( h\\right ) , \\end{aligned}\\ ] ] + the last line coming from the first order taylor s expansion for @xmath23 around @xmath104 .",
    "for the denumerator in ( [ pro1 ] ) we have@xmath207 = \\int k\\left ( t\\right ) dp^{\\left\\vert \\mathcal{x}-\\chi\\right\\vert /h}\\left",
    "( t\\right ) .\\ ] ] + finally , it appears clearly that the first order bias term is @xmath208 . *",
    "* proof of lemma [ b3 ] : * we note that@xmath209 applying fubini s theorem we get@xmath210 similarly , we have @xmath211 so the proof of this lemma is finished by applying the lebesgue s dominated convergence theorem since the denumerator may be easily bounded above by @xmath41 ( @xmath19 being decreasing ) . * * proof of lemma [ b4bis ] * : the first assertion follows directly from ( [ pro2 ] ) , while the second one can be proved similarly according to the following lines : @xmath212 \\\\ & = \\left ( r\\left ( \\chi\\right ) + o(1)\\right ) e\\left [ k\\left ( \\frac{\\left\\vert   \\mathcal{x}-\\chi\\right\\vert } { h}\\right ) \\right ] .\\end{aligned}\\ ] ] * * proof of lemma [ b5 ] * : we write the variance of @xmath213 as : + @xmath214 , \\ ] ] + and note that , as for getting ( [ pro2 ] ) , it holds : @xmath215 the first assertion of lemma [ b4bis ] gives @xmath216 at last @xmath217 which finishes the proof of the first assertion of our lemma .",
    "+ the same steps can be followed to prove the second assertion .",
    "we write @xmath218 .\\ ] ] the second term at right hand side of this expression is treated directly by using the second assertion of lemma [ b4bis ] , while the first one is treated as follows by conditioning on @xmath2 : @xmath219 the continuity of @xmath220 and of @xmath221 insure that @xmath222 combining this result with ( [ ek2 ] ) allows to finish the proof of the second assertion of our lemma .",
    "let us deal now with the covariance term : @xmath223 .",
    "\\notag\\end{aligned}\\ ] ] the last two terms were computed before , while the first one is treated by conditioning on @xmath2 and using continuity of @xmath3 : @xmath224 the proof of this lemma is now finished . * * proof of lemma [ b7 ] * : both assertions of this lemma are direct consequences of lemmas [ b4bis ] and [ b5 ] . * * proof of lemma [ tcl1 ] : * on one hand , ( [ bias ] ) and lemma [ b7 ] allows us to get @xmath225 on the other hand , the following decomposition holds : @xmath226 using slutsky s theorem and theorem [ theoremmse ] , we get @xmath227 noting that @xmath228 can be expressed as an array of independent centered random variables ( and the central limit theorem applies ) .",
    "let us remark that @xmath229 which achieves the proof of this lemma .",
    "* * proof of proposition [ prop1]-@xmath14 and @xmath230 * : obvious . * * proof of proposition [ prop1]-@xmath87 * : we have : @xmath231 if @xmath232 we have @xmath233 , while if @xmath234 we have @xmath235 . to complete this proof",
    "it suffices to note that , for @xmath236 , we have : @xmath237 and so we have @xmath238 as @xmath239 . *",
    "* proof of propostion [ prop1]-@xmath240 * : for any @xmath241 and any @xmath242 ,",
    "we have : @xmath243 and so we have @xmath244 . to complete this proof it suffices to note that @xmath245 . * * proof of propostion [ pr2]-@xmath14 * : by simple integration by parts we arrive at @xmath246 where @xmath247 .",
    "because of @xmath248 and because @xmath58 is non decreasing , there exists some nonempty interval @xmath249\\subset(0,1)$ ] such that both @xmath19 and @xmath250 do not vanish on @xmath249 $ ] , and therefore we arrive at : @xmath251 * * proof of propostion [ pr2]-@xmath14 * : obvious ."
  ],
  "abstract_text": [
    "<S> we consider the problem of predicting a real random variable from a functional explanatory variable . </S>",
    "<S> the problem is attacked by mean of nonparametric kernel approach which has been recently adapted to this functional context . </S>",
    "<S> we derive theoretical results by giving a deep asymptotic study of the behaviour of the estimate , including mean squared convergence ( with rates and precise evaluation of the constant terms ) as well as asymptotic distribution . </S>",
    "<S> practical use of these results are relying on the ability to estimate these constants . </S>",
    "<S> some perspectives in this direction are discussed . </S>",
    "<S> in particular a functional version of wild bootstrapping ideas is proposed and used both on simulated and real functional datasets .    </S>",
    "<S> * key words : * asymptotic normality , functional data , nonparametric model , quadratic error , regression , wild functional bootstrap . </S>"
  ]
}