{
  "article_text": [
    "numerous problems in science , from spectral analysis to image processing , require that we restore properties of a function @xmath0 from a set of integrals @xmath1 \\equiv \\int_{-\\infty}^{\\infty}\\ ! dz k(n , z )   a(z ) , \\ ; \\ ; n=1 , \\dots , n , \\label{eq : ac0}\\ ] ] where @xmath2 is a known kernel and @xmath3 is a finite set of experimental or numerical input data with error bars . an important class of such problems  known as numerical analytic continuation ( nac)deals with  pathological \" kernels featuring numerous eigenfunctions with anomalously small eigenvalues .",
    "an archetypal nac problem is the numerical spectral analysis at zero temperature , where the challenge is to restore the non - negative spectral function @xmath4 satisfying the equation @xmath5 from numerical data for @xmath6 .",
    "the nac problem is often characterized as _ ill - posed_. mathematically , the near - degeneracy of the kernel implies two closely related circumstances : ( i ) the absence of the resolvent , and ( ii ) a continuum of solutions satisfying the input data within their error bars ( even when integrals over @xmath7 are replaced with finite sums containing less or equal to @xmath8 terms ) .",
    "nowadays , the first circumstance is merely a minor technical problem , as there exists a number of methods allowing one to find solutions to ( [ eq : ac0 ] ) without compromising the error bars of @xmath9 .    the second circumstance ",
    "the ambiguity of the solution  is a more essential problem .",
    "it is clear that if one formulates the goal as to restore @xmath0 as a continuous curve , or to determine its value on a given grid of points , then the goal can not be reached as stated , irrespective of the properties of the kernel @xmath2 .",
    "the input data set is finite and noisy , thereby introducing a natural limit on the resolution of fine structures in @xmath0 .",
    "fortunately , the above - formulated goal has little to do with the practical world . in an experiment ,",
    "all devices are characterized by a finite resolution function and the data they collect always correspond to _",
    "integrals_. the data are processed by making certain _ assumptions _ about the underlying function .",
    "this motivates an alternative formulation of the nac goal involving integrals of @xmath0 that render the problem well - defined . with additional assumptions about the smoothness and other properties of @xmath0 behind these integrals , consistent with both",
    "_ a priori _ and _ a posteriori _ knowledge , the ambiguity of the solution can be substantially suppressed .",
    "the simplest setup is as follows : + _ given a set of finite intervals @xmath10 , determine the integrals of the spectral function over these intervals : @xmath11 along with the corresponding dispersions of fluctuations @xmath12 ( straightforwardly extendable to the dispersion correlation matrix @xmath13 ) . _",
    "+ naively one might think that nothing is achieved by going from the integrals in ( [ eq : ac0 ] ) to the integrals in ( [ eq : ac1 ] ) because the latter have exactly the same form with the kernel @xmath14 for @xmath15 and zero otherwise ( other forms of the  resolution function \" @xmath16 are discussed in sec .",
    "[ sec : socc ] ) : @xmath17 = \\int_{-\\infty}^{\\infty } \\ ! dz \\bar{k}(m , z ) a(z ) , \\qquad m=1,\\dots , m. \\quad \\label{eq : ac2}\\ ] ] this impression , however , is false because kernel properties are at the heart of the problem .",
    "if for appropriately small intervals ( sufficiently small to resolve the variations of @xmath0 ) , the uncertainties for @xmath18 remain small , then one can draw reliable conclusions for the underlying behavior of @xmath0 itself .",
    "the difference between  good \" ( e.g. as in fourier transforms ) and pathological kernels is that for the latter , due to the notorious saw - tooth instability , the uncertainties for @xmath18 quickly become too large for a meaningful analysis of fine structures in @xmath0 .    to obtain a solution from the integrals ( [ eq : ac1 ] )",
    ", one has to invoke the notion of _ conditional knowledge_. the most straightforward approach is to set the spectral function values at the middle points @xmath19 of the intervals @xmath20 to @xmath21 .",
    "this is only possible if the intervals can be made appropriately narrow without losing accuracy for the integrals . with this approach",
    "we assume that the function is nearly linear over the intervals in question .",
    "this is a typical procedure for experimental data . quantifying",
    "the error bar on @xmath22 necessarily involves _ two _ numbers : the `` vertical '' dispersion @xmath23 is directly inherited from @xmath18 , and the  horizontal \" error bar @xmath24 represents the interval half - width .",
    "the reader should be aware of two issues regarding such error bars .",
    "first , the error bars for different points are not independent but contain significant multi - point correlations .",
    "for example , an unrestricted integral @xmath25 is typically known with an accuracy that is orders of magnitude better than what would be predicted by the central limit theorem if this integral is represented by a finite sum of integrals over nonoverlapping intervals .",
    "second , the errors are not necessarily distributed as a gaussian .",
    "atypical fluctuations can have a significant probability and their analysis should not be avoided as the actual physical solution may well be one of them .",
    "to this end , it is important to explore the minimal and maximal values that the integral @xmath18 can take , and check that these are not significantly different from the typical value of @xmath18 . in certain cases",
    "this criterion can not be met without increasing the intervals @xmath20 to an extent when the assumption of linearity of @xmath0 becomes uncontrolled , implying that an important piece of information about the shape of @xmath0 in this interval is missing . a characteristic example that plays a key role in the subsequent discussion",
    "is presented in fig .",
    "[ fig : two_peaks ] , where the challenge is to extract the shape of the second peak .     for the nac problem ( [ eq : spectral ] ) .",
    "as shown in the main part of the text , the significant width of the first peak makes it essentially impossible to controllably restore the width of the second peak , even with small relative error bars ( @xmath26 ) on @xmath9 . on the other hand ,",
    "the first two moments of the second peak , characterizing its weight and position , can be extracted reliably . ]    in the more sophisticated approach used in this work , the values of @xmath22 can be further optimized ( without compromising the accuracy of the solution with respect to @xmath9 ) to produce a smooth curve . this protocol has the additional advantage of eliminating minima , maxima , gaps , and sharp features that are not guaranteed to exist by the quality of input data .",
    "the nature of the problem is such that very narrow peaks ( or gaps ) with tiny spectral weight can always be imagined to be present ( for narrow intervals they will certainly emerge due to the saw - tooth instability ) .",
    "our philosophy with respect to these features is to erase them _ within the established error bounds _ and obtain a solution that is insensitive to the interval parameters .",
    "having established a smooth solution , one may nevertheless ask whether a particular feature of the solution can , in principle , have significantly different properties .",
    "for example , if the nac procedure suggests a peak , one may wonder if the true spectral function could have a much narrower peak with the same area , and if so , what is its smallest possible width",
    ". a nac protocol should be able to answer this type of question fast and reliably . in this work ,",
    "we explain how these goals can be achieved in practice .",
    "many technical details of the protocol that we propose to abbreviate as socc ( stochastic optimization with consistent constraints ) were already published in refs .   as separate developments .",
    "the crucial advances here are ( i ) the final formulation based on integrals of the spectral function , and ( ii ) the idea of working with linear combinations of pre - calculated  basic \" solutions .",
    "the latter allows one to readily apply consistent constraints without compromising the error bars on the input data .",
    "consistent constraints are also crucial for assessing what features can be resolved and what information is unrecoverable .    in",
    "what follows , the term  consistent constraints \" applies to ( i ) the general principle of utilizing the _ a priori _ and revealing the _ a posteriori _ ( conditional ) knowledge without compromising the error bars of the input data and ( ii ) a particular set of numerical procedures based on ideas respecting this principle .",
    "our socc protocol involves two different consistent - constraint procedures . the first one , borrowed form the nac method of ref .",
    ", is now used solely to ( dramatically ! ) enhance the performance of the stochastic - optimization part of the protocol searching for basic solutions .",
    "the most important consistent - constraint procedure is used to post - process the set of basic solutions .",
    "the paper is organized as follows . in sec .",
    "[ sec : socc ] we describe the socc method consisting of three distinct stages , and explain how a smooth solution can be obtained without any bias with respect to solving eq .",
    "( [ eq : ac0 ] ) and analyzed for possible atypical deformations . in sec .",
    "[ sec : maxent ] we briefly review the maximum entropy method ( mem).@xcite in sec .  [",
    "sec : tests ] we explore what socc and mem methods predict for the test spectral function shown in fig .",
    "[ fig : two_peaks ] , and how one should analyze the final solution with respect to its possible smooth transformations .",
    "in sec .  [",
    "sec : polarons ] we apply our findings to the physical spectral function of the resonant fermi polaron.@xcite we conclude in sec .  [ sec : conclusions ] .",
    "the formulation of the socc method is relatively simple and consists of three parts : + 1 .  finding a large set of solutions @xmath27 [ @xmath28 to eq .",
    "( [ eq : ac0 ] ) that satisfy the input data within their error bounds . in what follows we call them `` basic '' solutions .",
    "basic solutions are not biased in any way to be smooth or to satisfy any other requirements based on knowledge about the problem outside of eq .",
    "( [ eq : ac0 ] ) .",
    "the irregularity of basic solutions embodies what is referred to as an ill - posed problem . in subsection",
    "[ subsec : so ] we briefly explain how these solutions are found by the stochastic optimization procedure ( most technical details were published previously in refs .  ) and how the consistent constraints method @xcite is used to improve drastically the speed of the stochastic optimization protocol .",
    "+ 2 .  using the basic solutions @xmath27 to compute the integrals  ( [ eq : ac2 ] ) with a different kernel @xmath16 .",
    "there are several choices here .",
    "one of them is given by eq .",
    "( [ eq : ac1 ] ) and amounts to computing integrals from @xmath29 over finite intervals @xmath10 centered at @xmath30 .",
    "however , one is also free to consider normalized continuous kernels with unrestricted integration over @xmath7 , such as lorentzian ( or gaussian ) shapes centered at points @xmath30 with the width @xmath10 at half - height , e.g. , @xmath31 thus obtained sets of integrals @xmath32 are then used straightforwardly to compute averages @xmath33 and dispersions @xcite @xmath34 to characterize possible two - point correlations , one should compute the correlation matrix @xmath35 strictly speaking , there is no reason to stop characterizing correlations at the two - point level .",
    "one may proceed with computing multi - point averages but the effort quickly becomes expensive and the outcome can not be presented in a single plot . an alternative  visualization \" of multi - point correlations is discussed in subsection [ subsec : final ] .",
    "+ 3 .  interpreting the result .",
    "the simplest interpretation and an estimate of the dispersion for typical fluctuations is to assume that @xmath0 is nearly linear over the range of each interval .",
    "this leads to the solution @xmath36 with vertical and horizontal  error bars \" @xmath37 and @xmath38 .",
    "the vertical error bars may be overestimated because fluctuations at different points are correlated . however , as explained in the introduction , the correct answer may correspond to some atypical shape , and this possibility has to be addressed as well .    an alternative protocol , discussed in subsection [ subsec : final ] , determines the final solution by selecting a superposition of basic solutions @xmath39 such that @xmath0 remains non - negative ( with high accuracy ) and the coefficients @xmath40 are optimized to impose smooth behavior or any other `` conditional knowledge '' .",
    "formally , the simplest interpretation corresponds to @xmath41 .",
    "the search for basic solutions relies on the minimization of @xmath42}{\\delta_n}\\right)^2 \\ ; , \\label{eq : chi2}\\ ] ] where @xmath43 is the error of the @xmath9 value . without loss of generality ,",
    "we assume that the components of the vector @xmath44 are uncorrelated ; otherwise , one has to perform a rotation to the eigenvector basis of the two - point correlation matrix @xmath45 where the components of @xmath46 become statistically independent .",
    "this linear transformation leads to an equation that has exactly the same form as ( [ eq : ac0 ] ) with a rotated kernel .",
    "we choose a maximal tolerance @xmath47 of order unity and search for functions @xmath48 with @xmath49 , which are then added to the set of basic solutions for further processing .",
    "information about the input data is limited to the objective function ( [ eq : chi2 ] ) .",
    "truly unbiased methods should not assume anything about @xmath0 that is not part of exact knowledge , such as the predetermined grid of points , and the number and parameters of peaks / gaps .",
    "in the stochastic optimization method of refs .  , each solution is represented by a set of positive - definite rectangular shapes ( the @xmath50-function can be viewed as the limiting case of an infinitely narrow and infinitely high rectangular shape with fixed area ) , which are allowed to have multiple overlaps , see panel ( a ) in fig  [ fig : slicing ] . more precisely , a solution is represented as a sum @xmath51 of rectangles @xmath52 , @xmath53 \\ , , \\\\",
    "0\\ , , & \\quad \\mbox{otherwise}\\ , , \\end{array } \\right .",
    "\\quad \\label{rectang}\\end{aligned}\\ ] ] where @xmath54 , @xmath55 , and @xmath56 are the height , width , and center of the rectangle @xmath57 , respectively . in what follows",
    "we refer to @xmath58 as a `` configuration . ''",
    "all rectangles are restricted to the specified range of the @xmath0 support , @xmath59 $ ] ; i.e. , for all rectangles @xmath60 and @xmath61 .",
    "the spectrum normalization is given by @xmath62 , and @xmath63 $ ] in eq .",
    "( [ eq : ac0 ] ) can be written as @xmath64= \\sum_{r=1}^{r } { \\cal k}(n , r ) h_r   \\ ; , \\label{simulated - discrete}\\ ] ] where @xmath65    the number of rectangles and all continuous parameters characterizing their position , width , and height are found by minimizing the objective ( [ eq : chi2 ] ) .",
    "optimization starts from a randomly generated set of rectangles , and finds a large number of dissimilar basic solutions @xmath66 with @xmath67 .",
    "more precisely , the search is based on a chain of randomly chosen updates over the configuration space of shapes which fully explore the saw - tooth fluctuations of basic solutions .",
    "this is important for the successful elimination of noise in the final solution .",
    "updates proposing small modifications of the shape ( `` elementary '' updates ) have the disadvantage of long computation time for a basic solution . to speed up the search , we supplement the standard protocol of refs .   with consistent - constraints ( cc ) updates , which propose a radical shape modification based on minimization of the positive - definite quadratic form @xmath68 by matrix inversion as described in ref .  .",
    "here @xmath69 are various positive - definite quadratic forms , or `` penalties , '' that ensure that the matrix to be inverted is well - defined .",
    "this is achieved by penalizing the derivatives of @xmath0 ( computed on the grid based on the current configuration @xmath70 , see below ) and enforcing @xmath71 .",
    "the cc - update involves a number of iterations when penalties @xmath69 are adjusted self - consistently in such a way that at the end of the update @xmath72 .",
    "explicit forms for @xmath73 and the adjustment protocols are described in detail in ref .",
    "( see also @xmath74 and @xmath75 forms in subsection [ subsec : final ] ) .",
    "even though the cc - updates do not compromise the goal of minimizing @xmath76 , their efficiency is based on penalties that suppress saw - tooth fluctuations . to exclude possible bias originating from cc - updates on basic solutions we proceed as follows .",
    "the global update of the socc method consists of thousands of elementary updates @xmath77 that are divided into two groups : @xmath78 stage - a updates and @xmath79 stage - b updates , where @xmath80 and @xmath81 .",
    "updates increasing @xmath76 are temporarily considered  accepted \" ( and the resulting configuration recorded ) with high probability during stage - a , but this probability is reduced during stage - b that favors updates decreasing @xmath76 .",
    "the idea is to use @xmath78 updates to escape from the local minimum of @xmath76 in the multi - dimensional configuration space in a hope to find a better minimum afterwards .",
    "the global update is accepted only if a smaller value of @xmath76 was recorded in the course of applying elementary updates , and the new configuration becomes the one with the smallest @xmath76 .",
    "we apply cc - updates during stage - a of a global update when the increase of @xmath76 is allowed , and proceed with a large number of elementary updates , which results in a configuration with fully developed saw - tooth instability .",
    "we found that cc - updates have no effect on the self - averaging of the saw - tooth noise in the equal - weight superposition of basic solutions , improve typical @xmath76-values for basic solutions , and significantly decrease the computation time required for finding basic solutions .    to run the cc - update",
    ", one has to re - parameterize the configuration as a collection of nonoverlapping rectangles in order to be able to use their heights for estimates of the function derivatives .",
    "panel ( b ) in fig .",
    "[ fig : slicing ] illustrates how overlaps of rectangles are understood in the socc method .",
    "this leads to an identical re - parametrization in terms of nonoverlapping rectangles @xmath82 .",
    "the conversion is done as follows .",
    "first , the set of rectangle parameters @xmath83 is ordered to form a grid of new bin boundaries that also include the support limits @xmath84 and @xmath85 .",
    "second , bin centers and widths become centers and widths of the ordered set of new rectangles , respectively : @xmath86 figures  [ fig : slicing](b ) and  [ fig : slicing](c ) illustrate how the conversion from @xmath87 to @xmath88 amounts to an identical representation of the spectrum : @xmath89 original rectangles introduce @xmath90 boundaries on the @xmath59 $ ] interval and split it into @xmath91 rectangles obeying conditions ( [ cond1])([cond2 ] ) . note that some rectangles have zero height when submitted into the cc - update .",
    "the update modifies the values of all @xmath92-parameters and generates a new set @xmath93 . since @xmath94 is a particular case of @xmath95 ,",
    "there is no need to perform any additional transformation to proceed with elementary updates .      because all basic solutions satisfy eq .",
    "( [ eq : ac0 ] ) , one can immediately check that a linear combination of basic solutions , eq .",
    "( [ eq : final ] ) , always leads to a solution of eq .",
    "( [ eq : ac0 ] ) with the same accuracy cutoff @xmath47 as the basic solutions , provided that all @xmath96-coefficients are non - negative . indeed , by linearity of the problem and the condition @xmath97 , @xmath98 & = & \\sum_{j=1}^{j } c_j g[n , a_j ] \\",
    "; , \\nonumber \\\\ g_n - g[n , a_{\\rm fin } ] & = & \\sum_{j=1}^{j } c_j \\left ( g_n - g[n , a_j ] \\right ) \\;. \\label{eq : lin1}\\end{aligned}\\ ] ] substituting these expressions into the @xmath76 form for the final solution and employing the cauchy  bunyakovsky ",
    "schwarz inequality for @xmath99 we get @xmath100 ) ( g_n - g[n , a_{j ' } ] ) } { \\delta_n^2 } \\;.",
    "\\qquad \\;\\ ; \\label{eq : lin2}\\end{aligned}\\ ] ]    if some @xmath96-coefficients are negative , the accuracy of the final solution is guaranteed only if @xmath101 is not large .",
    "one may argue that the upper bound @xmath102 is substantially overestimating deviations , and the actual accuracy is better .",
    "let @xmath103 and @xmath104 be the sums over all positive and all negative coefficients , respectively .",
    "then linear superpositions of basic solutions involving only positive and only negative coefficients and divided by @xmath105 and @xmath106 ( we denote them as @xmath107 and @xmath108 , respectively ) , have their @xmath76 measures smaller or equal to @xmath109 , by eq .",
    "( [ eq : lin2a ] ) . the final solution can be identically written as @xmath110 , and its @xmath76-measure is nothing but the two state version of eq .",
    "( [ eq : lin2a ] ) .",
    "since the @xmath63 $ ] values are derived from spectral density integrals they are smooth functions of @xmath111 and random point - to - point sign fluctuations of @xmath112 $ ] are arising predominantly from @xmath9 .",
    "thus , the expectation is that @xmath113 is positive , in which case @xmath114 in practice , sign - positivity of the spectral density severely restricts the possibility of having large @xmath115 and @xmath101 in the final solution and @xmath115 tends to remain smaller than unity automatically .",
    "finally , eq .  ( [ eq : lin2a ] ) is only an upper bound , and superpositions with @xmath101 as large as @xmath116 may still have @xmath117 .",
    "these considerations lead to an important possibility of modifying the shape of the final solution in order to satisfy additional criteria formulated outside of eq .",
    "( [ eq : ac0 ] ) .",
    "the key observation , and crucial difference to other nac methods , is that `` conditional knowledge '' protocols are invoked _",
    "after _ all basic solutions are determined , meaning that they remain unbiased with respect to the input data .    as discussed in the introduction , the most conservative philosophy regarding sharp spectral features , such as peaks and gaps , is to eliminate them if they are not warranted by the quality of the input data .",
    "( our method does allow to answer the question whether a given sharp feature is compatible with the input data , see below . ) to implement the idea , we formulate the problem of determining an appropriate set of @xmath118 coefficients as a linear self - consistent optimization problem closely following the consistent constraints method of ref .  .",
    "the objective function to be minimized consists of several terms , @xmath119 , each being a quadratic positive - definite form of @xmath40 .",
    "more terms can be added if necessary to control higher - order derivatives , enforce expected asymptotic behavior , etc .",
    "+ @xmath120 to suppress large derivatives we consider the following form @xmath121 ^ 2 + b_k^2 [ a''(z_k)]^2 \\right\\ } \\ ; , \\label{eq : o1}\\ ] ] where @xmath122 is the grid of points used to define the first and second discrete derivatives of the function @xmath0 .",
    "the sets of coefficients @xmath123 and @xmath124 are adjusted under iterations self - consistently in such a way that contributions of all @xmath125-points to @xmath74 are similar .",
    "@xmath120 the unity - sum constraint on the sum of all coefficients in the superposition is expressed as @xmath126 with a large constant @xmath127 .",
    "@xmath120 since @xmath128 does not constrain the amplitudes and signs of all @xmath40 the minimization can not proceed by matrix inversion . to improve matrix properties we add a `` soft '' penalty for large deviations of @xmath40 from the equal - weight superposition @xmath129    @xmath120 to ensure that the spectral function is non - negative ( with high accuracy ) we need @xmath7-dependent penalties ( to be set self - consistently ) that suppress the development of large negative fluctuations : @xmath130    @xmath120",
    "finally , we can introduce a penalty for the solution to deviate from some `` target '' function ( or `` default model '' ) @xmath131 : @xmath132 ^ 2 \\;. \\label{eq : o5}\\ ] ] the main purpose of @xmath133 is to address subtle multi - point correlations between allowed shapes : by forcing the solution to be close to a certain target function one can monitor how the solution starts developing additional saw - tooth - instability - related features or violates the unity - sum constraint .",
    "this penalty is zero when preparing @xmath134 in the absence of any target function .",
    "+ the self - consistent optimization protocol is as follows .",
    "we start with @xmath41 and compute @xmath135 .",
    "the initial sets of coefficients in @xmath74 are defined as @xmath136 , and @xmath137 , where @xmath138 is some small positive constant ( its initial value has no effect on the final solution because penalties for derivatives will be increased exponentially under iterations ) .",
    "since the positivity of @xmath0 is guaranteed in the initial state , we set @xmath139 . after the quadratic form for the objective function @xmath140",
    "is minimized , the new set of @xmath96-coefficients is used to define a new solution @xmath0 , penalties for derivatives are increased , @xmath141 , @xmath142 , @xmath143 by some factor @xmath144 , and then all penalties in in @xmath74 and @xmath75 are adjusted self - consistently as follows :    * if @xmath145 , we assign @xmath146 ; * if @xmath147 , we assign @xmath148 ; * if @xmath149 , we assign a large penalty suppressing the amplitude of the solution at this point , @xmath150 , where @xmath151 is a large constant ; otherwise the value of @xmath152 is increased by two orders of magnitude .    in this work we use @xmath153 , @xmath154 , and @xmath155 .",
    "this sets the stage for the next iteration of the @xmath140-optimization protocol .    since the accuracy expression , eq .",
    "( [ eq : lin1 ] ) , relies on the substitution @xmath156 , it is crucial that the unity - sum constraint is satisfied for all input data points , @xmath157 this provides the required criterion for terminating iterations .",
    "the final solution ( [ eq : final ] ) is based on the last set of @xmath96-coefficients that satisfied the condition @xmath49 .    in the absence of the target objective @xmath133 ,",
    "the procedure is guaranteed to produce a final solution @xmath158 with smooth behavior because our initial solution already satisfies all requirements .",
    "the derivative objective is forcing @xmath0 to be as smooth as possible within the subspace of fluctuations that keep @xmath76 small .    with the help of @xmath133 one can explore how the solution is modified if one forces it to go through some set of points.@xcite the simplest case would be to set @xmath159 for some point @xmath160 ( where @xmath161 is a large number ; in this work @xmath162 ) and zero otherwise , and shift @xmath163 away from @xmath164 .",
    "the solution going through the point @xmath163 is no longer guaranteed to be smooth in the vicinity of @xmath160 and for large deviations from the final solution will develop the saw - tooth instability at @xmath160 .",
    "the most interesting choices for @xmath160 are the minima and maxima of the spectrum . despite the fact that our protocol is to erase sharp features not warranted by the input data quality",
    ", we can still address questions such as `` can this spectral peak ( or gap ) be made narrower / higher / lower and by how much , before the solution becomes unstable against developing secondary features ? ''",
    "this question can not be fully answered at the level of the correlation matrix ( [ eq : corr ] ) because ( i ) spectral functions have subtle multi - point correlations , and ( ii ) the notion of a ",
    "typical \" solution has no physical meaning in this context . the only way to answer this question is to have access to a large representative set of unbiased basic solutions .",
    "the objective @xmath133 offers a generic way of exploring various possibilities for underlying features hidden behind the accuracy of input data .",
    "clearly , there are other alternatives for addressing specific questions . for example",
    ", one can isolate a spectral peak to some interval and compute the dispersion @xmath165 of each basic solution @xmath166 over this interval .",
    "next , the distribution function @xmath167 over all basic solutions is composed and analyzed .",
    "if @xmath167 has a narrow region of support around its average @xmath168 value , then the peak width can not significantly deviate from @xmath168 .",
    "if @xmath167 is nonzero for @xmath169 then one has to conclude that the actual peak might be much narrower ( and , correspondingly , have a much higher amplitude ) than what is predicted by the typical smooth solution @xmath134 .",
    "numerous nac schemes are based on a totally different philosophy and impose additional restrictions / penalties on the allowed functional shapes of @xmath0 _ in the process _ of solving eq .",
    "( [ eq : ac0 ] ) . in other words ,",
    "the search for solutions is biased with `` conditional knowledge '' from the very beginning .",
    "historically , the tikhonov - phillips regularization method@xcite was the first to advocate this approach . currently , the most popular scheme of this type is the maximum entropy method.@xcite other schemes worth mentioning are singular value decomposition,@xcite non - negative least - squares,@xcite stochastic regularization,@xcite and averaging pad approximants.@xcite in the stochastic sampling method of refs .  , the remaining bias is in the form of the predetermined grid of frequency points , and the final solution is an average over a certain `` thermal '' ensemble ( see also ref .   for a further refinement ) .",
    "fast effective modification of stochastic optimization ( fesom ) @xcite also uses the predetermined grid of frequency points .",
    "we now briefly review the maximum entropy method,@xcite which can be seen as a special case of stochastic sampling methods.@xcite instead of minimizing @xmath76 one constructs a functional @xmath170 $ ] , where @xmath171 $ ] is the `` entropy '' term .",
    "the positive parameter @xmath172 is a lagrange multiplier that can also be thought of as a `` temperature '' by analogy to classical statistics ( note that our definition of @xmath76 differs by a factor of @xmath8 from the standard mem formulation ; this is , however , only a matter of convention ) .",
    "the entropy term @xmath171 $ ] takes the form @xmath171 = -\\int dz a(z ) \\ln \\left [ a(z ) / m(z ) \\right]$ ] with @xmath173 being the default model . for very large values of @xmath172 ,",
    "the default model term dominates in @xmath174 , reflecting our ignorance about the system . for very low values of @xmath172 , the `` energy '' term @xmath76 dominates , reflecting the quality of the input data . for intermediate values of @xmath172 ,",
    "one interpolates between these two limits and obtains a trade - off between accuracy and smooth behavior enforced by the default model .",
    "we are using bryan s method  @xcite to implement the minimization procedure : the final answer is obtained by averaging over all values of @xmath172 weighted with the respective _ a posteriori _ probability ( we saw however little difference between bryan s method and the classical mem in the examples below ) . in bryan s method ,",
    "a singular value decomposition is also applied , which reflects the fact that the finite precision of storing floating point numbers in combination with the poor conditioning of the kernel puts severe limitations on the information that can possibly be retrieved .",
    "one can hence reduce the search space at no substantial loss ; in practice , only 5 to 20 search directions survive this step .",
    "the remaining minimization is performed by the levenberg - marquadt algorithm .",
    "bryan s method is , after 25 years , still the _ de facto _ standard for inversion problems in condensed matter physics .",
    "one of its most attractive features is its speed : a few seconds on a laptop usually suffice to get a reasonable answer provided good starting parameters ( for the grid , default model , and the range of @xmath172 ) have been found .",
    "nevertheless , the obtained answer ( including the _ a posteriori _ probability distributions ) should always be carefully checked .",
    "a major issue is that the solution may strongly depend on the default model .",
    "( note that the error bars which ref .",
    "calculates , are conditional on the default model and do hence not reflect variability with respect to different default models . )",
    "a practitioner usually wants to explore different ( classes of ) default models in order to get an idea of the robustness of the obtained answer , and sometimes to examine if lower values of @xmath76 can be found for other solutions which are equally smooth . in this regard , all these solutions are reminiscent of basic solutions discussed above , but the probability density of solutions is different due to the difference in protocols : in the spirit of mem one does not want default models that are too similar or default models that are too close to the obtained answer ( an iteration where the new default model is the answer from a previous run , is considered a self - defeating strategy ) .",
    "this raises an important question of what strategy should be used to produce a representative set of basic solutions within mem .",
    "one possibility is stochastic exploration of the configuration space of default models .",
    "we perform blind tests of our method for two different kernels .",
    "the function @xmath175 was prepared from equation ( [ eq : ac0 ] ) and uncorrelated gaussian noise was afterwards added to @xmath175 .      in this subsection",
    ", we assume that the spectral function @xmath0 is identically equal to zero at @xmath176 , non - negative at @xmath177 , and the kernel is @xmath178 , see eq .",
    "( [ eq : spectral ] ) .    .",
    "shown is the comparison between the actual spectrum ( red solid line ) , the smooth socc spectrum ( blue short - dashed line ) , and the pulled - up high - energy peak socc solution ( green dashed line ) .",
    "the error bars for the smooth socc spectrum @xmath179 are determined from eq .",
    "( [ eq : disp ] ) . ]    . shown is the comparison between the actual spectrum ( red solid line ) , the mem spectrum with a flat default model ( blue short dashed line ) , and the mem spectrum with a pulled - up high - energy peak in the default model ( green dashed line ) . ]    the spectral function for test  1 and test  2 ( shown in fig .",
    "[ fig : two_peaks ] ) contains two peaks of finite width and has the following form ( up to a normalization constant ) @xmath180 where @xmath181 , @xmath182 , @xmath183 , @xmath184 , @xmath185 , and @xmath186 .",
    "the spectrum is normalized to unity before adding uncorrelated gaussian noise with relative standard deviation @xmath187 for test  1 and @xmath188 for test 2 .    in the spectrum for test 3",
    "the low - frequency peak is not a gaussian but a @xmath50-function with the same position and weight , @xmath189 where @xmath181 , @xmath182 , @xmath184 , @xmath185 , and @xmath186 .",
    "the relative standard deviation of the uncorrelated gaussian noise is @xmath188 .     of the original spectrum . ]    . shown is the comparison between the actual spectrum ( red solid line ) and the smooth socc spectrum ( blue short - dashed line ) , the pulled - up high - energy peak socc solution ( green dashed line ) .",
    "the error bars for smooth socc spectrum @xmath179 determined from eq .",
    "( [ eq : disp ] ) . ]    . shown is the comparison between the actual spectrum ( red solid line ) , mem with a flat default model ( blue short - dashed line ) , and mem with a pulled - up high - energy peak in the default model ( green dashed line ) . ]",
    "the challenge for nac is to judge whether one can resolve the width of the high - frequency peak . to this end",
    "we consider two possible setups for mem and socc . in the standard setup",
    "we assume a flat default model for mem and the socc procedure of generating smooth solutions as described in sec .",
    "[ subsec : final ] in the absence of the default model penalty @xmath133 . to study possible deformations of the second peak",
    ", we then introduce a narrow - peak default model in mem and re - run the simulation , or , in the case socc , we insist that the final solution goes through a much higher point at the peak maximum .",
    "the width of the high - frequency peak is deemed impossible to resolve if one can reduce it by a factor of two , while the spectrum remains well - defined .",
    "we note that better reproducibility of the low - frequency peak is a particular property of the kernel ( [ eq : spectral ] ) .",
    "for example , for the analytic continuation of the current - current correlation function to the optical conductivity , the main challenge is to resolve the spectral density at zero frequency.@xcite    -function at low frequency and a peak of finite width at high frequency with noise level @xmath190 . shown is the comparison between the actual spectrum ( red solid line ) , the smooth socc spectrum ( blue short - dashed line ) , and the maximally pulled - up high - energy peak socc solution ( green dashed line ) .",
    "the error bars for smooth socc spectrum @xmath179 determined from eq .",
    "( [ eq : disp ] ) . ]",
    "-function at low frequency and a peak of finite width at high frequency with noise level @xmath190 .",
    "shown is the comparison between the actual spectrum ( red solid line ) , the mem spectrum with a flat default model ( blue short - dashed line ) , and the mem spectrum with a double gaussian default model where the second peak is maximally pulled up ( green dashed line ) . ]",
    "analysis of test  1 shows that the low - energy peak can be well resolved by both socc and mem ( see the insets in figs .",
    "[ fig : test_1_som ] and [ fig : test_1_mem ] , respectively ) . on the other hand ,",
    "the high - energy peak width is severely overestimated by both methods in the standard setup ( figs .",
    "[ fig : test_1_som ] and [ fig : test_1_mem ] ) .",
    "however , both methods allow one to pull the high - frequency peak up at least by a factor of four above the actual spectrum .",
    "specifically , if the second peak in the mem default model is set to be much narrower than the actual one , then mem produces an answer of the same width as this default model .",
    "similarly , the superposition of basic socc solutions can be forced to have a much higher amplitude at the second peak maximum by employing an appropriate target function . in socc ,",
    "one may also see direct evidence that the second peak width is questionable by considering statistics of the second moments @xmath191 for the high - frequency peak among all basic solutions . the corresponding distribution is presented in fig .",
    "[ fig : semom ] . the probability to find a solution with a vanishing width ( @xmath192 ) for the high - frequency peak does not go to zero for test  1 and , hence",
    ", the imaginary - time data for @xmath175 ( within their accuracy ) do not rule out a @xmath50-function for the high - frequency peak .",
    "further insight is provided by test  2 , which differs from test  1 only in the noise level , which is reduced by two orders of magnitude .",
    "one readily observes that , in contrast to test  1 , the standard setups of socc and mem give a very good description of the high - frequency peak ( figs .  [",
    "fig : test_2_som ] and [ fig : test_2_mem ] ) .",
    "does this mean that one can be absolutely sure that the width of the peak is finite ?",
    "the answer is no , because one can still easily pull the peak up by a factor of four .",
    "moreover , socc analysis of second moments ( see fig .  [",
    "fig : semom ] ) demonstrates that a @xmath50-functional shaped second peak is still a possibility , despite improved error bars . in these examples ,",
    "tighter error bars allow only to reduce the upper bound on the width of the second peak .",
    "much smaller error bars , which are unrealistic for monte carlo simulations of @xmath175 , would be required to controllably resolve the actual width of the second peak .    test  3 has the same gaussian noise as test 2 but the low - frequency peak is now replaced by a @xmath50-function , see eq .",
    "( [ case_ad ] ) .",
    "this crucially changes the results .",
    "now the high - frequency peak is well reproduced not only in the standard setup of socc and mem but also in attempts to pull the solution up , see figs .",
    "[ fig : test_3_som ] and [ fig : test_3_mem ] , respectively .",
    "stability of results for the second peak width is also evident in the probability distribution for the second moment shown in fig .",
    "[ fig : semom ] .",
    "the distribution is peaked at the correct value @xmath186 and is rather narrow , indicating that a narrower peak would compromise the error bars .",
    "we emphasize that the success of resolving the width of the second peak in test  3 is due to a combination of two circumstances : the small width of the first peak and the high accuracy of the input data . to see this , it is instructive to consider the physical example of the fermi polaron ( see sec .",
    "[ sec : polarons ] ) , where the width of the first peak is very small , but the accuracy of the input data is significantly lower than in test  3 .    .",
    "shown is the comparison between the actual spectrum ( red solid line ) , the smooth socc spectrum ( blue short - dashed line ) , and the maximally pulled up central peak socc solution ( dashed green line ) .",
    "the logarithmic plot in the inset highlights the comparison of low - intensity features .",
    "the error bars for the smooth socc spectrum @xmath179 are determined from eq .",
    "( [ eq : disp ] ) . ]    . shown is the comparison between the actual spectrum ( red solid line ) and the mem spectrum in the default setup ( blue dashed line ) . ]",
    "test  4 analyzes the possibility of resolving spectral densities at the fermi level from the analytic continuation of @xmath193 . here , the spectrum @xmath0 is defined in the range @xmath194 and the kernel is @xmath195 with @xmath196",
    ". the uncorrelated gaussian noise is added at the @xmath190 level .",
    "one can see in figs .",
    "[ fig : fermi_som ] and [ fig : fermi_mem ] that both socc and mem give a good description of the spectral function in the vicinity of the chemical potential ( at @xmath197 ) .",
    "also , the height of the middle peak at near zero frequency can not be significantly pulled up without distorting the rest of spectrum . specifically for mem ,",
    "trying different default models with one , two , or three gaussian peaks did not improve the answer ; in all cases trying to find narrower peaks resulted in secondary oscillations reminiscent of numerical instabilities .",
    "we conclude that the fermionic spectrum can be restored for the given parameters with high quality .",
    "we now test the socc method on a physical system  the resonant fermi polaron ( a spin - down fermion in a sea of noninteracting spin - up fermions)@xcite ; here in three dimensions and for equal mass of spin - up and spin - down particles .",
    "the coupling between the polaron and its environment is characterized by a single dimensionless parameter @xmath198 , where @xmath199 is the fermi wave vector and @xmath200 the s - wave scattering length . here",
    "we examine a typical situation at @xmath201 when the polaron state at zero momentum is metastable but has a very long relaxation time , implying that the lowest peak in the polaron spectral function is a sharp resonance nearly indistinguishable from a @xmath50-function .",
    "the imaginary - time polaron green s function at zero temperature and zero momentum , @xmath202 , was obtained with diagrammatic monte carlo , see refs .  .",
    "we are able to achieve very high precision in our results for @xmath193 with a relative error as low as @xmath203 at @xmath204 close to zero , @xmath205 around @xmath206 ( where @xmath207 is the fermi energy of spin - up fermions ) and a few percent at the largest @xmath204 considered for the analytic continuation .",
    "the kernel at zero temperature is @xmath208 .",
    "the polaron spectral function features two peaks .",
    "the position and weight of the first polaron peak are fixed with high accuracy by the asymptotic decay of the green s function , @xmath209 .",
    "our data at large @xmath210 can be fitted to a single exponential ( within error bars ) indicating that the polaron remains a well - defined quasi - particle in this parameter range .",
    "the particle - hole continuum emerges at higher frequencies as a second broad peak .",
    "a key question we want to address here is its spectral width .",
    "this has been discussed in the context of the repulsive polaron state.@xcite in order for it to qualify to be a well - defined quasiparticle , the peak width needs to be sufficiently narrow ( much smaller than the fermi energy , corresponding to a sufficiently long life time ) .",
    "thus resolving the width accurately is very important to correctly interpret this spectrum .",
    "note that this spectrum has the same general features as the spectrum for test 1 in the previous section .     at zero momentum and zero temperature .",
    "the smooth socc spectrum with the second peak dispersion @xmath211 is shown by the blue short dashed line . however",
    ", a much narrower solution for the second peak with @xmath212 ( green dashed line ) can also be obtained from the same set of basic solutions . ]",
    "when socc is used to produce a smooth solution the second peak emerges as a broad spectral feature .",
    "if this was indeed the case , the metastable repulsive polaron picture would be inapplicable for @xmath201 .",
    "however , the same set of basic solutions can be optimized to have a much narrower peak , see fig .",
    "[ fig : repulsive ] , implying that a well - defined repulsive polaron quasi - particle can not be ruled out .",
    "given that the second peak dispersion can be reduced by a factor of four without compromising the accuracy of the final solution , we have to conclude that the quality of the input data is insufficient to determine the actual width .",
    "the most challenging aspect of numerical analytic continuation is not the algorithm of finding a stable ( smooth ) solution consistent with the input data , but the protocol of assessing its accuracy and unambiguity .",
    "we have implemented such a protocol based on the method of stochastic optimization with consistent constraints and demonstrated how a similar strategy can be followed with the maximum entropy method by exploring the space of default models .",
    "irrespective of the method , the procedure has to deal with either integrals of the spectral function ( rather than the function itself ) and/or certain _ a priori _ and _ a posteriori _ constraints consistent with the error bars on the input data .",
    "it is important to distinguish between two cases .",
    "the first ( simplest ) case is when all physically meaningful solutions do not differ substantially , upon possible smearing of unimportant ( below the resolution ) fine details of the otherwise smooth spectral function .",
    "the second case , exemplified by the spectral function in fig .",
    "[ fig : two_peaks ] , is when a piece of important physical information is inevitably lost . in the first case",
    ", a reasonable characterization of uncertainties can be achieved by coarse - graining , like , e.g. , eq .  ( [ eq : ac1 ] ) . in the second case , one has to employ a more elaborate approach to reveal the different possible physical solutions that do not compromise the error bars of the input data .",
    "much of our attention has been paid to the protocol of treating the second case .",
    "we have shown how it can be handled with socc and modified mem . with mem one has to explore various default models and resulting solutions that remain consistent with input error bars .",
    "a useful feature of the socc approach is that such an analysis  and , more generally , the application of all possible consistent constraints  can be implemented at the post - processing stage using a representative set of  basic \" solutions generated by the stochastic - optimization protocol .",
    "the linearity of the problem ( [ eq : ac0 ] ) is crucial here , as it guarantees that any superposition of basic solutions with non - negative weights is also a solution to eq .",
    "( [ eq : ac0 ] ) within the same or better level of accuracy .",
    "even if the superposition coefficients are allowed to be negative , the procedure typically keeps the accuracy of the final solution at the level of basic solutions .",
    "this allows one to implement consistent constraints by choosing the superposition coefficients to minimize the corresponding objective function .",
    "_ acknowledgements .",
    "_ this work was supported by the simons collaboration on the many electron problem , the national science foundation under the grant phy-1314735 , the muri program ",
    "new quantum phases of matter \" from afosr , and fp7/erc starting grant no . 306897 .",
    "is supported by the impact program of the council for science , technology and innovation ( cabinet office , government of japan ) .",
    "our maximum entropy implementation builds on the alps implementation.@xcite"
  ],
  "abstract_text": [
    "<S> we formulate the problem of numerical analytic continuation in a way that lets us draw meaningful conclusions about properties of the spectral function based solely on the input data . apart from ensuring consistency with the input data ( within their error bars ) and </S>",
    "<S> the _ a priori _ and _ a posteriori _ ( conditional ) constraints , it is crucial to reliably characterize the accuracy  or even ambiguity  of the output . </S>",
    "<S> we explain how these challenges can be met with two approaches : stochastic optimization with consistent constraints and the modified maximum entropy method . </S>",
    "<S> we perform illustrative tests for spectra with a double - peak structure , where we critically examine which spectral properties are accessible and which ones are lost . for an important practical example , we apply our protocol to the fermi polaron problem . </S>"
  ]
}