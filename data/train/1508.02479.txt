{
  "article_text": [
    "we consider the problem of hierarchical classification .",
    "that is , a classification problem when the labels are leaves in a large hierarchy or taxonomy specifying the relationship between labels .",
    "such hierarchies have been extensively used to improve accuracy @xcite in domains such as document categorization @xcite , web content classification @xcite , and image annotation @xcite . in some problems , taking advantage of the hierarchy",
    "is essential since each individual labels ( leaves in the hierarchy ) might have only a few training examples associated with it .",
    "we focus on hierarchical svm @xcite , which is a structured svm problem with the structure specified by the given hierarchy .",
    "structured svms are simple compared to other hierarchical classification methods , and yield convex optimization problems with straight - forward gradients .",
    "however , as we shall see , adapting structured svms to large - scale hierarchical problems can be problematic and requires care .",
    "we will demonstrate that `` standard '' hierarchical svm suffers from several deficiencies , mostly related to lack of normalization with respect to different path - length and different label sizes in multi - label problems , which might result in poor performance , possibly not providing any improvement over a `` flat '' method which ignores the hierarchy . to amend these problems , we present the normalized hierarchical svm ( nhsvm ) .",
    "the nhsvm is based on normalization weights which we set according to the hierarchy , but not based on the data .",
    "we then go one step further and learn these normalization weights discriminatively . beyond improved performance , this results in a model that can be viewed as a constrained matrix factorization for multi - class classification , and allows us to understand the relationship between hierarchical svms and matrix - factorization based multi - class learning @xcite .",
    "we also extend hierarchical svms to issues frequently encountered in practice , such as multi - label problems ( each document might be labeled with several leaves ) and taxonomies that are dags rather then trees .",
    "we present a scalable training approach and apply our methods to large scale problems , with up to hundreds of thousands of labels and tens of millions of instances , obtaining significant improvements over standard hierarchical svms and state - of - the - art results on a hierarchical classification benchmark .",
    "much research was conducted regarding hierarchical multi - class or multi - label classification .",
    "the differences with other methods lies in normalization of structure , scalability of the optimization , and utilization of the existing label structure .",
    "our work is based upon hierarchical classification using svm which is introduced in @xcite .",
    "the model extends the multi - class svm to hierarchical structure .",
    "an extension to the multi - label case was presented by @xcite . in @xcite , an efficient dual optimization method for a kernel - based structural svm and",
    "weighted decomposable losses are presented for a tree structured multi - label problem .",
    "these methods focus on dual optimization which does not scale up to our focused datasets with large instances and large number of labels .",
    "also , the previous methods do not consider the normalization of the structures , which is important for such large structures .",
    "for instance , we focus on the wikipedia dataset in large scale hierarchical text classification competition ( lshtc )",
    ". it has 400k instances with a bag of words representation of wikipedia pages which are multi - labeled to its categories .",
    "the labels are the leaves from a dag structure with 65k nodes .",
    "notice that the scale of dataset is very large compared to dataset considered in previous mentioned methods .",
    "for instance , in @xcite the largest dataset has 7k instances and 233 nodes .",
    "extensions of knn , meta - learning , and ensemble methods were popular methods in the competition .",
    "@xcite presented a model with a multi - task objective and an efficient parallelizable optimization method for dataset with a large structure and number of instances .",
    "however , its regularization suffers the same normalization issue , and relies on the other meta learning method@xcite in the post - processing for high accuracy in multi - label problems .",
    "there are alternatives to svms approaches @xcite , however , the approaches are not scalable to large scale dataset with large structures .    another direction is to learn the structure rather than utilizing given structure .",
    "@xcite focus on learning a small structure from the data , which is is very different from using a known structure .",
    "a fast ranking method@xcite is proposed for a large dataset .",
    "it builds a tree structure for ranking of labels .",
    "however , it does not utilize given hierarchy , and is not directly a multi - label classifier .",
    "let @xmath0 be a tree or a _ directed acyclic graph _ ( dag ) representing a label structure with @xmath1 nodes .",
    "denote the set of leaves nodes in @xmath0 as @xmath2 .",
    "for each @xmath3 $ ] , define the sets of parent , children , ancestor , and descendent nodes of @xmath4 as @xmath5 , @xmath6 , @xmath7 , and @xmath8 , respectively .",
    "additionally , denote the ancestor nodes of @xmath4 including node @xmath4 as @xmath9 , and similarly , denote @xmath10 for @xmath11 .",
    "we also extend the notation above for sets of nodes to indicate the union of the corresponding sets , i.e. , @xmath12 .",
    "let @xmath13 be the training data of @xmath14 instances .",
    "each @xmath15 is a feature vector and it is labeled with either a leaf ( in single - label problems ) or a set of leaves ( in multi - label problems ) of @xmath0 .",
    "we will represent the labels @xmath16 as subsets of the nodes of the graph , where we include the indicated leaves and all their ancestors .",
    "that is , the label space ( set of possible labels ) is @xmath17 for single - label problems , and @xmath18 for multi - label problems .",
    "we review the hierarchical structured svm introduced in @xcite and extended to the multi - label case in @xcite .",
    "consider @xmath19 , and let the @xmath4-th row vector @xmath20 be be weights of the node @xmath3 $ ] .",
    "define @xmath21 to be the potential of label @xmath22 given feature @xmath23 , which is the sum of the inner products of @xmath23 with the weights of node @xmath24 @xmath25 . if we vectorize @xmath26 , @xmath27^t\\in \\mathbb{r}^{d\\cdot m}$ ] , and define the class - attribute @xmath28 , @xmath29_n=1 $ ] if @xmath30 or 0 otherwise , then @xmath31 where @xmath32 is the kronecker product . with weights",
    "@xmath20 , prediction of an instance @xmath23 amounts to finding the maximum response label @xmath33 given a structural error @xmath34 , for instance a hamming distance @xmath35 }   |\\textbf{1}_{n\\in y'}-\\textbf{1}_{n\\in y}|$ ] , a training a hierarchical structured svm is optimizing : @xmath36 equivalently , in terms of @xmath37 and class - attribute @xmath38 , @xmath39",
    "( left branch ) is halved to without changing decision boundary due to difference in the label structure . ]",
    "a major issue we highlight is that unbalanced structures ( which are frequently encountered in practice ) lead to non - uniform regularization with the standard hierarchical svm . to illustrate this issue ,",
    "consider the two binary tree structures with two leaves shown in figure [ fig : nregularization ] .",
    "implicitly both structures describes the same structure .",
    "recall that the regularization penalty is @xmath40 where each row of @xmath26 is a weight vector for each node . in the left structure ,",
    "the class attributes are @xmath41^t$ ] , and @xmath42^t$ ] , assume @xmath43 , and let the optimal weights of node 1 and node 2 in the left structure be @xmath44 and @xmath45 .",
    "now add a node 3 as a child of node 1 , so that @xmath46^t,\\wedge(y_2)=[0\\ ; \\ ; 1\\ ; 0]^t$ ] .",
    "let @xmath47 and @xmath48 be the new weights for the nodes 1 and 3 .",
    "if we assume @xmath49 , the potential function , and thus the decision boundary remain the same , but the regularization penalty for @xmath50 is halved so that @xmath51 , and @xmath52 .",
    "this can be generalized to any depth , and the regularization penalty can differ arbitrarily for the model with the same decision boundary for different structures . in the given example , the structure on the right imposes half the penalty for the predictor of @xmath50 than that of @xmath53 .",
    "the issue can also be understood in terms of the difference between the norms of @xmath38 for @xmath54 .",
    "let @xmath55 the feature map for an instance vector @xmath23 and a label @xmath22 such that @xmath56 . from ( [ classa ] ) , @xmath57    @xmath58 behaves as a feature map in hierarchical structured svm .",
    "while the model regularizes @xmath37 , the norm of @xmath59 is different for @xmath22 and scales as @xmath60 .",
    "@xmath61    note that @xmath62 and the differences in regularization can grow linearly with the depth of the structure .    to remedy this effect , for each node",
    "@xmath4 we introduce a weight @xmath63 such that the sum of the weights along each path to a leaf is one , i.e. , @xmath64 given such weights , we define the normalized class - attribute @xmath65 and the normalized feature map @xmath66 @xmath67_n=\\begin{cases } \\sqrt{\\alpha_n } & \\mbox{if } y\\in n\\\\ 0 & \\mbox{otherwise}\\end{cases } & & \\tilde{\\phi}(x , y)=\\tilde{\\wedge}(y)\\otimes x \\ ] ] the norm of these vectors are normalized to 1 , independent of @xmath22 , i.e. , @xmath68 for @xmath69 , and the class attribute for each node @xmath4 is fixed to @xmath70 or @xmath71 for all labels .",
    "the choice of @xmath72 is crucial and we present several alternatives ( in our experiments , we choose between them using a hold - out set ) .",
    "for instance , using @xmath73 on the leaves @xmath74 and 0 otherwise will recover the flat model and lose all the information in the hierarchy . to refrain from having a large number zero weight and preserve the information in the hierarchy",
    ", we consider setting @xmath72 optimizing : @xmath75 \\end{aligned}\\ ] ] where @xmath76 . in section [ sec : invar ] , we will show that as @xmath77 , we obtain weights that remedy the effect of the redundant nodes shown in figure [ fig : nregularization ] .",
    "we use with @xmath78 as a possible way of setting the weights . however , when @xmath79 , the optimization problem is no longer strongly convex and it is possible to recover weights of zeroes for most nodes . instead ,",
    "for @xmath79 , we consider the alternative optimization for selecting weights : @xmath80 \\\\   & & & \\alpha_n\\ge\\alpha_p , & & \\forall n\\in [ m ] , \\forall p\\in \\mathcal{p}(n ) \\end{aligned}\\ ] ] we refer to the last constraint as a `` directional constraint '' , as it encourage more of the information to be carried by the leaves and results more even distribution of @xmath72 .",
    "for some dag structures , constraining the sum @xmath81 to be exactly one can result in very flat solution .",
    "for dag structures we therefore relax the constraint to @xmath82 for some parameter @xmath83 ( @xmath84 in our experiments ) .",
    "another source of the imbalance is the non - uniformity of the required margin , which results from the norm of the differences of class - attributes , @xmath85 .",
    "the loss term of each instance in ( [ hsvm2 ] ) is , @xmath86 . and",
    "to have a zero loss @xmath87 , @xmath88 @xmath89 works as the margin requirement to have a zero loss for @xmath22 .",
    "the rhs of the bound scales as norm of @xmath90 scales .",
    "this calls for the use of structural error that scales with the bound .",
    "define normalized structural error @xmath91 @xmath92 and @xmath93 , and @xmath94 and @xmath72 are defined in ( [ nclassa])([beta_1 ] ) . without the normalization ,",
    "this is the square root of the hamming distance , and is similar to a tree induced distance in @xcite .",
    "this view of nonuniform margin gives a justification that the square root of hamming distance or tree induced distance is preferable to hamming distance .      summarizing the above discussion",
    ", we propose the normalized hierarchical svm ( nhsvm ) , which is given in terms of the following objective : @xmath95 instead of imposing a weight for each node , with change of variables @xmath96 , we can write optimization ( [ nhsvm ] ) as changing regularization , @xmath97 also optimization is equivalently written as @xmath98 note that for the single - label problem , normalized hierarchical svm can be viewed as a multi - class svm changing the feature map function to ( [ nclassa ] ) and the loss term to .",
    "therefore , it can be easily applied to problems where flat svm is used , and also popular optimization method for svm , such as @xcite , can be used .    another possible variant of optimization ( [ nhsvm2 ] ) which we experiment with is obtained by dividing inside the max with @xmath99 : @xmath100 there are two interesting properties of the optimization .",
    "the norm of the vector right side of @xmath37 is normalized , i.e. , @xmath101 also the loss term per instance at the decision boundary , which is also the required margin , is normalized to 1 .",
    "however , because normalized class attribute in ( [ nhsvm3 ] ) does not decompose w.r.t nodes as in ( [ nhsvm ] ) , loss augmented inference in ( [ nhsvm3 ] ) is not efficient for multi - label problems .      as we saw in figure",
    "[ fig : nregularization ] , different hierarchical structures can be used to describe the same data , and this causes undesired regularization problems . however , this is a common problem in real - world datasets .",
    "for instance , an _ action _ movie label can be further categorized into a _ cop - action _ movie and a _ hero - action _ movie in one dataset whereas the other dataset uses a action movie as a label .",
    "therefore , it is desired for the learning method of hierarchical model to adapt to this difference and learn a similar model if given dataset describes similar data . proposed normalization can be viewed as an adaptation to this kind of distortions . in particular",
    ", we show that nhsvm is invariant to node duplication .",
    "define duplicated nodes as follows .",
    "assume that there are no unseen nodes in the dataset , i.e. , @xmath102},\\exists i , n\\in \\mathcal{a}(y_i)$ ] .",
    "define two nodes @xmath103 and @xmath104 in @xmath105 $ ] to be _ duplicated _ if @xmath106 .",
    "define the minimal graph @xmath107 to be the graph having a representative node per each duplicated node set by merging each duplicated node set to a node . for the proof , see [ apx : invariance ] .",
    "[ thm : invarinace ] decision boundary of nhsvm with @xmath0 is arbitrarily close to that of nhsvm with the minimum graph @xmath107 as @xmath108 in ( [ beta_1 ] ) approaches 1 , @xmath76 .",
    "in the nhsvm , we set the weights @xmath72 based the graphical structure of the hierarchy , but disregard the data itself .",
    "we presented several options for setting the weights , but it is not clear what the best setting would be , or whether a different setting altogether would be preferable . instead",
    ", here we consider discriminative learning the weights from the data by optimizing a joint objective over the weights and the predictors .",
    "the resulting optimization is equivalent to regularization with a new norm which we call _ structured shared frobenius norm _ or _ structured shared norm_. it explicitly incorporates the information of the label structure @xmath0 .",
    "regularization with the structured shared frobenius norm promotes the models to utilize shared information , thus it is a complexity measure suitable for structured learning .",
    "notice that we only consider multi - class problem in this section .",
    "an efficient algorithm for tree structure is discussed in section [ sec : opt ] .",
    "consider the formulation as a joint optimization over both @xmath72 and @xmath109^t$ ] with fixed @xmath110 ( i.e.  we no longer normalize the margins , only the regularization ) : @xmath111 } \\left \\ { \\sum_{n \\in \\overline{\\mathcal{a}}(l)}u_n x_i \\right . -",
    "\\\\   & & & \\left .",
    "\\sum_{n \\in \\overline{\\mathcal{a}}(l_i ) } u_n x_i   + \\triangle(l , l_i ) \\right \\}\\\\   & \\mbox{s.t . } & \\sum_{n\\in \\overline{\\mathcal{a}}(l ) }   \\alpha_n&\\le1 , & \\forall   l\\in [ y]\\\\   & & \\alpha_n&\\ge0 & \\forall n\\in [ m ] \\end{aligned}\\ ] ] we can think of the first term as a regularization norm @xmath112 and write @xmath113 where the the _ structured shared frobenius norm",
    "_ @xmath112 is defined as : @xmath114 \\end{aligned}\\ ] ] where @xmath115 is the maximum of the @xmath116 norm of row vectors of @xmath117 .",
    "row vectors of @xmath117 can be viewed as coefficient vectors , and row vectors of @xmath118 as factor vectors which decompose the matrix @xmath119 .",
    "the factorization is constrained , though , and must represent the prescribed hierarchy",
    ". we will refer ( [ ssvm ] ) to _ shared svm _ or _",
    "ssvm_.    to better understand the ssvm , we can also define the _ shared frobenius norm _ without the structural constraint as @xmath120 the _ shared frobenius norm _ is a norm between the trace - norm ( aka nuclear norm ) and the max - norm ( aka @xmath121 norm ) , and an upper bounded by frobenius norm :    [ thm : compare ] for @xmath122 @xmath123 where @xmath124 is then the trace norm , and @xmath125 @xmath126 @xmath127 is so - called the max norm @xcite .",
    "the first inequality follows from the fact that @xmath128 , and the second inequality is from taking @xmath129 , or @xmath130 when @xmath4 is an unique node for @xmath131 or 0 for all other nodes in ( [ rho2 ] ) respectively .",
    "we compare the shared norm to the other norms to illustrate the behavior of the shared norm , and summarize in table [ compare_norms ] .",
    "shared norm is upper bounded by frobenius norm , and reduce from it only if sharing the factors @xmath118 is beneficial .",
    "if there is no reduction from sharing as in disjoint feature case in table [ compare_norms ] , it equals to frobenius norm , which is the norm used for multi - class svm .",
    "therefore , this justifies the view of ssvm that it extends multi - class svm to shared structure , i.e. , ssvm is equivalent to multi - class svm if no sharing of weights is beneficial .",
    "this differs from the trace norm , which we can see specifically in disjoint feature case .    [",
    "cols=\"<,^,^,^,^\",options=\"header \" , ]",
    "in this paper we considered the problem of large - scale hierarchical classification , with a given known hierarchy .",
    "our starting point was hierarchical structured svm of @xcite , and we also considered extensions for handling multi - label problems ( where each instance could be tagged with multiple labels from the hierarchy ) and of label hierarchies given by dags , rather then rooted trees , over the labels .",
    "our main contribution was pointing out a normalization problem with this framework , both in the effective regularization for labels of different depths , and in the loss associated with different length paths .",
    "we suggested a practical correction and showed how it yields to significant improvement in prediction accuracy .",
    "in fact , we demonstrate how on a variety of large - scale hierarchical classification tasks , including the large - scale hierarchical text classification competition data , our normalized hierarchical svms outperform all other relevant methods we are aware of ( that work using the same data and can be scaled to the data set sizes ) .",
    "we also briefly discussed connections with matrix factorization approaches to multi - label classification and plan on investigating this direction further in future research .",
    "thm : invarinace[invariance property of nhsvm ] decision boundary of nhsvm with @xmath0 is arbitrarily close to that of nhsvm with the minimum graph @xmath107 as @xmath108 in ( [ beta_1 ] ) approaches 1 , @xmath76 .",
    "we prove by showing that for any @xmath132 variable @xmath72 in ( [ beta_1 ] ) can be reduced to one variable per each set of duplicated nodes in @xmath0 using the optimality conditions , and optimizations ( [ beta_1])([nhsvm ] ) are equivalent to the corresponding optimizations of @xmath107 by change of the variables .",
    "assume there are no duplicated leaves , however , the proof can be easily generalized for the duplicated leaves by introducing an additional constraint on @xmath133 .",
    "let @xmath134 be a mapping from node @xmath135 in graph @xmath107 to a corresponding set of duplicated nodes in @xmath0 .",
    "denote the set of nodes in @xmath0 as @xmath136 , and the set of nodes in @xmath107 as @xmath137 , and the set of leaves in @xmath107 as @xmath138 .",
    "consider ( [ beta_1 ] ) for @xmath0 .",
    "note that ( [ beta_1 ] ) has a constraint on sum of @xmath139 to be 1 for @xmath140 . by the definition of the duplicity ,",
    "if two nodes @xmath103 and @xmath104 are duplicated nodes , they are the ancestors of the same set of the leaves , and term @xmath141 appears in the first constraints of ( [ beta_1 ] ) if and only if term @xmath142 appears , thus we conclude that all the duplicated nodes will appear altogether .",
    "consider a change of variable for each @xmath143 @xmath144 then , ( [ beta_1 ] ) are functions of @xmath145 and ( [ beta_1 ] ) decompose w.r.t @xmath145 . from the convexity of function @xmath146 with @xmath76 , @xmath147 , and jensen s inequality , @xmath148 , minimum of ( [ beta_1 ] )",
    "is attained when @xmath149 for @xmath150 . as @xmath151 approaches 0 ,",
    "where  @xmath152 , @xmath153    plugging ( [ kk ] ) ( [ sub_k ] ) into ( [ beta_1 ] ) , @xmath154 these formulations are same as ( [ beta_1 ] ) for @xmath107 .    thus given @xmath135 ,",
    "@xmath155 is fixed for @xmath156 , and with the same argument for @xmath157 in ( [ nhsvm ] ) , change of variables gives , @xmath158 .",
    "then ( [ nhsvm ] ) is a minimization w.r.t @xmath159 , and the minimum is when @xmath160 for @xmath150 , plugging this in ( [ nhsvm ] ) , @xmath161 by substituting @xmath162 , @xmath163    ( [ nhsvm]),([beta_1 ] ) for @xmath0 are equivalent to those of @xmath107 , thus two solutions are equivalent with a change of variables and the decision boundaries are the same .",
    "we first show a lower bound for @xmath164 , @xmath112 which will be useful for the later proofs .",
    "[ lower_bound]for @xmath165 , @xmath166 where @xmath167 is @xmath22-th row vector of @xmath119 .",
    "let @xmath168 be the matrices which attain minimum in @xmath169 .",
    "since @xmath170 and from the cauchy - schwarz , @xmath171 , and if we square both sides and sum over @xmath172 , @xmath173 which holds for all @xmath174 .    following are the detailed descriptions for table [ compare_norms ] and the sketch of the proofs ."
  ],
  "abstract_text": [
    "<S> we present improved methods of using structured svms in a large - scale hierarchical classification problem , that is when labels are leaves , or sets of leaves , in a tree or a dag . </S>",
    "<S> we examine the need to normalize both the regularization and the margin and show how doing so significantly improves performance , including allowing achieving state - of - the - art results where unnormalized structured svms do not perform better than flat models . </S>",
    "<S> we also describe a further extension of hierarchical svms that highlight the connection between hierarchical svms and matrix factorization models . </S>"
  ]
}