{
  "article_text": [
    "given an image , the goal of detecting and localizing objects is to place a bounding box around the instances of a pre - defined object class , such as cars , faces , person / people @xcite .",
    "with the recent advancement @xcite of deep convolutional neural networks ( cnn ) on object classification , generic object detection is also attracting more and more attention with fast increasing detection accuracy on popular benchmarks @xcite .",
    "the learning of such deep network based detector is often formulated as the problem of minimizing a loss function over a set of hypothesized target windows , where the loss function contains a classification term and optionally a bounding box regression term . in the classical sliding window approach @xcite ,",
    "the window set often contains hundreds of thousands of windows explicitly sampled on a regular grid at multiple scales .",
    "such methods are prohibitively slow for heavy load state - of - the - art cnn classifiers .",
    "recent detectors explore the idea of bottom - up object region proposals @xcite , where a relatively small set of a few thousand windows were pre - selected @xcite and evaluated .",
    "acceleration were made by sharing computation and pooling over the feature maps from the cnn layers @xcite .",
    "these works were further accelerated by integrating the separate region proposal step and the classification step into one network @xcite by using so - called  anchors \" which correspond to regular prototype grid in the image space .",
    "however , the number of windows to be evaluated remains several thousand .",
    "therefore , the speed of such region - based methods depends on a heavy use of fast gpus . when computation power is limited , _",
    "e.g. _ only cpus were available , these pipelines are inevitably slow .",
    "active search methods provide a promising complementary top - down scheme to reduce the number of windows to be evaluated @xcite.when searching or localizing objects , biological vision systems are believed to have a sequential process with changing retinal fixations that gradually accumulate evidence of certainty @xcite .",
    "it is therefore highly desirable , both biologically and computationally , to explore computational models that facilitate object search in such top - down behavior .    typically , these models learn policies to search for an object by sequentially translating and/or reshaping the bounding box detector .",
    "one can view such a search process as an agent searching for the rewarding ground truth bounding boxes and exploit reinforcement learning ( rl ) algorithms to learn a good policy . in general , these methods can achieve reasonably good performance using only dozens of steps ( effectively the number of windows evaluated ) .",
    "we examine the problem of joint active search of multiple objects under interaction . on one hand ,",
    "it is interesting to consider such a collaborative detection  game \" played by multiple agents under an rl setting ; on the other hand , it seems especially beneficial in the context of visual object localization where different objects often appear with certain correlated patterns , _",
    "e.g. _ person riding a bicycle , cups held on top of the table etc .",
    "such objects under interaction often can provide contextual cues to each other .",
    "these cues have good potential to facilitate more efficient search policies .",
    "we make an initial effort to validate such an hypothesis / intuition by devising a computational model .",
    "we present a collaborative multi - agent deep rl algorithm to learn the optimal policy for joint active object localization .",
    "our proposal follows existing wisdom to exploit rl methods but allows for collaborative behaviors among multiple agents in order to utilize contextual information . in this",
    "regard , two key questions are open .",
    "i ) how to make communications effective in between different agents ; and ii ) how to jointly learn good policies for all agents .",
    "we propose to learn inter - agent communication through gated cross connections between the q - networks .",
    "this is facilitated by a novel multi - agent deep q - learning algorithm with joint exploitation sampling and a virtual agent based implementation .",
    "finally , we verify our proposed method on multiple object detection benchmarks .",
    "our model helps to improve the performance of state - of - the - art active localization models and it also reveals interesting co - detection patterns that are intuitively interpretable .    in section [ sec : relate ] , we discuss literatures related to our work . in section [ sec : joint ] , we present the details of the proposed cross q - network structure and a novel multi - agent deep q - learning algorithm that effectively facilitate training of the crossed q - networks . in section [ sec : expr ] , we present comprehensive experiments on multiple popular benchmarks .",
    "section [ sec : concl ] concludes this paper .",
    "here , we summarize our major contributions as follows .    * to our best knowledge , this work presents the first collaborative deep rl solution for joint active object localization .",
    "* we propose a novel multi - agent q - learning solution that facilitates learnable inter - agent communication with gated cross connections between the q - networks . *",
    "our proposal effectively exploits beneficial contextual information between related objects and consistently improve the performance of state - of - the - art active localization models .",
    "* active search . *   the idea of active search for localization is not brand new . to name a few , ",
    "saccade and fixate \" biological pattern were explored in the field of visual attention @xcite . in @xcite , dollar",
    "et al . proposed to estimate pose through cascaded regression steps learnt through gradient descent etc .",
    "latest works on object localization managed to exploit the power of deep learning and achieved more competitive results @xcite .    in @xcite ,",
    "mnih et al . proposed a recurrent neural network ( rnn ) based localization network that accumulatively finds numbers from the cluttered translated mnist dataset . in @xcite ,",
    "garcia et al . proposed to explore statistical relations between consecutive windows and based their model on r - cnn @xcite for generic object detection . in @xcite , yoo et al .",
    "proposed  attentionnet \" where at each current window , a cnn was trained to predict quantized weak directions for the next step to simulate a gradual attention shift . in @xcite ,",
    "the authors explicitly deployed deep rl and achieved promising performance with much fewer window evaluations than main stream region proposal methods .",
    "however , none of these works examine the problem of joint active search of multiple objects . in order to exploit beneficial contextual information among differnt objects , we present collaborative multi - agent deep rl .",
    "we instantiate our idea with caicedo and lazebnik @xcite as a single active search model baseline , but our mechanism could be applied to other baseline models with minor adaptation .    *",
    "deep reinforcement learning .",
    "*   recently , the field of reinforcement learning revives with the power of deep learning @xcite . equipped with effective ideas such as experience replay etc .",
    ", conventional methods , _ e.g. _ q - learning , work out very effectively in learning good policies without intermediate supervision for challenging tasks .",
    "our model benefits from these effective ideas in a similar way as recent active methods @xcite but with specific novel designs motivated by the joint search problem of interest .    multi - agent machine learning and reinforcement learning are not new topics .",
    "however , conventional collaborative rl methods mostly explore hand - crafted communication protocols @xcite . during the preparation of this work , we realize two interesting work that proposed to facilitate learnable communication protocols for multi - agent deep rl @xcite and demonstrate superior performance to non - communication counterparts on control management and game related tasks . in @xcite ,",
    "sukhbaatar et al . proposed",
    " commnet \" where policy networks are facilitated with learnable communication channels learnt via back - propagation . in @xcite , foerster et al . proposed ",
    "differentiable inter - agent learning \" to effectively learn communication for deep q - networks .",
    "our proposal share the idea of utilizing back - propagation or designing differentiable communication channels but have different cross network structure with gates and a novel joint sampling q - learning method .",
    "specifically , our cross network structure used explicit gating mechanism to allow a specific agent to be responsible for certain actions .",
    "this is motivated by the problem of object search where one agent usually has primary contribution to the policy . also different from the training of the unfolded rnns as in @xcite , where long range back - propagation may be less effective",
    ", our joint sampling design facilitates immediate updates of the parameters and could be easily incorporated into the deep q - learning algorithm by introducing an auxiliary concept of virtual agent implementation .",
    "* contextual information for detection . *",
    "another interesting connection relates to works of pose estimation , landmark detection etc . , where one is assigned the task of localizing the positions of all joints or landmarks potentially highly related due to physical constraints .",
    "explicit learning the relationship of different joints / landmarks has been studied in such literatures @xcite .",
    "however similar ideas were rarely explored in general localization problems where such interactions are relatively implicit .",
    "our work partially fills the gap here and proves the concept is similarly applicable in many object - level localization problems .",
    "we start by recalling a state - of - the - art ( single agent ) rl method for object localization @xcite .",
    "reinforcement learning provides a formal framework concerned with how agents take actions in an environment so as to maximize some notion of cumulative reward .",
    "formally , rl defines a set of actions @xmath0 that an agent takes to achieve its goal ; a set of states @xmath1 that represents the agent s understanding / information of the current environment ; and a reward function @xmath2 that helps to learn an optimal policy to guide the agent s actions based on its states .    in @xcite , the entire image is viewed as the environment .",
    "the agent transforms a bounding box according to a set of actions .",
    "the goal of the agent is to land a bounding box at the target object s location .",
    "specifically , the set of actions were defined as follows .",
    "@xmath3 @xmath4__move right , move left , move up , move down , scale bigger , scale smaller , aspect ratio change fatter , aspect ratio change taller , trigger _ _ @xmath5 .",
    "each action makes a discrete change to the box by a factor relative to its current size .",
    "the action _ trigger _ means that the agent thinks it finds the object .",
    "the state representation is defined as a tuple @xmath6 .",
    "@xmath7 is a feature vector of the observed region ( plus some extra margin for context ) extracted from a cnn layer , and @xmath8 is a fixed - size vector of the action history .",
    "the concatenation of @xmath7 and @xmath8 is fed into a typical q - network of two fully connected layers .",
    "the network outputs a 9-dimensional vector corresponds to nine action choices . in figure",
    "[ fig : joint ] , the networks shown in the same color _",
    "e.g. _ in blue / red provide illustrations of this architecture .    the reward function @xmath9 is defined for an agent when it takes the action @xmath10 to move from state @xmath11 to @xmath12 .",
    "@xmath13 where @xmath14 is the intersection - over - union ( iou ) between the target object bounding box @xmath15 and the predicted box @xmath16 .    with the action set , state set and reward function defined , the authors in @xcite directly applied deep q - learning @xcite to learn the optimal policy .",
    "more details on setting parameters can be found in @xcite .",
    "they also proposed an interesting design for setting masks in the image after taking the trigger action .",
    "this design allows for effective detection of multiple instances of the same class . finally , the authors applied a post svm classifier to all windows in the trajectory to boost performance .",
    "we generalize the single agent rl model for joint object search .",
    "the key concepts include gated cross connections between different q - networks ; joint exploitation sampling for generating corresponding training data , and a virtual agent implementation that facilitates easy adaptation to existing deep q - learning algorithm .",
    "specifically , q - learning is an rl algorithm used to find an optimal action - selection policy .",
    "the q - function ( action - value function ) of a policy @xmath17 is defined as @xmath18 $ ] where the subscribes of @xmath19 denote the time step .",
    "the optimal action - value function obeys the bellman optimality equation @xmath20 $ ] where @xmath21 is the specific reward by taking action @xmath10 to move state @xmath11 to @xmath12 and @xmath22 $ ] is a discount factor for future returns .",
    "deep q - learning @xcite uses deep neural networks to represent the q - function , _",
    "i.e. _ @xmath23 where @xmath24 is the network parameters .",
    "( a common choice of the q - network consists of two fully connected layers as illustrated in figure [ fig : joint ] . )",
    "note that , suppose for each agent @xmath25 we instantiate one q - network @xmath26 , in the setting of multi - agent rl , one would naturally desire a q - function ( with a slight abuse of notation , we keep using q - function here ) that facilitates inter - agent communication @xmath27 where @xmath28 denotes some form of messages sent out from agent @xmath25 and @xmath29 denotes messages received from other agents .    conventionally , @xmath30 is often hand crafted based on prior knowledge about the actions and the states . this can be formalized as a function of @xmath31 where @xmath32 is manually designed .",
    "therefore , a natural idea would be to construct differential messages where @xmath32 could be learned via gradient back - propagation .",
    "this idea is intuitive and reasonable in the same sense of many deep learning successes where learnable features outperform hand crafted ones .",
    "specifically , we define an agent - wise q - function as @xmath33 where @xmath34 and @xmath32 represents parameters related to actions and messages respectively .",
    "we would now argue that when q - function were parameterized with deep networks , there are intuitively to the order of @xmath35 ( @xmath36 is the number of layers of the q - network ) possible configurations for us to construct message channels .",
    "this is because the messages could be emitted and received at any layers .",
    "moveover , there should be no global optimal configuration , instead suitable configuration of the message channel should be selected in a problem - dependent manner .",
    "we notice two recent work also propose to facilitate learnable communication protocols for multi - agent deep rl @xcite applied to control management and game related tasks respectively .",
    "however , we notice that one important insight is missing from the current trend .",
    "messages are often taken - in in a non - discriminative manner and merged with the information flows in the network directly .",
    "actually , allowing the messages to go through an explicit learnable gate ( as did in lstm cells @xcite ) helps better merging the information and facilitates agent - responsible actions .",
    "the idea is motivated from the object search problem of our interest . in general , when searching for a specific object , we would like the agent in charge of detecting the target class to be a primary source of making decisions .",
    "meanwhile we also want to allow other agents to contribute their advices especially when the primary source feels confused in certain situations .",
    "learnable gating mechanism is a natural fit .    specifically , we design our cross q - network message channels as illustrated in figure [ fig : joint ] .",
    "we add cross connection from the penultimate layer between q - networks of different agents .",
    "we denote the output from this layer of the q - network of agent @xmath25 as @xmath37 .",
    "we then have @xmath38 where @xmath39 represent the sigmoid function such that @xmath40 .",
    "now instead of directly inputting @xmath37 to the next layer as in the single agent case , we also take in the messages from other sources weighted by gates and define @xmath41 note that , the sigmoid function tends to push the output to approximately @xmath42 or @xmath43 . therefore , with this simple gating induced , we are able to learn effective agent - responsible decisions .",
    "this helps us to better understand the searching process .",
    "moreover , now that many actions were effectively determined by one primary agent ( and so will the corresponding gradient updates discussed later ) , one can directly apply learnt networks even when other agents do not co - exist .",
    "we now turn to the problem of jointly training all q - networks .",
    "since we do not have any immediate supervision in an rl setting , we can not directly back propagate gradients in a multi - task manner .",
    "the key idea is to jointly sample the next steps during the exploitation phase .",
    "specifically , in the case of a single agent , in order to reach the bellman optimality , the q - learning algorithm proceeds in an iterative fashion . at each iteration , one would sample / choose an action @xmath44 according to the current estimate of the q - function .",
    "one then executes this action @xmath44 in the emulator and observes reward @xmath45 and state @xmath46 . after this , one updates the parameters of the q - function by minimizing the distance of @xmath47 . here",
    "@xmath48 are the parameters of a target network .",
    "@xmath48 can be a copy of the online network but often is another network frozen for a number of iterations while one updates the online network @xmath49 @xcite .    in the multi - agent setting , we propose to sample the action @xmath50 of agent @xmath25 according to both the activations of itself and the messages from other agents .",
    "we jointly perform such sampling to all the agents .",
    "for instance , in figure [ fig : joint ] , this corresponds to a joint feed - forward pass from both networks .",
    "these samples are later used to update all parameters by jointly minimizing the following distance for all @xmath25 .",
    "@xmath51 since the messages are also differential , joint minimization of the above functions will update parameters related to each of the agents as well as all the message channels in - between . specifically , the gradient updates of @xmath52 comes from the loss of itself _",
    "i.e. _ @xmath53 , while the gradient updates of @xmath54 comes from the loss of other agents _ i.e. _ @xmath55 .",
    "note that , in principle we could view all agents under one global markov decision process ( mdp ) assumption and search for an optimality in the joint action space using the regular q - learning algorithm .",
    "the flip side of this choice , however , is a much larger searching space ( @xmath56 v.s .",
    "@xmath57 in the two agent case ) that may require combinatorially much more training data and time . in this regard , the proposed joint sampling strategy can be viewed as an upper - bounding approximation to global optimal .",
    "however , we observe that this proposal effectively facilitates gradient back - propagation to all the parameters and can jointly learn good policies for all the q - networks as desired .",
    "intuitively the joint sampling idea can be implemented via simultaneously forward and backward passes through all q - networks .",
    "however in practice , we adopted an alternative implementation with a concept of virtual agents .",
    "for each q - network of an object class , we assign an actual agent detector .",
    "meantime , for each cross network connection we assign a what we call virtual agent .",
    "the virtual agents share weights of the corresponding layers with the actual agents .",
    "figure [ fig : virtual ] illustrate this idea for the example of figure [ fig : joint ] .",
    "there are two major advantages of this implementation .",
    "1 ) by considering agents in such a separate manner ( and share weights afterwards ) , we can easily incorporate our design to almost all existing rl algorithms .",
    "one can simply implement an extra outer for - loop for all agents followed by necessary weight copying steps .",
    "2 ) more importantly , this also allows each agent , including virtual ones , to maintain its own pool ( replay memory @xcite ) of samples .",
    "these samples are used for updating the corresponding parameters .",
    "note that in modern rl algorithms with deep networks , the concept of replay memory pool are extremely important for stabilizing the learning process .",
    "for example , suppose we would like to jointly train person and bicycle detectors .",
    "during training , we have images that contain both classes @xmath58 and also images that only contain either person @xmath59 or bicycles @xmath60 .",
    "benefit from an agent - wise replay memory as proposed , the actual person and bicycle agents could be effectively trained with data from @xmath61 and @xmath62 respectively , while the cross connections ( represented by virtual agents ) are only trained with data from @xmath58 as desired .",
    "finally , we update the denotation of the q - functions in the context of the virtual agent implementation as follows . @xmath63",
    "the main changes from the definition in equation ( [ eq : q1 ] ) are to use @xmath64 to replace the conceptual out - message @xmath65 and to use post addition to replace the conceptual in - message @xmath66 .",
    "( note that , as illustrated in figure [ fig : virtual ] , we put the gating part inside the q - function by definition . ) specifically , we summarize the final multi - agent q - learning algorithm with joint sampling and virtual agent in algorithm [ alg : alg ] .",
    "although the algorithm applies in general cases , we usually consider only two object classes at the same time , therefore the number of virtual agents is very controllable .",
    "initialize replay memory of all agents @xmath67 initialize all q - networks with random weights ( or potentially with pre - trained networks )",
    "although different classes of objects co - exist in many situations in real life , there are few datasets explicitly collect data for joint detection tasks .",
    "however , we notice that many images from popular detection datasets such as the pascal voc datasets and the coco dataset have labeled objects of different classes and these images were categorized under all related classes .",
    "these images naturally provide a source for us to construct some useful datasets to validate our hypothesis and methods .",
    "specifically , we selected : \\{_person+bicycle ( voc ) , ball+racket ( coco ) , person+handbag ( coco ) , keyboard+laptop ( coco)_}. with these pairs , we construct two datasets for evaluation purpose . @xmath68 consists of images that only contain one object for each class . this dataset is used to prove certain concepts since learning and testing tend to be more effective on this relatively cleaned dataset .",
    "@xmath69 consists of all images of the person and bicycle categories from the pascal voc datasets .",
    "this one is used to evaluated our proposed method against results of existing single agent models .      for comparison purpose",
    ", we implemented the single agent model precisely according to @xcite .",
    "specifically , we use a cnn architecture of the zf - net @xcite as the cnn feature extractor and use the layer fc6 ( of dimension 4096 ) as the feature vector @xmath7 .",
    "( more advanced cnn models such as @xcite could also be used to potentially further increase detection accuracy , but they are not the focus of our study in this paper . ) action history @xmath8 encodes 10 past actions and therefore has a dimension of 90 .",
    "@xmath11 is then of dimension 4186 .",
    "the q - network consists 2 fully connected layers of dimension 1024 and the final output is a 9 dimensional vector representing q - values for 9 actions .",
    "@xmath70-greedy training were applied to balance between exploration and exploitation . during testing ,",
    "a maximum of 200 steps are used . for more details",
    ", please refer to @xcite .",
    "since the authors of @xcite did not release their code , we implemented our own version .",
    "we manage to have achieved very close performance as reported in @xcite though not exactly the same .",
    "the differences may be due to the randomness involved in sampling .    in the case of multiple agents ,",
    "cross connections between q - networks are implemented as a fully connected layer from one agent s penultimate layer to another agent s last layer with a post multiplication by a scalar gate as defined in equation ( [ eq : gate ] ) .",
    "the dimensions are consistent with the corresponding layers in the single agents . for joint training , we initialize each actual single agent network using pre - trained models and initialize cross connection with random weights .",
    "we applied the @xmath70-greedy strategy of @xcite where we have tuned the learning rates to achieve better convergence in our case .",
    "we report detection results from the joint model on dataset @xmath68 since it contains both classes by construction ; and report detection results using fine - tuned single agent model by joint training , which demonstrates the ability of the gating mechanism to facilitate agent specific inference and learning .      in table",
    "[ tab : test1 ] and table [ tab : test2 ] , we demonstrate the performance of our proposal when compared with single agent models .",
    "our joint model consistently outperforms the single agent model on dataset @xmath68 .",
    "we notice that on the combinations of _ person+bicycle ( voc ) _ and _ laptop+keyboard ( coco ) _ , the improvement is much more obvious .",
    "this is because the configuration of these combinations are relatively more stable across images , _",
    "e.g. _ person riding a bike and laptop contains the keyboard etc .",
    "meanwhile , the configurations of _ person+handbag ( coco ) _ and _ ball+racket ( coco ) _ have multiple modes in all the images and more  noisy \" images that contain little information for co - localization .    .localization accuracy on @xmath68 .",
    "top : single , bottom : joint . [ cols=\"^,^,^,^,^,^,^,^ \" , ]     we noticed that even though such pairs do not display a fixed spatial correlation , they often have several major configurations of coexisting patterns",
    ". therefore we can still consistently achieve better performance than single agent models , showcasing that meaningful messages were learned .",
    "the pair of  chair+tv \" is the least of this case and the positions of chairs and televisions in the images seem rather random even though they often coexist . in this",
    "setting , our joint model achieved similar performance with single models .",
    "this phenomenon shows that when no clear collaborative information exists , our proposal can perform as well as single agent models without messing up .",
    "we attribute this property to the gating mechanism by design .",
    "joint search of multiple objects under interaction often provides contextual cues to each other . by treating each detector as an agent , we present the first collaborative multi - agent deep reinforcement learning method that effectively learns the optimal policy for joint active object localization .",
    "our technical contributions lie in the learnable cross q - network communications and the joint exploitation sampling strategy .",
    "more importantly , we make a first stab to validate the concept of collaborative object localization by devising a computational model , which reveals interesting and intuitive co - detection patterns ."
  ],
  "abstract_text": [
    "<S> we examine the problem of joint top - down active search of multiple objects under interaction , _ </S>",
    "<S> e.g. _ , person riding a bicycle , cups held by the table , etc .. such objects under interaction often can provide contextual cues to each other to facilitate more efficient search . by treating each detector as an agent , we present the first collaborative multi - agent deep reinforcement learning algorithm to learn the optimal policy for joint active object localization , which effectively exploits such beneficial contextual information . </S>",
    "<S> we learn inter - agent communication through cross connections with gates between the q - networks , which is facilitated by a novel multi - agent deep q - learning algorithm with joint exploitation sampling . </S>",
    "<S> we verify our proposed method on multiple object detection benchmarks . </S>",
    "<S> not only does our model help to improve the performance of state - of - the - art active localization models , it also reveals interesting co - detection patterns that are intuitively interpretable . </S>"
  ]
}