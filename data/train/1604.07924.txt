{
  "article_text": [
    "compressed sensing ( cs ) techniques @xcite enable efficient reconstruction of a sparse signal under linear measurements far less than its physical dimension .",
    "mathematically , cs aims to recover an @xmath3-dimensional vector @xmath4 with few non - zero components from an under - determined linear system @xmath5 of just @xmath6 equations , where @xmath7 is a known measurement matrix .",
    "the first cs technique is the convex @xmath0 minimization or the so - called basis pursuit @xcite : @xmath8 breakthrough results @xcite have established that when matrix @xmath9 satisfies certain restricted isometry property ( rip ) , the solution to ( [ l1 ] ) is exactly @xmath10 .",
    "it was shown that with overwhelming probability , several random ensembles such as random gaussian , random bernoulli , and random partial fourier matrices , are of rip type @xcite .",
    "note that ( [ l1 ] ) is just a minimization principle rather than an algorithm for retrieving @xmath10 .",
    "algorithms for solving ( [ l1 ] ) and its associated @xmath0 regularization problem @xcite : @xmath11 include bregman methods @xcite , alternating direction algorithms @xcite , iterative thresholding methods @xcite among others @xcite",
    ".    inspired by the success of basis pursuit , researchers then began to investigate various non - convex cs models and algorithms .",
    "more and more empirical studies have shown that non - convex cs methods usually outperform basis pursuit when matrix @xmath9 is rip - like , in the sense that they require fewer linear measurements to reconstruct signals of interest .",
    "instead of minimizing @xmath0 norm , it is natural to consider minimization of non - convex ( concave ) sparse metrics , for instance , @xmath12 ( quasi-)norm ( @xmath13 ) @xcite , capped-@xmath0 @xcite , and transformed-@xmath0 @xcite .",
    "another category of cs methods in spirit rely on support detection of @xmath10 . to name a few , there are orthogonal matching pursuit ( omp ) @xcite , iterative hard thresholding ( iht ) @xcite , ( re)weighted-@xmath0 scheme @xcite , iterative support detection ( isd ) @xcite , and their variations @xcite .    on the other hand , it has been proved that even if @xmath9 is not rip - like and contains highly correlated columns , basis pursuit still enables sparse recovery under certain conditions of @xmath10 involving its support @xcite . in this scenario , most of the existing non - convex cs methods , however , are not that robust to the conditioning of @xmath9 , as suggested by @xcite .",
    "their success rates will drop as columns of @xmath9 become more and more correlated . in @xcite , based on the difference of convex functions algorithm ( dca ) @xcite , the authors propose dca-@xmath14 for minimizing the difference of @xmath0 and @xmath15 norms @xcite .",
    "extensive numerical experiments @xcite imply that dca-@xmath14 algorithm consistently outperforms @xmath0 minimization , irrespective of the conditioning of @xmath9 .",
    "stimulated by the empirical evidence found in @xcite , we propose a general dca - based cs framework for the minimization of a class of concave sparse metrics .",
    "more precisely , we consider the reconstruction of a sparse vector @xmath4 by minimizing sparsity - promoting metrics : @xmath16 throughout the paper , we assume that @xmath17 always takes the form @xmath18 unless otherwise stated , where @xmath19 defined on @xmath20 satisfies :    * @xmath19 is concave and increasing . * @xmath19 is continuous with the right derivative @xmath21",
    ".    the first condition encourages zeros in @xmath22 rather than small entries , since @xmath19 changes rapidly around the origin ; the second one is imposed for the good of the proposed algorithm , as will be seen later .",
    "a number of sparse metrics in the literature enjoy the above properties , including smoothly clipped absolute deviation ( scad ) @xcite , capped-@xmath0 , transformed-@xmath0 , and of course @xmath0 itself .",
    "although @xmath12 ( @xmath23 ) and logarithm functional do not meet the second condition , their smoothed versions @xmath24 and @xmath25 are differentiable at zero .",
    "these proposed properties will be essential in the algorithm design as well as in the proof of main results .",
    "our proposed algorithm calls for solving a sequence of minimization subproblems .",
    "the objective of each subproblem is @xmath26 plus a linear term , which is convex and tractable .",
    "we further validate robustness of this framework , by showing theoretically and numerically that it performs at least as well as basis pursuit in terms of uniform sparse recovery , independent of the conditioning of @xmath9 and sparsity metric .    the paper is organized as follows . in section 2 ,",
    "we overview rip and coherence of sensing matrices , as well as descent property of dca . in section 3",
    ", we provide the iterated @xmath0 framework for non - convex minimization , with worked out examples on representative sparse objectives including the total variation . in section 4 ,",
    "we prove the main exact recovery results based on unique recovery property of @xmath0 minimization instead of rip , which forms a theoretical basis of the better performance of dca . in section 5 , we compare iterative @xmath0 algorithms with two state - of - the - art non - convex cs algorithms , irls-@xmath12 @xcite and irl@xmath1 @xcite , and admm-@xmath0 , in cs test problems with varying degree of coherence .",
    "we find that iterative @xmath0 outperforms admm-@xmath0 independent of the sensing matrix coherence , and leads irls-@xmath12 @xcite and irl@xmath1 @xcite in the highly coherent regime .",
    "this is consistent with earlier findings of dca-@xmath14 algorithm @xcite to which our theory also applies .",
    "we also evaluate these two non - convex metrics on a two - dimensional example of reconstructing mri from a small number of projections , our iterative @xmath0 algorithm succeed with 7 projections for both metrics .",
    "using the same objective functions , the state - of - the - art algorithms need at least 10 projections .",
    "concluding remarks are in section 6 .",
    "* notations .",
    "* let us fix some notations .",
    "for any @xmath27 , @xmath28 is their inner product .",
    "@xmath29 is the vector of zeros , and similar to @xmath30 .",
    "@xmath31 is hadamard ( entry - wise ) product , meaning that @xmath32 .",
    "@xmath33 is the identity matrix of dimension @xmath34 . for any function @xmath35 on @xmath36",
    ", @xmath37 is a subgradient of @xmath35 at @xmath38 .",
    "the @xmath39 is the signum function on @xmath40 defined as @xmath41 for any set @xmath42 , @xmath43 is given by @xmath44",
    "the well - known cs concept during the past decade is the restricted isometry property ( rip ) introduced by cands _",
    "@xcite , which is used to characterize matrices that are nearly orthonormal .    for each number @xmath45 ,",
    "@xmath45-restricted isometry constant of @xmath9 is the smallest @xmath46 such that @xmath47 for all @xmath48 with sparsity of @xmath45 .",
    "the matrix @xmath9 is said to satisfy the @xmath45-rip with @xmath49 .",
    "mutual coherence @xcite is another commonly - used concept closely related to the success of cs task .",
    "the coherence of a matrix @xmath9 is the maximum absolute value of the cross - correlations between the columns of @xmath9 , namely @xmath50    when matrix @xmath9 have small mutual coherence ( incoherent ) or small rip constant , its columns tend to be more separated or distinguishable , which is intuitively favorable to identification of the supports of target signal . on the other hand , a highly coherent matrix with large coherence poses challenge to the reconstruction .",
    "next we give a brief review on the difference of convex functions algorithm ( dca ) .",
    "dca has been widely applied to sparse optimization problems in several works @xcite . for an objective function @xmath51 on the space @xmath36 , where @xmath52 and @xmath53 are lower semicontinuous proper convex functions , we call @xmath54 a dc decomposition of @xmath55 .",
    "dca takes the following form @xmath56 since @xmath57 , by the definition of subgradient , we have @xmath58 consequently , @xmath59 the fact that @xmath60 minimizes @xmath61 was used in the first inequality above .",
    "therefore , dca permits a decreasing sequence @xmath62 , leading to its convergence provided @xmath63 is bounded from below .",
    "our proposed iterative @xmath0 framework for solving ( [ cs ] ) is built on @xmath0 minimization and dca .",
    "note that ( [ cs ] ) can be equivalently written as @xmath64 we then rewrite the above objective in dc decomposition form : @xmath65 clearly the first term on the right - hand side is convex in terms of @xmath38 .",
    "we show below that the second term is also a convex function .",
    "[ prop ] @xmath66 is convex in @xmath38 .    for notational convenience ,",
    "define @xmath67 on @xmath68 .",
    "since p is concave on @xmath68 , we have that @xmath69 is convex on @xmath68 .",
    "we only need to show that @xmath70 is convex on @xmath71 , or equivalently , for all @xmath72 , @xmath73 , @xmath74    * case 1 . *",
    "if @xmath75 and @xmath76 have the same sign or one of them is @xmath77 . since @xmath78 and @xmath69 is convex on @xmath68 , then the above inequality holds .",
    "* case 2 . * if @xmath75 and @xmath76 are of the opposite sign . by the concavity of @xmath19 on @xmath68",
    ", we have @xmath79 that is , @xmath80 for all @xmath81 . without loss of generality ,",
    "we suppose @xmath82 .",
    "then @xmath83 in the first inequality above , we used the convexity of @xmath69 on @xmath68 , whereas in the second one , we used the fact that @xmath80 for @xmath81 .    at the @xmath84 iteration ,",
    "dca calls for linearization of the second convex term at the current guess @xmath85 , and solving the resulting convex subproblem for @xmath60 . after converting back the linear constraint and removing the constant and the factor of @xmath86 ,",
    "we iterate : @xmath87 where @xmath88 be aware that @xmath89 denotes subgradient of @xmath90 at @xmath22 rather than subgradient of @xmath91 at @xmath38 . in this way",
    ", the subproblem reduces to minimizing @xmath26 plus a linear term of @xmath38 , which can be effciently solved by a variety of state - of - the - art algorithms for basis pursuit ( with minor modifications ) . in table",
    "[ table ] , we list some non - convex metrics and the corresponding iterative @xmath0 algorithm .    .examples of sparse metrics and associated iterative @xmath0 scheme [ cols=\"<,<,<,<\",options=\"header \" , ]",
    "we developed an iterative @xmath0 framework for a broad class of lipschitz continuous non - convex sparsity promoting objectives , including those arising in statistics .",
    "the iterative @xmath0 algorithm is shown via theory and computation to improve on the @xmath0 minimization for cs problems independent of the coherence of the sensing matrices .",
    "* acknowledgments . *",
    "the authors would like to thank yifei lou ( university of texas at dallas ) and jong - shi pang ( university of southern california ) for helpful discussions .",
    "the authors would also like to thank anonymous reviewers for their helpful comments.the work was partially supported by nsf grants dms-1222507 and dms-1522383 .",
    "s. boyd , n. parikh , e. chu , b. peleato , and j. eckstein , distributed optimization and statistical learning via the alternating direction method of multipliers , _ foundations and trends in machine learning _ , 3(1):1 - 122 , 2011 .",
    "e. cands , j. romberg , and t. tao , robust uncertainty principles : exact signal reconstruction from highly incomplete fourier information , _ ieee transactions on information theory _ , 52(2):489 - 509 ( 2006 ) .",
    "s. foucart and h. rauhut , a mathematical introduction to compressive sensing , springer , 2013 .",
    "g. gasso , a. rakotomamonjy , and s. canu , recovering sparse signals with a certain family of nonconvex penalties and dc programming , _ ieee transactions on signal processing _ ,",
    "4686 - 4698 , 2009 ."
  ],
  "abstract_text": [
    "<S> an algorithmic framework , based on the difference of convex functions algorithm ( dca ) , is proposed for minimizing a class of concave sparse metrics for compressed sensing problems . </S>",
    "<S> the resulting algorithm iterates a sequence of @xmath0 minimization problems . </S>",
    "<S> an exact sparse recovery theory is established to show that the proposed framework always improves on the basis pursuit ( @xmath0 minimization ) and inherits robustness from it . </S>",
    "<S> numerical examples on success rates of sparse solution recovery illustrate further that , unlike most existing non - convex compressed sensing solvers in the literature , our method always out - performs basis pursuit , no matter how ill - conditioned the measurement matrix is . moreover , the iterative @xmath0 ( il@xmath1 ) algorithm lead by a wide margin the state - of - the - art algorithms on @xmath2 and logarithimic minimizations in the strongly coherent ( highly ill - conditioned ) regime , despite the same objective functions . </S>",
    "<S> last but not least , in the application of magnetic resonance imaging ( mri ) , il@xmath1 algorithm easily recovers the phantom image with just 7 line projections . </S>"
  ]
}