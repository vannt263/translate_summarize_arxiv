{
  "article_text": [
    "information transmission is central to economic interactions and to the operation of organizations .",
    "this paper is concerned with communication errors , which we call _ noise _ , and the stability of related communication protocols , as expressed by the condition of _ nash equilibrium _ in a game - theoretic setting .",
    "information transmission is often modeled as a sender - receiver game between an informed expert and an uninformed decision maker .",
    "we also study a sender - receiver game , but should say at the outset that our goal and assumptions differ from the common economic models , most notably signalling games ( spence , 1973 ) and the seminal work of crawford and sobel ( 1982 ) and its extensions .",
    "these models typically assume perfect communication , but find that information is not transmitted faithfully for _ strategic _ reasons because of conflicting incentives of sender and receiver ( see the surveys by kreps and sobel , 1994 , and sobel , 2010 ) .",
    "in contrast , we assume communication errors as _ given _ , due to distorted signals or imprecisely worded or misunderstood messages .",
    "this kind of noise is considered in information theory ( shannon , 1948 ) and in studies of language and ambiguity ( nowak and krakauer , 1999 ) .",
    "communication errors are common and usually have negative consequences for both parties ( to different degrees ) , as illustrated by the following examples : a pilot misunderstands the command of an air traffic controller , with a resulting aviation accident ( cushing , 1994 ) .",
    "a map on a webpage displays a holiday home close to the beach when it is not , and the holiday maker sues the landlord for misrepresentation afterwards .",
    "a landing card is filled in wrongly and immigration is delayed .",
    "a student does not understand a lecture and scores poorly in the exam .",
    "these errors may be attributable in various ways to the sender ( sometimes for strategic reasons ) , the transmission , or the receiver .",
    "we think that a basic model that assumes these errors as given deserves a game - theoretic analysis .",
    "we consider the classic model of the _ discrete noisy channel_. the channel has a finite set of input and output symbols and known transition probabilities that represent the possible communication successes and errors .",
    "the channel may also be used repeatedly , with independent errors , which we study for the important case of the binary channel that has only two symbols .",
    "the model of the noisy channel is due to shannon ( 1948 ) , who pioneered information theory , which has been crucial for the design of reliable data communication with devices that range from space probes to disk drives",
    ". these engineering efforts typically aim for `` codes '' that have a good reliability and rate of information transmission ( see cover and thomas , 1991 ; callager , 1968 ; mackay , 2003 ) .",
    "we ask a different question : given a communication protocol defined by a code , do sender and receiver have an incentive to follow this protocol ?    using game theory , we study this question with a sender - receiver game where the interests of sender and receiver are aligned .",
    "one of finitely many states of nature is chosen at random .",
    "the sender is informed the state and transmits a signal via a discrete noisy channel to an uninformed receiver who makes a decision . the sender s strategy or _",
    "code _ assigns to each state of nature a specific signal or `` codeword '' that is the input to the channel .",
    "the receiver s strategy decodes the distorted signal that is the channel output as one of the possible states .",
    "both players receive a ( possibly different ) positive payoff only if the state is decoded correctly , otherwise payoff zero .    in equilibrium ,",
    "the receiver decodes the channel output as the state with highest expected payoff .",
    "this receiver condition is the well - known `` maximum likelihood '' decoding in the special case of uniform priors and equal utilities .",
    "the equilibrium condition for the sender means that she chooses for each state the prescribed codeword as her best response , that is , no other channel input has a higher probability of being decoded correctly with the given receiver strategy .",
    "a _ nash code _ is a code together with a best - response decoding that defines a nash equilibrium .",
    "so we assume the straightforward equilibrium condition for the receiver and require that the code fulfills the more involved sender condition .",
    "( of course , both conditions are necessary for equilibrium . )",
    "we present two main results about nash codes .",
    "our first result concerns discrete channels with arbitrary finite sets of input and output symbols .",
    "we show that already for three symbols , not every code defines a nash equilibrium . however , a nash code results if the expected payoff to the receiver can not be increased by replacing a single codeword with another one ( theorem  [ t - opt ] ) .",
    "so these _ receiver - optimal _ codes are nash codes .",
    "this is closely related to _ potential games _ and provides a method to construct nash codes ( proposition  [ p - pot ] ) : essentially , if the sender can change a codeword to improve the probability that a state is decoded correctly ( which violates nash equilibrium ) , then this defines a better code for the receiver .    in short , without any constraints on the channel , and for any best - response decoding , `` good codes '' ( that is , receiver - optimal codes ) are nash codes .",
    "the method to show this result is not deep ; its purpose is to analyze our model .",
    "the key assumption is that an improvement in decoding probability benefits both sender and receiver ( see condition ( [ oneimprov ] ) below ) .",
    "however , a _ sender - optimal _ code is _ not _ necessarily a nash code if sender and receiver give different utilities to a correct decoding of the state of nature .",
    "this happens if the sender can use an unused message to transmit the information about the state more reliably ( see the discussion following the example in ( [ 6codes ] ) below ) .",
    "our second , more surprising and technically challenging result concerns the _ binary channel _ where codewords are strings of bits with independent error probabilities for each bit , a fundamental model of information transmission .",
    "then _ any _ code is a nash code ( theorem  [ t - bin ] ) .",
    "the only requirement for the decoding is that the receiver breaks ties between states _ monotonically _ , that is , in a consistent manner ; this holds for natural tie - breaking rules , and ties do not even occur if states of nature have different generic prior probabilities or utilities .",
    "( in a side result , proposition  [ p - fixedorder ] , we also characterize deterministic monotonic tie - breaking rules as those that select among the tied states in a fixed given order . )",
    "so binary codes , as nash codes , are very suitable for information transmission because the agents never have an incentive to deviate from them .",
    "there are several interpretations of this second result .",
    "one is `` good news for the engineer '' : binary codes , which are fundamental to the practice and theory of information theory , are `` incentive compatible '' .",
    "the fact that they are nash codes ( assuming monotonic decoding ) means that engineering issues such as high reliability and rate or algorithmic ease of coding and decoding are `` orthogonal '' to the stability of the code in terms of incentives . on the other hand , this can also be seen as a negative result : nash equilibrium is not related to transmission quality .",
    "this last observation is not restricted to binary channels ; for example , the sender - receiver game has always a `` babbling equilibrium '' where the sender s action is independent of the state and the receiver s action is independent of the channel output , with no information transmitted .",
    "the equilibrium property of codes with low payoffs may explain why inefficient ( say , very bureaucratic ) communication protocols persist in practice because no single participant has an incentive to deviate from them .",
    "sender - receiver games studied in the economic literature typically assume communication without transmission errors . in their seminal paper ,",
    "crawford and sobel ( 1982 ) study such a game where the set of possible states , messages , and receiver s actions is an interval .",
    "payoffs depend continuously on the difference between state and action , and differ for sender and receiver . in equilibrium",
    ", the interval is partitioned into finitely many intervals , and the sender sends as her message only the partition class that contains the state .",
    "thus , the sender only reveals partial information about the state , which can be seen as noise being introduced strategically .    even in rather simple sender - receiver games , players can get higher equilibrium payoffs when communicating over a channel with noise than with perfect communication ( myerson , 1994 , section  4 ) .",
    "blume , board , and kawamura ( 2007 ) extend the model by crawford and sobel ( 1982 ) by assuming communication errors .",
    "the noise allows for equilibria that improve welfare compared to the crawford - sobel model .",
    "the construction partly depends on the specific form of the errors so that erroneous transmissions can be identified ; this does not apply in our discrete model .",
    "in addition , in our model players only get positive payoff when the receiver decodes the state correctly , unlike in the continuous models by crawford and sobel ( 1982 ) and blume et al .",
    "( 2007 ) . on the other hand ,",
    "compared to perfect communication , noise may prevent players from achieving common knowledge about the state of nature ( koessler , 2001 ) .",
    "game - theoretic models of communication have been used in the study of language .",
    "lewis ( 1969 ) describes language as a `` convention '' with mappings between states and signals , and argues that these should be bijections .",
    "nowak and krakauer ( 1999 ) use evolutionary game theory to show how languages may evolve from `` noisy '' mappings ; wrneryd ( 2003 ) shows that only bijections are evolutionary stable .",
    "however , even ambiguous sender mappings ( where one signal is used for more than one state ) together with a mixed receiver population may be `` neutrally stable '' ( pawlowitsch , 2008 ) ; the randomized receiver strategy can be seen as noise .",
    "blume and board ( 2009 ) use the noisy channel to model vagueness in communication .",
    "lipman ( 2009 ) discusses how vagueness can arise even for coinciding interests of sender and receiver .",
    "ambiguous signals arise when the set of messages is smaller than the set of states , which may reflect communication costs for the sender ( jger , koch - metzger , and riedel , 2011 ) . for the sender - receiver game with a noisy binary channel , hernndez , urbano , and vila ( 2010a )",
    "describe the equilibria for a specific code that can serve as a `` universal grammar '' ; the explicit receiver strategy allows to characterize the equilibrium payoff .",
    "noise in communication is relevant to models of persuasion , where the sender wants to induce the receiver to take an action .",
    "glazer and rubinstein ( 2004 ; 2006 ) study binary receiver actions ; the sender may reveal limited information about the state of nature as `` evidence '' .",
    "the optimal way to do so is a receiver - optimal mechanism . in a more general",
    "setting , kamenica and gentzkow ( 2011 ) allow the sender to commit to a strategy that selects a message for each state , assuming the receiver s best response using bayesian updating ; the sender may generate noise by selecting the message at random .",
    "subject to a certain bayesian consistency requirement , the sender can commit to her best possible strategy .",
    "section  [ s - model ] describes our model and characterizes the nash equilibrium condition . for channels with any number of symbols , section  [ s - recopt ]",
    "gives an example that some codes may not be nash codes , shows that receiver - optimal codes are , and discusses the relation to potential functions . in section  [ s - bin ] ,",
    "we consider binary codes , and state the main theorem  [ t - bin ] , which is proved in the appendix .",
    "it requires the condition of `` monotonic '' decoding when ties occur , for example in a fixed order among the states as when they have generic priors . in section  [ s - mon ]",
    "it is shown that this is in fact the only general deterministic monotonic tie - breaking rule .",
    "we consider a game of two players , a sender ( she ) and a receiver ( he ) .",
    "first , nature chooses a _ state _",
    "@xmath0 from a set @xmath1 with positive _ prior _ probability  @xmath2 .",
    "then the sender is fully informed about  @xmath0 , and sends a message to the receiver via a noisy channel . after receiving the message as output by the channel",
    ", the receiver takes an action that affects the payoff of both players .",
    "the channel has finite sets @xmath3 and @xmath4 of input and output symbols , with noise given by transition probabilities @xmath5 for each @xmath6 , @xmath7 .",
    "the channel is used @xmath8 times independently without feedback .",
    "when an input @xmath9 is transmitted through the channel , it is altered to an output @xmath10 according to the probability @xmath5 given by @xmath11 this is the standard model of a memoryless noisy channel as considered in information theory ( cover and thomas , 1991 ; mackay , 2003 ) .",
    "the sender s strategy is to encode state @xmath0 by means of a coding function or _",
    "@xmath12 , which we write as @xmath13 .",
    "we call @xmath14 the _ codeword _ or _ message _ for state  @xmath0 in @xmath15 , which the sender transmits as input to the channel",
    ". the code @xmath16 is completely specified by the list of @xmath17 codewords @xmath18 , which is called the _",
    "codebook_.    the receiver s strategy is to decode the channel output  @xmath19 , given by a probabilistic _ decoding function _",
    "@xmath20 where @xmath21 is the probability that @xmath19 is decoded as  @xmath0 .    sender and receiver have the common interest that the message is decoded correctly .",
    "that is , if the receiver decodes the channel output as the state  @xmath0 chosen by nature , then sender and receiver get positive payoff @xmath22 and @xmath23 , respectively , otherwise both get payoff zero .",
    "the channel transition probabilities , the transmission length  @xmath8 , and the prior probabilities @xmath2 and utilities @xmath22 and @xmath23 for @xmath0 in @xmath15 are commonly known to the players .",
    "we are interested in conditions so that the pair @xmath24 defines a nash equilibrium . in that case",
    ", we call @xmath16 , under the assumption that decoding takes place according to @xmath25 , a _ nash code_. we denote the expected payoffs to sender and receiver by @xmath26 and @xmath27 , respectively .",
    "the code @xmath16 defines the sender s strategy .",
    "the best response of the receiver is the following . given that he receives channel output @xmath19 in @xmath28 , the probability that codeword @xmath14 has been sent is , by bayes s law , @xmath29 , where @xmath30 is the overall probability that @xmath19 has been received .",
    "the factor @xmath31 can be disregarded in the maximization of the receiver s expected payoff .",
    "hence , a best response of the receiver is to choose with positive probability @xmath21 only states  @xmath0 so that @xmath32 is maximal ,",
    "that is , so that @xmath19 belongs to the set @xmath33 defined by @xmath34 hence , the best response condition for the receiver states that for any @xmath35 and @xmath36 @xmath37 if @xmath38 for all @xmath36 , then @xmath33 in ( [ yi ] ) is the set of channel outputs @xmath19 so that the channel input @xmath14 has",
    "_ maximum likelihood_. ( this term is sometimes used only for uniform prior probabilities , e.g.  mackay , 2003 , p.  152 , which we do not assume . ) if the receiver has different positive utilities @xmath23 for different states  @xmath0 , then the receiver s best response maximizes @xmath32 .",
    "we say that for a given channel output @xmath19 , there is a _ tie _ between two states @xmath0 and @xmath39 ( or the states are _ tied _ ) if @xmath40 . if there are never any ties , then the sets @xmath33 for @xmath41 are pairwise disjoint , and the best - response decoding function is deterministic and unique according to  ( [ arbtie ] ) .",
    "we refer to the sets @xmath33 for @xmath36 as a `` partition '' of @xmath28 , which constrains the receiver s best - response decoding as in ( [ arbtie ] ) , even though some of these sets may be empty , and they may not always be disjoint if there are ties . in any case , @xmath42 .",
    "suppose that the receiver decodes the channel output with @xmath25 according to ( [ yi ] ) and ( [ arbtie ] ) for the given code @xmath16 with @xmath13 .",
    "then @xmath24 is a nash equilibrium if and only if , for any state  @xmath0 , it is optimal for the sender to transmit @xmath14 and not any other @xmath43 in @xmath44 as a message . when sending @xmath43 , the expected payoff to the sender in state  @xmath0 is @xmath45 when maximizing ( [ pay ] ) , the utility @xmath22 to the sender does not matter as long as it is positive ; given that the state is  @xmath0 , the sender only cares about the probability that the channel output  @xmath19 is decoded as  @xmath0 .",
    "we summarize these observations as follows .",
    "[ p - send ] the code @xmath16 with decoding function @xmath25 is a nash code if and only if the receiver decodes channel outputs according to @xmath46 and @xmath47 , and if and only if in every state  @xmath0 the sender transmits codeword @xmath13 which fulfills for any other possible channel input @xmath43 in @xmath44 @xmath48",
    "in this section , we first ask whether every code is a nash code , assuming that the receiver chooses a best response .",
    "we give a detailed example that demonstrates that this may not be the case , and that we use throughout the section .",
    "then we show that every code that maximizes the receiver s payoff is a nash code .",
    "the proof implies that this holds also if the receiver s payoff is locally maximal , that is , when changing only a single codeword , and the corresponding best response of the receiver , at a time .",
    "finally , we discuss the connection with potential functions .",
    "consider a channel with three symbols , @xmath49 , which is used only once ( @xmath50 ) , with the following transition probabilities : @xmath51 \\raise 2ex\\hbox{$p(y|x)$ } & \\hfil 0 & \\hfil 1 & \\hfil 2 \\\\ \\hline 0 & 0.85 & 0.1 & 0.05\\\\ ~$x$ \\hfill 1 & 0.1 & 0.65 & 0.25 \\\\ 2 & 0 & 0.3 & 0.7 \\\\ \\hline \\end{tabular}\\ ] ] suppose that nature chooses the two states in @xmath52 with uniform priors @xmath53 .",
    "the sender s utilities are @xmath54 when the state is @xmath55 and @xmath56 when the state is 1 , and the receiver s utilities are @xmath57 , @xmath58 .    consider the codebook @xmath16 with @xmath59 and @xmath60 , so the sender codifies the two states of nature as the two symbols @xmath55 and @xmath61 , respectively .",
    "given the parameters of this game and the sender s strategy  @xmath16 , the receiver s strategy assigns to each output symbol in @xmath62 one state . the following table ( [ nonash ] ) gives the expected payoff @xmath63 for the receiver when the state is @xmath0 and the output symbol is @xmath19 .",
    "@xmath64 \\raise 2ex\\hbox{$q_i v_i \\,p(y|x^i)$ } & \\hfil 0 &   \\hfil 1 & \\hfil 2 \\\\ \\hline 0 & 3.4 & 0.4 & 0.2\\\\[-1ex ] \\raise 2ex\\hbox{~~$i$ } \\hfill 1 & 0.1 & 0.65 & 0.25 \\\\ \\hline \\end{tabular}\\ ] ] table ( [ nonash ] ) allows us to compute the receiver s best response and the sets @xmath33 in ( [ yi ] ) . for each channel output @xmath19 ,",
    "the receiver chooses the state @xmath0 with highest expected payoff .",
    "hence , he decodes the channel output @xmath55 as state @xmath55 because @xmath65 . in the same way ,",
    "he decodes both channel outputs @xmath61 and @xmath66 as state  @xmath61 .",
    "notice that there are no ties , so the two sets @xmath67 and @xmath68 are disjoint , and the receiver s best response is unique and deterministic .",
    "that is , the receiver s best response @xmath25 is given by @xmath69 if and only if @xmath70 , where @xmath71 and @xmath72 .",
    "is this code @xmath16 given by the codebook @xmath73 a nash code ?    given the partition of @xmath4 into @xmath67 and @xmath68 by the receiver strategy @xmath25 , it easy to compute the sender payoff as in ( [ pay ] ) when the states @xmath55 and @xmath61 are realized . for the first state @xmath55 , her payoff is @xmath74 .",
    "for the second state @xmath61 , her payoff is @xmath75 the sender s ( ex - ante ) expected payoff is therefore @xmath76 .    in order to check the nash equilibrium property of @xmath24",
    ", there should be no code @xmath77 so that @xmath78 .",
    "consider now the new sender strategy @xmath77 with codebook @xmath79 , which differs from code @xmath16 in the codeword @xmath80 for state  @xmath61 .",
    "the receiver s strategy @xmath25 with @xmath67 and @xmath68 is fixed .",
    "state  @xmath55 is encoded by the same codeword @xmath81 , so the sender s payoff for that state is  1.7 as before .",
    "however , for state  @xmath61 , the signal sent is @xmath66 instead of  @xmath61",
    ". then the sender s payoff is @xmath82 , which is higher than her payoff 7.2 when sending signal  @xmath61 . her expected payoff increases to @xmath83",
    "consequently , the code @xmath16 with codebook @xmath84 is not a nash code .    in this example",
    ", changing the codebook @xmath16 to @xmath77 improves the sender payoff from @xmath26 to @xmath85 , where @xmath25 is the receiver s best - response decoding for code  @xmath16 .",
    "in addition , it is easily seen that the receiver payoff also improves from @xmath27 to @xmath86 , and his payoff @xmath87 for the best response @xmath88 to @xmath77 is possibly even higher .",
    "this observation leads us to a sufficient condition for nash codes .",
    "[ d - opt ] a _ receiver - optimal code _ is a code @xmath16 with highest expected payoff to the receiver , that is , so that @xmath89 for any other code @xmath90 , where @xmath25 is a best response to @xmath16 and @xmath91 is a best response to @xmath90 .",
    "note that in this definition , the expected payoff @xmath27 ( and similarly @xmath92 ) does not depend on the particular best - reponse decoding function @xmath25 in case @xmath25 is not unique when there are ties , because the receiver s payoff is the same for all best responses  @xmath25 .",
    "the following is the central theorem of this section .",
    "it is proved in three simple steps , which give rise to a generalization that we discuss afterwards , along with examples and further observations .",
    "[ t - global ] every receiver - optimal code is a nash code .",
    "let @xmath16 be a receiver - optimal code with codebook @xmath18 , and associated best - response decoding @xmath25 according to @xmath46 and @xmath47 .",
    "suppose @xmath16 is not a nash code .",
    "then there exists a code @xmath93 with codebook @xmath94 so that @xmath95 , that is , @xmath96 step one : clearly , ( [ hatc ] ) implies , but we want to refer later to ( [ hatc ] ) as well . ] that there exists at least one @xmath97 so that @xmath98 consider the new code @xmath77 which coincides with @xmath16 except for the codeword for state  @xmath0 , where we set @xmath99 .",
    "so the codebook for @xmath77 is @xmath100 . by ( [ oneimprov ] ) , we also have @xmath101 > & \\displaystyle \\sum_{j \\in \\mathstrut\\omega } q_j u_j \\sum_{y \\in \\mathstrut y^n } p(y| x^j ) d(y , j ) = u(c , d).\\hfill \\end{array}\\ ] ] step two : in the same manner , ( [ oneimprov ] ) implies an improvement of the receiver function , that is , @xmath102 step three : let @xmath88 be the best response to @xmath77 , which with ( [ improve ] ) implies @xmath103 hence , code @xmath77 has higher expected receiver payoff than @xmath16 .",
    "this contradicts the assumption that @xmath16 is a receiver - optimal code .",
    "the preceding theorem asserts that there is at least one nash code .",
    "it can be found as a code with highest receiver payoff .",
    "@xmath104 for our example , the table in ( [ 6codes ] ) lists the six possible codebooks @xmath105 , shown in the first column , that have distinct codewords ( @xmath106 ) . for each code",
    ", the receiver s best response is unique .",
    "the best - response partition @xmath107 is shown in the second column . using this partition",
    ", the third column gives the probabilities @xmath108 that the codeword @xmath14 is decoded correctly .",
    "the overall expected payoffs to sender and receiver are shown as @xmath109 and  @xmath110 .    according to the rightmost column in ( [ 6codes ] ) , the unique receiver - optimal codebook is @xmath111 , which is a nash code by theorem  [ t - global ] .",
    "we have already shown that @xmath84 is not a nash code .",
    "note , however , that this is the code with highest sender payoff .",
    "hence , a `` sender - optimal '' code is not necessarily a nash code .",
    "the reason is that , because sender and receiver have different payoffs for the two states , the sender prefers the code with large partition class @xmath68 for state 1 , but then can deviate to a better , unused message within @xmath68 .",
    "it also easily seen from ( [ 6codes ] ) that @xmath112 and @xmath113 are not nash codes , either : both codebooks have the same best - response partition @xmath114 and @xmath115 as the codebook @xmath111 , but have lower payoff to the sender , so the sender can profitably deviate from @xmath112 or @xmath113 to  @xmath111 .    in ( [ 6codes ] ) , the codebook @xmath116 has the interesting property that the receiver decodes _ any _ channel output  @xmath19 as state  @xmath55 ; this holds because even the unaltered codeword @xmath117 , when received as @xmath118 , fulfills @xmath119 , so the receiver prefers to decode it as state  @xmath55 .",
    "so here @xmath120 and @xmath68 is the empty set . given that the receiver s action is the same for any received channel output , the sender can not improve her payoff by transmitting anything else .",
    "so the codebook @xmath116 is a nash code .",
    "in fact , any sender - receiver game , irrespective of the players payoffs , has a trivial `` pooling '' equilibrium where the sender s signal does not depend on the state , and the receiver s best response decodes the uninformative channel output as the state @xmath0 with highest expected payoff , in our game  @xmath121 . in our example , such codes have equal codewords , with @xmath122 , all decoded as state  @xmath55 ; they are not listed in  ( [ 6codes ] ) .",
    "the codebook @xmath116 is potentially informative , but the receiver ignores the channel output due to his utility function .    finally , the codebook @xmath16 with codebook @xmath79 in ( [ 6codes ] ) is also a nash equilibrium , which is seen as follows .",
    "let @xmath25 be the best response to @xmath16 , with @xmath123 , @xmath124 . as shown in the proof of theorem  [ t - global ] ,",
    "if the sender could profitably deviate from @xmath16 to @xmath90 , then she could also profitably deviate to a code @xmath77 that differs from @xmath16 in one codeword only .",
    "the possible codes @xmath77 have codebooks @xmath116 , where @xmath125 is changed to  @xmath126 , and @xmath84 , where @xmath127 is changed to  @xmath128 . in the first case , by ( [ ternary ] ) , changing @xmath125 from @xmath55 to  @xmath61 changes @xmath129 to @xmath130 , which is not an improvement . in the second case ,",
    "changing @xmath127 from @xmath66 to  @xmath61 changes @xmath131 to @xmath132 , which is not an improvement either .",
    "so @xmath16 is indeed a nash code .    the code @xmath16 with codebook @xmath79 is also seen to be a nash code with the help of table  ( [ 6codes ] ) according to the proof of theorem  [ t - global ] .",
    "namely , it suffices to look for profitable sender deviations @xmath77 where only one codeword is altered , which would also imply an improvement to the receiver s payoff from @xmath27 to @xmath86 , and hence certainly an improvement to his payoff @xmath87 where @xmath88 is the best response to  @xmath77 .",
    "for the two possible codes @xmath77 given by @xmath116 and @xmath84 , the receiver payoff @xmath110 does not improve according to  ( [ 6codes ] ) , so @xmath16 is a nash code . by this reasoning , any `` locally '' receiver - optimal code , according the following definition , is also a nash code .",
    "[ d - local ] a _ locally receiver - optimal code _ is a code @xmath16 so that no code @xmath77 that differs from @xmath16 in only a single codeword gives higher expected payoff to the receiver .",
    "that is , for all @xmath77 with @xmath133 for some state  @xmath0 , and @xmath134 for all @xmath135 , @xmath136 where @xmath25 is a best response to  @xmath16 and @xmath88 is a best response to  @xmath77 .",
    "[ t - opt ] every locally receiver - optimal code is a nash code .",
    "apply the proof of theorem [ t - global ] from step  two onwards .",
    "clearly , every receiver - optimal code is also locally receiver - optimal , so theorem  [ t - global ] can be considered as a corollary to the stronger theorem  [ t - opt ] .",
    "local receiver - optimality is more easily verified than global receiver - optimality , because much fewer codes @xmath77 have to be considered as possible improvements for the receiver payoff according to definition  [ d - local ] .",
    "a locally receiver - optimal code can be reached by iterating profitable changes of single codewords at a time .",
    "this simplifies the search for nash codes .",
    "to conclude this section , we consider the connection to _ potential games _ which also allow for iterative improvements in order to find a nash equilibrium . as in monderer and shapley ( 1996 ,",
    "p.  127 ) , consider a game in strategic form with finite player set  @xmath137 , and pure strategy set @xmath138 and utility function @xmath139 for each player  @xmath0 .",
    "then the game has an ( ordinal ) _ potential function _",
    "@xmath140 if for all @xmath141 and @xmath142 and @xmath143 , @xmath144 the question is if in our game , the receiver s payoff is a potential function .",
    "the following proposition gives an answer .",
    "[ p - pot ] consider the game with @xmath145 players where for each state @xmath0 in @xmath15 , a separate agent  @xmath0 transmits a codeword @xmath146 over the channel , which defines a function @xmath12 , and where the receiver decodes each channel output with a decoding function @xmath25 as before .",
    "each agent receives the same payoff @xmath26 as the original sender .",
    "then    * any nash equilibrium @xmath24 of the @xmath147-player game is a nash equilibrium of the original two - player game , and vice versa . *",
    "the receiver s expected payoff is a potential function for the @xmath147-player game . * the receiver s expected payoff is not necessarily a potential function for the original two - player game .",
    "every profile @xmath16 of @xmath17 strategies for the agents in the @xmath147-player game can be seen as a sender strategy in the original game , and vice versa . to see ( a ) , let @xmath24 be a nash equilibrium of the @xmath147-player game .",
    "if there was a profitable deviation @xmath90 from @xmath16 for the sender in the two - player game as in ( [ hatc ] ) , then there would also be a profitable deviation @xmath77 that changes only one codeword @xmath146 as in ( [ deviation ] ) , which is a profitable deviation for agent  @xmath0 , a contradiction .",
    "the `` vice versa '' part of ( a ) holds because any profitable deviation of a single agent is also a deviation for the sender in the original game .",
    "assertion ( b ) holds because for any  @xmath0 in @xmath15 , ( [ deviation ] ) is , via ( [ oneimprov ] ) , equivalent to ( [ improve ] ) .    to see ( c ) , consider our example ( [ ternary ] ) with @xmath16 and @xmath90 given by the codebooks @xmath84 and @xmath116 , respectively , and @xmath25 decoding channel outputs @xmath148 as states @xmath149 , respectively . then the payoffs to sender and receiver are @xmath150 which shows that ( [ potential ] ) does not hold with @xmath139 as sender payoff and @xmath151 as receiver payoff , because these payoffs move in opposite directions when changing the sender s strategy from @xmath16 to @xmath90 , for this  @xmath25 .    a global maximum of the potential function gives a nash equilibrium of the potential game ( monderer and shapley , 1996 , lemma  2.1 ) . hence , ( a ) and ( b ) of proposition  [ p - pot ] imply that a maximum of the receiver payoff defines a nash equilibrium , as stated in theorem  [ t - global ] .",
    "it is also known that a `` local '' maximum of the potential function defines a nash equilibrium ( monderer and shapley , 1996 , footnote  4 ) . however",
    ", this does not imply our theorem  [ t - opt ] .",
    "the reason is that in a local maximum of the potential function , the function can not be improved by unilaterally changing a single player s strategy .",
    "in contrast , in a locally receiver - optimal code , the receiver s payoff can not be improved by changing a single codeword _ together _ with the receiver s best response .",
    "for example , the nash code @xmath116 in ( [ 6codes ] ) with best response partition @xmath152 is not locally receiver - optimal , but is a `` local maximum '' of the receiver payoff .    in a potential game ,",
    "improvements of the potential function can be used for dynamics that lead to nash equilibria . for our games",
    ", the study of such dynamics may be an interesting topic for future research .",
    "the main result of this section concerns the important _ binary channel _ with @xmath153 .",
    "the two possible symbols 0 and 1 for a single use of the channel are called _",
    "bits_. the binary channel is the basic model for the transmission of digital data and of central theoretical and practical importance in information theory ( see , for example , cover and thomas , 1991 , or mackay , 2003 ) .",
    "we assume that the channel errors @xmath154 and @xmath155 fulfill @xmath156 where @xmath157 is equivalent to either of the inequalities @xmath158 these assert that a received bit 0 is more likely to have been sent as 0 ( with probability @xmath159 ) than sent as bit 1 and received with error ( with probability  @xmath160 ) , and similarly that a received bit 1 is more likely to have been sent as 1 than received erroneously .",
    "it may still happen that bit 0 , for example , is transmitted with higher probability incorrectly than correctly , for example if @xmath161 and @xmath162 .",
    "condition ( [ errors ] ) can be assumed with very little loss of generality .",
    "if @xmath163 then the channel is error - free and every message can be decoded perfectly . if @xmath164 then the channel output is independent of the input and no information can be transmitted . for @xmath165 the signal is more likely to be inverted than not , so that one obtains ( [ errors ] ) by exchanging 0 and  1 in @xmath4 .",
    "condition ( [ errors ] ) does exclude the interesting case of a `` z - channel '' that has only one - sided errors , that is , @xmath166 or @xmath167 .",
    "we assume instead that this is modelled by vanishingly small error probabilities , in order to avoid case distinctions about channel outputs @xmath19 in @xmath28 that can not occur for some inputs  @xmath43 when @xmath166 or @xmath167 . with ( [ errors ] ) , every channel output @xmath19 has positive , although possibly very small , probability .    the binary channel is _ symmetric _ when @xmath168 , where @xmath169 by ( [ errors ] ) .",
    "the binary channel is used @xmath8 times independently .",
    "a code @xmath12 for @xmath170 is also called a _ binary code_. the main result of this section ( theorem  [ t - bin ] below ) states that _ any _ binary code is a nash code , provided the decoding is _",
    "monotone_. this monotonicity condition concerns how the receiver resolves ties when a received channel output @xmath19 can be decoded in more than one way .",
    "we first consider an example of a binary code that shows that the equilibrium property may depend on how the receiver deals with ties .",
    "assume that the channel is symmetric with error probability  @xmath171 .",
    "let @xmath172 , @xmath173 , and consider the codebook @xmath174 given by @xmath175 .",
    "all four states @xmath0 have equal prior probabilities @xmath176 and equal sender and receiver utilities @xmath177 .",
    "the sets @xmath33 in ( [ yi ] ) are given by @xmath178 this shows that for any channel output @xmath19 other than an original codeword @xmath14 , there are ties between at least two states . for example , @xmath179 because @xmath180 is received with probability @xmath181 for @xmath182 and @xmath183 as channel input . for @xmath184 ,",
    "all three states @xmath185 are tied .",
    "consider first the case that the receiver decodes the channel outputs @xmath186 as states @xmath185 , respectively , that is , according to @xmath187 we claim that this can not be a nash code , irrespective of the decoding probabilities @xmath188 which can be positive for any @xmath189 by ( [ y0123 ] ) .",
    "the situation is symmetric for @xmath189 , so assume that @xmath188 is positive when @xmath190 ; the case of a deterministic decoding where @xmath191 is shown on the left in figure  [ binary ] .",
    "then the receiver decodes @xmath19 as state  1 with positive probability when @xmath19 equals @xmath192 , @xmath180 , or @xmath193 .",
    "when @xmath194 is sent , these channel outputs are received with probabilities @xmath195 , @xmath181 , and @xmath196 , respectively , so the sender payoff is @xmath197 in ( [ pay ] ) .",
    "given this decoding , the sender can improve her payoff in state  1 by sending @xmath198 rather than @xmath194 because then the probabilities of the channel outputs @xmath192 and @xmath180 are just exchanged , whereas the probability that output @xmath193 is decoded as state  1 increases to @xmath199 ; that is , given this decoding , sending @xmath198 is more likely to be decoded correctly as state  1 than sending @xmath194 .",
    "this violates ( [ sender ] ) .",
    "the problem with the decoding in ( [ circular ] ) is that when the receiver is tied between states 1 , 2 , and  3 when the channel output is @xmath200 , he decodes @xmath201 as state  1 with positive probability @xmath202 , but when he is tied between even fewer states 1 and 3 when receiving @xmath203 , that decoding probability @xmath204 decreases to zero .",
    "this violates the following _ monotonicity _ condition .",
    "[ d - mono ] consider a codebook with codewords @xmath14 for @xmath36 and a decoding function @xmath25 in @xmath205 .",
    "then @xmath25 is called _ monotonic _ if it is a best response decoding function with @xmath46 and @xmath47 , and if for all @xmath206 and states  @xmath0 , @xmath207    in ( [ mono ] ) , @xmath208 is the set of tied states for channel output  @xmath19 , and @xmath209 is the set of tied states for channel output  @xmath201 , and both sets include state  @xmath0 .",
    "the condition states that the probability of decoding the channel output as state  @xmath0 can only decrease when the set of tied states increases .",
    "we study the monotonicity condition in definition  [ d - mono ] in more detail in the next section .",
    "we conclude with the main result of this section ; its proof and some technical comments are given in the appendix .",
    "[ t - bin ] every monotonically decoded binary code is a nash code .",
    "when is a decoding function monotonic ?",
    "suppose there is some fixed order on the set of states so that always the first tied state is chosen according to that order . in this section ,",
    "we show that this is essentially the only way to break ties with a deterministic monotonic decoding function .",
    "the monotonicity condition in definition  [ d - mono ] implies @xmath210 that is , the decoding probability @xmath21 of state  @xmath0 may only depend on the set @xmath208 of states that are tied with  @xmath0 , but not on the received channel output  @xmath19 .",
    "for that reason , we can define a monotonic decoding function also as a function @xmath211 of the set  @xmath208 of best - response states , @xmath212 which is well defined by ( [ equal ] ) .",
    "a natural example of a probabilistic monotonic decoding function is to break ties uniformly with @xmath213 for @xmath214 .",
    "a more general monotonic decoding function is @xmath215 for @xmath214 with a fixed positive weight @xmath216 for each state  @xmath39 .",
    "there are many other probabilistic monotonic decoding functions .",
    "for example , if ties between three or more states are broken uniformly , then ties between only two states are decoded monotonically if the decoding probabilities for both tied states are at least  @xmath217 .",
    "we will show that _ deterministic _ monotonic decoding functions are more restrictive .",
    "consider again the example ( [ circular ] ) with @xmath191 as shown on the left in figure  [ binary ] .",
    "( note that this decoding is not monotonic but fulfills the weaker condition ( [ equal ] ) which therefore does not suffice to guarantee a nash code . )",
    "the following decoding function , changed from ( [ circular ] ) so that @xmath218 is decoded as state  1 , is monotonic , @xmath219 shown in the right picture in figure  [ binary ] .",
    "this is a nash code because all @xmath19 in the set @xmath68 , see ( [ y0123 ] ) , are decoded as state  1 ; whichever @xmath43 in @xmath68 the sender decides to transmit instead of @xmath182 , there is one @xmath19 in @xmath68 for which @xmath220 , so that the payoff to the sender in ( [ pay ] ) does not increase by changing from @xmath182 to  @xmath43 .",
    "    as the right picture in figure  [ binary ] shows , the decoding function in ( [ nownash ] ) can be defined by the following condition : consider a fixed linear order @xmath221 on @xmath15 ( in this case @xmath222 ) so that @xmath223 a _ fixed - order _ decoding function @xmath25 fulfills ( [ fixedorder ] ) for some  @xmath221 .",
    "such a decoding function is deterministic and clearly monotonic .",
    "we want to show that any deterministic monotonic decoding function is a fixed - order decoding function .",
    "we have to make the additional assumption that the decoding function @xmath211 is _ general _ in the sense that it is defined for _ any _ nonempty set @xmath208 of states , not only the sets  @xmath208 that occur as sets of tied states for some channel output  @xmath19 as in  ( [ general ] ) .    without this assumption",
    ", we could add to the above example another state with codeword @xmath224 so that the `` circular '' decoding function in ( [ circular ] ) is monotonic and gives a nash code , but is clearly not a fixed - order decoding function .",
    "it is reasonable to require that a decoding function is defined generally and does not just coincidentally lead to a nash code because certain ties do not occur ( as argued above , with the decoding ( [ circular ] ) we do not have a nash code when ties have to be resolved for @xmath184 )",
    ".    for general decoding functions , ( [ mono ] ) translates to the requirement that for any @xmath225 , @xmath226    [ p - fixedorder ] every general deterministic monotonic decoding function is a fixed - order decoding function .    because the decoding function is deterministic , @xmath227 for any nonempty set @xmath208 of states .",
    "define the following binary relation @xmath221 on @xmath15 : @xmath228 clearly , either @xmath229 or @xmath230 for any two states @xmath231 .",
    "we claim that @xmath221 is transitive , that is , if @xmath229 and @xmath232 , then @xmath233 .",
    "otherwise , there would be a `` cycle '' of distinct @xmath234 with @xmath229 and @xmath232 and @xmath235 .",
    "this is symmetric in @xmath234 , so assume @xmath236 and therefore @xmath237 and @xmath238 .",
    "however , with @xmath239 and @xmath240 we have @xmath241 , which contradicts ( [ genmono ] ) .",
    "so @xmath221 defines a linear order on @xmath15 .",
    "we show that ( [ fixedorder ] ) holds , that is , for any nonempty set of states @xmath209 the state @xmath0 so that @xmath242 is the state @xmath0 that fulfills @xmath233 for all @xmath243 .",
    "this holds trivially and by definition if @xmath209 has at most two elements , otherwise , if @xmath235 for some @xmath243 , then we obtain with @xmath239 the same contradiction @xmath241 as before .",
    "so the decoded state is chosen according to the fixed order @xmath221 on @xmath15 as claimed .    when the prior probabilities @xmath2 or the receiver utilities @xmath23 for the states @xmath0 are _ generic _",
    ", then @xmath33 in ( [ yi ] ) is always a singleton , so no ties occur and decoding is deterministic .",
    "one can make any prior probabilities generic by perturbing them minimally so that ties are broken uniquely but decoding is otherwise unaffected .",
    "that is , if @xmath0 and @xmath244 are tied for some @xmath19 because @xmath245 , this tie is broken in favor of @xmath0 by slightly increasing  @xmath2 , which will then always happen whenever @xmath0 and  @xmath244 are tied originally .",
    "this induces a fixed - order decoding , where any linear order among the states can be chosen .",
    "thus , proposition  [ p - fixedorder ] asserts that general deterministic monotonic decoding functions are those obtained by generic perturbation of the priors .",
    "finally , we observe that the above codebook @xmath175 with decoding as in ( [ nownash ] ) defines a nash code ( and if priors are minimally perturbed so that @xmath246 there are no ties and decoding is unique ) , but this code is not locally optimal as in theorem  [ t - opt ] .",
    "namely , by changing the codeword @xmath192 to @xmath180 , all possible channel outputs @xmath19 differ in at most bit from one of the four codewords , which clearly improves the payoff to the receiver .",
    "so not all binary nash codes are locally receiver - optimal .",
    "we first give an outline of the proof of theorem  [ t - bin ] .",
    "we want to show that for each state @xmath0 in @xmath15 , the sender maximizes the probability of correct decoding by sending the prescribed codeword  @xmath14 , so that @xmath247 holds for any @xmath248 . for any channel output @xmath19 , comparing @xmath249 and @xmath5 is only affected by the bits where @xmath14 and @xmath43 differ , defined by the set @xmath250 in ( [ sd ] ) below . for these bits ,",
    "the corresponding channel outputs are ordered according to how far they agree with @xmath14 ( and hence differ from @xmath43 ) , indicated by the subset @xmath251 of @xmath250 in ( [ yja ] ) .",
    "the key property is that with increasing @xmath251 , such a channel output is more likely to be decoded as state  @xmath0 , which is stated in ( [ hage ] ) and the main technical challenge , proved with the help of the monotonicity assumption ( [ mono ] ) .",
    "the payoff in ( [ pay ] ) is a multilinear function of the probabilities for receiving the individual output bits , see ( [ pyd ] ) and ( [ f ] ) . by considering this multilinear expression for each of the transmitted bits in @xmath250 and using the error inequalities ( [ error2 ] ) , the monotonicity condition ( [ hage ] ) translates to the inequality ( [ sender ] ) , as shown in  ( [ pge ] )",
    ".    conditions @xmath46 and @xmath47 state that the receiver uses a best response , so the equilibrium property holds on the receiver s side .",
    "let @xmath36 be the state chosen by nature .",
    "let @xmath43 in @xmath44 be an arbitrary alternative message to the codeword  @xmath14 .",
    "we want to prove  ( [ sender ] ) .",
    "let @xmath252 and @xmath250 be the sets of bits in @xmath43 and @xmath14 that are the same and different , respectively , that is , @xmath253 for any sets @xmath254 and @xmath251 and elements @xmath255 in @xmath254 for @xmath256 we write @xmath257 and denote the set of these vectors @xmath258 by @xmath259 .",
    "for any @xmath260 we write @xmath261 , so that with ( [ pyx ] ) @xmath262 in particular , by ( [ sd ] ) , @xmath263 fix @xmath264 .",
    "we will show that @xmath265 because @xmath266 for @xmath35 , summation of ( [ yd ] ) over all @xmath264 then implies ( [ sender ] ) .    by ( [ yi ] ) , @xmath267 if and only if for all @xmath268 , @xmath269 if equality holds in ( [ igek ] ) , then @xmath270 and there is a tie between states @xmath0 and  @xmath39 , which affects @xmath21 where we will use  ( [ mono ] ) .",
    "it is useful to consider the channel outputs @xmath271 ( for the bits in  @xmath250 ) according to how they agree with @xmath272 . for @xmath273 ,",
    "let @xmath274 clearly , any @xmath271 in @xmath275 can be written as @xmath276 for a unique subset @xmath251 of  @xmath250 .",
    "let @xmath273 and @xmath277 . for @xmath278",
    ", consider the sets @xmath279 so that @xmath280 .",
    "then according to ( [ pyx ] ) , @xmath281 for @xmath268 , let @xmath282 then ( [ igek ] ) is equivalent to @xmath283 for all @xmath284 .",
    "let @xmath285 $ ] for @xmath286 be the usual sign function defined by @xmath287=\\cases{-1&if $ t<0$,\\cr      0&if $ t=0$,\\cr       1&if $ t>0$.\\cr}\\ ] ] let @xmath288 , where we write @xmath289 instead of @xmath290 .",
    "we will show @xmath291\\ge \\sign[q_k(a)].\\ ] ] because @xmath292 , either @xmath293 or @xmath294 .",
    "consider the case @xmath293 , that is , @xmath295 and @xmath296 by  ( [ d0 ] ) .",
    "the change from @xmath251 to @xmath289 means that @xmath297 is obtained from @xmath298 by changing @xmath299 from @xmath61 to @xmath55 , so that the input bit @xmath300 is now correctly transmitted ( which happens with probability @xmath159 ) rather than incorrectly ( probability @xmath301 ) . by ( [ pyd ] ) , this means @xmath302 note that it is possible that @xmath303 , which means that the `` more correct '' channel output @xmath297 ( relative to the input bits in  @xmath304 ) is less likely than @xmath298 ; this is why we consider signs in ( [ sign ] ) because @xmath305 is not generally true .    when comparing the output bit @xmath306 with the input bit @xmath307 from the codeword @xmath308 , either @xmath309 , in which case ( [ acupj ] ) holds with @xmath39 instead of  @xmath0 , and , by ( [ qa ] ) , @xmath310 , so that ( [ sign ] ) holds with equality ; or , alternatively , @xmath311 , that is , @xmath312 .",
    "changing @xmath313 from @xmath61 to @xmath55 to obtain @xmath314 implies that the input @xmath307 is now transmitted with error , so that @xmath315 using ( [ qa ] ) and ( [ acupj ] ) , this means @xmath316",
    "\\ge \\frac{1-\\eps_0}{\\eps_0 } \\,q_k(a)\\ ] ] by ( [ error2 ] ) .",
    "again , ( [ sign ] ) holds , where here the sign of @xmath317 relative to that of @xmath318 may strictly increase .    the case @xmath294 where @xmath319 and @xmath320 is entirely analogous , by exchanging @xmath55 and  @xmath61 ( and thus @xmath301 and @xmath160 ) in the preceding reasoning .",
    "this shows  ( [ sign ] ) .    for @xmath273 ,",
    "let @xmath321 we show that for @xmath288 , @xmath322 with @xmath323 and @xmath324 , let @xmath208 and @xmath209 be defined as in ( [ mono ] ) .",
    "we are going to show that @xmath325 . as observed after ( [ qa ] ) , @xmath326 if and only if @xmath283 for all @xmath268 , and @xmath327 if and only if @xmath328 for all @xmath268 .",
    "if @xmath329 for some @xmath268 , then @xmath330 by ( [ arbtie ] ) , and ( [ hage ] ) holds trivially .",
    "so we can assume that @xmath283 for all @xmath268 and therefore @xmath328 for all @xmath268 by ( [ sign ] ) , that is , @xmath214 , and the sets of all states @xmath39 that are tied with @xmath0 are given by @xmath331 which implies that @xmath325 by ( [ sign ] ) .",
    "using the assumption ( [ mono ] ) then implies ( [ hage ] ) as claimed .",
    "consider now the function @xmath332^d\\to \\reals$ ] which for @xmath333^d$ ] is defined by @xmath334 which is the unique multilinear interpolation of the values @xmath335 defined on the vertices @xmath336 of the unit cube @xmath337^d$ ] , where @xmath338 is @xmath61 for @xmath256 and @xmath55 otherwise , with @xmath339 by ( [ f ] ) .",
    "the monotonicity ( [ hage ] ) extends to the monotonicity of @xmath340 in each variable @xmath255 , where we write @xmath341 for @xmath342 and @xmath343 , because by ( [ f ] ) , @xmath344 that is , because @xmath345 by ( [ hage ] ) and all products are nonnegative , @xmath346    using ( [ d0 ] ) , let @xmath347 and define @xmath348 and @xmath349 in @xmath337^d$ ] by @xmath350 then @xmath351 in each component by ( [ error2 ] ) . using ( [ zjge ] ) inductively shows @xmath352 the grand finale is to expand ( [ f ] ) , using ( [ d0 ] ) and ( [ d00 ] ) , to @xmath353 and to observe that by ( [ zid ] ) , ( [ d0 ] ) , ( [ pyd ] ) for @xmath354 , ( [ ha ] ) , ( [ fge ] ) , ( [ sd ] ) and again ( [ zid ] ) and ( [ pyd ] ) for @xmath355 , @xmath356 multiplying this inequality by @xmath357 on both sides and using ( [ xs ] ) then gives ( [ yd ] ) ( with @xmath271 written as @xmath298 ) , which was to be shown .",
    "we conclude with two small remarks :    first , the equilibrium condition for the sender does not necessarily hold strictly ; if all codewords have the same bit in one particular position , then that bit is ignored by the receiver and correspondingly can be altered by the sender in any codeword .",
    "second , the preceding proof works also if for each of the @xmath8 times that the channel is used independently , different error probabilities apply , as long as these are common knowledge .",
    "we have not made this assumption to avoid further notational complications .",
    "= -26ptby26pt plus1pt minus1pt blume , a. , and o. j. board ( 2009 ) , intentional vagueness .",
    "mimeo , university of pittsburgh .",
    "hernndez , p. , a. urbano , and j. e. vila ( 2010b ) , nash equilibrium and information transmission coding and decoding rules .",
    "discussion papers in economic behaviour eri - ces 09/2010 , university of valencia .",
    "myerson , r. b. ( 1994 ) , communication , correlated equilibria and incentive compatibility . in : r. j. aumann and s. hart , eds . , handbook of game theory with economic applications , vol .  2 , elsevier , amsterdam , 827847 ."
  ],
  "abstract_text": [
    "<S> this paper studies the stability of communication protocols that deal with transmission errors . </S>",
    "<S> we consider a coordination game between an informed sender and an uninformed decision maker , the receiver , who communicate over a noisy channel . </S>",
    "<S> the sender s strategy , called a code , maps states of nature to signals . </S>",
    "<S> the receiver s best response is to decode the received channel output as the state with highest expected receiver payoff . given this decoding , an equilibrium or `` nash code '' results if the sender encodes every state as prescribed . </S>",
    "<S> we show two theorems that give sufficient conditions for nash codes . </S>",
    "<S> first , a receiver - optimal code defines a nash code . a second , more surprising observation holds for communication over a binary channel which is used independently a number of times , a basic model of information transmission : under a minimal `` monotonicity '' requirement for breaking ties when decoding , which holds generically , _ any _ code is a nash code .    </S>",
    "<S> * keywords : * sender - receiver game , communication , noisy channel .    * or / ms subject classification : * communications ; games / group decisions : noncooperative .    </S>",
    "<S> * jel subject classification : * c72 , d82 .    * </S>",
    "<S> ams subject classification : * 91a28 , 94a05 . </S>"
  ]
}