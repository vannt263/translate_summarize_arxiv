{
  "article_text": [
    "over the last 15 years , a lot of progress has been achieved in high - dimensional statistics where the number of parameters can be much larger than sample size , covering ( nearly ) optimal point estimation , efficient computation and applications in many different areas ; see , for example , the books by @xcite , @xcite or the review article by @xcite .",
    "the core task of statistical inference accounting for uncertainty , in terms of frequentist confidence intervals and hypothesis testing , is much less developed .",
    "recently , a few methods for assigning @xmath0-values and constructing confidence intervals have been suggested ( @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ) .",
    "the current paper has three main pillars : ( i ) a ( selective ) review of the development in frequentist high - dimensional inference methods for @xmath0-values and confidence regions ; ( ii ) presenting the first broad , comparative empirical study among different methods , mainly for linear models : since the methods are mathematically justified under noncheckable and sometimes noncomparable assumptions , a thorough simulation study should lead to additional insights about reliability and performance of various procedures ; ( iii ) presenting the ` r`-package ` hdi ` ( _ _ h__igh-__d__imensional _ _ i__nference ) which enables to easily use many of the different methods for inference in high - dimensional generalized linear models .",
    "in addition , we include a recent line of methodology allowing to detect significant groups of highly correlated variables which could not be inferred as individually significant single variables ( @xcite ) .",
    "the review and exposition in @xcite is vaguely related to points ( i ) and ( iii ) above , but much more focusing on an application oriented viewpoint and covering much less statistical methodology , theory and computational details .",
    "our comparative study , point ( ii ) , mentioned above , exhibits interesting results indicating that more `` stable '' procedures based on ridge - estimation or random sample splitting with subsequent aggregation are somewhat more reliable for type i error control than asymptotically power - optimal methods .",
    "such results can not be obtained by comparing underlying assumptions of different methods , since these assumptions are often too crude and far from necessary . as expected , we are unable to pinpoint to a method which is ( nearly ) best in all considered scenarios . in view of this , we also want to offer a collection of useful methods for the community , in terms of our -package ` hdi ` mentioned in point ( iii ) above .",
    "we consider first a high - dimensional linear model , while extensions are discussed in section  [ sec.glm ] : @xmath1 with @xmath2 fixed or random design matrix @xmath3 , @xmath4 response and error vectors @xmath5 and @xmath6 , respectively .",
    "the errors are assumed to be independent of @xmath3 ( for random design ) with i.i.d .",
    "entries having @xmath7 = 0 $ ] . we allow for high - dimensional settings where @xmath8",
    ". in further development , the active set or the set of relevant variables @xmath9 as well as its cardinality @xmath10 , are important quantities .",
    "the main goals of this section are the construction of confidence intervals and @xmath0-values for individual regression parameters @xmath11 and corresponding multiple testing adjustment .",
    "the former is a highly nonstandard problem in high - dimensional settings , while for the latter we can use standard well - known techniques .",
    "when considering both goals simultaneously , though , one can develop more powerful multiple testing adjustments .",
    "the lasso ( @xcite ) is among the most popular procedures for estimating the unknown parameter @xmath12 in a high - dimensional linear model .",
    "it exhibits desirable or sometimes even optimal properties for point estimation such as prediction of @xmath13 or of a new response @xmath14 , estimation in terms of @xmath15 for @xmath16 , and variable selection or screening ; see , for example , the book of @xcite . for assigning uncertainties in terms of confidence intervals or hypothesis testing",
    ", however , the plain lasso seems inappropriate .",
    "it is very difficult to characterize the distribution of the estimator in the high - dimensional setting ; @xcite derive asymptotic results for fixed dimension as sample size @xmath17 and already for such simple situations , the asymptotic distribution of the lasso has point mass at zero .",
    "this implies , because of noncontinuity of the distribution , that standard bootstrapping and subsampling schemes are delicate to apply and uniform convergence to the limit seems hard to achieve .",
    "the latter means that the estimator is exposed to undesirable super - efficiency problems , as illustrated in section  [ subsec.comparlm ] .",
    "all the problems mentioned are expected to apply not only for the lasso but also for other sparse estimators as well .    in high - dimensional settings and for general fixed design @xmath3 ,",
    "the regression parameter is not identifiable .",
    "however , when making some restrictions on the design , one can ensure that the regression vector is identifiable .",
    "the so - called compatibility condition on the design @xmath3 ( @xcite ) is a rather weak assumption ( @xcite ) which guarantees identifiability and oracle ( near ) optimality results for the lasso . for the sake of completeness",
    ", the compatibility condition is described in appendix  [ subsec.appadd ] .    when assuming the compatibility condition with constant @xmath18 ( @xmath18 is close to zero for rather ill - posed designs , and sufficiently larger than zero for well - posed designs ) , the lasso has the following property : for gaussian errors and if @xmath19 , we have with high probability that @xmath20 thus , if @xmath21 and @xmath22 , we have @xmath23 and , hence , the parameter @xmath12 is identifiable .    another often used assumption ,",
    "although not necessary by any means , is the so - called beta - min assumption : @xmath24 for some choice of constant @xmath25 .",
    "the result in ( [ lasso - ell1 ] ) immediately implies the screening property : if @xmath26 , then @xmath27 thus , the screening property holds when assuming the compatibility and beta - min condition .",
    "the power of the screening property is a massive dimensionality reduction ( in the original variables ) because @xmath28 ; thus , if @xmath29 , the selected set @xmath30 is much smaller than the full set of @xmath0 variables .",
    "unfortunately , the required conditions are overly restrictive and exact variable screening seems rather unrealistic in practical applications ( @xcite ) .",
    "we describe here three different methods for construction of statistical hypothesis tests or confidence intervals .",
    "alternative procedures are presented in sections  [ subsec.othermeth ] and [ subsec.comparlm ] .",
    "a generic way for deriving @xmath0-values in hypotheses testing is given by splitting the sample with indices @xmath31 into two equal halves denoted by @xmath32 and @xmath33 , that is , @xmath34 with @xmath35 , @xmath36 , @xmath37 and @xmath38 .",
    "the idea is to use the first half @xmath32 for variable selection and the second half @xmath33 with the reduced set of selected variables ( from @xmath32 ) for statistical inference in terms of @xmath0-values .",
    "such a sample - splitting procedure avoids the over - optimism to use the data twice for selection and inference after selection ( without taking the effect of selection into account ) .",
    "consider a method for variable selection based on the first half of the sample : @xmath39 a prime example is the lasso which selects all the variables whose corresponding estimated regression coefficients are different from zero .",
    "we then use the second half of the sample @xmath33 for constructing @xmath0-values , based on the selected variables @xmath40 .",
    "if the cardinality @xmath41 , we can run ordinary least squares estimation using the subsample @xmath33 and the selected variables @xmath40 , that is , we regress @xmath42 on @xmath43 where the sub - indices denote the sample half and the super - index stands for the selected variables , respectively .",
    "thereby , we implicitly assume that the matrix @xmath43 has full rank @xmath44 .",
    "thus , from such a procedure , we obtain @xmath0-values @xmath45 for testing @xmath46 , for @xmath47 , from the classical @xmath48-tests , assuming gaussian errors or relying on asymptotic justification by the central limit theorem .",
    "to be more precise , we define ( raw ) @xmath0-values @xmath49 an interesting feature of such a sample - splitting procedure is the adjustment for multiple testing . for example , if we wish to control the familywise error rate over all considered hypotheses @xmath50 , a naive approach would employ a bonferroni  holm correction over the @xmath0 tests .",
    "this is not necessary : we only need to control over the considered @xmath44 tests in @xmath33 .",
    "therefore , a bonferroni corrected @xmath0-value for @xmath51 is given by @xmath52 in high - dimensional scenarios , latexmath:[$p \\gg n > \\lfloor",
    "n/2 \\rfloor\\geq    which holds for the lasso ( under weak assumptions ) , and thus , the correction factor employed here is rather small . such corrected @xmath0-values control the familywise error rate in multiple testing when assuming the screening property in ( [ screening ] ) for the selector @xmath54 based on the first half @xmath32 only , exactly as stated in fact  [ th1 ] below .",
    "the reason is that the screening property ensures that the reduced model is a correct model , and hence the result is not surprising . in practice ,",
    "the screening property typically does not hold exactly , but it is not a necessary condition for constructing valid @xmath0-values ( @xcite ) .",
    "the idea about sample - splitting and subsequent statistical inference is implicitly contained in @xcite .",
    "we summarize the whole procedure as follows :    _ single sample - splitting for multiple testing of @xmath51 among @xmath55 _ :    split ( partition ) the sample @xmath56 with @xmath57 and @xmath35 and @xmath58 .    using @xmath32 only ,",
    "select the variables @xmath59 . assume or enforce that @xmath60 .    denote the design matrix with the selected set of variables by @xmath61 .",
    "based on @xmath33 with data @xmath62 , compute @xmath0-values @xmath63 for @xmath51 , for @xmath64 , from classical least squares estimation [ i.e. , @xmath48-test which can be used since @xmath65 . for @xmath66 , assign @xmath67 .",
    "correct the @xmath0-values for multiple testing : consider @xmath68 which is an adjusted @xmath0-value for @xmath51 for controlling the familywise error rate .",
    "-values @xmath69 for a single covariable , in the ` riboflavin ` data set , when doing 50 different ( random ) sample splits .",
    "the figure is taken from bhlmann , kalisch and meier ( @xcite ) . ]",
    "a major problem of the single sample - splitting method is its sensitivity with respect to the choice of splitting the entire sample : sample splits lead to wildly different @xmath0-values .",
    "we call this undesirable phenomenon a @xmath0-value lottery , and figure  [ fig : pval_lottery ] provides an illustration . to overcome the `` @xmath0-value lottery",
    ", '' we can run the sample - splitting method @xmath70 times , with @xmath70 large .",
    "thus , we obtain a collection of @xmath0-values for the @xmath71th hypothesis @xmath51 : @xmath72},\\ldots , p_{\\mathrm{corr},j}^{[b]}\\quad ( j=1 , \\ldots , p).\\ ] ] the task is now to do an aggregation to a single @xmath0-value . because of dependence among @xmath73 }",
    "; b=1,\\ldots , b\\}$ ] , because all the different half samples are part of the same full sample , an appropriate aggregation needs to be developed .",
    "a simple solution is to use an empirical @xmath74-quantile with @xmath75 : @xmath76}/\\gamma ; b=1,\\ldots , b\\bigr\\},\\\\ & & \\qquad 1 \\bigr).\\end{aligned}\\ ] ] for example , with @xmath77 , this amounts to taking the sample median @xmath73 } ; b=1,\\ldots , b\\}$ ] and multiplying it with the factor 2 .",
    "a bit more sophisticated approach is to choose the best and properly scaled @xmath74-quantile in the range @xmath78 ( e.g. , @xmath79 ) , leading to the aggregated @xmath0-value @xmath80 \\\\[-8pt",
    "] \\eqntext{(j=1 , \\ldots , p).}\\end{aligned}\\ ] ] thereby , the factor @xmath81 is the price to be paid for searching for the best @xmath82 .",
    "this multi sample - splitting procedure has been proposed and analyzed in @xcite , and we summarize it below . before doing so , we remark that the aggregation of dependent @xmath0-values as described above is a general principle as described in appendix  [ subsec.appadd ] .",
    "_ multi sample - splitting for multiple testing of @xmath51 among @xmath55 _ :",
    "apply the single sample - splitting procedure @xmath70 times , leading to @xmath0-values @xmath73 } ; b=1,\\ldots , b\\}$ ] .",
    "typical choices are @xmath83 or @xmath84 .    aggregate these @xmath0-values as in ( [ aggreg ] ) , leading to @xmath85 which are adjusted @xmath0-values for @xmath86 , controlling the familywise error rate .",
    "the multi sample - splitting method enjoys the property that the resulting @xmath0-values are approximately reproducible and not subject to a `` @xmath0-value lottery '' anymore , and it controls the familywise error rate under the following assumptions :    the screening property as in ( [ screening ] ) for the first half of the sample : @xmath87 \\ge1 - \\delta$ ] for some @xmath88 .    the reduced design matrix for the second half of the sample satisfies @xmath89 .    [ th1 ] consider a linear model as in ( [ mod.lin ] ) with fixed design @xmath3 and gaussian errors .",
    "then , for a significance level @xmath90 and denoting by @xmath70 the number of sample splits , @xmath91 \\le\\alpha+ b \\delta,\\ ] ] that is , the familywise error rate ( fwer ) is controlled up to the additional ( small ) value @xmath92 .    a proof is given in meinshausen , meier and bhlmann ( @xcite ) .",
    "we note that the multi sample - splitting method can be used in conjunction with any reasonable , sparse variable screening method fulfilling ( a1 ) for very small @xmath93 and ( a2 ) ; and it does not necessarily rely on the lasso for variable screening . see also section  [ subsec.othersparsemeth ] .",
    "assumption ( a2 ) typically holds for the lasso satisfying @xmath94 .",
    "_ the screening property _ ( a1 ) .",
    "the screening property ( a1 ) with very small @xmath93 is not a necessary condition for constructing valid @xmath0-values and can be replaced by a zonal assumption requiring the following : there is a gap between large and small regression coefficients and there are not too many small nonzero regression coefficients ( @xcite ) .",
    "still , such a zonal assumption makes a requirement about the unknown @xmath12 and the absolute values of its components : but this is the essence of the question in hypothesis testing to infer whether coefficients are sufficiently different from zero , and one would like to do such a test without an assumption on the true values .",
    "the lasso satisfies ( a1 ) with @xmath95 when assuming the compatibility condition ( [ compat ] ) on the design @xmath3 , the sparsity assumption @xmath96 [ or @xmath97 when requiring a restricted eigenvalue assumption ] and a beta - min condition ( [ beta - min ] ) , as shown in ( [ screening ] )",
    ". other procedures also exhibit the screening property such as the adaptive lasso ( @xcite ) , analyzed in detail in @xcite , or methods with concave regularization penalty such as scad ( @xcite ) or mc@xmath98 ( @xcite ) .",
    "as criticized above , the required beta - min assumption should be avoided when constructing a hypothesis test about the unknown components of @xmath12 .",
    "fact  [ th1 ] has a corresponding asymptotic formulation where the dimension @xmath99 and the model depends on sample size @xmath100 : if ( a1 ) is replaced by @xmath101 \\to1 $ ] and for a fixed number @xmath70 , @xmath102 \\le\\alpha$ ] .",
    "in such an asymptotic setting , the gaussian assumption in fact  [ th1 ] can be relaxed by invoking the central limit theorem ( for the low - dimensional part ) .",
    "the multi sample - splitting method is very generic : it can be used for many other models , and its basic assumptions are an approximate screening property ( [ screening ] ) and that the cardinality @xmath103 so that we only have to deal with a fairly low - dimensional inference problem .",
    "see , for example , section  [ sec.glm ] for glms .",
    "an extension for testing group hypotheses of the form @xmath104 for all @xmath105 is indicated in section  [ subsec.assfree ] .",
    "confidence intervals can be constructed based on the duality with the @xmath0-values from equation ( [ aggreg ] ) .",
    "a procedure is described in detail in appendix  [ subsec.appmssplitci ] . the idea to invert the @xmath0-value method is to apply a bisection method having a point in and a point outside of the confidence interval . to verify",
    "if a point is inside the _ aggregated _ confidence interval , one looks at the fraction of confidence intervals from the splits which cover the point .",
    "we describe here a method , first introduced by @xcite , which does not require an assumption about @xmath12 except for sparsity .",
    "it is instructive to give a motivation starting with the low - dimensional setting where @xmath106 and @xmath107 .",
    "the @xmath71th component of the ordinary least squares estimator @xmath108 can be obtained as follows .",
    "do an ols regression of @xmath109 versus all other variables @xmath110 and denote the corresponding residuals by @xmath111",
    ". then @xmath112 can be obtained by a linear projection . in a high - dimensional setting ,",
    "the residuals @xmath111 would be equal to zero and the projection is ill - posed .    for the high - dimensional case with @xmath113 , the idea is to pursue a regularized projection . instead of ordinary least squares regression , we use a lasso regression of @xmath109 versus @xmath110 with corresponding residual vector @xmath111 : such a penalized regression involves a regularization parameter @xmath114 for the lasso , and hence @xmath115 . as in ( [ proj - ols ] ) , we immediately obtain ( for any vector @xmath111 ) @xmath116 \\\\[-8pt ] \\nonumber p_{jk}&= & \\bigl({\\mathbf{x}}^{(k)}\\bigr)^t z^{(j)}/\\bigl({\\mathbf{x}}^{(j)}\\bigr)^t z^{(j)}.\\end{aligned}\\ ] ] we note that in the low - dimensional case with @xmath111 being the residuals from ordinary least squares , due to orthogonality , @xmath117 .",
    "when using the lasso - residuals for @xmath111 , we do not have exact orthogonality and a bias arises .",
    "thus , we make a bias correction by plugging in the lasso estimator @xmath118 ( of the regression @xmath5 versus @xmath3 ) : the bias - corrected estimator is @xmath119 using ( [ proj - lasso ] ) , we obtain @xmath120 the first term on the right - hand side has a gaussian distribution , when assuming gaussian errors ; otherwise , it has an asymptotic gaussian distribution assuming that @xmath121 for @xmath122 ( which suffices for the lyapunov clt ) .",
    "we will argue in appendix  [ subsec.appadd ] that the second term is negligible under the following assumptions :    the design matrix @xmath3 has compatibility constant bounded away from zero , and the sparsity is @xmath123 .",
    "the rows of @xmath3 are fixed realizations of i.i.d .",
    "random vectors @xmath124 , and the minimal eigenvalue of @xmath125 is bounded away from zero .",
    "the inverse @xmath126 is row - sparse with @xmath127 .",
    "[ th2 ] consider a linear model as in ( [ mod.lin ] ) with fixed design and gaussian errors .",
    "assume , and ( or an @xmath128-sparsity assumption on the rows of @xmath126 ) .",
    "then @xmath129[(x^{(k)})^t z^{(k ) } ] } , \\\\ \\|\\delta\\|_{\\infty } & = & o_p(1).\\end{aligned}\\ ] ] [ we note that this statement holds with probability tending to one , with respect to the variables @xmath130 as assumed in ] .",
    "the asymptotic implications of fact  [ th2 ] are as follows : @xmath131 from which we can immediately construct a confidence interval or hypothesis test by plugging in an estimate @xmath132 as briefly discussed in section  [ subsec.addissues ] . from a theoretical perspective , it is more elegant to use the square root lasso ( @xcite ) for the construction of @xmath111 ; then one can drop ( b3 ) [ or the @xmath128-sparsity version of ( b3 ) ] ( @xcite ) .",
    "in fact , all that we then need is formula ( [ ell1bound ] ) @xmath133 from a practical perspective , it seems to make essentially no difference whether one takes the square root or plain lasso for the construction of the @xmath111 s .",
    "more general than the statements in fact  [ th2 ] , the following holds assuming ( b1)(b3 ) ( @xcite ) : the asymptotic variance @xmath134 reaches the cramr ",
    "rao lower bound , which equals @xmath135 [ which is bounded away from zero , due to ( b2 ) ] , and the estimator @xmath136 is efficient in the sense of semiparametric inference .",
    "furthermore , the convergence in fact  [ th2 ] is uniform over the subset of the parameter space where the number of nonzero coefficients @xmath137 is small and , therefore , we obtain _ honest _ confidence intervals and tests .",
    "in particular , both of these results say that all the complications in post - model selection do not arise ( @xcite ) , and yet @xmath136 is optimal for construction of confidence intervals of a single coefficient @xmath138 .    from a practical perspective",
    ", we need to choose the regularization parameters @xmath139 ( for the lasso regression of @xmath5 versus @xmath3 ) and @xmath114 [ for the nodewise lasso regressions ( @xcite ) of @xmath109 versus all other variables @xmath110 ] .",
    "regarding the former , we advocate a choice using cross - validation ; for the latter , we favor a proposal for a smaller @xmath114 than the one from cv , and the details are described in appendix  [ subsec.appadd ] .    furthermore , for a group @xmath141",
    ", we can test a group hypothesis @xmath142 for all @xmath143 by considering the test - statistic @xmath144 holds true .",
    "the distribution of @xmath145 can be easily simulated from dependent gaussian random variables .",
    "we also remark that sum - type statistics for large groups can not be easily treated because @xmath146 might get out of control .",
    "related to the desparsified lasso estimator @xmath147 in ( [ despars - lasso ] ) is an approach based on ridge estimation .",
    "we sketch here the main properties and refer to @xcite for a detailed treatment .",
    "consider @xmath148 a major source of bias occurring in ridge estimation when @xmath113 comes from the fact that the ridge estimator is estimating a projected parameter @xmath149 where @xmath150 denotes a generalized inverse of @xmath151 .",
    "the minor bias for @xmath152 then satisfies @xmath153 - \\theta^0_j\\bigr| \\le\\lambda\\bigl\\| \\theta^0 \\bigr\\|_2 \\lambda_{\\mathrm{min } \\neq0}(\\hat{\\sigma})^{-1},\\end{aligned}\\ ] ] where @xmath154 denotes the minimal nonzero eigenvalue of @xmath155 ( @xcite ) .",
    "the quantity can be made small by choosing @xmath139 small .",
    "therefore , for @xmath156 and assuming gaussian errors , we have that @xmath157 where @xmath158 . since @xmath159",
    "the major bias for @xmath138 can be estimated and corrected with @xmath160 where @xmath118 is the ordinary lasso .",
    "thus , we construct a bias - corrected ridge estimator , which addresses the potentially substantial difference between @xmath152 and the target @xmath12 : @xmath161 \\\\[-8pt ]   \\eqntext{j=1 , \\ldots , p.}\\end{aligned}\\ ] ] based on ( [ ridge - distr ] ) , we derive in appendix  [ subsec.appadd ] that @xmath162 with the typical choice @xmath163",
    ". sufficient conditions for deriving ( [ ridge - repr ] ) are assumption ( b1 ) and that the sparsity satisfies @xmath164 for @xmath165 as above .    unlike as in fact  [ th2 ] , the term @xmath166 is typically not negligible and we correct the gaussian part in ( [ ridge - repr ] ) by the upper bound @xmath167 .",
    "for example , for testing @xmath168 we use the upper bound for the @xmath0-value @xmath169 similarly , for two - sided confidence intervals with coverage @xmath170 we use @xmath171 , \\\\ & & c_j = \\delta_{r\\mathrm{bound};j } + \\sigma_{{\\varepsilon } } \\omega _ { r;jj}^{1/2}/|p_{r;jj}| \\phi^{-1}(1- \\alpha/2).\\end{aligned}\\ ] ]    for testing a group hypothesis for @xmath172 , @xmath142 for all @xmath143 , we can proceed similarly as at the end of section  [ subsec.desparslasso ] : under the null - hypotheses @xmath173 , the statistic @xmath174 has a distribution which is approximately stochastically upper bounded by @xmath175 see also ( [ ridge - repr ] ) .",
    "when invoking an upper bound for latexmath:[$\\delta_{r\\mathrm{bound};j } \\ge    distribution from dependent gaussian random variables , which in turn can be used to construct a @xmath0-value ; we refer for further details to @xcite .      unlike the multi sample - splitting procedure in section  [ subsec.multisample-split ] , the desparsified lasso and ridge projection method outlined in sections  [ subsec.desparslasso][subsec.ridge-proj ] require to plug - in an estimate of @xmath177 and to adjust for multiple testing .",
    "the scaled lasso ( @xcite ) leads to a consistent estimate of the error variance : it is a fully automatic method which does not need any specification of a tuning parameter . in @xcite ,",
    "an empirical comparison of various estimators suggests that the estimator based on a residual sum of squares of a cross - validated lasso solution often yields good finite - sample performance .    regarding the adjustment when doing many tests for individual regression parameters or groups thereof , one can use any valid standard method to correct the @xmath0-values from the desparsified lasso or ridge projection method .",
    "the prime examples are the bonferroni ",
    "holm procedure for controlling the familywise error rate and the method from @xcite for controlling the false discovery rate .",
    "an approach for familywise error control which explicitly takes the dependence among the multiple hypotheses is proposed in @xcite , based on simulations for dependent gaussian random variables .",
    "we briefly outline here conceptual differences while section  [ subsec.comparlm ] presents empirical results .",
    "the multi sample - splitting method is very generic and in the spirit of breiman s appeal for stability ( @xcite , @xcite ) , it enjoys some kind of stability due to multiple sample splits and aggregation ; see also the discussion in sections  [ subsec.othersparsemeth ] and [ subsec.mainass ] .",
    "the disadvantage is that , in the worst case , the method needs a beta - min or a weaker zonal assumption on the underlying regression parameters : this is somewhat unpleasant since a significance test should _ find out _ whether a regression coefficient is sufficiently large or not .",
    "both the desparsified lasso and ridge projection procedures do not make any assumption on the underlying regression coefficient except sparsity .",
    "the former is most powerful and asymptotically optimal if the design were generated from a population distribution whose inverse covariance matrix is sparse .",
    "furthermore , the convergence is uniform over all sparse regression vectors and , hence , the method yields honest confidence regions or tests .",
    "the ridge projection method does not require any assumption on the fixed design but does not reach the asymptotic cramr  rao efficiency bound .",
    "the construction with the additional correction term in ( [ delta - bound ] ) leads to reliable type i error control at the cost of power .    in terms of computation ,",
    "the multi sample - splitting and ridge projection method are substantially less demanding than the desparsified lasso .",
    "all the methods described above are used `` in default mode '' in conjunction with the lasso ( see also section  [ subsec.hdilin ] ) .",
    "this is not necessary , and other estimators can be used .    for the multi sample - splitting procedure , assumptions ( a1 ) with @xmath178 and ( a2 )",
    "are sufficient for asymptotic correctness ; see fact  [ th1 ] .",
    "these assumptions hold for many reasonable sparse estimators when requiring a beta - min assumption and some sort of identifiability condition such as the restricted eigenvalue or the compatibility condition on the design matrix  @xmath3 ; see also the discussion after fact  [ th1 ] .",
    "it is unclear whether one could gain substantially by using a different screening method than the lasso .",
    "in fact , the lasso has been empirically found to perform rather well for screening in comparison to the elastic net ( @xcite ) , marginal correlation screening ( @xcite ) or thresholded ridge regression ; see @xcite .    for the desparsified lasso , the error of the estimated bias correction can be controlled by using a bound for @xmath179 .",
    "if we require ( b2 ) and ( b3 ) [ or an @xmath128 sparsity assumption instead of ( b3 ) ] , the estimation error in the bias correction , based on an estimator @xmath118 in ( [ despars - lasso ] ) , is asymptotically negligible if @xmath180 this bound is implied by ( b1 ) and ( b2 ) for the lasso , but other estimators exhibit this bound as well , as mentioned below .",
    "when using such another estimator , the wording `` desparsified lasso '' does not make sense anymore .",
    "furthermore , when using the square root lasso for the construction of @xmath111 , we only need ( [ ell1bound ] ) to obtain asymptotic normality with the @xmath181 convergence rate ( @xcite ) .    for the ridge projection method ,",
    "a bound for @xmath179 is again the only assumption such that the procedure is asymptotically valid .",
    "thus , for the corresponding bias correction , other methods than the lasso can be used .",
    "we briefly mention a few other methods for which we have reasons that ( a1 ) with very small @xmath93 and ( a2 ) , or the bound in ( [ ell1bound ] ) hold : the adaptive lasso ( @xcite ) analyzed in greater detail in @xcite , the mc@xmath98 procedure with its high - dimensional mathematical analysis ( @xcite ) , or methods with concave regularization penalty such as scad ( @xcite ) analyzed in broader generality and detail in @xcite .",
    "if the assumptions ( a1 ) with small @xmath93 and ( a2 ) fail for the multi sample - splitting method , the multiple sample splitting still allows to check the stability of the @xmath0-values @xmath182}$ ] across @xmath183 ( i.e. , across sample splits ) .",
    "if the variable screening is unstable , many of the @xmath182}$ ] ( across @xmath183 ) will be equal to 1 , therefore , the aggregation has a tendency to produce small @xmath0-values if most of them , each from a sample split , are stable and small .",
    "see also @xcite , section  5 . in connection with the desparsified method , a failure of the single sufficient condition in ( [ ell1bound ] ) , when using , for example , the square root lasso for construction of the @xmath111 s , might result in a too large bias . in absence of resampling or multi sample splitting",
    ", it seems difficult to diagnose such a failure ( of the desparsified or ridge projection method ) with real data .      in the -package ` hdi ` , available on r - forge ( @xcite ) , we provide implementations for the multi sample - splitting , the ridge projection and the desparsified lasso method .",
    "using the functions is straightforward :    .... > outmssplit    < - multi.split(x = x , y = y ) > outridge    < - ridge.proj(x = x , y = y ) > outlasso    < - lasso.proj(x = x , y = y ) ....    for users that are very familiar with the procedures , we provide flexible options . for example , we can easily use an alternative model selection or another `` classical '' fitting procedure using the arguments ` model.selector ` and ` classical.fit ` in ` multi.split ` .",
    "the default options should be satisfactory for standard usage .",
    "all procedures return @xmath0-values and confidence intervals . the ridge and desparsified lasso methods return both single testing @xmath0-values as well as multiple testing corrected @xmath0-values , unlike the multi sample - splitting procedure which only returns multiple testing corrected @xmath0-values .",
    "the confidence intervals are for individual parameters only ( corresponding to single hypothesis testing ) .",
    "the single testing @xmath0-values and the multiple testing corrected @xmath0-values are extracted from the fit as follows :    .... > outridge$pval > outridge$pval.corr ....    by default , we correct for controlling the familywise error rate for the @xmath0-values ` pval.corr ` .    confidence intervals are acquired through the usual ` confint ` interface . below we extract the 95 % confidence intervals for those @xmath0-values that are smaller than ` 0.05 ` :    .... > confint(outmssplit ,    parm = which(outmssplit$pval.corr     < = 0.05 ) ,     level = 0.95 ) ....    due to the fact that the desparsified lasso method is quite computationally intensive , we provide the option to parallelize the method on a user - specified number of cores .",
    "we refer to the manual of the package for more detailed information .",
    "recently , other procedures have been suggested for construction of @xmath0-values and confidence intervals .",
    "residual - type bootstrap approaches are proposed and analyzed in @xcite and @xcite .",
    "a problem with these approaches is the nonuniform convergence to a limiting distribution and exposure to the super - efficiency phenomenon , that is , if the true parameter equals zero , a confidence region might be the singleton @xmath184 ( due to a finite amount of bootstrap resampling ) , while for nonzero true parameter values , the coverage might be very poor or a big length of the confidence interval .",
    "the covariance test ( @xcite ) is another proposal which relies on the solution path of the lasso and provides @xmath0-values for conditional tests that all relevant variables enter the lasso solution path first .",
    "it is related to post - selection inference , mentioned in section  [ subsec.postsel ] .    in @xcite , a procedure",
    "was proposed that is very similar to the one described in section  [ subsec.desparslasso ] , with the only difference being that z is picked as the solution of a convex program rather than using the lasso .",
    "the method is aiming to relax the sparsity assumption ( b3 ) for the design .    a conservative _",
    "group - bound _ method which needs no regularity assumption for the design , for example , no compatibility assumption ( [ compat ] ) , has been proposed by @xcite .",
    "the method has the capacity to automatically determine whether a regression coefficient is identifiable or not , and this makes the procedure very robust against ill - posed designs .",
    "the main motivation of the method is in terms of testing groups of correlated variables , and we discuss it in more detail in section  [ subsec.assfree ] .",
    "while all the methods mentioned above are considered in a comparative simulation study in section  [ subsec.comparlm ] , we mention here some others .",
    "the idea of estimating a low - dimensional component of a high - dimensional parameter is also worked out in @xcite , @xcite , bearing connections to the approach of desparsifying the lasso . based on stability selection ( @xcite ) , @xcite",
    "propose a version which leads to @xmath0-values for testing individual regression parameters .",
    "furthermore , there are new and interesting proposals for controlling the false discovery rate , in a `` direct way '' ( @xcite @xcite ; @xcite ) .",
    "we discuss here some of the main assumptions , potential violations and some corresponding implications calling for caution when aiming for confirmatory conclusions .",
    "_ linear model assumption_. the first one is that the linear ( or some other ) model is correct",
    ". this might be rather unrealistic and , thus , it is important to interpret the output of software or a certain method . consider a nonlinear regression model @xmath185 where , with some slight abuse of notation , @xmath186 .",
    "we assume for the random design model , @xmath187 is independent from @xmath188 ,",
    "@xmath189 = 0 $ ] , @xmath190 = 0 $ ] , @xmath191 = 0 $ ] , and the data are @xmath100 i.i.d . realizations of @xmath192 ; for the fixed design model , the @xmath193 random vector @xmath194 has i.i.d .",
    "components with @xmath195=0 $ ] . for the random design model ,",
    "we consider @xmath196\\nonumber\\end{aligned}\\ ] ] [ where the latter is unique if @xmath197 is positive definite ] .",
    "we note that @xmath198 \\neq0 $ ] while @xmath199 = 0 $ ] and , therefore , the inference should be _ unconditional _ on @xmath3 and is to be interpreted for the projected parameter @xmath12 in ( [ betaproj ] ) .",
    "furthermore , for correct asymptotic inference of the projected parameter @xmath12 , a modified estimator for the asymptotic variance of the estimator is needed ; and then both the multi sample - splitting and the desparsified lasso are asymptotically correct ( assuming similar conditions as if the model were correct ) .",
    "the multi sample - splitting method is well suited for the random design case because the sample splitting ( resampling type ) is coping well with i.i.d",
    "this is in contrast to fixed design , where the data is not i.i.d . and the multi sample - splitting method for a misspecified linear model is typically not working anymore . the details are given in @xcite .    for a fixed design model with @xmath200",
    ", we can always write @xmath201 for many solutions @xmath12 . for ensuring that the inference is valid , one should consider a sparse @xmath12 , for example , the basis pursuit solution from compressed sensing ( @xcite ) as one among many solutions .",
    "thus , inference should be interpreted for a _ sparse _",
    "solution @xmath12 , in the sense that a confidence interval for the @xmath71th component would cover this @xmath71th component of all sufficiently sparse solutions @xmath12 . for the high - dimensional fixed design case",
    ", there is no misspecification with respect to linearity of the model ; misspecification might happen , though , if there is no solution @xmath202 which fulfills a required sparsity condition .",
    "the details are given again in @xcite .",
    "the assumption about constant error variance might not hold .",
    "we note that in the random design case of a nonlinear model as above , the error in ( [ betaproj ] ) has nonconstant variance when conditioning on @xmath3 , but , unconditionally , the noise is homoscedastic .",
    "thus , as outlined , the inference for a random design linear model is asymptotically valid ( unconditional on @xmath3 ) even though the conditional error distribution given @xmath3 has nonconstant variance .",
    "_ compatibility or incoherence - type assumption_. the methods in section  [ subsec.lm-methods ] require an identifiability assumption such as the compatibility condition on the design matrix @xmath3 described in ( [ compat ] ) . the procedure in section  [ subsec.assfree ]",
    "does not require such an assumption : if a component of the regression parameter is not identifiable , the method will not claim significance .",
    "hence , some robustness against nonidentifiability is offered with such a method .",
    "_ sparsity .",
    "_ all the described methods require some sparsity assumption of the parameter vector @xmath12 [ if the model is misspecified , this concerns the parameter @xmath12 as in ( [ betaproj ] ) or the basis pursuit solution ] ; see the discussion of ( a1 ) after fact  [ th1 ] or assumption ( b1 ) .",
    "such sparsity assumptions can be somewhat relaxed to require weak sparsity in terms of @xmath203 for some @xmath204 , allowing that many or all regression parameters are nonzero but sufficiently small ( cf .",
    "@xcite ; @xcite ) .",
    "when the truth ( or the linear approximation of the true model ) is nonsparse , the methods are expected to break down . with the multi sample - splitting procedure ,",
    "however , a violation of sparsity might be detected , since for nonsparse problems , a sparse variable screening method will be typically unstable with the consequence that the resulting aggregated @xmath0-values are typically not small ; see also section  [ subsec.othersparsemeth ] .",
    "finally , we note that for the desparsified lasso , the sparsity assumption ( b3 ) or its weaker version can be dropped when using the square root lasso ; see the discussion after fact  [ th2 ] .    _",
    "hidden variables_. the problem of hidden variables is most prominent in the area of causal inference ( cf .",
    "@xcite ) . in the presence of hidden variables ,",
    "the presented techniques need to be adapted , adopting ideas from , for example , the framework of em - type estimation ( cf .",
    "@xcite ) , low - rank methods ( cf .",
    "@xcite ) or the fci technique from causal inference ( cf .",
    "@xcite ) .",
    "we compare a variety of methods on the basis of multiple testing corrected @xmath0-values and single testing confidence intervals .",
    "the methods we look at are the multiple sample - splitting method _ ms - split _",
    "( section  [ subsec.multisample-split ] ) , the desparsified lasso method _ lasso - pro _ ( section  [ subsec.desparslasso ] ) , the ridge projection method _ ridge _ ( section  [ subsec.ridge-proj ] ) , the covariance test _ covtest _ ( section  [ subsec.othermeth ] ) , the method by javanmard and montanari _ jm2013 _ ( section  [ subsec.othermeth ] ) and the two bootstrap procedures mentioned in section  [ subsec.othermeth ] [ _ res - boot _ corresponds to @xcite and _ liuyu _ to @xcite ] .      for the estimation of the error variance , for the ridge projection or the desparsified lasso method ,",
    "the scaled lasso is used as mentioned in section  [ subsec.addissues ] .    for the choice of tuning parameters for the nodewise lasso regressions ( discussed in section  [ subsec.desparslasso ] ) , we look at the two alternatives of using either cross - validation or our more favored alternative procedure ( denoted by z&z ) discussed in appendix  [ subsec.appadd ] .",
    "we do not look at the bootstrap procedures in connection with multiple testing adjustment due to the fact that the required number of bootstrap samples grows out of proportion to go far enough in the tails of the distribution ; some additional importance sampling might help to address such issues .    regarding the covariance test",
    ", the procedure does not directly provide @xmath0-values for the hypotheses we are interested in .",
    "for the sake of comparison though , we use the interpretation as in @xcite .",
    "this interpretation does not have a theoretical reasoning behind it and functions more as a heuristic .",
    "thus , the results of the covariance test procedure should be interpreted with caution .    for the method _ jm2013 _ , we used our own implementation instead of the code provided by the authors .",
    "the reason for this is that we had already implemented our own version when we discovered that code was available and our own version was ( orders of magnitude ) better in terms of error control .",
    "posed with the dilemma of fair comparison , we stuck to the best performing alternative .",
    "for the empirical results , simulated design matrices as well as design matrices from real data are used .",
    "the simulated design matrices are generated @xmath205 with covariance matrix @xmath206 of the following three types : @xmath207 the sample size and dimension are fixed at @xmath208 and @xmath209 , respectively .",
    "we note that the toeplitz type has a banded inverse @xmath126 , and , vice - versa , the exp.decay type exhibits a banded @xmath125 .",
    "the design matrix realx from real gene expression data of bacillus subtilis ( @xmath210 ) was kindly provided by dsm ( switzerland ) and is publicly available ( @xcite ) . to make the problem somewhat comparable in difficulty to the simulated designs ,",
    "the number of variables is reduced to @xmath209 by taking the variables with highest empirical variance .",
    "the cardinality of the active set is picked to be one of two levels @xmath211 .    for each of the active set sizes ,",
    "we look at 6 different ways of picking the sizes of the nonzero coefficients : @xmath212    the positions of the nonzero coefficients as columns of the design @xmath213 are picked at random . results where the nonzero coefficients were positioned to be the first @xmath214 columns of @xmath213 can be found in the supplemental article ( @xcite ) .    once we have the design matrix @xmath213 and coefficient vector @xmath12 , the responses @xmath5",
    "are generated according to the linear model equation with @xmath215 .",
    "we investigate multiple testing corrected @xmath0-values for two - sided testing of the null hypotheses @xmath168 for @xmath216 .",
    "we report the power and the familywise error rate ( fwer ) for each method : @xmath217/s_0 , \\\\",
    "\\mbox{fwer } & = & { \\mathbb{p}}\\bigl[\\exists j \\in s_0^c : h_{0,j}\\mbox { is rejected}\\bigr].\\end{aligned}\\ ] ] we calculate empirical versions of these quantities based on fitting 100 simulated responses @xmath5 coming from newly generated @xmath218 .    for every design type , active set size and coefficient type combination we obtain 50 data points of the empirical versions of `` power '' and `` fwer , '' from 50 independent simulations .",
    "thereby , each data point has a newly generated @xmath219 , @xmath12 ( if not fixed ) and active set positions @xmath220 ; thus , the 50 data points indicate the variability with respect to the three quantities in the data generation ( for the same covariance model of the design , the same model for the regression parameter and its active set positions ) .",
    "the data points are grouped in plots by design type and active set size .",
    "we also report the average number of false positives ` avg(v ) ` over all data points per method next to the fwer plot .",
    "the results , illustrating the performance for various methods , can be found in figures  [ fig : lintoeplitz ] , [ fig : linexpdecay ] , [ fig : linequi ] and [ fig : linrealx ] .",
    "we investigate confidence intervals for the one particular setup of the toeplitz design , active set size @xmath221 and coefficients @xmath222\\ ( j \\in s_0)$ ] .",
    "the active set positions are chosen to be the first @xmath214 columns of @xmath213 .",
    "the results we show will correspond to a single data point in the @xmath0-value results .    in figure",
    "[ fig : lincitoeplitz ] , 100 confidence intervals are plotted for each coefficient for each method .",
    "these confidence intervals are the results of fitting 100 different responses y resulting from newly generated @xmath218 error terms .    for the multi sample - splitting method from section  [ subsec.multisample-split ] ,",
    "if a variable did not get selected often enough in the sample splits , there is not enough information to draw a confidence interval for it .",
    "this is represented in the plot by only drawing confidence intervals when this was not the case .",
    "if the ( uncheckable ) beta - min condition ( [ beta - min ] ) would be fulfilled , we would know that those confidence intervals cover zero .    for the bootstrapping methods ,",
    "an invisible confidence interval is the result of the coefficient being set to zero in all bootstrap iterations .      as a first observation , the impact of the sparsity of the problem on performance can not be denied .",
    "the power clearly gets worse for @xmath223 for the toeplitz and exp.decay setups .",
    "the fwer becomes too high for quite a few methods for @xmath223 in the cases of equi.corr and realx .",
    "for the sparsity @xmath221 , the ridge projection method manages to control the fwer as desired for all setups . in the case of @xmath223",
    ", it is the multi sample - splitting method that comes out best in comparison to the other methods . generally speaking",
    ", good error control tends to be associated with a lower power , which is not too surprising since we are dealing with the trade - off between type i and type ii errors .",
    "the desparsified lasso method turns out to be a less conservative alternative with not perfect but reasonable fwer control as long as the problem is sparse enough ( @xmath221 ) .",
    "the method has a slightly too high fwer for the equi.corr and realx setups , but fwer around 0.05 for toeplitz and exp.decay designs . doing",
    "the z&z tuning procedure helps the error control , as can be seen most clearly in the equi.corr setup . the results for the simulations where the positions for the nonzero coefficients were not randomly chosen , presented in the supplemental article ( @xcite ) , largely give the same picture .",
    "in comparison to the results presented before , the toeplitz setup is easier while the exp.decay setup is more challenging .",
    "the equi.corr results are very similar to the ones from before , which is to be expected from the covariance structure .",
    "looking into the confidence interval results , it is clear that the confidence intervals of the multi sample - splitting method and the ridge projection method are wider than the rest . for the bootstrapping methods ,",
    "the super - efficiency phenomenon mentioned in section  [ subsec.othermeth ] is visible .",
    "important to note here is that the smallest nonzero coefficient , the third column , has very poor coverage from these methods .",
    "we can conclude that the coverage of the zero coefficients is decent for all methods and that the coverage of the nonzero coefficients is in line with the error rates for the @xmath0-values .",
    "confidence interval results for many other setup combinations are provided in the supplemental article ( @xcite ) .",
    "the observations are to a large extent the same .",
    "consider a generalized linear model @xmath224\\bigr ) = \\mu^0 + \\sum_{j=1}^p \\beta^0_j x^{(j)},\\end{aligned}\\ ] ] where @xmath225 is a real - valued , known link function .",
    "as before , the goal is to construct confidence intervals and statistical tests for the unknown parameters @xmath226 , and maybe @xmath227 as well .",
    "the multi sample - splitting method can be modified for glms in an obvious way : the variable screening step using the first half of the data can be based on the @xmath128-norm regularized mle , and @xmath0-values and confidence intervals using the second half of the sample are constructed from the asymptotic distribution of the ( low - dimensional ) mle .",
    "multiple testing correction and aggregation of the @xmath0-values from multiple sample splits are done exactly as for linear models in section  [ subsec.multisample-split ] .",
    "a desparsified lasso estimator for glms can be constructed as follows ( @xcite ) : the @xmath128-norm regularized mle @xmath228 for the parameters @xmath229 is desparsified with a method based on the karush  kuhn ",
    "tucker ( kkt ) conditions for @xmath228 , leading to an estimator with an asymptotic gaussian distribution .",
    "the gaussian distribution can then be used to construct confidence intervals and hypothesis tests .",
    "the problem can be simplified in such a way that we can apply the approaches for the linear model from section  [ sec.lm ] .",
    "this can be done for all types of generalized linear models ( as shown in appendix  [ subsec.app.general.wsqerr ] ) , but we restrict ourselves in this section to the specific case of logistic regression .",
    "logistic regression is usually fitted by applying the iteratively reweighted least squares ( irls ) algorithm where at every iteration one solves a weighted least squares problem ( @xcite ) .",
    "the idea is now to apply a standard l1-penalized fitting of the model , build up the weighted least squares problem at the l1-solution and then apply our linear model methods on this problem .",
    "we use the notation @xmath230 for the estimated probability of the binary outcome .",
    "@xmath231 is the vector of these probabilities .        from @xcite ,",
    "the adjusted response variable becomes @xmath232 and the weighted least squares problem is @xmath233 with weights @xmath234    we rewrite @xmath235 and @xmath236 to get @xmath237    the linear model methods can now be applied to @xmath238 and @xmath239 , thereby the estimate @xmath132 has to be set to the value 1 .",
    "we note that in the low - dimensional case , the resulting @xmath0-values ( with unregularized residuals @xmath240 ) are very similar to the @xmath0-values provided by the standard ` r`-function ` glm ` .",
    "we provide a small empirical comparison of the methods mentioned in sections  [ subsec.glmmethods ] and [ subsec.glmweighted ] .",
    "when applying the linear model procedures , we use the naming from section  [ subsec.comparlm ] . the new glm - specific methods from section  [ subsec.glmmethods ] are referred to by their linear model names with a capital g added to them .    for simulating the data , we use a subset of the variations presented in section  [ subsubsec.data ] .",
    "we only look at toeplitz and equi.corr and an active set size of @xmath221 . the number of variables is fixed at @xmath209 , but the sample size is varied @xmath241 .",
    "the coefficients were randomly generated : @xmath242 the nonzero coefficient positions are chosen randomly in one case and fixed as the first @xmath214 columns of @xmath213 in the other .",
    "for every combination ( of type of design , type of coefficients , sample size and coefficient positions ) , 100 responses @xmath5 are simulated to calculate empirical versions of the `` power '' and `` fwer '' described in section  [ subsubsec.pvals ] .",
    "in contrast to the @xmath0-value results from section  [ subsubsec.pvals ] , there is only one resulting data point per setup combination ( i.e. , no additional replication with new random covariates , random coefficients and random active set ) . for each method",
    ", there are 18 data points , corresponding to 18 settings , in each plot .",
    "the results can be found in figure  [ fig : glmsimul ] .",
    "both the modified glm methods as well as the weighted squared error approach work adequately .",
    "the equi.corr setup does prove to be challenging for _ lasso - prog_.      in the ` hdi ` -package ( @xcite ) we also provide the option to use the ridge projection method and the desparsified lasso method with the weighted squared error approach .",
    "we provide the option to specify the ` family ` of the response @xmath5 as done in the -package ` glmnet ` :    .... > outridge    < - ridge.proj(x = x , y = y ,       family = ' ' binomial '' ) > outlasso    < - lasso.proj(x = x , y = y ,       family = ' ' binomial '' ) ....    @xmath0-values and confidence intervals are extracted in the exact same way as for the linear model case ; see section  [ subsec.hdilin ] .",
    "the previous sections and methods assume in some form or another that the effects are strong enough to enable accurate estimation of the contribution of _ individual variables_.    variables are often highly correlated for high - dimensional data . working with a small sample size , it is impossible to attribute any effect to an individual variable if the correlation between a block of variables is too high .",
    "confidence intervals for individual variables are then very wide and uninformative .",
    "asking for confidence intervals for individual variables thus leads to poor power of all procedures considered so far .",
    "perhaps even worse , under high correlation between variables the coverage of some procedures will also be unreliable as the necessary conditions for correct coverage ( such as the compatibility assumption ) are violated .",
    "in such a scenario , the individual effects are not granular enough to be resolved .",
    "however , it might yet still be possible to attribute an effect to a group of variables .",
    "the groups can arise naturally due to a specific structure of the problem , such as in applications of the _ group lasso _ ( @xcite ) .",
    "perhaps more often , the groups are derived via hierarchical clustering ( @xcite ) , using the correlation structure or some other distance between the variables .",
    "the main idea is as follows .",
    "a hierarchy @xmath243 is a set of clusters or groups @xmath244 with @xmath245 .",
    "the root node ( cluster ) contains all variables @xmath246 .",
    "for any two clusters @xmath247 , either one cluster is a subset of the other or they have an empty intersection .",
    "usually , a hierarchical clustering has an additional notion of a level such that , on each level , the corresponding clusters build a partition of @xmath246 .",
    "we consider a hierarchy @xmath243 and first test the root node cluster @xmath248 with hypothesis @xmath249 .",
    "if this hypothesis is rejected , we test the next clusters @xmath250 in the hierarchy ( all clusters whose supersets are the root node cluster @xmath251 only ) : the corresponding cluster hypotheses are @xmath252 for all @xmath253 . for the hypotheses which can be rejected",
    ", we consider all smaller clusters whose only supersets are clusters which have been rejected by the method before , and we continue to go down the tree hierarchy until no more cluster hypotheses can be rejected .    with the hierarchical scheme in place , we still need a test for the null hypothesis @xmath254 of a cluster of variables .",
    "the tests have different properties .",
    "for example , whether a multiplicity adjustment is necessary will depend on the chosen test .",
    "we will describe below some methods that are useful for testing the effect of a group of variables and which can be used in such a hierarchical approach .",
    "the nice and interesting feature of the procedures is that they adapt automatically to the level of the hierarchical tree : if a signal of a small cluster of variables is strong , and if that cluster is sufficiently uncorrelated from all other variables or clusters , the cluster will be detected as significant .",
    "vice - versa , if the signal is weak or if the cluster has too high a correlation with other variables or clusters , the cluster will not become significant .",
    "for example , a single variable can not be detected as significant if it has too much correlation to other variables or clusters .",
    "the _ group - bound _ proposed in @xcite gives confidence intervals for the @xmath128-norm @xmath255 of a group @xmath256 of variables .",
    "if the lower - bound of the @xmath170 confidence interval is larger than 0 , then the null hypothesis @xmath257 can be rejected for this group .",
    "the method combines a few properties :    the confidence intervals are valid without an assumption like the compatibility condition ( [ compat ] ) . in general , they are conservative , but if the compatibility condition holds , they have good `` power '' properties ( in terms of length ) as well .",
    "the test is hierarchical .",
    "if a set of variables can be rejected , all supersets will also be rejected . and vice - versa ,",
    "if a group of variables can not be rejected , none of its subsets can be rejected .",
    "the estimation accuracy has an optimal detection rate under the so - called group effect compatibility condition , which is weaker than the compatibility condition necessary to detect the effect of individual variables .",
    "the power of the test is unaffected by adding highly or even perfectly correlated variables in @xmath258 to the group .",
    "the compatibility condition would fail to yield a nontrivial bound , but the group effect compatibility condition is unaffected by the addition of perfectly correlated variables to a group .    the price to pay for the assumption - free nature of the bound is a weaker power than with previously discussed approaches when the goal is to detect the effect of individual variables .",
    "however , for groups of highly correlated variables , the approach can be much more powerful than simply testing all variables in the group .",
    "we remark that previously developed tests can be adapted to the context of hierarchical testing of groups with hierarchical adjustment for familywise error control ( @xcite ) ; for the multi sample - splitting method , this is described next .",
    "the multi sample - splitting method ( section  [ subsec.multisample-split ] ) can be adapted to the context of hierarchical testing of groups by using hierarchical adjustment of familywise error control ( @xcite ) .",
    "when testing a cluster hypotheses @xmath254 , one can use a modified form of the partial @xmath259-test for high - dimensional settings ; and the multiple testing adjustment due to the multiple cluster hypotheses considered can be taken care of by a hierarchical adjustment scheme proposed in @xcite . a detailed description of the method , denoted here by _ hier .",
    "ms - split _ , together with theoretical guarantees is given in @xcite .",
    "simultaneous inference for all possible groups can be achieved by considering @xmath0-values @xmath260 of individual hypotheses @xmath168 ( @xmath55 ) and adjusting them for simultaneous coverage , namely , @xmath261 .",
    "the individual @xmath0-values @xmath260 can be obtained by the ridge or desparsified lasso method in section  [ sec.lm ] .",
    "we can then test any group hypothesis @xmath262 for all @xmath105 by simply looking whether @xmath263 , and we can consider as many group hypotheses as we want without any further multiple testing adjustment .",
    "a semi - real data example is shown in figure  [ fig : treeridge ] , where the predictor variables are taken from the riboflavin data set ( @xcite ) ( @xmath264 ) and the coefficient vector is taken to have entries 0 , except for 2 clusters of highly correlated variables .",
    "in example 1 , the clusters both have size 3 with nonzero coefficient sizes equal to 1 for all the variables in the clusters and gaussian noise level @xmath265 .",
    "in example 2 , the clusters are bigger and have different sizes 11 and 21 ; the coefficient sizes for all the variables in the clusters is again 1 , but the gaussian noise level here is chosen to be @xmath266 .    in the first example , 6 out of the 6 relevant variables are discovered as individually significant by the _ lasso - pro _ , _ ridge _ and _ ms - split _ methods ( as outlined in sections  [ subsec.multisample-split][subsec.desparslasso ] ) , after adjusting for multiplicity .    in the second example",
    ", the methods can not reject the single variables individually any longer .",
    "the results for the _ group - bound _",
    "estimator are shown in the right column .",
    "the _ group - bound _ can reject a group of 4 and 31 variables in the first example , each containing a true cluster of 3 variables .",
    "the method can also detect a group of 2 variables ( a subset of the cluster of  4 ) which contains 2 out of the 3 highly correlated variables . in the second example , a group of 34 variables is rejected with the _ group - bound _",
    "estimator , containing 16 of the group of 21 important variables .",
    "the smallest group of variables containing the cluster of 21 that the method can detect is of size 360 .",
    "it can thus be detected that the variables jointly have a substantial effect even though the null hypothesis can not be rejected for any variable individually .",
    "the hierarchical multi sample - splitting method ( outlined in section  [ subsec.mssplitgroup ] ) manages to detect the same clusters as the _ group - bound _ method .",
    "it even goes one step further by detecting a smaller subcluster .",
    "variables ( bottom ) .",
    "the design matrix used is of type _",
    "block equi.corr_ which is similar to the equi.corr setup in that @xmath125 is block diagonal with blocks ( of size @xmath267 ) being the  @xmath125 of equi.corr .",
    "the power is plotted as a function of the correlations in the blocks , quantified by  @xmath268 .",
    "the ridge - based method loses power as the correlation between variables increases , while the group bound , hier .",
    "ms - split and lasso - pro methods can maintain power close to 1 for both measures of power . ]",
    "[ fig : testgrouppower ]     variables ( top ) and type i error rate corresponding to the rejection of the group - hypothesis of all @xmath269 variables ( bottom ) for the design matrix of type _",
    "block equi.corr_ when changing the correlation @xmath268 between variables .",
    "the design matrix type is described in detail in the caption of figure [ fig : testgrouppower ] and in the text .",
    "the desparsified lasso , hier .",
    "ms - split and the ridge - based method lose power as the correlation between variables increases , while the _ group - bound _ can not reject the small group of variables @xmath270 ( 3  in this case ) .",
    "the desparsified lasso and ms - split methods also exceed the nominal type i error rate for high correlations ( as the design assumptions break down ) , whereas the ridge - based method and the _ group - bound _ are both within the nominal 5% error rate for every correlation strength . ]",
    "[ fig : testgroup ]    we also consider the following simulation model .",
    "the type of design matrix was chosen to be such that the population covariance matrix @xmath125 is a block - diagonal matrix with blocks of dimension @xmath267 being of the same type as @xmath125 for equi.corr ( see section  [ subsubsec.data ] ) with off - diagonal @xmath268 instead of @xmath271 .",
    "the dimensions of the problem were chosen to be @xmath209 number of variables , @xmath208 number of samples and noise level @xmath272 .",
    "there were only 3 nonzero coefficients chosen with three different signal levels @xmath273 $ ] , @xmath274 $ ] and @xmath275 $ ] being used for the simulations . aside from varying signal level , we studied the two cases where in one case all the nonzero coefficients were contained in one single highly correlated block and in the other case each of those variables was in a different block .",
    "we look at 3 different measures of power .",
    "one can define the power as the fraction of the 100 repeated simulations that the method managed to reject the group of all variables @xmath276 .",
    "this is shown at the top in figure  [ fig : testgrouppower ] .",
    "alternatively , one can look at the rejection rate of the hypothesis for the group @xmath277 that contains all variables in the highly correlated blocks that contain a variable from @xmath270 .",
    "this is the plot at the bottom in figure  [ fig : testgrouppower ] .",
    "finally , one can look at the rejection rate of the hypothesis where the group @xmath277 contains only the variables in @xmath270 ( of size 3 in this case ) .",
    "the type i error we define to be the fraction of the simulations in which the method rejected the group hypothesis @xmath278 where all regression coefficients are equal to zero .",
    "these last two measures are presented in figure  [ fig : testgroup ] .",
    "the power of the ridge - based method ( @xcite ) drops substantially for high correlations .",
    "the power of the _ group - bound _ stays close to 1 at the level of the highly correlated groups ( block - power ) and above ( power @xmath279 ) throughout the entire range of correlation values .",
    "the _ lasso - pro _ and _ ms - split _ perform well here as well .",
    "the power of the _ group - bound _ is 0 when attempting to reject the small groups @xmath280 .",
    "the type i error rate is supposedly controlled at level @xmath281 with all three methods .",
    "however , the _ lasso - pro _ and the hierarchical _ ms - split _ methods fail to control the error rates , with the type i error rate even approaching 1 for large values of the correlation . the _ group - bound _ and ridge - based estimator have , in contrast , a type i error rate close to 0 for all values of the correlation .    for highly correlated groups of variables , trying to detect the effect of individual variables",
    "has thus two inherent dangers .",
    "the power to detect interesting groups of variables might be very low . and the assumptions for the methods might be violated , which invalidates the type",
    "i error control .",
    "the assumption - free _ group - bound _",
    "method provides a powerful test for the group effects even if variables are perfectly correlated , but suffers in power , relatively speaking , when variables are not highly correlated .",
    "an implementation of the _ group - bound _ method is provided in the ` hdi ` -package ( @xcite ) .    for specific groups",
    ", one can provide a vector or a list of vectors where the elements of the vector specify the desired columns of @xmath3 to be tested for .",
    "the following code tests the group hypothesis if the group contains all variables :    .... > group    < - 1:ncol(x ) > outgroupbound    < - groupbound(x = x , y = y ,       group = group , alpha = 0.05 ) > rejection    < - outgroupbound > 0 ....    note that one needs to specify the significance level  @xmath282 .",
    "one can also let the method itself apply the hierarchical clustering scheme as described at the beginning of section  [ sect.hierinf ] .",
    "this works as follows :    .... > outclustergroupbound    < - clustergroupbound(x =",
    "x ,       y = y , alpha = 0.05 ) ....    the output contains all clusters that were tested for significance in ` members ` .",
    "the corresponding lower bounds are contained in ` lowerbound ` .",
    "to extract the significant clusters , one can do    ....",
    "> significant.cluster.numbers    < - which       ( outclustergroupbound       $ lowerbound > 0 ) > significant.clusters    < - outclustergroupbound$members       [ [ significant.cluster.numbers ] ] ....    the figures in the style of figure  [ fig : treeridge ] can be achieved by using the function ` plot ` on ` outcluster - groupbound ` .    note that one can specify the distance matrix used for the hierarchical clustering , as done for ` hclust ` .    to test group hypotheses @xmath173 for the ridge and desparsified lasso method as described in section  [ subsec.simulcovridgelasso ]",
    ", one uses the output from the original single parameter fit , as illustrated for the group of all variables :    .... > outridge    < - ridge.proj(x = x , y = y ) > outlasso    < - lasso.proj(x = x , y = y ) > group",
    "< - 1:ncol(x ) > outridge$grouptest(group ) > outlasso$grouptest(group ) ....    to apply a hierarchical clustering scheme as done in `",
    "clustergroupbound ` , one calls ` cluster - grouptest ` :    .... > outridge$clustergrouptest    ( alpha = 0.95 ) ....    to summarize , the -package provides functions to test individual groups as well as to test according to a hierarchical clustering scheme for the methods _ group - bound _ , ridge and desparsified lasso .",
    "an implementation of the hierarchical multi sample - splitting method is not provided at this point in time .",
    "stability selection ( @xcite ) is another methodology to guard against false positive selections , by controlling the expected number of false positives @xmath283 $ ] .",
    "the focus is on selection of a single or a group of variables in a regression model , or on a selection of more general discrete structures such as graphs or clusters .",
    "for example , for a linear model in ( [ mod.lin ] ) and with a selection of single variables , stability selection provides a subset of variables @xmath284 such that for @xmath285 we have that @xmath283 \\le m$ ] , where @xmath286 is a prespecified number .    for selection of single variables in a regression model ,",
    "the method does not need a beta - min assumption , but the theoretical analysis of stability selection for controlling @xmath283 $ ] relies on a restrictive exchangeability condition ( which , e.g. , is ensured by a restrictive condition on the design matrix ) .",
    "this exchangeability condition seems far from necessary though ( @xcite ) .",
    "a refinement of stability selection is given in @xcite .",
    "an implementation of the stability selection procedure is available in the ` hdi ` -package . it is called in a very similar way as the other methods .",
    "if we want to control , for example , @xmath283 \\le1 $ ] , we use    .... > outstability    < - stability       ( x = x , y = y , ev = 1 ) ....    the `` stable '' predictors are available in the element ` select ` .    the default model selection algorithm is the lasso ( the first @xmath287 variables entering the lasso paths ) . the option ` model.selector ` allows to apply a user defined model selection function .",
    "we go through a possible ` r ` workflow based on the riboflavin data set ( @xcite ) and methods provided in the ` hdi ` ` r`-package :    .... > library(hdi ) >",
    "data(riboflavin ) ....    we assume a linear model and we would like to investigate which effects are statistically significant on a significance level of @xmath281 .",
    "moreover , we want to construct the corresponding confidence intervals .",
    "we start by looking at the individual variables .",
    "we want a conservative approach and , based on the results from section  [ subsec.comparlm ] , we choose the ridge projection method for its good error control :    .... > outridge    < - ridge.proj       ( x = riboflavin$x ,        y = riboflavin$y ) ....    we investigate if any of the multiple testing corrected @xmath0-values are smaller than our chosen significance level :    .... >",
    "any(outridge$pval.corr < = 0.05 ) [ 1 ] false ....    we calculate the 95% confidence intervals for the first 3 predictors :    .... > confint(outridge , parm=1:3 ,    level=0.95 ) lower upper aadk_at -0.8848403 1.541988 aapa_at -1.4107374 1.228205 abfa_at -1.3942909 1.408472 ....    disappointed with the lack of significance for testing individual variables , we want to investigate if we can find a significant group instead . from the procedure proposed for the ridge method in section  [ sect.hierinf ] , we know that if the ridge method can not find any significant individual variables , it would not find a significant group either .",
    "we apply the group - bound method with its clustering option to try to find a significant group :    .... > outclustergroupbound    < - clustergroupbound       ( x = riboflavin$x ,       y = riboflavin$y ,       alpha = 0.05 ) > significant.cluster.numbers",
    "< - which(outclustergroupbound       $ lowerbound       > 0 ) > significant.clusters    < - outclustergroupbound       $ members       [ [ significant.cluster.numbers ] ] > str(significant.clusters ) num [ 1:4088 ] 1 2 3 4 5 6 7 8 9 10 ... ....    only a single group , being the root node of the clustering tree , is found significant .",
    "these results are in line with the results achievable in earlier studies of the same data set in @xcite and @xcite .",
    "we present a ( selective ) overview of recent developments in frequentist high - dimensional inference for constructing confidence intervals and assigning values for the parameters in linear and generalized linear models .",
    "we include some methods which are able to detect significant groups of highly correlated variables which can not be individually detected as single variables .",
    "we complement the methodology and theory viewpoints with a broad empirical study .",
    "the latter indicates that more `` stable '' procedures based on ridge estimation or sample splitting with subsequent aggregation might be more reliable for type i error control , at the price of losing power ; asymptotically , power - optimal methods perform nicely in well - posed scenarios but are more exposed to fail for error control in more difficult settings where the design or degree of sparsity are more ill - posed .",
    "we introduce the ` r`-package ` hdi ` which allows the user to choose from a collection of frequentist inference methods and eases reproducible research .",
    "since the main assumptions outlined in section  [ subsec.mainass ] might be unrealistic in practice , one can consider a different route .",
    "the view and `` posi '' ( post - selection inference ) method by @xcite makes inferential statements which are protected against all possible submodels and , therefore , the procedure is not exposed to the issue of having selected an `` inappropriate '' submodel .",
    "the way in which @xcite deal with misspecification of the ( e.g. , linear ) model is closely related to addressing this issue with the multi sample splitting or desparsified lasso method ; see section  [ subsec.mainass ] and @xcite .",
    "the method by @xcite is conservative , as it protects against any possible submodel , and it is not feasible yet for high - dimensional problems .",
    "@xcite briefly describes the `` harness '' ( high - dimensional agnostic regression not employing structure or sparsity ) procedure : it is based on single data splitting and making inference for the selected submodel from the first half of the data . when giving up on the goal to infer the true or best approximating parameter @xmath12 in ( [ betaproj ] ) , one can drop many of the main assumptions which are needed for high - dimensional inference .",
    "the `` harness '' is related to post - selection inference where the inefficiency of sample splitting is avoided .",
    "some recent work includes exact post - selection inference , where the full data is used for selection and inference : it aims to avoid the potential inefficiency of single sample splitting and to be less conservative than `` posi '' , thereby restricting the focus to a class of selection procedures which are determined by affine inequalities , including the lasso and least angle regression ( @xcite ; @xcite ; @xcite ) .    under some conditions , the issue of selective inference can be addressed by using an adjustment factor ( @xcite ) : this could be done by adjusting the output of our high - dimensional inference procedures , for example , from the ` hdi ` ` r`-package .",
    "_ compatibility condition _ ( @xcite , page106 ) .",
    "consider a fixed design matrix @xmath3 .",
    "we define the following :    the compatibility condition holds if for some @xmath288 and all @xmath289 satisfying @xmath290 , @xmath291 here @xmath292 denotes the components @xmath293 where @xmath294 .",
    "the number @xmath295 is called the compatibility constant .    _ aggregation of dependent @xmath0-values .",
    "_ aggregation of dependent @xmath0-values can be generically done as follows .",
    "assume that we have @xmath70 @xmath0-values @xmath296 for testing a null - hypothesis @xmath297 , that is , for every @xmath298 and any @xmath90 , @xmath299 \\le \\alpha$ ] .",
    "consider for any @xmath300 the empirical @xmath74-quantile @xmath301 and the minimum value of @xmath302 , suitably corrected with a factor , over the range @xmath78 for some positive ( small ) @xmath303 : @xmath304 then , both @xmath302 [ for any fixed @xmath305 and @xmath306 are conservative @xmath0-values satisfying for any @xmath90 , @xmath307 \\le \\alpha$ ] or @xmath308 \\le\\alpha$ ] , respectively",
    ".    _ bounding the error of the estimated bias correction in the desparsified lasso . _ we will argue now why the error from the bias correction @xmath309 is negligible . from the kkt conditions when using the lasso of @xmath109 versus @xmath110 , we have ( bhlmannand van  de geer , @xcite , cf .",
    "lemma  2.1 ) @xmath310 therefore , @xmath311 assuming sparsity and the compatibility condition ( [ compat ] ) , and when choosing @xmath312 , one can show that @xmath313 and @xmath314 [ for the latter , see ( [ lasso - ell1 ] ) ] .",
    "therefore , @xmath315 where the last bound follows by assuming @xmath316 .",
    "thus , if @xmath317 , the error from bias correction is asymptotically negligible .",
    "_ choice of @xmath114 for desparsified lasso .",
    "_ we see from ( [ kkt ] ) that the numerator of the error in the bias correction term ( i.e. , the @xmath318 s ) is decreasing as @xmath319 ; for controlling the denominator , @xmath114 should not be too small to ensure that the denominator [ i.e. , @xmath320 behaves reasonable ( staying away from zero ) for a fairly large range of @xmath114 .",
    "therefore , the strategy is as follows :    compute a lasso regression of @xmath109 versus all other variables @xmath110 using cv , and the corresponding residual vector is denoted by @xmath111 .",
    "compute @xmath321 which is the asymptotic variance of @xmath322 , assuming that the error in the bias correction is negligible .    increase the variance by 25% ,",
    "that is , @xmath323 .",
    "search for the smallest @xmath114 such that the corresponding residual vector @xmath324 satisfies @xmath325    this procedure is similar to the choice of @xmath114 advocated in @xcite .",
    "_ bounding the error of bias correction for the ridge projection . _",
    "the goal is to derive the formula ( [ ridge - repr ] ) .",
    "based on ( [ ridge - distr ] ) , we have @xmath326    in relation to the result in fact  [ th2 ] for the desparsified lasso , the problem here is that the behaviors of @xmath327 and of the diagonal elements @xmath328 are hard to control , but , fortunately , these quantities are fixed and observed for fixed design @xmath3 .    by invoking the compatibility constant for the design  @xmath3 , we obtain the bound for @xmath329 in ( [ lasso - ell1 ] ) and , therefore , we can upper - bound @xmath330 asymptotically , for gaussian errors , we have with high probability @xmath331 \\\\[-8pt ] \\nonumber & \\le & o\\biggl(\\bigl(\\log(p)/n\\bigr)^{1/2 - \\xi}\\max_{k \\neq j } \\biggl{\\vert}\\frac{p_{r;jk}}{p_{r;jj}}\\biggr{\\vert}\\biggr),\\end{aligned}\\ ] ] where the last inequality holds due to assuming @xmath332 for some @xmath333 . in practice , we use the bound from ( [ delta - bound ] ) in the form @xmath334 with the typical choice @xmath163 .      we construct confidence intervals that satisfy the duality with the @xmath0-values from equation ( [ aggreg ] ) , and , thus , they are corrected already for multiplicity : @xmath335}\\bigr)/\\gamma\\bigr)\\geq\\\\ & & \\qquad \\alpha/(1-\\log{\\gamma_{\\mathrm{min}}})\\bigr\\ } , \\\\ & & \\quad= \\bigl\\{c | \\forall\\gamma\\in(\\gamma_{\\mathrm{min}},1):\\\\ & & \\qquad \\mathrm{emp.}\\   \\gamma\\ \\mathrm{quantile } \\bigl(p_{\\mathrm{corr};j}^{[b]}\\bigr)/\\gamma\\geq\\\\ & & \\qquad\\alpha/(1-\\log { \\gamma_{\\mathrm{min}}})\\bigr\\ } , \\\\ & & \\quad = \\biggl\\{c | \\forall\\gamma\\in(\\gamma_{\\mathrm{min}},1):\\\\ & & \\qquad \\mathrm{emp.}\\ \\gamma\\ \\mathrm{quantile } \\bigl(p_{\\mathrm{corr};j}^{[b]}\\bigr ) \\geq\\frac{\\alpha \\gamma}{(1-\\log{\\gamma_{\\mathrm{min}}})}\\biggr \\}.\\end{aligned}\\ ] ]    we will use the notation @xmath336}$ ] for the position of @xmath337}$ ] in the ordering by increasing the value of the corrected @xmath0-values @xmath338}$ ] , divided by @xmath70",
    ".    we can now rewrite our former expression in a form explicitly using our information from every sample split @xmath339 } \\leq \\gamma_{\\mathrm{min}}\\bigr)\\\\ & & \\qquad{}\\lor\\biggl(p_{\\mathrm{corr};j}^{[b ] } \\geq \\frac{\\alpha\\gamma^{[b]}}{(1-\\log{\\gamma_{\\mathrm{min}}})}\\biggr ) \\biggr\\ } \\\\ & & \\quad= \\biggl\\{c | \\forall b = 1,\\ldots , b : \\bigl(\\gamma^{[b ] } \\leq\\gamma_{\\mathrm{min}}\\bigr)\\\\ & & \\qquad{}\\lor \\biggl(c \\in\\mbox { the } \\biggl(1-\\frac{\\alpha\\gamma^{[b]}}{(1-\\log{\\gamma_{\\mathrm{min}}})|\\hat { s}^{[b]}| } \\biggr)\\\\ & & \\qquad{}\\cdot 100\\% \\mbox { ci for split $ b$}\\biggr ) \\biggr\\}. \\ ] ]    for single testing ( not adjusted for multiplicity ) , the corresponding confidence interval becomes @xmath340 } \\leq \\gamma_{\\mathrm{min}}\\bigr)\\\\ & & \\qquad{}\\lor\\biggl(c \\in\\mbox { the } \\biggl(1- \\frac{\\alpha\\gamma^{[b]}}{(1-\\log{\\gamma_{\\mathrm{min } } } ) } \\biggr)\\\\ & & \\qquad{}\\cdot 100\\% \\mbox { ci for split $ b$}\\biggr ) \\biggr\\}.\\end{aligned}\\ ] ]    if one has starting points with one being in the confidence interval and the other one outside of it , one can apply the bisection method to find the bound in between these points .",
    "we describe the approach presented in section  [ subsec.glmweighted ] in a more general way .",
    "one algorithm for fitting generalized linear models is to calculate the maximum likelihood estimates @xmath118 by applying iterative weighted least squares ( @xcite ) .    as in section",
    "[ subsec.glmweighted ] , the idea is now to apply a standard l1-penalized fitting of the model , then build up the weighted least squares problem at the l1-solution and apply our linear model methods on this problem .    from @xcite , using the notation @xmath341 , the adjusted response variable becomes @xmath342    we then get a weighted least squares problem @xmath233 with weights @xmath343 with variance function @xmath344 .",
    "the variance function @xmath344 is related to the variance of the response @xmath5 . to more clearly define this relation",
    ", we assume that the response @xmath5 has a distribution of the form described in @xcite : @xmath345},\\ ] ] with known functions @xmath346 , @xmath347 and @xmath348 .",
    "@xmath349 is the canonical parameter and @xmath350 is the dispersion parameter .    as defined in @xcite , the variance function",
    "is then related to the variance of the response in the following way : @xmath351    we rewrite @xmath235 and @xmath236 to get @xmath237    the linear model methods can now be applied to @xmath238 and @xmath239 , thereby the estimate @xmath132 has to be set to the value 1 .",
    "we would like to thank some reviewers for insightful and constructive comments ."
  ],
  "abstract_text": [
    "<S> we present a ( selective ) review of recent frequentist high - dimensional inference methods for constructing @xmath0-values and confidence intervals in linear and generalized linear models . </S>",
    "<S> we include a broad , comparative empirical study which complements the viewpoint from statistical methodology and theory . </S>",
    "<S> furthermore , we introduce and illustrate the ` r`-package ` hdi ` which easily allows the use of different methods and supports reproducibility .    ./style / arxiv - general.cfg    ,    , </S>"
  ]
}