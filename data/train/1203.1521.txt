{
  "article_text": [
    "compressive sensing , or compressive sampling ( cs ) @xcite , is a novel signal processing technique proposed to effectively sample and compress sparse signals , i.e. , signals that can be represented by few significant coefficients in some basis .",
    "assume that the signal of interest @xmath0 can be represented by @xmath1 , where @xmath2 is the basis matrix and @xmath3 is @xmath4-sparse , which means only @xmath4 out of its @xmath5 entries are nonzero .",
    "one of the essential issues of cs theory lies in recovering @xmath6 ( or equivalently , @xmath7 ) from its linear observations , @xmath8 where @xmath9 is a sensing matrix with more columns than rows and @xmath10 is the measurement vector .",
    "unfortunately , directly finding the sparsest solution to ( [ y = ax ] ) is np - hard , which is not practical for sparse recovery .",
    "this leads to one of the major aspects of cs theory  designing effective recovery algorithms with low computational complexity and fine recovery performance .",
    "a family of convex relaxation algorithms for sparse recovery @xcite had been introduced before the theory of cs was established .",
    "based on linear programming ( lp ) techniques , it is shown that @xmath11 norm optimization , also known as basis pursuit ( bp ) , @xmath12 yields the sparse solution as long as @xmath13 satisfies the restricted isometry property ( rip ) with a constant parameter @xcite .",
    "recovery algorithms based on convex optimization include interior - point methods @xcite and homotopy methods @xcite .",
    "in contrast with convex relaxation algorithms , non - convex optimization algorithms solve ( [ y = ax ] ) by minimizing @xmath14 norm with respect to @xmath15 , which is not convex .",
    "typical algorithms include focal underdetermined system solver ( focuss ) @xcite , iteratively reweighted least squares ( irls ) @xcite , smoothed @xmath16 ( sl0 ) @xcite , and zero - point attracting projection ( zap ) @xcite . compared with convex relaxation algorithms , theoretical analysis based on rip shows that fewer measurements are required for exact recovery by non - convex optimization methods @xcite .",
    "a family of iterative greedy algorithms has received much attention due to their simple implementation and low computational complexity .",
    "the basic idea underlying these algorithms is to iteratively estimate the support set of the unknown sparse signal , i.e. , the set of locations of its nonzero entries . in each iteration",
    ", one or more indices are added to the support estimation by correlating the columns of @xmath17 with the regularized measurement vector .",
    "typical examples include orthogonal matching pursuit ( omp ) @xcite , regularized omp ( romp ) @xcite , and stage - wise omp ( stomp ) @xcite .",
    "compared with convex relaxation algorithms , greedy pursuits need more measurements , but they tend to be more computationally efficient .    recently , several greedy algorithms including compressive sampling matching pursuit ( cosamp ) @xcite and subspace pursuit ( sp ) @xcite have been proposed by incorporating the idea of backtracking . in each iteration",
    ", sp algorithm refines the @xmath4 columns of matrix @xmath17 that span the subspace where the measurement vector @xmath18 lies .",
    "specifically , sp adds @xmath4 more indices to the @xmath4 candidates of support estimate , and discards the most unreliable @xmath4 ones .",
    "similarly , cosamp adds @xmath19 more indices in each iteration , while computes the regularized measurement vector in a different way . by evaluating the reliability of all candidates in each iteration",
    ", these algorithms can provide comparable performance to convex relaxation algorithms , and exhibit low computational complexity as matching pursuit algorithms .",
    "another kind of greedy pursuits , including iterative hard thresholding ( iht ) @xcite and its normalized variation @xcite , is proposed with the advantages of low computational complexity and theoretical performance guarantee . in each iteration ,",
    "the entries of the iterative solution except for the most @xmath4 reliable ones are set to zero .",
    "together with cosamp and sp , these algorithms can be considered as greedy pursuits with replacement involved in each iteration .",
    "to apply the theory of cs in practice , the effect of noise and perturbation must be taken into consideration .",
    "the common analysis is the additive noise to the measurements , i.e. , @xmath20 where @xmath21 is termed measurement noise .",
    "most existing algorithms have been proved to be stable in this scenario , including theoretical analysis of bp @xcite , romp @xcite , cosamp @xcite , sp @xcite , and iht @xcite .",
    "it is shown that the error bounds of the recovered solutions are proportional to the @xmath22 norm of @xmath21 .",
    "a certain distribution of the measurement noise can be introduced to achieve better results , such as gaussian @xcite or others @xcite .",
    "until recently , only a few researches involve the perturbation to the sensing matrix @xmath23 , which is also termed system perturbation .",
    "existing works include analysis of bp @xcite , cosamp @xcite , sp @xcite , @xmath14 norm minimization with @xmath24 $ ] @xcite , and omp @xcite . in these works , ( [ meapur ] )",
    "is extended by introducing a perturbed sensing matrix , i.e. , @xmath25 .",
    "it is of great significance to analyze the stability of recovery algorithms against both perturbations when the theory of cs is applied in practice .",
    "other related works include mismatch of sparsity basis @xcite and sparsity - cognizant total least - squares @xcite .",
    "two practical scenarios are usually considered in cs applications .",
    "first , when @xmath23 represents a system model , @xmath26 denotes the precision error when the system is physically implemented .",
    "thus the whole sensing process is @xmath27 and only the nominal sensing matrix and contaminated measurement vector are available for recovery , i.e. , @xmath28 where @xmath29 denotes a certain recovery algorithm .    in countless other problems ,",
    "@xmath26 is involved due to mis - modeling of the actual system @xmath23 .",
    "thus @xmath25 and the sensing process is @xmath30 both the sensing matrix and measurement vector are contaminated , and the recovered solution is @xmath31 .",
    "existing works @xcite are all based on the latter scenario , thus it is well considered and fully analyzed in this paper . the first scenario ( [ sysper1 ] ) is briefly discussed after that as a remark .",
    "this paper mainly considers the recovery performance of greedy pursuits with replacement against general perturbations .",
    "specifically , when both measurement noise and system perturbation exist , the error bounds of the solutions of cosamp , sp , and iht are derived and discussed in detail for not strictly sparse signals , i.e. , compressible signals .",
    "it is shown that these relative error bounds are linear in the relative perturbations of the measurement vector and the sensing matrix , which indicates that the recovery performance is stable against general perturbations .",
    "these error bounds are also compared with that of oracle recovery , and it reveals that the results in this paper are optimal up to the coefficients .",
    "numerical simulations are performed to verify the conclusions in this paper .",
    "previous related works including @xcite are compared with this work in section vi .",
    "the remainder of this paper is organized as follows .",
    "section ii gives a brief review of rip and three greedy pursuits with replacement .",
    "section iii presents the main theorems about the recovery performance of greedy pursuits with replacement against general perturbations , and the results are shown to be of the same order as oracle recovery by comparing them in section iv .",
    "numerical simulations are performed in section v. several related works are discussed in section vi , and the paper is concluded in section vii . some detailed descriptions and discussions of greedy pursuits with replacement and proofs are postponed to appendix .",
    "in this section , the definition of rip and descriptions of several greedy pursuits , including cosamp , sp , and iht , as well as their recovery performance against measurement noise , are introduced .",
    "the restricted isometry property ( rip ) for any matrix @xmath17 describes the degree of orthogonality among its different columns .",
    "@xcite for positive integer @xmath4 , define the restricted isometry constant ( ric ) @xmath32 of a matrix @xmath17 as the smallest non - negative number such that @xmath33 holds for any @xmath4-sparse vector @xmath7 .",
    "according to definition  1 , @xmath34 implies that every @xmath4 columns of @xmath17 are linearly independent , and @xmath35 implies that @xmath17 almost maintains the @xmath22 distance between any pair of @xmath4-sparse signals .",
    "it is also easy to check that if @xmath17 satisfies the rip with @xmath36 and @xmath37 , and @xmath38 , then @xmath39 .",
    "calculating the exact value of ric is intractable because it involves all submatrices comprised of @xmath4 columns of @xmath17 .",
    "fortunately , random matrices possess small rics for overwhelming probability .",
    "for example , if the entries of @xmath40 are independently and identically distributed gaussian random variables with zero mean and variance @xmath41 , and @xmath42 satisfies @xmath43 then @xmath44 with at least probability @xmath45 , where @xmath46 and @xmath47 are two constants @xcite .",
    "recent theoretical results about the bounds of ric can be found in @xcite .    though rip",
    "is usually used to show the impact of matrix on sparse signals , the following lemma permits us to generalize the result from sparse signals to general signals .",
    "[ ripgeneral ] ( proposition  3.5 in @xcite ) suppose that the matrix @xmath17 satisfies rip of level @xmath4 with @xmath48 , then for any signal @xmath49 , it holds that @xmath50    lemma  [ ripgeneral ] is used in the theoretical analysis of this paper for compressible signals .",
    "greedy pursuits with replacement include cosamp , sp , and iht algorithms , and they are briefly introduced as follows .",
    "the pseudo codes for these algorithms and some discussions are postponed to appendix  a.    the cosamp algorithm iteratively refines the support of @xmath4-sparse vector @xmath7 . in each iteration , @xmath19 more indices are selected by correlating @xmath17 with the residue vector @xmath51 , then the best @xmath4 candidates out of at most @xmath52 ones are kept and the residue vector is updated .",
    "the details of cosamp can be found in @xcite .    the sp algorithm is firstly proposed in @xcite and further developed in @xcite . unlike cosamp , sp adds @xmath4 more indices in each iteration , and keeps the best @xmath4 candidates .",
    "in addition , the residue vector of sp is orthogonal to the subspace spanned by the columns of @xmath17 indexed by the @xmath4 candidates , making the new @xmath4 indices added in each iteration totally different from the previously identified @xmath4 ones",
    ". please refer to @xcite for more details .",
    "the iht algorithm is firstly proposed in @xcite , and later developed and analyzed in @xcite . to improve the convergence performance of the method , a normalized variation niht is proposed in @xcite which retains theoretical performance guarantee similar to iht . without loss of generality ,",
    "only iht algorithm is discussed in the following analysis .",
    "the following theorem  [ gpra ] reveals the error bounds of the solutions of greedy pursuits with replacement when only measurement noise exists . for two sets @xmath53 and @xmath54",
    ", @xmath55 denotes the set comprised of all elements @xmath56 and @xmath57 .",
    "@xmath58 denotes the subvector composed of entries of @xmath7 indexed by set @xmath59 .",
    "[ gpra ] given a noisy measurement vector @xmath60 where @xmath3 is @xmath4-sparse , the estimated solution @xmath61}$ ] in the @xmath62-th iteration of cosamp and iht algorithms satisfies @xmath63}\\big\\|_2\\leq c\\big\\|{\\bf s}-{\\bf s}^{[l-1]}\\big\\|_2 + c_1\\left\\|{\\bf e}\\right\\|_2,\\end{aligned}\\ ] ] and the estimated support set @xmath64 in the @xmath62-th iteration of sp algorithm satisfies @xmath65 where @xmath66 denotes the support of @xmath7 .    furthermore , if the matrix @xmath17 satisfies @xmath67 , then @xmath68 , and it can be derived that @xmath69}\\big\\|_2\\le ac^l\\left\\|{\\bf s}\\right\\|_2 + d\\left\\|{\\bf e}\\right\\|_2\\end{aligned}\\ ] ] holds for greedy pursuits with replacement .",
    "the specific values of the constants @xmath70 , @xmath71 , @xmath47 , @xmath46 , @xmath72 , and @xmath73 are illustrated in table  [ tableconstant1 ] .",
    "[ tableconstant1 ]    .the specification of the constants [ cols=\"^,^,^,^\",options=\"header \" , ]     based on theorem  [ gpra ] , two remarks are derived as follows .",
    "* remark 4 * in the noiseless scenario , after finite iterations , the recovered solutions of cosamp and sp are guaranteed to be identical to the sparse signal @xmath3 .",
    "the result can be verified through the following statement .",
    "suppose @xmath74 is the smallest magnitude of the nonzero entries of @xmath3",
    ". then after @xmath75 iterations , the recovered solution @xmath61}$ ] obeys @xmath76}\\big\\|_2 < s_{\\min}.\\end{aligned}\\ ] ] if the support of @xmath7 is perfectly recovered , then for cosamp and sp , the solution is already identical to @xmath7 .",
    "otherwise , at least one nonzero entry is not detected , thus the recovery error is no less than @xmath74 , which contradicts ( [ remarkequ1 ] ) .",
    "notice that the solution of iht does not possess the above property , since exact support recovery for iht does not imply exact signal recovery .",
    "* remark 5 * according to ( [ gprerror2 ] ) , after @xmath77 iterations , the error bound of the recovered solution satisfies @xmath78}\\big\\|_2\\le ( d+1)\\left\\|{\\bf e}\\right\\|_2,\\end{aligned}\\ ] ] which means that the error bound is proportional to the @xmath22 norm of the noise , and the recovery performance of greedy pursuits with replacement is stable in this scenario .",
    "for cosamp algorithm , the inequality ( [ gprerror1 ] ) can be obtained by following the steps of the proof of theorem  4.1 in @xcite , while preserving rics during the derivation . as for the second part of the theorem , it is easy to derive that if @xmath79 , then @xmath68 . by recursion",
    ", it can be proved from ( [ gprerror1 ] ) that @xmath78}\\big\\|_2\\le c^l\\big\\|{\\bf s}-{\\bf s}^{[0]}\\big\\|_2 + \\frac{c_1}{1-c}\\left\\|{\\bf e}\\right\\|_2,\\end{aligned}\\ ] ] which arrives ( [ gprerror2 ] ) .        [ splemma1 ] ( lemma  3 in @xcite ) let @xmath49 be a @xmath4-sparse vector , and @xmath80 is a noisy measurement vector where @xmath81 satisfies the rip with parameter @xmath82 . for an arbitrary set @xmath83 such that @xmath84 , define @xmath85 as @xmath86 then @xmath87      lemma  [ splemma2 ] directly implies inequality ( [ sperror1 ] ) .",
    "it s easy to check that if @xmath90 , then @xmath68 , and @xmath91 applying lemma  [ splemma1 ] to ( [ spapp1 ] ) and with the fact that @xmath90 , inequality ( [ gprerror2 ] ) can be derived .",
    "e.  cands , j.  romberg , and t.  tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ ieee trans .",
    "information theory _ ,",
    "52 , no .  2 , pp .  489 - 509 , feb .",
    "2006 .",
    "s.  kim , k.  koh , m.  lusig , s.  boyd , and d.  gorinevsky , `` an interior - point method for large - scale @xmath11-regularized least squares , '' _ ieee journal of selected topics in signal process .",
    "_ , vol .  1 , no .  4 , pp .  606 - 617 , dec . 2007 .",
    "h.  mohimani , m.  babaie - zadeh , and c.  jutten , `` a fast approach for overcomplete sparse decomposition based on smoothed @xmath16 norm , '' _ ieee trans .",
    "signal process .",
    "_ , vol .",
    "57 , no .  1 ,",
    "pp .  289 - 301 , jan .",
    "j.  jin , y.  gu , and s.  mei , `` a stochastic gradient approach on compressive sensing signal reconstruction based on adaptive filtering framework , '' _ ieee journal of selected topics in signal process .",
    "_ , vol .  4 , no .  2 , pp .",
    "409 - 420 , apr . 2010 .",
    "x.  wang , y.  gu , and l.  chen , `` proof of convergence and performance analysis for sparse recovery via zero - point attracting projection , '' _ ieee trans .",
    "signal processing _ , vol .",
    "60 , no .  8 ,",
    "4081 - 4093 , aug .",
    "2012 .",
    "d.  needell and r.  vershynin , `` uniform uncertainty principle and signal recovery via regularized orthogonal matching pursuit , '' _ foundations of computational mathematics _ , vol .  9 , no .  3 , pp .",
    "317 - 334 , june 2009 .",
    "d.  donoho , y.  tsaig , i.  drori , and j.  starck , `` sparse solution of underdetermined systems of linear equations by stagewise orthogonal matching pursuit , '' _ ieee trans .",
    "information theory _ ,",
    "58 , no .  2 , pp .",
    "1094 - 1121 , feb .",
    "2012 .",
    "t.  blumensath and m.  davies , `` normalized iterative hard thresholding : guaranteed stability and performance , '' _ ieee journal of selected topics in signal process . _ , vol .  4 , no .  2 , pp .  298 - 309 , apr .",
    "d.  needell and r.  vershynin , `` signal recovery from incomplete and inaccurate measurements via regularized orthogonal matching pursuit , '' _ ieee journal of selected topics in signal process .",
    "_ , vol .  4 , no .  2 , pp .",
    "310 - 316 , apr . 2010 .",
    "z.  ben - haim , y.  eldar , and m.  elad , `` coherence - based performance guarantees for estimating a sparse vector under random noise , '' _ ieee trans .",
    "signal process .",
    "_ , vol .  58 , no .  10 , pp .  5030 - 5043 , oct",
    ". 2010 .",
    "w.  dai and o.  milenkovic , `` subspace pursuit for compressive sensing : closing the gap between performance and complexity , '' available online : http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.154.8384 ."
  ],
  "abstract_text": [
    "<S> applying the theory of compressive sensing in practice always takes different kinds of perturbations into consideration . in this paper , </S>",
    "<S> the recovery performance of greedy pursuits with replacement for sparse recovery is analyzed when both the measurement vector and the sensing matrix are contaminated with additive perturbations . </S>",
    "<S> specifically , greedy pursuits with replacement include three algorithms , compressive sampling matching pursuit ( cosamp ) , subspace pursuit ( sp ) , and iterative hard thresholding ( iht ) , where the support estimation is evaluated and updated in each iteration . </S>",
    "<S> based on restricted isometry property , a unified form of the error bounds of these recovery algorithms is derived under general perturbations for compressible signals . </S>",
    "<S> the results reveal that the recovery performance is stable against both perturbations . </S>",
    "<S> in addition , these bounds are compared with that of oracle recovery least squares solution with the locations of some largest entries in magnitude known a priori . </S>",
    "<S> the comparison shows that the error bounds of these algorithms only differ in coefficients from the lower bound of oracle recovery for some certain signal and perturbations , as reveals that oracle - order recovery performance of greedy pursuits with replacement is guaranteed . </S>",
    "<S> numerical simulations are performed to verify the conclusions .    </S>",
    "<S> * keywords : * compressive sensing , sparse recovery , general perturbations , performance analysis , restricted isometry property , greedy pursuits , compressive sampling matching pursuit , subspace pursuit , iterative hard thresholding , oracle recovery . </S>"
  ]
}