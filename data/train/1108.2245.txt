{
  "article_text": [
    "recently , walker et . al ( 2010 ) introduced and demonstrated the merits of a non - mcmc approach called direct sampling ( ds ) for conducting bayesian inference .",
    "they argued that with their method there is no need to concern oneself with issues like chain convergence and autocorrelation .",
    "they also point out that their method generates independent samples from a target posterior distribution _ in parallel _ , unlike mcmc for which , in the absence of parallel independent chains , samples are collected sequentially .",
    "walker et al . also prove that the sample acceptance probabilities using ds are better than those from standard rejection algorithms .",
    "put simply , for many common bayesian models , they demonstrate improvement over mcmc in terms of its efficiency , resource demands and ease of implementation .",
    "however , ds suffers from some important shortcomings that limit its broad applicability .",
    "one is the failure to separate the specification of the prior from the specifics of the estimation algorithm .",
    "another is an inability to generate accepted draws for even moderately sized problems ; the largest number of parameters that walker et al .",
    "consider is 10 .",
    "our interest is in conducting full bayesian inference on hierarchical models in high dimensions , with or without conjugacy , sans mcmc .",
    "the method proposed in this paper , strictly speaking , is not a generalization of the ds algorithm , but since it shares some important features with ds , we call it generalized direct sampling ( gds ) .",
    "ds and gds differ in the following respects .    1 .",
    "while ds focuses on the shape of the data likelihood alone , gds is concerned with the characteristics of the entire posterior density .",
    "gds bypasses the need for bernstein polynomial approximations , which are integral to the ds algorithm .",
    "while ds takes proposal draws from the prior ( which may conflict with the data ) , gds samples proposals from a separate density that is ideally a good approximation to the target posterior density itself .",
    "in addition to the above improvements over ds , gds maintains many improvements over mcmc estimation :    1 .",
    "all samples are collected independently , so there is no need to be concerned with autocorrelation , convergence of estimations chains , and so forth .",
    "2 .   there is no particular advantage to choosing model components that maintain conditional conjugacy , as is common with gibbs sampling .",
    "3 .   gds generates samples from the target posterior entirely in parallel , which takes advantage of the most recent advances in grid computing and placing multiple cpu cores in a single computer .",
    "gds permits fast and accurate estimation of marginal likelihoods of the data .",
    "gds is introduced in section [ sec : gds ] .",
    "section [ sec : empirical ] includes examples of gds in action , starting with a small , but important , two - parameter example for which mcmc is known to fail , and concluding with a complex nonconjugate application with over 29,000 parameters . in section [ sec : margll ] , gds is used to estimate marginal likelihoods .",
    "finally , in section [ sec : practical ] , we discuss practical issues that one should consider when implementing gds , including limitations of the approach .",
    "like ds , gds is a variant on well - known importance sampling methods .",
    "the goal is to sample @xmath0 from a posterior density @xmath1 where @xmath2 is the joint density of the data and the parameters ( the unnormalized posterior density ) .",
    "let @xmath3 be the mode of @xmath2 , and define @xmath4 .",
    "choose some proposal distribution @xmath5 that also has its mode at @xmath3 , and define @xmath6 . also , define the function @xmath7 obviously @xmath8 an important restriction on",
    "the choice of @xmath9 is that the inequality @xmath10 must hold , at least for any @xmath11 with a non - negligible posterior density .",
    "discussion of the choice of @xmath5 is given in detail a little later .",
    "next , let @xmath12 be an auxiliary variable that is distributed uniformly on @xmath13 , so that @xmath14 .",
    "we then construct a joint density of @xmath15 and @xmath16 , where @xmath17 \\end{aligned}\\ ] ] from equation [ eq : joint ] , the marginal density of @xmath15 is @xmath18 therefore , simulating from @xmath19 is equivalent to simulating from the target posterior @xmath20 .    using equations [ eq : postdefphi ] and [ eq : joint ] ,",
    "the marginal density of @xmath21 is @xmath22   d\\theta\\\\ & = \\frac{c_1}{c_2{\\mathcal{l}(y)}}\\int \\mathbb{1}\\left [    u<\\phi(\\theta|y)\\right ] g(\\theta)~d\\theta\\\\ & = \\frac{c_1}{c_2{\\mathcal{l}(y)}}q(u)\\label{eq : marg_uy}\\end{aligned}\\ ] ] where @xmath23 g(\\theta)~d\\theta$ ] is defined as the probability that @xmath24 for any @xmath11 drawn from @xmath5 .",
    "the gds sampler comes from recognizing that @xmath25 can written differently from , but equivalently to , equation [ eq : joint ] .",
    "@xmath26 the strategy behind gds is to sample from an approximation to @xmath27 , and then sample from @xmath28 . using the definitions in equations [ eq : phidef ] , [ eq : postdefphi ] , and [ eq : joint ] ,",
    "we get @xmath29~g(\\theta)}{p(u|y)}\\end{aligned}\\ ] ] consequently , to sample _ directly _ from @xmath20 , one needs only to sample from @xmath27 and then sample repeatedly from @xmath9 until @xmath30 .",
    "how does one simulate from @xmath27 , which is proportional to @xmath31 ?",
    "walker et al .",
    "sample from a similar kind of density by first taking @xmath32 proposal draws from the prior to construct an empirical approximation to @xmath31 , and then constructing a continuous approximation using bernstein polynomials .",
    "however , in high - dimensional models , this approximation tends to be a poor one at the endpoints , even with an extremely large number of bernstein polynomial components .",
    "it is for this reason that the largest number of parameters that walker et .",
    "tackle is 10 .",
    "the gds strategy to sample @xmath21 is to sample a transformed variable @xmath33 , where @xmath34 . applying a change of variables , @xmath35 . with @xmath36 denoting the `` true '' cdf of @xmath37 , let @xmath38 be the empirical cdf after taking @xmath32 proposal draws from @xmath5 , and ordering the proposals @xmath39 . to be clear , @xmath38 is the proportion of proposal draws that are _ strictly _ less than @xmath37 . because @xmath38 is discrete , we can sample from a density proportional to @xmath40 by partitioning the domain into @xmath41 segments , partitioned at each @xmath42 .",
    "the probability of sampling a new @xmath37 that falls between @xmath42 and @xmath43 is now @xmath44 $ ] .",
    "therefore , we first sample a @xmath42 from a multinomial density with weights proportional to @xmath45 , and then let @xmath46 , where @xmath47 is a draw from a standard exponential density , truncated on the right at @xmath48 .",
    "one can sample from this truncated exponential density by first sampling a standard uniform random variate @xmath49 , and setting @xmath50 $ ] .",
    "to sample @xmath51 draws from the target posterior , we sample @xmath51 `` threshold '' draws of @xmath37 using this method .",
    "then , for each @xmath37 , we repeatedly sample from @xmath5 until @xmath52 .",
    "note that the inequality sign is flipped from the @xmath53 expression because of the negative sign in the transformation .    in summary ,",
    "the steps of the gds algorithm are as follows :    1 .",
    "find the mode of @xmath2 , @xmath3 and compute the unnormalized log posterior density @xmath4 at that mode .",
    "2 .   choose a distribution @xmath9 so that its mode is also at @xmath3 , and let @xmath6 .",
    "sample @xmath54 independently from @xmath9 .",
    "compute @xmath55 for these proposal draws . if @xmath56 for any of these draws , repeat step 2 and choose another proposal distribution for which @xmath57 does hold .",
    "4 .   compute @xmath58 for the @xmath32 proposal draws , and place them in increasing order .",
    "5 .   evaluate , for each proposal draw , @xmath59\\end{aligned}\\ ] ] which is the empirical cdf of @xmath42 for the @xmath32 proposal draws . then compute @xmath44 $ ] for all @xmath60 .",
    "sample @xmath51 draws of @xmath61 , where a particular @xmath42 is chosen according to the multinomial distribution with probabilities proportional to @xmath45 , and @xmath47 is a standard exponential random variate , truncated to @xmath48 . 7 .",
    "for each of the @xmath51 required samples from the target posterior , sample @xmath11 from @xmath9 until @xmath52 .",
    "consider each first accepted draw to be a single draw from the target posterior @xmath20 .    choosing @xmath5 is an important part of this algorithm .",
    "naturally , the closer @xmath5 is to the target posterior , the more efficient the algorithm will be in terms of acceptance rates . in principle , it is up to the researcher to choose @xmath5 , which is similar in spirit to selecting a dominating density while implementing standard rejection algorithms , or even metropolis - hastings algorithms . for gds , in practice , a multivariate normal proposal distribution with mean at @xmath3 and",
    "covariance matrix of the inverse hessian at @xmath3 , multiplied by a scaling constant @xmath62 , works well .",
    "there is nothing special about this choice , except to note that it is easy to implement with a little trial and error in the selection of @xmath62 .",
    "( this is similar , in spirit , to the concept of tuning an m - h algorithm via trial and error . )",
    "if the log posterior happens to be multimodal , and the location of the local modes are known , then one could let @xmath5 be a mixture of multivariate normals instead .",
    "importantly , we address the sensitivity of the gds algorithm to @xmath32 as part of the analysis in section [ sec : margll ] .    clearly , an advantage of gds is that the samples one collects from the target posterior density are _ independent _ , and that lets us collect them in parallel .",
    "some researchers have investigated alternative approaches for mcmc - based bayesian inference that also take advantage of parallel computation ; see , for example , suchard et al .",
    "one notable example is a parallel implementation of a multivariate slice sampler ( mss ) , as in tibbits et .",
    "the benefits of parallelizing the mss come from parallel evaluation of the target density at each of the vertices of the multivariate slice , and from more efficient use of resources to execute linear algebra operations ( e.g , cholesky decompositions ) .",
    "but the mss itself remains a markovian algorithm , and thus will still generate dependent draws .",
    "using parallel technology to generate a single draw from a distribution is not the same as generating all of the required draws themselves in parallel . on the other hand ,",
    "the sampling steps of gds can be run in their _ entirety _ in parallel .",
    "we now provide some examples of gds in action , especially on problems for which mcmc fails , or for which the dimensionality , model structure , and sample size make mcmc methods somewhat unattractive .",
    "consider this motivating example of a linear hierarchical model discussed by papaspiliopoulous and roberts ( 2008 ) .",
    "@xmath63 for an observed value @xmath64 , @xmath65 is the latent mean for the prior on @xmath64 , @xmath66 is the prior mean of @xmath65 , and @xmath67 and @xmath68 are random error terms , each with mean 0 .",
    "papaspiliopoulous and roberts note that to improve the robustness of inference on @xmath65 to outliers of @xmath64 , it is common to model @xmath67 as having heavier tails than @xmath68 .",
    "let @xmath69 , @xmath70 , and @xmath71 , and suppose there is only one observation available , @xmath72 .",
    "the posterior joint distribution of @xmath65 and @xmath66 is given in figure [ fig : cauchycontours ] ; the contours represent the logs of the computed posterior densities .",
    "note that around the mode , @xmath65 and @xmath66 appear uncorrelated , but in the tails they are highly dependent .",
    "papaspiliopoulous and roberts present this example as a deceptively simple case in which gibbs sampling performs extremely poorly .",
    "indeed , they note that almost all diagnostic tests will erroneously conclude that the chain has converged .",
    "the reason for this failure is that the mcmc chains are attracted to , and get `` stuck '' in , the modal region where the variables are uncorrelated .",
    "once the chain enters the tails , where the variables are more correlated , the chains moves slowly , or not at all .",
    "gds is a more effective alternative for sampling from the posterior distribution .",
    "the posterior mode and hessian of the log posterior at the mode , are @xmath73 and @xmath74 . the gds proposal distribution @xmath5 is taken to be a bivariate normal with mean @xmath3 and covariance @xmath75 , with @xmath76 .",
    "this scaling factor was the smallest value of @xmath62 for which @xmath77 for all @xmath78 of the proposal draws .",
    "two hundred independent samples were collected using the gds algorithm .",
    "figure [ fig : cauchydraws ] plots each of the gds draws , where darker areas represent higher values of log posterior density .",
    "gds not only picks up the correct shape of the regions of high posterior mass near the origin , but also the dependence in the long tails .",
    "the acceptance rate to collect these draws was about 0.013 .",
    "in contrast , consider figure [ fig : cauchymh ] . which plots samples collected using mcmc .",
    "specifically , we used the rh - mala method in girolami and calderhead ( 2012 ) , with constant curvature , estimating the hessian at each iteration .",
    "these are samples from 25,000 iterations , collected after starting at the posterior mode and running through 25,000 burn - in iterations , thinned every 10 draws .",
    "just as papaspiliopolous and roberts predicted , the chain tends to get stuck near the mode .",
    "it is only after some serendipitously large proposal jumps that the chain ever finds itself in the tails ( hence the gaps in the plot ) , but the chain does not move very far along those tails at all .",
    "next , we consider a hierarchical model with a large number of parameters . the dependent variable @xmath79 is measured @xmath80 times for heterogenous units @xmath81 . for each unit",
    ", there are @xmath82 covariates , including an intercept . the intercept and coefficients @xmath83 are heterogeneous , with a gaussian prior with mean @xmath84 and covariance @xmath85 , which in turn have weakly information standard hyperpriors .",
    "this model structure is given by :    @xmath86    to construct simulated datasets , we set `` true '' values of @xmath87 and @xmath88 . for each unit",
    ", we simulated @xmath89 observations , where the non - intercept covariates are all i.i.d draws from a standard normal distribution .",
    "three different values for @xmath90 were entertained : 100 , 500 and 1000 . in all cases , there are 14 population - level parameters , so the total number of parameters are 414 , 2014 and 4014 , respectively .",
    "the parameters of the hyperpriors are @xmath91 , @xmath92 , and @xmath93 .",
    "table [ tab : hiermvn_gds ] summarizes the performance of the gds algorithm , averaged over 10 replications of the experiment . for each case",
    ", we collected 100 samples from the posterior , with @xmath94 proposal draws .",
    "the proposal distributions are all multivariate normal , with mean at the posterior mode and the covariance set as the inverse of the hessian at the mode , multiplied by a scale factor that changes with @xmath90 . for each value of @xmath90",
    ", we set the scale factor to roughly be the smallest value for which @xmath95 for all @xmath94 proposal draws .",
    "the `` mean proposals '' column is the average number of proposals it took to collect 100 posterior draws during the accept - reject phase of the algorithm ( this is the inverse of the acceptance rate ) .",
    "we also recorded the time it took to run each stage of the algorithm .",
    "the `` post mode '' column is the number of minutes it took to find the posterior mode , starting at the origin ( after transforming all parameters to have a domain on the real line ) .",
    "the `` proposals '' column is the amount of time it took to collect the @xmath94 proposal draws .",
    "`` acc - rej '' is the time it took to execute the accept - reject phase of the algorithm to collect 100 independent samples from the posterior , and `` total '' is the total time required to run the algorithm .",
    "the study was conducted on an apple mac pro with 12 cpu cores running at 2.93ghz and 32 gb of ram .",
    "ten of the 12 cores were allocated to this algorithm .",
    ".efficiency of gds for hierarchical gaussian example [ cols= \" > , > , > , > , > , > , > , > \" , ]",
    "this section discusses some practical issues while implementing gds . like",
    "the entire body of mcmc research continues to teach us , more insights will likely be gleaned as we , and hopefully others , gain additional experience with the method . here ,",
    "we provide some suggestions on how to implement gds effectively , and mention some areas in which more research or investigation is needed .",
    "searching for the posterior mode is considered , in general , to be `` good practice '' for bayesian inference even when using mcmc ; see step 1 of the `` recommended strategy for posterior simulation '' in section 11.10 of gelman et .",
    "al ( 2003 ) . for small problems ,",
    "like the example in section [ sec : cauchy ] , standard nonlinear optimization algorithms , such as those found in common statistical packages like r , are sufficient for finding posterior modes and estimating hessians . for larger problems , finding the mode and estimating",
    "the hessian can be more difficult when using those same tools .",
    "however , there are many different ways to find the extrema of a function , and some may be more appropriate for some kinds of problems than for others .",
    "therefore , one should not immediately conclude that finding the posterior mode ( or modes ) is a barrier to adopting gds for large or ill - conditioned problems , like the @xmath96-dimensional model in section [ sec : ads ] .",
    "when the log posterior density is smooth and unimodal , a natural algorithm for finding a posterior mode is one that exploits gradient and hessian information in a way that is related to newton s method .",
    "nocedal and wright ( 2006 ) describe many different nonlinear optimization methods , but most can be classified as either `` line search '' or `` trust region '' methods .",
    "line search methods may be more common ; for instance , all of the gradient - based algorithms implemented in the ` optim ` function in r are line search methods . but",
    "these methods could be subject to numerical problems when the log posterior is nearly flat , or has a ridge , in which case the algorithm may try to evaluate the log posterior at a point that is so far away from the current value that it generates numerical overflow .",
    "trust region methods ( conn , et .",
    "al . , 2000 ) , on the other hand , tend to be more stable , because each proposed step is constrained to be within a particular distance ( the `` radius '' of the trust region ) of the current point .",
    "in short , if one finds that a `` standard '' optimizer for a particular programming environment is having trouble finding the posterior mode , there may be other common algorithms that can find the mode more easily .",
    "neither line - search nor trust - region algorithms necessarily require explicit expressions for gradients and hessians , but generating these structures exactly can also speed up the mode - finding step of gds .",
    "this approach is in contrast to approximations that use finite differencing or quasi - newton hessian updates . of course",
    ", one can always derive the gradient of the log posterior density analytically , but this can be a tedious process . we have had success with algorithmic differentiation ( ad ) software such as the cppad library ( bell 2012 ) . with ad , we need only to write a function that computes the log posterior density .",
    "the ad library includes functions that automatically return derivatives of that function .",
    "the time to compute the gradient of a function is a small multiple of the time it takes to compute the original function , and otherwise does not depend on the dimension of the problem .",
    "estimating the hessian is useful not only for the mode - finding step , but also for choosing the covariance matrix of a multivariate proposal density .",
    "the time it takes for ad software to compute a hessian can depend on the dimension of the problem , and working with a dense hessian for a large problem can be prohibitively expensive in terms of computation and memory usage .",
    "however , for many hierarchical models , we assume conditional independence across heterogeneous units . for these models , the hessian of the log posterior is sparse , with a `` block - diagonal - arrow '' structure ( block - diagonal , but dense on the bottom and right margins ) .",
    "thus , we can achieve substantial computational improvements by exploiting this sparsity .",
    "the advantage comes in storing the hessian in a compressed format , such that zeros are not stored explicitly .",
    "not only does this permit estimating larger models on computers with less memory , but it also lets us use efficient computational routines that exploit that sparsity . for example ,",
    "powell and toint ( 1979 ) and coleman and more ( 1983 ) explain how to efficiently estimate sparse hessians using graph coloring techniques .",
    "coleman et al .",
    "( 1985a , 1985b ) offer a useful fortran implementation to estimate sparse hessians using graph coloring and finite differencing .",
    "algorithmic differentiation libraries like cppad can also exploit sparsity when computing hessians . both matlab and r ( through the matrix package ) can store sparse symmetric matrices in compressed format .",
    "one important consideration is the case of multimodal posteriors .",
    "gds does require finding the global posterior mode , and all the models discussed in this paper have unimodal posterior distributions .",
    "when the posterior is multimodal , one could instead use a mixture of normals as the proposal distribution .",
    "the idea is to not only find the global mode , but any local ones as well , and center each mixture component at each of those local modes .",
    "the gds algorithm itself remains unchanged , as long as the global posterior mode matches the global proposal mode .",
    "we recognize that finding all of the local modes could be a hard problem , and there is no guarantee that any optimization algorithm will find all local extrema .",
    "but , by the same token , this problem can be resolved efficiently in a multitude of complex bayesian statistical models if one uses the correct tools . and",
    "it is only a matter of time before these tools are more widely available in standard statistical programming languages like r. the nonlinear optimization literature is rife with methods that help facilitate efficient location of multiple modes , even if there is no guarantee of finding them all .",
    "also , note that even though mcmc sampling chains are , in _ theory _ , guaranteed to explore the entire space of any posterior distribution ( including multiple regions of high posterior mass ) , there is no guarantee that this will happen after a large finite number of iterations for general nonconjugate hierarchical models .",
    "other estimation algorithms that purport to be robust to multimodal posteriors offer no such guarantees either .      like many other methods that collect random samples from posterior distributions ,",
    "the efficiency of gds depends in part on a prudent selection of the proposal density @xmath9 .",
    "for the examples in this paper , we used a multivariate normal density that is centered at the posterior mode , with a covariance matrix that is proportional to the inverse of the hessian at the mode .",
    "one might then wonder if there is an optimal way to determine just how `` scaled out '' the proposal covariance needs to be . at this time",
    ", we think that trial and error is , quite frankly , the best alternative .",
    "for example , if we start with a small @xmath32 ( say , 100 draws ) , and find that @xmath97 for any of the @xmath32 proposals , we have learned that the proposal density is not valid , at little computational or real - time cost",
    ". we can then re - scale the proposal until @xmath98 , and then gradually increase @xmath32 until we get a good approximation to @xmath99 . in our experience , even if an acceptance rate appears to be low ( say , 0.0001 ) , we can still collect draws in parallel , so the `` clock time '' remains much less than the time we spend trying to optimize selection of the proposal .",
    "for example , in the cauchy example in section [ sec : cauchy ] , we set the proposal covariance to be the inverse hessian at the posterior mode , scaled by a factor of 200 .",
    "we needed such a large scale factor because the normal approximation at the mode shows no correlation , even though there is obvious correlation in the tails .",
    "if one knew upfront the extent of the tail dependence , one might have chosen a proposal density that is more highly correlated , and that might give a higher acceptance rate .",
    "but of course one seldom , if ever , knows the shape of any target posterior density up front .",
    "so even though an acceptance percentage of 1.3% may appear to be low , we should consider the amount of time it would take to improve the proposal density , and especially the number of mcmc iterations it would take to get enough draws that are equivalent to the same number of independent gds draws .",
    "this paper demonstrated that gds is a viable alternative to mcmc for a large class of bayesian non - gaussian and gaussian hierarchical models .",
    "of course it would be myopic to claim that gds is appropriate for all models . by the same token",
    ", we can not assert that gds would not work for any of the models described below . these models are topics requiring additional research .",
    "[ [ models - with - discrete - or - combinatorial - optimization - elements ] ] models with discrete or combinatorial optimization elements + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in models that include both discrete and continuous parameters , finding the posterior mode becomes a mixed - integer nonlinear program ( minlp ) .",
    "an example is the bayesian variable selection problem ( george and mcculloch 1997 ) .",
    "the difficulty lies in the fact that minlps are known to be np - complete , and thus may not scale well for large problems .",
    "hidden markov models with multiple discrete states might be similarly difficult to estimate using gds . also , it is not immediately clear how one might select a proposal density when some parameters are discrete .    [ [ intractable - likelihoods - or - posteriors ] ] intractable likelihoods or posteriors + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    there are many popular models , namely binary , ordered and multinomial probit models , for which the likelihood of the observed data is not available in closed form .",
    "when direct numerical approximations to these likelihoods ( e.g. , monte carlo integration ) is not tractable , mcmc with data augmentation is a popular estimation tool ( e.g. , albert and chib 1993 ) .",
    "that said , recent advances in parallelization using graphical processing units ( gpus ) might make numerical estimation of integrals more practical than it was even 10 years ago ; see suchard et al . ( 2010 ) .",
    "if this is the case , and the log posterior remains sufficiently smooth , then gds could be a viable , efficient alternative to data augmentation in these kinds of models .",
    "[ [ missing - data - problems ] ] missing data problems + + + + + + + + + + + + + + + + + + + + +    mcmc - based approaches to multiple imputation of missing data could suffer from the same kinds of problems : the latent parameter , introduced for the data augmentation step , is only weakly identified on its own .",
    "normally , we are not interested in the missing values themselves . if the number of missing data points is small , perhaps one could treat the representation of the missing data points as if they were parameters .",
    "but the implications of this require additional research .",
    "[ [ spatial - models - and - other - models - with - dense - hessians ] ] spatial models , and other models with dense hessians + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    gds does not explicitly require conditional independence , so one might consider using it for spatial or contagion models ( e.g. , yang and allenby 2003 ) .",
    "however , without a conditional independence assumption , the hessian of the log posterior will not be sparse , and that may restrict the size of datasets for which gds is practical .",
    "in this paper , we presented a new method , generalized direct sampling ( gds ) , to sample from posterior distributions .",
    "this method has the potential to bypass mcmc - based bayesian inference for large , complex models with continuous , bounded posterior densities . unlike mcmc , gds generates independent draws that one could collect in parallel .",
    "the implementation of gds is straightforward , and requires only a function that returns the value of the unnormalized log posterior density . in addition",
    ", gds allows for fast and accurate computation of marginal likelihoods , which can then be used for model comparison .",
    "there are many other ways to conduct bayesian inference , and continued improvement of mcmc remains an important stream of research .",
    "nevertheless , it would be hard to ignore the opportunities for parallelization that make algorithms like gds very attractive alternatives .",
    "of course , one could employ parallel computational techniques as part of a sequential algorithm .",
    "but , to repeat an earlier sentence , using parallel technology to generate a single draw is not the same as generating all of the required draws themselves in parallel . by exploiting the advantages of parallel computing , as in this paper",
    ", gds could prove to be a successful addition to the bayesian practitioner s computational toolkit .",
    "99 atchade , y. f. ( 2006 ) an adaptive version for the metropolis adjusted langevin algorithm with a truncated drift . methodology and computing in applied probability 8 , 235 - 254 .",
    "bell , b. ( 2012 ) cppad : a package for c++ algorithmic differentiation .",
    "_ computational infrastructure for operations research _ http://www.coin-or.org/cppad .",
    "braun , m. and moe , w. ( 2012 ) online advertising response models : incorporating multiple creatives and impression histories . working paper .",
    "massachusetts institute of technology .",
    "coleman , t. f. , garbow , b. s. and mor , j. j. ( 1985a ) software for estimating sparse hessian matrices .",
    "_ acm transactions on mathematical software _ 11(4 ) 363 - 377 .",
    "coleman , t. f. , garbow , b. s. and mor , j. j. ( 1985b ) algorithm 636 : fortran subroutines for estimating sparse hessian matrices .",
    "_ acm transactions on mathematical software _ 11(4 ) 378 .",
    "conn , a. , gould , n. , and toint , p. ( 2000 ) _ trust - region methods _ philadelphia : society for industrial and applied mathematics and mathematical programming society .",
    "gelman , a. , carlin , j. b. , stern , h. s. , and rubin , d. b. ( 2003 ) _ bayesian data analysis_. boca raton , fla . : chapman and hall .",
    "george , e. i. and mcculloch , r. e. ( 1997 ) approaches for bayesian variable selection .",
    "_ statistica sinica _ 7 , 339 - 373 .",
    "geweke , j. ( 1991 ) evaluating the accuracy of sampling - based approaches to the calculation of posterior moments .",
    "_ bayesian statistics _ new york : oxford university press .",
    "169 - 193 girolami , m. , and calderhead , b. ( 2011 ) riemann manifold langevin and hamiltonian monte carlo .",
    "_ journal of the royal statistical society , series b _ ( with discussion ) , 73 , 1 - 37 .",
    "lenk , p. ( 2009 ) simulation pseudo - bias correction to the harmonic mean estimator of integrated likelihoods .",
    "_ journal of computational and graphical statistics _ , 18(4 ) 941 - 960 .",
    "nocedal , j. and wright , s. j. ( 2006 ) _ numerical optimization _",
    "new york : springer .",
    "papaspiliopoulous , o. and roberts , g. ( 2008 ) .",
    "stability of the gibbs sampler for bayesian hierarchical models .",
    "_ annals of statistics _ , 36 , 95 - 117 .",
    "powell , m. j. d. and toint , ph . l. ( 1979 ) on the estimation of sparse hessian matrices .",
    "_ siam journal on numerical analysis _",
    "16(6 ) 1060 - 1074 .",
    "tibbits , m.m . , haran , m. and liechty , j.c .",
    "parallel multivariate slice sampling .",
    "_ statistics and computing _ , 21(3 ) , 415 - 430 .",
    "walker , s.g . , laud , p.w . ,",
    "zantedeschi , d. , and damien , p. ( 2010 ) .",
    "direct sampling .",
    "_ journal of computational and graphical statistics _ , 20(3 ) , 692 - 713 .",
    "suchard , m.a . ,",
    "wang , q. , chan , c. , frelinger , j. , cron , a. , and west , m. ( 2010 ) .",
    "understanding gpu programming for statistical computation : studies in massively parallel massive mixtures .",
    "_ journal of computational and graphical statistics _",
    ", 19(2 ) , 419 - 438 .",
    "yang , s. , and allenby , g. ( 2003 ) modeling interdependent consumer preferences .",
    "_ journal of marketing research _",
    "40(3 ) , 282 - 294 ."
  ],
  "abstract_text": [
    "<S> we develop a new method to sample from posterior distributions in hierarchical models without using markov chain monte carlo . </S>",
    "<S> this method , which is a variant of importance sampling ideas , is generally applicable to high - dimensional models involving large data sets . </S>",
    "<S> samples are independent , so they can be collected in parallel , and we do not need to be concerned with issues like chain convergence and autocorrelation . additionally , the method can be used to compute marginal likelihoods . </S>"
  ]
}