{
  "article_text": [
    "assume that we have a @xmath3 matrix @xmath0 representing @xmath4 points @xmath5 in @xmath6 . in this paper",
    ", we will be concerned with linear feasibility problems that ask if there exists a vector @xmath7 that makes positive dot - product with every @xmath8 , i.e. @xmath9 where boldfaced @xmath10 is a vector of zeros .",
    "the corresponding algorithmic question is `` if ( p ) is feasible , how quickly can we find a @xmath11 that demonstrates ( p ) s feasibility ? '' .",
    "such problems abound in optimization as well as machine learning . for example , consider _ binary linear classification _ - given @xmath4 points @xmath12 with labels @xmath13 , a classifier @xmath11 is said to separate the given points if @xmath14 has the same sign as @xmath15 or succinctly @xmath16 for all @xmath17 .",
    "representing @xmath18 shows that this problem is a specific instance of ( p ) .",
    "we call ( p ) the _ primal _ problem , and ( we will later see why ) we define the _ dual _ problem ( d ) as : @xmath19 and the corresponding algorithmic question is `` if ( d ) is feasible , how quickly can we find a certificate @xmath20 that demonstrates feasibility of ( d ) ? '' .",
    "+ our aim is to deepen the geometric , algebraic and algorithmic understanding of the problems ( p ) and ( d ) , tied together by a concept called _",
    "margin_. geometrically , we provide intuition about ways to interpret margin in the primal and dual settings relating to various balls , cones and hulls . analytically , we prove new margin - based versions of classical results in convex analysis like gordan s and hoffman s theorems .",
    "algorithmically , we give new insights into the classical perceptron algorithm .",
    "we begin with a gentle introduction to some of these concepts , before getting into the details .",
    "[ [ notation ] ] * notation * + + + + + + + + + +    when we write @xmath21 for vectors",
    "@xmath22 , we mean @xmath23 for all their indices @xmath17 ( similarly @xmath24 ) . to distinguish surfaces and interiors of balls more obviously to the eye in mathematical equations , we choose to denote euclidean balls in @xmath25 by @xmath26 , @xmath27 and the probability simplex @xmath28 by @xmath29 .",
    "we denote the linear subspace spanned by @xmath0 as lin@xmath30 , and convex hull of @xmath0 by conv@xmath30 .",
    "lastly , define @xmath31 and @xmath32 is the ball of radius @xmath33 ( @xmath34 are similarly defined ) .",
    "the margin of the problem instance @xmath0 is classically defined as @xmath36    if there is a @xmath11 such that @xmath37 , then @xmath38 . if for all @xmath11 , there is a point at an obtuse angle to it , then @xmath39 . at the boundary @xmath35",
    "can be zero .",
    "the @xmath40 in the definition is important  if it were @xmath41 , then @xmath35 would be non - negative , since @xmath42 would be allowed .",
    "this definition of margin was introduced by goffin  @xcite who gave several geometric interpretations .",
    "it has since been extensively studied ( for example , @xcite and @xcite ) as a notion of complexity and conditioning of a problem instance .",
    "broadly , the larger its magnitude , the better conditioned the pair of feasibility problems ( p ) and ( d ) are , and the easier it is to find a witnesses of their feasibility .",
    "ever since @xcite , the margin - based algorithms have been extremely popular with a growing literature in machine learning which it is not relevant to presently summarize .    in sec .  [",
    "sec : affmargin ] , we define an important and `` corrected '' variant of the margin , which we call _ affine - margin _ , that turns out to be the actual quantity determining convergence rates of iterative algorithms .    [",
    "[ gordans - theorem ] ] * gordan s theorem * + + + + + + + + + + + + + + + + + + +    this is a classical _ theorem of the alternative _ , see @xcite .",
    "it implies that exactly one of ( p ) and ( d ) is feasible .",
    "specifically , it states that exactly one of the following statements is true .    1 .",
    "there exists a @xmath11 such that @xmath37 .",
    "2 .   there exists a @xmath43 such that @xmath44 .",
    "this , and other separation theorems like farkas lemma ( see above references ) , are widely applied in algorithm design and analysis .",
    "we will later prove generalizations of gordan s theorem using affine - margins .",
    "[ [ hoffmans - theorem ] ] * hoffman s theorem * + + + + + + + + + + + + + + + + + + +    the classical version of the theorem from @xcite characterizes how close a point is to the solution set of the feasibility problem @xmath45 in terms of the amount of violation in the inequalities and a problem dependent constant . in a nutshell , if @xmath46 then @xmath47_+ \\big\\|\\ ] ] where @xmath48 is the `` hoffman constant '' and it depends on @xmath0 but is _ independent of @xmath49_. this and similar theorems have found extensive use in convergence analysis of algorithms - examples include @xcite .",
    "gler , hoffman , and rothblum  @xcite generalize this bound to any norms on the left and right hand sides of the above inequality",
    ". we will later prove theorems of a similar flavor for ( p ) and ( d ) , where @xmath50 will almost magically turn out to be the affine - margin .",
    "such theorems are used for proving rates of convergence of algorithms , and having the constant explicitly in terms of a familiar quantity is useful .      *",
    "* geometric * : in sec.[sec : affmargin ] , we define the _ affine - margin _ , and argue why a subtle difference from eq.([eq : margin ] ) makes it the `` right '' quantity to consider , especially for problem ( d ) .",
    "we then establish geometrical characterizations of the affine - margin when ( p ) is feasible as well as when ( d ) is feasible and connect it to well - known _",
    "radius theorems_. this is the paper s appetizer . * * analytic * : using the preceding geometrical insights , in sec.[sec : gordan ] we prove two generalizations of gordan s theorem to deal with alternatives involving the affine - margin when either ( p ) or ( d ) is strictly feasible . building on this intuition further , in sec.[sec : hoffman ] ,",
    "we prove several interesting variants of hoffman s theorem , which explicitly involve the affine - margin when either ( p ) or ( d ) is strictly feasible .",
    "this is the paper s main course . * * algorithmic * : in sec.[sec : np ] , we prove new properties of the normalized perceptron , like its margin - maximizing and margin - approximating property for ( p ) and dual convergence for ( d ) .",
    "this is the paper s dessert .",
    "we end with a historical discussion relating von - neumann s and gilbert s algorithms , and their advantage over the perceptron .",
    "an important but subtle point about margins is that the quantity determining the difficulty of solving ( p ) and ( d ) is actually _ not _ the margin as defined classically in eq.([eq : margin ] ) , but the affine - margin which is the margin when @xmath11 is restricted to lin(@xmath0 ) , i.e. @xmath51 for some coefficient vector @xmath52 .",
    "the affine - margin is defined as @xmath53 where @xmath54 is a key quantity called the gram matrix , and @xmath55 is easily seen to be a self - dual semi - norm . intuitively , when the problem ( p ) is infeasible but @xmath0 is not full rank , i.e. lin(@xmath0 ) is not @xmath6 , then @xmath35 will never be negative ( it will always be zero ) , because one can always pick @xmath11 as a unit vector perpendicular to lin@xmath30 , leading to a zero dot - product with every @xmath8 . since no matter how easily inseparable @xmath0 is",
    ", the margin is always zero if @xmath0 is low rank , this definition does not capture the difficulty of verifying linear infeasibility .",
    "similarly , when the problem ( p ) is feasible , it is easy to see that searching for @xmath11 in directions perpendicular to @xmath0 is futile , and one can restrict attention to lin@xmath30 , again making this the right quantity in some sense . for clarity , we will refer to @xmath56 when the problem ( p ) is strictly feasible ( @xmath57 ) or strictly infeasible ( @xmath58 ) respectively .",
    "we remark that when @xmath38 , we have @xmath59 , so the distinction really matters when @xmath60 , but it is still useful to make it explicit .",
    "one may think that if @xmath0 is not full rank , performing pca would get rid of the unnecessary dimensions .",
    "however , we often wish to only perform elementary operations on ( possibly large matrices ) @xmath0 that are much simpler than eigenvector computations .      unfortunately , the behaviour of @xmath61 is quite finicky  unlike @xmath62",
    "it is not stable to small perturbations when conv(@xmath0 ) is not full - dimensional . to be more specific ,",
    "if ( p ) is strictly feasible and we perturb all the vectors by a small amount or add a vector that maintains feasibility , @xmath62 can only change by a small amount .",
    "however , if ( p ) is strictly _ _",
    "in__feasible and we perturb all the vectors by a small amount or add a vector that maintains infeasibility , @xmath61 can change by a large amount .    for example , assume lin@xmath30 is not full - dimensional , and @xmath63 is large .",
    "if we add a new vector @xmath64 to @xmath0 to form @xmath65 where @xmath64 has a even a tiny component @xmath66 orthogonal to lin(@xmath0 ) , then @xmath67 suddenly becomes zero .",
    "this is because it is now possible to choose a vector @xmath68 which is in lin@xmath69 , and makes zero dot - product with @xmath0 , and positive dot - product with @xmath64 .",
    "similarly , instead of adding a vector , if we perturb a given set of vectors so that lin(@xmath0 ) increases dimension , the negative margin can suddenly jump from to zero .    despite its instability and lack of `` continuity '' , it is indeed this negative affine margin that determines rate of convergence of algorithms for ( d ) . in particular , the convergence rate of the von neumann ",
    "gilbert algorithm for ( d ) is determined by @xmath61 much the same way as the convergence rate of the perceptron algorithm for ( p ) is determined by @xmath62 .",
    "we discuss these issues in detail in section  [ sec : np ] and section  [ sec.vng ] .",
    "the positive margin has many known geometric interpretations ",
    "it is the width of the feasibility cone , and also the largest ball centered on the unit sphere that can fit inside the dual cone ( @xmath70 is the dual cone of cone@xmath30 )  see , for example @xcite . here",
    ", we provide a few more interpretations .",
    "remember that @xmath71 when eq .",
    "is feasible .",
    "[ margindual ] the distance of the origin to conv@xmath30 is @xmath72 .",
    "@xmath73    when @xmath74 , @xmath75 and eq .",
    "holds because ( d ) is feasible making the right hand side also zero . when @xmath76 , @xmath77 note that the first two equalities holds when @xmath76 , the next by the minimax theorem , and the last by self - duality of @xmath78 .    the quantity @xmath62 is also closely related to a particular instance of the minimum enclosing ball ( meb ) problem . while it is common knowledge that meb is connected to margins ( and support vector machines ) , it is possible to explicitly characterize this relationship , as we have done below .",
    "[ meb ] assume @xmath79 \\in { \\mathbb{r}}^{d\\times n}$ ] and @xmath80",
    ". then the radius of the minimum enclosing ball of conv(@xmath0 ) is @xmath81 .",
    "it is a simple exercise to show that the following are the meb problem , and its lagrangian dual @xmath82 the result then follows from proposition  [ margindual ] .    as we show in section  [ sec : np ] , the ( normalized ) perceptron and related algorithms that we introduce later yields a sequence of iterates that converge to the center of the meb , and if the distance of the origin to conv(@xmath0 ) is zero ( because @xmath60 ) , then the sequence of iterates coverges to the origin , and the meb just ends up being the unit ball .",
    "[ thm : radthm ] if @xmath74 then @xmath63 is the radius of the largest euclidean ball centered at the origin that completely fits inside the _ relative interior _ of the convex hull of @xmath0 .",
    "mathematically , @xmath83 .",
    "* choose any @xmath84 such that @xmath85 for any @xmath86 .",
    "given an arbitrary @xmath87 , put @xmath88 . by our assumption on @xmath84 , since @xmath89 , we can infer that @xmath90 implying there exists a @xmath91 such that @xmath92 .",
    "also @xmath93 thus @xmath94 since this holds for any @xmath95 , it follows that @xmath96 in other words , @xmath97    * ( 2 ) for inequality @xmath98 . *",
    "it suffices to show @xmath99 .",
    "we will prove the contrapositive @xmath100 . since @xmath101 is closed and convex , if @xmath102 , then there exists a hyperplane separating @xmath103 and @xmath101 in lin@xmath30 .",
    "that is , there exists @xmath104 with @xmath105 in lin(@xmath0 ) and a constant @xmath106 such that @xmath107 and @xmath108 for all @xmath43 . in particular , @xmath109 since @xmath110 , it follows that @xmath111    one might be tempted to deal with the usual margin and prove that latexmath:[\\[\\label{eq : oldrhoaminus }    while the two definitions are equivalent for full - dimensional lin@xmath30 , they differ when lin@xmath30 is not full - dimensional , which is especially relevant in the context of infinite dimensional reproducing kernel hilbert spaces , but could even occur when @xmath0 is low rank . in this case , eq.([eq : oldrhoaminus ] ) will always be zero since a full - dimensional ball can not fit inside a finite - dimensional hull .",
    "the right thing to do is to only consider balls ( @xmath86 ) in the linear subspace spanned by columns of @xmath0 ( or the relative interior of the convex hull of @xmath0 ) and not full - dimensional balls ( @xmath113 ) .",
    "the reason it matters is that it is this altered @xmath63 that determines rates for algorithms and the complexity of problem ( d ) , and not the classical margin in eq.([eq : margin ] ) as one might have expected .",
    "recall that @xmath114 conv@xmath30 , @xmath115 , and @xmath116 is just @xmath117 of radius r. since @xmath118 , an enlightening restatement of eq .",
    "and eq . is @xmath119 for brevity , @xmath120 this highlights the role of the margin is a measure of conditioning of the linear feasibility systems ( p ) and ( d ) .",
    "indeed , there are a number of far - reaching extensions of the classical `` radius theorem '' of @xcite .",
    "the latter states that the euclidean distance from a square non - singular matrix @xmath121 to the set of singular matrices in @xmath122 is precisely @xmath123 . in an analogous fashion , for the feasibility problems ( p ) and ( d ) , the set @xmath124 of _ ill - posed _",
    "matrices @xmath0 are those with @xmath125 .",
    "cheung and cucker  @xcite show that for a given a matrix @xmath126 with normalized columns , the margin is the largest perturbation of a row to get an ill - posed instance or the `` distance to ill - posedness '' , i.e. @xmath127 see @xcite for related discussions .",
    "we would like to make quantitative statements about what happens when either of the alternatives is satisfied _ easily _ ( with large positive or negative margin ) .",
    "our preceding geometrical intuition suggests a refinement of gordan s theorem , namely theorem  [ thm : gordan ] below , that accounts for margins .",
    "related results have been previously derived and discussed by li and terlaky  @xcite as well as by todd and ye  @xcite . in particular",
    ", it can be shown that part 2 of theorem  [ thm : gordan ] could be obtained from ( * ? ? ?",
    "* lemma 2.1 and lemma 2.2 ) .",
    "similarly , parts 2 and 3 could be recovered from ( * ? ? ?",
    "* theorem 5 and theorem 6 ) . we give a succinct and simple proof of theorem  [ thm : gordan ] by relying on proposition  [ margindual ] and proposition  [ thm : radthm ] .",
    "theorem  [ thm : gordan ] could also be proven , albeit less succinctly , via separation arguments from convex analysis .",
    "[ thm : gordan ] for any problem instance @xmath0 and any constant @xmath128 ,    1 .",
    "either @xmath129 s.t .",
    "@xmath37 , or @xmath130 s.t .",
    "either @xmath129 s.t .",
    "@xmath132 , or @xmath130 s.t .",
    "either @xmath129 s.t .",
    "@xmath134 , or @xmath135  @xmath136 s.t .",
    "@xmath137 .",
    "the first statement is the usual form of gordan s theorem .",
    "it is also a particular case of the other two when @xmath138 .",
    "thus , we will prove the other two :    1 .   if the first alternative does not hold , then from the definition of @xmath139 it follows that @xmath140 .",
    "in particular , @xmath141 . to finish ,",
    "observe that by proposition  [ margindual ] there exists @xmath43 such that @xmath142 2 .",
    "analogously to the previous case , if the first alternative does not hold , then @xmath143 . in particular",
    ", it captures @xmath144 observe that by proposition  [ thm : radthm ] , every point @xmath145 must be inside @xmath101 , that is , @xmath137 for some distribution @xmath146 .",
    "one can similarly argue that in each case if the first alternative is true , then the second must be false .    in the spirit of radius theorems introduced in the previous section",
    ", the statements in theorem  [ thm : gordan ] can be equivalently written in the following succinct forms :    1 .   either @xmath147 , or @xmath148 2 .   either @xmath149 , or @xmath150 3",
    "either @xmath151 , or @xmath152    as noted in the proof of theorem  [ thm : gordan ] , the first statement is a special case of the other two when @xmath153 . in case 2",
    ", we have at least one witness @xmath20 close to the origin , and in 3 , we have an entire ball of witnesses close to the origin .",
    "hoffman - style theorems are often useful to prove the convergence rate of iterative algorithms by characterizing the distance of a current iterate from a target set . for example , a hoffman - like theorem was also proved by @xcite ( lemma 2.3 ) , where they use it to prove the linear convergence rate of the alternating direction method of multipliers , and in @xcite ( lemma 4 ) , where they use it to prove the linear convergence of a first order algorithm for calculating @xmath154-approximate equilibria in zero sum games .",
    "it is worth pointing out that hoffman , in whose honor the theorem is named and also an author of @xcite whose proof strategy we follow in the alternate proof of theorem [ thm : hoffpos ] , himself appeared to have overlooked the intimate connection of the `` hoffman constant '' ( @xmath48 in eq.([eq : hoffintro ] ) ) to the positive and negative margin , as we present in our theorems below .",
    "assume @xmath126 is such that @xmath156 . for @xmath157 define the `` witness '' set @xmath158 . if @xmath159 then for all @xmath160 , @xmath161 where @xmath162 is the distance from @xmath163 to @xmath164 measured by the @xmath165 norm @xmath166 .    given @xmath160 with @xmath167 ,",
    "consider a point @xmath168 note that @xmath169 and crucially @xmath170 ( since @xmath171 since @xmath172 ) .",
    "hence , by theorem [ thm : gordan ] , there exists a distribution @xmath20 such that @xmath173 .",
    "define @xmath174 then , by substitution for @xmath20 and @xmath64 one can see that @xmath175 hence @xmath176 , and @xmath177",
    ".    the following variation ( using witnesses only in @xmath178 ) on the above theorem also holds . this result",
    "is closely related to ( * ? ? ?",
    "* lemma 2 ) and has essentially the same proof .",
    "[ thm : hoffneg ] assume @xmath126 is such that @xmath156 .",
    "define the set of witnesses @xmath179 . then at any @xmath43 ,",
    "@xmath180    assume @xmath181 as otherwise there is nothing to show .",
    "consider @xmath182 since @xmath183 and @xmath169 , proposition  [ thm : radthm ] implies that @xmath184 for some @xmath185 .",
    "thus for @xmath186 we have @xmath187 and @xmath188      [ thm : hoffpos ] define @xmath190 for some vector @xmath191 .",
    "then , for all @xmath192 , @xmath193 ^ -\\|_\\infty}{\\rho_a^+}\\ ] ] where @xmath194 is the @xmath195-distance from @xmath11 to @xmath196 and @xmath197 .    since @xmath198 , the definitions of margin   and affine margin",
    "imply that there exists @xmath199 with @xmath200 .",
    "suppose , @xmath201 .",
    "then we can add a multiple of @xmath202 to @xmath11 as follows .",
    "let @xmath203^+ = -[a^t w -c]^-$ ] where @xmath204 and @xmath197 . since @xmath205 and @xmath206 , we have @xmath207 and consequently @xmath208 hence , @xmath209 whose distance from @xmath11 is precisely @xmath210 .",
    "the interpretation of the preceding theorem is that the distance to feasibility for the problem ( p ) is governed by the magnitude of the largest mistake and the positive affine - margin of the problem instance @xmath0 .",
    "we also provide an alternative proof of the theorem above , since proving the same fact from completely different angles can often yield insights .",
    "we follow the techniques of @xcite , though we significantly simplify it .",
    "this is perhaps a more classical proof style , and possibly more amenable to other bounds not involving the margin , and hence it is instructive for those unfamiliar with proving these sorts of bounds .    for",
    "any given @xmath11 , define @xmath211 and hence note that @xmath212 .",
    "@xmath213 we used the self - duality of @xmath78 in eq.([eq : l2dual ] ) , lp duality for eq.([eq : lpdual ] ) , @xmath214 by definition for eq.([eq:2 g ] ) , and holder s inequality in eq.([eq : cb ] ) .",
    "the last equality follows because @xmath215 , since @xmath216 by proposition  [ margindual ] .",
    "the perceptron algorithm was introduced and analysed by @xcite to solve the primal problem ( p ) , with many variants in the machine learning literature .",
    "for ease of notation throughout this section assume @xmath79 \\in { \\mathbb{r}}^{d\\times n}$ ] and @xmath80 .",
    "the classical algorithm starts with @xmath217 for any @xmath17 , and in iteration @xmath218 performs    & & a_i &  :   w_t-1^t a_i 0 . &",
    "+ & & w_t &   w_t-1+a_i . &    a variant called normalized perceptron which , as we point out in theorem  [ thm : npmargin ] below , is a subgradient method , only updates on the worst mistake , and tracks a normalized @xmath11 that which is a convex combination of @xmath8 s .    & & a_i &  =   _ a_i \\{w_t-1^t a_i } & + & & w_t &   ( 1 - 1 t ) w_t-1+(1 t ) a_i . &",
    "the best known property of the unnormalized perceptron or the normalized perceptron algorithm is that when ( p ) is strictly feasible with margin @xmath62 , it finds such a solution @xmath11 in @xmath219 iterations , as proved by @xcite .",
    "what is less obvious is that the perceptron is actually _ primal - dual _ in nature , as stated in the following result of li and terlaky  @xcite . in the following statement by an _",
    "@xmath154-certificate for ( d ) _ we mean a vector @xmath220 such that @xmath221    [ prop : primdual ] if ( d ) is feasible , the perceptron algorithm ( when normalized ) yields an @xmath154-certificate @xmath222 for ( d ) in @xmath223 steps .",
    "proposition  [ prop : primdual ] and proposition  [ thm : hoffneg ] readily yield the following result .",
    "[ the.corollary ] assume ( d ) is feasible and @xmath156 . define the set of witnesses @xmath224 .",
    "if @xmath225 is the sequence of np iterates then @xmath226    we prove two more nontrivial facts about the normalized perceptron that we have not found in the published literature for the case when ( p ) is feasible . in this case not only does the normalized perceptron produce a _ feasible _ @xmath11 in @xmath227 steps , but on continuing to run the algorithm , @xmath228 will approach the _ optimal _ @xmath11 that maximizes margin , i.e. , achieves margin @xmath62 .",
    "this is actually _ not _ true with the classical perceptron .",
    "the normalization in the following theorem is needed because @xmath229 .",
    "+    [ thm : npmargin ] assume ( p ) is feasible .",
    "if @xmath230 is the sequence of np iterates with margin @xmath231 , and the optimal point @xmath232 achieves the optimal margin @xmath233 , then @xmath234    let @xmath235 then @xmath236 the last step because @xmath237 and @xmath238    for the second inequality , first observe that @xmath239 where the first inequality follows by the triangle inequality , and because @xmath240 .",
    "the second inequality holds because @xmath241 and @xmath242 implies that @xmath243 the rest of the proof hinges on the fact that np can be interpreted as a subgradient algorithm for the following problem : @xmath244    we reproduce a short argument from @xcite which shows that @xmath245 is minimized at @xmath246 .",
    "let @xmath247 for some @xmath248 and some @xmath249 . substituting this into eq . , we see that @xmath250 achieved at @xmath251 and @xmath252 .",
    "hence @xmath253 .",
    "note that the @xmath254-th iteration in the np algorithm can be written as @xmath255 for @xmath256 .",
    "hence , the np algorithm is a subgradient method for . by construction , @xmath245 is a 1-strongly convex function .",
    "since it is minimized at @xmath257 , it follows that @xmath258 in addition , @xmath259 so @xmath260 it thus follows by induction on @xmath218 that @xmath261 this yields the required bound of @xmath262 when plugged into eq.([eq : rss2 ] ) .",
    "let us revisit the primal - dual formulation of the minimum enclosing ball problem .",
    "the center of the minimum enclosing ball is precisely @xmath263 .",
    "consequently the following result readily follows .",
    "[ cor.2 ] the sequence @xmath264 of np iterates converges to the center @xmath265 of the minimum enclosing ball problem .",
    "the normalized perceptron algorithm also gives for free an estimate of @xmath62 .",
    "[ prop : approxmargin ] the normalized perceptron gives an @xmath154-approximation to the _ value _ of the positive margin in @xmath266 steps .",
    "specifically , @xmath267    the proof follows from eq .",
    "and eq . , which imply that @xmath228 satisfies @xmath268 whose rearrangement with @xmath269 completes the proof .",
    "it is worth noting that in sharp contrast to the estimate on @xmath62 given by proposition  [ prop : approxmargin ] , the question of finding elementary algorithms to estimate @xmath63 remains open .",
    "von - neumann described an iterative algorithm for solving dual ( d ) in a private communication with dantzig in 1948 , which was subsequently analyzed by the latter , but only published in @xcite , and goes by the name of von - neumann s algorithm in optimization circles . independently , gilbert  @xcite described an essentially identical algorithm that goes by the name of gilbert s algorithm in the computational geometry literature .",
    "we respect the independent findings in different literatures , and refer to it as the von - neumann - gilbert ( vng ) algorithm .",
    "it starts from a point in conv(@xmath0 ) , say @xmath270 and loops :    & & a_i &  =   _ a_i\\{w_t-1-a_i } + & & w_t &   _ w _ w _ ;   w_= w_t-1 + ( 1-)a_i    dantzig s paper showed that the von - neumann - gilbert ( vng ) algorithm can produce an @xmath154-approximate solution ( @xmath20 such that @xmath271 ) to ( d ) in @xmath223 steps , establishing it as a dual algorithm as conjectured by von - neumann .",
    "though designed for ( d ) , epelman and freund  @xcite proved that when ( p ) is feasible , vng also produces a feasible @xmath11 in @xmath219 steps and hence vng is also primal - dual like the perceptron ( as proved in proposition [ prop : primdual ] ) .",
    "it readily follows that theorem [ thm : npmargin ] , corollary  [ the.corollary ] , corollary  [ cor.2 ] , and proposition [ prop : approxmargin ] hold as well with the von - neumann - gilbert algorithm in place of the normalized perceptron algorithm .",
    "nesterov was the first to point out in a private note to @xcite that vng is a frank - wolfe algorithm for @xmath272 note that eq.([eq : npsgd ] ) is a relaxed version of eq.([affmargin ] ) , and also that eq.([eq : vngfw ] ) and eq.([affmargin ] ) are lagrangian duals of each other as seen in eq .. in this light , it is not surprising that np and vng algorithms have such similar properties . moreover , bach  @xcite recently pointed out the strong connection via duality between subgradient and frank - wolfe methods .    however , vng possesses one additional property . restating a result of @xcite  if @xmath156 , then vng has linear convergence .",
    "we include a simple geometrical proof of this result .",
    "assume ( d ) is feasible , @xmath79 \\in { \\mathbb{r}}^{d\\times n}$ ] with @xmath80 , and @xmath273 .",
    "then the iterates @xmath225 generated by the vng algorithm satisfy @xmath274 in particular , the algorithm finds @xmath275 with @xmath276 in at most @xmath277 steps .",
    "figure  [ fig : vng ] illustrates the idea of the proof .",
    "assume @xmath278 as otherwise there is nothing to show .",
    "by the definition of affine margin , there must exist a point @xmath8 such that @xmath279 or equivalently @xmath280 .",
    "vng sets @xmath281 to be the nearest point to the origin on the line joining @xmath228 with @xmath8 .",
    "consider @xmath282 as the nearest point to the origin on a ( dotted ) line parallel to @xmath8 through @xmath228 .",
    "note @xmath283 ( internal angles of parallel lines ) .",
    "then , @xmath284 .",
    "hence , vng can converge linearly with strict infeasibility of ( p ) , but np can not .",
    "nevertheless , np and vng can both be seen geometrically as trying to represent the center of circumscribing or inscribing balls ( in ( p ) or ( d ) ) of conv(a ) as a convex combination of input points .      in this paper , we advance and unify our understanding of margins through a slew of new results and connections to old ones .",
    "first , we point out the correctness of using the affine margin , deriving its relation to the smallest ball enclosing conv(a ) , and the largest ball within conv(a ) .",
    "we proved generalizations of gordan s theorem , whose statements were conjectured using the preceding geometrical intuition . using these tools , we then derived interesting variants of hoffman s theorems that explicitly use affine margins .",
    "we ended by proving that the perceptron algorithm turns out to be primal - dual , its iterates are margin - maximizers , and the norm of its iterates are margin - approximators .",
    "right from his seminal introductory paper in the 1950s , hoffman - like theorems have been used to prove convergence rates and stability of algorithms .",
    "our theorems and also their proof strategies can be very useful in this regard , since such hoffman - like theorems can be very challenging to conjecture and prove ( see @xcite for example ) . similarly , gordan s theorem has been used in a wide array of settings in optimization , giving a precedent for the possible usefulness of our generalization .",
    "lastly , large margin classification is now such an integral machine learning topic , that it seems fundamental that we unify our understanding of the geometrical , analytical and algorithmic ideas behind margins .",
    "this research was partially supported by nsf grant cmmi-1534850 .",
    "10    francis bach .",
    "duality between subgradient and conditional gradient methods . , 2012 .",
    "hd  block .",
    "the perceptron : a model for brain functioning .",
    "i. , 34(1):123 , 1962 .",
    "jonathan borwein and adrian lewis . ,",
    "volume  3 .",
    "springer , 2006 .",
    "dennis cheung and felipe cucker . a new condition number for linear programming .",
    ", 91(1):163174 , 2001 .",
    "vasek chvatal . .",
    "macmillan , 1983 .",
    "george dantzig .",
    "an @xmath154-precise feasible solution to a linear program with a convexity constraint in @xmath223 iterations independent of problem size .",
    "technical report , stanford university , 1992 .",
    "carl eckart and gale young .",
    "the approximation of one matrix by another of lower rank .",
    ", 1(3):211218 , 1936 .",
    "marina epelman and robert  m freund .",
    "condition number complexity of an elementary algorithm for computing a reliable solution of a conic linear system .",
    ", 88(3):451485 , 2000 .    marina  a epelman , robert  m freund , et  al . .",
    "citeseer , 1997 .",
    "robert  m freund and jorge  r vera . some characterizations and properties of the  distance to ill - posedness and the condition measure of a conic linear system .",
    ", 86(2):225260 , 1999 .    elmer  g gilbert .",
    "an iterative procedure for computing the minimum of a quadratic form on a convex set .",
    ", 4(1):6180 , 1966 .",
    "andrew gilpin , javier pea , and tuomas sandholm .",
    "first - order algorithm with @xmath285 convergence for @xmath154-equilibrium in two - person zero - sum games .",
    ", 133(1 - 2):279298 , 2012 .",
    "jl  goffin .",
    "the relaxation method for solving systems of linear inequalities .",
    ", pages 388414 , 1980 .",
    "osman gler , alan  j hoffman , and uriel  g rothblum .",
    "approximations to solutions to systems of linear inequalities . , 16(2):688696 , 1995 .",
    "alan  j hoffman .",
    "on approximate solutions of systems of linear inequalities . , 49(4):263265 , 1952 .",
    "mingyi hong and zhi - quan luo . on the linear convergence of the alternating direction method of multipliers . , 2012 .",
    "dan li and tams terlaky .",
    "the duality between the perceptron algorithm and the von neumann algorithm .",
    ", 62:113136 , 2013 .    albert  bj novikoff . on convergence proofs for perceptrons",
    ". technical report , 1962 .",
    "aaditya ramdas and javier pea .",
    "margins , kernels and non - linear smoothed perceptrons .",
    "in _ proceedings of the 31st international conference on machine learning ( icml ) _ , 2014 .",
    "james renegar . some perturbation theory for linear programming .",
    ", 65(1):7391 , 1994 .",
    "james renegar .",
    "incorporating condition measures into the complexity theory of linear programming .",
    ", 5(3):506524 , 1995 .",
    "frank rosenblatt .",
    "the perceptron : a probabilistic model for information storage and organization in the brain . , 65(6):386 , 1958 .",
    "negar soheili and javier pea . a primal  dual smooth perceptron ",
    "von neumann algorithm . in _ discrete geometry and optimization _ , pages 303320 .",
    "springer , 2013 .",
    "m.  todd and y.  ye .",
    "approximate farkas lemmas and stopping rules for iterative infeasible - point iterates for linear programming .",
    ", 81:121 , 1998 .",
    "vladimir  n vapnik . statistical learning theory ."
  ],
  "abstract_text": [
    "<S> given a matrix @xmath0 , a linear feasibility problem ( of which linear classification is a special case ) aims to find a solution to a primal problem @xmath1 or a certificate for the dual problem which is a probability distribution @xmath2 . </S>",
    "<S> inspired by the continued importance of `` large - margin classifiers '' in machine learning , this paper studies a condition measure of @xmath0 called its _ margin _ that determines the difficulty of both the above problems . </S>",
    "<S> to aid geometrical intuition , we first establish new characterizations of the margin in terms of relevant balls , cones and hulls . </S>",
    "<S> our second contribution is analytical , where we present generalizations of gordan s theorem , and variants of hoffman s theorems , both using margins . </S>",
    "<S> we end by proving some new results on a classical iterative scheme , the perceptron , whose convergence rates famously depends on the margin . </S>",
    "<S> our results are relevant for a deeper understanding of margin - based learning and proving convergence rates of iterative schemes , apart from providing a unifying perspective on this vast topic . </S>"
  ]
}