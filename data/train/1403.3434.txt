{
  "article_text": [
    "cooperative control is used in systems where a set of control agents with limited sensing , communication and computational capabilities seeks to achieve objectives defined globally or individually @xcite,@xcite .",
    "uncertain environments further require the agents to respond to random events .",
    "examples arise in uav teams , cooperative classification , mobile agent coordination , rendez - vous problems , task assignment , persistent monitoring , coverage control and consensus problems ; see @xcite,@xcite,@xcite,@xcite,@xcite,@xcite,@xcite,@xcite,@xcite and references therein .",
    "both centralized and decentralized control approaches are used ; in the latter case , communication between the agents in order to make collaborative decisions plays a crucial role .    in this paper , we consider maximum reward collection problems ( mrcp ) where @xmath0 agents are collecting time - dependent rewards associated with @xmath1 targets in an uncertain environment . in a deterministic setting with",
    "equal target rewards a one - agent mrcp is an instance of a traveling salesman problem ( tsp ) , @xcite,@xcite .",
    "the multi - agent mrcp is similar to the vehicle routing problem ( vrp ) @xcite .",
    "these are combinatorial problems for which globallly optimal solutions are found through integer programming .",
    "for example , in @xcite,@xcite , a deterministic mrcp with a linearly decreasing reward model is cast as a dynamic scheduling problem and solved via heuristics .",
    "because of the mrcp complexity , it is natural to resort to decomposition techniques .",
    "one approach is to seek a functional decomposition that divides the problem into smaller sub - problems @xcite,@xcite which may be defined at different levels of the system dynamics .",
    "an alternative is a time decomposition where the main idea is to solve a finite horizon optimization problem , then continuously extend this _ planning horizon _ forward ( either periodically or in event - driven fashion ) .",
    "this is in the spirit of receding horizon techniques used in model predictive control ( mpc ) to solve optimal control problems for which obtaining infinite horizon feedback control solutions is extremely difficult @xcite . in such methods ,",
    "the current control action is calculated by solving a finite horizon open - loop optimal control problem using the current state of the system as the initial state . at each instant",
    ", the optimization yields an optimal control sequence executed over a shorter _ action horizon _ before the process is repeated . in the context of multi - agent systems ,",
    "a cooperative receding horizon ( crh ) controller was introduced in @xcite with the controller steps defined in event - driven fashion ( with events dependent on the observed system state ) as opposed to being invoked periodically , in time - driven fashion .",
    "a key feature of this controller is that it does not attempt to make any explicit agent - to - target assignments , but only to determine headings that , at the end of the current planning horizon , would place agents at positions such that a total expected reward is maximized .",
    "nonetheless , as shown in @xcite , a stationary  trajectory for each agent is guaranteed under certain conditions , in the sense that an agent trajectory always converges to some target in finite time .    in this paper",
    ", we consider mrcps in uncertain environments where , for instance , targets appear / disappear at random times and a target may have a random initial reward and a random reward decreasing rate .",
    "the contribution is to introduce a new crh controller , allowing us to overcome several limitations of the controller in @xcite , including potential instabilities in the agent trajectories and poor performance due to inaccurate estimation of the reward - to - go function .",
    "we accomplish this by reducing , at each event - driven control evaluation step , the originally infinite - dimensional feasible control set to a finite set and by improving the estimation process for the reward to go , including a new travel cost factor  for each target which accommodates different target configurations in a mission space .",
    "we also establish some properties of this new controller whose overall performance is significantly better relative to the original one , as illustrated through various simulation examples .    in section [ probform ] , the mrcp is formulated and in section [ mdp ] we place the problem in a broader context of event - driven optimal control . in sections",
    "[ origcrh ] and [ newcrh ] the original crh controller is reviewed and the proposed new controller and some of its properties are established . in section [",
    "numerical ] simulation examples are presented and future research directions are outlined in the conclusions .",
    "we consider a mrcp where agents and targets are located in a mission space @xmath2 .",
    "there are @xmath1 targets defining a set @xmath3 and @xmath0 agents defining a set @xmath4 .",
    "the mission space may have different topological characteristics . in a euclidean topology , @xmath5 as illustrated in fig .",
    "[ obstaclemission ] with a triangle denoting a base , circles are agents and squares are targets . in this case , the distance metric @xmath6 is a simple euclidean norm such that @xmath7 is the length of the shortest path between points @xmath8 .",
    "moreover , the feasible agent headings are given by the set @xmath9,$ ] @xmath10 .",
    "if there are obstacles in @xmath2 , the feasible headings and the shortest path between two points should be defined accordingly .",
    "alternatively , the mission space may be modeled as a graph @xmath11 with @xmath12 representing the location of targets and the base .",
    "feasible headings are defined by the ( directed ) edges at each node and the distance @xmath13 is the sum of the edge weights on the shortest path between @xmath14 and @xmath15 . in this paper , we limit ourselves to a euclidean mission topology .",
    "targets are located at points @xmath16 , @xmath17 .",
    "target @xmath18 s reward is denoted by @xmath19 where @xmath20 is the initial maximum reward and @xmath21 $ ] is a non - increasing discount function . by using the appropriate discounting function",
    "we can incorporate constraints such as hard or soft deadlines for targets .",
    "an example of a discount function is @xmath22{ll}1-\\frac{\\alpha_{i}}{d_{i}}t & \\mbox {   if $ t \\le d_i$}\\\\ ( 1-\\alpha_{i})e^{-\\beta_{i}(t - d_{i } ) } & \\mbox {    if $ t > d_i$ } \\end{array } \\right .",
    "\\label{reward}\\ ] ] where @xmath23 , @xmath24 and @xmath25 are given parameters .",
    "agents are located at @xmath26 .",
    "each agent has a controllable heading at time @xmath27 , @xmath28 $ ] .",
    "the velocity of agent @xmath29 is @xmath30 where we assume that @xmath31 is a fixed speed .",
    "we define a _ mission _ as the process of the @xmath0 agents cooperatively collecting the maximum possible total reward from @xmath1 targets within a given mission time @xmath32 . upon collecting rewards from all targets ,",
    "the agents deliver it to a _ base _ located at @xmath33 .",
    "events occurring during a mission can be controllable ( e.g. , collecting a target s reward ) or random ( e.g. , the appearance / disappearance of targets or changes in their location ) .",
    "the event - driven crh controller we will develop , handles these random events by re - solving the optimal control problem as in the original crh controller in @xcite . in order to ensure that agents collect target rewards in finite time",
    ", we assume that each target has a radius @xmath34 and that agent @xmath29 collects reward @xmath18 at time @xmath27 if and only if @xmath35 .    ( 0,0 ) grid ( 7.5,7.5 ) ; ( 3,3 ) circle ( 0.4 ) ; ( 6.5,3 ) circle ( 0.4 ) ; ( 3,3 ) node@xmath36 ; ( 6.5,3 ) node@xmath37 ; ( 5,4 ) + ( -12pt,-12pt ) rectangle + ( 12pt,12pt ) ; ( 6,1 ) + ( -12pt,-12pt ) rectangle + ( 12pt,12pt ) ; ( 3,1.5 ) + ( -12pt,-12pt ) rectangle + ( 12pt,12pt ) ; ( 2.1,6 ) + ( -12pt,-12pt ) rectangle + ( 12pt,12pt ) ; ( 1,5.3 ) + ( -12pt,-12pt ) rectangle + ( 12pt,12pt ) ; ( 5,4 ) node@xmath36 ; ( 6,1 ) node@xmath37 ; ( 3,1.5 ) node@xmath38 ; ( 2.1,6 ) node@xmath39 ; ( 1,5.3 ) node@xmath40 ; ( 3.5,3.5 )  ( 3.75,4 )  ( 4,3.5 )  cycle ; ( 3.75,3.3 ) nodeb ; ( 2,0 ) .. controls ( 3,3 ) and ( 2,4 ) .. ( 0,3 ) ; ( 3,6 ) .. controls ( 6,6.3 ) and ( 7,6 ) .. ( 4,7.1 ) ;",
    "we view the solution of a mrcp as a sequence of headings for all agents and associated heading switching times .",
    "we define a policy @xmath41 as a vector @xmath42 $ ] where @xmath43 $ ] are the switching time intervals over which headings are maintained with @xmath44 , and @xmath45 .",
    "the control @xmath46 $ ] with @xmath47 $ ] is the vector of all the agent headings at time @xmath48 . with",
    "@xmath1 bounded , there exist policies @xmath41 such that all targets are visited over a finite number of switching events .",
    "each switching time @xmath48 is either the result of a controllable event ( e.g. , visiting a target ) or an uncontrollable random event .",
    "this is a complex stochastic control problem where the state space @xmath49 is the set of all possible location of agents @xmath50 $ ] and targets @xmath51 $ ] with @xmath52 and @xmath53 is the set of unvisited targets at time @xmath48 .",
    "as the mission evolves , @xmath54 decreases and the mission is complete when either @xmath55 or a given mission time @xmath32 is reached . the complete system state at time @xmath48 is @xmath56 .",
    "we define the optimization problem @xmath57 as : @xmath58 where @xmath59 the time a target is visited is a controllable event associated with a heading switching . in a deterministic problem , there is no need to switch headings unless a target is visited , but in an uncertain setting the switching times are not limited to these events .",
    "we define a subsequence @xmath60 of @xmath61 , @xmath62 so that @xmath63 is the time target @xmath18 is visited .",
    "note that @xmath64 is not a monotonic sequence , since targets can be visited in any order .",
    "therefore , can be rewritten as @xmath65 defining the immediate reward as being collected during a time period @xmath66 and the reward - to - go as being aggregated over all @xmath67 , an optimality equation for this problem is : @xmath68 $ } \\label{optequation}\\ ] ] where @xmath69 denotes the maximum total reward at time @xmath48 with current state @xmath70 and @xmath71 is the immediate reward collected in the interval @xmath72 $ ] .",
    "finally , @xmath73 is the maximum reward - to - go at @xmath74 assuming no future uncertainty , i.e. , we avoid the use of an _ a priori _ stochastic model for the environment , opting instead to react to random events by re - solving when this happens .",
    "letting @xmath75 , we set @xmath76 .",
    "henceforth , we write @xmath77 for brevity . had we assumed a fixed value for @xmath78 _ a priori _ , the optimization problem could have been solved using dynamic programming ( dp ) with the terminal state reached when no target is left in the mission space .",
    "however , a fixed @xmath79 does not allow for real - time reactions to new events .",
    "this fact , along with the size of the state space renders dp impractical and motivates a receding horizon control approach where we set @xmath80 based on a _ planning horizon _ @xmath81 selected at time step @xmath48 .",
    "then , a finite horizon optimal control problem over @xmath82 $ ] is solved to determine the optimal control @xmath83 .",
    "this control is maintained for an _ action horizon _ @xmath84 .",
    "a new optimization problem is re - solved at @xmath85 or earlier if any random event is observed .",
    "following , the optimization problem @xmath86 is @xmath87 \\label{dp_problem}\\ ] ] where @xmath88 and @xmath89 were defined above assuming @xmath80 .",
    "the immediate reward is zero if agents do not visit any target during @xmath82 $ ] , otherwise it is the reward collected over this interval .",
    "fixing the value of @xmath81 is not constraining , since it is always possible to stop and re - solve a new problem at any @xmath90 .",
    "in this section we briefly review the crh controller introduced in @xcite and identify several limitations of it to motivate the methods we will use use to overcome them .",
    "_ cooperation scheme : _ in @xcite the agents divide the mission space into a _",
    "dynamic _ partition at each mission step .",
    "the degree of an agent s responsibility for each target depends on the relative proximity of the agent to the target .",
    "a neighbor set is defined for each target which includes its @xmath91 closest agents , @xmath92 , sharing the responsibility for that target until another agent moves closer .",
    "a value of @xmath93 is used in the previous and current work for simplicity . defining @xmath94 to be the direct distance between target @xmath18 and agent @xmath29 at time @xmath27 ,",
    "let @xmath95 be the @xmath96th closest agent to target @xmath18 at time @xmath27 .",
    "formally , @xmath97 let @xmath98 be a neighbor set based on which a _ relative distance _ function is defined for all @xmath10 : @xmath99{ll}{\\displaystyle\\frac{c_{ij}(t)}{\\sum\\limits_{k\\in\\beta^{b}(i , t)}c_{ik}(t ) } } & \\mbox{if $ j \\in \\beta^b(i , t)$};\\\\ 1 & \\mbox{otherwise } \\end{array } \\right.\\ ] ] obviously , if @xmath100 , then @xmath101 .",
    "the _ relative proximity function _",
    "@xmath102 defined in @xcite is viewed as the probability that target @xmath18 will be visited by agent @xmath29 : @xmath103{ll}1 , & \\mbox{if } \\delta\\leq\\delta\\\\ \\frac{1-\\delta-\\delta}{1 - 2\\delta } , & \\mbox{if } \\delta\\leq\\delta\\leq1-\\delta\\\\ 0 , & \\mbox{if } \\delta>1-\\delta \\end{array } \\right .",
    "\\label{proximity}\\ ] ] here , @xmath104 defines the level of cooperation between the agents . by increasing @xmath105",
    "an agent will take full responsibility for more targets , hence less cooperation .",
    "each agent takes on full responsibility for target @xmath18 if @xmath106 . as shown in @xcite , when @xmath107 the regions converge to the voronoi tessellation of the mission space , with the location of agents at the centers of the voronoi tiles .",
    "there is no cooperation region in this case and each agent is fully responsible for the targets within its own voronoi tile . on the other hand , when @xmath108 no matter how close an agent is to a target , the two agents are still responsible for that target .    _",
    "planning and action horizons : _ in @xcite , @xmath81 is defined as the earliest time of an event such that one of the agents can visit one of the targets : @xmath109 this definition of _ planning horizon _ for the crh controller ensures no controllable event can take place during this horizon .",
    "it also ensures that re - evaluation of the crh  control is event - driven , as opposed to being specified by a clock which involves a tedious synchronization over agents .",
    "[ activetarget ] illustrates how @xmath81 is determined when @xmath110 . the crh control calculated at @xmath48",
    "is maintained for an _ action horizon _ @xmath84 . in @xcite @xmath111",
    "is defined either @xmath112 through a random event that may be observed at @xmath113 $ ] so that @xmath114 , or @xmath115 as @xmath116 , @xmath117 .",
    "it is also shown in @xcite that under ( [ hkdef ] ) the crh controller generates a stationary trajectory for each agent under certain conditions , in the sense that an agent trajectory always converges to some target in finite time .      _ instabilities in agent trajectories : _ the optimization problem considered in @xcite uses a potential function which is minimized in order to maximize the total reward .",
    "the stationary  trajectory guarantee mentioned above is based on the assumption that all minima of this function are at the target locations . if this assumption fails to hold , the agents are directed toward the weighted center of gravity of all targets .",
    "this can happen in missions where targets attain a symmetric configuration , leading to oscillatory behavior in the agent trajectories .",
    "an example is shown in fig .",
    "[ oldcrh3target ] with the original crh controller applied to a single agent , resulting in oscillations between three targets with equal rewards .",
    "this problem was addressed in @xcite by introducing a monotonically increasing cost factor ( or penalty ) @xmath118 on the heading @xmath119 .",
    "while this prevents some of the instabilities , it has to be appropriately tuned for each mission .",
    "we show how to overcome this problem in section [ newcrh ] .    _",
    "hedging and mission time : _ the agent trajectories in @xcite are specifically designed to direct them to positions close to targets but not exactly towards them unless they are within a certain capture distance ,  the motivation being that no agent should be committed to a target until the latest possible time so as to hedge against the uncertainty of new , potentially more attractive , randomly appearing targets .",
    "this hedging effect is helpful in handling such uncertainties , but it can create excessive loss of time , especially when rewards are declining fast . this can be addressed by more direct movements towards targets , while also re - evaluating the control frequently enough .",
    "the feasible control set in the original crh is the continuous set @xmath120^{n}$ ] , and by appropriately reducing this to a discrete set of control values we will show how we can eliminate unnecessary hedging .",
    "this also reduces the complexity of the optimal control problem at each time step and facilitates the problem solution over a finite number of evaluations .",
    "_ estimation of reward - to - go : _ in the original crh control scheme , the visit times are estimated as the earliest time any agent @xmath29 would reach some target @xmath18 , given a control @xmath121 at time @xmath48 and maintained over @xmath82 $ ] .",
    "thus , the estimated visit time @xmath122 for any @xmath123 is @xmath124 where @xmath125 is the location of the agent @xmath29 in the next time step given the control @xmath126 .",
    "this is a lower bound for @xmath127 feasible only when @xmath128 , leading also to a mostly unattainable upper bound for the total reward .",
    "we will show how this estimate is improved by a more accurate projection of each agent s future trajectory .",
    "in this section we present a new version of the crh controller in @xcite . using the definition of @xmath125 given above and assuming @xmath110 for all agents , the feasible set for @xmath125",
    "is defined as @xmath129 in a euclidean mission space with no obstacles , @xmath130 is the circle centered at @xmath131 with radius @xmath81 .",
    "let @xmath132 be the indicator function capturing whether agent @xmath29 visits target @xmath18 at time @xmath27 .",
    "we define the immediate reward at @xmath48 : @xmath133 following the definition of @xmath134 as the visit time of target @xmath18 in , we define @xmath127 as the estimated visit time of target @xmath18 by agent @xmath29 . here",
    "@xmath135 and any of the agents in the mission space has a chance to visit target @xmath18 .",
    "at time @xmath48 we define an estimate of the reward - to - go @xmath88 for each @xmath121 as @xmath136 we previously mentioned that the original crh control approach used a lower bound for estimating @xmath127 .",
    "we improve this estimate and at the same time address the other two limitations presented above through three modifications : @xmath112 we introduce a new _ travel cost _ for each target , which combines the distance of a target from agents , its reward , and a local sparsity factor . @xmath115 we introduce an _ active target set _ associated with each agent at every control evaluation instant @xmath48 .",
    "this allows us to reduce the infinite dimensional feasible control set at @xmath48 to a finite set .",
    "@xmath137 we introduce a new event - driven action horizon @xmath111 which makes use of the active target set definition . with these three modifications ,",
    "we finally present a new crh control scheme based on a process of looking ahead over a number of crh control steps and aggregating the remainder of a mission through a reward - to - go estimation process .",
    "* travel cost factor : * at each control iteration instant @xmath48 , we define @xmath138 for target @xmath18 to measure the sparsity of rewards in its vicinity .",
    "let @xmath139 be such that @xmath140 for each @xmath17 and set @xmath141 so that the average reward decreasing rate of @xmath18 over the mission is given by @xmath142 .",
    "let the set @xmath143 contain the indices of the @xmath144 closest targets to @xmath18 at time @xmath48 .",
    "we then define the _ sparsity factor _ for target @xmath18 as @xmath145 where @xmath146 $ ] is a parameter used to shift the weight among the @xmath144 targets .",
    "note that @xmath138 is time - dependent since the set of @xmath144 closest targets changes over time as rewards are collected .",
    "a larger @xmath138 implies that target @xmath18 is located in a relatively sparse area and vice versa .",
    "the parameter @xmath144 is chosen based on the number of targets in the mission space and the computation capacity of the controller .",
    "the main idea for @xmath138 comes from @xcite where it was used to solve tsp problems with clustering .",
    "next , for any point in @xmath147 , we define target @xmath18 s travel cost at time @xmath48 as @xmath148 the travel cost is proportional to the distance metric , so the farther a target is from @xmath149 the more costly is the visit to that target .",
    "it is inversely proportional to the reward s average decreasing rate , implying that the faster the reward decreases , the less the travel cost is .",
    "adding @xmath138 gives a target in a sparse area a higher travel cost as opposed to one where there is an opportunity for a visiting agent to collect additional rewards from its vicinity .",
    "* active targets : * at each control iteration instant @xmath48 , we define for each agent @xmath29 a subset of targets with the following property relative to the planning horizon @xmath81 : @xmath150 this is termed the _ active target set _ and ( [ setpi ] ) implies that @xmath151 is an active target for agent @xmath29 if and only if it has the smallest travel cost from at least one point on the reachable set @xmath130 .",
    "this means that every @xmath152 is associated with one of the active targets and , therefore , so does every feasible heading @xmath126 . which corresponds to active target @xmath96 if and only if : @xmath153 when @xmath154 is continuous , active targets partition the reachable set @xmath130 into several arcs as illustrated in fig .",
    "[ activetarget ] where , for simplicity , we assume @xmath155 in and all @xmath20 and @xmath156 , @xmath157 are the same .",
    "in this case , agent 1 has four active targets : @xmath158 .",
    "the common feature of all points on an arc is that they correspond to the same active target with the least travel cost .",
    "_ construction of _",
    "_ for each target @xmath123 and each agent @xmath29 , let @xmath160 be the set of points @xmath147 defining the shortest path from @xmath131 to @xmath161 .",
    "the intersection of this set with @xmath130 is the set of closest points to target @xmath96 in the feasible set : @xmath162 in a euclidean mission space , @xmath160 is a convex combination ( line segment ) of @xmath131 and @xmath161 , while @xmath163 is a single point where this line crosses the circle @xmath130 .",
    "the following lemma provides a necessary and sufficient condition for identifying targets which are active for an agent at @xmath48 using @xmath163 .",
    "[ mrcplem1 ] target @xmath96 is an active target for agent @xmath29 at time @xmath48 if and only if , @xmath164 @xmath165    see appendix .    * action horizon : * the definition of @xmath111 in @xcite requires frequent iterations of the optimization problem through which @xmath83 is determined in case no random event is observed to justify such action . instead ,",
    "when there are no random events , we define a new _ multiple immediate target event _ to occur when the minimization in returns more than one target meaning the agent is at an equal distance to at least two targets .",
    "we then define @xmath111 to be the shortest time until the first multiple immediate target event occurs in @xmath82 $ ] : @xmath166 consequently , this definition of @xmath111 eliminates any unnecessary control evaluation .      in order to solve the optimization problem @xmath86 in using the crh approach ,",
    "we need the estimated visit time @xmath167 for each @xmath121 through which @xmath168 in ( [ jk ] ) can be evaluated .",
    "this estimate is obtained by using a projected path for each agent .",
    "this path projection consists of a _ look ahead _ and an _ aggregate _ step . in the first step ,",
    "the active target set @xmath159 is determined for agent @xmath29 . with multiple agents in a mission , at each iteration step the remaining targets are partitioned using the relative proximity function in .",
    "we denote the target subset for agent @xmath29 as @xmath169 where : @xmath170 let @xmath171 .",
    "all @xmath167 are estimated as if @xmath29 would visit targets in its own subset by visiting the one with the least travel cost first .",
    "we define the agent @xmath29 s tour as the permutation @xmath172 specifying the order in which it visits targets in @xmath169 . for simplicity , we write @xmath173 and let @xmath174 denote the @xmath175 target in agent @xmath29 s tour .",
    "then , for all @xmath176 and @xmath177 : @xmath178 and with @xmath179 , for all @xmath180 , @xmath181 where @xmath182 this results in the corresponding @xmath122 for all @xmath176 .",
    "we can now obtain the reward - to - go estimate as @xmath183 recalling the immediate reward in , the optimization problem @xmath86 becomes : @xmath184^{n}}\\big[j_{\\mathbf{i}}(\\mathbf{u}_{k},t_{k},h_{k})+j_{\\mathbf{a}}(\\mathbf{u}_{k},t_{k},h_{k})\\big]\\label{pkest}\\ ] ] in we defined the feasible set for the location of agent @xmath29 in the next step @xmath177 . in a euclidean mission space , each point @xmath152 corresponds to a heading @xmath185 relative to the agent s location @xmath186 . using the definition in [ cp ] let :",
    "@xmath187 and @xmath188 in the next lemma , we prove that in a single - agent mission with the objective function defined in the optimal control is @xmath189 for some @xmath190 .",
    "[ mrcplem2 ] in a single agent @xmath191 mission , if @xmath192 is an optimal solution to the problem : @xmath193}\\big[j_{\\mathbf{i}}(u_{k},t_{k},h_{k})+j_{\\mathbf{a}}(u_{k},t_{k},h_{k})\\big]\\ ] ] then @xmath194    see appendix .",
    "the implication of this lemma is that we can reduce the number of feasible controls to a finite set as opposed to the infinite set @xmath120 $ ] .",
    "[ mrcptheorem1 ] in a multi - agent mrcp mission , if @xmath195 $ ] is the optimal solution to the problem in then @xmath196 .",
    "see appendix    theorem [ mrcptheorem1 ] reduces the problem @xmath86 to a maximization problem over a finite set of feasible controls : @xmath197\\label{countablej}\\ ] ] this reduces the size of the problem compared to the original crh controller .",
    "the following algorithm generates controls in this manner at each step @xmath48 and is referred to as the one - step lookahead  crh controller ( extended to a @xmath198-step lookahead  algorithm in what follows ) . +",
    "* crh one - step lookahead algorithm : *    1 .",
    "determine @xmath81 through ( [ hkdef ] ) .",
    "2 .   determine the active target set @xmath159 through ( [ setpi ] ) for all @xmath10 .",
    "3 .   evaluate @xmath199 for all @xmath200 through and 4 .",
    "solve @xmath86 in ( [ pkest ] ) and determine @xmath201 .",
    "evaluate @xmath111 through 6 .",
    "execute @xmath201 over @xmath202 $ ] and repeat step 1 with @xmath85 .",
    "the one - step lookahead crh controller can be extended to a @xmath198-step lookahead controller with @xmath203 by exploring additional possible future paths for each agent at each time step @xmath48 . in the one - step lookahead algorithm , the optimal reward - to - go",
    "is estimated based on a single tour over the remaining targets .",
    "the @xmath198-step lookahead algorithm estimates this reward by considering more possible tours for each agent as follows . for any feasible @xmath204 the agent",
    "is hypothetically placed at the corresponding next step location @xmath205 .",
    "this is done for all agents to maintain synchronicity of the solution .",
    "at @xmath205 , a new active target set is determined , implying that agent @xmath29 can have @xmath206 possible paths . at this point",
    ", we can repeat the same procedure by hypothetically moving the agent to a new feasible location from the set @xmath207 or we can stop and estimate the reward - to - go for each available path . thus , for a two - step lookahead , problem @xmath86 becomes : @xmath208\\big]\\end{aligned}\\ ] ] we extend the previous algorithm to a 2-step lookahead in the following . for a @xmath198-step we should repeat steps 1 and 2 for @xmath198 times before moving to step 4 of the algorithm . +",
    "* crh 2-step lookahead algorithm : *    1 .",
    "determine @xmath81 through ( [ hkdef ] ) .",
    "2 .   determine the active target set @xmath159 through ( [ setpi ] ) for all @xmath10 .",
    "3 .   repeat steps @xmath209 for @xmath210 and @xmath211 for all @xmath212 .",
    "4 .   evaluate @xmath213 for all @xmath214 through and 5 .",
    "solve @xmath86 in ( [ 2step ] ) and determine @xmath201 .",
    "evaluate @xmath111 through 7 .",
    "execute @xmath201 over @xmath202 $ ] and repeat step 1 with @xmath85 .",
    "( 7,5 ) circle ( 0.65 ) ; ( 7,5 ) node@xmath215 ; ( 5,5 ) + ( -13pt,-13pt ) rectangle + ( 13pt,13pt ) ; ( 5,5 ) node@xmath216 ; ( 9,2.5 ) + ( -13pt,-13pt ) rectangle + ( 13pt,13pt ) ; ( 9,2.5 ) node@xmath217 ; ( 2,4 ) + ( -13pt,-13pt ) rectangle + ( 13pt,13pt ) ; ( 2,4 ) node@xmath218 ; ( 6,1.5 ) + ( -13pt,-13pt ) rectangle + ( 13pt,13pt ) ; ( 6,1.5 ) node@xmath219 ; ( 3,1 ) + ( -13pt,-13pt ) rectangle + ( 13pt,13pt ) ; ( 3,1 ) node@xmath220 ; ( 1.5,0.5 ) grid ( 9.5,6 ) ;    [ 5targetmission ]    ( root ) 0 child node1 child node3 childnode4 child node 2 child node 5 child node 5 child node 2 child node4 child node2 child node3 child node5 child node3 child node 2 child node 5 child node 5 child node 2 child node5 child node3 child node2 child node2 child node3 child[blue ] node2 child node4 child node1 child node 3 child node 5 child node 5 child node 3 child node5 child node 1 child node 3 child node3 child node1 ;    [ 5targetmissiontree ]    this procedure can easily be repeated and the whole process can be represented as a tree structure where the root is the initial location of the agent and a path from the root to each leaf is a possible target sequence for the agent . in fig . [ 5targetmission ] a sample mission with 5 targets is shown with its corresponding tree in fig . [ 5targetmissiontree ] .",
    "a brute - force method involves @xmath221 possible paths , whereas the tree structure for this mission is limited to 11 paths .",
    "the active target set for agent 1 consists of targets @xmath222 .",
    "each of these active targets would then generate several branches in the tree , as shown .",
    "we calculate the total reward for each path to find the optimal one .",
    "determining the complete tree for large @xmath198 is time consuming .",
    "the @xmath198-step lookahead crh controller enables us to investigate the tree down to a few levels and then calculate an estimated reward - to - go for the rest of the selected path . however , there is no guarantee on the monotonicity of the results with more lookahead steps and in some cases the final result degrades with one more lookahead step .",
    "the simplest case of the mrcp is the case with one agent and two targets .",
    "obviously , this is an easy routing problem whose solution is one of the two possible paths the agent can take .",
    "we prove that the one - step lookahead algorithm solves the problem with any linearly decreasing reward function .",
    "consider a mission with one agent and two targets with initial rewards and deadlines @xmath223 and @xmath224 respectively .",
    "the analytical solution for this case reveals whether path @xmath225 or @xmath226 is optimal .",
    "following the previous analysis , we assume that @xmath227 and set @xmath228 for the sake of brevity .",
    "we also assume the rewards are linearly decreasing to zero : @xmath229 .",
    "the two possible rewards are given by : @xmath230+\\lambda_{2}\\big[1-\\frac{\\mathit{d}(\\mathbf{x},\\mathbf{y}_{1})+\\mathit{d}(\\mathbf{y}_{1}-\\mathbf{y}_{2})}{d_{2}}]\\label{r12}\\]]@xmath231+\\lambda_{1}\\big[1-\\frac{\\mathit{d}(\\mathbf{x},\\mathbf{y}_{2})+\\mathit{d}(\\mathbf{y}_{2}-\\mathbf{y}_{1})}{d_{1}}]\\label{r21}\\ ] ] therefore , if @xmath232 , it follows that the following inequality must hold : @xmath233<\\nonumber\\\\ \\quad\\frac{\\lambda_{2}}{d_{2}}\\big[\\mathit{d}(\\mathbf{x},\\mathbf{y}_{2})-\\mathit{d}(\\mathbf{x},\\mathbf{y}_{1})+\\mathit{d}(\\mathbf{y}_{1},\\mathbf{y}_{2})\\big]\\label{twotargetanalytic}\\ ] ] and the optimal path is @xmath234 . letting @xmath235 denote the path obtained by the one - step lookahead crh controller , we show next that this controller recovers the optimal path @xmath236 .",
    "[ twotargettheorem ] consider a two - target , one - agent mission .",
    "if @xmath155 in and target @xmath18 s reward at time @xmath27 is @xmath237 , then @xmath238 .",
    "see appendix .",
    "questions that come into mind after introducing the multiple look ahead steps crh controller are : how many look ahead steps should we perform ?",
    "is it always better to do more look ahead steps ? or in a simple way , does the more steps look ahead always gives a better answer than less ?",
    "the answer to the first question is that it depends on the size of the problem and our computation capability .",
    "we can even adjust the number of look ahead steps during the course of the solution .",
    "we can start with more when there is more targets available and lower the number once there is only a few targets in the mission space .",
    "the answer to the other two questions is no .",
    "as much as one would like to have a sort of monotonicity effect in this problem , the complexity of the problem and its significant dependence on the mission topology causes the non - monotone results with different number of look ahead steps .",
    "here we are going to show a case with 10 equally important targets and one agent .",
    "this is a straight forward tsp for which the optimal path can be obtained through an exhaustive search . for this case",
    "the one and two look ahead steps crh controllers find the same path with a reward of 92.6683 .",
    "however , once we move up to three look ahead steps , the crh controller degrade to a worst path with 92.5253 reward .",
    "the path for these controllers is shown in figures [ 1s ] and [ 3s ] .",
    "the optimal path that is calculated through the exhaustive search is obtained by the crh controllers when we go up to six look ahead steps ( fig .",
    "the observation is that the non - monotone results from higher number of look ahead is a local effect and once we increase the look ahead steps crh controller can solve the problem to the optimality .",
    "this obviously is not the case for all missions and in some cases the optimal path can not be retrieved by crh controller with any look ahead steps .",
    "[ 1s ]     [ 3s ]     +     [ 5s ]     [ 6s ]    [ monotonicity ]",
    "we provide several mrcp examples in which the performance of the original and new crh controllers is compared . in all examples ,",
    "we use parameters @xmath108 , @xmath110 , @xmath239 , @xmath240 .",
    "* tsp benchmark comparison : * we use the crh controller as a path planning algorithm for some benchmark tsp problems . table [ tsptable ] shows the result of the 2-step and 3-step lookahead algorithm compared to the optimal results from @xcite .",
    "we emphasize that the crh controller is not designed for deterministic tsp problems so it is not expected to perform as well as highly efficient tsp algorithms .",
    "nonetheless , as a starting basis of comparison , we note that the errors are relatively small , ranging from 7.8 to 23.8% .",
    "[ tsptable ]    in an attempt to measure the sensitivity of the results of the new crh controller to partial mission information , we also tested cases where agents have limited sensing range ( see fifth column in table [ tsptable ] ) . in these cases , the agent only senses a target if it is within its sensing range which we have assumed to be @xmath241 of the maximum dimension of the mission space . the results in most cases are comparable to the full - information cases .",
    "the computation time for the limited range agents is about an order of magnitude shorter than the other one .",
    "these results show the low sensitivity of the crh controller performance to non - local information for each agent .",
    "this observation suggests that crh controller is likely to provide good performance in a distributed implementation or in cases where targets are not known a priori and should be locally sensed by the agents .    * addressing instabilities * : as already mentioned",
    ", the original crh controller may give rise to oscillatory trajectories and fail to complete a mission .",
    "this is illustrated in fig .",
    "[ oldcrh3target ] for a simple mission with three linearly discounted reward targets . in fig .",
    "[ newcrh3target ] , it is shown that the new crh controller can easily determine the optimal path in this simple case .",
    "[ oldcrh3target ]      [ newcrh3target ]    * comparison between original and new crh controller * : a mission with 25 targets distributed uniformly and 2 agents starting at a base is considered as shown in fig .",
    "[ mission25 ] , with uniformly distributed initial rewards : @xmath242 and @xmath243 as in . in this case , the original crh ( fig . [ oldcrh25 ] ) underperforms compared to 3-step and 5-step lookahead crh controller ( figs .",
    "[ 3stepcrh25 ] , [ 5stepcrh25 ] ) by a large margin .",
    "we have used a value of @xmath244 and @xmath245 in .",
    "this comes at the price of a slightly longer mission time in the 3-step look ahead case , since the original controller never reaches some targets before their rewards are lost .",
    "however , minimizing time is not an objective of the mrcp considered here and reward maximization dictates the final length of the mission .     [ mission25 ]     [ oldcrh25 ]     [ 3stepcrh25 ]     [ 5stepcrh25 ]    * randomly generated missions * : to compare the overall performance of the new crh controller , we generated 10 missions , each with 20 targets that are uniformly located in a @xmath246 mission space and two agents initially at the base .",
    "we have used @xmath247 and @xmath248 .",
    "the results are shown in table [ 20targets ] where we can see that the average total reward is increased by @xmath249 while the average mission time is increased by @xmath250 .",
    "[ 20targets ]    in another case 10 missions were generated , each with 20 targets where 10 targets are only initially available to the agents .",
    "the other 10 targets would randomly appear during the mission .",
    "we use an initial reward @xmath247 and the parameter @xmath243 .",
    "the comparison of the original and new crh controller is shown in table [ randomtargettable ] .",
    "an increase of @xmath251 is seen in the total reward with a slight @xmath252 increase in the total mission time .",
    "[ randomtargettable ]    * sparsity factor in clustered missions * : we considered 8 random mission with 20 targets that are located uniformly in one case and in 9 clusters in  a second case .",
    "the goal here is to investigate the contribution of the sparsity factor @xmath253 in .",
    "we have again used @xmath247 and @xmath248 .",
    "we consider a case with @xmath155 which eliminates the effect of @xmath253 and a second case with @xmath244 and @xmath254 in .",
    "the results in table [ sparsitytable ] indicate that in the clustered missions rewards are improved by about @xmath255 whereas in the uniform cases the reward is unaffected on average .",
    "[ sparsitytable ]",
    "in this work a new crh controller was developed for solving cooperative multi - agent problems in uncertain environments using the framework of the previous work in @xcite .",
    "we overcame several limitations of the controller developed in @xcite , including agent trajectory instabilities and inaccurate estimation of a reward - to - go function while improving the overall performance .",
    "the event - driven crh controller is developed to solve the mrcp , where multiple agents cooperate to maximize the total reward collected from a set of stationary targets in the mission space .",
    "the mission environment is uncertain , for example targets can appear at random times and agents might have a limited sensing range .",
    "the controller sequentially solves optimization problems over a planning horizon and executes the control for a shorter action horizon , where both are defined by certain events associated with new information becoming available . unlike the earlier crh controller",
    ", the feasible control set is finite instead of an infinite dimensional set . in the numerical comparisons",
    ", we showed that the new crh controller has a better performance than the original one . in future work",
    ", the same framework will be applied to problems such as data harvesting where each target is generating data that should be collected and delivered to the base . here",
    "the base will act as a target with dynamic reward .",
    "also the new crh controller can be extended into a decentralized version where each agent is responsible for calculating its own control .",
    "* proof of lemma [ mrcplem1 ] * [ proof : mrcplem1 ] from the definition of @xmath256 in and @xmath163 in we have : @xmath257 dividing both sides by @xmath258 and adding @xmath259 we get , for all @xmath152 , @xmath260 to prove the forward lemma statement , we use a contradiction argument and assume there exists a target @xmath261 such that @xmath262 using , we get @xmath263 for all @xmath152 .",
    "this implies that there exists no @xmath152 such that @xmath264 .",
    "therefore , @xmath96 can not be an active target , which contradicts the assumption , hence is true.to prove the reverse statement , we assume that holds for any @xmath151 , i.e. , @xmath265 by the definition of active targets ( [ setpi ] ) , we then know that @xmath96 is an active target for agent @xmath29 at time @xmath48 .",
    "< 1.5em -1.5em plus0em minus0.5em height0.4em width0.5em depth0.25em**proof of lemma [ mrcplem2 ] * * [ proof : mrcplem2 ] the active target set creates a partition of the set @xmath130 where each subset is an arc in a euclidean mission space . for an active target @xmath266 , let the @xmath96th arc be @xmath267 . for each @xmath268 , we prove that the heading @xmath269 satisfies , for all @xmath270 : @xmath271 there are two possible cases:_case 1 _ : @xmath272 .",
    "this means @xmath273 .",
    "also , from , this guarantees that @xmath274 : @xmath275{rl}1 & \\mbox {   if $ r = l$}\\\\ 0 & \\text{otherwise}\\end{array } \\right.\\ ] ] setting @xmath276 , we have @xmath277 here , @xmath278 and @xmath279 since reward @xmath96 will be already collected at time @xmath280 .",
    "the estimated visit time @xmath281 is determined based on a tour @xmath282 that starts at point @xmath161 .",
    "now let us calculate the objective function for any other heading @xmath185 where @xmath283 .",
    "setting @xmath284 , @xmath285 since @xmath286 so that @xmath287 for all @xmath288 .",
    "the aggregated tour is determined over the set @xmath289 sarting at @xmath283 . by definition ,",
    "the target with the least travel cost from point @xmath149 is the active target @xmath96 and this is the first target in the tour .",
    "the rest of the tour consists of targets in @xmath290 starting at @xmath161 .",
    "let us call this tour @xmath291 .",
    "since in both tours @xmath282 and @xmath291 the starting point and the set of available targets are the same , the order of targets will be identical and we have @xmath292 .",
    "the visit times in @xmath282 are given by @xmath293 in @xmath291 , the visit time for target @xmath294 is : @xmath295 . for the rest of the targets , with @xmath296 , @xmath297 for all @xmath298 , we have @xmath299 and @xmath300 . by assumption , for all @xmath17",
    ", @xmath156 is non - increasing , therefore @xmath301 , and it follows that @xmath302 the right - hand - side above is @xmath303 and the left - hand - side is @xmath304 , so we have proved that for any @xmath283 , @xmath286 we have @xmath305._case 2 _ : @xmath306 . in this case , for any point @xmath270 we have a zero immediate reward .",
    "thus , only the rewards - to - go need to be compared .",
    "using , for any @xmath270 we know the aggregation tour @xmath307 for any point @xmath149 starts with target @xmath96 and the rest of it would also be the same .",
    "similarly , let us assume @xmath307 is the tour for @xmath308 and @xmath309 is the tour for any other point @xmath149 .",
    "the estimated visit times for @xmath307 are : @xmath310 and for @xmath309 : @xmath311 by the definition in , @xmath312 is on the shortest path from @xmath131 to @xmath161 , i.e. , @xmath313 .",
    "again , with @xmath156 being non - increasing we have @xmath314 , which implies @xmath315 .",
    "we have thus proved the lemma statement that the optimal heading of the agent is one of the direct headings towards an active target .",
    "< 1.5em -1.5em plus0em minus0.5em height0.4em width0.5em depth0.25em**proof of theorem [ mrcptheorem1 ] * * [ proof : mrcptheorem1 ] in the multi - agent mission , calculating the immediate reward and reward - to - go in and for each agent is like a one - agent mission limited to its own target subset @xmath169 .",
    "therefore , the result follows directly from lemma [ mrcplem2 ] .",
    "< 1.5em -1.5em plus0em minus0.5em height0.4em width0.5em depth0.25em**proof of theorem [ twotargettheorem ] * * [ proof : twotargettheorem ] we assume wlog that @xmath316 so that at time @xmath48 we have @xmath317 .",
    "this implies that target 1 is always an active target ( the travel cost of target 1 at time @xmath177 is equal to 0 ) . recalling and setting @xmath318",
    ", we have @xmath319 .",
    "this results in : @xmath320 from lemma [ mrcplem1 ] , target 2 is an active target if and only if @xmath321 .",
    "therefore , from , target 2 is an active target if and only if : @xmath322 which is rewritten as : @xmath323 we now consider two possible cases regarding target 2 .",
    "first , assume target 2 is not an active target , i.e. , @xmath324 starting with the trivial inequality : @xmath325\\nonumber\\ ] ] add @xmath326 $ ] to both sides and use to get : @xmath327>\\frac{\\lambda_{2}}{d_{2}}\\big[\\mathit{d}(\\mathcal{c}_{2,1},\\mathbf{y}_{1})\\big]>\\\\ &   \\frac{-\\lambda_{1}}{d_{1}}\\big[\\mathit{d}(\\mathcal{c}_{2,1},\\mathbf{y}_{2})\\big]+(\\frac{\\lambda_{2}}{d_{2}}-\\frac{\\lambda_{1}}{d_{1}})\\mathit{d}(\\mathcal{c}_{2,1},\\mathbf{y}_{1})\\big ] \\end{split}\\ ] ] adding the positive quantity of @xmath328 $ ] to both sides and invoking the triangle inequality : @xmath329\\\\ &   > ( \\frac{\\lambda_{2}}{d_{2}}-\\frac{\\lambda_{1}}{d_{1}})\\big[\\mathit{d}(\\mathcal{c}_{2,1},\\mathbf{y}_{2})\\big]+(\\frac{\\lambda_{2}}{d_{2}}-\\frac{\\lambda_{1}}{d_{1}})\\big[\\mathit{d}(\\mathcal{c}_{2,1},\\mathbf{y}_{1})\\big]\\\\ & > ( \\frac{\\lambda_{2}}{d_{2}}-\\frac{\\lambda_{1}}{d_{1}})\\mathit{d}(\\mathbf{y}_{1},\\mathbf{y}_{2 } ) \\end{split}\\ ] ] rearranging the last inequality and using results in : @xmath330+\\frac{\\lambda_{2}}{d_{2}}\\mathit{d}(\\mathbf{x},\\mathbf{y}_{2})\\\\ &   > \\frac{\\lambda_{1}}{d_{1}}\\mathit{d}(\\mathbf{x},\\mathbf{y}_{1})+\\frac{\\lambda_{2}}{d_{2}}\\big[\\mathit{d}(\\mathbf{x},\\mathbf{y}_{1})+\\mathit{d}(\\mathbf{y}_{2},\\mathbf{y}_{1})\\big ] \\end{split } \\label{finallemma3}\\ ] ] which is the same as implying that path @xmath331 * * is optimal , i.e. , the crh controller finds the optimal path .",
    "next , assume that target 2 is also an active target along with target 1 .",
    "let @xmath332 and @xmath333 be the headings for target 1 and 2 respectively , i.e. , @xmath334 and @xmath335 , the objective function of the crh controller under @xmath332 and @xmath333 is : @xmath336@xmath337 note that in order to evaluate the objective function for @xmath333 we find a tour starting at point @xmath338 which goes to the target with minimum travel cost .",
    "however , for target 2 to be active at @xmath48 it has to have the smallest travel cost at that point , which results in @xmath339 to be the reward of going to target 2 and then target 1 .",
    "we can see that using the reward of each path from and we can write : @xmath340 thus , the objective function of the crh controller under @xmath332 and @xmath333 is identical to the corresponding path rewards .",
    "hence , the crh controller selects the correct optimal heading at @xmath48 .",
    "< 1.5em -1.5em plus0em minus0.5em height0.4em width0.5em depth0.25em              c.  yao , x.  c. ding , and c.  cassandras , `` cooperative receding horizon control for multi - agent rendezvous problems in uncertain environments , '' in _ 49th ieee conference on decision and control ( cdc ) , 2010 _ , pp .",
    "4511 4516 , dec .",
    "2010 .",
    "h.  tang , e.  miller - hooks , and r.  tomastik , `` scheduling technicians for planned maintenance of geographically distributed equipment , '' _ transportation research part e : logistics and transportation review _ , vol .  43 , no .  5 , pp .  591  609 , 2007",
    ".    j.  s. bellingham , m.  tillerson , m.  alighanbari , and j.  p.",
    "how , `` cooperative path planning for multiple uavs in dynamic and uncertain environments , '' in _ ieee conference on decision and control ( cdc ) _ , pp .",
    "28162822 vol.3 , 10 - 13 december 2002 .",
    "w.  li and c.  g. cassandras , `` centralized and distributed cooperative receding horizon control of autonomous vehicle missions , '' _ mathematical and computer modelling _ , vol .",
    "43 , no .  9 , pp .",
    "12081228 , 2006 ."
  ],
  "abstract_text": [
    "<S> in previous work , a cooperative receding horizon ( crh ) controller was developed for solving cooperative multi - agent problems in uncertain environments . in this paper </S>",
    "<S> , we overcome several limitations of this controller , including potential instabilities in the agent trajectories and poor performance due to inaccurate estimation of a reward - to - go function . </S>",
    "<S> we propose an event - driven crh controller to solve the maximum reward collection problem ( mrcp ) where multiple agents cooperate to maximize the total reward collected from a set of stationary targets in a given mission space . </S>",
    "<S> rewards are non - increasing functions of time and the environment is uncertain with new targets detected by agents at random time instants . </S>",
    "<S> the controller sequentially solves optimization problems over a planning horizon and executes the control for a shorter action horizon , where both are defined by certain events associated with new information becoming available . </S>",
    "<S> in contrast to the earlier crh controller , we reduce the originally infinite - dimensional feasible control set to a finite set at each time step . </S>",
    "<S> we prove some properties of this new controller and include simulation results showing its improved performance compared to the original one . </S>"
  ]
}