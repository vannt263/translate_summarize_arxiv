{
  "article_text": [
    "many machine learning applications require dealing with data - sets having complex structures , e.g. natural language processing , image segmentation , reconstruction or captioning , pose estimation , protein folding prediction to name a few . @xcite .",
    "structured prediction problems pose a challenge for classic off - the - shelf learning algorithms for regression or binary classification .",
    "indeed , this has motivated the extension of methods such as support vector machines to structured problems @xcite .",
    "dealing with structured prediction problems is also a challenge for learning theory .",
    "while the theory of empirical risk minimization provides a very general statistical framework , in practice computational considerations make things more involved .",
    "indeed , in the last few years , an effort has been made to analyze specific structured problems , such as multiclass classification @xcite , multi - labeling @xcite , ranking  @xcite or quantile estimation @xcite .",
    "a natural question is then whether a unifying learning theoretic framework can be developed encompassing a wide range of problems as special cases .",
    "in this paper we take a step in this direction proposing and analyzing a regularization approach to a wide class of structured prediction problems defined by loss functions satisfying mild conditions . indeed , our starting observation is that a large class of loss functions naturally define an embedding of the structured outputs in a linear space .",
    "this fact allows to define a ( least squares ) surrogate loss and cast the problem in a multi - output regularized learning framework @xcite .",
    "the corresponding algorithm essentially reduces to a form of kernel ridge regression and generalizes the approach proposed in @xcite , hence providing also a novel derivation for this latter algorithm .",
    "our theoretical analysis allows to characterize the generalization properties of the proposed approach .",
    "in particular , it allows to quantify the impact due to the surrogate approach and establishes universal consistency as well as finite sample bounds .",
    "an experimental analysis shows promising results on a variety of structured prediction problems .",
    "the rest of this paper is organized as follows : in sec .",
    "[ sec : statement&algorithm ] we introduce the structured prediction problem in its generality and present our algorithm to approach it . in sec .  [ sec : surrogate ] we introduce and discuss a surrogate framework for structured prediction , from which we derive our algorithm . in sec .",
    "[ sec : analysis ] , we analyze the theoretical properties of the proposed algorithm . in sec .  [",
    "sec : connections ] we draw connections with previous work in structured prediction while in sec .",
    "[ sec : experiments ] we report a preliminary empirical analysis and comparison of the proposed approach .",
    "finally , sec .",
    "[ sec : conclusions ] concludes the paper outlining relevant directions for future research .",
    "the goal of supervised learning is to learn functional relations @xmath0 between two sets @xmath1 , given a finite number of examples .",
    "in particular in this work we are interested to _ structured prediction _ , namely the case where @xmath2 is a set of structured outputs ( such as histograms , graphs , time sequences , points on a manifold , etc . ) .",
    "moreover , structure on @xmath2 can be implicitly induced by a suitable loss @xmath3 ( such as edit distance , ranking error , geodesic distance , indicator function of a subset , etc . ) .",
    "then , the problem of structured prediction becomes @xmath4 and the goal is to find a good estimator for the minimizer of the above equation , given a finite number of ( training ) points @xmath5 sampled from a unknown probability distribution @xmath6 on @xmath7 . in the following we introduce an estimator @xmath8 to approach eq .",
    "( [ eq : expected_risk_minimization ] ) .",
    "the rest of this paper is devoted to prove that @xmath9 it a consistent estimator for a minimizer of eq .",
    "( [ eq : expected_risk_minimization ] ) .    [ [ our - algorithm - for - structured - prediction . ] ] our algorithm for structured prediction .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this paper we propose and analyze the following estimator @xmath10 given a positive definite kernel @xmath11 and training set @xmath5 . in the above expression , @xmath12 is @xmath13-th entry in @xmath14 , @xmath15 is the kernel matrix @xmath16 , @xmath17 the vector with entires @xmath18 , @xmath19 a regularization parameter and @xmath20 the identity matrix . from a computational perspective ,",
    "the procedure in  [ eq : algorithm ] is divided in two steps : a _ learning _",
    "step where input - dependents weights @xmath21 are computed ( which essentially consists in solving a kernel ridge regression problem ) and a _ prediction _ step where the @xmath12-weighted linear combination in [ eq : algorithm ] is optimized , leading to a prediction @xmath22 given an input @xmath23 .",
    "this idea was originally proposed in @xcite , where a `` score '' function @xmath24 was learned to estimate the `` likelihood '' of a pair @xmath25 sampled from @xmath6 , and then used in @xmath26 to predict the best @xmath27 given @xmath28 .",
    "this strategy was extended in @xcite for the popular _ svmstruct _ and adopted also in a variety of approaches for structured prediction @xcite .",
    "+    [ [ intuition . ] ] intuition .",
    "+ + + + + + + + + +    while providing a principled derivation of [ eq : algorithm ] for a large class of loss functions is a main contribution of this work , it is useful to first consider the special case where @xmath29 is induced by a reproducing kernel @xmath30 on the output set , such that @xmath31 this choice of @xmath29 was originally considered in _ kernel dependency estimation ( kde ) _  @xcite . in particular , for the case of normalized kernels ( i.e. @xmath32 ) , [ eq : algorithm ] essentially reduces to @xcite and recalling their derivation is insightful .",
    "note that , since a kernel can be written as @xmath33 , with @xmath34 a non - linear map into a feature space @xmath35  @xcite , then eq .",
    "( [ eq : ker_asm ] ) can be rewritten as @xmath36 directly minimizing the equation above with respect to @xmath37 is generally challenging due to the non linearity @xmath38 .",
    "a possibility is to replace @xmath39 by a function @xmath40 that is easier to optimize .",
    "we can then consider the regularized problem @xmath41 with @xmath42 a space of functions is the reproducing kernel hilbert space for vector - valued functions  @xcite with inner product @xmath43 @xmath40 of the form @xmath44 with @xmath45 and @xmath46 a reproducing kernel . indeed , in this case the solution to eq .",
    "( [ eq : kde ] ) is @xmath47 where the @xmath48 are the same as in [ eq : algorithm ] .",
    "since we replaced @xmath49 by @xmath50 , a natural question is how to recover an estimator @xmath9 from @xmath51 . in @xcite",
    "it was proposed to consider @xmath52 which corresponds to [ eq : algorithm ] when @xmath53 is a normalized kernel .",
    "the discussion above provides an intuition on how [ eq : algorithm ] is derived but raises also a few questions .",
    "first , it is not clear if and how the same strategy could be generalized to loss functions that do not satisfy eq .",
    "( [ eq : ker_asm ] ) .",
    "second , the above reasoning hinges on the idea of replacing @xmath9 with @xmath51 ( and then recovering @xmath9 by eq .",
    "( [ eq : decoding_kde ] ) ) , however it is not clear whether this approach can be justified theoretically .",
    "finally , we can ask what are the statistical properties of the resulting algorithm .",
    "we address the first two questions in the next section , while the rest of the paper is devoted to establish universal consistency and generalization bounds for algorithm  [ eq : algorithm ] .",
    "[ [ notation - and - assumptions . ] ] notation and assumptions .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + +    we introduce here some minimal technical assumptions that we will use throughout this work .",
    "we will assume @xmath54 and @xmath2 to be polish spaces , namely separable completely metrizable spaces equipped with the associated borel sigma - algebra .",
    "given a borel probability distribution @xmath6 on @xmath7 we denote with @xmath55 the associated conditional measure on @xmath2 ( given @xmath28 ) and with @xmath56 the marginal distribution on @xmath54 .",
    "to derive [ eq : algorithm ] we consider ideas from surrogate approaches @xcite and in particular @xcite .",
    "the idea is to tackle eq .",
    "( [ eq : expected_risk_minimization ] ) by substituting @xmath49 with a `` relaxation '' @xmath57 that is easy to optimize .",
    "the corresponding surrogate problem is @xmath58 and the question is how a solution @xmath59 for the above problem can be related to a minimizer @xmath60 of eq .",
    "( [ eq : expected_risk_minimization ] ) .",
    "this is made possible by the requirement that there exists a _",
    "@xmath61 , such that @xmath62 hold for all @xmath40 , where @xmath63 is such that @xmath64 for @xmath65 .",
    "indeed , given an estimator @xmath51 for @xmath59 , we can `` decode ''",
    "it considering @xmath66 and use the _ excess risk _",
    "@xmath67 to control @xmath68 via the comparison inequality in eq .",
    "( [ eq : comparison_init ] ) .",
    "in particular , if @xmath51 is a data - dependent predictor trained on @xmath69 points and @xmath70 when @xmath71 , we automatically have @xmath72 . moreover , if @xmath73 in eq .",
    "( [ eq : comparison_init ] ) is known explicitly , generalization bounds for @xmath51 are automatically extended to @xmath9 .",
    "provided with this perspective on surrogate approaches , here we revisit the discussion of sec .",
    "[ sec : statement&algorithm ] for the case of a loss function induced by a kernel @xmath53 . indeed , by assuming the surrogate @xmath74 , eq .",
    "( [ eq : kde ] ) becomes the empirical version of the surrogate problem at eq .",
    "( [ eq : surrogate ] ) and leads to an estimator @xmath51 of @xmath59 as in eq .",
    "( [ eq : kde_solution ] ) .",
    "therefore , the approach in @xcite to recover @xmath75 can be interpreted as the result @xmath76 of a suitable decoding of @xmath77 .",
    "an immediate question is whether the above framework satisfies eq .",
    "( [ eq : fisher_init ] ) and ( [ eq : comparison_init ] ) .",
    "moreover , we can ask if the same idea could be applied to more general loss functions .    in this work",
    "we identify conditions on @xmath29 that are satisfied by a large family of functions and moreover allow to design a surrogate framework for which we prove eq .",
    "( [ eq : fisher_init ] ) and ( [ eq : comparison_init ] ) .",
    "the first step in this direction is to introduce the following assumption .",
    "assumptionamain[assumption : main ] there exists a separable hilbert space @xmath35 with inner product @xmath78 , a continuous embedding @xmath34 and a bounded linear operator @xmath79 , such that @xmath80    is similar to eq .",
    "( [ eq : ker_asm2 ] ) and in particular to the definition of a reproducing kernel .",
    "note however that by not requiring @xmath81 to be positive semidefinite ( or even symmetric ) , we allow for a surprisingly wide range of functions .",
    "indeed , below we discuss some examples of functions that satisfy ( see appendix sec .",
    "[ sec : losses ] for more details ) :    [ example : general]_the following functions of the form @xmath3 satisfy : _    1 .   _",
    "any loss on @xmath2 of finite cardinality_. several problems belong to this setting , such as multi - class classification , multi - labeling , ranking , predicting graphs ( e.g. protein foldings ) .",
    "2 .   _ regression and classification loss functions _ : least - squares , logistic , hinge , @xmath82-insensitive , @xmath83-pinball .",
    "robust loss functions _ most loss functions used for _ robust estimation _",
    "@xcite such as the absolute value , huber , cauchy , german - mclure , `` fair '' and @xmath84 . see @xcite or the appendix for their explicit formulation .",
    "kde_. loss functions @xmath29 induced by a kernel such as in eq .",
    "( [ eq : ker_asm ] ) .",
    "distances on histograms / probabilities_. the @xmath85 and the squared hellinger distances .",
    "6 .   _ diffusion distances on manifolds_. the squared diffusion distance induced by the heat kernel ( at time @xmath86 ) on a compact reimannian manifold without boundary  @xcite .",
    "[ [ the - least - squares - loss - surrogate - framework . ] ] the least squares loss surrogate framework .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    implicitly defines the space @xmath35 similarly to eq .",
    "( [ eq : ker_asm2 ] ) .",
    "the following result motivates the choice of the least squares surrogate and moreover suggests a possible choice for the decoding .",
    "lemmalrelation[lm : relation_expected_risk ] let @xmath3 satisfy with @xmath34 bounded . then the expected risk in eq .",
    "( [ eq : expected_risk_minimization ] ) can be written as @xmath87 for all @xmath0 , where @xmath88 minimizes @xmath89    shows how eq .",
    "( [ eq : surrogate_ls ] ) arises naturally as surrogate problem .",
    "in particular , eq .",
    "( [ eq : relation_expected_risk ] ) suggests how to chose the decoding .",
    "indeed , consider an @xmath0 such that for each @xmath28 , @xmath90 is a minimizer of the argument in eq .",
    "( [ lm : relation_expected_risk ] ) , namely @xmath91 for all @xmath92 . then we have @xmath93 for any @xmath94 . following this observation , in this work we consider the decoding @xmath61 such that @xmath95 so that @xmath96 for all @xmath94 . indeed , for this choice of decoding , we have the following result .",
    "theoremtsurrogate[teo : surrogate ] let @xmath3 satisfy with @xmath2 a compact set .",
    "then , for every measurable @xmath40 and @xmath97 satisfying eq .",
    "( [ eq : decoding_function ] ) , the following holds @xmath98 with @xmath99 .",
    "shows that for all @xmath29 satisfying , the corresponding surrogate framework identified by the surrogate in eq .",
    "( [ eq : surrogate_ls ] ) and decoding eq .",
    "( [ eq : decoding_function ] ) satisfies fisher consistency and the comparison inequality in eq .",
    "( [ eq : comparison_inequality_ls ] ) .",
    "we recall that a finite set @xmath2 is always compact , and moreover , assuming the discrete topology on @xmath2 , we have that any @xmath34 is continuous",
    ". therefore , applies in particular to any structured prediction problem on @xmath2 with finite cardinality .    suggest to approach structured prediction by first learning @xmath51 and then decoding it to recover @xmath66 .",
    "a natural question is how to choose @xmath51 in order to compute @xmath9 in practice . in the rest of this section",
    "we propose an approach to this problem .",
    "[ [ derivation - for - eqalgorithm . ] ] derivation for [ eq : algorithm ] .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    minimizing @xmath100 in eq .",
    "( [ eq : surrogate_ls ] ) corresponds to a vector - valued regression problem @xcite . in this work",
    "we adopt an empirical risk minimization approach to learn @xmath51 as in eq .",
    "( [ eq : kde ] ) .",
    "the following result shows that combining @xmath51 with the decoding in eq .",
    "( [ eq : decoding_function ] ) leads to the @xmath9 in [ eq : algorithm ] .",
    "lemmapcomputable[prop : computable ] let @xmath3 satisfy with @xmath2 a compact set .",
    "let @xmath101 be the minimizer of eq .",
    "( [ eq : kde ] ) . then , for all @xmath28 @xmath102    concludes the derivation of [ eq : algorithm ] .",
    "an interesting observation is that computing @xmath9 does not require explicit knowledge of the embedding @xmath38 and the operator @xmath81 , which are implicitly encoded within the loss @xmath29 by . in analogy to the _ kernel trick _  @xcite we informally refer to such assumption as the `` loss trick '' .",
    "we illustrate this effect with an example .",
    "[ example : ranking ] in ranking problems the goal is to predict ordered sequences of a fixed number @xmath103 of labels . for these problems",
    ", @xmath2 corresponds to the set of all ordered sequences of @xmath103 labels and has cardinality @xmath104 , which is typically dramatically larger than the number @xmath69 of training examples ( e.g. for @xmath105 , @xmath106 ) .",
    "therefore , given an input @xmath28 , directly computing @xmath107 is impractical . on the opposite , the loss trick allows to express @xmath108 only in terms of the @xmath69 weights @xmath12 in [ eq : algorithm ] , making the computation of the @xmath109 easier to approach in general . for details on the rank loss @xmath110 and the corresponding optimization over @xmath2",
    "we refer to the empirical analysis of sec .",
    "[ sec : experiments ] .    in this section",
    "we have shown a derivation for the structured prediction algorithm proposed in this work . in we",
    "have shown how the expected risk of the proposed estimator @xmath9 is related to an estimator @xmath51 via a comparison inequality . in the following",
    "we will make use of these results to prove consistency and generalization bounds for alg .",
    "[ eq : algorithm ] .",
    "in this section we study the statistical properties of [ eq : algorithm ] .",
    "in particular we made use of the relation between the structured and surrogate problems via the comparison inequality in .",
    "we begin our analysis by proving that [ eq : algorithm ] is _ universally consistent_.    theoremtuniversal[teo : universal_consistency ] let @xmath3 satisfy , @xmath54 and @xmath2 be compact sets and @xmath111 a continuous universal reproducing kernel . ] . for any @xmath112 and any distribution @xmath6 on @xmath7",
    "let @xmath113 be obtained by [ eq : algorithm ] with @xmath5 training points independently sampled from @xmath6 and @xmath114 .",
    "then , @xmath115    shows that , when the @xmath29 satisfies , [ eq : algorithm ] approximates a solution @xmath60 to eq .",
    "( [ eq : expected_risk_minimization ] ) arbitrarily well , given a sufficient number of training examples . to the best of our knowledge",
    "this is the first consistency result for structured prediction in the general setting considered in this work and characterized by , in particular for the case of @xmath2 with infinite cardinality ( dense or discrete ) .    the _ no free lunch _",
    "theorem  @xcite states that it is not possible to prove uniform convergence rates for eq .",
    "( [ eq : universal_consistency ] ) . however , by imposing suitable assumptions on the regularity of @xmath59 it is possible to prove generalization bounds for @xmath51 and then , using , extend them to @xmath9 . to show this , it is sufficient to require that @xmath59 belongs to @xmath116 the reproducing kernel hilbert space used in the ridge regression of eq .",
    "( [ eq : kde ] ) .",
    "note that in the proofs of and , our analysis on @xmath51 borrows ideas from @xcite and extends their result to our setting for the case of @xmath35 infinite dimensional ( i.e. when @xmath2 has infinite cardinality ) .",
    "indeed , note that in this case @xcite can not be applied to the estimator @xmath51 considered in this work ( see appendix , for details ) .",
    "theoremtsimplebound[teo : simple_bound ] let @xmath3 satisfy , @xmath2 be a compact set and @xmath111 a bounded continuous reproducing kernel .",
    "let @xmath117 denote the solution of [ eq : algorithm ] with @xmath69 training points and @xmath118 . if the surrogate risk @xmath100 defined in eq .",
    "( [ eq : surrogate_ls ] ) admits a minimizer @xmath119 , then @xmath120 holds with probability @xmath121 for any @xmath122 , with @xmath123 a constant not depending on @xmath69 and @xmath83 .",
    "the bound in eq .",
    "( [ teo : simple_bound ] ) is of the same order of the generalization bounds available for the least squares binary classifier@xcite .",
    "indeed , in sec .",
    "[ sec : connections ] we show that in classification settings [ eq : algorithm ] reduces to least squares classification .",
    "[ rem : comparison ] the generalization bounds for the least squares classifier can be improved by imposing regularity conditions on @xmath6 via the _ tsybakov condition _",
    "this was observed in @xcite for binary classification with the least squares surrogate , where a tighter comparison inequality than the one in was proved .",
    "therefore , a natural question is whether the inequality of could be similarly improved , consequently leading to better rates for .",
    "promising results in this direction can be found in @xcite , where the tsybakov condition was generalized to the multi - class setting and led to a tight comparison inequality analogous to the one for the binary setting . however , this question deserves further investigation .",
    "indeed , it is not clear how the approach in @xcite could be further generalized to the case where @xmath2 has infinite cardinality .    in this paper we focused on a least squares surrogate loss function and corresponding framework .",
    "a natural question is to ask whether other loss functions could be considered to approach the structured prediction problem , sharing the same or possibly even better properties .",
    "this question is related also to , since different surrogate frameworks could lead to sharper comparison inequalities .",
    "this seems an interesting direction for future work .",
    "in this section we draw connections between [ eq : algorithm ] and previous methods for structured prediction learning .    [ [ binary - and - multi - class - classification . ] ] binary and multi - class classification .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    it is interesting to note that in classification settings , [ eq : algorithm ] corresponds to the least squares classifier  @xcite .",
    "indeed , let @xmath124 be a set of labels and consider the _ misclassification _ loss @xmath125 for @xmath126 and @xmath127 otherwise .",
    "then @xmath128 with @xmath129 the @xmath13-the element of the canonical basis of @xmath130 and @xmath131 , where @xmath20 is the @xmath132 identity matrix and @xmath133 the matrix with all entries equal to @xmath134 . in the notation of surrogate methods adopted in this work , @xmath135 and @xmath136 .",
    "note that both least squares classification and our approach solve the surrogate problem at eq .",
    "( [ eq : kde ] ) @xmath137 to obtain a vector - valued predictor @xmath138 as in eq .",
    "( [ eq : kde_solution ] ) .",
    "then , the least squares classifier @xmath139 and the decoding @xmath66 are respectively obtained by @xmath140 however , since @xmath131 , it is easy to see that @xmath141 for all @xmath28 .",
    "[ [ kernel - dependency - estimation . ] ] kernel dependency estimation",
    ". + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in we discussed the relation between kde  @xcite and [ eq : algorithm ] . in particular",
    ", we have observed that if @xmath29 is induced by a kernel @xmath142 as in eq .",
    "( [ eq : ker_asm ] ) and @xmath143 is normalized , i.e. @xmath144 , then algorithm eq .",
    "( [ eq : decoding_kde ] ) proposed in @xcite leads to the same predictor as [ eq : algorithm ] .",
    "therefore , we can apply and [ teo : simple_bound ] to prove universal consistency and generalization bounds for methods such as @xcite .",
    "we are not aware of previous results proving consistency ( and generalization bounds ) for the kde methods in @xcite .",
    "note however that when the kernel @xmath143 is not normalized , the `` decoding '' in eq .",
    "( [ eq : decoding_kde ] ) is not equivalent to [ eq : algorithm ] . in particular , given the surrogate solution @xmath59 , applying eq .",
    "( [ eq : decoding_kde ] ) leads to predictors that are do not minimize eq .",
    "( [ eq : expected_risk_minimization ] ) . as a consequence",
    "the approach in @xcite is not consistent in the general case .",
    "[ [ support - vector - machines - for - structured - output . ] ] support vector machines for structured output .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a popular approach to structured prediction is the _ support vector machine for structured outputs ( svmstruct ) _",
    "@xcite that extends ideas from the well - known svm algorithm to the structured setting .",
    "one of the main advantages of svmstruct is that it can be applied to a variety of problems since it does not impose strong assumptions on the loss . in this view , our approach , as well as kde , shares similar properties , and in particular allows to consider @xmath2 of infinite cardinality .",
    "moreover , we note that generalization studies for svmstruct are available @xcite ( ch .",
    "@xmath145 ) .",
    "however , it seems that these latter results do not allow to derive universal consistency of the method .",
    ".comparison of ranking methods on the movielens dataset  @xcite with the normalized rank loss @xmath110 . [ cols=\">,^\",options=\"header \" , ]     [ [ robust - estimation . ] ] robust estimation .",
    "+ + + + + + + + + + + + + + + + + +    we considered a regression problems with many outliers and evaluated [ eq : algorithm ] using the cauchy loss ( see example  [ example : general ] - ( 3 ) ) for robust estimation .",
    "indeed , in this setting , @xmath146\\subset{\\mathbb{r}}$ ] is not structured , but the non - convexity of @xmath29 can be an obstacle to the learning process .",
    "we generated a dataset according to the model @xmath147 , where @xmath23 was sampled uniformly on @xmath148 $ ] and @xmath82 according to a zero - mean gaussian with variance @xmath149 .",
    "@xmath150 modeled the outliers and was sampled according to a zero - mean random variable that was @xmath127 with probability @xmath151 and a value uniformly at random in @xmath152 $ ] with probability @xmath149 ) .",
    "we compared [ eq : algorithm ] with the _ nadaraya - watson _ robust estimator ( rnw )  @xcite and kernel ridge regression ( krr ) with a gaussian kernel as baseline . to train [ eq : algorithm ]",
    "we used a gaussian kernel on the input and performed predictions ( i.e. solved eq .",
    "( [ eq : algorithm_with_weights ] ) ) using matlab fminunc function for unconstrained minimization .",
    "experiments were performed with training sets of increasing dimension ( @xmath153 repetitions each ) and test set of @xmath154 examples . @xmath155-fold cross - validation for model selection .",
    "results are reported in fig .",
    "[ fig : robust ] , showing that our estimator significantly outperforms the others .",
    "moreover , our method appears to greatly benefit from training sets of increasing size .",
    "in this work we considered the problem of structured prediction from a statistical learning theory perspective .",
    "we proposed a learning algorithm for structured prediction that is split into a learning and prediction step similarly to previous methods in the literature .",
    "we studied the statistical properties of the proposed algorithm by adopting a strategy inspired to surrogate methods .",
    "in particular , we identified a large family of loss functions for which it is natural to identify a corresponding surrogate problem .",
    "this perspective allows to prove a derivation of the algorithm proposed in this work . moreover , by exploiting a comparison inequality relating the original and surrogate problems we were able to prove universal consistency and generalization bounds under mild assumption .",
    "in particular , the bounds proved in this work recover those already known for least squares classification , of which our approach can be seen as a generalization .",
    "we supported our theoretical analysis with experiments showing promising results on a variety of structured prediction problems .",
    "a few questions were left opened .",
    "first , we ask whether the comparison inequality can be improved ( under suitable hypotheses ) to obtain faster generalization bounds for our algorithm .",
    "second , the surrogate problem in our work consists of a vector - valued regression ( in a possibly infinite dimensional hilbert space ) , we solved this problem by plain kernel ridge regression but it is natural to ask whether approaches from the multi - task learning literature could lead to substantial improvements in this setting . finally , an interesting question is whether alternative surrogate frameworks could be derived for the setting considered in this work , possibly leading to tighter comparison inequalities",
    ". we will investigate these questions in the future .",
    "the appendix of this work is divided in the following three sections :    1 .",
    "proofs of fisher consistency and comparison inequality ( ) .",
    "2 .   universal consistency and generalization bounds for [ eq : algorithm ] . ( and [ teo : simple_bound ] ) .",
    "3 .   the characterization of a large family of @xmath29s satisfying ( ) .",
    "in the following we will always assume @xmath54 and @xmath2 to be polish spaces , namely separable complete metrizable spaces , equipped with the associated borel sigma - algebra .",
    "when referring to a probability distribution @xmath6 on @xmath156 we will always assume it to be a borel probability measure , with @xmath157 the marginal distribution on @xmath54 and @xmath55 the conditional measure on @xmath2 given @xmath158 .",
    "we recall @xcite that @xmath159 is a regular conditional distribution and its domain , which we will denote @xmath160 in the following , is a measurable set contained in the support of @xmath157 and corresponds to the support of @xmath157 up to a set of measure zero .",
    "for convenience , we recall here the main assumption of our work .",
    "[ [ basic - notation ] ] basic notation + + + + + + + + + + + + + +    we recall that a hilbert space @xmath161 is a vector space with inner product @xmath162 , closed with respect to the norm @xmath163 for any @xmath164 .",
    "we denote with @xmath165 the lebesgue space of square integrable functions on @xmath54 with respect to a measure @xmath157 and with values in a separable hilbert space @xmath161 .",
    "we denote with @xmath166 the inner product @xmath167 , for all @xmath168 . in particular when @xmath169 we denote with @xmath170 the space @xmath171 .    given a linear operator @xmath172 between two hilbert spaces @xmath173",
    ", we denote with @xmath174 the trace of @xmath81 and with @xmath175 the adjoint operator associated to @xmath81 , namely such that @xmath176 for every @xmath164 , @xmath177 .",
    "moreover , we denote with @xmath178 and @xmath179 respectively the operator norm and hilbert - schmidt norm of @xmath81 .",
    "we recall that a linear operator @xmath81 is continuous if and only if @xmath180 and we denote @xmath181 the set of all continuous linear operators from @xmath161 to @xmath182 .",
    "moreover , we denote @xmath183 the set of all operators @xmath172 with @xmath184 and recall that @xmath183 is isometric to the space @xmath185 , with @xmath186 denoting the tensor product .",
    "indeed , for the sake of simplicity , with some abuse of notation we will not make the distinction between the two spaces .    note that in most of our results we will require @xmath2 to be non - empty and compact , so that a continuous functional over @xmath2 always attains a minimizer on @xmath2 and therefore the operator @xmath187 is well defined .",
    "note that for a finite set @xmath2 , we will always assume it endowed with the discrete topology , so that @xmath2 is compact and any function @xmath3 is continuous .    [",
    "[ on - the - argmin ] ] on the argmin + + + + + + + + + + + + +    notice that for simplicity of notation , in the paper we denoted the minimizer of [ eq : algorithm ] as @xmath188 however note that the correct notation should be @xmath189 since a loss function @xmath29 can have more than one minimizer in general . in",
    "the following we keep this more pedantic , yet correct notation .",
    "[ [ expected - risk - minimization ] ] expected risk minimization + + + + + + + + + + + + + + + + + + + + + + + + + +    note that whenever we write an expected risk minimization problem , we implicitly assume the optimization domain to be the space of measurable functions . for instance , eq .",
    "( [ eq : expected_risk_minimization ] ) would be written more rigorously as @xmath190    in the next lemma , following @xcite we show that the problem in eq .",
    "( [ eq : expected_risk_minimization ] ) admits a measurable pointwise minimizer .",
    "[ lemma : solution - structured - risk ] let @xmath191 be a continuous function .",
    "then , the expected risk minimization at eq .",
    "( [ eq : expected_risk_minimization ] ) admits a measurable minimizer @xmath192 such that @xmath193 for every @xmath194 .",
    "moreover , the function @xmath195 defined as follows , is measurable    @xmath196 m(x ) = _ y r(x , y ) , r(x , y ) = \\ {    cc _ ( y , y)d(y|x ) & x + 0 &    .",
    "since @xmath29 is continuous and @xmath159 is a regular conditional distribution , then @xmath197 is a carathodory function ( see definition @xmath198 ( pp .",
    "@xmath199 ) of @xcite ) , namely continuous in @xmath200 for each @xmath28 and measurable in @xmath23 for each @xmath92 .",
    "thus , by theorem @xmath201 ( pp . @xmath202 ) of @xcite ( or aumann s measurable selection principle @xcite ) , we have that @xmath203 is measurable and that there exists a measurable @xmath204 such that @xmath205 for all @xmath28 .",
    "moreover , by definition of @xmath203 , given any measurable @xmath206 , we have @xmath207 .",
    "therefore , @xmath208 we conclude @xmath209 and , since @xmath60 is measurable , @xmath210 and @xmath60 is a global minimizer .",
    "we have an immediate corollary to .",
    "[ cor : solution - surrogate - risk ] with the hypotheses of , let @xmath211 such that @xmath212 for almost every @xmath213",
    ". then @xmath214 .",
    "the result follows directly from by noting that @xmath215 almost everywhere on @xmath160 .",
    "hence , since @xmath160 is equal to the support of @xmath157 up to a set of measure zero , @xmath216 .    with the above basic notation and results , we can proceed to prove the results presented in this work .",
    "in this section we focus on the surrogate framework introduced in sec .",
    "[ sec : surrogate ] and prove that it is _ fisher consistent _ and that the _ comparison inequality_. to do so , we will first characterizes the solution(s ) of the surrogate expected risk minimization introduced at eq .",
    "( [ eq : surrogate_ls ] ) .",
    "we recall that in our setting , the surrogate risk was defined as the functional @xmath217 , where @xmath34 is continuous ( by ) . in the following ,",
    "when @xmath38 is bounded , we will denote with @xmath218 .",
    "note that in most our results we will assume @xmath2 to be compact . in these settings",
    "we always have @xmath219 by the continuity of @xmath38 .",
    "we start with a preliminary lemma necessary to prove and .",
    "[ lemma : surrogate - problem - sol ] let @xmath35 a separable hilbert space and @xmath34 measurable and bounded .",
    "then , the function @xmath88 such that @xmath220 and @xmath221 otherwise , belongs to @xmath222 and is a minimizer of the surrogate expected risk at eq .",
    "( [ eq : surrogate_ls ] ) . moreover , any minimizer of eq .",
    "( [ eq : surrogate_ls ] ) is equal to @xmath59 almost everywhere on the domain of @xmath157 .    by hypothesis ,",
    "@xmath223 is measurable and bounded .",
    "therefore , since @xmath159 is a regular conditional probability , we have that @xmath59 is measurable on @xmath54 ( see for instance @xcite ) .",
    "moreover , the norm of @xmath59 is dominated by the constant function of value @xmath224 , thus @xmath59 is integrable on @xmath54 with respect to @xmath157 and in particular it is in @xmath225 since @xmath157 is a finite regular measure . recall that since @xmath159 is a regular conditional distribution , for any measurable @xmath40 , the functional in eq .",
    "( [ eq : surrogate_ls ] ) can be written as @xmath226 notice that @xmath227 almost everywhere on @xmath160 .",
    "indeed , @xmath228 for all @xmath229 , which is minimized by @xmath230 for all @xmath229 .",
    "therefore , since @xmath160 is equal to the support of @xmath157 up to a set of measure zero , we conclude that @xmath231 and , since @xmath59 is measurable , @xmath232 and @xmath59 is a global minimizer as required .    finally ,",
    "notice that for any @xmath40 we have @xmath233 therefore , for any measurable minimizer @xmath234 of the surrogate expected risk at eq .",
    "( [ eq : surrogate_ls ] ) , we have @xmath235 which , by the relation above , implies @xmath236 a.e . on @xmath160 .    by we know that @xmath237 almost everywhere on @xmath160 and is the minimizer of @xmath238 .",
    "therefore we have @xmath239 for almost every @xmath229 .",
    "thus , for any measurable function @xmath206 we have @xmath240    for the sake of clarity , the result for the fisher consistency and the comparison inequality are proven respectively in , .",
    "the two results are proven below .",
    "theoremprelation[proposition : fisher_consistency ] let @xmath3 satisfy with @xmath2 a compact set .",
    "let @xmath88 be a minimizer of the surrogate problem at eq .",
    "( [ eq : surrogate_ls ] ) .",
    "then , for any decoding @xmath61 satisfying eq .",
    "( [ eq : decoding_function ] ) @xmath241    it is sufficient to show that @xmath242 satisfies eq .",
    "( [ eq : solution_expected_risk ] ) almost everywhere on @xmath160 .",
    "indeed , by directly applying we have @xmath243 as required .",
    "we recall that a mapping @xmath97 is a decoding for our surrogate framework if it satisfies eq .",
    "( [ eq : decoding_function ] ) , namely @xmath244 by we know that @xmath237 almost everywhere on @xmath160 .",
    "therefore , we have @xmath239 for almost every @xmath229 . as a consequence , for any @xmath61 satisfying eq .",
    "( [ eq : decoding_function ] ) , we have @xmath245 almost everywhere on @xmath160 .",
    "we are therefore in the hypotheses of with @xmath246 , as desired .",
    "the fisher consistency of the surrogate problem allows to prove the comparison inequality ( ) between the excess risk of the structured prediction problem , namely @xmath247 , and the excess risk @xmath248 of the surrogate problem .",
    "however , before showing such relation , in the following result we prove that for any measurable @xmath40 and measurable decoding @xmath61 , the expected risk @xmath249 is well defined .",
    "[ lm : exp - risk - well - def ] let @xmath2 be compact and @xmath3 satisfying .",
    "let @xmath40 be measurable and @xmath61 a measurable decoding satisfying eq .",
    "( [ eq : decoding_function ] ) .",
    "then @xmath249 is well defined and moreover @xmath250 .",
    "@xmath251 is measurable in both @xmath23 and @xmath200 since @xmath29 is continuous and @xmath252 is measurable by hypothesis ( combination of measurable functions ) .",
    "now , @xmath29 is pointwise bounded by @xmath253 since @xmath254 hence , by theorem @xmath255 pp .",
    "@xmath256 in @xcite the integral of @xmath251 exists and therefore @xmath257    a question introduced by is whether a _",
    "measurable _ decoding always exists .",
    "the following result guarantees that , under the hypotheses introduced in this work , a decoding @xmath61 satisfying eq .",
    "( [ eq : decoding_function ] ) always exists .",
    "let @xmath2 be compact and @xmath34 and @xmath79 satisfy the requirements in .",
    "define @xmath258 as @xmath259 then , @xmath203 is measurable and there exists a measurable decoding @xmath61 satisfying eq .",
    "( [ eq : decoding_function ] ) , namely such that @xmath260 for each @xmath261 .    similarly to the proof of",
    ", the result is a direct application of theorem @xmath201 ( pp .",
    "@xmath202 ) of @xcite ( or aumann s measurable selection principle @xcite ) .",
    "we now prove the _ comparison inequality _ at eq .",
    "( [ eq : comparison_inequality_ls ] ) .",
    "theoremtcomparison[teo : comparison_inequality ] let @xmath3 satisfy with @xmath2 a compact or finite set .",
    "let @xmath204 and @xmath88 be respectively solutions to the structured and surrogate learning problems at eq .",
    "( [ eq : expected_risk_minimization ] ) and eq .",
    "( [ eq : surrogate_ls ] ) .",
    "then , for every measurable @xmath40 and @xmath97 satisfying eq .",
    "( [ eq : decoding_function ] ) @xmath262    let us denote @xmath263 and @xmath264 .",
    "[ proposition : fisher_consistency ] we have that @xmath265 and so @xmath266 .",
    "now , by combining with , we have @xmath267 where @xmath268 now , the term a can be minimized by taking the supremum over @xmath2 so that @xmath269 for b , we observe that , by the definition of the decoding @xmath270 , we have @xmath271 for all @xmath28l",
    ". therefore , @xmath272 where we have used the fact that for any given two functions @xmath273 we have @xmath274 therefore , by combining the bounds on @xmath275 and @xmath276 we have @xmath277 where for the last inequality we have used the jensen s inequality . the proof is concluded by recalling that ( see eq .",
    "( [ eq : equation_excess_ls_risk ] ) ) @xmath278",
    "in this section we focus on the analysis of the structured prediction algorithm proposed in this work ( [ eq : algorithm ] ) .",
    "in particular , we will first prove that , given the minimizer @xmath101 of the empirical risk at eq .",
    "( [ eq : kde ] ) , its decoding can be computed in practice according to [ eq : algorithm ] .",
    "then , we report the proofs for the universal consistency of such approach ( ) and generalization bounds ( ) .",
    "let @xmath111 a positive semidefinite function on @xmath54 , we denote @xmath279 the hilbert space obtained by the completion @xmath280 according to the norm induced by the inner product @xmath281 .",
    "spaces @xmath279 constructed in this way are known as _ reproducing kernel hilbert spaces _ and there is a one - to - one relation between a kernel @xmath46 and its associated rkhs . for more details on rkhs",
    "we refer the reader to @xcite . given a kernel @xmath46 , in the following we will denote with @xmath282 the feature map @xmath283 for all @xmath28 .",
    "we say that a kernel is bounded if @xmath284 with @xmath285 .",
    "note that @xmath46 is bounded if and only if @xmath286 for every @xmath287 . in the following",
    "we will always assume @xmath46 to be continuous and bounded by @xmath285 .",
    "the continuity of @xmath46 with the fact that @xmath54 is polish implies @xmath279 to be separable @xcite .",
    "we introduce here the ideal and empirical operators that we will use in the following to prove the main results of this work .",
    "* @xmath288 s.t .",
    "@xmath289 , with adjoint * @xmath290 s.t .",
    "@xmath291 , * @xmath292 s.t .",
    "@xmath293 , with adjoint * @xmath294 s.t .",
    "@xmath295 , * @xmath296 and @xmath297 ,    with @xmath237 defined according to eq .",
    "( [ eq : g_average ] ) , ( see ) .",
    "given a set of input - output pairs @xmath5 with @xmath298 independently sampled according to @xmath6 on @xmath7 , we define the empirical counterparts of the operators just defined as    * @xmath299 s.t .",
    "@xmath300 , with adjoint * @xmath301 s.t .",
    "@xmath302 , * @xmath303 s.t .",
    "@xmath304 , with adjoint * @xmath305 s.t .",
    "@xmath306 , * @xmath307 and @xmath308 is the empirical kernel matrix .    in the rest of this section",
    "we denote with @xmath309 , the operator @xmath310 , for any symmetric linear operator @xmath275 , @xmath311 and @xmath20 the identity operator .",
    "we recall here a basic result characterizing the operators introduced above .",
    "[ prop : basic_operator_result ] with the notation introduced above , @xmath312 where @xmath186 denotes the tensor product .",
    "moreover , when @xmath73 and @xmath38 are bounded by respectively @xmath313 and @xmath314 , we have the following facts    a.   @xmath315 b.   @xmath316 .",
    "by definition of @xmath317 , for each @xmath318 we have @xmath319 since @xmath320 is the operator such that @xmath321 .",
    "the characterization for @xmath322 is analogous .",
    "now , @xmath323 .",
    "the relation @xmath324 holds by definition .",
    "moreover @xmath325 by linearity of the trace .",
    "@xmath326 is analogous .",
    "note that @xmath327 . by since @xmath38 is bounded by hypothesis .",
    "we begin our analysis by introducing the concept of reproducing kernel hilbert space ( rkhs ) for vector - valued functions . here",
    "we provide a brief summary of the main properties that will be useful in the following .",
    "we refer the reader to @xcite for a more in - depth introduction on the topic .",
    "analogously to the case of scalar functions , a rkhs for vector - valued functions @xmath328 , with @xmath161 a separable hilbert space , is uniquely characterized by a so - called _ kernel of positive type _ , which is an operator - valued @xmath329 generalizing the concept of scalar reproducing kernel .",
    "let @xmath54 be a set and @xmath161 be a hilbert space , then @xmath329 is a _ kernel of positive type _ if for each @xmath112 , @xmath330 , @xmath331 we have @xmath332    a kernel of positive type @xmath333 defines an inner product @xmath334 on the space @xmath335 then , the completion @xmath336 with respect to the norm induced by @xmath337 is known as the reproducing kernel hilbert space ( rkhs ) for vector - valued functions associated to the kernel @xmath333 .",
    "indeed , we have that a reproducing property holds also for rkhs of vector - valued functions , namely for any @xmath28 , @xmath338 and @xmath339 we have@xmath340 and that for each @xmath28 the function @xmath341 is the evaluation functional in @xmath23 on @xmath42 , namely @xmath342 .      in this work",
    "we restrict to the special case of rkhs for vector - valued functions with associated kernel @xmath343 of the form @xmath344 for each @xmath287 , where @xmath111 is a scalar reproducing kernel and @xmath345 is the identity operator on @xmath35 .",
    "notice that this choice is not restrictive in terms of the space of functions that can be learned by our algorithm .",
    "indeed , it was proven in @xcite ( see example 14 ) that if @xmath46 is a universal scalar kernel , then @xmath346 is universal .",
    "below , we report a useful characterization of rkhs @xmath42 associated to a separable kernel .",
    "lemmalhxy[lemma : hxy_bis ] the rkhs @xmath42 associated to the kernel @xmath347 is isometric to @xmath348 and for each @xmath339 there exists a unique @xmath349 such that @xmath350    we explicitly define the isometry @xmath351 as the linear operator such that @xmath352 for each @xmath112 , @xmath330 and @xmath353 . by construction , @xmath354 , with @xmath355 the linear space defined at eq .",
    "( [ eq : construction_of_rkhsvv ] ) and moreover @xmath356 implying that @xmath355 is isometrically contained in @xmath348 . since @xmath348 is complete , also @xmath357 .",
    "therefore @xmath42 is isometrically contained in @xmath348 , since @xmath358 .",
    "moreover note that @xmath359 from which we conclude that @xmath42 is isometric to @xmath348 via @xmath360 .    to prove eq .",
    "( [ eq : evaluation_hs ] ) , let us consider @xmath28 and @xmath339 with @xmath361 .",
    "then , @xmath362 we have that @xmath363 since the equation above is true for each @xmath364 we can conclude that @xmath365 as desired .",
    "the isometry @xmath366 allows to characterize the closed form solution for the surrogate risk introduced in eq .",
    "( [ eq : surrogate_ls ] ) .",
    "we recall that in , we have shown that @xmath100 always attains a minimizer on @xmath222 . in the following",
    "we show that if @xmath100 attains a minimum on @xmath366 , we are able to provide a close form solution for one such element .",
    "[ lemma : expected_risk_solution ] let @xmath38 and @xmath35 satisfying and assume that the surrogate expected risk minimization of @xmath367 at eq .",
    "( [ eq : surrogate_ls ] ) attains a minimum on @xmath42 , with @xmath368 .",
    "then the minimizer @xmath369 of @xmath100 with minimal norm @xmath370 is of the form @xmath371    by hypothesis we have @xmath119 .",
    "therefore , by applying lemma  [ lemma : hxy_bis ] we have that there exists a unique linear operator @xmath372 such that @xmath373 .",
    "now , expanding the least squares loss on @xmath35 , we obtain @xmath374 where we have used and the linearity of the trace .",
    "therefore @xmath100 is a quadratic functional , and is convex since @xmath375 is positive semidefinite .",
    "we can conclude that @xmath100 attains a minimum on @xmath42 if and only if the range of @xmath376 is contained in the range of @xmath375 , namely @xmath377 ( see @xcite chap .",
    "2 ) . in this case",
    "@xmath378 exists and is the minimum norm minimizer for @xmath100 , as desired .",
    "analogously to , a closed form solution exists for the regularized surrogate empirical risk minimization problem introduced in eq .",
    "( [ eq : kde ] ) .",
    "we recall that the associated functional is @xmath379 defined as @xmath380 for all @xmath339 and @xmath5 points in @xmath7 .",
    "the following result characterizes the closed form solution for the empirical risk minimization when @xmath366 and guarantees that such a solution always exists .",
    "[ lemma : empirical_risk_solution ] let @xmath366 .",
    "for any @xmath381 , the solution @xmath382 of the empirical risk minimization problem at eq .",
    "( [ eq : kde ] ) exists , is unique and is such that @xmath383    the proof of is analogous to that of and we omit it .",
    "note that , since @xmath384 is always bounded for @xmath385 , its range corresponds to @xmath279 and therefore the range of @xmath376 is always contained in it .",
    "then @xmath386 exists for any @xmath385 and is unique since @xmath387 is strictly convex .",
    "the closed form solutions provided by and will be key in the analysis of the structured prediction algorithm [ eq : algorithm ] in the following .",
    "in this section we prove that [ eq : algorithm ] corresponds to the decoding of the surrogate empirical risk minimizer @xmath51 ( eq .  ( [ eq : kde ] ) ) via a map @xmath388 satisfying eq .",
    "( [ eq : decoding_function ] ) .    recall that in we proved that the vector - valued rkhs @xmath42 induced by a kernel @xmath344 , for a scalar kernel @xmath46 on @xmath54 , is isometric to @xmath348 . for the sake of simplicity , in the following , with some abuse of notation , we will not make the distinction between @xmath42 and @xmath348 when it is clear from context .    from",
    "we know that @xmath389 for all @xmath28 .",
    "recall that @xmath390 and @xmath391 , is the empirical kernel matrix associated to the inputs , namely such that @xmath392 for each @xmath393 .",
    "therefore we have @xmath394 .",
    "now , by denoting @xmath395 , we have @xmath396 therefore , by applying the definition of the operator @xmath303 , we have @xmath397 by plugging @xmath77 in the functional minimized by the decoding ( eq .  ( [ eq : decoding_function ] ) ) , @xmath398 where we have used the bilinear form for @xmath29 in for the final equation .",
    "we conclude that @xmath399 as required .",
    "focuses on the computational aspects of [ eq : algorithm ] . in the following",
    "we will analyze its statistical properties .      in following ,",
    "we provide a probabilistic bound on the excess risk @xmath400 for any @xmath401 that will be key to prove both universal consistency ( ) and generalization bounds ( ) . to do so , we will make use of the comparison inequality from to control the structured excess risk by means of the excess risk of the surrogate @xmath248 .",
    "note that the surrogate problem consists in a vector - valued kernel ridge regression estimation . in this",
    "setting , the problem of finding a probabilistic bound has been studied ( see @xcite and references therein ) .",
    "indeed , our proof will consist of a decomposition of the surrogate excess risk that is similar to the one in @xcite",
    ". however , note that we can not direct apply @xcite to our setting , since in @xcite the operator - valued kernel @xmath333 associated to @xmath42 is required to be such that @xmath402 is trace class @xmath403 , which does not hold for the kernel used in this work , namely @xmath347 when @xmath35 is infinite dimensional .    in order to express the bound on the excess risk more compactly , here",
    "we introduce a measure for the approximation error of the surrogate problem . according to @xcite , we define the following quantity @xmath404    [ lemma : prob - bound ] let @xmath2 be compact , @xmath3 satisfying and @xmath46 a bounded positive definite kernel on @xmath54 with @xmath405 and associated rkhs @xmath279 .",
    "let @xmath6 a borel probability measure on @xmath7 and @xmath5 independently sampled according to @xmath6 .",
    "let @xmath60 be a solution of the problem in eq .",
    "( [ eq : expected_risk_minimization ] ) , @xmath66 as in [ eq : algorithm ] .",
    "then , for any @xmath406 and @xmath407 , the following holds with probability @xmath408 : @xmath409 with @xmath410 .    according to @xmath411 from we know that @xmath412 for all @xmath28 , with @xmath413 . by , we know that @xmath414 almost everywhere on the support of @xmath157 .",
    "therefore , a direct application of leads to @xmath415 to bound @xmath416 , we proceed with a decomposition similar to the one in @xcite . in particular @xmath417 , with @xmath418 let @xmath419 .",
    "now , for the term @xmath420 , we have @xmath421 to control the term @xmath422 , note that @xmath423 with @xmath424 the random variable @xmath425 . by , for any @xmath426",
    "we have @xmath427 and @xmath428 almost surely on the support of @xmath6 on @xmath7 , and so @xmath429 .",
    "thus , by applying lemma  2 of @xcite , we have @xmath430 with probability @xmath431 , since @xmath432 . to control @xmath433 we proceed by recalling that @xmath434 and that for any @xmath19 @xmath435 and @xmath436 .",
    "we have @xmath437 to control @xmath438 , note that @xmath439 where @xmath424 is the random variable defined as @xmath440 for @xmath426 .",
    "note that @xmath441 , @xmath442 almost surely and so @xmath443 for @xmath426 .",
    "thus we can again apply lemma  2 of @xcite , obtaining @xmath444 with probability @xmath445 .",
    "thus , by performing an intersection bound , we have @xmath446 with probability @xmath447 .",
    "the term @xmath448 can be controlled as follows @xmath449 where we have used the fact that for two invertible operators @xmath275 and @xmath276 we have @xmath450 .",
    "now , by controlling @xmath451 as for @xmath420 and performing an intersection bound , we have @xmath452 with probability @xmath447 . finally the term @xmath453 is equal to @xmath454 where @xmath20 denotes the identity operator .",
    "thus , by performing an intersection bound of the events for @xmath420 and @xmath448 , we have @xmath455 with probability @xmath456 . since @xmath457 we obtain the desired bound .",
    "now we are ready to give the universal consistency result .    by applying ,",
    "we have @xmath409 with probability @xmath458 .",
    "note that , since @xmath317 , @xmath459 and @xmath460 , we have @xmath461 therefore @xmath462 now by choosing @xmath463 , we have @xmath464 with probability @xmath465 .",
    "now we study @xmath466 , let @xmath467 be the eigendecomposition of the compact operator @xmath468 , with @xmath469 for @xmath470 and @xmath471 .",
    "now , let @xmath472 for @xmath473 .",
    "we need to prove that @xmath474 is a basis for @xmath170 .",
    "let @xmath475 be the support of @xmath157 , note that @xmath476 is compact and polish since it is closed and subset of the compact polish space @xmath54 .",
    "let @xmath477 be the rkhs defined by @xmath478 , with the same inner product of @xmath279 .",
    "by the fact that @xmath476 is a compact polish space and @xmath46 is continuous , then @xmath477 is separable . by the universality of @xmath46 we have",
    "that @xmath477 is dense in @xmath479 , and , by corollary 5 of @xcite , we have @xmath480 .",
    "thus , since @xmath479 is dense in @xmath170 , we have @xmath474 is a basis of @xmath170 .",
    "thus @xmath481 .",
    "therefore @xmath482 let @xmath483 , and @xmath484 .",
    "for any @xmath112 we have @xmath485 since @xmath486 .",
    "we recall that @xmath468 is a trace class operator , namely @xmath487 .",
    "therefore @xmath488 for @xmath489 , from which we conclude @xmath490 now , for any @xmath112 , let @xmath491 and @xmath492 be the event associated to the equation @xmath493 by , we know that the probability of @xmath492 is at most @xmath494 . since @xmath495 , we can apply the borel - cantelli lemma ( theorem 8.3.4 .",
    "pag 263 of @xcite ) on the sequence @xmath496 and conclude that the statement @xmath497 holds with probability @xmath127 .",
    "thus , the converse statement @xmath498 holds with probability @xmath134 .",
    "finally , we show that under the further hypothesis that @xmath59 belongs to the rkhs @xmath366 , we are able to prove generalization bounds for the structured prediction algorithm .    by applying [ lemma : prob - bound ] , we have @xmath409 with probability @xmath458 . by assumption , @xmath119 and",
    "therefore there exists a @xmath499 such that @xmath500 this implies that @xmath501 since , by definition of @xmath502 and @xmath503 , for any @xmath504 , @xmath505 thus , since @xmath506 and @xmath507 and @xmath508 for any @xmath385 , we have @xmath509 moreover , since @xmath434 , we have @xmath510 now , let @xmath463 , we have @xmath511 with probability @xmath512 , where we have set @xmath513 and @xmath514 to obtain the desired inequality .",
    "in this section we prove to show that a wide range of functions @xmath3 useful for structured prediction learning satisfies the loss trick ( ) . in the following we state , then we use it to prove that all the losses considered in satisfy .",
    "finally we give two lemmas , necessary to prove and then conclude with its proof .      1 .",
    "@xmath2 is a finite set , with discrete topology .",
    "2 .   @xmath516^d$ ] with @xmath517 , and the mixed partial derivative @xmath518 exists almost everywhere , where @xmath519 , and satisfies @xmath520 3 .   @xmath2 is compact and @xmath29 is a continuous kernel , or @xmath29 is a function in the rkhs induced by a kernel @xmath521 . here @xmath521 is a continuous kernel on @xmath522 , of the form @xmath523 with @xmath524 a bounded and continuous kernel on @xmath2 .",
    "@xmath2 is compact and @xmath525 that is the restriction of @xmath526 on @xmath2 , and @xmath527 satisfies on @xmath528 , 5 .",
    "@xmath2 is compact and @xmath529 with @xmath530 continuous maps from @xmath2 to a set @xmath531 with @xmath532 satisfying and @xmath533 , bounded and continuous .",
    "@xmath2 compact and @xmath534 where @xmath535^d \\to { \\mathbb{r}}$ ] is an analytic function ( e.g. a polynomial ) , @xmath536 and @xmath537 satisfy on @xmath2 . here",
    "@xmath538 where @xmath539 is the operator associated to the loss @xmath540 and @xmath541 is the value that bounds the norm of the feature map @xmath542 associated to @xmath540 , with @xmath543 .      1 .   _ any loss with , @xmath2 finite .",
    "_   this is a direct application of thm .",
    "[ teo : big_taxonomy ] , point 1 . 2 .",
    "_ regression and classification loss functions .",
    "_   here @xmath2 is an interval on @xmath544 and the listed loss functions satisfies thm .",
    "[ teo : big_taxonomy ] , point 2 .",
    "for example , let @xmath545 $ ] the mixed partial derivative of the hinge loss @xmath546 is defined almost everywhere as @xmath547 when @xmath548 and @xmath549 otherwise .",
    "note that @xmath468 satisfies eq .",
    "( [ eq : mix - der - def ] ) , for any @xmath385 .",
    "robust loss functions .",
    "_   here , again @xmath2 is an interval on @xmath544 .",
    "the listed loss functions are : _ cauchy _",
    "@xmath550 , _ german - mclure _ @xmath551 _ `` fair '' _ @xmath552 or the",
    "_ `` @xmath553 '' _ @xmath554 .",
    "they are differentiable on @xmath544 , hence satisfy , point 2 .",
    "the _ absolute value _",
    "@xmath555 is lipschitz and satisfies , point 2 , as well .",
    "_   when @xmath2 is a compact set and @xmath29 is a kernel , the point 3 of is applicable .",
    "diffusion distances on manifolds .",
    "_   let @xmath556 and @xmath557 be a compact reimannian manifold .",
    "the _ heat kernel _",
    "( at time @xmath86 ) , @xmath558 induced by the laplace - beltrami operator of @xmath2 is a reproducing kernel @xcite .",
    "the _ squared diffusion distance _ is defined in terms of @xmath559 as follows @xmath560 .",
    "then , point 3 of is applicable .",
    "6 .   _ distances on histograms / probabilities .",
    "_   let @xmath556 .",
    "a discrete probability distribution ( or a normalized histogram ) over @xmath561 entries can be represented as a @xmath562^m$ ] the @xmath561-simplex , namely @xmath563 and @xmath564 @xmath565 .",
    "a typical measure of distance on @xmath2 is the squared _",
    "hellinger ( or bhattacharya ) _",
    "@xmath566 , with @xmath567 . by , points 4 , 6 we have that @xmath568 satisfies .",
    "indeed , consider the kernel @xmath46 on @xmath544 , @xmath569 with feature map @xmath570 , then @xmath571 @xmath568 is obtained by @xmath561 summations of @xmath527 on @xmath572 $ ] , by , point 6 ( indeed @xmath573 with @xmath574 and the function @xmath575 defined as @xmath576 , which is analytic on @xmath577 ) , and then restriction on @xmath2 , point 4 .",
    "a similar reasoning holds when the loss function is the @xmath85 distance on histograms .",
    "indeed the function @xmath578 satisfies point 2 on @xmath579 $ ] , then point 6 and 4 are applied .",
    "[ lemma : abs - cont - functions ] let @xmath588^d$ ] with @xmath517 , and @xmath515 defined by @xmath589 with @xmath590 for any @xmath591 , @xmath585 and @xmath592 for any @xmath593 .",
    "the loss @xmath29 satisfies when @xmath594    note that by applying with @xmath595 , the function @xmath29 is bounded continuous .",
    "now we introduce the following sequences @xmath596 note that @xmath597 and that @xmath598 bounded by @xmath134 and is continuous by for any @xmath599 .",
    "therefore @xmath600 now we define two feature maps @xmath601 , with @xmath602 , as @xmath603 now we prove that the two feature maps are continuous .",
    "define @xmath604 and @xmath605 for all @xmath606 .",
    "we have @xmath607 with @xmath608 , for @xmath609 , therefore @xmath610 , @xmath611 are bounded and continuous by with @xmath595 , since @xmath612 and @xmath613 .",
    "note that @xmath614 and @xmath615 are bounded , since @xmath610 and @xmath611 are .",
    "moreover for any @xmath616 , we have @xmath617 and the same holds for @xmath615 with respect to @xmath611 .",
    "thus the continuity of @xmath614 is entailed by the continuity of @xmath610 and the same for @xmath615 with respect to @xmath611 .",
    "now we define @xmath618 and @xmath619 and @xmath620 as @xmath621 where @xmath622 is the identity operator .",
    "note that @xmath38 is bounded continuous , @xmath81 is bounded and @xmath623        1 .",
    "let @xmath624 and @xmath625 be a one - to - one function .",
    "let @xmath626 , @xmath627 defined by @xmath628 for any @xmath591 with @xmath629 the canonical basis for @xmath35 , finally @xmath630 with @xmath631 for any @xmath632 .",
    "then @xmath29 satisfies , with @xmath38 and @xmath81 .",
    ", we know that any loss @xmath29 whose fourier expansion is absolutely summable , satisfies .",
    "the required conditions in points 2 are sufficient ( see theorem  6 pag .",
    "291 of @xcite ) .",
    "3 .   let @xmath2 be a compact space .",
    "for the first case let @xmath29 be a bounded and continuous reproducing kernel on @xmath2 and let @xmath35 the associated rkhs , then there exist a bounded and continuous map @xmath633 such that @xmath634 for any @xmath635 , which satisfies with @xmath636 the identity on @xmath35 . for the second case ,",
    "let @xmath521 defined in terms of @xmath524 as in equation .",
    "let @xmath637 be the rkhs induced by @xmath524 and @xmath38 the associated feature map , then , by definition , the @xmath638 induced by @xmath521 will be @xmath639 .",
    "since @xmath29 belongs to @xmath161 , then there exists a @xmath640 such that @xmath641 .",
    "now note that @xmath639 is isomorphic to @xmath642 , that is the linear space of hilbert - schmidt operators from @xmath637 to @xmath637 , thus , there exist an operator @xmath643 such that @xmath644 finally note that @xmath38 is continuous and bounded , since it is @xmath524 , and @xmath81 is bounded since it is hilbert - schmidt",
    ". thus @xmath29 satisfies .",
    "4 .   since @xmath527 satisfies",
    "we have that there exists a kernel on @xmath528 such that holds .",
    "note that the restriction of a kernel on a subset of its domain is again a kernel .",
    "thus , let @xmath645 , we have that @xmath646 satisfies with @xmath38 and the same bounded operator @xmath81 as @xmath29",
    "let @xmath647 satisfy , then @xmath648 for all @xmath649 with @xmath650 bounded and continuous and @xmath637 a separable hilbert space .",
    "now we define two feature maps @xmath651 and @xmath652 .",
    "note that both @xmath653 are bounded and continuous .",
    "we define @xmath618 , @xmath619 as @xmath654 for any @xmath591 , and @xmath655 .",
    "note that now @xmath656 6 .",
    "let @xmath2 be compact and @xmath540 satisfies with @xmath657 the associated rkhs , with continuous feature maps @xmath658 bounded by @xmath541 and with a bounded operator @xmath539 , for @xmath543 .",
    "since an analytic function is the limit of a series of polynomials , first of all we prove that a finite polynomial in the losses satisfies , then we take the limit . first of all",
    ", note that @xmath659 , satisfies , for any @xmath660 .",
    "indeed we define @xmath661 , and @xmath662 for any @xmath591 , so that @xmath663 for any @xmath635 , where @xmath38 is continuous , @xmath664 and @xmath665 . in a similar way we have that",
    "@xmath666 satisfies , indeed , we define @xmath667 and @xmath38 to be @xmath668 for any @xmath591 , thus @xmath669 for any @xmath635 , where @xmath38 is continuous , @xmath670 and @xmath671 . given a polynomial @xmath672 , with @xmath673",
    "we write it as @xmath674 where the @xmath675 s are the coefficents of the polynomial and such that only a finite number of them are non - zero . by applying the construction of the product on each monomial and of the sum on the resulting monomials",
    ", we have that @xmath672 is a loss satisfying for a continuous @xmath38 and a @xmath81 such that @xmath676 and @xmath665 , where @xmath677 and @xmath678 .",
    "note that @xmath35 is again separable .",
    "let now consider @xmath679 assume that @xmath680 .",
    "then by repeating the construction for the polynomials , we produce a bounded @xmath38 and a bounded @xmath81 such that @xmath681 in particular @xmath35 is the same for the polynomial case and @xmath682 , with @xmath683 , for any @xmath684 .",
    "now we prove that @xmath38 is continuous on @xmath2 .",
    "let @xmath685 be the feature map defined for the polynomial @xmath686 .",
    "we have that @xmath687 with @xmath688 for any @xmath684 . no note that @xmath689 thus @xmath690 .",
    "therefore @xmath691 now , since @xmath685 is a sequence of continuous bounded functions , and the sequence converges uniformly to @xmath38 , then @xmath38 is continuous bounded .",
    "so @xmath692 is a loss function satisfying , with a continuous @xmath38 and an operator @xmath81 such that @xmath693 and @xmath665 ."
  ],
  "abstract_text": [
    "<S> we propose and analyze a regularization approach for structured prediction problems . </S>",
    "<S> we characterize a large class of loss functions that allows to naturally embed structured outputs in a linear space . </S>",
    "<S> we exploit this fact to design learning algorithms using a surrogate loss approach and regularization techniques . </S>",
    "<S> we prove universal consistency and finite sample bounds characterizing the generalization properties of the proposed methods . </S>",
    "<S> experimental results are provided to demonstrate the practical usefulness of the proposed approach . </S>"
  ]
}