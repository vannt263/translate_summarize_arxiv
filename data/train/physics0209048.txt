{
  "article_text": [
    "in this paper we consider the dynamics of automatic design of learning algorithms for neural networks .",
    "we use genetic programming ( gp ) as a tool to generate a sequence of generations of populations of programs which implement a learning algorithm .",
    "programs at one generation give rise through cross over and mutations to offspring programs in the next generation according to their fitness . the fitness -which defines the problem-",
    "is related in this study to the efficiency of the learning algorithm implemented by the program .",
    "we choose a measure of efficiency based on the ability of generalization , related to the expected error of the output on examples which are statistically independent from the training set .",
    "although gp is similar in spirit and actually inspired by the genetic algorithm ( ga ) of holland @xcite , since both mimic natural evolution , the idea of gp put forward by koza @xcite , differs from ga in very important ways .",
    "gp deals with programs , represented by strings of symbols -variables or functional operators- which can have as inputs different types of variables and operators across the population , as well as along the generations . in some loose sense gp allows for great variability and thus for the emergence of more of something that could be dubbed complexity .",
    "usually the automatic design of programs has as an aim the solution of a problem and a measure of how far a given candidate goes in that direction is given by the fitness .",
    "we are not only interested in final results , but rather the road towards that goal and its characterization are the main issues .",
    "we have chosen to study perceptron learning , a sufficiently simple learning problem that can be studied , as far as final results are concerned , by analytical means but which presents a wealth of interesting results . by analyzing the development of learning algorithms",
    "we expect to learn something about the dynamics along which different variable combinations become useful and invade the population of programs .",
    "we find dynamic phase transitions as different functional structures change from being irrelevant to useful and find evidence that point to a strict temporal order in the sequence of such appearances .",
    "some structures , though useful at later stages are irrelevant at first and remain so until some other structure is mature enough and thus potentialize the utility of the former .",
    "the characterization of a population can be made through the use of several different complementary tools .",
    "what we call the phenotypic or functional level description deals with quantities that measure the expression of important traits . at this level",
    "the program differences are irrelevant as long as they give rise to the implementations of the same function .",
    "the main tool , the phenotypic entropy @xmath0 describes the distribution of fitness in the population . at the genotypic or program level , different programs are different even if they give rise to the same numbers , for their potential of generating new successful programs in the following generation depends on the particular symbols which exist at present .",
    "we can introduce several genotypic entropies which describe the distribution of probabilities of symbols , of two contiguous symbols and so on .",
    "we will restrict to dealing with single symbol distributions and we characterize them by @xmath1 the genotypic entropy @xcite .    the crossover of programs , obtained by a cutting and pasting process described bellow , can be difficult to implement in common programming languages such as c and fortran . the major part of the work in gp has been developed in lisp which is also the case in this study .",
    "we have developed also a protocol for simulation of lisp on a parallel architecture on a cluster of machines running linux , which is described in @xcite .",
    "the paper is organized as follows . in section 2 a brief description of gp from the very special point of view which interests us here",
    "is followed by a description of the problem which gp aims at solving .",
    "section 3 presents the results and concluding remarks can be found in the last section .",
    "the learning problem to be analyzed by the gp must strike a balance between being complex enough so that interesting dynamics arises and simple to the point that details can be understood and simulations performed .",
    "the perceptron meets these demands and has a long and distinguished history . for an extensive view from a statistical mechanics perspective",
    "see @xcite .",
    "we consider the realizable teacher - student learning scenario .",
    "the perceptron classifies vectors @xmath2 ( here obtained i.i.d from a uniform distribution ) in two categories with labels @xmath3 according to the rule @xmath4 .",
    "the objective of the learning dynamics is to determine the weight or synaptic vector @xmath5 from pairs of examples @xmath6 which carry information about a rule .",
    "we restrict ourselves to the simplest case of noiseless realizable rules , which mean that the labels were uncorrupted and generated by another perceptron with a weight vector @xmath7 unknown to us .",
    "we consider on - line learning , which means that @xmath8 will be built sequentially by modifications induced by the arrival of new pairs of examples .",
    "we even concentrate on the particular form of modulated hebbian learning , where the increments of @xmath8 are described by a modulation function @xmath9 , thus @xmath10 .",
    "this is not very restrictive as a large fraction of the previously studied algorithms , both on - line and off - line , may be put in a similar way and in the ( thermodynamic ) limit of large networks it can represent asymptotically efficient learning , which even saturate bayesian bounds .",
    "we deal with questions about the modulation function , such as : ( i ) what are the variables upon which the modulation function depends ?",
    "( ii ) what is the best function ? ( iii ) in the event that the machine has no access to all of the useful variables , which ones can be left out and which are relevant ? that is , in the path towards the development of a more sophisticated algorithm , the machines at earlier stages may not dispose all relevant variables , then which are the ones that are relevant in the earlier stages and which become so later ?",
    "is there any discernible pattern in the order these variables are incorporated ?",
    "that we can indeed identify such time ordering in our simulations is the main result of this paper .",
    "related questions have been addressed before @xcite , see : about best results and bayesian bounds @xcite , for a variational point of view about the perceptron learning in @xcite , about feedforward architectures with hidden units in @xcite , for drifting rules in @xcite , in an unsupervised scenario @xcite , from a more general bayesian perspective in @xcite ; in the case of off - line learning in @xcite . from the perspective of time ordering it has been discussed in @xcite .      in this section",
    "we describe briefly our implementation of gp for the problem at hand .",
    "we do not consider the evolution of machine architecture , which is left for future work and just deal with the evolution of the modulation function .",
    "conventional ga work manipulating fixed - length character strings that represent candidate solutions of a given problem . for many problems ,",
    "hierarchical computer programs are the most natural representation for the solution .",
    "since the size and the shape of the program that represents the solution are unknown in advance , the program should have the potential of changing its size and shape .",
    "the aim of gp is getting computers to program themselves by providing a domain independent way to search the space of possible computer programs , for one that solves a given problem . the principle that rules gp is , as in ga , the survival of the fittest .    starting from a population of randomly created computer programs ,",
    "the gp operations are used to generate the population of the next generation .",
    "the programs are ranked by their fitness and then the gp operations are applied again .",
    "these two steps are then iterated .",
    "the most common computer language used in gp is lisp , therefore we will refer to the population individuals as programs or lisp s - expressions indistinctly .",
    "we call _ faithful s - expressions _ ( fses ) a lists of symbols that do not return an error message when evaluated . components , also called atoms , of the s - expressions can be either functional operators or variables .",
    "the set of all operators used in the s - expressions is @xmath11 and the set of all variables is @xmath12 .",
    "the choice of these sets depends on the nature of the problem being faced .",
    "for instance , if the solution of a problem can be represented by a quotient of polynomials , @xmath13 = \\{+ - * / } and @xmath14 = \\{x 1}. for example , a fse is ( + ( + x x ) ( * x ( - x ( - x x ) ) ) ) , which is a ( non unique ) lisp representation of the function @xmath15 .",
    "the simplest fse is an operator followed by the appropriate number of variables ( two in the example above ) .",
    "all fses have an operator as first element , and following elements should be variables or fses .",
    "unfaithful s - expressions are for instance : ( x x ) , ( + x * ) and ( x - x ) .",
    "lisp s most prominent characteristic with regard to gp is that programs and data have a common form and are treated in the same manner .",
    "this common form is equivalent to the parse tree for the computer program and allows to genetically manipulate the parts of the program ( i.e. , subtrees of the parse tree ) .",
    "= .48    the gp operations considered in the present work are asexual reproduction , mutation and cross - over . in the operation of asexual reproduction a certain fraction of the top ranked individuals",
    "are copied without any modification into the new generation , ensuring the preservation of structures that made them successful .",
    "mutation is implemented by randomly changing an atom of an individual chosen at random .",
    "the new and old atoms must be of the same kind to ensure faithfulness .",
    "finally the modified tree is copied into the new generation . in order to accelerate the dynamics different mutation rates can be used for different atom types .",
    "although there are no sexes associated to the programs , cross - over can be better described as the sexual gp operation . in our experiments ,",
    "the first parent is chosen among the reproduced fraction of the population ( those programs that have been copied from the past generation ) by tournament @xcite .",
    "the second parent is chosen by tournament among the entire population .",
    "an atom is selected randomly in each parent .",
    "the subtrees ( or leaves ) with roots in the selected atoms are interchanged to generate two offsprings . in order to avoid uncontrolled growth",
    "if the depth of any of the offsprings is above a given threshold , the program is deleted .    after a new population is created , the fitness of each individual is measured and so a new ranking is built .",
    "there is a great freedom in choosing the fitness function .",
    "it is always a macroscopic or phenotypic quantity , i.e. a function of the expressed characters , and although it reflects the microstructure , it is not a function of the genetic details of the individual .",
    "errors in the measurement of the fitness have a bearing on the dynamics , not entirely different from the temperature in simulated annealing .",
    "our numerical experiments have been performed in a pentium iii , 800 mhz pc , linux cluster , using the strategy described in @xcite .",
    "the gp parameters used in the simulation are presented in table 1 . at generation zero a population of 500 faithful s - expressions",
    "is created at random .",
    "the programs have ( in agreement with table 1 ) a maximum depth of @xmath16 nested parenthesis .",
    "the sets used to build the programs are @xmath17 where @xmath18 , and @xmath19 where psqr , pexp , plog , and % are the protected square root , exponential , logarithm and division ; abs , + , - , and are the usual absolute value , addition , subtraction and multiplication ; and p. , pn . , ev@xmath20 , vv@xmath21 , and vv@xmath22 are the inner product , normalized inner product , the product of a scalar times a vector , the addition of two vectors and the subtraction of two vectors respectively .",
    "protected functions are functions whose definition domains have been extended in order to accept a larger set of arguments .",
    "the definitions of these functions appear in table 2 .",
    "= .48    the inner product is the usual inner product among vectors . if @xmath23 , * * * * @xmath24 * @xmath25 * then the normalized inner product is @xmath26 .",
    "other operations involving vectors have to be defined .",
    "the @xmath27 takes two arguments , a scalar x and a vector @xmath28 , and returns a vector @xmath29 * * * * with components @xmath30 .",
    "the sum ( difference ) of two vectors @xmath31 ( @xmath32 ) takes two vectors * @xmath28 * and @xmath29 , and returns a vector @xmath33 , with components @xmath34 @xmath35 @xmath36 .",
    "when the process of creation of programs is done , before performing the gp operations to generate the next generation , the fitness has to be calculated .",
    "because the programs represent the learning algorithm of a neural network , a good measure of the learning algorithm performance , should be based on the generalization error , which measures the probability that the classification of the network @xmath37 is different from the correct label @xmath38 @xmath39 which in the thermodynamic limit @xmath40 where @xmath41 and @xmath42 indicates how many examples have been presented to the network , which we call the age of the individual .",
    "the average is over training sets of @xmath42 pairs of examples .",
    "since the aim is to obtain algorithms with the smallest possible generalization error , which depends on the age -taken as the number of examples already to which the network has been exposed- , we chose a fitness that incorporates the variation of @xmath43 with age and average over age so that the asymptotic stage is at least as important as earlier stages .",
    "for the @xmath44 member of the population @xmath45 is the fitness and @xmath46 is the total number of examples ( maximum age ) presented to the network .",
    "the population is ranked according to fitness and the best @xmath47 are asexually reproduced into the next generation ( according to the reproduction rate on table 1 ) .",
    "the other @xmath48 is generated by cross - over .",
    "the first parent is chosen from the best @xmath47 of the population .",
    "to do so we select first a number @xmath49 such that @xmath50 with a probability proportional to _ @xmath49 _ ( the higher the _ @xmath49 _ the higher the probability to choose it ) . _",
    "@xmath49 _ is the age of the individuals that are going to participate in the tournament . from the best @xmath47 of the population ,",
    "ten individuals are selected at random . from comparison of their generalization error at age _",
    "@xmath49 _ , the individual with smaller @xmath51 is selected for cross - over . to select the second parent",
    "a similar mechanism is applied .",
    "ten individuals are selected at random from the entire population , and their generalization errors at age _",
    "@xmath49 _ are compared .",
    "the winner is chosen to mate . to perform the cross - over ,",
    "sub - trees of both parents are selected at random .",
    "internal points ( i.e. operators ) are selected more frequently than external points ( i.e. variables ) in order to make the individuals grow ( see table 1 ) .    if either one of the offsprings has a depth bigger than 17 , it is deleted . with a mutation rate of 0.01% ( one every 20 generations ) a mutation is performed to the offsprings . because the pairs @xmath52 become rare after few generations ( at the beginning of the simulation , the learning algorithms that use * j *",
    "are not efficient ) we keep injecting this pair with a rate of 0.2% ( at least one individual per generation receives this pair ) .",
    "different mutation rates just serve the purpose of accelerating the dynamics and decrease the time scale of the typical time that it takes for interesting things to happen .",
    "the process is repeated until the new population reaches the full size fixed here at 500 . to calculate the generalization error an average",
    "is taken over at least 50 sets of examples @xmath53 .    [",
    "cols=\"^,^\",options=\"header \" , ]     table 2 .",
    "definition of the protected function as fses .",
    "the protected square root is just the square root of the absolute value of its argument . in this manner",
    "we extended its domain into the negatives .",
    "the exponential is well defined in the reals .",
    "although , in order to avoid overflows we have to impose a cut - off .",
    "the protected logarithm has a cut - off at a small positive number to extend its domain to the non - positive numbers .",
    "and the protected quotient allows the division by zero ( if the absolute value of the denominator is smaller than a tiny number the protected quotient returns a big number , if not it just returns the usual quotient ) .",
    "to characterize the distribution of the fitness across the population we introduced the normalized fitness , a measure of the fraction of the total ( exponential ) fitness that an individual has , in a way analogous to the canonical state at temperature @xmath54 ( although the system is not in equilibrium with any temperature reservoir ) : @xmath55 where @xmath56 is the fitness measure of th @xmath44 individual of the population .",
    "note that smaller values of the fitness are associated to better performances .",
    "the use of the exponential amplifies the importance of the individuals with better performance and @xmath54 was kept equal to @xmath57 .",
    "we introduce the entropy of the normalized fitness    = .98    2 @xmath58 a function of the expressed characters of the population ( fitness ) , thus dubbed the phenotypic entropy or ph - entropy .",
    "note that this entropy is largest when all the members of a population have the same fitness and that the appearance of a distinguished individual , for better or worst , is signaled by a decrease in ph - entropy .",
    "each fse in the population has a well defined length @xmath59 , i.e the number of atoms ( operators and variables ) that make it up .",
    "we define the mean length _",
    "@xmath60 _ as @xmath61    to characterize the internal structure of the programs , we estimate for each position @xmath62 the probability that symbol @xmath63 ( a variable or an operator ) appears at position @xmath62 , @xmath64 by measuring the frequency over all the population . the genotypic entropy ( or g - entropy ) which is a function of the micro structure of the individuals in the population",
    "is then defined as @xcite @xmath65 where @xmath66=@xmath13 @xmath67 @xmath14    several numerical experiments , starting from different random seeds , have been performed using the gp described above .",
    "= .48    although the history of the population varies from run to run , we have identified some systematic occurrences .",
    "= .48    in most of the runs we have found a drastic change in behavior which can be well described as a phase transition , although we have neither taken thermodynamic limits associated to infinite network dimension nor infinite population .",
    "the time of the occurrence varied widely from one simulation to other . in some simulations",
    ", the population did not undergo the transition but it could well happen that we just did not wait long enough . in what follows we consider an illustrative run which presents clearly some features that are typical of other runs .",
    "we found a dramatic change of behavior around generation @xmath68 that can be seen by using several different signatures .",
    "figure [ fg ] ( left ) shows the fitness of the most adapted program or best - of - generation ( bog ) as a function of time .",
    "the exponent that governs the decay of @xmath43 shows a sharp change , specially if the population average is compared to that of the bog .",
    "finite size errors are responsible for the fact that exponents larger than one can be found . to understand how representative of the whole population is the bog we composed a color coded bar graph ( see fig . [ gen0 ] ) where each vertical bar represents the bog program written as a string of symbols , time is measured in generations in the horizontal axis . at the position of each symbol in the program",
    "a colored square represents the empiric probability of the symbol in the population .",
    "note that quite rapidly an initial symbol is predominant in the population .",
    "this is invariantly found in all runs and it is always a symbol that ensures that the modulation function is positive , for other wise the learning would be anti - hebbian and inefficient .",
    "the initial part of the code is very robust and thus is shared by almost all the population .",
    "there is an obvious change in the length of the bog which will be considered bellow , but notice before the transition the upper part is moderately common ( green ) and after the transition the upper part is more variable or less frequent .",
    "these changes can also be monitored by the entropies .",
    "= .98    2    = .48    both entropies ( phenotypic and genotypic ) present changes about the same time ( figs .",
    "[ entropf ] and [ entropg ] ) .",
    "the ph - entropy shows a much larger variability after the transition , the g - entropy and the mean length both have an almost discontinuous break at the transition . the fact that the ph - entropy has a decreasing trend after the transition can be attributed to the fact that the g - entropy increases and thereby makes the bog less frequent than immediately after the transition .",
    "now the fitness distribution is sharper around the bog and therefore the ph - entropy decreases and oscillates over a wider range .",
    "the g - entropy and the mean length are linearly correlated .",
    "this is natural since g - entropy , as defined should be extensive .",
    "what is not as expected is the fact that there are two distinct linear regimes before and after the transition . to see this we did a linear fit to the whole data set and plotted an histogram of the residuals , that is the difference between the actual value of a data point and the corresponding value of the linear model .",
    "the two histograms in the inset of fig .",
    "[ entropg ] show clearly a systematic error for the single linear model .",
    "these results prove the existence of a quite sharp transition , but do not hint at the nature of the changes in the individual programs nor the reasons for the improvements in fitness .",
    "the question is trying to understand what happened from a functional point of view that led to such an improvement in generalization ability .",
    "there are two quantities or functional structures that are of interest both in a quantitative and qualitative analysis of the of learning algorithms .",
    "the first , which can be associated to the product @xmath69 can be functionally described as quantifying a measure of surprise .",
    "this is because if @xmath70 the network will classify correctly the example with classification label @xmath38 , while if @xmath71 , the classification is wrong .",
    "thus it gives a signal of how wrong or correct was the classification and also how stable that classification is under changes of the weight vector .",
    "this is obviously an important factor to take into account while incorporating the information in a given example .",
    "the second functional structure we will concentrate on is something that can estimate the performance or acquired experience of the network in the implementation of the rule .",
    "this , if properly used is akin to annealing of the learning rate or of the functional annealing in learning algorithms .",
    "this can be implemented by using the length of the weight vector @xmath72 . in fig .",
    "[ errorj ] we show a graph of @xmath73 as a function of the generalization error for a program with a good fitness in the later stages of the simulation .",
    "the monotonic behavior is a typical result .",
    "it can be shown , at least in the thermodynamic limit , that for algorithms which do not measure surprise their generalization error decays as @xmath74 and for them annealing is useless . learning algorithms that use surprise",
    "have a better performance @xmath75 and algorithms that use both surprise and annealing by experience have an even better performance since can have smaller coefficients of @xmath76 .",
    "a crude measure of the capacity of a population of using a functional structure may be given by the frequency that the combination of variables is found .",
    "this is admittedly crude since the position in the program determines whether it is useful or not . on the other hand",
    "the absence of such combination does not rule out the possibility that some other combination is doing the job in a more cumbersome manner . in fig .",
    "[ syeg ] we plot the density of pairs @xmath69 ( surprise ) and the density of pairs @xmath77 ( performance ) in the entire population , as functions of the number of generations .",
    "= .48    it is possible to observe a fast change in the frequency of pairs of symbols related surprise before 20 generations . @xmath77 pairs are almost immediately all but extinguished from the population .",
    "@xmath69 pairs are distributed very frequently and its presence oscillates across the population and through the generations , while @xmath77 pairs introduced by mutations are not able to invade the population . at the time of the transition , surprise is being correctly measured and now the appearance of @xmath77 leads to an improvement in fitness since it leads to an estimate of the generalization error and permits the implementation of correct annealing schedules .",
    "this successful strategy invades the population .",
    "it is reasonable to associate the improvement in the fitness with the emergent use of experience by the elements of the population .",
    "notice that injections through mutations of performance structures were non invading before the transition .",
    "of course this can be explained by claiming that not every kind of annealing is beneficial but most important , before surprise is measured correctly , no annealing scheme is useful , and therefore individuals which could measure @xmath77 did not benefit from such knowledge .",
    "the sequence of symbols of the bog individual before and after the change in the density of pairs @xmath77 mirror that increase . in fig .",
    "[ bicho0 ] we present the most adapted individual at generations 300 , 350 , 400 and 450 . just before the transition",
    "there is no pair @xmath77 * * * * present in the program ( the two first programs ) .",
    "after the transition the best individual suffers a decrease in size and several pairs @xmath77 * * * * appear . according to the color scale ,",
    "red symbols are extremely frequent in the population at that position , green symbols are just frequent at that position , and violet are quite unlikely to be found .",
    "we can see that after the transition , the third program , presents symbols mostly in the violet .",
    "50 generations later there are islands of green in the bog .",
    "that means that the genetic character of the best individual has invaded the population .",
    "a more general analysis of the density of pairs can be done with the help of fig .",
    "[ pares ] . in these pictures we present the relative frequencies at which each possible pair appear in the population",
    "the vertical axis represents the first element of the pair , the horizontal axis the second element .",
    "the size of the white squares represent the frequency of the pair , relative to the most frequent pair ( represented by the largest square in each picture ) . in panel ( a ) we present the density of pairs at generation 300 , ( b ) corresponds to generation 350 , ( c ) to generation 400 and ( d ) to generation 450 . in ( a ) and ( b ) there are no pairs @xmath77 .",
    "the most frequent pair is the combination @xmath78 , which is just a @xmath57 , but not quite since it can evolve into different directions .",
    "after the transition , in panels ( c ) and ( d ) , this pair remains the most frequent , but important changes have happened .",
    "there are small white squares for the pair @xmath77 * * * * representing the emergence of the use of experience by the learning algorithms .",
    "= .98    2    the modulation functions of bog s at different stages of the evolution can also be understood along the line of surprise - performance analysis . at earlier stages",
    "the bog is unable to use surprise .",
    "although surprise functional structures are found throughout the population , their incorrect use makes the bog an annealed hebbian algorithm .",
    "it is known that annealing will not improve the hebbian learning and the frequency of performance functional structures decreases until it vanishes .",
    "it will only appear in very modest ways through mutation and , repeatedly individuals which use it becomes extinct .",
    "later on surprise is finally well accounted for and correctly classified examples cause typically smaller hebbian corrections than those incorrectly classified . at that point",
    "the correct use of surprise potentializes the beneficial use of functional structures that measure performance .",
    "then a correctly annealed algorithm emerges that resembles quite closely the modulation functions found through bayesian or variational approaches ( fig .",
    "[ fmod ] ) .",
    "evolutionary programming techniques provide the means to automatically design programs which solve certain class of problems . in this paper , however we were not interested in the final result , the problem that gp was set out to solve has been previously analyzed from many angles and a detailed understanding of online learning in perceptrons has been achieved .",
    "rather we concentrated on the dynamics of evolution and have detected dynamical changes in the behavior of the gp solutions that we have not hesitated in dubbing dynamical transitions .",
    "this is not a conventional phase transition associated to singularities arising in the thermodynamic limit .",
    "a few runs failed to present the transition , maybe because of time limitations , but it was seen in many different runs .",
    "some features were never reproducible but others were present in every transition . as examples of those features that depend upon contingencies",
    "we include the number of generations before the transition takes place , the width of the transitions ( some were just about ten generations wide , others took several tens of generations ) and the result of the gp , i.e. the program that implements the best learning algorithm .",
    "these are mainly important from a constructive point of view when the solution to the problem is the main concern .",
    "we tried , instead to identify robust features which can be confidently expected to occur every time the transition takes place . in serving such purpose",
    "we have characterized the dynamics by looking at ph- and g - entropies which give a picture of the distribution of phenotypic fitness and functional or symbolic structure respectively .",
    "large entropic fluctuations are well described by power laws .",
    "= .98    2    the conformation diagram gives a bird s eye view of the relation of the bog and the frequency of symbols in the population as well as its length the main robust feature can be identified once the transition has been understood from a functional point of view , in terms of two concepts : the surprise that newly arrived information elicits and how such information should be taken into account based on how much experience the network has in solving the task at hand .",
    "a temporal order can be identified in every transition .",
    "it was never found otherwise .",
    "performance can be useful only after surprise is measured correctly .",
    "there are several possible extensions of this problem . from a biological point of view",
    "there is a suggestive similarity with the time order in which certain structures responsible for measuring surprise and performance have appeared .",
    "will this order be found in more complex artificial settings ? is this biologically significant ? can it be extended to other functional structures ?",
    "it should also be quite interesting to further analyze phase transitions in the automatic design of programs .",
    "the simulations described here were done on a cluster made possible through the efforts of j. l. delyra , c. e. i. carneiro and coworkers . the cluster s construction",
    "was partially supported by fapesp and cnpq .",
    "jpn received financial support from fapesp and nc received partial support from cnpq .",
    "discussions with osame kinouchi and mauro copelli where important during the earlier stages of this work .",
    "10 j. h. holland `` _ _ adaptations in natural and artificial systems : an introductory analysis with applications to biology , control and artificial intelligence _ _ '' u. of michigan press , ann arbor ( 1975 ) .",
    "j. r. koza `` _ _ genetic programming : on the programming of computers by means of natural selection _ _ '' , mit press ( 1992 ) .",
    "this is similar to that introduced in c. adami , c. ofria , and t. c. collier , proc .",
    "usa * 97 * , 4463 ( 2000 ) , but not restricted to fixed size program length .",
    "j. p. neirotti and n. caticha , available at http://www.fge.if.usp/ nestor/. a. engel and c. van den broeck , `` _ _ statistical mechanics of learning _ _ '' , cambridge university press ( 2000 ) .",
    "s. amari , ieee transactions * ec-16 * , 299 ( 1967 ) . m. opper and d. haussler , phys .",
    "lett . * 66 * , 2677 ( 1991 ) .",
    "o. kinouchi and n. caticha , _ _ _ _ j .",
    "phys . a : math . and gen . * 25 * , 6243 ( 1992 ) . m. copelli and n. caticha , j. phys . a : math . and gen .",
    "_ _ _ _ * * 28 * * , 1615 ( 1994 ) .",
    "r. vicente and n. caticha , _ _ _ _ j .",
    "phys . a : math . and",
    "_ _ _ _ * * 30 * * , l599 ( 1997 ) .",
    "r. simonetti and n. caticha , _ _ _ _ j",
    ". phys . a : math . and gen .",
    "* 29 * , 6243 ( 1996 ) .",
    "m. biehl and schwarze , _ _ _ _ j .",
    "phys . a : math . and gen .",
    "_ _ _ _ * * 20 * * , 733 ( 1993 ) .",
    "o. kinouchi and n. caticha , j. phys . a : math . and gen .",
    "_ _ _ _ * * 26 * * , 6161 ( 1993 ) . c. van den broeck and p. reiman , phys .",
    "lett . * 76 * , 8874 ( 1996 ) .",
    "m. opper , phys .",
    "lett . * 77 * , 4671 ( 1996 ) .",
    "m. opper in `` _ _ on - line learning in neural networks _ _ '' , pg .",
    "363 , d. saad ed . , cambridge university press ( 1998 ) .",
    "s. sara and o. winther in `` _ _ on - line learning in neural networks _ _ '' , pg .",
    "379 , d. saad ed . , cambridge university press ( 1998 ) .",
    "o. kinouchi and n. caticha , phys .",
    "e * 54 * , r54 ( 1996 ) .",
    "n. caticha and o. kinouchi , philosophical magazine b * 77 * , 1565 ( 1998 ) ."
  ],
  "abstract_text": [
    "<S> we study the evolution of artificial learning systems by means of selection . </S>",
    "<S> genetic programming is used to generate a sequence of populations of algorithms which can be used by neural networks for supervised learning of a rule that generates examples . in opposition to concentrating on final results , which would be the natural aim </S>",
    "<S> while designing good learning algorithms , we study the evolution process and pay particular attention to the temporal order of appearance of functional structures responsible for the improvements in the learning process , as measured by the generalization capabilities of the resulting algorithms . </S>",
    "<S> the effect of such appearances can be described as dynamical phase transitions . </S>",
    "<S> the concepts of phenotypic and genotypic entropies , which serve to describe the distribution of fitness in the population and the distribution of symbols respectively , are used to monitor the dynamics . in different runs the phase transitions might be present or not , with the system finding out good solutions , or staying in poor regions of algorithm space . </S>",
    "<S> whenever phase transitions occur , the sequence of appearances are the same . </S>",
    "<S> we identify combinations of variables and operators which are useful in measuring experience or performance in rule extraction and can thus implement useful annealing of the learning schedule . </S>",
    "<S> we also find combinations that can signal surprise , measured , on a single example , by the difference between prediction and the correct output . </S>",
    "<S> structures that measure performance always appear after those for measuring surprise . </S>",
    "<S> invasions of the population by such structures in the reverse order were never observed .    * </S>",
    "<S> pacs * numbers : 05 . , 84.35.+i </S>",
    "<S> , 87.23.kg    = 17.5 cm = 58    * abstract *    2 </S>"
  ]
}