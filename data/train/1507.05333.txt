{
  "article_text": [
    "[ sec : intro ] standard approaches to supervised learning assume that training and test data can be modeled as an i.i.d .  sample from a joint probability distribution @xmath0 over inputs @xmath1 and outputs  @xmath2 .",
    "the inputs are often vectorial , and the outputs might take the form of labels ( classification ) or continuous values ( regression ) . the i.i.d.setting is theoretically well understood and yields remarkable predictive accuracy in problems such as image classification , speech recognition and machine translation ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "however , many real world problems do not fit into this setting .",
    "distributions may change between training and testing , and work in the field of domain adaptation attempts to address this .",
    "we first describe the problems of multi - task learning and transfer learning .",
    "then , we present some existing assumptions made in order to address the problem of knowledge transfer , as well as the new assumption for domain adaptation we introduce in this paper .",
    "assume that we want to predict a target variable @xmath3 from some predictor variable @xmath4 .",
    "consider now @xmath5 source ( or training ) tasks @xmath6 where each @xmath7 , @xmath8 , represents a joint probability distribution generating data @xmath9 . in transfer learning ( tl )",
    "( e.g. , @xcite and references therein ) , we are interested in using information from these source tasks in order to predict @xmath10 from @xmath11 in a related test domain @xmath12 .",
    "the closely related multi - task learning ( mtl ) setting ( e.g. * ? ? ? * ; * ? ? ?",
    "* ) aims at improving the prediction in the test task when additional labeled examples from the test task are available .",
    "although slight variations of these definitions exist , we will refer to these two problems as tl and mtl for the remainder of this article .",
    "table  [ tab : taxo ] summarizes the two problem statements .",
    "@xmath13    at training time , we observe a sample @xmath14 for each source task @xmath15 . at test time",
    ", we are interested in predicting the target values of an unlabeled sample from the task @xmath16 of interest . our objective is to learn a mapping @xmath17 with small expected loss @xmath18 on the test task @xmath16 for some loss function @xmath19 .    to beat simple baseline techniques , regularity conditions on the differences of the tasks",
    "are required . indeed ,",
    "if the test task differs significantly from the source tasks , we may run into the problem of negative transfer @xcite and tl becomes impossible , see also @xcite . in mtl ,",
    "if enough labeled data are available from the test task , we will not be able to beat a method that learns on the test task and ignores the training tasks , at least in the limit of infinitely many data .",
    "we give a brief and non - comprehensive summary of some existing approaches to tl and mtl .",
    "a first family of methods assumes that * covariate shift * holds ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "this states that for all @xmath20 , the conditionals @xmath21 are * invariant * between tasks .",
    "therefore , the differences in the joint distribution originate from a difference in the marginal distribution of @xmath22 .",
    "for instance , if an unlabeled sample from the test task is available at training in the tl setting , the training sample can be re - weighted via importance sampling @xcite so that it becomes representative of the test task .",
    "another line of work focuses on * sharing parameters * between tasks .",
    "this idea originates in the hierarchical bayesian literature  @xcite . for instance , @xcite introduce a model for mtl in which the mapping @xmath23 in each task @xmath8 is drawn independently from a common gaussian process ( gp ) , and the likelihood of the latent functions depends on a shared parameter @xmath24 .",
    "a similar approach is introduced by @xcite : they consider an svm with weight vector @xmath25 , where @xmath26 is shared across tasks and @xmath27 is task specific .",
    "this allows for tasks to be similar ( in which case @xmath27 does not have a significant contribution to predictions ) or quite different .",
    "@xcite use a related approach for mtl .",
    "an alternative approach is based on learning a set of * common features * for all tasks .",
    "this family of methods is applied both to mtl  @xcite and tl  @xcite . in the mtl",
    "setting , @xcite propose to learn a set of low dimensional features shared between tasks using @xmath28 regularization , and then learn all tasks independently using these features . in @xcite , the authors construct a similar set of features using @xmath28 regularization but make use of only unlabeled examples .",
    "finally , the assumption introduced in this paper is based on a * causal * view on domain adaptation and transfer . @xcite",
    "relate multi - task learning with the independence between cause and mechanism , but do not propose a concrete algorithm . this notion is closely related to exogeneity @xcite , which roughly states that a causal mechanism mapping a cause @xmath29 to @xmath2 should not depend on the distribution of @xmath29 .",
    "additionally , @xcite consider the problem of target and conditional shift when the target variable is causal for the features .",
    "they assume that there exists a linear mapping between the covariates in different tasks , and the parameters of this mapping only depend on the distribution of the target variable .",
    "moreover , @xcite argue that the availability of multiple domains is sufficient to drop this previous assumption when the distribution of @xmath30 and the conditional @xmath31 change independently .",
    "the conditional in the test task can then be written as a linear mixture of the conditionals in the source domains .",
    "the concept of invariant conditionals and exogeneity can also be used for causal discovery @xcite .",
    "our approach to mtl and tl is related to the covariate shift assumption , but weakens it in a way taking into account causal knowledge . from the point of view of causal modeling @xcite , assuming invariance of conditionals makes sense if the conditionals represent causal mechanisms @xcite .",
    "intuitively , we would expect that a causal mechanism is a property of the physical world , and it does not depend on what we feed into it .",
    "if the input ( which in this case coincides with the covariates ) shifts , the mechanism should thus remain invariant @xcite . in the anti - causal direction , however , a shift of the input should normally lead to a changing conditional distribution @xcite . in practice ,",
    "prediction problems are often not causal . in the generic case",
    ", we should allow for the possibility that the set of predictors contains variables that are causal , anticausal , or confounded , i.e. , statistically dependent variables without a directed causal link to the target variable .",
    "we thus expect that there is a _ subset _",
    "@xmath32 of predictors for which the covariate shift assumption holds true , i.e. , the conditionals of output given predictor @xmath33 are invariant across @xmath34 @xcite .",
    "we prove that in this case , knowing @xmath32 leads to robust properties for tl and mtl .",
    "moreover , we introduce a way to incorporate knowledge from the test task if this is available in the form of a labeled sample ( in this case , we might want to use all available predictors ) .",
    "since in practice , the set @xmath32 is often unknown a priori , we introduce a method to infer such an invariant subset from data .",
    "section  [ sec : invcond ] formally describes our approach and its underlying assumptions . although some results are stated in a general form ( e.g. , thm .",
    "[ prop : adversarial ] ) , we present our method in a linear framework . in section  [ sec : invcond ] , we assume that the set @xmath32 leading to invariant conditionals is known , and study theoretical properties of our approach for tl in section  [ sec : tl ] and mtl in section  [ sec : mtl ] . we discuss a link to causal inference in section  [ sec : causality ] .",
    "section  [ sec : learninv ] presents a method for _ inferring _ such an invariant set @xmath32 from data .",
    "section  [ sec : experiments ] contains experiments on simulated and real data .",
    "[ sec : invcond ] consider a domain adaptation regression problem with source tasks @xmath6 , where @xmath35 for @xmath8 .",
    "we now formulate our variant of the covariate shift assumption .",
    "[ ass : train ]    there exists a subset @xmath32 of predictor variables such that @xmath36 we say that @xmath32 leads to _ invariant conditionals _ across the training tasks . here , @xmath37 denotes equality in distribution .",
    "this invariance also holds in the test task @xmath16 , i.e. , holds for @xmath38 .",
    "note that assumption  [ ass : train](b ) is stronger than assumption  [ ass : train](a ) only in the tl settings .",
    "assumption  [ ass : train](a ) is testable from training data , using an hsic independence test or a levene test for equality of variances , for example , see section  [ sec : learninv ] .",
    "however , assumption  [ ass : train](b ) is not testable during training ( for tl ) .    for the remainder of this section",
    ", we assume that we are given a subset @xmath32 of predictor variables that leads to invariant conditionals @xmath39 .",
    "we first use this assumption for the tl problem and for mtl later . here and below , we focus on regression using squared loss @xmath40 ( the superscript @xmath16 corresponds to the test task , not to be confused with the transpose , indicated by superscript  @xmath41 ) .",
    "we denote by @xmath42 the squared error averaged over the training tasks @xmath43 .",
    "[ sec : tl ] we first study the tl setting in which we receive no labeled examples from the test task during training time . throughout this section",
    ", we assume that assumptions  [ ass : train](a ) and  ( b ) hold .",
    "first , we introduce our proposed estimator , which uses the conditional mean of the target variable given an invariant subset in the training tasks .",
    "we prove that this estimator is optimal in an adversarial setting .",
    "moreover , we focus on a particular example to prove that this estimator minimizes the variance in the predictions in an unseen task . finally , we perform a theoretical comparison in a simple setting between our estimator and the estimator obtained from pooling all the training data .    [",
    "[ proposed - estimator . ] ] proposed estimator .",
    "+ + + + + + + + + + + + + + + + + + +    let us first focus on the population case .",
    "the optimal predictor obtained by minimizing   is the conditional mean @xmath44,\\ ] ] which is not available during training time . given a set @xmath32 that leads to invariant prediction as in assumption  [ ass : train](a ) , we propose to use the corresponding conditional expectation as a predictor , that is @xmath45 , \\end{array}\\,\\ ] ] which we assume to be a continuous function .",
    "given assumption  [ ass : train](a ) , the conditional expectation in   is the same in all training tasks .",
    "[ [ optimality - in - an - adversarial - setting . ] ] optimality in an adversarial setting .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in an adversarial setting , predictor   satisfies the following optimality condition ; as for the other results , the proof is provided in appendix  [ app : proofs ] .    [ prop : adversarial ] consider @xmath5 tasks @xmath46,@xmath47 , @xmath48 that satisfy assumption  [ ass : train](a ) .",
    "then the estimator   satisfies @xmath49 where @xmath50 contains all distributions over @xmath51 that are absolutely continuous with respect to the same product measure @xmath52 and satisfy @xmath53 .    unlike the optimal predictor  , the proposed estimator   can be learned from the data available in the training tasks . given a sample @xmath54 from domain @xmath43 , we can estimate the conditional mean in   by regressing @xmath30 on @xmath55 . due to assumption  [ ass : train](a ) , we may also pool the data over the different tasks and use @xmath56 as a training sample for this regression .",
    "we now provide robustness properties in a linear setting where the tasks are drawn from a meta - distribution .",
    "we denote by @xmath57 the vector such that @xmath58 . in this section , we consider the following setting :    setting 1 : : :    consider a vector of independent gaussian variables    @xmath55 in task @xmath59 .",
    "let the    target @xmath30 satisfy    @xmath60    where for each @xmath8 ,    @xmath61 is gaussian and independent of    @xmath55 .",
    "we have    @xmath62 ,    where each variable in @xmath63 is generated    as    @xmath64    for some @xmath65 and where    @xmath66 is gaussian and independent of    @xmath30 .",
    ", this corresponds to a gaussian sem with dag    shown in fig .",
    "the components in    @xmath63 are conditionally independent given    @xmath30 .",
    "] moreover , assume that each task @xmath59    has the same probability .",
    "[ [ minimal - variance . ] ] minimal variance .",
    "+ + + + + + + + + + + + + + + + +    the following result proves that the variance of the predictions is minimal for the invariant predictor  .",
    "[ prop : caus ] consider the model described in setting  1 .",
    "moreover , assume that the tasks differ as follows : the variances of the components of @xmath55 and @xmath67 are i.i.d . with mean zero and variance @xmath68 .",
    "let @xmath69 be the expected loss in the unseen task @xmath16 as in  .",
    "moreover , let @xmath70 be @xmath71 followed by @xmath72 zeros .",
    "then @xmath71 satisfies : @xmath73    note that the variance in   is measured over unseen test tasks , thus in this setting over the variances of @xmath55 and @xmath67 .",
    "we see that @xmath71 minimizes the variance of the predictions in an unseen domain , and this variance equals zero .",
    "intuitively , if the noise in @xmath74 has large variance this will lead to high variance in the predictions if the learned parameter has corresponding nonzero components .",
    "however , if only the predictors in @xmath75 are used for prediction , the variance of the residuals is not affected .",
    "[ [ comparison - to - the - pooled - estimator . ] ] comparison to the pooled estimator .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we compare properties of estimator   against the least squares estimator obtained from pooling the training data .",
    "again , we restrict the analysis to setting  1 , but we conjecture that a corresponding result holds in a more general case .",
    "the setting is similar to the one from proposition  [ prop : caus ] , but the tasks differ in coefficients @xmath76 instead of the noise variances and @xmath77 .",
    "we prove that the squared loss averaged over unseen test tasks is always larger for the pooled approach , when coefficients @xmath78 are centered around zero . in the case where they are centered around a non - zero mean",
    ", we prove that when the variance between tasks ( in this case , for coefficients @xmath76 ) becomes large enough , the invariant approach also outperforms pooling the data .",
    "[ prop : threenodes ] consider the model described in setting  1 .",
    "moreover , assume that the tasks differ as follows : the coefficients @xmath79 are i.i.d .",
    "with mean zero and variance @xmath68 .",
    "assume further that @xmath80 is one - dimensional for all @xmath8 .",
    "for @xmath81 , the variance of @xmath82 is @xmath83 , and is equal in all tasks",
    ". then the least squares predictor obtained from pooling the @xmath5 training tasks  @xmath84 satisfies :    @xmath85    where @xmath86 , @xmath87 , @xmath88 and @xmath89 is the diagonal matrix with @xmath90th diagonal element @xmath83 . having computed @xmath91 from the training tasks",
    ", we then have : @xmath92 in particular , this implies the following : @xmath93 moreover , if the coefficients @xmath76 are sampled from a distribution with non zero mean @xmath52 ,   holds if @xmath94 and @xmath95 , where @xmath96 is a polynomial in @xmath52 and @xmath97 is non - negative , see section  [ sec : threenodes_pf ] for details .",
    "figure  [ fig : error_proba ] visualizes proposition  [ prop : threenodes ] for two training tasks .",
    "we are interested in how the expected errors from change as the variance @xmath98 increases .",
    "recall that @xmath98 corresponds to the variance of coefficients @xmath76 , and thus measures how different the tasks are .",
    "we plot the average error for the pooled and invariant approaches as the variance @xmath98 increases . for proposition  [ prop : threenodes ] and the corresponding figure  [ fig : error_proba ] , coefficients @xmath76 are centered around zero , which leads to equation   holding true for all possible parameters in the model . as @xmath98 tends to zero",
    ", @xmath76 is close to zero in all tasks , which explains the equality of both the pooled and invariant errors for the limit case @xmath99 . for coefficients @xmath76 centered around a non zero value , equation   does not necessarily hold for small @xmath98 .",
    "[ sec : mtl ] assume now that a labeled sample @xmath100 is available from the test task and that assumption  [ ass : train](a ) holds .",
    "how can we combine the knowledge of the invariant structure and the new labeled sample ? can we perform better than a method that trains only on the data in the test task ?",
    "these questions are only relevant for a finite sample . given the whole target distribution @xmath101 and considering squared loss  , no method outperforms the conditional mean  , which ignores the data from the other tasks .",
    "we now present a method that exploits the invariant structure in the case of linear regression .",
    "we further assume that the ground truth for target @xmath30 is @xmath102 , where the noise @xmath61 has zero mean and finite variance , has the same distribution in the different tasks and is independent of @xmath55 .",
    "let @xmath103 be the set of predictors which are not in @xmath75 .",
    "as in the previous section , we assume that @xmath32 is known . for",
    "now , assume that we are given the precise value of @xmath71 .",
    "our objective is to use the knowledge of @xmath71 to find the regression vector @xmath104 which minimizes the expected squared loss in the test domain : @xmath105 the following result provides an expression for a set of optimal coefficients .",
    "[ prop : combinelabels ] assume that @xmath106 follows an arbitrary distribution and that assumption  [ ass : train](a ) holds in a linear setting , that is : @xmath107 where for all @xmath59 , @xmath108 for some random variable @xmath109 and @xmath110 .",
    "assume @xmath111 , where @xmath112 and the components of @xmath67 can be correlated and are independent of @xmath61 .",
    "then the regression coefficients @xmath104 minimizing the expected squared loss satisfy @xmath113 where @xmath114 , and @xmath115 , @xmath116 , @xmath117 are the corresponding gram matrices . to lighten the notation . ]",
    "we can now estimate @xmath118 and @xmath71 from the pooled data over source domains using linear regression , and compute the predictor in the test domain using   and  .",
    "the gram matrices and @xmath119 are estimated using the available labeled examples in the test task @xmath16 .",
    "proposition  [ prop : combinelabels ] introduces a method to combine the invariant structure extracted from the source tasks with the labeled sample from the test task , thus leveraging task specific knowledge and transferable knowledge from related tasks .",
    "consider a binary classification problem with labels @xmath120 and features @xmath121 .",
    "let @xmath122 be the sigmoid function .",
    "we extend our assumption to the classification setting : there exists a subset of predictor variables such that @xmath123 has the same distribution between different tasks .",
    "the extension of the robustness properties from theorem  [ prop : adversarial ] to this setting is straightforward .",
    "however , propositions  [ prop : caus ] and [ prop : threenodes ] are less easy to extend because of the non - linearity introduced by the sigmoid function . moreover , in order to use labeled examples from the task of interest , analogously to proposition  [ prop : combinelabels ] , we can solve numerically the following problem : @xmath124 where @xmath125 .",
    "this could be done by replacing the expectation by a sample approximation and doing gradient descent .",
    "we do however not implement it for the experiments in this paper .",
    "[ sec : causality ] structural equation models ( sems ) @xcite are one possibility to formalize causal statements .",
    "we say that a distribution over random variables @xmath126 is induced by a structural equation model with corresponding graph @xmath127 if each variable @xmath128 can be written as a deterministic function of its parents @xmath129 ( in @xmath127 ) and some noise variable @xmath130 : @xmath131 here , the graph is required to be acyclic and the noise variables are assumed to be jointly independent .",
    "an sem comes with the ability to describe _",
    "interventions_. intervening in the system corresponds to replacing one of the structural equations  .",
    "the resulting joint distribution is called an intervention distribution . changing the equation for variable",
    "@xmath128 usually affects the distribution of its children for example , but never the distribution of its parents .",
    "consider now an sem over variables @xmath132 . here",
    ", we do not specify the graphical relation between @xmath2 and the other nodes : @xmath2 may or may not have children or parents .",
    "suppose further that the different tasks @xmath133 are intervention distributions of an underlying sem with graph structure @xmath127 .",
    "if the target variable has not been intervened on , then the set  @xmath134 satisfies assumptions  [ ass : train](a ) and  [ ass : train](b ) .",
    "this means that as long as the interventions will not take place at the target variable , the set @xmath32 of causal parents will satisfy assumptions  [ ass : train](a ) and  [ ass : train](b ) .",
    "recently , @xcite have given several sufficient conditions for the identifiability of the causal parents in the linear gaussian framework .",
    "e.g. , if the interventions take place at informative locations , or if we see sufficiently many different interventions , the set of causal parents is the _ only _ set @xmath32 that satisfies assumptions  [ ass : train](a ) and  [ ass : train](b ) .",
    "if there exists more than one set leading to invariant predictions , they consider the intersection of all such subsets . in this sense , seeing more environments helps for identifying the causal structure .",
    "in this work , we are interested in prediction rather than causal discovery .",
    "therefore , we try to find a trade - off between models that predict well and invariant models that generalize well to other domains .",
    "that is , we are interested in the subset which leads to invariant conditionals and minimizes the prediction error across training tasks .    * inputs : * sample @xmath135 for tasks @xmath8 , threshold @xmath136 for independence test .",
    "* outputs : * estimated invariant subset @xmath137 .    set @xmath138",
    "[ sec : learninv ] in the previous section , we have seen how a known subset @xmath32 of predictors leading to invariant conditionals @xmath39 , see assumption  [ ass : train](a ) , can be beneficial in the problems of transfer learning and multi - task learning . in practice ,",
    "such a set @xmath32 is often unknown .",
    "we now present a method that aims at _ inferring _ such a subset @xmath32 from data . throughout this paper ,",
    "we denote by @xmath139 any subset of predictors , while @xmath32 is a subset leading to invariant predictions ( which is not necessarily unique ) .",
    "the method we propose provides an estimator @xmath137 of @xmath32 .",
    "q    [ [ our - method . ] ] our method .",
    "+ + + + + + + + + + +    consider a set of @xmath5 tasks , a target variable @xmath30 and a vector @xmath22 of  @xmath140 predictor variables in task  @xmath59 .",
    "we define a linear mapping from @xmath141 to @xmath142 and define the residual in task @xmath59 as : @xmath143 by assumption  [ ass : train](a ) applied to a linear setting ( see equation  ) , there exists a subset @xmath32 and some vector @xmath144 such that for all @xmath145 , @xmath146 and @xmath147 .",
    "such a set @xmath32 is not necessarily unique . as stated in @xcite , the number of invariant subsets decreases as more different tasks",
    "are observed at training time .",
    "we propose to do an exhaustive search over subsets @xmath139 of predictors and statistically test for equality of the distribution of the residuals in the training tasks ( see the section below ) . among the accepted subsets , we select the subset @xmath137 which leads to the smallest error on the training data .",
    "if the number of predictors @xmath148 is too large for such an exhaustive search , a variable selection technique such as the lasso  @xcite can be used as a first step .",
    "the procedure is summarized in algorithm  [ alg ] , code is provided in https://bitbucket.org/mrojascarulla/causal_transfer .    [",
    "[ statistical - tests - for - equality - of - distributions . ] ] statistical tests for equality of distributions .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    given a subset @xmath139 , denote by @xmath149 the linear regression coefficient obtained from regressing @xmath2 on @xmath150 using a pooled sample from all available training tasks .",
    "suppose now that also the index of the task can be considered as a random variable @xmath151 .",
    "we can consider that sample @xmath152 is drawn from a joint distribution over residuals and task indices , where @xmath153 and @xmath154 is a discrete value indicating the index of the corresponding task .",
    "the residuals have the same distribution in all training tasks if and only if @xmath155 and @xmath151 are independent .",
    "let therefore @xmath156 denote the value of the hsic  @xcite between @xmath155 and @xmath151 , and let @xmath157 be the corresponding test statistic .",
    "two characteristic kernels are used : a kernel @xmath158 is used for embedding the residuals and a trivial kernel @xmath159 such that @xmath160 is used for @xmath151 .",
    "we accept @xmath139 as an invariant set if we accept the null hypothesis of independence between @xmath155 and @xmath151 at level @xmath136 . in order to compute p - values ,",
    "a gamma approximation is used for the distribution of @xmath157 under the null .",
    "we select the subset @xmath137 which leads to accepting the null hypothesis and to the smallest loss on the training set .",
    "note that other tests , such as a levene test for equality of variances @xcite , can be used instead .",
    "( 1,2.5 ) node(x1 ) [ circle , draw ] @xmath161 ; ( 0.3,2 ) node(x2 ) [ circle , draw ] @xmath162 ; ( 1,1.5 ) node(x3 ) [ circle , draw ] @xmath163 ; ( 2,2 ) node(y ) [ circle , draw ] @xmath164 ; ( 3,2.5 ) node(x4 ) [ circle , draw ] @xmath165 ; ( 3.7,2 ) node(x5 ) [ circle , draw ] @xmath166 ; ( 3,1.5 ) node(x6 ) [ circle , draw ] @xmath167 ; ( 4,1 ) node(xw ) [ circle ] @xmath168 ; ( 0,1 ) node(xe ) [ circle ] @xmath168 ; ( x1 )  ( y ) ; ( x2 )  ( y ) ; ( x3 )  ( y ) ; ( y )  ( x4 ) ; ( y )  ( x5 ) ; ( y )  ( x6 ) ;    [ rem : level ] the level of the independence test @xmath136 is given as an input to algorithm  [ alg ] and allows for a trade - off between predictive accuracy and exploiting invariance .",
    "as @xmath136 tends to zero , @xmath169 is rejected for all subsets .",
    "algorithm  [ alg ] then selects all predictors ( high predictive accuracy ) and is equivalent to pooling the data . on the other hand , when @xmath136 is slightly larger than one , no subset is accepted as invariant .",
    "hence , our method reduces to the mean prediction .",
    "l|xestimator & description + @xmath170 & linear regr .  with true causal predictors ( often unknown in practice ) .",
    "+ @xmath171 & update the invariant conditional from the ground truth using   and  .",
    "+ @xmath172 & finding the invariant set @xmath137 using algorithm  [ alg ] and performing lin .",
    "regr .  using predictors in @xmath137 .",
    "+ @xmath173 & update the invariant conditional with the available labeled sample from @xmath16 using and  .",
    "+ @xmath91 & pooling the training data and using linear regr .  ( or lasso if large @xmath148 ) .",
    "+ @xmath174 & pooling the training data and outputting the mean of the target .",
    "+ @xmath175 & ridge regression using only the available labeled sample from @xmath16 .",
    "+ @xmath176 & multi - task feature learning estimator  @xcite .",
    "+ @xmath177 & dica  @xcite with rbf kernel .",
    "0.46   and @xmath178 are of size  @xmath179 , such that @xmath180 is @xmath181-dimensional .",
    "the results show averages and standard deviations over @xmath182 repetitions .",
    "left : mtl setting , and we vary the number @xmath183 of available examples from @xmath16 at training time .",
    "we add @xmath181 noise variables so that @xmath180 is @xmath184-dimensional .",
    "right : tl setting , and we vary the number of tasks @xmath5 available at training time .",
    ", title=\"fig : \" ]    0.46   and @xmath178 are of size  @xmath179 , such that @xmath180 is @xmath181-dimensional .",
    "the results show averages and standard deviations over @xmath182 repetitions .",
    "left : mtl setting , and we vary the number @xmath183 of available examples from @xmath16 at training time .",
    "we add @xmath181 noise variables so that @xmath180 is @xmath184-dimensional .",
    "right : tl setting , and we vary the number of tasks @xmath5 available at training time .",
    ", title=\"fig : \" ]",
    "[ sec : experiments ]    we present results on synthetic data and on a real data set from genetics .",
    "we compare to different methods , which are summarized in table  [ tab : est ] .",
    "@xmath170 uses the ground truth for @xmath32 when it is known , @xmath172 corresponds to algorithm  [ alg ] , @xmath91 uses the pooled training data , @xmath176 performs the multi - task feature learning algorithm  @xcite for the mtl setting and @xmath177 performs dica  @xcite for tl . for dica , the kernel matrices are constructed using an rbf kernel , and the length - scale of the kernel is selected according to the median heuristic . in the mtl setting , we combine the invariance with task specific information using proposition  [ prop : combinelabels ] , resulting in regression coefficients @xmath173 and @xmath171 when the ground truth is known . in all cases , we use cross - validation to select the regularization parameter for the ridge regressions and the lasso .    [ [ synthetic - data - set ] ] synthetic data set + + + + + + + + + + + + + + + + + +    in this section , we generate a synthetic data set in which the causal structure of the problem is known .",
    "we consider @xmath179 causal variables and @xmath179 effect variables . as for all experiments ,",
    "we choose @xmath185 as a rejection level for the statistical test in algorithm  [ alg ] .",
    "for each task @xmath186 , we sample a set of causal variables from a multivariate gaussian @xmath187 where the covariance matrix @xmath188 is computed as @xmath189 where @xmath190 is a @xmath191 matrix of uniformly distributed random variables with values between @xmath192 and @xmath193 .",
    "the target variable @xmath30 is drawn as @xmath194 where @xmath195 . finally , we sample the remaining predictor variables as @xmath196 where @xmath197 and the gram matrices are sampled similarly to @xmath188 . both @xmath71 and @xmath76",
    "are sampled from a uniform distribution , @xmath198 and @xmath199 respectively .",
    "the vector @xmath71 is the same in all tasks .",
    "our goal is to linearly predict target @xmath200 using predictors @xmath201 on the test domain . given regression coefficient @xmath202 , we measure the performance in the test task using the logarithm of the empirical estimator of @xmath203 . in figure",
    "[ fig : synt_numex ] ( left ) , we consider an mtl setting . when few labeled examples from the test task are observed @xmath173 performs similarly to @xmath176 and performs slightly better than @xmath175 . at larger sample size , @xmath173 and @xmath175",
    "converge to the same solution and outperform @xmath176 .    in figure",
    "[ fig : synt_numex ] ( right ) , we are in the tl setting ( thus , no labeled examples from @xmath16 are observed at training ) .",
    "we see that when more than six training tasks are available , algorithm  [ alg ] is able to recover an invariant subset and outperforms pooling the data , as well as @xmath177 , which is doing mean prediction . in this case , @xmath172 performs like @xmath170 , which uses knowledge of the ground truth . with fewer training tasks",
    ", however , it fails at extracting the invariant structure and performs similarly to @xmath91 .",
    "[ [ gene - perturbation - experiment ] ] gene perturbation experiment + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we apply our method to gene perturbation data from  @xcite .",
    "this data set consists of the m - rna expression levels of @xmath204 genes of the saccharomyces cerevisiae ( yeast ) .",
    "it contains both @xmath205 observational examples and @xmath206 examples from intervention experiments . in each of these interventions , one known gene ( out of @xmath148 genes ) is deleted .",
    "we address the problem of predicting the activity of a given gene from the remaining genes .",
    "we then consider two training tasks @xmath207 , the first task consists of the observational sample and the second task contains the pooled sample of all @xmath208 interventions ( we shall subsequently remove some points for testing , see below ) .",
    "for computational reasons , we first select the @xmath181 top predictor variables using the lasso , and then apply algorithm  [ alg ] to select a set of invariant predictors @xmath137 out of those @xmath181 .",
    "we now construct test tasks with a single data point in the following manner : for each target gene , we consider those experiments in which one of the @xmath181 predictors has been intervened on .",
    "note that ( a ) not all of such experiments exist , ( b ) these data points were removed from the training sample and ( c ) this is an instance of tl .",
    "we expect two different scenarios : ( 1 ) if the intervened gene is a _ cause _ of the target gene , it should still be a good predictor ( see section  [ sec : causality ] ) ; then , it should be beneficial to have this gene included in the set of predictors .",
    "( 2 ) if the intervened gene is anticausal or confounded ( we refer to this scenario as _ non - causal _ ) , however , the statistical relation to the target gene might change dramatically after the intervention and therefore , one may not want to base the prediction on this gene .    in order to see this effect and understand how the different approaches for tl in table  [ tab : est ]",
    "can handle it , we consider two scenarios .",
    "( 1 ) we select the target genes @xmath2 for which one of the predictors is causal for the activity of @xmath2 and has been intervened on .",
    "( here , we use the definition of a causal effect proposed by @xcite . )",
    "@xmath209 cases fall in this causal scenario .",
    "that is , we have @xmath209 _ experiments _ , each of which corresponds to two training tasks ( data from observations and interventions ) and an unknown test task with a single data point .",
    "( 2 ) out of the remaining experiments we chose target genes with ( non - causal ) predictors that have been intervened on and  in order to increase the difficulty of the problem  that are strongly correlated with the target gene .",
    "we therefore select @xmath210 cases for which a pearson correlation test ( the null hypothesis corresponds to no correlation ) outputs a p - value equal to zero .",
    "figure  [ fig : bp_genes ] shows box plots for the errors of the different methods for the causal scenario ( 1 ) on the left and for the non - causal scenario ( 2 ) in the middle .",
    "we do not plot outliers in order to improve presentation .",
    "figure  [ fig : bp_genes ] ( right ) shows that in the non - causal scenario ( 2 ) , prediction using an invariant subset leads to less large mistakes on test genes than pooling the tasks .    for comparison ,",
    "since we know which predictors are being intervened on at test time , we included a method that makes use of causal knowledge : @xmath170 uses all @xmath181 predictors in the causal scenario ( 1 ) and all but the intervened gene for in the non - causal scenario ( 2 ) . in practice , this causal knowledge is often not available . we regard it as promising that the fully automated procedure @xmath172 performs comparably to @xmath170 .",
    "moreover , we left out a random @xmath211 sample from the training data .",
    "this set should be less challenging than the above .",
    "as expected , @xmath174 has little predictive power and performs worse than @xmath91 , @xmath172 and @xmath170 who all perform similarly ( not shown ) .",
    "[ sec : conclusion ] we propose a method for domain adaptation that is motivated by causal modeling and exploits a set of invariant predictors . if the underlying causal structure is known and the tasks correspond to interventions on variables other than the target variable , the causal parents of the target variable constitute such a set of invariant predictors .",
    "we proved that predicting using an invariant subset is optimal in an adversarial setting , and illustrated attractive properties of such an approach in specific examples . in practice , the invariant structure may not be known . for these cases , we proposed an algorithm that automatically detects subsets of invariant predictors , while also focusing on good prediction .",
    "we show that our algorithm successfully finds a set of predictors leading to invariant conditionals when enough training tasks are available .",
    "our method can incorporate additional data from the test task and outperforms other methods on synthetic data .",
    "although an invariant subset may not always exist , our experiment on real data indicates that exploiting invariance leads to methods which are robust against domain adaptation .",
    "we expect that feature maps leading to invariant conditionals may be found in other ways than testing all possible subsets .",
    "extending our framework to non - linear mappings seems straight - forward and may prove to be useful for many applications .",
    "we believe , finally , that the link to causal assumptions and the exploitation of causal structure may lend itself well to proving additional theoretical results on transfer and multi - task learning .",
    "@xmath168    0 consider a function @xmath212 that is not the same as @xmath213 from  , say @xmath214 , but miniizes  . by continuity of the functions we then have an @xmath215 and a subset @xmath216 around @xmath217 s.t .",
    "@xmath218 for all @xmath219 .",
    "we will now construct a distribution @xmath220 over @xmath221 such that @xmath222 and thus @xmath223 .",
    "but then @xmath224 where @xmath225 is the expectation with respect to @xmath220 and @xmath226 holds because @xmath227 ) = 0 $ ] .",
    "this leads to a contradiction and therefore proves the result .    consider a function @xmath212 that is possibly different from @xmath228 , see  .",
    "for each distribution @xmath229 , we will now construct a distribution @xmath230 such that @xmath231 in this proof , we assume that the probability distributions in @xmath232 are absolutely continuous with respect to lebesgue measure . the extension to the case where they are absolutely continuous with respect to a same product measure",
    "@xmath52 is straightforward .",
    "let us therefore assume that @xmath233 has a density @xmath234 .",
    "define @xmath235 to be the distribution that corresponds to @xmath236 , where @xmath237 contains all components of @xmath238 that are not in @xmath75 . in the distribution @xmath235 ,",
    "the random vector @xmath239 is independent of @xmath240 .",
    "but then @xmath241 @xmath168 @xmath168 + @xmath168 @xmath168      @xmath168    without loss of generality , assume that all variables have zero mean .",
    "let @xmath242 be the set of parents ( causes ) and let @xmath178 be the set of direct descendants ( effects ) of @xmath2 . in task @xmath59",
    ", we have : for @xmath243 , @xmath244 and for @xmath245 . here , the variance @xmath246 of the target is the same for all tasks @xmath8 .",
    "we consider interventions in the different tasks as a distribution over the variances of each node in the graph ( other than the target ) .",
    "more precisely , for task @xmath59 and variable @xmath90 , @xmath247 and @xmath248 , where @xmath249 has finite variance @xmath250 .",
    "let @xmath251 be the residual in task @xmath59 for parameter @xmath202 .",
    "given the values of the training variances @xmath252 , @xmath253 follows a gaussian distribution with mean zero and variance @xmath254 .",
    "consider now an unseen task @xmath16 .",
    "we compute the expected squared loss for some @xmath202 : @xmath255 the noise variables are jointly independent , thus the variance of the previous quantity is equal to @xmath256 .",
    "this variance is clearly minimal for @xmath257 if @xmath258 and @xmath259 for @xmath260 . for these parameters ,",
    "the variance equals zero .",
    "we consider three variables and the following generative process : @xmath261 , @xmath262 , where @xmath263 , @xmath264 and @xmath265 . in this model , @xmath76 is the parameter responsible for the difference between the tasks , while the other parameters are shared between the tasks .    at training time , @xmath5 tasks are available .",
    "we first aim to obtain an explicit formula for the linear regression coefficients @xmath84 obtained from pooling all the training tasks together .",
    "denote by @xmath180 , @xmath2 and @xmath266 the pooled training data . for fixed @xmath267 ,",
    "the expected loss in the training data satisfies for coefficient @xmath202 verifies : @xmath268 by differentiating   with respect to @xmath202 , we obtain the following expression for the pooled coefficients : @xmath85 where @xmath269 and @xmath270 were introduced in the proposition statement .",
    "consider now an unseen test task with coefficient @xmath119 .",
    "the expected loss on the test task using the pooled coefficients is : @xmath271 therefore , the expectation with respect to @xmath119 is : @xmath272 denote by @xmath273 the expected loss when using the invariant conditional predictor @xmath274 .",
    "then : @xmath275 by replacing @xmath276 .",
    "this inequality holds true for any value of the variance @xmath98 , and the pooled coefficient leads to larger error in expectation .",
    "consider now that the coefficients @xmath76 are centered around a non - zero value @xmath52 .",
    "then the expectation with respect to @xmath119 of the loss in the test task is the following : @xmath277 then , if @xmath94 : @xmath278 where @xmath279 and @xmath280 .    0 in addition , assume that the task specific component verifies @xmath281 .",
    "consider equation  .",
    "we can compute the probability that the pooled error on a given test task is larger than the error using the invariant coefficient . since @xmath282 :    @xmath283    by simplifying as with the expectation , we obtain the desired probability statement .",
    "@xmath168    to simplify notation , we write @xmath200 , @xmath284 and @xmath285 as @xmath2 , @xmath106 and @xmath239 . we compute the gradients of the expected squared loss after replacing the expression for @xmath2 and @xmath106 : @xmath286 the gradients satisfy @xmath287 by setting these to zero , we find the stated values for @xmath288 and @xmath289 .    0      first , we prove the following result :    [ prop : monot ] let @xmath290 and let @xmath291 .",
    "then @xmath212 is a non - decreasing , continuous function of @xmath292 and @xmath97 is a non - increasing , continuous function of @xmath292 .",
    "we function to be optimized is the following : @xmath293 it is easy to show that there exists a constant @xmath71 such that   is equivalent to the following constrained optimization problem : @xmath294 where @xmath71 _ decreases _ linearly with increasing @xmath292 .",
    "consider now @xmath295 and @xmath296 such that @xmath297 .",
    "this implies that the corresponding @xmath71s verify @xmath298 .",
    "thus , for @xmath296 , the space of feasible solutions ( the space of @xmath202s such that @xmath299 ) is contained in the space of feasible solutions for the problem corresponding to @xmath295 .",
    "this implies that @xmath300 .",
    "therefore , @xmath212 is a non - decreasing function of @xmath292 .",
    "moreover , for continuous kernels and fixed kernel parameters , the value of the hsic statistic is continuous in its parameters .",
    "thus , @xmath212 is a continuous function of @xmath292 .",
    "similarly , one can show that there exists @xmath78 such that   is equivalent to : @xmath301 and @xmath78 _ increases _ linearly with increasing @xmath292 .",
    "an argument similar to the previous one proves that @xmath97 is a non - increasing continuous function of @xmath292 .",
    "we now prove proposition  [ prop : optlambd ] .",
    "first , consider @xmath292 such that @xmath302 ( if no such @xmath292 exists , then our algorithm would stop since no linear map would lead to invariant predictions ) .",
    "then by definition of @xmath303 , @xmath304 .",
    "as this is true for any such @xmath292 , we have :    @xmath305    we now prove the inverse inequality .",
    "lemma  [ prop : monot ] proves that the function @xmath290 is non - decreasing and continuous in @xmath292 and @xmath306 is non - increasing in @xmath292 .",
    "note that the parameters of the kernels being used are assumed to be fixed , and the value of the @xmath307 function is the only quantity that changes in the independence test .",
    "therefore , the monotonicity of @xmath97 implies that the p - value of the independence test is a non - decreasing function of @xmath292 , and thus for all @xmath308 , @xmath309 and for @xmath310 , @xmath311 .",
    "moreover , since @xmath212 is non - decreasing and continuous , @xmath312 . since by construction @xmath313 , by continuity of @xmath212 , there exists @xmath314 such that @xmath315 .",
    "therefore , @xmath316 . by using  ,",
    "we conclude that :    @xmath317",
    "in nonlinear classification tasks , kernel methods embed the data points into higher dimensional spaces in which the data is expected to be linearly separable .",
    "this can be accomplished via the _ kernel trick _ : if an algorithm depends on the data only via dot products on the data points , kernels can be used to map the points into a high or infinite dimensional space without having to compute the feature expansion of the data explicitly .",
    "@xcite ( see also references therein ) extend this notion to embedding probability distributions into a hilbert space .",
    "this allows us , for example , to test whether two sets of samples are drawn from the same distribution @xcite .",
    "more precisely , let @xmath318 be a positive definite kernel and @xmath235 a probability distribution .",
    "let  @xmath319 be the reproducing kernel hilbert space ( rkhs ) uniquely defined by @xmath158 and denote by @xmath320 its dot product .",
    "we define the mean embedding @xmath321 as the unique element in @xmath319 such that for any function @xmath322 , @xmath323 . for characteristic kernels called characteristic ,",
    "the mapping @xmath324 is _ injective _ , see @xcite , that is two probability distributions @xmath235 and @xmath233 have the same mean embedding if and only if @xmath325 .",
    "this is at the center of the idea that we can assess a distance between distributions by estimating the distance of their mean embeddings in @xmath319 .",
    "in fact , the distance of the mean embeddings is closely related to the maximum mean discrepancy @xmath326 ; @xcite prove that @xmath327 where @xmath328 is the unit ball of @xmath319 .",
    "in particular , with the choice of a polynomial kernel , the corresponding mmd measures the difference between the distribution s moments , up to the moment corresponding to the kernel s degree . for universal kernels , such as the rbf kernel , the discrepancy is measured with respect to a large class of functions .",
    "we present a brief introduction to hsic following  @xcite . similarly to the previous section , let @xmath329 be a second kernel and @xmath127 its corresponding rkhs . to simplify notation",
    ", we write @xmath330 .",
    "define the cross - covariance operator @xmath331 such that for all @xmath332 , for all @xmath333 , @xmath334    theorem 6 in  @xcite proves that if @xmath158 and @xmath335 are universal kernels , the largest singular value of @xmath336 is zero if and only if @xmath29 and @xmath2 are independent .",
    "another possible norm of @xmath336 is the hilbert - schmidt norm , leading to the following population expression called the hilbert schmidt independence criterion : @xmath337 where @xmath338 and @xmath339 are independent copies of @xmath29 and @xmath2 .",
    "let @xmath341 be a sample from the joint distribution @xmath342 .",
    "let @xmath343 and @xmath344 be the kernel matrices computed on the sample and @xmath345 where @xmath346 is a vector of ones of size @xmath347 . then the following is a biased estimate of  : @xmath348 moreover , the distribution of @xmath349 under @xmath169 can be approximated by a gamma distribution @xmath350 with : @xmath351"
  ],
  "abstract_text": [
    "<S> methods of domain adaptation try to combine knowledge from several related domains ( or tasks ) to improve performance on a test domain . inspired by causal methodology </S>",
    "<S> , we assume that the covariate shift assumption holds true for a _ subset _ of predictor variables : the conditional of the target variable given this subset of predictors is invariant over all tasks . </S>",
    "<S> we prove that in an adversarial setting using this subset for prediction is optimal if no examples from the test task are observed . for a specific scenario , in </S>",
    "<S> which tasks are drawn from a meta distribution , further optimality results are available . </S>",
    "<S> we introduce a practical method which allows for automatic inference of the above subset and provide corresponding code . </S>",
    "<S> we present results on synthetic data sets and a gene deletion data set . </S>"
  ]
}