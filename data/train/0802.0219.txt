{
  "article_text": [
    "in the past three decades non - gaussian time series have attracted a lot of interest , see e.g. cox ( 1981 ) , kaufmann ( 1987 ) , kitagawa ( 1987 ) , shephard and pitt ( 1997 ) , and durbin and koopman ( 2000 ) , among others . in the context of regression modelling , generalized linear models ( mccullagh and nelder , 1989 ; dobson , 2002 )",
    "offer a solid theoretical basis for statistical analysis of independent non - normal data .",
    "a general framework for dealing with time series data is the dynamic generalized linear model ( dglm ) , which considers generalized linear modelling with time - varying parameters and hence it is capable to model time series data for a wide range of response distributions .",
    "dglms have been widely adopted for non - normal time series data , see e.g. west _ et al . _ ( 1985 ) , gamerman and west ( 1987 ) , fahrmeir ( 1987 ) , frhwirth - schnatter , s. ( 1994 ) , lindsey and lambert ( 1995 ) , chiogna and gaetan ( 2002 ) , hemming and shaw ( 2002 ) , godolphin and triantafyllopoulos ( 2006 ) , and gamerman ( 1991 , 1998 ) .",
    "dynamic generalized linear models are reported in detail in the monographs of west and harrison ( 1997 , chapter 14 ) , fahrmeir and tutz ( 2001 , chapter 8) , and kedem and fokianos ( 2002 , chapter 6 ) .    in this paper",
    "we propose a unified treatment of dglms that includes approximate bayesian inference and multi - step forecasting . in this to end we adopt the estimation approach of west _ et al . _",
    "( 1985 ) , but we extend it as far as model diagnostics and forecasting are concerned .",
    "in particular , we discuss likelihood - based model assessment as well as bayesian model monitoring . in the literature , discussion on the dglms is usually restricted to the binomial and the poisson models , see e.g. fahrmeir and tutz ( 2001 , chapter 8) . even for these response distributions",
    ", discussion is limited on estimation , while forecasting and in particular multi - step forecasting does not appear to have received much attention .",
    "we provide detailed examples of many distributions , including binomial , poisson , negative binomial , geometric , normal , log - normal , gamma , exponential , weibull , pareto , two special cases of the beta , and inverse gaussian .",
    "we give numerical illustrations for all distributions , except for the normal ( for which one can find numerous illustrations in the time series literature ) using real and simulated data .",
    "the paper is organized as follows . in section [ dglm ]",
    "we discuss bayesian inference of dglms .",
    "section [ examples ] commences by considering several examples , where the response time series follows a particular distribution .",
    "section [ discussion ] gives concluding comments .",
    "the appendix includes some proofs of arguments in section [ examples ] .",
    "suppose that the time series @xmath0 is generated from a probability distribution , which is a member of the exponential family of distributions , that is @xmath1 where @xmath2 , known as the natural parameter , is the parameter of interest and other parameters that can be linked to @xmath3 , @xmath4 , @xmath5 and @xmath6 are usually referred to as nuisance parameters or hyperparameters .",
    "the functions @xmath4 , @xmath5 and @xmath6 are assumed known , @xmath7 , @xmath8 is twice differentiable and according to dobson ( 2002 ,  3.3 ) @xmath9 the function @xmath10 is usually a simple function in @xmath11 and in many cases it is the identity function ; an exception of this is the binomial distribution . if @xmath12 , distribution ( [ exp ] ) is said to be in the _ canonical _ or _ standard _ form .",
    "dobson ( 2002 ,  3.3 ) gives expressions of the score statistics and the information matrix , although the consideration of these may not be necessary for bayesian inference .",
    "the idea of generalized linear modelling is to use a non - linear function @xmath13 , which maps @xmath14 to the linear predictor @xmath15 ; this function is known as link function . if @xmath16 , this is referred to as _ canonical link _ , but other links may be more useful in applications ( see e.g. the inverse gaussian example in section [ continuous ] ) . in glm theory , @xmath15 is modelled as a linear model , but in dglm theory , the linear predictor is replaced by a state space model , i.e. @xmath17 where @xmath18 is a @xmath19 design vector , @xmath20 is a @xmath21 evolution matrix , @xmath22 is a @xmath19 random vector and @xmath23 is an innovation vector , with zero mean and some known covariance matrix @xmath24 .",
    "it is assumed that @xmath23 is uncorrelated of @xmath25 ( for @xmath26 ) and @xmath23 is uncorrelated of @xmath27 , for all @xmath28 .",
    "it is obvious that if one sets @xmath29 ( the @xmath21 identity matrix ) and @xmath30 ( i.e. its covariance matrix is the zero matrix ) , then the above model is reduced to a usual glm .    for the examples of section [ examples ] we consider simple state space models , which assume that @xmath31 , @xmath32 , @xmath33 are time - invariant .",
    "however , in the next sections , we present bayesian inference and forecasting for time - varying @xmath18 , @xmath20 , @xmath24 in order to cover the general situation .",
    "suppose that we have data @xmath34 and we form the information set @xmath35 , for @xmath36 . at time @xmath37 we assume that the posterior mean vector and covariance matrix of @xmath38 are @xmath39 and @xmath40 , respectively , and we write @xmath41 .",
    "then from @xmath42 , it follows that @xmath43 , where @xmath44 and @xmath45 .    the next step is to form the prior mean and variance of @xmath15 and @xmath22 , that is @xmath46 \\right|y^{t-1 } \\sim   \\left(\\left [ \\begin{array}{c } f_t \\\\",
    "\\end{array}\\right ] , \\left [ \\begin{array}{cc } q_t & f_t'r_t \\\\ r_tf_t & r_t \\end{array } \\right ] \\right),\\ ] ] where @xmath47 and @xmath48 .",
    "the quantities @xmath49 and @xmath50 are the forecast mean and variance of @xmath15 .    in order to proceed with bayesian inference , we assume the conjugate prior of @xmath2 , so that @xmath51 for some known @xmath52 and @xmath53 . these parameters can be found from @xmath54 and @xmath55 , @xmath56 , which are known from ( [ prior : p2 ] ) .",
    "the normalizing constant @xmath57 can be found by @xmath58 where the integral is lebesque integral , so that it includes summation / integration of discrete / continuous variables .",
    "we note that in most of the cases , the above distribution will be recognizable ( e.g. gamma , beta , normal ) and so there is no need of evaluating the above integral .",
    "one example that this is not the case is the inverse gaussian distribution ( see section [ continuous ] ) .    then observing @xmath11 , the posterior distribution of @xmath2 is @xmath59 in many situations we are interested in parameters that are given as functions of @xmath2 .",
    "in such cases we derive the prior / posterior distributions of @xmath2 as above and then we apply a transformation to obtain the prior / posterior distribution of the parameter in interest .",
    "the examples of section [ examples ] are illuminative .",
    "finally , the posterior mean vector and covariance matrix of @xmath22 are approximately given by @xmath60 with @xmath61 where @xmath62 and @xmath63 can be found from @xmath54 and the posterior ( [ post : g1 ] ) .",
    "the priors ( [ prior : p2 ] ) , ( [ prior : g1 ] ) and the posteriors ( [ post : g1 ] ) , ( [ post : th1 ] ) provide an algorithm for estimation , for any @xmath36 . for a proof of the above algorithm the reader",
    "is referred to west _",
    "et al . _ ( 1985 ) .    an alternative approach for the specification of @xmath52 and @xmath53 is to make use of _ power discounting _ and this is briefly discussed next .",
    "the idea of power discounting stem in the work of smith ( 1979 ) ; power discounting is a method of obtaining the prior distribution at time @xmath64 , from the posterior distribution at time @xmath28 .",
    "here we consider a minor extension of the method by replacing @xmath64 by @xmath65 , for some positive integer @xmath66 .",
    "then , according to the principle of power discounting , the prior distribution at time @xmath65 is proportional to @xmath67 , where @xmath68 is a discount factor .",
    "thus we write @xmath69 this ensures that the prior distribution of @xmath70 is flatter than the posterior distribution of @xmath2 .",
    "the above procedure assumes that @xmath71 and @xmath72 , which implicitly assumes a random walk type evolution of the posterior / prior updating , in the sense that bayes decisions in the interval @xmath73 remain constant , while the respective expected loss ( under step loss functions ) increase ( smith , 1979 ) .",
    "suppose that the time series @xmath0 is generated by density ( [ exp ] ) and let @xmath74 be the information set up to time @xmath28 .",
    "then the @xmath66-step forecast distribution of @xmath75 is @xmath76 where @xmath77 and @xmath78 are evaluated from @xmath79 and @xmath80 , the mean and variance of @xmath81 , and the distribution of @xmath82 , which takes a similar form as the distribution of @xmath83 .",
    "model assessment can be done via the likelihood function , residual analysis , and bayesian model comparison , e.g. based on bayes factors .",
    "the likelihood function of @xmath84 , based on information @xmath85 is @xmath86 where the first probability in the product is the distribution ( [ exp ] ) and the second indicates the evolution of @xmath2 , given @xmath87 .",
    "then the log - likelihood function is @xmath88 the likelihood function can be used as a means of model comparison ( for example looking at two model specifications , which differ in some quantitative parts , we choose the model that has larger likelihood ) . for model assessment",
    "the likelihood function can be used in order to choose some hyperparameters ( discount factors , or nuisance parameters ) so that the likelihood function is maximized in terms of these hyperparameters .",
    "the evaluation of ( [ logl ] ) requires the distribution @xmath89 .",
    "this depends on the state space model for @xmath15 used . in the examples of section [ examples ] we look at these probabilities , based mainly on gaussian random walk evolutions for @xmath15 , but",
    "also we consider a linear trend model for @xmath15 .",
    "note that the consideration of @xmath23 following a gaussian distribution does not imply that @xmath90 follows a gaussian distribution too , since the distribution of @xmath27 may not be gaussian .",
    "for the sequential calculation of the bayes factors ( which for gaussian responses are discussed in salvador and gargallo , 2005 ) , a typical setting suggests the formation of two models @xmath91 and @xmath92 , which differ in some quantitative aspects , e.g. some hyperparameters .",
    "then , the cumulative bayes factor of @xmath91 against @xmath92 is defined by @xmath93 where @xmath94 , for all @xmath28 , and @xmath95 denotes the joint distribution of @xmath96 , given @xmath97 , for some integer @xmath98 and @xmath99 .",
    "then preference of model 1 would imply larger forecast distribution of this model ( or @xmath100 ) ; likewise preference of model 2 would imply @xmath101 ; @xmath102 implies that the two models are probabilistically equivalent in the sense they provide the same forecast distributions .",
    "the binomial distribution ( johnson _ et al . _ , 2005 ) is perhaps the most popular discrete distribution .",
    "it is typically generated as the sum of independent success / failure bernoulli trials and in the context of generalized linear modelling is associated with logistic regression ( dobson , 2002 ) .",
    "consider a discrete - valued time series @xmath0 , which , for a given probability @xmath103 , follows the binomial distribution @xmath104 where @xmath105 denotes the binomial coefficient .",
    "it is easy to verify that the above distribution is of the form ( [ exp ] ) with @xmath106 , @xmath107 , @xmath108 , @xmath109 , and @xmath110 .",
    "the logarithmic , known also as logit , link @xmath111 maps @xmath103 to the linear predictor @xmath15 , which with the setting @xmath112 and @xmath113 , generates the dynamic evolution of the model .",
    "the prior of @xmath114 , follows by the prior of @xmath83 and the transformation @xmath115 as beta distribution @xmath116 , with density @xmath117 where @xmath118 denotes the gamma function and @xmath119 . then , observing @xmath11 , the posterior of @xmath120 is @xmath121 .    in the appendix",
    "it is shown that , with @xmath49 and @xmath50 the prior mean and variance of @xmath15 , an approximation of @xmath52 and @xmath53 is given by @xmath122 in order to proceed with the posterior moments of @xmath90 as in ( [ post : th1 ] ) , we can see that @xmath123 where @xmath124 denotes the digamma function ( see the poisson example and the appendix ) . in the appendix approximations of @xmath124 and of its first derivative ( also known as trigamma function ) are given .",
    "these definitions as well as the parameters of the beta prior are slightly different from the ones obtained by west and harrison ( 1997 ) , as these authors use a different parameterization , which does not appear to be consistent with the prior / posterior updating .    given information @xmath74 , the @xmath66-step forecast distribution is obtained by first noting that @xmath125 where @xmath77 and @xmath78 are given by @xmath52 and @xmath53 , if @xmath49 and @xmath50 are replaced by @xmath126 and @xmath127 , which are calculated routinely by the kalman filter ( see section [ dglm ] )",
    ". then the @xmath66-step forecast distribution is given by @xmath128 we can use conditional expectations in order to calculate the forecast mean and variance , i.e. @xmath129 and @xmath130    for the specification of @xmath52 and @xmath53 , we can alternatively use power discounting ( see section [ dglm ] ) .",
    "this yields @xmath131 where @xmath68 is a discount factor and @xmath132 are initially given .    for the evolution of @xmath15 via @xmath22 ,",
    "the obvious setting is the random walk , which sets @xmath133 . from the logit link",
    "we have @xmath134 and so the evolution of @xmath22 yields @xmath135 which gives the evolution of @xmath103 , given @xmath136 , as a function of the gaussian shock @xmath23 .",
    "then the distribution of @xmath137 is @xmath138 and so from ( [ logl ] ) the log - likelihood function is @xmath139 the bayes factors are easily computed from ( [ bf1 ] ) and the forecast distribution @xmath140 .",
    "if we use a linear trend evolution on @xmath22 , we can specify @xmath141\\left[\\begin{array}{c } \\theta_{1 t } \\\\ \\theta_{2t}\\end{array}\\right ] \\quad \\textrm{and } \\quad \\left[\\begin{array}{c } \\theta_{1 t } \\\\",
    "\\theta_{2t}\\end{array}\\right ] = \\left[\\begin{array}{cc } 1 & 1 \\\\ 0 & 1 \\end{array}\\right ] \\left[\\begin{array}{c } \\theta_{1,t-1 } \\\\",
    "\\theta_{2,t-1}\\end{array}\\right ] + \\left[\\begin{array}{c } \\omega_{1 t } \\\\",
    "\\omega_{2t}\\end{array}\\right].\\ ] ] here @xmath142'$ ] is a 2-dimensional random vector and @xmath143'$ ] follows a bivariate normal distribution with zero mean vector and some known covariance matrix .",
    "then , conditional on @xmath136 , from the logit link function we can recover the relationship of @xmath103 as @xmath144    to illustrate the binomial model , we consider the data of godolphin and triantafyllopoulos ( 2006 ) , consisting of quarterly binomial data over a period of 11 years . in each quarter",
    "@xmath145 bernoulli trials are performed and @xmath11 , the number of successes , is recorded .",
    "the data , which are plotted in figure [ fig1 ] , show a clear seasonality and therefore , modelling this data with glms is inappropriate . the data exhibit",
    "a trend / periodic pattern , which can be modelled with a dglm , by setting @xmath112 and @xmath113 , where the design vector @xmath146 has dimension @xmath147 and the @xmath148 evolution matrix @xmath149 comprises a linear trend component and a seasonal component .",
    "one way to do this is by applying the trend / full harmonic state space model @xmath150 \\quad \\textrm{and } \\quad g=\\left [ \\begin{array}{ccccc } 1 & 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & \\cos ( \\pi/2 ) & \\sin ( \\pi/2 ) & 0 \\\\ 0 & 0 & -\\sin ( \\pi/2 ) & \\cos ( \\pi/2 ) & 0 \\\\ 0 & 0 & 0 & 0 & -1 \\end{array}\\right],\\ ] ] where @xmath149 is a block diagonal matrix , comprising the linear trend component and the seasonal component , for the latter of which , with a cycle of @xmath151 , we have @xmath152 harmonics and the frequencies are @xmath153 for harmonic 1 and @xmath154 for harmonic 2 ( the nyquist frequency ) .",
    "similar models , with gaussian responses , are described in west and harrison ( 1997 ) , and harvey ( 2004 ) .",
    "the covariance matrix @xmath155 of @xmath23 is set as the block diagonal matrix @xmath156 , where @xmath157 corresponds to the linear trend component , @xmath158 corresponds to the seasonal component and it is chosen so that the trend has more variability than the seasonal component ( west and harrison , 1997 ) . the priors @xmath159 and @xmath160 are set as @xmath161'$ ] and @xmath162 , suggesting a weakly informative prior specification .",
    "figure [ fig1 ] plots the one - step forecast mean of @xmath0 against @xmath0 .",
    "we see that the forecasts fit the data very closely proposing a good model fit .      in the context of generalized linear models , the poisson distribution ( johnson _ et al .",
    "_ , 2005 ) is associated with modelling count data ( dobson , 2002 ) . in a time series setting count data",
    "are developed as in jung _",
    "( 2006 ) .",
    "suppose that @xmath0 is a count time series , so that , for a positive real - valued @xmath163 , @xmath164 follows the poisson distribution , with density @xmath165 where @xmath166 denotes the factorial of @xmath11 .",
    "we can easily verify that this density is of the form ( [ exp ] ) , with @xmath12 , @xmath167 , @xmath168 , @xmath169 , and @xmath170 .",
    "we can see that @xmath171 and @xmath172 .    from the prior of @xmath83 and the transformation @xmath168",
    ", we obtain the prior of @xmath173 as a gamma distribution , i.e. @xmath174 , with density @xmath175 for @xmath176 .",
    "then it follows that the posterior of @xmath177 is the gamma @xmath178 .    for the definition of @xmath52 and @xmath53 we use the logarithmic link @xmath179 or @xmath180 . based on an evaluation of the mean and variance of @xmath181 and a numerical approximation of the digamma function ( see appendix )",
    ", we can see @xmath182 where @xmath49 and @xmath50 are the mean and variance of @xmath15 .    for the computation of @xmath183 and @xmath184 , the posterior mean and variance of @xmath2 , first define the digamma function @xmath124 as @xmath185 , where @xmath118 denotes the gamma function and of course @xmath186 .",
    "then we have @xmath187 which can be computed by the recursions @xmath188 and @xmath189 . using the approximations @xmath190 and @xmath191 , we can write @xmath192 with @xmath52 , @xmath53 , @xmath183 and @xmath184 we can compute the first two moments of @xmath90 as in ( [ post : th1 ] ) . for a detailed discussion on digamma functions the reader",
    "is referred to abramowitz and stegun ( 1964 ,  6.3 ) .    defining @xmath77 and @xmath78 according to @xmath79 and @xmath80 and equation ( [ eq : poisson : rt ] ) ,",
    "the @xmath66-step forecast distribution of @xmath193 is given by @xmath194 which is a negative binomial distribution .",
    "the forecast mean and variance can be calculated by using conditional expectations , i.e. @xmath195 and @xmath196    the power discounting yields @xmath197    considering the random walk evolution for @xmath22 so that @xmath133 , where @xmath198 , for some variance @xmath155 , we can see that @xmath199 since @xmath200 .",
    "then from the normal distribution of @xmath23 , the distribution of @xmath201 is @xmath202 which is a log - normal distribution ( see section [ continuous ] ) .",
    "tnen from ( [ logl ] ) the log - likelihood function is @xmath203 bayes factors can be calculated using ( [ bf1 ] ) and the negative binomial one - step ahead forecast probability functions @xmath204 .    in order to illustrate the poisson model we consider us annual immigration data , in the period of 1820 to 1960 .",
    "the data , which are described in kendall and ord ( 1990 , page 13 ) , are shown in figure [ fig2 ] .",
    "the nature of the data fits to the assumption of a poisson distribution , but it can be argued that , after applying a suitable transformation , some gaussian time series model can be appropriate .",
    "the data are non - stationary and a visual inspection shows that they exhibit a local level behaviour .",
    "one simple model to consider is the random walk evolution of @xmath205 as described above .",
    "we use power discounting with @xmath206 , which is a low discount factor capable to capture the peak values of the data .",
    "figure [ fig2 ] shows the one - step forecast mean against the actual data and as we see the forecasts capture well the immigration data .      the negative binomial distribution ( johnson _ et al .",
    "_ , 2005 ) arises in many practical situations and it can be generated via independent bernoulli trails or via the poisson / gamma mixture . in time",
    "series analysis , an application of negative binomial responses is given in houseman _",
    "we note that the negative binomial distribution includes the geometric as a special case ( see below ) .",
    "suppose that the time series @xmath0 is generated from the negative binomial distribution , with probability function @xmath207 where @xmath103 is the probability of success and @xmath208 is the number of successes .",
    "this distribution belongs to the exponential family ( [ exp ] ) , with @xmath12 , @xmath167 , @xmath209 , @xmath210 , and @xmath211 .",
    "then it follows that @xmath212 and @xmath213 .",
    "we note that by setting @xmath214 and @xmath215 , the time series @xmath216 follows a geometric distribution and thus all what follows applies readily to the geometric distribution too .    by using the prior of @xmath83 and the transformation @xmath209 ,",
    "the prior of @xmath114 is the beta distribution @xmath217 and the posterior of @xmath120 is the beta @xmath218 . using the logit link , as in the binomial example",
    ", the definitions of @xmath52 and @xmath53 are @xmath219 and the posterior moments @xmath183 and @xmath184 are @xmath220 which can be approximated by @xmath221 and @xmath222 thus we can compute the moments of @xmath90 as in ( [ post : th1 ] ) and so we obtain an approximation of the quantities @xmath77 and @xmath78 , as functions of @xmath79 and @xmath80 .    the @xmath66-step forecast distribution is given by @xmath223 the forecast mean and variance of @xmath75 are given by @xmath224 and @xmath225    the power discounting yields @xmath226 where as usual @xmath68 is a discount factor .",
    "considering the random walk evolution for @xmath133 , the link @xmath227 , yields the evolution for @xmath103 @xmath228 given that @xmath229 , for a known variance @xmath155 , the distribution of @xmath137 is @xmath230 and so from ( [ logl ] ) the log - likelihood function is @xmath231 bayes factors can be computed using ( [ bf1 ] ) and the predictive distribution @xmath204 .    to illustrate the above model we have simulated 100 observations from the above model ; we simulate one draw from @xmath232 so that @xmath233 , we simulate 100 innovations @xmath234 from a @xmath235 , then using ( [ nb : pi ] ) we generate @xmath236 and finally , for each time @xmath28 , we simulate one draw from a negative binomial with parameters @xmath237 and @xmath103",
    ". figure [ fig2a ] shows the simulated data ( solid line ) together with the one - step ahead forecast means @xmath238 .",
    "for the fit , we pretend we did not have knowledge of the simulation process and so we have specified @xmath239'$ ] , @xmath240 ( the @xmath241 identity matrix ) , @xmath242'$ ] , and @xmath243 , the last indicating a weakly informative prior specification ( i.e. @xmath244 ) .",
    "we observe that the forecasts follow the data closely indicating a good fit .",
    "we have found that as it is well known for gaussian time series , these prior settings are insensitive to forecasts , since prior information is deflated with time .        normal or gaussian time series are discussed extensively in the literature , see e.g. west and harrison ( 1997 ) for a bayesian treatment of gaussian state - space models . here",
    "we discuss gaussian responses in the dglm setting , for completeness purposes , but also because the normal distribution has many similarities with the log - normal distribution that follows .",
    "suppose that @xmath0 is a time series generated from a normal distribution , i.e. @xmath245 , with density @xmath246 where @xmath247 is the level of @xmath11 . the variance @xmath248 of the process can be time - varying , but for simplicity here , we assume it time - invariant . here , this variance is assumed known , while @xmath247 is assumed unknown .",
    "if @xmath248 is unknown , bayesian inference is possible by assuming that @xmath249 follows a gamma distribution and this model leads to a conjugate analysis ( resulting to a posterior gamma distribution for @xmath249 and to a student @xmath28 distribution for the forecast distribution of @xmath75 ) .",
    "this model is examined in detail in west and harrison ( 1997 , chapter 4 ) .",
    "returning to the above normal density , we can easily see that @xmath250 is of the form of ( [ exp ] ) , with @xmath12 , @xmath251 , @xmath252 , @xmath253 and @xmath254 .",
    "the prior for @xmath255 is the normal distribution @xmath256 and the posterior of @xmath257 is the normal distribution @xmath258 the link function is the identity link , i.e. @xmath259 and so we have @xmath260 , which implies @xmath261 and @xmath262 . by replacing these quantities in the above prior and posterior densities",
    ", we can verify the kalman filter recursions .",
    "it turns out that the @xmath66-step forecast distribution is also a normal distribution , i.e. @xmath263    the power discounting yields @xmath264    adopting the random walk evolution for @xmath265 , from the identity link @xmath266 , we have that @xmath267 , where @xmath229 . from ( [ logl ] )",
    "the log - likelihood function is @xmath268 bayes factors can be easily computed from the forecast density @xmath204 and the bayes factor formula ( [ bf1 ] ) .",
    "the log - normal distribution has many applications , e.g. in statistics ( johnson _ et al .",
    "_ , 1994 ) , in economics ( aitchison and brown , 1957 ) , and in life sciences ( limpert _ et al .",
    "_ , 2001 ) .",
    "suppose that the time series @xmath0 is generated from a log - normal distribution , with density @xmath269 where @xmath270 .",
    "we will write @xmath271 .",
    "this distribution is of the form of ( [ exp ] ) , with @xmath272 , @xmath251 , @xmath273 , @xmath253 and @xmath274 .",
    "from the normal part we can see @xmath275 and from the log - normal part we can see @xmath276 from the latter of which the logarithmic link can be suggested , i.e. @xmath277 .    from the normal distribution of @xmath278 , it follows that the prior distribution of @xmath173 is @xmath279 and the posterior distribution of @xmath280 is @xmath281 where @xmath52 and @xmath53 are calculated as in the normal case , i.e. @xmath261 and @xmath262 . with the definitions of @xmath77 and @xmath78",
    ", we have that the @xmath66-step forecast distribution of @xmath75 is @xmath282 the forecast mean of @xmath75 is @xmath283 where @xmath79 and @xmath80 are the respective mean and variance of @xmath284 , given information @xmath74 .",
    "considering power discounting , the updating of @xmath52 and @xmath53 is @xmath285    adopting the random walk evolution for @xmath133 , the distribution of @xmath201 is normal , i.e. @xmath286 , where @xmath155 is the variance of @xmath23 . from ( [ logl ] ) the log - likelihood function is obtained as @xmath287 bayes factors can be calculated from ( [ bf1 ] ) and the log - normal predictive density @xmath204 . as an example , consider the comparison of two models @xmath91 and @xmath92 , which differ in the variances @xmath288 and @xmath289 , respectively .",
    "then , by denoting @xmath290 , @xmath291 , @xmath292 and @xmath293 , the values of @xmath52 , @xmath53 , for @xmath294 @xmath295 , we can express the logarithm of the bayes factor @xmath296 as @xmath297 by comparing @xmath298 to 0 , we can conclude preference of @xmath91 or @xmath92 , i.e. if @xmath299 we favour @xmath91 , if @xmath300 we favour @xmath91 , while if @xmath301 the two models are equivalent , in the sense that they both produce the same one - step forecast distributions .",
    ".mean square error ( mse ) and log - likelihood function @xmath302 for several values of the discount factor @xmath68 for the log - normal data . [ cols=\"^,^,^,^,^,^,^,^,^,^ \" , ]      the inverse gaussian or wald ( chhikara and folks , 1989 ; johnson _ et al .",
    "_ , 1994 ) is a skewed distribution that can describe phenomena in economics and in many other sciences .",
    "this distribution is known as the first passage time distribution of brownian motion with positive drift .",
    "recently , huberman _",
    "( 1998 ) used an inverse gaussian distribution to model internet flow and internet traffic .",
    "suppose that the time series @xmath0 is generated from an inverse gaussian distribution , that is for given @xmath247 and @xmath177 , the density function of @xmath11 is @xmath303 this is a unimodal distribution , which converges to the normal distribution , as @xmath304 . to the following",
    "we will assume that @xmath177 is a known parameter and interest will be placed on @xmath247 ; hence we write @xmath305 .",
    "we can see that the above distribution is of the form of ( [ exp ] ) , with @xmath12 , @xmath306 , @xmath307 , @xmath308 , @xmath309 and @xmath310 .",
    "then we can verify that @xmath311 and @xmath312    the canonical link maps @xmath247 to @xmath2 , or @xmath313 , but this is not convenient , since @xmath314 and hence we need to find an appropriate definition of @xmath146 and @xmath149 in the state space representation of @xmath54 in order to guarantee @xmath315 .",
    "the logarithmic link , @xmath316 , seems to work better , since it maps @xmath247 to the real line and so @xmath317 is defined easily .",
    "the prior distribution of @xmath247 can be defined via the prior distribution of @xmath2 and the transformation @xmath308 . in the appendix",
    "it is shown that @xmath318 in the appendix it is shown that @xmath319    the posterior distribution of @xmath247 is obtained from the posterior distribution of @xmath2 as @xmath320 where in the appendix it is shown that @xmath321 the approximation of @xmath52 and @xmath53 is difficult , since the moment generating function of @xmath322 ( which is needed in order to compute @xmath52 and @xmath53 ) is not available in close form .",
    "thus power discounting should be applied . from the posterior of @xmath323 , given by ( [ post : g1 ] ) , we have @xmath324 and so from the prior of @xmath325 ( equation ( [ prior : g1 ] ) ) and the power discounting law we obtain @xmath326 with @xmath71 and @xmath72 ,",
    "the @xmath66-step forecast distribution of @xmath193 is @xmath327 where the normalizing constant @xmath328 is @xmath329 the @xmath66-step forecast mean can be deduced by ( [ ig : prior : m ] ) as @xmath330    of course the above power discounting specifies @xmath52 and @xmath53 , for a random walk type evolution for the prior ( [ eq : igaussian:2 ] ) . following this , we can specify @xmath331 , with @xmath229 , and so @xmath332 which leads to the density @xmath333 therefore , using ( [ logl ] ) , the log - likelihood function is @xmath334 bayes factors can be easily computed from @xmath204 and the bayes factor formula ( [ bf1 ] ) .    to illustrate the inverse gaussian distribution we consider data consisting of 30 daily observations of toluene exposure concentrations ( tec ) for a single worker doing stain removing .",
    "the data can be found in takagi _",
    "( 1997 ) who propose a simple model fit using maximum likelihood estimation for the inverse gaussian distribution .",
    "however , it may be argued that these data are autocorrelated and so an appropriate time series should be fitted .",
    "figure [ fig : ig1 ] shows one - step forecasts means against the tec data .",
    "the forecast means are computed using the above dglm model for the inverse gaussian response , using @xmath335 .",
    "the results show that a low value of the discount factor @xmath206 and a low value of @xmath336 yield the best forecasts .",
    "the posterior mean @xmath337 is plotted in figure [ fig : ig2 ] , from which we can clearly see that there is a time - varying feature of the parameters of the inverse gaussian distribution .",
    "this is failed to be recognized in takagi _",
    "these authors propose estimates for the mean and the scale of the inverse gaussian distribution as 16.7 and 6.4 , which are both larger than the mean of the posterior means @xmath338 and @xmath336 .",
    "we note that from figure [ fig : ig1 ] as @xmath339 increases , the forecast performance deteriorates so that a value of @xmath339 near 6.4 would yield poor forecast accuracy .",
    "the model we propose here exploits the dynamic behaviour of @xmath247 and it is an appropriate model for forecasting .",
    "in this paper we discuss approximate bayesian inference of dynamic generalized linear models ( dglms ) , following west _ et al . _ ( 1985 ) and co - authors .",
    "such an approach allows the derivation of the multi - step forecast distribution , which is a useful consideration for carrying out error analysis based on residuals , on the likelihood function , or on bayes factors .",
    "we explore all the above issues by examining in detail several examples of distributions including binomial , poisson , negative binomial , geometric , normal , log - normal , gamma , exponential , weibull , pareto , two special cases of the beta , and inverse gaussian .",
    "we believe that dglms offer a unique statistical framework for dealing with a range of statistical problems , including business and finance , medicine , biology and genetics , and behavioural sciences . in most of these areas ,",
    "researchers are not well aware of the advantages that bayesian inference for dglms can offer . in this context",
    "we believe that the present paper offers a clear description of the methods with detailed examples of many useful response distributions .",
    "first we calculate the mean and variance of the log - gamma and the log - beta distributions .",
    "let @xmath340 follow the gamma distribution with parameters @xmath341 and @xmath342 , with density function @xmath343 where @xmath118 denotes the gamma function and @xmath344 .",
    "the density function of @xmath345 is @xmath346 the moment generating function of @xmath347 is @xmath348 and the cumulant generating function is @xmath349 .",
    "then we have @xmath350 where @xmath124 is the digamma function , which is defined by @xmath185 and the derivative @xmath124 is known as the trigamma function ( abramowitz and stegun , 1964 ) .    for the log - beta distribution ,",
    "let @xmath340 follow the beta distribution , with density function @xmath351 where @xmath344 and @xmath352 .",
    "the density function of @xmath353 is @xmath354 with moment generating function @xmath355 for @xmath356 .",
    "the cumulant generating function is @xmath357 and so @xmath358      thus , for the calculation of @xmath52 and @xmath53 in equation ( [ eq : binom : rt ] ) , from the prior @xmath116 , we have @xmath364 and @xmath365 we obtain ( [ eq : binom : rt ] ) by solving ( [ rt : solv1 ] ) and ( [ st : solv1 ] ) for @xmath52 and @xmath53 .    the calculation of @xmath52 and @xmath53 of ( [ eq : poisson : rt ] ) follows a similar pattern . to this end",
    ", we note the gamma prior @xmath366 and with the logarithmic link we have @xmath367 and @xmath368 equation ( [ eq : poisson : rt ] ) is obtained by the solution of ( [ rt : solv2 ] ) and ( [ st : solv2 ] ) for @xmath52 and @xmath53 .",
    "since @xmath49 and @xmath50 are only guides of the mean and variance of the prior of @xmath15 , the above approximations of @xmath360 and @xmath362 can be used even when @xmath359 is small .",
    "the posterior quantities @xmath62 and @xmath369 are calculated in a similar way , but here we use the full approximations @xmath370 and @xmath191 , the details of which can be found in abramowitz and stegun ( 1964 ) .",
    "the prior distribution of @xmath2 is @xmath371 this is not a known distribution and so we need to use integration in order to find the constant @xmath372 . since @xmath373",
    ", we need to evaluate @xmath374 by applying the substitution @xmath375 we have @xmath376 now @xmath377 can be written as @xmath378 integral @xmath379 can be evaluated via the gaussian integral , i.e. @xmath380 for @xmath381 we use the substitution @xmath382 and so we get @xmath383 thus , combining @xmath377 , @xmath379 and @xmath381 , we obtain @xmath384 the required prior distribution of @xmath247 is immediately obtained by density ( [ app1 ] ) , if we apply the transformation @xmath308 and we use @xmath372 as above .    proceeding with the proof of ( [ ig : prior : m ] )",
    "we have @xmath385 where @xmath386 . to evaluate integral @xmath387 we note that @xmath388 and by applying the substitution @xmath389 and using the gaussian integral , we have @xmath390 the required mean ( [ ig : prior : m ] ) is obtained as @xmath391 .",
    "durbin , j. and koopman , s.j .",
    "( 2000 ) time series analysis of non - gaussian observations based on state space models from both classical and bayesian perspectives ( with discussion ) .",
    "_ journal of the royal statistical society series b _ , * 62 * , 3 - 56 .",
    "ferreira , m.a.r . and gamerman , d. ( 2000 ) dynamic generalized linear models . in",
    "generalized linear models : a bayesian perspective _ , d.k .",
    "dey , s.k .",
    "ghosh and b.k .",
    "mallick ( eds . ) .",
    "marcel dekker , new york .",
    "houseman , e.a .",
    ", coull , b.a . and shine , j.p .",
    "( 2006 ) a nonstationary negative binomial time series with time - dependent covariates : enterococcus counts in boston harbor . _ journal of the american statistical association _ , * 101 *",
    ", 1365 - 1376 ."
  ],
  "abstract_text": [
    "<S> the purpose of this paper is to provide a discussion , with illustrating examples , on bayesian forecasting for dynamic generalized linear models ( dglms ) . adopting approximate bayesian analysis , based on conjugate forms and on bayes linear estimation , we describe the theoretical framework and then we provide detailed examples of response distributions , including binomial , poisson , negative binomial , geometric , normal , log - normal , gamma , exponential , weibull , pareto , beta , and inverse gaussian . </S>",
    "<S> we give numerical illustrations for all distributions ( except for the normal ) . </S>",
    "<S> putting together all the above distributions , we give a unified bayesian approach to non - gaussian time series analysis , with applications from finance and medicine to biology and the behavioural sciences . throughout the models we discuss bayesian forecasting and , for each model </S>",
    "<S> , we derive the multi - step forecast mean . </S>",
    "<S> finally , we describe model assessment using the likelihood function , and bayesian model monitoring .    _ some key words : _ bayesian forecasting , non - gaussian time series , dynamic generalized linear model , state space , kalman filter . </S>"
  ]
}