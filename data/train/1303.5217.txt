{
  "article_text": [
    "quicksort  @xcite is a thoroughly analyzed classical sorting algorithm , described in standard textbooks such as  @xcite and with implementations in practically all algorithm libraries . following the divide - and - conquer paradigm , on an input consisting of @xmath1 elements",
    "quicksort uses a pivot element to partition its input elements into two parts , those smaller than the pivot and those larger than the pivot , and then uses recursion to sort these parts .",
    "it is well known that if the input consists of @xmath1 elements with distinct keys in random order and the pivot is picked by just choosing an element then on average quicksort uses @xmath2 comparisons . in 2009 , yaroslavskiy announced that he had found an improved quicksort implementation , the claim being backed by experiments .",
    "after extensive empirical studies , in 2009 yaroslavskiy s algorithm became the new standard quicksort algorithm in oracle s java 7 runtime library .",
    "this algorithm employs two pivots to split the elements .",
    "if two pivots @xmath4 and @xmath5 with @xmath6 are used , the partitioning step partitions the remaining @xmath7 elements into 3 parts : those smaller than @xmath4 ( _ small _ elements ) , those in between @xmath4 and @xmath5 ( _ medium _ elements ) , and those larger than @xmath5 ( _ large _ elements ) , see  fig .",
    "[ fig : dual : pivot : partition ] .",
    "recursion is then applied to the three parts .",
    "as remarked in @xcite , it came as a surprise that two pivots should help , since in his thesis  @xcite sedgewick had proposed and analyzed a dual pivot approach that was inferior to classical quicksort .",
    "later , hennequin in his thesis  @xcite studied the general approach of using @xmath8 pivot elements . according to @xcite",
    ", he found only slight improvements that would not compensate for the more involved partitioning procedure .",
    "( see @xcite for a short discussion . )    in @xcite , nebel and wild formulated and analyzed a simplified version of yaroslavskiy s algorithm .",
    "( for completeness , this algorithm is given as algorithm  [ algo : yaroslavskiy : partition ] in appendix  [ app : sec : yaroslavskiy ] . )",
    "they showed that it makes @xmath9 key comparisons on average , in contrast to the @xmath10 of standard quicksort and the @xmath11 of sedgewick s dual pivot algorithm . on the other hand",
    ", they showed that the number of swap operations in yaroslavskiy s algorithm is @xmath12 on average , which is much higher than the @xmath13 swap operations in classical quicksort . in this paper , also following tradition , we concentrate on the comparison count as cost measure and on asymptotic results .    the authors of @xcite state that the reason for yaroslavskiy s algorithm being superior were that his `` partitioning method is able to take advantage of certain asymmetries in the outcomes of key comparisons '' .",
    "they also state that `` [ sedgewick s dual pivot method ] fails to utilize them , even though being based on the same abstract algorithmic idea '' .",
    "so the abstract algorithmic idea of using two pivots can lead to different algorithms with different behavior . in this paper",
    "we describe the design space from which all these algorithms originate .",
    "we fully explain which simple property makes some dual pivot algorithms perform better and some perform worse w.r.t .",
    "the average comparison count and identify optimal members ( up to lower order or linear terms ) of this design space .",
    "the best ones use @xmath14 comparisons on average  even less than yaroslavskiy s method .",
    "the first observation is that everything depends on the cost , i.e. , the comparison count , of the partitioning step .",
    "this is not new at all .",
    "actually , in hennequin s thesis  @xcite the connection between partitioning cost and overall cost for quicksort variants with more than one pivot is analyzed in detail .",
    "the result relevant for us is that if two pivots are used and the ( average ) partitioning cost for @xmath1 elements can be bounded by @xmath15 , for a constant @xmath16 , then the average cost for sorting @xmath1 elements is @xmath17 throughout the present paper all that interests us is the constant factor with the leading term .",
    "( the reader should be warned that for real - life @xmath1 the linear term , which can even be negative , can have a big influence on the average number of comparisons . )",
    "the second observation is that the partitioning cost depends on certain details of the partitioning procedure .",
    "this is in contrast to standard quicksort with one pivot where partitioning always takes @xmath18 comparisons . in @xcite",
    "it is shown that yaroslavskiy s partitioning procedure uses @xmath19 comparisons on average , while sedgewick s uses @xmath20 many .",
    "the analysis of these two algorithms is based on the study of how certain pointers move through the array , at which positions elements are compared to the pivots , which of the two pivots is used for the first comparison , and how swap operations exchange two elements in the array . for understanding what is going on , however , it is helpful to forget about concrete implementations with loops in which pointers sweep across arrays and entries are swapped , and look at partitioning with two pivots in a more abstract way . for simplicity we shall always assume that the input is a permutation of @xmath21 .",
    "now pivots @xmath4 and @xmath5 with @xmath6 are chosen .",
    "the task is to _ classify _ the remaining @xmath7 elements into classes `` small '' ( @xmath22 many ) , `` medium '' ( @xmath23 many ) , and `` large '' ( @xmath24 many ) , by comparing these elements one after the other with the smaller pivot or the larger pivot , or both of them if necessary .",
    "note that for symmetry reasons it is inessential in which order the elements are treated .",
    "the only choice the algorithm can make is whether to compare the current element with the smaller pivot or the larger pivot first . let the random variable @xmath25 denote the number of small elements compared with the larger pivot first , and let @xmath26 denote the number of large elements compared with the smaller pivot first .",
    "then the total number of comparisons is @xmath27    averaging over all inputs and all possible choices of the pivots the term @xmath28 will lead to @xmath29 key comparisons on average , independently of the algorithm .",
    "let @xmath30 , the number of elements that are compared with the `` wrong '' pivot first .",
    "then @xmath31 is the only quantity that is influenced by a particular partitioning procedure .    in the paper",
    ", we will first devise an easy method to calculate @xmath31 .",
    "the result of this analysis will lead to an ( asymptotically ) optimal strategy .",
    "the basic approach is the following .",
    "assume a partitioning procedure is given , and assume @xmath32 and hence @xmath22 and @xmath33 are fixed , and let @xmath34 .",
    "denote the average number of elements compared to the smaller [ larger ] pivot first by @xmath35 [ @xmath36 . if the elements to be classified were chosen to be small , medium , and large independently with probabilities @xmath37 , @xmath38 , and @xmath39 , resp .",
    ", then the average number of small elements compared with the large pivot first would be @xmath40 , similarly for the large elements .",
    "of course , the actual input is a sequence with exactly @xmath41 [ @xmath42 , @xmath43 small [ medium , large ] elements , and there is no independence .",
    "still , we will show that the randomness in the order is sufficient to guarantee that @xmath44 the details of the partitioning procedure will determine @xmath35 and @xmath45 , and hence @xmath46 up to @xmath47 .",
    "this seemingly simple insight has two consequences , one for the analysis and one for the design of dual pivot algorithms :    * in order to _ analyze _ the average comparison count of a dual pivot algorithm ( given by its partitioning procedure ) up to lower order terms determine @xmath35 and @xmath45 for this partitioning procedure",
    ". this will give @xmath46 up to lower order terms , which must then be averaged over all @xmath48 to find the average number of comparisons in partitioning .",
    "then apply . * in order to _ design _ a good partitioning procedure w.r.t",
    ". the average comparison count , try to make @xmath49 small .",
    "we shall demonstrate approach ( i ) in section  [ sec : methods ] .",
    "an example : as explained in @xcite , if @xmath41 and @xmath50 are fixed , in yaroslavskiy s algorithm we have @xmath51 and @xmath52 .",
    "by we get @xmath53 .",
    "this must be averaged over all possible values of @xmath41 and @xmath50 .",
    "the result is @xmath54 , which together with @xmath55 gives @xmath56 , close to the result from @xcite .    principle ( ii ) will be used to identify an ( asymptotically ) optimal partitioning procedure that makes @xmath57 key comparisons on average . in brief",
    ", such a strategy should achieve the following : if @xmath58 , compare ( almost ) all entries with the smaller pivot first ( @xmath59 and @xmath60 ) , otherwise compare ( almost ) all entries with the larger pivot first ( @xmath61 and @xmath62 ) .",
    "of course , some details have to be worked out : how can the algorithm decide which case applies ? in which technical sense is this strategy optimal ?",
    "we shall see in section  [ sec : decreasing ] how a sampling technique resolves these issues .",
    "in section  [ sec : optimal : strategies ] , we will consider the following simple and intuitive strategy : _ compare the current element to the smaller pivot first if more small elements than large elements have been seen so far , otherwise compare it to the larger pivot first . _ we will show that this is optimal w.r.t . the average comparison count with an error term of only @xmath63 instead of @xmath64 .    in implementations of quicksort , the pivot is usually not chosen directly from the input .",
    "instead , one takes a sample of three elements from the input and chooses the median of these elements as the pivot .",
    "theoretically , this decreases the average comparison count from @xmath65 to @xmath66 . in section  [ sec : pivot : sample ] we will analyze the comparison count of dual pivot quicksort algorithms that use the tertiles in a sample of @xmath67 elements as the two pivots .",
    "yaroslavskiy s algorithm has an average comparison count of @xmath68 in this case , while the optimal average cost is @xmath69 .",
    "at the end of that section we will compare the average comparison count of classical quicksort and optimal dual pivot quicksort algorithms for some sample sizes .",
    "the result is surprising : when both algorithms use the same sample size , classical quicksort beats ( optimal variants of ) dual pivot quicksort even at such small sample sizes as @xmath67 .    in order to arrive at a full dual pivot quicksort algorithm we must also take into account the work for moving elements around .",
    "elements can be moved around by exchanging two elements in the input , a so - called _ swap _ operation . in section  [ sec : swaps ] , we will point out the obvious connection between this `` moving issue '' in dual pivot quicksort and the so - called `` dutch national flag '' problem , in which an input of elements  each having either the color `` blue '' , `` white '' , or `` red''must be re - arranged such that they resemble the national flag of the netherlands . in this turn",
    ", we will identify strategies for moving elements around that can be used in dual pivot quicksort algorithms , too .",
    "we will also state a simple lower bound on the average number of swaps needed to find a partition of the input as depicted in figure  [ fig : dual : pivot : partition ] .    as noted by wild _",
    "@xcite , considering only key comparisons and swap operations does not suffice for evaluating the practicability of sorting algorithms . in section  [ sec : experiments ] , we will present preliminary experimental results that indicate the following : when sorting integers , the `` optimal '' method of section  [ sec : decreasing ] is slower than yaroslavskiy s algorithm .",
    "when making key comparisons artificially expensive , e.g. , by sorting strings , we gain a small advantage .",
    "we emphasize that the purpose of this paper is not to arrive at better and better quicksort algorithms by using all kinds of variations , but rather to thoroughly analyze the situation with two pivots , showing the potential and the limitations of this approach .",
    "we assume the input sequence @xmath70 to be a random permutation of @xmath21 , each permutation occurring with probability @xmath71 .",
    "if @xmath72 , there is nothing to do ; if @xmath73 , sort by one comparison .",
    "otherwise , choose the first element @xmath74 and the last element @xmath75 as pivots , and set @xmath76 and @xmath77 .",
    "partition the remaining elements into elements smaller than @xmath4 ( `` small '' elements ) , elements in between @xmath4 and @xmath5 ( `` medium '' elements ) , and elements larger than @xmath5 ( `` large '' elements ) , see fig .  [ fig : dual : pivot : partition ] . then apply the procedure recursively to these three groups .",
    "clearly , each pair @xmath32 with @xmath78 appears as pivots with probability @xmath79 .",
    "our cost measure is the number of key comparisons needed to sort the given input .",
    "let @xmath80 be the random variable counting this number .",
    "let @xmath81 denote the partitioning cost to partition the @xmath7 non - pivot elements into the three groups . as explained by wild and nebel ( * ? ? ?",
    "* appendix a ) , the average number of key comparisons obeys the following recurrence : @xmath82 if @xmath83 , for a constant @xmath16 , this can be solved ( _ cf . _",
    "@xcite ) to give @xmath84 abstracting from moving elements around in arrays , we arrive at the following `` classification problem '' : given a random permutation @xmath70 of @xmath21 as the input sequence and @xmath74 and @xmath75 as the two pivots @xmath4 and @xmath5 , with @xmath85 , classify each of the remaining @xmath7 elements as being small , medium , or large . note that there are exactly @xmath86 small elements , @xmath87 medium elements , and @xmath88 large elements .",
    "although this classification does not yield an actual partition of the input sequence , a classification algorithm can be turned into a partitioning algorithm using only swap operations but no additional key comparisons . since elements are only compared with the two pivots , the randomness of subarrays is preserved .",
    "thus , in the recursion we may always assume that the input is arranged randomly .",
    "we make the following observations ( and fix notation ) for all classification algorithms . one key comparison is needed to decide which of the elements @xmath74 and @xmath75 is the smaller pivot @xmath4 and which is the larger pivot @xmath5 . for classification , each of the remaining @xmath7 elements has to be compared against @xmath4 or @xmath5 or both .",
    "each _ medium _",
    "element has to be compared to @xmath4 _ and _ @xmath5 . on average , there are @xmath89 medium elements .",
    "let @xmath25 denote the number of small elements that are compared to the larger pivot first , i.e. , the number of small elements that need @xmath90 comparisons for classification .",
    "analogously , let @xmath26 denote the number of large elements compared to the smaller pivot first .",
    "conditioning on the pivot choices , and hence the values of @xmath41 and @xmath50 , we may calculate @xmath91 as follows : @xmath92 we call the third summand the _ additional cost term ( act ) _ , as it is the only value that depends on the actual classification algorithm .",
    "we will use the following formalization of a partitioning procedure : a _ classification strategy _ is given as a three - way decision tree @xmath93 with a root and @xmath7 levels of inner nodes as well as one leaf level .",
    "the root is on level @xmath94 .",
    "each node @xmath95 is labeled with an index @xmath96 and an element @xmath97 .",
    "if @xmath98 is @xmath99 , then at node @xmath95 element @xmath100 is compared with the smaller pivot first ; otherwise , i.e. , @xmath101 , it is compared with the larger pivot first .",
    "the three edges out of a node are labeled @xmath102 resp .",
    ", representing the outcome of the classification as small , medium , large , respectively .",
    "the label of edge @xmath103 is called @xmath104 . on each of the @xmath105 paths each index occurs exactly once .",
    "each input determines exactly one path @xmath106 from the root to a leaf in the obvious way ; the classification of the elements can then be read off from the node and edge labels along this path .    identifying a path @xmath107 from the root to a leaf @xmath95 by the sequence of nodes and edges @xmath108 on it",
    ", we define the cost @xmath109 as @xmath110 for a given input , the cost of the path associated with this input exactly describes the number of additional comparisons on this input .",
    "an example for such a decision tree is given in figure  [ fig : decision : tree : ex1 ] .",
    "we now describe how we can calculate the act of a decision tree @xmath93 .",
    "fix @xmath41 and @xmath50 , and let the input excepting the pivots be arranged randomly . for a node @xmath95 in @xmath93 ,",
    "we let @xmath111 , @xmath112 , and @xmath113 , resp . , denote the number of edges labeled @xmath114 , @xmath115 , and @xmath116 , resp .",
    ", from the root to @xmath95 . by the randomness of the input , the probability that the element classified at @xmath95 is `` small '' , i.e. , that the edge labeled @xmath114 is used , is exactly @xmath117 .",
    "the probability that it is `` medium '' is @xmath118 , and that it is `` large '' is @xmath119 .",
    "the probability @xmath120 that node @xmath95 in the tree is reached is then just the product of all these edge probabilities on the unique path from the root to @xmath95 .",
    "the probability that the edge labeled @xmath114 out of a node @xmath95 is used can then be calculated as @xmath121 .",
    "similarly , the probability that the edge labeled @xmath116 is used is @xmath122 . note that this is independent of the actual ordering in which the decision tree inspects the elements .",
    "we can thus always assume some fixed ordering and forget about the label @xmath123 of a node @xmath95 .    for a random input ,",
    "we let @xmath124 [ @xmath125 denote the random variable that counts the number of small [ large ] elements classified in nodes with label @xmath126 [ @xmath99 ] . by linearity of expectation , we can sum up the contribution to the additional comparison count for each node separately .",
    "thus , we may calculate @xmath128 the setup developed so far makes it possible to describe the connection between a decision tree @xmath93 and its average comparison count in general .",
    "let @xmath129 resp .",
    "@xmath130 be two random variables that denote the number of elements that are compared with the smaller resp .",
    "larger pivot first when using @xmath93 .",
    "then let @xmath131 resp .",
    "@xmath132 denote the average number of comparisons with the larger resp .",
    "smaller pivot first , given @xmath41 and @xmath50 . now , if it was decided in each step by independent random experiments with the correct expectations @xmath37 , @xmath38 , and @xmath39 , resp .",
    ", whether an element is small , medium , or large , it would be clear that for example @xmath133 is the average number of small elements that are compared with the larger pivot first .",
    "the next lemma shows that one can indeed use this intuition in the calculation of the average comparison count , excepting that one gets an additional @xmath47 term due to the elements tested not being independent .",
    "[ lemma:01 ] let @xmath93 be a decision tree .",
    "let @xmath134 be the average number of key comparisons for classifying an input of @xmath1 elements using @xmath93 .",
    "then    @xmath135    fix @xmath4 and @xmath5 ( and thus @xmath136 and @xmath50 ) .",
    "we will show that @xmath137 the lemma then follows by plugging in this result into . for a given node @xmath95 in @xmath93 , we call @xmath95 _ good _ if @xmath101 and @xmath138 or @xmath139 and @xmath140 otherwise , @xmath95 is called _ bad_.    using , we calculate : @xmath141 where the first and second summand follow by the definition of @xmath35 and @xmath142 . for the third summand , consider each of the levels of the decision tree separately . since the probabilities @xmath120 for nodes @xmath95 on the same level sum up to @xmath143 , the contribution of the @xmath144 terms is bounded by @xmath47 .",
    "note that this calculation yields an upper bound .",
    "however , a similar calculation shows that @xmath145 is a lower bound for @xmath146 as well .",
    "so , to show it remains to bound the last summand in by @xmath47 .    to see this , consider a random input that is classified using @xmath93",
    ". we will show that with very high probability we do not reach a bad node in the decision tree for the first @xmath147 levels .",
    "let @xmath148 be the @xmath94-@xmath143 random variable that is @xmath143 if the @xmath149-th classified element is small ; let @xmath150 be the @xmath94-@xmath143 random variable that is @xmath143 if the @xmath149-th classified element is large .",
    "let @xmath151 and @xmath152 .",
    "we will use the method of averaged bounded differences to show that these random variables are tightly concentrated around their expectation .",
    "let @xmath153 .",
    "then @xmath154    we prove the first inequality .",
    "first , we calculate the difference @xmath155 between the expectation of @xmath156 conditioned on @xmath157 resp .",
    "@xmath158 for @xmath159 .",
    "using linearity of expectation we may calculate @xmath160 in this situation we may apply the following bound known as the method of averaged bounded differences ( see ( * ? ? ? * theorem 5.3 ) ) : @xmath161 and get @xmath162 which is not larger than @xmath163 .    assume that @xmath164 .",
    "we may calculate @xmath165 that means that for each of the first @xmath166 levels , we are with very high probability in a _ good node _ on level @xmath167 , because the deviation from the ideal case that we see a small element with probability @xmath37 is at most @xmath168 .",
    "thus , for the first @xmath147 levels the contribution of the sums of the probabilities of bad nodes is not more than @xmath47 to the last summand in .",
    "for the last @xmath169 levels of the tree , we use that the contribution of the probabilities that we reach a bad node on level @xmath167 is at most @xmath143 for a fixed level .",
    "this shows that the last summand in is @xmath47 .",
    "substituting this result in proves the lemma .",
    "there are two technical complications when using this lemma in analyzing a strategy that is turned into a dual pivot quicksort algorithm .",
    "the cost bound is @xmath170 .",
    "equation can not be applied directly to such partitioning costs .",
    "furthermore , the @xmath47 term in lemma  [ lemma:01 ] will get out of control for subarrays appearing in the recursion that are too small .",
    "however , the next theorem says that the leading term of applies to this situation as well , although we get an error term of @xmath64 instead of @xmath63 .",
    "[ thm:10 ] let @xmath171 be a dual pivot quicksort algorithm that gives rise to a decision tree @xmath172 for each subarray of length @xmath1 .",
    "assume @xmath173 for all @xmath1 , for some constant @xmath16",
    ". then @xmath174 .",
    "fix an arbitrarily small number @xmath175 . then there is some @xmath176 such that for all @xmath177 the average partitioning cost on a subarray of size @xmath178 is smaller than @xmath179 .",
    "we only consider @xmath1 so large that @xmath180 and that @xmath181 .",
    "we split the analysis of the average cost into two parts . for each subarray of length @xmath182",
    "the average partitioning cost is at most @xmath179 ; for each subarray of size @xmath183 we charge @xmath184 to the first part of the analysis .",
    "then can be used to estimate the contribution of the first part as @xmath185 . in the second part",
    "we collect the contributions from splitting subarrays of size @xmath186 not captured by the first part .",
    "each such contribution is bounded from above by @xmath187 ( even in absolute value ) , and from below by @xmath178 .",
    "the second part of the analysis consists in adding @xmath187 resp .",
    "@xmath178 for each subarray of size @xmath183 and @xmath94 for each larger subarray .",
    "we will focus on the upper bound .",
    "calculations for the lower bound are analogous . to this end , we wait until the algorithm has created a subarray of size @xmath183 and assess the total contribution from the recursion starting from this subarray as not more than @xmath188 , by .",
    "this means we must sum @xmath189 , @xmath190 , over a sequence of disjoint subarrays of length @xmath191",
    ". since all @xmath192 are smaller than @xmath193 , @xmath194 , and since @xmath195 is a convex function , this sums up to no more than @xmath196 for @xmath1 large enough . adding both parts , and choosing @xmath1 so large that the two @xmath63 terms can be bounded by @xmath197 we get the bound @xmath198 , which is sufficient since @xmath199 was arbitrary .",
    "lemma  [ lemma:01 ] and theorem  [ thm:10 ] tell us that for the analysis of the average comparison count of a dual pivot quicksort algorithm we just have to find out what @xmath200 and @xmath201 are for this algorithm .",
    "moreover , to design a good algorithm ( w.r.t . the average comparison count ) , we should try to make @xmath202 small for each pair @xmath48 .",
    "in this section , we will study different classification strategies in the light of the formulas from section  [ sec : additional : cost : term ] .",
    "[ [ oblivious - strategies . ] ] oblivious strategies .",
    "+ + + + + + + + + + + + + + + + + + + + +    we will first consider strategies that do not use information of previous classifications for future classifications . to this end , we call a decision tree _ oblivious _",
    "if for each level all nodes @xmath95 on this level share the same label @xmath98 , i.e. , either in all nodes on a given level an element is compared with the smaller pivot first , or in all nodes on a given level an element is compared with the larger pivot first .",
    "this means that these algorithms do not react to the outcome of previous classifications , but use a fixed sequence of pivot choices .",
    "examples for such strategies are , e.g. ,    * always compare to the smaller pivot first , * always compare to the larger pivot first , * alternate the pivots in each step .",
    "let @xmath203 denote the average number of comparisons to the larger pivot first . by assumption",
    "this value is independent of @xmath41 and @xmath50 .",
    "hence these strategies make sure that @xmath204 and @xmath205 for all pairs of values @xmath206 .",
    "applying lemma  [ lemma:01 ] gives us @xmath207 using theorem  [ thm:10 ] we get @xmath208the leading term being the same as in standard quicksort .",
    "so , for each strategy that does not adapt to the outcome of previous classifications , there is no difference to the average comparison count of classical quicksort .",
    "note that this also holds for _ randomized strategies _ such as `` flip a coin to choose the pivot used in the first comparison '' , since such a strategy can be seen as a probability distribution on oblivious strategies .",
    "[ [ yaroslavskiys - algorithm . ] ] yaroslavskiy s algorithm .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + +    following ( * ? ? ?",
    "* section 3.2 ) , yaroslavskiy s algorithm is an implementation of the following strategy @xmath209 : _ compare @xmath50 elements to @xmath5 first , and compare the other elements to @xmath4 first .",
    "_ we get that @xmath210 and @xmath211 . applying lemma  [ lemma:01 ]",
    ", we calculate @xmath212 .",
    "using theorem  [ thm:10 ] gives @xmath213 , as in @xcite .",
    "[ [ sec : sedgewick ] ] sedgewick s algorithm .",
    "+ + + + + + + + + + + + + + + + + + + + + +    following ( * ? ? ?",
    "* section 3.2 ) , sedgewick s algorithm amounts to an implementation of the following strategy @xmath214 : _ compare ( on average ) a fraction of @xmath215 of the keys with @xmath5 first , and compare the other keys with @xmath4 first .",
    "_ we get @xmath216 and @xmath217 . using lemma  [ lemma:01 ]",
    ", we calculate @xmath218 . applying theorem  [ thm:10 ]",
    "gives @xmath219 , as known from @xcite .",
    "obviously , this is worse than strategy @xmath220 considered before .",
    "this is easily explained intuitively : if the fraction of small elements is large , it will compare many elements with @xmath5 first . but",
    "this costs two comparisons for each small element .",
    "conversely , if the fraction of large elements is large , it will compare many elements to @xmath4 first , which is again the wrong decision .",
    "since sedgewick s strategy seems to do exactly the opposite of what one should do to lower the comparison count , we consider the following modified strategy @xmath221 : _ for given @xmath4 and @xmath5 , compare ( on average ) a fraction of @xmath215 of the keys with @xmath4 first , and compare the other keys with @xmath5 first . _",
    "( @xmath221 simply uses @xmath4 first when @xmath214 would use @xmath5 first and vice versa . )    using the same analysis as above , we get @xmath222 which yields @xmath223improving on the standard algorithm and even on yaroslavskiy s algorithm ! note that this has been observed by wild in his master s thesis as well @xcite .    [ [ remark . ] ] remark .",
    "+ + + + + + +    exchanging @xmath35 and @xmath142 as in the strategy described above is a general technique . in fact , if the leading coefficient of the average number of comparisons for a fixed choice of @xmath35 and @xmath45 is @xmath224 , e.g. , @xmath225 for strategy @xmath214 , then the leading coefficient of the strategy that exchanges @xmath226 and @xmath142 is @xmath227 , e.g. , @xmath228 as in strategy @xmath221 .",
    "looking at the previous sections , all strategies used the idea that we should compare a certain fraction of elements to @xmath4 first , and all other elements to @xmath5 first . in this section , we will study the following strategy @xmath229 : _",
    "if @xmath230 then always compare with @xmath4 first , otherwise always compare with @xmath5 first .",
    "_    of course , for an implementation of this strategy we have to deal with the problem of finding out which case applies before all comparisons have been made .",
    "we shall analyze a guessing strategy to resolve this .",
    "assume for a moment that for a given random input with pivots @xmath32 the strategy `` magically '' knows whether @xmath231 or not and correctly determines the pivot that should be used for all comparisons .",
    "for fixed @xmath41 and @xmath50 this means that for @xmath231 the classification strategy makes exactly @xmath50 additional comparisons , and for @xmath232 it makes @xmath41 additional comparisons .",
    "since we can exactly describe the average cost term , we directly apply and do not need theorem  [ thm:10 ] .",
    "a standard calculation shows that @xmath233 applying , we get @xmath234 , which is by @xmath235 smaller than the average number of key comparisons in yaroslavskiy s algorithm .    to see that this method is ( asymptotically ) optimal , recall that according to lemma  [ lemma:01 ] the average comparison count is determined up to lower order terms by the parameters @xmath45 and @xmath236 .",
    "strategy @xmath229 chooses these values such that @xmath45 is either @xmath94 or @xmath7 , minimizing each term of the sum in lemma  [ lemma:01]and thus minimizing the sum .",
    "we explain how the ideal classification strategy just described can be approximated by an implementation .",
    "the idea simply is to make a few comparisons and use the outcome as a basis for a guess .",
    "after @xmath4 and @xmath5 are chosen , classify the first @xmath238 many elements ( the _ sample _ ) and calculate @xmath239 and @xmath240 , the number of small and large elements in the sample .",
    "if @xmath241 , compare the remaining @xmath242 elements with @xmath5 first , otherwise compare them with @xmath4 first .",
    "we say that the guess was _ correct _ if the relation `` @xmath241 '' correctly reflects whether @xmath237 or not .",
    "we incorporate guessing errors into as follows : @xmath243&\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\,\\,\\pr(\\text{guess wrong } ) \\cdot      \\max(s , \\ell)\\bigg ) + o(n)\\notag\\\\[-1em ]      & =   \\frac{4}{3}n + \\frac{2}{\\binom{n}{2}}\\sum_{s=0}^{n/2}\\sum_{\\ell = s+1}^{n - s } \\bigg(\\pr(\\text{guess correct } ) \\cdot s + \\notag\\\\[-1em ] & \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\,\\,\\,\\,\\,\\pr(\\text{guess wrong } ) \\cdot \\ell\\bigg ) + o(n).\\end{aligned}\\ ] ] the following lemma says that for a wide range of values @xmath41 and @xmath50 the probability of a guessing error is exponentially small .",
    "[ lem : estimation : error ] let @xmath41 and @xmath50 with @xmath244 and @xmath245 for @xmath246 be given .",
    "let @xmath247 .",
    "then @xmath248    let @xmath249 .",
    "let @xmath250 be a random variable that is @xmath251 if the @xmath167-th classified element is large , @xmath94 if it is medium , and @xmath143 if it is small .",
    "let @xmath252 .    as in the proof of lemma  [ lemma:01 ] , we want to apply the method of averaged bounded differences . using the assumptions on the values of @xmath41 and @xmath50 ,",
    "straightforward calculations show that @xmath253 .",
    "furthermore , we have that @xmath254 to see this , we let @xmath156 resp . @xmath255",
    "denote the number of small resp .",
    "large elements that are still present if @xmath256 are classified .",
    "furthermore , let @xmath257 be the @xmath94-@xmath143 random variable that is @xmath143 iff @xmath250 is @xmath143 , and let @xmath258 be the @xmath94-@xmath143 random variable that is @xmath143 iff @xmath250 is @xmath251 .",
    "we calculate : @xmath259 \\\\ & \\quad\\quad- \\sum_{j =      1}^{i-1}{x_j}- \\sum_{j = i}^{\\textit{sz } } \\bigg[\\pr(x_j = 1 \\mid x_1 ,      \\ldots , x_{i-1 } ) - \\pr(x_j = -1 \\mid x_1,\\ldots , x_{i-1})\\bigg]\\bigg|\\\\      & = \\bigg| x_i + \\sum_{j = i + 1}^{\\textit{sz } } \\left[\\frac{s_i}{n - i } -      \\frac{\\ell_i}{n - i}\\right ] - \\sum_{j = i}^{\\textit{sz } } \\left[\\frac{s_{i",
    "- 1}}{n - i + 1 } -      \\frac{\\ell_{i - 1}}{n - i+1}\\right]\\bigg|\\\\      & = \\bigg| x_i + \\sum_{j = i + 1}^{\\textit{sz } } \\left[\\frac{s_{i-1 } - y_i}{n - i } -      \\frac{\\ell_{i-1 } - z_i}{n - i}\\right ] - \\sum_{j = i}^{\\textit{sz } } \\left[\\frac{s_{i - 1}}{n - i + 1 } -      \\frac{\\ell_{i - 1}}{n - i+1}\\right]\\bigg|\\\\      & = \\bigg| x_i   - \\frac{(\\textit{sz}-i ) \\cdot ( z_i - y_i)}{n - i } + s_{i - 1 }      \\cdot\\frac{\\textit{sz}-i}{n - i } -      \\frac{\\textit{sz}-i+1}{n - i+1 } + \\ell_{i - 1 } \\cdot\\frac{\\textit{sz } - i + 1}{n - i+1 } -      \\frac{\\textit{sz } - i}{n - i}\\bigg|\\\\      & = \\bigg|x_i   - \\frac{(\\textit{sz}-i ) \\cdot ( z_i - y_i)}{n - i } + s_{i - 1 }      \\cdot\\frac{\\textit{sz}-n}{(n - i)(n - i+1 ) }      + \\ell_{i - 1 } \\cdot\\frac{n - \\textit{sz}}{(n - i)(n - i+1)}\\bigg|\\\\      & = \\bigg|x_i   - \\frac{(\\textit{sz}-i ) \\cdot ( z_i - y_i)}{n - i } + ( \\ell_{i -      1 } - s_{i - 1 } ) \\cdot \\frac{n-\\textit{sz}}{(n - i)(n - i+1 ) }      \\bigg|\\\\      & \\leq \\bigg|x_i   - ( z_i - y_i ) - 1\\bigg| \\leq 3\\\\      \\end{aligned}\\ ] ] the method of averaged bounded differences ( see ( * ? ? ? * theorem 5.3 ) ) says that @xmath260 which with @xmath261 yields @xmath262    of course , we get an analogous result for @xmath263 and @xmath264 .",
    "our classification strategy will now work as follows .",
    "it starts by classifying @xmath249 many elements and then decides which pivot is used in the first comparison for the remaining @xmath265 elements .",
    "then it classifies the elements according to this decision .",
    "we can now analyze the average number of key comparisons of this strategy turned into a dual pivot quicksort algorithm .",
    "_ = @xmath266",
    ". then the average comparison count of the strategy described above turned into a dual pivot quicksort algorithm is @xmath267 .",
    "first , classify _ sz _ many elements .",
    "the number of key comparisons for these classifications is at most @xmath268 . by symmetry",
    ", we may focus on the case that @xmath269 .",
    "we distinguish the following three cases :    1 .",
    "@xmath270 : the contribution of terms in for this case is at most @xmath271 2 .",
    "@xmath272 : the contribution of terms in in this case is at most @xmath273 3 .",
    "@xmath274 and @xmath275 .",
    "let @xmath276 .",
    "following lemma  [ lem : estimation : error ] , the probability of guessing wrong is at most @xmath277 .",
    "the contribution of this case in is hence at most @xmath278    thus , the contribution of sampling and estimation errors is @xmath47 .",
    "as discussed in the proof of theorem  [ thm:10 ] , the probability bounds in lemma  [ lem : estimation : error ] do not hold for small subarrays created in the recursion .",
    "reasoning exactly as in the proof of that theorem , we get that sorting these small subarrays results in an additional summand of @xmath64 to the average number of key comparisons .    in conclusion ,",
    "we expect a partitioning step to make @xmath279 key comparisons , see .",
    "applying theorem  [ thm:10 ] , we get @xmath280 .    while being optimal , this strategy has an error term of @xmath64 . in the next section",
    "we will present a different strategy that will be optimal up to an @xmath63 error term .",
    "we will consider two more strategies , an optimal ( but not algorithmic ) strategy , and an algorithmic strategy that is optimal up to a very small error term .",
    "we first study the ( unrealistic ! ) setting where @xmath41 and @xmath50 , i.e. , the number of small resp .",
    "large elements , are known to the algorithm after the pivots are chosen , and the decision tree can have different node labels for each such pair of values . recall that @xmath111 and @xmath113 , resp .",
    ", denote the number of elements that have been classified as small and large , resp .",
    ", when at node @xmath95 in the decision tree .",
    "we consider the following strategy @xmath281 : _ _ given @xmath41 and @xmath50 , the comparison at node @xmath95 is with the smaller pivot first if @xmath282 , otherwise , it is with the larger pivot first .",
    "_ _    [ thm : o : optimal ] strategy @xmath281 is optimal , i.e. , its act is at most as large as act@xmath283 for every single tree @xmath93 . when using @xmath281 in a dual pivot quicksort algorithm , we get @xmath284 .",
    "the proof of the first statement ( optimality ) is surprisingly simple .",
    "fix the two pivots , and consider equality .",
    "strategy @xmath281 chooses for each node @xmath95 in the decision tree the label such that the contribution of this node to is minimized .",
    "so , it minimizes each term of the sum , and thus minimizes the additional cost term in .",
    "now we prove the second statement . to this end",
    ", we first derive an upper bound for the number of additional comparisons by pessimistically assuming that strategy @xmath281 always makes the wrong decision when @xmath285 , i.e. , the number of small elements left in the input equals the number of large elements left in the input .",
    "assume @xmath231 and omit medium elements in the input .",
    "let @xmath178 denote the number of small and large elements in the remaining input .",
    "now consider the example run of strategy @xmath281 depicted in figure  [ fig : o : run ] . in a trajectory that starts at @xmath286",
    "one can pair off the @xmath94-edges and @xmath143-edges ( from a @xmath143-edge go horizontally to the right until the next @xmath94-edge at the same level is hit ) , excepting for the @xmath287 @xmath94-edges in the first part of the trajectory that go down to a level that has not been touched before until the @xmath288 axis is hit .",
    "hence the number of additional comparisons , i.e. , the number of @xmath143-edges , is @xmath289 . if @xmath290 , then the same argument shows that the number of additional comparisons is @xmath41 .",
    "the additional cost term of strategy @xmath281 ( see ) is thus at most @xmath291 which gives an average number of at most @xmath292 comparisons .",
    "for such a partitioning cost we can use and obtain an average comparison count for sorting via strategy @xmath281 of at most @xmath293 .    now it remains to show that this is tight .",
    "for this it suffices to estimate ( for a random input ) the average number of positions @xmath294 in which the number of small elements in the rest of the input , i.e. , at positions @xmath295 , equals the number of large elements in the rest of the input .",
    "a somewhat involved calculation shows that this number is @xmath296 .",
    "additionally , one can also show that a ( negative ) term of @xmath296 decreases the total average number of key comparisons only by @xmath63 when we use such a strategy to implement a dual pivot quicksort algorithm .",
    "the details of these calculations can be found in appendix  [ app : proof : thm : o ] .    while being optimal w.r.t . minimizing the act , the assumption that the exact number of small and large elements is known is of course not true for a real algorithm or for a fixed tree .",
    "we can , however , identify a real , algorithmic partitioning strategy whose act differs from the optimal one only by a logarithmic term .",
    "we study the following strategy @xmath297 : _ the comparison at node @xmath95 is with the smaller pivot first if @xmath298 , otherwise it is with the larger pivot first . _",
    "while @xmath281 looks into the future ( `` are there more small elements or more large elements left ? '' ) , strategy @xmath297 looks into the past ( `` have i seen more small or more large elements so far ? '' ) .",
    "it is not hard to see that for a given input the number of additional comparisons of strategy @xmath281 and @xmath297 can differ significantly .",
    "the next theorem shows that averaged over all possible inputs , however , there is only a small difference .",
    "[ thm : optimal : strategies : diff ] let act@xmath299 resp .",
    "act@xmath300 be the act for classifying @xmath1 elements using strategy @xmath281 resp . @xmath297 .",
    "then act@xmath301 . when using @xmath297 in a dual pivot quicksort algorithm , we get @xmath302 .",
    "assume that strategy @xmath281 inspects the elements in the order @xmath303 , while @xmath297 uses the order @xmath304 .",
    "if the strategies compare the element @xmath305 to different pivots , then there are exactly as many small elements as there are large elements in @xmath306 or @xmath307 , depending on whether @xmath167 is even or odd , see figure  [ fig : strategy : difference ] .",
    "the same calculation as in appendix  [ app : proof : thm : o ] shows that @xmath308 is @xmath296 , which sums up to a total additive contribution of @xmath63 when using strategy @xmath297 in a dual pivot quicksort algorithm .",
    "see appendix  [ app : proof : thm : o ] for details .",
    "thus , dual pivot quicksort with strategy @xmath297 has average cost at most @xmath63 larger than dual pivot quicksort using the ( unrealistic ) optimal strategy @xmath281 .",
    "we have now identified three optimal strategies , two of which can be used in an actual algorithm . note that the bound for the average comparison count of strategy @xmath297 has an additional summand of @xmath63 , while the bound for strategy @xmath229 has an additional summand of @xmath309 .",
    "the difference in the comparison count is clearly visible in an actual implementation , as we shall see in section  [ sec : experiments ] .",
    "however , in comparison to strategy @xmath229 strategy @xmath297 requires an additional counter and a conditional jump for each element .",
    "we consider here the variation of dual pivot quicksort in which the two pivots are chosen from a sample of @xmath67 elements .",
    "we choose the second - largest and fourth - largest as pivots .",
    "( this is the pivot choice that is used in yaroslavskiy s algorithm in the jre7 implementation , see @xcite for further discussion . )",
    "the probability that @xmath4 and @xmath5 , @xmath6 , are chosen as pivots is exactly @xmath310 following hennequin @xcite , for partitioning costs @xmath83 we get @xmath311 where @xmath312 denotes the @xmath1-th harmonic number .",
    "note that only strategy @xmath281 and @xmath297 guarantee such partitioning cost in our setting .",
    "when applying lemma  [ lemma:01 ] , we have average partitioning cost of @xmath170 .",
    "using the same argument as in the proof of theorem  [ thm:10 ] , the average comparison count becomes @xmath313 in this case .",
    "we will now investigate the effect on the average number of key comparisons in yaroslavskiy s resp .",
    "the optimal partitioning method @xmath229 .",
    "the average number of medium elements remains @xmath89 . for strategy @xmath209 , we calculate @xmath314 applying , we get @xmath315 key comparisons .",
    "( note that wild _",
    "@xcite calculated this leading coefficient as well . )",
    "this is slightly better than `` clever quicksort '' , which uses the median of a sample of three elements as a single pivot element and achieves @xmath316 key comparisons on average @xcite . for our proposed partitioning method from section  [ sec : decreasing ] ,",
    "we get @xmath317 again using , we obtain @xmath318 , which is optimal as well . by applying random sampling as above",
    ", we get an algorithm that makes @xmath319 key comparisons on average , improving further on the leading coefficient compared to clever quicksort and yaroslavskiy s algorithm .      in the previous subsection",
    ", we have shown that optimal dual pivot quicksort using a sample of size @xmath67 clearly beats clever quicksort which uses the median of three elements .",
    "we will now investigate how these two variants compare when the sample size grows .",
    "the following proposition , which is a special case of ( * ? ? ?",
    "* proposition iii.9 and proposition iii.10 ) , will help in this discussion .",
    "let @xmath320 be the average partitioning cost of a quicksort algorithm @xmath171 that chooses the pivot(s ) from a sample of size @xmath321 , for constants @xmath16 and @xmath321 .",
    "then the following holds :    1 .",
    "if @xmath322 is even and @xmath171 is a classical quicksort variant that chooses the median of these @xmath321 samples , then the average sorting cost is @xmath323 2 .",
    "if @xmath322 is divisible by 3 and @xmath171 is a dual pivot quicksort variant that chooses the two tertiles of these @xmath321 samples as pivots , then the average sorting cost is @xmath324    note that for classical quicksort we have partitioning cost of @xmath325 .",
    "thus , the average sorting cost becomes @xmath326 .",
    "for an optimal dual pivot partitioning algorithm , we get the following : the probability that @xmath4 and @xmath5 , @xmath85 , are chosen as pivots in a sample of size @xmath321 where @xmath322 is divisible by @xmath327 is exactly @xmath328 thus , the average partitioning cost can be calculated as follows : @xmath329 unfortunately , we could not find a closed form of @xmath330",
    ". some calculated values in which classical and dual pivot quicksort use the same sample size can be found in table  [ tab : sample : sorting : cost ] .",
    "these values clearly indicate that starting from a sample of size @xmath67 classical quicksort has a smaller average comparison count than dual pivot quicksort . furthermore , while it is well known that for classical quicksort in which the pivot is chosen as the median of a sample , the average comparison count converges with increasing sample size to lower bound of @xmath331 comparisons on average for comparison based sorting , this does not seem to be the case for dual pivot quicksort .",
    "it is an interesting question which multi - pivot quicksort algorithms converge to the optimal value and which do not .",
    "we strongly believe that the ( obvious ) symmetric partitioning algorithms using @xmath332 pivots ( experimentally studied by tan @xcite ) are all optimal for sample sizes of , e.g. , @xmath333 .",
    ".comparison of the leading term of the average cost of classical quicksort and dual pivot quicksort for specific sample sizes .",
    "note that for real - world input sizes , however , the linear term can make a big difference . [ cols=\"<,^,^,^,^\",options=\"header \" , ]",
    "after the discussion of key comparisons in dual pivot quicksort , we will now turn the focus on minimizing swap operations .",
    "fortunately , we can build on work that has been done for the so - called _ dutch national flag _ problem .",
    "this problem is defined as follows : given an array containing @xmath1 elements , where each element is either red , blue or white , rearrange the elements using swaps such that they resemble the national flag of the netherlands ( red followed by white followed by blue ) .",
    "one immediately sees the connection between this problem and the partitioning problem in the context of dual pivot quicksort , for the partitioning problem deals with classifying elements into three different types called small , medium , and large , resp , and rearranging these elements such that small elements are followed by medium elements which are followed by large elements .",
    "we quickly review the work done on this problem , and then analyze the average swap count of these algorithms when using them for dual pivot quicksort .",
    "many results on the dnf can not be applied directly , since the probability spaces differ .      here",
    ", we describe three algorithms that solve the dnf problem .",
    "we state them in our notation , using small , medium , and large elements , resp .",
    ", instead of red , white , and blue elements , resp .",
    "we assume that @xmath334 is an array of length @xmath1 containing small , medium , and large elements .",
    "the first algorithm is due to dijkstra @xcite .",
    "[ algo : dijkstra : swap ]    = = = = = = = = = = = + * procedure * _ dijsktraswap_(@xmath335 $ ] ) + 1 @xmath336 + 2 * while * @xmath337 + 3 * classify * @xmath338 $ ] + 4 * case small * : _ swap_(`i`,`j ` ) ; ` i`@xmath339 ; + 5 * case medium * : ` j` ; + 6 * case large * : _",
    "swap_(`j`,`k ` ) ; ` j` ; ` k` ; + 7 * end while * +    the second algorithm is due to meyer @xcite .",
    "it avoids swapping elements that have not been inspected so far ( that might be small and hence already present on a suitable position ) as done in algorithm  [ algo : dijkstra : swap ] on line 4 .",
    "[ algo : meyer : swap ]    = = = = = = = = = = = + * procedure * _ meyerswap_(@xmath335 $ ] ) + 1 @xmath336 + 2 * while * @xmath337 + 3 * classify * @xmath338 $ ] + 4 * case small * : + 5 * while * @xmath340 $ ] is small + 6 ` i`@xmath339 ; + 7 * end while * + 8 * if * @xmath341 + 9 _ swap_(`i`,`j ` ) ; ` i`@xmath339 ; + 10 * case medium * : j ; + 11 * case large * : swap(j , k ) ; j ; k ; + 12 * end while * +    the next algorithm was explicitly stated by chen in @xcite , but has been discussed in @xcite , too . instead of classifying elements and moving them to their final position ,",
    "it uses two stages . in the first stage",
    ", it moves all small elements to the left of the array .",
    "the second stage moves all large elements to the right , but does not inspect the array part that contains the small elements .",
    "[ algo : chen : swap ]    = = = = = = = = = = = + * procedure * _ swapc_(@xmath335 $ ] ) + 1 @xmath342 + 2 * while * @xmath341 + 3 * while * @xmath340 $ ] is small * and * @xmath341 + 4 ` i`@xmath339 ; + 5 * end while * + 6 * while * @xmath338 $ ] is not small * and * @xmath341 + 7 ` j` ; + 8 * end while * + 9 * if * @xmath341 + 10 _ swap_(`i`,`j ` ) ; ` i`@xmath339 ; ` j` ; + 11 * end while * + 12 * if * @xmath343 + 13 @xmath344 + 14 * while * @xmath345 + 15 * while * @xmath338 $ ] is not large * and * @xmath345 + 16 ` j`@xmath339 ; + 17 * end while * + 18 * while * @xmath346 $ ] is large * and * @xmath345 + 19 ` k` ; + 20 * end while * + 21 * if * @xmath345 + 22 _ swap_(`j`,`k ` ) ; ` j`@xmath339 ; ` k` ; + 23 * end while * +      given a strategy , e.g. , the optimal strategy from section  [ sec : decreasing ] , we get a partitioning procedure for dual pivot quicksort by using algorithm  [ algo : dijkstra : swap ] or algorithm  [ algo : meyer : swap]replacing all classification steps according to the given strategy . in fact , yaroslavskiy s algorithm uses strategy @xmath209 together with meyer s algorithm .",
    "the situation is a little bit different with algorithm  [ algo : chen : swap ] , for it uses that the smaller pivot is used for the first comparison .",
    "( this case was analyzed as strategy @xmath220 in section  [ sec : methods ] . )    for now , we focus on calculating the average swap count of these algorithms . note that these algorithms have been analyzed in the setting of the dutch national flag problem ( see , e.g. , @xcite ) , but in a different probability space , in which each element independently chooses to be small , medium or large with probability @xmath347 .",
    "[ [ analysis - of - algorithmalgodijkstraswap . ] ] analysis of algorithm  [ algo : dijkstra : swap ] .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    algorithm  [ algo : dijkstra : swap ] has the following properties with respect to swap operations :    1 .",
    "each small element causes exactly one swap .",
    "2 .   each large element causes exactly one swap .",
    "3 .   no medium element causes a swap operation .    for the average swap count @xmath348 for partitioning an array of length @xmath1conditioning on the pivot choices as before",
    ", we get the following formula : to the average swap count .",
    "we omit this summand here . ]",
    "@xmath349 note that also holds for the swap case ( _ cf .",
    "_ @xcite ) . for the total average number of swaps @xmath350 for sorting an array of length @xmath1 using a dual pivot quicksort approach",
    ", we hence obtain @xmath351    [ [ analysis - of - algorithmalgomeyerswap . ] ] analysis of algorithm  [ algo : meyer : swap ] .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    algorithm  [ algo : meyer : swap ] has the following properties with respect to swap operations for @xmath41 small and @xmath50 large elements :    1 .",
    "each small element that resides in the array positions @xmath352 causes exactly one swap .",
    "2 .   each large element causes exactly one swap .",
    "3 .   no medium element causes a swap .",
    "for the average swap count @xmath353 for algorithm  [ algo : meyer : swap ] , we get @xmath354 which yields a total average swap count of @xmath355 . to no surprise",
    ", this is exactly the same number as the average swap count calculated by wild and nebel @xcite of yaroslavskiy s algorithm .    when we use the optimal strategy of section  [ sec : decreasing ]",
    ", we can further improve this algorithm as follows : after the random sampling step , we make a guess whether @xmath231 .",
    "if this is the case , we use algorithm  [ algo : meyer : swap ] and always compare to the smaller pivot first . otherwise , we slightly change algorithm  [ algo : meyer : swap ] by letting @xmath356 run from @xmath143 to @xmath357 , swapping the small and large case , i.e. , using the inner while loop when @xmath358 $ ] is large to ignore large elements",
    ". conditioning on @xmath41 and @xmath50 , almost the same calculations as above show that this algorithm yields a total average swap count of @xmath359 .",
    "[ [ analysis - of - algorithmalgochenswap . ] ] analysis of algorithm  [ algo : chen : swap ] .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    algorithm  [ algo : chen : swap ] has the following behavior with respect to swap operations for @xmath41 small and @xmath50 large elements .    1 .   in the first stage",
    ", each small element that resides in the array positions @xmath352 causes exactly one swap .",
    "2 .   in the second stage ,",
    "each large element that resides in the array positions @xmath360 causes exactly one swap .",
    "3 .   no other element causes a swap .    given a random array containing @xmath41 small , @xmath42 medium , and @xmath50 large elements , note that after the first stage the sequence of array elements in positions @xmath352 is still random , i.e. , each such sequence is equally likely .",
    "( this is the same as the property that subarrays occurring in recursive steps of quicksort are fully random , _ cf . _",
    "* section 3.1 ) . )    for the average swap count @xmath361 for algorithm  [ algo : chen : swap ] , we get @xmath362 which gives @xmath363and hence significantly decreases the number of swap operations compared to the previous algorithms .    again , we can use the optimal strategy of section  [ sec : decreasing ] by deciding if we use algorithm  [ algo : chen : swap ] or its modified version , in which we first split into large and non - large elements after the random sampling step .      while chen s algorithm decreases the average swap count compared to meyer s algorithm",
    ", it is not clear whether it achieves the lowest possible average swap count . in the related dnf problem",
    ", there exists an algorithm @xcite that is asymptotically optimal .",
    "however , this algorithm heavily relies on the ( different ! ) randomness assumptions in this particular problem . we give a simple lower bound for our setting .    to achieve this bound",
    ", we simply calculate the average number of misplaced elements in the input , i.e. , medium and large elements in the input which reside in array cells that are left of the final position of the smaller pivot , small and large elements that reside in array cells that lie in between the two pivots , and small and medium elements that lie right of the larger pivot .",
    "actually , these exact numbers have been calculated by wild and nebel , see ( * ? ? ?",
    "* table 3 ) .",
    "they show that each `` group of misplaced elements '' , e.g. , small elements in the input that reside in a location right of the larger pivot , has exactly @xmath364 elements on average .",
    "adding together all these values gives an average number of about @xmath365 elements .",
    "now , with a single swap operation we can move at most @xmath90 elements into their final position .",
    "thus , at least @xmath366 swap operations are needed to move these misplaced elements to their final locations .",
    "this is smaller than the @xmath367 swap operation chen s algorithm needs on average .",
    "it is an interesting open question whether chen s algorithm is optimal or not ( as in the dnf problem ) w.r.t .",
    "minimizing the average swap count .",
    "we have implemented the methods presented in this paper in c++ .",
    "these algorithms have not been fine - tuned ; this section is hence meant to provide only preliminary results and does not replace a thorough experimental study .",
    "our experiments were carried out on an intel xeon e5645 at 2.4 ghz with 48 gb ram running ubuntu 12.04 with kernel version 3.2.0 .",
    "for measuring running times , we used three different scenarios .    1 .   c++ code compiled with _ gcc _ using no optimization flags , 2 .",
    "c++ code compiled with _",
    "gcc _ using the _ -o2 _ optimization flag , and 3 .",
    "java code using oracle s java 7 .",
    "we have incorporated random sampling into the partitioning step of our method ( strategy @xmath229 ) by comparing the first @xmath368 elements with @xmath4 first .",
    "we switched to comparing with @xmath5 first if the algorithm has seen more large than small elements after @xmath178 steps .",
    "in section  [ sec : comparison : swap : count ] , we give an experimental evaluation of the comparison and swap count of the algorithms considered in this paper . in section  [ sec : running : times ] , we focus on the actual running times needed to sort a given input .",
    "the charts of our experiments can be found at the end of this paper .",
    "we first have a look at the comparison and swap count needed to sort a random input of up to @xmath369 integers .",
    "we did not switch to a different sorting algorithm , e.g. , insertion sort , to sort short subarrays .",
    "figure  [ fig : comp : direct ] shows the results of our experiments for algorithms that choose the pivots directly .",
    "we see that the linear term in the average comparison count has a big influence on the number of comparisons . for yaroslavskiy s algorithm",
    "this linear term is @xmath370 , as calculated by the authors of @xcite .",
    "the results confirm our theoretical studies and show that our algorithm beats all other algorithms with respect to the comparison count , although we incorporated random sampling directly into the partitioning step .",
    "we also see that the modified version of sedgewick s algorithm beats yaroslavskiy s algorithm . on the other hand ,",
    "sedgewick s original algorithm makes the most key comparisons and is even worse than standard quicksort , as described in section  [ sec : sedgewick ] .",
    "figure  [ fig : comp : sample ] shows the same experiment for the algorithms that choose the pivots from a small sample .",
    "this plot confirms the theoretical results from section  [ sec : pivot : sample ] .",
    "figure  [ fig : swaps : direct ] shows the same experiment for the swap count .",
    "these results confirm the theoretical study conducted in section  [ sec : swaps ] .",
    "we see that sedgewick s strategy and dijkstra s strategy make the most swaps .",
    "yaroslavskiy s algorithm is clearly better and shows exactly the behavior calculated for a strategy that uses meyer s algorithm .",
    "we see that the modified version of meyer s algorithm that makes a small random sampling step and then decides which version it uses is better . the best swap strategy  as calculated in section  [ sec : swaps]is algorithm  [ algo : chen : swap ] .",
    "it almost matches the swap count of standard quicksort .",
    "however , in our initial experiments , the running time of this variant was not competitive to algorithms that used algorithm  [ algo : dijkstra : swap ] or algorithm  [ algo : meyer : swap ] .",
    "we now consider the running times of our algorithms to sort a given input .",
    "to measure running times , all algorithms used a small sample to choose the pivots from .",
    "clever quicksort uses the median of a sample of three elements .",
    "yaroslavskiy and our algorithm use the second- and fourth - largest elements from a sample of five elements .",
    "we sorted subarrays of size at most @xmath371 directly using insertion sort .",
    "with respect to running times , we see that yaroslavskiy s algorithm is superior to the other algorithms when sorting random permutations of @xmath21 .",
    "our method is about @xmath372 slower , see figure  [ fig : running : times : ints ] . when sorting strings ( and key comparisons become more expensive ) ,",
    "our algorithm can actually beat yaroslavskiy s algorithm , see figure  [ fig : running : times : ints ] .",
    "however , the difference is only about @xmath373 , see figure  [ fig : running : times : strings ] .",
    "note that java 7 does not use yaroslavskiy s algorithm when sorting strings .",
    "we have studied dual pivot quicksort algorithms in a unified way and found optimal partitioning methods that minimize the average number of key comparisons up to lower order terms and even up to @xmath63 .",
    "this minimum is @xmath374 .",
    "we pointed out the obvious relation between dual pivot quicksort and the dutch national flag problem , this identifying an algorithm which decreases the average swap count of @xmath375 ( yaroslavskiy s algorithm ) to @xmath376 .",
    "several open questions remain . while we are now in a situation where we can compare classical and dual pivot quicksort variants w.r.t .",
    "the average comparison count resp . the average swap count",
    ", it does not seem to be the case that these cost measures can fully explain the different running time behavior observed in practice . to this end",
    ", one might look into different cost measures that explain these differences more accurately . from an engineering point of view",
    ", one might look for a clever implementation of our classification idea that beats yaroslavskiy s algorithm . while we restricted ourselves to analyze the average comparison count and average swap count , one can of course study the actual distribution of these random variables in much more detail , as , e.g. , done for yaroslavskiy s algorithm by wild _",
    "moreover , it has been shown by wild _",
    "_ in @xcite that in difference to quicksort , the well - known quickselect algorithm for finding order statistics is not improved by using yaroslavskiy s algorithm in the partitioning step .",
    "it is an interesting question whether this holds for all dual pivot approaches",
    ".    * acknowledgements . *",
    "the authors thank thomas hotz for a useful suggestion regarding the optimal strategy of section  [ sec : optimal : strategies ] , and pascal klaue for implementing the algorithms and carrying out initial experiments .",
    "the first author thanks michael rink for interesting and useful discussions .",
    "we thank the referees of the conference submission for their insightful comments , which helped a lot in improving the presentation .",
    "10 [ 1]`#1 `    bitner , j.r .",
    ": an asymptotically optimal algorithm for the dutch national flag problem .",
    "siam j. comput .",
    "11(2 ) , 243262 ( 1982 )    chen , w.m . :",
    "probabilistic analysis of algorithms for the dutch national flag problem .",
    "sci . 341(1 - 3 ) , 398410 ( 2005 )    cormen , t.h . ,",
    "leiserson , c.e . ,",
    "rivest , r.l . ,",
    "stein , c. : introduction to algorithms ( 3 . ed . ) .",
    "mit press ( 2009 )    dijkstra , e.w . : a discipline of programming .",
    "prentice - hall ( 1976 )    dubhashi , d.p . , panconesi , a. : concentration of measure for the analysis of randomized algorithms .",
    "cambridge university press ( 2009 )    hennequin , p. : analyse en moyenne dalgorithmes : tri rapide et arbres de recherche .",
    "thesis , ecole politechnique , palaiseau ( 1991 )    hoare , c.a.r . : quicksort .",
    "j. 5(1 ) , 1015 ( 1962 )    knuth , d.e .",
    ": the art of computer programming , volume iii : sorting and searching .",
    "addison - wesley ( 1973 )    meyer , s.j . : a failure of structured programming .",
    "zilog corp . ,",
    "software dept . technical rep .",
    "5 , cupertino , ca ( 1978 )    sedgewick , r. : quicksort . ph.d .",
    "thesis , standford university ( 1975 )    sedgewick , r. : quicksort with equal keys .",
    "siam j. comput .",
    "6(2 ) , 240268 ( 1977 )    sedgewick , r. , flajolet , p. : an introduction to the analysis of algorithms .",
    "addison - wesley - longman ( 1996 )    tan , k.h . : an asymptotic analysis of the number of comparisons in multipartition quicksort .",
    "thesis , carnegie mellon university ( 1993 )    wild , s. : java 7 s dual pivot quicksort .",
    "master s thesis , university of kaiserslautern ( 2013 )    wild , s. , nebel , m.e . :",
    "average case analysis of java 7 s dual pivot quicksort . in : esa12 .",
    "825836 ( 2012 )    wild , s. , nebel , m.e . , mahmoud , h. : analysis of quickselect under yaroslavskiy s dual - pivoting algorithm .",
    "corr abs/1306.3819 ( 2013 )    wild , s. , nebel , m.e . ,",
    "neininger , r. : average case and distributional analysis of java 7 s dual pivot quicksort .",
    "corr abs/1304.0988 ( 2013 )    wild , s. , nebel , m.e . ,",
    "reitzig , r. , laube , u. : engineering java 7 s dual pivot quicksort using malijan . in : alenex13 ( 2013 )",
    "in section  [ sec : optimal : strategies ] we have analyzed the additional comparisons count of the optimal strategy @xmath281 . a simple argument showed that for fixed pivots , the number of additional comparisons on an input is at most @xmath41 when @xmath232 , and @xmath50 otherwise .",
    "this showed that the total average sorting cost when this strategy is turned into a dual pivot algorithm is at most @xmath14 .",
    "we got this upper bound by assuming that the optimal strategy always decides for the wrong pivot when there are the same number of small elements as the number of large elements in the unclassified part of the input .",
    "we will now analyze how much this upper bound deviates from the exact average additional cost term .    for this assume that the optimal strategy @xmath281 inspects the elements in the order @xmath70 .",
    "( for simplicity of the calculation , we assume that @xmath1 elements are to be classified , i.e. , the input consists of @xmath377 elements including the two pivots . ) we have to calculate the average number of positions @xmath378 where we have the same number of small elements as large elements in @xmath379 .",
    "we call a position @xmath167 where this happens a _ zero - crossing _ at position @xmath167 .    by symmetry",
    ", we may assume that the number of small elements is at most as large as the number of large elements .",
    "we temporarily omit medium elements to simplify calculations , i.e. , we assume that the number of small and large elements is @xmath1 .",
    "let @xmath380 be the random variable that counts the number of zero - crossings for an input of @xmath1 elements .",
    "we calculate :    @xmath381    by using the well - known identity @xmath382 ( which follows directly from stirling s approximation ) , we continue by @xmath383 we continue by bounding the product . @xmath384 using a standard computer algebra system for the integral , we obtain @xmath385 involving the standard gamma function @xmath386 . since @xmath387 , we may continue by calculating    @xmath388    now we consider the case that the input contains medium elements .",
    "fix the number of small elements and the number of large elements . since medium elements",
    "have no influence on the value of the additional cost term , we have that @xmath389 equals @xmath390 , i.e. , it equals the average number of zero - crossings for a smaller input containing @xmath391 elements .    we can now simply calculate the average number of zero - crossing for an arbitrary input as follows : @xmath392 which concludes the first part of the proof of theorem  [ thm : o : optimal ] .",
    "we now know that the difference between the upper bound on the additional cost term shown in and the actual additional cost term is @xmath393 .",
    "it remains to show that the influence of these @xmath393 terms for the total average sorting cost is bounded by @xmath63 .",
    "by linearity of expectation , we consider these terms in the average sorting cost of separately .",
    "so , assume that the cost associated with a partitioning step involving a subarray of length @xmath1 is @xmath394 for some constant @xmath395 .",
    "we show by induction on the input size that the contributions of the @xmath396 terms sum up to at most @xmath63 for the total average comparison count .",
    "let @xmath397 denote the sum of the error terms in the average comparison count .",
    "we will show that @xmath398 for suitable constants @xmath399 and @xmath400 .",
    "let @xmath401 .",
    "for the base case , let @xmath402 and set @xmath399 such that @xmath403 for all @xmath404 .",
    "as the induction hypothesis , assume that holds for all @xmath405 .",
    "for the induction step , we calculate : @xmath406 we use that @xmath407 for monotone functions @xmath408 defined on @xmath409 $ ] and obtain @xmath410 an easy calculation shows that from @xmath411 it follows that @xmath412 which finishes the induction step .",
    "thus , the additional @xmath296 terms sum up to @xmath63 in the total average comparison count .",
    "thus , the difference between the upper bound of @xmath413 derived in the proof of theorem  [ thm : o : optimal ] and the exact cost is @xmath63 , and so the total average sorting cost of strategy @xmath281 is @xmath414 .",
    "= = = = = = = = = + * procedure * _ dqs_(@xmath415 , @xmath416 , @xmath417 ) + 1 * if * @xmath418 * then * + 2 * if * @xmath419 >          \\textit{a}[\\textit{right } ] \\textbf { then}$ ] + 3 swap @xmath419 $ ] and @xmath420 $ ] + 4 @xmath421 $ ] ; + 5 @xmath422 $ ] ; + 6 _ partition_(@xmath334 , @xmath423 , @xmath424 , @xmath416 , @xmath417 , @xmath425 ) + 7 _ dqs_(a , _ left _ ,",
    "@xmath426 ) + 8 _ dqs_(a , @xmath427 , @xmath428 ) + 9 _ dqs_(a , @xmath429 , _ right _ ) + 10 * end if *    to get an actual algorithm we have to implement a _ partition _ function that partitions the input as depicted in figure  [ fig : dual : pivot : partition ] .",
    "a partition procedure in this paper has two output variables @xmath430 and @xmath431 that are used to return the positions of the two pivots in the partitioned array .    as mentioned earlier , the _ partition _ function has two components : a _ classification algorithm _ and a _",
    "swap algorithm_. for each of the following algorithms , we will shortly describe these strategies before giving the actual algorithm .      as mentioned in section  [ sec : methods ] , yaroslavskiy s algorithm makes sure that for @xmath50 large elements in the input it will compare @xmath50 elements on average to the larger pivot first .",
    "how does it accomplish this ?",
    "by default , it compares to the smaller pivot first , but for each large elements that it sees , it will compare the next element to the larger pivot first .",
    "the swap strategy used is meyer s swap algorithm ( algorithm  [ algo : meyer : swap ] in section  [ sec : swaps ] ) .",
    "= = = = = = = = = = = + * procedure * _ y - partition_(@xmath415 , @xmath432 , @xmath433 , @xmath416 , @xmath417 , @xmath430 , @xmath431 ) + 1 @xmath434 + 2 * while * @xmath435 + 3 * if * @xmath346",
    "< p$ ] + 4 swap @xmath346 $ ] and @xmath436 $ ] + 5 ` l ` : = ` l ` + 1 + 6 * else * + 7 * if * @xmath346 >          q$ ] + 8 * while * @xmath437 > q$ ] and @xmath438 * do * @xmath439 * end while * + 9 swap @xmath346 $ ] and @xmath437 $ ] + 10@xmath440 + 11 * if * @xmath346 < p$ ] + 12 swap @xmath346 $ ] and @xmath436 $ ] + 13 @xmath441 + 14 * end if * + 15 * end if * + 16 * end if * + 17 @xmath442 + 18 * end while * + 19 swap @xmath419 $ ] and @xmath443 $ ] + 20 swap @xmath420 $ ] and @xmath444 $ ] + 21 @xmath445 +      the partitioning method from section  [ sec : decreasing ] uses a mix of two classification algorithms : _ always compare to the smaller pivot first _ , and _ always compare to the larger pivot first_. we present these two classification algorithms separately .",
    "the actual partitioning method uses algorithm  [ algo : simple : partition ] for the first @xmath446 classifications and then decides which of the two algorithms should be used for the rest of the input based on the number of small elements resp .",
    "large elements seen so far .",
    "( this is done by comparing the two variables ` l ` and ` g ` in algorithm  [ algo : simple : partition ] below . )",
    "= = = = = = = = = = = + * procedure * _ simplepartitionsmall_(@xmath415 , @xmath432 , @xmath433 , @xmath416 , @xmath417 , @xmath430 , @xmath431 ) + 1 @xmath434 + 2 * while * @xmath435 + 3 * if * @xmath346",
    "< p$ ] + 4 swap @xmath346 $ ] and @xmath436 $ ] + 5 ` l ` : = ` l ` + 1 + 6 ` k ` : = ` k ` + 1 + 7 * else * + 8 * if * @xmath346",
    "<          q$ ] + 9 @xmath442 + 10 * else * + 11 swap @xmath346 $ ] and @xmath437 $ ] + 12 @xmath440 + 13 * end if * + 14 * end if * + 15 * end while * + 16 swap @xmath419 $ ] and @xmath443 $ ] + 17 swap @xmath420 $ ] and @xmath444 $ ] + 18 @xmath447 +    algorithm  [ algo : simple : partition : large ] presents a partitioning algorithm that always compares to the larger pivot first .",
    "note that it uses meyer s swap algorithm and is thus not symmetrical to algorithm  [ algo : simple : partition ] . in our experiments ,",
    "this combination of algorithm  [ algo : simple : partition ] and algorithm  [ algo : simple : partition : large ] outperformed the method that used algorithm  [ algo : simple : partition ] and its symmetrical variant .",
    "= = = = = = = = = = = + * procedure * _ simplepartitionlarge_(@xmath415 , @xmath432 , @xmath433 , @xmath416 , @xmath417 , @xmath430 , @xmath431 ) + 1 @xmath434 + 2 * while * @xmath435 + 3 * while * @xmath437 > q$ ] + 4 ` g ` : = ` g ` - 1 + 5 * end while * + 6 * while * @xmath346 < q$ ] + 7 * if * @xmath346 <          p$ ] + 8 swap @xmath346 $ ] and @xmath436 $ ] + 9 @xmath448 + 10 * end if * + 11 @xmath442 + 12 * end while * + 13 * if * ` g ` @xmath449 ` k ` + 14 swap @xmath346 $ ] and @xmath437 $ ] + 15 * if * _ a_[`k ` ] @xmath450 + 16 swap @xmath436 $ ] and @xmath346 $ ] + 17 ` l ` : = ` l ` + 1 + 18 ` g ` : = ` g ` - 1 + 19 * end if * + 20 ` k ` : = ` k ` + 1 + 21 * end while * + 22 swap @xmath419 $ ] and @xmath443 $ ] + 23 swap @xmath420 $ ] and @xmath444 $ ] + 24 @xmath447 +        sedgewick s partitioning method uses two pointers @xmath451 and @xmath356 to scan through the input .",
    "it does not swap entries in the strict sense , but rather has two `` holes '' at positions @xmath452 resp . @xmath453",
    "that can be filled with small resp .",
    "large elements .",
    "`` moving a hole '' is not a swap operation in the strict sense ( three elements are involved ) , but requires the same amount of work as a swap operation ( in which we have to save the content of a variable into a temporary variable @xcite ) .",
    "an intermediate step in the partitioning algorithm is depicted in figure  [ fig : sedgewick : layout ] .",
    "the algorithm works as follows : using @xmath451 it scans the input from left to right until it has found a large element , always comparing to the larger pivot first .",
    "small elements found in this way are moved to a correct final position using the hole at @xmath452 .",
    "subsequently , using @xmath356 it scans the input from right to left until it has found a small element , always comparing to the smaller pivot first .",
    "large elements found in this way are moved to a correct final position using the hole at @xmath453 .",
    "now it exchanges the two elements at positions @xmath451 resp . @xmath356 and continues until @xmath451 and @xmath356 have met .",
    "= = = = = = = = = = = + * procedure * _ s - partition_(@xmath415 , @xmath432 , @xmath433 , @xmath416 , @xmath417 , @xmath430 , @xmath431 ) + 1 @xmath454 + 2 * while true * + 3 @xmath455 + 4 * while * @xmath340 \\leq          \\textit{q}$ ] + 5 * if * @xmath456 * then break * outer while * end if * + 6 * if * @xmath340",
    "<   \\textit{p}$ ] * then * @xmath457 : =          \\textit{a}[\\texttt{i } ] ; \\texttt{i}_1 : = \\texttt{i}_1 + 1 ;          \\textit{a}[\\texttt{i } ] : = \\textit{a}[\\texttt{i}_1]$ ] * end if * + 7 @xmath458 ; + 8 * end while * + 9 @xmath459 + 10 * while * @xmath338 \\geq          \\textit{p}$ ] + 11 * if * @xmath338 >   \\textit{q}$ ] * then * @xmath460 : =          \\textit{a}[\\texttt{j } ] ; \\texttt{j}_1 : = \\texttt{j}_1 - 1 ;          \\textit{a}[\\texttt{j } ] : = \\textit{a}[\\texttt{j}_1]$ ] * end if * + 12 * if * @xmath456 * then break * outer while * end if * + 13 @xmath461 ; + 14 * end while * + 15 @xmath457 : =          \\textit{a}[\\texttt{j } ] ; \\textit{a}[\\texttt{j}_1 ] : =          \\textit{a}[i];$ ] + 16 @xmath462 + 17 @xmath340 : =          \\textit{a}[\\texttt{i}_1 ] ; \\textit{a}[\\texttt{j } ] : =          \\textit{a}[\\texttt{j}_1];$ ] + 18 * end while * + 19 @xmath457 : = \\textit{p } ;          \\textit{a}[\\texttt{j}_1 ] : = \\textit{q};$ ] + 20 @xmath463 +    algorithm  [ algo : sedgewick : partition : modified ] shows an implementation of the modified partitioning strategy from section  [ sec : sedgewick ] . in the same way as algorithm  [ algo : sedgewick : partition ] it scans the input from left to right until it has found a large element . however , it uses the smaller pivot for the first comparison in this part .",
    "subsequently , it scans the input from right to left until it has found a small element . here",
    ", it uses the larger pivot for the first comparison .",
    "= = = = = = = = = = = + * procedure * _ s2-partition_(@xmath415 , @xmath432 , @xmath433 , @xmath416 , @xmath417 , @xmath430 , @xmath431 ) + 1 @xmath454 + 2 * while true * + 3 @xmath455 + 4 * while true * + 5 * if * @xmath456 * then break * outer while * end if * + 6 * if * @xmath340",
    "<   \\textit{p}$ ] * then * @xmath457 : =          \\textit{a}[\\texttt{i } ] ; \\texttt{i}_1 : = \\texttt{i}_1 + 1 ;          \\textit{a}[\\texttt{i } ] : = \\textit{a}[\\texttt{i}_1]$ ] + 7 * else if * @xmath340 >   \\textit{q}$ ] * then break * inner while * end if * + 8 @xmath458 ; + 9 * end while * + 10 @xmath459 + 11 * while true * + 12 * if * @xmath338 >   \\textit{q}$ ] * then * @xmath460 : =          \\textit{a}[\\texttt{j } ] ; \\texttt{j}_1 : = \\texttt{j}_1 - 1 ;          \\textit{a}[\\texttt{j } ] : = \\textit{a}[\\texttt{j}_1]$ ] + 13 * else if * @xmath338          < \\textit{p}$ ] * then break * inner while * end if * + 14 * if * @xmath456 * then break * outer while * end if * + 15 @xmath461 ; + 16 * end while * + 17 @xmath457 : =          \\textit{a}[\\texttt{j } ] ; \\textit{a}[\\texttt{j}_1 ] : =          \\textit{a}[i];$ ] + 18 @xmath462 + 19 @xmath340 : =          \\textit{a}[\\texttt{i}_1 ] ; \\textit{a}[\\texttt{j } ] : =          \\textit{a}[\\texttt{j}_1];$ ] + 20 * end while * + 21 @xmath457 : = \\textit{p } ;          \\textit{a}[\\texttt{j}_1 ] : = \\textit{q};$ ] + 22 @xmath463 +"
  ],
  "abstract_text": [
    "<S> _ dual pivot quicksort _ refers to variants of classical quicksort where in the partitioning step two pivots are used to split the input into three segments . </S>",
    "<S> this can be done in different ways , giving rise to different algorithms . </S>",
    "<S> recently , a dual pivot algorithm due to yaroslavskiy received much attention , because it replaced the well - engineered quicksort algorithm in oracle s java 7 runtime library . </S>",
    "<S> nebel and wild ( esa 2012 ) analyzed this algorithm and showed that on average it uses @xmath0 comparisons to sort an input of size @xmath1 , beating standard quicksort , which uses @xmath2 comparisons . </S>",
    "<S> we introduce a model that captures all dual pivot algorithms , give a unified analysis , and identify new dual pivot algorithms that minimize the average number of key comparisons among all possible algorithms up to lower order or linear terms . </S>",
    "<S> this minimum is @xmath3 . for the case </S>",
    "<S> that the pivots are chosen from a small sample , we include a comparison of dual pivot quicksort and classical quicksort . </S>",
    "<S> we also present results about minimizing the average number of swaps . </S>"
  ]
}