{
  "article_text": [
    "kernel methods offer state - of - the - art estimation performance .",
    "they provide function classes that are flexible and easy to control in terms of regularization properties .",
    "however , the use of kernels in large - scale machine learning problems has been beset with difficulty .",
    "this is because using kernel expansions in large datasets is too expensive in terms of computation and storage . in order to solve this problem , @xcite proposed an approximation algorithm , called fastfood , based on random kitchen sinks by @xcite , that speeds up the computation of a large range of kernel functions , allowing us to use them in big data .",
    "+ at its heart , fastfood requires scalar multiplications , a permutation , access to trigonometric functions , and two walsh - hadamard transforms ( wht ) for implementation .",
    "the key computational bottleneck here is the wht .",
    "we provide a fast , cache friendly simd oriented implementation that outperforms state - of - the - art codes such as spiral @xcite . to allow for very compact distribution of models",
    ", we use hash functions and a pseudorandom permutation generator for portability . in this way , for each feature dimension , we only need one floating point number . in summary ,",
    "our implementation serves as a drop - in feature generator for linear methods where attributes are generated _ on the fly _ , such as for regression , classification , or two - sample tests .",
    "this obviates the need for explicit kernel computations , particularly on large amounts of data .",
    "[ [ outline ] ] outline : + + + + + + + +    we begin with a brief operational description of the fastfood feature construction in section  [ sec : fastfood ] .",
    "this is followed by a discussion of the computational issues for a fast simd implementation in section  [ sec : simd ] .",
    "the concepts governing the api are described in section  [ sec : api ] .",
    "a detailed reference for all classes and methods is relegated to the appendix .",
    "kernel methods work by defining a kernel function @xmath0 on a domain @xmath1",
    ". we can write @xmath2 as inner product between feature maps , as follows    @xmath3    for some suitably chosen @xmath4",
    ". random kitchen sinks @xcite approximate this feature mapping @xmath4 by a fourier expansion in the case of radial basis function kernels , i.e.  whenever @xmath5 .",
    "this is possible since the fourier transform diagonalizes the corresponding integral operator @xcite .",
    "this leads to @xmath6 for some @xmath7 measurable function @xmath8 that is given by the fourier transform of @xmath9 .",
    "random kitchen sinks exploit this by replacing the integral by sampling @xmath10 .",
    "this allows for finite dimensional expansions but it is costly due to the large number of inner products required .",
    "fastfood resolves this for rotationally invariant @xmath9 by providing a fast approximation of the matrix @xmath11 .",
    "this is best seen for the gaussian rbf kernel where @xmath12 .",
    "since fourier transforms of gaussians are gaussians , albeit with inverse covariance , it follows that @xmath13 and that @xmath14 contains i.i.d gaussian random variables .",
    "it is this matrix that fastfood approximates via @xmath15 here @xmath16 and @xmath17 are diagonal matrices , @xmath18 is a random permutation matrix and @xmath19 is the hadamard matrix . whenever the number of rows in @xmath14 exceeds the dimensionality of the data , we can simply generate multiple instances of @xmath20 , drawn i.i.d , until the required number of dimensions is obtained .",
    "this is a matrix with entries @xmath21 , drawn from the uniform distribution . to avoid memory footprint , we simply use murmurhash as hash function and extract bits from @xmath22 with @xmath23 .",
    "this matrix is iteratively composed of @xmath24 .",
    "it is fixed and matrix - vector products are carried out efficiently in @xmath25 time using the fast walsh - hadamard transform ( fwht ) .",
    "we will discuss implementation details for a fast variant below .",
    "we generate a random permutation using the fisher - yates shuffle .",
    "that is , given a list @xmath26 we generate permutations recursively as follows : pick a random element from @xmath27 .",
    "use this as the image of @xmath28 and move @xmath28 to the position where the element was removed .",
    "the algorithm runs in linear time and its coefficients can be stored in @xmath29 space . moreover , to obtain a deterministic mapping , replace the random number generator with calls to the hash function .",
    "this is a diagonal matrix with i.i.d gaussian entries .",
    "we generate the random variates using the box - muller transform @xcite while substituting the random number generator by calls to the hash function to allow us to recompute the values at any time without the need to store random numbers .",
    "this is a random scaling operator whose behavior depends on the type of kernel chosen , such as the matern kernel , the gaussian rbf kernel or any other radial spectral distribution .",
    "a key part of the library is an efficient implementation of the fast walsh - hadamard transform . in particular , f2foffers considerable improvement over the spiral library , due to automatic code generation , the use of intel simd intrinsics ( sse2 using 128 bit registers ) and loop unrolling .",
    "this decreases the memory overhead .",
    "+ f2fproceeds with vectorized sums and subtractions iteratively for the first @xmath30 input vector positions ( where @xmath28 is the length of the input vector and @xmath2 the iteration starting from @xmath31 ) , computing the intermediate operations of the cooley - tukey algorithm till a small hadamard routine that fits in cache",
    ". then the algorithm continues in the same way but starting from the smallest length and doubling on each iteration the input dimension until the whole fwht is done in - place .",
    "for instance , on an intel i5 - 4200 cpu @ 1.60ghz laptop the following performance can be obtained    table[row sep= + ] x y + 1024 0.0333 + 2048 0.0667 + 4096 0.167 + 8192 0.2 + 16384 0.467 + 32768 0.9 + 65536 1.667 + 131072 3.5 + 262144 7.667 + 524288 15.9667 + 1048576 35.7 + ; table[row sep= + ]",
    "x y + 1024 0 + 2048 0.0333 + 4096 0.1 + 8192 0.0667 + 16384 0.2 + 32768 0.2 + 65536 0.7 + 131072 1.3 + 262144 3.6 + 524288 7.86 + 1048576 15.9667 + ;    our code outperforms spiral by a factor of 2 consistently throughout the range of arguments .",
    "furthermore , spiral needs to precompute trees and by default can only perform the computation up to matrix size @xmath32 . on the other hand",
    ", our implementation works for any size since it computes the high - level partitioning dynamically .",
    "the api for the fastfood implementation follows the factory design pattern .",
    "that is , while the fastfood object is fairly generic in terms of feature computation , we have a factory that acts as a means of instantiating the parameters according to pre - specified sets of parameters for e.g.  a gaussian rbf or a matern kernel .",
    "the so - chosen parameters are _ deterministic _ , given by the values of a hash function .",
    "the advantage of this approach is that there is no need to save the coefficients generated for fastfood when deploying the functions .",
    "that said , we also provide a constructor that consumes @xmath33 and @xmath34 to allow for arbitrary kernels .",
    ".... void fwht_opt(float * data , unsigned long lgn ) ....    computes the _ in place _ fast hadamard - walsh transform on data .",
    "the length of the vector it operates on is given by @xmath35 .",
    "note that fwht_opt does not check whether data points to a sufficiently large amount of memory .",
    "for instance , to generate each @xmath36 entry for matern kernel we draw @xmath37 iid samples from the @xmath28-dimensional unit ball @xmath38 , add them and compute its euclidean norm . to draw efficiently samples from @xmath38 we use the algorithm provided below .",
    "+ let @xmath39 be a vector of iid random variables drawn from @xmath40 , and let @xmath41 be the euclidean norm of @xmath42 , then @xmath43 is uniformly distributed over the @xmath28-sphere , where it is obvious to see that @xmath44 is the projection of @xmath42 onto the surface of the @xmath28-dimensional sphere . to draw uniform random variables in the @xmath28-ball",
    ", we multiply @xmath44 by @xmath45 where @xmath46 .",
    "this can be proved as follows : let @xmath47 be a random vector uniformly distributed in the unit ball .",
    "then , the radius @xmath48 satisfies @xmath49 .",
    "now , by the inverse transform method we get @xmath50.therefore to sample uniformly from the @xmath28-ball the following algorithm is used : @xmath51 .",
    "f2fclass instance initializes the object by computing and storing @xmath17 and @xmath18 diagonal matrices .",
    "the factory design lets you specify the creation of the matrix @xmath52 and @xmath34 by calling the proper kernel constructor .",
    "+ f2fobjects contain the following methods :    * ` ff_fastfood(float * data , float * features ) ` computes the features by using the real version of the complex feature map @xmath4 in @xcite , @xmath53_{j } ) , \\sin([vx]_{j } ) \\ } $ ] .",
    "simd vectorized instructions and cache locality are used to increase speed performance .",
    "to avoid a bottleneck in the basic trigonometric function computation a sse2 sincos function is used .",
    "these allow an speed improvement of 18x times for a @xmath54 dimension input matrix .",
    "* ` float ff_eval ( float * data , float * weights ) ` computes the inner product between the input vector and the weights .",
    "to illustrate how to embedded f2fin the code header definitions and examples are included in appendix b. + f2fcan be embedded in front of a linear classifier .",
    "for instance , first doing feature extraction , then f2fand finally going through linear svm .",
    "another example is to embedded f2fin a deep neural network architecture either as non linear mapping to the activation function , or as a separate fastfood layer .",
    "if you are only interested in using fwht and no other functionalities of this library , then enclose ` # include \" hpp / fast2food.hpp \" ` in your test file and consider the following usage guidelines .",
    "+ * description * + ` fwht_opt ` computes in place the fast walsh hadamard transform of a given input vector . +",
    "* header *    .... void fwht_opt(float * data , unsigned long lgn ) ....    * parameters *    * ` data ` : input data vector * ` lgn ` : log2 input vector length    * usage *    .... // in - place fwht input vector with dimension dim fwht_opt(data , log2(dim ) ) ; ....    if you are interested in using the function for a @xmath28-input vector matrix , do as follows    .... // in - place fwht n - input vector matrix with dimension dim for(unsigned long",
    "i = 0 ; i <",
    "n ; + + i )      fwht_opt(data + i * dim , log2(dim ) ) ; ....",
    "to embedded f2fin the code , enclose ` # include \" hpp / fast2food_factory.hpp \" ` in your test file and consider the following usage guidelines .            ....",
    "static fast2food * createfast2food(fast2foodtype fast2foodtype , float * data ,               const unsigned long nvec , const unsigned long dim , const unsigned long d ,               const float sigma = 1.0 , const unsigned long t = 5 ) ; ....      * ` fast2foodtype ` : ` fast2foodfactory::rbf ` or ` fast2foodfactory::matern ` * ` data ` : input data matrix * ` nvec ` : number of feature vectors * ` dim ` : feature dimension * ` d ` : number of basis kernel expansions * ` sigma ` : rbf sigma parameter * ` t ` : experimental matern kernel parameter      * ` ff_nvec ` : number of input feature vectors , type unsigned long . * ` ff_dim ` : feature dimension , type unsigned long . * ` ff_dimpad ` : padded feature dimension , type unsigned long . * ` ff_d ` : integer part of the quotient between the input number of basis kernel expansions ` d ` and ` ff_dimpad ` , type unsigned long . * ` ff_dimd ` : number of kernel basis expansions defined as the product ` ff_d * ff_dimpad ` , type unsigned long . *",
    "` ff_data ` : pointer input data matrix , type float . * ` ff_dataout ` : pointer internal data matrix , type float . * ` diag_pi ` : random permutation diagonal matrix @xmath18 , type unsigned long vector . * ` diag_g ` : random gaussian diagonal matrix @xmath52 , type float vector . *",
    "` diag_s ` : scaling diagonal matrix @xmath34 , type float vector . * ` diag_b ` : random diagonal matrix @xmath17 with @xmath55 entries , type unsigned long vector .",
    "the _ distributed _ oriented version is intended to be embedded in a distributed system and allows to compute random numbers by the use of hashes .",
    "+ * description * + ` createfast2food ` initializes the ` fast2food ` object given the initialization parameters + * header *    .... static fast2food * createfast2food(fast2foodtype fast2foodtype , float * data ,               const unsigned long nvec , const unsigned long dim , const unsigned long d ,               const unsigned long seed , const float sigma = 1.0 , const unsigned long t = 5 ) ; ....      * ` fast2foodtype ` : ` fast2foodfactory::rbf ` or ` fast2foodfactory::matern ` . *",
    "` data ` : input data matrix * ` nvec ` : number of feature vectors * ` dim ` : feature dimension * ` d ` : number of basis kernel expansions * ` seed ` : seed to generate random distributions * ` sigma ` : rbf sigma parameter * ` t ` : experimental matern kernel parameter      * ` ff_nvec ` : number of input feature vectors , type unsigned long . * ` ff_dim ` : feature dimension , type unsigned long . * ` ff_dimpad ` : padded feature dimension , type unsigned long . * ` ff_d ` : integer part of the quotient between the input number of basis kernel expansions ` d ` and ` ff_dimpad ` , type unsigned long . * ` ff_dimd ` : number of kernel basis expansions defined as the product ` ff_d * ff_dimpad ` , type unsigned long . * ` ff_data ` : pointer to input data matrix , type float . * ` ff_dataout ` : pointer to internal data matrix , type float . * ` diag_pi ` : random permutation diagonal matrix @xmath18 , type unsigned long vector . * ` diag_g ` : random gaussian diagonal matrix @xmath52 , type float vector . *",
    "` diag_s ` : scaling diagonal matrix @xmath34 , type float vector . * ` diag_b ` : random diagonal matrix @xmath17 with @xmath55 entries , type unsigned long vector .",
    "//seed random distributions const unsigned long seed = ( unsigned long)rd ( ) ;           fast2food * fast2food =   fast2food * fast2food = fast2foodfactory::createfast2food(fast2foodfactory::rbf , datain , nvec , dim , d , seed , sigma ) ;          //seed",
    "random distributions const unsigned long seed = ( unsigned long)rd ( ) ;           fast2food * fast2food =   fast2food",
    "* fast2food = fast2foodfactory::createfast2food(fast2foodfactory::matern , datain , nvec , dim , d , seed , sigma , t ) ;"
  ],
  "abstract_text": [
    "<S> f2fis a c++ library for large - scale machine learning . </S>",
    "<S> it contains a cpu optimized implementation of the fastfood algorithm in @xcite , that allows the computation of approximated kernel expansions in loglinear time . </S>",
    "<S> the algorithm requires to compute the product of walsh - hadamard transform ( wht ) matrices . a cache friendly simd fast walsh - hadamard transform ( fwht ) that achieves compelling speed and outperforms current state - of - the - art methods has been developed . </S>",
    "<S> f2fallows to obtain non - linear classification combining fastfood and a linear classifier .    </S>",
    "<S> approximate kernel expansions , fast walsh - hadamard transform , simd , gaussian rbf kernel , matern kernel </S>"
  ]
}