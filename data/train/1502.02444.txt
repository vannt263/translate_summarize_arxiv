{
  "article_text": [
    "in the research field of electrical engineering , following the principle of parsimony , many natural as well as artificial systems were modeled as linear systems . however it was realized that many natural / artificial phenomena are inherently non - linear .",
    "for instance , many models of neural circuits require non - linear difference / differential equations for modeling their dynamics .",
    "hopfield effectively succeeded in arriving at a model of associative memory based on mcculloch - pitts neuron .",
    "the discrete time hopfield network represents a non - linear dynamical system .",
    "hopfield neural network spurred lot of research efforts on modeling associative memories ( such as the bi - directional associative memory ) .",
    "however , it was realized that the hopfield neural network is based on undirected graph(for topological representation of network architecture ) and hence does not have any directed cycles in the network architecture .",
    "thus , in that sense hopfield neural network ( hnn ) does not constitute a recurrent neural network .",
    "many recurrent networks , such as jordan - elman networks were proposed and studied extensively .",
    "after literature survey , we were motivated to propose an artificial neural network ( ann ) , in the spirit of hnn , but the network architecture is based on a directed graph with directed cycles in it .",
    "our research efforts are summarized in this paper . in section-2 ,",
    "a novel real as well as complex recurrent hopfield network is discussed . in section-3 , experimental investigations of recurrent hopfield neural network are discussed . in section-4 ,",
    "some applications are proposed .",
    "the research paper concludes in section-5 .",
    "recurrent hopfield neural network ( rhnn ) is an artificial neural network model .",
    "it is a nonlinear dynamical system represented by a weighted , directed graph .",
    "the nodes of the graph represent artificial neurons and the edge weights correspond to synaptic weights . at each neuron / node",
    ", there is a threshold value .",
    "thus , in summary , a recurrent hopfield neural network can be represented by a synaptic weight matrix , m that is not necessarily symmetric and a threshold vector , t. the order of the network corresponds to the number of neurons .",
    "such a neural network potentially has some directed cycles in the associated graph .",
    "every neuron is in one of the two possible states + 1 or -1 .",
    "thus , the state space of nth order recurrent hopfield neural network ( rhnn ) is the symmetric n - dimensional unit hypercube .",
    "let the state of _ _ i__th neuron at time",
    "t be denoted by    @xmath0    thus , the state of the non - linear dynamical system is represented by the n x 1 vector , @xmath1 .",
    "the state updation at the _",
    "_ i__th node is governed by the following equation    @xmath2    where sign ( . )",
    "is the signum function . in other words",
    ", @xmath3 is + 1 if the term in the brackets is non - negative , otherwise @xmath3 is -1 .",
    "depending on the set of nodes at which the state updation by eq .",
    "[ eq:2.1 ]   is performed at any time _",
    "t _ , the recurrent hopfield neural network ( rhnn ) operation is classified into the following modes .    *",
    "serial mode : the state updation in eq .",
    "[ eq:2.1 ]   is performed exactly at one of the nodes / neurons at time _",
    "t_. * fully parallel mode : the state updation in eq .",
    "[ eq:2.1 ]   is performed simultaneously at all the n nodes / neurons at time _",
    "t_.    if the matrix m is symmetric ,",
    "then the dynamics of recurrent hopfield neural network ( rhnn ) is exactly same as the same as that of ordinary hopfield neural network ( ohnn ) .    for the sake of completeness",
    ", we now include the dynamics of ordinary hopfield neural network ( ohnn ) with m being a symmetric matrix [ 1 ] . in the state space of ordinary hopfield neural network ( ohnn ) , a non - linear dynamical system , there are certain distinguished states , called the _ stable states_.    a state , @xmath4 is called a stable state if and only if    @xmath5    thus , once the hopfield neural network ( hnn ) reaches the stable state , irrespective of the mode of operation of the network , the hnn will remain in that state for ever .",
    "thus , there is no further change of the state of hnn once a stable state is reached .",
    "the following convergence theorem summarizes the dynamics of ordinary hopfield neural network ( ohnn ) .",
    "it characterizes the operation of neural network as an associative memory .",
    "[ theorem1 ] let the pair @xmath6 specify a ordinary hopfield neural network ( with m being symmetric).then the following hold true :    [ 1 ] hopfield : if n is operating in a serial mode and the elements of the diagonal of m are non - negative , the network will always converge to a stable state ( i.e. there are no cycles in the state space ) .",
    "[ 2 ] goles : if n is operating in the fully parallel mode , the network will always converge to a stable state or to a cycle of length 2 ( i.e the cycles in the state space are of length @xmath7 ) .",
    "it should be noted that in [ 4 ] , it is shown that the synaptic weight matrix of ohnn can be assumed to be an arbitrary symmetric matrix .",
    "we now consider a recurrent hopfield neural network where the neurons ( artificial ) are organized into finitely many layers ( with @xmath8 neurons in each layer ) .",
    "thus the synaptic weight matrix of such a neural network can be captured by a block matrix , w which is not necessarily symmetric .",
    "traditionally , the convergence theorem associated with ordinary hopfield neural network ( ohnn ) ( i.e. theorem  [ theorem1 ] ) effectively considered only ( i ) serial mode and ( ii ) fully parallel mode .",
    "but the arrangement of neurons in multiple layers naturally leads to operation of the hopfield network ( ordinary as well as recurrent ) in other parallel modes of operation .",
    "this is accomplished in the following manner :    * it should be noted that the state vector of the ordinary / recurrent hopfield neural network at time _",
    "t _ can be partitioned into sub - vectors in the following manner .",
    "+    + where @xmath9 denotes the state of neurons in layer 1 and l is the number of layers .",
    "+ for the sake of notational convenience , suppose that all the l layers have the same number of neurons in it and state updation at any time instant takes place simultaneously at all nodes in any one layer .",
    "this clearly corresponds to parallel mode of operation which is not the fully parallel mode ( if there is a single neuron in each layer , it corresponds to the serial mode of operation ) .",
    "the state updation in such a parallel mode can be captured through the following equation : for @xmath10 , we have that + @xmath11 + where @xmath12 is the sub - matrix of w and @xmath13 is the corresponding threshold vector .",
    "it can be shown that there is no loss of generality in assuming that the threshold vector is a zero vector .",
    "+ * note : * to the best of our knowledge , the dynamics of even ohnn in other parallel modes of operation ( not fully parallel ) has not been fully investigated .",
    "+ this idea of layering the neurons enables one to study the dynamics of recurrent hopfield neural network with various types of directed cycles in it .",
    "theoretical efforts to capture the dynamics of such neural networks are underway . in the section-3 ,",
    "we summarize our experimental efforts . *",
    "* energy landscape synthesis : * + now we investigate the energy landscape visited by a recurrent hopfield neural network ( rhnn ) and relate it to the energy landscape visited by the associated ordinary hopfield neural network ( ohnn ) . *",
    "consider the quadratic energy function associated with the dynamics of a rhnn whose synaptic weight matrix is the non - symmetric matrix , w. it is easy to see that + @xmath14 + where @xmath15 is the symmetric matrix associated with w , i.e. symmetric part .",
    "* now , from the above equation , it is clear that the energy landscape visited by the rhnn with synaptic weight matrix w is exactly same as the energy landscape visited by the ohnn with the symmetric synaptic weight matrix @xmath16 .",
    "now , we synthesize the energy landscape of rhnn ( or associated ohnn ) in the following manner : + we need the following definition in the succeeding discussion .",
    "the local minimum vectors of the energy function associated with w are called * anti - stable states*. thus , if u is an anti - stable state of matrix w , then it satisfies the condition    @xmath17    using the definitions , we have the following general result .",
    "if a corner of unit hypercube is an eigenvector of w corresponding to positive / negative eigenvalue , then it is also a stable / anti - stable state",
    ".    follows from the utilization of definitions of eigenvectors , stable / anti - stable states .",
    "* * synthesis of symmetric synaptic weight matrix with desired energy landscape : * + let @xmath18 where @xmath19 be desired positive eigenvalues ( with @xmath20 being the desired stable value ) and let @xmath21 be the desired stable states of @xmath15 .",
    "then it is easy to see that the following symmetric matrix constitutes the desired synaptic weight matrix of ordinary hopfield neural network ( using the spectral representation of symmetric matrix @xmath15 ) : + @xmath22 + it is clear that the synaptic weight matrix is positive definite . in the same spirit as above",
    ", we now synthesize a synaptic weight matrix , with desired stable / anti- stable values and the corresponding stable / anti - stable states .",
    "let @xmath23 where @xmath24 be desired orthogonal stable states and @xmath25 where @xmath26 be the desired orthogonal anti - stable states .",
    "let the desired stable states be eigenvectors corresponding to positive eigenvalues and let the desired anti - stable states be eigenvectors corresponding to negative eigenvalues .",
    "the spectral representation of desired synaptic weight matrix is given by @xmath27 where @xmath28 s are desired positive eigenvalues and @xmath29 s are desired negative eigenvalues .",
    "hence the above construction provides a method of arriving at desired energy landscape ( with orthogonal stable / anti - stable states and the corresponding positive / negative energy values ) .",
    "it can be easily shown that trace(w ) is constant contribution ( dc value ) to the value of quadratic form @xmath30 at all corners , x of unit hypercube .",
    "thus , the location of stable/ anti - stable states is invariant under modification of trace(w ) value .",
    "thus , from the standpoint of location / computation of stable / anti - stable states , trace(w ) can be set to zero .",
    "* it should be kept in mind that the dynamics of ohnn with the symmetric synaptic weight matrix @xmath15 is summarized by theorem [ theorem1 ] , i.e. either there is convergence to stable state or cycle of length 2 is reached .",
    "but in the case rhnn with non - symmetric synaptic weight matrix , there may be no convergence or the cycles are of length strictly larger than 2 . in the section-3",
    ", we summarize our experimental efforts .      in research literature on artificial neural networks , there were attempts by many researchers to propose complex valued neural networks ( cvnns ) in which the synaptic weights , inputs are necessarily complex numbers [ 2 , mgz ] . in our research efforts on cvnns , we proposed and studied one possible ordinary complex hopfield neural network ( ochnn ) first discussed in [ 3 , rap ] , [ 5 - 12 ] .",
    "the detailed descriptions of such an artificial neural network requires the following concepts .    1 .",
    "_ complex hypercube _ : consider a vector of dimension n whose components assume values in the following set @xmath31.thus there are points as the corners of a set called the ",
    "complex hypercube  .",
    "_ complex signum function _ : consider a complex number a + jb. the complex signum function is defined as follows : @xmath32    in the same spirit of the real valued rhnn , we briefly summarize complex recurrent hopfield neural network ( crhnn ) :    * the complex synaptic weight matrix of such an artificial neural network ( crhnn ) need not be a hermitian matrix .",
    "the activation function in such a complex valued neural network is the complex signum function .",
    "the convergence theorem associated with ordinary complex hopfield neural network ( ochnn ) is provided in [ 3 , rap ] .",
    "we are experimentally investigating the dynamics of crhnn .",
    "in this section , certain network configurations are simulated with various initial conditions ( lying on the unit hypercube ) in order to provide insight on the dynamics of the recurrent hopfield networks .",
    "the recurrent neural networks whose dynamics are presented in this study have asymmetric synaptic weight matrices with zeros on the diagonal ( i.e. no self loops are allowed . as discussed in _",
    "remark(4 ) _ , this assumption can be made without loss of generality ) .",
    "edge weights ( the synaptic weights ) are allowed to assume both positive and negative signs .",
    "the size of the network ( i.e. number of neurons ) directly determines the size of synaptic weight matrix . in the experiments",
    ", we are going to investigate three cases : synaptic weight matrices with ( i ) _ both positive and negative _ , ( ii ) _ only positive _ and ( iii ) _ only negative _ entries .      a recurrent hopfield network , in general , can be randomly initialized in a state _ s _ where    @xmath33    is the state space and    @xmath34    the size of the state space exponentially increases as the number of neurons",
    "are increased . in order to reduce the computational overhead , the number of initial states utilized",
    "are limited to a maximum of 1024 .    *",
    "the decimals numbers in the range [ 0,1024 ) are converted to binary and then all 0 s are mapped to -1 s ( from this point on , states are going to be referred by using their decimal values ) .      during the experiments ,",
    "both parallel and serial modes are tested . however",
    ", serial mode results in two specific cases .",
    "the first is the case where network converges and the second is when there is a cycle of 2 as iterations proceed . on the other hand ,",
    "parallel mode exhibits more interesting outcomes .",
    "hence , the focus during the experiments have been placed on this update mode .        networks with various polarities of synaptic connections behave expectedly in different manners .",
    "the experiments have led to certain types of observations in terms of convergence and cycle characteristics . in vast majority of trials , it has been observed that given an initial state from @xmath35 , the network either converges to a single state or converges into a certain pattern of cycle in multiple iterations .",
    "+    @xmath36    * in networks with both positive and negative synaptic connections , depending on the network size , convergence can take place immediately if the initial state is located in a small proximity to one of the basins of attraction in the energy landscape .",
    "however simple generalization of the network dynamics is not possible .",
    "the network can be led to certain cycles which are of various lengths .",
    "one main observation is that a collection of distinct initial states can be driven into one or more different cycles .",
    "this result is interesting since it may imply that a recurrent hopfield network can be used as a function to map signals into certain common cycles . to illustrate the possible state transitions , the network in [ fig : toymatrix ]",
    "is to be used .",
    "* when parallel mode of update is applied on this network , the results revealed that there exist 6 cycles in which the rhnn loops given all possible initializations . in this mode , length of the cycles varied between 2 and 4 while in the serial mode of operation , the network reached cycles of length of at most two .",
    "furthermore , it often converged to certain minima points in state space . to note , the cycles of length 1 are used to refer to the stable states .",
    "the observed cycles , following the naming convention provided previously , are listed in  [ cycles1 ] . *",
    "if the non - symmetric synaptic weight matrix is non - negative or non - positive , it is observed that the network converges to a single state or to a cycle of length 2 . *",
    "if the non - symmetric synaptic weight matrix has positive and negative elements , it is observed that the cycles are of length strictly equal or larger than 2 .",
    "* it is observed that in the case of various rhnns , there are more than one directed cycles in the state space ( reached with a change of initial conditions ) . * in some special cases a single cycle is reached .    *",
    "the simulations show that the networks of mixed - polarity connections are favoring certain states or combinations of the whole possible state space .",
    "the parallel mode yielded that inputs that are close in terms of hamming distance usually go through the same cycles or end up at certain stable states . * larger networks with more than 20 neurons",
    "are observed to fail to reach at detectable patterns or single state in the parallel mode within reasonable number of iterations .",
    "next section will focus on the energy values associated with various states .",
    "the energy values may be useful to make better conclusions on the observations .",
    ".the cycles for the toy example [ cols=\"^,^ \" , ]      the recurrent hopfield networks experimented in this study exhibit two characteristics .",
    "first characteristic is that the network reaches a constant energy value when it gets trapped in a single state or a set of states ( a cycle ) .",
    "the second characteristic is that the energy of the network fluctuates around a certain dc value .",
    "the latter has been observed to occur in cycles only .",
    "the typical energy states of the toy example during 128 trials are given in figures [ fig : energy1 ] - [ fig : energy5 ] .",
    "it is useful to notice that the network s energy states ( during convergence or cyclic period ) which are caused by a collection of distinct initial states are the same recapitulating what has been stated previously .    * in [ fig : energy1 ]",
    ", a common energy profile of network is shown .",
    "usually , in the toy example , the network configures itself into one of the low energy regions . *",
    "[ fig : energy2 ] and [ fig : energy4 ] shows that the network obtains a steady energy even during the cycles while [ fig : energy3 ] and [ fig : energy5 ] indicates that the network s energy oscillates as well as the state it assumes on each iteration . *",
    "the observations indicate that larger networks exhibit similar behavior in the parallel mode but with the exception that energy fluctuations are more often present during cyclic period . * in serial mode of operation",
    ", the energy of the network remains same if it undergoes a single state convergence whereas it fluctuates if in a cycle of 2 .",
    "non - linear dynamical systems exhibit interesting dynamic behavior such as ( i ) _ convergence _ , ( ii ) _ oscillations _ ( periodic behavior ) and ( iii ) _",
    "chaos_.    researchers have identified certain applications ( such as cryptography ) for interesting non - linear dynamical systems .",
    "specifically neural circuits based on non - linear dynamical systems were identified with certain functional capabilities of biological brains .",
    "the authors realized that recurrent hopfield network in this research paper could be capitalized for various applications .    *",
    "the main observation is that the cycles of various lengths in the state space could be utilized for applications such as `` multi - pattern '' based associative memory .",
    "based on the length of cycle , a memory utilizing multiple patterns ( corners of the hypercube ) can be synthesized . * in the rhnns investigated , sometimes consensus to a single cycle is achieved .",
    "thus , such networks could be utilized for memorizing a collection of states corresponding to a cycle . * in some rhnns simulated , multiple cycles are reached when the initial condition is varied .",
    "thus , in this case consensus to multiple memory groups ( of states ) is achieved .",
    "in this research paper , our efforts have focused on introducing a novel real / complex valued recurrent hopfield neural network . in addition , we have shown a method to synthesize a desired energy landscape for such networks . in the experimental section",
    ", the dynamics of such real valued recurrent hopfield networks have been investigated in terms of the state transitions and energy . finally , certain potential applications which exploit the shown properties are proposed .",
    "1 .   [ 1 ] j.j .",
    "hopfield , `` neural networks and physical systems with emergent collective computational abilities , '' proceedings of national academy of sciences , usa vol .",
    "2554 - 2558 , 1982 .",
    "[ 2 ] mehmet kerem muezzinoglu , cuneyt guzelis and jacek m. zurada , `` a new design method for the complex - valued multistate hopfield associative memory , '' ieee transactions on neural networks , vol .",
    "4 , july 2003 .",
    "+ 3 .   [ 3 ] g. rama murthy and d. praveen , `` complex - valued neural associative memory on the complex hypercube , '' ieee conference on cybernetics and intelligent systems , singapore , 1 - 3 december , 2004 .",
    "[ 4 ] g. rama murthy , `` optimal signal design for magnetic and optical recording channels , '' bellcore technical memorandum , tm - nwt-018026 , april 1st , 1991 .",
    "+ 5 .   [ 5 ] g. rama murthy and b. nischal , `` hopfield - amari neural network : minimization of quadratic forms , '' the 6th international conference on soft computing and intelligent systems , kobe convention center ( kobe portopia hotel ) ,",
    "november 20 - 24 , 2012 , kobe , japan .",
    "[ 6 ] akira hirose , `` complex valued neural networks : theories and applications , '' world scientific publishing co ,",
    "november 2003 .",
    "[ 7 ] g. rama murthy and d. praveen , `` a novel associative memory on the complex hypercube lattice , '' 16th european symposium on artificial neural networks , april 2008 .",
    "[ 8 ] g. rama murthy , `` some novel real/ complex valued neural network models , '' advances in soft computing , springer series on computational intelligence : theory and applications , proceedings of 9th fuzzy days , september 18 - 20 2006 , dortmund , germany .",
    "[ 9 ] g. jagadeesh , d. praveen and g. rama murthy , `` heteroassociative memories on the complex hypercube , '' proceedings of 20th ijcai workshop on complex valued neural networks , january 6 - 12 2007 .",
    "[ 10 ] v. sree hari rao and g. rama murthy , `` global dynamics of a class of complex valued neural networks , '' special issue on cvnns of international journal of neural systems , april 2008 .",
    "[ 11 ] g. rama murthy , `` infinite population , complex valued state neural network on the complex hypercube , '' proceedings of international conference on cognitive science , 2004 .",
    "[ 12 ] g. rama murthy , `` multidimensional neural networks - unified theory , '' new age international publishers , new delhi , 2007 ."
  ],
  "abstract_text": [
    "<S> in this research paper novel real / complex valued recurrent hopfield neural network ( rhnn ) is proposed . </S>",
    "<S> the method of synthesizing the energy landscape of such a network and the experimental investigation of dynamics of recurrent hopfield network is discussed . </S>",
    "<S> parallel modes of operation ( other than fully parallel mode ) in layered rhnn is proposed . also , certain potential applications are proposed . </S>"
  ]
}