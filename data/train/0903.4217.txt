{
  "article_text": [
    "the central question in this paper is how to efficiently estimate the conditional probability of label @xmath3 given an observation @xmath4 .",
    "virtually all approaches for solving this problem require @xmath5 time . a commonly used one - against - all approach , which tries to predict the probability of label @xmath6 versus all other labels , for each @xmath7 , requires @xmath5 time per training example .",
    "another common @xmath5 approach is to learn a scoring function @xmath8 and convert it into a conditional probability estimate according to @xmath9 , where @xmath10 is a normalization factor .    the motivation for dealing with the computational difficulty is the usual one",
    " we want the capability to solve otherwise unsolvable problems .",
    "for example , one of our experiments involves a probabilistic prediction problem with roughly @xmath2 labels and @xmath11 examples , where any @xmath5 solution is intractable .      in section  [ sec : online ] , we provide the first online supervised learning algorithm that trains and predicts with @xmath0 computation per example .",
    "the algorithm does not require knowledge of @xmath1 in advance ; it adapts naturally as new labels are encountered .",
    "the prediction algorithm uses a binary tree where regressors are used at each node to predict the conditional probability that the true label is to the left or right .",
    "the probability of a leaf is estimated as the product of the appropriate conditional probability estimates on the path from root to leaf . in our experiments , we use linear regressors trained via stochastic gradient descent .    the difficult part of this algorithm is constructing the tree itself . when the number of labels is large , it becomes critical to construct easily solvable binary problems at the nodes . in section  [ sec : online - analysis ] , we introduce a tree - construction rule with two desirable properties .",
    "first , it always results in depth @xmath0 .",
    "it also encourages natural problems by minimizing expected loss at the nodes .",
    "the technique used in the algorithm is also useful for other prediction problems such as multiclass classification .",
    "we test the algorithm empirically on two datasets ( in section  [ sec : online - experiments ] ) , and find that it both improves performance over naive tree - building approaches and competes in prediction performance with the common one - against - all approach , which is exponentially slower .",
    "finally , we analyze a broader set of logarithmic time probability estimation methods . in section",
    "[ sec : cpt ] we prove that any tree based approach has squared loss bounded by the tree depth squared times the average squared loss of the node regressors used .",
    "in contrast , the pecoc approach  @xcite has squared loss bounded by just @xmath12 times the average squared loss but uses @xmath5 computation .",
    "this suggests a tradeoff between computation and squared loss multiplier .",
    "section  [ sec : hpecoc ] describes a @xmath13-parameterized construction achieving a ratio of @xmath14 while using @xmath15 computation , where @xmath16 gives the tree approach and @xmath17 gives pecoc .",
    "there are many methods used to solve conditional probability estimation problems , but very few of them achieve a logarithmic dependence on @xmath1 .",
    "the ones we know are batch constructed regression trees , c4.5",
    "@xcite , id3  @xcite , or treenet  @xcite , which are both too slow to consider on datasets with the scale of interest , and incapable of reasonably dealing with new labels appearing over time .",
    "mnih and hinton  @xcite constructed a special purpose tree - based algorithm for language modeling , which is perhaps the most similar previous work .",
    "the algorithm there is specialized to word prediction and is substantially slower since it involves many iterations through the training data",
    ". however , the general analysis we provide in section  [ sec : cpt ] applies to their algorithm .",
    "we regard the empirical success of their algorithm as further evidence that tree - based approaches merit investigation .",
    "section  [ sec : static ] states and analyses methods for logarithmic time probabilistic prediction given a tree structure .",
    "section  [ sec : online ] gives an algorithm for building the tree structure .",
    "the analysis in the first section is sufficiently general so that it applies to the second .",
    "given samples from a distribution @xmath18 over @xmath19 , where @xmath20 is an arbitrary observation space and @xmath21 , the goal is to estimate the conditional probability @xmath22 of a label @xmath23 for a new observation @xmath4 .    for an estimator @xmath24 of @xmath22 , the _ squared loss _ of @xmath25 with respect to @xmath18 is defined as @xmath26 it is more common to define an observable squared loss where @xmath27 in equation  ( [ lossdef ] ) is replaced by @xmath28 .",
    "we consider _ regret _ with respect to the common definition , since it is well known that the difference between observable squared loss and the minimum possible observable squared loss is equal to @xmath29 .",
    "we therefore use regret and squared loss interchangeably in this paper .",
    "it is well known that squared loss is a strictly proper scoring rule  @xcite , thus @xmath30 is uniquely minimized by @xmath31 .",
    "our analysis focuses on squared loss because it is a bounded proper scoring rule .",
    "the boundedness implies that convergence guarantees hold under weaker assumptions than for unbounded proper scoring rules such as log loss .",
    "this section assumes that a tree structure is given , and analyzes how to use it for probabilistic logarithmic time prediction .",
    "consider a fixed binary tree whose leaves are the @xmath1 labels . for a leaf node @xmath23 , let @xmath32 be the set of non - leaf nodes on the path from the root to @xmath33 in the tree .",
    "each non - leaf node @xmath6 is associated with the regression problem of predicting the probability , under @xmath18 , that the label @xmath33 of a given observation @xmath4 is in the left subtree of @xmath6 , conditioned on @xmath34 .",
    "the following procedure shows how to transform multiclass examples into binary examples for each non - leaf node in the tree . here",
    "@xmath35 is @xmath36 when @xmath33 is in the left subtree of node @xmath6 , and @xmath28 otherwise .",
    "given a new observation @xmath4 and a label @xmath23 , we use the learned binary regressors @xmath37 to estimate @xmath22 . letting @xmath38 and @xmath39",
    ", we define the estimate @xmath40      algorithm  [ rt - train ] implicitly defines a distribution @xmath41 over @xmath42 induced at node @xmath6 : a sample from @xmath41 is obtained by drawing @xmath43 according to @xmath18 until @xmath44 , and outputting @xmath45 ( although we never explicitly perform this sampling ) . the following theorem bounds the squared loss of @xmath25 given the average squared loss of the binary regressors .    [",
    "tree : cor ] for any distribution @xmath18 , any set of node estimators @xmath46 , and any pair @xmath43 , with @xmath47 given by equation  , @xmath48 where @xmath49 and the expectation is over @xmath6 chosen uniformly at random from @xmath32 .",
    "we use lemma  [ tree : thm ] . using the notation of its proof , observe that @xmath50 using jensen s inequality .",
    "most of the theorem is proved with the following core lemma .",
    "for a node @xmath6 on the path from the root to label @xmath33 , define @xmath51 , the conditional probability that the label is consistent with the next step from @xmath6 given that all previous steps are consistent .",
    "similarly define @xmath52 .",
    "[ tree : thm ] for any distribution @xmath18 , any set of node estimators @xmath46 , and any pair @xmath43 , with @xmath47 given by equation  ,",
    "@xmath53    -.2 in the last inequality is the simplest  it says the differences in errors add .",
    "however , the quantity after the first inequality can be much tighter .",
    "we first note that @xmath54 since @xmath55 and @xmath56 .",
    "we use a geometric argument . with @xmath57 defining the volume of one `` corner '' of a cube with sides @xmath58 , slabs @xmath59",
    "fill in the remaining volume ( with overlap ) .",
    "consequently , we can bound the difference in volume as    @xmath60    since all @xmath61 and @xmath62 are bounded by 1 .    as suggested by the proof",
    ", the lemma s bound can be asymptotically tight .",
    "if all @xmath63 are equal to some @xmath64 and all @xmath65 are small , the left side is approximately @xmath66 , a factor @xmath67 times the right side .",
    "the conditional probability tree is as computationally tractable as we could hope for , but is not as robust as we could hope for .",
    "for example , the pecoc approach  @xcite yields a squared loss multiplier of @xmath12 independent of the number of labels .",
    "is there an approach more robust than the tree , but requiring less computation than pecoc ?",
    "we provide a construction which trades off between the extremes of pecoc and the conditional probability tree .",
    "the essential idea is to shift from a binary tree to a @xmath13-way tree , where pecoc with @xmath68 regressors is used at each node in the tree to estimate the probability of any child conditioned on reaching the node . for simplicity , we assume that @xmath13 is a power of 2 , and @xmath1 is a power of @xmath13 .",
    "[ thm : hp ] pick a @xmath13-way tree on the set of @xmath1 labels , where @xmath13 is a power of 2 . for all distributions @xmath18 and all sets of learned regressors , with @xmath68 regressors per node of the tree , for all pairs @xmath43 , @xmath69 where @xmath70 is the average squared loss of the @xmath71 questioned regressors .",
    "the proof is by composition of two lemmas .    in each node of the tree ,",
    "lemma  [ pecoc : thm ] bounds the power of the adversary to disturb the probability estimate as a function of the adversary s regret .",
    "similarly , lemma  [ tree : thm ] bounds the power of the adversary to induce an overall misestimate as a function of the adversary s power to disturb the estimates within each node on the path .",
    "the curve below illustrates how the construction trades off computation for a better regret bound as a function of @xmath13 .        to complete the proof of theorem  [ thm : hp ]",
    "we describe the pecoc construction in section  [ sec : pe ] and prove lemma  [ pecoc : thm ] in section  [ sec : careful ] .",
    "the pecoc construction is defined by a binary matrix @xmath72 with each column a label and each row defining a regression problem .",
    "the regression problem corresponding to row @xmath6 is to predict the probability given @xmath73 that the correct label is in the subset @xmath74    we use an explicit family of hadamard codes given by the recursive formula @xmath75 we use a matrix @xmath76 with @xmath77 , noting that its size @xmath78 is less than @xmath79 ; if @xmath80 we simply add dummy labels . we henceforth assume without loss of generality that @xmath1 is a power of  2 .",
    "we train pecoc according to the following algorithm .",
    "[ alg : pecoc ]    given a new observation @xmath4 and a label @xmath23 , pecoc uses the binary regressors @xmath81 learned in algorithm  [ alg : pecoc ] to estimate @xmath22 using the formula @xmath82 - 1 ,          \\label{pecocest}\\end{aligned}\\ ] ] where the expectation is over @xmath6 drawn uniformly from the rows of @xmath72 .",
    "the reason for this formula is clarified by the proof of lemma  [ pecoc : thm ] .",
    "the following theorem gives the precise regret bound , which follows from the analysis in @xcite but is tighter for small values of @xmath1 than the bound stated there .",
    "[ pecoc : thm]_(pecoc regret  @xcite ) for all distributions @xmath18 and all sets of regressors @xmath81 ( as defined in algorithm  [ alg : pecoc ] ) , for all @xmath4 and @xmath23 , @xmath83 where @xmath84 is the subset defined by row @xmath6 per  .",
    "_    since the code and the prediction algorithm are symmetric with respect to set inclusion , we can assume without loss of generality that @xmath33 is in every subset ( complementing all subsets not containing @xmath33 ) .",
    "thus every entry @xmath85 , and by the pecoc output estimate of @xmath22 is @xmath86    let @xmath87 denote the perfect subset estimators , and write @xmath88 . by the nature of @xmath72 , the label @xmath33 under consideration occurs in every subset , and every other label @xmath89 in exactly half the subsets , so that @xmath90 this gives @xmath91 , for squared loss @xmath92 .",
    "one of the subsets , say the first , is trivial ( it includes all labels ) , and for it we stipulate the true probability @xmath93 , so @xmath94 . letting @xmath95",
    "denote the mean of the other @xmath96 errors @xmath97 , the squared loss is @xmath98 , establishing the theorem .",
    "[ sec : online ] the analysis of section  [ sec : binary ] applies to any binary tree , and motivates the creation of trees which have small depth and small regret at the nodes .",
    "this leaves the question , `` which tree should we use ? ''",
    "we give an online tree construction algorithm with several useful properties .",
    "in particular , the algorithm does nt require any prior knowledge of the labels , and takes @xmath0 computation per example , when there are @xmath1 labels .",
    "the algorithm guarantees a tree with @xmath0 maximum depth using a decision rule that trades off between depth and ease of prediction .",
    "algorithm  [ alg : rt ] builds and maintains a tree , whose leaves are in one - to - one correspondence with the labels seen so far .",
    "each node @xmath6 in the tree is associated with a regressor @xmath99 $ ] .",
    "given a new sample @xmath100 , we consider two cases .",
    "if @xmath33 already exists as a label of some leaf in the tree , then there is an associated root - to - leaf path and we can use the conditional probability tree algorithms of the previous section to train and test on @xmath43 , with one minor modification when training : we add a regressor at the leaf and train it with the example @xmath101 .    if @xmath33 does not exist in the tree , then the algorithm still traverses the tree to some leaf @xmath102 , using a decision rule that computes a direction ( left or right ) at each non - leaf node encountered .",
    "once leaf @xmath102 is reached , it necessarily corresponds to some label @xmath103 .",
    "we convert @xmath102 to a non - leaf node with left child @xmath104 and right child @xmath33 .",
    "the regressor at node @xmath102 is duplicated for @xmath104 .",
    "a new regressor is created for @xmath33 and trained on the example @xmath101 .",
    "we now describe the decision rule used to decide which way to go ( left or right ) at each non - leaf node @xmath6 encountered during the traversal .",
    "first , let @xmath105 denote the number of children to the left of node @xmath6 , and @xmath106 the number to the right . if @xmath107 , where @xmath108 is the current prediction associated with node @xmath6 on @xmath73 , then the regressor favors the right subtree for this input , and otherwise the left subtree .",
    "if the regressor favors the side with the smaller number of elements , then this direction is chosen .",
    "if the regressor favors the side with more elements , then the algorithm faces a dilemma . on one hand ,",
    "sending the new label to the right would result in a more highly balanced tree , but on the other hand it would result in a training sample disagreeing with the current regressor s prediction .",
    "our resolution is to define an objective function @xmath109 and send the label to the right of node @xmath6 if @xmath110 here @xmath111 is a free parameter set for the run of the entire algorithm .",
    "when @xmath112 , the rule indicates that we should place new labels on the side with fewer current labels , resulting in a perfectly balanced tree . when @xmath113 , the direction chosen is always the one currently favored by the regressor",
    "a trade - off between these two objectives is provided by values of @xmath111 between these two extremes .          in this section",
    "we analyze algorithm  [ alg : rt ] . throughout the section , for any tree node under consideration",
    ", we will use @xmath115 for the total number of leaves under the node , @xmath116 the number on the left and @xmath117 on the right , with @xmath118 .",
    "we note that rule is symmetric with respect to @xmath116 and  @xmath117 .",
    "we also define @xmath119 claim   will establish that at most about a fraction @xmath120 of the leaves can fall on either side of a node , with @xmath121 for @xmath122 and @xmath123 as @xmath124 .    if a node has @xmath116 leaves in its left subtree , @xmath117 in the right , and @xmath125 altogether , if @xmath126 then a new leaf is added to the left subtree regardless of the prediction value @xmath64 at the node ( and symmetrically for  @xmath116 ) .",
    "we prove this inductively for @xmath117 ; the result for @xmath116 follows symmetrically .",
    "a non - leaf node starts with one left and one right child , and @xmath132 , @xmath133 satisfies the claim .",
    "given that @xmath117 , @xmath116 , and @xmath115 satisfy the claim , we now prove that when a leaf is added , so do the next values @xmath134 ( either @xmath117 or @xmath135 ) , @xmath136 ( respectively @xmath137 or @xmath116 ) , and @xmath138 .",
    "there are two cases .",
    "if @xmath139 then @xmath140 if @xmath141 then the next addition is to @xmath116 not @xmath117 , and @xmath142      if the root node has @xmath1 leaves below it , then by the preceding claim a child ( `` depth 1 '' ) of the root has at most @xmath145 leaves , a grandchild has at most @xmath146 leaves , and a depth-@xmath147 child has at most @xmath148 leaves , using @xmath149 . with @xmath150 ,",
    "a depth-@xmath147 child has at most 2 leaves , and thus further depth one , and we add one more to account for the ceiling function .",
    "that is , a disagreement occurs when the regressor s prediction is at most @xmath151 and the label is inserted to the right , or when the prediction is greater than @xmath151 and the label is inserted to the left .",
    "note that the number of disagreements incurred when adding a new label ( leaf ) is at most the depth of that leaf , and as the tree evolves the `` same '' leaf ( per the copying rule of the algorithm ) may become deeper but never shallower .",
    "thus the total number of disagreements incurred in building a tree is at most the sum of the depths of all leaves of the final tree .    to get a grasp on this quantity , for simplicity we disregard the additive @xmath152 in claim  [ bigside ] coming from adding vertices discretely , one at a time .",
    "( the effect is most dramatic when a node has just two children , @xmath153 , and adding a leaf necessarily produces a lopsided tree with @xmath154 and @xmath155 or vice - versa . for large values of @xmath118",
    "the effect of discretization is negligible . )",
    "the proof is by induction on  @xmath1 , starting from the base case @xmath160 where the total of the depths ( or total depth for short ) is  2 .",
    "it is well known that the entropy function @xmath161 is maximized by @xmath162 , so in the base case we do indeed have @xmath163 since @xmath164 .",
    "proceeding inductively , the total depth for an @xmath115-leaf tree with @xmath116- and @xmath117-leaf subtrees is the total depth of @xmath116 ( at most @xmath165 ) , plus the total depth of @xmath117 ( at most @xmath166 ) , plus @xmath115 ( since each leaf is 1 deeper in the full tree ) .",
    "since @xmath167 is a convex function , the worst case comes from the most unequal split , and applying the inductive hypothesis , the total depth for @xmath115 is at most @xmath168 completing the proof that @xmath169 is an upper bound .",
    "we conducted experiments on two datasets .",
    "the purpose of the first experiment is to show that the conditional probability tree ( cpt ) competes in prediction performance with existing exponentially slower approaches . to do this",
    ", we derive a label probability prediction problem from the publicly available reuters rcv1 dataset  @xcite .",
    "the second experiment is a full - scale test of the system where an exponentially slower approach is too intractable to seriously consider .",
    "we use a proprietary dataset that consists of webpages and associated advertisements , where the derived problem is to predict the probability that an ad would be displayed on the webpage .",
    "each dataset was split into a training and test set .",
    "each training or test sample is of the form @xmath43 .",
    "the algorithms train on the training set and produce a probabilistic rule @xmath170 that maps pairs of the form @xmath43 to numbers in the range @xmath171 $ ] , where we interpret @xmath172 as an approximation to @xmath22 .",
    "the algorithms are evaluated on the test set by computing the empirical squared loss , @xmath173 .",
    "the algorithms are allowed to continue learning as they are tested , however the predictions @xmath172 used above are computed before training on the sample @xmath43 .",
    "this type of evaluation is called `` progressive validation '' @xcite and accurately measures the performance of an online algorithm . in particular , it is an unbiased estimate of the algorithm s performance under the assumption that the @xmath43 pairs are identically and independently distributed . in the motivating applications of our algorithm",
    ", we expect new labels to appear throughout the learning process , which requires learning to occur continually in an online fashion .",
    "thus , turning learning off and computing a `` test loss '' is less natural",
    ". nevertheless , for the reuters dataset , we verified that the test loss and progressive validation are quite similar .",
    "for the web advertising dataset , the two measures were drastically different ( all methods performed much worse under test loss ) , due to the large number of labels that appear only in the test set .",
    "the cpt algorithm was executed with three tree - building construction methods : a random tree where uniform random left / right decisions were made until a leaf was encountered , a balanced tree according to algorithm  [ alg : rt ] with @xmath174 , and a general tree according to algorithm  [ alg : rt ] with @xmath175 . for the binary regression problems ( at the nodes ) , we used vowpal wabbit  @xcite , which is a simple linear regressor trained by stochastic gradient descent .",
    "one essential enabling feature of vw is a hashing trick ( described in  @xcite ) which allows us to represent @xmath176 linear regressors on a sparse feature space in a reasonable amount of ram .",
    "the reuters dataset consists of about @xmath177 documents , each assigned to one or more categories .",
    "a total of approximately 100 categories appear in the data .",
    "we split the data into a training set of @xmath178 documents and a test set of @xmath179 documents , opposite to its original use . for each document @xmath180 , we formed an example of the form @xmath43 , as follows",
    ". the vector @xmath73 uses a `` bag of words '' representation of @xmath180 , weighted by the normalized tf - idf scores , exactly as done in the paper @xcite .",
    "the label @xmath33 is one of the categories assigned to @xmath180 , chosen uniformly at random if more than one category was assigned to @xmath180 .",
    "we compared the cpt to the one - against - all algorithm , a standard approach for reducing multi - class regression to binary regression .",
    "the one - against - all approach regresses on the probability of each category @xmath181 versus all other categories . given a base training example @xmath43 , the example used to train the regressor @xmath182 for category @xmath181 is @xmath183)$ ] , where @xmath184 $ ] is the indicator function .",
    "predictions for a new test example @xmath43 are done according to @xmath185 .",
    "the learning algorithm used for training the binary regressors in both approaches was incremental gradient descent with squared loss . for each algorithm",
    ", we ran several versions with different learning rates , chosen from a coarse grid , and picked the setting that yielded the smallest training error . for the cpt algorithm , we performed a similar search over @xmath111 .",
    "the one - against - all approach used one pass over the training data , while the cpt used two passes . note that even with an additional pass , the cpt is much faster than one - against - all for training , due to the fact that cpt requires training only about @xmath186 regressors ( nodes in the tree ) per example , whereas one - against - all trains one regressor per category . on our machine ,",
    "the cpt took 108 seconds to train , while one - against - all took 2300 seconds .",
    "we use progressive validation  @xcite to compute an average squared loss over the test set with results appearing in the following table , where the confidence intervals are computed by hoeffding s inequality  @xcite with @xmath187 .       here , the `` equivalent '' column is the number of labels for which a uniform random process produces the same loss .",
    "the `` best possible '' line is an unachievable bound on performance found by examining the empirical frequency of ad - webpage pairs in the test set .",
    "the magnitude of squared loss improvement is modest , but substantial enough to be useful .",
    "since many of the webpages are seen many times , the conditional distribution over ads can be approximated well by empirical frequencies .",
    "thus , the table - based method forms a strong baseline .",
    "a small but significant fraction of the webpages were seen only a few times , and for these webpages , it was necesssary to generalize ( predict which ads would appear based on which ads appeared on pages similar to the current one ) . on these examples ,",
    "the tree performed substantially better .",
    "1 a. blum , a. kalai , and j. langford . beating the holdout : bounds for @xmath13-fold and progressive cross - validation , _ proceedings of the 12th annual conference on computational learning theory _ ( colt ) , 203208 , 1999 .",
    "q. shi , j. patterson , g. dror , j. langford , a. smola , a. strehl , and v. vishwanathan .",
    "hash kernels , _ proceedings of the 12th international conference on artificial intelligence and statistics _ ( aistats ) , 2009 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of estimating the conditional probability of a label in time @xmath0 , where @xmath1 is the number of possible labels . we analyze a natural reduction of this problem to a set of binary regression problems organized in a tree structure , proving a regret bound that scales with the depth of the tree . </S>",
    "<S> motivated by this analysis , we propose the first online algorithm which provably constructs a logarithmic depth tree on the set of labels to solve this problem . </S>",
    "<S> we test the algorithm empirically , showing that it works succesfully on a dataset with roughly @xmath2 labels . </S>"
  ]
}