{
  "article_text": [
    "learning in a single layer of feed forward neural network with binary synapses has been studied either based on statistical mechanics analysis  @xcite or in algorithmic aspects  @xcite .",
    "this network can learn an extensive number @xmath0 of random patterns , where @xmath1 is the number of synapses and @xmath2 denotes the constraint density .",
    "the critical @xmath2 ( also called the capacity ) separating the learnable phase from unlearnable phase is predicted to be @xmath3 where the entropy vanishes  @xcite .",
    "a solution is defined as a configuration of synaptic weights to implement the correct classification of @xmath4 random input patterns . above @xmath5",
    ", no solutions can be found with high probability ( converging to @xmath6 in the thermodynamic limit ) . the replica symmetric solution presented in ref .",
    "@xcite has been shown to be stable up to the capacity , which is in accordance with the convexity of the solution space  @xcite .",
    "note that the solutions disappear at the threshold @xmath5 still maintaining a typical finite value of hamming distance between them , which is quite distinct from the case in the continuous perceptron with real - valued synapses . in the continuous perceptron",
    ", this distance tends to zero when the solutions disappear at the corresponding threshold  @xcite . on the other hand , many local search algorithms  @xcite were proposed to find solutions of the perceptron learning problem , however , the search process slows down with increasing @xmath2 , and the critical @xmath2 for the local search algorithm  @xcite decreases when the number of synapses increases .",
    "this typical behavior of the stochastic local search algorithm is conjectured to be related to the geometrical organization of the solution space  @xcite . in order to acquire a better understanding for the failure of the local search strategy",
    ", we compute the entropy landscape both from a reference configuration and for solution - pairs with a given distance in the solution space .",
    "both distance landscapes contain rich information about the detailed structure of the solution space and then can help us understand the observed glassy behavior of the local search algorithms . throughout the paper",
    ", the term distance refers to the hamming distance .",
    "the distance landscape has been well studied in random constraint satisfaction problems defined on diluted or sparse random graphs  @xcite .",
    "learning in the binary perceptron can be mapped onto a bipartite graph where variable node represents synaptic weight and function node represents the input random pattern to be learned ( see figure  [ perc ] ( b ) ) .",
    "this graph is also called graphical model or factor graph  @xcite .",
    "the efficient message passing learning algorithm for the binary perceptronal learning problem has been derived using the cavity method and this factor graph representation  @xcite . in this paper",
    ", we focus on the typical property of the solution space in random ensembles of the binary perceptronal learning problem .",
    "we apply the replica trick widely used to study disordered systems  @xcite to compute the statistical properties in the thermodynamic limit . to confirm the mean field result computed using the replica approach , we derive the message passing equations in the cavity context which can be applied on single random instances of the current problem . in this context",
    ", we apply the decorrelation assumption as well as the central limit theorem to derive the formula at the replica symmetric level .",
    "this assumption arises from the weak correlation among synaptic weights ( within one pure state  @xcite ) and among input patterns  @xcite .",
    "the efficiency of the inspired message passing algorithms in loopy systems has been observed in refs .",
    "@xcite while the underlying mechanism still needs to be fully understood .",
    "however , our cavity method focuses on the physical content and yields the same result as that obtained using replica approach  @xcite .",
    "the remainder of this paper is organized as follows .",
    "the random classification by the binary perceptron is defined in sec .",
    "[ sec_bperc ] . in sec .",
    "[ sec_sdr ] , we derive the self - consistent equations to compute the distance landscape ( entropy landscape ) from a reference configuration , i.e. , to count the number of solutions at a distance from the reference configuration .",
    "both the annealed and replica symmetric ( rs ) computations of this entropy landscape are presented .",
    "we also derive the message passing equations for single instances in this section using the cavity method and factor graph representation . in sec .",
    "[ sec : sdp ] , the landscape of hamming distances between pairs of solutions is evaluated at both annealed approximation and rs ansatz , and the associated message passing equations are proposed as well .",
    "discussion and conclusion are given in sec .",
    "[ sec_con ] .",
    "input units ( open circles ) feed directly to a single output unit ( solid circle ) .",
    "a binary input pattern @xmath7 of length @xmath1 is mapped through a sign function to a binary output @xmath8 , i.e. , @xmath9 .",
    "the set of @xmath1 binary synaptic weights @xmath10 is regarded as a solution of the perceptron problem if the output @xmath11 for each of the @xmath12 input patterns @xmath13 $ ] , where @xmath14 is a preset binary value .",
    "( b ) each circle denotes the variable node whose value takes @xmath15 with @xmath16 its index .",
    "the square is the function node denoting a random binary pattern to be learned . if the pattern is learned by the synaptic vector @xmath17 , the value of the corresponding function node takes zero .",
    "the dotted line represents other @xmath18 function nodes while the dashed line for variable node @xmath16 means @xmath16 is connected to other @xmath18 function nodes and that for function node means the function node ( e.g. , @xmath19 ) is connected to other @xmath20 variable nodes .",
    ", title=\"fig : \" ] .5 cm   input units ( open circles ) feed directly to a single output unit ( solid circle ) .",
    "a binary input pattern @xmath7 of length @xmath1 is mapped through a sign function to a binary output @xmath8 , i.e. , @xmath9 .",
    "the set of @xmath1 binary synaptic weights @xmath10 is regarded as a solution of the perceptron problem if the output @xmath11 for each of the @xmath12 input patterns @xmath13 $ ] , where @xmath14 is a preset binary value .",
    "( b ) each circle denotes the variable node whose value takes @xmath15 with @xmath16 its index .",
    "the square is the function node denoting a random binary pattern to be learned .",
    "if the pattern is learned by the synaptic vector @xmath17 , the value of the corresponding function node takes zero .",
    "the dotted line represents other @xmath18 function nodes while the dashed line for variable node @xmath16 means @xmath16 is connected to other @xmath18 function nodes and that for function node means the function node ( e.g. , @xmath19 ) is connected to other @xmath20 variable nodes .",
    ", title=\"fig:\"].2 cm",
    "the binary perceptron realizes a random classification of @xmath4 random input patterns ( see figure  [ perc](a ) ) . to be more precise",
    ", the learning task is to find an optimal set of binary synaptic weights ( solution ) @xmath21 that could map correctly each of random input patterns @xmath22 to the desired output @xmath23 which is assigned a value @xmath24 at random .",
    "@xmath4 is proportional to @xmath1 with the coefficient @xmath2 defining the constraint density ( each input pattern serves as a constraint for all synaptic weights , see figure  [ perc ] ( b ) ) .",
    "the critical value is @xmath3 below which the solution space is non - empty  @xcite . given the input pattern @xmath25 , the actual output @xmath8 of the perceptron is @xmath26 where @xmath15 takes @xmath24 and @xmath27 takes @xmath24 with equal probabilities . if @xmath28 , we say that the synaptic weight vector @xmath17 has learned the @xmath29-th pattern .",
    "therefore we define the number of patterns mapped incorrectly as the energy cost @xmath30 where @xmath31 is a step function with the convention that @xmath32 if @xmath33 and @xmath34 otherwise . the prefactor @xmath35 is introduced to ensure that the argument of the step function remains at the order of unity , for the sake of the following statistical mechanical analysis in the thermodynamic limit . in the current setting , both @xmath36 and the desired output @xmath37 are generated randomly independently . without loss of generality ,",
    "we assume @xmath38 for any input pattern in the remaining part of this paper , since one can perform a gauge transformation @xmath39 to each input pattern without affecting the result .",
    "in this section , we consider the entropy landscape from a reference configuration ( which is not a solution ) . this entropy counts the number of solutions at a distance @xmath40 from the reference configuration @xmath41 .",
    "the behavior of this entropy landscape reflects the geometrical organization of the solution space .",
    "since we concentrate on the ground state ( @xmath42 ) , we take the inverse temperature @xmath43 and introduce a coupling field @xmath44 to control the distance between solutions and the reference configuration . the partition function for this setting",
    "is @xmath45\\ ] ] where the sum @xmath46 goes over all possible synaptic weight vectors and @xmath47 means the sum over all variable nodes . under the definition of the overlap @xmath48",
    ", the partition function can be written as @xmath49\\ ] ] where @xmath50 is the number of solutions with the overlap @xmath51 . in the thermodynamic limit @xmath52",
    ", the saddle point analysis leads to @xmath53 $ ] where @xmath54 is defined as the free energy density .",
    "therefore , we can determine the entropy @xmath55 by a legendre transform  @xcite @xmath56,\\\\ \\tilde{q}(x)&=&\\frac{df(x)}{dx}\\label{legendre02}\\end{aligned}\\ ] ] where @xmath51 is related to @xmath57 through @xmath58 and then the entropy density can be expressed as a function of the distance @xmath57 which can be understood as the probability that a synaptic weight takes different values in @xmath17 and @xmath41 .",
    "one recovers the total number of solutions by setting @xmath59 in eq .",
    "( [ ptf ] ) .",
    "we first calculate the annealed entropy density @xmath61 which serves as the upper bound ( jensen s inequality  @xcite ) for the true value of the entropy density .",
    "actually , the free energy @xmath62 should be averaged over the random input patterns .",
    "however , the annealed approximation alternatively performs the average of the partition function first and then takes the logarithmic operation as @xmath63 where the average is taken over the distribution of the random input patterns .",
    "this can be computed as @xmath64\\right > \\nonumber\\\\ & = & \\int d\\tilde{q}\\int\\frac{d\\hat{q}}{2\\pi \\mathrm{i}/n}\\exp\\left[n\\left(-\\hat{q}\\tilde{q}+x\\tilde{q}-\\alpha\\log2+\\log(2\\cosh\\hat{q})\\right)\\right]\\end{aligned}\\ ] ] where the integral representation of @xmath65 is used and the conjugated counterpart @xmath66 of the overlap @xmath51 is introduced as a dirac delta function @xmath67 is inserted  @xcite .",
    "a saddle point analysis results in @xmath68 where the saddle point equation reads @xmath69 . using eq .",
    "( [ legendre01 ] ) and the saddle point equation , we get the annealed entropy density @xmath70      the free energy density @xmath54 is a self - averaging quantity whose value concentrates in probability around its expectation in the thermodynamic limit  @xcite , and its average over the random input patterns is very difficult to compute because the logarithm appears inside the average .",
    "the replica trick bypasses this difficulty by using the identity @xmath71 .",
    "then the disorder averaged free energy density can be computed by first averaging an integer power of the partition function and then letting @xmath72 as @xmath73 although the replica method is not generally rigorous , the obtained theoretical result can be checked by numerical simulations .",
    "to compute @xmath74 , we introduce @xmath75 replicated synaptic weight vectors @xmath76(@xmath77 ) as follows .",
    "@xmath78\\right>\\nonumber\\\\ \\fl=\\int\\prod_{a < b}\\frac{dq^{ab}d\\hat{q}^{ab}}{2\\pi \\mathrm{i}/n}\\exp\\left[-n\\sum_{a < b}q^{ab}\\hat{q}^{ab}+n\\alpha\\log g_{0}(\\{q^{ab}\\})+ng_{1}(\\{\\hat{q}^{ab}\\})\\right],\\end{aligned}\\ ] ] where @xmath79 and @xmath80 are expressed respectively as @xmath81e^{\\mathrm{i}\\sum_{a}\\lambda^{a}t^{a}-\\sum_{a < b}\\lambda^{a}\\lambda^{b}q^{ab}-\\frac{1}{2}\\sum_{a}(\\lambda^{a})^{2}},\\\\ g_{1}(\\{\\hat{q}^{ab}\\})&=&\\log\\sum_{j^{a}:a=1,\\ldots , n}e^{\\sum_{a < b}\\hat{q}^{ab}j^{a}j^{b}+x\\sum_{a}j^{a}j^{*}},\\end{aligned}\\ ] ] where we have introduced the replica overlap @xmath82 and its associated conjugated counterpart @xmath83 .",
    "the replica symmetric ansatz assumes @xmath84 for @xmath85 . now using the saddle point analysis",
    ", we finally arrive at the formula of the free energy density and the corresponding saddle point equations , @xmath86,\\\\ q&=&\\int dz\\tanh^{2}(\\sqrt{\\hat{q}}z+x),\\label{repf03a}\\\\ \\hat{q}&=&\\frac{\\alpha}{1-q}\\int dz\\left[g\\left(\\sqrt{\\frac{q}{1-q}}z\\right)/h\\left(\\sqrt{\\frac{q}{1-q}}z\\right)\\right]^{2},\\label{repf03b}\\end{aligned}\\ ] ] where @xmath87 and @xmath88 with the gaussian measure @xmath89 .",
    "after the fixed point of the self - consistent equations  ( [ repf03a ] ) and  ( [ repf03b ] ) is obtained , the entropy landscape @xmath60 is computed as @xmath90 note that the final expression of @xmath91 does not depend on the reference configuration and the integral in the second term of eq .",
    "( [ sdrep ] ) is @xmath92 defined in eq .",
    "( [ legendre02 ] ) .      in this section ,",
    "we derive the message passing equations to compute the entropy landscape for single instances under the replica symmetric ansatz . to derive the self - consistent equation",
    ", we apply the cavity method  @xcite and first define two kinds of cavity probabilities .",
    "one is the probability @xmath93 that variable node @xmath16 in figure  [ perc ] ( b ) takes value @xmath15 in the absence of constraint @xmath94 .",
    "the other is @xmath95 staying for the probability that constraint @xmath19 is satisfied ( pattern @xmath96 is learned ) if synaptic weight @xmath16 takes @xmath15 .",
    "according to the above definitions , the self - consistent equation for these two kinds of probabilities is readily obtained as @xmath97 where @xmath98 is a normalization constant , @xmath99 denotes the neighbors of node @xmath16 except constraint @xmath94 and @xmath100 denotes the neighbors of constraint @xmath19 except variable node @xmath16 .",
    "( [ bp0 ] ) and  ( [ bp1 ] ) are actually the belief propagation equations  @xcite . for the binary perceptron , directly solving the belief propagation equations is impossible . to reduce the computational complexity , we define @xmath101 .",
    "note that the sum involves @xmath102 independent random terms , as a result , the central limit theorem implies that @xmath103 follows a gaussian distribution with mean @xmath104 and variance @xmath105 where @xmath106 and @xmath107 . within the rs ansatz , the clustering property @xmath108 for @xmath109 in the thermodynamic limit",
    "is used to get the variance  @xcite .",
    "@xmath110 is the magnetization in statistical physics language . by separating the term @xmath111 from the sum in the @xmath65 of eq .",
    "( [ bp1 ] ) , and approximating the sum @xmath112 by an integral over @xmath103 , we get finally @xmath113 where @xmath114 and @xmath115 in which the cavity magnetization @xmath116 . using eqs .",
    "( [ bp0 ] ) and  ( [ phat ] ) , the cavity field @xmath117 can be obtained in the log - likelihood representation @xmath118.\\label{cavh02}\\end{aligned}\\ ] ] notice that the cavity bias @xmath119 can be approximated by @xmath120 in the large @xmath1 limit .",
    "( [ cavh01 ] ) and  ( [ cavh02 ] ) constitute the recursive equations to compute the free energy density in the bethe approximation  @xcite @xmath121,\\\\ \\delta f_{a}&=&\\log h\\left(-\\frac{\\hat{w}_{a}}{\\sqrt{\\hat{\\sigma}_{a}}}\\right),\\end{aligned}\\ ] ] where @xmath122 and @xmath123 are the free energy shifts due to variable node ( @xmath16 ) addition ( and all its function nodes ) and function node ( @xmath94 ) addition  @xcite respectively .",
    "actually @xmath124 is the normalization constant of the full probability @xmath125 and @xmath126 the normalization constant of @xmath127  @xcite . @xmath128 and @xmath129 .",
    "equations  ( [ cavh01 ] ) and  ( [ cavh02 ] ) can be solved by an iterative procedure with a random initialization of the corresponding messages .",
    "after the iteration converges , the entropy landscape @xmath60 from the fixed reference configuration @xmath41 can be computed according to the legendre transform eqs .",
    "( [ legendre01 ] ) and  ( [ legendre02 ] ) .",
    "the computational complexity is of the order @xmath130 for this densely connected graphical model .",
    "@xmath54 computed based on eq .",
    "( [ frs01 ] ) does not depend on the reference configuration since the change of @xmath131 does not affect the final result , consistent with eq .",
    "( [ repf03 ] ) .    ) ) for @xmath132 ( from the top to the bottom ) respectively .",
    "the horizontal dashed line indicates the zero entropy value . the line connecting symbols is a guide to the eye .",
    "the empty symbols stay for the numerical simulation results on systems with @xmath133 ( from the top to the bottom ) using message passing algorithms .",
    "the result is the average over @xmath134 random instances .",
    "solid symbols are the replica symmetric results computed numerically by solving the saddle point equations .",
    ", scaledwidth=80.0% ]    the distance landscape from a reference configuration is reported in figure  [ sdr ] .",
    "we choose the reference configuration @xmath135 for simplicity .",
    "other choices of the reference configuration still yield the same behavior of the landscape .",
    "note that the annealed entropy provides an upper bound for the rs one , and it roughly coincides with the rs one at low @xmath2 ( around the maximal point ) while the large deviation is observed when @xmath2 further increases .",
    "it is clear that most of the solutions concentrate around the dominant point where the maximum of the entropy is reached .",
    "when the given distance is larger or less than certain values ( @xmath136 or @xmath137 ) , the number of solutions at those distances becomes exponentially small in @xmath1 . in the intermediate range ( @xmath138 $ ] ) , as the distance increases , the number of solutions separated by the distance from the reference point in the solution space increases first and then reaches the maximum which dominates the typical value of the entropy in the original systems ( by setting @xmath59 , see figure  [ sd ] ) .",
    "the maximum is then followed by a decreasing trend as the distance is further increased . this mean field behavior is confirmed by the numerical simulations on large - size single random instances using the message passing algorithms derived in sec .  [ subsec : mpa01 ] .",
    "the consistency between the mean field result obtained by replica approach and the simulation result obtained on single random instances is clearly shown in figure  [ sdr ] .",
    "the bell shape in figure  [ sdr ] is similar to that observed in calculating the growth rate of expected distance enumerator for a random code ensemble  @xcite .",
    "note that as the constraint density increases , the distance range where solutions exist shrinks , which illustrates clearly how the solution space changes as more patterns are presented to the binary perceptron .",
    "we also compute the typical value of the entropy in the original system ( by setting @xmath59 ) and of the distance between any two solutions as a function of the constraint density using the replica method .",
    "the result is reported in figure  [ sd ] . here",
    "we define the typical value of the distance between any two solutions as @xmath139 where @xmath140 is obtained from the stationary value of eq .",
    "( [ repf03a ] ) .",
    "the entropy vanishes at @xmath3 with a finite typical value of distance  @xcite .",
    "this typical distance is also in accordance with that computed on single instances by sampling a finite number of solutions  @xcite .",
    "note that this distance is evaluated here based on the rs ansatz .",
    "one can further check its stability by the population dynamics  @xcite on the one - step replica symmetry breaking ( @xmath6rsb ) solution  @xcite where we define two typical distances : one is inter - cluster distance @xmath141}{2}$ ] where @xmath142 means the average over clusters and @xmath143 $ ] over the disorder , and the other is intra - cluster distance defined by @xmath144}{2}$ ]  @xcite where @xmath145 is the local field defined in eq .",
    "( [ cavh01 ] ) by including all contributions of patterns around the synaptic weight ( @xmath59 ) . in general , solutions within a single cluster",
    "are separated by a sub - extensive number of synaptic weights while any two clusters are separated by an extensive number of synaptic weights .",
    "our numerical simulations confirmed that @xmath146 and @xmath147 will turn out to be identical ( equal to @xmath148 ) after sufficient iterations implying that the rs ansatz is unbroken below the capacity .",
    "however , for constraint density close to the capacity , one needs a much larger sampling interval ( by way of the metropolis importance sampling method )  @xcite in the population dynamics algorithm . to probe the fine structure of the connection pattern in the solution space",
    ", we study the distance landscape of solution - pairs in the following section .",
    "this is similar to the study of the spherical @xmath149-spin model in the presence of an attractive coupling with quenched configuration in ref .",
    "@xcite , however , the rich information about the solution space structure of the current problem can also be attained by calculating the distance landscape of solution - pairs .",
    "the geometrical property of the solution space can also be studied by counting the number of solution - pairs with a predefined distance @xmath57 , equivalently an overlap of value @xmath51 .",
    "actually this entropy value may be much larger than the entropy density of the original problem ( which is obtained by setting @xmath59 in eq .",
    "( [ frs01 ] ) ) .",
    "as we shall present later , this case becomes more involved for the binary perceptron with an increasing computational cost .",
    "considering distance between solutions , we write the partition function as @xmath150\\ ] ] where the coupling field @xmath44 is used to control the distance between a pair of solutions @xmath151 and the associated overlap @xmath152 .",
    "this partition function has been used to predict optimal coupling field for a multiple random walking strategy to find a solution for the perceptronal learning problem  @xcite . in the following sections , we present an annealed computation as well as rs computation of the distance landscape @xmath60 . note that in this setting , eqs .",
    "( [ ptf02 ] ) ,  ( [ legendre01 ] ) and  ( [ legendre02 ] ) can also be used but here @xmath57 should be understood as the distance separating two solutions in the weight space .      following the same techniques used in sec .",
    "[ subsec : ann01 ] , we obtain the annealed free energy density as ( see also ref .",
    "@xcite ) @xmath153 the maximization with respect to @xmath51 and @xmath66 leads to the following saddle point equation @xmath154 where the identity @xmath155 has been used . using eq .",
    "( [ legendre01 ] ) and the above saddle point equation , we get the final expression for @xmath156 : @xmath157,\\label{anns02}\\end{aligned}\\ ] ] where @xmath158 which is actually the annealed entropy density of the original system  @xcite . if @xmath159 , then @xmath160 which is in accord with the fact that the number of solution - pairs with a distance @xmath161 should be the total number of solutions @xmath162 if no constraints are present .",
    "in this section , we derive the free energy density @xmath54 for the landscape of solution - pairs under the replica symmetric approximation , using the replica trick introduced in sec .",
    "[ subsec : rs01 ] .",
    "since the partition function in this case involves a sum of all possible configurations of two synaptic weight vectors , the computation becomes a bit complicated .",
    "the disorder average of the integer power of the partition function can be evaluated as @xmath163 where @xmath164 and @xmath165 . under the replica symmetric ansatz ,",
    "the disorder average is carried out as @xmath166^{n},\\ ] ] where @xmath167 , @xmath168 and we have used @xmath169 under the rs ansatz . after the computation of the summation in eq .",
    "( [ repair01 ] ) by using @xmath170 and the hubbard - stratonovich transform , we get the replica symmetric free energy density @xmath171 where @xmath172 , @xmath173 and @xmath174 where @xmath175 .",
    "the rs order parameters @xmath176 are determined by the following self - consistent equations @xmath177^{2}},\\\\ \\fl\\hat{q}&=\\frac{\\hat{r}}{2}+\\frac{\\alpha}{2(1-q - r+r)}\\int d\\boldsymbol{z}\\left[\\frac{\\int dy\\left[g(y_{1})h(y_{2})- g(y_{2})h(y_{1})\\right]}{\\int dy h(y_{1})h(y_{2})}\\right]^{2},\\label{pairb}\\\\   \\fl r&=\\int d\\boldsymbol{\\hat{z}}\\frac{\\tanh a_{3}+\\tanh a_{1}\\tanh a_{2}}{1+\\tanh a_{3}\\tanh a_{1}\\tanh a_{2}},\\label{pairr}\\\\   \\fl r&=\\int d\\boldsymbol{\\hat{z}}\\frac{\\tanh a_{3}(\\tanh^{2}a_{1}+\\tanh^{2}a_{2})+\\tanh a_{1}\\tanh a_{2}(1+\\tanh^{2}a_{3})}{(1+\\tanh a_{3}\\tanh a_{1}\\tanh a_{2})^{2}},\\\\ \\fl q&=r+\\int d\\boldsymbol{\\hat{z}}\\frac{(\\tanh a_{3}-1)^{2}(\\tanh a_{1}-\\tanh a_{2})^{2}}{2(1+\\tanh a_{3}\\tanh a_{1}\\tanh a_{2})^{2}}.\\end{aligned}\\ ] ] in the derivation of the above saddle point equations , we have used a useful property of the gaussian measure @xmath178 where @xmath179 is the derivative of the function @xmath180 with respect to @xmath181 .",
    "after the fixed point of the above saddle point equations is obtained , one can compute the entropy density @xmath182 with @xmath183 .",
    "note that @xmath184 may become negative , in this case we replace @xmath185 and @xmath186 by @xmath187 and @xmath188 respectively , @xmath189 by @xmath190 only for @xmath191 in eqs .",
    "( [ paira ] ) to  ( [ pairb ] ) .      by analogy with definitions in sec .",
    "[ subsec : mpa01 ] , we define by @xmath192 the probability that the synaptic weight @xmath16 takes a two - component vector state @xmath193 in the absence of constraint @xmath94 and by @xmath194 the probability that constraint @xmath19 is satisfied given the vector state @xmath193 of weight @xmath16 .",
    "these two cavity probabilities obey the following recursive equations @xmath195 where @xmath98 is a normalization constant .",
    "in fact the belief propagation equations  ( [ bp2 ] ) and  ( [ bp3 ] ) correspond to the stationary point of the bethe free energy function of the current system  @xcite .",
    "the exchange of @xmath196 and @xmath197 does not change the partition function eq .",
    "( [ ptf03 ] ) , thus the cavity probabilities have the property that @xmath198 and @xmath199 .",
    "this symmetry property will simplify the following analysis a lot .    to simplify eqs .",
    "( [ bp2 ] ) and  ( [ bp3 ] ) , we need the joint distribution of @xmath200 and @xmath201 where @xmath202 and @xmath203 . since we impose a distance constraint upon two solutions @xmath196 and @xmath197 in eq .",
    "( [ ptf03 ] ) , there exists correlation between these two normally distributed random numbers and this correlation is characterized by the correlation coefficient @xmath204 due to the symmetry property . based on eq .",
    "( [ bp2 ] ) , messages @xmath205 and @xmath206 are determined respectively by the following equations , @xmath207 - 2e^{-x}\\prod_{a\\in\\partial j\\backslash b}\\hat{p}_{a\\rightarrow j}^{+1,-1}}{e^{x}\\left[\\prod_{a\\in\\partial j\\backslash b}\\hat{p}_{a\\rightarrow j}^{+1,+1}+\\prod_{a\\in\\partial j\\backslash b}\\hat{p}_{a\\rightarrow j}^{-1,-1}\\right]+2e^{-x}\\prod_{a\\in\\partial j\\backslash b}\\hat{p}_{a\\rightarrow j}^{+1,-1}},\\label{q}\\\\   m_{j\\rightarrow b}&=&\\frac{e^{x}\\left[\\prod_{a\\in\\partial j\\backslash b}\\hat{p}_{a\\rightarrow j}^{+1,+1}-\\prod_{a\\in\\partial j\\backslash b}\\hat{p}_{a\\rightarrow j}^{-1,-1}\\right]}{e^{x}\\left[\\prod_{a\\in\\partial j\\backslash b}\\hat{p}_{a\\rightarrow j}^{+1,+1}+\\prod_{a\\in\\partial j\\backslash b}\\hat{p}_{a\\rightarrow j}^{-1,-1}\\right]+2e^{-x}\\prod_{a\\in\\partial j\\backslash b}\\hat{p}_{a\\rightarrow j}^{+1,-1}}.\\label{cavh03}\\end{aligned}\\ ] ] therefore , both @xmath200 and @xmath201 obey a bivariate normal distribution and @xmath194 is reduced to be @xmath208 where @xmath114 and @xmath115 .",
    "the overlap @xmath51 is determined by @xmath209 where @xmath210 is given by @xmath211 - 2e^{-x}\\prod_{b\\in\\partial i}\\hat{p}_{b\\rightarrow i}^{+1,-1}}{e^{x}\\left[\\prod_{b\\in\\partial i}\\hat{p}_{b\\rightarrow i}^{+1,+1}+\\prod_{b\\in\\partial i}\\hat{p}_{b\\rightarrow i}^{-1,-1}\\right]+2e^{-x}\\prod_{b\\in\\partial i}\\hat{p}_{b\\rightarrow i}^{+1,-1}}.\\ ] ] eq .",
    "( [ phat02 ] ) is more computationally demanding than eq .",
    "( [ phat ] ) since an additional numerical integral is required to compute @xmath212 here .",
    "however , the integral in eq .",
    "( [ phat02 ] ) can be approximated by @xmath213 if we write the right hand side of eq .",
    "( [ phat02 ] ) as @xmath214 and expand @xmath215 up to the second order in @xmath216 .",
    "the constants @xmath217 , @xmath218 and @xmath219 can be determined as a function of @xmath94 and @xmath19 .",
    "therefore , this approximation is accurate only when large @xmath216 has vanishing contribution to the integral",
    ".    the free energy shift due to variable node addition ( and all its adjacent constraints ) can be obtained as @xmath122 and the free energy shift due to constraint addition @xmath220 where @xmath221 + 2e^{-x}\\prod_{b\\in\\partial i}\\hat{p}_{b\\rightarrow i}^{+1,-1},\\\\ z_{a}&=&\\int_{-\\frac{\\hat{w}_{a}}{\\sqrt{\\hat{\\sigma}_{a}}}}^{\\infty}dth\\left(-\\frac{\\hat{w}_{a}}{\\sqrt{(1-\\hat{\\rho}_{a}^{2})\\hat{\\sigma}_{a}}}-\\frac{\\hat{\\rho}_{a}t}{\\sqrt{1-\\hat{\\rho}_{a}^{2}}}\\right),\\end{aligned}\\ ] ] where @xmath222 , @xmath223 and @xmath224 .",
    "the free energy density can then be obtained using eq .",
    "( [ frs01 ] ) and the entropy landscape can be obtained correspondingly .",
    "the recursive equations eqs .",
    "( [ rho ] ) ,  ( [ q ] ) ,  ( [ cavh03 ] ) and  ( [ phat02 ] ) can be solved by an iterative procedure similar to that used in sec .",
    "[ subsec : mpa01 ] .    .",
    "the solid lines are the analytic annealed approximation ( eq .",
    "( [ anns02 ] ) ) for @xmath132 ( from the top to the bottom ) respectively .",
    "the line connecting symbols is a guide to the eye .",
    "the empty symbols stay for the numerical simulation results on systems with @xmath225 ( from the top to the bottom ) using message passing algorithms .",
    "the result is the average over @xmath134 random instances .",
    "solid symbols are the replica symmetric results computed numerically by solving the saddle point equations . , scaledwidth=80.0% ]     as a function of the constraint density .",
    "the data points are computed by solving numerically the saddle point equations in sec .",
    "[ subsec : rs02 ] .",
    "the error bars characterize the numerical fluctuations from ten different random initializations .",
    "the upper inset shows the corresponding entropy values .",
    "the lower inset shows _ schematically _ the typical concave and non - concave behavior of the entropy landscape , where the horizontal dashed line indicates @xmath226 and the vertical dashed line denotes @xmath227 , and the black point @xmath228 marks the first change of the concavity , i.e. , @xmath229 .",
    "the vertical dotted line marks the first order thermodynamic transition point @xmath230 , where the dash - dotted line going through @xmath231 touches the concave part of @xmath60 and has the slope @xmath232 .",
    "note that @xmath227 corresponds to the spinodal point @xmath233 ( @xmath234 ) , i.e. , @xmath235 .",
    ", scaledwidth=80.0% ]     for fixed @xmath57 .",
    "the data points are computed by solving numerically the saddle point equations in sec .",
    "[ subsec : rs02 ] .",
    "the error bars characterize the numerical fluctuations from ten different random initializations .",
    "inset : the corresponding entropy curve as a function of @xmath57 .",
    "( a ) @xmath236 .",
    "( b ) @xmath237 . , title=\"fig:\",scaledwidth=80.0% ] .05 cm   for fixed @xmath57 .",
    "the data points are computed by solving numerically the saddle point equations in sec .",
    "[ subsec : rs02 ] .",
    "the error bars characterize the numerical fluctuations from ten different random initializations .",
    "inset : the corresponding entropy curve as a function of @xmath57 .",
    "( a ) @xmath236 .",
    "( b ) @xmath237 .",
    ", title=\"fig:\",scaledwidth=80.0%].05 cm    as is seen from figure  [ sdp ] , the entropy density increases smoothly until a maximum is reached for @xmath238 and then decreases as the distance further grows .",
    "interestingly , this behavior observed in figure  [ sdp ] can be well fitted by the annealed approximation keeping the concavity of the entropy function .",
    "however , as @xmath2 increases , large deviation from the annealed approximation occurs .",
    "the mean field calculations are supported by the numerical simulations on single instances using the proposed message passing algorithms , as shown in figure  [ sdp ] .",
    "the distance corresponding to the maximum of the entropy landscape curve in figure  [ sdp ] is actually the typical distance @xmath148 calculated in figure  [ sd ] , and @xmath239 recovers the typical entropy density of the original problem . by taking the limit @xmath240 in eq .",
    "( [ repair02 ] ) , one can show that @xmath241 where @xmath54 is given by eq .",
    "( [ repf03 ] ) . as the constraint density increases ,",
    "the maximal point of the entropy curve moves to the left , however , solution - pairs still maintain a relatively broad distribution in the solution space when @xmath2 approaches @xmath5 , e.g. , @xmath242 at @xmath243 ( @xmath244 ) , which may be responsible for the algorithmic hardness in this region .",
    "as @xmath2 increases , the message passing algorithm requires a large number of iteration steps to converge ( especially at small distances ) and additionally a computationally expensive monte carlo integral involved in eq .",
    "( [ phat02 ] ) can not be avoided . on the other hand ,",
    "when @xmath2 is large enough , one can easily observe a rapid growth of the order parameter @xmath185 to unity , i.e. , at some critical coupling field @xmath245 , @xmath185 changes sharply from a value smaller than one to one .",
    "this implies that at @xmath245 , @xmath246 becomes a globally stable solution of the saddle point equations in sec .",
    "[ subsec : rs02 ] .",
    "the first order thermodynamic transition is signalled by the change of the concavity at @xmath247 .",
    "we define @xmath227 as the minimal distance before @xmath246 becomes a unique stable solution .",
    "figure  [ gap ] shows the entropy gap and @xmath227 as a function of the constraint density . the corresponding coupling field",
    "@xmath233 marks the point where the concavity starts to change , i.e. , @xmath248 , as shown in the lower inset of figure  [ gap ] ( @xmath249 ) .",
    "after @xmath233 , @xmath246 becomes the unique stable solution of the saddle point equations .",
    "note that in the entropy gap , there exists a non - concave part of the entropy curve ( for small distances ) , which can only be obtained by fixing @xmath57 instead of @xmath44 and searching for a compatible @xmath44 ( by the secant method ) .",
    "the result is shown in figure  [ nonconcav ] for @xmath236 and @xmath250 .",
    "the compatible @xmath44 for small distances ( the left branch ) is smaller than @xmath233 .",
    "when @xmath251 , the right branch is no longer globally stable solution but becomes metastable solution of the saddle point equations until @xmath252 , i.e. , the spinodal point is reached . by fixing @xmath253",
    ", one typically observes the right branch or @xmath246 , which describes the equilibrium properties of the boltzmann measure in eq .",
    "( [ ptf03 ] ) .",
    "thus , the non - concave behavior observed in @xmath254 is thermodynamically non - dominant and unstable , suggesting that the solution space is made of isolated solutions instead of separated clusters of exponentially many close - by solutions , and this behavior becomes much more evident as @xmath2 increases .",
    "this explains why the multiple random walking strategy is extremely difficult to find a solution by tuning the coupling field at high @xmath2 and large @xmath1  @xcite .",
    "we argue that for any finite @xmath2 , the slope of the entropy curve @xmath60 near to @xmath161 ( @xmath246 ) tends to negative infinity .",
    "letting @xmath255 , we can obtain @xmath256 , where @xmath257 is a positive constant independent of @xmath2 and @xmath258 , and @xmath259 is a constant as well .",
    "the derivation is given in  [ sec : appendix - a ] .",
    "thus , as long as @xmath260 , the non - concave part exists in the entropy curve for the interval @xmath261 , implying that such solution - pairs are exponentially less numerous than the typical ones .",
    "furthermore , @xmath246 is always a stable solution , and in this case @xmath262 .    as shown in figure  [ gap]@xmath263 increases as @xmath2 grows , making a uniform sampling of solutions extremely hard .",
    "in addition , @xmath227 seems to grow continuously , being the order of @xmath264 or less for @xmath265 .",
    "the isolations of solutions can be explained by the nature of the hard constraints  @xcite for the binary perceptron .",
    "unlike the random @xmath266-sat and graph - coloring problems  @xcite , the hard constraint in the binary perceptron problem implies that the synaptic weight on one node in the factor graph is completely determined by the values of other nodes .",
    "but for finite @xmath1 , solutions may not be strictly isolated .",
    "this explains why some local search heuristics can find a solution when @xmath1 and @xmath2 is not large enough .",
    "as @xmath2 increases , some frozen solutions are more prone to disappear , thus solutions become much more far apart from each other , as shown by increasing @xmath227 in figure  [ gap ] .",
    "however , the thermodynamic properties can still be derived from the rs solution before @xmath5 .",
    "we conjecture that clustering and freezing coexist for @xmath267 , which is consistent with the computation in ref .",
    "@xcite in the sense that the total entropy ( displayed in figure  [ sd ] ) @xmath268 where @xmath269 is the complexity of clusters of entropy density @xmath270 and @xmath271 for the current problem .",
    "the structure of the solution space is described by the dynamical one - step replica symmetry breaking scenario ( at the parisi parameter @xmath272 )  @xcite . by contrast , in the random @xmath266-xorsat , there exists a phase where going from a solution to another one involves a large but finite hamming distance  @xcite and in locked constraint satisfaction problem , there exists logarithmic hamming distance separation between solutions in the liquid phase  @xcite . for the binary perceptron problem",
    ", we can say that the solution space is simple in the sense that it is made of isolated solutions instead of separated clusters of exponentially many solutions ; however , it becomes rather difficult to find a solution via stochastic local search algorithm .",
    "below @xmath5 , @xmath273 , meaning that there exist exponentially many solutions , but they are widely dispersed ( much more apparent at large @xmath2 ) .",
    "in other words , solution - pairs maintain a relatively broad distribution .",
    "the typical property of the distance landscape either from a reference configuration or for pairs of solutions is studied . for the first distance landscape , as the distance increases ,",
    "the number of associated solutions grows first and then reaches its maximum ( dominating the typical value of the entropy in the original system ) followed by a decreasing behavior .",
    "this typical trend is confirmed by the numerical simulations on single instances using the proposed message passing algorithms .",
    "this behavior suggests that most of the solutions concentrate around the dominant point ( the maximum in the distance landscape ) in the @xmath1-dimensional weight space .",
    "it is clear that as the constraint density increases , the distance landscape shows larger and larger deviation from the analytic annealed approximation .",
    "we also calculate the second distance landscape characterizing the number of solution - pairs separated by a given distance . in this case",
    ", the replica symmetric result is in good agreement with the annealed computation at low @xmath2 , while the large deviation is observed between the replica symmetric approximation and annealed computation for high @xmath2 .",
    "both landscapes are evaluated in the thermodynamic limit and confirmed by message passing simulations on large - size single instances .    in this paper",
    ", we calculate the whole picture of the distance ( entropy ) landscape and show that the entropy value rises to a maximum before declining at higher values of distance at certain range of distances . from the first landscape ( a random configuration as a reference ) , we see clearly how the solution space shrinks as more constraints are added . from the second landscape of solution - pairs , we deduce a picture in which each global minimum ( referred to as a canyon ) is occupied by a single solution with zero ground state energy , and is surrounded by local minima ( referred to as valleys ) with positive energy  @xcite .",
    "this is also known as the valleys - dominated energy landscape  @xcite .",
    "the isolation of solutions implies that one can not expect to satisfy all constraints by flipping a few synapses .",
    "the necessary number of synapses to be flipped should be proportional to @xmath1 .",
    "the distance between the isolated solutions increases as the constraint density grows .",
    "this is the very reason why some simple local search heuristics can not find a solution at high @xmath2 or large @xmath1  @xcite and the critical @xmath2 for the local search algorithm decreases when the number of synapses increases .",
    "simulated annealing process used in refs .",
    "@xcite suffers from a critical slowing down when approaching a certain temperature , therefore , it would be interesting to study this picture within a finite temperature framework by focusing on the metastable states around the isolated solutions .",
    "the structure of these states should also be responsible for the algorithmic hardness .",
    "this issue will be addressed in our future work .",
    "the distance landscape evaluated here is very similar to the weight enumerator in coding theory  @xcite and the method can be extended to consider the landscape analysis for low - density parity - check codes or code - division multiple access multiuser detection problems  @xcite , which will help to clarify what role the distance landscape plays with respect to the decoding performance .",
    "we are grateful to haijun zhou for helpful comments on earlier versions of this paper .",
    "this work was partially supported by the research council of hong kong ( grant nos .",
    "hkust @xmath274 , @xmath275)(mw , hh ) and the jsps fellowship for foreign researchers grant no .",
    "@xmath276 ( hh ) and jsps / mext kakenhi nos .",
    "@xmath277 , @xmath278 , and @xmath279 ( yk ) .",
    "in this section , we give a derivation of @xmath282 in the limit of @xmath281 . by noting that @xmath283 , one can write the derivative as @xmath284 where @xmath285 and @xmath286 is determined by eq .",
    "( [ pairr ] ) .",
    "we know that @xmath281 implies that @xmath287 , and let @xmath288 in eq .",
    "( [ pairr ] ) , we can then obtain @xmath289 to derive eq .",
    "( [ derr01 ] ) , we have used @xmath290 @xmath291 . when @xmath281 , @xmath292 and @xmath293 , thus the first term in eq .",
    "( [ derr ] ) can be reexpressed in the limit @xmath281 as @xmath294\\times      \\epsilon^{-1/2}.\\ ] ] therefore , @xmath295 where the constants can be obtained from the above equations .",
    "note that @xmath257 is positive , and the first term in the derivative dominates the divergent behavior when @xmath296 .",
    "following the same line , one can also prove that @xmath297 reduces to @xmath298 in the limit of @xmath240 ."
  ],
  "abstract_text": [
    "<S> the statistical picture of the solution space for a binary perceptron is studied . </S>",
    "<S> the binary perceptron learns a random classification of input random patterns by a set of binary synaptic weights . </S>",
    "<S> the learning of this network is difficult especially when the pattern ( constraint ) density is close to the capacity , which is supposed to be intimately related to the structure of the solution space . </S>",
    "<S> the geometrical organization is elucidated by the entropy landscape from a reference configuration and of solution - pairs separated by a given hamming distance in the solution space . </S>",
    "<S> we evaluate the entropy at the annealed level as well as replica symmetric level and the mean field result is confirmed by the numerical simulations on single instances using the proposed message passing algorithms . from the first landscape ( a random configuration as a reference ) , we see clearly how the solution space shrinks as more constraints are added . from the second landscape of solution - pairs , we deduce the coexistence of clustering and freezing in the solution space . </S>"
  ]
}