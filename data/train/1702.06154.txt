{
  "article_text": [
    "over the years , the development of storage capacities has allowed to collect huge amounts of data ( food - webs , human interactions , word classification ... ) .",
    "this data can be represented as network structures with agents stored as nodes and their mutual information as edges .",
    "graph theory then allows us to understand and analyse those large networks and retrieve the graph structure .",
    "an important structure in a graph of communication is its different roles .",
    "a role is a group of nodes sharing similar behaviour or flow patterns within the network . to derive the role model",
    "we will use pairwise similarity measures based on the similarity matrix @xmath0 .",
    "the measure we will use principally is a low rank iterative scheme proposed by browet in @xcite and @xcite .",
    "it computes the matrix @xmath1 of a low rank factorisation @xmath2 . we will then introduce a non - iterative and faster measure based on the one proposed by browet .",
    "when this is done , one needs to apply a community detection algorithm on the matrix @xmath0 to find the partition into the different subsets and detect the roles afterwards .",
    "however , @xmath0 is dense and contains @xmath3 non - zero elements . to efficiently implement the community detection , which is crucial on large databases",
    ", we will implicitly work on the lowest rank factor @xmath1 of size @xmath4 elements . for that purpose",
    ", we will use k - means algorithm on @xmath1 in the case the number of clusters k is given . to improve the classification ,",
    "we will use properties of the adjacency matrix to decide whether restarting k - means algorithm is necessary and if the number of chosen clusters is adequate . if @xmath5 has not previously been given , we make the assumption to have the knowledge of an upper bound @xmath6 of @xmath5 .",
    "several methods based on the hierarchical classification and singular value projection will then be implemented in order to find the correct number @xmath5 .",
    "finally , we will illustrate our results on erdos - renyi random graphs and on real data structures .",
    "we consider a directed graph @xmath7 with @xmath8 the set of vertices and @xmath9 the set of edges .",
    "a directed graph is such that an edge @xmath10 has a source @xmath11 and a destination @xmath12 .",
    "the adjacency matrix @xmath13 of the graph is defined as @xmath14 the role extraction problem or bloc modelling consist of finding a permutation matrix @xmath15 such that the edges of the relabelled graph , associated with the adjacency matrix @xmath16 , are mainly concentrated within blocks as shown in figure [ fig_perm ] .",
    "the role extraction problem is based on the assumption that nodes can be clustered according to a suitable measure of equivalence .",
    "pairwise self - similarity measure compares each node of the input graph with all the nodes of the same graph .",
    "it thus computes the similarity between each pair of nodes and then cluster highly similar nodes together .     with",
    "the source nodes @xmath11 and @xmath12 represented as dark circles and the target node represented as light gray circles .",
    ", adapted from `` algorithms for community and role detection in networks , '' by a. browet and p. van dooren , 2013 , width=508 ]    if we define a neighbourhood pattern of length @xmath17 for a node as the sequence of length @xmath17 of incoming ( i ) and outgoing ( o ) edges starting from the source node and reaching the target node ( see figure [ fig_parentchild ] ) , one similarity criterion is to ask a similar pair of nodes to have the same neighbourhood patterns in common . in other words , a pair of similar nodes",
    "should reach many common targets with the same neighbourhood patterns and do so for patterns of various lengths . to illustrate this concept ,",
    "let us first define a few terms . in a directed graph , a neighbour @xmath12 of a node @xmath11",
    "is called a child when @xmath18 and a parent when @xmath19 . for a given node @xmath11",
    ", one can thus compute the number of parents or in - degree @xmath20 and the number of children or out - degree @xmath21 .",
    "the vectors of out and in - degrees are given by @xmath22 where @xmath23 is a vector of 1 s of appropriate dimension .",
    "the number of common children between a pair of nodes @xmath10 is thus given by @xmath24_{i , j } = \\ # \\left\\{k | i \\rightarrow k \\text { and } j \\rightarrow k \\right\\}\\ ] ] and the number of common parents by @xmath25_{i , j } = \\ # \\left\\{k | k \\rightarrow",
    "i \\text { and } k \\rightarrow j \\right\\}\\ ] ] the total number of common parents and children for nodes @xmath11 and @xmath12 are thus given by @xmath26_{i , j}$ ] .",
    "generalising it for a pattern of length @xmath17 , one obtains that the number of common target nodes for neighbourhood patterns of length @xmath17 is given by @xmath27 browet then defines its pairwise node similarity measure as the weighted sum of the number of common target nodes using neighbourhood patterns of any length @xmath28\\ ] ] where @xmath29 = a x a^t + a x a\\ ] ] and @xmath30 is a scaling parameter to balance the relative importance of long neighbourhood patterns with respect to short neighbourhood patterns since the number of common targets tends to naturally grow when using longer patterns .",
    "the similarity matrix @xmath0 can be computed as the fixed point solution of @xmath31\\ ] ] if we initialise the sequence with @xmath32 , the iteration can be written as @xmath33 \\\\",
    "s_1 & = a a^t + a^t a\\end{aligned}\\ ] ] if @xmath34 is small enough , the sequence converges to @xmath35 using the property of the kronecker product , the fixed point solution can be written as @xmath36^{-1 } vec(s_1)\\ ] ] to ensure convergence , one can choose @xmath34 small enough",
    ". however , even if @xmath34 is small enough to ensure convergence , it might be impossible to compute the fixed point solution of equation [ itschemeeq ] because of the increasing computational cost and memory requirement . indeed , even if @xmath37 is sparse , the matrix @xmath38 tends to fill in as @xmath5 increases and each iteration of [ itschemeeq ] is @xmath39 .",
    "therefore , browet defines a low rank similarity approximation of rank at most @xmath6 of @xmath40 as @xmath41 ] = x_{k+1 } x_{k+1}^t\\ ] ] where @xmath42 and @xmath43 $ ] is the best low - rank projector on the dominant subspace of dimension at most @xmath6 which can be computed using a truncated singular value decomposition ( svd ) .",
    "@xmath44 is the best low - rank approximation of @xmath45 which can be written as @xmath46[a | a^t]^t\\ ] ] where @xmath47 $ ] is the horizontal concatenation of @xmath37 and @xmath48 .",
    "the singular value decomposition of this concatenation can be computed as @xmath49 = u_1 \\sigma_1 v_1^t + u_2 \\sigma_2 v_2^t\\ ] ] where the columns of the unitary matrix @xmath50 span the dominant subspace of dimension at most @xmath6 of @xmath47 $ ] and @xmath51 is the diagonal matrix of the dominant singular values , @xmath52 .",
    "this leads to @xmath53 [ a|a^t]^t = u_1 \\sigma_1 ^ 2 u_1^t + u_2 \\sigma_2 u_2^t\\ ] ] which implies the low rank projection of @xmath45 is given by @xmath54 to compute each iterative solution of equation [ itschemeredeq ] , one can see that @xmath55 & = y_k y_k^t \\\\",
    "y_k & = [ x_1 | \\beta a x_k | \\beta a^t x_k ] \\\\",
    "x_{k+1 } x_{k+1}^t & = \\pi^{(r)}[y_k y_k^t]\\end{aligned}\\ ] ] to efficiently compute @xmath56 , we first apply a qr factorisation to @xmath57 , then compute a truncated svd of rank at most @xmath6 of @xmath58 such that @xmath59 and finally compute @xmath60 browet proved in his thesis that the low rank iteration converges to @xmath61\\\\      y y^t    & = [ x_1 | \\beta a x | \\beta a^t x]\\end{aligned}\\ ] ] if the spectral gap of @xmath62 at the @xmath6th eigenvalue is sufficiently large and choosing @xmath34 sufficiently small .",
    "one way to ensure convergence is to choose @xmath63      we will now introduce a new definition of similarity matrices which we intuitively created but found later in the thesis of thomas p. cason .",
    "this method is called the salton index method .",
    "its purpose is to find a new similarity matrix which can show more efficiently that if two nodes belong to one same cluster : it does only take into account `` brothers '' relationships ( two nodes are `` brothers '' if they possess a large number of children and parent nodes in common ) .",
    "two `` brother '' nodes should belong to one same cluster .",
    "thus , the method of browet seems to consider too much information .",
    "it considers further family relatives by checking if two nodes possess same grandparents or grandchildren , etc . indeed , when two nodes possess the same parents and children , they automatically possess the same grand parents and grand children .",
    "we expect to have values near to 0 or 1 in the similarity matrix by examining only the number of common parents and children they possess .",
    "if the common number of children and parents are already known , the information of common grandparents and grandchildren become useless to decide if the two nodes belong to one same cluster and risk to cloud the similarity matrix , making the block identification harder .",
    "the common grandparents who do not come from the same parents is considered as a perturbation .",
    "we start by taking @xmath64 in browet s method , @xmath65 for this similarity matrix , we only consider the common number of parents and children but we do not take into account the total number of and children that each node possesses .",
    "this may lead to incorrect clusters .         for the instance in the figure [ fig : example1 ] example 1",
    ", one can observe that node 1 and node 2 have the same number of common parents and children as node 1 and node 3 .",
    "using the similarity matrix defined by [ simi_sibo ] , we will obtain @xmath66 and thus the similarity matrix is not able to separate nodes 1 and 2 from node 3 .",
    "figure [ fig : example1 ] example 2 shows another disadvantage of this similarity matrix .",
    "one can see that node 1 and node 2 should belong to one same cluster because they have same parents and children .",
    "having a small number of parents and children for both nodes , the value of @xmath67 will be relatively small so it can be ignored when implementing low rank projection .",
    "the main idea of the new method is to check the percentage of common `` parents '' or `` children '' of two nodes .",
    "we will thus make the total number of `` parents '' or `` children '' for nodes @xmath11 and @xmath12 appear in the denominator .",
    "for instance , for two nodes @xmath11 and @xmath12 , the number of common `` children '' is given by @xmath68 ) .",
    "the common percentage of `` children '' for nodes @xmath11 and @xmath12 can thus be expressed by : @xmath69 the percentage of common `` parents '' can be expressed in an analogous way . finally the similarity matrix can be written as : @xmath70 to compute the new similarity matrix , first normalise each row / column of the adjacency matrix : @xmath71 the new similarity matrix @xmath72 can then be written as @xmath73 \\cdot [ c | d^{t}]^{t}\\ ] ] like browet s method , to reduce the total complexity of the algorithm , we use a low rank approximation .",
    "the @xmath6-dimensional projection of the similarity matrix , @xmath74 , is given by : @xmath75 with @xmath76 = u \\cdot x_{r } \\cdot v\\ ] ] where @xmath77 is a real matrix of size @xmath78 , @xmath79 and @xmath8 are orthogonal matrices of size respectively @xmath80 and @xmath81 , which means @xmath82 @xmath83    the new similarity takes only one step to compute because we consider only direct connections ( `` children '' and `` parents '' relationships ) .",
    "it provides a factor matrix which is easier to classify .",
    "however , by executing this method , we loose other information in the similarity matrix such as the relationships like `` cousin '' , `` nephew '' etc . from figure",
    "[ fig : comp_similarity ] , one can see that the similarity matrix obtained by the new measure is actually more `` orthogonal '' than the one obtained via browet s method .     and @xmath84 , title=\"fig : \" ]   and @xmath84 , title=\"fig : \" ]      the main objective of the project is to find a permutation matrix @xmath15 such that @xmath85 is as close as possible to a block - diagonal matrix with blocks of ones .",
    "this should be done without computing the matrix @xmath0 directly .",
    "thus , we need to find an efficient way to build a classification of rows of matrix @xmath1 where similar lines are gathered together .      [",
    "[ k - means - algorithm ] ] @xmath5-means algorithm + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    k - means is a well - known method of vector classification which aims to partition @xmath86 observations into @xmath5 clusters where the number of clusters @xmath5 is given .",
    "given the lines @xmath87 of the matrix @xmath1 , the @xmath5-means method assign each of the lines to one of the @xmath5 clusters @xmath88 , with @xmath89 , to minimise the sum of squared distances of each point in the cluster to the @xmath5 centres . in other words ,",
    "the objective is to find @xmath90 where @xmath91 is the centroid of the cluster @xmath11 for all @xmath92 and @xmath93 is the euclidean norm.@xcite knowing the group to which each line of @xmath1 should belong , it is then easy to find a permutation such that those lines are grouped together .",
    "the goal is thus to regroup @xmath86 vectors in a @xmath94-dimensional space to @xmath5 different clusters .",
    "the algorithm proceeds are as follows :    * start with @xmath5 initial centres @xmath95 ( not necessarily among the @xmath86 given points ) chosen randomly or intentionally for each cluster ( figure [ fig_kmeans](b ) ) . * for @xmath96 , let the cluster @xmath97 be the set of points @xmath98 that are closer to @xmath99 than to @xmath100 with @xmath101 ( figure [ fig_kmeans](c ) . *",
    "once the @xmath86 elements have been placed into one of the @xmath5 groups , compute the centroid of each group and replace the @xmath5 centroids used in the previous step .",
    "* repeat step 2 and 3 until the method converges i.e. when groups do nt change any more ( figure [ fig_kmeans](e ) ) or the number of iterations attains the maximum limit .",
    "the time complexity of k - means algorithm is given by @xmath102 with @xmath103 the maximum number of iterations and @xmath104 the time to calculate the distance between two points @xmath87 and @xmath105 of x @xcite .",
    "it should be pointed that the k - means method may converge to different results depending on the initial centres .",
    "it is thus of major importance to choose the initial centroids wisely .",
    "a good choice may reduce the number of steps needed to get convergence and therefore the total complexity . a bad choice",
    "may furthermore lead to arbitrarily bad classification .",
    "[ [ initial - guess - of - k - means - k - means - algorithm ] ] initial guess of @xmath5-means : @xmath5-means + + algorithm + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    instead of selecting the initial centres randomly , the k - means + + algorithm ( which gives the default initialisation of the centroids in matlab ) proceeds as follows :    * select an observation uniformly at random from the data set , @xmath1 .",
    "the chosen observation is the first centroid , and is denoted @xmath106 .",
    "* let @xmath107 denote the shortest distance from a data point to the closest centre we have already chosen .",
    "choose the new centre @xmath108 to be @xmath109 with probability @xmath110 * repeat step 2 until the @xmath5 initial centres have been chosen    according to arthur and vassilvitskii in @xcite , k - means++ improves the running time of lloyd s algorithm , and the quality of the final solution .",
    "this randomised greedy strategy takes only @xmath111 iterations and immediately grants an expected approximation ratio of @xmath112 .",
    "[ [ improving - k - means - algorithm - angle - between - clusters - and - co - linearity - between - elements - of - the - same - cluster ] ] improving @xmath5-means algorithm : angle between clusters and co - linearity between elements of the same cluster + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to improve the previous algorithm , one might use the properties of the matrix @xmath1 .",
    "two lines of @xmath1 should have an inner product close to one when the nodes to which they refer belong to the same cluster .",
    "they should have an inner product far smaller than one when the nodes belong to different clusters .",
    "the value of @xmath113 is the inner product of @xmath114 and @xmath115 .",
    "as we have normalised the rows of matrix @xmath1 , the previous result leads to the co - linearity of @xmath114 and @xmath115 when @xmath116 . to check",
    "if the classification given by k - means is suitable , we compute the centroid of each group .",
    "the inner product of two vectors belonging to different clusters should be smaller than a maximum limit ( in most cases , we have chosen this limit as 0.7 by observing the result in figure [ fig_innerproduct ] ) .",
    "furthermore , two vectors belonging to the same cluster should have an inner product greater than a minimum limit ( we have chosen the minimum value to be 0.9 ) .",
    "if the condition is not achieved , k - means will be executed once again until those conditions are satisfied or the maximum number of iterations is reached .",
    "if it is the conditions were not satisfied , the algorithm will then just output the result of the last test by default , leading to a decreasing accuracy .      until know",
    ", the number of clusters @xmath5 given by the user to the algorithm was supposed to be exact .",
    "however , in real life situations when facing graphs with many nodes , represented by huge adjacency matrices , it might be way more difficult to know beforehand the correct number of clusters . in order to find the correct integer @xmath5 , we will present three different methods . in the low rank similarity approximation ,",
    "browet uses a projection on dominant subspaces of dimension at most @xmath6 .",
    "@xmath6 , which is the dimension of the factor matrix @xmath1 , should be an upper bound of @xmath5 .",
    "if the true value of @xmath5 is higher than @xmath6 ( which means we have under - estimated @xmath6 ) , it is impossible to make a correct classification on actual matrix @xmath1 .",
    "thus , when the methods fail for all @xmath117 , one should check whether @xmath6 was not under - estimated .",
    "[ [ k - moving - method ] ] @xmath5-moving method + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the most natural idea is to try all possible values of @xmath5 with by increasing or decreasing the initial guess until an acceptable classification is found . considering @xmath6 as an upper bound of cluster number @xmath5",
    ", we can simply start with @xmath118 .",
    "since we want the algorithm to identify the correct number @xmath5 automatically , we used the same stopping criterion as in the k - means method using the properties of the matrix @xmath1 .",
    "the most useful criterion will be the orthogonality condition since overestimating the value @xmath5 leads to group elements which indeed belong to the same cluster ( and which are thus co - linear ) but separates elements which should belong to the same group ( and which are thus far from fulling the orthogonality criterion ) .",
    "those conditions should not be all satisfied when @xmath5 is not correct . in the case",
    "the algorithm finds the correct @xmath5 , it needs @xmath119 steps to do so .",
    "however , it might try to cluster the nodes with the correct @xmath5 but decide ( due to noise or to the complexity of the graph ) that it ca nt find a correct classification with this @xmath5 .",
    "furthermore , this method is more suitable when @xmath5 is close to @xmath6 .",
    "when this is not the case , the complexity will be no longer acceptable .",
    "+ figure [ figure_findkhier ] illustrates this method with a simple directed graph of 5 clusters .",
    "we can see from this example that at the beginning , we have over - estimated @xmath5 by taking @xmath120 , thus we start testing the classification method when decreasing k by computing at most 201 tests for each value of @xmath5 .",
    "finally , we found the correct number k=5 after testing @xmath121 , and @xmath122 .",
    "starting from @xmath120 by k - moving method ]    [ [ hierarchical - method ] ] hierarchical method + + + + + + + + + + + + + + + + + + +    hierarchical classification method allows to find a correct @xmath5 with a reduced complexity when @xmath5 is far from @xmath6 .",
    "given @xmath86 nodes @xmath123 to be classified , hierarchical clustering proceeds as follows :    * define @xmath86 groups of singleton : @xmath124 with @xmath125 * combine two groups which possess the minimum distance between all pairs of groups , i.e. @xmath126 with @xmath127 and @xmath91 the centroids of the groups @xmath128 and @xmath129 * take @xmath130 and @xmath131 * repeat step 2 and 3 until the minimum distance between two groups is high enough    the algorithm should stop when the number of non empty clusters is @xmath5 . an example is given on figure [ fig_hierarchicalmethod ] for the case k=2 .",
    "this method has however a complexity @xmath3",
    ". we can thus not directly apply it on matrix @xmath1 .",
    "given an upper bound @xmath6 of @xmath5 , we find @xmath6 clusters using k - means algorithm and asking only for co - linearity between elements of the same cluster .",
    "this gives us @xmath6 `` sub - clusters '' where the true clusters consist of several `` sub - clusters '' .",
    "the goal is to find clusters such that elements belonging to two different clusters are orthogonal .",
    "we start the hierarchical method by giving it the centroids of the @xmath6 `` sub - clusters '' .",
    "the hierarchical method iterates until the centroids of different clusters are not collinear any more ( orthogonality criterion ) .",
    "when the correct number of clusters @xmath5 has been found , we apply the @xmath5-means method with the found number of clusters @xmath5 .",
    "starting from @xmath132 by hierarchical method ]    look for instance at the figure [ figure_findkhierb ] , where @xmath133 and @xmath132 .",
    "we start the preliminary classification by applying @xmath134-means method to find @xmath135 `` sub - clusters '' in red .",
    "we then identify that 3 among 6 `` sub - cluster '' are almost collinear because they are members of one big cluster .",
    "we can then regroup another two `` sub - clusters '' as well .",
    "finally , we find @xmath133 for this directed graph . using this method",
    ", ideally we will only need to execute k - means method two times .",
    "[ [ svd - method ] ] svd method + + + + + + + + + +    another way to correctly estimate @xmath5 when @xmath136 consists in computing the singular value decomposition of a matrix @xmath1 and compute the number of non - negligible singular values .",
    "more formally , for @xmath137 where @xmath138 is the matrix of singular values , @xmath139 for @xmath140 , if @xmath141 , we will then start the @xmath5-means method with @xmath142 groups .",
    "this method is very efficient when there exist a big difference between @xmath6 and @xmath5 .",
    "however , when the level of noise is high , the clustering problem becomes harder : it becomes hard to decide whether a singular value is negligible .",
    "+ for instance , when the reduced graph b , containing 50 elements in each cluster , and the matrix @xmath138 are given by : @xmath143 we see that only three singular values of @xmath138 are not negligible : @xmath144 , @xmath145 and @xmath146 and thus set @xmath142 to @xmath147 .",
    "[ [ comparison - of - the - different - methods ] ] comparison of the different methods + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the first method has the highest accuracy among all three methods as we test many possible values for @xmath5 . in reality , it is possible to have several possible classifications ( even several possible values of k ) . because we have taken both conditions ( orthogonality and co - linearity ) as a stopping criteria , the algorithm is likely to return an acceptable result . if it comes to @xmath148 , it ensures the absence of an acceptable classification . the second and the third methods are more efficient when @xmath6 is much bigger than @xmath5 .",
    "thus , they are peculiarly useful when we have a vague upper bound of @xmath5 . however , the performance of those two methods can be influenced by a high level of perturbation .",
    "to compare the results of each algorithm , we need a quantitative criterion to measure how close the extracted partitions are to the true partition of the benchmark .",
    "the most commonly used measures in community detection are based on information theory . as the clustering problem is a generalisation of the community detection problem , it seems fit to use the nmi measure as well .",
    "the objective is to quantify how much information about the true partition one can infer from the extracted partition .",
    "+ in information theory , if we us assume @xmath1 is an event that may occur with a probability @xmath149 , the information contained in the event @xmath1 is defined by @xmath150 the entropy is a measure of uncertainty of a random variable .",
    "it is defined as the expected information of a single realisation of x , i.e. @xmath151 the joint entropy measures the uncertainty of the joint probability distribution @xmath152 to observe @xmath153 and @xmath154 and is given by @xmath155 the mutual information is defined as the shared information between two distributions @xmath156 @xmath157 when @xmath1 and @xmath158 are totally independent . in our context , the random variables @xmath1 and @xmath158 correspond to community partitions , so @xmath159 is the probability that a node taken at random belongs to community @xmath160 in the partition @xmath1 .",
    "if @xmath161 is the number of nodes in the community @xmath160 in the partition @xmath1 and @xmath162 is the number of nodes that belong to community @xmath160 in the partition @xmath1 and to community @xmath163 in the partition @xmath158 , we compute @xmath164 there is no upper bound for @xmath165 , so for easier interpretation and comparisons , a normalised version of the mutual information that ranges from 0 to 1 is desirable ( nmi ) .",
    "@xmath166@xcite      we applied the pairwise similarity measures to extract roles in erdos - renyi graphs containing a prescribed block structure .",
    "we first choose the reduced graph @xmath167 where each node represents a role we would like to identify .",
    "the random graph @xmath168 is then made by assigning a chosen number of nodes per role , meaning that for each node @xmath169 corresponds a given role @xmath170 .",
    "we then create the edges @xmath171 with a probability @xmath172 $ ] if there exists a given edge between the corresponding roles in @xmath173 , i.e. if @xmath174 .",
    "if there exist no edges for the corresponding roles in @xmath173 , the edge might still be added with a probability @xmath175 in @xmath176 .",
    "if @xmath177 is much larger than @xmath175 , then the role graph @xmath173 is accurately representing the different roles in the graph @xmath176 and the pairwise similarity @xmath0 between the vertices @xmath178 should allow the extraction of those roles . if @xmath175 is much larger than @xmath177 , then the different roles in @xmath176 are much closely represented by the complement graph of @xmath173 represented by the adjacency matrix @xmath179 . as the role structure still strongly exists in this complement graph , the similarity measure @xmath0 should still be able to differentiate them .",
    "if probabilities @xmath177 and @xmath175 are close to each other , the role extraction becomes challenging because the graph becomes closer to a uniform erdos - renyi graph with is known to be free of any structure .",
    "for instance , choosing a reduced graph of 5 clusters and its adjacency matrix @xmath180 with 1000 elements in each cluster .",
    "the directed graph g and the ideal adjacency matrix a ( for which @xmath181 and @xmath182 ) associated are thus given by figures [ fig_adjacencymatrix ] .",
    "the similarity matrix @xmath0 defined by browet is represented on figure [ fig_similaritymatrix ] .",
    "values one in the similarity matrix are represented by black pixels and zero values by white ones .",
    "we observe from the figure that for two nodes @xmath183 in same cluster @xmath184 which means they are almost collinear .",
    "the white zones outside the diagonal blocks show the orthogonality of two nodes belonging to different clusters .",
    "the gray boxes illustrate the fact that elements from different clusters are not always orthogonal to each other , they possess a given angle with respect to each other . in the case of a cyclic graph ( shown in the figure [ fig_directedgraph2 ] ) , one can see from the similarity matrix that the vectors in different clusters are not all orthogonal to each other .",
    "[ fig_adjacencymatrix ]     [ fig_adjacencymatrix ]     [ fig_similaritymatrix ]        indeed , the orthogonality is influenced by the level of noise within the graph . to better detect the influence of noise on the angles between clusters , we choose three different couples @xmath185 and the adjacency matrix of the reduced graph to be @xmath186 with 200 nodes per block .",
    "the histograms of values of the inner product of all pairs of rows in the matrix x are represented in figures [ fig_innerproduct ] .",
    "+    , @xmath187,@xmath188,title=\"fig : \" ] , @xmath187,@xmath188,title=\"fig : \" ] , @xmath187,@xmath188,title=\"fig : \" ]    analysing the three figures [ fig_innerproduct ] , we observe two separated peak values of inner products .",
    "the first group linked to the first peak has an inner product for from 1 , meaning the lines in @xmath1 are linked to nodes belonging to different clusters .",
    "the second group linked to the second peak has an inner product close to one , which means nodes do belong to the same cluster .",
    "from the same figures , we easily deduce that the amplitude of the perturbation reduces the orthogonality from the vectors ( i.e. the minimum angle between two elements from different clusters increase as a function of the perturbation ) .",
    "for instance , in the case @xmath189 and @xmath190 , there are only two possible values 0 or 1 for inner products .",
    "this means vectors in the same cluster are all parallel to each other and two vectors from different clusters are always orthogonal .",
    "we can observe also that for @xmath177 smaller than 0.7 the set of inner product values is actually dense in the interval [ 0.6,1 ] .",
    "this will make it more difficult to separate elements from different clusters .",
    "the distribution of inner product values depends also on the reduced graph b and number of elements in each cluster , it is easier to separate the clusters when the vectors are `` homogeneously '' distributed in each group .",
    "more examples are shown in the appendix .",
    "we compute the normalised mutual information between the exact role structure and the extracted role partition using the low rank similarity approximation and the similarity measure salton index method for @xmath191 with @xmath5 the number of clusters .",
    "we generated 20 random realisations for each couple of probability parameters pin and pout in [ 0 , 1 ] with a step size of 0.05 , and we computed the average nmi on those 20 realisations .    [ [ comparison - between - clustering - algorithms ] ] comparison between clustering algorithms + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the community detection algorithm using @xmath5-means and taking into account the properties of the matrix @xmath1 performs better than @xmath5-means alone , as shown on figure [ fig_3block_kmeans ] and figure [ fig_4block_kmeans ] .    [ [ comparison - between - similarity - measures ] ] comparison between similarity measures + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    if we compare the results obtained by the two similarity measures , we see the nmi of the new method is more homogeneous when @xmath177 is far from @xmath175 especially in the structure represented on figure   [ fig : blockb ] .",
    "however , even thought this method is faster , the nmi obtained seems slightly worse ( especially for the three - blocks structure ) .",
    "[ [ effect - of - size ] ] effect of size + + + + + + + + + + + + + +    when using the same similarity matrix , both community detection algorithms perform better when the number of nodes per group is approximately equal .",
    "when the size of one of the groups becomes small compared with the other , the quality of the results decreases up to a certain point when this group becomes too small to strongly impact the nmi .",
    "this is shown on figure [ fig : size_firstgroup ] where we increased the number of nodes in the first group in the range [ 0 , 100 ] by step size of 2 while simultaneously decreasing the number of nodes in groups two and three in the range [ 150 , 100 ] by step size of 1 .",
    "furthermore , the greater the total number of nodes in the graph , the better the performance is when @xmath177 is close to @xmath175 .",
    "[ fig : blocka ]     [ fig : blockb ]    .",
    "+ first line ( from left to right ) : k - means + @xmath192 20 nodes in the first block and 140 in the two other + @xmath193 100 nodes per block + @xmath193 100 nodes per block using the new similarity matrix and additional properties of matrix @xmath1 + second line ( from left to right ) : k - means using additional properties of the matrix @xmath1 + @xmath193 30 nodes in the first block and 210 nodes in the other + @xmath193 20 nodes in the first block and 140 nodes in the other + @xmath193 100 nodes per block + , title=\"fig:\",width=211 ] .",
    "+ first line ( from left to right ) : k - means + @xmath192 20 nodes in the first block and 140 in the two other + @xmath193 100 nodes per block + @xmath193 100 nodes per block using the new similarity matrix and additional properties of matrix @xmath1 + second line ( from left to right ) : k - means using additional properties of the matrix @xmath1 + @xmath193 30 nodes in the first block and 210 nodes in the other + @xmath193 20 nodes in the first block and 140 nodes in the other + @xmath193 100 nodes per block + , title=\"fig:\",width=211 ] .",
    "+ first line ( from left to right ) : k - means + @xmath192 20 nodes in the first block and 140 in the two other + @xmath193 100 nodes per block + @xmath193 100 nodes per block using the new similarity matrix and additional properties of matrix @xmath1 + second line ( from left to right ) : k - means using additional properties of the matrix @xmath1 + @xmath193 30 nodes in the first block and 210 nodes in the other + @xmath193 20 nodes in the first block and 140 nodes in the other + @xmath193 100 nodes per block + , title=\"fig:\",width=211 ] .",
    "+ first line ( from left to right ) : k - means + @xmath192 20 nodes in the first block and 140 in the two other + @xmath193 100 nodes per block + @xmath193 100 nodes per block using the new similarity matrix and additional properties of matrix @xmath1 + second line ( from left to right ) : k - means using additional properties of the matrix @xmath1 + @xmath193 30 nodes in the first block and 210 nodes in the other + @xmath193 20 nodes in the first block and 140 nodes in the other + @xmath193 100 nodes per block + , title=\"fig:\",width=211 ] .",
    "+ first line ( from left to right ) : k - means + @xmath192 20 nodes in the first block and 140 in the two other + @xmath193 100 nodes per block + @xmath193 100 nodes per block using the new similarity matrix and additional properties of matrix @xmath1 + second line ( from left to right ) : k - means using additional properties of the matrix @xmath1 + @xmath193 30 nodes in the first block and 210 nodes in the other + @xmath193 20 nodes in the first block and 140 nodes in the other + @xmath193 100 nodes per block + , title=\"fig:\",width=211 ] . + first line ( from left to right ) : k - means + @xmath192 20 nodes in the first block and 140 in the two other + @xmath193 100 nodes per block + @xmath193 100 nodes per block using the new similarity matrix and additional properties of matrix @xmath1 + second line ( from left to right ) : k - means using additional properties of the matrix @xmath1 + @xmath193 30 nodes in the first block and 210 nodes in the other + @xmath193 20 nodes in the first block and 140 nodes in the other + @xmath193 100 nodes per block + , title=\"fig:\",width=211 ]    .",
    "+ first line ( from left to right ) : k - means + @xmath193 10 nodes in the first block and 90 in the three other + @xmath193 70 nodes per block + @xmath193 70 nodes per block using the new similarity measure and additional properties of matrix @xmath1 + second line ( from left to right ) : k - means using additional properties of the matrix @xmath1 + @xmath193 15 nodes in the first block and 135 nodes in the other + @xmath193 10 nodes in the first block and 90 nodes in the other + @xmath193 70 nodes per block + , title=\"fig:\",width=211 ] .",
    "+ first line ( from left to right ) : k - means + @xmath193 10 nodes in the first block and 90 in the three other + @xmath193 70 nodes per block + @xmath193 70 nodes per block using the new similarity measure and additional properties of matrix @xmath1 + second line ( from left to right ) : k - means using additional properties of the matrix @xmath1 + @xmath193 15 nodes in the first block and 135 nodes in the other + @xmath193 10 nodes in the first block and 90 nodes in the other + @xmath193 70 nodes per block + , title=\"fig:\",width=211 ] .",
    "+ first line ( from left to right ) : k - means + @xmath193 10 nodes in the first block and 90 in the three other + @xmath193 70 nodes per block + @xmath193 70 nodes per block using the new similarity measure and additional properties of matrix @xmath1 + second line ( from left to right ) : k - means using additional properties of the matrix @xmath1 + @xmath193 15 nodes in the first block and 135 nodes in the other + @xmath193 10 nodes in the first block and 90 nodes in the other + @xmath193 70 nodes per block + , title=\"fig:\",width=211 ] .",
    "+ first line ( from left to right ) : k - means + @xmath193 10 nodes in the first block and 90 in the three other + @xmath193 70 nodes per block + @xmath193 70 nodes per block using the new similarity measure and additional properties of matrix @xmath1 + second line ( from left to right ) : k - means using additional properties of the matrix @xmath1 + @xmath193 15 nodes in the first block and 135 nodes in the other + @xmath193 10 nodes in the first block and 90 nodes in the other + @xmath193 70 nodes per block + , title=\"fig:\",width=211 ] .",
    "+ first line ( from left to right ) : k - means + @xmath193 10 nodes in the first block and 90 in the three other + @xmath193 70 nodes per block + @xmath193 70 nodes per block using the new similarity measure and additional properties of matrix @xmath1 + second line ( from left to right ) : k - means using additional properties of the matrix @xmath1 + @xmath193 15 nodes in the first block and 135 nodes in the other + @xmath193 10 nodes in the first block and 90 nodes in the other + @xmath193 70 nodes per block + , title=\"fig:\",width=211 ] . + first line ( from left to right ) : k - means + @xmath193 10 nodes in the first block and 90 in the three other + @xmath193 70 nodes per block + @xmath193 70 nodes per block using the new similarity measure and additional properties of matrix @xmath1 + second line ( from left to right ) : k - means using additional properties of the matrix @xmath1 + @xmath193 15 nodes in the first block and 135 nodes in the other + @xmath193 10 nodes in the first block and 90 nodes in the other + @xmath193 70 nodes per block + , title=\"fig:\",width=211 ]    -means and its additional properties , when varying the size of the first group in figure [ fig_3block_kmeans ] , the total number of nodes kept constant to 300 and the two other groups having the same number of nodes ]      again , choosing the graph roles as in figure @xmath194 , we verify on figure @xmath195 a linear complexity when the true number of clusters is known . furthermore ,",
    "when trying to detect correctly the different clusters and estimating @xmath5 with the @xmath5-moving method , the complexity increase with the estimated value for @xmath5 but remains less than quadratic .",
    "and when detecting the true value of @xmath5 using @xmath5-moving algorithm ]    let s now compare the hierarchical method and the singular value method based on their efficiency and accuracy .",
    "the figure [ figure : comparison ] presents the accuracy and time needed for a simple graph of 3 clusters of 200 nodes in total , the number of nodes in the first cluster varying from 0 to 100 .",
    "the rest of the nodes are distributed homogeneously into two other clusters .",
    "one can observe that the svd method is more accurate than the hierarchical method and costs way less computation time .",
    "however , in practice , the gap between the singular values can be much less clear .",
    "this may require to ask the users to choose the dimension of projection .",
    "as predicted , our method to compute the similarity matrix is far faster than browet s method ( see figure [ fig : time_simi ] ) .     and @xmath196     and @xmath196     and @xmath197      in order to analyse real networks , we analysed the foodweb of the florida bay ecosystem .",
    "the 122 different biological species can be classified into 7 subgroups : the primary producers ( 1 ) , the microfauna ( 2 ) , the macroinvertebrates ( 3 ) , the fishes ( 4 ) , the herpetofauna ( 5 ) , the avifauna ( 6 ) and the mammals ( 7 ) .",
    "the erdos - renyi graphs for which we validate our algorithm do not have weights on their edges .",
    "therefore , we only considered binary weight on the edges : the element @xmath198 in the adjacency matrix of the food web has value 1 if the animal @xmath11 is being eaten by the animal @xmath12 and 0 otherwise .",
    "the initial adjacency matrix is shown on figure [ fig : florida_adj ] and its corresponding reduced graph after classified by the biologist on figure [ fig_redfloridabio ] .",
    "the goal is to extract groups of animals sharing the same diet , those groups will be the different clusters . using the biological classification , the reduced graph of the food web",
    "is shown on figure [ fig_redfloridabio ] .",
    "for clarity , if the proportion of species @xmath11 in the diet of species @xmath12 is less than @xmath199 , it is not shown on the graph .",
    "+ after obtaining the clustering of the nodes ( for further details see table [ tab : classflorida ] ) , the adjacency matrix after the corresponding permutations is shown on figure [ fig : florida_adjb ] .",
    "the corresponding reduced graph stands on figure [ fig : florida_roles ] .",
    "+ if we analyse table [ tab : classflorida ] in detail , we see that group e includes 71 % of the macro invertebrates and does not include any other species .",
    "the other groups are unfortunately not as uniform .",
    "half of the primary consumers belong to group a while the other half belong to group b. similarly , if half of the fish are placed in group e , the rest is placed in groups f and g. the first half of the primary consumers is placed in group a and the second half in group b. the herptofauna is splitted in the groups e , f and g while the mammals are placed in groups f and g.    the difficulty to obtain clear groups can be explained by the fact that some groups like mammals or herptofauna are very small compared to others like fishes .",
    "furthermore , the number of nodes is very small compared to the graphs used before , which increases the difficulty to obtain correct clusters .",
    "we could also add that those graphs are very noisy ( @xmath200 and @xmath201 ) and in those cases , the methods using the property of the graph become useless .",
    "\\(b ) @xmath202 ; ( a ) [ above left = of b ] @xmath37 ; ( c ) [ above right = of b ] @xmath203 ; ( d ) [ below left = of b ] @xmath204 ; ( e ) [ above right = of a ] @xmath9 ; ( f ) [ below = of c ] @xmath205 ; ( g ) [ below right = of f ] @xmath206 ; ( c ) edge [ bend right = 10 ] node[left](e ) ; ( c ) edge [ bend right = 0 ] node[left](b ) ; ( e ) edge [ bend right = 0 ] node[left](b ) ; ( g ) edge [ bend right = 40 ] node[left](e ) ; ( g ) edge [ loop below ] node[left](g ) ; ( g ) edge [ bend left = 20 ] node[left](b ) ; ( f ) edge [ bend right = 20 ] node[left](b ) ; ( d ) edge [ loop left ] node[left](d ) ; ( d ) edge [ bend left = 10 ] node[left](a ) ; ( d ) edge [ bend right = 10 ] node[left](b ) ; ( a ) edge [ bend left = 10 ] node[left](e ) ; ( b ) edge [ bend right = 10 ] node[left](f ) ;",
    "( b ) edge [ bend right = 10 ] node[left](d ) ; ( b ) edge [ loop left ] node[left](b ) ;    .role structure compared to the biological compartments in the florida bay network [ cols=\"<,<,<\",options=\"header \" , ]      we then classified countries based on trade data from several manufactures of metal among 80 countries in 1993 ( austria , seychelles , bangladesh , croatia , and barbados ) , 1994 and 1995 data ( south africa and ecuador ) .",
    "most missing countries are located in central africa and the middle east , or belong to the former ussr .",
    "the edges represent imports by one country from another for the class of commodities designated as miscellaneous manufactures of metal , which represents high technology products or heavy manufacture .",
    "the absolute value of imports ( in 1,000 us$ ) is used but imports with values less than 1% of the country s total imports were omitted .      to have a first idea of the interaction between countries , we fix the number of clusters to @xmath208 .",
    "we clearly see on figure [ fig : countries_color2 ] that most industrial countries are set in group b and less industrialised countries in group a.    to compute the reduced graph shown on figure [ fig : metal_roles2 ] , we computed the adjacency matrix and , separating the different clusters , we computed the mean number of its elements per block .",
    "we created an edge @xmath10 with @xmath209 when the mean number of elements in the block @xmath210 was greater than 0.1 . we can then conclude that industrialised countries do trade more than non - industrialised countries , which could have been predicted .",
    ", made using https://mapchart.net/detworld.html ]    \\(a ) @xmath37 ; ( b ) [ right = of a ] @xmath202 ; ( b ) edge [ loop right ] node[left](b ) ; ( b ) edge [ bend right = 0 ] node[left](a ) ;    [ fig : metal_roles2 ]     [ fig : countries_adj2 ]      the most industrialised countries are set in group d. the less industrialised countries are set in group a ( america ) group b ( mainly oceania ) and group c ( mainly africa and eastern europe ) .    to compute the reduced graph shown on figure [ fig : metal_roles ] , did the same as in the previous section and created an edge @xmath10 with @xmath212 when the mean number of elements in the block @xmath210 was greater than 0.1 .",
    "the same conclusions about trade can be made as in the previous section .",
    "furthermore , less industrialised countries do not exchange much with other less industrialised countries located in another geographical area .",
    "more industrialised countries possess a higher pib and can thus afford to trade with countries situated in other geographical areas .",
    "this agrees with economical models such as the gravity model of trade ( for further details , see ) .",
    "\\(a ) @xmath37 ; ( b ) [ above = of a ] @xmath202 ; ( c ) [ right = of a ] @xmath203 ; ( d ) [ above = of c ] @xmath204 ; ( a ) edge [ loop left ] node[left](a ) ; ( b ) edge [ loop left ] node[left](b ) ; ( d ) edge [ loop above ] node[left](d ) ; ( d ) edge [ bend right = 0 ] node[left](a ) ; ( d ) edge [ bend right = 0 ] node[left](b ) ; ( d ) edge [ bend right = 0 ] node[left](c ) ;    [ fig : metal_roles ]     [ fig : countries_adj ]    , made using https://mapchart.net/detworld.html ]      using the new similarity measure , we see that figure [ fig : countries_colornew ] is very similar to the figure [ fig : countries_color ] .",
    "the only differences are argentina and denmark .",
    "both classifications thus seem almost equivalent .",
    "( new similarity method ) , made using https://mapchart.net/detworld.html ]    our method is more efficient on such graphs because the number of exchanges between members of a same cluster is higher than in the florida web .",
    "let s summarise our main achievements :    * the clustering algorithm has a reduced complexity by using @xmath5-means algorithm .",
    "* comparing our two similarity measures , we concluded that one should choose browet s method when robustness is required but our method concerning speed .",
    "* we analysed the effect of different parameters ( @xmath5 , @xmath177 , @xmath175 , the number of elements in each cluster and the total number of elements ) on the performance of our algorithm .",
    "our best community detection algorithm when @xmath5 is known is the @xmath5-means algorithm using additional properties of matrix @xmath1 .",
    "* when @xmath5 is unknown , the best methods for computation time and accuracy , seems to be the svd method .",
    "using real networks , we saw some flaws of our algorithm .",
    "if the classification of the different countries seems to be accurate , this is not the case for the florida web .",
    "indeed , the level of perturbation is high compared to the number of edges between clusters ( @xmath213 ) . for such graphs , we know in advance the reduced graph .",
    "we could use this information by checking the structure of the graph after @xmath5-means by correcting the classification of doing @xmath5-means again until the classification seems to fit the structure .",
    "however , the method would then be very dependant on the type of graph .",
    "from the examples above , one can see that when the level of noise is high , there will be less difference between inner product values .",
    "once , the distribution of inner products become totally dense , it will be very difficult to have a good classification .",
    "we have to mention that for a complex graph structure , even with no noise , the inner product of two vectors from different clusters can be nonzero .",
    "arthur , d. , & vassilvitskii , s. ( 2007 , january ) .",
    "_ k - means++ : the advantages of careful seeding_. in proceedings of the eighteenth annual acm - siam symposium on discrete algorithms ( pp .",
    "1027 - 1035 ) .",
    "society for industrial and applied mathematics .",
    ", http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf    strehl , a. , & ghosh , j. ( 2002 ) .",
    "_ cluster ensembles  a knowledge reuse framework for combining multiple partitions .",
    "_ journal of machine learning research , 3(dec ) , 583 - 617 ."
  ],
  "abstract_text": [
    "<S> computing meaningful clusters of nodes is crucial to analyse large networks . in this paper </S>",
    "<S> , we apply new clustering methods to improve the computational time . </S>",
    "<S> we use the properties of the adjacency matrix to obtain better role extraction . </S>",
    "<S> we also define a new non - recursive similarity measure and compare its results with the ones obtained with browet s similarity measure . </S>",
    "<S> we will show the extraction of the different roles with a linear time complexity . finally , we test our algorithm with real data structures and analyse the limit of our algorithm .    </S>",
    "<S> * _ key words _ * role model , community detection , similarity measure , k - means , clustering </S>"
  ]
}