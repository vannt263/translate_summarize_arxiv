{
  "article_text": [
    "big data is encountered in genomics for two reasons : the size of the genome and the heterogeneity of populations .",
    "complex organisms , such as plants and animals , have genomes on the order of billions of base pairs ( the human genome consists of over three billion base pairs ) .",
    "the diversity of populations , whether of organisms , tissues or cells , means we need to sample deeply to detect low frequency events . to interrogate long and/or numerous genomic sequences , many measurements are necessary .",
    "for example , a typical whole genome sequencing experiment will consist of over one billion reads of 75100  bp each .",
    "the reads are aligned across billions of positions , most of which have been annotated in some way .",
    "this experiment may be repeated for thousands of samples .",
    "such a data set does not fit within the memory of a current commodity computer , and is not processed in a timely and interactive manner . to successfully wrangle a large data set ,",
    "we need to intimately understand its structure and carefully consider the questions posed of it .",
    "there are three primary types of data in genomics : sequence vectors , annotated ranges and feature - by - sample matrices of summary statistics ( figure  [ fig : three - data - types ] ) .",
    "biological sequences are represented as strings of characters from a restricted alphabet .",
    "for example , a dna sequence consists of the letters a , c , g and t , each referring to a particular type of nucleotide .",
    "a genome consists of a set of dna sequences , one for each chromosome .",
    "we generalize the concept of sequence and define the term _ sequence vector _ to mean either a sequence or a vector that runs parallel to a sequence .",
    "the latter may be some curated or computed value , such as the cross - species conservation or coverage from a sequencing experiment .",
    "the coverage is a common summary that represents the number of features overlapping each position in the reference sequence .",
    "as we learn more about a genome , we annotate genomic ranges with information like gene structures and regulatory binding sites .",
    "the alignment of sequences to a reference genome is another type of range - based annotation .    to compare data across samples ,",
    "we often summarize experimental annotations over a set of reference features to yield a feature - by - sample matrix .",
    "for example , we might count read alignments overlapping a common set of genes across a number of samples .",
    "larger matrices often arise in genetics , where thousands of samples are compared over millions of snps , positions that are known to vary within a population . in every case , the summaries are tied to a genomic range .    to analyze the results of an experiment , we need to integrate data of different types .",
    "for example , we might have alignments for a chip - seq experiment , where the sequences have been enriched for binding to a particular transcription factor .",
    "a typical analysis involves checking coincidence with annotated binding sites for that transcription factor , as well as looking for correlation between gene expression and binding signal upstream of the gene .",
    "the gene expression values might be drawn from a gene by a sample matrix summarized from an rna - seq experiment .",
    "the genomic range is the common thread that integrates all three types of data .",
    "the sequence , that is , the genome , acts as the scaffold , and ranges coordinate the annotations and summarized features by locating them on the same sequence .",
    "the r language @xcite is widely applied to problems in statistics and data analysis , including the analysis of genomic data @xcite , as evidenced by the large number of available software packages providing features ranging from data manipulation to machine learning .",
    "r provides high - level programming abstractions that make it accessible to statisticians and bioinformatics professionals who are not software engineers per se .",
    "one aspect of r that is particularly useful is its `` copy on write '' memory semantics , which insulates the user from the details of reference - based memory management .",
    "the fundamental r data structure is the _ atomic vector _ , which is both convenient and efficient for moderately sized data .",
    "an atomic vector is homogeneous in data type and so easily stored in one contiguous block of memory .",
    "many vector operations are implemented in native ( c ) code , which avoids invoking the r interpreter as it iterates over vector elements . in a typical multivariate data set ,",
    "there is heterogeneity in data type across the columns and homogeneity along a column , so vectors are naturally suited for column - oriented data storage , as in the basic _ data.frame_. vectorized computations can usually be expressed with simpler and more concise code compared to explicit iteration .",
    "the strengths of r are also its weaknesses : the r api encourages users to store entire data sets in memory as vectors .",
    "these vectors are implicitly and silently copied to achieve copy - on - write semantics , contributing to high memory usage and poor performance .",
    "there are general strategies for handling large genomic data that are well suited to r programs .",
    "sometimes the analyst is only interested in one aspect of the data , such as that overlapping a single gene . in such cases",
    ", restricting the data to that subset is a valid and effective means of data reduction .",
    "however , once our interests extend beyond a single region or the region becomes too large , resource constraints dictate that we can not load the entire data set into memory at once , and we need to iterate over the data to reduce them to a set of interpretable summaries .",
    "iteration lends itself to parallelism , that is , computing on multiple parts of the same problem simultaneously .",
    "thus , in addition to meeting memory constraints , iteration lets us leverage additional processing resources to reduce overall computation time .",
    "investing in additional hardware is often more economical than investment in software optimization .",
    "this is particularly relevant in scientific computing , where we are faced with a diverse , rapidly evolving set of unsolved problems , each requiring specialized software .",
    "the costs of investment in general purpose hardware are amortized over each problem , rather than paid each time for software optimization .",
    "this also relates to maintainability : optimization typically comes at a cost of increased code complexity .",
    "many types of summary and filter operations are cheap to implement in parallel because the data partitions can be processed independently .",
    "we call this type of operation _ embarrassingly _ parallel . for example",
    ", the counting of reads overlapping a gene does not depend on the counting for a different gene .",
    "given the complexity and scope of the data , analysts often rely on visual tools that display summaries and restricted views to communicate information at different scales and level of detail , from the whole genome to single nucleotide resolution .",
    "plot interactivity is always a useful feature when exploring data , and this is particularly true with big data .",
    "the view is always restricted in terms of its region and detail level , so , in order to gain a broader and deeper understanding of the data , the viewer will need to adjust the view , either by panning to a different region , zooming to see more details or adjusting the parameters of the summary step .",
    "the size of the genome and the range of scales make it infeasible to pre - render every possible view .",
    "thus , the views need to be generated dynamically , in lazy reaction to the user .",
    "performance is an important factor in interpretability : slow transitions distract the viewer and obfuscate relationships between views .",
    "dynamic generation requires fast underlying computations to load , filter and summarize the data , and fast rendering to display the processed data on the screen .",
    "this paper describes strategies to surmount computational and visualization challenges in the analysis of large genomic data and how they have been implemented in the r programming language by a number of packages from the bioconductor project @xcite .",
    "we will demonstrate their application to a real data set : the whole - genome sequencing of the hapmap cell line na12878 , the daughter in the ceu trio .",
    "the gatk project genotyped the sample according to their best practices and included the calls in their resource bundle , along with the alignments for chr20 , one of the shortest chromosomes .",
    "realistically , one would analyze the data for the entire genome , but the chr20 subset is still too large to be processed on a commodity laptop and thus is sufficient for our purposes .",
    "our ultimate goal is to process and summarize a large data set in its entirety , and iteration enables this by limiting the resource commitment at a given point in time . limiting resource consumption generalizes beyond iteration and is a fundamental technique for computing with big data . in many cases",
    ", it may render iteration unnecessary .",
    "two effective approaches for being frugal with data are restriction and compression .",
    "restriction means controlling which data are loaded and lets us avoid wasting resources on irrelevant or excessive data .",
    "compression helps by representing the same data with fewer resources .",
    "restriction is appropriate when the question of interest requires only a fraction of the available data .",
    "it is applicable in different ways to sequence vectors , range - based annotations and feature - by - sample matrices .",
    "we can restrict data along two dimensions : row / record - wise and/or column / attribute - wise , with genomic overlap being an important row filter .",
    "sequences and genomic vectors are relatively simple structures that are often restricted by range , that is , extraction of a contiguous subsequence of per - position values .",
    "row - wise restriction is useful when working with large sets of experimentally generated short sequences .",
    "the sequence aligner generates alignments as annotations on a reference sequence , and these alignments have many attributes , such as genomic position , score , gaps , sequence and sequence quality .",
    "restriction can exclude the irrelevant attributes .",
    "analysts often slice large matrices , such as those consisting of snp calls , by both row ( snp ) and column ( individual ) .",
    "a special mode of restriction is to randomly generate a selection of records .",
    "down - sampling can address many questions , especially during quality assessment and data exploration .",
    "for example , short reads are initially summarized in fastq files containing a plain text representation of base calls and corresponding quality scores .",
    "basic statistics of quality assessment such as the nucleotide count as a function of sequencing cycle or overall gc content are very well characterized by random samples of a million reads , which might be 1% of the data .",
    "this sample fits easily in memory .",
    "computations on this size of data are very nimble , enabling interactive exploration on commodity computers .",
    "an essential requirement is that the data represent a random sample .",
    "the _ shortread _ package is designed for the qa and exploratory analysis of the output from high - througput sequencing instruments .",
    "it defines the ` fastqsampler ` object , which draws random samples from fastq files .",
    "the sequence reads in our data set have been extracted into a fastq file from the publicly available alignments .",
    "we wish to check a few quality statistics before proceeding .",
    "we begin by loading a random sample of one million reads from the file : + with the sequences loaded , we can compute some qa statistics , like the overall base call tally : + in a complete workflow , we would generate an html qa report via the ` report ` function .",
    "an example of a situation where random sampling does _ not _ work is when prototyping a statistical method that depends on a significant amount of data to achieve reasonable power .",
    "variant calling is a specific example : restricting the number of reads would lead to less coverage , less power and less meaningful results .",
    "instead , we need to restrict the analysis to a particular region and include all of the reads falling within it .    to optimize range - based queries , we often sort and index our data structures by genomic coordinates .",
    "we should consider indexing an investment because an index is generally expensive to generate but cheap to query .",
    "the justification is that we will issue a sufficient number of queries to outweigh the initial generation cost .",
    "three primary file formats follow this pattern : bam , tabix and bigwig @xcite .",
    "each format is best suited for a particular type of data .",
    "the bam format is specially designed for sequence alignments and stores the complex alignment structure , as well as the aligned sequence .",
    "tabix is meant for indexing general range - based annotations stored in tabular text files , such as bed and gff .",
    "finally , bigwig is optimized for storing genome - length vectors , such as the coverage from a sequencing experiment .",
    "bam and tabix compress the primary data with block - wise gzip compression and save the index as a separate file .",
    "bigwig files are similarly compressed but are self - contained .",
    "the _ rsamtools _ package is an interface between r and the _ samtools _ library , which implements access to bam , tabix and other binary file formats .",
    "_ rsamtools _ enables restriction of bam queries through the ` scanbamparam ` object .",
    "this object can be used as an argument to all bam input functions , and enables restriction to particular fields of the bam file , to specific genomic regions of interest and to properties of the aligned reads ( e.g. , restricting input to paired - end alignments that form proper pairs ) .",
    "one common scenario in high - throughput sequencing is the calculation of statistics such as coverage ( the number of short sequence reads overlapping each nucleotide in the genome ) .",
    "the data required for this calculation usually come from very large bam files containing alignment coordinates ( including the alignment `` cigar '' ) , sequences and quality scores for tens of millions of short reads . only the smallest element of these data , the alignment coordinates ,",
    "is required for calculation of coverage . by restricting input to alignment",
    "coordinates , we transform the computational task from one of complicated memory management of large data to simple vectorized operations on in - memory objects .",
    "we can directly implement a coverage estimation by specifying a ` scanbamparam ` object that restricts to the alignment information . the underlying coverage calculation is implemented by the _ iranges _ package , which sits at the core of the bioconductor infrastructure and provides fundamental algorithms and data structures for manipulating and annotating ranges .",
    "it is extended by _",
    "genomicranges _ to add conveniences for manipulating ranges on the genome : +   + this is only an estimate , however , because we have ignored the complex structure of the alignments , for example , the insertions and deletions . _ rsamtools _ provides a convenience function for the more accurate calculation : +      some vectors , in particular , the coverage , have long stretches of repeated values , often zeroes .",
    "an efficient compression scheme for such cases is run - length encoding .",
    "each run of repeated values is reduced to two values : the length of the run and the repeated value .",
    "this scheme saves space and also reduces computation time by reducing computation size .",
    "for example , the vector @xmath0 would have run - values @xmath1 and run - lengths @xmath2 .",
    "the data have been reduced from a size of 8 to a size of 6 ( 3 values plus 3 lengths ) .",
    "the _ iranges _ ` rle ` class is a run - length encoded vector that supports the full r vector api on top of the compressed representation .",
    "operations on an ` rle ` gain efficiency by taking advantage of the compression . for example , the ` sum ` method computes a sum of the run values , using the run lengths as weights .",
    "thus , the time complexity is on the order of the number of runs , rather than the length of the vector .",
    "the ` cov ` object we generated in the previous section is a list of ` rle ` objects , one per chromosome . + for this whole - genome sequencing , the data are quite dense and complex , so the compression actually decreases efficiency .",
    "however , in the course of analysis we often end up with sparser data and thus better compression ratios . in this analysis , we are concerned about regions with extremely high coverage : these are often due to alignment artifacts .",
    "+ calculating the ` sum ` is then more efficient than with conventional vectors : + sometimes we are interested in the values of a genomic vector that fall within a set of genomic features .",
    "examples include the coverage values within a set of called chip - seq peaks or the conservation scores for a set of motif hits .",
    "we could extract the subvectors of interest into a list .",
    "however , large lists bring undesirable overhead , and the data would no longer be easily indexed by genomic position .",
    "instead , we combine the original vector with the ranges of interest . in _ iranges _",
    ", this is called a ` views ` object .",
    "there is an ` rleviews ` object for defining views on top of an ` rle ` .    to demonstrate , we ` slice ` our original coverage vector by our high coverage cutoff to yield the regions of high coverage , overlaid on the coverage itself , as an ` rleviews ` object : + this lets us efficiently calculate the average coverage in each region : + the _ biostrings _ package @xcite provides ` xstringviews ` for views on top of dna , rna and amino acid sequences .",
    "` xstring ` is a reference , rather than a value as is typical in r , so we can create multiple ` xstringviews ` objects without copying the underlying data .",
    "this is an application of the _ fly - weight _ design pattern : multiple objects decorate the same primary data structure , which is stored only once in memory .",
    "we can apply ` xstringviews ` for tabulating the nucleotides underlying the high coverage regions .",
    "first , we need to load the sequence for chr20 , via the _ bsgenome _ package and the addon package for human : + while the human genome consists of billions of bases , our genome object is tiny .",
    "this is an example of lazy loading : chromosomes are loaded , and cached , as requested . in our case , we restrict to chr20 and form the ` xstringviews ` .",
    "+ we verify that the cached sequence occupies the same memory as the subject of the views : + finally , we calculate and compare the nucleotide frequencies : + we notice that the high coverage regions are a / t - rich , which is characteristic of low complexity regions .      the high coverage regions in our data may be associated with the presence of repetitive elements that confuse the aligner .",
    "we obtain the repeat annotations from the ucsc genome browser with the _ rtracklayer _ package , which , in addition to a browser interface , handles input and output for various annotation file formats , including bigwig .",
    "our query for the repeats is restricted to chr20 , which saves download time .",
    "we subset to the simple and low complexity repeats , which are the most likely to be problematic : +   + our goal is to calculate the percent of each high coverage region covered by a repeat .",
    "first , we split the repeats according to overlap with a high coverage region : + the ` repeats.split ` object is not an ordinary list .",
    "long lists are expensive to construct , store and process . creating a new vector for each group requires time , and there is storage overhead for each vector .",
    "furthermore , data compression is less efficient when the data are split across objects .",
    "depending on the implementation of the list elements , these costs can be significant .",
    "this is particularly true of the s4 object system in r @xcite .",
    "another detriment to r lists is that list elements can be of mixed type .",
    "thus , there are few native routines for computing on lists . for example , the r ` sum ` function efficiently sums the elements of a homogeneous numeric vector , but there is no support for calling ` sum ` to calculate the sum of each numeric vector in a list . even if such routines did exist for native data types , there are custom data types , such as ranges , and we aim to facilitate grouping of any data that we can model as a vector .    while the r ` sum ` function is incapable of computing group sums , there is an oddly named function called ` rowsum ` that will efficiently compute them , given a numeric vector and a grouping factor .",
    "this hints that a more efficient approach to grouping may be to store the original vector along with a partitioning .",
    "the _ iranges _",
    "r package includes a ` compressedlist ` framework that follows this strategy .",
    "a ` compressedlist ` consists of the data vector , sorted by group , and a vector of indexes where each group ends in the data vector ( see figure  [ fig : last - part ] ) . _ iranges _ provides ` compressedlist ` implementations for native atomic vectors and other data types in the _ iranges _ infrastructure , and the framework is extensible to new data types .",
    "a ` compressedlist ` is homogeneous , so it is natural to define methods on subclasses to perform operations particular to a type of data .",
    "for example , there is a ` sum ` method for the ` numericlist ` class that delegates internally to ` rowsum ` .",
    "this approach bears similarity to storing data by columns : we improve storage efficiency by storing fewer objects , and we maintain the data in its most readily computable form .",
    "it is also an application of _ lazy _ computing , where we delay the partitioning of the data until a computation requires it .",
    "we are then in position to optimize the partitioning according to the specific requirements of the operation .    since ` repeats.list ` is a ` compressedlist ` , we can take advantage of these optimizations : + this value can be compared to the percent of chr20 covered : + instead of a ` compressedlist ` , we could have = 0.2em plus 0.05em minus 0.04em solved this problem using ` coverage ` and ` rleviews ` .",
    "the downside of compression is that there is overhead to explicit iteration because we need to extract a new vector with each step .",
    "the _ biostrings _ package has explored a solution .",
    "we can convert our ` xstringviews ` object ` chr20.views ` to a ` dnastringset ` that contains one ` dnastring ` for each view window . the data for each `",
    "dnastring ` has never been copied from the original chr20 sequence , and any operations on a ` dnastring ` operate directly on the shared data .",
    "while this solution may seem obvious , it relies heavily on native code and is far from the typical behavior of r data structures : + the nascent _ xvector _ package aims to do the same for other r data types , such as integer , double and logical values .",
    "iterative summarization of data may be modeled as three separate steps : split , apply and combine @xcite .",
    "the split step is typically the only one that depends on the size of the input data .",
    "the apply step operates on data of restricted size , and it should reduce the data to a scale that facilitates combination .",
    "thus , the most challenging step is the first : splitting the data into chunks small enough to meet resource constraints .",
    "two modes of splitting are particularly applicable to genomic data : sequential chunking and genomic partitioning .",
    "sequential chunking is a popular and general technique that simply loads records in fixed - count chunks , according to the order in which they are stored .",
    "genomic partitioning iterates over a disjoint set of ranges that cover the genome .",
    "typical partitioning schemes include one range per chromosome and sub - chromosomal ranges of some uniform size .",
    "efficient range - based iteration , whether over a partitioning or list of interesting regions , depends on data structures , file formats and algorithms that are optimized for range - based queries .    under the assumption that repeat regions are leading to anomalous alignments",
    ", we aim to filter from our bam file any alignments overlapping a repeat . as we will be performing many overlap queries against the repeat data set ,",
    "it is worth indexing it for faster queries .",
    "the algorithms for accessing bam , tabix and bigwig files are designed for genome browsers and have not been optimized for processing multiple queries in a batch .",
    "each query results in a new search .",
    "this is unnecessarily wasteful , at least when the query ranges are sorted , as is often the case .",
    "we could improve the algorithm by detecting whether the next range is in the same bin and , if so , continuing the search from the current position .",
    "the _ iranges _",
    "package identifies interval trees @xcite as an appropriate and well - understood data structure for range - based queries , and implements these using a combination of existing c libraries @xcite and new c source code .",
    "the query is sorted , and every new search begins at the current node , rather than at the root .",
    "we build a ` gintervaltree ` for the repeats : +   + the ` gintervaltree ` from _ genomicranges _ enables the same optimization when data are aligned to multiple chromosomes .",
    "to configure streaming , we specify a ` yieldsize ` when constructing the object representing our bam file .",
    "we will filter at the individual read level , but it should be noted that for paired - end data _ rsamtools _ supports streaming by read pair , such that members of the same pair are guaranteed to be in the same chunk . since",
    "bam files are typically sorted by position , not pair , this is a significant benefit : + to filter the bam , we first need to define a filter rule that excludes reads that overlap a repeat . the low - level _ rsamtools _ interface provides the read data as a ` dataframe ` , which we convert into a ` galignments ` object from the _ genomicalignments _ package , which provides data structures and utilities for analyzing read alignments : +   + since we are writing a new bam file , this is iteration with a side effect rather than a reduction .    to demonstrate reduction , we will calculate the coverage in an iterative fashion , which ends up identical to our original calculation : + choosing an appropriate yield size for each iteration is important .",
    "there is overhead to each iteration , mostly due to i / o and memory allocation , as well as the r evaluator .",
    "thus , one strategy is to increase the size of each iteration ( and reduce the number of iterations ) until the data fit comfortably in memory .",
    "it is relatively easy to estimate a workable yield size from the consumption of processing a single chunk .",
    "the ` gc ` function exposes the maximum amount of memory consumed by r between resets : + the memory usage started at about 500 mb and peaked at about 1200 mb , so the iteration consumed up to 700 mb . with 8  gb of ram , we might be able to process up to 10 million reads at once , assuming linear scaling .",
    "as an alternative to streaming over chunks , we can iterate over a partitioning of the genome or other domain .",
    "genomic partitioning can be preferable to streaming when we are only interested in certain regions .",
    "the ` tilegenome ` function is a convenience for generating a set of ranges that partition a genome .",
    "we rely on it to reimplement the ` coverage ` iterative calculation with a partitioning : + a caveat with partitioning is that since many query algorithms return ranges with any overlap of the query , care must be taken to intersect the results with each partition , so that reads are not double counted , for example .    by computing the coverage ,",
    "we have summarized the data .",
    "computing summaries is often time consuming , but since the summaries are smaller than the original data , it is feasible to store them for later use . caching the results of computations is an optimization technique known as _",
    "an analysis rarely follows a linear path . by caching the data at each stage of the analysis , as we proceed from the raw data to a feature - level summary , often with multiple rounds of feature annotation",
    ", we can avoid redundant computation when we inevitably backtrack and form branches .",
    "this is an application of _ incremental computing_. we export our coverage as a bigwig file , for later use : +      there are two basic modes of parallelism : data - level and task - level .",
    "embarrassingly parallel problems illustrate data parallelism .",
    "work flows might less frequently involve task parallelism , where different tasks are applied to the same data chunk . these are generally more challenging to implement , especially with r , which does not offer any special support for concurrency .",
    "the _ streamer _",
    "package has explored this direction .",
    "multicore and cluster computing are similar in that they are modular , and scaling algorithms to use multiple cores or multiple nodes can involve conceptually similar steps , but there are some critical differences . multiple cores in the same system",
    "share the same memory , as well as other resources . shared memory configurations offer fast inter - thread data transfer and communication",
    "however , the shared resources can quickly become exhausted and present a bottleneck . computing on",
    "a cluster involves significant additional expertise to access and manage cluster resources that are shared between multiple users and governed by a scheduler .",
    "interacting with a scheduler introduces an extra step into a workflow .",
    "we place jobs in a queue , and the jobs are executed asynchronously .",
    "another complication is that we need to share the data between every computer .",
    "a naive but often sufficient method is to store the data in a central location on a network file system and to distribute the data via the network .",
    "the network overhead implied by this approach may penalize performance .",
    "when the ratio of communication to computation time is large , communication dominates the overall calculation .",
    "the main strategies for addressing this are to ( a ) ensure each task represents a significant amount of work and ( b ) identify points where data sizes of inputs ( e.g. , file names ) and outputs ( e.g. , vector of counts across regions of interest ) are small .",
    "data partitioning is usually conveyed to workers indirectly , for example , via specification of the range of data to be processed , rather than inputting and explicitly partitioning data .",
    "this approach reduces the communication costs between the serial and parallel portions of the computation and avoids loading the entire data set into memory .",
    "the r packages _ foreach _",
    "@xcite , _ parallel _ ( distributed with r @xcite ) , _ pbdr _ @xcite and _ batchjobs _",
    "@xcite provide abstractions and implementations for executing tasks in parallel and support both the shared memory and cluster configurations .",
    "batchjobs and _ pbdr _ are primarily designed for asynchronous execution , where jobs are submitted to a scheduling system , and the user issues commands to query for job status and collect results upon completion . the other two , _ foreach _ and _ parallel _ , follow a synchronous model conducive to interactive use .    different use cases and hardware configurations benefit from different parallelization strategies .",
    "an analyst might apply multiple strategies in the course of an analysis .",
    "this has motivated the development of an abstraction oriented toward genomics workflows .",
    "the _ biocparallel _",
    "package defines this abstraction and implements it on top of _ batchjobs _ , _ parallel _ and _ foreach _ to support the most common configurations .",
    "an important feature of _ biocparallel _ is that it encapsulates the parallelization strategy in a parameter object that can be passed down the stack to infrastructure routines that implement the iteration .",
    "thus , for common use cases the user can take advantage of parallelism by solely indicating the appropriate implementation .",
    "iteration is carried out in a functional manner , so the api mirrors the * apply functions in base r : ` bplapply ` , ` bpmapply ` , etc .    to illustrate use of parallel iteration",
    ", we diagnose the gatk genotype calls introduced earlier .",
    "one approach is to generate our own set of nucleotide tallies , perform some simple filtering to yield a set of variant calls , and compare our findings to those from gatk .",
    "the set of nucleotide tallies is a more detailed form of the coverage that consists of the count of each nucleotide at each position , as well as some other per - position statistics .",
    "tallies are useful for detecting genetic variants through comparison to a reference sequence .",
    "the _ varianttools _ package provides a facility for summarizing the nucleotide counts from a bam file over a given range .",
    "we can iterate over the tiling in parallel using the ` bplapply ` function .",
    "the ` bpparam ` argument specifies the parallel implementation . `",
    "multicoreparam ` is appropriate for a multicore workstation , whereas we might use ` batchjobsparam ` for scheduling each iteration as a job on a cluster : +   + the above is an example of explicit iteration .",
    "thanks to the encapsulation and abstraction afforded by ` biocparallelparam ` , the ` pileupvariants ` function supports parallel iteration directly , so the implementation becomes much simpler : +   + this is an example of an embarrassingly parallel solution : each iteration is a simple counting exercise and is independent of the others .",
    "an example of a _ _ non__embarrassingly parallel algorithm is our demonstration of bam filtering : each iteration has the side effect of writing to the same file on disk .",
    "the increased complexity of coordinating the i / o across jobs undermines the value of parallelism in that case .",
    "graphics software is special in that it performs two roles : distilling information from the data and visually communicating that information to the user .",
    "the first role is similar to any data processing pipeline ; the unique aspect is the communication .",
    "the communication bandwidth of a plot is limited by the size and resolution of the display device and the perceptive capabilities of the user .",
    "these limitations become particularly acute in genomics , where it would be virtually impossible to communicate the details of a billion alignments along a genome of 3 billion nucleotides .    when considering how best to manage graphical resources , we recall the general technique of restriction .",
    "restriction has obvious applicability to genomic graphics : we can balance the size of the view and the level of detail .",
    "as we increase the size of the view , we must decrease the level of detail and vice versa .",
    "this means only so much information can be communicated in a single plot , so the user needs to view many plots in order to comprehend the data .",
    "it would be infeasible to iteratively generate every possible plot , so we need to lazily generate plots in response to user interaction .",
    "for example , the typical genome browser supports panning and zooming about the genome , displaying data at different levels of detail , depending on the size of the genomic region .",
    "when plotting data along a restricted range , graphics software can rely on the support for range - based queries presented previously .",
    "controlling the level of detail is more challenging because it relies on summaries . as the viewed region",
    "can be as large as the genome , generating summaries is often computationally intensive and introduces undesirable latency to plot updates .",
    "one solution to this problem is caching summaries at different levels of detail .",
    "global summaries will be regularly accessed and expensive to compute , and thus are worth caching , whereas the detailed data exposed upon drill - down can be computed lazily . this strategy",
    "is supported by the bigwig format .",
    "in addition to storing a full genomic vector , bigwig files also contain summary vectors , computed over a range of resolutions , according to the following statistics : mean , min , max and standard deviation . plotting the aggregate coverage is a shortcut that avoids pointless rendering of data that is beyond the display resolution and the perceptive abilities of the viewer : +    a good summary will guide the user to the most interesting parts of the data .",
    "genomic data are typically sparsely distributed along the genome , due to the nonuniform distribution of genes and experimental protocols that enrich for regions of interest .",
    "coverage is a particularly useful summary , as it helps guide the viewer to the regions with the most data .",
    "the following gets the average coverage for 800 windows ( perhaps appropriate for an 800 pixel plot ) .",
    "the result is shown in the top panel of figure  [ fig : cov ] : + in cases where a bigwig file or other cached summary is unavailable , we can rely on a heuristic that estimates the coverage from the index of a bam or tabix file .",
    "the index stores offsets into the bam for efficient range - based queries . instead of accessing the index to resolve queries",
    ", we calculate the difference in the file offsets for each range and derive a relative coverage estimate at a coarse level of resolution . in practice , this reduces the required time to compute the coverage from many minutes to a few seconds .",
    "when the plot resolution exceeds the resolution of the index , we again rely on the index to query the bam file for the reads that fall within the relatively small region and compute the coverage directly .",
    "a heuristic seems acceptable in this case , because improved accuracy is immediately accessible by zooming .",
    "this is in contrast to pure statistical computations , where crude estimates are less appreciated , even in the exploratory context , since resolution is not so readily forthcoming .",
    "the ` estimatecoverage ` function from the _ biovizbase _ package @xcite estimates the coverage from the bam index file .",
    "the bottom panel of figure  [ fig : cov ] shows the output of ` estimatecoverage ` for the example data set and allows for comparison with the more exact calculation derived from the bigwig file .",
    "the two results are quite similar and both required only a few seconds to compute on a commodity laptop .",
    "the design of interactive graphics software typically follows the model  view  controller pattern ( see figure  [ fig : mvc - genomics ] ) .",
    "the view renders data retrieved from the data model , and the controller is the interface through which the user manipulates the view and data model .",
    "the data model abstracts the underlying data source , which might be memory , disk or a dynamic computation .",
    "the abstraction supports the implementation of complex optimizations without exposing any of the complexity to client code .",
    "data is communicated to the user through the view , and user input is received through the controller .",
    "a complex application will consist of multiple interactive views , linked through a common data model , itself composed of multiple modules , chained together as stages in a pipeline .",
    "the viewer , plots and pipeline stages are interlinked to form a network .",
    "a simple data model abstracts access to the primary data , such as an in - memory granges object of transcript annotations or a bam file on disk .",
    "we can extend the simple model to one that dynamically computes on data as they are requested by the application .",
    "this is an example of lazy computing .",
    "each operation is encapsulated into a data model that proxies an underlying model .",
    "the proxy models form a chain , leading from the raw data to the processed data that are ready for plotting @xcite .",
    "dynamic computation avoids unnecessarily processing the entire genome when the user is only interested in a few small regions , especially when the parameters of the transformations frequently change during the session .",
    "the data may be cached as they are computed , and the pipeline might also anticipate future requests ; for example , it might prepare the data on either side of the currently viewed region , in anticipation of scrolling .",
    "caching and prediction are examples of complex optimizations that are hidden by the data model .",
    "the _ plumbr _",
    "r package @xcite provides a proxy model framework for implementing these types of ideas behind the data frame api .",
    "we have been experimenting with extending these approaches to genomic data .",
    "the _ biovizbase _ package implements a graphics - friendly api for restricted queries to bioconductor - supported data sources . the _ ggbio _",
    "package builds on _ biovizbase _ to support genomic plot objects that are regenerated as the user adjusts the viewport .    to diagnose the gatk genotype calls",
    ", we combine the reference sequence , nucleotide pileup and the genotype calls .",
    "the result is shown in figure  [ fig : tracks ] .",
    "the _ ggbio _",
    "package produced the plot by relying on restricted query support in _",
    "biovizbase_. we have already introduced the extraction of genomic sequence and the calculation of nucleotide pileups .",
    "the genotypes were drawn by the _",
    "package from a variant call format ( vcf , @xcite ) file with a range - based index provided by tabix .    to generate the plot , we first select the region of interest : + next , we construct the plot object and render it : +   +",
    "since the plot object is a logical representation of the plot , that is , it references the original data , we can adjust various aspects of it and generate a new rendering .",
    "in particular , we can change the currently viewed region , and the data for the new region are processed dynamically to generate the new plot . in this example",
    ", we zoom out to a larger region around the first region : + current work is focused on the _ mutableranges _ package , which generalizes and formalizes the designs in _ biovizbase_. it defines dynamic versions of the _ genomicranges _ data structures , for example , there is a _ dynamicgranges _ that implements the _ granges _ api on top of a bam file .",
    "only the requested regions are loaded , and they are optionally cached for future queries .",
    "a _ proxygranges _ performs dynamic computations based on another _",
    "granges_. this will enable a new generation of interactive genomic visualization tools in r. an early adopter is _ epivizr _ , the r interface to the web - based _ epiviz _ , a web - based genome browser with support for general interactive graphics , including scatterplots and histograms .",
    "we have introduced software and techniques for and plotting big genomic data .",
    "the bioconductor project distributes the software as a number of different r packages , including _ rsamtools _ , _ iranges _ , _ genomicranges _ , _ genomicalignments_,_biostrings _ , _ rtracklayer _ , _ biovizbase _ and _ biocparallel_. the software enables the analyst to conserve computational resources , iteratively generate summaries and visualize data at arbitrary levels of detail .",
    "these advances have helped to ensure that r and bioconductor remain relevant in the age of high - throughput sequencing .",
    "we plan to continue in this direction by designing and implementing abstractions that enable user code to be agnostic to the mode of data storage , whether it be memory , files or databases .",
    "this will bring much needed agility to resource allocation and will enable the user to be more resourceful , without the burden of increased complexity .",
    "supported in part by the national human genome research institute of the national institutes of health ( u41hg004059 to m. m. ) and the national science foundation ( 1247813 to m. m. ) ."
  ],
  "abstract_text": [
    "<S> this paper reviews strategies for solving problems encountered when analyzing large genomic data sets and describes the implementation of those strategies in r by packages from the bioconductor project . </S>",
    "<S> we treat the scalable processing , summarization and visualization of big genomic data . </S>",
    "<S> the general ideas are well established and include restrictive queries , compression , iteration and parallel computing . </S>",
    "<S> we demonstrate the strategies by applying bioconductor packages to the detection and analysis of genetic variants from a whole genome sequencing experiment . </S>"
  ]
}