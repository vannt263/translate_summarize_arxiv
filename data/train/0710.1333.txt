{
  "article_text": [
    "recognition memory is supported by at least two different types of retrieval processes : recollection and familiarity .",
    "while recollection requires detailed information about an experienced event , familiarity just distinguishes whether or not the stimulus was previously encountered .",
    "a well known example is the encounter with a colleague during a conference : one might recognize the person , but fail to remember the time and place of an earlier meeting .",
    "familiarity memory is thought to have a very large capacity . in the early 1970s , standing and collaborators @xcite tested the capacity in humans by presenting participants with a large number ( 10,000 ) of images .",
    "surprisingly , after this one - shot learning , participants were able to successfully recognize most of the previously seen pictures , suggesting that the capacity of recognition memory for pictures is very large indeed .",
    "experimental psychologists have formulated _ dual - process _ theories which characterize the precise contribution of familiarity and recollection to recognition memory , for a review see @xcite .",
    "anatomically , researchers have proposed that different brain areas are engaged during recollection and familiarity processing .",
    "single item familiarity is believed to be processed in the perirhinal cortex , whereas recollection is believed to engage the hippocampus , for a review see @xcite .",
    "furthermore , electro - physiological studies using single cell recordings in monkeys and rats @xcite report that about @xmath0 percent of neurons in the perirhinal cortex show increased activity after presenting new compared to old stimuli .",
    "these neurons have been interpreted as novelty detectors . however , this association between the memory processes and brain area is still unclear and seems to depend on the nature of the stimulus @xcite .",
    "recent neuroimaging studies using , for example , event - related potentials ( erps ) @xcite , have revealed that familiarity and recollection have distinct temporal characteristics .",
    "familiarity is linked to a frontal erp modulation that occurs around @xmath1-@xmath2ms post - stimulus presentation , whilst recollection is thought to evoke a parietal erp modulation around @xmath2-@xmath3ms after stimulus presentation @xcite .",
    "therefore , the speed of processing of familiarity discrimination is more rapid than recollection .",
    "behavioral experiments provide further evidence for this : if only very limited time is available for a recognition decision , participants rely primarily on familiarity as opposed to recollection processes @xcite .    in the field of computational neuroscience ,",
    "modeling of recollection via attractor neural networks has a long history using auto - associator hopfield networks @xcite .",
    "familiarity discrimination has only been studied much more recently @xcite .",
    "computational models of familiarity discrimination have a much higher storage capacity for recognition than associative memory networks that perform associative recall . for a wide range of conditions , bogacz et al .",
    "showed that the maximum storage is proportional to the number of synapses within the network @xcite .",
    "this is much larger than the capacity to recall , which is proportional to the square root of the number of synapses ( i.e.  the number of neurons in a fully connected network ) @xcite .",
    "intuitively this is easily understood ; familiarity needs to store just a single bit ( familiar versus non - familiar ) per pattern , whereas to recall an event requires the retrieval of the whole pattern ( pattern completion ) .    in this paper",
    "we study how the dynamics of the network affects familiarity discrimination .",
    "we compare two different familiarity discriminators : familiarity based on energy , fame , which was previously introduced by bogacz et al @xcite , and a familiarity discriminator which is the time derivative of fame @xcite . from here on , we will call this latter discriminator the slope , and label it fams .",
    "we show in our model how the signal for both familiarity discriminators decays very quickly after stimulus presentation , in concordance with the results from neuroimaging @xcite .",
    "in addition , to investigate the robustness of familiarity detection , we study how it is affected by random fluctuations that are ubiquitously present in the nervous system . as in models of attractor neural networks",
    "@xcite , this external source of noise is taken to be independent of the learned patterns and is controlled by a",
    "_ temperature _",
    "we consider a network of @xmath4 binary neurons , each with an activity @xmath5 , the two states corresponding respectively to firing and not firing .",
    "the complete network activity is characterized by @xmath6 .",
    "any two neurons are connected by synaptic weights @xmath7 . as standard in artificial network models ,",
    "the network has a learning phase in which it encodes @xmath8 stimuli @xmath9 ( @xmath10 ) in its weights using a hebbian learning rule @xmath11 it can be shown that this learning rule is optimal in the limit of large @xmath12 ( unpublished results ) . during the subsequent test phase",
    ", the network s performance is evaluated . at @xmath13 , the probe stimulus @xmath14 ( which is either a familiar or novel stimulus )",
    "is loaded into the network , @xmath15 .    to define the network dynamics we assume that each neuron is updated precisely once , probabilistically and asynchronously , in each unit of time .",
    "( the exact duration that a time unit in the model corresponds to is hard to extract by comparing the model to , say , erp data given the additional delays present in the biology , but it should probably be on the order of 10 .. 100ms . ) as standard in artificial neural networks , and in analogy with magnetic systems in physics , the random fluctuations are controlled by a temperature parameter @xmath16 .",
    "these so - called glauber dynamics have been extensively studied in many different stochastic systems , for instance @xcite .",
    "the probability distribution , after update , is given then by @xmath17},\\label{tr}\\end{aligned}\\ ] ] where @xmath18 is the inverse temperature parameter , and @xmath19 is the total presynaptic current to the neuron @xmath20 .",
    "accordingly , for low temperature , the noise is small and there is a strong positive correlation between the input current @xmath21 and the output @xmath22 , whilst for high temperature the output of a node is dominated by noise and is more or less independent of its input .    the energy in the network at time @xmath23 is defined as @xmath24 as was previously reported in @xcite , the energy @xmath25 is able to effectively discriminate between old and novel stimuli . as we explain later ,",
    "this energy is of order @xmath26 for learned stimuli , but of order @xmath27 for novel stimuli .",
    "consequently , the energy or familiarity for old and novel stimuli are macroscopically different ( they differ by order @xmath4 , while the std.dev.@xmath28 ) and the difference can thus be used as a familiarity discriminator .",
    "we call this discriminator fame",
    ".    however , the use of the energy is only one possible approach to model familiarity discrimination .",
    "the time derivative , or slope , of the energy @xmath29 can also be used as a familiarity discriminator .",
    "it indicates how quickly the network s energy changes , when either a novel or old stimulus is presented .",
    "interestingly , this familiarity measure was originally proposed by hopfield in his seminal 1982 paper @xcite , but to the best of our knowledge it has never received further exploration .",
    "we call this discriminator fams .    for convenience",
    ", we shall express the energy and the slope as functions of the @xmath8-dimensional vector @xmath30 , the overlaps between the current network activity and each of the stored patterns .",
    "the components of this overlap vector are defined by @xmath31 assuming the hebbian learning rule ( [ ws ] ) , the energy ( [ e ] ) in terms of the overlaps is given by @xmath32^{2},\\label{em}\\ ] ] whilst the slope ( first derivative of the energy ) is given by @xmath33 and is thus proportional to the time derivative @xmath34 of the overlaps .",
    "to mathematically address the network dynamics we assume the mean field approximation , i.e.  @xmath35 . under",
    "this approximation one obtains from equation ( [ tr ] ) , the dynamical equations for the overlaps ( [ ms ] ) : @xmath36.\\label{dms}\\ ] ] the mean field formulation provides an accurate description of the dynamics of the system provided the temperature is not too high ( see below ) .    knowing the dynamics , we focus on the time evolution of the two discriminators , energy and slope , defined in the previous section . to measure the temporal persistence , figure [ fig1 ] illustrates the time evolution of fame and fams when tested with novel or old stimuli .",
    "we compare the time evolution by simulations with glauber dynamics given by equation ( [ tr ] ) , and by using the mean field dynamical equations ( [ dms ] ) .    _",
    "( figure 1 here ) _    figure [ fig1 ] a and b , shows how the energy associated with old stimuli is much lower than for new stimuli .",
    "however , after a short transient of @xmath37-@xmath38 units of time , both signals become similar to each other , i.e.  familiarity discrimination based on energy deteriorates rapidly post stimulus presentation .    like the energy ,",
    "the slope also shows a transient signal when the network is presented with a novel vs old stimulus , figure [ fig1 ] graphs c and d. for low temperature , the slope for old stimuli is practically zero .",
    "this can be easily interpreted .",
    "an old stimulus corresponds to one of the local minima ( attractors ) of the energy landscape . because the temperature is low , and therefore the system is not receiving any external perturbation , the energy does not change , and its time derivative is practically zero .",
    "similar to the energy , the slopes associated with old and new stimuli show significant differences immediately after stimulus presentation , but this difference diminishes shortly thereafter .    summarizing , both discriminators can distinguish old from new stimuli immediately after stimulus presentation , but after a very short transient ( of the order of five time units ) , the discrimnation ability disappears .",
    "the slope tends to zero as time progresses because the network evolves towards a fixed point and becomes stationary ( i.e.  @xmath39 ) .",
    "though measures to discriminate spurious from non - spurious attractor states have been proposed @xcite , such measures do not directly translate into a discrimination between old and novel stimuli .",
    "to examine the performance of the two familiarity discriminators introduced in the previous section , we quantify the discriminability between the network responses to either new or old stimuli by the signal - to - noise ratio ( snr ) .",
    "assuming two gaussian probability distributions , @xmath40 $ ] and @xmath41 $ ] , associated with new and old stimuli , we define @xmath42 to check that the distributions are indeed gaussian , we repeated the simulation of fig .",
    "[ fig1 ] @xmath43 times and computed the probability distributions . for both fame and fams the 4th moments of their distribution satisfied @xmath44 , with a relative error smaller than @xmath45 , ( where @xmath46 denotes the mean and @xmath47 the variance ) , indicating that the distributions are well approximated by gaussians .",
    "we address here how random fluctuations in neural activity ( independent of the learned patterns ) affect the performance of the familiarity discriminators .",
    "we study the effect of temperature at two different time points , @xmath13 and @xmath48 .",
    "as stated above , time is defined such that in one unit , all neurons are asynchronously updated once .",
    "the choice of @xmath48 is not special ; we just study the network properties at this time to gain understanding as to how the network evolves .",
    "the results are illustrated in figure [ fig2 ] .",
    "immediately after stimulus ( @xmath13 ) , we observe that fame is independent of the temperature value ( figure [ fig2].a ) , whilst fams has a non - linear dependence on the temperature ( figure [ fig2].c ) . for high temperature , fams performs better as a familiarity discriminator .",
    "this finding can be intuitively understood .",
    "the energy and its time derivative can be separated into signal and noise contributions .",
    "the signal for the slope is proportional to the rate of change of the energy , and therefore proportional to the rate of change of the overlap between the network activity and the stimulus . at low temperatures ,",
    "the signal associated with an old stimulus is very low as the overlap with the stimulus is almost invariant .",
    "contrarily , at higher temperature , the overlap with old stimuli changes very quickly ; it decays from @xmath49 to @xmath50 , and consequently the slope - signal relationship increases considerably ( the higher temperature , the higher signal for fams ) .",
    "the noise component for the slope , although dependent on @xmath16 , is similar for both old and novel stimuli . as a result",
    "the main temperature dependence stems from the signal term . in figure",
    "[ fig2 ] , the case of @xmath51 is not explicitely studied because in this region the network can not retrieved any of the learned patterns , i.e. the only stable solution is @xmath52 , what is so - called _ paramagnetic _ or _ non - memory _",
    "solution @xcite .    _",
    "( figure 2 here ) _",
    "in contrast to time @xmath13 , at time @xmath48 , post stimulus presentation , both discriminators fame and fams show a similar breakdown in discrimination for increased temperature ( figure [ fig2].f ) . in the next section",
    ", we analytically study the maximum storage capacity for both fame and fams at time @xmath13 .",
    "the results are in agreement with the simulations . for",
    "@xmath48 , the mean field predictions , however , do not reproduce the network simulations . to study such situations ( which we do not explicitly deal with here )",
    ", one would need to use other techniques , for example , generating functional analysis @xcite .",
    "when the number of stimuli encoded in the weights increases , the snr decreases .",
    "one can define the storage capacity ( or maximum number of stimuli encoded in the learning rule and successfully discriminated ) as the point where the snr drops below one .",
    "this gives the maximum number of stimuli @xmath53 that can be encoded in the network . in this section",
    "we present explicit calculations for both discriminators fame and fams for time @xmath13 .",
    "let @xmath54 label an old stimulus presented to the network .",
    "as is common in these calculations @xcite , we separate the sum appearing in equation ( [ e ] ) into a signal ( @xmath54 ) plus noise contributions .",
    "the latter is determined by interference from previously stored stimuli ( @xmath55 ) . from equation ( [ em ] ) it follows that the energy associated with old stimuli is distributed as @xmath56^{2}-n\\sum_{\\rho\\neq\\hat{\\rho}}[m^{\\rho}(t)]^{2}.\\ ] ] the first term on the right hand side is the signal and the second one the noise contribution . at @xmath13 , we obtain @xmath57 because the pattern @xmath14 was an old stimulus . as for large @xmath4",
    "the central limit theorem applies , the overlaps with the other patterns @xmath55 have a gaussian distribution with @xmath50 average and variance @xmath58 @xcite .",
    "accordingly , we can easily compute the expected value and the variance for the energy . using that the sum of two gaussian distributed variables is again a gaussian distribution , @xmath59.\\label{eoldt0}\\ ] ] analogously ,",
    "the energy for novel stimuli is distributed as @xmath60.\\label{enewt0}\\ ] ] from equation ( [ snr ] ) we obtain @xmath61 , in agreement with the simulations ( see figure [ fig2].e ) .",
    "equivalently , the maximum storage capacity , ( the @xmath8 for which @xmath62 ) , is given by @xmath63=\\frac{n^{2}}{2},\\label{mmaxet0}\\ ] ] and thus the storage is of order @xmath64 , which has been reported in previous computational models using fame @xcite .      following the same strategy applied to fame to fams , we are able to separate signal ( @xmath54 ) and noise ( @xmath55 ) terms for the slope . at the instant of the stimulus presentation ( @xmath13 ) , we substitute equation ( [ dms ] ) in equation ( [ s ] ) .",
    "next , we apply the central limit theorem , which is a good approximation for large @xmath4 .",
    "it ensures that the sum over the different sites @xmath20 of the noise contribution @xmath65 appearing inside the @xmath66 function , is equivalent to the average over a gaussian noise with mean @xmath50 and variance @xmath67 , the network _",
    "load_. using these considerations , it is straightforward to obtain @xmath68,\\label{soldt0}\\ ] ] and for novel stimuli @xmath69.\\label{snewt0}\\ ] ] the integrals @xmath70 , @xmath71 and @xmath72 appearing in equations ( [ soldt0 ] ) and ( [ snewt0 ] ) are @xmath73 where @xmath18 is the inverse temperature . from equations ( [ soldt0 ] ) and ( [ snewt0 ] )",
    "it follows that @xmath74 $ ] , which can be computed numerically .",
    "the results are represented in figure [ fig2].e .",
    "the expected values used in the signal - to - noise ratio calculation ( figure [ fig2].c ) fits well with the simulations .",
    "however , the theoretical predictions for the variance of both fams(old ) and fams(new ) equals @xmath75 , independent of temperature , which is in disagreement with the simulations .",
    "therefore , the signal - to - noise ratio calculation disagrees with simulations for high temperatures ( figure [ fig2].e ) . see the appendix for a more detailed calculation of how the mean field prediction is affected by high temperatures .    _",
    "( figure 3 here ) _    the maximum storage for fams is again obtained by solving snr@xmath76 , which yields @xmath77=\\frac{n^{2}}{2}(1-i_{1}(\\alpha_{\\mathrm{max}},\\beta)-i_{2}(\\alpha_{\\mathrm{max}},\\beta)+i_{3}(\\alpha_{\\mathrm{max}},\\beta))^{2}.\\label{mmaxst0}\\ ] ] because the integrals @xmath78 , @xmath79 and @xmath80 depend on @xmath8 , this expression does not give us @xmath53 explicitly .",
    "the dependence on @xmath4 is more complicated than for other computational models of familiarity discrimination @xcite , ( and in particular for fame above ) , for which the maximum storage capacity is directly proportional to @xmath64 .",
    "interestingly , @xmath53 for fams at @xmath13 is dependent on the temperature , whilst fame is completely independent of temperature ( recall figure [ fig2 ] , graphs a and c ) .    in the two limits @xmath81 and @xmath82 we can perform the integrals in equation ( [ mmaxst0 ] ) to obtain @xmath53 explicitly .",
    "for @xmath81 , the integrals ( [ i1i2i3 ] ) can be computed using @xmath83\\right ) & = & \\mathrm{erf}\\left(\\frac{b}{\\sqrt{2}a}\\right),\\nonumber \\\\ \\lim_{\\beta\\to\\infty}\\int\\frac{\\mathrm{d}z}{\\sqrt{2\\pi}}\\exp\\left(-z^{2}/2\\right)\\tanh\\left(\\beta\\left[az+b\\right]\\right)z & = & \\sqrt{\\frac{2}{\\pi}}\\exp\\left(-\\frac{b^{2}}{2a^{2}}\\right),\\label{appt0}\\end{aligned}\\ ] ] giving @xmath84 , @xmath85 and @xmath86 . here",
    ", @xmath87 is the error function @xmath88 .",
    "therefore at @xmath81 equation ( [ mmaxst0 ] ) becomes @xmath89\\right)^{2}.\\label{mst0}\\ ] ] in figure [ fig3 ] we plot , as a function of @xmath4 , the ratio of the initial zero temperature storage for fams and fame .",
    "we see that although fams performs slightly worse than fame , both storage capacities grow proportional to @xmath64 . by way of example , for @xmath90 , we see @xmath91\\approx96\\% m_{\\mathrm{max}}[\\mathrm{fame},t=0]$ ] , i.e.  the capacities are almost identical .    in the other limit",
    "that @xmath82 , random fluctuations in neural activity dominate the network dynamics .",
    "all the integrals of ( [ i1i2i3 ] ) are zero , and hence @xmath92\\approx m_{\\mathrm{max}}[\\mathrm{fame},t=0]$ ] .",
    "that is , in this limit , the theoretical maximum storage is the same for both fams and fame , and is independent of @xmath16 .",
    "familiarity describes a retrieval process that supports recognition memory by providing a feeling that something has been encountered before .",
    "numerous empirical studies have investigated familiarity processes in humans @xcite and non - humans @xcite .",
    "recently , some neuronal networks modeling familiarity discrimination have also been proposed @xcite .",
    "however , no computational work has addressed the dynamics of familiarity discrimination , which is relevant when comparing these models to experiments .",
    "furthermore , we have studied how noise affects the familiarity performance .",
    "we have compared the energy discriminator ( fame ) used by @xcite to its time derivative , the slope ( fams ) .",
    "interestingly , the fams discriminator was already suggested by hopfield in his seminal work @xcite .",
    "an interesting consequence is that the original hopfield model can be used to model both recollection ( stationary properties of the retrieval dynamics ) and familiarity ( transient dynamics after the stimulus presentation ) .",
    "the slope discriminator ( fams ) is affected by the temporal dependency of the energy discriminator ( fame ) .",
    "in other words , the slope discriminator captures the fact that the speed of discrimination is predictive for the discrimination outcome per se .    for both discriminators",
    "the familiarity signals decay quickly after stimulus presentation and are detectable only for a short period of time .",
    "this can be compared to the speed of recollection . assuming that recollection memories correspond to attractors in the hopfield model , recollection information only becomes available once the attractor state is reached . by that time , the slope is zero , and the energy difference is very small .",
    "thus the experimentally observed timing difference of familiarity and recollection follows naturally from our model .",
    "the storage capacity of these familiarity discriminators is much larger ( proportional to @xmath64 ) compared to recollection ( proportional to @xmath4 ) , we demonstrated that this capacity is dependent on the _",
    "temperature_. we have presented a detailed derivation of the maximum storage immediately after stimulus presentation ( @xmath13 ) .",
    "we have shown that for low temperature , the storage capacity related to fams is lower than that for fame , but still scales with the number of synapses , e.g.  for @xmath90 , the slope gives a storage capacity @xmath93 as good as the energy . for high temperatures",
    "the difference between the storage capacities of fams and fame is negligible ( the storage capacity for both is the approximately , @xmath94 ) .    interestingly , this means that the performance of fams improves as one goes to the high temperature regime , a fact which is a priori counterintuitive , especially given how the temperature affects _ recollection _ in hopfield nets @xcite , i.e.  the higher the temperature , the worse the recollection performance .",
    "however , after some time steps , our simulations ( figure [ fig2].f ) show that , for both fame and fams , high noise levels produce a stochastic disruption of the discrimination , decreasing the snr and the performance of familiarity , a concurrence with the dynamics of recollection .",
    "the authors acknowledge rafal bogacz ( univ .",
    "bristol ) and david donaldson ( univ .",
    "stirling ) for helpful discussions and financial support from epsrc ( project ref .",
    "ep / co 10841/1 ) , hfsp ( project ref .",
    "rgp0041/2006 ) and from the doctoral training center in neuroinformatics at the university of edinburgh .      to compute the slope in equation ( [ s ] ) , we need an analytic expression for @xmath95 , or equivalently , given the definition ( [ ms ] ) , we have to compute the derivative @xmath96 , which is governed by the glauber dynamics given by ( [ tr ] ) , see @xcite for more detailed situations",
    ". given @xmath97 , the glauber dynamics give an uncertainty in @xmath98 , such that @xmath99=\\mathrm{sech}^{2}(\\beta h_{i}(t))\\,,\\ ] ] which implies @xmath100 we use this result to find the error induced in our calculation of @xmath101 .",
    "when a new pattern is presented , the @xmath102 are all of order @xmath103 .",
    "this implies that the local fields , defined as @xmath104 , are of order @xmath105 .",
    "hence , by equations ( [ ms ] ) and ( [ err ] ) , the error in our calculation of @xmath95 is given by @xmath106 for each @xmath107 . finally , by ( [ s ] ) , we conclude that @xmath108 since @xmath109 decays exponentially with large @xmath110 , but is of order 1 for small @xmath110 , the error in our calculation of @xmath101 , coming from the mean field approximation , is only going to be negligible in the limit in which @xmath111 is large .",
    "this explains why there is a growing discrepancy between theory and simulation as the temperature @xmath16 is increased ( see figure [ fig2].e ) .",
    "brown , f.a.w .",
    "wilson , and i.p .",
    "neuronal evidence that inferomedial temporal cortex is more important than hippocampus in certain processes underlying recognition memory .",
    "_ brain res _ , 409:0 158162 , 1987 .",
    "a.  greve , m.c.w .",
    "van rossum , and d.i .",
    "donaldson . investigating the functional interaction between semantic and episodic memory : convergent behavioral and electrophysiological evidence for the role of familiarity .",
    "_ neuroimage _ , 340 ( 2):0 801814 , jan 2007 ."
  ],
  "abstract_text": [
    "<S> when one is presented with an item or a face , one can sometimes have a sense of recognition without being able to recall where or when one has encountered it before . </S>",
    "<S> this sense of recognition is known as familiarity . following previous computational models of familiarity memory we investigate the dynamical properties of familiarity discrimination , and contrast two different familiarity discriminators : one based on the energy of the neural network , and the other based on the time derivative of the energy . </S>",
    "<S> we show how the familiarity signal decays after a stimulus is presented , and examine the robustness of the familiarity discriminator in the presence of random fluctuations in neural activity . for both discriminators </S>",
    "<S> we establish , via a combined method of signal - to - noise ratio and mean field analysis , how the maximum number of successfully discriminated stimuli depends on the noise level .    </S>",
    "<S> keywords : recognition memory , familiarity discrimination , storage capacity .    </S>",
    "<S> abbreviations : snr , signal - to - noise ratio ; fame , familiarity discrimination based on energy ; fams , familiarity discrimination based on slope . </S>"
  ]
}