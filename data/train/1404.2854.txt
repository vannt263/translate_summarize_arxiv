{
  "article_text": [
    "the fisher information matrix or simply fisher matrix has become one of the most widely used statistical tools for forecasting the errors in parameter estimation problems .",
    "it provides lower limits on the variances , and the expected covariances of estimates of model parameters from maximum likelihood , or maximum posterior , techniques , for a given experimental design . .",
    "for data pairs @xmath3 with no errors in @xmath4 , the problem was solved many years ago @xcite .",
    "the main value of the fisher matrix technique is in being able to obtain error forecasts without any data , real or simulated , and is generally much faster than computing full posterior distributions with simulations @xcite .",
    "it is however only a first step , as it assumes the posteriors are well described by multivariate gaussian distributions , and this may not hold ( e.g. , * ? ? ?",
    "* ) , when more sophisticated analysis may be required , but it is still a very valuable tool for experimental design .",
    "furthermore , more sophisticated forecasts for likelihood surfaces which are non - gaussian in the parameter space now exist @xcite .    from the initial derivations of the fisher matrix in the cosmological context @xcite ,",
    "we have arrived today at very mature applications and implementations ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "the fisher matrix has been useful in proposals and projections for surveys , such as for the cosmic microwave background @xcite , spectroscopic galaxy surveys @xcite , the dark energy survey @xcite , large - scale structure @xcite , and in the broader discussion of the investigation of dark energy @xcite and estimation of neutrino masses with the future european space agency euclid mission @xcite .    for the purposes of review and",
    "later reference in this work , we summarise the basic fisher matrix formalism .",
    "we begin with the likelihood of a set of data , @xmath5 given ( or conditional upon ) a set of model parameters , represented by a vector @xmath6 : @xmath7 . in the simplest case",
    ", @xmath8 represents only the ordinates , @xmath1 . later in the paper",
    ", we will take it to be the union of the ordinates and any other measured quantities on which @xmath1 depends , such as abscissa values , and which may be subject to error . in practice what is typically required",
    "is the posterior distribution of @xmath6 , given the data @xmath5 .",
    "assuming an uninformative prior on the parameters , @xmath9 constant , bayes theorem implies @xmath10 , the likelihood .",
    "the log - likelihood is then taylor - expanded about its maximum .",
    "the first term is a constant , irrelevant for the discussion of parameter constraint forecasts ; the second term is the first derivative , which vanishes at the point of maximum likelihood ; the third term is the hessian ( curvature matrix ) of the likelihood , and is the term whose ensemble average ( over the data ) gives the fisher matrix : @xmath11 where @xmath12 and @xmath13 label the parameters . for the case of a gaussian likelihood",
    ", this is analytically computable , and can depend only on the expectation values of the data , @xmath14 , and the covariance , @xmath15 .",
    "this results in the following form for the fisher matrix @xcite . @xmath16 . \\label{eqn :",
    "originalfm}\\ ] ] an early example of dealing with errors in both variables was straight - line fitting , where both the statistics and astronomy communities used either _ ad hoc _ choices for the axis , or ultimately arbitrary combinations e.g. , the bisector or the average of the one - dimensional fits on either axis .",
    "the evolution to two - dimensional or joint - distribution fitting was accompanied by a slow transition to the bayesian perspective @xcite .",
    "new tools for fitting data in the presence of two - dimensional errors have been developed and used to extract improved cosmological constraints from supernovae populations @xcite . here",
    ", we develop the application of two - dimensional errors in the predictive fisher matrix formalism itself , . for pedagogical discussions of straight - line fitting and bayesian approaches to fitting ,",
    "see for example @xcite .",
    "the remainder of the paper is organized as follows :  [ sec : formalism ] describes the formal derivation of the generalized fisher matrix for the case of dependence of @xmath1 on an arbitrary set of gaussian - distributed variables @xmath2 ;  [ sec : examples ] describes an application of this formalism to a particular experiment , with tests on simulated data .",
    "we present conclusions in  [ sec : conclusion ] . .",
    "throughout this paper , we follow the formalism and notation of @xcite . in this method , we use a taylor expansion of the log - likelihood , and derive the generalised fisher matrix from first principles .",
    "the general aim is to find an expression for the fisher matrix for an experiment with gaussian errors in @xmath2 and @xmath1 , arbitrary correlations of errors ( i.e. errors in @xmath17 can be correlated with errors in @xmath18 , for any @xmath19 ) .",
    "let the set of measurements be @xmath21 , with @xmath22 and @xmath23 . in the simplest case , @xmath24 and",
    "the dataset is a set of ( @xmath25 ) data pairs , but this is not necessary ; all that is required is that there a model which returns the expectation value of @xmath1 as a function of @xmath2 , and which in general will depend also on some model parameters , represented collectively by @xmath6 , being a vector @xmath26 with @xmath27 .",
    "it is the posterior probability of @xmath6 which we wish to calculate .",
    "we give an example later .",
    "we assume @xmath2 and @xmath1 have gaussian errors , around true values @xmath28 , @xmath29 , with a covariance matrix @xmath30 . @xmath28 and @xmath29 are not directly observed .",
    "this amounts to a hierarchical model , where the observables @xmath31 depend on some unobservable latent variables @xmath28 , which are essentially nuisance parameters .",
    "the @xmath29 are not independent nuisance parameters as they are assumed to be related precisely by a theoretical model @xmath32 , which also depends on @xmath6 .",
    "we seek the posterior @xmath33 . with a uniform",
    "prior for @xmath6 , this is proportional to the likelihood @xmath34 . we write this as the marginalised distribution over @xmath28 and @xmath29 as @xmath35 where",
    "we have expanded the condition to include the latent variables , and then further expanded the condition of @xmath36 to include @xmath28 .",
    "we integrate over @xmath29 using a delta function , @xmath37 , and assume for now a uniform prior for @xmath28 : @xmath38 at the cost of some algebraic complexity , we can introduce an informative prior ( parent distribution ) for @xmath28 . in appendix b , we generalise the analysis by assuming a gaussian population prior @xmath39 , and show that we recover the simpler result obtained in the main text in the limit that the errors in @xmath28 are small enough that the prior can be considered constant across the error range of individual data points .",
    "see @xcite and @xcite for further discussion of these points . in this paper",
    "we are not explicitly concerned with biases , but it is important to note gull s point that estimates of parameters , such as the slope of a straight - line fit with errors in both coordinates , will be biased , even with an informative prior , unless the width of the prior is a hyperparameter that is marginalized over .",
    "no doubt similar considerations will be important in applications of the more complicated situation considered here .",
    "next , we make the critical assumption that we can truncate at the linear term of the taylor expansion of @xmath40 : @xmath41 where    we are essentially assuming that the function @xmath42 is linear across the width of the gaussian error distribution of @xmath28 , and this allows the likelihood to be integrated analytically , as it is simply a gaussian integral : @xmath43 where @xmath44 , and @xmath45 and @xmath46 are @xmath47-dimensional vectors : @xmath48 and @xmath49 for @xmath50 , @xmath51 and @xmath52_j$ ] .    the covariance matrix of the data can be written in block form as @xmath53 note that @xmath54 is not symmetrical , nor invertible or even square in general ; although @xmath55 and @xmath56 are .",
    "the covariance matrix may include a number of elements , such as intrinsic scatter and measurement noise , with individual covariance matrices adding to give the final @xmath30 .",
    "the inverse of @xmath30 is @xmath57 where @xmath58 defining , and @xmath59 , we collect together the terms as follows : @xmath60 has the quadratic form @xmath61 where with the definition of @xmath60 in eqn .",
    "[ eqn : qdefinition ] , the gaussian integral of eqn .",
    "[ eqn : likelihood_gaussian ] can be performed , using @xmath62 and noting that @xmath63 is independent of @xmath64 .",
    "the likelihood then simplifies after a few lines of algebra to @xmath65 where the inverse of the marginal covariance matrix of @xmath66 is which is the key result of the calculation .",
    "we can also simplify the pre - factor , @xmath67 ( see appendix  [ app : car ] for the proof ) .",
    "thus @xmath68 we see that this looks just like a normal gaussian ( in terms of data ) likelihood , but with the covariance matrix @xmath30 ( @xmath69 in our current notation ) replaced by @xmath70 . hence to compute the fisher matrix , we can use the standard formula found in eqn .",
    "[ eqn : originalfm ] and eqn .",
    "15 of @xcite , and simply replace @xmath30 by @xmath70 : @xmath71.\\ ] ] this is the main result of this paper .",
    "note that @xmath70 depends not only on the standard covariance , but also on the covariance in the independent variable , @xmath55 , the meta - covariance , @xmath54 , and the first partial derivatives of the model function @xmath40 . in the case of uncorrelated data pairs",
    ", the result reduces to that found in march et al ( 2011 ) . for the simple case of no correlations between @xmath2 and @xmath1 values @xmath72 , and with diagonal covariance matrices @xmath56 and @xmath55 we recover the propagation of error result that the variance of @xmath73 for each data point is effectively @xmath74 where @xmath75 and @xmath30 can be replaced in the standard fisher expression ( [ eqn : originalfm ] ) by a diagonal @xmath76 matrix with these enhanced entries .",
    "we now briefly make a few key observations .",
    "first , when the derivatives of the model function are zero ( @xmath77 ) , then the latent variable @xmath28 has no bearing on @xmath70 , and we recover the usual formula for the fisher matrix : when @xmath77 , @xmath78 .",
    "also , in the limit of infinitesimal errors in @xmath2 , we recover the usual fisher matrix formula . as remarked earlier , if the errors in @xmath2 and @xmath1 are uncorrelated , and in the limit that the errors in @xmath2 are small in comparison with the width of the prior , we recover the result obtained from propagation of errors , namely that the variance of @xmath1 is effectively increased from @xmath79 to @xmath80 .",
    "also , although the main focus of the paper has been on the fisher matrix , the expression for the likelihood itself ( equation [ likelihoodr ] ) can be used without the usual interpretation that it is gaussian in the parameter space , to make predictions for the shape of the likelihood surfaces beyond ellipses .",
    "thus the technology of dali @xcite can be generalized straightforwardly by replacing the data covariance matrix by @xmath70 .",
    "finally , even if the covariance matrix of the data ( the original @xmath30 , which is @xmath56 ) is independent of the parameters , @xmath70 is not , because in general @xmath81 does depend on the parameters .",
    "as an example for illustration , consider the type 1a supernova hubble diagram , which consists of data pairs corresponding to the redshift of the host galaxy of each supernova , and its apparent brightness . in the case",
    "presented here @xmath2 and @xmath1 have the same length , and represent the redshifts and distance moduli of the supernovae .",
    "various corrections , based on colour and the timescale of decline of the light curve ( ` stretch ' ) , are applied such that these supernovae act as standard candles with a small dispersion of around 10% .",
    "colours and stretch could be added to @xmath2 , in which case @xmath82 , but @xmath2 could also include variables which are not associated with a single @xmath83 value ( e.g. instrumental calibration ) .",
    "the @xmath84 cold dark matter model plus empirical corrections for colour and stretch then relate @xmath1 to @xmath2 , dependent on parameters of interest , such as the matter and dark energy content .",
    "see @xcite for a full bayesian hierarchical model description , and @xcite for a principled analysis of data , and further discussion of background .",
    "redshift errors obtained from spectroscopy are negligibly small , but if they are photometric redshifts , based on broad - band colours of the host galaxy , then two complications arise .",
    "one is that the redshift errors may be large ( typically around 5 or 10% for 5-band photometry ) .",
    "the second is that errors in the photometry ( such as zero - point errors ) will introduce errors in the redshifts , but could also affect the colour corrections for the supernovae themselves .",
    "this potentially couples the errors in @xmath4 and @xmath20 for a given data pair .",
    "@xcite investigated correlations between redshift and magnitude errors in photometric surveys , and found rather variable correlation coefficients between about 0.35 and 0.95 .",
    "a scenario which could couple the errors in @xmath4 and @xmath20 for different data pairs arises if one takes weighted averages of the data .",
    "this one might do in order to make the errors closer to gaussian , as we do not know the error distribution for individual supernovae . if this is done with overlapping sub - samples , to maintain a good sampling in redshift ( see fig .",
    "[ snsamples ] ) , then the errors will be coupled . furthermore ,",
    "if the @xmath20 values are referred to a fiducial model ( such as the standard cosmological model ) , as shown , then this involves dividing by a function of the supernova redshift , which then couples the errors in @xmath4 to the errors in @xmath20 across different ( weighted ) data pairs .",
    "so we see in this example how one can get full covariance between @xmath4 and @xmath20 sets , with non - zero off - diagonal terms of all types .    to illustrate results using the generalised fisher matrix , we have simulated supernovae with correlated errors in redshift and distance modulus , obtaining an estimate of the posterior for the matter density parameter and cosmological constant , using markov chain monte carlo techniques .",
    "for illustration we show the simplest non - trivial case , where 200 supernovae are drawn from a uniform distribution of redshifts @xmath85 in the range @xmath86 , each having uncorrelated gaussian errors of 0.1 in distance modulus and 0.01 in @xmath85 ; more complicated examples look essentially the same .",
    "[ snsim ] shows the comparison of the mcmc error ellipse with the expected error contours from the generalised fisher matrix technique , showing good agreement .",
    "in this paper we have considered the fisher information matrix where some subset of the data ( @xmath1 ) depends via a theoretical model @xmath87 on some other set of measured variables ( @xmath2 ) , and a set of model parameters @xmath6 whose posterior distribution is desired .",
    "@xmath2 and @xmath1 are assumed to have gaussian errors which can have arbitrary covariance .",
    "this includes as a subset the case of ( @xmath25 ) data pairs with errors in both coordinates , with correlations between one independent variable and a different dependent variable , but the analysis is more general , and @xmath2 can included any other measured quantities .",
    "the main result , equation ( [ fnew ] ) , is similar to the standard fisher matrix , but with the covariance matrix replaced by a more complicated matrix ( [ rmatrix ] ) derived from the expanded covariance matrix of all variables , and the partial derivatives of the expected signals with respect to the dependent variables .",
    "the result is valid for situations where two conditions hold : the first is that a taylor expansion of the expected signal to linear order is valid across the gaussian error of the independent variables ; the second is that the errors in the independent variables are small compared with the width of the prior distribution . at the price of some complexity",
    ", we present a perturbative correction when the latter condition does not hold . in the case",
    "when the errors are uncorrelated between data pairs , the result reduces to the result one obtains from propagation of errors , where the variance of the dependent variable is increased from @xmath79 to @xmath88 . since we compute the likelihood itself , it may be used to evaluate the expected likelihood surface when it is not gaussian in the parameter space , straightforwardly generalizing the dali technique of @xcite .",
    "finally , the generalised fisher matrix will be implemented in the fisher4cast software , available at http://www.mathworks.com/matlabcentral/fileexchange/20008-fisher-matrix-toolbox-fisher4cast .    *",
    "acknowledgments * + we are grateful to the organisers of the cape town international cosmology school , where this work started as a student project , to roberto trotta , daniel mortlock and andrew jaffe for useful discussions , and to the anonymous referee for very helpful comments and suggestions .",
    "we now generalise the method to apply to cases where the prior in @xmath28 is not uniform .",
    "we illustrate this with a simplifying assumption that the prior is a gaussian of specified width , and demonstrate that in the limit of a prior width which is much larger than the errors in @xmath28 , we recover the results in the main text , and we expect this to hold for any broad prior .",
    "we can consider a prior which is dependent on each point , with a mean vector @xmath90 and variance @xmath91 ( we assume that @xmath91 is a diagonal matrix ) .",
    "in the normal case where the abscissa values are drawn from the same distribution , then all elements of @xmath90 are identical , and @xmath91 is proportional to the identity matrix .    assuming a gaussian prior @xmath92\\ ] ] we get for the posterior @xmath93\\right\\}\\,d^n{\\mathbf x}\\,,\\ ] ] defining @xmath94 , we get @xmath95 where @xmath96 we perform the gaussian integral as before , finding @xmath97.\\ ] ] in the case when the prior in @xmath28 is informative , then there is information in the values of @xmath2 , so the data vector should include both @xmath2 and @xmath1 .",
    "the likelihood is then @xmath98 where @xmath99 and , collecting terms and using the woodbury identity again , we find @xmath100 in the limit of an infinitely broad prior , we see that , as expected , @xmath101 contains no useful information , and the likelihood depends only on @xmath102 , with the quadratic simplifying to @xmath103 , and as expected , we recover the results of the main text .    to investigate departures from the main text result , we consider terms linear in @xmath104 .",
    "this approximation only makes sense if @xmath105 as @xmath91 is a diagonal matrix , the elements of the matrix @xmath106 are given by @xmath107_{ij } & = &   \\left(\\left[\\sigma^{-1}\\right]_{ii}\\left[{{\\sf{a}}}^{-1}\\right]_{ii}\\right)^{n-1 } \\left[\\sigma^{-1}\\right]_{ii}\\left[{{\\sf{a}}}^{-1}\\right]_{ij } \\nonumber\\\\ & = & \\left(\\left[{{\\sf{a}}}^{-1}\\right]_{ii}/\\sigma_{ii}\\right)^{n-1 } \\left[{{\\sf{a}}}^{-1}\\right]_{ij}/\\sigma_{ii}\\end{aligned}\\ ] ] thus condition is fulfilled if @xmath108_{ii } \\ll \\sigma_{ii}\\ ] ] for all @xmath109 .",
    "we will assume this and neglect higher order terms in @xmath104 .",
    "then we can approximate @xmath110 by @xmath111 inserting this result in equation , we get @xmath112 with @xmath113 and @xmath114.\\ ] ] @xmath115 is the zeroth order result from the main text . the fisher matrix is then given by @xmath116 with @xmath117 we already know the result for @xmath118 , so we just need to calculate the first - order term : @xmath119 \\right\\rangle.\\ ] ] using the approximation @xmath120 and with @xmath121 , @xmath122 , and @xmath123 we find after some tedious calculations @xmath124 \\nonumber\\\\ & &   { } -\\tilde{\\mathbf x}^t \\sigma^{-1 } \\left\\{{{\\sf{a}}}^{-1 }   \\left({{\\sf{h}}}-{{\\sf{t}}}^t{{\\sf{e}}}\\right ) \\bmu \\right\\}_{,\\alpha\\beta } + \\tilde{\\mathbf x}^t \\sigma^{-1 }    \\nonumber\\\\ & &   \\left\\{{{\\sf{a}}}^{-1 } \\left({{\\sf{h}}}-{{\\sf{t}}}^t{{\\sf{e}}}\\right ) \\right\\}_{,\\alpha\\beta } \\bmu   \\label{fisher } { } + \\bmu_{,\\alpha}^t \\left({{\\sf{h}}}^t-{{\\sf{e}}}{{\\sf{t}}}\\right ) { { \\sf{a}}}^{-1}\\sigma^{-1}{{\\sf{a}}}^{-1 } \\left({{\\sf{h}}}-{{\\sf{t}}}^t{{\\sf{e}}}\\right ) \\bmu_{,\\beta}\\,.\\end{aligned}\\ ] ] as @xmath125 , each term in contains the factor @xmath104 , so @xmath126 gives the first - order corrections in terms of this parameter .",
    "acquaviva v. , gawiser e. , bickerton s.j .",
    ", grogin n.a . , guo y. , lee s .- k . , 2012 , the astrophysical journal , 749 , 72 albrecht a. , bernstein g. , cahn r. , freedman w.l . , hewitt j. , hu w. , huth j. , kamionkowski m. , kolb e.w . , knox l. , mather j.c . , staggs s. , suntzeff n.b . , 2006 , arxiv:0609591 bassett b.a . , fantaye y. , hlozek r. , kotze j. , 2009 , arxiv.org , astro-ph.co coe d. , 2009 , arxiv preprint arxiv:0906.4123 dark energy survey collaboration , 2005 , arxiv:0510346 cunha c. , 2009 , physical review d , 79 , 63009 dagostini g. , 2005 , arxiv:0511182 fisher r.a . , 1935 , j. roy . stat .",
    "soc . , 98 , 39 gull s.f . , 1989 , in skilling j. ( ed .",
    ") , in  maximum entropy and bayesian methods \" , kluwer publishing , 511 , 518 hogg d.w . , bovy j. , lang d. , 2010 , arxiv:1008.4686 kelly b.c . , 2011 , in feigelson e. , babu j. ( eds . ) ,  statistical challenges in modern astronomy v \" , penn state , arxiv:1112.1745 kim a.g .",
    ", miquel r. , 2007 , astroparticle physics , 28 , 448 kitching t.d . , heavens a. f. , verde l. , serra p. , melchiorri a. , 2008 , prd , 77 , 3008 mandel k.s . ,",
    "narayan g. , kirshner r.p . , 2011 , apj , 731 , 120 march m.c .",
    ", trotta r. , berkes p. , starkman g.d .",
    ", vaudrevange p.m. , 2011 , mnras , 418 , 2308 refregier a. , amara a. , kitching t. d. , rassat a. , 2011 , a&a , 528 , 33 schlegel d. et al . , 2011 , arxiv:1106.1706 sellentin e. , quartin m. , amendola l. , 2014 , arxiv:1401.6892 taylor a. , heavens a. , ballinger b. , tegmark m. , 1997 , in `` proceedings of the particle physics and early universe conference '' ( ppeuc ) , university of cambridge , arxiv:9707265 tegmark m. , taylor a. , heavens a. , 1997 , apj , 480 , 22 vogeley , m. , szalay a. , 1996 , apj , 465 , 34 wolz l. et al . , 2012 , jcap , 9 , 009 woodbury m.a . , 1950 , statistical research group , memo rep . no ."
  ],
  "abstract_text": [
    "<S> the fisher information matrix formalism @xcite is extended to cases where the data is divided into two parts ( @xmath0 ) , where the expectation value of @xmath1 depends on @xmath2 according to some theoretical model , and @xmath2 and @xmath1 both have errors with arbitrary covariance . in the simplest case , ( @xmath0 ) </S>",
    "<S> represent data pairs of abscissa and ordinate , in which case the analysis deals with the case of data pairs with errors in both coordinates , but @xmath2 can be _ any _ measured quantities on which @xmath1 depends . </S>",
    "<S> the analysis applies for arbitrary covariance , provided all errors are gaussian , and provided the errors in @xmath2 are small , both in comparison with the scale over which the expected signal @xmath1 changes , and with the width of the prior distribution . </S>",
    "<S> this generalises the fisher matrix approach , which normally only considers errors in the ` ordinate ' @xmath1 . in this work , </S>",
    "<S> we include errors in @xmath2 by marginalising over latent variables , effectively employing a bayesian hierarchical model , and deriving the fisher matrix for this more general case . </S>",
    "<S> the methods here also extend to likelihood surfaces which are not gaussian in the parameter space , and so techniques such as dali ( derivative approximation for likelihoods ) can be generalised straightforwardly to include arbitrary gaussian data error covariances . </S>",
    "<S> for simple mock data and theoretical models , we compare to markov chain monte carlo experiments , illustrating the method with cosmological supernova data . </S>",
    "<S> we also include the new method in the fisher4cast software .    </S>",
    "<S> statistics : general  statistics : fisher matrix  cosmology : forecasts </S>"
  ]
}