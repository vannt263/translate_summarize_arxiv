{
  "article_text": [
    "in the last decades , the technological evolution of optical sensors has provided remote sensing analysts with rich spatial , spectral , and temporal information . in particular",
    ", the increase in spectral resolution of hyperspectral sensors in general , and of infrared sounders in particular , opens the doors to new application domains and poses new methodological challenges in data analysis .",
    "the distinct highly - resolved spectra offered by hyperspectral images ( hsi ) allow us to characterize land - cover classes with unprecedented accuracy .",
    "for instance , hyperspectral instruments such as nasa s airborne visible infra - red imaging spectrometer ( aviris ) covers the wavelength region from 0.4 to 2.5@xmath0 m using more than 200 spectral channels , at a nominal spectral resolution of 10 nm .",
    "the metop / iasi infrared sounder poses even more complex image processing problems , as it acquires more than 8000 channels per ifov .",
    "actually , such improvements in spectral resolution have called for advances in signal processing and exploitation algorithms capable of summarizing the information content in as few components as possible  @xcite .",
    "in addition to its eventual high dimensionality , the complex interaction between radiation , atmosphere , and objects in the surface leads to irradiance manifolds which consist of non - aligned clusters that may change nonlinearly in different acquisition conditions  @xcite .",
    "fortunately , it has been shown that , given the spatial - spectral smoothness of the signal , the intrinsic dimensionality of the data is small , and this can be used both for efficient signal coding  @xcite , and for knowledge extraction from a reduced set of features  @xcite .",
    "the high dimensionality problem is not only affecting the hyperspectral data : very often , multispectral data processing applications involve using spatial , multi - temporal or multi - angular features that are combined with the spectral features  @xcite . in such cases , the representation space becomes more redundant and pose challenging problems of collinearity for the algorithms . in both cases , the key in coding , classification , and in bio - geo - physical parameter retrieval applications reduces to finding the appropriate set of features , that should be necessarily flexible and nonlinear .    in order to find these features , in recent years a number of feature extraction and dimensionality reduction methods",
    "have been presented .",
    "most of them are based on nonlinear functions to allow describing data manifolds that exhibit nonlinear relations ( see  @xcite for a comprehensive review ) .",
    "approaches range from local methods  @xcite , kernel - based and spectral decompositions  @xcite , neural networks  @xcite , or projection pursuit formulations  @xcite . despite the theoretical advantages of nonlinear methods ,",
    "the fact is that classical principal component analysis ( pca )  @xcite is still the most widely used dimensionality reduction technique in real remote sensing applications  @xcite .",
    "this is mainly because pca has different properties that make it useful in real examples : it is easy to apply since it involves solving a linear and convex problem , and it has a straightforward out - of - sample extension . moreover , the pca transformation is invertible and , as a result , the features extracted can be easily interpreted .",
    "the new dimensionality reduction algorithms that involve nonlinearities rarely fulfill the above properties .",
    "nonlinear models usually have complex formulations , which introduce a number of non - intuitive free parameters .",
    "tuning these parameters implies strong assumptions about the manifold characteristics ( e.g. local gaussianity or special symmetries ) , or a high computational cost training .",
    "this complexity reduces the applicability of nonlinear feature extraction to specific data , i.e. the performance of these methods do not significantly improve that of pca on many remote sensing problems  @xcite .",
    "moreover , these methods have problems to obtain out - of - sample predictions , which is mandatory in most of the real applications .",
    "another critical point is that the transform involved by the nonlinear models is hard to interpret .",
    "this problem could be alleviated if the methods were invertible since then one could get the data back to the input domain ( where units are meaningful ) and understand the results therein .",
    "invertibility allows to characterize the transformed domain , and to evaluate its quality .",
    "however , invertibility is scarcely achieved in the manifold learning literature .",
    "for instance , spectral and kernel methods involve _ implicit _ mappings between the original and the curvilinear coordinates , but these _ implicit _ features are not easily invertible nor interpretable @xcite .",
    "the desirable properties of pca are straightforward in methods that find projections onto _ explicit _ features in the input domain .",
    "these _ explicit _ features may be either straight lines or curves .",
    "this family of projection methods may be understood as a generalization of linear transforms extending linear components to curvilinear components .",
    "this family ranges between two extreme cases : ( 1 ) * rigid * approaches where features are straight lines in the input space ( e.g. conventional pca , independent components analysis -ica- @xcite ) , and ( 2 ) * flexible * non - parametric techniques that closely follow the data , such as self - organizing maps ( som )  @xcite , or the related sequential principal curves analysis ( spca )  @xcite .",
    "this family is discussed in section  [ related ] below .",
    "both extreme cases are undesirable because of different reasons : limited performance ( in too rigid methods ) , and complex tuning of free parameters and/or unaffordable computational cost ( in too flexible methods ) . in this",
    "_ projection - onto - explicit - features _ context , autoencoders such as nonlinear - pca ( nlpca )  @xcite , and approaches based on fitting functional curves , such as principal polynomial analysis ( ppa )  @xcite , represent convenient intermediate points between the extreme cases in the family .",
    "note that these methods have shown better performance than pca on a variety of real data  @xcite .",
    "actually , in the case of ppa , it is theoretically ensured to obtain better results than pca .",
    "the method proposed here , _ dimensionality reduction via regression _ ( drr ) , represents a qualitative step towards the flexible end in this family because of the multivariate nature of the regression ( as opposed to the univariate regressions done in ppa ) while keeping the convenient properties of ppa and pca which make it suitable for practical high dimensional problems ( as opposed to spca and som ) .",
    "therefore , it extends the applicability of ppa to more general manifolds , such as those encountered in remote sensing data analysis .",
    "following the taxonomy in  @xcite these three methods ( nlpca , ppa and drr ) could be included in the _ principal curves analysis _",
    "framework @xcite .",
    "this framework includes both parametric ( fitting analytic curves )  @xcite , and non - parametric  @xcite methods .",
    "nlpca , ppa and drr exploit the idea behind this framework to define generalizations of pca of _ controlled _ flexibility .",
    "preliminary results of drr were presented in @xcite . here",
    "we extend the theoretical analysis of the method and the experimental confirmation of the performance in hyperspectral images .",
    "the remainder of the paper is organized as follows .",
    "section  [ related ] reviews the properties and shortcomings of the _ projection - onto - explicit - features family pointing out the qualitative advantages of the proposed drr . _ section  [ the_math_details ] introduces the mathematical details of drr . we describe the drr transform and the key differences with ppa .",
    "we derive an explicit expression for the inverse and we prove the volume preservation property of drr .",
    "the theoretical properties of drr are demonstrated and illustrated in controlled toy examples of different complexity . in section  [ experiments ] , we address two important high dimensional problems in remote sensing : the estimation of atmospheric state vectors from infrared atmospheric sounding interferometer ( iasi ) hyperspectral sounding data , and the dimensionality reduction and classification of spatio - spectral landsat image patches . in the experiments ,",
    "drr is compared with conventional pca  @xcite , and with recent fast nonlinear generalizations that belong to the same class of invertible transforms , ppa  @xcite and nlpca  @xcite .",
    "comparisons are made both in terms of reconstruction error and of expressive power of the extracted features .",
    "we end the paper with some concluding remarks in section  [ conclusions ] .",
    "here we illustrate how drr represents a step forward with regard to nlpca and ppa in the family of projections onto explicit curvilinear features ranging from rigid to flexible extremes .",
    "first , we review the basic details of previous projection methods , and then we illustrate the advantages of the proposed method in an easy to visualize example .",
    "classical techniques such as pca @xcite or ica @xcite represent the _ rigid _ extreme of the family , where , zero - mean samples @xmath1 are projected onto @xmath2 rectilinear features through the projection matrix , @xmath3 : @xmath4 where @xmath5 are the principal components ( pc scores for pca ) or the independent components ( for ica ) , and the @xmath2 linear features in the input space are the column vectors ( straight directions ) in @xmath6 .",
    "these rigid techniques use a single set of global features regardless of the input .    on the contrary ,",
    "flexible techniques adapt the set of features to the local properties of the input .",
    "examples include som @xcite where a flexible grid is adapted to the data and samples can be represented by projections onto the local axes defined by the edges of the parallelepiped corresponding to the closest node . similarly , local - pca @xcite and local - ica @xcite project the data onto local axes corresponding to the closest code vector .",
    "more generally , local - to - global methods integrate these local - linear representations into a single global curvilinear representation @xcite .",
    "in particular , using the fact that local eigenvectors are tangent to first and secondary principal curves @xcite , sequential principal curves analysis ( spca ) @xcite integrates local pcas , @xmath7 , along a sequence of @xmath2 principal curves to get a curvilinear representation    @xmath8 where the local metric , @xmath9 , sets the line element along the curves .",
    "spca is inverted by taking the lengths , @xmath10 , along the sequence of principal curves drawn from the origin , @xmath11 . similarly to som , spca assumes a grid of curves adapted to the data .",
    "however , as opposed to som , spca does not learn the whole grid , but only @xmath2 segments of principal curves per sample .",
    "the above methods identify explicit curves / features that follow the data , but they are hard to train ( e.g. parameters to control their flexibility depend on the problem ) and require many samples to be reliable , which make them hard to use in high - dimensional scenarios .",
    "other methods have been proposed to generalize the rigid representations by considering curvilinear features instead of straight lines @xcite .",
    "for instance , in nlpca @xcite an invertible internal representation is computed through a two stage neural network , @xmath12 where the matrices @xmath13 represent sets of linear receptive fields , and @xmath14 is a set of fixed point - wise nonlinearities .",
    "the inverse of this autoencoder @xcite can be used to make the curvilinear coordinates explicit .",
    "fitting general parametric curves in @xmath15 , as done in @xcite , is difficult because of the unconstrained nature of the problem @xcite .",
    "alternatively , ppa @xcite follows a deflationary sequence in which a single polynomial depending on a straight line ( univariate fit ) is computed at a time . specifically , the @xmath16-th stage of ppa accounts for the @xmath16-th curvilinear dimension by using two elements : ( 1 ) one - dimensional projection onto the leading vector @xmath17 , and ( 2 ) polynomial prediction of the average at the orthogonal subspace ,    @xmath18    where the polynomial prediction , @xmath19 , is removed from the data in the orthogonal subspace .",
    "superindices in the above formula represent the stage . as a result",
    ", data at the @xmath16-th stage is represented by @xmath5 and by the @xmath20-dimensional residual that can not be predicted from that projection .",
    "prediction using this univariate polynomial is a way to remove possible nonlinear dependencies between the linear subspace of @xmath17 and its orthogonal complement . despite its convenience ,",
    "the univariate nature of the fits restricts the kind of dependencies that can be taken into account since more information about the orthogonal subspace ( better predictions ) could be obtained if more dimensions were used in the prediction .",
    "moreover , using a single parameter , @xmath5 , to build the @xmath16-th polynomial implies that the @xmath16-th curvilinear feature has the same shape along the @xmath21-th curve .",
    "drr addresses these limitations by using multivariate instead of univariate regressions in the nonlinear predictions . as a result",
    ", drr improves energy compaction and extends the applicability of ppa to more general manifolds while keeping its simplicity , which make it suitable in high dimensional problems ( as opposed to spca and som ) .",
    "the advantages of drr are illustrated in fig .",
    "[ fig1 ] where we compare representative invertible representations of this family on two curved and noisy manifolds of the class introduced by delicado  @xcite ( in red and blue ) .",
    "this class of manifolds , originally presented to illustrate the concept of _ secondary principal curves _",
    "@xcite , is convenient since one can easily control the complexity of the problem by introducing tilt ( non - stationarity ) on the secondary principal curves ( dark color ) along the first principal curve ( light color ) .",
    "this controlled complexity is useful to point out the limitations of previous techniques ( e.g. required symmetry in the manifold ) and how these limitations are alleviated by using the ( more general ) drr model .",
    "the performance is compared in the input domain through the dimensionality reduction error and through the accuracy of the identified curvilinear features .",
    "these manifolds come from a two - dimensional space of latent variables ( positions along the first and secondary curves ) . as a result ,",
    "the dimensionality reduction error depends on the unfolding ability of the forward transform : the closer the transformed data fit a flat rectangle , the smaller the error when truncating the representation . on the other hand , the identified features depend on how the inverse transform bends a cartesian grid in the latent space : the better the model represents the curvature of data , the bigger the fidelity of the identified features .",
    "let us start by considering the performance on the easy case : manifold in red with no tilt along the second principal curve .",
    "the previously reported techniques perform as expected : on the one hand , progressively more flexible techniques ( from pca to spca ) reduce the distortion after dimensionality reduction ( in _ _",
    "mse__@xmath22 terms ) because they better unfold test data . as a result , removing the third dimension in the rigid - to - flexible family progressively introduces less error . on the other hand ,",
    "the identified features in the input domain are progressively more similar to the actual curvilinear latent variables when going from the rigid to the flexible extremes . in this specific _ easy _ example",
    "the proposed drr outperforms even the flexible spca in _ _ mse__@xmath22 terms .",
    "moreover , since this particular manifold may not require increased flexibility ( and hence may be better suited to the ppa model ) , ppa slightly outperforms drr in _ _ mse__@xmath23 terms .",
    "cccccc & & * manifold 1 * & & * manifold 2 * & + & & ( * easy * : no tilt ) & & ( * difficult * : tilt ) & + &    90    &   & &   & +    cccc|c|c + & * rigid extreme * & & & & * flexible extreme * + [ 2 mm ] & pca & nlpca & ppa & drr & spca +    90    &   &    &   &   &   +    90    &   &   &   &   &   + [ 5 mm ] & 100 & 48 @xmath24 2 & 23.1 @xmath24 0.6 & 3.3 @xmath24 0.3 & 17 @xmath24 3 + & _ _ mse__@xmath25 100 & 66.4 @xmath24 0.7 & 42.1 @xmath24 0.4 & 49 @xmath24 1 & 12 @xmath24 1 +   + [ 2 mm ] + [ 2 mm ] & pca & nlpca & ppa & drr & spca +    90    &   &    &   &   &   +    90    &   &   &   &   &   + [ 5 mm ] & 269 @xmath24 4 & 80 @xmath24 40 & 97 @xmath24 2 & 26.5 @xmath24 0.3 & 19 @xmath24 3 + & _ _ mse__@xmath25 209 @xmath24 2 & 85 @xmath24 30 & 85.6 @xmath24 0.4 & 69 @xmath24 2 & 15 @xmath24 4 +    results for the more complex manifold ( tilted secondary curves , in blue ) provide more insight into the techniques since it pushes them ( specifically ppa ) to their limits .",
    "firstly , note that the increase in complexity is illustrated by an increase in the errors in all methods .",
    "for instance , linear pca , that identifies the same features in both cases , doubles the normalized mses .",
    "while the reduction in performance is not that relevant in spca ( remember these flexible techniques can not be applied in high dimensional scenarios ) , this twisted manifold certainly challenges fast generalizations of pca : the mses dramatically increase for nlpca and ppa .",
    "even though nlpca identifies certain tilt in the secondary feature along the first curve , it seems too rigid to follow the data structure .",
    "ppa displays a different problem : as stated above , by construction , the @xmath16-th curvilinear feature in ppa can not handle relations with the @xmath21-th curve beyond the prediction of the mean .",
    "this is because the data in all orthogonal subspaces along the @xmath21-th curve collapse , and are described by a single curve depending on a single parameter ( _ univariate regression _ ) .",
    "this leads to using the same @xmath16-th curve all along the @xmath21-th feature ( note the repeated secondary curves along the first curve in both , red and blue , cases ) .",
    "this is good enough when data manifolds have the required symmetry ( ppa performance is over nlpca in the first case ) , but leads to dramatic errors when the method have to deal with relations between three or more variables , as for the manifold in blue , where ppa performance is below nlpca .",
    "this latter effect frequently appears in hyperspectral images , as different ( non - stationary ) nonlinear relations between spectral channels occur for different objects  @xcite .",
    "finally , note that drr clearly improves ppa in the challenging example in blue , mainly because it uses multiple dimensions ( instead of a single one ) to predict each lower variance dimension in the data . as a result",
    ", it can handle non - stationarity along the principal curves leading to better unfolding ( lower _ _ mse__@xmath22 ) and tilted secondary features ( lower _ _ mse__@xmath23 ) .",
    "this removes the symmetry requirement in ppa and broadens the class of manifolds suited to drr .",
    "pca removes the second order dependencies between the signal components , i.e. pca scores are decorrelated  @xcite .",
    "equivalently , pca can be casted as the _ linear _ transform that minimizes reconstruction error when a fixed number of features are neglected .",
    "however , for general non - gaussian sources , and in particular for hyperspectral images , pca scores still display significant statistical relations , see  @xcite[ch .",
    "the scheme presented here tries to _ nonlinearly _ remove the information still shared by different pca components .",
    "it is well known that , even though pca leads to a domain with decorrelated dimensions , complete independence ( or non redundant coefficients ) is guaranteed only if the signal has a gaussian probability density function ( pdf ) . here",
    ", we propose a scheme to remove this redundancy ( or uninformative data ) .",
    "the idea is simple : just predict the redundant information in each coefficient that can be extracted from the others . only the non - predictable information ( the residual prediction error )",
    "should be retained for data representation . specifically , we start from the linear pca representation outlined above : @xmath26 . then , we propose to predict each coefficient , @xmath5 , through a multivariate regression function , @xmath27 , that takes the higher variance components as inputs for prediction .",
    "the non - predictable information is :    @xmath28    and this residual , @xmath29 , is the @xmath16-th dimension of the drr domain .",
    "prediction+substraction _ is applied @xmath30 times , @xmath31 , where @xmath2 is the dimension of the input . as a result ,",
    "the drr representation of each input vector @xmath32 , is : @xmath33      [ [ drr - generalizes - pca ] ] drr generalizes pca + + + + + + + + + + + + + + + + + + +    in the particular case of using linear regressions in @xmath27 , i.e. linear - drr , the prediction @xmath34 would be equal to zero .",
    "note that this is the result when using classical ( least squares ) solution since @xmath35 is uncorrelated with each @xmath36 .",
    "therefore @xmath37 , and then @xmath38 , i.e. linear - drr reduces to pca .    as a result , if the employed nonlinear functions @xmath27 generalize the linear functions , drr will obtain at least as good results as pca .",
    "the flexibility of these functions with regard to the linear case will reduce the variance of the residuals , and hence the reconstruction error in dimensionality reduction .",
    "[ [ drr - is - invertible ] ] drr is invertible + + + + + + + + + + + + + + + + +    given the drr transformed vector , @xmath39 , and knowing the functions of model @xmath27 , the inverse is straightforward since it reduces to sequentially undo the forward transformation : we first predict coefficient @xmath40-th from previous ( known ) coefficients using the known regression function , and then , we use the known residual to correct the prediction : @xmath41    [ [ drr - has - an - easy - out - of - sample - extension ] ] drr has an easy out - of - sample extension + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    note that forward and inverse drr transforms can be applied to new data ( not in the training set ) since there is no restriction to apply the prediction functions @xmath27 .",
    "[ krr ] for a discussion on the selected regression functions in this work .",
    "[ [ drr - is - a - volume - preserving - transform ] ] drr is a volume preserving transform + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a nonlinear transform preserves the volume of the input space if the determinant of its jacobian is one for all @xmath32 @xcite .",
    "here we prove that the nature of drr ensures that its jacobian fulfills this property .",
    "ddr can be thought of as a sequential algorithm in which only one dimension is addressed at a time through the elementary transform @xmath42 consisting of prediction and substraction ( eq . ) . yet",
    "mathematically convenient to formulate the jacobian , this sequential view is does not prevent the parallelization discussed later .",
    "hence , the ( global ) jacobian of drr , @xmath43 , is the product of the jacobians of the elementary transforms in this sequence times the matrix of the initial pca as follows : @xmath44    the @xmath16-th elementary transform leaves all , but the @xmath16-th dimension , unaltered .",
    "therefore , each elementary jacobian is the identity matrix except for the @xmath16-th row , which depends on @xmath45 through the derivatives of the @xmath16-th regression function with regard to each component in the previous stage : @xmath46    whatever these derivatives are ( whatever regression function is selected ) , the determinant of such a simple matrix is always one at every point @xmath32 .",
    "therefore , the determinant of the global jacobian is guaranteed to be one including the pca transform , @xmath3 , which is orthonormal .",
    "[ [ parallelization - of - drr ] ] parallelization of drr + + + + + + + + + + + + + + + + + + + + + +    multivariate regression in drr is a qualitative advantage over other methods ( as discussed in section [ related ] ) .",
    "however it is computationally expensive .",
    "fortunately , the proposed drr allows trivial parallel implementation of the forward transform .",
    "note that the prediction of each component is actually done from a subset of the original pc scores .",
    "therefore , all the prediction functions , @xmath27 , can be applied at the same time after the initial pca step , and sequential implementation is not necessary .",
    "this is an obvious _ computational advantage _ over ppa , which necessarily requires a sequential implementation , but it represents a _ qualitative advantage _ too , since in ppa each feature depends on the previous nonlinear features ( see eq .",
    "[ ppaapprox ] ) , while in drr nonlinear regressions only depend on linear features , but not on previous curvilinear coefficients .",
    "as opposed to the forward transform , the inverse is not parallelizable since , in order to predict the @xmath16-th pca coefficient , we need the previous linear pcs , which implies operating sequentially from @xmath47 .      in practice , the prediction functions @xmath48 reduce to training a set of nonlinear regression models . in our experiments",
    ", we used the kernel ridge regression ( krr )  @xcite to implement the prediction functions @xmath27 , although any alternative regression method could be also applied .",
    "notationally , given @xmath49 data points , the prediction for all of them is estimated as : @xmath50 where @xmath51 is a kernel ( similarity ) function reproducing a dot product in hilbert space , @xmath52 , @xmath53 is the matrix containing all the @xmath49 training samples in rows , @xmath54 is the @xmath16-th column of @xmath55 to be estimated , @xmath56 denotes a submatrix containing columns @xmath57 of @xmath55 used as input data to fit the prediction model , and @xmath58 represents the feature vector in row @xmath59 of @xmath60 . in this prediction function",
    ", @xmath61 is the dual weight vector obtained by solving the least squares linear regression problem in hilbert space : @xmath62 where @xmath63 is the kernel matrix with entries @xmath64 , being @xmath65 .",
    "two parameters must be tuned for the regression : the regularization parameter @xmath66 and the kernel parameters . in our experiments",
    "we used the standard squared exponential kernel function , @xmath67 , as it is a universal kernel which involves estimating only the length - scale @xmath68 .",
    "both @xmath68 and @xmath66 can be estimated by standard cross - validation .",
    "krr can be quite convenient in the drr scheme because it implements flexible nonlinear regression functions , and reduces to solving a matrix inversion problem with unique solution .",
    "krr offers a moderate training and testing computational cost for training , recent sparse and low - rank approximations  @xcite along with divide - and - conquer schemes  @xcite can make krr very efficient .",
    "] , includes regularization in a natural way , and also offers the possibility to generate multi - output nonlinear regression .",
    "the latter is an important feature to extend the drr scheme to multiple outputs approximation .",
    "finally , krr has been successfully used in many real applications  @xcite including remote sensing data analysis involving hyperspectral data  @xcite .",
    "however , it should be noted that , even in such cases , a previous feature extraction was mandatory to attain significant results  @xcite .",
    "in this section , we give experimental evidence of the performance of the proposed algorithm in two illustrative settings .",
    "first , we show results on the truncation error in a multispectral image classification problem including spatial context .",
    "then we evaluate the performance of drr in terms of both the reconstruction error and the expressive power of the features to perform multi - output regression of a challenging problem involving hyperspectral infrared sounding data .",
    "focusing in these two experiments is not arbitrary .",
    "the two applications imply challenging high dimensional data : ( 1 ) multispectral image classification in which contextual information is stacked to the spectral information highly increases the dimensionality , and ( 2 ) hyperspectral infrared sounding data used to estimate atmospheric state vectors is densely sampled . in both cases the input space may become redundant because of the collinearity introduced either by the ( locally stationary ) spatial features or by the spectral continuity of natural sources . in these experiments , in which @xmath69 , we compare drr with members of the invertible projection family described in section [ related ] suited to high dimensional scenarios .",
    "this implies focusing on pca , nlpca and ppa , excluding spca and som because of their prohibitive cost .",
    "[ cols=\"^,^ \" , ]     [ time_eum ]",
    "we introduced a novel unsupervised method for dimensionality reduction via the application of a multivariate nonlinear regression to approximate each projection from the higher variance scores .",
    "the method is shown to generalize pca and to achieve more data compression ( smaller mse for a fixed number of retained components ) and better features for prediction ( less error in classification and regression problems ) than competitive nonlinear methods like nlpca and ppa .",
    "besides , unlike other nonlinear dimensionality reduction methods , drr is easy to apply , it has out - of - sample extension , it is invertible , and the learned transformation is volume - preserving .",
    "we focused on the challenging problems of spatial - spectral multispectral land cover classification , and atmospheric parameter retrieval from hyperspectral infrared sounding data .",
    "extension of drr to cope with multiset / output regression , as well as impact of the data dimensionality and noise sources , will be explored in the future .",
    "the authors wish to thank tim hultberg from the european organisation for the exploitation of meteorological satellites ( eumetsat ) in darmstadt , germany , for kindly providing the iasi datasets used in this paper .",
    "a.  plaza , j.  a. benediktsson , j.  boardman , j.  brazile , l.  bruzzone , g.  camps - valls , j.  chanussot , m.  fauvel , p.  gamba , a.  gualtieri , and j.  tilton , `` recent advances in techniques for hyperspectral image processing , '' _ remote sensing of environment _ , vol .",
    "113 , no .",
    "110122 , sept 2009 .",
    "g.  camps - valls , d.  tuia , l.  gmez , s.  jimnez , and j.  malo , _ remote sensing image processing _ , ser .",
    "synthesis lectures on image , video and multimedia processing , a.  bovik , ed.1em plus 0.5em minus 0.4em morgan & claypool publishers , 2011 .",
    "g.  camps - valls , d.  tuia , l.  bruzzone , and j.  atli  benediktsson , `` advances in hyperspectral image classification : earth monitoring with statistical learning methods , '' _ signal processing magazine , ieee _ , vol .",
    "31 , no .  1 ,",
    "4554 , jan 2014 .            j.  arenas - garca , k.  petersen , g.  camps - valls , and l.  hansen , `` kernel multivariate analysis framework for supervised subspace learning : a tutorial on linear and kernel multivariate methods , '' _ signal processing magazine , ieee _ , vol .",
    "30 , no .  4 , pp . 1629 , 2013 .",
    "m.  fauvel , j.  a. benediktsson , j.  chanussot , and j.  r. sveinsson , `` spectral and spatial classification of hyperspectral data using svms and morphological profiles , '' _ ieee trans .",
    "remote sens .",
    "_ , vol .",
    "46 , no .  11 , pp .",
    "3804  3814 , 2008 .",
    "d.  tuia , f.  pacifici , m.  kanevski , and w.  emery , `` classification of very high spatial resolution imagery using mathematical morphology and support vector machines , '' _ ieee trans .",
    "remote sens .",
    "_ , vol .",
    "47 , no .  11 , pp . 38663879 , 2009 .",
    "s.  t. roweis , l.  k. saul , and g.  e. hinton , `` global coordination of local linear models , '' in _ advances in neural information processing systems 14_.1em plus 0.5em minus 0.4emmit press , 2002 , pp .",
    "889896 .",
    "j.  j. verbeek , n.  vlassis , and b.  krose , `` coordinating principal component analyzers , '' in _ in proc . international conference on artificial neural networks_.1em plus 0.5em minus 0.4emspringer , 2002 , pp .",
    "914919 .",
    "g.  camps - valls , j.  muoz  and , l.  gmez , l.  guanter , and x.  calbet , `` nonlinear statistical retrieval of atmospheric profiles from metop - iasi and mtg - irs infrared sounding data , '' _ ieee trans .",
    "_ , vol .",
    "50 , no .  5 , pp . 17591769 , 2012 .",
    "t.  kohonen , `` self - organized formation of topologically correct feature maps , '' _ biological cybernetics _ , vol .",
    "43 , no .  1 , pp . 5969 , january 1982 .",
    "[ online ] .",
    "available : http://dx.doi.org/10.1007/bf00337288      v.  laparra , d.  tuia , s.  jimnez , g.  camps - valls , and j.  malo , `` nonlinear data description with principal polynomial analysis , '' in _ ieee workshop on machine learning for signal processing _ , 2012 , pp",
    ". 16 .",
    "d.  donnell , a.  buja , and w.  stuetzle , `` analysis of additive dependencies and concurvities using smallest additive principal components , '' _ the annals of statistics _ , vol .",
    "22 , no .  4 ,",
    "pp . 16351668 , 1994 .",
    "g.  camps - valls , l.  guanter , j.  muoz , l.  gmez , and x.  calbet , `` nonlinear retrieval of atmospheric profiles from metop - iasi and mtg - irs data , '' in _ proc .",
    "xvi _ , vol .",
    "7830.1em plus 0.5em minus 0.4emspie , 2010 , p.",
    "78300z .",
    "f.  hilton , n.  c. atkinson , s.  j. english , and j.  r. eyre , `` assimilation of iasi at the met office and assessment of its impact through observing system experiments , ''",
    "_ q. j. r. meteorol .",
    "495505 , 2009 ."
  ],
  "abstract_text": [
    "<S> this paper introduces a new _ unsupervised _ method for dimensionality reduction via regression ( drr ) . </S>",
    "<S> the algorithm belongs to the family of _ invertible transforms _ that generalize principal component analysis ( pca ) by using curvilinear instead of linear features . </S>",
    "<S> drr identifies the nonlinear features through multivariate regression to ensure the reduction in redundancy between the pca coefficients , the reduction of the variance of the scores , and the reduction in the reconstruction error . </S>",
    "<S> more importantly , unlike other nonlinear dimensionality reduction methods , the invertibility , volume - preservation , and straightforward out - of - sample extension , makes drr interpretable and easy to apply . </S>",
    "<S> the properties of drr enable learning a more broader class of data manifolds than the recently proposed non - linear principal components analysis ( nlpca ) and principal polynomial analysis ( ppa ) . </S>",
    "<S> we illustrate the performance of the representation in reducing the dimensionality of remote sensing data . </S>",
    "<S> in particular , we tackle two common problems : processing very high dimensional spectral information such as in hyperspectral image sounding data , and dealing with spatial - spectral image patches of multispectral images . </S>",
    "<S> both settings pose collinearity and ill - determination problems . </S>",
    "<S> evaluation of the expressive power of the features is assessed in terms of truncation error , estimating atmospheric variables , and surface land cover classification error . </S>",
    "<S> results show that drr outperforms linear pca and recently proposed invertible extensions based on neural networks ( nlpca ) and univariate regressions ( ppa ) . </S>"
  ]
}