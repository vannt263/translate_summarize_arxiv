{
  "article_text": [
    "the day - to - day task of particle physicists is to suggest , build , test , discard and , or , refine models of the observed regularities in nature with the ultimate goal of building a comprehensive model that answers all the scientific questions we might think to ask .",
    "one goal of experimental particle physicists is to make quantitative statements about the parameters @xmath0 of a model given a set of experimental observations @xmath1 .",
    "however , in order to make such statements , the the connection between the observations and the model parameters must itself be modeled , and herein lies a difficulty . while there is general agreement about how to connect model parameters to data , there is long history  @xcite of disagreement about the best way to solve the inverse problem , that is , to go from observations to model parameters .",
    "the solution of this inverse problem requires a theory of inference .",
    "these lectures introduce to two broad classes of theories of inference , the frequentist and bayesian approaches .",
    "while our focus is on the practical , we do not shy away from brief discussions of foundations .",
    "we do so in order to make two points .",
    "the first is that when it comes to statistics , there is no such thing as  the \" answer ; rather there are answers based on assumptions , or proposals , on which reasonable people may disagree for purely intellectual reasons .",
    "second , none of the current theories of inference is perfect .",
    "it is worth appreciating these points , even superficially , if only to avoid fruitless arguments that can not be resolved because they are ultimately about intellectual taste rather than mathematical correctness .    for more in - depth expositions of the topics here covered , and different points of view",
    ", we highly recommend the excellent textbooks on statistics written for physicists , by physicists  @xcite .",
    "suppose we have a sample of @xmath2 data @xmath3 .",
    "it is often useful to summarize these data with a few numbers called statistics .",
    "a * statistic * is any number that can be calculated from the data and known parameters .",
    "for example , @xmath4 is a statistic , but if the value of @xmath0 is unknown @xmath5 is not .",
    "however , a word of caution is in order : we particle physicists are prone to misuse the jargon of professional statisticians .",
    "for example , we tend to refer to _ any _ function of the data as a statistic including those that contain unknown parameters .    the two most important statistics are @xmath6 the sample average is a measure of the center of the distribution of the data , while the sample variance is a measure of its spread .",
    "statistics that merely characterize the data are called * descriptive statistics * , of which the sample average and variance are the most important .",
    "if we order the data , say from the smallest value to the largest , we can compute another interesting statistic @xmath7 , where @xmath8 and @xmath9 denotes the datum at the @xmath10 position .",
    "the statistic @xmath11 is called the @xmath10 * order statistic * and is a measure of the value of outlying data .",
    "the average and variance , eqs .",
    "( [ eq : xbar ] ) and ( [ eq : xvar ] ) , are numbers that can always be calculated given a data sample @xmath1 .",
    "but now we consider numbers that can not be calculated from the data alone .",
    "imagine the repetition , infinitely many times , of whatever data generating system yielded our data sample @xmath1 thereby creating an infinite sequence of data sets .",
    "we shall refer to the data generating system as an experiment and the infinite sequence as an infinite ensemble .",
    "the latter , together with all the mathematical operations we may wish to apply to it , are abstractions .",
    "after all , it is not possible to realize an infinite ensemble . the ensemble and all the operations on it exist in the same sense that the number @xmath12 exists along with all valid mathematical operations on @xmath12 .    the most common operation to perform on an ensemble is to compute the average of the statistics .",
    "this * ensemble average * suggests several potentially useful characteristics of the ensemble , which we list below .",
    "@xmath13 notice that none of these numbers can be calculated in practice because the data required to do so do not concretely exist . even in an experiment simulated on a computer",
    ", there are very few of these numbers we can calculate .",
    "if we know the mean @xmath14 , perhaps because we have chosen its value  for example , we may have chosen the mass of the higgs boson in our simulation , we can certainly calculate the error @xmath15 for any simulated datum @xmath16 .",
    "but , we can only _ approximate _ the ensemble average @xmath17 , bias @xmath18 , variance @xmath19 , and mse , since our virtual ensemble is always finite .",
    "the point is this : the numbers that characterize the infinite ensemble are also abstractions , albeit useful ones .",
    "for example , the mse is the most widely used measure of the closeness of an ensemble of numbers to some parameter @xmath14",
    ". the square root of the mse is called the root mean square ( rms ) , while the standard deviation is computed with respect to the ensemble average @xmath20 .",
    "the rms and standard deviations are identical only if the bias is zero . ] .",
    "the mse can be written as @xmath21 the mse is the sum of the variance and the square of the bias , a very important result with practical consequences .",
    "for example , suppose that @xmath14 represents the mass of the higgs boson and @xmath16 represents some ( typically very complicated ) statistic that is considered an * estimator * of the mass . an estimator is any function , which when data are entered into it , yields an * estimate * of the quantity of interest , which we may take to be a measurement .",
    "words are important ; `` bias '' is a case in point .",
    "it is an unfortunate choice for the difference @xmath22 because the word `` bias '' biases attitudes towards bias !",
    "something that , or someone who , is biased is surely bad and needs to be corrected . perhaps .",
    "but , it would be wasteful of data to make the bias zero if the net effect is to make the mse larger than an mse in which the bias is non - zero . the price for achieving @xmath23 in our example",
    "would be not only throwing away expensive data  which is bad enough  but also measuring a mass that is more likely to be further away from the higgs boson mass . this may , or may not , be what we want to achieve .    as noted , many of the numbers listed in eq .",
    "( [ eq : ensemble ] ) can not be calculated because the information needed is unknown .",
    "this is true , in particular , of the bias .",
    "however , sometimes it is possible to relate the bias to another ensemble quantity .",
    "consider the ensemble average of the sample variance , eq .",
    "( [ eq : xvar ] ) , @xmath24 the sample variance has a bias of @xmath25 , which many argue should be corrected .",
    "unfortunately , we can not calculate the bias because it depends on an unknown parameter , namely , the variance @xmath19 . however",
    ", if we replace the sample variance by @xmath26,where the correction factor @xmath27 , we find that for the corrected variance estimator @xmath28 the bias is zero .",
    "surely the world is now a better place ?",
    "well , not necessarily . consider the ratio of @xmath29 to @xmath30 , where @xmath31 , @xmath32 with @xmath33 , and @xmath34 , @xmath35.\\end{aligned}\\ ] ] from this we deduce that if @xmath36 > 1 $ ] , the unbiased estimate will be further away on average from @xmath19 than the biased estimate .",
    "this is the case , for example , for a uniform distribution .",
    "when the weather forecast specifies that there is a 80% chance of rain tomorrow , most people have an intuitive sense of what this means . likewise , most people have an intuitive understanding of what it means to say that there is a 50 - 50 chance for a tossed coin to land heads up .",
    "probabilistic ideas are thousands of years old , but , starting in the sixteenth century these ideas were formalized into increasingly rigorous mathematical theories of probability . in the theory formulated by kolmogorov in 1933 ,",
    "@xmath37 is some fixed mathematical space , @xmath38 are subsets ( called events ) defined in some reasonable way are meaningful subsets of @xmath37 , so to is the complement @xmath39 of each , as are countable unions and intersections of these subsets . ] , and @xmath40 is a number associated with subset @xmath41 .",
    "these numbers satisfy the @xmath42 consider two subsets @xmath43 and @xmath44 .",
    "the quantity @xmath45 means @xmath46 _ and _ @xmath47 , while @xmath48 means @xmath46 _ or _ @xmath47 , with associated probabilities @xmath49 and @xmath50 , respectively",
    ". kolmogorov assumed , not unreasonably given the intuitive origins of probability , that probabilities sum to unity ; hence the axiom @xmath51 .",
    "however , this assumption can be dropped so that probabilities remain meaningful even if @xmath52  @xcite .",
    "figure  [ fig : venn ] suggests another probability , namely , the number @xmath53 , called the * conditional probability * of @xmath46 given @xmath47 .",
    "this permits statements such as :  the probability that this track was created by an electron given the measured track parameters \" or  the probability to observe 17 events given that the mean background is 3.8 events \" .",
    "r0.5     conditional probability is a very powerful idea , but the term itself is misleading .",
    "it implies that there are two kinds of probability : conditional and unconditional .",
    "in fact , _ all _ probabilities are conditional in that they always depend on a specific set of conditions , namely , those that define the space @xmath37 .",
    "it is entirely possible to embed a family of subsets of @xmath37 into another space @xmath54 which assigns to each family member a different probability @xmath55 .",
    "a probability is defined only relative to some space of possibilities @xmath37 .",
    "@xmath46 and @xmath47 are said to be mutually exclusive if @xmath56 , that is , if the truth of one denies the truth of the other .",
    "they are said to be exhaustive if @xmath57 .",
    "figure  [ fig : venn ] suggests the theorem @xmath58 which can be deduced from the rules given above .",
    "another useful theorem is an immediate consequence of the commutativity of  anding \" @xmath59 and the definition of @xmath60 , namely , @xmath61 which provides a way to convert the probability @xmath60 to the probability @xmath62 .",
    "using bayes theorem , we can , for example , deduce the probability @xmath63 that a particle is an electron , @xmath64 , given a set of measurements , @xmath16 , from the probability @xmath65 of a set of measurements given that the particle is an electron .      in this section ,",
    "we illustrate the use of these rules to derive more complicated probabilities .",
    "first we start with a definition :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ a * bernoulli trial * , named after the swiss mathematician jacob bernoulli ( 1654  1705 ) , is an experiment with only two possible outcomes : @xmath66 or @xmath67 . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ [ [ example ] ] example + + + + + + +    each collision between protons at the large hadron collider ( lhc ) is a bernoulli trial in which something interesting happens ( @xmath68 ) or does not ( @xmath69 ) .",
    "let @xmath70 be the probability of a success , which is assumed to be the _",
    "same for each trial_. since @xmath68 and @xmath69 are exhaustive , the probability of a failure is @xmath71 . for a given order @xmath72 of @xmath73 proton - proton collisions and exactly @xmath74 successes , and therefore exactly @xmath75 failures , the probability @xmath76 is given by @xmath77",
    "if the order @xmath72 of successes and failures is judged to be irrelevant , we can eliminate the order from the problem by summing over all possible orders , @xmath78 this procedure is called * marginalization*. it is one of the most important operations in probability calculations . every term in the sum in eq .",
    "( [ eq : pkn ] ) is identical and there are @xmath79 of them .",
    "this yields the * binomial distribution * , @xmath80 by definition , the mean number of successes @xmath81 is given by @xmath82 at the lhc @xmath73 is a number in the trillions , while for successes of interest such as the creation of a higgs boson the probability @xmath83 . in this case",
    ", it proves convenient to consider the limit @xmath84 in such a way that @xmath81 remains constant . in this limit @xmath85 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    below we list the most common probability distributions . @xmath86 / ( \\sigma \\sqrt{2\\pi } )   \\nonumber\\\\ & \\textrm{(also known as the normal density)}\\nonumber\\\\ & \\textrm{lognormal}(x , \\mu , \\sigma ) & \\exp[-(\\ln x - \\mu)^2 / ( 2 \\sigma^2 ) ] / ( x \\sigma \\sqrt{2\\pi } ) \\nonumber\\\\ & \\textrm{chisq}(x , n )           & x^{n/2 -1 } \\exp(-x /2 ) / [ 2^{n/2 } \\gamma(n/2 ) ]     \\nonumber\\\\ & \\textrm{gamma}(x , a , b )            & x^{a -1 } a^b \\exp(- a x ) / \\gamma(b )   \\nonumber\\\\ & \\textrm{exp}(x , a )              & a \\exp(- a x ) \\nonumber\\\\ & \\textrm{beta}(x , n , m )          & \\frac{\\gamma(n+m)}{\\gamma(m ) \\ , \\gamma(n ) } x^{n-1 } \\ , ( 1 - x)^{m-1 }   \\label{eq : dist}\\end{aligned}\\ ] ] particle physicists tend to use the term probability distribution for both discrete and continuous functions , such as the poisson and gaussian distributions , respectively .",
    "but , strictly speaking , the continuous functions are probability _ densities _ , not probability distributions . in order to compute a probability from a density",
    "we need to integrate the density over a finite set in @xmath16 .",
    "[ [ discussion ] ] discussion + + + + + + + + + +    probability is the foundation for models of non - deterministic data generating mechanisms , such as particle collisions at the lhc .",
    "a * probability model * is the probability distribution together with all the assumptions on which the distribution is based .",
    "for example , suppose we wish to count , during a given period of time , the number of entries @xmath2 in a given transverse momentum ( @xmath87 ) bin due to particles created in proton - proton collisions at the lhc ; that is , suppose we wish to perform a counting experiment . if we assume that the probability to obtain a count in this bin is very small and that the number of proton - proton collisions is very large , then it is common practice to use a poisson distribution to model the data generating mechanism , which yields the bin count @xmath2 .",
    "if we have multiple independent bins , we may choose to model the data generating mechanism as a product of poisson distributions .",
    "or , perhaps , we may prefer to model the possible counts conditional on a fixed total count in which case a multinomial distribution would be appropriate .",
    "so far , we have assumed the meaning of the word probability to be self - evident .",
    "however , the meaning of probability  @xcite has been the subject of debate for more than two centuries and there is no sign that the debate will end anytime soon .",
    "probability , in spite of its intuitive beginnings , is an abstraction .",
    "therefore , for it to be of practical use it must be _ interpreted_. the two most widely used interpretations of probability are :    1 .",
    "* degree of belief * in , or plausibility of , a proposition , for example ,  it will snow at cern on december 18th \" , and the 2 .",
    "* relative frequency * of outcomes in an _",
    "infinite _ ensemble of trials , for example , the relative frequency of higgs boson creation in an infinite number of proton - proton collisions .",
    "the first interpretation is the older , while the second was championed by influential mathematicians and logicians starting in the mid - nineteenth century and became the dominant interpretation .",
    "of the two interpretations , however , the older is the more general in that it encompasses the latter and can be used in contexts in which the latter makes no sense . the relative frequency , or * frequentist *",
    ", interpretation is useful for situations in which one can contemplate counting the number of times @xmath74 a given outcome is realized in @xmath73 trials , as in the example of a counting experiment .",
    "the relative frequency @xmath88 is expected to converge , in a subtle but well - defined sense , to some number @xmath70 that satisfies the rules of probability .",
    "it should noted , however , that the numbers @xmath89 and @xmath70 are conceptually distinct .",
    "the former is something we can actually calculate , while there is no _ finite _ operational way to calculate the latter from data .",
    "the probability @xmath70 , even when interpreted as a relative frequency , remains an abstraction .",
    "on the other hand , the degrees of belief , which is the basis of the _ bayesian _ approach to statistics ( see lecture 2 ) , are just that : the degree to which a rational being _ ought _ to believe in the veracity of a given statement .",
    "the word  ought \" in the last sentence is important : probability theory , with probabilities interpreted as degrees of belief , is _ not _ a model of how human beings actually reason in situations of uncertainty ; rather probability theory when interpreted this way is a normative theory in that it specifies how an idealized reasoning being , or system , ought to reason when faced with uncertainty .",
    "there is a school of thought that argues that degrees of belief should be an individual s own assessment of her or his degree of belief in a statement , which are then to be updated using the probability rules .",
    "the problem with this position is that it presupposes probability theory to be a model of human reasoning , which we argue it is not  a position confirmed by numerous psychological experiments .",
    "it is perhaps better to think of degrees of belief as numbers that inform one s reasoning rather than as numbers that describe it , and relative frequencies as numbers that characterize stochastic data generation mechanisms . both are probabilities and both are useful .",
    "let us assume that @xmath90 is a * probability density function * ( pdf ) such that @xmath91 is the probability of the statement @xmath92 , where @xmath16 denotes possible data , @xmath0 the parameters that characterize the probability model , and @xmath93 is a finite set . if @xmath16 is discrete , then both @xmath90 and @xmath94 are probabilities . the * likelihood function * is simply the probability model @xmath90 evaluated at the data @xmath95 actually obtained , i.e. , the function @xmath96 .",
    "the following are examples of likelihoods .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ [ [ example-1 ] ] example 1 + + + + + + + + +    in 1995 , cdf and d  discovered the top quark  @xcite at fermilab .",
    "the dcollaboration found @xmath97 events ( @xmath98 ) . for a counting experiment",
    ", the datum can be modeled using @xmath99 we shall analyze this example in detail in lectures 2 and 3 .",
    "[ [ example-2 ] ] example 2 + + + + + + + + +    figure  [ fig : ci ] shows the transverse momentum spectrum of jets in @xmath100 events measured by the cms collaboration  @xcite .",
    "the spectrum has @xmath101 bins with total count @xmath2 that was modeled using the likelihood @xmath102 this is an example of a _ binned _ likelihood .",
    "events measured by cms compared with the qcd prediction at next - to - leading order .",
    "this spectrum was used to search for evidence of contact interactions  @xcite ( courtesy cms collaboration).,scaledwidth=50.0% ]    [ [ example-3 ] ] example 3 + + + + + + + + +    figure  [ fig : type1a ] shows a plot of the distance modulus versus redshift for @xmath103 type 1a supernovae  @xcite .",
    "these heteroscedastic data , or group of items has a different uncertainty . ]",
    "@xmath104 are modeled using the likelihood @xmath105 which is an example of an _ un - binned _ likelihood .",
    "the cosmological model is encoded in the distance modulus function @xmath106 , which depends on the redshift @xmath107 and the matter density and cosmological constant parameters @xmath108 and @xmath109 , respectively .",
    "( see ref .",
    "@xcite for an accessible introduction to the analysis of these data . )     for 580 type 1a supernovae  @xcite showing a fit of the standard cosmological model ( with a cosmological constant ) to these data ( curve).,scaledwidth=50.0% ]    [ [ example-4 ] ] example 4 + + + + + + + + +    the discovery of a neutral higgs boson in 2012 by atlas  @xcite and cms  @xcite in the di - photon final state ( @xmath110 ) made use of an un - binned likelihood of the form , @xmath111 \\prod_{i=1}^n [ s f_s(x_i | m , w ) + b f_b(x_i ) ]       \\\\",
    "\\textrm{where }      x    & = \\textrm{di - photon masses } \\\\                  m    & = \\textrm{mass of boson }   \\\\                  w    & = \\textrm{width of resonance }",
    "\\\\                  s    & = \\textrm{expected ( i.e. , mean ) signal count } \\\\                  b    & = \\textrm{expected background count }   \\\\                  f_s & = \\textrm{signal probability density } \\\\                  f_b & = \\textrm{background probability density } \\\\ \\\\                  &   \\framebox{\\parbox{0.5\\textwidth}{\\textbf{exercise 6b : } show that a binned multi - poisson\\\\ likelihood yields an un - binned likelihood of this\\\\ form as the bin widths go to zero}}\\end{aligned}\\ ] ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the likelihood function is arguably the most important quantity in a statistical analysis because it can be used to answer questions such as the following .    1 .",
    "how do i estimate a parameter ? 2 .",
    "how do i quantify its accuracy ?",
    "how do i test an hypothesis ?",
    "how do i quantify the significance of a result ?",
    "writing down the likelihood function requires :    1 .   identifying all that is _",
    "known _ , e.g. , the observations , 2 .   identifying all that is _ unknown _ , e.g. , the parameters , 3 .   constructing a probability model for _",
    "both_.    many analyses in particle physics do not use likelihood functions explicitly .",
    "however , it is worth spending time to think about them because doing so encourages a deeper reflection on what is being done , a more systematic approach to the statistical analysis , and ultimately leads to better answers .",
    "being explicit about what is and is not known in an analysis problem may seem a pointless exercise ; surely these things are obvious .",
    "consider the d  top quark discovery data  @xcite , @xmath98 events observed with a background estimate of @xmath112 events .",
    "the uncertainty in 17 is invariably said to be @xmath113 . not so !",
    "the count 17 is perfectly known : it is 17 . what we are uncertain about is the mean count @xmath114 , that is , the parameter of the probability model , which we take to be a poisson distribution .",
    "the @xmath115 must somehow be a statement not about 17 but rather about the unknown parameter @xmath114 .",
    "we shall explain what the @xmath115 means in lecture 2 .",
    "in this lecture , we consider the two most important approaches to statistical inference , frequentist and bayesian . both are needed to make sense of statistical inference , though this is not the dominant opinion in particle physics .",
    "most particle physicists , if pressed , will say they are frequentist in their approach .",
    "the typical reason given is that this approach is objective , whereas the bayesian approach is not .",
    "moreover , they would argue , the frequentist approach is less arbitrary whereas the bayesian approach is plagued with arbitrariness that renders its results suspect .",
    "we wish , however , to focus on the practical , therefore , we shall sidestep this debate and assume a pragmatic attitude to both approaches .",
    "we begin with a description of salient features of the frequentist approach , followed by a description of the bayesian approach .",
    "the most important principle in this approach is that enunciated by the polish statistician jerzy neyman in the 1930s , namely ,    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * the frequentist principle *    the goal of a frequentist analysis is to construct statements so that a fraction @xmath116 of them are guaranteed to be true over an infinite ensemble of statements . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the fraction @xmath117 is called the * coverage probability * , or coverage for short , and @xmath70 is called the * confidence level * ( c.l . ) . a procedure which satisfies the frequentist principle",
    "is said to _",
    "cover_. the confidence level as well as the coverage is a property of the ensemble of statements .",
    "consequently , the confidence level may change if the ensemble changes .",
    "here is an example of the frequentist principle in action .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ [ [ example-5 ] ] example + + + + + + +    over the course of a long career , a doctor sees thousands of patients . for each patient",
    "he issues one of two conclusions :  you are sick \" or  you are well \" depending on the results of diagnostic measurements .",
    "because he is a frequentist , he has devised an approach to medicine in which although he does not know which of his conclusions were correct , he can at least retire happy in the knowledge that he was correct at least 75% of the time ! _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in a seminal paper published in 1937 , neyman  @xcite invented the concept of the confidence interval , a way to quantify uncertainty , that respects the frequentist principle .",
    "the confidence interval is such an important idea , and its meaning so different from the superficially similar bayesian concept of a credible interval , that it is worth working through the concept in detail .",
    "the confidence interval is a concept best explained by example .",
    "consider an experiment that observes @xmath118 events with expected ( that is , mean ) signal @xmath119 and no background .",
    "neyman devised a way to make statements of the form @xmath120,\\end{aligned}\\ ] ] with the _ a priori _ guarantee that at least a fraction @xmath70 of them will be true , as required by the frequentist principle . a procedure for constructing such intervals",
    "is called a * neyman construction*. the frequentist principle must hold for any ensemble of experiments , not necessarily all making the same kind of observations and statements . for simplicity",
    ", however , we shall presume the experiments to be of the same kind and to be completely specified by a single unknown parameter @xmath119 .",
    "the neyman construction is illustrated in fig .",
    "[ fig : neyman ] .    , and the space of observations with potential observations @xmath118 . for a given value of @xmath119",
    ", the observation space is partitioned into three disjoint intervals , such that the probability to observe a count @xmath118 within the interval demarcated by the two vertical lines is @xmath116 , where p = c.l .",
    "is the desired confidence level .",
    "the inequality is needed because , for discrete data , it may not be possible to find an interval with @xmath121 exactly.,scaledwidth=80.0% ]    the construction proceeds as follows .",
    "choose a value of @xmath119 and use some rule to find an interval in the space of observations ( or , more generally , a region ) , for example , the interval defined by the two vertical lines in the center of the figure , such that the probability to obtain a count in this interval is @xmath116 , where @xmath70 is the desired confidence level .",
    "we move to another value of @xmath119 and repeat the procedure .",
    "the procedure is repeated for a sufficiently dense set of points in the parameter space over a sufficiently large range . when this is done , as illustrated in fig .",
    "[ fig : neyman ] , the intervals of probability content @xmath117 will form a band in the cartesian product of the parameter space and the observation space .",
    "the upper edge of this band defines the curve @xmath122 , while the lower edge defines the curve @xmath123 .",
    "these curves are the product of the neyman construction .    for a given value of the parameter of interest @xmath119 ,",
    "the interval with probability content @xmath117 in the space of observations is not unique ; different rules for choosing the interval will , in general , yield different intervals .",
    "neyman suggested choosing the interval so that the probability to obtain an observation below or above the interval are the same .",
    "the neyman rule yields the so - called * central intervals*. one virtue of central intervals is that their boundaries can be more efficiently calculated by solving the equations , @xmath124 a mathematical fact that becomes clear after staring at fig .",
    "[ fig : neyman ] long enough .",
    "another rule was suggested by feldman and cousins  @xcite . for our example",
    ", the feldman - cousins rule requires that the potential observations @xmath125 be ordered in descending order , @xmath126 , of the likelihood ratio @xmath127 , where @xmath128 is the maximum likelihood estimator ( see sec .  [",
    "sec : profile ] ) of the parameter @xmath119 .",
    "once ordered , we compute the running sum @xmath129 until @xmath117 equals or just exceeds the desired confidence level @xmath70 .",
    "this rules does not guarantee that the potential observations @xmath118 are contiguous , but this does not matter because we simply take the minimum element of the set @xmath130 to be the lower bound of the interval and its maximum element to be the upper bound .",
    "another simple rule is the mode - centered rule : order @xmath118 in descending order of @xmath131 and proceed as with the feldman - cousins rule . in principle ,",
    "absent criteria for choosing a rule , there is nothing to prevent the use of _ ordering rules _ randomly chosen for different values of @xmath119 !",
    "figure  [ fig : ciwidths ] compares the widths of the intervals @xmath132 $ ] for three different ordering rules , central , feldman - cousins , and mode - centered as a function of the count @xmath118 .",
    "it is instructive to compare these widths with those provided by the well - known root(n ) interval , @xmath133 and @xmath134 . of the three sets of intervals , the ones suggested by neyman are the widest , the feldman - cousins and mode - centered ones are of similar width , while the root(n ) intervals are the shortest .",
    "so why are we going through all the trouble of the neyman construction ?",
    "we shall return to this question shortly .",
    "r0.5     having completed the neyman construction and found the curves @xmath122 and @xmath123 we can use the latter to make statements of the form @xmath135 $ ] : for a given observation @xmath118 , we simply read off the interval @xmath132 $ ] from the curves .",
    "for example , suppose in fig .",
    "[ fig : neyman ] that the true value of @xmath119 is represented by the horizontal line that intersects the curves @xmath122 and @xmath123 and which therefore defines the interval demarcated by the two vertical lines .",
    "if the observation @xmath118 happens to fall in the interval to the left of the left vertical line , or to the right of the right vertical line , then the interval @xmath136 $ ] will not bracket @xmath119 .",
    "however , if @xmath118 falls between the two vertical lines , the interval @xmath136 $ ] will bracket @xmath119 .",
    "moreover , by virtue of the neyman construction , a fraction @xmath117 of the intervals @xmath136 $ ] will bracket the value of @xmath119 whatever its value happens to be , which brings us back to the question about the root(n ) intervals .",
    "figure  [ fig : coverage ] shows the coverage probability over the parameter space of @xmath119 . as expected , the three rules , neyman s , that of feldman - cousins , and the mode - centered , satisfy the condition coverage probability @xmath137 confidence level over all values of @xmath119 that are possible _ a priori _ ; that is , the intervals cover",
    ". however , the root(n ) intervals do not and indeed fail badly for @xmath138 .",
    "l0.5     however , notice that the coverage probability of the root(n ) intervals bounces around the ( 68% ) confidence level for vaues of @xmath139 .",
    "therefore , if we knew for sure that @xmath139 , it would seem that using the root(n ) intervals may not be that bad after all . whether it is or not depends entirely on one s attitude towards the frequentist principle . some will lift mountains and carry them to the moon in order to achieve exact coverage , while others , including the author , is entirely happy with coverage that bounces around a little .",
    "[ [ discussion-1 ] ] discussion + + + + + + + + + +    we may summarize the content of the neyman construction with a statement of the form : there is a probability of at least @xmath70 that @xmath140 $ ] .",
    "but it would be a misreading of the statement to presume it is about that particular interval .",
    "it is not because @xmath70 , as noted , is a property of the ensemble to which this statement belongs .",
    "the precise statement is this : @xmath140 $ ] is a member of an ( infinite ) ensemble of statements a fraction @xmath116 of which are true .",
    "this mathematical fact is the principal reason why the frequentist approach is described as objective ; the probability @xmath70 is something for which there seems , in principle , to be an operational definition : we just count how many statements of the form @xmath140 $ ] are true and divide by the total number of statements .",
    "unfortunately , in the real world this procedure can not be realized because in general we are not privy to which statements are true and , even if we came down from a mountain with the requisite knowledge , we would need to examine an infinite number of statements , which is impossible .",
    "nevertheless , the neyman construction is a remarkable procedure that always yields exact coverage for any problem that depends on a _ single _ unknown parameter .",
    "matters quickly become less tidy , however , when a probability model contains more than one unknown parameter . in almost every particle physics experiment",
    "there is background that is usually not known precisely .",
    "consequently , even for the simplest experiment we must contend with at least two parameters , the expected signal @xmath119 and the expected background @xmath18 , neither of which is known .",
    "neyman required a procedure to cover whatever the value of _ all _ the parameters be they known or unknown .",
    "this is a very tall order , which can not be met in general . in practice , we resort to approximations , the most widely used of which is the profile likelihood to which we now turn .      as noted in sec .",
    "[ sec : likelihood ] , likelihood functions can be used to estimate the parameters on which they depend .",
    "the method of choice to do so , in a frequentist analysis , is called * maximum likelihood * , a method first used by karl frederick gauss , _ the prince of mathematics _ , but developed into a formidable statistical tool in the 1930s by sir ronald a. fisher  @xcite , perhaps the most influential statistician of the twentieth century .",
    "fisher showed that a good way to estimate the parameters of a likelihood function is to pick the value that maximizes it .",
    "such estimates are called ( mle ) . in general , a function into which data can be inserted to yield an mle of a parameter is called a maximum likelihood estimator . for simplicity , we shall use the same abbreviation mle to mean both the estimate and the estimator and we shall not be too picky about distinguishing the two . the d  top quark discovery example illustrates the method .    _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ [ [ example - top - quark - discovery - revisited ] ] example : top quark discovery revisited + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we start by listing @xmath141 next , we construct a probability model for the data @xmath142 assuming that @xmath2 and @xmath47 are statistically independent .",
    "since this is a counting experiment , we shall assume that @xmath143 is a poisson distribution with mean count @xmath144 . in the absence of details about how the background @xmath47 was arrived at ,",
    "the standard assumption is that data of the form @xmath145 can be modeled with a gaussian ( or normal ) density .",
    "however , we can do a bit better .",
    "background estimates are usually based on auxiliary experiments , either real or simulated , that define control regions .",
    "suppose that the observed count in the control region is @xmath146 and the mean count is @xmath147 , where @xmath74 ( ideally ) is the known scale factor between the control and signal regions .",
    "we can model these data with a poisson distribution with count @xmath146 and mean @xmath147 .",
    "but , we are given @xmath47 and @xmath148 rather than @xmath146 and @xmath74 , so we need a model to relate the two pairs of numbers .",
    "the simplest model is @xmath149 and @xmath150 from which we can infer an effective count @xmath146 using @xmath151 .",
    "what of the scale factor @xmath74 ?",
    "well , since it is not given , it must be estimated .",
    "the obvious estimate is @xmath152 . with these assumptions ,",
    "our likelihood function is @xmath153 the first term in eq .",
    "( [ eq : toplh ] ) is the likelihood for the count @xmath154 , while the second term is the likelihood for @xmath155 , or equivalently the count @xmath146 .",
    "the fact that @xmath146 is not an integer causes no difficulty : we merely write the poisson distribution as @xmath156 , which permits continuation to non - integer counts @xmath146 .",
    "the maximum likelihood estimators for @xmath119 and @xmath18 are found by maximizing eq .",
    "( [ eq : toplh ] ) , that is , by solving the equations @xmath157 as expected .",
    "a more complete analysis would account for the uncertainty in @xmath74 .",
    "one way is to introduce two more control regions with observed counts @xmath19 and @xmath158 and mean counts @xmath159 and @xmath160 , respectively , and extend eq .",
    "( [ eq : toplh ] ) with two more poisson distributions . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the maximum likelihood method is the most widely used method for estimating parameters because it generally leads to reasonable estimates . but the method has features , or encourages practices , which , somewhat uncharitably , we label the good , the bad , and the ugly !    * _ the good _ * * maximum likelihood estimators are consistent : the rms goes to zero as more and more data are included in the likelihood .",
    "this is an extremely important property , which basically says it makes sense to take more data because we shall get more accurate results .",
    "one would not knowingly use an inconsistent estimator ! * * if an unbiased estimator for a parameter exists the maximum likelihood method will find it .",
    "* * given the mle for @xmath119 , the mle for any function @xmath161 of @xmath119 is , very conveniently , just @xmath162 .",
    "this is a very nice practical feature which makes it possible to maximize the likelihood using the most convenient parameterization of it and then transform back to the parameter of interest at the end . * _ the bad ( according to some ! ) _ * * in general , mles are biased .",
    "+   + * _ the ugly ( according to some ! ) _ * * the fact that most mles are biased encourages the routine application of bias correction , which can waste data and , sometimes , yield absurdities .    here is an example of the seriously ugly .    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ [ [ example-6 ] ] example + + + + + + +    for a discrete probability distribution @xmath163 , the * moment generating function * is the ensemble average @xmath164 for the binomial , with parameters @xmath70 and @xmath73 , this is @xmath165 which is useful for calculating * moments * @xmath166 e.g. , @xmath167 for the binomial distribution . given that @xmath74 events out @xmath73 pass a set of cuts , the mle of the event selection efficiency is the obvious estimate @xmath168 .",
    "the equally obvious estimate of @xmath169 is @xmath170 .",
    "but , @xmath171 so @xmath172 is a biased estimate of @xmath169 with positive bias @xmath173 .",
    "the unbiased estimate of @xmath169 is @xmath174 , \\quad \\framebox{\\textbf{exercise 8c : } show this}\\end{aligned}\\ ] ] which , for a single success , i.e. , @xmath175 , yields the sensible estimate @xmath176 , but the less than helpful one @xmath177 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in order to infer a value for the parameter of interest , for example , the signal @xmath119 in our 2-parameter likelihood function in eq .",
    "( [ eq : toplh ] ) , the likelihood must be reduced to one involving the parameter of interest only , here @xmath119 , by somehow getting rid of all the * nuisance * parameters , here the background parameter @xmath18 .",
    "a nuisance parameter is simply a parameter that is not of current interest . in a strict frequentist calculation",
    ", this reduction to the parameter of interest must be done in such a way as to respect the frequentist principle : _ coverage probability @xmath137 confidence level_. in general , this is very difficult to do exactly .    in practice",
    ", we replace all nuisance parameters by their * conditional maximum likelihood estimates * ( cmle ) .",
    "the cmle is the maximum likelihood estimate conditional on a _ given _ value of the current parameter ( or parameters ) of interest . in the top discovery example",
    ", we construct an estimator of @xmath18 as a function of @xmath119 , @xmath178 , and replace @xmath18 in the likelihood @xmath179 by @xmath178 to yield a function @xmath180 called the * profile likelihood*.    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ since the profile likelihood entails an approximation , namely , replacing unknown parameters by their conditional estimates , it is not the likelihood but rather an approximation to it . consequently , the frequentist principle is not guaranteed to be satisfied exactly . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    this does not seem to be much progress .",
    "however , things are much better than they may appear because of an important theorem proved by wilks in 1938 .",
    "if certain conditions are met , roughly that the mles do not occur on the boundary of the parameter space and the likelihood becomes ever more gaussian as the data become more numerous  that is , in the so - called * asymptotic limit * , then if the true density of @xmath16 is @xmath143 the random number @xmath181 has a probability density that converges to a @xmath182 density with one degree of freedom .",
    "more generally , if the numerator of @xmath183 contains @xmath184 free parameters the asymptotic density of @xmath185 is a @xmath182 density with @xmath184 degrees of freedom . therefore , we may take @xmath186 to be a @xmath182 variate , at least approximately , and solve @xmath187 for @xmath119 to get approximate @xmath73-standard deviation confidence intervals .",
    "in particular , if we solve @xmath188 , we obtain approximate 68% intervals .",
    "this calculation is what minuit , and now tminuit , has done countless times since the 1970s ! wilks theorem provides the main justification for using the profile likelihood .",
    "we again use the top discovery example to illustrate the procedure .    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ [ [ example - top - quark - discovery - revisited - again ] ] example : top quark discovery revisited again + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the conditional mle of @xmath18 is found to be @xmath189    .",
    "( b ) plot of @xmath190 versus the expected signal @xmath119 .",
    "the vertical lines show the boundaries of the approximate 68% interval.,title=\"fig:\",scaledwidth=48.0% ] .",
    "( b ) plot of @xmath190 versus the expected signal @xmath119 .",
    "the vertical lines show the boundaries of the approximate 68% interval.,title=\"fig:\",scaledwidth=48.0% ]    the likelihood @xmath179 is shown in fig .",
    "[ fig : toppl](a ) together with the graph of @xmath178 .",
    "the mode ( i.e. the peak ) occurs at @xmath191 . by solving @xmath192 for @xmath119",
    "we get two solutions @xmath193 and @xmath194 .",
    "therefore , we can make the statement @xmath195 $ ] at approximately 68% c.l .",
    "figure  [ fig : toppl](b ) shows a plot of @xmath190 created using the roofit  @xcite and roostats  @xcite packages .",
    "intervals constructed this way are not guaranteed to satisfy the frequentist principle . in practice , however , their coverage is very good for the typical probability models used in particle physics , even for modest amounts of data .",
    "this is illustrated in fig .",
    "[ fig : wilks ] , which shows how rapidly the density of @xmath196 converges to a @xmath182 density for the probability distribution @xmath197 .",
    "the figure also shows what happens if we impose the restriction @xmath198 , that is , we forbid negative signal estimates .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    , of the @xmath182 density for one degree of freedom compared with the cdf @xmath199 for four different values of the mean signal and background , @xmath119 and @xmath18 .",
    "the left plot shows that even for a mean signal or background count as low as 10 , the density @xmath200 is already close to @xmath201 and therefore largely independent of @xmath119 and @xmath18 .",
    "this is true , however , only if most of the time the maximum of the likelihood occurs away from the boundary of the parameter space . in the left plot ,",
    "the signal is estimated using @xmath202 , which can , in principle , be arbitrarily negative .",
    "but , if we choose to set @xmath203 whenever @xmath204 in order to avoid negative signal estimates , we obtain the curves in the right plot . we see that for small signals , @xmath205 still depends on the parameters.,title=\"fig:\",scaledwidth=48.0% ] , of the @xmath182 density for one degree of freedom compared with the cdf @xmath199 for four different values of the mean signal and background , @xmath119 and @xmath18 .",
    "the left plot shows that even for a mean signal or background count as low as 10 , the density @xmath200 is already close to @xmath201 and therefore largely independent of @xmath119 and @xmath18 .",
    "this is true , however , only if most of the time the maximum of the likelihood occurs away from the boundary of the parameter space . in the left plot ,",
    "the signal is estimated using @xmath202 , which can , in principle , be arbitrarily negative .",
    "but , if we choose to set @xmath203 whenever @xmath204 in order to avoid negative signal estimates , we obtain the curves in the right plot . we see that for small signals , @xmath205 still depends on the parameters.,title=\"fig:\",scaledwidth=48.0% ]      it is hardly possible in experimental particle physics to avoid the testing of hypotheses , testing that invariably leads to decisions . for example , electron identification entails hypothesis testing ; given data @xmath118 we ask : is this particle an isolated electron or is it not an isolated electron ? then we decide whether or not it is and proceed on the basis of the decision that has been made . in the discovery of the higgs boson , we had to test whether , given the data available in early summer 2012 , the standard model without a higgs boson , a somewhat ill - founded background - only model , or the standard model with a higgs boson , the background @xmath206 signal model , was the preferred hypothesis .",
    "we decided that the latter model was preferred and announced the discovery of a new boson .",
    "given the ubiquity of hypothesis testing , it is important to have a grasp of the methods that have been invented to implement it .",
    "one method was due to fisher  @xcite , another was invented by neyman , and a third ( bayesian ) method was proposed by sir harold jeffreys , all around the same time .",
    "today , we tend to merge the approaches of fisher and neyman , and we hardly ever use the method of jeffreys even though in several respects the method of jeffreys and their modern variants are arguably more natural . in particle physics , we regard our fisher / neyman hybrid as sacrosanct , witness the near - religious adherence to the @xmath207 discovery rule . however , the pioneers disagreed strongly with each other about how to test hypotheses , which suggests that the topic is considerably more subtle than it seems .",
    "we first describe the method of fisher , then follow with a description of the method of neyman . for concreteness",
    ", we consider the problem of deciding between a background - only model and a background @xmath206 signal model .",
    "r0.5     [ [ fishers - approach ] ] fisher s approach + + + + + + + + + + + + + + + + +    in fisher s approach , we construct a * null hypothesis * , often denoted by @xmath208 , and _ reject _ it should some measure be judged small enough to cast doubt on the validity of this hypothesis . in our example , the null hypothesis is the background - only model , for example , the sm without a higgs boson .",
    "the measure is called a * p - value * and is defined by @xmath209 where @xmath16 is a statistic designed so that large values indicate departure from the null hypothesis .",
    "this is illustrated in fig .",
    "[ fig : pvalue1 ] , which shows the location of the observed value @xmath210 of @xmath16 .",
    "the p - value is the probability that @xmath16 could have been higher than the @xmath16 actually observed .",
    "it is argued that a small p - value implies that either the null hypothesis is false or something rare has occurred .",
    "if the p - value is extremely small , say @xmath211 , then of the two possibilities the most common response is to presume the null to be false .",
    "if we apply this method to the d  top quark discovery data , and neglect the uncertainty in null hypothesis , we find @xmath212 in order to report a more intuitive number , the common practice is to map the p - value to the @xmath213 scale defined by @xmath214 this is the number of gaussian standard deviations away from the mean is the error funtion . ] .",
    "a p - value of @xmath215 corresponds to a @xmath213 of @xmath216 .",
    "the @xmath213-value can be calculated using the root function @xmath217    r0.5     [ [ neymans - approach ] ] neyman",
    "s approach + + + + + + + + + + + + + + + + +    in neyman s approach _ two _ hypotheses are considered , the null hypothesis @xmath208 and an alternative hypothesis @xmath218 .",
    "this is illustrated in fig .",
    "[ fig : neymantest1 ] . in our example",
    ", the null is the same as before but the alternative hypothesis is the sm with a higgs boson .",
    "again , one generally chooses @xmath16 so that large values would cast doubt on the validity of @xmath208 .",
    "however , the neyman test is specifically designed to respect the frequentist principle , which is done as follows .",
    "a _ fixed _",
    "probability @xmath219 is chosen , which corresponds to some threshold value @xmath220 defined by @xmath221 called the significance ( or size ) of the test .",
    "should the observed value @xmath222 , or equivalently , p - value(@xmath210 ) @xmath223 , the hypothesis @xmath208 is rejected in favor of the alternative . in particle physics , in addition to applying the neyman hypothesis test , we also report the p - value .",
    "this is sensible because there is a more information in the p - value than merely reporting the fact that a null hypothesis was rejected at a significance level of @xmath219 .",
    "the neyman method satisfies the frequentist principle by construction . since the significance of the test",
    "is fixed , @xmath219 is the relative frequency with which true null hypotheses would be rejected and is called the * type i * error rate .    l0.5     however , since we have specified an alternative hypothesis there is more that can be said . figure  [ fig : neymantest1 ] shows that we can also calculate @xmath224 which is the relative frequency with which we would reject the hypothesis @xmath218 if it is true .",
    "this mistake is called a error .",
    "the quantity @xmath225 is called the * power * of the test and is the relative frequency with which we would accept the hypothesis @xmath218 if it is true .",
    "obviously , for a given @xmath219 we want to maximize the power .",
    "indeed , this is the basis of the neyman - pearson lemma ( see for example ref .",
    "@xcite ) , which asserts that given two simple hypotheses  that is , hypotheses in which all parameters have well - defined values  the optimal statistic @xmath185 to use in the hypothesis test is the likelihood ratio @xmath226 . maximizing the power seems sensible . consider fig .",
    "[ fig : neymantest2 ] .",
    "the significance of the test in this figure is the same as that in fig .",
    "[ fig : neymantest1 ] , so the type i error rate is identical . however , the type ii error rate is much greater in fig .",
    "[ fig : neymantest2 ] than in fig .",
    "[ fig : neymantest1 ] , that is , the power of the test is considerably weaker in the former . in that case , there may be no compelling reason to reject the null since the alternative is not that much better .",
    "this insight was one source of neyman s disagreement with fisher .",
    "neyman objected to possibility that one might reject a null hypothesis regardless of whether it made sense to do so .",
    "neyman insisted that the task is always one of deciding between competing hypotheses .",
    "fisher s counter argument was that an alternative hypothesis may not be available , but we may nonetheless wish to know whether the only hypothesis that is available is worth keeping . as we shall see , the bayesian approach also requires an alternative , in agreement with neyman , but in a way that neither he nor fisher agreed with !",
    "we have assumed that the hypotheses @xmath208 and @xmath218 are simple , that is , fully specified .",
    "unfortunately , most of the hypotheses that arise in realistic particle physics analyses are not of this kind . in the higgs boson discovery analyses by atlas and",
    "cms the probability models depend on many nuisance parameters for which only estimates are available .",
    "consequently , neither the background - only nor the background @xmath206 signal hypotheses are fully specified .",
    "such hypotheses are called * compound hypotheses*. in order to illustrate how hypothesis testing proceeds in this case , we again turn again to the top discovery example .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ [ [ example-7 ] ] example + + + + + + +    as we saw in sec .",
    "[ sec : profile ] , the standard way to handle nuisance parameters in the frequentist approach is to replace them by their conditional mles and thereby reduce the likelihood function to the profile likelihood . in the top discovery example , we obtain a function @xmath180 that depends on the single parameter , @xmath119 .",
    "we now treat this function as if it were a likelihood and invoke both the neyman - pearson lemma , which suggests the use of likelihood ratios , and wilks theorem to motivate the use of the function @xmath196 given in eq .",
    "( [ eq : wilks ] ) to distinguish between two hypotheses : the hypothesis @xmath218 in which @xmath191 and the hypothesis @xmath208 in which @xmath227 , for example , the background - only hypothesis @xmath228 . in the context of testing",
    ", @xmath196 is called a * test statistic * , which , unlike a statistic as we have defined it ( see sec .",
    "[ sec : statistics ] ) , usually depends on at least one unknown parameter .    in principle , the next step is the computationally arduous task of simulating the distribution of the statistic @xmath196 .",
    "the task is arduous because _ a priori _ the probability density @xmath200 can depend on _ all _ the parameters that exist in the original likelihood .",
    "if this is really the case , then after all this effort we seem to have achieved a pyrrhic victory ! but , this is where wilks theorem saves the day , at least approximately .",
    "we can avoid the burden of simulating @xmath196 because the latter is approximately a @xmath182 variate .    using @xmath154 and @xmath228",
    ", we find @xmath229 . according to the results shown in fig .",
    "( [ fig : toppl])(a ) , @xmath154 may can be considered  a lot of data \" ; therefore , we may use @xmath230 to implement a hypothesis test by comparing @xmath230 with a fixed value @xmath231 corresponding to the significance level @xmath219 of the test . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _",
    "in this lecture , we introduce the bayesian approach to inference starting with a description of its salient features and ending with a detailed example , again using the top quark discovery data from d .    the main point to be understood about the bayesian approach",
    "is that it is merely applied probability theory ( see sec .",
    "[ sec : prob ] ) .",
    "a method is bayesian if    * it is based on the degree of belief interpretation of probability and * it uses bayes theorem @xmath232    for _ all _ inferences .",
    "the result of a bayesian inference is the posterior density @xmath233 from which , if desired , various summaries can be extracted .",
    "the parameters can be discrete or continuous and nuisance parameters are eliminated by marginalization , @xmath234 the function @xmath235 , called the prior , encodes whatever information we have about the parameters @xmath0 and @xmath236 independently of the data @xmath118 . a key feature of the bayesian approach is recursion ; the use of the posterior density @xmath237 or one , or more , of its marginals as the prior in a subsequent analysis .",
    "these simple rules yield an extremely powerful and general inference model . why then is the bayesian approach not more widely used in particle physics ?",
    "the answer is partly historical : the frequentist approach was dominant at the dawn of particle physics .",
    "it is also partly the widespread perception that the bayesian approach is too subjective to be useful for scientific work .",
    "however , there is published evidence that this view is mistaken , witness the success of bayesian methods in high - profile analyses in particle physics such as the discovery of single top quark production at the tevatron  @xcite .",
    "conceptually , hypothesis testing in the bayesian approach ( also called model selection ) proceeds exactly the same way as any other bayesian calculation : we compute the posterior density , @xmath238 and marginalize it with respect to all parameters except the ones that label the hypotheses or models , @xmath239 , @xmath240 equation  ( [ eq : phd ] ) is the probability of hypothesis @xmath239 given the observed data @xmath118 . in principle , the parameters @xmath236 could also depend on @xmath239 .",
    "for example , suppose that @xmath239 labels different parton distribution function ( pdf ) models , say ct10 , mstw , and nnpdf , then @xmath236 would indeed depend on the pdf model and should be written as @xmath241 .",
    "it is usually more convenient to arrive at the probability @xmath242 in stages .    1 .",
    "factorize the prior in the most convenient form , @xmath243 often , we can assume that the parameters of interest @xmath0 are independent , _ a priori _ , of both the nuisance parameters @xmath241 and the model label @xmath239 , in which case we can write , @xmath244 .",
    "2 .   then , for each hypothesis , @xmath239 , compute the function @xmath245 3 .   then , compute the probability of each hypothesis , @xmath246    clearly , in order to compute @xmath247 it is necessary to specify the priors @xmath248 and @xmath249 . with some effort , it is possible to arrive at an acceptable form for @xmath248",
    ", however , it is highly unlikely that consensus could ever be reached on the discrete prior @xmath249 . at best , one may be able to adopt a convention .",
    "for example , if by convention two hypotheses @xmath208 and @xmath218 are to be regarded as equally likely , _ a priori _ , then it would make sense to assign @xmath250 .",
    "one way to circumvent the specification of the prior @xmath249 is to compare the probabilities , @xmath251 \\",
    ",               \\frac { \\pi(h_1 ) } { \\pi(h_0)}.          \\end{aligned}\\ ] ] and use only the term in brackets , called the global * bayes factor * , @xmath252 , as a way to compare hypotheses .",
    "the bayes factor specifies by how much the relative probabilities of two hypotheses changes as a result of incorporating new data , @xmath118 .",
    "the word global indicates that we have marginalized over all the parameters of the two models .",
    "the _ local _",
    "bayes factor , @xmath253 is defined by @xmath254 are the * marginal * or integrated likelihoods in which we have assumed the _ a priori _ independence of @xmath0 and @xmath255 .",
    "we have further assumed that the marginal likelihood @xmath208 is independent of @xmath0 , which is a very common situation .",
    "for example , @xmath0 could be the expected signal count @xmath119 , while @xmath256 could be the expected background @xmath18 . in this case",
    ", the hypothesis @xmath208 is a special case of @xmath218 , namely , it is the same as @xmath218 with @xmath228 .",
    "an hypothesis that is a special case of another is said to be * nested * in the more general hypothesis .",
    "the bayesian example , discussed below , will make this clearer .",
    "there is a subtlety that may be missed : because of the way we have defined @xmath257 , we need to multiply @xmath258 by the prior @xmath259 and then integrate with respect to @xmath0 in order to calculate @xmath260 .",
    "constructing a prior for nuisance parameters is generally neither controversial ( for most parameters ) nor problematic .",
    "such difficulties as do arise occur when the priors must , of necessity , depend on expert judgement .",
    "for example , one theorist may insist that a uniform prior within a finite interval is a reasonable prior for the factorization scale in a qcd calculation , while in the expert judgement of another the interval should be twice as large .",
    "clearly , in this case , there is no getting around the fact that the prior for this parameter is unavoidably subjective .",
    "however , once a choice is made , a prior @xmath261 that integrates to one can be constructed .",
    "the achilles heal of the bayesian approach is the need to specify the prior @xmath259 , for the parameters of interest , at the start of the inference chain when we know almost nothing about these parameters .",
    "careless specification of this prior can yield results that are unreliable or even nonsensical .",
    "the mandatory requirement is that the posterior density be proper , that is integrate to unity .",
    "ideally , the same should hold for priors .",
    "a very extensive literature exists on the topic of prior specification when the available information is extremely limited .",
    "however , a discussion of this topic is beyond the scope of these lectures ; but , we shall make a few remarks .    for model selection , we need to proceed with caution because bayes factor are sensitive to the choice of priors and therefore less robust than posterior densities .",
    "suppose that the prior @xmath262 , where @xmath263 is a normalization constant .",
    "the global bayes factor for the two hypotheses @xmath218 and @xmath208 can be written as @xmath264 therefore , if the constant @xmath263 is ill defined , typically because @xmath265 , the bayes factor will likewise be ill defined . for this reason , it is generally recommended that an improper prior not be used for parameters @xmath0 that occur only in one hypothesis , here @xmath218 . however , for parameters that are common to all hypotheses , it is permissible to use improper priors because the ill defined constant cancels in the bayes factor .",
    "the discussion so far has been somewhat abstract .",
    "the next section therefore works through a detailed example of a possible bayesian analysis of the d  top discovery data .",
    "r0.5       in this section we shall perform the following calculations as a way to illustrate a typical bayesian analysis ,    1 .",
    "compute the posterior density @xmath266 , 2 .",
    "compute a 68% credible interval @xmath132 $ ] , and 3 .",
    "compute the global bayes factor @xmath267 .",
    "the first step in any serious statistical analysis is to think deeply about what has been done in the physics analysis ; for example , to trace in detail the steps that led to the background estimates , determine the independent systematic effects and identify explicitly what is known about them .",
    "although , by tradition , we tend to think of potential data @xmath16 separately from the parameters @xmath119 and @xmath18 , it should be recognized that this is done for convenience .",
    "the full probability model is the joint probability @xmath268 which , as is true of _ all _ probability models , is conditional on the information and assumptions , @xmath269 , that define the abstract space @xmath37 ( see sec .  [",
    "sec : prob ] ) .",
    "in these lectures , we have omitted the conditioning data @xmath269 , and will continue to do so here , but it should not be forgotten that it is always present and may differ from one probability model to another .",
    "the full probability model @xmath270 can be factorized is several ways , all of which are mathematically valid .",
    "however , we find it convenient to factorize the model in the following way @xmath271 where we have introduced the symbol @xmath12 in order to highlight the distinction we choose to make between this part of the model and the remainder .",
    "we are entirely free to decide how much of the model we place in @xmath272 and how much in @xmath273 ; what matters is the form of the full model @xmath270 . in the frequentist analysis of the top quark discovery data , we took @xmath2 and @xmath47 to be the data @xmath118 .",
    "we did so because in the frequentist approach , the function @xmath273 does not exist and consequently we have no choice but to include everything in the function @xmath143 .",
    "one virtue of a bayesian perspective is that we are not bound by this stricture . to make the point explicitily ,",
    "we take the probability distribution , @xmath272 , to be @xmath274 the interpretation of @xmath272 is clear : it is the probability to observe @xmath16 events _ given _ that the mean event count is @xmath144 . what does @xmath273 represent",
    "this function is the * prior * that encodes what we _ know _ , or _",
    "assume _ , about the mean background and signal independently of the potential observations @xmath16 .",
    "the prior @xmath273 can be factored in two ways , @xmath275 both of which accord with the probability rules .",
    "the factorizations remind us that the parameters @xmath119 and @xmath18 may not be probabilistically independent .",
    "however , we shall assume that they are , at least at this stage of the analysis , in which case it is permissible to write , @xmath276    we first consider the background prior @xmath277 and ask : what do we know about the background ?",
    "we know the count @xmath146 in the control region and we have an estimate of the control region to signal region scale factor @xmath74 .",
    "the likelihood for @xmath146 is taken to be @xmath278 from which , together with a prior @xmath279 , we can compute the posterior density @xmath280 as usual , we factorize the prior , @xmath281 , where we have introduced the subscript @xmath282 to distinguish @xmath283 from the background prior associated with eq .",
    "( [ eq : pxsb ] ) .",
    "then , we consider the separate factors @xmath283 and @xmath284",
    ".    what do we know about @xmath18 at this stage ?",
    "clearly , @xmath285 .",
    "but , that is all we know apart from the background likelihood , eq .  ( [ eq : pqkb ] ) . today , after a century of argument and discussion",
    ", the consensus amongst statisticians is that there is no unique way to represent such vague information .",
    "however , well founded ways to construct such priors are available , see for example ref .",
    "@xcite and references therein ; but for simplicity we take the prior @xmath286 , that is , the * flat prior*. if the uncertainty in @xmath74 can be neglected , the ( proper ! ) prior for @xmath74 is @xmath287 , which amounts to replacing @xmath74 in eq .",
    "( [ eq : pbqk ] ) by @xmath288 .",
    "when the dust settles , we find @xmath289 for the posterior density of @xmath18 , which can serve as the prior @xmath277 associated with eq .",
    "( [ eq : pxsb ] ) .    by construction , @xmath270 is identical in form to the likelihood in eq .",
    "( [ eq : toplh ] ) ; we have simply availed ourselves of the freedom to factorize @xmath270 as we wish and therefore to reinterpret the factors .",
    "this freedom is useful because it makes it possible to keep the likelihood simple while relegating the complexity to the prior .",
    "this may not seem , at first , to be terribly helpful ; after all , we arrived at the same mathematical form as eq .",
    "( [ eq : toplh ] ) .",
    "however , the complexity can be substantially mitigated through the numerical treatment of the prior , as discussed at the end of the next section .",
    "the likelihood , as we have conceptualized the problem , is given by @xmath290 where @xmath98 events .    the final ingredient is the prior @xmath291 . at this stage ,",
    "all we know is that @xmath292 .",
    "again , there is no unique way to specify @xmath291 , though as noted there are well founded methods to construct it .",
    "we shall variously assume either the improper prior @xmath293 or the proper prior @xmath294 .",
    "after this somewhat discursive discussion of the probability model , we have done the hard part : building the full probability model .",
    "hereafter , the rest of the bayesian analysis is mere computation .",
    "it is convenient to eliminate the nuisance parameter @xmath18 , @xmath295 and thereby arrive at the marginal likelihood @xmath296 .",
    "this example , the * poisson - gamma * model is particularly simple and lends itself to exact calculation",
    ". however , the complexity rapidly increases as the prior becomes more and more complicated . in the probability model that is used in the higgs boson analyses at the lhc , the part we would consider the prior , @xmath297 , is of enormous complexity . however , the part that we would call the likelihood , @xmath298 , is relatively simple .",
    "the parameter @xmath14 denotes one or more signal strengths  the ratio of the cross section times branching fraction to that predicted by the standard model ( sm ) , and @xmath299 is the higgs boson mass .",
    "the parameter @xmath236 represent the expected ( and therefore unknown ) sm signal predictions and the expected backgrounds .",
    "when faced with such complexity , it proves useful to use a * hierarchical bayesian model*. briefly , the prior @xmath297 is written as @xmath300 the prior @xmath301 models the lowest level systematic parameters that define quantities such as the jet energy scale , lepton efficiencies , trigger efficiencies , and the parton distribution functions .",
    "it is usually straightforward to sample from this prior .",
    "moreover , the function @xmath302 is nothing more than prior for the expected signal and background parameters @xmath236 , which through estimates @xmath303 depend implicitly on the parameters @xmath304 .",
    "the prior @xmath302 is generally quite simple ; for binned data it is just a product of gamma ( or gamma mixture ) densities ; more generally , it is a product of gamma , gaussian , or log - normal densities .",
    "consequently , the marginalizations over @xmath236 can be done in two steps : first generate a point @xmath305 from @xmath306 , then generate a point @xmath307 from @xmath308 . in that way",
    ", the enormous complexity of explicitly modeling the dependence of @xmath236 on @xmath304 is avoided , with the added benefit that all , possibly very complicated , correlations ( in principle , to all orders ) are accounted for automatically .",
    "the marginal likelihood can be approximated by @xmath309 what we have just described is merely integration via a monte carlo approximation .",
    "the point is that the sampling required to compute @xmath310 can be run in @xmath311 parallel analysis jobs , each of which is given a different random number seed in order to sample a single pair of points @xmath312 and @xmath313 .",
    "the results of such a bayesian analysis would be the likelihood @xmath314 and an ensemble of points @xmath315 .",
    "given the marginal likelihood @xmath296 and a prior @xmath291 we can compute the posterior density , @xmath316 again , for simplicity , we assume a flat prior for the signal , @xmath293 and find @xmath317 from which we can compute the central * credible interval * @xmath318 $ ] for @xmath119 at 68% c.l . , which is shown in fig .",
    "[ fig : post ] .      as noted , the number @xmath319 can be used to perform a hypothesis test .",
    "but , as argued above , we need to use a proper prior for the signal , that is , a prior that integrates to one .",
    "the simplest such prior is a @xmath320-function , e.g. , @xmath294 . using this prior",
    ", we find @xmath321 since the background - only hypothesis @xmath208 is nested in @xmath218 , and defined by @xmath228 , the number @xmath322 is given by @xmath323 , which yields @xmath324 we conclude that the hypothesis @xmath325 is favored over @xmath228 by a bayes factor of 24,000 . in order to avoid large numbers ,",
    "the bayes factor can be mapped into a ( signed ) measure akin to the frequentist  @xmath73-sigma \"  @xcite , @xmath326 which gives @xmath327 .",
    "negative values of @xmath213 correspond to hypotheses that are excluded .",
    "these lectures gave an overview of the main ideas of statistical inference in a form directly applicable to statistical analysis in particle physics .",
    "two widely used approaches were covered , frequentist and bayesian . while we tried to focus on the practical",
    ", our hope is that we have given just enough commentary about the topics to place them in some intellectual context .",
    "we hope that the take away message is that is it worth learning a bit more about statistics if only to avoid fruitless arguments and discussions with co - workers .",
    "statistics is not physics . nature is the ultimate arbiter of which physics ideas are  correct \" .",
    "unfortunately , the ultimate arbiter of statistical ideas , apart from the mundanity of mathematical correctness , is intellectual taste .",
    "therefore , the other take home message is    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  have the courage to you use your own understanding \"    immanuel kant _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _",
    "i thank nick ellis , martijn mulders , kate ross , and their counterparts from jinr , for organizing and hosting a very enjoyable school , and the students for their keen participation and youthful enthusiasm .",
    "these lectures were supported in part by us department of energy grant de - fg02 - 13er41942 .",
    "f.  abe _ et al . _",
    "[ cdf collaboration ] , `` observation of top quark production in @xmath328 collisions , '' phys .",
    "lett .   * 74 * , 2626 ( 1995 ) [ hep - ex/9503002 ] .",
    "s.  abachi _ et al . _",
    "[ d0 collaboration ] , `` observation of the top quark , '' phys .",
    "lett .   * 74 * , 2632 ( 1995 ) [ hep - ex/9503003 ] .",
    "s.  chatrchyan _ et al . _",
    "[ cms collaboration ] , `` search for contact interactions using the inclusive jet @xmath329 spectrum in @xmath70p collisions at @xmath330 tev , '' phys .",
    "d * 87 * , 052017 ( 2013 ) [ arxiv:1301.5023 [ hep - ex ] ] .",
    "n.  suzuki , d.  rubin , c.  lidman , g.  aldering , r.  amanullah , k.  barbary , l.  f.  barrientos and j.  botyanszki _ et al .",
    "_ , `` the hubble space telescope cluster supernova survey : v. improving the dark energy constraints above z>1 and building an early - type - hosted supernova sample , '' astrophys .",
    "j.   * 746 * , 85 ( 2012 ) [ arxiv:1105.3470 [ astro-ph.co ] ] .",
    "r.  dungan and h.  b.  prosper , `` varying - g cosmology with type ia supernovae , '' arxiv:0909.5416 [ astro-ph.co ] .",
    "g.  aad _ et al . _ [ atlas collaboration ] , `` observation of a new particle in the search for the standard model higgs boson with the atlas detector at the lhc , '' phys .",
    "b * 716 * , 1 ( 2012 ) [ arxiv:1207.7214 [ hep - ex ] ] .",
    "s.  chatrchyan _ et al . _",
    "[ cms collaboration ] , `` observation of a new boson at a mass of 125 gev with the cms experiment at the lhc , '' phys .",
    "b * 716 * , 30 ( 2012 ) [ arxiv:1207.7235 [ hep - ex ] ] .",
    "j. neyman ,  outline of a theory of statistical estimation based on the classical theory of probability , \" phil .",
    "london a236 , 333 ( 1937 ) .",
    "g.  fidecaro _ et al . _",
    "[ cern - rutherford - ill - sussex - padua ( crisp ) collaboration ] , `` experimental search for neutron anti - neutron transitions with free neutrons , '' phys .",
    "b * 156 * , 122 ( 1985 ) . v.  m.  abazov _ et al . _",
    "[ d0 collaboration ] , `` observation of single top quark production , '' phys .",
    "lett .   * 103 * , 092001 ( 2009 ) [ arxiv:0903.0850 [ hep - ex ] ] .",
    "t.  aaltonen _ et al .",
    "_ [ cdf collaboration ] , `` first observation of electroweak single top quark production , '' phys .  rev .",
    "lett .   * 103 * , 092002 ( 2009 ) [ arxiv:0903.0885 [ hep - ex ] ] .",
    "l.  demortier , s.  jain and h.  b.  prosper , `` reference priors for high energy physics , '' phys .  rev .",
    "d * 82 * , 034002 ( 2010 ) [ arxiv:1002.1111 [ stat.ap ] ] .",
    "s. sekmen _",
    "_ ,  phenomenological mssm interpretation of the cms 2011 5fb-1 results , \" cms physics analysis summary , cms - pas - sus-12 - 030 , cern ( 2012 ) ."
  ],
  "abstract_text": [
    "<S> these lectures introduce the basic ideas and practices of statistical analysis for particle physicists , using a real - world example to illustrate how the abstractions on which statistics is based are translated into practical application . </S>"
  ]
}