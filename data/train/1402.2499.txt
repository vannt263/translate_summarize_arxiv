{
  "article_text": [
    "while conventional causal inference methods @xcite use conditional independences to infer a directed acyclic graph of causal relations among at least three random variables , there is a whole family of recent methods that employ more information from the joint distribution than just conditional independences @xcite .",
    "therefore , these methods can even be used for inferring the causal relation between just two observed variables ( i.e. , the task to infer whether @xmath0 causes @xmath1 or @xmath1 causes @xmath0 , given that there is no common cause and exactly one of the alternatives is true , becomes solvable ) .",
    "as theoretical basis for such inference rules , @xcite postulate the following asymmetry between cause and effect : if @xmath0 causes @xmath1 then @xmath2 and @xmath3 represent independent mechanisms of nature and therefore contain no information about each other . here ,",
    "`` information '' is understood in the sense of description length , i.e. , knowing @xmath2 provides no shorter description of @xmath3 and vice versa , if description length is identified with kolmogorov complexity .",
    "this makes the criterion empirically undecidable because kolmogorov complexity is uncomputable @xcite .",
    "@xcite pointed out that `` information '' can also be understood in terms of predictability and used this to formulate the following hypothesis : semi - supervised learning ( ssl ) is only possible from the effect to the cause but not visa versa .",
    "this is because , if @xmath0 causes @xmath1 , knowing the distribution @xmath4 may help in better predicting @xmath0 from @xmath1 since it may contain information about @xmath5 , but @xmath2 can not help in better predicting @xmath1 from @xmath0 .",
    "information - geometric causal inference ( igci ) @xcite has been proposed for inferring the causal direction between just two variables @xmath0 and @xmath1 . in its original formulation",
    "it applies only to the case where @xmath0 and @xmath1 are related by an invertible functional relation , i.e. , @xmath6 and @xmath7 , but some positive empirical results have also been reported for noisy relations @xcite .",
    "we will also restrict our attention to the noiseless case .",
    "this is because attempts to generalize the theory to non - deterministic relations are only preliminary @xcite .",
    "moreover , the deterministic toy model nicely shows _ what kind _ of dependences between @xmath4 and @xmath5 occur while the dependences in the non - deterministic case are not yet well understood .",
    "we first rephrase how igci has been introduced in the literature and then explain our new interpretations .",
    "they also provide a better intuition about the relation to ssl . for didactic reasons",
    "we restrict the attention to the case where @xmath8 is a monotonously increasing diffeomorphism of @xmath9 $ ] .",
    "we assume that @xmath2 and @xmath4 have strictly positive densities @xmath10 and @xmath11 .",
    "we often write @xmath12 instead of @xmath13 whenever this causes no confusion .",
    "then @xcite assumes :    [ psot : uncorr ] for @xmath0 causing @xmath1 , @xmath14    the interpretation that ( [ uncorr ] ) is an independence condition becomes more clear when the functions @xmath15 and @xmath16 are interpreted as random variables on @xmath9 $ ] .",
    "then , the difference between the left and the right hand side of ( [ uncorr ] ) is the covariance of @xmath10 and @xmath17 with respect to the uniform distribution @xcite .",
    "the intuition is that it is unlikely , if @xmath8 and @xmath2 are chosen independently , that regions where the slope of @xmath8 is large ( i.e. large @xmath17 ) , meet regions where @xmath10 is large and others where @xmath10 is small .",
    "simple calculations @xcite show that ( [ uncorr ] ) implies that @xmath11 is positively correlated with the slope of @xmath18 since @xmath19 with equality iff @xmath20 .",
    "this is illustrated in figure  [ fig : corr ] a ) .",
    "a ) ( taken from @xcite ) if the fluctuations of @xmath10 do nt correlate with the slope of the functions , regions of high density @xmath11 tend to occur more often in regions where @xmath8 is flat .",
    "b ) @xmath21 maps the cube to itself .",
    "the regions of points @xmath22 with large @xmath23 ( here : the leftmost sphere ) can only be a small fraction of the cube.,title=\"fig:\",scaledwidth=30.0% ]   a ) ( taken from @xcite ) if the fluctuations of @xmath10 do nt correlate with the slope of the functions , regions of high density @xmath11 tend to occur more often in regions where @xmath8 is flat .",
    "b ) @xmath21 maps the cube to itself .",
    "the regions of points @xmath22 with large @xmath23 ( here : the leftmost sphere ) can only be a small fraction of the cube.,title=\"fig:\",scaledwidth=33.0% ]    moreover , using @xmath24 , eq .",
    "( [ uncorr ] ) implies @xmath25 using @xmath26 we get @xmath27 with equality only for @xmath28 . for empirical data @xmath29 , with @xmath30 ( and hence @xmath31 ) , this suggests the following inference method :    [ def : igci ] infer @xmath32 whenever @xmath33    some robustness of igci with respect to adding noise has been reported @xcite when the following modification is used : on the left hand side of ( [ empigci ] ) the @xmath34-tuples are ordered such that @xmath30 , while the right hand side assumes @xmath35 .",
    "note that in the noisy case , the two conditions are not equivalent .",
    "moreover , the left hand side of ( [ empigci ] ) is no longer minus the right hand side since eq .",
    "( [ minus ] ) no longer makes sense .",
    "albeit hard to formulate explicitly , it is intuitive to consider the left hand side as measuring `` non - smoothness '' of @xmath3 and the right hand side the one of @xmath5",
    ". then , the causal direction is the one with the smoother conditional .    to describe the information theoretic content of ( [ uncorr ] ) and ( [ pc ] )",
    ", we introduce the uniform distributions @xmath36 and @xmath37 for @xmath0 and @xmath1 , respectively .",
    "their images under @xmath8 and @xmath18 are given gy the probability densities @xmath38 and @xmath39 , respectively .",
    "we will drop the superscripts @xmath8 and @xmath18 whenever the functions they refer to are clear . then",
    "( [ uncorr ] ) reads @xmath40 and is equivalent to the following additivity of relative entropies @xcite : @xmath41 likewise , ( [ pc ] ) reads @xmath42 in the terminology of information geometry @xcite , ( [ orth1 ] ) means that the vector connecting @xmath10 and @xmath36 is orthogonal to the one connecting @xmath36 and @xmath43 .",
    "thus the `` independence '' between @xmath10 and @xmath8 has been phrased in terms of orthogonality , where @xmath8 is represented by @xmath44 .",
    "likewise , the dependence between @xmath11 and @xmath18 corresponds to the fact that the vector connecting @xmath11 and @xmath37 is not orthogonal to the one connecting @xmath37 and @xmath45 .",
    "the information - theoretic formulation motivates why one postulates uncorrelatedness of @xmath10 and @xmath17 instead of one between @xmath10 and @xmath46 itself .",
    "a further advantage of this reformulation is that @xmath36 and @xmath37 can then be replaced with other `` reference measures '' , e.g. , gaussians with the same variance and mean as @xmath10 and @xmath11 , respectively ( which is more appropriate for variables with unbounded domains ) @xcite .",
    "however , both conditions ( [ uncorr ] ) and ( [ orth1 ] ) are quite abstract .",
    "therefore , we want to approach igci from completely different directions . in section  [ sec : untyp ] we will argue that a large positive value for @xmath47 shows that the observed @xmath34-tuple @xmath48 is untypical in the space of all possible @xmath34-tuples . in section  [ sec : numb ] we show that condition ( [ empigci ] ) implies that there are , in a sense , more functions from @xmath0 to @xmath1 than vice versa . in section  [ sec : ssl ] we explain why the correlation between distribution and slope that occurs in the anticausal direction helps for unsupervised and semi - supervised learning .",
    "let us consider again a monotonously increasing diffeomorphism @xmath49\\rightarrow [ 0,1]$ ] and explain in which sense a point @xmath50 $ ] can have a `` typical '' or an `` untypical '' position relative to @xmath8 .",
    "consider the function @xmath8 shown in figure  [ fig : one_point ] a ) .",
    "the point @xmath51 is untypical because it meets @xmath8 in a region whose slope is larger than for the majority of points . of course , @xmath51 can also be untypical in the sense that the slope of @xmath51 is smaller than for the majority of points , see figure  [ fig : one_point ] b )",
    ". there is , however , an important asymmetry between large slope and small slope : if the slope at @xmath51 is significantly higher than the _ average _ slope over the entire domain , then @xmath51 is necessarily untypical because the slope can significantly exceed the average only for a small fraction of points .",
    "if the slope is significantly _ below _ the average , this does not mean that the point is untypical because this may even be the case for most of the points , as one can easily see on figure  [ fig : one_point ] a ) .",
    "this asymmetry is known from statistics : a non - negative random variable may quite often attain values that are smaller than their expected value by orders of magnitude , but exceeding the expectation by a large factor is unlikely due to the markov inequality .",
    "( a ) @xmath51 has an untypical position in both cases because it meets in a small region with large slope or in a region with small slope ( b ) .",
    "c ) function on a grid .",
    "the large dots denote the observed points , the small ones visualize one option of interpolating by a monotonic function that crosses @xmath52 and @xmath53.,title=\"fig:\",scaledwidth=30.0% ]   ( a ) @xmath51 has an untypical position in both cases because it meets in a small region with large slope or in a region with small slope ( b ) .",
    "c ) function on a grid .",
    "the large dots denote the observed points , the small ones visualize one option of interpolating by a monotonic function that crosses @xmath52 and @xmath53.,title=\"fig:\",scaledwidth=30.0% ]   ( a ) @xmath51 has an untypical position in both cases because it meets in a small region with large slope or in a region with small slope ( b ) .",
    "c ) function on a grid .",
    "the large dots denote the observed points , the small ones visualize one option of interpolating by a monotonic function that crosses @xmath52 and @xmath53.,title=\"fig:\",scaledwidth=30.0% ]    the above idea straightforwardly generalizes to mappings between multi - dimensional spaces : then a point @xmath54 can be untypical relative to a function @xmath21 in the sense that the jacobian of @xmath21 is significantly larger than the average .",
    "this is , for instance , the case for the points in the leftmost sphere of figure  [ fig : corr ] b ) .",
    "we first introduce `` untypical '' within a general measure theoretic setting :    [ thm : unt ] let @xmath55 and @xmath56 be probability distributions on measure spaces @xmath57 and @xmath58 , respectively .",
    "let @xmath59 be measurable and let the image of @xmath55 under @xmath21 have a strictly positive density @xmath60 with respect to @xmath56 .",
    "then , points @xmath22 for which @xmath61 are unlikely ( `` untypical '' ) in the sense that @xmath62 for all @xmath63 .",
    "[ cor : hyper ] let @xmath64^n \\rightarrow [ 0,1]^n$ ] be a diffeomorphism .",
    "then the volume of all points @xmath22 for which @xmath65 is at most @xmath66 .",
    "the corollary is very intuitive : since the average of @xmath23 over the hypercube is @xmath67 , the fraction of @xmath22 for which the jacobian is significantly larger than @xmath67 is small .",
    "whenever we observe a point @xmath22 whose jacobian is significantly larger than @xmath67 , we are skeptical about whether it has been chosen independently of @xmath21 .",
    "we now describe in which sense igci rejects observations @xmath68 that are untypical .",
    "for @xmath69 we observe @xmath70 if the right hand side of ( [ fact ] ) is significantly larger than zero , then @xmath22 is untypical because this holds only for a small fraction of the hypercube .",
    "this suggests the following reinterpretation of igci : due to ( [ expec ] ) and ( [ minus ] ) , the right hand side of ( [ empigci ] ) will usually be positive when @xmath32 is the true causal direction . whenever it attains a `` large '' positive value , the expression @xmath71 is also large because the former is an approximation of the latter .",
    "then , @xmath72 is untypical for the function @xmath73 , which makes us rejecting the causal hypothesis @xmath74 .",
    "we now argue that igci , roughly speaking , amounts to choosing the direction for which there is a larger number of functions that fit the data .",
    "we will also discuss some connections to inductive principles of statistical learning theory @xcite .    to get a clear sense of `` number of functions '' ,",
    "we discretize the interval @xmath9 $ ] for @xmath0 and @xmath1 and assume that all @xmath75 are taken from the grid @xmath76 as in figure  [ fig : one_point ] c ) .",
    "we assume furthermore that @xmath77 and , similarly , @xmath78 and denote these observations by @xmath68 and @xmath72 .",
    "let @xmath79 be the set of all monotonic functions for which @xmath80 with @xmath81 .",
    "our decision which causal direction is more plausible for the observation @xmath82 will now be based on the following generating models : a function @xmath8 is chosen uniformly at randomly from @xmath83 , i.e. , the set of functions from @xmath0 to @xmath1 that pass the points @xmath52 and @xmath53 . then , each @xmath84 with @xmath85 is chosen uniformly at random from @xmath86 .",
    "this yields the following distribution on the set of possible observations : @xmath87 likewise , we obtain a distribution for the causal direction @xmath74 given by @xmath88 where @xmath89 denotes the corresponding set of functions from @xmath1 to @xmath0 . + for a general grid @xmath90 , elementary combinatorics shows that the number of monotonic functions from @xmath0 to @xmath1 that pass the corners @xmath52 and @xmath91 is given by @xmath92 therefore , @xmath93 the pair @xmath82 defines @xmath94 grids @xmath95 and @xmath96 is the product of the numbers for each grid .",
    "thus , @xmath97 where we have applied rule ( [ comb ] ) to each grid @xmath98 . combining ( [ genxy ] ) , ( [ genyx ] ) , ( [ comb ] ) , and ( [ combcomb ] ) yields @xmath99 we now consider the limit of arbitrarily fine grid , i.e. , @xmath100 ( while keeping the ratios of all @xmath84 and those of all @xmath101 constant ) . then expression ( [ igcid ] )",
    "becomes independent of the grid and can be replaced with @xmath102 thus , igci as given by definition  [ def : igci ] simply compares the loglikelihoods of the data with respect to the two competing generating models above since ( [ discrigci ] ) coincides with the left hand side of ( [ empigci ] ) after normalizing @xmath103 such that @xmath104 .",
    "the above link is intriguing , but the function counting argument required that we discretized the space , leading to finite function classes , and it is not obvious how the analysis should be done in the continuous domain . in statistical learning theory",
    "@xcite , the core of the theoretical analysis is the following : for consistency of learning , we need uniform convergence of risks over function classes . for finite classes ,",
    "uniform convergence follows from the standard law of large numbers , but for infinite classes , the theory builds on the idea that whenever these classes are evaluated on finite samples , they get reduced to finite collections of equivalence classes consisting of functions taking the same values on the given sample . in transductive inference as well as in a recently proposed inference principle referred to as inference with the `` universum , '' the size of such equivalence classes plays a central role @xcite .",
    "the proposed new view of the igci principle may be linked to this principle .",
    "universum inference builds on the availability of additional data that is not from the same distribution as the training data  in principle , it might be observed in the future , but we havent seen it yet and it may not make sense for the current classification task .    let us call two pattern recognition functions equivalent if they take the same values on the training data . we can measure the size of an equivalence class by how many possible labellings the functions from the class can produce on the universum data .",
    "a classifier should then try to correctly separate the training data using a function from a large equivalence class  i.e. , a function from a class that allows many possible labellings on the universum data , i.e. , one that does not make a commitment on these points .",
    "loosely speaking , the universum is a way to adjust the geometry of the space such that it makes sense for the kind of data that might come up .",
    "this is consistent with a paper that linked the universum - svm @xcite to a rescaled version of fisher s discriminant @xcite . taken to its extreme , it would advocate the view that there may not be any natural scaling or embedding of the data , but data points are only meaningful in how they relate to other data points .    in our current setting , if we are given a set of universum points in addition to the training set , we use them to provide the discretization of the space .",
    "we consider all functions equivalent that interpolate our training points , and then determine the size of the equivalence classes by counting , using the universum points , how many such functions there are .",
    "the size of these equivalence classes then determines the causal direction , as described above  our analysis works exactly the same no matter whether we have a regular discretization or a discretization by a set of universum points .",
    "we now argue that the correlations between @xmath11 and @xmath105 are relevant for prediction in two respects : first , knowing @xmath18 tells us something about @xmath11 , and second , @xmath11 tells us something about @xmath18 . note that section  [ sec : untyp ] already describes the first part :",
    "assume @xmath106 is large .",
    "then , knowing @xmath18 ( and , in addition , a lower bound for @xmath107 ) restricts the set of possible @xmath34-tuples to a region with small volume .",
    "we know explore what @xmath11 tells us about @xmath18 .",
    "this scenario is the one in unsupervised and semi - supervised learning ( ssl ) @xcite since the distribution of unlabeled points is used to get information about the labels .",
    "@xcite hypothesized that this is not possible in causal direction , i.e. , if the labels are the effect .",
    "in anticausal direction , the labels are the cause and unsupervised learning employs the information that @xmath108 contains about @xmath109 .",
    "as opposed to the standard notation in machine learning , where @xmath1 is the variable to be predicted from @xmath0 , regardless of which of the variables is the cause , we prefer to keep the convention that @xmath0 causes @xmath1 throughout the paper .",
    "thus , we consider the task of predicting @xmath0 from @xmath1 and discuss in which sense knowing the distribution @xmath11 helps .",
    "we study this question within the finite grid @xmath110 to avoid technical difficulties with defining priors on the set of differentiable functions .",
    "we use essentially the generating model @xmath32 from section  [ sec : numb ] with monotonic functions on the grid with the following modification : we restrict the set of functions to the set of surjective functions @xmath111 to ensure that the image of the uniform distribution is a strictly positive distribution . to avoid that this is a strong restriction , we assume that @xmath112 .",
    "since we use the grid only to motivate ideas for the continuous case , this does not introduce any artificial asymmetry between @xmath0 and @xmath1 .",
    "then we assume that a function @xmath8 is drawn uniformly at random from @xmath111 .",
    "afterwards , @xmath34 @xmath113-values are drawn uniformly at random from @xmath114 .",
    "this generating model defines a joint distribution for @xmath34-tuples @xmath115 and functions @xmath8 via @xmath116 where @xmath117 denotes the application of @xmath8 in each component to @xmath115 yields the same distribution as in section  [ sec : numb ] up to the technical modifications of having fixed endpoints and surjective functions . ] .    in analogy to the continuous case",
    ", we introduce the image of the uniform distribution on @xmath114 under @xmath8 by @xmath118 and obtain @xmath119 hence , @xmath120 where we have used the fact that all functions are equally likely .",
    "we rephrase ( [ x > y ] ) as @xmath121 where @xmath11 denotes the distribution of empirical relative frequencies defined by the @xmath34-tuple @xmath122 and @xmath123 is a summand that does not depend on @xmath8 .",
    "( [ predf ] ) provides a prediction of @xmath8 from @xmath122 .",
    "we now ask why this prediction should be _ useful _ although it is based on the wrong model because we assume that the true data generating model does not draw @xmath113-values from the uniform distribution ( instead , @xmath10 only `` behaves like the uniform one '' in the sense of ( [ uncorr ] ) ) . to this end",
    ", we show that the likelihood of @xmath8 is unusually high compared to other functions that are , in a sense , equivalent . to define a set of equivalent functions ,",
    "we first represent @xmath8 by the following list of non - negative integers : @xmath124 and observe that this list describes @xmath8 uniquely because @xmath8 is monotonic .",
    "then every permutation @xmath125 on @xmath126 defines a new monotonic function @xmath127 by the list @xmath128 with @xmath129 .",
    "note that @xmath130 .",
    "therefore , one can easily argue that for large @xmath131 , most permutations induce functions @xmath127 for which @xmath132 this is because the difference between left and right hand side can be interpreted as covariance of the random variables @xmath11 and @xmath133 with respect to the uniform distribution on @xmath126 ( see also section  [ sec : int ] ) and a random permutation yields approximately uncorrelated samples with high probability and upper bounds on @xmath11 , which goes beyond the scope of this paper . ] .",
    "therefore , if we observe that @xmath134 in the sense of significant violation of equality , the true function @xmath8 has a higher likelihood than the overwhelming majority of the functions @xmath127 . in other words , @xmath135 prefers the true function within a huge set of functions that are equivalent in the sense of having the same numbers of pre - images .    translating this into the continuous setting",
    ", we infer @xmath8 from @xmath11 by defining a loglikelihood function over some appropriate set of sufficiently smooth functions via @xmath136 with a free parameter @xmath137 , since we have explained in which sense this provides a useful prediction in the discrete setting .    rather than getting a distribution over the possible functions for @xmath8 we often want to get a single function @xmath138 that predicts @xmath0 from @xmath1 , i.e. , an estimator for @xmath139 .",
    "we define @xmath140 and observe that @xmath138 maps @xmath11 to the uniform distribution due to @xmath141 , i.e. , @xmath138 provides the correct prediction if @xmath10 is uniform .",
    "moreover , its inverse @xmath142 is the unique maximizer of ( [ flike ] ) since it maps @xmath36 to @xmath11 .    to understand in",
    "what sense @xmath138 still provides a good prediction even if @xmath10 strongly deviates from @xmath36 , we observe that the error remains small if the cumulative distribution function @xmath143 does not deviate too much from the one for the uniform distribution .",
    "furthermore , @xmath138 shares some qualitative behavior with @xmath144 because it tends to have large slope where @xmath144 has large slope because @xmath145 correlates with @xmath146 due to ( [ pc ] ) .",
    "figure  [ fig : exp ] visualizes unsupervised prediction based on @xmath138 for a simple function .     a simple function @xmath8 from @xmath0 to @xmath1 ( left ) and the functions @xmath138 inferred from the empirical distributions of @xmath1 for two different input distributions @xmath10.,scaledwidth=110.0% ]    we now argue that information theory provides theoretical results on how close @xmath138 is to @xmath144 . to this end , we define an ( admittedly uncommon ) distance of functions by the relative entropy distance of the densities that they map to the uniform distribution . thus , @xmath147 measures the distance between @xmath138 and @xmath144 .",
    "since relative entropy is conserved under bijections @xcite , we have @xmath148 i.e. , the deviation between @xmath138 and @xmath144 coincides with the deviation of @xmath10 from the uniform distribution .",
    "together with ( [ orth1 ] ) , ( [ transf ] ) implies @xmath149 with equality only for @xmath150 .",
    "note that @xmath10 represents the functions @xmath151 obtained from the analog of ( [ ghat ] ) when trying to infer @xmath8 from @xmath10 ( although we know that this is pointless when @xmath10 and @xmath8 are chosen independently ) .",
    "since @xmath44 represents the true function @xmath8 , we conclude : no matter how much @xmath138 deviates from @xmath144 , @xmath151 deviates even more from @xmath8 , i.e. , the error of unsupervised prediction in causal direction always exceeds the one in anticausal direction .    for the semi - supervised version ,",
    "we are given a few labeled points @xmath152 as well as a large number of unlabeled points @xmath153 .",
    "we consider again the limit where @xmath34 is infinite and the observations tell us exactly the distribution @xmath11 .",
    "then we use the information that @xmath11 provides on @xmath8 for interpolating between the labeled points via @xmath154 whenever @xmath155 $ ] .",
    "note that the above schemes for un- and semisupervised prediction are not supposed to compete with existing methods for real - world applications ( the assumption of a noiseless invertible relation does not occur too often anyway ) .",
    "the goal of the above ideas is only to present a toy model that shows that the independence between @xmath156 and @xmath157 typically yields a dependence between @xmath108 and @xmath158 that can be employed for prediction .",
    "generalizations of these insights to the noisy case could be helpful for practical applications .",
    "the authors are grateful to joris mooij for insightful discussions .",
    "p.  hoyer , d.  janzing , j.  mooij , j.  peters , and b  schlkopf .",
    "nonlinear causal discovery with additive noise models . in d.",
    "koller , d.  schuurmans , y.  bengio , and l.  bottou , editors , _ proceedings of the conference neural information processing systems ( nips ) 2008 _ , vancouver , canada , 2009 . mit press .",
    "http://books.nips.cc/papers/files/nips21/nips2008_0266.pdf .",
    "j.  peters , d.  janzing , and b.  schlkopf . identifying cause and effect on discrete data using additive noise models . in _ proceedings of the thirteenth international conference on artificial intelligence and statistics ( aistats ) 2010 , jmlr : w&cp 9 , chia laguna , sardinia , italy , 2010_. http://jmlr.csail.mit.edu / proceedings / papers / v9/.          j.  peters , j.  mooij , d.  janzing , and b.  schlkopf .",
    "identifiability of causal graphs using functional models . in _ proceedings of the 27th conference on uncertainty in artificial intelligence ( uai 2011)_. http://uai.sis.pitt.edu/papers/11/p589-peters.pdf .",
    "jason weston , ronan collobert , fabian sinz , lon bottou , and vladimir vapnik .",
    "inference with the universum .",
    "in _ in icml 06 : proceedings of the 23rd international conference on machine learning _ , pages 10091016 .",
    "acm , 2006 .",
    "sinz , o.  chapelle , a.  agarwal , and b.  schlkopf .",
    "an analysis of inference with the universum . in jc  platt , d  koller , y  singer , and s  roweis , editors , _ advances in neural information processing systems 20 _ , pages 13691376 , 9 2008 ."
  ],
  "abstract_text": [
    "<S> information geometric causal inference ( igci ) is a new approach to distinguish between cause and effect for two variables . </S>",
    "<S> it is based on an independence assumption between input distribution and causal mechanism that can be phrased in terms of orthogonality in information space . </S>",
    "<S> we describe two intuitive reinterpretations of this approach that makes igci more accessible to a broader audience .    </S>",
    "<S> moreover , we show that the described independence is related to the hypothesis that unsupervised learning and semi - supervised learning only works for predicting the cause from the effect and not vice versa . </S>"
  ]
}