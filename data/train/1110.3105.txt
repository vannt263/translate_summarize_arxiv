{
  "article_text": [
    "many problems in computational science and engineering require the solution of large , dense linear systems .",
    "standard direct methods based on gaussian elimination , of course , require @xmath5 work , where @xmath1 is the system size .",
    "this quickly becomes infeasible as @xmath1 increases . as a result ,",
    "such systems are typically solved iteratively , combining gmres @xcite , bi - cgstab @xcite or some other iterative scheme with fast algorithms to apply the system matrix , when available . for the integral equations of classical physics ,",
    "this combination has led to some of the fastest solvers known today , with dramatically lower complexity estimates of the order @xmath0 or @xmath3 ( * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* and references therein ) .",
    "despite their tremendous success , however , iterative methods still have several significant disadvantages when compared with their direct counterparts :    _ the number of iterations required by an iterative solver is highly sensitive to the conditioning of the system matrix . _",
    "ill - conditioning arises , for example , in the solution of problems near resonance ( particularly in the high frequency regime ) , in geometries with `` close - to - touching '' interactions , in multi - component physics models with large contrasts in material properties , etc . under these circumstances",
    ", the solution time can be far greater than expected .",
    "direct methods , by contrast , are robust in the sense that their solution time does not degrade with conditioning .",
    "thus , they are often preferred in production environments , where reliability of the solver and predictability of the solution time are important .",
    "_ one often wishes to solve a linear system governed by a fixed matrix with multiple right - hand sides .",
    "_ this occurs , for example , in scattering problems , in optimization , and in the modeling of time - dependent processes in fixed geometry .",
    "most iterative methods are unable to effectively exploit the fact that the system matrix is the same , and simply treat each right - hand side as a new problem .",
    "direct methods , on the other hand , are extremely efficient in this regard : once the system matrix has been factored , the matrix inverse can be applied to each right - hand side at a much lower cost .",
    "_ one often wishes to solve problems when the system matrix is altered by a low - rank modification . _",
    "standard iterative methods do a poor job of exploiting this fact .",
    "direct methods , on the other hand , can update the factorization of the original matrix using the sherman - morrison - woodbury formula @xcite or use the existing factorization as a preconditioner .    in this paper , we present an algorithm for the solution of structured linear systems that overcomes these deficiencies , while remaining competitive with modern fast iterative solvers in many practical situations .",
    "the algorithm directly constructs a compressed ( `` data - sparse '' ) representation of the system matrix inverse , assuming only that the matrix has a block low - rank structure similar to that utilized by fast matrix - vector product techniques like the fast multipole method ( fmm ) @xcite . such matrices typically arise from the discretization of integral equations , where the low - rank structure can be understood in terms of far - field interactions between clusters of points , but the procedure is general and makes no _ a priori _ assumptions about rank .",
    "our scheme is a multilevel extension of the work described in @xcite , which itself is based on the fast direct multilevel method developed for 2d boundary integral equations by martinsson and rokhlin @xcite .",
    "while we do not seek to review the literature on fast direct solvers here , it is worth noting that similar efforts have been ( and continue to be ) pursued by various groups , most notably in the context of hierarchically semiseparable ( hss ) matrices @xcite and @xmath6-matrices @xcite",
    ". a short historical discussion can be found in @xcite as well as in the recent article by gillman _",
    ". _ @xcite .",
    "the latter paper makes several improvements on the algorithm of @xcite , and presents a simple framework for understanding , implementing , and analyzing schemes for inverting integral equations on curves ( that is , domains parametrized by a single variable ) .",
    "planar domains with corners were treated recently in @xcite .",
    "applications to electromagnetic wave problems were considered in @xcite . finally , it should be noted that gillman s dissertation @xcite includes 3d experiments that also extend the martinsson - rokhlin formalism to the case of integral equations on surfaces .",
    "the present paper provides a mix of analysis , algorithmic work , and applications .",
    "the novelty of our contribution lies :    in the use of compression and auxilliary variables to embed an approximation of the original dense matrix into a sparse matrix framework that can make use of standard and well - developed sparse matrix technology ;    in providing detailed numerical experiments in both 2d and 3d ; and    in demonstrating the utility of fast direct solvers in several applications .",
    "we believe that the scheme is substantially simpler to implement than prior schemes and that it leads to a more stable solution process .    as in previous schemes ( see , e.g. , @xcite ) , the core algorithm in our work computes a compressed matrix representation using the interpolative decomposition ( i d ) @xcite via a multilevel procedure that we refer to as _",
    "recursive skeletonization_. once obtained , the compressed representation serves as a platform for fast matrix algebra including matrix - vector multiplication and matrix inversion . in its former capacity",
    ", the algorithm may be viewed as a generalized or kernel - independent fmm @xcite ; we explore this application in  [ sec : numerical - examples ] . for matrix inversion ,",
    "we show how to embed the compressed representation in an equivalent ( but larger ) sparse system , much in the style of @xcite .",
    "we then use a state - of - the - art sparse matrix solver to do the rest .",
    "we are grateful to david bindel for initially suggesting an investigation of the sparse matrix formalism and rely in this paper on the sparse direct solver software umfpack @xcite . as in dense lu factorization ,",
    "the direct solver is a two - phase process .",
    "first , following the generation of the compressed matrix embedding , a factored representation of the inverse is constructed .",
    "second , in the solution phase , the matrix inverse is applied in a rapid manner to a specified right - hand side . as expected",
    ", the solution phase is very inexpensive , often beating a single fmm call by several orders of magnitude . for boundary integral equations without highly oscillatory kernels , e.g. , the green s function for the laplace or low - frequency helmholtz equation , both phases typically have complexity @xmath0 in 2d . in 3d ,",
    "the complexities in our current implementation are @xmath2 and @xmath3 for precomputation ( compression and factorization ) and solution , respectively .",
    "the remainder of this paper is organized as follows . in  [ sec : preliminaries ] , we define the matrix structure of interest and review certain aspects of the i d . in  [ sec : algorithm ] , we review the recursive skeletonization algorithm for matrix compression and describe the new formalism for embedding the compressed matrix in a sparse format . in  [ sec : complexity - analysis ] , we study the complexity of the algorithm for non - oscillatory problems , while in  [ sec : error - analysis ] , we give error estimates for applying a compressed matrix and its inverse . in  [ sec : numerical - examples ] , we demonstrate the efficiency and generality of our scheme by reporting numerical results from its use as a generalized fmm , as a direct solver , and as an accelerator for molecular electrostatics and scattering problems . finally , in  [ sec : generalizations - conclusions ] , we summarize our findings and discuss future work .",
    "in this section , we discuss the precise matrix structure that makes our fast solver possible . for this ,",
    "let @xmath7 be a matrix whose index vector @xmath8 is grouped into @xmath9 contiguous blocks of @xmath10 elements each , where @xmath11 : @xmath12 then the linear system @xmath13 can be written in the form @xmath14 where @xmath15 and @xmath16 .",
    "solution of the full linear system by classical gaussian elimination is well - known to require @xmath5 work .",
    "the matrix a is said to be _ block separable _ if each off - diagonal submatrix @xmath17 can be decomposed as the product of three low - rank matrices : @xmath18 where @xmath19 , @xmath20 , and @xmath21 , with @xmath22 .",
    "note that in ( [ eq : block separable ] ) , the left matrix @xmath23 depends only on the index @xmath24 and the right matrix @xmath25 depends only on the index @xmath26 .",
    "we will see how such a factorization arises below .",
    "the term _ block separable _ was introduced in @xcite , and is closely related to that of semiseparable matrices @xcite and @xmath6-matrices @xcite . in @xcite ,",
    "the term _ structured _ was used , but block separable is somewhat more informative .",
    "@xmath24th off - diagonal block row _ of @xmath27 is the submatrix @xmath28 $ ] consisting of the @xmath24th block row of @xmath27 with the diagonal block @xmath29 deleted ; the _ off - diagonal block columns _ of @xmath27 are defined analogously .    clearly , the block separability condition ( [ eq : block separable ] ) is equivalent to requiring that the @xmath24th off - diagonal block row and column have rank @xmath30 and @xmath31 , respectively , for @xmath32 ( see  [ sec : algorithm ] for details ) .",
    "when @xmath27 is block separable , it can be written as @xmath33 where @xmath34 \\in \\mathbb{c}^{n \\times n}\\ ] ] is block diagonal , consisting of the diagonal blocks of @xmath27 , @xmath35 \\in \\mathbb{c}^{n \\times k_{{\\mathrm{r } } } } , \\qquad r = \\left [    \\begin{array}{ccc }     r_{1}\\\\     & \\ddots\\\\     & & r_{p }    \\end{array } \\right ] \\in \\mathbb{c}^{k_{{\\mathrm{c } } } \\times n}\\ ] ] are block diagonal , where @xmath36 and @xmath37 , and @xmath38 \\in \\mathbb{c}^{k_{{\\mathrm{r } } } \\times k_{{\\mathrm{c}}}}\\ ] ] is dense with zero diagonal blocks .",
    "it is convenient to let @xmath39 and @xmath40 .",
    "we can then write the original system in the form @xmath41 \\left [    \\begin{array}{c }     \\mathbf{x}\\\\     \\mathbf{y}\\\\     \\mathbf{z }    \\end{array } \\right ] = \\left [    \\begin{array}{c }     \\mathbf{b}\\\\     \\mathbf{0}\\\\     \\mathbf{0 }    \\end{array } \\right ] .",
    "\\label{eq : sparse - embedding}\\ ] ] this system is highly structured and sparse , and can be efficiently factored using standard techniques .",
    "if we assume that each block corresponds to @xmath42 unknowns and that the ranks @xmath43 of the off - diagonal blocks are all the same , it is straightforward to see @xcite that a scheme based on ( [ eq : compressed - representation ] ) or ( [ eq : sparse - embedding ] ) requires an amount of work of the order @xmath44 .    in many contexts ( including integral equations ) , the notion of block separability is applicable on a hierarchy of subdivisions of the index vector .",
    "that is to say , a decomposition of the form ( [ eq : compressed - representation ] ) can be constructed at each level of the hierarchy .",
    "when a matrix has this structure , much more powerful solvers can be developed , but they will require some additional ideas ( and notation ) .",
    "our treatment in this section follows that of @xcite .",
    "let @xmath8 be the index vector of a matrix @xmath7 .",
    "we assume that a tree structure @xmath45 is imposed on @xmath46 which is @xmath47 levels deep . at level @xmath48 ,",
    "we assume that there are @xmath49 nodes , with each such node @xmath50 corresponding to a contiguous subsequence of @xmath46 such that @xmath51 we denote the _ finest level _ as level @xmath52 and the coarsest level as level @xmath47 ( which consists of a single block ) .",
    "each node @xmath50 at level @xmath53 has a finite number of children at level @xmath54 whose concatenation yields the indices in @xmath50 ( fig .",
    "[ fig : tree - struct ] ) .    .",
    "at each level of the hierarchy , a contiguous block of indices is divided into a set of children , each of which corresponds to a contiguous subset of indices . ]",
    "the matrix @xmath27 is _ hierarchically block separable _",
    "@xcite if it is block separable at each level of the hierarchy defined by @xmath45 .",
    "in other words , it is structured in the sense of the present paper if , on each level of @xmath45 , the off - diagonal block rows and columns are low - rank ( fig .",
    "[ fig : mat - struct ] ) .",
    "such matrices arise , for example , when discretizing integral equations with non - oscillatory kernels ( up to a specified precision ) .    _",
    "example 1_. consider the integral operator @xmath55 where @xmath56 is the green s function for the 2d laplace equation , and the domain of integration is a square @xmath57 in the plane .",
    "this is a 2d _ volume integral operator_. suppose now that we discretize ( [ eq : integral - operator ] ) on a @xmath58 grid : @xmath59 ( this is not a high - order quadrature but that is really a separate issue . ) let us superimpose on @xmath57 a quadtree of depth @xmath47 , where @xmath57 is the root node ( level @xmath47 ) .",
    "level @xmath60 is obtained from level @xmath47 by subdividing the box @xmath57 into four equal squares and reordering the points @xmath61 so that each child holds a contiguous set of indices .",
    "this procedure is carried out until level @xmath52 is reached , reordering the nodes at each level so that the points contained in every node at every level correspond to a contiguous set of indices .",
    "it is clear that , with this ordering , the matrix corresponding to ( [ eq : integral - operator - discret ] ) is hierarchically block separable , since the interactions between nonadjacent boxes at every level are low - rank to any specified precision ( from standard multipole estimates @xcite ) .",
    "adjacent boxes are low - rank for a more subtle reason ( see  [ sec : complexity - analysis ] and fig .  [",
    "fig : recur - subdiv ] ) .    _",
    "example 2_. suppose now that we wish to solve an interior dirichlet problem for the laplace equation in a simply connected 3d domain @xmath62 with boundary @xmath63 : @xmath64 potential theory @xcite suggests that we seek a solution in the form of a double - layer potential @xmath65 where @xmath66 is the green s function for the 3d laplace equation , @xmath67 is the unit outer normal at @xmath68 , and @xmath69 is an unknown surface density .",
    "letting @xmath70 approach the boundary , this gives rise to the second - kind fredholm equation @xmath71 using a standard collocation scheme based on piecewise constant densities over a triangulated surface , we enclose @xmath63 in a box @xmath57 and bin sort the triangle centroids using an octree where , as in the previous example , we reorder the nodes so that each box in the hierarchy contains contiguous indices .",
    "it can be shown that the resulting matrix is also hierarchically block separable ( see  [ sec : complexity - analysis ] and @xcite ) .",
    "we turn now to a discussion of the i d , the compression algorithm that we will use to compute low - rank approximations of off - diagonal blocks .",
    "a useful feature of the i d is that it is able to compute the rank of a matrix on the fly , since the exact ranks of the blocks are difficult to ascertain _ a priori_that is to say , the i d is _ rank - revealing_.      many decompositions exist for low - rank matrix approximation , including the singular value decomposition , which is well - known to be optimal @xcite . here",
    ", we consider instead the i d @xcite , which produces a near - optimal representation that is more useful for our purposes as it permits an efficient scheme for multilevel compression when used in a hierarchical setting .",
    "let @xmath72 be a matrix , and @xmath73 the matrix @xmath74-norm .",
    "a rank-@xmath75 approximation of @xmath27 in the form of an _ interpolative decomposition ( i d ) _ is a representation @xmath76 , where @xmath77 , whose columns constitute a subset of the columns of @xmath27 , and @xmath78 , a subset of whose columns makes up the @xmath79 identity matrix , such that @xmath80 is small and @xmath81 , where @xmath82 is the ( @xmath83)st greatest singular value of @xmath27 .",
    "we call @xmath57 and @xmath84 the _ skeleton _ and _ projection matrices _ , respectively .",
    "clearly , the i d compresses the column space of @xmath27 ; to compress the row space , simply apply the i d to @xmath85 , which produces an analogous representation @xmath86 , where @xmath87 and @xmath88 .",
    "the row indices that corrrespond to the retained rows in the i d are called the _ row _ or _ incoming skeletons_. the column indices that corrrespond to the retained columns in the i d are called the _ column _ or _ outgoing skeletons_.    reasonably efficient schemes for constructing an i d exist @xcite . by combining such schemes with methods for estimating the approximation error",
    ", we can compute an i d to any relative precision @xmath89 by adaptively determining the required rank @xmath75 @xcite .",
    "this is the sense in which we will use the i d .    while previous related work @xcite used the deterministic @xmath90 algorithm of @xcite , we employ here the latest compression technology based on random sampling , which typically requires only @xmath91 operations @xcite .",
    "in this section , we first describe the `` standard '' id - based fast multilevel matrix compression algorithm ( as in @xcite ) .",
    "the hss and @xmath6-matrix formalisms use the same underlying philosophy @xcite .",
    "we then describe our new inversion scheme .",
    "let @xmath7 be a matrix with @xmath92 blocks , structured in the sense of  [ sec : preliminaries : matrix - structure ] , and @xmath89 a target relative precision .",
    "we first outline a one - level matrix compression scheme :    for @xmath32 , use the i d to compress the row space of the @xmath24th off - diagonal block row to precision @xmath93 .",
    "let @xmath23 denote the corresponding row projection matrix .",
    "similarly , for @xmath94 , use the i d to compress the column space of the @xmath26th off - diagonal block column to precision @xmath93 .",
    "let @xmath25 denote the corresponding column projection matrix .",
    "approximate the off - diagonal blocks of @xmath27 by @xmath95 for @xmath96 , where @xmath97 is the submatrix of @xmath17 defined by the row and column skeletons associated with @xmath23 and @xmath25 , respectively .",
    "this yields precisely the matrix structure discussed in ",
    "[ sec : preliminaries ] , following ( [ eq : block separable ] ) .",
    "the one - level scheme is illustrated graphically in fig .",
    "[ fig : mat - comp ] .",
    "the multilevel algorithm is now just a simple extension based on the observation that by ascending one level in the index tree and regrouping blocks accordingly , we can compress the skeleton matrix @xmath98 in ( [ eq : compressed - representation ] ) in exactly the same form , leading to a procedure that we naturally call _ recursive skeletonization _ ( fig .",
    "[ fig : multi - comp ] ) .        the full algorithm may be specified as follows :    starting at the leaves of the tree , extract the diagonal blocks and perform one level of compression of the off - diagonal blocks .",
    "move up one level in the tree and regroup the matrix blocks according to the tree structure .",
    "terminate if the new level is the root ; otherwise , extract the diagonal blocks , recompress the off - diagonal blocks , and repeat .    the result is a telescoping representation @xmath99 r^{\\left ( 1 \\right ) } ,    \\label{eq : multilevel - compression}\\ ] ] where the superscript indexes the compression level @xmath100 .",
    "_ example 3 .",
    "_ as a demonstration of the multilevel compression technique , consider the matrix defined by @xmath101 points uniformly distributed in the unit square , interacting via the 2d laplace green s function ( [ eq:2d - laplace ] ) and sorted according to a quadtree ordering . the sequence of skeletons remaining after each level of compression to @xmath102 is shown in fig .",
    "[ fig : sparsify ] , from which we see that compression creates a _ sparsification _ of the sources which , in a geometric setting , leaves skeletons along the boundaries of each block",
    ".     points in the unit square are compressed to relative precision @xmath102 using a five - level quadtree - based scheme . at each level ,",
    "the surviving skeletons are shown , colored by block index , with the total number of skeletons remaining given by @xmath103 for compression level @xmath104 , where @xmath105 denotes the original uncompressed system . ]",
    "the computational cost of the algorithm described in the previous section is dominated by the fact that each step is global : that is , compressing the row or column space for each block requires accessing all other blocks in that row or column .",
    "if no further knowledge of the matrix is available , this is indeed necessary . however , as noted by @xcite , this global work can often be replaced by a local one , resulting in considerable savings .",
    "a sufficient condition for this acceleration is that the matrix correspond to evaluating a potential field for which some form of green s identities hold .",
    "it is easiest to present the main ideas in the context of laplace s equation . for this , consider fig .",
    "[ fig : proxy - comp ] , which depicts a set of sources in the plane .",
    "we assume that block index @xmath24 corresponds to the sources in the central square @xmath57 .",
    "the @xmath24th off - diagonal block row then corresponds to the interactions of all points outside @xmath57 with all points inside @xmath57 .",
    "we can separate this into contributions from the near neighbors of @xmath57 , which are local , and the distant sources , which lie outside the near - neighbor domain , whose boundary is denoted by @xmath106 . but any field induced by the distant sources induces a harmonic function inside @xmath106 and can therefore be replicated by a charge density on @xmath106 itself .",
    "thus , rather than using the detailed structure of the distant points , the row ( incoming ) skeletons for @xmath57 can be extracted by considering just the combination of the near - neighbor sources and an artifical set of charges placed on @xmath106 , which we refer to as a _",
    "proxy surface_. likewise , the column ( outgoing ) skeletons for @xmath57 can be determined by considering only the near neighbors and the proxy surface .",
    "if the potential field is correct on the proxy surface , it will be correct at all more distant points ( again via some variant of green s theorem ) .",
    "the interaction rank between @xmath106 and @xmath57 is constant ( depending on the desired precision ) from standard multipole estimates @xcite . in summary ,",
    "the number of points required to discretize @xmath106 is constant , and the dimension of the matrix to compress against for the block corresponding to @xmath57 is essentially just the number of points in the physically neighboring blocks .     due to a distribution of exterior sources ( left ) can be decomposed into neighboring and well - separated contributions . by representing the latter via a proxy surface @xmath106 ( center ) , the matrix dimension to compress against for the block corresponding to @xmath57 ( right )",
    "can be reduced to the number of neighboring points plus a constant set of points on @xmath106 , regardless of how many points lie beyond @xmath106 . ]",
    "similar arguments hold for other kernels of potential theory including the heat , helmholtz , yukawa , stokes , and elasticity kernels , though care must be taken for oscillatory problems which could require a combination of single and double layer potentials to avoid spurious resonances in the representation for the exterior .",
    "the compressed representation ( [ eq : multilevel - compression ] ) admits an obvious fast algorithm for computing the matrix - vector product @xmath107 .",
    "as shown in @xcite , one simply applies the matrices in ( [ eq : multilevel - compression ] ) from right to left . like the fmm",
    ", this procedure can be thought of as occurring in two passes :    an upward pass , corresponding to the sequential application of the column projection matrices @xmath108 , which hierarchically compress the input data @xmath109 to the column ( outgoing ) skeleton subspace .",
    "a downward pass , corresponding to the sequential application of the row projection matrices @xmath110 , which hierarchically project onto the row ( incoming ) skeleton subspace and , from there , back onto the output elements @xmath111 .",
    "the representation ( [ eq : multilevel - compression ] ) also permits a fast algorithm for the direct inversion of nonsingular matrices .",
    "the one - level scheme was discussed in  [ sec : preliminaries ] . in the multilevel scheme ,",
    "the system @xmath112 in ( [ eq : sparse - embedding ] ) is itself expanded in the same form , leading to the sparse embedding @xmath113 \\left [    \\begin{array}{c }     \\mathbf{x}\\\\     \\mathbf{y}^{\\left ( 1 \\right)}\\\\     \\mathbf{z}^{\\left ( 1 \\right)}\\\\     \\vdots\\\\     \\vdots\\\\     \\mathbf{y}^{\\left (",
    "\\lambda \\right)}\\\\     \\mathbf{z}^{\\left ( \\lambda \\right ) }    \\end{array } \\right ] = \\left [    \\begin{array}{c }     \\mathbf{b}\\\\     \\mathbf{0}\\\\     \\mathbf{0}\\\\     \\vdots\\\\     \\vdots\\\\     \\mathbf{0}\\\\     \\mathbf{0 }    \\end{array } \\right ] .",
    "\\label{eq : multilevel - embedding}\\ ] ]    to understand the consequences of this sparse representation , it is instructive to consider the special case in which the row and column skeleton dimensions are identical for each block , say @xmath75 , so that the total row and column skeleton dimensions are @xmath114",
    ". then , studying ( [ eq : sparse - embedding ] ) first and assuming that @xmath115 is invertible , block elimination of @xmath109 and @xmath111 yields @xmath116 where @xmath117 is block diagonal . back",
    "substitution then yields @xmath118 \\mathbf{b}.\\ ] ] in other words , the matrix inverse is @xmath119 where @xmath120 and @xmath121 are all block diagonal , and @xmath122 is dense . note that @xmath123 is equal to the skeleton matrix @xmath98 with its diagonal blocks filled in .",
    "thus , ( [ eq : compressed - inverse ] ) is a compressed representation of @xmath124 with minimal fill - in over the original compressed representation ( [ eq : compressed - representation ] ) of @xmath27 . in the multilevel setting , one carries out the above factorization recursively , since @xmath123 can now be inverted in the same manner : @xmath125 \\mathcal{r}^{\\left ( 1 \\right)}.    \\label{eq : multilevel - inverse}\\ ] ] this point of view is elaborated in @xcite .    in the general case",
    ", the preceding procedure may fail if @xmath115 happens to be singular and ( more generally ) may be numerically unstable if care is not taken to stabilize the block elimination scheme using some sort of pivoting .",
    "thus , rather than using the `` hand - rolled '' gaussian elimination scheme of @xcite to compute the telescoping inverse ( [ eq : multilevel - inverse ] ) , we rely instead on the existence of high - quality sparse direct solver software .",
    "more precisely , we simply supply umfpack with the sparse representation ( [ eq : multilevel - embedding ] ) and let it compute the corresponding factorization .",
    "numerical results show that the performance is similar to that expected from ( [ eq : multilevel - inverse ] ) .",
    "for the sake of completeness , we briefly analyze the complexity of the algorithm presented in  [ sec : algorithm ] for a typical example : discretization of the integral operator ( [ eq : integral - operator ] ) , where the integral kernel has smoothness properties similar to that of the green s function for the laplace equation .",
    "we follow the analysis of @xcite and estimate costs for the `` hand - rolled '' gaussian elimination scheme .",
    "we ignore quadrature issues and assume that we are given a matrix @xmath27 acting on @xmath1 points distributed randomly in a @xmath126-dimensional domain , sorted by an orthtree that uniformly subdivides until all block sizes are @xmath127 .",
    "( in 1d , an orthtree is a binary tree ; in 2d , it is a quadtree ; and in 3d , it is an octree . )    for each compression level @xmath100 , with @xmath128 being the finest , let @xmath49 be the number of matrix blocks , and @xmath129 and @xmath130 the uncompressed and compressed block dimensions , respectively , assumed equal for all blocks and identical across rows and columns , for simplicity .",
    "we first make the following observations :    the total matrix dimension is @xmath131 , where @xmath132 , so @xmath133 .",
    "each subdivision increases the number of blocks by a factor of roughly @xmath134 , so @xmath135 .",
    "in particular , @xmath136 , so @xmath137 .    the total number of points at level @xmath53 is equal to the total number of skeletons at level @xmath54 , i.e. , @xmath138 , so @xmath139 .",
    "furthermore , we note that @xmath130 is on the order of the interaction rank between two adjacent blocks at level @xmath48 , which can be analyzed by recursive subdivision of the source block to expose well - separated structures with respect to the target ( fig .",
    "[ fig : recur - subdiv ] ) .        assuming only that the interaction between a source subregion separated from a target by a distance of at least its own size",
    "is of constant rank ( to a fixed precision @xmath93 ) , we have @xmath140 where , clearly , @xmath141 , so @xmath142      from  [ sec : preliminaries : interpolative - decomposition ] , the cost of computing a rank-@xmath75 i d of an @xmath143 matrix is @xmath91 .",
    "we will only consider the case of proxy compression , for which @xmath144 for a block at level @xmath48 , so the total cost is @xmath145      the cost of applying @xmath146 is @xmath147 , while that of applying @xmath110 or @xmath108 is @xmath148 .",
    "combined with the @xmath149 cost of applying @xmath98 , the total cost is @xmath150      we turn now to the analysis of the cost of factorization using ( [ eq : multilevel - inverse ] ) . at each level @xmath48 ,",
    "the cost of constructing @xmath151 and @xmath152 is @xmath153 , after which forming @xmath154 , @xmath155 , and @xmath156 all require @xmath147 operations ; at the final level , the cost of constructing and inverting @xmath123 is @xmath157 .",
    "thus , the total cost is @xmath158 which has complexity ( [ eq : complexity - cm ] ) .",
    "finally , we note that the dimensions of @xmath154 , @xmath155 , @xmath156 , and @xmath159 are the same as those of @xmath146 , @xmath110 , @xmath108 , and @xmath98 , respectively .",
    "thus , the total cost of applying the inverse , denoted by @xmath160 , has the same complexity as @xmath161 , namely ( [ eq : complexity - mv ] ) .    in our umfpack - based approach ,",
    "the estimation of cost is a rather complicated task , and we do not attempt to carry out a detailed analysis of its performance . suffice it to say , there is a one - to - one correspondence between the `` hand - rolled '' gaussian elimination approach and one possible elimination scheme in umfpack .",
    "since that solver is highly optimized , the asymptotic cost should be the same ( or better ) .",
    "for some matrices , it is possible that straight gaussian elimination may be unstable without pivoting , while umfpack will carry out a backward - stable scheme .",
    "this is a distinct advantage of the sparse matrix approach although the complexity and fill - in analysis then becomes more involved .",
    "an important issue in direct solvers , of course , is that of storage requirements . in the present",
    "setting the relevant matrices are the compressed sparse representation ( [ eq : multilevel - embedding ] ) and the factorization computed within umfpack . this will be ( [ eq : complexity - mv ] ) for the forward operator and , in the absence of pivoting , for the sparse factorization as well .",
    "if pivoting is required , the analysis is more complex as it involves some matrix fill - in and is postponed to future work .",
    "we now state some simple error estimates for applying and inverting a compressed matrix .",
    "let @xmath27 be the original matrix and @xmath162 its compressed representation , constructed using the algorithm of  [ sec : algorithm ] such that @xmath163 for some @xmath89 .",
    "note that this need not be the same as the specified local precision in the i d since errors may accumulate across levels .",
    "however , as in @xcite , we have found that such error propagation is mild .",
    "let @xmath109 and @xmath164 be vectors such that @xmath13 .",
    "then it is straightforward to verify that for @xmath165 , @xmath166 where @xmath167 is the condition number of @xmath27 .",
    "furthermore , if @xmath168 , then @xmath169 in particular , if @xmath27 is well - conditioned , e.g. , @xmath27 is the discretization of a second - kind integral equation , then @xmath170 , so @xmath171",
    "in this section , we investigate the efficiency and flexibility of our algorithm by considering some representative examples . we begin with timing benchmarks for the laplace and helmholtz kernels in 2d and 3d , using the algorithm both as an fmm and as a direct solver , followed by applications in molecular electrostatics and multiple scattering .",
    "all matrices were blocked using quadtrees in 2d and octrees in 3d , uniformly subdivided until all block sizes were @xmath127 , but adaptively truncating empty boxes during the refinement process .",
    "only proxy compression was considered , with proxy surfaces constructed on the boundary of the supercell enclosing the neighbors of each block .",
    "we discretized all proxy surfaces using a constant number of points , independent of the matrix size @xmath1 : for the laplace equation , this constant depended only on the compression precision @xmath93 , while for the helmholtz equation , it depended also on the wave frequency , chosen to be consistent with the nyquist - shannon sampling theorem .",
    "computations were performed over @xmath172 instead of @xmath173 , where possible .",
    "the algorithm was implemented in fortran , and all experiments were performed on a 2.66 ghz processor in double precision .    in many instances ,",
    "we compare our results against those obtained using lapack / atlas @xcite and the fmm @xcite .",
    "all fmm timings were computed using the open - source fmmlib package @xcite , which is a fairly efficient implementation but does not include the plane - wave optimizations of @xcite or the diagonal translation operators of @xcite .",
    "we first consider the use of recursive skeletonization as a generalized fmm for the rapid computation of matrix - vector products .",
    "we considered two point distributions in the plane : points on the unit circle and in the unit square , hereafter referred to as the 2d surface and volume cases , respectively .",
    "we assumed that the governing matrix corresponds to the interaction of charges via the green s function ( [ eq:2d - laplace ] ) .",
    "the surface case is typical of layer - potential evaluation when using boundary integral equations . since a domain boundary in 2d",
    "can be described by a single parameter ( such as arclength ) , it is a 1d domain , so the expected complexities from  [ sec : complexity - analysis ] correspond to @xmath174 : @xmath0 work for both matrix compression and application .",
    "( see @xcite for a detailed discussion of the @xmath174 case . ) in the volume case , the dimension is @xmath175 , so the expected complexities are @xmath2 and @xmath3 for compression and application , respectively .    for the 3d laplace kernel ( [ eq:3d - laplace ] ) , we considered surface and volume point geometries on the unit sphere and within the unit cube , respectively .",
    "the corresponding dimensions are @xmath175 and @xmath176 .",
    "thus , the expected complexities for the 3d surface case are @xmath2 and @xmath3 for compression and application , respectively , while those for the 3d volume case are @xmath177 and @xmath178 , respectively .",
    "we present timing results for each case and compare with lapack / atlas and the fmm for a range of @xmath1 at @xmath179 .",
    "detailed data are provided in tables [ tab : apply - lap-2ds][tab : apply - lap-3dv ] and plotted in fig .",
    "[ fig : apply - lap ] .    .",
    "for lp and rs , the computation is split into two parts : precomputation ( pc ) , for lp consisting of matrix formation and for rs of matrix compression , and matrix - vector multiplication ( mv ) .",
    "the precision of the fmm and rs was set at @xmath179 .",
    "dotted lines indicate extrapolated data . ]",
    "it is evident that our algorithm scales as predicted .",
    ".numerical results for applying the laplace kernel in the 2d surface case at precision @xmath179 : @xmath1 , uncompressed matrix dimension ; @xmath180 , row skeleton dimension ; @xmath181 , column skeleton dimension ; @xmath182 , matrix compression time ( s ) ; @xmath161 , matrix - vector multiplication time ( s ) ; @xmath183 , relative error ; @xmath184 , required storage for compressed matrix ( mb ) . [ cols=\">,>,>,^,^,^,^\",options=\"header \" , ]     as expected , more iterations were necessary for smaller @xmath185 , though the difference was not too dramatic .",
    "the ratio of the total solution time required for all solves was @xmath186 for the unpreconditioned versus the preconditioned method .",
    "we have presented a multilevel matrix compression algorithm and demonstrated its efficiency at accelerating matrix - vector multiplication and matrix inversion in a variety of contexts .",
    "the matrix structure required is fairly general and relies only on the assumption that the matrix have low - rank off - diagonal blocks . as a fast direct solver for the boundary integral equations of potential theory",
    ", we found our algorithm to be competitive with fast iterative methods based on fmm / gmres in both 2d and 3d , provided that the integral equation kernel is not too oscillatory , and that the system size is not too large in 3d .",
    "in such cases , the total solution times for both methods were very comparable .",
    "our solver has clear advantages , however , for problems with ill - conditioned matrices ( in which case the number of iterations required by fmm / gmres can increase dramatically ) , or those involving multiple right - hand sides ( in which case the cost of matrix compression and factorization can be amortized ) .",
    "the latter category includes the use of our solver as a preconditioner for iterative methods , which we expect to be quite promising , particularly for large - scale 3d problems with complex geometries .",
    "a principal limitation of the approach described here is the growth in the cost of factorization in 3d or higher , which prohibits the scheme from achieving optimal @xmath0 or nearly optimal @xmath3 complexity .",
    "it is , however , straightforward to implement and quite effective .",
    "all of the hierarchical compression - based approaches ( hss matrices @xcite , @xmath6-matrices @xcite and skeletonization @xcite ) are capable of overcoming this obstacle .",
    "the development of simple and effective schemes that curtail this growth is an active area of research , and we expect that @xmath3 direct solvers with small pre - factors in higher dimensions will be constructed shortly , at least for non - oscillatory problems .",
    "it is clear that all of these techniques provide improved solution times for high - frequency _ volume _ integral equations , due to the compression afforded by green s theorem in moving data from the volume to the boundary .",
    "more precisely , the cost of solving high - frequency volume wave scattering problems in 2d are @xmath2 and @xmath3 for precomputation and solution , respectively . for related work , see @xcite .    finally ,",
    "although all numerical results have presently been reported for a single processor , the algorithm is naturally parallelizable : many computations are organized in a block sweep structure , where each block can be processed independently .",
    "this is clearly true of the recursive skeletonization phase using proxy surfaces ( with a possible loss of @xmath187 in performance since there are @xmath187 levels in the hierarchy ) . as for the solver phase",
    ", arguments can be made in support of both the original `` hand - rolled '' gaussian elimination approach and our framework that relies on sparse embedding .",
    "we expect that , by making use of umfpack and other state - of - the - art parallel sparse solvers ( e.g. , superlu @xcite , mumps @xcite , pardiso @xcite , wsmp @xcite ) , our overall strategy will help simplify the implementation of skeletonization - based schemes on high - performance computing platforms as well .",
    "we would like to thank zydrunas gimbutas and mark tygert for many helpful discussions .    00 , _ a fully asynchronous multifrontal solver using distributed dynamic scheduling _ , siam j.  matrix anal .",
    "appl . , 23 ( 2001 ) , pp .",
    "1541 . , _ lapack users guide _ , society for industrial and applied mathematics , philadelphia , pa , usa , 3rd  ed . , 1999 .",
    ", _ protonation of interacting residues in a protein by a monte carlo method : application to lysozyme and the photosynthetic reaction center of rhodobacter sphaeroides _ , proc .",
    "usa , 88 ( 1991 ) , pp .",
    ", _ a fast direct solver for the integral equations of scattering theory on planar curves with corners _ ,",
    "j.  comput .",
    "phys . , 231 ( 2012 ) , pp .",
    "18791899 . , _ the amber biomolecular simulation programs _ , j.  comput",
    ", 26 ( 2005 ) , pp .",
    "16681688 . , _ a fast solver for hss representations via sparse matrices _ ,",
    "siam j.  matrix anal .",
    "appl . , 29 ( 2006 ) , pp .",
    "6781 . , _ a fast @xmath188 decomposition solver for hierarchically semiseparable representations _ , siam j.  matrix anal .",
    "appl . , 28 ( 2006 ) , pp .",
    "603622 . , _ a fast , direct algorithm for the lippmann - schwinger integral equation in two dimensions _ , adv .",
    "comput .  math .",
    ", 16 ( 2002 ) , pp .",
    "175190 . , _ a wideband fast multipole method for the helmholtz equation in three dimensions _",
    ", j.  comput .",
    ", 216 ( 2006 ) , pp .",
    ", _ on the compression of low rank matrices _ , siam j.  sci .  comput",
    ". , 26 ( 2005 ) , pp .",
    "13891404 . ,",
    "_ fast and efficient algorithms in computational electromagnetics _ , artech house , boston , ma , usa , 2001 .",
    ", _ algorithm 832 : umfpack v4.3an unsymmetric - pattern multifrontal method _ ,",
    "acm trans .  math .",
    "softw . , 30 ( 2004 ) , pp .",
    "196199 . ,",
    "_ an unsymmetric - pattern multifrontal method for sparse lu factorization _ , siam j.  matrix anal .",
    "appl . , 18 ( 1997 ) , pp .  140158 . , _",
    "pdb2pqr : an automated pipeline for the setup of poisson - boltzmann electrostatics calculations _ , nucleic  acids res . , 32 ( 2004 ) , pp .",
    "w665w667 . ,",
    "_ structure of a b - dna dodecamer : conformation and dynamics _ , proc .",
    "usa , 78 ( 1981 ) , pp .",
    ", _ fast direct solvers for elliptic partial differential equations _ , ph.d .  thesis , department of applied mathematics , university of colorado at boulder , 2011 . , _ a direct solver with @xmath189 complexity for integral equations on one - dimensional domains _ , front . math .",
    "china , 7 ( 2012 ) , pp .",
    "217247 . , _",
    "fmmlib : fast multipole methods for electrostatics , elastostatics , and low frequency acoustic modeling _ , in preparation . , _ a generalized fast multipole method for nonoscillatory kernels _ , siam j.  sci",
    ".  comput . , 24 ( 2003 ) ,",
    ", _ matrix computations _ , the johns hopkins university press , baltimore , md , usa , 3rd  ed . , 1996 .",
    ", _ fast direct solvers for integral equations in complex three - dimensional domains _ , acta numer . , 18 ( 2009 ) , pp .  243275 . , _ a fast algorithm for particle simulations _ , j.  comput .  phys . , 73 ( 1987 ) , pp .",
    "325348 . ,",
    "_ a new version of the fast multipole method for the laplace equation in three dimensions _ , acta numer . , 6 ( 1997 ) ,",
    "229269 . , _ partial differential equations of mathematical physics and integral equations _",
    ", prentice - hall , englewood cliffs , nj , usa , 1988 . , _ wsmp : watson sparse matrix package .",
    "part ii  direct solution of general sparse systems _ , technical report rc 21888 , ibm t.  j.  watson research center , 2000 .",
    ", _ a sparse matrix arithmetic based on @xmath6-matrices .",
    "part i : introduction to @xmath6-matrices _ , computing , 62 ( 1999 ) , pp .",
    "89108 . , _ data - sparse approximation by adaptive @xmath190-matrices _ , computing , 69 ( 2002 ) , pp .  135 . , _ a sparse @xmath6-matrix arithmetic .",
    "part ii : application to multi - dimensional problems _ , computing , 64 ( 2000 ) , pp .",
    ", _ updating the inverse of a matrix _ , siam rev . , 31 ( 1989 ) , pp .",
    ", _ high - order corrected trapezoidal quadrature rules for singular functions _ , siam j.  numer .",
    ", 34 ( 1997 ) , pp .",
    "13311356 . , _",
    "an overview of superlu : algorithms , implementation , and user interface _ , acm trans .  math .",
    "softw . , 31 ( 2005 ) ,",
    "302325 . , _",
    "randomized algorithms for the low - rank approximation of matrices _ , proc .",
    "usa , 104 ( 2007 ) , pp .",
    ", _ fast multipole boundary element method : theory and applications in engineering _ , cambridge university press , new york , ny , usa , 2009 . , _ fast evaluation of electro - static interactions in multi - phase dielectric media _ ,",
    "j.  comput .",
    "phys . , 211 ( 2006 ) ,",
    ", _ a fast direct solver for boundary integral equations in two dimensions _ ,",
    "j.  comput .",
    "phys . , 205 ( 2005 ) , pp .",
    "123 . , _ an accelerated kernel - independent fast multipole method in one dimension _",
    ", siam j.  sci .",
    "comput . , 29 ( 2007 ) ,",
    "11601178 . , _ a fast direct solver for scattering problems involving elongated structures _ , j.  comput .",
    "phys . , 221 ( 2007 ) , pp .",
    ", _ fast multipole accelerated boundary integral equation methods _ , appl .",
    ", 55 ( 2002 ) , pp .",
    "299324 . , _",
    "multipole for scattering computations : spectral discretization , stabilization , fast solvers _ , ph.d .",
    "thesis , department of electrical and computer engineering , university of california , santa barbara , 2004 . , _ rapid solution of integral equations of scattering theory in two dimensions _ , j.  comput .",
    "phys . , 86 ( 1990 ) , pp .",
    "414439 . ,",
    "_ gmres : a generalized minimal residual algorithm for solving nonsymmetric linear systems _ , siam j.  sci .",
    ", 7 ( 1986 ) , pp .",
    ", _ reduced surface : an efficient way to compute molecular surfaces _ ,",
    "biopolymers , 38 ( 1996 ) , pp .",
    "305320 . , _ solving unsymmetric sparse systems of linear equations with pardiso _ , future gener .",
    "20 ( 2004 ) , pp .",
    "475487 . , _ bi - cgstab : a fast and smoothly converging variant of bi - cg for the solution of nonsymmetric linear systems _ , siam j.  sci .",
    ", 13 ( 1992 ) , pp .",
    "631644 . , _ a fast direct matrix solver for surface integral equation methods for electromagnetic wave problems in @xmath191 _ , in proceedings of the 27th international review of progress in applied computational electromagnetics , williamsburg , va , usa , 2011 , pp",
    "121126 . , _ automated empirical optimization of software and the atlas project _ , parallel comput",
    ". , 27 ( 2001 ) , pp .",
    "335 . , _ a multilevel fast direct solver for em scattering from quasi - planar objects _ , in proceedings of the international conference on electromagnetics in advanced applications , torino , italy , 2009 , pp",
    "640643 . ,",
    "_ a fast randomized algorithm for the approximation of matrices _ , appl .",
    ", 25 ( 2008 ) , pp .",
    "335366 . ,",
    "_ superfast multifrontal method for large structured linear systems of equations _ , siam j.  matrix anal .",
    "appl . , 31 ( 2009 ) , pp .",
    "13821411 . , _ a kernel - independent adaptive fast multipole algorithm in two and three dimensions",
    ", j.  comput .",
    "phys . , 196 ( 2004 ) ,"
  ],
  "abstract_text": [
    "<S> we present a fast direct solver for structured linear systems based on multilevel matrix compression . using the recently developed interpolative decomposition of a low - rank matrix in a recursive manner </S>",
    "<S> , we embed an approximation of the original matrix into a larger , but highly structured sparse one that allows fast factorization and application of the inverse . </S>",
    "<S> the algorithm extends the martinsson / rokhlin method developed for 2d boundary integral equations and proceeds in two phases : a precomputation phase , consisting of matrix compression and factorization , followed by a solution phase to apply the matrix inverse . for boundary </S>",
    "<S> integral equations which are not too oscillatory , e.g. , based on the green s functions for the laplace or low - frequency helmholtz equations , both phases typically have complexity @xmath0 in two dimensions , where @xmath1 is the number of discretization points . in our current implementation , the corresponding costs in three dimensions are @xmath2 and @xmath3 for precomputation and solution , respectively . </S>",
    "<S> extensive numerical experiments show a speedup of @xmath4 for the solution phase over modern fast multipole methods ; however , the cost of precomputation remains high . </S>",
    "<S> thus , the solver is particularly suited to problems where large numbers of iterations would be required . </S>",
    "<S> such is the case with ill - conditioned linear systems or when the same system is to be solved with multiple right - hand sides . </S>",
    "<S> our algorithm is implemented in fortran and freely available .    </S>",
    "<S> fast algorithms , multilevel matrix compression , interpolative decomposition , sparse direct solver , integral equations , fast multipole method    65f05 , 65f50 , 65r20 , 65y15 , 65z05 </S>"
  ]
}