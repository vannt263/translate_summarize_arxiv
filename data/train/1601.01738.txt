{
  "article_text": [
    "a @xmath0th - order @xmath1-dimensional real tensor @xmath2 consists of @xmath3 entries in real numbers : @xmath4,\\ ] ] where @xmath5=\\{1,2,\\ldots , n\\}.$ ] denote the set of all real @xmath0th - order @xmath1-dimensional tensors by @xmath6}$ ] .",
    "@xmath2 is called _ symmetric _ if the value of @xmath7 is invariant under any permutation of its indices @xmath8 .",
    "denote the set of all real symmetric @xmath0th - order @xmath1-dimensional tensors by @xmath9}$ ] . for any vector @xmath10",
    ", @xmath11 is a vector in @xmath12 with its @xmath13th component as @xmath14 a real symmetric tensor @xmath2 of order @xmath0 dimension @xmath1 uniquely defines a @xmath0th degree homogeneous polynomial function @xmath15 with real coefficient by @xmath16    we call that the tensor @xmath2 is positive definite if @xmath17 for all @xmath18 .    in 2005 , qi @xcite and lim @xcite proposed the definition of eigenvalues and eigenvectors for higher order tenors , independently . furthermore , in @xcite , these definitions were unified by chang , person and zhang .",
    "let @xmath2 and @xmath19 be real - valued , @xmath0th - order @xmath1-dimensional symmetric tensors .",
    "assume further that @xmath0 is even and @xmath19 is positive definite .",
    "we call @xmath20 is a * generalized eigenpair * of @xmath21 if @xmath22    when the tensor @xmath19 is an identity tensor @xmath23 such that @xmath24 for all @xmath25 @xcite , the eigenpair reduces to @xmath26-eigenpair @xcite .",
    "another special case is that when @xmath27 with @xmath28 the real scalar @xmath29 is called an @xmath30-eigenvalue and the real vector @xmath31 is the associated @xmath30-eigenvector of the tensor @xmath2 @xcite . in the last decade ,",
    "tensor eigenproblem has received much attention in the literature @xcite , which has numerous applications @xcite .    in this paper",
    ", we consider the tensor eigenvalue complementarity problem ( teicp ) :    finding a scalar @xmath32 and @xmath33 such that + @xmath34 where @xmath35}$ ] , and @xmath36}$ ] is positive definite .",
    "the solution of teicp @xmath37 is called pareto eigenpair of @xmath21 . in some special case",
    ", we can call it pareto h - eigenpair or pareto z - eigenpair @xcite if the tensor @xmath19 has special form as shown above in the generalized eigenpairs ( [ generalized eigenpair ] ) . replacing the nonnegative cones in ( [ teicp.problem ] ) by a closed convex cone and its dual cone , ling , he and qi investigated the cone eigenvalue complementarity problem for higher - order tensor in @xcite .",
    "moreover , in @xcite , they studied the high - degree eigenvalue complementarity problem for tensors as a natural extension of quadratic eigenvalue complementarity problem for matrices .",
    "teicp is also closely related to the optimality conditions for polynomial optimization @xcite , a class of differential inclusions with noncovex processes @xcite , and a kind of nonlinear differential dynamical system @xcite .",
    "the properties of pareto eigenvalues and their connection to polynomial optimization are studied in @xcite .",
    "recently , as a special type of nonlinear complementarity problems , the tensor complementarity problem is inspiring more and more research in the literature @xcite .",
    "a shifted projected power method for teicp was proposed in @xcite , in which they need an adaptive shift to force the objective to be ( locally ) convex to guarantee the convergence of power method . in @xcite , ling ,",
    "he and qi presented a scaling - and - projection algorithm ( spa ) for teicp .",
    "one main shortcoming of spa is the stepsize will approach to zero as the sequence gets close to a solution of teicp @xcite .",
    "recently , by introducing an ncp - function , chen and qi @xcite reformulated the teicp as a system of nonlinear equations .",
    "and then , they proposed a semismooth newton method for solving the system of nonlinear equations @xcite .    in this paper",
    ", we will investigate two spectral projected gradient algorithms for teicp .",
    "the rest of this paper is organized as follows . in section 2 ,",
    "some properties of the solutions of teicp and two optimization reformulations of teicp are presented . in section 3 ,",
    "two spectral projected gradient algorithms are proposed .",
    "global convergence results could be established under some suitable assumptions .",
    "we also present a shifted scaling - and - projection algorithm ( sspa ) in section 4 , which is a great improvement of the original spa method @xcite .",
    "numerical experiments are reported in section 4 to show the efficiency of the proposed methods .",
    "finally , we have a conclusion section .    throughout this paper ,",
    "let @xmath38 , and @xmath39 .",
    "given a set @xmath40 $ ] , the principal sub - tensor of a tensor @xmath35}$ ] , denoted by @xmath41 , is tensor in @xmath42}$ ] , such that @xmath43 for all @xmath44 .",
    "here , the symbol @xmath45 denotes the cardinality of @xmath46 .",
    "the following proposition shows the relationship between the solution of teicp ( [ teicp.problem ] ) and the generalized eigenvalue problem ( [ generalized eigenpair ] ) .",
    "@xmath37 is a solution of teicp ( [ teicp.problem ] ) if and only if there exists a subset @xmath47 $ ] , such that @xmath29 is a generalized eigenvalue of @xmath48 and @xmath49 is a corresponding eigenvector , and @xmath50\\backslash i.\\ ] ] in such a case , the pareto eigenvector @xmath31 satisfies @xmath51 .",
    "this proposition was firstly presented in @xcite for pareto h - eigenpair and pareto z - eigenpair , and then unified by xu and ling for teicp @xcite .",
    "denote the set of solutions of ( [ teicp.problem ] ) by @xmath52 , i.e. , @xmath53    if @xmath54 , then @xmath55 for any @xmath56 .",
    "on the other hand , given a tensor @xmath35}$ ] , we know that there exists the unique semi - symmetric tensor @xcite @xmath57 such that @xmath58 .",
    "it is clear that @xmath59 . without loss of generality",
    ", we always assume that @xmath60}$ ] and just consider the solutions on the unit - sphere with @xmath61 .",
    "the symmetric teicp ( [ teicp.problem ] ) is equivalent to the following optimization problem @xmath62 in the sense that any equilibrium solution @xmath31 of ( [ max - optimization - problem ] ) is a solution of the symmetric teicp .    by some simple calculations",
    ", we can get its gradient and hessian are as follows @xmath63 and its hessian is @xmath64 where @xmath65 , and @xmath66 is a matrix with its component as @xmath67.\\ ] ]    according to ( [ gradient ] ) , we can derive that the gradient @xmath68 is located in the tangent plane of @xmath69 at @xmath31 , since @xmath70    the lagrangian function is @xmath71 where @xmath72 and @xmath73 are the lagrange multipliers .",
    "any equilibrium solution of the nonlinear programming problem ( [ max - optimization - problem ] ) satisfies the kkt conditions @xmath74    using @xmath75 and @xmath76 , by taking the dot product with @xmath31 in the first equation , we get that @xmath77 .",
    "so , the first equation could be written as @xmath78 .",
    "since @xmath79 and @xmath19 is positive definite , it follows that @xmath80 i.e. any equilibrium solution @xmath31 of ( [ max - optimization - problem ] ) is a solution of the symmetric teicp ( [ teicp.problem ] ) .    furthermore , the global maximum / minmum of @xmath81 in @xmath82 is corresponding to the extreme value of pareto eigenpair of @xmath21 @xcite if @xmath19 is strictly copositive , i.e. , @xmath83 for any @xmath84 .",
    "the concept of copositive tensor is introduced by qi @xcite .",
    "a tensor @xmath2 is said copositive if @xmath85 for all @xmath84 .",
    "@xmath2 is copositive ( strictly copostive ) if and only if all of its pareto h - eigenvalues or z - eigenvalues are nonnegative ( positve , respectively ) @xcite .",
    "let @xmath86}$ ] , and @xmath19 is copositive .",
    "let @xmath87 and @xmath88 then @xmath89 and @xmath90 .    if both @xmath2 and @xmath19 are symmetric and strictly copositive tensors , then we can use logarithmic function as the merit function in ( [ max - optimization - problem ] ) .",
    "in such a case , teicp ( [ teicp.problem ] ) could be reformulated to the following nonlinear optimization problem :    @xmath91    its gradient and hessian are respectively    @xmath92    and @xmath93    the hessian is much simpler than that of rayleigh quotient function in ( [ max - optimization - problem ] ) .",
    "if one need to use hessian for computing pareto eigenvalue , the logarithmic merit function may be a favorable choice .",
    "in this section , the spectral projected gradient ( spg ) method is applied to the nonlinear programming problem ( [ max - optimization - problem ] ) .",
    "one main feature of spg is the spectral choice of step length ( also called bb stepsize ) along the search direction , originally proposed by barzilai and borwein @xcite .",
    "the barzilai - borwein method performs much better than the steepest descent gradient method or projected gradient method in practice @xcite . especially , when the objective function is a convex quadratic function and @xmath94 , a sequence generated by the bb method converges @xmath95-superlinearly to the global minimizer @xcite . for any dimension convex quadratic function , it is still globally convergent @xcite but the convergence is @xmath95-linear @xcite .",
    "we firstly present the following spectral projected gradient method with monotone line search .    ' '' ''     + * given tensors * @xmath96}$ ] and @xmath97}$ ] , an initial unit iterate @xmath98 , parameter @xmath99 .",
    "let @xmath100 be the tolerance of termination .",
    "calculate gradient @xmath101 , @xmath102 .",
    "set k=0 . +",
    "* step 1 : * compute @xmath103 and the direction @xmath104 + * step 2 : * if @xmath105 then stop : @xmath106 is a pareto eigenvalue , and @xmath31 is a corresponding pareto eigenvector of teicp .",
    "otherwise , set @xmath107 . + * step 3 : * if @xmath108 then define @xmath109 , @xmath110 , @xmath111 .",
    "otherwise , set @xmath112 and try again .",
    "+ * step 4 : * compute @xmath113 .",
    "if @xmath114 set @xmath115 ; else , compute @xmath116 and @xmath117 set @xmath118 and go to * step 1*. +    ' '' ''    here @xmath119 is a close convex set . by the projection operation and the convexity of @xmath120 , we know that for all @xmath121 and @xmath122 , @xmath123 set @xmath124 and @xmath125 in the above inequality , then we have @xmath126+\\|x - p_{\\omega}(x+\\beta g(x)\\|^2\\le0.\\ ] ] let @xmath127 with @xmath128 , then we have the following lemma .",
    "for all @xmath129 , @xmath130 $ ] , we have @xmath131    from ( [ eq : ascentdirection ] ) , we know that @xmath132 is an ascent direction . hence , a stepsize satisfying ( [ linesearch ] ) will be found after a finite number of trials , and the spg algorithm is well defined .",
    "when @xmath133 in @xmath134 , we call it spectral projected gradient ( spg ) . the vector @xmath135 vanishes if and only if @xmath136 is a constrained stationary point of optimization problem ( [ max - optimization - problem])/([log - max - optimization - problem ] ) .",
    "the convergence of spg method is established as follows .",
    "the proof is similar to that in @xcite .    [",
    "the : globalconvergence ] let @xmath137 is generated by spg1 algorithm .",
    "if there is a vector @xmath138 such that @xmath139 , then @xmath140 is a solution of the symmetric teicp .",
    "otherwise , any accumulation point of the sequence @xmath137 is a constrained stationary point , i.e. , the sequence @xmath141 converges to a pareto eigenvalue of the symmetric teicp .",
    "* let @xmath136 be an accumulation point of @xmath137 , and relabel @xmath137 a subsequence converging to @xmath136 . according to the proposition 2",
    ", we just need to show that @xmath136 is a constrained stationary point of the optimization problem .",
    "let us suppose by way of contradiction that @xmath136 is not a constrained stationary point .",
    "so , by continuity and compactness , there exist @xmath142 such that @xmath143 for all @xmath130 $ ] . furthermore , using the lemma 1 , we have @xmath144 for all @xmath130 $ ] , which implies that for @xmath145 larger enough on the subsequence that converges to @xmath136 , @xmath146 for all @xmath147 $ ] . here , we can set @xmath148 .",
    "we consider two cases .",
    "firstly , assume that @xmath149 . by continuity , for sufficiently large @xmath145 , @xmath150 .",
    "from the line search condition ( [ linesearch ] ) , we have @xmath151 clearly , when @xmath152 , @xmath153 , which is a contradiction .",
    "in fact , @xmath154 is a continuous function and so @xmath155 .",
    "assume that @xmath156 .",
    "since @xmath156 , there exists a subsequence @xmath157 such that @xmath158 .",
    "in such a case , from the way @xmath159 is chosen in ( [ linesearch ] ) , there exists an index @xmath160 sufficiently large such that for all @xmath161 , @xmath162 , for which @xmath163 fails to satisfy condition ( [ linesearch ] ) , i.e. , @xmath164 .",
    "hence , @xmath165 by the mean value theorem , we can rewrite this relation as @xmath166 where @xmath167 $ ] that goes to zero as @xmath162 goes to infinity .",
    "taking limits in the above inequality , we deduce that @xmath168 . since @xmath169 and @xmath170 for all k , then @xmath171 by continuity , this indicates that for @xmath145 large enough on the subsequence we have that @xmath172 , which contradicts to @xmath173 .",
    "therefore , any accumulation point of the sequence @xmath137 is a constrained stationary point . by using the proposition 2",
    ", it follows that the sequence @xmath141 converges to a pareto eigenvalue of the symmetric teicp .",
    "@xmath174 + in the rest of this section , we would like to present the following spg algorithm for teicp with curvilinear search . its global convergence could be established similarly .    ' '' ''     + * given tensors * @xmath96}$ ] and @xmath97}$ ] , an initial unit iterate @xmath98 , parameter @xmath99 .",
    "let @xmath100 be the tolerance of termination .",
    "calculate gradient @xmath101 , @xmath102 .",
    "set k=0 . +",
    "* step 1 : * if @xmath175 , stop , declaring @xmath106 is a pareto eigenvalue , and @xmath31 is a corresponding pareto eigenvector of teicp . + * step 2 : * set @xmath176 . + * step 3 : * set @xmath177 if @xmath178 then define @xmath179 , @xmath110 , @xmath111 .",
    "otherwise , set @xmath112 and try again .",
    "+ * step 4 : * compute @xmath113 . if @xmath114 set @xmath115 ; else , compute @xmath116 and @xmath180 set @xmath118 and go to * step 1*. +    ' '' ''",
    "in this section , we present some numerical results to illustrate the effectiveness of the spectral projected gradient ( spg ) methods , which were compared with the scaling - and - projection algorithm ( spa ) proposed by ling , he and qi @xcite and the shifted projected power ( spp ) method for teicp proposed in @xcite .    both spg1 and spg2 are monotone ascent method .",
    "@xmath181 are always located in the feasible region @xmath120 . in general , the merit function @xmath182 is chosen to be the rayleigh quotient function in ( [ max - optimization - problem ] ) . in the line search procedure of the spg1 method",
    ", we used the one - dimensional quadratic interpolation to compute the stepsize @xmath183 such as @xmath184 in the implementation , we terminate the algorithm once @xmath185 we accept @xmath186 and set the parameter @xmath187 , @xmath188 and @xmath189 . for spp and sspa , the parameter @xmath190 . in all numerical experiments ,",
    "the maximum iterations is 500 .",
    "the experiments were done on a laptop with intel core 2 duo cpu with a 4 gb ram , using matlab r2014b , and the tensor toolbox @xcite .",
    "we firstly describe the so - called shifted projected power ( spp ) algorithm and the scaling - and - projection algorithm ( spa ) as follows .    ' '' ''     + * given tensors * @xmath96}$ ] and @xmath97}$ ] , an initial unit iterate @xmath98 .",
    "let @xmath100 be the tolerance on termination .",
    "let @xmath191 be the tolerance on being positive definite .",
    "+ * for * @xmath192 * do * + * 1 : * compute the gradient @xmath193 and the hessian @xmath194 , respectively .",
    "let @xmath195 , @xmath196 .",
    "+ * 2 : * let @xmath197 + * 3 : * if @xmath198 , stop .",
    "otherwise , @xmath199 . set k = k+1 and go back to step 1 . +",
    "* end for * +    ' '' ''    ' '' ''     + * given tensors * @xmath96}$ ] and @xmath97}$ ] . for an initial point @xmath200 ,",
    "define @xmath201{\\mathcal{b}(u_0)^m}$ ] .",
    "let @xmath100 be the tolerance on termination .",
    "+ * for * @xmath192 * do * + * 1 : * compute @xmath202 , the gradient @xmath203 . + * 2 : * if @xmath204 , stop . otherwise , let @xmath205 , compute @xmath206 , and @xmath207{\\mathcal{b}(u_k)^m}$ ] . set k = k+1 and go back to step 1 . + * end for * +    ' '' ''    since the stepsize @xmath159 in spa approaches to zero as the sequence @xmath181 gets close to a solution of teicp , as shown in @xcite , the number of iterations will increase significantly . in order to improve the efficiency of spa method",
    ", they try to amplify the stepsize and proposed a modification of spa such as @xmath208 with @xmath209 being a constant parameter . a suitable choice",
    "@xmath210 will get an improvement .",
    "but , how to choose it ? anyway , the stepsize @xmath211 also approaches to zero when the sequence @xmath181 gets close to a solution of teicp .",
    "when the merit function @xmath182 is ( locally ) convex , this situation will be better .",
    "so , we present the following shifted spa method , in which an adaptive shift could force the objective to be ( locally ) convex @xcite .    ' '' ''     + * given tensors * @xmath96}$ ] and @xmath97}$ ] .",
    "for an initial point @xmath200 , define @xmath201{\\mathcal{b}(u_0)^m}$ ] .",
    "compute @xmath212 .",
    "let @xmath100 be the tolerance on termination .",
    "let @xmath191 be the tolerance on being positive definite .",
    "+ * for * @xmath192 * do * + * 1 : * compute @xmath213 , the hessian @xmath194 , respectively .",
    "let @xmath195 , @xmath214 .",
    "+ * 2 : * let @xmath215 , compute @xmath216 , and @xmath207{\\mathcal{b}(u_k)^m}$ ] , @xmath217 .",
    "+ * 3 : * if @xmath218 , stop .",
    "otherwise , set k = k+1 and go back to step 1 . +",
    "* end for * +    ' '' ''      the following example is originally from @xcite and was used in evaluating the ss - hopm algorithm in @xcite and the geap algorithm in @xcite for computing z - eigenpairs .",
    "_ example 1 _ ( kofidis and regalia @xcite ) .",
    "let @xmath219}$ ] be the symmetric tensor defined by @xmath220    [ fig : spg - spa - z - eigen-1 ] @xmath221{spg - spp - spa - pareto - z - eigenvalue-1.eps } } \\end{array}\\ ] ]    [ fig : spg - spp - sspa - z - eigen-100 - 1 ] @xmath221{spg - spp - sspa - pareto - z - eigenvalue-100 - 1.eps } } \\end{array}\\ ] ]    to compare the convergence in terms of the number of iterations .",
    "figure 1 shows the results for computing pareto z - eigenvalues of @xmath2 from * _ example 1 _ * , and the starting point is @xmath222 $ ] . in this case , all of the spg1 , spg2 , spp , sspa can reach the same pareto z - eigenvalue 0.3633 .",
    "spg1 method just need run 9 iterations in 0.1716 seconds while spa method need run 260 iterations in 3.3696 seconds .",
    "spp is similar to spg1 method in this case .",
    "spg2 need run 13 iterations in 0.4368 seconds . as we can see , comparing with spa method , sspa method get a great improvement .",
    "sspa method just need run 19 iterations in 0.2964 seconds .     +    [",
    "cols=\"^,^,^,^,^,^\",options=\"header \" , ]",
    "in this paper , two monotone ascent spectral projected gradient algorithms were investigated for the tensor eigenvalue complementarity problem ( teicp ) .",
    "we also presented a shifted scaling - and - projection algorithm , which is a great improvement of the original spa method @xcite .",
    "numerical experiments show that spectral projected gradient methods are efficient and competitive to the shifted projected power method .",
    "this work was supported in part by the national natural science foundation of china ( no.61262026 , 11571905 , 11501100 ) , ncet programm of the ministry of education ( ncet 13 - 0738 ) , jgzx programm of jiangxi province ( 20112bcb23027 ) , natural science foundation of jiangxi province ( 20132bab201026 ) , science and technology programm of jiangxi education committee ( ldjh12088 ) , program for innovative research team in university of henan province ( 14irtsthn023 ) ."
  ],
  "abstract_text": [
    "<S> this paper looks at the tensor eigenvalue complementarity problem ( teicp ) which arises from the stability analysis of finite dimensional mechanical systems and is closely related to the optimality conditions for polynomial optimization . </S>",
    "<S> we investigate two monotone ascent spectral projected gradient ( spg ) methods for teicp . </S>",
    "<S> we also present a shifted scaling - and - projection algorithm ( spa ) , which is a great improvement of the original spa method proposed by ling , he and qi [ comput . </S>",
    "<S> optim . </S>",
    "<S> appl . </S>",
    "<S> , doi 10.1007/s10589 - 015 - 9767-z ] . numerical comparisons with some existed gradient methods in the literature are reported to illustrate the efficiency of the proposed methods .    </S>",
    "<S> * keywords : * tensor , pareto eigenvalue , pareto eigenvector , projected gradient method , eigenvalue complementarity problem . </S>"
  ]
}