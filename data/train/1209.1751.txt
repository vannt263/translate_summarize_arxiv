{
  "article_text": [
    "in his pioneering research , g. k. zipf showed that more frequent words tend to be shorter @xcite , and parallels of this brevity law have been reported for the behavior of other species @xcite .",
    "recently , it has been argued that `` average information content is a much better predictor of word length than frequency '' and that this `` indicates that human lexicons are efficiently structured for communication by taking into account interword statistical dependencies . ''",
    "@xcite . according to the uniform information density hypothesis ( e.g. , @xcite ) , ``",
    "language users make choices that keep the number of bits of information communicated per unit of time approximately constant '' and thus `` the amount of information conveyed by a word should be linearly related to the amount of time it takes to produce approximately , its length to convey the same amount of information in each unit of time '' @xcite . here",
    "it will be shown that hitting keys from a keyboard at random ( e.g. , @xcite ) generates words that reproduce this linear relationship .",
    "therefore , the observation of such a linear relationship does not constitute unequivocal evidence for any kind of optimal choices made by speakers .    throughout this paper",
    ", @xmath0 denotes contexts and @xmath1 denotes words . as in ref .",
    "@xcite , the context of a word consists of a fixed number of preceding words , and the information content of a word @xmath2 is given by @xmath3 the expected information content of words of length @xmath4 is defined as @xcite @xmath5 where @xmath6 is the length ( in letters ) of a word @xmath2 and @xmath4 is a fixed parameter value . in this study",
    ", we detail some connections between @xmath7 and standard information theory measures . the definition of @xmath7 that we borrow from ref .",
    "@xcite is somewhat idiosyncratic in relation to standard information - theory .",
    "we found that , ref .",
    "@xcite , the reference supplied in ref .",
    "@xcite as a justification for eq .",
    "[ information_content_versus_length_equation ] , does not in fact justify the equation in any evident way . in this study",
    "we demonstrate that @xmath8 is a linear function of @xmath4 for a general class of random typing processes .",
    "the only requirement is that the context is defined by means of neighbouring words ( as in @xcite ) or that empty words ( words of length zero ) are allowed as in many variants of the random typing process @xcite .",
    "we now introduce our basic notation and conventions . the self - information of an event that has probability @xmath9 is @xmath10 .",
    "we consider @xmath0 and @xmath1 independent if and only if @xmath11 . as usual , by the definition of conditional probability , independence implies both @xmath12 and @xmath13 , for any individual @xmath14 and @xmath2 .",
    "therefore , under independence between @xmath0 and @xmath1 , it holds that @xmath15 , that is to say , @xmath7 is just the self - information of @xmath2 .",
    "the expected self - information content of a word of length @xmath4 is @xmath16 in sum , under independence between @xmath0 and @xmath1 , @xmath8 and @xmath17 coincide .",
    "the conditional entropy is defined as , @xmath18 given only the joint probability , i.e. @xmath19 , one can use bayes theorem for calculating the conditional and marginal probabilities , as it was done in previous work @xcite and is assumed by various information theoretic models of zipf s law for word frequencies @xcite .",
    "simple application of bayes theorem to the definition of @xmath20 in shows that the conditional entropy is the expectation of @xmath7 : @xmath21 . \\label{relationship_between_word_conditional_entropy_and_information_content_equation}\\end{aligned}\\ ] ]    it is not difficult to see that @xmath22 is the upper bound of @xmath7 and @xmath23 is its lower bound ; formally , @xmath24 as for a lower bound of @xmath7 , the relative entropy ( or kullback - leibler divergence ) between the context conditional probability and the word conditional probability is @xcite @xmath25 therefore @xmath26 by the non - negativity of the relative entropy @xcite . as for the upper bound of @xmath7 , the non - negativity of mutual information , i.e. @xmath27 @xcite and , yields @xmath28 if and only if @xmath29 , as we wanted to prove . combining and results in @xmath30 where @xmath31 is defined as @xmath32",
    "random typing @xcite is a process in which a sequence of characters is produced by sampling randomly from a set of possible characters . here",
    "we consider a generalized random typing model based upon variants allowing for unequal letter probabilities as in @xcite and allowing one to specify a minimum word length @xcite .",
    "assume that characters are produced from an alphabet @xmath33 , where @xmath34 is the alphabet size , @xmath35 represents the word delimiter ( i.e. , the space character ) and the remaining characters of @xmath36 are letters .",
    "we assume that all the characters in @xmath36 are produced at random and independently , with the only exception that two instances of the space character must be separated by at least @xmath37 intervening characters other than the space . in such model",
    ", the production of a word is separated into two phases : generation of the space - free prefix of length @xmath37 , and generation of the remainder .",
    "@xmath38 is a random variable taking values from @xmath36 as generated by the random typing process .",
    "@xmath39 is defined as the probability of producing character @xmath40 as the @xmath41-@xmath42 character after the last space produced ( or after the beginning of the sequence if no space has been produced yet ) , for any value @xmath43 .",
    "@xmath44 is the same probability as @xmath39 for values of @xmath45 .",
    "the abbreviation @xmath46 will be used hereafter .",
    "we assume that @xmath47 for all characters in @xmath36 with the additional constraint that @xmath48 .",
    "@xmath44 is defined in terms of @xmath49 , @xmath50 the generalized random typing process with unequal letter probabilities is defined by @xmath34 parameters : @xmath37 and the @xmath51 probabilities @xmath52 for @xmath53 with @xmath54 notice the additional parameter @xmath37 that is not considered in other versions of the random typing model and allows for unequal character probabilities @xcite .    in the remainder of this section",
    "we start by proving that @xmath17 is a linear function of @xmath4 , providing exact analytical expressions for its slope and intercept .",
    "we continue by showing that @xmath8 can be inferred from @xmath17 . if the context is defined by words , as in ref .",
    "@xcite , then @xmath55 because our generalized random typing process produces words independently from the previous ones .",
    "if the context are characters , then @xmath55 is also warranted when @xmath56 because this is the case where self - repulsion of the space is suppressed .",
    "when @xmath57 , indicates that @xmath8 can not exceed @xmath17 .    in order to calculate the probability of producing a concrete word @xmath58 , where @xmath59 is the @xmath60-th character from @xmath36 of @xmath2 , we use the shorthand @xmath61 by the independence between characters ( except for space self - repulsion at distances smaller than @xmath37 ) , the probability that a random word @xmath1 that has length @xmath4 coincides with @xmath58 is @xmath62 the probability that a word has length @xmath4 is @xmath63 and the probability of a word @xmath2 given its length is therefore @xmath64 applying , the self - information of a word @xmath2 of length @xmath4 is @xmath65 where @xmath66 is defined as @xmath67 combining and with the definition of @xmath17 in , gives @xmath68 bearing in mind that @xmath69 one can write @xmath70 notice that @xmath71 & = & \\nonumber \\\\ ( h_\\sigma(s ) + p_0 \\ln p_0)\\sum_{s_1 , ... ,",
    "s_{j-1 } , s_{j+1}, ... ,s_\\ell } { \\cal p}_{1,j-1 } { \\cal p}_{j+1,\\ell } & = & \\nonumber \\\\ ( h_\\sigma(s ) + p_0 \\ln p_0)(1-p_0)^{\\ell-1 } , & & \\label{character_entropy_extraction_equation}\\end{aligned}\\ ] ] where @xmath72 is the character entropy after the space - free prefix of length @xmath37 .",
    "therefore , applying to one finally obtains @xmath73 , where @xmath74 and @xmath66 is defined as in .",
    "notice that the slope @xmath75 is always positive because @xmath76 as any entropy and , according to , @xmath77 provided that @xmath78 ( recall that no character from @xmath36 has probability zero of occurring after the free - space prefix ) .",
    "therefore , @xmath17 grows linearly with @xmath4 for @xmath78 .",
    "summarizes the parameters of the linear relationship between @xmath17 for our generalized random typing process and two particular cases : ( a ) equal letter probabilities ( all characters except the space must be equally likely ) @xcite and ( b ) equal character probabilities ( all characters including the space are equally likely ) and empty words are allowed , i.e. @xmath56 @xcite .",
    "notice that ( b ) is a particular case of ( a ) .",
    "variant ( a ) @xcite means that @xmath79 and is defined only by three parameters : @xmath37 , @xmath34 and @xmath80 .",
    "the random typing process defined in @xcite is a particular case with @xmath56 . in a random typing process with equal letter probabilities , the character entropy after the space - free prefix is @xmath81 variant ( b ) , the simplest random typing that has ever been presented to our knowledge , is defined with only one parameter , i.e. @xmath34 ( @xmath56 and @xmath82 in that case ) .",
    "( b ) is known as the fair die rolling experiment @xcite ( see @xcite for a version with @xmath83 and @xmath82 ) .",
    "@lrrr & & random typing & + & generalized & equal letter & equal character + & & probabilities @xcite & probabilities + & & & ( with @xmath56 @xcite ) + @xmath75 & @xmath84 & @xmath85 & @xmath86 + @xmath66 & @xmath87 & @xmath87 & @xmath86 + @xmath88 & @xmath89 & @xmath90 & @xmath86 + & @xmath91 & @xmath91 & + @xmath80 & @xmath80 & @xmath80 & @xmath92 + @xmath93 & @xmath94 & @xmath95 & @xmath92 + @xmath96 & @xmath97 & @xmath98 & @xmath98 +",
    "we have shown that @xmath99 does not imply that speakers have made optimal choices as argued in @xcite .",
    "uniform information density or related hypotheses ( e.g. , @xcite ) are not at all necesary to account for the linear correlation between @xmath8 and @xmath4 : typing at random yields the same dependency independently from context .",
    "our main point is that a linear correlation between information content and word length may simply arise internally , from the units making a word ( e.g. , letters ) and not necessarily from the interplay between words and their context as suggested in @xcite .",
    "however , future research should investigate if the parameters of the linear relationship predicted by random typing coincide with those of real texts or if a linear relationship is sufficient to account for the actual dependency between @xmath8 and @xmath4 in real languages as it is suggested by the long - range correlations in texts at the level of words @xcite or letters @xcite and the differences between random typing and real language at the level of the distribution of word frequencies @xcite or word lengths @xcite .",
    "this work was supported by the project openmt-2",
    "( tin2009 - 14675-c03 ) from the spanish ministy of science and innovation ( rfc ) .",
    "10                  u.  cohen priva . using information content to predict phone deletion . in _ proceedings of the 27th west coast conference on formal linguistics _ , pages 9098 .",
    "cascadilla proceedings project , somerville , ma , 2008 ."
  ],
  "abstract_text": [
    "<S> recently , it has been claimed that a linear relationship between a measure of information content and word length is expected from word length optimization and it has been shown that this linearity is supported by a strong correlation between information content and word length in many languages ( _ piantadosi et al . </S>",
    "<S> 2011 , pnas 108 , 3825 - 3826 _ ) . here </S>",
    "<S> , we study in detail some connections between this measure and standard information theory . </S>",
    "<S> the relationship between the measure and word length is studied for the popular random typing process where a text is constructed by pressing keys at random from a keyboard containing letters and a space behaving as a word delimiter . </S>",
    "<S> although this random process does not optimize word lengths according to information content , it exhibits a linear relationship between information content and word length . </S>",
    "<S> the exact slope and intercept are presented for three major variants of the random typing process . </S>",
    "<S> a strong correlation between information content and word length can simply arise from the units making a word ( e.g. , letters ) and not necessarily from the interplay between a word and its context as proposed by piantadosi _ </S>",
    "<S> et al_. in itself , the linear relation does not entail the results of any optimization process .    </S>",
    "<S> _ keywords _ : zipf s law of brevity , random typing , uniform information density . </S>"
  ]
}