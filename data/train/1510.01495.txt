{
  "article_text": [
    "whenever we create behavior in autonomous robots we strive for a suitable measure in order to quantify success , learning progress or compare algorithms with each other .",
    "when a specific task is given , then typically the task provides a natural measure of success , for instance walking may be measured by velocity and perturbation stability . in cases where behavior is learned via optimization of a global objective function",
    "then the same function can also be used as a quantification , so creating and quantifying behavior often go hand in hand .",
    "this also applies in principle to behavior from task - independent objectives that have been recently more and more successful in generating emergent autonomous behavior in robots  @xcite .",
    "however , there are several cases of emergent behavior where this strategy fails : if the behavior arises from a ) optimizing a local function  @xcite , b ) optimizing a crude approximation of a computationally expensive objective function  @xcite , c ) local interaction rules without an explicit optimization function  @xcite , and d ) a biological system ( e.g.freely moving animals ) where we do nt know the underlying optimization function .",
    "thus , independent of the origin of behavior it would be useful to have a quantitative description of its structure .",
    "this would allow to objectively compare algorithms with each other and to compare technical with biological autonomous systems .",
    "this paper presents a measure of behavioral complexity that is suitable for the analysis of this kind of emergent behavior .",
    "we base our measure on the predictive information  ( pi )  @xcite of a set of observables , such as joint angles .",
    "the pi is the mutual information between past and future of a time - series and captures how much information ( in bits ) can be used from the past to predict the future .",
    "it is also closely linked to the excess entropy  @xcite or effective measure complexity  @xcite , and it is a natural measure for the complexity of a time series because it provides a lower bound for the information necessary for optimal prediction .",
    "thus it is a promising choice as a measure . given these favorable properties , the pi was also proposed as an intrinsic drive for behavior generation in autonomous robots  @xcite . on an intuitive level maximizing the pi of the sensor process , leads to a high variety of sensory input  large marginal entropy  while keeping a temporal structure  high predictability corresponding to a small entropy rate .",
    "unfortunately , there are conceptual and practical challenges in using the pi for generating and measuring autonomous behavior . conceptually",
    ", for systems with continuous values the pi provides a family of functions depending on the resolution of the measurements and practically it is difficult to estimate the pi . for a fixed partition ( single resolution ) in a low - dimensional case it is possible to estimate the one - step pi and adapt a controller to maximize it  @xcite . in high - dimensional systems",
    "it can not be used directly in an online algorithm .",
    "an alternative approach  @xcite uses a dynamical systems model , linearization and a time - local version of the pi  @xcite to obtain an explicit gradient for locally maximizing pi for continuous and high - dimensional state spaces ( no global optimization ) .",
    "a simplified version of the resulting neural controller  @xcite was used for generating the examples presented in this paper .",
    "the aim of the present paper is to provide a measure of behavioral complexity that can be applied to emergent autonomous behavior , animal behavior and potentially other time - series . in order to do this",
    "it turns out that the predictive information as a complexity measure has to be refined by taking into account its scale dependency . because the systems of interest are high - dimensional we investigate different methods to estimate information theoretic quantities in a resolution dependent way and present results for different robotic experiments .",
    "the emergent behavior in these experiments are generated by the above mentioned learning rules .",
    "on the one hand we want to increase our understanding of the learning process guided by the time - local version of the pi and on the other hand demonstrate how the resolution dependence allows us to quantify behavior on different length - scales .",
    "we argue that many natural behaviors are low - dimensional , at least on a coarse scale .",
    "for instance in walking all joint angles can be typically related to a single phase , so in the extreme case it is one - dimensional  @xcite . however , as we show below in [ ss : entrdimexcessent ] , this contradicts to the global maximization of the pi which would maximize the dimension .",
    "we discuss this contradiction and how it was circumvented in the practical applications . along with the paper",
    "there is a supplementary material located at http://playfulmachines.com/quantbeh2015/ [ ] and the source code can be found in the repository https://github.com/georgmartius/behavior-quant [ ] .",
    "the plan of the paper is as follows . in sec .",
    "[ ss : entrdimexcessent ] we introduce the information theoretic quantities and in sec .",
    "[ subsec : methods - estimation ] the two estimation methods used in the paper . in sec .",
    "[ sec : decomp : algo ] we describe the algorithm used to calculate the quantification measures . in sec .",
    "[ ss : example ] we demonstrate their behavior at the example of the lorenz system including the effects of noise .",
    "[ s : results ] starts with explaining the robots and the control framework used for the experiments which are described in [ experiments ] .",
    "the results of the experiments quantification are presented in sec .",
    "[ s : quant - behavior ] and are discussed in sec .",
    "[ s : discussion ] .",
    "starting point is a ( in general ) vector valued stationary time series @xmath0 . it is used to reconstruct the phase space of the underlying dynamical system by using delay embedding : @xmath1 .",
    "if the original time series was @xmath2dimensional the reconstructed phase space will be of dimension @xmath3 . in the following",
    ", we will only consider the case @xmath4 , i.e. scalar time series .",
    "the generalization to vector - values time series is straight forward .",
    "let us assume that we are able to reconstruct approximately the joint probability distribution @xmath5 in a reconstructed phase space",
    ". then we can characterize its structure using measures from information theory .",
    "information theoretic measures represent a general and powerful tool for the characterization of the structure of joint probability distributions @xcite .",
    "the uncertainty about the outcome of a single measurement of the state , i.e. about @xmath6 is given by its _",
    "entropy_. for discrete - valued random variable @xmath7 with values @xmath8 and a probability distribution @xmath9 it is defined as @xmath10 an alternative interpretation for the entropy is the average number of bits required to encode a new measurement . in our case , however , the @xmath11 are considered as continuous - valued observables ( that are measured with a finite resolution ) . for continuous random variables with a probability density @xmath12 one can also define an entropy , the _ differential entropy _ @xmath13    however , it behaves differently than its discrete counterpart : it can become negative and it will get even minus infinity if the probability measure for @xmath7 is not absolutely continuous w.r.t.to the lebesgue measure  for instance , in the case of the invariant measure of a deterministic system with an attractor dimension smaller than the phase space dimension .",
    "therefore , when using information theoretic quantities for characterizing dynamical systems researchers often prefer using the entropy for discrete - valued random variables . in order to use them for dynamical systems with continuous variables usually either partitions of the phase space or entropy - like quantities based on coverings are employed .",
    "these methods do not explicitly reconstruct the underlying invariant measure , but exploit the neighbor statistics directly .",
    "alternatively one could use direct reconstructions using kernel density estimators  @xcite or methods based on maximizing relative entropy  @xcite to gain parametric estimates .",
    "these reconstructions , however , will always lead to probability densities , and are not suitable for reconstructing fractal measures which appear as invariant measures of deterministic systems .    in this paper",
    "we use estimators based on coverings , i.e.correlation entropies @xcite and nearest neighbors based methods  @xcite considered in [ subsec : methods - estimation ] below . for the moment let us consider a partition of the phase space into hypercubes with side - length @xmath14 . for a more general definition of an @xmath14-partition ( see * ? ? ?",
    "in principle one might consider scaling the different dimensions of @xmath11 differently , but for the moment we assume that the time series was measured using an appropriate rescaling .",
    "the entropy of the state vector @xmath6 observed with a @xmath15-partition will be denoted in the following as @xmath16 with @xmath14 parameterizing the resolution .",
    "how does the entropy change if we change @xmath15 ?",
    "the uncertainty about the potential outcome of a measurement will increase if the resolution of the measurement is increased , because of the larger number of potential outcomes . if @xmath17 is a m - dimensional random variable and distributed according to a corresponding probability density function @xmath12 we have asymptotically for @xmath18 ( ( see cover and thomas * ? ? ?",
    "* ch.8 , p.248 , theorem 8.3.1 ) or @xcite ) .",
    "@xmath19 this is what we would expect for a stochastic system .",
    "however , if we observe a deterministic system the behavior of an observable depends how its dimension relates to the attractor dimension .",
    "if the embedding dimension is smaller than the attractor dimension the deterministic character will not be resolved and eq .",
    "[ eps - entr - m ] still applies .",
    "however , if the embedding dimension is sufficiently high ( @xmath20  @xcite ) then instead of a density function @xmath12 we have to deal with a @xmath21-dimensional measure @xmath22 and the entropy will behave as @xmath23 if an behavior such as in eqs .",
    "( [ eps - entr - m ] ) or ( [ eps - entr - d ] ) is observed for a range of @xmath14 values we will call this range a _ stochastic _ or _ deterministic scaling range _ , respectively .",
    "let us consider two discrete - valued random variables @xmath7 and @xmath24 with values @xmath8 and @xmath25 , respectively .",
    "then the uncertainty of a measurement of @xmath7 is quantified by @xmath26 .",
    "now we might ask , what is the average remaining uncertainty about @xmath7 if we have seen already @xmath24 ?",
    "this is quantified by the _",
    "conditional entropy _ @xmath27 the reduction of uncertainty about @xmath7 knowing @xmath24 is the information that @xmath24 provides about @xmath7 and is called the _ mutual information _ between @xmath7 and @xmath24 @xmath28    having defined the @xmath15-dependent state entropy @xmath16 we can now ask , how much information the present state contains about the state of the system at the next time step . the answer is given by the mutual information between @xmath29 and @xmath30 : @xmath31    using eq .",
    "[ eps - entr - m ] one see , that for stochastic systems the mutual information will remain finite in the limit @xmath32 and can be expressed by the differential entropies : @xmath33 note , that this mutual information is invariant with respect to coordinate transformation of the system state , i.e. if @xmath34 is a continuous and invertible function , then @xmath35 however , in the case of a deterministic system , the mutual information will diverge @xmath36 this is reasonable behavior because in principle the actual state contains an arbitrary large amount of information about the future . in practice , however , the state is known only with a finite resolution determined by the measurement device or the noise level .",
    "the unpredictability of a time series can be characterized by the conditional entropy of the next state given the previous states . in the following we will use an abbreviated notation for these conditional entropies and the involved entropies : @xmath37 the _ entropy rate _ ( ( see cover and thomas * ? ? ?",
    "* ch 4.2 ) ) is this conditional information if we condition on the infinite past @xmath38    in the following we assume stationarity , i.e. we have no explicit time dependence of the joint probabilities and therefore also of the entropies . moreover , if it is clear from the context , which stochastic process is considered , we will write @xmath39 and @xmath40 instead of @xmath41 and @xmath42 , respectively and it holds @xmath43    for deterministic systems the entropy rate will converge in the limit @xmath32 to the kolmogorov - sinai ( ks-)entropy @xcite which is a dynamical invariant of the system in the sense that it is independent on the specific state space reconstruction . moreover , already for finite @xmath44 , @xmath40 will not depend on @xmath15 for sufficiently small @xmath15 because of ( [ eps - entr - d ] ) .",
    "to quantify the amount of predictability in a state sequence one might consider subtracting the unpredictable part from the total entropy of a state sequence . by doing this one ends up with a well known complexity measure for time series , the _ excess entropy _ @xcite or _ effective measure complexity _",
    "@xcite @xmath45 with @xmath46 the excess entropy provides a lower bound for the amount of information necessary for an optimal prediction . for deterministic systems ,",
    "however , it will diverge because @xmath47 will behave according to @xmath48 and @xmath49 will become @xmath14-independent for sufficiently large @xmath50 and small @xmath14 , respectively @xmath51 with @xmath21 being the attractor dimension .    the _ predictive information _  @xcite is the mutual information between the semi - infinite past and the future time series @xmath52 with @xmath53 if the limits  ( [ limem ] ) and  ( [ limpin ] ) exist the predictive information @xmath54 is equal to the excess entropy @xmath55 . for the finite time variants in general @xmath56 : @xmath57 however , if the system is markov of order @xmath58 the conditional probabilities will only depend on the previous @xmath58 time steps , @xmath59 , hence @xmath60 for @xmath61 and therefore @xmath62 .      in the literature",
    "@xcite both the excess entropy and the predictive information were studied only for a given partition  usually a generating partition .",
    "thus , using the excess entropy as a complexity measure for continuous valued time series has to deal with the fact that its value will be different for different partitions  even for different generating ones .",
    "in ( [ eqn : em - det ] ) we have seen that the excess entropy for deterministic systems becomes infinite in the limit @xmath32 .",
    "the same applies to the predictive information ( [ eqn : pi ] ) .",
    "moreover , we have seen that the increase of these complexity measures with decreasing @xmath15 is controlled by the attractor dimension of the system .",
    "does this means that in the case of deterministic systems the excess entropy as a complexity measure reduces to the attractor dimension ?",
    "not completely . the constant in ( [ eqn : em - det ] ) reflects not only the scale of the signal , but also statistical dependencies or memory effects in the signal , in the sense that it will be larger if the conditional entropies converge slower towards the ks - entropy .",
    "how can we separate the different contributions ? we will start by rewriting eq   as a sum . using the conditional entropies ( [ eq : cond_entr ] ) we get @xmath63 using the differences between the conditional entropies @xmath64 the excess entropy can be rewritten as @xmath65 note that the difference @xmath66 is the conditional mutual information @xmath67 it measures dependencies over @xmath50 time steps that are not captured by dependencies over @xmath68 time steps . in other words ,",
    "how much uncertainty about @xmath69 can by reduced if in addition to the @xmath68 step past also the @xmath50th is taken into account . for",
    "a markov process of order @xmath50 the @xmath70 vanish for @xmath71 . in this case",
    "the sum eq   contains only a finite number of terms . on the other side truncating the sum at finite @xmath50",
    "could be interpreted as approximating the excess entropy by the excess entropy of an approximating markov process .",
    "what can be said about the scale dependency of the @xmath66 ? from eqs .",
    "( [ eps - entr - m ] ) and ( [ eps - entr - d ] ) follows that @xmath72 for @xmath73 and @xmath74 for @xmath44 . using this and eq .",
    "( [ eq : deltah ] ) we have to distinguish four cases for deterministic systems .",
    "note that @xmath75 denotes the fractal part of the attractor dimension .",
    "@xmath76 thus the sum eq .",
    "( [ eqn : em_deltah ] ) can be decomposed into three parts : @xmath77 with the @xmath15 dependence showing up only in the middle term ( mt ) : @xmath78 with @xmath79 denoting the length scale where the deterministic scaling range starts .",
    "therefore we have decomposed the excess entropy in three terms : two ideally @xmath15 independent terms and one @xmath15-dependent term . the first term",
    "@xmath80 will be called `` state complexity '' in the following because it is related to the information encoded in the state of the system .",
    "the constant @xmath81 was added here in order to ensure that the @xmath14-dependent term vanishes at @xmath82  the beginning of the deterministic scaling range .",
    "the second @xmath15-independent term @xmath83 will be called `` memory complexity '' because it is related to the dependencies between the states on different time steps .",
    "what we call `` state '' in this context is related to the minimal embedding dimension to see the deterministic character of the dynamics which is @xmath84 @xcite . in order to be able to get a one to one reconstruction of the attractor",
    "a higher embedding dimension might be necessary @xcite . both @xmath85-independent terms together we will call `` core complexity '' @xmath86    so far we only addressed the case of a deterministic scaling range . in the case of a noisy chaotic system",
    "we have to distinguish two @xmath15 regions : the deterministic scaling range described above and the noisy scaling range with @xmath87 with @xmath88 determined by the noise level . in the stochastic scaling range",
    "all @xmath89 become @xmath15-independent and the decomposition ( [ eq : e_decomp ] ) seems to become unnecessary .",
    "this is not completely the case .",
    "let us assume that the crossover between the two regions happens at a length scale @xmath90 .",
    "moreover , let us assume that for sufficiently large @xmath50 we have in the deterministic scaling range @xmath91 ( cf . figs .",
    "[ fig : lorenz : quant][l:0:h2][l : n2:h2 ] ) .",
    "then we have @xmath92 which allows to express the cross - over scale @xmath90 in terms of the ks - entropy and the noise level related continuous entropy @xmath88 @xmath93 moreover , the excess entropy in the _ deterministic scaling range _ will behave as @xmath94 evaluating this expressing at the crossover length scale @xmath90 allows to express the value of the excess entropy in the _",
    "stochastic scaling range _ as @xmath95 in particular , this expression shows that in decreasing the noise level , i.e.@xmath88 will increase the asymptotic value of the excess entropy for noisy systems .",
    "thus , an increased excess entropy or predictive information for a fixed length scale or partition can be achieved in many ways :    1 .   by increasing dimension @xmath21 of the dynamics 2 .   by decreasing the noise level @xmath90 3 .   by increasing the amplitude",
    "@xmath96 4 .   by increasing the state complexity 5 .   by increasing the correlations measured by the `` memory '' complexity , i.e. by increasing the predictability 6 .   by decreasing the entropy rate @xmath97 , i.e. by decreasing the unpredictability    naturally",
    ", the effect of the noise level will be observed in the stochastic scaling range only . in practice",
    "there might be more than one deterministic and stochastic scaling range or even no clear scaling range at all .",
    "how we will deal with these cases will be described below when we introduce our algorithm .      reliably estimating entropies and mutual information is very difficult in high - dimensional spaces due to the increasing bias of entropy estimates .",
    "therefore we will employ two different approaches .",
    "on the one hand we will use an algorithm for the estimation of the mutual information proposed by @xcite based on nearest neighbor statistics which allows to reduce the bias by employing partitions of different sizes in spaces of different dimensions . on the other hand",
    "we calculate a proxy for the excess entropy using correlation entropies @xcite of order @xmath98 .",
    "these are related to the rnyi entropies of second order and the correlation sum provides an unbiased estimator .",
    "both methods do not require binning but differ substantially in what they compute .",
    "[ [ estimation - via - local - densities - from - nearest - neighbor - statistics - ksg ] ] estimation via local densities from nearest neighbor statistics ( ksg ) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the most common approach to estimate information quantities of continuous processes , such as the mutual information , is to calculate the differential entropies ( [ eqn : entropy : diff ] ) directly from the nearest neighbor statistics .",
    "the key idea is to use nearest neighbor distances @xcite as proxies for the local probability density .",
    "this method corresponds in a way to an adaptive bin - size for each data point .",
    "for the mutual information @xmath99 ( required e.g.to calculate the pi ( [ eqn : pi ] ) ) , however , it is not recommended to naively calculate it directly from the individual entropies of @xmath7 , @xmath24 and their joint @xmath100 because they may have very dissimilar scale such that the adaptive binning leads to spurious results .",
    "for that reason a new methods was proposed by @xcite , that we call _ ksg _ , which only uses the nearest neighbor statistics in the joint space .",
    "we denote @xmath101 the mutual information estimate where @xmath102 nearest neighbors where used for the local estimation .",
    "the length scale on which the mutual information is estimated by this algorithm depends on the available data . in the limit of infinite amount of data @xmath103 for @xmath104 .",
    "however , in order to evaluate the quantity at a certain length scale ( similar to @xmath15 above ) and assuming the same underlying space for @xmath7 and @xmath24 , noise of strength @xmath105 is added to the data resulting in @xmath106 where @xmath107 is the uniform distribution in the interval @xmath108 $ ] .",
    "the idea of adding noise is to make the processes x and y independent within neighborhood sizes below the length scale of the noise . in this way",
    "only the structures above the added noise - level contribute to the mutual information .",
    "note that for small @xmath105 the actual scale ( @xmath102-neighborhood size ) may be larger due to sparsity of the available data .",
    "the correlation sum is one of the standard tools in nonlinear time series analysis  ( * ? ? ?",
    "* chapter  6 ) , @xcite .",
    "normally it is used to estimate the attractor dimension .",
    "however , it can also be used to provide approximate estimates of entropies and derived quantities such as the excess entropy .",
    "the correlation entropies for a random variable @xmath109 with measure @xmath110 are defined as @xcite      where @xmath112 is the `` ball '' at @xmath113 with radius @xmath15 . for @xmath98 ( [ eq : hq ] )",
    "becomes @xmath114 .",
    "the integral in this formula is also known as `` correlation integral '' . for @xmath115 data points @xmath116 it can be estimated using the correlation sum , which is the averaged relative number of pairs in an @xmath15-neighborhood  ( * ? ? ?",
    "* chapter  6),@xcite      @xmath118 denotes the heaviside function @xmath119 then the correlation entropy is @xmath120 for sufficiently small @xmath15 it behaves as @xmath121 with @xmath122 being the correlation dimension of the system @xcite .",
    "a scale dependent correlation dimension can be defined as difference quotient      for a temporal sequence of states ( or state vectors ) @xmath124 we can now define block entropies @xmath125 by using @xmath126-dimensional delay vectors @xmath127 .",
    "now , we can define also the quantities corresponding to conditional entropies and to the excess entropy using the correlation entropy @xmath128    we expect the same qualitative behavior regarding the @xmath15-dependence of these quantities as for those based on shannon entropies , see eqs  ( [ eqn : hn ] , [ eqn : em ] ) .",
    "quantitatively there might be differences , in particular for large @xmath15 and strongly non - uniform measures .",
    "a comparison of the two methods with analytical results are given in the appendix [ sec : ar2 ] , where we find a good agreement .",
    "although , the ksg method seems to underestimate the mutual information for larger embeddings ( higher dimensional state space ) .",
    "the correlation integral method uses a unit ball of diameter @xmath129 whereas the ksg method measures the size of the hypercube enclosing @xmath102 neighbors where the data was subject to additive noise in the interval @xmath108 $ ] .",
    "thus comparable results are obtained with @xmath130 .",
    "we are now describing the algorithm used to compute the proposed decomposition of the excess entropy in sec .",
    "[ sec : decomp ] .",
    "the algorithm is composed of several steps : preprocessing , determining the middle term ( mt ) ( [ eq : e_middle ] ) , determining the constant in mt , and the calculation of the decomposition and of quality measures .      ideally the @xmath131 curves are composed of straight lines in a log - linear representation , i.e.of the form @xmath132 .",
    "we will refer to @xmath133 as the slope ( it is actually the inverted slope ) .",
    "thus we perform fitting , that attempts to find segments following this form , details are provided in the appendix  [ sec : app : decomp ] .",
    "then the data is substituted by the fits in the intervals where the fits are valid . as for very small scales the @xmath131",
    "become very noisy we extrapolate below the fit with the smallest scale .",
    "in addition we calculate the derivative @xmath134 in each point , either from the fits ( @xmath133 , where available ) or from finite differences of the data ( using 5 points averaging )",
    ".      in theory only two @xmath89 should have a non - zero slope at each scale @xmath15 , see eq  .",
    "however , in practice we have often more terms , such that we need to find for each @xmath15 the maximal range @xmath135 , where @xmath136 :   \\hat s(i,{\\ensuremath{\\varepsilon}})>s_{\\textrm{min}}$ ] , i.e.the slope is larger than the threshold @xmath137 .",
    "however , this is only valid for deterministic scaling ranges . in stochastic ranges",
    "all @xmath131 should have zero slope .",
    "we introduce a measure of stochasticity , defined as @xmath138 which is 0 for purely deterministic ranges and 1 for stochastic ones . the separation between state and memory complexity",
    "is then inherited from the next larger deterministic range . thus if @xmath139 we use @xmath135 at @xmath140 , where @xmath141 .",
    "note that the here algorithmically defined @xmath142 is not necessarily equal to the @xmath143 defined above ( [ eq : eps_ast ] ) for an ideal - typical noisy deterministic system .      in order to obtain the scale - invariant constant @xmath81 of the mt ,",
    "see eq  , we would have to define a certain length scale @xmath79 .",
    "since this can not be done robustly in practice ( in particular because it may not be the same @xmath79 for each @xmath50 ) we resort to a different approach .",
    "the constant parts of the @xmath144 terms in the mc can be determined from plateaus on larger scales .",
    "thus , we define @xmath145}\\delta h_m(e)$ ] , where @xmath140 is smallest scale @xmath146 where we have a near - zero slope , i.e .. @xmath147 . in case",
    "there is no such @xmath140 then @xmath148 .      the decomposition of the excess entropy follows eqs  ( [ eqn : eestate ] , [ eqn : eemem ] ) with @xmath149 and @xmath150 used for splitting the terms : @xmath151 in addition we can compute different quality measures to indicate the reliability of the results , see table  [ tab : quality ] .",
    ".quality measures for decomposition algorithm .",
    "we use the iverson bracket for boolean expression : @xmath152 and @xmath153 .",
    "they are all normalized to @xmath154 $ ] where typically 0 is the best score and 1 is the worst . [ cols=\">,<,<\",options=\"header \" , ]     first , all fits with a certain number of data points ( here 10 ) are computed and their quality measure is computed : @xmath155 , where @xmath156 is the length of the segment in number of points .",
    "there will be segments with low residual errors and segments with very high residuals , namely those that cover the regions between ideal - typical behavior .",
    "the heuristics for the quality threshold is the @xmath157 percentile of @xmath158 plus a small portion of the standard deviation : @xmath159 .",
    "the reasoning for the percentile is to pick a threshold among the good fits , and we only assume that at least @xmath157 of the curve has fitting segments .",
    "if there is however an extended range of good fits then a high percentage ( say @xmath160 ) of short segments have very similar low @xmath161 values . in this case a too low threshold would be selected which cuts away good fitting regions .",
    "the tens of the standard deviation is added to lift the threshold above the potential plateau , see fig  [ fig : fitting](b ) .",
    "for all `` good '' segments @xmath162 the longest extension of the fitting range is determined that keeps the quality of the fit below the threshold , i.e.@xmath163 .",
    "thus we get many overlapping regions of good fits . for each pair of two regions that overlap more than @xmath164 ( w.r.t .",
    "the smaller ) we discard the shorter one .",
    "the remaining regions are our final fits , as displayed in fig  [ fig : fitting](a ) .",
    "the components @xmath165 for calculating the constant of the middle term ( [ eqn : e_decomp : algo ] ) are also displayed in fig  [ fig : fitting](a ) . for @xmath166 ,",
    "the value of @xmath165 is @xmath167 for @xmath168 because of the shallow region at @xmath169 $ ] .",
    "this leads to the value of @xmath170 ( @xmath171 ) for the state complexity in fig  [ fig : lorenz : decomp][l : n1:de ] .",
    "for @xmath172 the constant @xmath173 takes the value of @xmath174 of the next ( smaller scale ) plateau .",
    "similarly @xmath175 raise from zero to the value of the plateaus , which leads to the increase in state - complexity in the stochastic scaling range .",
    "remember that in the stochastic scaling range ( @xmath172 ) @xmath176 and @xmath177 are taken from the preceding deterministic scaling range ."
  ],
  "abstract_text": [
    "<S> quantifying behaviors of robots which were generated autonomously from task - independent objective functions is an important prerequisite for objective comparisons of algorithms and movements of animals . </S>",
    "<S> the temporal sequence of such a behavior can be considered as a time series and hence complexity measures developed for time series are natural candidates for its quantification . </S>",
    "<S> the predictive information and the excess entropy are such complexity measures . </S>",
    "<S> they measure the amount of information the past contains about the future and thus quantify the nonrandom structure in the temporal sequence . </S>",
    "<S> however , when using these measures for systems with continuous states one has to deal with the fact that their values will depend on the resolution with which the systems states are observed . for deterministic systems </S>",
    "<S> both measures will diverge with increasing resolution . </S>",
    "<S> we therefore propose a new decomposition of the excess entropy in resolution dependent and resolution independent parts and discuss how they depend on the dimensionality of the dynamics , correlations and the noise level . for the practical estimation we propose to use estimates based on the correlation integral instead of the direct estimation of the mutual information using the algorithm by kraskov et al .  </S>",
    "<S> ( 2004 ) which is based on next neighbor statistics because the latter allows less control of the scale dependencies . </S>",
    "<S> using our algorithm we are able to show how autonomous learning generates behavior of increasing complexity with increasing learning duration . </S>"
  ]
}