{
  "article_text": [
    "the definition of a system of units on the basis of conventional values of fundamental constant of physics @xcite is motivating efforts on determinations of the planck constant @xcite .",
    "the most accurate data come from the comparison of mechanical and electrical powers by watt - balance experiments @xcite and the count of the atoms in @xmath0 enriched silicon balls @xcite .",
    "four @xmath1 determinations comply with the accuracy required to make the kilogram redefinition feasible @xcite .",
    "a statistical analysis of these results is necessary to check their consistency and to chose a reference value of the planck constant .",
    "data analysis is usually carried out by selecting a model and by processing the measurement results as if they had generated by it .",
    "this approach ignores the model uncertainty and can lead to underestimates of the uncertainty , to overconfident inferences , and to decisions that are more risky than one thinks they are .",
    "questions are : how accurately does a model explain the data and what is the impact of the model uncertainty on the measurand estimate and the inferences that we draw from the measurement results ? given an uncertain data model and a measurand estimate based on it , how can the total uncertainty of the measurand value be assessed ?",
    "probability calculus can select the model most supported by the data and include the uncertainty into the analysis and uncertainty budget @xcite ; an example investigating the choice of the degree when fitting a polynomial to noisy data is given in @xcite .",
    "the choice of a measurand value from inconsistent data - sets is investigated in @xcite .",
    "this paper builds on these works and delivers some additional results .",
    "firstly , it considers models where the standard deviations of a data subset  the empty set and the whole data set included  might be larger than the associated uncertainties ; but , we do not know what this subset is .",
    "secondly , it chooses the uninformative prior distribution of the unknown standard - deviations by requiring that gaussian sampling - distributions of the measurement results are equiprobable .",
    "a novelty is that , if these standard - deviations are not of interest , marginalization allows an analytical expression of the measurement - result distributions to be given , no matter what the standards deviations  greater than or equal to the associated uncertainties  may be .",
    "eventually , since one of the subset does apply , this paper tests the data consistency by comparing the probability of each subset is the right one given the data and suggests a reference value of the planck constant by averaging over all the subsets . in this way , all the data determine the reference value , no measurement result is excluded , and none is considered fully reliable or suspicious .",
    "the starting point of the analysis is the list in table [ t1 ] . in 2014 , the bureau international des poids et mesures ( bipm ) carried out a campaign of mass calibration with respect to the international prototype , in anticipation of the redefinition of the kilogram @xcite .",
    "this brought to light an offset of the bipm as - maintained mass unit , which was traceable to the prototype in 1992 .",
    "therefore , the mass values used in the watt - balance and atom counting experiments , were suitably corrected .",
    "the iac s @xmath2 values are converted into planck constant values via the molar planck constant @xmath3 js mol@xmath4 , which has a negligible uncertainty @xcite .",
    "the correlation of the @xmath2 values reported in 2011 and 2015 by the iac is investigated in @xcite , which gives also the mean of the correlated values . to avoid complications due to the correlation , the input datum for this analysis is the mean of the 2011 and 2015 iac s values .",
    "the values selected for this analysis are labelled from 1 to 3 in table [ t1 ] ; they are shown in fig .",
    "[ h : data ] .",
    "the bipm estimated the calibration uncertainty as 3 @xmath5 g ; this uncertainty affects all the mass values in the watt - balance and atom counting experiments .",
    "the table [ t1 ] gives the fractions of this systematic component of the uncertainty budget ; the correlation of any pair of @xmath1 values can be obtained by multiplying the pair s systematic fractions .    llllll lab & year & reference & label & @xmath6 / js & @xmath7 + iac@xmath8 & 2011 & @xcite & - & 6.62606991(20 ) + iac@xmath8 & 2015 & @xcite & - & 6.62607016(13 ) + iac@xmath9 & 2015 & @xcite & 1 & 6.62607009(12 ) & 0.16 + nist & 2015 & @xcite & 2 & 6.62606936(37 ) & 0.05 + nrc & 2014 & @xcite & 3 & 6.62607011(12 ) & 0.17 + & & this paper & - & 6.626070073(94 ) +   +   +   +   +   +   +     j s is the value recommended by the committee on data for science and technology.,width=283 ]    in 2012 , the consultative committee for mass and related quantities of the international committee for weights and measures recommended that \" _ ... the values provided by the different experiments be consistent at the 95% level of confidence \" _ @xcite .",
    "since the confidence level is a concept associated to the neyman s confidence interval @xcite , the meaning of this recommendation is not very clear .",
    "a way to examine the data consistency might be the significance test of fisher @xcite .",
    "assuming that the data are independent normal - variables having the same mean @xmath1 and standard deviations @xmath10 equal to the associated uncertainties , @xmath11  which is the consistent - data or null hypothesis , a test statistic is the pearson @xmath12 variable @xcite . by choosing a 5% significance level , the expected 95% quantile is @xmath13 .",
    "for this data set , the observed value is @xmath14 ; since this value is less than the @xmath15 rejection level , the consistent - data hypothesis is accepted .",
    "the test ensures that the probability of rejecting the consistent - data model when it is true is 5% , but to accept the consistent - data hypothesis as correct is an _ argumentum ad ignorantiam _ fallacy .    in order to assess the data consistency",
    ", we must calculate the probability of the null hypothesis ; this requires to include it into a wider hypothesis space . to this end",
    ", we consider underestimations of the data uncertainties .",
    "accordingly , each datum @xmath16 is thought to be a random variable having mean @xmath1 and variance @xmath17 , where , when reporting the measurement uncertainty , a datum - specific contribution to the variance , @xmath18 , was omitted .",
    "it is also possible that some measurement uncertainty was correctly evaluated  that is , for these measurements , @xmath19 .",
    "of course , all the measurement uncertainty might be correctly evaluated .",
    "our assumption is that there exists a subset of good data  which might be empty set or the full data list  having correct uncertainty assessments ; its elements @xmath16 are independent realizations of random variables having variances @xmath20 . for the remaining data , the uncertainties @xmath11 are only lower bounds to the standard deviations , that are additional model parameters .",
    "the good data can not be predetermined ; instead , all the subsets will be taken in turn as the sought good - data subset .",
    "the final @xmath1 estimate will be obtained by model averaging using the probability of the each subset being the good one .",
    "before going into the specific application to the data in the table [ t1 ] , this section outlines the theoretical framework of the analysis .      in order to explain the measurement results , we consider a number of parametric statistical models ",
    "say , @xmath21  where each model is parameterized by the measurand @xmath1 and , possibly , a set of nuisance parameters @xmath22 .",
    "we assume that the set s models are mutually exclusive and complete , that is , @xmath23 . given the measurement results @xmath24^{\\rm t}$ ] and the data likelihood @xmath25 , one proceeds by assigning a prior probability distribution @xmath26 to the model parameters and a prior probability @xmath27 to each model .",
    "next , by using the product rule of the probabilities , the joint distribution of the data , parameters , and models is @xmath28 according this hierarchical model , firstly , @xmath29 is sampled from @xmath27 ; then , the model parameters @xmath1 and @xmath22 are sampled from @xmath26 ; eventually , the data @xmath30 are sampled from @xmath25 .    through conditioning and marginalization",
    ", @xmath31 can be used to obtain the post - data distributions of interest . by conditioning ( [ product ] ) on @xmath30 and @xmath29 , one gets the post - data probability distribution of the parameters given the model and data , @xmath32 where the normalizing factor @xmath33 is the data evidence and the integration is carried out over the parameter space .",
    "the marginalization of ( [ product ] ) over the model parameter and the conditioning on the data yields the model probability given the data , @xmath34 within this framework , @xmath35 is the updated probability that @xmath29 is the model sampled in the first step of ( [ product ] ) .",
    "the pre - data distributions @xmath26 and @xmath27 synthesize the uncertainty before the measurements are carried out ; subsequently , the updated distributions @xmath36 and @xmath35 synthesize the uncertainty after the data @xmath30 have been observed .      the simplest way to select",
    "a model is to choose the most probable .",
    "when no single model stands out , the expression of the uncertainty may require to report a set of models along with their probabilities .",
    "model averaging is an alternative that incorporates model uncertainty .",
    "after marginalization of ( [ product ] ) over the models and nuisance parameters @xmath22 and conditioning on the data , the distribution of the @xmath1 values is @xmath37 where @xmath38 by averaging over the models , ( [ measurand ] ) incorporates the model uncertainty embedded in @xmath27 .",
    "a point estimate of @xmath1 is the mean of ( [ measurand ] ) .",
    "hence , @xmath39 where @xmath40 is the mean of ( [ marginal ] ) .",
    "to explain the data , our hypothesis is as follows : the measured values @xmath16 of the planck constant are independently sampled from distributions having the same mean @xmath1 and different variances . by maximizing the shannon entropy , this information is synthesised by gaussian sampling distributions , that is , @xmath41 .",
    "both @xmath1 and @xmath10 are unknown , but the uncertainties @xmath11 associated to the data are lower bounds for @xmath10 , that is , @xmath42 . in this way , we allow for unknown errors that are datum - specific and have not been included in the uncertainty budgets of the measured values",
    ". we do not assume the existence of these errors : for some datum  may be none , may be all ",
    "the @xmath43 identity might hold .",
    "therefore , our hypothesis space contains as many models as the number of subsets of the measured values  the empty set and the input data included , where each subset identifies the measurements whose associated uncertainty is the standard deviation , that is , @xmath43 .",
    "the measurements are assumed uncorrelated , which is not exactly true .",
    "though it is possible to include correlations @xcite , this is beyond the scope of this analysis .",
    "let us consider any measured value .",
    "if the associated uncertainty @xmath44 is the standard deviation , its sampling distribution is @xmath45 .",
    "contrary , if the associated uncertainty is a lower bound for the standard deviation , the sampling distribution is @xmath46 , where @xmath47 ; in both cases , we omitted the @xmath48 subscript . in the latter case , the unknown variance @xmath49 is a nuisance parameter that will be eliminated by the marginalization ( [ marginal ] ) .",
    "the data likelihood is @xmath50 where @xmath51 is a subset of good data @xmath52 , @xmath53 is the complement of @xmath51 in the hypothesis space , that is , @xmath54 , and @xmath55^{\\rm t}$ ] .      from a theoretical viewpoint , when assigning probabilities to the values of @xmath1 , it is impossible to get rid of the prior distribution @xmath56 .",
    "this distribution must synthesize the pre - data knowledge about @xmath1 and @xmath22 .",
    "ignorance means that the sampling distributions @xmath57 must be equiprobable . since they form a riemannian manifold  whose natural metric is the fisher information @xmath58 @xcite , the distribution of the @xmath59 coordinates is proportional to the volume element @xmath60 .",
    "after normalisation , one gets the jeffreys prior @xmath61 where @xmath62 is the volume of the @xmath1 subspace and @xmath63 .",
    "the relevance of ( [ prior ] ) resides in making ( [ post ] ) consistent _ vs. _ the transformations of the @xmath1 and @xmath22 variables .",
    "in fact , if we transform @xmath1 and @xmath22 , the left - hand side of ( [ post ] ) transforms according to the usual change - of - variable rule .",
    "what happens to the right - hand side is that the transformation jacobian combines with @xmath58 to give the fisher information about the new variables .",
    "for example , if we consider a single datum and reparameterise the sampling distribution by @xmath64  which corresponds to a coordinate change in the @xmath46 manifold ",
    "the volume element changes from @xmath65 to @xmath66 .",
    "as regards the pre - data distribution , by applying the distribution transformation rule , it changes from @xmath67 to @xmath68 , which are both consistent with @xmath69 .",
    "the integral ( [ marginal ] ) reduces to the calculation of the sampling distribution of the measurement result @xmath70 , given the planck constant and the variance lower bound @xmath71 , @xmath72 u } { \\sqrt{2\\pi } ( h - x)^2 } .\\ ] ] it is worth noting that , after the marginalization eliminates the unknown standard deviation from @xmath46 , this same result is obtained by assuming the generalised birge - ratio model @xmath73 , where the scale parameters @xmath74 are datum - specific and take the uncertainty underestimations into account .",
    "as expected being known only its lower bound , the variance of ( [ sampling:2 ] ) is infinite .",
    "eventually , by taking ( [ sampling:2 ] ) into account , the data likelihood ( [ lik ] ) can be rewritten as @xmath75 which is parameterised only by @xmath1 , to which the pre - data distribution @xmath76 corresponds .",
    "the three planck constant values are grouped into eight @xmath77 subset - pairs , where the @xmath51 subsets collect the @xmath43 good data and the @xmath53s the @xmath42 ones .",
    "the subset pairs are sorted according increasing cardinality of @xmath51 , with @xmath78 first and later elements in table [ t1 ] omitted first .",
    "the @xmath51 s probabilities are given by ( [ model ] ) , where we assumed @xmath79 const .",
    ", the data evidence is @xmath80 and @xmath81 is given by ( [ lik:2 ] ) .",
    "the integration in ( [ int ] ) must be carried out numerically ; the results are given in fig .",
    "[ figure:2 ] .",
    "all the subsets are roughly equally probable ; none stands clearly out .",
    "the probability that at least one of the uncertainty values was underestimated is 85% ; conversely , the probability of a purely statistical origin of the data scatter is 15% .",
    "the probabilities of the good - data subsets including the nist value  the second in the data list  are local minima .",
    "additionally , the most probable good - data set excludes it ; this might suggest that the uncertainty associated to the nist value is underestimated .",
    "the mean values and standard deviations of the post - data probability distributions of the possible values of @xmath1 for any subset of good data are shown in fig .",
    "[ figure:3 ] .",
    "none value differs significantly from the others .",
    "it is also worth noting that the scale factor between the minimum standard deviation  corresponding to the set including where all the data  and maximum one  corresponding to the empty set  is 1.5 .",
    "in order to check the data consistency , we can also compare the assumption that the data are sampled or not sampled from distributions having the same mean . in this case , the hypotheses are as follows : the measured values are independently sampled from distributions having the same mean ( h0 , null hypothesis ) or different means ( h1 , alternative hypothesis ) . in both cases ,",
    "the standard deviations are equal to the associated uncertainties .    as before",
    ", this information is synthesised by gaussian sampling distributions ; the data likelihoods are    @xmath82    which is parameterized by the one - dimensional coordinate @xmath1 , and @xmath83    which is parameterized by the three - dimensional coordinate @xmath84^{\\rm t}$ ] .",
    "given the values labelled 1  3 in table [ t1 ] and the indicated subsets of good data .",
    "@xmath85 j s is the value recommended by the committee on data for science and technology.,width=283 ]    with uniform probability densities in the h0 and h1 hypothesis - spaces , the pre - data distributions of the space coordinates are @xmath76 ( h0 hypothesis ) and @xmath86 ( h1 hypothesis ) , where the volumes @xmath62 and @xmath87 of the @xmath1 and @xmath88 spaces are large enough to allow the limits of the evidence integrals to be extended up to the infinity .    eventually , the data evidences are given by    @xmath89    and @xmath90    where the dimensionless variables @xmath91 , @xmath92 , @xmath93 , and @xmath94 were used .",
    "the inverse proportionality of ( [ z01]-b ) to the volumes of the hypothesis - spaces is known as ockham s razor and penalizes the model having the greater adaptability to the data .",
    "consequently , in the case of a large pre - data range of the @xmath1 values , that is , when @xmath95 , the data support always the consistent - data hypothesis .",
    "since  in order to extend the limits of the ( [ z01]-b ) integrals to the infinity  we assumed that @xmath96 , @xmath97 must be at least 10 . in this case ,",
    "@xmath98 , @xmath99 , and the probability of consistent data is about 16% ; not far from the value found in the previous analysis .     whatever the subset of good data may be .",
    "@xmath85 j s is the value recommended by the committee on data for science and technology.,width=283 ]",
    "to implement a kilogram definition based on the planck constant they are necessary at least three measured values having uncertainties less than @xmath100 , with at least a value having an uncertainty less than @xmath101 .",
    "these values must be consistent at the 95% level of confidence .",
    "in addition , the task group on fundamental constants of the committee on data for science and technology must provide a value that minimises discontinuities .",
    "this paper investigated the data consistency by explaining the measurement results by random effect  which allow , but not assume , missing contributions to the error budgets  and fixed effect  which allow , but not assume , different means of the sampling distributions  models . in both cases ,",
    "the data look inconsistent .    in the first case , after averaging over all the subsets of good data , the probability density of the possible values of @xmath1 is shown in fig .",
    "[ figure:4 ] .",
    "the mean value is @xmath102 the standard deviation , @xmath103 , can be compared with the weighed - mean uncertainty , @xmath104 .",
    "the quadratic difference , @xmath105 , is the contribution to the error budget of the uncertainty about the actual subset of good data .",
    "this work was jointly funded by the european metrology research programme ( emrp ) participating countries within the european association of national metrology institutes ( euramet ) , the european union , and the italian ministry of education , university , and research ( awarded project p6 - 2013 , implementation of the new si ) .",
    ".  milton , r.  davis , n.  fletcher , towards a new si : a review of progress made since 2011 , metrologia 51 ( 2014 ) r21 - 30 e.  massa , g.  mana the avogadro and the planck constants for redefinition of the kilogram , rivista del nuovo cimento 35 ( 2012 ) 353 - 88 m.  stock , watt balance experiments for the determination of the planck constant and the redefinition of the kilogram , 50 ( 2013 ) r1 - 16 r.  steiner , history and progress on accurate measurements of the planck constant , rep .  prog .  phys .  76 ( 2013 )",
    "016101 h.  bettin , k.  fujii , j.  man , g.  mana , e.  massa , a.  picard , accurate measurements of the avogadro and planck constants by counting silicon atoms , ann .",
    "phys .  525 ( 2013 ) 680 - 7 b.  andreas _",
    "_ counting the atoms in a @xmath0 crystal for a new kilogram definition , metrologia 48 ( 2011 ) s1 - 14 a.  azuma _ et al . _ improved measurement results for the avogadro constant using a 28si - enriched crystal , metrologia 52 ( 2015 ) 360 - 75 s.  schlamminger , r.l .",
    "steiner , d.  haddad , d.b .",
    "newell , f.  seifert , l.s .",
    "chao , r.  liu , e.r .",
    "williams , j.r .",
    "pratt , a summary of the planck constant measurements using a watt balance with a superconducting solenoid at nist , metrologia 52 ( 2015 ) l5-l8 c.a .",
    "sanchez , b.m .",
    "wood , r.g .",
    "green , j.o .",
    "liard , d.  inglis , a determination of planck s constant using the nrc watt balance , metrologia 51 ( 2014 ) s5-s14 c.a .",
    "sanchez , b.m .",
    "wood , communication to the codata task group on fundamental constants ( 2015 ) w.  von der linden , v.  dose , u.  von toussaint , bayesian probability theory , cambridge university press , cambridge , 2014 g.  mana , p.a .",
    "giuliano albo , s.  lago , bayesian estimate of the degree of a polynomial given a noisy data sample , measurement 55 ( 2014 ) 564 - 70 v.  dose , bayesian estimate of the newtonian constant of gravitation , meas .",
    "technol .",
    "18 ( 2007 ) 176 - 82 c.  elster , b.  toman , analysis of key comparisons : estimating laboratories biases by a fixed effects model using bayesian model averaging , metrologia 47 ( 2010 ) 113 - 9 b.  toman , j.  fischer , c.  elster , alternative analyses of measurements of the planck constant , metrologia 49 ( 2012 ) 567 - 71 g.  mana , e.  massa , m.  predescu , model selection in the average of inconsistent data : an analysis of the measured planck - constant values , metrologia 49 ( 2012 ) 492 - 500 m.  stock , p.  barat , r.s .",
    "davis , a.  picard , m.j.t .",
    "milton , calibration campaign against the international prototype of the kilogram in anticipation of the redefinition of the kilogram part 1 : comparison of the international prototype with its official copies , metrologia 52 ( 2015 ) 310 - 6 p.j .",
    "mohr , b.n .",
    "taylor , d.b .",
    "newell , codata recommended values of the fundamental physical constants : 2010 , rev .",
    "phys .  84 ( 2012 ) 1527 - 605 m.  borys , i.  busch , k.  fujii , n.  kuramoto , g.  mana , e.  massa , s.  mizushima , t.  narukawa , a.  nicolaus , a.  pramann , c.p .",
    "sasso , m.  stock , the correlation of the @xmath2 measurements by counting @xmath106si atoms , j.  phys .",
    "chem .  ref .",
    "data 44 ( 2015 ) submitted consultative committee for mass and related quantities , report of the 12th meeting ( 26 march 2010 ) to the international committee for weights and measures , bureau international des poids et mesures , svres , 2012 j.  neyman , outline of a theory of statistical estimation based on the classical theory of probability , phil .",
    "r.  soc .  a 236 ( 1937 ) 333 - 80 g.  mana , c.  palmisano",
    ", interval estimations in metrology , metrologia 51 ( 2014 ) 191 - 6 k.  pearson , on the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling , philos .  mag .",
    "series 5 50 ( 1900 ) 157 - 75 o.  bodnar , a.  link , c.  elster , objective bayesian inference for a generalized marginal random effects model , bayesian anal .  advance publication ( 2015 ) doi 10.1214/14-ba933 v.  rodriguez , are we really cruising a hypothesis space , in : maximum entropy and bayesian methods , w.  von der linden , v.  dose , r.  fischer , r.  preuss , eds .",
    "kluwer academic publishers , dordrecht , 1999 s.  amari , h.  nagaoka , methods of information geometry , oxford university press , oxford , 2007"
  ],
  "abstract_text": [
    "<S> statistical parametric models are proposed to explain the values of the planck constant obtained by comparing electrical and mechanical powers and by counting atoms in @xmath0 enriched crystals . </S>",
    "<S> they assume that uncertainty contributions  having heterogeneous , datum - specific , variances  might not be included in the error budgets of some of the measured values . </S>",
    "<S> model selection and model averaging are used to investigate data consistency , to identify a reference value of the planck constant , and to include the model uncertainty in the error budget .    </S>",
    "<S> bayesian inference , hypothesis testing , determination of fundamental constants , probability theory 62f15 , 62f03 06.20.jr , 06.20.dk </S>"
  ]
}