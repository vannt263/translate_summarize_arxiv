{
  "article_text": [
    "the data mining and machine learning communities have recently shown great interest in deep learning , specifically in stacked restricted boltzmann machines ( rbms ) ( see r. salakhutdinov and hinton 2009 ; r. salakhutdinov and hinton 2012 ; srivastava , salakhutdinov , and hinton 2013 ; le roux and bengio 2008 for examples ) .",
    "a rbm is a probabilistic undirected graphical model ( for discrete or continuous random variables ) with two layers , one hidden and one visible , with no conditional dependency within a layer ( smolensky 1986 ) .",
    "these models have reportedly been used with success in classification of images ( larochelle and bengio 2008 ; srivastava and salakhutdinov 2012 ) .",
    "however , the model properties are largely unexplored in the literature and the commonly cited fitting methodology remains heuristic - based and abstruse ( hinton , osindero , and teh 2006 ) . in this paper",
    ", we provide steps toward a thorough understanding of the model class and its properties from the perspective of statistical theory , and then explore the possibility of a rigorous fitting methodology .",
    "we find the rbm model class to be deficient in two fundamental ways .",
    "first , the models are often unsatisfactory as conceptualizations of how data are generated . recalling fisher ( 1922 ) ,",
    "the aim of a statistical model is to represent data in a compact way .",
    "neyman and box further state that a model should `` provide an explanation of the mechanism underlying the observed phenomena '' ( lehmann 1990 ; g. e. p. box 1967 ) . at issue , rbms often produce data lacking realistic variability and may thereby fail to provide a satisfactory conceptualization of a data generation process .",
    "in addition to such degeneracy , we find that rbms can easily exhibit types of instability . in practice , this may seen when a single pixel change in an image results in a wildly different classification in an image classification problem .",
    "such model impropriety issues have recently been documented in rbms ( li 2014 ) , as well as other deep architectures ( szegedy et al .",
    "2013 ; nguyen , yosinski , and clune 2014 ) .",
    "we investigate these phenomena for rbms in section [ explorations ] through simulations of small , manageable examples .",
    "secondly , the fitting of these models is problematic . as the size of these models grows ,",
    "both maximum likelihood and bayesian methods of fitting quickly become intractable .",
    "the literature often suggests markov chain monte carlo ( mcmc ) tools for approximate maximization of likelihoods to fit these models ( e.g. , gibbs sampling to exploit conditional structure in hidden and visible variables ) , but little is said about achieving convergence ( hinton 2010 ; hinton , osindero , and teh 2006 ) .",
    "related to this , these mcmc algorithms require updating potentially many latent variables ( hiddens ) which can critically influence convergence in mcmc - based likelihood methods .    in section [ bayesian - model - fitting ] ,",
    "we compare three bayesian techniques involving mcmc approximations that are computationally more accessible than direct maximum likelihood , which also aim to avoid parts of a rbm parameter space that yield unattractive models . as might be expected , with greater computational complexity",
    "comes an increase in fitting accuracy , but at the cost of practical feasibility .    for a rbm model with enough hidden variables",
    ", it has been shown that any distribution for the visibles can be approximated arbitrarily well ( le roux and bengio 2008 ; montufar and ay 2011 ; and montfar , rauh , and ay 2011 ) . however , the empirical distribution of a training set of vectors of visibles is the best fitting model for observed cell data . as a consequence , we find that any fully principled fitting method based on the likelihood for a rbm with enough hidden variables will simply seek to reproduce the ( discrete ) empirical distribution of a training set .",
    "thus , there can be no `` smoothed distribution '' achieved in fitting a rbm model of sufficient size with a rigorous likelihood - based method .",
    "we are therefore led to be skeptical that any model built using these structures ( like a deep boltzmann machine ) can achieve useful prediction or inference in a principled way without intentionally limiting the flexibility of the fitted model .",
    "this paper is structured as follows .",
    "section [ background ] formally defines the rbm including the joint distribution of hidden and visible variables and explains the model s connection to deep learning .",
    "additionally , measures of model impropriety and methods of quantifying / detecting it are defined .",
    "section [ explorations ] details our explorations into the model behavior and propriety ( or lack thereof ) of the rbm class .",
    "we discuss three bayesian fitting techniques intended to avoid model impropriety in section [ model - fitting ] and conclude with a discussion in section [ discussion ] . on - line",
    "supplementary materials provide proofs for results on rbm parameterizations and data codings described in section [ data - coding - to - mitigate - apparent - degeneracy ] .",
    "a restricted boltzmann machine ( rbm ) is an undirected graphical model specified for discrete or continuous random variables , binary variables being most commonly considered . in this paper",
    ", we consider the binary case for concreteness .",
    "a rbm architecture has two layers , hidden ( @xmath0 ) and visible ( @xmath1 ) , with no dependency connections within a layer .",
    "an example of this structure is in figure [ fig : rbm ] with the hidden nodes indicated by gray circles and the visible nodes indicated by white circles .",
    "a common use for rbms is to create features for use in classification .",
    "for example , binary images can be classified through a process that treats the pixel values as the visible variables @xmath2 in a rbm model ( hinton , osindero , and teh 2006 ) .",
    "let @xmath3 represent the states of the visible and hidden nodes in a rbm for some integers @xmath4 .",
    "each single binary random variable , visible or hidden , will take its values in a common coding set @xmath5 , where we allow one of two possibilities for the coding set , @xmath6 or @xmath7 , with `` @xmath8 '' always indicating the `` high '' value of the variable . while @xmath6 may be a natural starting point",
    ", we argue later that the coding @xmath7 induces more interpretable model properties for the rbm ( cf .",
    "section [ explorations ] ) .",
    "a standard parametric form for probabilities corresponding to a potential vector of states , @xmath9 , for the nodes is    @xmath10    where @xmath11 denotes the vector of model parameters and the denominator @xmath12 is the normalizing function that ensures the probabilities ( [ eqn : pmf ] ) sum to one . for",
    "@xmath13 and    @xmath14    let @xmath15 be the set of possible values for the vector of variables needed to compute probabilities ( [ eqn : pmf ] ) in the model , and write @xmath16 for the `` neg - potential '' function .",
    "the rbm model is parameterized by @xmath17 containing two types of parameters , main effects and interaction effects .",
    "the main effects parameters ( @xmath18 ) weight the values of the visible @xmath2 and hidden @xmath19 nodes in probabilities ( [ eqn : pmf ] ) and the interaction effect parameters ( @xmath20 ) weight the values of the connections @xmath21 , or dependencies , between hidden and visible layers .    due to the potential size of the model , the normalizing constant @xmath22 can be practically impossible to calculate , making simple estimation of the model parameter vector problematic .",
    "a kind of gibbs sampling can be tried ( due to the conditional architecture of the rbm , i.e.  visibles given hiddens or vice verse ) .",
    "specifically , the conditional independence of nodes in each layer ( given those nodes in the other layer ) allows for stepwise simulation of both hidden layers and model parameters ( e.g. , see the contrastive divergence of hinton ( 2002 ) or bayes methods in section [ model - fitting ] ) .",
    "rbms have risen to prominence in recent years due to their connection to deep learning ( see hinton , osindero , and teh 2006 ; r. salakhutdinov and hinton 2012 ; srivastava , salakhutdinov , and hinton 2013 for examples ) . by stacking multiple layers of rbms in a deep architecture ,",
    "proponents of the models claim to produce the ability to learn `` internal representations that become increasingly complex , which is considered to be a promising way of solving object and speech recognition problems '' ( r. salakhutdinov and hinton 2009 , 450 ) .",
    "the stacking is achieved by treating a hidden layer of one rbm as the visible layer in a second rbm , and so on , until the desired multi - layer architecture is created .",
    "the highly flexible nature of a rbm ( having as it does @xmath23 parameters ) creates at least three kinds of potential model impropriety that we will call _ degeneracy _ , _ instability _ , and _ uninterpretability_. in this section we define these characteristics , consider how to detect them in a rbm , and point out relationships among them .      in random graph model theory ,",
    "_ model degeneracy _",
    "means there is a disproportionate amount of probability placed on only a few elements of the sample space , @xmath24 , by the model ( handcock 2003 ) . for random graph models , @xmath24 denotes all possible graphs that can be constructed from a set of nodes and an exponentially parameterized random graph model has a distribution of the form    @xmath25    where @xmath26 is the model parameter , and @xmath27 is a vector of statistics based on the adjacency matrix of a graph . here , as earlier",
    ", @xmath28 is the normalizing function .",
    "let @xmath29 denote the convex hull of the potential outcomes of sufficient statistics , @xmath30 , under the model above .",
    "handcock ( 2003 ) classify an exponentially parametrized random graph model at @xmath17 as _ near - degenerate _ if the mean value of the vector of sufficient statistics under @xmath17 , @xmath31 , is close to the boundary of @xmath29 .",
    "this makes sense because if a model is near - degenerate in the sense that only a small number of elements of the sample space @xmath24 have positive probability , the expected value @xmath32 is an average of that same small number of values of @xmath33 and can be expected to _ not _ be pulled deep into the interior of @xmath29 .",
    "a rbm model can be thought to exhibit an analogous form of _ near - degeneracy _ when there is a disproportionate amount of probability placed on a small number of elements in the sample space of visibles and hiddens , @xmath34 .",
    "using the idea of handcock ( 2003 ) , when the random vector @xmath35 from ( [ eqn : t ] ) , appearing in the neg - potential function @xmath36 , has a mean vector @xmath37 close to the boundary of the convex hull of @xmath38 , then the rbm model can be said to exhibit near - degeneracy at @xmath39 . here",
    "the mean of @xmath40 is    @xmath41      considering exponential families of distributions , schweinberger ( 2011 ) introduced a concept of model deficiency related to _",
    "instability_. instability can be roughly thought of as excessive sensitivity in the model , where small changes in the components of potential data outcomes , @xmath42 , may lead to substantial changes in the probability function @xmath43 .",
    "to quantify `` instability '' more rigorously ( particularly beyond the definition given by schweinberger ( 2011 ) ) it is useful to consider how rbm models might be expanded to incorporate more and more visibles . when increasing the size of rbm models , it becomes necessary to grow the number of model parameters ( and in this process one may also arbitrarily expand the number of hidden variables used ) . to this end , let @xmath44 , denote an element of a sequence of rbm parameters indexed by the number @xmath45 of visibles @xmath46 and define a ( scaled ) extremal log - probability ratio of the rbm model at @xmath47 as    @xmath48 \\equiv \\frac{1}{{{n_{\\scriptscriptstyle v } } } } \\text{elpr}(\\boldsymbol \\theta_{{n_{\\scriptscriptstyle v } } } ) \\label{eqn : elpr}\\end{aligned}\\ ] ]    where @xmath49 is the rbm probability of observing outcome @xmath50 for the visible variables @xmath46 under parameter vector @xmath51 .    in formulating a rbm model for a potentially large number of visibles ( i.e. , as @xmath52 )",
    ", we will say that the ratio ( [ eqn : elpr ] ) needs to stay bounded for the sequence of rbm models to be stable .",
    "that is , we make the following convention .",
    "let @xmath53 , be an element of a sequence of rbm parameters where the number of hiddens , @xmath54 , can be an arbitrary function of the number @xmath45 of visibles .",
    "a rbm model formulation is _ schweinberger - unstable _ or _ s - unstable _ if @xmath55    in other words , given any @xmath56 , there exists an integer @xmath57 so that @xmath58 for all @xmath59 .",
    "this definition of _ s - unstable _ is a generalization or re - interpretation of the `` unstable '' concept of schweinberger ( 2011 ) in that here rbm models for visibles @xmath50 do not form an exponential family and the dimensionality of @xmath51 is not fixed , but rather grows with @xmath45 .",
    "s - unstable rbm model sequences are undesirable for several reasons .",
    "one is that , as mentioned above , small changes in data can lead to overly - sensitive changes in probability . consider , for example , @xmath60 denoting the biggest log - probability ratio for a one component change in data outcomes ( visibles ) at a rbm parameter @xmath51 .",
    "we then have the following result .",
    "[ prop : instab ] let @xmath56 and let @xmath61 be as in ( [ eqn : elpr ] ) for an integer @xmath62 .",
    "if @xmath58 , then @xmath63 .    in other words",
    ", if the probability ratio ( [ eqn : elpr ] ) is too large , then a rbm model sequence will exhibit large probability shifts for very small changes in data configurations ( i.e. , will exhibit instability ) . recall the applied example of rbm models as a means to classify images . for data as pixels in an image ,",
    "the instability result in proposition [ prop : instab ] manifests itself as a one pixel change in an image ( one component of the visible vector ) resulting in a large shift in the probability , which in turn could result in a vastly different classification of the image .",
    "examples of this behavior have been presented in szegedy et al .",
    "( 2013 ) for other deep learning models , in which a one pixel change in a test image results in a wildly different classification .",
    "additionally , s - unstable rbm model sequences are connected to the near - degeneracy of section [ near - degeneracy ] ( in which model sequences place all probability on a small portion of their sample spaces ) . to see this ,",
    "define an arbitrary modal set of possible outcomes ( i.e.  set of highest probability outcomes ) in rbm models with parameters @xmath64 as @xmath65 for a given @xmath66 .",
    "then s - unstable model sequences are guaranteed to be degenerate , as the following result shows .",
    "[ prop : degen ] for an s - unstable rbm model sequence and any @xmath66 , @xmath67 as @xmath52 .",
    "in other words , s - unstable rbm model sequences are guaranteed to stack up all probability on an arbitrarily narrow set of outcomes for visibles .",
    "proofs of propositions [ prop : instab ] and [ prop : degen ] appear in kaplan , nordman , and vardeman ( 2016 ) .",
    "these findings also have counterparts in results in schweinberger ( 2011 ) , but unlike results there , we do not limit consideration to exponential family forms with a fixed number of parameters .      for spatial markov models , kaiser ( 2007 )",
    "defines a measure of model impropriety he calls _",
    "uninterpretability _ , which is characterized by dependence parameters in a model being so extreme that marginal mean - structures fail to hold as anticipated by a model statement .",
    "we adapt this notion to rbm models as follows .",
    "note that in a rbm , the parameters @xmath68 and @xmath69 are naturally associated with main effects of visible and hidden variables and can be interpreted as ( logit functions of ) means for variables @xmath70 in a model with no interaction parameters , @xmath71 .",
    "that is , with no interaction parameters , we have from ( [ eqn : pmf ] ) that @xmath72 so that , for example , @xmath73 ( or @xmath74 ) under the coding @xmath6 ( or @xmath75 ) . hence , these main effect parameters have a clear interpretation under an independence model ( one with @xmath76 ) but this interpretation can break down as interaction parameters increase in magnitude relative to the size of the main effects . in such cases , the main effect parameters @xmath77 and",
    "@xmath78 are no longer interpretable as originally intended in the models ( statements of marginal means ) and the dependence parameters are so large as to dominate the entire model probability structure ( also destroying the model interpretation of dependence as local conditional modifications of an overall marginal mean structure ) . to assess which parameter values @xmath17 may cause difficulties in interpretation",
    ", we use the difference @xmath79 - \\text{e}\\left[\\boldsymbol x | \\boldsymbol \\theta^ * \\right]$ ] between two model expectations : e@xmath80 $ ] at @xmath17 and expectations e@xmath81 $ ] where @xmath82 matches @xmath17 for all main effects but otherwise has @xmath76 for @xmath83 . hence , @xmath82 and",
    "@xmath17 have the same main effects but @xmath82 has @xmath84 dependence parameters .",
    "uninterpretability is then avoided at a parametric specification @xmath17 if the model expected value at @xmath17 is not very different from the corresponding model expectation under independence . using this , it is possible to investigate what parametric conditions lead to uninterpretability in a model versus those that guarantee interpretable models .",
    "if @xmath79 - \\text{e}\\left[\\boldsymbol x | \\boldsymbol \\theta^*\\right]$ ] is large , then the rbm model with parameter vector @xmath17 is said to be uninterpetable .",
    "the quantities to compare in the rbm case are    @xmath85 = \\sum\\limits_{\\boldsymbol x \\in \\mathcal{c}^{{{n_{\\scriptscriptstyle v}}}+ { { n_{\\scriptscriptstyle h } } } } } \\boldsymbol x f_{\\boldsymbol \\theta}(\\boldsymbol x ) = \\sum\\limits_{\\boldsymbol x \\in \\mathcal{c}^{{{n_{\\scriptscriptstyle v}}}+ { { n_{\\scriptscriptstyle h } } } } } \\boldsymbol x \\frac{\\exp\\left(\\sum\\limits_{i = 1}^{{n_{\\scriptscriptstyle v}}}\\sum\\limits_{j=1}^{{n_{\\scriptscriptstyle h}}}\\theta_{ij } v_i h_j + \\sum\\limits_{i = 1}^{{n_{\\scriptscriptstyle v}}}\\theta_{v_i } v_i + \\sum\\limits_{j = 1}^{{n_{\\scriptscriptstyle h}}}\\theta_{h_j } h_j\\right)}{\\sum\\limits_{\\boldsymbol x \\in \\mathcal{c}^{{{n_{\\scriptscriptstyle v}}}+ { { n_{\\scriptscriptstyle h}}}}}\\exp\\left(\\sum\\limits_{i = 1}^{{n_{\\scriptscriptstyle v}}}\\sum\\limits_{j=1}^{{n_{\\scriptscriptstyle h}}}\\theta_{ij } v_i h_j + \\sum\\limits_{i = 1}^{{n_{\\scriptscriptstyle v}}}\\theta_{v_i } v_i +",
    "\\sum\\limits_{j = 1}^{{n_{\\scriptscriptstyle h}}}\\theta_{h_j } h_j\\right)}\\ ] ] and    @xmath86 & = \\sum\\limits_{\\boldsymbol x \\in \\mathcal{c}^{{{n_{\\scriptscriptstyle v}}}+ { { n_{\\scriptscriptstyle h } } } } } \\boldsymbol x \\frac{\\exp\\left(\\sum\\limits_{i = 1}^{{n_{\\scriptscriptstyle v}}}\\theta_{v_i } v_i + \\sum\\limits_{j = 1}^{{n_{\\scriptscriptstyle h}}}\\theta_{h_j } h_j\\right)}{\\sum\\limits_{\\boldsymbol x \\in \\mathcal{c}^{{{n_{\\scriptscriptstyle v}}}+ { { n_{\\scriptscriptstyle h}}}}}\\exp\\left(\\sum\\limits_{i = 1}^{{n_{\\scriptscriptstyle v}}}\\theta_{v_i } v_i + \\sum\\limits_{j = 1}^{{n_{\\scriptscriptstyle h}}}\\theta_{h_j } h_j\\right ) } \\\\\\end{aligned}\\ ] ]",
    "we next explore and numerically explain the relationship between values of @xmath17 and the three notions of model impropriety , near - degeneracy , instability , and uninterpretability , for rbm models of varying sizes .      to illustrate the ideas of model near - degeneracy , instability , and uninterpretability in a rbm",
    ", we consider first the smallest possible ( toy ) example that consists of one visible node @xmath87 and one hidden node @xmath88 that are both binary .",
    "a schematic of this model can be found in figure [ fig : toymodel ] . because it seems most common",
    ", we shall begin by employing @xmath89 encoding of binary variables ( both @xmath88 and @xmath87 taking values in @xmath90 ) .",
    "( eventually we shall argue in section [ data - coding - to - mitigate - apparent - degeneracy ] that @xmath91 coding has substantial advantages . )      for this small model , we are able to investigate the symptoms of model impropriety , beginning with near - degeneracy . to this end ,",
    "recall from section [ near - degeneracy ] that one characterization requires consideration of the convex hull of possible values of statistics @xmath40 , @xmath92 appearing in the rbm probabilities for this model . as",
    "this set is in three dimensions , we are able to explicitly illustrate the shape of boundary of the convex hull of @xmath38 and explore the behavior of the mean vector @xmath93 as a function of the parameter vector @xmath17 . figure [ fig : toyhull ] shows the convex hull of our `` statistic space , '' @xmath94 , for this toy problem from two perspectives ( enclosed by the unit cube @xmath95 ^ 3 $ ] , the convex hull of @xmath96 ) . in this small model ,",
    "note that the convex hull of @xmath38 does not fill the unrestricted hull of @xmath96 because of the relationship between the elements of @xmath97 ( i.e. @xmath98 only if @xmath99 ) .",
    "we can compute the mean vector for @xmath40 as a function of the model parameters as    @xmath100 =   \\sum\\limits_{\\boldsymbol x = ( v_1 , h_1 ) \\in \\{0,1\\}^2 } \\left\\ { t(x ) \\frac{\\exp\\left ( \\theta_{11 } h_1 v_1 + \\theta_{h1 } h_1 + \\theta_{v1 } v_1\\right)}{\\gamma(\\boldsymbol \\theta ) } \\right\\ } = \\left [ \\begin{matrix } \\frac{\\exp\\left(\\theta_{v_1}\\right ) + \\exp\\left(\\theta_{11 } + \\theta_{v_1 } + \\theta_{h_1}\\right)}{\\gamma(\\boldsymbol \\theta ) } \\\\",
    "\\frac{\\exp\\left(\\theta_{h_1}\\right ) + \\exp\\left(\\theta_{11 } + \\theta_{v_1 } + \\theta_{h_1}\\right)}{\\gamma(\\boldsymbol \\theta ) } \\\\ \\frac{\\exp\\left(\\theta_{11 } + \\theta_{v_1 } + \\theta_{h_1}\\right)}{\\gamma(\\boldsymbol \\theta ) } \\\\ \\end{matrix } \\right]\\end{aligned}\\ ] ]    where @xmath101 .",
    "the three parametric coordinate functions of @xmath102 can be represented as in figure [ fig : degen_toy ] .",
    "( contour plots for three coordinate functions are shown in columns for various values of @xmath103 . ) in examining these , we see that as coordinates of @xmath17 grow larger in magnitude , at least one mean function for the entries of @xmath40 approaches a value 0 or 1 , forcing @xmath93 to be near to the boundary of the convex hull of @xmath38 , as a sign of model near - degeneracy .",
    "thus , for a very small example we can see the relationship between values of @xmath17 and model degeneracy .",
    "contour plots for the three parametric mean functions of sufficient statistics for a rbm with one visible and one hidden node . ]",
    "secondly , we can look at @xmath104 from ( [ eqn : elpr ] ) for this tiny model in order to consider model instability as a function of rbm parameters . recall that large values of @xmath104 are associated with an extreme sensitivity of the model probabilities @xmath43 to small changes in @xmath42 ( see proposition [ prop : instab ] ) .",
    "the quantity @xmath104 for this small rbm is    @xmath105 = \\log \\left[\\frac{\\max\\limits_{v_1 \\in \\mathcal{c}}\\sum\\limits_{h_1 \\in \\mathcal{c}}\\exp\\left\\{\\theta_{11 } h_1 v_1 + \\theta_{h_1}h_1 + \\theta_{v_1 } v_1 \\right\\}}{\\min\\limits_{v_1 \\in \\mathcal{c}}\\sum\\limits_{h_1 \\in \\mathcal{c}}\\exp\\left\\{\\theta_{11 } h_1 v_1 + \\theta_{h_1}h_1 + \\theta_{v_1 } v_1 \\right\\}}\\right ] .",
    "\\\\\\end{aligned}\\ ] ]    figure [ fig : instab ] shows contour plots of @xmath106 for various values of @xmath17 in this model with @xmath107 .",
    "we can see that this quantity is large for large magnitudes of @xmath17 , especially for large values of the dependence / interaction parameter @xmath103 .",
    "this suggests instability as @xmath108 becomes large , agreeing also with the concerns about near - degeneracy produced by consideration of @xmath102 .",
    "@xmath106 for various values of @xmath17 for the tiny example model .",
    "recall here @xmath45 is the number of visible nodes and here is @xmath8 .",
    "this quantity is large for large magnitudes of @xmath17 . ]    finally to consider the effect of @xmath17 on potential model uninterpretability , we can look at the difference between model expectations , e@xmath80 $ ] , and expectations given independence , e@xmath109 $ ] for the tiny toy rbm model where @xmath110 .",
    "this difference is given by    @xmath111 -    \\text{e}\\left[\\boldsymbol x | \\boldsymbol \\theta^ * \\right ]   & = \\left [ \\begin{matrix } \\frac{\\exp\\left(\\theta_{11 } + \\theta_{v_1 } + 2\\theta_{h_1}\\right ) - \\exp\\left ( \\theta_{v_1 } + 2\\theta_{h_1}\\right ) } { \\left(\\exp\\left(\\theta_{v_1}\\right ) + \\exp\\left(\\theta_{h_1}\\right ) + \\exp\\left(\\theta_{11 } + \\theta_{v_1 } + \\theta_{h_1}\\right)\\right)\\left(\\exp\\left(\\theta_{v_1}\\right ) + \\exp\\left(\\theta_{h_1}\\right ) + \\exp\\left(+ \\theta_{v_1 } + \\theta_{h_1}\\right)\\right ) } \\\\ \\frac{\\exp\\left(\\theta_{11 } + 2\\theta_{v_1 } + \\theta_{h_1}\\right ) - \\exp\\left ( 2\\theta_{v_1 } + \\theta_{h_1}\\right )   } { \\left(\\exp\\left(\\theta_{v_1}\\right ) + \\exp\\left(\\theta_{h_1}\\right ) + \\exp\\left(\\theta_{11 } + \\theta_{v_1 } + \\theta_{h_1}\\right)\\right)\\left(\\exp\\left(\\theta_{v_1}\\right ) + \\exp\\left(\\theta_{h_1}\\right ) + \\exp\\left(+ \\theta_{v_1 } + \\theta_{h_1}\\right)\\right ) } \\end{matrix}\\right].\\end{aligned}\\ ] ]    again , we can inspect these coordinate functions of this vector difference to look for a relationship between parameter values and large values of @xmath112 - \\text{e}[\\boldsymbol x| \\boldsymbol \\theta^*]$ ] as a signal of uninterpretability for the toy rbm .",
    "the absolute difference between coordinates of model expectations , e@xmath80 $ ] , and expectations given independence , e@xmath113 $ ] for a rbm with one visible and one hidden node . as an indicator of uninterpretability , note that differences in expectations increase as the dependence parameter @xmath103 deviates from zero . ]",
    "figure [ fig : uninterp ] shows that the absolute difference between coordinates of the vector of model expectations , e@xmath80 $ ] and corresponding expectations e@xmath109 $ ] given independence grow for the toy rbm as the values of @xmath17 are farther from zero , especially for large magnitudes of the dependence parameter @xmath103 .",
    "this is a third indication that parameter vectors of large magnitude lead to model impropriety in a rbm .",
    "multiple encodings of the binary variables are possible . for example , we could allow hiddens @xmath114 and visibles @xmath115 , as in the previous sections or we could instead encode the state of the variables as @xmath116 and @xmath117 .",
    "this will result in variables @xmath118 from ( [ eqn : t ] ) satisfying @xmath119 or @xmath120 depending on how we encode `` on '' and `` off '' states in the nodes . to explore the effect of this encoding on the potential for apparent near - degeneracy of a rbm",
    ", we can consider the ratio of the volume of the hypercube with corners at elements of @xmath38 to the volume of the convex hull of @xmath38 under both possible encodings ( i.e. , compare convex hull of @xmath38 to either @xmath95 ^ 3 $ ] or @xmath121 ^ 3 $ ] under the respective encodings ) .",
    "for the small two node example , the @xmath89 encoding loses 83.33% of the volume of @xmath95 ^ 3 $ ] and the @xmath91 encoding loses 66.67% of the volume of @xmath121 ^ 3 $ ] . while there is clearly a one - to - one mapping between models for these two possible codings , this notion of lost volume can be helpful to geometrically conceptualize how difficult it will be for the mean vector @xmath102 to avoid the boundary of @xmath38 , and thus avoid apparent near - degeneracy .",
    "in fact , if we look at the ratio of volume within the convex hull defined by @xmath38 and the corresponding hypercube , we can see a relationship emerge as the number of nodes ( and thus parameters ) increases . in figure [ fig : volume_plot ] , it is evident that as the number of parameters increases , this ratio is decreasing at an increasing rate , meaning it gets more and more difficult to avoid the boundary of the convex hull and thus appearance of near degeneracy .",
    "additionally , it appears that the @xmath91 encoding suffers slightly less from this problem .",
    "this suggests that if intuition about models is to be formed in terms of a data encoding that encourages of `` non - pathological '' means @xmath122 for @xmath40 , then the @xmath91 encoding is preferable to @xmath89 coding .    the relationship between volume of the convex hull of possible values of the rbm sufficient statistics and the cube containing it for different configurations of nodes.,height=288 ]    in addition to the argument that the @xmath91 data encoding mitigates some perceived prevalence of near - degeneracy it also has the benefit of providing a guaranteed - to - be non - degenerate model at @xmath123 , where the zero vector then serves as the natural center of the parameter space and induces the simplest possible model properties for the rbm ( i.e. , at @xmath124 , all variables are independent and visible variables are independent and uniformly distributed on @xmath117 ) . the proof of this and",
    "further exploration of the equivalence of the @xmath17 parameterization of the rbm model class and parameterization by @xmath102 is in the on - line supplementary materials .",
    "hence , while from some computing perspectives @xmath89 coding might seem most natural , the @xmath91 coding is far more convenient and interpretable from the point of view of statistical modeling , where it makes parameters simply interpreted in terms of symmetrically defined main effects and interactions . in light of all of these matters",
    "we will henceforth employ the @xmath91 coding .      to explore the impact of rbm parameter vector @xmath17 magnitude on near - degeneracy , instability , and uninterpretability",
    ", we consider models of small size .",
    "for @xmath125 , we sample 100 values of @xmath17 with various magnitudes ( details to follow ) .",
    "for each set of parameters we then calculate metrics of model impropriety introduced in section [ degeneracy - instability - and - uninterpretability - oh - my ] based on @xmath102 , @xmath106 , and the absolute coordinates of @xmath79 - \\text{e}\\left[\\boldsymbol x | \\boldsymbol \\theta^ * \\right]$ ] . in the case of near - degeneracy",
    ", we classify each model as near - degenerate or `` viable '' based on the distance of @xmath102 from the boundary of the convex hull of @xmath38 and look at the fraction of models that are `` near - degenerate , '' meaning they are within a small distance @xmath126 of the boundary of the convex hull .",
    "we define `` small '' through a rough estimation of the volume of the hull for each model size .",
    "we pick @xmath127 for @xmath128 and then , for every other @xmath129 and @xmath45 , set @xmath130 and pick @xmath131 so that @xmath132 . in this way",
    ", we roughly scale the volume of the `` small distance '' to the boundary of the convex hull to be equivalent across model dimensions .    in our numerical experiment",
    ", we split @xmath133 into @xmath134 and @xmath135 , in reference to which variables in the probability function the parameters correspond ( whether they multiply a @xmath2 or a @xmath19 or they multiply a @xmath21 ) , and allow the two types of terms to have varying average magnitudes , @xmath136 and @xmath137 .",
    "these average magnitudes vary on a grid between 0.001 and 3 with 24 breaks , yielding 576 grid points .",
    "( by looking at the average magnitudes , we are able to later consider the potential benefit of shrinking each parameter value @xmath138 towards zero in a bayesian fitting technique . ) at each point in the grid , 100 vectors ( @xmath134 ) are sampled uniformly on a sphere with radius corresponding to the first coordinate in the grid and 100 vectors ( @xmath139 ) are sampled uniformly on a sphere with radius corresponding to the second coordinate in the grid via sums of squared and scaled iid normal@xmath140 variables .",
    "these vectors are then paired to create 100 values of @xmath17 with magnitudes at each point in the grid .",
    "the results of this numerical study are summarized in figures [ fig : degen_plots ] , [ fig : instab_plots ] , and [ fig : uninterp_plots ] . from these three figures , it is clear that all three measures of model impropriety show higher values for larger magnitudes of the parameter vectors .",
    "additionally , since there are @xmath141 interaction terms in @xmath17 versus only @xmath142 main effect terms , for large models there are many more interaction parameters than main effects in the models . and",
    "so , severely limiting the magnitude of the individual interactions may well help prevent model impropriety .    results from the numerical experiment , here looking at the fraction of models that were near - degenerate for each combination of magnitude of @xmath17 and model size .",
    "black lines show the contour levels for fraction of near - degeneracy , while the thick black line shows the level where the fraction of near - degenerate models is .05 . ]",
    "results from the experiment , here looking at the sample mean value of @xmath143 at each grid point for each combination of magnitude of @xmath17 and model size . as the magnitude of @xmath17 grows , so does the value of this metric , indicating typical instability in the model . ]",
    "the sample mean of the maximum component of the absolute difference between the model expectation vector , e@xmath80 $ ] , and the expectation vector given independence , e@xmath113 $ ] .",
    "larger magnitudes of @xmath17 correspond to larger differences , thus indicating reduced interpretability . ]",
    "figure [ fig : split_plots_dist ] shows the fraction of near - degenerate models for each magnitude of @xmath17 for each model architecture . for each number @xmath45 of visibles in the model ,",
    "as the number @xmath129 of hiddens increase , the fraction near - degenerate diverges from zero at increasing rates for larger values of @xmath144 .",
    "this shows that as model size gets larger , the risk of degeneracy starts at a slightly larger magnitude of parameters , but very quickly increases until reaching close to 1 .",
    "the fraction of near - degenerate models for each magnitude of @xmath17 . for each number @xmath145 of visibles in the model , the fraction near - degenerate becomes greater than zero at larger values of @xmath144 as @xmath129 increases and the slope becomes steeper as @xmath129 increases as well . ]    these manageable examples indicate that rbms are near - degenerate , unstable , and uninterpretable for large portions of the parameter space with large @xmath146 .",
    "this , however , is not the only potential problem to be faced when using these models .",
    "there is the matter of principled / rigorous fitting of rbm models .",
    "typically , fitting a rbm via maximum likelihood ( ml ) methods will be infeasible due mainly to the intractability of the normalizing term @xmath22 in a model ( [ eqn : pmf ] ) of any realistic size .",
    "ad hoc methods are used instead , which aim to avoid this problem by using stochastic ml approximations that employ a small number of mcmc draws ( i.e. , contrastive divergence , ( hinton 2002 ) )",
    ".    however , computational concerns are not the only issues with fitting a rbm using ml .",
    "in addition , a rbm model , with the appropriate choice of parameters and number of hiddens , has the potential to re - create any distribution for the data ( i.e. , reproduce any specification of cell probabilities for the binary data outcomes ) .",
    "for example , montufar and ay ( 2011 ) show that any distribution on @xmath147 can be approximated arbitrarily well by a rbm with @xmath148 hidden units .",
    "we provide a small example that illustrates that in fact there can be many such approximations .    for simplicity , consider a model with two visible variables @xmath149 and one hidden @xmath150 . in this case , there are four possible data realizations for @xmath149 given by @xmath151 and we may express the model probabilities as @xmath152 \\right),\\ ] ] for @xmath153 , in terms of real - valued parameters @xmath154 .",
    "given any specified cell probabilities , say    @xmath155    for the outcomes",
    "@xmath151 , values for parameters @xmath156 may be chosen to approximate such cell probabilities with arbitrary closeness .",
    "in fact , when the cell probabilities ( [ eqn:0 ] ) are all strictly positive , parameters in the rbm model can be specified to reflect these probabilities exactly .",
    "and , when one or more of the cell probabilities ( [ eqn:0 ] ) are zero , the corresponding rbm probabilities @xmath157 may never be identically zero ( due to exponential terms in the model ) but parameters can be still selected to make the appropriate rbm cell probabilities arbitrarily small .    to demonstrate , we assume @xmath158 ( without loss of generality ) in the specified cell probabilities ( [ eqn:0 ] ) and replace parameters @xmath159 with @xmath160 and @xmath161 .",
    "we may then prescribe values of @xmath162 so that the model probability ratio @xmath163 matches the corresponding ratio @xmath164 over three values of @xmath165 .",
    "for instance , assuming the cell probabilities from ( [ eqn:0 ] ) are all positive , these probabilities can be exactly reproduced by choosing    @xmath166    and selecting @xmath167 to solve    @xmath168    based on a monotonically increasing function @xmath169 , @xmath170 .",
    "if @xmath171/ [ p_{(-1,1)}p_{(1,-1 ) } ] \\geq 1 $ ] , one can pick any values for @xmath172 and solve ( [ eqn:1 ] ) for @xmath173 ; likewise , when @xmath171/ [ p_{(-1,1)}p_{(1,-1 ) } ] < 1 $ ] in ( [ eqn:1 ] ) , one may solve for @xmath174 upon choosing any values for @xmath175 .",
    "alternatively , if exactly one specified cell probability in ( [ eqn:0 ] ) is zero , say @xmath176 ( without loss of generality ) , we can select parameters @xmath177 as above based on a sequence @xmath178 , @xmath179 of the remaining parameter values such that @xmath180 and @xmath181 hold .",
    "this guarantees that the resulting rbm model matches the given cell probabilities ( [ eqn:0 ] ) in the limit :    @xmath182    if exactly two specified probabilities in ( [ eqn:0 ] ) are zero , say @xmath176 and @xmath183 ( without loss of generality ) , then a limit approximation as in ( [ eqn:2 ] ) follows by picking @xmath77 as above based on any choices of @xmath184 and choosing a sequence of @xmath185 values for which @xmath186 .",
    "the previous discussion illustrates the fact that the rbm model class suffers from parameter identifiability issues that go beyond mere symmetries in the parametrization .",
    "not only it is possible to approximate any distribution on the visibles arbitrarily well ( cf . montufar and ay 2011 ) , but quite different parameter settings can induce the same essential rbm model . however , this is not the most disastrous implication of the rbm parameterization .",
    "a far worse consequence is that , when fitting the rbm model by likelihood - based methods , we already know the nature of the answer before we begin : namely , such fitting will simply aim to reproduce the empirical distribution from the training data if sufficiently many hiddens are in the model . that is , based on a random sample of vectors of visible variables , the model for the cell probabilities that has the highest likelihood over _ all possible model classes _",
    "( i.e. , rbm - based or not ) is the empirical distribution , and the over - parametrization of the rbm model itself ensures that this empirical distribution can be arbitrarily well - approximated .",
    "for illustration , continue the simple example from above with @xmath187 iid observations , each consisting of two realized visibles @xmath149 . in which case",
    ", when the specified cell probabilities @xmath188 in ( [ eqn:0 ] ) are taken as the empirical cell frequencies from the sample , there is no better model based on maximum likelihood , and the discussion above ( cf .",
    "( [ eqn:2 ] ) ) shows that rbm model parameters can be chosen to re - create this empirical distribution to an arbitrarily close degree .",
    "hence , rbm model fitting based on ml will simply seek to reproduce the empirical distribution .",
    "what s more , whenever this empirical distribution contains empty cells , fitting steps for the rbm model will necessarily aim to choose parameters that necessarily diverge to infinity in magnitude in order to zero - out the corresponding rbm cell probabilities . in data applications with a large sample space ,",
    "it is unlikely that the training set will include at least one of each possible vector outcome ( unlike this small example ) .",
    "this implies that some rbm model parameters must diverge to @xmath189 to mimic the empirical distribution with empty cells and , as we have already discussed in section [ explorations ] , large magnitudes of @xmath17 lead to model impropriety in the rbm .    here",
    "we consider what might be done in a principled manner to prevent both overfitting and model impropriety , testing on a @xmath190 case that already stretches the limits of what is computable - in particular we consider bayes methods .      to avoid model impropriety for a fitted rbm",
    ", we want to avoid parts of the parameter space @xmath191 that lead to near - degeneracy , instability , and uninterpretability .",
    "motivated by the insights in section [ exploring - manageable - examples ] , one idea is to shrink @xmath133 toward @xmath192 by specifying priors that place low probability on large values of @xmath144 , specifically focusing on shrinking @xmath135 more than @xmath134 .",
    "this is similar to an idea advocated by hinton ( 2010 ) called _ weight decay _ , in which a penalty is added to the interaction terms in the model , @xmath135 , shrinking their magnitudes .",
    ".[tab : theta]parameters used to fit a test case with @xmath193 .",
    "this parameter vector was chosen as a sampled value of @xmath17 that was not near the convex hull of the sufficient statistics for a grid point in figure [ fig : degen_plots ] with @xmath194% near - degeneracy . [ cols=\">,>,>,>,>,>\",options=\"header \" , ]     the effective sample sizes for a chain of length @xmath195 for inference about each of the @xmath196 model probabilities are presented in table [ tab : m_eff ] . these range from @xmath197 to @xmath198 for bwtnml , while bwtnlv only yields between @xmath199 and @xmath200 effective draws .",
    "thus , bwtnlv would require at least @xmath201 times as many iterations to be run of the mcmc chain in order to achieve the same amount of effective information about the posterior distribution .",
    "for this reason , consistent with the acf results in figure [ fig : acf ] , bwtnlv does not seem to be an effective method for fitting the rbm if computing resources are at all limited .",
    "figure [ fig : fitting_plot ] shows the posterior probability of each possible @xmath202 after fitting the rbm model in the two ways detailed in this section ( excluding bwtnlv ) .",
    "the black vertical lines show the true probabilities of each image based on the parameters used to generate the training set while the red vertical lines show the empirical distribution for the training set of @xmath203 vectors . from these posterior predictive checks ,",
    "it is evident that bwtnml produces the best fit to the data .",
    "however , this method requires a marginalization step to obtain the probability function of visible observations alone , which is infeasible for a model with @xmath129 of any real size .",
    "posterior probabilities of @xmath204 possible realizations of @xmath205 visibles using two of the three bayesian fitting techniques , bwtplv and bwtnml .",
    "the black vertical lines show the true probabilities of each vector of visibles based on the parameters used to generate the training data while the red vertical lines show the empirical distribution .",
    "bwtnml produces the best fit for the data , however is also the most computationally intensive and least feasible with a real dataset . ]",
    "rbm models constitute an interesting class of undirected graphical models that are thought to be useful for supervised learning tasks . however , when viewed as generative statistical models , rbms are prone to forms of model impropriety such as near - degeneracy , s - instability , and uninterpretability .",
    "additionally , these models are difficult to fit using a rigorous methodology , due to the dimension of the parameter space coupled with the size of the latent variable space .    in this paper , we have presented three honest bayes mcmc - based methods for fitting rbms .",
    "common practice is to use a kind of mcmc to overcome fitting complexities .",
    "however because of the size of the space to be filled with mcmc iterates , convergence and mixing of these methods will be slow .",
    "marginalization over the latent variables in the model can improve mixing , but is numerically intractable due to the necessity of repeated calculation of the normalizing constant .    ultimately , it is not clear that rbm models are useful as generative models . due to the extreme flexibility in this model class , rigorous likelihood - based fitting for a rbm will typically merely return the ( discrete ) empirical distribution for visibles . even if a rigorous likelihood - based fitting method is practically possible , we know what it will produce for fitted probabilities before we even begin . for these reasons ,",
    "we are skeptical of the claims made about rbms as generative tools .",
    "fisher , r. a. 1922 .",
    "`` on the mathematical foundations of theoretical statistics . ''",
    "_ philosophical transactions of the royal society of london a : mathematical , physical and engineering sciences _ 222 ( 594 - 604 ) .",
    "the royal society : 30968 .",
    "kaiser , mark s. 2007 .",
    "`` statistical dependence in markov random field models . '' _ statistics preprints _ paper 57 . digital repository @ iowa state university .",
    "/ stat_las_preprints/57/.      larochelle , hugo , and yoshua bengio .",
    "2008 . `` classification using discriminative restricted boltzmann machines . '' in _ proceedings of the 25th international conference on machine learning _ , 53643 .",
    "acm .",
    "li , jing .",
    "`` biclustering methods and a bayesian approach to fitting boltzmann machines in statistical learning . ''",
    "phd thesis , iowa state university ; graduate theses ; dissertations .",
    "/ etd/14173/.      montfar , guido f , johannes rauh , and nihat ay",
    "`` expressive power and approximation errors of restricted boltzmann machines . '' in _ advances in neural information processing systems _ , 41523 . nips .",
    "nguyen , anh mai , jason yosinski , and jeff clune .",
    "`` deep neural networks are easily fooled : high confidence predictions for unrecognizable images . ''",
    "_ arxiv preprint arxiv:1412.1897_. http://arxiv.org/abs/1412.1897 .",
    "szegedy , christian , wojciech zaremba , ilya sutskever , joan bruna , dumitru erhan , ian j. goodfellow , and rob fergus .",
    "2013 . `` intriguing properties of neural networks . ''",
    "_ arxiv preprint arxiv:1312.6199_. http://arxiv.org/abs/1312.6199 .",
    "zhou , wen .",
    "`` some bayesian and multivariate analysis methods in statistical machine learning and applications . ''",
    "phd thesis , iowa state university ; graduate theses ; dissertations .",
    "http://lib.dr.iastate.edu / etd/13816/."
  ],
  "abstract_text": [
    "<S> a restricted boltzmann machine ( rbm ) is an undirected graphical model constructed for discrete or continuous random variables , with two layers , one hidden and one visible , and no conditional dependency within a layer . in recent years , rbms have risen to prominence due to their connection to deep learning . by treating a hidden layer of one rbm as the visible layer in a second rbm , a deep architecture can be created . </S>",
    "<S> rbms are thought to thereby have the ability to encode very complex and rich structures in data , making them attractive for supervised learning . </S>",
    "<S> however , the generative behavior of rbms is largely unexplored . in this paper </S>",
    "<S> , we discuss the relationship between rbm parameter specification in the binary case and the tendency to undesirable model properties such as degeneracy , instability and uninterpretability . </S>",
    "<S> we also describe the difficulties that arise in likelihood - based and bayes fitting of such ( highly flexible ) models , especially as gibbs sampling ( quasi - bayes ) methods are often advocated for the rbm model structure .    </S>",
    "<S> # 1    _ keywords : _ degeneracy , instability , classification , deep learning , graphical models </S>"
  ]
}