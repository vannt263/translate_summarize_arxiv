{
  "article_text": [
    "one of the important approaches to statistical physics is provided by information theory erected by claude shannon in the late 1940s .",
    "central tenet of this approach lies in a construction of a measure of the  amount of uncertainty \" inherent in a probability distribution  @xcite .",
    "this measure of information ( or shannon s entropy ) quantitatively equals to the number of binary ( yes / no ) questions which brings us from our present state of knowledge about the system in question to the one of certainty .",
    "the higher is the measure of information ( more questions to be asked ) the higher is the ignorance about the system and thus more information will be uncovered after an actual measurement .",
    "usage of shannon s entropy is particularly pertinent to the bayesian statistical inference where one deals with the probability distribution assignment subject to prior data one possess about a system  @xcite . here",
    "the prescription of maximal shannon s entropy ( i.e. , maximal ignorance ) - maxent , subject to given constraints yields the least biased probability distribution which naturally protects against conclusions which are not warranted by the prior data . in classical maxent the maximum ",
    "entropy distributions are always of an exponential form and hence the name generalized canonical distributions .",
    "note that maxent prescription , in a sense , resembles the principle of minimal action of classical physics as in both cases extremization of a certain functionals - the entropy or action functionals - yields physical predictions .",
    "in fact , the connection between information and the action functional was conjectured by e.t .",
    "jaynes  @xcite and j.a .",
    "wheeler  @xcite , and most recently this line of reasonings has been formalized e.g. , by b.r .",
    "frieden in hisprinciple of extreme physical information - epi \"  @xcite .    on a formal level",
    "the passage from information theory to statistical thermodynamics is remarkably simple . in this case",
    "a maximal  entropy probability distribution subject to constraints on average energy , or constant average energy and number of particles yields the usual canonical or grand ",
    "canonical distributions of gibbs , respectively .",
    "applicability in physics is , however , much wider . aside of statistical thermodynamics ,",
    "maxent has now become a powerful tool in non  equilibrium statistical physics  @xcite and is equally useful in such areas as astronomy , geophysics , biology , medical diagnosis and economics . for the latest developments in classical maxent",
    "the interested reader may consult ref .",
    "@xcite and citations therein .",
    "as successful as shannon s information theory has been , it is clear by now that it is capable of dealing with only a limited class of systems one might hope to address in statistical physics .",
    "in fact , only recently it has become apparent that there are many situations of practical interest requiring more ",
    "exotic \" statistics which does not conform with the generalized canonical prescription of the classical maxent ( often referred as boltzmann  gibbs statistics ) .",
    "percolation , polymers , protein folding , critical phenomena or stock market returns provide examples . on the other hand",
    ", it can not be denied that maxent approach deals with statistical systems in a way that is methodically appealing , physically plausible and intrinsically nonspeculative ( i.e. , maxent invokes no hypotheses beyond the sample space and the evidence that is in the available data ) .",
    "it might be therefore desirable to inspect the axiomatics of shannon s information theory to find out whether some  plausible \" generalization is possible .",
    "if so , such an extension could provide a new conceptual frame in which generalized measures of information ( i.e. , entropies ) could find their theoretical justification .",
    "the additivity of independent mean information is the most natural axiom to attack . on this level three",
    "modes of reasoning can be formulated .",
    "one may either keep the additivity of independent information but utilize more general definition of means , or keep the the usual definition of linear means but generalize the additivity law or combine both these approaches together .    in the first case",
    "the crucial observation is that the most general means compatible with kolmogorov s axioms of probability are the so called quasi ",
    "linear means which are implemented via kolmogorov  nagumo functions  @xcite .",
    "this approach was pioneered by a.  rnyi  @xcite , j.  aczl and z.  darczy  @xcite in 60s and 70s",
    ". the corresponding measure of information is then called rnyi s entropy .",
    "because the independent information are in this generalization still additive and because the quasi  linear means basically probe dimensionality of the sample space one may guess that this theory should play an important rle in classical information  theoretical systems with a non  standard geometry , such as fractals , multi  fractals or systems with embedded self  similarity .",
    "these include phase transitions and critical phenomena , chaotic dynamical systems with strange attractors , fully developed turbulence , hadronic physics , cosmic strings , etc .",
    "second case amounts to a modification of the additivity law . out of the infinity of possible generalizations",
    "the so called @xmath0additivity prescription has found widespread utility .",
    "the @xmath0calculus was introduced by f.  jackson  @xcite in 20 s and more recently developed in the framework of quantum groups by v.  drinfeld  @xcite and m.  jimbo  @xcite . with the help of @xmath0calculus",
    "one may formalize the entire approach in an unified manner by defining @xmath0derivative ( jackson derivative ) , @xmath0integration ( jackson integral ) , @xmath0logarithms , @xmath0exponentials , etc .",
    "the corresponding measure of information is called tsallis or non  extensive entropy .",
    "the @xmath0additivity is in a sense minimal generalization because the non  additive part is proportional to both respective information and is linearly parametrized by the only one  coupling \" constant .",
    "the non  additivity prescription might be understood as a claim that despite given phenomena being statistically independent there still might be non  vanishing correlation between them and hence information might get  entangled \" .",
    "one may thus expect that tsallis entropy should be important in systems with long  range correlations or long  time memories .",
    "one may even guess that quantum non ",
    "locality might become crucial playground for non  extensive statistics .",
    "third generalization is still not explored in the literature .",
    "it can be expected that it should become relevant in e.g. , critical phenomena in a strong quantum regime .",
    "the latter can be found , for instance , in the early universe cosmological phase transitions or in currently much studied quantum phase transitions ( frustrated spin systems or quantum liquids being examples ) .",
    "the structure of this lecture is the following : in sections [ re ] we review the basic information ",
    "theoretic setup for rnyi s entropy .",
    "we show its relation to ( multi)fractal systems and illustrate how the rnyi parameter is related to multifractal singularity spectrum .",
    "the connection of rnyi s entropy with fisher information and the metric structure of statistical manifolds ( i.e. , fisher ",
    "rao metric ) are also discussed . in section  [ tsa ] the information theoretic rationale of tsallis entropy are presented .",
    "rnyi entropies ( re ) were introduced into mathematics by a.  rnyi  @xcite in mid 60 s .",
    "the original motivation was strictly formal .",
    "rnyi wanted to find the most general class of information measures which preserved the additivity of statistically independent systems and were compatible with kolmogorov s probability axioms .",
    "let us assume that one observes the outcome of two independent events with respective probabilities @xmath1 and @xmath0 .",
    "additivity of information then requires that the corresponding information obeys cauchy s functional equation @xmath2 therefore , aside from a multiplicative factor , the amount of information received by learning that an event with probability @xmath1 took place must be @xmath3 here the normalization was chosen so that ignorant s probability ( i.e. @xmath4 ) sets the unit of information - bit .",
    "formula ( [ hartley ] ) is known as hartley s information measure  @xcite . in general ,",
    "if the outcomes of some experiment are @xmath5 with respective probabilities @xmath6 , and if @xmath7 outcome delivers @xmath8 bits of information then the mean received information reads @xmath9 here @xmath10 is an arbitrary invertible function - kolmogorov  nagumo function .",
    "the mean defined in eq.([hartley1 ] ) is the so called quasi ",
    "linear mean and it constitutes the most general mean compatible with kolmogorov s axiomatics  @xcite .",
    "rnyi then proved that when the postulate of additivity for independent events is applied to eq.([hartley1 ] ) it dramatically restricts the class of possible @xmath10 s .",
    "in fact , only two classes are possible ; @xmath11 which implies the shannon information measure @xmath12 and @xmath13 which implies @xmath14 with @xmath15 ( @xmath16 and @xmath17 are arbitrary constants ) . in both cases",
    "note that for linear @xmath10 s the quasi  linear mean turns out to be the ordinary linear mean and hence shannon s information is the averaged information in the usual sense .",
    "information measure defined by ( [ g2 ] ) is called rnyi s information measure ( of order @xmath0 ) or rnyi s entropy . term",
    " entropy \" is chosen in a close analogy with shannon s theory because rnyi s entropy also represents the disclosed information ( or removed ignorance ) after performed experiment . on a deeper level",
    "it might be said that rnyi s entropy measures a diversity ( or dissimilarity ) within a given distribution  @xcite . in section  [ fish ] we will see that in parametric statistics fisher information plays a similar rle .",
    "it will be shown that the latter measures a diversity between two statistical populations .    to find the most fundamental ( and possibly irreducible ) set of properties characterizing rnyi s information it is desirable to axiomatize it .",
    "various axiomatizations can be proposed  @xcite . for our purpose",
    "the most convenient set of axioms is the following  @xcite :    1 .   for a given integer @xmath19 and given @xmath20 ( @xmath21 ) , @xmath22 is a continuous with respect to all its arguments . 2 .",
    "for a given integer @xmath19 , @xmath23 takes its largest value for @xmath24 ( @xmath25 ) with the normalization @xmath26 .",
    "3 .   for a given @xmath27 ; @xmath28 with +   + @xmath29 , +   + and @xmath30 ( distribution @xmath31 corresponds to the experiment @xmath32 ) .",
    "@xmath10 is invertible and positive in @xmath33 .",
    "@xmath34 , i.e. , adding an event of probability zero ( impossible event ) we do not gain any new information .",
    "note particularly the appearance of distribution @xmath35 in axiom 3 .",
    "this , so called , zooming ( or escort ) distribution will prove crucial is sections  [ can ] and [ tsa ] .    further characteristics of expressions ( [ g2 ] ) were studied extensively in  @xcite .",
    "we list here a few of the key ones .",
    "( a )  re is symmetric : @xmath36 ;    ( b )  re is nonnegative : @xmath37 ;    ( c )  re is decisive : @xmath38 ;    ( d )  for @xmath39 rnyi s entropy is concave . for @xmath40 rnyi s entropy in not    pure convex nor pure concave ;    ( e )  re is bounded , continuous and monotonous in @xmath0 ;    ( f )  re is analytic in @xmath41 @xmath42 for @xmath43 it equals to shannon s entropy ,    i.e. , @xmath44 .    despite its formal origin rnyi s",
    "entropy proved important in variety of practical applications .",
    "coding theory  @xcite , statistical inference  @xcite , quantum mechanics  @xcite , chaotic dynamical systems  @xcite and multifractals provide examples .",
    "the rest of section  2 will be dedicated to applications in multifractal systems . for this purpose",
    "it is important to introduce the concept of renormalized information .",
    "let us assume that the outcome space ( or sample space ) is a continuous @xmath17dimensional manifold .",
    "it is then heuristically clear that as we refine the measurement the information obtained tends to infinity . yet , under certain circumstances a finite information can be extracted from the continuous measurement .    to show this we pave the outcome space . ] with boxes of the size @xmath45 .",
    "this divides the @xmath17dimensional sample space into cells labelled by an index @xmath46 which runs from @xmath47 up to @xmath48 . if @xmath49 is a continuous probability density function ( pdf ) , the corresponding integrated probability distribution @xmath50 is generated via prescription @xmath51 generic form of @xmath52 it then represented as @xmath53 where the symbol @xmath54 means that the residual error tends to @xmath55 for @xmath56 .",
    "the @xmath57 part ( @xmath58 ) is fixed by the requirement ( or by renormalization prescription ) that it should fulfill the postulate of additivity in order to be identifiable with an information measure . incidentally ,",
    "the latter uniquely fixes the divergent part  @xcite as @xmath59 .",
    "so we may write @xmath60 which implies that @xmath61 the latter might be generalized to piecewise  continuous @xmath49 s ( stiltjes integration ) and to lebesgue measurable sets  @xcite .",
    "needless to say that rnyi s entropy @xmath62 exists iff .",
    "the integral on the rhs of ( [ eq2 ] ) exists .",
    "note that ( [ eq2 ] ) can be recast into a form @xmath63 with @xmath64 being the uniform distribution .",
    "expression ( [ negentropy ] ) represents nothing but rnyi s generalization of the szilard ",
    "brillouin negentropy .",
    "aforementioned renormalization issue naturally extends beyond simple metric outcome spaces ( like @xmath65 ) .",
    "our aim in this subsection and the subsection to follow is to discuss the renormalization of information in cases when the outcome space is fractal or when the statistical system in question is multifractal .",
    "conclusions of such a reneormalization will be applied is subsection  [ can ] .",
    "fractals are sets with a generally non  integer dimension exhibiting property of self  similarity .",
    "the key characteristic of fractals is fractal dimension which is defined as follows : consider a set @xmath66 embedded in a @xmath17dimensional space .",
    "let us cover the set with a mesh of @xmath17dimensional cubes of size @xmath67 and let @xmath68 is a number of the cubes needed for the covering .",
    "the fractal dimension of @xmath66 is then defined as  @xcite @xmath69 in most cases of interest the fractal dimension ( [ fra1 ] ) coincides with the hausdorff  besicovich dimension used by mandelbrot  @xcite .",
    "multifractals , on the other hand , are related to the study of a distribution of physical or other quantities on a generic support ( be it or not fractal ) and thus provide a move from the geometry of sets as such to geometric properties of distributions .",
    "let a support is covered by a probability of some phenomenon .",
    "if we pave the support with a grid of spacing @xmath70 and denote the integrated probability in the @xmath71th box as @xmath72 , then the scaling exponent @xmath73 is defined  @xcite @xmath74 the exponent @xmath73 is called singularity or lipshitz  hlder exponent .",
    "counting boxes @xmath75 where @xmath72 has @xmath76 , the singularity spectrum @xmath77 is defined as  @xcite @xmath78 thus a multifractal is the ensemble of intertwined ( uni)fractals each with its own fractal dimension @xmath79 . for further investigation",
    "it is convenient to define a  partition function \"  @xcite @xmath80 in the small @xmath70 limit the method of steepest descent yields the scaling  @xcite @xmath81 with @xmath82 this is precisely legendre transform relation .",
    "so pairs @xmath83 and @xmath84 , are conjugates with the same mathematical content .",
    "connection of rnyi entropies with multifractals is frequently introduced via generalized dimensions @xmath85 these have direct applications in chaotic attractors  @xcite and they also characterize , for instance , intermittency of turbulence  @xcite or diffusion  limited aggregates ( dla ) like patterns  @xcite . in chaotic dynamical systems",
    "all @xmath86 are necessary to describe uniquely e.g. , strange attractors  @xcite .",
    "while the proof in  @xcite is based on a rather complicated self  similarity argumentation , by employing the information theory one can show that the assumption of a self  similarity is not really fundamental  @xcite .",
    "for instance , when the outcome space is discrete then all @xmath86 with @xmath87 are needed to reconstruct the underlying distribution , and when the outcome space is @xmath17dimensional subset of @xmath65 then all @xmath86 , @xmath88 , are required to pinpoint uniquely the underlying pdf .",
    "the latter examples are nothing but the information theoretic variants of hausforff s moment problem of mathematical statistics .      in a close analogy with section  [ co1 ]",
    "it can be shown  @xcite that for a fractal outcome space the following asymptotic expansion of rnyi s entropy holds @xmath89 where @xmath90 corresponds to the hausdorff dimension .",
    "the finite part @xmath91 is , as before , chosen by the renormalization prescription - additivity of information for independent experiments .",
    "then @xmath92 measure @xmath93 in ( [ h1 ] ) is the hausdorff measure @xmath94 technical issues connected with integration on fractal supports can be found , for instance , in  @xcite .",
    "again , renormalized entropy is defined as long as the integral on the rhs of ( [ h1 ] ) exists",
    ".    we may proceed analogously with multifractals .",
    "the corresponding asymptotic expansion now reads  @xcite @xmath95 this implies that @xmath96 here the multifractal measure is defined as  @xcite @xmath97 it should be stressed that integration on multifractals is rather delicate technical issue which is not yet well developed in the literature  @xcite .",
    "we shall now present an important connection of rnyi s entropy with multifractal systems .",
    "the connection will be constructed in a close analogy with canonical formalism of statistical mechanics . as this approach",
    "is thoroughly discussed in  @xcite we will , for shortness s sake , mention only the salient points here .",
    "let us first consider a multifractal with a density distribution @xmath98 .",
    "if we use , as previously , the covering grid of spacing @xmath70 then the coarse  grained shannon s entropy of such a process will be @xmath99 important observation of the multifractal theory is that when @xmath43 then @xmath100 describes the hausdorff dimension of the set on which the probability is concentrated - measure theoretic support .",
    "in fact , the relative probability of the complement set approaches zero when @xmath101 .",
    "this statement is known as billingsley theorem  @xcite or curdling  @xcite .    for the following considerations it is useful to introduce a one  parametric family of normalized measures @xmath35 ( zooming or escort distributions ) @xmath102^q}{\\sum_j [ p_j(l)]^q } \\sim l^{f(a_i)}\\ , .",
    "\\label{zoom2}\\end{aligned}\\ ] ] because @xmath103 we obtain after integrating ( [ zoom1 ] ) from @xmath104 to @xmath105 that @xmath106 so for @xmath107 @xmath35 puts emphasis on the more singular regions of @xmath108 , while for @xmath109 the accentuation is on the less singular regions .",
    "parameter @xmath0 thus provides a  zoom in \" mechanism to probe various regions of a different singularity exponent .    as the distribution ( [ zoom2 ] ) alters the scaling of original @xmath108 , also the measure theoretic support changes .",
    "the fractal dimension of the new measure theoretic support @xmath110 of @xmath35 is @xmath111 note that the curdling ( [ measure2 ] ) mimics the situation occurring in equilibrium statistical physics .",
    "there , in the canonical formalism one works with ( usually infinite ) ensemble of identical systems with all possible energy configurations .",
    "but only the configurations with @xmath112 dominate at @xmath113 .",
    "choice of temperature then prescribes the contributing energy configurations .",
    "in fact , we may define the ",
    "microcanonical \" partition function as @xmath114 then the microcanonical ( boltzmann ) entropy is @xmath115 and hence @xmath116 interpreting @xmath117 as `` energy''we may define the  inverse temperature \" @xmath118 ( note that here @xmath119 ) as @xmath120 on the other hand , with the  canonical \" partition function @xmath121 and @xmath122 and @xmath123 the corresponding means read @xmath124 let us note particularly that the fractal dimension of the measure theoretic support @xmath125 is simply @xmath126 . by",
    "gathering the results together we have    [ cols=\"^,^ \" , ]     looking at fluctuations of @xmath127 in the ",
    "canonical \" ensemble we can establish an equivalence between unifractals and multifractls . recalling eq.([tau ] ) and realizing that @xmath128 we obtain for the relative standard deviation of  energy \" @xmath129 so for small @xmath130 ( i.e. , exact multifractal ) the @xmath127fluctuations become negligible and almost all @xmath131 equal to @xmath132 . if @xmath0 is a solution of the equation @xmath133 then in the  thermodynamic \" limit ( @xmath134 ) @xmath135 and the microcanonical and canonical entropies coincide . hence @xmath136 the subscript @xmath126 emphasizes that the shannon entropy @xmath137 is basically the entropy of an unifractal specified by the fractal dimension @xmath126 .",
    "legendre transform then implies that @xmath138 employing the renormalization prescriptions ( [ h1 ] ) and ( [ h2 ] ) we finally receive that @xmath139 so by changing the @xmath0 parameter rnyie s entropy  skims over \" all renormalized unifractal shannon s entropies .",
    "rnyi s entropy thus provides a unified information measure which keeps track of all respective unifractal shannon entropies .    the passage from multifractals to single ",
    "dimensional statistical systems is done by assuming that the @xmath127interval gets infinitesimally narrow and that pdf is smooth .",
    "in such a case both @xmath127 and @xmath140 collapse to @xmath141 and @xmath142 .",
    "for instance , for a statistical system with a smooth measure and the support space @xmath65 eq.([result ] ) constitutes a trivial identity .",
    "we believe that this is the primary reason why shannon s entropy plays such a predominant role in physics of single  dimensional sets .",
    "discussion of ( [ result ] ) can be found in  @xcite .",
    "let us present here an interesting connection which exists between riemaniann geometry on statistical parameter spaces and rnyi entropies .",
    "consider a family of pdf s characterized by a vector parameter @xmath143 @xmath144 we further assume that @xmath145 @xmath146 .",
    "the gibbs pdf s ( with @xmath147 being the inverse temperature @xmath148 , the external field @xmath149 , etc . ) represent example of ( [ param1 ] ) .    to construct a metric on @xmath150 which reflects the statistical properties of the family ( [ param1 ] ) rao a co - workers  @xcite proposed to adopt various measures of dissimilarity between two probability densities , and then use them to derive the metric .",
    "important class of dissimilarity measures are measures based on information theory . typically it is utilized the gain of information when a density @xmath151 is replaced with a density @xmath152 . in the case of rnyi",
    "s entropy this is  @xcite @xmath153 the information metric on @xmath150 is then defined via the leading order of dissimilarity between @xmath152 and @xmath154 , namely @xmath155 note that because @xmath156 is minimal at @xmath157 , linear term in ( [ gij ] ) vanishes .",
    "so we have @xmath158_{\\theta = \\phi } = \\frac{q}{2 \\ln 2}\\left ( \\int_m dx \\ p(x,\\theta ) \\frac{\\partial \\ln p(x , \\theta)}{\\partial \\theta_i}\\frac{\\partial \\ln p(x , \\theta)}{\\partial \\theta_j } \\right)\\nonumber \\\\ & = & \\frac{q}{2 \\ln 2 } \\",
    "f_{ij}(p(x,\\theta)\\ , .\\end{aligned}\\ ] ] here @xmath159 is the fisher information matrix ( or fisher  rao metric )  @xcite .",
    "fisher matrix is the only riemaniann metric which is invariant under transformation of variables as well as reparametrization  @xcite . in addition , the diagonal elements of fisher s information matrix represent the amount of information on @xmath147 in an element of a sample chosen from a population of density functions @xmath152 . due to its relation with cramr ",
    "rao inequality fisher information matrix plays a crucial rle in parametric estimation  @xcite .",
    "let us stress that the latter is used in quantum mechanics to formulate information uncertainty relations  @xcite .",
    "tsallis entropy ( or non  extensive entropy , or @xmath0order entropy of havrda and charvt  @xcite ) has been recently much studied in connection with long  range correlated systems and with non  equilibrium phenomena .",
    "although firstly introduced by havrda and charvt in the cybernetics theory context  @xcite it was tsallis and co ",
    "workers  @xcite who exploited its non  extensive features and placed it in a physical setting .",
    "applications of tsallis entropy are ranging from 3dimensional fully developed hydrodynamic turbulence , 2dimensional turbulence in pure electron plasma , hamiltonian systems with long  range interactions to granular systems and systems with strange non  chaotic attractors .",
    "the explicit form of tsallis entropy reads @xmath160 \\ , , \\;\\;\\;\\ ; q > 0 \\ , .",
    "\\label{tsallis1}\\ ] ] this form indicates that @xmath161 is a positive and concave function in @xmath31 . in the limiting case @xmath162 one",
    "has @xmath163 .",
    "in addition , ( [ tsallis1 ] ) obeys a peculiar non  extensivity rule @xmath164 it might be proved that the following axioms uniquely specify tsallis entropy  @xcite :    1 .   for a given integer @xmath19 and",
    "given @xmath20 ( @xmath21 ) , @xmath165 is a continuous with respect to all its arguments . 2 .",
    "for a given integer @xmath19 , @xmath166 takes its largest value for @xmath24 ( @xmath25 ) .",
    "3 .   for a given @xmath27 ; @xmath167 with +   + @xmath168 , +   + and @xmath30 ( distribution @xmath31 corresponds to the experiment @xmath32 ) .",
    "4 .   @xmath169 .",
    "note that these axioms bear remarkable similarity to the axioms of rnyi s entropy .",
    "only the axiom of additivity is altered .",
    "we keep here the linear mean but generalize the additivity law .",
    "in fact , the additivity law in axiom  3 is the jackson sum ( or @xmath0additivity ) of @xmath0calculus .",
    "the jackson basic number @xmath170_q$ ] of quantity @xmath171 is defined as @xmath170_q = ( q^x -1)/(q-1)$ ] .",
    "this implies that for two quantities @xmath171 and @xmath172 the jackson basic number @xmath173_q = [ x]_q + [ y]_q + ( q-1)[x]_q[y]_q$ ] .",
    "the connection with axiom  3 is established when @xmath174 .",
    "the former axiomatics might be viewed as the @xmath0deformed extension of shannon s information theory .",
    "obviously , in the @xmath162 limit the jackson sum reduces to ordinary @xmath175 and above axioms boil down to shannon  khinchin axioms of classical information theory  @xcite .",
    "emergence of @xmath0deformed structure allows to formalize many calculations .",
    "for instance , using the @xmath0logarithm  @xcite , i.e. , @xmath176 , tsallis entropy immediately equals to the @xmath0deformed shannon s entropy ( again after @xmath174 ) , i.e. @xmath177 the interested reader may find some further applications of @xmath0calculus in non  extensive statistics , e.g. , in  @xcite    let us finally add a couple of comments .",
    "firstly , it is possible to show  @xcite that rnyi s entropy prescribes in a natural way the renormalization for tsallis entropy in cases when the pdf is absolutely continuous .",
    "this might be achieved by analytically continuing the result for renormalized rnyi entropy from the complex neighborhood of @xmath43 to the entire right half of the complex plane  @xcite .",
    "thus , if @xmath178 is the corresponding pdf then @xmath179 extension to multifratals is more delicate as a possible non  analytic behavior of @xmath140 and @xmath180 invalidates the former argument of analytic continuation .",
    "these `` phase transitions '' are certainly an interesting topic for further investigation .",
    "secondly , note that tsallis and rnyi s entropies are monotonic functions of each other and thus both are maximized by the same @xmath31 .",
    "this particularly means that whenever one uses maxent approach ( e.g. , in thermodynamics , image processing or pattern recognition ) both entropies yield the same results .",
    "in this lecture we have reviewed some information  theoretic aspects of generalized statics of rnyi and tsallis .",
    "major part of the lecture - section  2 - was dedicated to rnyi s entropy .",
    "we have discussed the information  theoretic foundations of rnyi s information measure and its applicability to systems with continuous pdf s .",
    "the latter include systems with both smooth ( usually part of @xmath181 ) and fractal sample spaces .",
    "particular attention was also paid to currently much studied multifractal systems .",
    "we have shown how the rnyi parameter @xmath0 is related to multifractal singularity spectrum and how rnyi s entropy provides a unified framework for all unifractal shannon entropies .    in cases",
    "when the physical system is described by a parametric family of probability distributions one can construct a riemannian metric over this probability space - information metric .",
    "such a metric is then a natural measure of diversity between two populations .",
    "we have shown that when one employs rnyi s information measure then the information metric turns out to be fisher  rao metric ( or fisher s information matrix ) .    in section  3",
    "we have dealt with tsallis entropy . because detailed discussions of various aspects of tsallis statistics are presented in other lectures of this series we have confined ourselves to only those characteristics of tsallis entropy which make it interesting from information  theory point of view .",
    "we have shown how the @xmath0additive extension of original shannon ",
    "khinchin postulates of information theory gives rise to @xmath0deformed shannon s measure of information - tsallis entropy .",
    "we would like to thank to organizers of the piombino workshop dice2002 on  decoherence , information , complexity and entropy \" for their kind invitation .",
    "it is a pleasure to thank to prof c.  tsallis and dr a.  kobrin for reading a preliminary draft of this paper , and for their comments .",
    "the work was supported by the esf network coslab and the jsps fellowship .",
    "jaynes , _ information theory and statistical mechanics _ , vol .  3 , ed . , k.w .",
    "ford , ( w.a.benjamin , inc . ,",
    "new york , 1963 ) ; _ papers on probability and statistics and statistical physics _",
    "( d.reidel publishing company , boston , 1983 ) .                                                            c.  tsallis , j.  stat",
    "* 52 * ( 1988 ) 479 ; braz .  j.  phys .",
    "* 29 * ( 1999 ) 1 ; e.m.f .",
    "curado and c.  tsallis , j.  phys .",
    "a :  math .  gen . * 24 * ( 1991 ) l69 ; c.  tsallis , r.s .",
    "mandes and a.r .",
    "plastino , physica a  * 261 * ( 1998 ) 534 ; http://tsallis.cat.cbpf.br/biblio.htm ."
  ],
  "abstract_text": [
    "<S> in this lecture we present a discussion of generalized statistics based on rnyi s , fisher s and tsallis s measures of information . the unifying conceptual framework which we employ here </S>",
    "<S> is provided by information theory . </S>",
    "<S> important applications of generalized statistics to systems with ( multi)fractal structure are examined . </S>"
  ]
}