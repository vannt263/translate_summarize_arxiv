{
  "article_text": [
    "symbolic methods have been used in the study of dynamical systems from the earliest days , most notably kolmogorov and sinai s  @xcite use of metric entropy as a dynamical invariant , which spawned a significant mathematical industry in symbolic dynamics .",
    "fraser  @xcite applied information theoretical concepts to construct useful algorithms and criteria for time - delay embeddings .",
    "stationarity , the notion that a system may be modeled well without time as an explicit parameter , is a prerequisite for the vast majority of nonlinear data analysis techniques .",
    "only recently has there been some effort in constructing useful hypothesis tests suitable for realistic chaotic and nonlinear dynamical data.@xcite in this work , we advocate a symbolic approach , on account of computational ease , and the connection to well - studied and powerful techniques of data compression heretofore rarely used in the physics literature which justify our statistical assumptions .",
    "comparing information in the symbolic dynamics observed from time series has a variety of uses besides stationarity tests  @xcite .",
    "one that we mention here is a time - reversibility test : is the symbol stream statistically the same as its time - reversed version ?",
    "this is important because gaussian linear processes produce time - reversible time series , thus rejecting time - reversibility in an observed symbol stream implies that the data can not be from that sort of process .",
    "we have a series of symbols , either quantized from continuous - valued observations or directly measured , observed in discrete time : @xmath0 , each symbol from some alphabet @xmath1 re - expressed as integers @xmath2 . in symbolic data analysis ,",
    "the distribution of multi - symbol words provides information about time - dependent structure and correlation , just as , with continuous nonlinear data , time - delay embedding provides a vector space revealing dynamical information .    a first attempt at a stationarity test would be to apply the classical @xmath3-test to observed counts of distinct multi - symbol words observed in ( say ) the front and back halves of the data .",
    "unfortunately , the assumption underlying this inference  that each datum is randomly and _ independently _ drawn from some distribution is not true in realistic dynamical data .",
    "short time correlations in physical data strongly couple symbols near in time ; thus naive application of such tests fail miserably , usually towards spurious rejection .",
    "indeed , dynamical dependence makes it difficult to construct a proper statistical null test for any hypothesis which allows chaotic or general nonlinear data in the null class , and few examples of this sort exist .",
    "this work proposes a test procedure which quantifies whether two observed symbol streams have `` the same dynamics '' and its statistical significance , even in the presence of serial correlation and dependence .",
    "the algorithm is computationally rapid and does not require monte carlo simulation .",
    "there are two phases : construction of a symbolic predictive model , and the evaluation of a combination of classical statistics , this time on data constructed to be nearly independent .    a model based on an universal data compression algorithm factors out learnable dependence .",
    "good compressors learn the conditional dependencies of symbols characteristic of the source process , thus less new information need be transmitted to reproduce the input data , assuming the decompresser can reconstruct the same model using the transmitted symbols . fundamental results of information theory  @xcite require that optimally compressed data are independent : this is the central theoretical justification for our subsequent application of classical statistical inference , and we feel one of the most useful concepts outside the specific application presented here .",
    "our model for the symbolic dynamics is a `` context tree '' : the recent symbols in the stream themselves define the state , known here as the _ context _ ; contexts are analogous to the states reconstructed by time - delay embedding in conventional nonlinear dynamical analysis .",
    "context tree modeling is a prominent contemporary development in the data compression field .",
    "we describe elsewhere  @xcite other applications to nonlinear dynamics but in the present paper we specialize to stationarity testing .",
    "the tree structure accumulates the statistics of observed symbol vectors down to maximum depth @xmath4 , with distinct branches corresponding to distinct symbols of alphabet @xmath1 which occurred at prior times .",
    "the top node corresponds to no history , the first @xmath5 nodes a one - dimensional reconstruction of the most recent symbol , their @xmath6 descendents a 2-dimensional reconstruction of the two most recent and so forth .",
    "naturally one only constructs the non - empty nodes .",
    "each node stores @xmath5 integers which record the distribution of every symbol @xmath7 that occurred immediately after its particular context @xmath8 . from this information",
    "we can estimate the conditional probability for seeing the next symbol at every step : @xmath9 . observed data provides a data - based estimate of the emission , and thus , transition , probabilities .    in principle",
    "we can build a context tree to arbitrary depth : the most recently seen symbol can have a context that goes back to the start of the data stream .",
    "it is possible to construct such a tree efficiently ( in time and space linear in the number of data  @xcite ) but it is not useful to use the leaves directly to estimate conditional probabilities , since each leaf will have only one observation recorded .",
    "we need a balance between greater depth , which will reveal more structure , and larger numbers of observations at the nodes , which will give greater robustness against noise : the ubiquitous over - fitting issue .",
    "there are various methods of either selecting a specific subtree  @xcite or blending  @xcite different subtrees , all of which have varying compression properties .",
    "we use a state selection algorithm which is quite convenient for our stationarity test .",
    "we do not claim it is necessarily the state - of - the - art compressor , though its performance usually appears to be competitive on physical time - series data .    at each time step @xmath10 , the time series history",
    "so far selects a set of possible contexts , or `` excited nodes '' of the tree , namely all nodes reachable by following branches which match the symbol history .",
    "we use an additional criterion to select from among the the excited nodes the `` encoding node '' at time @xmath10 , which contains the estimated @xmath11 to be employed .",
    "if one were literally compressing data , one would feed the successive estimated @xmath12 for the actual symbols @xmath7 into an arithmetic coder , a well - studied algorithmic device  @xcite which generates the output bit stream with total code length at most two bits greater than @xmath13 . at a node with observed counts @xmath14",
    ", we use the krichevsky - trofimov  @xcite estimator , @xmath15 for symbol value @xmath16 .",
    "@xmath12 must be estimated before the new future symbol is included in the counts , because the decoder must be able to reconstruct the symbol statistics from previously processed input at every stage .",
    "there is no separate model or dictionary sending step . in our application",
    ", this is not essential and it is possible to envisage batch encoders , which differ in detail but not in principle from what we describe here .",
    "our method of encoding selects a specific encoding node for any symbol using a `` predictive minimum description length ( mdl ) '' principle due to rissanen and later improved by others  @xcite .",
    "each node stores a differential code length @xmath17 , the difference in code lengths which would have been emitted had the symbol been encoded using estimated distributions @xmath12 at itself ( @xmath18 ) , or code length @xmath19 estimated using the parent s counts .",
    "one descends the excited nodes from the top down , computing @xmath20 , summing over _ all _ of the current node s children s value of @xmath21 .",
    "when this sum first becomes negative ( assigning @xmath22 to nonexistent child nodes ) we have found the encoding node .",
    "the selection process has found the shortest matching context for which it would have been cheaper to have encoded past data using that context than longer matching contexts .",
    "after encoding the current symbol , one updates @xmath21 for each excited node : @xmath23 the first expression using the node s parent s counts and the second using the node s own counts .",
    "note that @xmath21 is a quantity maintained for each node independent of any previous choice of encoding node .",
    "lastly , the conditional counts @xmath24 for all excited nodes are appropriately incremented with the new symbol , and new branches of the tree grafted for histories never seen before .",
    "it is important that the three phases be carried out in this particular sequence , repeating all three for each new symbol to encode .",
    "notice that , unlike fixed markov modeling or the simplest version of time - delay embedding , the number of past symbols which contribute to predicting the future in a context tree is not uniform .",
    "some past histories need to be deeply examined because there , long - past history influences the future significantly , whereas for other past histories , there is less need , either because future evolution is more unpredictable or there has been less information previously observed regarding those histories . in this respect",
    ", contexts are like `` variable embeddings ''  @xcite .",
    "after encoding all the symbols we carry out the stationarity test .",
    "the overall goal , answering the question `` do two data sets appear to arise from the same underlying dynamical system '' , translates to combining hypothesis tests performed at each encoding node regarding whether the encodings observed for both data sets ( the distribution of future symbols actually encoded ) could have come from a single underlying probability distribution @xmath25 , and if any apparent difference is statistically significant . at encoding contexts",
    ", we may use standard tests because these events ought to be nearly independent using a good compression algorithm .",
    "( a perfectly compressed data stream would be indistinguishable from a stream of independent random bernoulli bits under any statistical test . )    at any node @xmath26 , we have recorded the frequency with which symbol @xmath27 was encoded here , @xmath28 in the first set and @xmath29 in the second .",
    "( note that @xmath30 , the latter accumulating frequencies whenever a context was excited . ) assuming independence , the statistic @xmath31 with @xmath32 follows the standard @xmath3 distribution with @xmath33 degrees of freedom under the null hypothesis that both empirical probability distributions came from the same underlying distribution@xcite .",
    "given the value of @xmath3 and the degrees of freedom , standard numerical algorithms provide a likelihood @xmath34 asymptotically uniform @xmath35 under the null .",
    "small values of @xmath34 reject the null at the given significance level , e.g. @xmath36 .",
    "the asymptotic distribution of the @xmath3 test used in the computation of @xmath34 becomes increasingly inaccurate as the number of observations decreases .",
    "thus for @xmath37 ( an arbitrary cutoff ) we instead use a combinatorial test for differences in proportions , called _ fisher s exact test_. as the test is much simpler in the @xmath38 case , we keep the observation for the most frequent symbol ( bin @xmath39 which achieves @xmath40 ) and merge the others into @xmath41 , resulting in four quantities conventionally expressed in a `` contingency table '' , with cumulative row and column sums : @xmath42 under the null that the difference in proportions between @xmath39 and @xmath43 counts is independent of being in set 1 and 2 , the probability for seeing any particular table with the given marginal sums is : @xmath44 one directly enumerates all tables with the given observed marginals ( only a 1-d sum for a @xmath45 table ) and sums @xmath46 for every table with a difference in proportions at least as great as that observed@xcite , resulting in a likelihood @xmath34 for accepting the null hypothesis at this node .",
    "we combine these @xmath47 likelihoods , each measuring some aspect of of the same null hypothesis , into a single overall test . under the null ,",
    "the quantity @xmath48 is @xmath3 distributed with @xmath49 degrees of freedom , from which we compute our final @xmath50 , again uniform in @xmath51 under the null .",
    "especially small values of @xmath50 imply a small likelihood that this level of difference would have been observed had the two symbol datasets been generated by the same underlying dynamical process .",
    "this completes our desired test procedure .",
    "we first test the accuracy of the null hypothesis .",
    "we produced an ensemble of 1000 time series from the @xmath52 coordinate of the `` lorenz 84 '' attractor : a tiny geophysical model with attractor dimension @xmath53  @xcite .",
    "figure [ fig : lorstatio ] shows the distribution of @xmath50 comparing the first and second halves of each set , demonstrating @xmath50 is close to uniform @xmath54 .",
    "this is a stringent requirement and shows the success of our independence assumption , as it is difficult to get a high - quality null distribution with complicated arbitrarily correlated chaotic data in the null class . with this number of data ,",
    "the test is also quite powerful .",
    "we demonstrate discrimination power with a set of pressure data from an experimental model of a `` fluidized bed reactor '' @xcite .",
    "this experimental system consists of a vertical cylindrical tube of granular particles excited from below by an externally input gaseous flow . in some parameter regimes",
    "( `` slugging '' ) , the particles exhibit complex motion which appears to be a combination of collective low - dimensional bulk dynamics and small - scale high - dimensional turbulence of the individual particles  @xcite .",
    "the observed variable was an azimuthally averaged pressure difference between two vertically separated taps . figure  [ fourbeds ] shows portions of time - delay embedding of orbits sections of the dataset taken at the same experimental parameters , and one when the flow was boosted by 5% .",
    "the change in the attractor is rather subtle and difficult to reliably diagnose by eye .",
    "figure  [ bednulltest ] shows the result of calculating @xmath50 on a data set whose flow was increased at the midpoint . as the alphabet size increased and hypothesized breakpoint approached the true value of 50% , the strength of the rejection increased , @xmath55 .",
    "even the binary alphabet case showed a significant rejection of the null . on data",
    "taken in stationary conditions @xmath50 fluctuates randomly in @xmath51 , as expected .",
    "the southern oscillation index , the normalized pressure difference between tahiti and darwin , is a proxy for the el nino southern oscillation , as ocean temperature influences atmospheric dynamics .",
    "the period from mid 1990 to 1995 exhibited an anomalously sustained period of el nino - like conditions ( fig .",
    "[ fig : soi ] ) , perhaps indicative of global climate change .",
    "one statistical analysis  @xcite found such an anomaly quite unlikely assuming stationarity , but another group  @xcite used a different analysis and found it significantly more likely to be a chance fluctuation . both papers used traditional linear forecasting models , with the difference centered around an auto - correlation based correction for serial correlation to arbitrarily reduce the degrees of freedom .",
    "we applied our algorithm to the 3-month moving average soi ( binary symbolized ) testing the 5.4 year period in question against the rest of the series , with a resulting @xmath56 , meaning that one would expect to see a region this anomalous by chance every 540 years .",
    "the result is closer to those of  @xcite than  @xcite but we certainly do not want to take any particular position regarding climate ; rather , we wish to point out an application for our method where correcting for serial correlation automatically is useful .",
    "recent work has successfully used a distance in the symbolic space to fit unknown parameters of a physically motivated continuous model to observed data , including substantial observational and dynamic noise all in one framework , the situation where fitting models directly is difficult or unreliable .",
    "tang et al  @xcite first proposed minimizing over free parameters the difference between an observed distribution of symbol words and that produced by discretizing some proposed model s output .",
    "daw et al  @xcite successfully used employed this technique to fit experimental internal combustion engine measurements to a low - dimensional dynamical model .",
    "the optimization target was a euclidean norm in @xcite and a chi - squared distance in @xcite . on account of serial correlation ,",
    "a true hypothesis test confirming or rejecting the apparent compatibility of observed data to best - fitting model was not possible in those works .",
    "we feel that our current method ought to provide a more intelligent and less ad hoc optimization goal , either by maximizing average @xmath57 or perhaps minimizing the code length of the physical model s output , encoded using the symbolic model learned from the observed data .",
    "j. rissanen , ieee trans .",
    "infor . theory , * 29 * 656664 ( 1983 ) ; s. bunton `` a percolating state selector for suffix - tree context models '' proceedings data compression conference 1997 , 3241 , ed",
    ". j. a. storer ; m. cohn , ( ieee comput .",
    "press , los alamitos , 1997 )          the @xmath3 analytics degrade for small bin counts . for those bins ,",
    "we merge any symbols whose expected count  for either set one or two  is less than five , reducing the degrees of freedom appropriately .",
    "if , after merging , there remain fewer than two symbols passing this criterion , then this node is wholly excluded .    in this discrete case ,",
    "the sum will encounter tables exactly as likely as the observed one ( such as the observed table itself ) ; the summed @xmath46 for these tables is weighted by a uniform random deviate @xmath58 ."
  ],
  "abstract_text": [
    "<S> we construct a statistic and null test for examining the _ stationarity _ of time - series of discrete symbols : whether two data streams appear to originate from the same underlying unknown dynamical system , and if any difference is statistically significant . </S>",
    "<S> using principles and computational techniques from the theory of data compression , the method intelligently accounts for the substantial serial correlation and nonlinearity found in realistic dynamical data , problems which bedevil naive methods . </S>",
    "<S> symbolic methods are computationally efficient and robust to noise . </S>",
    "<S> we demonstrate the method on a number of realistic experimental datasets . </S>"
  ]
}