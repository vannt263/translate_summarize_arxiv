{
  "article_text": [
    "it is known that the usual uncertainty relations , as given by the heinserberg uncertainty principle , @xmath0 ( which are based on standard deviations , @xmath1 and @xmath2 ) frequently encounter serious difficulties @xcite .",
    "the best examples are the cases of probability distributions for which these deviations lose their usefulness ( being , for example , divergent ) .",
    "it was therefore argued that one should base the formulation of these relations on the information theory approach ( see , for example , discussion in @xcite and references therein ) . in this way",
    "one avoids the above mentioned problems .",
    "the price to be paid is , however , the fact that the information theory approach depends on the type of information measure used , which amounts to dependence on the type of information entropy defining this measure . examples of shannon , rnyi and tsallis information entropies used for this purpose are presented , for example , in @xcite , @xcite and @xcite , respectively ( for more information see references therein ) .",
    "let us notice that the entropic inequality relations involve sums of entropies and are quite different from the standard uncertainty relations .",
    "in standard uncertainty relations the product @xmath3 is strictly determined ( i.e. , @xmath2 is given by @xmath4 and vice versa ) for a given distribution function and can not take any values as will be the case further on below .",
    "the uncertainty relation such as @xmath5 is not a statement about the accuracy of our measuring instruments .",
    "in contrast , entropic uncertainty relations do depend on the accuracy of the measurement as they explicitly contain the area of the phase space determined by the resolution of the measuring instruments . in this paper",
    "we shall revisit , in section [ sec : ii ] , uncertainty relations emerging from tsallis entropy @xcite and discuss them in detail .",
    "our main result is present in section [ sec : iii ] in which we derive the new entropy saturation function .",
    "section [ sec : iv ] contains our summary .",
    "let us define probability distributions associated with the measurements of momentum ( @xmath6 ) and position ( @xmath7 ) of a quantum particle in a pure state as @xmath8 where indices @xmath9 and @xmath10 run from @xmath11 to @xmath12 and the fourier transform is defined with the physical normalization ( @xmath13 is planck constant ) , i.e. , @xmath14 from the probability distributions @xmath15 and @xmath16 we may construct the corresponding tsallis entropies @xcite , which measure the uncertainties in momentum and position spaces : @xmath17 in the respective limits of @xmath18 entropies @xmath19 reduce to the shannon entropy @xmath20 : @xmath21 for which the uncertainty relation has been derived long ago and takes the form of a condition imposed on the sum of entropies @xcite , @xmath22 ( where @xmath23 is the basis of natural logarithm ) .",
    "the relation ( [ eq : spsx ] ) reflects the fact that , although probability distributions in eq .",
    "( [ eq : px ] ) correspond to different observables , nevertheless they describe the same quantum physical state and therefore must be , in general , correlated . recently eq .",
    "( [ eq : spsx ] ) has been generalized to the case of renyi entropies @xcite , @xmath24 for which one gets @xcite that @xmath25 where parameters @xmath26 and @xmath27 are assumed to be positive and constrained by the relation @xmath28    let us now proceed to the case of nonextensive tsallis entropy and derive for it the corresponding entropic inequality .",
    "our approach differs from that already presented in @xcite in that we are attempting from the very beginning to provide condition on the sum of the corresponding @xmath29 entropies ( where @xmath30 , respectively ) . to do this",
    "we shall start from the following babenko - beckner inequality relation @xcite , @xmath31^{\\frac{1}{\\alpha}}\\!\\!\\ !",
    "\\leq \\left ( \\frac{2\\alpha}{h}\\right)^{\\frac{-1}{2\\alpha}}\\!\\!\\!\\left ( \\frac{2\\beta}{h}\\right)^{\\frac{1}{2\\beta}}\\!\\!\\!\\left [ ( \\delta x)^{1-\\beta } \\sum_lx_l^{\\beta } \\right]^{\\frac{1}{\\beta } } \\label{eq : inequality}\\ ] ] which has been also used in @xcite ( cf . , eq . ( 21 ) there ) .",
    "parameters @xmath26 and @xmath27 satisfy condition ( [ eq : condition ] ) and we shall assume at this moment that @xmath32 . notice that ( [ eq : condition ] ) means that the effects of nonextensivity in @xmath7 and @xmath6 spaces , as measured by @xmath26 and @xmath27 , can not be identical ( @xmath33 only for @xmath34 and @xmath35 , i.e. , in the case of the shannon entropy ) . the more general case of independent indices has been recently discussed in @xcite but we shall not comment on it here . the inequality ( [ eq : inequality ] ) can be rewritten as @xmath36 where @xmath37 or as @xmath38 where we have used the first order homogenous entropy defined as ( as before , @xmath39 ) : @xmath40   \\label{eq : a}\\ ] ] ( it has been firstly introduced in @xcite , and then subsequently given a complete characterization in @xcite ) . by making use of eq .",
    "( [ eq : condition ] ) one can rewrite eq .",
    "( [ eq:111 ] ) in the following way : @xmath41 = \\nonumber\\\\ & = & \\frac{\\alpha}{1-\\alpha}\\left [ \\left ( \\frac{\\beta}{\\alpha } \\right)^{\\frac{1}{2\\alpha } } \\left ( \\frac{2\\beta}{h } \\delta x \\delta p \\right)^{\\frac{\\alpha -1}{\\alpha } } - 1 \\right ] .",
    "\\label{eq:222}\\end{aligned}\\ ] ]    ) of limitations of the sum of entropies on the size of cell in phase space , @xmath42 .",
    "results for renyi and shannon entropies are practically indistinguishable ( to expose both the large and small values we used the linear - log scale here , in this case for both entropies one gets straight lines ) .",
    "horizontal lines indicate the corresponding bounds for limitation imposed on tsallis entropies for which @xmath43.,width=302 ]    further discussion depends on whether defined by eq .",
    "( [ eq : etanew ] ) coefficient @xmath44 is smaller or greater than unity . in the first case",
    "@xmath45 we can now write the lhs of eq .",
    "( [ eq:222 ] ) as @xmath46 make use of the fact that for @xmath47 and @xmath48 one has ( see eq .",
    "( [ eq : condition ] ) ) , @xmath49 and get finally that @xmath50 .",
    "\\label{eq : hphx}\\ ] ] it means that in this case one has @xmath51 .",
    "\\label{eq : txtp1}\\end{aligned}\\ ] ] in the second case , for @xmath52 , one gets @xmath53 .",
    "\\label{eq : txtp2}\\end{aligned}\\ ] ] both results generalize eq .",
    "( [ eq : spsx ] ) , the result for shannon entropy , to which they converge when @xmath54 and @xmath55 .    to extend the above results to the case of @xmath56",
    "one should use the same babenko - beckner inequality @xcite as in eq .",
    "( [ eq : inequality ] ) but with the role of @xmath6 and @xmath7 interchanged , @xmath57 ) .",
    "the dependence of the limitations on the sum of entropies on the size of cell in phase space is visualized in fig .",
    "the inequalities presented above are , so far , purely mathematical in the sense that they allow for _ negative _ lower limits for the corresponding sum of entropies . for example , the rhs of equation eq .",
    "( [ eq : spsx ] ) is positive only for @xmath58 because the sum of entropies must be _ non - negative _ therefore the condition provided by eq .",
    "( [ eq : spsx ] ) only works together with eq .",
    "( [ eq : lims ] ) .",
    "the same reasoning can be performed for the remain two entropies leading to the following additional requirements for the products @xmath59 : @xmath60    the occurrence of negative values in the limitations of the sum of entropies , @xmath61 , is the consequence of the fact that for large values of @xmath62 we have @xmath63 .",
    "we shall now look at this problem more closely .",
    "evaluating @xmath64 we use the integral form of jensen s inequalities ( which state that for convex functions the values of the function at the average point does not exceeds the average value of the function , the opposite being true for concave functions @xcite ) : @xmath65^{\\alpha } & \\leq &   \\frac{1}{\\delta p } \\int^{(k+1)\\delta p}_{k\\delta p } dp \\left [ \\tilde{\\rho}(p)\\right]^{\\alpha } , \\label{eq : d2a}\\\\ \\!\\!\\!\\!\\ ! \\left [ \\frac{1}{\\delta x } \\int^{(l+1)\\delta x}_{l\\delta x } dx \\rho ( x)\\right]^{\\beta } & \\ge &   \\frac{1}{\\delta x } \\int^{(l+1)\\delta x}_{l\\delta x } dx \\left [ \\rho(x)\\right]^{\\beta } ,    \\label{eq : d2b}\\end{aligned}\\ ] ] where the probability densities are @xmath66 and @xmath67 ( cf .",
    "@xcite for more details ) .",
    "it turns out that differences between the left ( @xmath68 ) and the right ( @xmath69 ) hand sides of inequalities ( [ eq : d2a ] ) and ( [ eq : d2b ] ) can be rather substantial and can introduces serious bias to the results .",
    "its magnitude can be estimated using taylor expansion : @xmath70 \\sim f\\left [ e\\left ( p_k\\right)\\right ] + \\frac{1}{2}f''\\left [ e\\left ( p_k\\right)\\right ] var\\left ( p_k\\right)$ ] where @xmath71 @xcite . however ,",
    "this is possible only when the functional form of probability @xmath15 is known . in fig .",
    "[ fig2 ] we show an example of the ration @xmath72 for inequality ( [ eq : d2a ] ) calculated for a gaussian shape of @xmath73 .",
    "the increase in discrepancy is clearly visible . instead of this",
    ", we shall now demonstrate that the accuracy of jensen s inequality can be dramatically improved by a suitable change of variables .",
    "namely , we consider the following maps , which transform an infinite interval to some fine interval , @xmath74 : @xmath75 where @xmath76 is scale parameter such that @xmath77 . in new variables",
    "the probability densities are given by @xmath78\\left| \\frac{dx}{dt_x}\\right| = \\rho ( x ) \\frac{s_x}{\\left ( \\left| t_x\\right| - 1\\right)^2 } , \\label{eq : tx}\\\\ \\tilde{\\rho}\\left ( t_p\\right ) & = & \\tilde{\\rho}\\left[\\ p\\left(t_p\\right ) \\right]\\left|",
    "\\frac{dp}{dt_p}\\right| = \\tilde{\\rho } ( p ) \\frac{s_p}{\\left ( \\left| t_p\\right| - 1\\right)^2}. \\label{eq : tp}\\end{aligned}\\ ] ] using these new variables in analogous way as in eqs .",
    "( [ eq : d2a ] ) and ( [ eq : d2b ] ) , one can now write the following inequalities : @xmath79^{\\alpha}\\!\\!\\!\\!\\ ! & \\leq&\\!\\ !",
    "\\frac{1}{\\delta t_p } \\int^{(k+1)\\delta t_p}_{k\\delta t_p}\\!\\!\\!\\!\\ !",
    "\\left [ \\tilde{\\rho}\\left ( t_p\\right)\\right]^{\\alpha } dt_p , \\label{eq : d4a}\\\\ \\!\\!\\!\\!\\!\\!\\!\\!\\!\\ ! \\left [ \\frac{1}{\\delta t_x } \\int^{(l+1)\\delta t_x}_{l\\delta t_x } \\!\\!\\!\\!\\!\\rho \\left ( t_x\\right ) dt_x \\right]^{\\beta}\\!\\!\\!\\!\\ ! & \\ge&\\!\\ ! \\frac{1}{\\delta t_x } \\int^{(l+1)\\delta t_x}_{l\\delta t_x}\\!\\!\\!\\!\\",
    "! \\left [ \\rho\\left ( t_x\\right)\\right]^{\\beta}dt_x .",
    "\\label{eq : d4b}\\end{aligned}\\ ] ] the ratio @xmath72 for inequality ( [ eq : d4a ] ) calculated for a gaussian shape of @xmath73 is shown in fig .",
    "[ fig2 ] and , as one can see , grows very weakly with the bin size @xmath80 .",
    "( calculated for @xmath81 and @xmath82 ) . broken",
    "line shows the ratio of the right to the left side of inequality ( [ eq : d2a ] ) whereas the solid line shows the same for inequality ( [ eq : d4a ] ) where we put @xmath83.,width=302 ]    establishing this finding , let us now proceed to a calculation of the corresponding entropic inequalities using new variables .",
    "the probabilities corresponding to ( [ eq : px ] ) are now : @xmath84 ( notation is such that primed quantities correspond to using the new variable @xmath85 and non - primed ones to the standard variable @xmath86 ) . whereas before , in variables @xmath87 , @xmath9 and @xmath10 were varying from @xmath11 to @xmath12 , now @xmath88 and @xmath89 where @xmath90 and @xmath91 . for these probabilities",
    "we get the following equivalent of eq .",
    "( [ eq : ineq ] ) , @xmath92 where now @xmath93 notice that now @xmath94 _ always _ , this means that we shall no more encounter problems with negative values for the limits of the sum of entropies .    to be more specific , notice that for entropies @xmath95/(\\alpha - 1)$ ] and @xmath96/(\\beta - 1)$ ] we have that @xmath97 \\label{eq : d8}.\\end{aligned}\\ ] ] putting @xmath98 and proceeding to shannon entropy one gets that ( in bits ) @xmath99\\frac{1}{\\ln 2 } - 1 .",
    "\\label{eq : d9}\\ ] ] it is interesting to note that for the uniform distribution in the variable @xmath100 one has @xmath101\\frac{1}{\\ln 2 } + 2\\ ] ] bits of information ( the number of bins are @xmath102 and @xmath103 ) . the interval of variability of @xmath104 is narrow and equals @xmath105 bits ( this is the difference between the maximal and minimal limitations ) .",
    "let us notice at this point that , whereas inequalities ( [ eq : spsx ] ) , ( [ eq : rxrp ] ) , ( [ eq : txtp1 ] ) and ( [ eq : txtp2 ] ) are for the fixed values of intervals @xmath106 and @xmath80 , the inequality ( [ eq : d8 ] ) is for the fixed values of intervals @xmath107 and @xmath108",
    ". formal recalculation of these intervals results in their dependence on @xmath9 and @xmath10 , they are not fixed anymore but their values change in the following way : for @xmath109 one has @xmath110 } , \\label{eq : d10}\\ ] ] whereas for @xmath111 one has @xmath112 } .",
    "\\label{eq : d11}\\ ] ] notice that because eq .",
    "( [ eq : d3 ] ) is an odd function of @xmath113 and has rotational symmetry with respect to the origin , one has exactly the same intervals @xmath114 and @xmath115 for the negative values of @xmath9 , @xmath116 , and for its positive values , @xmath117 , where @xmath118 .",
    "the natural question is then in what way , for some given fixed intervals @xmath119 , one should choose intervals @xmath120 in inequality ( [ eq : d8 ] ) .",
    "if we take the maximal values of intervals @xmath115 ( corresponding to @xmath121 or @xmath122 ) and make use of the fact that now @xmath123 then we obtain that the right - hand - side of inequality ( [ eq : d8 ] ) will be limited by @xmath124 \\ge \\nonumber\\\\   & & \\!\\!\\!\\!\\!\\ ! \\!\\!\\!\\!\\ ! \\!\\!\\!\\ge \\frac{1}{\\alpha -1}\\left [ 1 - \\left ( \\frac{\\beta}{\\alpha}\\right)^{\\frac{1}{2\\alpha } } \\left(2\\beta \\frac{\\delta x \\delta p}{h + \\delta x\\delta p}\\right)^{\\frac{\\alpha -1}{\\alpha}}\\right ] .",
    "\\label{eq : d13}\\end{aligned}\\ ] ] actually , taking exactly the results of ( [ eq : d10 ] ) and ( [ eq : d11 ] ) we would obtain equality , not inequality in eq .",
    "( [ eq : d13 ] ) . however , in such case one would not have at the same time @xmath125 and @xmath126 ( or @xmath127 and @xmath128 ) .",
    "choosing intervals corresponding to @xmath129 or @xmath122 ( for which we have maximal interval @xmath108 equal to @xmath130 or , equivalently , minimal interval @xmath80 equal to @xmath131 ) we can see that for each @xmath132 ( given by eq .",
    "( [ eq : d5 ] ) ) we have @xmath15 ( given by eq .",
    "( [ eq : px ] ) ) , which satisfies the inequality @xmath133 and for @xmath134 we have @xmath135 ( see ) and ( [ eq : tp ] ) . for exact transformation of bins ( given by eqs .",
    "( [ eq : d10 ] ) and ( [ eq : d11 ] ) ) we have the same probabilities , @xmath136 . choosing maximal interval @xmath137 , for the same probability densities we have inequality @xmath138 .",
    "notice now that the number of bins in both cases is different and that probabilities in this inequality are not for the same bin number @xmath9 but for the corresponding position in variables @xmath113 and @xmath85 . ] ) .",
    "however , because the number of bins in both cases is different , there will be some @xmath15 left for which there will be no @xmath132 assigned . nevertheless , one can construct some new @xmath15 s by performing division of @xmath132 .",
    "preserving always the normalization , i.e. , assuming that @xmath139 one has that increasing the number of divisions leads , for @xmath134 , to decreasing of @xmath140 ( and to its increasing for @xmath141 ) .",
    "let us notice that @xmath142 . putting therefore @xmath143 for @xmath144 and @xmath145 one gets , for @xmath146 , that @xmath147 , which leads to @xmath148 . repeating this procedure of dividing the @xmath149 ( and possibly also dividing again @xmath15 ) one",
    "gets that @xmath150 , where @xmath151 . actually , this inequality is true also for @xmath152 . to see it ,",
    "let us consider @xmath153 and @xmath154 for probabilities @xmath155 with normalization @xmath156 .",
    "we can write @xmath157 .",
    "because for the physically motivated probability distributions both @xmath158 and @xmath159 , we get that @xmath160 . ]",
    "@xmath161 we have then for entropies @xmath162/(\\alpha - 1)$ ] and @xmath163/(\\alpha - 1)$ ] the inequality that @xmath164 .",
    "analogously , repeating the above procedure for probabilities @xmath16 and @xmath165 one gets that @xmath166 ( where now , according to ( [ eq : condition ] ) , @xmath167 ) .",
    "the limitation for the left - hand - side of inequality ( [ eq : d8 ] ) is then @xmath168    finally , for the tsallis entropy we can write : @xmath169 .",
    "\\label{eq : d16}\\ ] ] for @xmath170 we recover the previous result given by eq .",
    "( [ eq : txtp1 ] ) whereas in the limit of @xmath171 we have @xmath172 > 0 .",
    "\\label{eq : d17}\\ ] ] notice that now the limit is always positive .    in the limit @xmath98",
    "we get a limitation for shannon entropy , which now reads @xmath173 for large intervals , i.e , for @xmath174 , one gets @xmath175 .",
    "- \\ln 2 $ ] .",
    "( [ eq : d17 ] ) extends therefore hirschman uncertainty to tsallis entropy .",
    "] it should be noticed that this new inequality ( [ eq : d18 ] ) for shannon entropy is stronger ( for all values of interval @xmath176 ) than the previous limitation ( [ eq : spsx ] ) derived in @xcite .",
    "the new dependencies of limitations on different entropies on the size of the phase space cell @xmath177 are displayed in fig .",
    "[ fig3 ] ) and ( [ eq : d18 ] ) let us consider , as example , gaussian probability densities ( corresponding to the gaussian wave - function ) with dispersion equal unity , for which we can evaluate @xmath178 and @xmath179 using definition ( [ eq : s ] ) .",
    "for @xmath180 the sum of entropies is equal to @xmath181 .",
    "the lower limit provided in this case by eq .",
    "( [ eq : d18 ] ) is @xmath182 , which is much stringent than the corresponding limit @xmath183 provided by eq .",
    "( [ eq : spsx ] ) ( notice that it leads to negative value of the sum of entropies ) . ] .    ) of limitations of the sum of entropies on the size of cell in phase space , @xmath177",
    ". results for renyi and shannon entropies are practically indistinguishable .",
    "horizontal lines indicate corresponding limits for the sum of tsallis entropies for which @xmath184/(\\alpha -              1)$].,width=377 ]",
    "we have derived uncertainty relations based on tsallis entropy . we have also found a positively defined function that saturates the so called entropic inequalities for entropies characterizing physical states under consideration , cf .",
    "( [ eq : d16 ] ) . in case of shannon entropy ( eq .",
    "( [ eq : d18 ] ) ) the limit provided is more stringent than the previously derived . formally ,",
    "our results show that changing @xmath185 to @xmath186 one avoids ( in all cases : shannon , renyi and tsallis entropies ) the appearance of unphysical negative values in the entropy bounds .",
    "let us close with the remark that in some applications of the nonextensive statistics the nonextensivity parameter @xmath187 ( corresponding to @xmath26 and @xmath27 here ) describes intrinsic fluctuations existing in the physical system under consideration @xcite .",
    "this raises an interesting question of the possible existence of such relations also in the applications mentioned above .",
    "in particular there still remains the question of whether our results will survive the other choices of inequality used in ( [ eq : inequality ] ) and/or in the case of independent indices ( @xmath26 , @xmath27 ) as discussed in @xcite .",
    "we plan to address this point elsewhere .",
    "+        cf .",
    ", for example , i.biaynicki-birula in _ foundations of probability and physics _ , eds .",
    "g. adenier , c. a. fuchs , and a. yu .",
    "khrennikov , _ aip conference proceedings _ 889 , 52 ( 2007 ) and references therein .",
    "c.tsallis , _ j. stat .",
    "phys . _ * 52 * ( 1988 ) 479 ; cf .",
    "also c.tsallis , _ chaos , solitons and fractals _ * 13 * ( 2002 ) 371 , _ physica _",
    "* a305 * ( 2002 ) 1 and in _ nonextensive statistical mechanics and its applications _ ,",
    "s.abe and y.okamoto ( eds . ) , lecture notes in physics lpn560 , springer ( 2000 ) .",
    "for an updated bibliography on this subject see http://tsallis.cat.cbpf.br/biblio.htm .",
    "j.l.jensen , _ acta mathematica _ * 30 * , 175 ( 1906 ) .",
    "see also : g.hardy , j.l.littlewood and g.plya , _ inequalities _ , cambridge univ .",
    "press , cambridge , 1934 or j.m.steele , _ the cauchy - schwartz master class _ , cambridge unic . press , cambridge , 2004 ,"
  ],
  "abstract_text": [
    "<S> uncertainty relations emerging from the tsallis entropy are derived and discussed . </S>",
    "<S> in particular we found a positively defined function that saturates the so called entropic inequalities for entropies characterizing the physical states under consideration . </S>"
  ]
}