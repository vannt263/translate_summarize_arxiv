{
  "article_text": [
    "two ubiquitous aspects of large - scale data analysis are that the data often have heavy - tailed properties and that diffusion - based or spectral - based methods are often used to identify and extract structure of interest . in the absence of strong assumptions on the data , popular distribution - independent methods such as those based on the vc dimension fail to provide nontrivial results for even simple learning problems such as binary classification in these two settings . at root ,",
    "the reason is that in both of these situations the data are formally very high dimensional and that ( without additional regularity assumptions on the data ) there may be a small number of `` very outlying '' data points . in this paper , we develop distribution - dependent learning methods that can be used to provide dimension - independent sample complexity bounds for the maximum margin version of the binary classification problem in these two popular settings . in both cases , we are able to obtain nearly optimal linear classification hyperplanes since the distribution - dependent tools we employ are able to control the aggregate effect of the `` outlying '' data points . in particular , our results will hold even though the data may be infinite - dimensional and unbounded .",
    "spectral - based kernels have received a great deal of attention recently in machine learning for data classification , regression , and exploratory data analysis via dimensionality reduction  @xcite .",
    "consider , for example , laplacian eigenmaps  @xcite and the related diffusion maps  @xcite . given a graph @xmath0 ( where this graph could be constructed from the data represented as feature vectors ,",
    "as is common in machine learning , or it could simply be a natural representation of a large social or information network , as is more common in other areas of data analysis ) , let @xmath1 be the eigenfunctions of the normalized laplacian of @xmath2 and let @xmath3 be the corresponding eigenvalues .",
    "then , the diffusion map is the following feature map @xmath4 and laplacian eigenmaps is the special case when @xmath5 . in this case , the support of the data distribution is unbounded as the size of the graph increases ; the vc dimension of hyperplane classifiers is @xmath6 ; and thus existing results do not give dimension - independent sample complexity bounds for classification by empirical risk minimization ( erm ) .",
    "moreover , it is possible ( and indeed quite common in certain applications ) that on some vertices @xmath7 the eigenfunctions fluctuate wildly  even on special classes of graphs , such as random graphs @xmath8 , a non - trivial uniform upper bound stronger than @xmath9 on @xmath10 over all vertices @xmath7 does not appear to be known .",
    "even for maximum margin or so - called `` gap - tolerant '' classifiers , defined precisely in section  [ sxn : background ] and which are easier to learn than ordinary linear hyperplane classifiers , the existing bounds of vapnik are not independent of the number @xmath11 of nodes . on the vc dimension of gap - tolerant classifiers applied to the diffusion map feature space corresponding to a graph with @xmath11 nodes .",
    "( recall that by lemma  [ lem : vap1 ] below , the vc dimension of the space of gap - tolerant classifiers corresponding to a margin @xmath12 , applied to a ball of radius @xmath13 is @xmath14 . )",
    "of course , although this bound is quadratic in the number of nodes , vc theory for ordinary linear classifiers gives an @xmath9 bound . ]",
    "a similar problem arises in the seemingly very - different situation that the data exhibit heavy - tailed or power - law behavior .",
    "heavy - tailed distributions are probability distributions with tails that are not exponentially bounded  @xcite .",
    "such distributions can arise via several mechanisms , and they are ubiquitous in applications  @xcite .",
    "for example , graphs in which the degree sequence decays according to a power law have received a great deal of attention recently .",
    "relatedly , such diverse phenomenon as the distribution of packet transmission rates over the internet , the frequency of word use in common text , the populations of cities , the intensities of earthquakes , and the sizes of power outages all have heavy - tailed behavior .",
    "although it is common to normalize or preprocess the data to remove the extreme variability in order to apply common data analysis and machine learning algorithms , such extreme variability is a fundamental property of the data in many of these application domains .",
    "there are a number of ways to formalize the notion of heavy - tailed behavior for the classification problems we will consider , and in this paper we will consider the case where the _ magnitude _ of the entries decays according to a power law .",
    "( note , though , that in appendix  [ sxn : learnht ] , we will , for completeness , consider the case in which the probability that an entry is nonzero decays in a heavy - tailed manner . ) that is , if @xmath15 represents the feature map , then @xmath16 for some absolute constant @xmath17 , with @xmath18 . as in the case with spectral kernels , in this heavy - tailed situation",
    ", the support of the data distribution is unbounded as the size of the graph increases , and the vc dimension of hyperplane classifiers is @xmath6 .",
    "moreover , although there are a small number of `` most important '' features , they do not `` capture ''",
    "most of the `` information '' of the data .",
    "thus , when calculating the sample complexity for a classification task for data in which the feature vector has heavy - tailed properties , bounds that do not take into account the distribution are likely to be very weak .    in this paper , we develop distribution - dependent bounds for problems in these two settings . clearly , these results are of interest since vc - based arguments fail to provide nontrivial bounds in these two settings , in spite of ubiquity of data with heavy - tailed properties and the widespread use of spectral - based kernels in many applications . more generally , however , these results are of interest since the distribution - dependent bounds underlying them provide insight into how better to deal with heterogeneous data with more realistic noise properties .",
    "our first main result provides bounds on classifying data whose magnitude decays in a heavy - tailed manner . in particular , in the following theorem we show that if the weight of the @xmath19 coordinate of random data point is less than @xmath20 for some @xmath21 , then the number of samples needed before a maximum - margin classifier is approximately optimal with high probability is independent of the number of features .",
    "[ heavy - tailed data ] [ thm : learn_heavytail ] let the data be heavy - tailed in that the feature vector : @xmath22 satisfy @xmath23 for some absolute constant @xmath17 , with @xmath18 .",
    "let @xmath24 denote the riemann zeta function .",
    "then , for any @xmath25 , if a maximum margin classifier has a margin @xmath26 , with probability more than @xmath27 , its risk is less than @xmath28 where @xmath29 hides multiplicative polylogarithmic factors .",
    "this result follows from a bound on the annealed entropy of gap - tolerant classifiers in a hilbert space that is of independent interest .",
    "in addition , it makes important use of the fact that although individual elements of the heavy - tailed feature vector may be large , the vector has bounded moments .",
    "our second main result provides bounds on classifying data with spectral kernels .",
    "in particular , in the following theorem we give dimension - independent upper bounds on the sample complexity of learning a nearly - optimal maximum margin classifier in the feature space of the diffusion maps .",
    "[ spectral kernels ] [ thm : learn_spectral ] let the following diffusion map be given : @xmath30 where @xmath31 are normalized eigenfunctions ( whose @xmath32 ) norm is @xmath33 , @xmath34 being the uniform distribution ) , @xmath35 are the eigenvalues of the corresponding markov chain and @xmath36 .",
    "then , for any @xmath25 , if a maximum margin classifier has a margin @xmath26 , with probability more than @xmath27 , its risk is less than @xmath37 where @xmath29 hides multiplicative polylogarithmic factors .    as with the proof of our main heavy - tailed learning result , the proof of our main spectral learning result makes essential use of an upper bound on the annealed entropy of gap - tolerant classifiers . in applying it ,",
    "we make important use of the fact that although individual elements of the feature vector may fluctuate wildly , the norm of the diffusion map feature vector is bounded .    as a side remark ,",
    "note that we are not viewing the feature map in theorem  [ thm : learn_spectral ] as necessarily being either a random variable or requiring knowledge of some marginal distribution  as might be the case if one is generating points in some space according to some distribution ; then constructing a graph based on nearest neighbors ; and then doing diffusions to construct a feature map .",
    "instead , we are thinking of a data graph in which the data are adversarially presented , _",
    "e.g. _ , a given social network is presented , and diffusions and/or a feature map is then constructed .",
    "these two theorems provides a dimension - independent ( _ i.e. _ , independent of the size @xmath11 of the graph and the dimension of the feature space ) upper bound on the number of samples needed to learn a maximum margin classifier , under the assumption that a heavy - tailed feature map or the diffusion map kernel of some scale is used as the feature map .",
    "as mentioned , both proofs ( described below in sections  [ sxn : gaphs_htapp ] and  [ sxn : gaphs_skapp ] ) proceed by providing a dimension - independent upper bound on the annealed entropy of gap - tolerant classifiers in the relevant feature space , and then appealing to theorem  [ thm : vapnik ] ( in section  [ sxn : background ] ) relating the annealed entropy to the generalization error . for this bound on the annealed entropy of these gap - tolerant classifiers ,",
    "we crucially use the fact that @xmath38 is bounded , even if @xmath39 is unbounded as  @xmath40 .",
    "that is , although bounds on the individual entries of the feature map do not appear to be known , we crucially use that there exist nontrivial bounds on the magnitude of the feature vectors .",
    "since this bound is of more general interest , we describe it separately .",
    "the distribution - dependent ideas that underlie our two main results ( in theorems  [ thm : learn_heavytail ] and  [ thm : learn_spectral ] ) can also be used to bound the sample complexity of a classification task more generally under the assumption that the expected value of a norm of the data is bounded , _",
    "i.e. _ , when the magnitude of the feature vector of the data in some norm has a finite moment . in more detail",
    ":    * let @xmath41 be a probability measure on a hilbert space @xmath42 , and let @xmath43 . in theorem  [ thm : ddb_hs ] ( in section  [ sxn : gaphs_annent ] ) , we prove that if @xmath44 , then the annealed entropy of gap - tolerant classifiers ( defined in section  [ sxn : background ] ) in @xmath42 can be upper bounded in terms of a function of @xmath45 , @xmath12 , and ( the number of samples ) @xmath25 , independent of the ( possibly infinite ) dimension of @xmath42 .    it should be emphasized that the assumption that the expectation of some moment of the norm of the feature vector is bounded is a _ much _ weaker condition than the more common assumption that the largest element is bounded , and thus this result is likely of more general interest in dealing with heterogeneous and noisy data .",
    "for example , similar ideas have been applied recently to the problem of bounding the sample complexity of learning smooth cuts on a low - dimensional manifold  @xcite .    to establish this result",
    ", we use a result ( see lemma  [ lem : vap1 ] in section  [ sxn : gaphs_vcdim ] . ) that the vc dimension of gap - tolerant classifiers in a hilbert space when the margin is @xmath12 over a bounded domain such as a ball of radius @xmath13 is bounded above by @xmath46 .",
    "such bounds on the vc dimension of gap - tolerant classifiers have been stated previously by vapnik  @xcite . however , in the course of his proof bounding the vc dimension of a gap - tolerant classifier whose margin is @xmath12 over a ball of radius @xmath13 ( see  @xcite , page  353 . ) , vapnik states , without further justification , that due to symmetry the set of points in a ball that is extremal in the sense of being the hardest to shatter with gap - tolerant classifiers is the regular simplex .",
    "attention has been drawn to this fact by burges ( see  @xcite , footnote  20 . ) , who mentions that a rigorous proof of this fact seems to be absent . here",
    ", we provide a new proof of the upper bound on the vc dimension of such classifiers without making this assumption .",
    "( see lemma  [ lem : vap1 ] in section  [ sxn : gaphs_vcdim ] and its proof . ) hush and scovel  @xcite provide an alternate proof of vapnik s claim ; it is somewhat different than ours , and they do not extend their proof to banach spaces .",
    "the idea underlying our new proof of this result generalizes to the case when the data need not have compact support and where the margin may be measured with respect to more general norms .",
    "in particular , we show that the vc dimension of gap - tolerant classifiers with margin @xmath12 in a ball of radius @xmath13 in a banach space of rademacher type @xmath47 $ ] and type constant @xmath48 is bounded above by @xmath49 , and that there exists a banach space of type @xmath50 ( in fact @xmath51 ) for which the vc dimension is bounded below by @xmath52 .",
    "( see lemmas  [ lem : banach_ub ] and  [ lem : banach_lb ] in section  [ sxn : gapbs_vcdim ] . )",
    "using this result , we can also prove bounds for the annealed entropy of gap - tolerant classifiers in a banach space .",
    "( see theorem  [ thm : ddb_bs ] in section  [ sxn : gapbs_annent ] . ) in addition to being of interest from a theoretical perspective , this result is of potential interest in cases where modeling the relationship between data elements as a dot product in a hilbert space is too restrictive , and thus this may be of interest , _",
    "e.g. _ , when the data are extremely sparse and heavy - tailed .",
    "gap - tolerant classifiers  see section  [ sxn : background ] for more details  are useful , at least theoretically , as a means of implementing structural risk minimization ( see , _ e.g. _ , appendix a.2 of  @xcite ) . with gap - tolerant classifiers ,",
    "the margin @xmath53 is fixed before hand , and does not depend on the data .",
    "e.g. _ ,  @xcite . with maximum margin classifiers , on the other hand ,",
    "the margin is a function of the data . in spite of this difference ,",
    "the issues that arise in the analysis of these two classifiers are similar .",
    "for example , through the fat - shattering dimension , bounds can be obtained for the maximum margin classifier , as shown by shawe - taylor _ et al . _",
    "@xcite . here",
    ", we briefly sketch how this is achieved .",
    "let @xmath54 be a set of real valued functions .",
    "we say that a set of points @xmath55 is @xmath56shattered by @xmath54 if there are real numbers @xmath57 such that for all binary vectors @xmath58 and each @xmath59=\\{1,\\ldots , s\\}$ ] , there is a function @xmath60 satisfying , f_(x_i ) = \\ {    ll > t_i + , & + < t_i - , &    . for each @xmath61 , the _ fat shattering dimension",
    "_ @xmath62 of the set @xmath54 is defined to be the size of the largest @xmath56shattered set if this is finite ; otherwise it is declared to be infinity .",
    "note that , in this definition , @xmath63 can be different for different @xmath64 , which is not the case in gap - tolerant classifiers .",
    "however , one can incorporate this shift into the feature space by a simple construction .",
    "we start with the following definition of a banach space of type @xmath50 with type constant @xmath48 .",
    "[ banach space , type , and type constant ] [ def : rad ] a _ banach space _ is a complete normed vector space . a banach space @xmath65 is said to have ( rademacher ) type @xmath50 if there exists @xmath66 such that for all @xmath11 and @xmath67 _ t^p _",
    "i=1^n x_i_^p . the smallest @xmath48 for which the above holds with @xmath50 equal to the type ,",
    "is called the type constant of  @xmath65 .    given a banach space @xmath68 of type @xmath50 and type constant @xmath48 , let @xmath69 consist of all tuples @xmath70 for @xmath71 and @xmath72 , with the norm @xmath73 noting that if @xmath68 is a banach space of type @xmath50 and type constant @xmath48 ( see sections  [ sxn : gapbs_prelim ] and  [ sxn : gapbs_vcdim ] ) , one can easily check that @xmath69 is a banach space of type @xmath50 and type constant @xmath74 .    in our distribution - specific setting , we can not control the fat - shattering dimension , but we can control the logarithm of the expected value of @xmath75 for any constant @xmath76 by applying theorem  [ thm : ddb_bs ] to @xmath69 . as seen from lemma 3.7 and corollary 3.8 of the journal version of  @xcite ,",
    "this is all that is required for obtaining generalization error bounds for maximum margin classification . in the present context",
    ", the logarithm of the expected value of the exponential of the fat shattering dimension of linear @xmath33-lipschitz functions on a random data set of size @xmath25 taken i.i.d from @xmath41 on @xmath68 is bounded by the annealed entropy of gap - tolerant classifiers on @xmath69 with respect to the push - forward @xmath77 of the measure @xmath41 under the inclusion @xmath78 .",
    "this allows us to state the following theorem , which is an analogue of theorem  4.17 of the journal version of  @xcite , adapted using theorem  [ thm : ddb_bs ] of this paper .",
    "[ thm : margin_bs ] let @xmath79 .",
    "suppose inputs are drawn independently according to a distribution @xmath41 be a probability measure on a banach space @xmath68 of type @xmath50 and type constant @xmath48 , and @xmath80 .",
    "if we succeed in correctly classifying @xmath25 such inputs by a maximum margin hyperplane of margin @xmath53 , then with confidence @xmath27 the generalization error will be bounded from above by @xmath81 where @xmath29 hides multiplicative polylogarithmic factors involving @xmath82 and @xmath53 .",
    "specializing this theorem to a hilbert space , we have the following theorem as a corollary .",
    "[ thm : margin_hs ] let @xmath79 .",
    "suppose inputs are drawn independently according to a distribution @xmath41 be a probability measure on a hilbert space @xmath42 , and @xmath83 .",
    "if we succeed in correctly classifying @xmath25 such inputs by a maximum margin hyperplane with margin @xmath53 , then with confidence @xmath27 the generalization error will be bounded from above by @xmath84 where @xmath29 hides multiplicative polylogarithmic factors involving @xmath85 and @xmath53 .",
    "note that theorem  [ thm : margin_hs ] is an analogue of theorem  4.17 of the journal version of  @xcite , but adapted using theorem  [ thm : ddb_hs ] of this paper .",
    "in particular , note that this theorem does not assume that the distribution is contained in a ball of some radium @xmath13 , but instead it assumes only that some moment of the distribution is bounded .      in the next section ,",
    "section  [ sxn : background ] , we review some technical preliminaries that we will use in our subsequent analysis . then , in section  [ sxn : gaphs ] , we state and prove our main result for gap - tolerant learning in a hilbert space , and we show how this result can be used to prove our two main theorems in maximum margin learning . then , in section  [ sxn : gapbs ] , we state and prove an extension of our gap - tolerant learning result to the case when the gap is measured with respect to more general banach space norms ; and then , in sections  [ sxn : discussion ] and  [ sxn : conclusion ] we provide a brief discussion and conclusion . finally , for completeness , in appendix  [ sxn : learnht ] , we will provide a bound for exact ( as opposed to maximum margin ) learning in the case in which the probability that an entry is nonzero ( as opposed to the value of that entry ) decays in a heavy - tailed manner .",
    "in this paper , we consider the supervised learning problem of binary classification , _ i.e. _ , we consider an input space @xmath86 ( _ e.g. _ , a euclidean space or a hilbert space ) and an output space @xmath87 , where @xmath88 , and where the data consist of pairs @xmath89 that are random variables distributed according to an unknown distribution .",
    "we shall assume that for any @xmath90 , there is at most one pair @xmath91 that is observed .",
    "we observe @xmath25 i.i.d .",
    "pairs @xmath92 sampled according to this unknown distribution , and the goal is to construct a classification function @xmath93 which predicts @xmath87 from @xmath86 with low probability of error .    whereas an ordinary linear hyperplane classifier consists of an oriented hyperplane , and points are labeled @xmath94 , depending on which side of the hyperplane they lie , a",
    "_ gap - tolerant classifier _ consists of an oriented hyperplane and a margin of thickness @xmath12 in some norm .",
    "any point outside the margin is labeled @xmath95 , depending on which side of the hyperplane it falls on , and all points within the margin are declared `` correct , '' without receiving a @xmath95 label .",
    "this latter setting has been considered in  @xcite ( as a way of implementing structural risk minimization ",
    "apply empirical risk minimization to a succession of problems , and choose where the gap @xmath12 that gives the minimum risk bound ) .",
    "the _ risk _ @xmath96 of a linear hyperplane classifier @xmath97 is the probability that @xmath98 misclassifies a random data point @xmath99 drawn from @xmath41 ; more formally , @xmath100 $ ] . given a set of @xmath25 labeled data points @xmath101 , the _ empirical risk _",
    "@xmath102 of a linear hyperplane classifier @xmath97 is the frequency of misclassification on the empirical data ; more formally , @xmath103 $ ] , where @xmath104 $ ] denotes the indicator of the respective event .",
    "the risk and empirical risk for gap - tolerant classifiers are defined in the same manner .",
    "note , in particular , that data points labeled as `` correct '' do not contribute to the risk for a gap - tolerant classifier , _",
    "i.e. _ , data points that are on the `` wrong '' side of the hyperplane but that are within the @xmath12 margin are not considered as incorrect and do not contribute to the risk .    in classification , the ultimate goal is to find a classifier that minimizes the true risk , _",
    "i.e. _ , @xmath105 , but since the true risk @xmath96 of a classifier @xmath98 is unknown , an empirical surrogate is often used . in particular ,",
    "_ empirical risk minimization ( erm ) _ is the procedure of choosing a classifier @xmath98 from a set of classifiers @xmath106 by minimizing the empirical risk @xmath107 .",
    "the consistency and rate of convergence of erm  see  @xcite for precise definitions  can be related to uniform bounds on the difference between the empirical risk and the true risk over all @xmath108 .",
    "there is a large body of literature on sufficient conditions for this kind of uniform convergence .",
    "for instance , the vc dimension is commonly - used toward this end .",
    "note that , when considering gap - tolerant classifiers , there is an additional caveat , as one obtains uniform bounds only over those gap - tolerant classifiers that do not contain any data points in the margin  the appendix a.2 of  @xcite addresses this issue .    in this paper ,",
    "our main emphasis is on the annealed entropy :    [ annealed entropy ] [ def : an_ent ] let @xmath41 be a probability measure supported on a vector space @xmath42 .",
    "given a set @xmath106 of decision rules and a set of points @xmath109 , let @xmath110 be the number of ways of labeling @xmath111 into positive and negative samples such that there exists a gap - tolerant classifier that predicts _ incorrectly _ the label of each @xmath112 .",
    "given the above notation , h_ann^(k ) : = _ p^k ( z_1 ,  , z_k ) is the annealed entropy of the classifier @xmath106 with respect to @xmath41 .",
    "note that although we have defined the annealed entropy for general decision rules , below we will consider the case that @xmath106 consists of linear decision rules .    as the following theorem states",
    ", the annealed entropy of a classifier can be used to get an upper bound on the generalization error .",
    "this follows from theorem @xmath113 in @xcite and a remark on page 198 of  @xcite .",
    "note that the class @xmath114 is itself random , and consequently , @xmath115 is the supremum over a random class .",
    "[ thm : vapnik ]",
    "let @xmath114 be the family of all gap - tolerant classifiers such that no data point lies inside the margin .",
    "then , @xmath116      <   8 \\exp{\\left(\\left({h_{ann}^\\lambda(\\ell)}\\right ) - \\frac{\\epsilon^2\\ell}{32}\\right)}\\ ] ] holds true , for any number of samples @xmath25 and for any error parameter @xmath117 .",
    "the key property of the function class that leads to uniform bounds is the sublinearity of the logarithm of the expected value of the `` growth function , '' which measures the number of distinct ways in which a data set of a particular size can be split by the function class .",
    "a finite vc bound guarantees this in a distribution - free setting .",
    "the annealed entropy is a distribution - specific measure , _",
    "i.e. _ , the same family of classifiers can have different annealed entropies when measured with respect to different distributions . for a more detailed exposition of uniform bounds in the context of gap - tolerant classifiers ,",
    "we refer the reader to  ( @xcite , appendix a.2 ) .",
    "note also that normed vector spaces ( such as hilbert spaces and banach spaces ) are relevant to learning theory for the following reason .",
    "data are often accompanied with an underlying metric which carries information about how likely it is that two data points have the same label .",
    "this makes concrete the intuition that points with the same class label are clustered together .",
    "many algorithms can not be implemented over an arbitrary metric space , but require a linear structure .",
    "if the original metric space does not have such a structure , as is the case when classifying for example , biological data or decision trees , it is customary to construct a feature space representation , which embeds data into a vector space",
    ". we will be interested in the commonly - used hilbert spaces , in which distances in the feature space are measure with respect to the @xmath118 distance ( as well as more general banach spaces , in section  [ sxn : gapbs ] ) .    finally , note that our results where the margin is measured in @xmath118 can be transferred to a setting with kernels . given a kernel @xmath119 , it is well known that linear classification using a kernel @xmath119 is equivalent to mapping @xmath120 onto the functional @xmath121 and then finding a separating halfspace in the reproducing kernel hilbert space ( rkhs ) which is the hilbert space generated by the functionals of the form @xmath121 . since the span of any finite set of points in a hilbert space can be isometrically embedded in @xmath118 , our results hold in the setting of kernel - based learning as well , when one first uses the feature map @xmath122 and works in the rkhs .",
    "in this section , we state and prove theorem  [ thm : ddb_hs ] , our main result regarding an upper bound for the annealed entropy of gap - tolerant classifiers in @xmath118 .",
    "this result is of independent interest , and it was used in a crucial way in the proof of theorems  [ thm : learn_heavytail ] and  [ thm : learn_spectral ] .",
    "we start in section  [ sxn : gaphs_annent ] with the statement and proof of theorem  [ thm : ddb_hs ] , and then in section  [ sxn : gaphs_vcdim ] we bound the vc dimension of gap - tolerant classifiers over a ball of radius @xmath13 .",
    "then , in section  [ sxn : gaphs_htapp ] , we apply these results to prove our main theorem on learning with heavy - tailed data , and finally in section  [ sxn : gaphs_skapp ] , we apply these results to prove our main theorem on learning with spectral kernels .",
    "the following theorem is our main result regarding an upper bound for the annealed entropy of gap - tolerant classifiers .",
    "the result holds for gap - tolerant classification in a hilbert space , _ i.e. _ , when the distances in the feature space are measured with respect to the @xmath118 norm .",
    "analogous results hold when distances are measured more generally , as we will describe in section  [ sxn : gapbs ] .",
    "[ annealed entropy ; upper bound ; hilbert space ] [ thm : ddb_hs ] let @xmath41 be a probability measure on a hilbert space @xmath42 , and let @xmath43 .",
    "if @xmath44 , then then the annealed entropy of gap - tolerant classifiers in @xmath42 , where the gap is @xmath12 , is @xmath123    let @xmath25 independent , identically distributed ( i.i.d ) samples @xmath124 be chosen from @xmath125 .",
    "we partition them into two classes : @xmath126 and @xmath127 our objective is to bound from above the annealed entropy @xmath128 $ ] . by lemma  [ lem : submul ] , @xmath129 is sub - multiplicative .",
    "therefore , @xmath130 taking an expectation over @xmath25 i.i.d samples from @xmath125 , @xmath131 \\leq { \\mathbb{e}}[n^{\\lambda}(x_1 , \\dots , x_{\\ell - k})n^{\\lambda}(y_1 , \\dots , y_k)].\\ ] ] now applying lemma  [ lem : vap1 ] , we see that @xmath131     \\leq { \\mathbb{e}}[n^{\\lambda}(x_1 , \\dots , x_{\\ell -k})(k+1)^{r^2/\\delta^2 + 1 } ]   .\\ ] ] moving @xmath132 outside this expression , @xmath131     \\leq { \\mathbb{e}}[n^{\\lambda}(x_1 , \\dots , x_{\\ell -k})](k+1)^{r^2/\\delta^2 + 1 }    .\\ ] ] note that @xmath133 is always bounded above by @xmath134 and that the random variables @xmath135 $ ] are i.i.d .",
    "let @xmath136 $ ] , and note that @xmath137 is the sum of @xmath25 independent bernoulli variables .",
    "moreover , by markov s inequality , @xmath138   \\,\\ , \\leq \\,\\ , \\frac{{\\mathbb{e}}[\\|z_i\\|^2]}{r^2 }   , \\ ] ] and therefore @xmath139 .",
    "in addition , @xmath140 \\leq { \\mathbb{e}}[2^{\\ell - k}].\\ ] ] let @xmath141 $ ] denote an indicator variable . @xmath142 $ ] can be written as @xmath143 } ] = ( 1+\\rho)^\\ell \\leq   e^{\\rho \\ell}.\\ ] ] putting everything together , we see that [ eqn : jensen1 ] [ n^(z_1 ,  , z _ ) ] ( ( ) ^2 + ( + 1 ) ( + 1 ) ) .",
    "if we substitute @xmath144 , it follows that @xmath145    \\\\     & \\leq & \\left(\\ell^{\\frac{1}{2}}\\left(\\frac{r}{\\delta}\\right ) + 1\\right ) ( 1+\\ln(\\ell+1 ) )   .\\end{aligned}\\ ] ]    for ease of reference , we note the following easily established fact about @xmath146 .",
    "this lemma is used in the proof of theorem  [ thm : ddb_hs ] above and theorem  [ thm : ddb_bs ] below .",
    "[ lem : submul ] let @xmath147 be a partition of the data @xmath148 into two parts . then",
    ", @xmath146 is submultiplicative in the following sense : @xmath149    this holds because any partition of @xmath150 into two parts by an element @xmath151 induces such a partition for the sets @xmath152 and @xmath153 , and for any pair of partitions of @xmath152 and @xmath154 , there is at most one partition of @xmath148 that induces them .      as an intermediate step in the proof of theorem  [ thm : ddb_hs ] , we needed a bound on the vc dimension of a gap - tolerant classifier within a ball of fixed radius .",
    "lemma  [ lem : vap1 ] below provides such a bound and is due to vapnik  @xcite .",
    "note , though , that in the course of his proof ( see  @xcite , page 353 . ) , vapnik states , without further justification , that due to symmetry the set of points that is extremal in the sense of being the hardest to shatter with gap - tolerant classifiers is the regular simplex .",
    "attention has also been drawn to this fact by burges ( @xcite , footnote  20 ) , who mentions that a rigorous proof of this fact seems to be absent .",
    "vapnik s claim has since been proved by hush and scovel  @xcite . here",
    ", we provide a new proof of lemma  [ lem : vap1 ] . it is simpler than previous proofs , and in section  [ sxn :",
    "gapbs ] we will see that it generalizes to cases when the margin is measured with norms other than  @xmath118 .    [ vc dimension ; upper bound ; hilbert space ] [ lem : vap1 ] in a hilbert - space , the vc dimension of a gap - tolerant classifier whose margin is @xmath12 over a ball of radius @xmath13 can by bounded above by @xmath155 .",
    "suppose the vc dimension is @xmath11 .",
    "then there exists a set of @xmath11 points @xmath156 in @xmath157 that can be completely shattered using gap - tolerant classifiers .",
    "we will consider two cases , first that @xmath11 is even , and then that @xmath11 is odd .",
    "first , assume that @xmath11 is even , i.e. , that @xmath158 for some positive integer @xmath159 .",
    "we apply the probabilistic method to obtain a upper bound on @xmath11 .",
    "note that for every set @xmath160 $ ] , the set @xmath161 can be separated from @xmath162 using a gap - tolerant classifier .",
    "therefore the distance between the centroids ( respective centers of mass ) of these two sets is greater or equal to @xmath163 . in particular , for each @xmath164 having @xmath165 elements , @xmath166 suppose now that @xmath164 is chosen uniformly at random from the @xmath167 sets of size @xmath159 .",
    "then , @xmath168    \\\\     & = &       k^{-2}\\left\\{\\frac{2k+1}{2k}\\sum_{i=1}^n \\|x_i\\|^2 - \\frac{\\|\\sum_1^n x_i\\|^2}{2k } \\right\\ }    \\\\     & \\leq &   \\frac{4(n+1)}{n^2 } r^2     .\\end{aligned}\\ ] ] therefore , @xmath169 and so @xmath170    next , assume that @xmath11 is odd .",
    "we perform a similar calculation for @xmath171 .",
    "as before , we average over all sets @xmath164 of cardinality @xmath159 the squared distance between the centroid of @xmath172 and the centroid ( center of mass ) of @xmath173 . proceeding as before , @xmath174   \\\\     & = &     \\frac{\\sum_{i=1}^n \\|x_i\\|^2 ( 1 + \\frac{1}{2n } ) -",
    "\\frac{1}{2n}\\|\\sum_{1 \\leq i \\leq n } x_i\\|^2}{k(k+1 ) }   \\\\     & \\leq & \\frac{\\sum_{i=1}^n \\|x_i\\|^2 ( 1 + \\frac{1}{2n})}{k(k+1 ) }   \\\\     & = &     \\frac{4k+3}{2k(2k+1)(k+1)}\\{(2k+1)r^2\\ }   \\\\     & < &     \\frac{4r^2}{n-1 }   .\\end{aligned}\\ ] ] therefore , @xmath175 .      for a random data sample @xmath120 , ^2 & & _ i=1^n ( c i^-)^2 + & & c^2 ( 2 ) , where @xmath176 is the riemann zeta function .",
    "the theorem then follows from theorem  [ thm : margin_hs ] .      a diffusion map for the graph @xmath177 is the feature map that associates with a vertex @xmath120 , the feature vector @xmath178 , when the eigenfunctions corresponding to the top @xmath179 eigenvalues are chosen .",
    "let @xmath34 be the uniform distribution on @xmath180 and @xmath181 .",
    "we note that if the @xmath182 are normalized eigenfunctions , _",
    "i.e. _ , @xmath183 ^2 & = & 1 .",
    "the above inequality holds because the eigenvalues have magnitudes that are less or equal to @xmath33 : @xmath184 the theorem then follows from theorem  [ thm : margin_hs ] .",
    "in this section , we state and prove theorem  [ thm : ddb_bs ] , our main result regarding an upper bound for the annealed entropy of gap - tolerant classifiers in a banach space .",
    "we start in section  [ sxn : gapbs_prelim ] with some technical preliminaries ; then in section  [ sxn : gapbs_vcdim ] we bound the vc dimension of gap - tolerant classifiers in banach spaces over a ball of radius @xmath13 ; and finally in section  [ sxn : gapbs_annent ] we prove theorem  [ thm : ddb_bs ] . we include this result for completeness since it is of theoretical interest ; since it follows using similar methods to the analogous results for hilbert spaces that we presented in section  [ sxn : gaphs ] ; and since this result is of potential practical interest in cases where modeling the relationship between data elements as a dot product in a hilbert space is too restrictive , _ e.g. _ , when the data are extremely sparse and heavy - tailed . for recent work in",
    "machine learning on banach spaces , see  @xcite .",
    "recall the definition of a banach space from definition  [ def : rad ] above .",
    "we next state the following form of the chernoff bound , which we will use in the proof of lemma  [ lem : banach_ub ] below .",
    "[ chernoff bound ] [ lem : chernoff ] let @xmath185 be discrete independent random variables such that @xmath186=0 $ ] for all @xmath64 and @xmath187 for all @xmath64 .",
    "let @xmath188 for all @xmath64 and @xmath189 be the variance of @xmath90",
    ". then @xmath190 \\leq 2e^{-\\lambda^2/4}\\ ] ] for any @xmath191 .",
    "the idea underlying our new proof of lemma  [ lem : vap1 ] ( of section  [ sxn : gaphs_vcdim ] , and that provides an upper bound on the vc dimension of a gap - tolerant classifier in hilbert spaces ) generalizes to the case when the the gap is measured in more general banach spaces",
    ". we state the following lemma for a banach space of type @xmath50 with type constant @xmath48 .",
    "e.g. _ , that @xmath51 for @xmath192 is a banach space of type @xmath193 and type constant @xmath33 .",
    "[ vc dimension ; upper bound ; banach space ] [ lem : banach_ub ] in a banach space of type @xmath50 and type constant @xmath48 , the vc dimension of a gap - tolerant classifier whose margin is @xmath12 over a ball of radius @xmath13 can by bounded above by @xmath194    since a general banach space does not possess an inner product , the proof of lemma  [ lem : vap1 ] needs to be modified here . to circumvent this difficulty",
    ", we use inequality  ( [ ineq : rad ] ) determining the rademacher type of @xmath65 .",
    "this , while permitting greater generality , provides weaker bounds than previously obtained in the euclidean case .",
    "note that if @xmath195 , then by repeated application of the triangle inequality , @xmath196 this shows that if we start with @xmath197 having norm @xmath198 , @xmath199 for all @xmath64 .",
    "the property of being shattered by gap - tolerant classifiers is translation invariant .",
    "then , for @xmath200 $ ] , it can be verified that @xmath201 the rademacher inequality states that [ ineq : rad]_t^p _",
    "i=1^n x_i^p.using the version of chernoff s bound in lemma  [ lem : chernoff ] [ twelve ] [ |_i=1^n _ i| ] 1- 2 e^-^2/4 . we shall denote the above event by @xmath202 .",
    "now , let @xmath197 be @xmath11 points in @xmath65 with a norm less or equal to @xmath13 .",
    "let @xmath203 as before .",
    "@xmath204\\\\                      & \\geq & { \\mathbb{e}}_\\epsilon [ \\|\\epsilon_i ( x_i-\\mu)\\|^p| e_{\\lambda}]\\,\\,{\\mathbb{p}}[e_{\\lambda}]\\\\                      & \\geq & { \\mathbb{e}}_\\epsilon [ ( n - \\lambda^2)^p(2\\delta)^p ( 1 - 2                      e^{-\\lambda^2/4})]\\,\\,\\end{aligned}\\ ] ] the last inequality follows from ( [ eleven ] ) and ( [ twelve ] ) .",
    "we infer from the preceding sequence of inequalities that @xmath205 the above is true for any @xmath206 , by the conditions in the chernoff bound stated in lemma  [ lem : chernoff ] .",
    "if @xmath207 , choosing @xmath208 equal to @xmath113 gives us @xmath209 therefore , it is always true that @xmath210    finally , for completeness , we next state a lower bound for vc dimension of gap - tolerant classifiers when the margin is measured in a norm that is associated with a banach space of type @xmath47 $ ] . since we are interested only in a lower bound , we consider the special case of @xmath211 .",
    "note that this argument does not immediately generalize to banach spaces of higher type because for @xmath212 , @xmath51 has type @xmath213 .",
    "[ vc dimension ; lower bound ; banach space ] [ lem : banach_lb ] for each @xmath47 $ ] , there exists a banach space of type @xmath50 such that the vc dimension of gap - tolerant classifiers with gap @xmath12 over a ball of radius @xmath13 is greater or equal to @xmath214 further , this bound is achieved when the space is @xmath51 .",
    "we shall show that the first @xmath11 unit norm basis vectors in the canonical basis can be shattered using gap - tolerant classifiers , where @xmath215 .",
    "therefore in this case , the vc dimension is @xmath216 .",
    "let @xmath217 be the @xmath218 basis vector . in order to prove that the set @xmath219 is shattered , due to symmetry under permutations , it suffices to prove that for each @xmath159 , @xmath220 can be separated from @xmath221 using a gap - tolerant classifier .",
    "points in @xmath51 are infinite sequences @xmath222 of finite @xmath51 norm .",
    "consider the hyperplane @xmath223 defined by @xmath224 .",
    "clearly , it separates the sets in question .",
    "we may assume @xmath217 to be @xmath225 , replacing if necessary , @xmath159 by @xmath226 .",
    "let @xmath227 clearly , all coordinates @xmath228 of @xmath120 are @xmath229 . in order to get a lower bound on the @xmath51 distance",
    ", we use the power - mean inequality : if @xmath192 , and @xmath230 , @xmath231 this implies that @xmath232 for @xmath233 , the type of @xmath51 is @xmath213  @xcite . since @xmath234 is a decreasing function of @xmath50 in this regime , we do not recover any useful bounds .",
    "the following theorem is our main result regarding an upper bound for the annealed entropy of gap - tolerant classifiers in banach spaces .",
    "note that the @xmath118 bound provided by this theorem is slightly weaker than that provided by theorem  [ thm : ddb_hs ] .",
    "note also that it may seem counter - intuitive that in the case of @xmath118 ( _ i.e. _ , when we set @xmath235 ) , the dependence of @xmath12 is @xmath236 , which is weaker than in the vc bound , where it is @xmath237 .",
    "the explanation is that the bound on annealed entropy here depends on the number of samples @xmath25 , while the vc dimension does not .",
    "therefore , the weaker dependence on @xmath12 is compensated for by a term that in fact tends to @xmath238 as the number of samples  @xmath239 .",
    "[ annealed entropy ; upper bound ; banach space ] [ thm : ddb_bs ] let @xmath41 be a probability measure on a banach space @xmath68 of type @xmath50 and type constant @xmath48 .",
    "let @xmath240 , and let @xmath241 .",
    "if @xmath242 , then the annealed entropy of gap - tolerant classifiers in @xmath68 , where the gap is @xmath12 , is @xmath243    the proof of this theorem parallels that of theorem  [ thm : ddb_hs ] , except that here we use lemma  [ lem : banach_ub ] instead of lemma  [ lem : vap1 ] .",
    "we include the full proof for completeness .",
    "let @xmath25 independent , identically distributed ( i.i.d ) samples @xmath124 be chosen from @xmath125 .",
    "we partition them into two classes : @xmath126 and @xmath127 our objective is to bound from above the annealed entropy @xmath128 $ ] . by lemma  [ lem : submul ] , @xmath129 is sub - multiplicative .",
    "therefore , @xmath130 taking an expectation over @xmath25 i.i.d samples from @xmath125 , @xmath131 \\leq { \\mathbb{e}}[n^{\\lambda}(x_1 , \\dots , x_{\\ell - k})n^{\\lambda}(y_1 , \\dots , y_k)].\\ ] ] now applying lemma  [ lem : banach_ub ] , we see that @xmath131     \\leq { \\mathbb{e}}[n^{\\lambda}(x_1 , \\dots , x_{\\ell -k})(k+1)^{(3tr/\\delta)^{\\frac{p}{p-1}}+ 64 } ]   .\\ ] ] moving @xmath244 outside this expression , @xmath131     \\leq { \\mathbb{e}}[n^{\\lambda}(x_1 , \\dots , x_{\\ell -k})](k+1)^{(3tr/\\delta)^{\\frac{p}{p-1}}+ 64 }    .\\ ] ] note that @xmath133 is always bounded above by @xmath134 and that the random variables @xmath135 $ ] are i.i.d .",
    "let @xmath136 $ ] , and note that @xmath137 is the sum of @xmath25 independent bernoulli variables .",
    "moreover , by markov s inequality , @xmath138   \\,\\ , \\leq \\,\\ , \\frac{{\\mathbb{e}}[\\|z_i\\|^\\gamma]}{r^\\gamma }   , \\ ] ] and therefore @xmath245 .",
    "in addition , @xmath140 \\leq { \\mathbb{e}}[2^{\\ell - k}].\\ ] ] let @xmath141 $ ] denote an indicator variable .",
    "@xmath142 $ ] can be written as @xmath143 } ] = ( 1+\\rho)^\\ell \\leq   e^{\\rho \\ell}.\\ ] ] putting everything together , we see that [ eqn : jensen2 ] [ n^(z_1 ,  , z _ ) ] ( ( ) ^ + ( k+1)(64 + ) ^ ) . by setting @xmath246 and adjusting @xmath13 so that @xmath247 we see that @xmath248 thus , it follows that @xmath249   \\\\    & \\leq & \\left(\\eta^{-\\eta}(1-\\eta)^{-1+\\eta }              \\left(\\frac{\\ell}{\\ln(\\ell+1 ) }              \\left(\\frac{3tr}{\\delta}\\right)^\\gamma\\right)^\\eta + 64\\right )              \\ln(\\ell+1 )   .\\end{aligned}\\ ] ]",
    "in recent years , there has been a considerable amount of somewhat - related technical work in a variety of settings in machine learning .",
    "thus , in this section we will briefly describe some of the more technical components of our results in light of the existing related literature .",
    "* techniques based on the use of rademacher inequalities allow one to obtain bounds without any assumption on the input distribution as long as the feature maps are uniformly bounded .",
    "e.g. _ ,  @xcite .",
    "viewed from this perspective , our results are interesting because the uniform boundedness assumption is not satisfied in either of the two settings we consider , although those settings are ubiquitous in applications . in the case of heavy - tailed data ,",
    "the uniform boundedness assumption is not satisfied due to the slow decay of the tail and the large variability of the associated features . in the case of spectral learning ,",
    "uniform boundedness assumption is not satisfied since for arbitrary graphs one can have localization and thus large variability in the entries of the eigenvectors defining the feature maps . in both case , existing techniques based on rademacher inequalities or vc dimensions fail to give interesting results , but we show that dimension - independent bounds can be achieved by bounding the annealed entropy . * a great deal of work has focused on using diffusion - based and spectral - based methods for nonlinear dimensionality reduction and the learning a nonlinear manifold from which the data are assumed to be drawn  @xcite .",
    "these results are very different from the type of learning bounds we consider here .",
    "for instance , most of those learning results involve convergence to an hypothesized manifold laplacian and not of learning process itself , which is what we consider here .",
    "* work by bousquet and elisseeff  @xcite has focused on establishing generalization bounds based on stability .",
    "it is important to note that their results assume a given algorithm and show how the generalization error changes when the data are changed , so they get generalization results for a given algorithm .",
    "our results make no such assumptions about working with a given algorithm .",
    "* gurvits  @xcite has used rademacher complexities to prove upper bounds for the sample complexity of learning bounded linear functionals on @xmath51 balls .",
    "the results in that paper can be used to derive an upper bound on the vc dimension of gap - tolerant classifiers with margin @xmath12 in a ball of radius @xmath13 in a banach space of rademacher type @xmath47 $ ] .",
    "constants were not computed in that paper , therefore our results do not follow . moreover , our paper contains results on distribution specific bounds which were not considered there .",
    "finally , our paper considers the application of these tools to the practically - important settings of spectral kernels and heavy - tailed data that were not considered there .",
    "we have considered two simple machine learning problems motivated by recent work in large - scale data analysis , and we have shown that although traditional distribution - independent methods based on the vc - dimension fail to yield nontrivial sampling complexity bounds , we can use distribution - dependent methods to obtain dimension - independent learning bounds . in both cases , we take advantage of the fact that , although there may be individual data points that are `` outlying , '' in aggregate their effect is not too large . due to the increased popularity of vector space - based methods ( as opposed to more purely combinatorial methods ) in machine learning in recent years , coupled with the continued generation of noisy and poorly - structured data ,",
    "the tools we have introduced are likely promising more generally for understanding the effect of noise and noisy data on popular machine learning tasks .",
    "in this appendix section , we state and prove a second result for dimension - independent learning from data in which the feature map exhibits a heavy - tailed decay .",
    "the heavy - tailed model we consider here is different than that considered in theorem  [ thm : learn_heavytail ] , and thus we are able to prove bounds for exact ( as opposed to maximum margin ) learning . nevertheless , the techniques are similar , and thus we include this result in this paper for completeness .",
    "consider the following toy model for classifying web pages using keywords .",
    "one approach to this problem could be to associate with each web page the indicator vector corresponding to all keywords that it contains .",
    "the dimension of this feature space is the number of possible keywords , which is typically very large , and empirical evidence indicates that the frequency of words decays in a heavy - tailed manner .",
    "thus the vc dimension of the feature space is very large , and in a distribution - free setting it is not possible to classify data in such a feature space unless the number of samples is of the order of the vc dimension .",
    "more generally , one might be interested in a bipartite graph , _",
    "e.g. _ , an `` advertiser - keyword '' or `` author - to - paper '' graph , in which the nodes are the stated entities and the edges represent some sort of `` interaction '' between the entities , in which case similar issues arise .    here , we show that if the probability that the @xmath19 keyword in the above toy example is present is heavy - tailed as a function of @xmath64 , then the sample complexity of the binary classification problem is dimension - independent .",
    "more precisely , the following theorem provides a dimension - independent ( _ i.e. _ , independent of the size @xmath11 of the graph and the dimension of the feature space ) upper bound on the number of samples needed to learn by erm , with a given accuracy and confidence , a linear hyperplane that classifies heavy - tailed data into positive and negative labels , under the assumption that the probability of the @xmath19 coordinate of a random data point being non - zero is less than @xmath20 for some @xmath21 .",
    "the proof of this result proceeds by providing providing a dimension - independent upper bound on the annealed entropy of the class of linear classifiers in @xmath250 , and then appealing to theorem  [ thm : vapnik ] relating the annealed entropy to the generalization error .",
    "* remark : * note that although the generalization bound provided by the following theorem seems to be pessimistic in @xmath97 , the dependence on @xmath97 is tight , at least as @xmath97 tends to @xmath33 . clearly , when @xmath251 , the expected number of @xmath33 s in a random sample becomes asymptotically equal to @xmath252 , where @xmath11 is the dimension , in which case , we do not expect a sample complexity that is dimension - independent .",
    "[ bounds for heavy - tailed data ] [ thm : learnht_exact ] let @xmath41 be a probability distribution in @xmath250 .",
    "ci^{-\\alpha}$ ] for some absolute constant @xmath17 , with @xmath18 .",
    "then , the annealed entropy of ordinary linear hyperplane classifiers is h_ann^ ( ) & & ( ^+1)consequently , the minimum number of random samples @xmath254 needed to learn , by erm , a classifier whose risk differs from the minimum risk @xmath96 by @xmath255 with probability @xmath256 is less than or equal to 2 ( ( + ) ) ^ ( ( ( + ) ) ^ ) .",
    "let the event that a sample @xmath257 has a non - zero coordinate @xmath258 for some @xmath259 be denoted @xmath260 .",
    "the probability of this event can be bounded as follows . if @xmath261 and  @xmath262 , then [ e_i ] & = & [ k > ^1/ , z_ik 0 ] + & & c_i = k+1^i^- + & & .",
    "we partition the @xmath112 into two classes : @xmath263 and @xmath264 @xmath146 is sub - multiplicative by lemma  [ lem : submul ] .",
    "taking an expectation over @xmath25 i.i.d samples from @xmath41 , [ ( z_1 ,  , z _ ) ] & & [ ( x_1 ,  , x_-m)(y_1 ,  , y_m)]the dimension of the span of @xmath265 is at most @xmath159 , and by a result from vc theory ( @xcite , page 159 ) we have @xmath266 then , @xmath267 \\leq { \\mathbb{e}}[\\nl(x_1 , \\dots , x_{\\ell -m})e m^k].\\ ] ] moving @xmath268 outside this expression , @xmath267     \\leq { \\mathbb{e}}[\\nl(x_1 , \\dots , x_{\\ell -k})]e m^k    .\\ ] ] note that @xmath269 is always bounded above by @xmath134 and that the events @xmath270 are independent identically distributed .",
    "let @xmath271 $ ] , and note that @xmath137 is the sum of @xmath25 independent @xmath50-bernoulli variables .",
    "in addition , @xmath272 \\leq { \\mathbb{e}}[2^{\\ell - k}],\\ ] ] and @xmath142 $ ] can be written as _",
    "i=1^(1 + [ e_i ] ) & = & ( 1+p)^ + & & e^p + & = & e^().putting everything together , we see that @xmath267     \\leq e(\\ell)^k",
    "e^{\\frac{c \\ell\\ , k^{-a + 1}}{\\a - 1 } }    .\\ ] ] since @xmath273 , we see that h_ann^ ( ) & = & [ ( z_1 ,  , z _ ) ] + & & ( ^+1 ) ( ) . in order to obtain sample complexity bounds , we need to apply theorem  [ thm : vapnik ] and substitute the above expression for annealed entropy . for the probability that the error of erm exceeds @xmath274 to be less than @xmath275 ( where @xmath98 is the optimal classifier ) ,",
    "it is sufficient that @xmath25 satisfy @xmath276 for this to be true , it is enough that @xmath277 a calculation shows that @xmath278 is a value of @xmath25 that satisfies the previous expression .                        l.  gurvits .",
    "a note on a scale - sensitive dimension of linear bounded functionals in banach spaces .",
    "in _ proceedings of the 8th international conference on algorithmic learning theory _ , pages 352363 , 1997 .          v.  koltchinskii and d.  panchenko .",
    "rademacher processes and bounding the risk of function learning . in e.",
    "gine , d.  mason , and j.  wellner , editors , _ high dimensional probability ii _ , pages 443459 .",
    "birkhauser , 2000 .",
    "j.  leskovec , k.j .",
    "lang , a.  dasgupta , and m.w .",
    "statistical properties of community structure in large social and information networks . in _",
    "www 08 : proceedings of the 17th international conference on world wide web _ ,",
    "pages 695704 , 2008 .",
    "j.  leskovec , k.j .",
    "lang , and m.w .",
    "mahoney . empirical comparison of algorithms for network community detection . in _ www 10 : proceedings of the 19th international conference on world wide web _ , pages 631640 , 2010 .",
    "l.  k. saul , k.  q. weinberger , j.  h. ham , f.  sha , and d.  d. lee .",
    "spectral methods for dimensionality reduction . in o.",
    "chapelle , b.  schoelkopf , and a.  zien , editors , _ semisupervised learning _ , pages 293308 .",
    "mit press , 2006 ."
  ],
  "abstract_text": [
    "<S> two ubiquitous aspects of large - scale data analysis are that the data often have heavy - tailed properties and that diffusion - based or spectral - based methods are often used to identify and extract structure of interest . </S>",
    "<S> perhaps surprisingly , popular distribution - independent methods such as those based on the vc dimension fail to provide nontrivial results for even simple learning problems such as binary classification in these two settings . in this paper </S>",
    "<S> , we develop distribution - dependent learning methods that can be used to provide dimension - independent sample complexity bounds for the binary classification problem in these two popular settings . </S>",
    "<S> in particular , we provide bounds on the sample complexity of maximum margin classifiers when the magnitude of the entries in the feature vector decays according to a power law and also when learning is performed with the so - called diffusion maps kernel . </S>",
    "<S> both of these results rely on bounding the annealed entropy of gap - tolerant classifiers in a hilbert space . </S>",
    "<S> we provide such a bound , and we demonstrate that our proof technique generalizes to the case when the margin is measured with respect to more general banach space norms . </S>",
    "<S> the latter result is of potential interest in cases where modeling the relationship between data elements as a dot product in a hilbert space is too restrictive . </S>"
  ]
}