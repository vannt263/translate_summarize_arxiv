{
  "article_text": [
    "multi - label classification is the set of classification problems where the output vector has a variable length .",
    "the average number of labels per review varies across datasets , and in general is a function of the semantics of the text rather than the syntax .",
    "the learning algorithm needs to estimate the number of labels and make the correct predictions .",
    "+ previously multi - label classification problems were solved using problem transformation techniques ( converting the problem into binary classification problems per output label ) , or by adapting the algorithm to directly perform multi - label classification @xcite . this paper extracts correlation information between labels and factors the joint probabilities into the model . +",
    "given large datasets of product reviews from amazon and twitter and manually labelled multi - label classifications for each ( ground truth ) , the algorithm aims to predict all classifications for a review @xcite .",
    "we aim to make the algorithm portable across various datasets , i.e. , training a model on amazon reviews and using it to classify tweets from twitter .",
    "the amazon dataset contains reviews for 400,000 products across 40,000 different categories . among these",
    ", we focused on books and book subcategories .",
    "the twitter dataset has 100,000 tweets .",
    "both the datasets have the same schema , and each review and tweet has a unique i d , pre - pruned content , and a tree of all of the product labels for the review up to the book category at the root .",
    "our approach augments the independent binary classification model .",
    "in addition to the individual probabilities , this also looks at the probability of labels occurring together .",
    "hence the inference algorithm models the formula  + @xmath0 where @xmath1 + @xmath2 represents the probability of the independent binary model for label @xmath3 classifying as 0 or 1 given input @xmath4 .",
    "@xmath5 represents the assignment of 0 or 1 to all @xmath6 that maximizes the joint probability .",
    "+ the pairwise correlation probability between each pair of labels is stored in the correlation matrix .",
    "this normalized rows of the matrix represent probability of a particular label s coupling with every other label except for itself .",
    "this probability is computed as a prior for the entire dataset and is not dependent on the input feature vector itself .",
    "it is represented by @xmath7 .",
    "+ @xmath8 represents the joint probability of all pairwise combinations of predicted labels , each discounted by @xmath9 .",
    "the probabilities obtained from the correlation matrix are given less weight than the probabilities obtained from independent models , hence the discount .",
    "this is because co - occurrence of labels is not completely characterized by correlation , it also requires the higher order moments which significantly increase the computational overhead .",
    "the entire algorithm is split into 3 steps after parsing  preprocessing , training independent classifiers , and incorporating co - occurrence probability to make the final label set .",
    "hyper - parameters need to be tuned for each dataset .",
    "+ liblinear works well for larger datasets while libsvm works well for custom and smaller datasets .",
    "liblinear only supports a linear kernel but is very fast relative to libsvm and hence used in the learning algorithms @xcite .",
    "+      the amazon and twitter datasets are parsed to extract book reviews and their corresponding labels ( ground truth ) .",
    "the labels are structured in a hierarchy - for books , there are 31 top level labels and for each label there are several sub - labels .",
    "for instance a top level label is `` literature and fiction '' and few sub labels associated with it are `` folklore '' , `` mysteries '' and `` classics . ''",
    "the reviews are labelled with these sub - level labels which we map to one of the 31 top - level labels , and use for multi - label classification .",
    "+      training and testing of the algorithm is done on at most 3,000 reviews per dataset due to computational constraints during learning .",
    "each book review is mapped to a bag - of - words based input feature .",
    "on parsing a review , its content is pruned and stemmed and then a tf - idf based vector is generated which is used as the input feature @xmath4 .",
    "the labels corresponding to this review are associated as its outputs @xmath10 .",
    "+        the correlation matrix is built by parsing 100,000 book reviews and creating pairwise counts of different labels  a 31 @xmath11 31 row normalized matrix with laplace smoothing .",
    "after normalization the matrix is not symmetric , so the geometric mean of cells @xmath12and @xmath13 is treated as the actual correlation probability for two labels .",
    "plots the row normalized covariance matrix for amazon dataset .",
    "the existence of large peaks is the motivation behind the algorithm .",
    "+      * the inputs to the learning algorithm are tf - idf based input features , their manually assigned labels , and the prior generated correlation matrix * 31 independent binary models are trained using l2-regularized svm .",
    "this serves as the * baseline*. * for a review , the j labels with highest probabilities from the independent models are selected .",
    "* probabilities of pairwise - combinations of these labels are computed as @xmath14 * k @xmath15 pairs with maximum probability product are chosen and distinct labels constitute the predicted set .",
    "the algorithm does not compute the theoretical model exactly but is an approximation .",
    "it is similar to beam search .",
    "+      .hyperparameters for amazon dataset [ cols=\"<,<,<,<\",options=\"header \" , ]     the correlation algorithm weighs false positives and false negatives differently , hence the @xmath16 score gives a more accurate understanding of the algorithm s performance .",
    "@xmath16 for the correlation algorithm s test on amazon is 30.19% better than the baseline s test , 2.82% better than the baseline s test on cross - domain learning , and 0.3% worse than baseline s test on twitter .",
    "this validates the trend in the plots .",
    "+ the false positive counts when training and testing purely on one dataset are usually higher than the false positive counts from the baseline , as in our approach we tend to discount the error of making false positives given we meet true positives .",
    "+ table v shows the confusion matrix for cross domain learning where the correlation algorithm s true negative count is much higher than that of the baseline . by using the correlation between output labels ,",
    "the algorithm discards labels that independently had a high probability but were not well correlated with other high probability labels .",
    "+      as evident , the observed training error for each dataset was much less than the observed test error , indicating high variance and low bias .",
    "to further investigate , principal component analysis was used . since the feature vectors were based on word occurrences in text , they were significantly larger than the dataset size  the observed feature size for 2500 dataset size of amazon reviews was 12000 . due to computational limitations ,",
    "straightforward pca was infeasible .",
    "+ sparse notation was used to represent feature vector , from python s numpy library . then sparse svd ( singular value decomposition )",
    "found the smallest subspace that the feature matrix mapped to , keeping all singular values greater than 1 .",
    "this significantly reduced the dataset size .",
    "the 2500 dataset of amazon reviews , now reduced to a feature vector of size 2300 .",
    "while the number of features was still comparable to the number of data points , it was substantially lower than the size of the raw feature vector ( 1/6 ) .",
    "+ this experimentation exposed two facts .",
    "the errors were still the same , but the computation time rose significantly ( about five times the previous duration ) .",
    "it was seen that liblinear , the python library , was tuned to work with large datasets with document based feature vectors ( as compared to libsvm ) , and not for any general features .",
    "+ since the baseline itself suffers from the issue of high variance and low bias , any augmentations over it would be unable to resolve this by themselves , and would require modifying how the tf - idf vectors are generated .",
    "we had already stemmed the words to check this issue , but evidently this would require a more careful processing of the reviews , before they are converted to features .",
    "moreover , since svms enforce larger margin as a metric to evaluate each point , they end with a much lower vc dimension than the size of the feature vector .",
    "this was another reason in favor of svm over logistic regression , naive bayes , and random forest .",
    "+ the baseline resulted in about 10% training error , while our algorithm resulted in about 15% training error .",
    "thus there is an increased bias , although the higher variance is a more pressing issue because the test error is significantly higher than training error for both algorithms . +",
    "using the correlation matrix to help classify reviews appeared to be a good approach , justified by peaks in correlation .",
    "the results show , however , that when performing multi - label classification with a sufficiently sized feature set , augmentation of an independent model with second order correlation probability shows only marginal improvements .",
    "+ the model was modified from the baseline by adding a single feature : second order correlation .",
    "this is the greedy method of selecting features , in contrast to exploring correlation of all combinations of labels .",
    "in such a case where we did not explore higher order correlation we could have missed , for example , that labels a , b , and c never occur together  a piece of information that could have proved vital to the model s representation of the true data .",
    "thus , if computation time and space are not an issue , higher order correlation should be considered .",
    "+ additionally , svm , as was already known , has shown to be best for learning on large feature sets because it will attempt to reduce the set and hence generalize better .",
    "the correlation matrix was built specific for every dataset , so to apply the learning algorithms on cross datasets a generalized correlation framework is needed .",
    "+ currently the hyper - parameter k is optimized to choose number of predicted labels using supervised techniques .",
    "the number of labels for each review could be predicted by using unsupervised techniques such as k - means clustering on label probabilities .",
    "+ the datasets have different inherent structures which do not generalize well when using a common learning algorithm . for the twitter dataset ,",
    "each feature vector is about 12% the size of a corresponding feature vector in the amazon dataset .",
    "the features should be supplemented with options such as semantic relations in text to make classifications and context of user history .",
    "+ the reviews have sub - level labels associated with them which are rolled up to first - level labels to make the classifications .",
    "the algorithm should extend to make hierarchical classifications .",
    "sincere thanks to professor ashutosh saxena and to aditya jami for advising the research team and providing the labelled datasets of product reviews from amazon and twitter ."
  ],
  "abstract_text": [
    "<S> this paper attempts multi - label classification by extending the idea of independent binary classification models for each output label , and exploring how the inherent correlation between output labels can be used to improve predictions . logistic regression , naive bayes , random forest , and svm models were constructed , with svm giving the best results : an improvement of 12.9% over binary models was achieved for hold out cross validation by augmenting with pairwise correlation probabilities of the labels . </S>"
  ]
}