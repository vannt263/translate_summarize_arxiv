{
  "article_text": [
    "the covariance matrix adaptation evolution strategy ( cma - es , @xcite ) is a stochastic search algorithm for non - separable and ill - conditioned black - box continuous optimization . in the cma - es , search points are generated from a gaussian distribution and the mean vector and the covariance matrix of the gaussian distribution are adapted by using the sampled points and their objective value ranking .",
    "these parameters update rules are designed so as to enhance the probability of generating superior points in the next iteration in a way similar to but slightly different from the ( weighted ) maximum likelihood estimation .",
    "adaptive - ess including the cma - es are successfully applied in practice .",
    "however , their theoretical analysis even on a simple function is complicated and linear convergence has been proven only for simple algorithms compared to the cma - es @xcite .",
    "resent studies @xcite demonstrate the link between the parameter update rules in the cma - es and the natural gradient method , the latter of which is the steepest ascent / descent method on a riemannian manifold and is often employed in machine learning @xcite",
    ". the natural gradient view of the cma - es has been developed and extended in @xcite and the information - geometric optimization ( igo ) algorithm has been introduced as the unified framework of natural gradient based stochastic search algorithms . given a family of probability distributions parameterized by @xmath1 , the igo transforms the original objective function , @xmath2 , to a fitness function , @xmath3 , defined on @xmath4 .",
    "the igo algorithm performs a natural gradient ascent aiming at maximizing @xmath3 . for the family of gaussian distributions",
    ", the igo algorithm recovers the pure rank-@xmath0 update cma - es @xcite , for the family of bernoulli distributions , pbil @xcite is recovered .",
    "the igo algorithm can be viewed as the deterministic model of a recovered stochastic algorithm in the limit of the number of sample points going to infinity .",
    "the igo offers a mathematical tool for analyzing the behavior of stochastic algorithms . in this paper",
    ", we analyze the behavior of the deterministic model of the pure rank-@xmath0 update cma - es , which is slightly different from the igo algorithm .",
    "we are interested in knowing what is the target matrix of the covariance matrix update and how fast the covariance matrix learns the target .",
    "the cma is designed to solve ill - conditioned objective function efficiently by adapting the metric  covariance matrix in the cma - es  as well as other variable metric methods such as quasi - newton methods @xcite .",
    "speed of optimization depends on the precision and the speed of metric adaptation to a great extend .",
    "there is a lot of empirical evidence that the covariance matrix tends to be proportional to the inverse of the hessian matrix of the objective function in the cma - es .",
    "however , it has not been mathematically proven yet .",
    "we are also interested in the speed of convergence of the mean vector and the covariance matrix .",
    "convergence of the cma - es has not been reported up to this time .",
    "we tackle these issues in this work .    in this paper , we derive a novel natural gradient algorithm in a similar way to the igo algorithm , where the objective function @xmath2 is transformed to a function @xmath3 in a different way from the igo so that we can derive the explicit form of the natural gradient for composite functions of a strictly increasing function and a convex quadratic function .",
    "we call the composite functions _ monotonic convex - quadratic - composite _ functions .",
    "the resulting algorithm inherits important properties of the igo and the cma - es , such as invariance under monotone transformation of the objective function and invariance under affine transformation of the search space .",
    "we theoretically study this natural gradient method on monotonic convex - quadratic - composite functions .",
    "we prove that the covariance matrix adapts to be proportional to the inverse of the hessian matrix of the objective function .",
    "we also investigate the speed of the covariance matrix adaptation and the speed of convergence of the parameters .",
    "the rest of the paper is organized as follows . in section  [ sec : algo ] we propose a novel natural gradient method and present a stochastic algorithm that approximates the natural gradient from finite samples .",
    "the basic properties of both algorithms are described . in section",
    "[ sec : conv ] we study the convergence properties of the deterministic algorithm on monotonic convex - quadratic - composite functions .",
    "the convergence of the condition number of the product of the covariance matrix and the hessian matrix of the objective function to one and its linear convergence are proven .",
    "moreover , the rate of convergence of the parameter is shown . in section",
    "[ sec : exp ] , we conduct experiments to see how accurately the stochastic algorithm approximates the deterministic algorithm and to see how similarly our algorithm and the cma - es behave on a convex quadratic function .",
    "finally , we summarize and conclude this paper in section  [ sec : conc ] .",
    "we first introduce a generic framework of the natural gradient algorithm that includes the igo algorithm .",
    "the original objective is to minimize @xmath5 , where @xmath6 is a metric space .",
    "let @xmath7 and @xmath8 be the borel @xmath9-field and a measure on @xmath6 .",
    "hereunder , we assume that @xmath2 is @xmath8-measurable .",
    "let @xmath10 represent any monotonically increasing set function on @xmath7 , i.e. , @xmath11 for any @xmath12 , @xmath13 s.t .",
    "we transform @xmath2 to an _ invariant cost _ function defined as @xmath15 $ ] . given a family of probability distributions @xmath16 on @xmath6",
    ", we define a _",
    "quasi_-objective function @xmath3 on the parameter space @xmath4 as the expected value of @xmath17 over @xmath16 , namely @xmath18 } \\enspace.\\label{eq : f}\\ ] ]    our algorithm performs the natural gradient descent on a riemannian manifold @xmath19 equipped with the fisher metric @xmath20 .",
    "the fisher metric is the unique metric that does not depend on the choice of parameterization @xcite . the natural gradient  the gradient taken w.r.t .",
    "the fisher metric  is given by the product of the inverse of the fisher information matrix @xmath20 and the `` vanilla '' gradient @xmath21 of the function .",
    "therefore , the natural gradient of @xmath3 is @xmath22 and the parameter update follows @xmath23 where @xmath24 is the learning rate .      in the following , we focus on the optimization in @xmath25 .",
    "thus , @xmath26 , @xmath8 is the lebesgue measure @xmath27 on @xmath25 , and @xmath7 is the borel @xmath9-field @xmath28 on @xmath25 .",
    "we choose as the sampling distribution the gaussian @xmath16 parameterized by @xmath1 , where the mean vector @xmath29 is in @xmath25 and the covariance matrix @xmath30 is a symmetric and positive definite matrix of dimension @xmath31 .",
    "we define the invariant cost @xmath32 by using the lebesgue measure @xmath27 as @xmath33 $ ] .",
    "then , the infimum of @xmath34}$ ] is zero located on a boundary of the domain @xmath4 where the mean vector equals the global minimum of @xmath2 and the covariance matrix is zero .",
    "the choice of the parameterization of gaussian distributions affects the behavior of the natural gradient update with finite learning rate @xmath24 , although the steepest direction of @xmath3 on the statistical manifold @xmath4 is invariant under the choice of parameterization .",
    "we choose the mean vector and the covariance matrix of the gaussian distribution as the parameter as well as are chosen in the cma - es and in other algorithms such as emna @xcite and cross entropy method @xcite .",
    "let @xmath35^{\\mathrm{t}}$ ] , where @xmath36 be the vectorization of @xmath37 such that the ( @xmath38 , @xmath39)th element of @xmath37 corresponds to @xmath40th element of @xmath36 ( see @xcite ) .",
    "then the fisher information matrix has an analytical form @xmath41 where @xmath42 denotes the kronecker product operator . under some regularity conditions for the exchange of integration and differentiation we have @xmath43},\\label{eq : grad}\\ ] ] where @xmath44 is the log - likelihood .",
    "the gradient of the log - likelihood @xmath45 can be written as @xmath46 then , the natural gradient @xmath47^{{\\mathrm{t}}}$ ] at @xmath48 can be written by part @xmath49 \\\\",
    "\\delta c^{t } & = { \\mathbb{e}}_{x \\sim p_{\\theta^{t}}}\\bigl [ { v_{f}}(x ) \\bigl((x - m^{t})(x - m^{t})^{\\mathrm{t}}- c^{t } \\bigr ) \\bigr ] \\end{split}\\ ] ] with different learning rates for mean vector and covariance matrix updates , the natural gradient descent reads @xmath50 we refer to for the deterministic natural gradient descent ( ngd ) method .",
    "when @xmath21 is not given , we need to estimate the gradient .",
    "we approximate the natural gradient and simulate the natural gradient descent as follows .",
    "initialize the mean vector @xmath51 and the covariance matrix @xmath52 and repeat the following steps until some termination criterion is satisfied :    * compute the eigenvalue decomposition of @xmath53 , @xmath54 = \\text{eig}(c^{t})$ ] , where @xmath55 is an orthogonal matrix and @xmath56 is a diagonal matrix such that @xmath57 . *",
    "compute the square root of @xmath53 , @xmath58 .",
    "* generate normal random vectors @xmath59 .",
    "* compute @xmath60 , for @xmath61 . *",
    "evaluate the objective values @xmath62 ; * estimate @xmath63 as @xmath64 * compute the baseline @xmath65 . *",
    "compute the weights @xmath66 .",
    "* estimate the natural gradient @xmath67 and @xmath68 as @xmath69 * compute the learning rates @xmath70 and @xmath71 .",
    "* update the parameters as @xmath72 and @xmath73 .",
    "we refer to this algorithm for the stochastic ngd algorithm .",
    "this algorithm generates @xmath74 samples @xmath75 from @xmath76 in steps 14 and evaluates their objective values in step 5 . in step 6 ,",
    "the invariant costs @xmath63 are evaluated .",
    "the estimates @xmath77 are obtained as follows . by definition",
    "we have @xmath78 applying monte - carlo approximation we have @xmath79 since @xmath80 , we have the estimates @xmath77 in step 6 .",
    "step 7 computes the baseline @xmath81 that is often introduced to reduce the estimation variance of gradients while adding no bias @xcite .",
    "we simply choose the mean value of the @xmath77 as the baseline . replacing the expectation in with the sample mean and adding the baseline ( in step 8) we have the monte - carlo estimate of the natural gradient in step 9 .",
    "finally in step 11 , we update the parameters along the estimated natural gradient with the learning rates computed in step 10 .",
    "the learning rates are chosen in the following so that they are inverse proportional to the largest eigenvalue of the following matrix @xmath82      the difference between the igo algorithm and our deterministic algorithm is that the invariant cost in the igo algorithm is defined by negative of the weighted quantile , @xmath83)$ ] , where @xmath84 \\mapsto { \\mathbb{r}}$ ] is non - increasing weight function . since the quantile @xmath85 $ ] depends on the current parameter @xmath86 , @xmath32 for each @xmath87 in the igo algorithm changes from iteration to iteration , whereas it is fixed in our algorithm .",
    "this property makes our algorithm easier to analyze mathematically .",
    "the difference between our stochastic algorithm and the pure rank-@xmath0 update cma - es @xcite is the same as the difference between the deterministic one and the igo algorithm .",
    "the pure rank-@xmath0 update cma - es approximates the quantile value @xmath85 $ ] by the number of better solutions divided by the number of samples @xmath74 , @xmath88 .",
    "therefore , the pure rank-@xmath0 update cma - es simulates the same lines as the stochastic ngd algorithm described in section  [ sec : stoc ] with the weights @xmath89 .    in section  [ sec : exp ] we compare the stochastic ngd algorithm with the pure rank-@xmath0 update cma - es where @xmath90      * invariance . * our algorithms inherit two important invariance properties from the igo and the cma - es : invariance under monotonic transformation of the objective function and invariance under affine transformation of the search space ( with the same transformation of the initial parameters ) .",
    "invariance under monotonic transformation of the objective function makes the algorithm perform equally on a function @xmath2 and on any composite function @xmath91 where @xmath92 is any strictly increasing function .",
    "for example , the convex sphere function @xmath93 is equivalent to the non - convex function @xmath94 for this algorithm , whereas conventional gradient methods , e.g.  newton method , assume the convexity of the objective function and require a fine line search to solve non - convex functions .",
    "this invariance property is obtained as a result of the transformation @xmath95 .",
    "invariance under affine transformation of the search space is the essence of variable metric methods such as newton s method . by adapting the covariance matrix ,",
    "this algorithm attains universal performance on ill - conditioned objective functions .",
    "* positivity .",
    "* the covariance matrix of the gaussian distribution must be positive definite and symmetric at each iteration .",
    "the next proposition gives the condition on the learning rate @xmath71 such that the covariance matrix is always positive definite symmetric .",
    "suppose that the learning rate for the covariance update @xmath96 in the deterministic ngd algorithm , where @xmath97 denotes the largest eigenvalue of the argument matrix . if @xmath52 is positive definite symmetric , @xmath53 is positive definite symmetric for each @xmath98 .",
    "similarly , if @xmath99 in the stochastic ngd algorithm , where @xmath100 is defined in , and if @xmath52 is positive definite symmetric , the same result holds .    consider the deterministic case .",
    "suppose that @xmath101 is positive definite and symmetric .",
    "then , @xmath102 since @xmath103 by the assumption , all the eigenvalues of @xmath104 is smaller than one .",
    "thus , the inside of the brackets is positive definite symmetric and hence @xmath105 is positive definite symmetric . by mathematical induction , we have that @xmath53 is positive definite and symmetric for all @xmath106 . the analogous result for the stochastic case is obtained in the same way .    * consistency . *",
    "the gradient estimator is not necessarily unbiased , yet it is consistent as is shown in the following proposition .",
    "therefore , one can expect that the stochastic ngd approximates the deterministic ngd well when the sample size @xmath74 is large .",
    "let @xmath107 be the natural gradient operator .",
    "[ prop : consistency ] let @xmath108 be independent and identically distributed random vectors following @xmath16 .",
    "let @xmath109 and @xmath110^{{\\mathrm{t}}}$ ] be the invariant cost and the natural gradient where @xmath111 are replaced with @xmath108 .",
    "suppose that @xmath112 < \\infty .",
    "\\label{eq : propasm3}\\end{gathered}\\ ] ] then , @xmath113 , where @xmath114 represents almost sure convergence .    by the cauchy - schwarz inequality we have that @xmath115^{2 } < { \\mathbb{e}}[{v_{f}}(x)^{2}]{\\mathbb{e}}[{\\lvert { \\tilde \\nabla}l(\\theta ; x ) \\rvert}^{2}]$ ] .",
    "note that @xmath116 = \\operatorname{tr}({{\\mathcal{i}}_{\\theta}}^{-1 } ) < \\infty$ ] . by jensen s inequality we have that @xmath117^{2 } { \\leqslant}{\\mathbb{e}}[{v_{f}}(x)^{2}]$ ] .",
    "therefore , implies @xmath118 < \\infty \\text { and } \\label{eq : propasm1}\\\\ { \\mathbb{e}}[{v_{f}}(x ) ]   < \\infty \\enspace .",
    "\\label{eq : propasm2}\\end{gathered}\\ ] ]    define @xmath119 and decompose @xmath120 as @xmath121 by and the strong law of large numbers ( lln ) , the first summand converges to @xmath122 = { \\tilde \\nabla}{j}(\\theta)$ ] almost surely as @xmath123 .",
    "so we have to show that the second term and the third term of converge almost surely to zero .",
    "first , we show the following almost sure convergence @xmath124 by the definition of @xmath125 , we have @xmath126 < \\infty$ almost everywhere , we have by { lln } } & = { \\mu_\\mathrm{leb}}^{2/d}[y : f(y ) { \\leqslant}f(x ) ] = { { v_{f}}}(x)\\end{aligned}\\ ] ] almost surely and almost everywhere in @xmath87 .",
    "this implies @xmath127 almost everywhere in @xmath87 .    for @xmath128 ,",
    "we have @xmath129 since @xmath127 almost everywhere in @xmath87 as @xmath123 , we have @xmath130 almost everywhere in @xmath87 as @xmath131 . by the lebesgue s dominated convergence theorem we have @xmath132 \\to 0 $ ] as @xmath131",
    "therefore , we have that @xmath132 < \\infty$ ] for @xmath133 large enough . then , by applying lln , we have that the right most side of converges to @xmath132 $ ] as @xmath123 and this expectation converges to @xmath134 as @xmath131 .",
    "this ends the proof of .",
    "now we can obtain the almost sure convergence of the third term of to zero .",
    "indeed , the almost sure convergence implies that the limit @xmath135 agrees with @xmath136 and we have from and lln  that @xmath137 < \\infty$ ] . also , by lln  we have that @xmath138 as @xmath123 .",
    "therefore , the third term of converges to zero almost surely .    to show the convergence of the second term of to zero",
    ", we apply the cauchy - schwarz inequality to it and we have @xmath139 by lln  we have that the second term of the right hand side converges to @xmath116 = \\operatorname{tr}({{\\mathcal{i}}_{\\theta}}^{-1})$ ] .",
    "so we have to prove that the first term on the right hand side converges to zero almost surely .",
    "the proof of this convergence is done in the same way as above with @xmath140 replacing @xmath141 .",
    "we remark that is the necessary and sufficient condition for @xmath142 to exist and that is a sufficient condition for the exchange of integral and differentiation used in .",
    "see e.g.  ( * ? ? ?",
    "* theorem  16.8 ) .",
    "we investigate the convergence properties of the deterministic ngd algorithm on a monotonic convex - quadratic composite function @xmath143 , where @xmath92 is any strictly increasing function and @xmath12 is a positive definite symmetric matrix .",
    "[ prop : ng ] the natural gradient can be written as @xmath144    since @xmath145 $ ] is equivalent to the volume of the ellipsoid @xmath146 , we have that @xmath147 = \\frac{2}{\\det(a ) } v_{d}(\\sqrt{x^\\mathrm{t } a x}),\\ ] ] where @xmath148 denotes the volume of the sphere with radius @xmath149 and is proportional to @xmath150 . therefore @xmath33 \\propto x^{\\mathrm{t}}a x$ ]",
    ". since the proportionality constant is independent of @xmath87 , we have @xmath151}\\\\ & \\propto { \\mathbb{e}}_{x\\sim p_{\\theta}}\\bigl [ x^{\\mathrm{t}}a x \\bigr ] = m^{\\mathrm{t}}a m + \\operatorname{tr}(ac ) .",
    "\\end{split}\\ ] ] differentiating the both side of the above relation , we have @xmath152 premultiplying by @xmath153 , we obtain the intended result .",
    "now the deterministic ngd algorithm on @xmath154 is implicitly written as @xmath155 where @xmath156 is the proportionality constant appearing in the proof of proposition  [ prop : ng ] .    in the following , we work on the following assumption : there are @xmath157 and @xmath158 such that @xmath159 these assumptions are satisfied , for example , if @xmath70 and @xmath71 are set for each iteration so that @xmath160 . in this case",
    "the natural gradient can be considered to be normalized by @xmath161 and the pseudo - learning rate is @xmath162 .",
    "the next theorem states that the covariance matrix converges proportionally to the inverse of the hessian matrix .",
    "[ thm : cond ] assume .",
    "the condition number of @xmath163 converges to one with the rate of convergence @xmath164 moreover , we have an upper bound @xmath165 if the limit @xmath166 exists , @xmath167 is replaced with @xmath168 in .",
    "since @xmath12 is positive definite and symmetric , there exists the square root @xmath169 .",
    "premultiplying and postmultiplying both side of covariance matrix update by @xmath170 , we have @xmath171 since @xmath172 is positive definite and symmetric , there exists an eigenvalue decomposition @xmath173 , where the diagonal elements of @xmath174 are the eigenvalues of @xmath172 and each column of @xmath175 is the eigenvector corresponding to each diagonal element of @xmath176 .",
    "then , @xmath177 this means , @xmath178 also diagonalizes @xmath179 . by mathematical induction",
    "we have that an orthogonal matrix @xmath180 which diagonalizes @xmath181 diagonalizes @xmath172 for any @xmath106 and we have @xmath182    next , we show that the condition number of @xmath176 converses to @xmath183 as @xmath184",
    ". remember @xmath185 .",
    "we have @xmath186 .",
    "then , by assumption   we have @xmath187 moreover , since @xmath188 for any @xmath38 , we have @xmath189 for any @xmath38 , @xmath39 .",
    "suppose @xmath190 without loss of generality . from and",
    "the inequality @xmath191 , we have @xmath192 with equality holding if and only if @xmath193 . therefore , if @xmath194 , then @xmath195 , which implies that if @xmath38th and @xmath39th diagonal elements of @xmath196 are the maximum and minimum elements , @xmath38th and @xmath39th elements of @xmath197 are also the maximum and minimum elements of @xmath197 . without loss of generality we suppose @xmath198 for any @xmath199 for all @xmath106 .",
    "then , @xmath200 . according to we",
    "have @xmath201 since @xmath202 we have @xmath203 moreover , since the right - hand side of the above inequality is maximized when @xmath204 is minimized and @xmath205 is bounded from below by @xmath167 because of , we have @xmath206 this implies @xmath207 .",
    "the rate of convergence and the upper bound are immediate consequences of the above inequality .    if the limit @xmath168 exists , it is easy to see from that @xmath167 can be replaced with @xmath168 in .",
    "this completes the proof .",
    "note that if @xmath208 and @xmath209 , we have that @xmath210 .",
    "we have from that @xmath211 and the rate of convergence becomes @xmath212 .",
    "the next theorem states the global convergence of @xmath133 and @xmath37 and the speed of the convergence . in the following ,",
    "we let @xmath213 denote the frobenius norm of @xmath214 , namely @xmath215 .",
    "[ thm : rate ] assume and .",
    "then , @xmath216 and @xmath217 converge to zero with the rate of convergence @xmath218 where @xmath219 is either @xmath133 or @xmath37 and @xmath220 is either @xmath221 or @xmath53 . if the limit @xmath222 exists , @xmath223 is replaced with @xmath224 in .",
    "let @xmath225 denote the @xmath38th largest singular value of the argument matrix .",
    "according to j.  von neumann s trace inequality @xcite we have @xmath226 , where @xmath227 and @xmath228 are any matrices in @xmath229 .",
    "let @xmath230 be nonnegative definite and @xmath231 is nonnegative definite symmetric . from the above inequality ,",
    "we have @xmath232 applying this matrix norm inequality and the vector norm inequality @xmath233 to and , we have @xmath234 in light of theorem  [ thm : cond ] , we have that @xmath235 .",
    "then , from the assumptions and we have @xmath236 this implies linear convergence of @xmath219 with rate of convergence at most @xmath237 .    if the limit @xmath224 exists , we can easily see from the above inequality that @xmath223 is replaced with @xmath224 in .",
    "this ends the proof .",
    "now we can see the importance of the covariance matrix adaptation quantitatively .",
    "let @xmath238 .",
    "then , the covariance matrix becomes proportional to the inverse of the hessian at the speed given by and the rate of convergence of the parameter becomes @xmath239 .",
    "meanwhile , if the covariance matrix is restricted to a product of a scalar @xmath240 and the identity matrix , @xmath241 , then the rate of convergence is in @xmath242 $ ] , i.e.  @xmath243 .",
    "the rate of convergence is in @xmath244 $ ] , where @xmath245 .",
    "we omit the derivation due to the space limitation . ] .",
    "therefore , the rate of convergence becomes close to one in the worst case if @xmath246 .    from theorem  [ thm : cond ]",
    "we know that the deterministic ngd algorithm learns the inverse of the hessian .",
    "the convergence of the covariance matrix to the inverse of the hessian matrix in the cma - es has been anticipated but it has not been proven .",
    "theorem  [ thm : cond ] demonstrates this anticipation affirmatively for the deterministic ngd algorithm .",
    "theorem  [ thm : rate ] exhibits the linear convergence of the parameters .",
    "this implies that the rate of convergence of the expected objective value @xmath247 is also linear and equals to the rate of convergence of @xmath217 .",
    "the results in the previous section are for the deterministic ( ideal ) ngd algorithm .",
    "thanks to proposition  [ prop : consistency ] we can expect that the stochastic ngd algorithm proposed in section  [ sec : stoc ] approximates the deterministic one arbitrarily close as the sample size @xmath74 is taken sufficiently large . in this section ,",
    "we evaluate how well the stochastic variant with finite sample size approximates the deterministic one on a quadratic function .",
    "we consider the @xmath248-dimensional ellipsoid function @xmath249 note that the ellipsoid function is separable and convex but our algorithm does not exploit the separability and convexity .",
    "the eigenvalues ( diagonal elements ) of the hessian matrix of the ellipsoid function range in @xmath250 $ ] .",
    "we set the initial parameters as @xmath251 and @xmath252 .",
    "we design the learning rates as @xmath253 here , @xmath100 is a matrix defined in .      [",
    "cols=\"^ \" , ]     first , we investigate the effect of the sample size @xmath74 and the coefficient @xmath254 of the learning rate @xmath71 .",
    "we try the following sample sizes : @xmath255 , @xmath31 , @xmath256 , @xmath257 , @xmath258 , @xmath259 .",
    "figure  [ fig : lambda ] illustrates the slope of the condition number @xmath260 and the theoretical curve and the slope of the expected objective function value @xmath261 = ( m^{t})^{\\mathrm{t}}a m^{t } + \\operatorname{tr}(c^{t}a)$ ] , averaged over @xmath262 independent trials .",
    "when the sample size is larger , we see the closer performance to the theoretical result . when @xmath263 and @xmath264 , the convergence curve of the condition number approximated well the theoretical curve and the final condition number is @xmath265 . when @xmath266 and @xmath264 , it takes more than @xmath267 times longer to learn the covariance matrix and the final condition number becomes @xmath268 , although the stochastic algorithm still works successfully .",
    "we attain a little higher condition numbers when we choose larger learning rates @xmath269 , @xmath270 . for example , the final condition numbers are @xmath271 for @xmath263 and @xmath269 , and @xmath272 for @xmath263 and @xmath273 .",
    "this is because smaller learning rates have more effect of averaging the natural gradient estimates over iterations and reducing the estimation variance .",
    "note that we observe a slightly slower adaptation of the covariance matrix at the beginning in case that we set @xmath274 , although the adaptation behavior does not change in theory .",
    "see figure  [ fig : m ] .",
    "this attributes to the estimation precision of @xmath275 .",
    "if the squared mahalanobis distance @xmath276 between the origin ( the global optimum ) and the current mean with respect to @xmath53 is larger , the function landscape around @xmath221 looks more like linear function",
    ". then @xmath277 are far from the exact values , especially in case a small sample size is chosen .",
    "for @xmath278 , @xmath279 , @xmath259 with initial mean vector @xmath280 or @xmath281 .",
    "other settings are the same as in figure  [ fig : lambda ] . ]      finally , we study how well this stochastic algorithm simulates the cma - es .",
    "we test the pure rank-@xmath0 update cma - es with weight scheme .",
    "we set the learning rates following @xcite @xmath282 where @xmath283 .",
    "we choose @xmath254 for our algorithm so that the speed of adaptation for each model is almost the same .",
    "figure  [ fig : cma ] shows the results for each method for @xmath278 and @xmath284 . in both case , we confirm similar behaviors of the pure rank-@xmath0 update cma - es and our algorithm despite their dissimilar weight - value settings . the similar change of performance illustrated in figure  [ fig : m ]",
    "is also observed for the pure rank-@xmath0 update cma - es . from this result , we conclude that it is possible to estimate the performance of the pure rank-@xmath0 update cma - es by our natural gradient algorithm , which is theoretically more attractive .    however , note that the pure rank-@xmath0 update cma - es is not the standard cma - es @xcite and the standard cma - es performs better than the pure rank-@xmath0 update cma - es .",
    "the standard cma - es employes so - called evolution paths to adapt the covariance matrix and the global scale of the covariance matrix , which is called step - size in the cma - es context . moreover , the standard cma - es employes weighted recombination , where different values are assigned to the weights for @xmath285 , which is only slightly better than intermediate recombination and even similar to our setting .",
    "furthermore , the similar performance observed is only on a quadratic function . if there are certain functions which distinguish our algorithm from the ( rank-@xmath0 ) cma - es , this may help to understand both the ngd algorithm and the cma - es .",
    "further study on these topics is required .",
    "cc     and @xmath286 on the left , and for @xmath287 and @xmath288 on the right .",
    "other settings are the same as in figure  [ fig : lambda ] . ]     and @xmath286 on the left , and for @xmath287 and @xmath288 on the right .",
    "other settings are the same as in figure  [ fig : lambda].,title=\"fig : \" ] +     +     and @xmath286 on the left , and for @xmath287 and @xmath288 on the right .",
    "other settings are the same as in figure  [ fig : lambda].,title=\"fig : \" ] +     and @xmath286 on the left , and for @xmath287 and @xmath288 on the right .",
    "other settings are the same as in figure  [ fig : lambda].,title=\"fig : \" ] +",
    "we have proposed a novel natural gradient descent ( ngd ) method where the objective function is transformed to a function defined on the parameter space of probability distributions .",
    "we have proven that the deterministic ngd method learns the inverse of the hessian of the original objective function that is any monotonic convex - quadratic - composite function .",
    "linear convergence of the mean vector and the covariance matrix has been also proven .",
    "the numerical results for the stochastic ngd algorithm have shown that the stochastic algorithm approximates the deterministic one well when the sample size is sufficiently large . moreover , we have confirmed that the stochastic ngd algorithm and the pure rank-@xmath0 update cma - es behave very similarly on a quadratic function .",
    "the contribution of the paper is to derive a novel ngd algorithm that can be viewed as a variant of the cma - es from the first principle of information geometry .",
    "this allows us to analyze the algorithm theoretically . our theoretical results in section  [ sec : conv ]",
    "imply that there is at least one weight - value setting in the cma - es such that the covariance matrix learns the inverse of the hessian of the objective function .",
    "moreover , since our algorithm does not only share most of the important properties of the rank-@xmath0 update cma - es , but also is confirmed to perform similarly to the pure rank-@xmath0 update cma - es on a quadratic function by numerical simulations , we could study our algorithm to find out limitations of the pure rank-@xmath0 update cma - es and to discover a way to improve the cma - es ."
  ],
  "abstract_text": [
    "<S> in this paper we investigate the convergence properties of a variant of the covariance matrix adaptation evolution strategy ( cma - es ) . </S>",
    "<S> our study is based on the recent theoretical foundation that the pure rank-@xmath0 update cma - es performs the natural gradient descent on the parameter space of gaussian distributions . </S>",
    "<S> we derive a novel variant of the natural gradient method where the parameters of the gaussian distribution are updated along the natural gradient to improve a newly defined function on the parameter space . </S>",
    "<S> we study this algorithm on composites of a monotone function with a convex quadratic function . </S>",
    "<S> we prove that our algorithm adapts the covariance matrix so that it becomes proportional to the inverse of the hessian of the original objective function . </S>",
    "<S> we also show the speed of covariance matrix adaptation and the speed of convergence of the parameters . </S>",
    "<S> we introduce a stochastic algorithm that approximates the natural gradient with finite samples and present some simulated results to evaluate how precisely the stochastic algorithm approximates the deterministic , ideal one under finite samples and to see how similarly our algorithm and the cma - es perform .    </S>",
    "<S> = 10000 = 10000    [ global optimization , gradient methods , unconstrained optimization ] </S>"
  ]
}