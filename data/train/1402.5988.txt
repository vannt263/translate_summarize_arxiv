{
  "article_text": [
    "the growing amounts of temporal data collected during the execution of various tasks within organizations are hard to utilize without the assistance of automated processes .",
    "event recognition  @xcite refers to the automatic detection of event occurrences within a system . from a sequence of _ low - level events _",
    "( for example sensor data ) an event recognition system recognizes _ high - level events _ of interest , that is , events that satisfy some pattern .",
    "event recognition systems with a logic - based representation of event definitions , such as the event calculus  @xcite , are attracting significant attention in the event processing community for a number of reasons , including the expressiveness and understandability of the formalized knowledge , their declarative , formal semantics  @xcite and their ability to handle rich background knowledge . using logic programs in particular , has an extra advantage , due to the close connection between logic programming and machine learning in the field of inductive logic programming ( ilp )  @xcite",
    ". however , such applications impose challenges that make most ilp systems inappropriate .",
    "several logical formalisms which incorporate time and change employ non - monotonic operators as a means for representing commonsense phenomena @xcite .",
    "normal logic programs with negation as failure ( naf ) in particular are a prominent non - monotonic formalism .",
    "most ilp learners can not handle naf at all , or lack a robust naf semantics  @xcite .",
    "another problem that often arises when dealing with events , is the need to infer implicit or missing knowledge , for instance the indirect effects of events , or possible causes of observed events . in ilp",
    "the ability to reason with missing , or indirectly observable knowledge is called _ non - observational predicate learning ( non - opl ) _ @xcite .",
    "this is a task that most ilp systems have difficulty to handle , especially when combined with naf in the background knowledge @xcite .",
    "one way to address this problem is through the combination of ilp with abductive logic programming ( alp )  @xcite .",
    "abduction in logic programming is usually given a non - monotonic semantics @xcite and in addition , it is by nature an appropriate framework for reasoning with incomplete knowledge .",
    "although it has a long history in the literature  @xcite , only recently has this combination brought about systems such as xhail  @xcite , tal  @xcite and aspal @xcite that may be used for the induction of event - based knowledge .    the above three systems which , to the best of our knowledge , are the only ilp learners that address the aforementioned learnability issues , are _ batch learners _ , in the sense that all training data must be in place prior to the initiation of the learning process",
    "this is not always suitable for event - oriented learning tasks , where data is often collected at different times and under various circumstances , or arrives in streams . in order to account for new training examples ,",
    "a batch learner has no alternative but to re - learn a hypothesis from scratch .",
    "the cost is poor scalability when `` learning in the large '' @xcite from a growing set of data .",
    "this is particularly true in the case of temporal data , which usually come in large volumes .",
    "consider for instance data which span a large period of time , or sensor data transmitted at a very high frequency .    an alternative approach is learning incrementally , that is , processing training instances when they become available , and altering previously inferred knowledge to fit new observations , instead of discarding it and starting from scratch .",
    "this process , also known as _ theory revision _",
    "@xcite , exploits previous computations to speed - up the learning , since revising a hypothesis is generally considered more efficient than learning it from scratch @xcite .",
    "numerous theory revision systems have been proposed in the literature  see @xcite for a review  however their applicability in non - monotonic domains is limited @xcite .",
    "this issue is addressed by recent approaches to _ theory revision as non - monotonic ilp _",
    "@xcite , where a non - monotonic learner is used to extract a set of prescriptions , which can in turn be interpreted into a set of syntactic transformations on the theory at hand .",
    "however , scaling to the large volumes of today s datasets or handling streaming data remains an open issue , and the development of scalable algorithms for theory revision has been identified as an important research direction @xcite . as historical data grow over time , it becomes progressively harder to revise knowledge , so that it accounts both for new evidence and past experience .",
    "one direction towards scaling theory revision systems is the development of techniques for reducing the need for reconsulting the whole history of accumulated experience , while updating existing knowledge .",
    "this is the direction we take in this work .",
    "we build on the ideas of non - monotonic ilp and use xhail as the basis for a scalable , incremental learner for the induction of event definitions in the form of event calculus theories .",
    "xhail has been used for the induction of action theories  @xcite . moreover , in  @xcite it has been used for theory revision in an incremental setting , revising hypotheses with respect to a recent , user - defined subset of the perceived experience . in contrast , the learner we present here performs revisions that account for all examples seen so far .",
    "we describe a compressive `` memory '' structure , incorporated in the learning process , which reduces the need for reconsulting past experience in response to a revision . using this structure ,",
    "we propose a method which , given a stream of examples , a theory which accounts for them and a new training instance , requires at most one pass over the examples in order to revise the initial theory , so that it accounts for both past and new evidence.we evaluate empirically our approach on real and synthetic data from an activity recognition application and a transport management application .",
    "our results indicate that our approach is significantly more efficient than xhail , without compromising predictive accuracy , and scales adequately to large data volumes .",
    "the rest of this paper is structured as follows .",
    "section [ sec : background ] provides a brief overview of abductive and inductive logic programming . in section [ sec :",
    "event_related ] we present the event calculus dialect that we employ , describe the domain of activity recognition that we use as a running example and show how event definitions may be learnt using xhail . in section [ sec : incremental_learning ] we present our proposed method , prove its correctness and present the details of its abductive - inductive mechanism . in section [ sec : discussion ]",
    "we discuss some theoretical and practical implications of our approach . in section [ sec",
    ": experiments ] we present the experimental evaluation , and finally in sections [ sec : related ] and [ sec : concl ] we discuss related work and draw our main conclusions .",
    "we assume a first - order language as in @xcite where  in front of literals denotes negation as failure ( naf ) .",
    "we call a logic program _ horn _ if it is naf - free and _ normal _ otherwise . for more details on the basic terminology and conventions of logic programming used in this work",
    "see appendix [ app : appendix_lp ] .",
    "we define the entailment relation between normal logic programs in terms of the _ stable model semantics _",
    "@xcite and in particular its credulous version , under which program @xmath0 entails program @xmath1 , denoted by @xmath2 , if at least one stable model of @xmath0 is a stable model of @xmath1 .",
    "following prolog s convention , throughout this paper , predicates and ground terms in logical formulae start with a lower case letter , while variable terms start with a capital letter .",
    "inductive logic programming ( ilp ) is a subfield of machine learning based on logic programming .",
    "given a set of positive and negative examples represented as logical facts , an ilp algorithm derives a set of non - ground rules which discriminate between the positive and the negative examples , potentially taking into account some background knowledge .",
    "definition [ def : ilp ] provides a formal account .",
    "[ def : ilp ] an ilp task is a triplet @xmath3 where @xmath4 is a normal logic program , @xmath5 is a set of ground literals called positive ( @xmath6 ) and negative ( @xmath7 ) examples and @xmath8 is a set of clauses called _ language bias_. a normal logic program @xmath9 is called an inductive hypothesis for the ilp task if @xmath10 and @xmath11 _ covers the examples _ , that is , @xmath12 and @xmath13 .",
    "the language bias mentioned in definition [ def : ilp ] reduces the complexity of an ilp task by imposing syntactic restrictions on hypotheses that may be learnt .",
    "a commonly used language bias in ilp , also employed in this work is _ mode declarations _",
    "a mode declaration is a template literal that can be placed either in the head or the body of a hypothesis clause and contains special placemarkers for variables and ground terms .",
    "a set of mode declarations @xmath8 defines a language @xmath14 , called _",
    "mode language_. a clause is in @xmath14 if it is constructed from head and body mode declarations by replacing variable placemarkers by actual variable symbols and ground placemarkers by ground terms .",
    "formal definitions for mode declarations and the mode language are provided in appendix [ app : appendix_lp ] .",
    "ilp algorithms that use mode declarations work by using @xmath14 as a search space for clauses , trying to optimize an objective function which takes into account example coverage and hypothesis size .",
    "typically , the search space @xmath14 is structured via @xmath15-subsumption .",
    "clause @xmath16 @xmath15-subsumes clause @xmath17 , denoted @xmath18 , if there exists a substitution @xmath15 such that @xmath19 and @xmath20 , where @xmath21 and @xmath22 denote the head and the body of clause @xmath16 respectively .",
    "@xmath15-subsumes program @xmath1 if for each clause @xmath23 there exists a clause @xmath24 such that @xmath25 .",
    "@xmath15-subsumption provides a syntactic notion of generality @xcite which may be used to search for clauses based on their example coverage .",
    "clause @xmath16 is more general than clause @xmath17 ( resp .",
    "@xmath17 is more specific than @xmath16 ) if @xmath25 , in which case the examples covered by @xmath17 are a subset of the examples covered by @xmath16 . the generality order between clauses",
    "is naturally extended to hypotheses via @xmath15-subsumption between programs .",
    "given an ilp task @xmath26 , a hypothesis @xmath9 is called _ incomplete _ if @xmath11 does not cover some positive examples from @xmath27 and _ inconsistent _ if it covers some negative examples .",
    "an inductive hypothesis for the ilp task , that is , a hypothesis that is both complete and consistent , is called _",
    "an incomplete hypothesis @xmath9 can be made complete by _ generalization _ , that is , a set of syntactic transformations that aim to increase example coverage , and may include the addition of new clauses , or the removal of literals from existing clauses .",
    "similarly , an inconsistent hypothesis can be made consistent by _ specialization _ , a process that aims to restrict example coverage and may include removal of clauses from the hypothesis , or addition of new literals to existing clauses in the hypothesis .",
    "theory revision is the process of acting upon a hypothesis by means of syntactic transformations ( _ generalization and specialization operators _ ) , in order to change the answer set of the hypothesis @xcite , that is , the examples it accounts for .",
    "theory revision is at the core of incremental ilp systems . in an incremental setting ,",
    "examples are provided over time .",
    "a learner induces a hypothesis from scratch , from the first available set of examples , and treats this hypothesis as a revisable background theory in order to account for new examples .",
    "ll & + @xmath28 & event @xmath27 occurs at time @xmath29 + @xmath30 & at time @xmath29 a period of time + & for which fluent @xmath31 holds is initiated + @xmath30 & at time @xmath29 a period of time + & for which fluent @xmath31 holds is terminated + @xmath30 & fluent @xmath31 holds at time @xmath29 + & + @xmath32 &     @xmath33 +    the event calculus @xcite is a temporal logic for reasoning about events and their effects .",
    "it is a formalism that has been successfully used in numerous event recognition applications @xcite .",
    "the ontology of the event calculus comprises _ time points _ , i.e. integers of real numbers ; _ fluents _ , i.e. properties which have certain values in time ; and _ events _ , i.e. occurrences in time that may affect fluents and alter their value .",
    "the domain - independent axioms of the formalism incorporate the common sense _ law of inertia _",
    ", according to which fluents persist over time , unless they are affected by an event .",
    "we call the event calculus dialect used in this work simplified discrete event calculus ( ) . as its name implies , it is a simplified version of the discrete event calculus , a dialect which is equivalent to the classical event calculus when time ranges over integer domains @xcite .",
    "the building blocks of and its domain - independent axioms are presented in table [ table : ec ] . the first axiom in table [ table : ec ]",
    "states that a fluent @xmath31 holds at time @xmath29 if it has been initiated at the previous time point , while the second axiom states that @xmath31 continues to hold unless it is terminated .",
    "/2   and /2   are defined in an application - specific manner .",
    "examples will be presented shortly .",
    ".an annotated stream of lles [ cols= \" < , < \" , ]     [ example : incons ] table [ table : refinement_example ] presents the process of iled s refinement .",
    "the annotation lacks positive examples and the running hypothesis consists of a single clause @xmath16 , with a support set of two clauses .",
    "clause @xmath16 is inconsistent since it entails two negative examples , namely @xmath34 and @xmath35 .",
    "the program that results by applying the to the support set of clause @xmath16 is presented in table [ table : refinement_example ] , along with a minimal abductive explanation of the examples , in terms of @xmath36 atoms .",
    "atoms @xmath37 and @xmath38 correspond respectively to the second and third body literals of the first support set clause , which are added to the body of clause @xmath16 , resulting in the first specialization presented in table [ table : refinement_example ] .",
    "the third abduced atom @xmath39 corresponds to the second body literal of the second support set clause , which results in the second specialization in table [ table : refinement_example ] .",
    "together , these specializations form a refinement of clause @xmath16 that subsumes @xmath40 .",
    "minimal abductive solutions imply that the running hypothesis is minimally revised .",
    "revisions are minimal w.r.t .",
    "the length of the clauses in the revised hypothesis , but are not minimal w.r.t . the number of clauses , since the refinement strategy described above may result in refinements that include redundant clauses : selecting randomly one specialization from each support set clause to generate a refinement of a clause is sub - optimal , since there may exist other refinements with fewer clauses that also subsume the whole support set , as example [ ex : supported_refinement_example ] demonstrates . to avoid unnecessary increase at the hypothesis size , the generation of refinements is followed by a `` reduction '' step ( line [ line : reduce_refined ] of algorithm [ alg : revise ] )",
    "the function works as follows . for each refined clause @xmath16",
    ", it first generates all possible refinements from @xmath40 .",
    "this can be realized with the abductive refinement technique described above .",
    "the only difference is that the abductive solver is instructed to find all abductive explanations in terms of @xmath36 atoms , instead of one .",
    "once all refinements are generated , searches the revised hypothesis , augmented with all refinements of clause @xmath16 , to find a reduced set of refinements of @xmath16 that subsume @xmath40 .      in this section",
    "we prove the correctness of iled ( algorithm [ alg : iled_overall_strategy ] ) and show that it requires at most one pass over the historical memory to revise an input hypothesis .",
    "assume the incremental learning setting described in definition [ def : incremental_learning ] .",
    "iled ( algorithm [ alg : iled_overall_strategy ] ) requires at most one pass over @xmath41 to compute @xmath42 from @xmath43 .    for simplicity and without loss of generality",
    ", we assume that when a new example window @xmath44 arrives , iled revises @xmath43 by ( a ) refining an single clause @xmath45 or ( b ) adding a new clause @xmath46 .    in case ( a )",
    ", clause @xmath16 is replaced by a refinement @xmath47 such that @xmath48 . by property ( iii ) of the support set definition ( definition [ def : support_set ] ) , @xmath47 covers all positive examples that @xmath16 covers in @xmath41 , hence for the hypothesis @xmath49 it holds that @xmath50 and furthermore @xmath51 .",
    "hence @xmath52 , from which soundness for @xmath42 follows . in this case @xmath42",
    "is constructed from @xmath43 in a single step , i.e. by reasoning within @xmath44 without re - seeing other windows from @xmath41 .    in case",
    "( b ) , @xmath43 is revised w.r.t .",
    "@xmath44 to a hypothesis @xmath53 , where @xmath46 is a new clause that results from the generalization of a kernel set of @xmath44 . in response to the new clause addition",
    ", each window in @xmath41 must be checked and @xmath46 must be refined if necessary , as shown in line [ algline : iled - alg - line - ref-6 ] of algorithm [ alg : iled_overall_strategy ] .",
    "let @xmath54 denote the fragment of @xmath41 that has been tested at each point in time .",
    "initially , i.e. once @xmath46 is generated from @xmath44 , it holds that @xmath55 . at each window that is tested ,",
    "clause @xmath46 may ( i ) remain intact , ( ii ) be refined , or ( iii ) one of its refinements may be further refined .",
    "assume that @xmath56 is the first window where the new clause @xmath46 must be refined .",
    "at this point , @xmath57 , and it holds that @xmath46 is preservable in @xmath54 , since @xmath46 has not yet been refined . in @xmath58 , clause @xmath46 is replaced by a refinement @xmath59 such that @xmath60 .",
    "@xmath59 is preservable in @xmath54 , since it is a refinement of a preservable clause , and furthermore , it covers all positive examples that @xmath46 covers in @xmath44 , by means of the properties of the support set .",
    "hence the hypothesis @xmath61 is complete & consistent w.r.t .",
    "the same argument shows that if @xmath59 is further refined later on ( case ( iii ) above ) , the resulting hypothesis remains complete an consistent w.r.t .",
    "@xmath54 . hence , when all windows have been tested , i.e. when @xmath62 , the resulting hypothesis @xmath42 is complete & consistent w.r.t .",
    "@xmath63 and furthermore , each window in @xmath41 has been re - seen exactly once , thus @xmath42 is computed with a single pass over @xmath41 .",
    "non - monotonic ilp , and xhail in particular , have some important properties , by means of which they extend traditional ilp systems . as briefly discussed in section [ sec : learning_difficulties ] , these properties are related to some challenging issues that occur when learning normal logic programs , which non - monotonic ilp addresses in a robust and elegant way .",
    "we next discuss which of these properties are preserved by iled and which are sacrificed as a trade - off for efficiency , while briefly indicating directions for improvement in future work .    like xhail ,",
    "iled aims for soundness , that is , hypotheses which cover all given examples .",
    "xhail ensures soundness by generalizing all examples in one go .",
    "in contrast , iled preserves a memory of past experience for which newly acquired knowledge must account .",
    "soundness imposes restrictions on the tasks on which iled may be applied . in particular , we assume that the supervision is correct ( i.e. it contains no contradictions or missing knowledge ) and the domain is stationary , in the sense that knowledge already induced remains valid w.r.t",
    ". future instances , and retracting clauses or literals from the hypothesis at hand is never _ necessary _ in order to account for new incoming example windows .",
    "iled terminates in case its computations result in a dead - end , returning no solution .",
    "this results in treating cases such as _ concept drift _ @xcite , as noise .",
    "it is possible to relax the requirement for soundness and aim at an implementation that best - fits the training instances .",
    "handling noise and concept drift are promising extensions of iled .",
    "xhail is a state - of - the - art system among its inverse entailment - based peer algorithms , in terms of completeness .",
    "that is , the hypotheses computable by xhail form a superset of those computable by other prominent inverse entailment systems like progol and aleph @xcite .",
    "although iled preserves xhail s soundness , it does not preserve its completeness properties , due to the fact that iled operates incrementally to gain efficiency .",
    "thus there are cases where a hypothesis can be discovered by xhail , but be missed by iled . as an example , consider cases where a target hypothesis captures long - term temporal relations in the data , as for instance , in the following clause :    @xmath64    in such cases , if the parts of the data that are connected via a long - range temporal relation are given in different windows , iled has no way to correlate these parts in order to discover the temporal relation .",
    "however , one can always achieve xhail s functionality by increasing appropriately iled s window size .    an additional trade - off for efficiency",
    "is that not all of iled s revisions are fully evaluated on the historical memory .",
    "for example , a new clause generated by a kernel set of an incoming window @xmath65 is selected randomly among a set of possible choices , which are equally good locally , i.e. in window @xmath65 , but their quality may substantially differ globally .",
    "for instance , selecting a particular clause in order to cover a new example , may result in a large number of refinements and an unnecessarily lengthy hypothesis , as compared to one that may have been obtained by selecting a different initial clause .",
    "on the other hand , fully evaluating all possible choices throughout @xmath41 requires extensive inference in @xmath41 .",
    "thus simplicity and compression of hypotheses in iled has been sacrificed for efficiency .    in iled",
    ", a large part of the theorem proving effort that is involved in clause refinement reduces to computing subsumption between clauses , which is a hard task .",
    "moreover , just as the historical memory grows over time , so do ( in the general case ) the support sets of the clauses in the running hypothesis , increasing the cost of computing subsumption .",
    "however , as in principle the largest part of a search space is redundant and the support set focuses only on its interesting parts , one would not expect that the support set will grow to a size that makes subsumption computation less efficient than inference over the entire @xmath41 .",
    "moreover , the length of kernel set clauses ( hence that of support clauses ) is restricted by the size of incoming sliding windows .",
    "smaller windows result to smaller clauses , making the computation of subsumption relations tractable .",
    "in addition , a number of optimization techniques have been developed over the years and several generic subsumption engines have been proposed @xcite , some of which are able to efficiently compute subsumption relations between clauses comprising thousands of literals and hundreds of distinct variables .    the basic idea behind iled is to compress examples via bottom clause - like structures , in order to facilitate clause refinement , while learning a hypothesis incrementally .",
    "we see the idea behind the support set as being generic enough to be applied to any inverse entailment system that uses bottom clauses to guide the search , in order to provide support for more efficient clause refinement . in that case",
    ", the use of the support set should be modified accordingly to comply with the search method adopted by each system .",
    "for instance , in the work presented here , the support set works with xhail s search procedure , a minimality - driven , full search in the space of theories that subsume the kernel set , designed to address the non - monotonicity of normal logic programs .",
    "different settings may be developed .",
    "for example , once the requirement for soundness is abandoned in an effort to address noise , a heuristic search strategy could be adopted , like for example progol s @xmath66-like search .",
    "different settings would require changes to the way the support set works .",
    "in this section , we present experimental results from two real - world applications : activity recognition , using real data from the benchmark caviar video surveillance dataset , as well as large volumes of synthetic caviar data ; and city transport management ( ctm ) using data from the pronto project .",
    "part of our experimental evaluation aims to compare iled with xhail . to achieve this aim we had to implement xhail , because the original implementation was not publicly available until recently @xcite .",
    "all experiments were conducted on a 3.2 ghz linux machine with 4 gb of ram .",
    "the algorithms were implemented in python , using the clingo answer set solver @xcite as the main reasoning component , and a mongodb nosql database for the historical memory of the examples .",
    "the code and datasets used in these experiments can be downloaded from http://cer.iit.demokritos.gr/iled/experiments .      in activity recognition ,",
    "our goal is to learn definitions of high - level events , such as _ fighting , moving _ and _ meeting _ , from streams of low - level events like _ walking , standing , active _ and _ abrupt _ , as well as spatio - temporal knowledge .",
    "we use the benchmark caviar dataset for experimentation .",
    "details on the caviar dataset and more information about activity recognition applications may be found in @xcite .",
    "consider for instance the following definition of the _ fighting _ high - level event :    @xxx@ @xmath67 & @xmath68 + @xmath69 & @xmath70    clause ( [ eq : fighting_def_1 ] ) dictates that a period of time for which two persons @xmath71 and @xmath72 are assumed to be fighting is initiated at time @xmath29 if one of these persons is _ active _ , the other one is not _ inactive _ and their distance is smaller than 23 pixel positions .",
    "clause ( [ eq : fighting_def_2 ] ) states that _ fighting _ is initiated between two people when one of them moves _ abruptly _ , the other is not _ inactive _ , and the two persons are sufficiently close .",
    "clauses ( [ eq : fighting_def_3 ] ) and ( [ eq : fighting_def_4 ] ) state that _ fighting _ is terminated between two people when one of them walks or runs away from the other .",
    "caviar contains noisy data mainly due to human errors in the annotation @xcite .",
    "thus , for the experiments we manually selected a noise - free subset of caviar .",
    "the resulting dataset consists of 1000 examples ( that is , data for 1000 distinct time points ) concerning the high - level events _ moving _ , _ meeting _ and _ fighting_. these data , selected from different parts of the caviar dataset , were combined into a continuous annotated stream of narrative atoms , with time ranging from 0 to 1000 .",
    "in addition to the real data , we generated synthetic data on the basis of the manually - developed caviar event definitions described in @xcite .",
    "in particular , streams of low - level events concerning four different persons were created randomly and were then classified using the rules of @xcite .",
    "the final dataset was obtained by generating negative supervision via the closed world assumption and appropriately pairing the supervision with the narrative .",
    "the generated data consists of approximately @xmath73 examples , which amounts to 100 mb of data .",
    "the synthetic data is much more complex than the real caviar data .",
    "this is due to two main reasons : first , the synthetic data includes significantly more initiations and terminations of a high - level event , thus much larger learning effort is required to explain it .",
    "second , in the synthetic dataset more than one high - level event may be initiated or terminated at the same time point .",
    "this results in kernel sets with more clauses , which are hard to generalize simultaneously .",
    "the purpose of this experiment was to assess whether iled can efficiently generate hypotheses comparable in size and predictive quality to those of xhail .",
    "to this end , we compared both systems on real and synthetic data using 10-fold cross validation with replacement . for the real data ,",
    "90% of randomly selected examples , from the total of 1000 were used for training , while the remaining 10% was retained for testing . at each run",
    ", the training data were presented to iled in example windows of sizes 10 , 50 , 100 .",
    "the data were presented in one batch to xhail . for the synthetic data , 1000 examples were randomly sampled at each run from the dataset for training , while the remaining data were retained for testing .",
    "similar to the real data experiments , iled operated on windows of sizes of 10 , 50 , 100 examples and xhail on a single batch .",
    "table [ table : expr_xhail_iled ] presents the experimental results .",
    "training times are significantly higher for xhail , due to the increased complexity of generalizing kernel sets that account for the whole set of the presented examples at once .",
    "these kernel sets consisted , on average , of 30 to 35 16-literal clauses , in the case of the real data , and 60 to 70 16-literal clauses in the case of the synthetic data .",
    "in contrast , iled had to deal with much smaller kernel sets .",
    "the complexity of abductive search affects iled as well , as the size of the input windows grows .",
    "iled handles the learning task relatively well ( in approximately 30 seconds ) when the examples are presented in windows of 50 examples , but the training time increases almost 15 times if the window size is doubled .",
    "[ table : expr_xhail_iled ]    concerning the size of the produced hypothesis , the results show that in the case of real caviar data , the hypotheses constructed by iled are comparable in size with a hypothesis constructed by xhail . in the case of synthetic data ,",
    "the hypotheses returned by both xhail and iled were significantly more complex .",
    "note that for iled the hypothesis size decreases as the window size increases .",
    "this is reflected in the number of revisions that iled performs , which is significantly smaller when the input comes in larger batches of examples . in principle , the richer the input , the better the hypothesis that is initially acquired , and consequently , the less the need for revisions in response to new training instances .",
    "there is a trade - off between the window size ( thus the complexity of the abductive search ) and the number of revisions .",
    "a small number of revisions on complex data ( i.e. larger windows ) may have a greater total cost in terms of training time , as compared to a greater number of revisions on simpler data ( i.e. smaller windows ) .",
    "for example , in the case of window size 100 for the real caviar data , iled performs 5 revisions on average and requires significantly more time than in the case of a window size 50 , where it performs 9 revisions on average . on the other hand ,",
    "training times for windows of size 50 are slightly better than those obtained when the examples are presented in smaller windows of size 10 . in this case , the `` unit cost '' of performing revisions w.r.t a single window are comparable between windows of size 10 and 50 . thus the overall cost in terms of training time is determined by the total number of revisions , which is greater in the case of window size 10 .",
    "concerning predictive quality , the results indicate that iled s precision and recall scores are comparable to those of xhail . for larger input windows , precision and recall are almost the same as those of xhail .",
    "this is because iled produces better hypotheses from larger input windows .",
    "precision and recall are smaller in the case of synthetic data for both systems , because the testing set in this case is much larger and complex than in the case of real data .      the purpose of this experiment was to assess the scalability of iled .",
    "the experimental setting was as follows : sets of examples of varying sizes were randomly sampled from the synthetic dataset .",
    "each such example set was used as a training set in order to acquire an initial hypothesis using iled .",
    "then a new window which did not satisfy the hypothesis at hand was randomly selected and presented to iled , which subsequently revised the initial hypothesis in order to account for both the historical memory ( the initial training set ) and the new evidence . for historical memories ranging from @xmath74 to @xmath73 examples , a new training window of size 10 , 50 and",
    "100 was selected from the whole dataset .",
    "the process was repeated ten times for each different combination of historical memory and new window size .",
    "figure [ fig : iled - scalability ] presents the average revision times .",
    "the revision times for new window sizes of 10 and 50 examples are very close and therefore omitted to avoid clutter .",
    "the results indicate that revision time grows polynomially in the size of the historical memory .          in this section we present experimental results from the domain of city transport management ( ctm ) .",
    "we use data from the pronto project . in pronto ,",
    "the goal was to inform the decision - making of transport officials by recognising high - level events related to the punctuality of a public transport vehicle ( bus or tram ) , passenger / driver comfort and safety .",
    "these high - level events were requested by the public transport control centre of helsinki , finland , in order to support resource management .",
    "low - level events were provided by sensors installed in buses and trams , reporting on changes in position , acceleration / deceleration , in - vehicle temperature , noise level and passenger density . at the time of the project ,",
    "the available datasets included only a subset of the anticipated low - level event types as some low - level event detection components were not functional .",
    "for the needs of the project , therefore , a synthetic dataset was generated .",
    "the synthetic pronto data has proven to be considerably more challenging for event recognition than the real data , and therefore we chose the former for evaluating iled @xcite .",
    "the ctm dataset contains @xmath75 examples , which amount approximately to 70 mb of data .",
    "in contrast to the activity recognition application , the manually developed high - level event definitions of ctm that were used to produce the annotation for learning , form a hierarchy . in these hierarchical event definitions , it is possible to define a function level that maps all high - level events to non - negative integers as follows : a level-1 event is defined in terms of low - level events ( input data ) only .",
    "an level-@xmath76 event is defined in terms of at least one level-@xmath77 event and a possibly empty set of low - level events and high - level events of level below @xmath77 .",
    "hierarchical definitions are significantly more complex to learn as compared to non - hierarchical ones .",
    "this is because initiations and terminations of events in the lower levels of the hierarchy appear in the bodies of event definitions in the higher levels of the hierarchy , hence all target definitions must be learnt simultaneously . as we show in the experiments , this has a striking effect on the required learning effort .",
    "a solution for simplifying the learning task is to utilize knowledge about the domain ( the hierarchy ) , learn event definitions separately , and use the acquired theories from lower levels of the event hierarchy as non - revisable background knowledge when learning event definitions for the higher levels .",
    "below is a fragment of the ctm event hierarchy :    @xmath78    clauses ( [ eq : punctuality_init1 ] ) and ( [ eq : punctuality_init2 ] ) state that a period of time for which vehicle @xmath79 is said to be _ non - punctual _ is initiated if it enters a stop later , or leaves a stop earlier than the scheduled time .",
    "clauses ( [ eq : punctuality_term1 ] ) and ( [ eq : punctuality_term2 ] ) state that the period for which vehicle @xmath79 is said to be non - punctual is terminated when the vehicle arrives at a stop earlier than , or at the scheduled time .",
    "the definition of non - punctual vehicle uses two low - level events , @xmath80 and @xmath81 .",
    "clauses ( [ eq : driving_qual_init1])-([eq : driving_qual_term2 ] ) define _ low driving quality_. essentially , driving quality is said to be low when the driving style is unsafe and the vehicle is non - punctual . driving quality",
    "is defined in terms of high - level events ( we omit the definition of driving style to save space ) .",
    "therefore , the bodies of the clauses defining driving quality include /2 and /2 literals .",
    "[ table : expr_ctm_zero_knowledge ]    in this experiment , we tried to learn simultaneously definitions for all target concepts , a total of nine interrelated high - level events , seven of which are level-1 , one is level-2 and one is level-3 .",
    "according to the employed language bias , each such high - level event must be learnt , while at the same time it may be present in the body of another high - level event in the form of ( potentially negated ) /2 , /2 , or /2 predicate .",
    "the total number of low - level events involved is 22 .",
    "we used tenfold cross validation with replacement , on small amounts of data , due to the complexity of the learning task . in each run of the cross validation",
    ", we randomly sampled 20 examples from the ctm dataset , 90% of which was used for training and 10% was retained for testing .",
    "this example size was selected after experimentation , in order for xhail to be able to perform in an acceptable time frame .",
    "each sample consisted of approximately 150 atoms ( narrative and annotation ) .",
    "the examples were given to iled in windows of granularity 5 and 10 , and to xhail in one batch .",
    "table [ table : expr_ctm_zero_knowledge ] presents the average training times , hypothesis size , number of revisions , precision and recall .",
    "iled took on average 1 - 2 hours to complete the learning task , for windows of 5 and 10 examples , while xhail required more than 4 hours on average to learn hypotheses from batches of 20 examples .",
    "compared to activity recognition , the learning setting requires larger kernel set structures that are hard to reason with .",
    "an average kernel set generated from a batch of just 20 examples consisted of approximately 30 - 35 clauses , with 60 - 70 literals each .",
    "like the activity recognition experiments , precision and recall scores for iled are comparable to those of xhail , with the latter being slightly better . unlike the activity recognition experiments , precision and recall had a large diversity between different runs .",
    "due to the complexity of the ctm dataset , the constructed hypotheses had a large diversity , depending on the random samples that were used for training .",
    "for example , some high - level event definitions were unnecessarily lengthy and difficult to be understood by a human expert .",
    "on the other hand , some level-1 definitions could in some runs of the experiment , be learnt correctly even from a limited amount of data .",
    "such definitions are fairly simple , consisting of one initiation and one termination rule , with one body literal in each case .",
    "this experiment demonstrates several limitations of learning in large and complex applications .",
    "the complexity of the domain increases the intensity of the learning task , which in turn makes training times forbidding , even for small amount of data such as 20 examples ( approximately 150 atoms ) .",
    "this forces one to process small sets of examples at at time , which in complex domains like ctm , results to over - fitted theories and rapid increase in hypothesis size .      in an effort to improve the experimental results , we utilized domain knowledge about the event hierarchy in ctm and attempted to learn high - level events in different levels separately .",
    "to do so , we had to learn a complete definition from the entire dataset for a high - level event , before utilizing it as background knowledge in the learning process of a higher - level event . to facilitate the learning task",
    "further , we also used expert knowledge about the relation between specific low - level and high - level events , excluding from the language bias mode declarations which were irrelevant to the high - level event that is being learnt at each time .",
    "[ table : expr_level_wise_learning ]    the experimental setting was therefore as follows : starting from the level-1 target events , we processed the whole ctm dataset in windows of 10 , 50 and 100 examples with iled .",
    "each high - level event was learnt independently of the others .",
    "once complete definitions for all level-1 high - level events were constructed , they were added to the background knowledge .",
    "then we proceeded with learning the definition for the single level-2 event .",
    "finally , after successfully constructing the level-2 definition , we performed learning in the top - level of the hierarchy , using the previously constructed level-1 and level-2 event definitions as background knowledge . we did not attempt a comparison with xhail , since due to the amounts of data in ctm , the latter is not able to operate on the entire dataset .",
    "table [ table : expr_level_wise_learning ] presents the results . for level-1 events ,",
    "scores are presented as minimum - maximum pairs .",
    "for instance , the training times for level-1 events with windows of 10 examples , ranges from 4.46 to 4.88 minutes .",
    "levels 2 and 3 have just one definition each , therefore table [ table : expr_level_wise_learning ] presents the respective scores from each run .",
    "training times , hypothesis sizes and overall numbers of revisions are comparable for all levels of the event hierarchy .",
    "level-1 event definitions were the easiest to acquire , with training times ranging approximately between 4.50 to 7 minutes .",
    "this was expected since clauses in level-1 definitions are significantly simpler than level-2 and level-3 ones .",
    "the level-2 event definition was the hardest to construct with training times ranging between 8 and 10 minutes , while a significant number of revisions was required for all window granularities .",
    "the definition of this high - level event ( _ drivingstyle _ ) is relatively complex , in contrast to the simpler level-3 definition , for which training times are comparable to the ones for level-1 events .",
    "the largest parts of training times were dedicated to checking an already correct definition against the part of the dataset that had not been processed yet .",
    "that is , for all target events , iled converged to a complete definition relatively quickly , i.e. in approximately 1.5 to 3 minutes after the initiation of the learning process . from that point on , the extra time",
    "was spent on testing the hypothesis against the new incoming data .",
    "window granularity slightly affects the produced hypothesis for all target high - level events .",
    "indeed , the definitions constructed with windows of 10 examples are slightly larger than the ones constructed with larger window sizes of 50 and 100 examples .",
    "notably , the definitions constructed with windows of granularity 50 and 100 , were found concise , meaningful and very close to the actual hand - crafted rules that were utilized in pronto .",
    "a thorough review of the drawbacks of state - of - the - art ilp systems with respect to non - monotonic domains , as well as the deficiencies of existing approaches to learning event calculus programs can be found in @xcite .",
    "the main obstacle , common to many learners which combine ilp with some form of abduction , like progol5 @xcite , alecto @xcite , hail @xcite and imparo @xcite , is that they can not perform abduction through negation and are thus essentially limited to observational predicate learning .",
    "tal @xcite is a top - down non - monotonic learner which is able to solve the same class of problems as xhail .",
    "it obtains a top theory by appropriately mapping the ilp problem at hand to a corresponding alp instance , so that solutions to the latter may be translated to solutions for the initial ilp problem .",
    "recently , the main ideas behind tal were employed in the aspal system @xcite , an inductive learner which relies on answer set programming as a unifying abductive - inductive framework .",
    "aspal obtains a top theory of _ skeleton rules _ by forming all possible clause structures that may be formed from the mode declarations .",
    "each such structure is complemented by a set of properly formed abducible predicates .",
    "abductive reasoning on a proper _ meta - level _ representation of the original ilp problem returns a set of such abducibles , which , due to their construction , allow to hypothesize on how variables and constants in the skeleton rules are linked together .",
    "thus , the abduced atoms are prescriptions on how variable and constant terms in the original skeleton rules should be handled in order to obtain a hypothesis .",
    "this way , aspal may induce all possible hypotheses w.r.t to a certain ilp problem , as well as optimal ones , by computing minimal sets of abducibles .",
    "the combination of ilp with alp has recently been applied to _ meta - interpretive learning _ ( mil ) , a learning framework , where the goal is to obtain hypotheses in the presence of a meta - interpreter .",
    "the latter is a higher - order program , hypothesizing about predicates or even rules of the domain .",
    "given such background knowledge and sets of positive and negative examples , mil uses abduction w.r.t . the meta - interpreter to construct first - order hypotheses .",
    "mil can be realized both in prolog and in answer set programming , and it has been implemented in the metagol system @xcite .",
    "application examples involve learning definite clause grammars @xcite , discovering relations between entities and learning simple robot action strategies @xcite .",
    "mil is an elegant framework , able to address difficult problems that are under - explored in traditional ilp , like handling predicate invention and learning mutually recursive programs .",
    "however , it has a number of important drawbacks .",
    "first , its expressivity is significantly limited , as iml is currently restricted to _ dyadic datalog _ , i.e. datalog where the arity of each predicate is at most two . as noted in @xcite , constructing meta - interpreters for richer fragments of first - order logic is not a straight - forward task and requires careful mathematical analysis .",
    "second , given the increased computational complexity of higher - order reasoning , scaling to large volumes of data is a potential bottleneck for mil .    in the non - monotonic setting , traditional ilp approaches that cover the examples sequentially can not ensure soundness and completeness @xcite . to deal with this issue , non - monotonic learners like xhail , tal and aspal generalize all available examples in one go .",
    "the disadvantage of this approach , however , is poor scalability .",
    "a recent advancement which addresses the issue of scalability in non - monotonic ilp is presented in @xcite .",
    "this approach combines the top - down , meta - level learning of tal and aspal , with theory revision as `` non - monotonic ilp '' @xcite , to address the `` grounding bottleneck '' in aspal s functionality . the top theory derived by aspal , as a starting point for its search ,",
    "is based on combinations of the available mode declarations and grows exponentially with the length of its clauses .",
    "thus , obtaining a ground program from this top theory is often very expensive and can cause a learning task to become intractable @xcite .",
    "raspal , the system proposed in @xcite , addresses this issue by imposing bounds on the length of the top theory .",
    "partial hypotheses of specified clause length are iteratively obtained in a refinement loop . at each iteration of this loop , the partial hypothesis obtained from the previous refinement step",
    "is further refined using theory revision as described in @xcite .",
    "the process continues until a complete and consistent hypothesis is obtained .",
    "the authors show that this approach results in shorter ground programs and derives a complete and consistent hypothesis , if one is derivable from the input data .",
    "an important difference between raspal and our approach is that the former addresses scalability as related to application domains , which may require a complex language bias , while our approach scales to potentially simpler , but massive volumes of sequential data , typical in temporal applications .",
    "tal , aspal and raspal are top - down learners . in the work presented here ,",
    "xhail , being a bottom - up non - monotonic learning system was the natural choice as the basis of our approach , since we intended to provide a clause refinement search bias by means of most - specific clauses , as in @xcite . in that work ,",
    "the theory revision system forte @xcite is enhanced by porting progol s bottom set construction routine to its functionality , towards a more efficient refinement operator .",
    "the resulting system , forte_mbc , works as follows : when a clause @xmath16 must be refined , forte_mbc uses mode declarations and an inverse entailment search in the background knowledge to construct a bottom clause from a positive example covered by @xmath16 .",
    "it then searches for antecedents within the bottom clause . as in the case of iled , the constrained search space results in a more efficient clause refinement process",
    "however forte_mbc ( like forte itself ) learns horn theories and does not support non - observational predicate learning , thus it can not be used for the revision of event calculus programs .",
    "in addition , it can not operate on an empty hypothesis ( i.e. it can not induce a hypothesis from scratch ) . another important difference between forte_mbc and iled",
    "is the way that the former handles a potential incompleteness which may result from the specialization of a clause . in particular , once a clause is specialized , forte_mbc checks again the whole database of examples .",
    "if some positive examples have become unprovable due to the specialization , forte_mbc picks a different positive example covered by the initial , inconsistent clause @xmath16 , constructs a new bottom clause and searches for a new specialization of clause @xmath16 .",
    "the process continues until the original coverage in the example database is restored .",
    "in contrast , by means of the support set , the specializations performed by iled preserve prior coverage in the historical memory , thus saving inference effort significantly .    as mentioned in @xcite",
    ", there is a renewed interest in scaling theory revision systems and applications in the last few years , due to the availability of large - scale domain knowledge in various scientific disciplines @xcite .",
    "temporal and stream data are no exception and there is a need for scalable theory revision techniques in event - based domains . however , most theory revision systems , such as the systems described in @xcite limit their applicability to horn theories .",
    "a well - known theory revision system is inthelex @xcite .",
    "it is a fully incremental system that learns / revises datalog theories and has been used in the study of several aspects of incremental learning . in particular , order effects in some simple learning tasks with ilp are discussed in @xcite , and concept drift in @xcite . in @xcite",
    "the authors present an approach towards scaling inthelex .",
    "in contrast to most ilp systems that keep all examples in the main memory , @xcite follows an external memory implementation , which is the approach adopted by iled .",
    "moreover , in that work the authors associate clauses in the theory at hand with examples they cover , via a relational schema .",
    "thus , when a clause is refined , only the examples that were previously covered by this clause are checked .",
    "similarly , when a clause is generalized , only the negative examples are checked again .",
    "the scalable version of inthelex presented in @xcite maintains alternative versions of the hypothesis at each step , allowing to backtrack to previous states .",
    "in addition , it keeps in memory several statistics related to the examples that the system has already seen , such as the number of refinements that each example has caused , a `` refinement history '' of each clause , etc .    on the other hand",
    ", inthelex has some limitations that make it inappropriate for inducing / revising event calculus programs for event recognition applications .",
    "first , the restriction of its input language to datalog limits its applicability to richer , relational event domains .",
    "for instance , complex relations between entities can not be easily expressed in inthelex .",
    "second , the use of background knowledge is limited , excluding for instance auxiliary clauses that may be used for spatio - temporal reasoning during learning time .",
    "third , although inthelex uses abduction for the completion of imperfect input data , it relies on observational predicate learning , meaning that it is not able to reason with predicates which are not directly observable in the examples .",
    "therefore it can not be used for learning event definitions .",
    "we presented an incremental ilp system , iled , for machine learning knowledge bases for event recognition , in the form of event calculus theories .",
    "iled combines techniques from non - monotonic ilp and in particular , the xhail algorithm , with theory revision .",
    "it acquires an initial hypothesis from the first available piece of data , and revises this hypothesis as new data arrive .",
    "revisions account for all accumulated experience .",
    "the main contribution of iled is that it scales - up xhail to large volumes of sequential data with a time - like structure , typical of event - based applications . by means of a compressive memory structure that supports clause refinement ,",
    "iled has a scalable , single - pass revision strategy , thanks to which the cost of theory revision grows as a tractable function of the perceived experience . in this work ,",
    "iled was evaluated on an activity recognition application and a transport management application .",
    "the results indicate that iled is significantly more efficient than xhail , without compromising the quality of the generated hypothesis in terms of predictive accuracy and hypothesis size",
    ". moreover , iled scales adequately to large data volumes which xhail can not handle .",
    "future work concerns mechanisms for handling noise and concept drift .",
    "this work is partly funded by the eu project speedd ( fp7 619435 ) .",
    "we would like to thank the reviewers of the machine learning journal for their valuable comments on the first version of the paper .",
    "[ def : lp_basic_notions ] a term is a constant , a variable , or an expression of the form @xmath82 where @xmath83 is a function symbol and @xmath84 are terms .",
    "a term substitution is a function from the set of terms to itself .",
    "an atom is an expression of the form @xmath85 where @xmath86 is a predicate symbol and @xmath84 are terms .",
    "a literal is either an atom @xmath87 ( positive literal ) or its negation @xmath88 ( negative literal ) .",
    "a clause @xmath16 is an expression of the form @xmath89 where @xmath87 is an atom and @xmath90 are literals .",
    "@xmath87 is called the head of clause @xmath16 , and @xmath91 is called the body of the clause .",
    "a fact is a clause of the form @xmath92 and an integrity constraint is a clause of the form @xmath93 .",
    "a logic program is a collection of clauses .",
    "a clause or a logic program is _ horn _ if it contains no negated literals and _ normal _ otherwise .",
    "[ def : interpretations_and_models ] given a logic program @xmath94 an herbrand interpretation @xmath95 is a subset of the set of all possible groundings of @xmath94 .",
    "@xmath95 satisfies a literal @xmath87 ( resp .",
    "@xmath96 ) iff @xmath97 ( resp .",
    "@xmath98 ) .",
    "@xmath95 satisfies a set of ground atoms iff it satisfies each one of them and it satisfies a ground clause iff it satisfies the head , or does not satisfy at least one body literal .",
    "@xmath95 is a herbrand model of @xmath94 iff it satisfies every ground instance of every clause in @xmath94 and it is a minimal model iff no strict subset of @xmath95 is a model of @xmath94 .",
    "@xmath95 is a _ stable model _ of @xmath94 iff",
    "it is a minimal model of the horn program that results from the ground instances of @xmath94 after the removal of all clauses with a negated literal not satisfied by @xmath95 , and all negative literals from the remaining clauses .",
    "[ def : mode_declarations ] a _ mode declaration _ is either a head or body declaration , respectively , @xmath99 and @xmath100 , where @xmath101 is called a schema . a schema @xmath101 is a ground literal containing _ placemarkers_. a placemarker is either @xmath102 ( input ) @xmath103 ( output ) or @xmath104 ( ground ) , where @xmath105 is a constant .",
    "the distinction between input and output terms in mode declarations is that any input term in a body literal must be an input term in the head , or an output term in some preceding body literal .",
    "[ def : mode_language ] a set @xmath8 of mode declarations defines a language @xmath14 .",
    "a clause @xmath16 is in @xmath14 if it results from the declarations in @xmath8 by replacing input and output placemarkers by variables and replacing ground placemarkers with ground terms . in particular @xmath106",
    "iff its head atom ( respectively each of its body literals ) is constructed from the schema @xmath101 in a @xmath99 atom ( resp . in a @xmath100 atom ) in @xmath8 as follows :    * by replacing an output @xmath107 placemarker by a new variable .",
    "* by replacing an input @xmath108 placemarker by a variable that appears in the head atom , or in a previous body literal .",
    "* by replacing a ground @xmath109 placemarker by a ground term .",
    "[ def : variable_depth ] let @xmath16 be a clause and @xmath71 a variable symbol .",
    "the depth @xmath112 of @xmath71 is defined recursively as follows : + @xmath113 +   + where @xmath114 are the variable symbols that appear in all literals in the body of @xmath16 in which @xmath71 also appears .",
    "[ def : depth_mode_language ] let @xmath8 be a set of mode declarations , @xmath115 a non - negative integer and @xmath16 a clause .",
    "@xmath16 is in the depth - bounded mode language @xmath116 iff @xmath106 ( see definition [ def : mode_language ] ) and for each variable symbol @xmath71 that appears in @xmath16 it holds that @xmath117 ( see definition [ def : variable_depth ] ) .",
    "a hypothesis @xmath9 is in @xmath116 iff @xmath118 for each @xmath111 .",
    "[ def : most_specific_clause ] let @xmath116 be the depth - bounded mode language as in definition [ def : depth_mode_language ] , @xmath27 a set of examples and @xmath4 some background theory .",
    "let @xmath119 .",
    "a clause @xmath120 is most - specific , relative to @xmath27 , iff it does not @xmath15-subsume any other clause in @xmath121 ."
  ],
  "abstract_text": [
    "<S> event recognition systems rely on properly engineered knowledge bases of event definitions to infer occurrences of events in time . </S>",
    "<S> the manual development of such knowledge is a tedious and error - prone task , thus event - based applications may benefit from automated knowledge construction techniques , such as inductive logic programming ( ilp ) , which combines machine learning with the declarative and formal semantics of first - order logic . </S>",
    "<S> however , learning temporal logical formalisms , which are typically utilized by logic - based event recognition systems is a challenging task , which most ilp systems can not fully undertake . </S>",
    "<S> in addition , event - based data is usually massive and collected at different times and under various circumstances . </S>",
    "<S> ideally , systems that learn from temporal data should be able to operate in an incremental mode , that is , revise prior constructed knowledge in the face of new evidence . </S>",
    "<S> most ilp systems are batch learners , in the sense that in order to account for new evidence they have no alternative but to forget past knowledge and learn from scratch . given the increased inherent complexity of ilp and the volumes of real - life temporal data , this results to algorithms that scale poorly . in this work we present an incremental method for learning and revising event - based knowledge , in the form of event calculus programs . </S>",
    "<S> the proposed algorithm relies on abductive - inductive learning and comprises a scalable clause refinement methodology , based on a compressive summarization of clause coverage in a stream of examples . </S>",
    "<S> we present an empirical evaluation of our approach on real and synthetic data from activity recognition and city transport applications . </S>"
  ]
}