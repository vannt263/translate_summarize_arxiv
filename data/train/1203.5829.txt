{
  "article_text": [
    "non - linear functionals of probability densities @xmath6 of the form @xmath7 arise in applications of information theory , machine learning , signal processing and statistical estimation .",
    "important examples of such functionals include shannon @xmath8 and rnyi @xmath9 entropy , and the quadratic functional @xmath10 . in these applications ,",
    "the functional of interest often must be estimated empirically from sample realizations of the underlying densities .",
    "functional estimation has received significant attention in the mathematical statistics community .",
    "however , estimators of functionals of multivariate probability densities @xmath6 suffer from mean square error ( mse ) rates which typically decrease with dimension @xmath1 of the sample as @xmath2 , where @xmath0 is the number of samples and @xmath11 is a positive rate parameter .",
    "examples of such estimators include kernel density estimators  @xcite , @xmath4-nearest neighbor ( @xmath4-nn ) density estimators  @xcite , @xmath4-nn entropy functional estimators  @xcite , intrinsic dimension estimators  @xcite , divergence estimators  @xcite , and mutual information estimators .",
    "this slow convergence is due to the curse of dimensionality . in this paper",
    ", we introduce a simple affine combination of an ensemble of such slowly convergent estimators and show that the weights in this combination can be chosen to significantly improve the rate of mse convergence of the weighted estimator .",
    "in fact our ensemble averaging method can improve mse convergence to the parametric rate @xmath5 .",
    "specifically , for @xmath1-dimensional data , it has been observed that the variance of estimators of functionals @xmath12 decays as @xmath5 while the bias decays as @xmath13 . to accelerate the slow rate of convergence of the bias in high dimensions ,",
    "we propose a weighted ensemble estimator for ensembles of estimators that satisfy conditions @xmath14([be ] ) and @xmath15([ve ] ) defined in sec .",
    "optimal weights , which serve to lower the bias of the ensemble estimator to @xmath16 , can be determined by solving a convex optimization problem .",
    "remarkably , this optimization problem does not involve any density - dependent parameters and can therefore be performed offline .",
    "this then ensures mse convergence of the weighted estimator at the parametric rate of @xmath5 .",
    "when the density @xmath6 is @xmath17 times differentiable , certain estimators of functionals of the form @xmath18 , proposed by birge and massart  @xcite , laurent  @xcite and gin and mason  @xcite , can achieve the parametric mse convergence rate of @xmath5 .",
    "the key ideas in  @xcite are : ( i ) estimation of quadratic functionals @xmath19 with mse convergence rate @xmath5 ; ( ii ) use of kernel density estimators with kernels that satisfy the following symmetry constraints : @xmath20 for @xmath21 ; and finally ( iii ) truncating the kernel density estimate so that it is bounded away from @xmath22 . by using these ideas ,",
    "the estimators proposed by  @xcite are able to achieve parametric convergence rates .",
    "in contrast , the estimators proposed in this paper require additional higher order smoothness conditions on the density , i.  e.   the density must be @xmath23 times differentiable . however , our estimators are much simpler to implement in contrast to the estimators proposed in  @xcite . in particular , the estimators in  @xcite require separately estimating quadratic functionals of the form @xmath19 , and using truncated kernel density estimators with symmetric kernels ( [ eq : symmetrickernels ] ) , conditions that are not required in this paper .",
    "our estimator is a simple affine combination of an ensemble of estimators , where the ensemble satisfies conditions @xmath14 and @xmath15 .",
    "such an ensemble can be trivial to implement .",
    "for instance , in this paper we show that simple uniform kernel plug - in estimators ( [ eq : plugin ] ) satisfy conditions @xmath14 and @xmath15 .",
    "ensemble based methods have been previously proposed in the context of classification .",
    "for example , in both boosting  @xcite and multiple kernel learning  @xcite algorithms , lower complexity weak learners are combined to produce classifiers with higher accuracy .",
    "our work differs from these methods in several ways .",
    "first and foremost , our proposed method performs estimation rather than classification .",
    "an important consequence of this is that the weights we use are _ data independent _ , while the weights in boosting and multiple kernel learning must be estimated from training data since they depend on the unknown distribution .",
    "the remainder of the paper is organized as follows .",
    "we formally describe the weighted ensemble estimator for a general ensemble of estimators in section  [ sec : genmeth ] , and specify conditions @xmath14 and @xmath15 on the ensemble that ensure that the ensemble estimator has a faster rate of mse convergence . under the assumption that conditions @xmath14 and @xmath15 are satisfied",
    ", we provide an mse optimal set of weights as the solution to a convex optimization([convexsoltheory ] ) .",
    "next , we shift the focus to entropy estimation in section  [ sec : entropyest ] , propose an ensemble of simple uniform kernel plug - in entropy estimators , and show that this ensemble satisfies conditions @xmath14 and @xmath15 .",
    "subsequently , we apply the ensemble estimator theory in section  [ sec : genmeth ] to the problem of entropy estimation using this ensemble of kernel plug - in estimators . we present simulation results in section  [ sec : exp ] that illustrate the superior performance of this ensemble entropy estimator in the context of ( i ) estimation of the panter - dite distortion - rate factor  @xcite and ( ii ) testing the probability distribution of a random sample .",
    "we conclude the paper in section  [ sec : con ] .",
    "we will use bold face type to indicate random variables and random vectors and regular type face for constants .",
    "we denote the statistical expectation operator by the symbol @xmath24 and the conditional expectation given random variable @xmath25 using the notation @xmath26 .",
    "we also define the variance operator as @xmath27 = { { { \\mathbb{e}}}}[({\\mathbf}{x}-{{{\\mathbb{e}}}}[{\\mathbf}{x}])^2 ] $ ] and the covariance operator as @xmath28 = { { { \\mathbb{e}}}}[({\\mathbf}{x}-{{{\\mathbb{e}}}}[{\\mathbf}{x}])({\\mathbf}{y}-{{{\\mathbb{e}}}}[{\\mathbf}{y } ] ) ] $ ] .",
    "we denote the bias of an estimator by @xmath29 .",
    "[ sec : genmeth ]    let @xmath30 denote a set of parameter values",
    ". for a parameterized ensemble of estimators @xmath31 of @xmath32 , define the weighted ensemble estimator with respect to weights @xmath33 as @xmath34 where the weights satisfy @xmath35 .",
    "this latter sum - to - one condition guarantees that @xmath36 is asymptotically unbiased if the component estimators @xmath31 are asymptotically unbiased .",
    "let this ensemble of estimators @xmath31 satisfy the following two conditions :    * @xmath14 the bias is given by @xmath37 where @xmath38 are constants that depend on the underlying density , @xmath39 is a finite index set with cardinality @xmath40 , @xmath41 and @xmath42 , and @xmath43 are basis functions that depend only on the estimator parameter @xmath44 .",
    "* @xmath15 the variance is given by @xmath45    [ lemma : weightedensemble ] for an ensemble of estimators @xmath31 , assume that the conditions @xmath14 and @xmath15 hold . then , there exists a weight vector @xmath46 such that @xmath47   = o(1/t).\\ ] ] this weight vector can be found by solving the following convex optimization problem : @xmath48 where @xmath43 is the basis defined in ( 2.1 ) .",
    "the bias of the ensemble estimator is given by @xmath49    denote the covariance matrix of @xmath50 by @xmath51 .",
    "let @xmath52 .",
    "observe that by ( [ variance_ensemble ] ) and the cauchy - schwarz inequality , the entries of @xmath53 are @xmath54 . the variance of the weighted estimator @xmath36 can then be bounded as follows : @xmath55    we seek a weight vector @xmath56 that ( i ) ensures that the bias of the weighted estimator is @xmath16 and ( ii ) has low @xmath57 norm @xmath58 in order to limit the contribution of the variance , and the higher order bias terms of the weighted estimator . to this end , let @xmath59 be the solution to the convex optimization problem defined in ( 2.3 ) .",
    "the solution @xmath46 is the solution of @xmath60 where @xmath61 and @xmath62 are defined below .",
    "let @xmath63 be the vector of ones : @xmath64_{1 \\times l}$ ] ; and let @xmath65 , for each @xmath66 be given by @xmath67 $ ] . define @xmath68'$ ] , @xmath69'$ ] and @xmath70_{(i+1 ) \\times 1}$ ] .    since @xmath71 , the system of equations @xmath72",
    "is guaranteed to have at least one solution ( assuming linear independence of the rows @xmath73 ) .",
    "the minimum squared norm @xmath74 is then given by @xmath75    consequently , by ( [ bias_ensembleestimate ] ) , the bias @xmath76 = o(\\sqrt{l\\eta_l(d)}/\\sqrt{t})$ ] . by ( [ eq : var ] ) ,",
    "the estimator variance @xmath77 = o(l\\eta_l(d)/t)$ ] .",
    "the overall mse is also therefore of order @xmath78 .    for any fixed dimension @xmath1 and fixed number of estimators @xmath79 in the ensemble independent of sample size @xmath0 ,",
    "the value of @xmath80 is also independent of @xmath0 .",
    "stated mathematically , @xmath81 for any fixed dimension @xmath1 and fixed number of estimators @xmath79 independent of sample size @xmath0 .",
    "this concludes the proof .    in the next section",
    ", we will verify conditions @xmath82([be ] ) and @xmath83([ve ] ) for plug - in estimators @xmath84 of entropy - like functionals @xmath7 .",
    "our focus is the estimation of general non - linear functionals @xmath12 of @xmath1-dimensional multivariate densities @xmath6 with known finite support @xmath85^d$ ] , where @xmath12 has the form @xmath86 for some smooth function @xmath87 .",
    "let @xmath88 denote the boundary of @xmath89 .",
    "assume that @xmath90 i.i.d realizations @xmath91 are available from the density @xmath6 .      the truncated _ uniform kernel _ density estimator is defined below . for any positive real number @xmath92 ,",
    "define the distance @xmath93 to be : @xmath94 .",
    "define the truncated kernel region for each @xmath95 to be @xmath96 , and the volume of the truncated uniform kernel to be @xmath97 .",
    "note that when the smallest distance from @xmath98 to @xmath88 is greater than @xmath99 , @xmath100 .",
    "let @xmath101 denote the number of samples falling in @xmath102 : @xmath103 .",
    "the truncated uniform kernel density estimator is defined as @xmath104    the plug - in estimator of the density functional is constructed using a data splitting approach as follows .",
    "the data is randomly subdivided into two parts @xmath105 and @xmath106 of @xmath107 and @xmath108 points respectively . in the first stage , we form the kernel density estimate @xmath109 at the @xmath107 points @xmath105 using the @xmath108 realizations @xmath106 . subsequently , we use the @xmath107 samples @xmath105 to approximate the functional @xmath12 and obtain the plug - in estimator : @xmath110 also define a standard kernel density estimator @xmath111 , which is identical to @xmath112 except that the volume @xmath113 is always set to the untruncated value @xmath114 . define @xmath115 the estimator @xmath116 is identical to the estimator of gyrfi and van der meulen  @xcite . observe that the implementation of @xmath116 , unlike @xmath117 , does not require knowledge about the support of the density .",
    "we make a number of technical assumptions that will allow us to obtain tight mse convergence rates for the kernel density estimators defined above .",
    "@xmath118 : assume that @xmath119 for some rate constant @xmath120 , and assume that @xmath108 , @xmath107 and @xmath0 are linearly related through the proportionality constant @xmath121 with : @xmath122 , @xmath123 and @xmath124 .",
    "@xmath125 : let the density @xmath6 be uniformly bounded away from @xmath22 and upper bounded on the set @xmath89 , i.e. , there exist constants @xmath126 , @xmath127 such that @xmath128 @xmath129 .",
    "@xmath130 : assume that the density @xmath6 has continuous partial derivatives of order @xmath1 in the interior of the set @xmath89 , and that these derivatives are upper bounded .",
    "@xmath131 : assume that the function @xmath87 has @xmath132 partial derivatives w.r.t .",
    "the argument @xmath6 , where @xmath133 satisfies the condition @xmath134 .",
    "denote the @xmath135-th partial derivative of @xmath87 wrt @xmath136 by @xmath137 .",
    "@xmath138 : assume that the absolute value of the functional @xmath87 and its partial derivatives are strictly upper bounded in the range @xmath139 for all @xmath136 .",
    "@xmath140 : let @xmath141 and @xmath142 .",
    "let @xmath143 be a positive function satisfying the condition @xmath144 .",
    "for some fixed @xmath145 , define @xmath146 and @xmath147 .",
    "assume that the conditions @xmath148 @xmath149 @xmath150 @xmath151 are satisfied by @xmath152 and @xmath153 , for some constants @xmath154 , @xmath155 , @xmath156 and @xmath157 .",
    "these assumptions are comparable to other rigorous treatments of entropy estimation .",
    "the assumption @xmath118 is equivalent to choosing the bandwidth of the kernel to be a fractional power of the sample size  @xcite .",
    "the rest of the above assumptions can be divided into two categories : ( i ) assumptions on the density @xmath6 , and ( ii ) assumptions on the functional @xmath158 .",
    "the assumptions on the smoothness , boundedness away from @xmath22 and @xmath159 of the density @xmath6 are similar to the assumptions made by other estimators of entropy as listed in section ii ,  @xcite .",
    "the assumptions on the functional @xmath158 ensure that @xmath158 is sufficiently smooth and that the estimator is bounded .",
    "these assumptions on the functional are readily satisfied by the common functionals that are of interest in literature : shannon @xmath160 and rnyi @xmath161 entropy , where @xmath162 is the indicator function , and the quadratic functional @xmath10 .      under the assumptions stated above",
    ", we have shown the following in the appendix :    [ knnbiash ] the biases of the plug - in estimators @xmath163 are given by @xmath164 where @xmath165 , @xmath166 and @xmath167 are constants that depend on @xmath158 and @xmath6 .",
    "[ knnvarh ] the variances of the plug - in estimators @xmath163 are identical up to leading terms , and are given by @xmath168 where @xmath169 and @xmath170 are constants that depend on @xmath158 and @xmath6 .      from theorem [ knnbiash ] ,",
    "observe that the conditions @xmath171 and @xmath172 are necessary for the estimators @xmath173 and @xmath174 to be unbiased . likewise from theorem [ knnvarh ] ,",
    "the conditions @xmath175 and @xmath176 are necessary for the variance of the estimator to converge to @xmath22 .",
    "below , we optimize the choice of bandwidth @xmath4 for minimum mse , and also show that the optimal mse rate is invariant to the choice of @xmath121 .",
    "[ [ optimal - choice - of - k ] ] optimal choice of @xmath4 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    minimizing the mse over @xmath4 is equivalent to minimizing the square of the bias over @xmath4 .",
    "the optimal choice of @xmath4 is given by @xmath177 and the bias evaluated at @xmath178 is @xmath179 .",
    "[ [ choice - of - alpha_frac ] ] choice of @xmath121 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    observe that the mse of @xmath173 and @xmath174 are dominated by the squared bias @xmath180 as contrasted to the variance @xmath181 .",
    "this implies that the asymptotic mse rate of convergence is invariant to the selected proportionality constant @xmath121 .    in view of ( a ) and ( b ) above , the optimal mse for the estimators @xmath182 and",
    "@xmath183 is therefore achieved for the choice of @xmath184 , and is given by @xmath185 .",
    "our goal is to reduce the estimator mse to @xmath5 .",
    "we do so by applying the method of weighted ensembles described in section  [ sec : genmeth ] .      for a positive integer @xmath186 , choose @xmath187 to be positive real numbers . define the mapping @xmath188 and",
    "let @xmath189 .",
    "define the weighted ensemble estimator @xmath190 from theorems [ knnbiash ] and [ knnvarh ] , we see that the biases of the ensemble of estimators @xmath191 satisfy @xmath14([be ] ) when we set @xmath192 and @xmath193 .",
    "furthermore , the general form of the variance of @xmath194 follows @xmath15([ve ] ) because @xmath195 .",
    "this implies that we can use the weighted ensemble estimator @xmath196 to estimate entropy at @xmath78 convergence rate by setting @xmath56 equal to the optimal weight @xmath46 given by ( [ convexsoltheory ] ) .",
    "we illustrate the superior performance of the proposed weighted ensemble estimator for two applications : ( i ) estimation of the panter - dite rate distortion factor , and ( ii ) estimation of entropy to test for randomness of a random sample .    for finite @xmath0 direct use of theorem 1 can lead to excessively high variance .",
    "this is because forcing the condition ( 2.3 ) that @xmath197 is too strong and , in fact , not necessary .",
    "the careful reader may notice that to obtain @xmath198 mse convergence rate in theorem 1 it is sufficient that @xmath199 be of order @xmath200 .",
    "therefore , in practice we determine the optimal weights according to the optimization :    @xmath201    the optimization is also convex .",
    "note that , as contrasted to , the norm of the weight vector @xmath56 is bounded instead of being minimized . by relaxing the constraints @xmath197 in to the softer constraints in",
    ", the upper bound @xmath202 on @xmath203 can be reduced from the value @xmath80 obtained by solving .",
    "this results in a more favorable trade - off between bias and variance for moderate sample sizes . in our experiments , we find that setting @xmath204 yields good mse performance .",
    "note that as @xmath205 , we must have @xmath206 for @xmath207 in order to keep @xmath208 finite , thus recovering the strict constraints in . for fixed sample size @xmath0 and dimension @xmath1 , observe that increasing @xmath209 increases the number of degrees of freedom in the convex problem , and therefore will result in a smaller value of @xmath208 and in turn improved estimator performance . in our simulations",
    ", we choose @xmath210 to be @xmath211 equally spaced values between @xmath212 and @xmath213 , ie the @xmath214 are uniformly spaced as @xmath215 with scale and range parameters @xmath216 and @xmath217 respectively .",
    "we limit @xmath209 to 50 because we find that the gains beyond @xmath211 are negligible .",
    "the reason for this diminishing return is a direct result of the increasing similarity among the entries in @xmath210 , which translates to increasingly similar basis functions @xmath192 .      for a @xmath1-dimensional source with underlying density @xmath6 , the panter - dite distortion - rate function  @xcite for a @xmath218-dimensional vector quantizer with @xmath135 levels of quantization",
    "is given by @xmath219 the panter - dite factor corresponds to the functional @xmath12 with @xmath220 .",
    "the panter - dite factor is directly related to the rnyi @xmath221-entropy , for which several other estimators have been proposed  @xcite .    in our simulations",
    "we compare six different choices of functional estimators - the three estimators previously introduced : ( i ) the standard kernel plug - in estimator @xmath116 , ( ii ) the boundary truncated plug - in estimator @xmath117 and ( iii ) the weighted estimator @xmath196 with optimal weight @xmath222 given by ( [ convexsol ] ) , and in addition the following popular entropy estimators : ( iv ) histogram plug - in estimator  @xcite , ( v ) @xmath4-nearest neighbor ( @xmath4-nn ) entropy estimator  @xcite and ( vi ) entropic @xmath4-nn graph estimator  @xcite . for both @xmath116 and @xmath117 ,",
    "we select the bandwidth parameter @xmath4 as a function of @xmath108 according to the optimal proportionality @xmath223 and @xmath224 .",
    "we choose @xmath6 to be the @xmath1 dimensional mixture density @xmath225 ; where @xmath226 , @xmath227 is a @xmath1-dimensional beta density with parameters @xmath228 , @xmath229 is a @xmath1-dimensional uniform density and the mixing ratio @xmath230 is @xmath231 .",
    "the reason we choose the beta - uniform mixture for our experiments is because it trivially satisfies all the assumptions on the density @xmath6 listed in section 3.1 , including the assumptions of finite support and strict boundedness away from 0 on the support .",
    "the true value of the panter - dite factor @xmath232 for the beta - uniform mixture is calculated using numerical integration methods via the mathematica software ( http://www.wolfram.com/mathematica/ ) .",
    "numerical integration is used because evaluating the entropy in closed form for the beta - uniform mixture is not tractable .",
    "the mse values for each of the six estimators are calculated by averaging the squared error @xmath233 ^ 2 $ ] , @xmath234 over @xmath235 monte - carlo trials , where each @xmath236 corresponds to an independent instance of the estimator .",
    "the mse results of the different estimators are shown in fig .",
    "[ a - compare ] as a function of sample size @xmath0 , for fixed dimension @xmath226 .",
    "it is clear from the figure that the proposed ensemble estimator @xmath196 has significantly faster rate of convergence while the mse of the rest of the estimators , including the truncated kernel plug - in estimator , have similar , slow rates of convergence .",
    "it is therefore clear that the proposed optimal ensemble averaging significantly accelerates the mse convergence rate .      for fixed sample size @xmath0 and fixed number of estimators @xmath209",
    ", it can be seen that @xmath208 increases monotonically with @xmath1 .",
    "this follows from the fact that the number of constraints in the convex problem [ convexsol ] is equal to @xmath237 and each of the basis functions @xmath192 monotonically approaches @xmath238 as @xmath1 grows , .",
    "this in turn implies that for a fixed sample size @xmath0 and number of estimators @xmath209 , the overall mse of the ensemble estimator should increase monotonically with the dimension @xmath1 .",
    "the mse results of the different estimators are shown in fig .",
    "[ ad - compare ] as a function of dimension @xmath1 , for fixed sample size @xmath239 . for the standard kernel plug - in estimator and truncated kernel plug - in estimator ,",
    "the mse increases rapidly with @xmath1 as expected .",
    "the mse of the histogram and @xmath4-nn estimators increase at a similar rate , indicating that these estimators suffer from the curse of dimensionality as well . on the other hand",
    ", the mse of the weighted estimator also increases with the dimension as predicted , but at a slower rate . also observe that the mse of the weighted estimator is smaller than the mse of the other estimators for all dimensions @xmath240 .      in this section ,",
    "we illustrate the weighted ensemble estimator for non - parametric estimation of shannon differential entropy .",
    "the shannon differential entropy is given by @xmath12 where @xmath160 .",
    "the improved accuracy of the weighted ensemble estimator is demonstrated in the context of hypothesis testing using estimated entropy as a statistic to test for the underlying probability distribution of a random sample .",
    "specifically , the samples under the null and alternate hypotheses @xmath241 and @xmath242 are drawn from the probability distribution @xmath243 , described in section iv.a , with fixed @xmath226 , @xmath244 and two sets of values of @xmath245 under the null and alternate hypothesis , @xmath246 versus @xmath247 .",
    "first , we fix @xmath248 and @xmath249 .",
    "the density under the null hypothesis @xmath250 has greater curvature relative to @xmath251 and therefore has smaller entropy .",
    "five hundred ( 500 ) experiments are performed under each hypothesis with each experiment consisting of 1000 samples drawn from the corresponding distribution .",
    "the true entropy and estimates @xmath116 , @xmath117 and @xmath196 obtained from each instance of @xmath252 samples are shown in fig .",
    "[ c - compare ] for the 1000 experiments .",
    "this figure suggests that the ensemble weighted estimator provides better discrimination ability by suppressing the bias , at the cost of some additional variance .    to demonstrate that the weighted estimator provides better discrimination",
    ", we plot the histogram envelope of the entropy estimates using standard kernel plug - in estimator , truncated kernel plug - in estimator and the weighted estimator for the cases corresponding to the hypothesis @xmath241 ( color coded blue ) and @xmath242 ( color coded red ) in fig .",
    "[ d - compare ] .",
    "furthermore , we quantitatively measure the discriminative ability of the different estimators using the deflection statistic @xmath253 where @xmath254 and @xmath255 ( respectively @xmath256 and @xmath257 ) are the sample mean and standard deviation of the entropy estimates .",
    "the deflection statistic was found to be @xmath258 , @xmath259 and @xmath260 for the standard kernel plug - in estimator , truncated kernel plug - in estimator and the weighted estimator respectively . the receiver operating curves ( roc ) for this entropy - based test using the three different estimators are shown in fig .  [ b - compare ] .",
    "the corresponding areas under the roc curves ( auc ) are given by @xmath261 , @xmath262 and @xmath263 .",
    "in our final experiment , we fix @xmath264 and set @xmath265 , perform 500 experiments each under the null and alternate hypotheses with samples of size 5000 , and plot the auc as @xmath266 varies from @xmath22 to @xmath238 in fig .",
    "[ e - compare ] . for comparison",
    ", we also plot the auc for the neyman - pearson likelihood ratio test .",
    "the neyman - pearson likelihood ratio test , unlike the shannon entropy based tests , is an omniscient test that assumes knowledge of both the underlying beta - uniform mixture parametric model of the density and the parameter values @xmath63 , @xmath267 and @xmath268 , @xmath269 under the null and alternate hypothesis",
    "figure 4 shows that the weighted estimator _ uniformly and significantly _ outperforms the individual plug - in estimators and comes closest to the performance of the omniscient neyman - pearson likelihood test .",
    "the relatively superior performance of the neyman - pearson likelihood test is due to the fact that the weighted estimator is a nonparametric estimator that has marginally higher variance ( proportional to @xmath270 ) as compared to the underlying parametric model for which the neyman - pearson test statistic provides the most powerful test .",
    "we have proposed a new estimator of functionals of a multivariate density based on weighted ensembles of kernel density estimators . for ensembles of estimators that satisfy general conditions on bias and variance as specified by @xmath14([be ] ) and @xmath15([ve ] ) respectively , the weight optimized ensemble estimator has parametric @xmath5 mse convergence rate that can be much faster than the rate of convergence of any of the individual estimators in the ensemble .",
    "the optimal weights are determined as a solution to a convex optimization problem that can be performed offline and does not require training data .",
    "we illustrated this estimator for uniform kernel plug - in estimators and demonstrated the superior performance of the weighted ensemble entropy estimator for ( i ) estimation of the panter - dite factor and ( ii ) non - parametric hypothesis testing .",
    "several extensions of the framework of this paper are being pursued : ( i ) using @xmath4-nearest neighbor ( @xmath4-nn ) estimators in place of kernel estimators ; ( ii ) extending the framework to the case where support @xmath89 _ is not known _ , but for which conditions @xmath14 and @xmath15 hold ; ( iii ) using ensemble estimators for estimation of other functionals of probability densities including divergence , mutual information and intrinsic dimension ; and ( iv ) using an @xmath271 norm @xmath272 in place of the @xmath273 norm @xmath274 in the weight optimization algorithm ( 2.3 ) so as to introduce sparsity into the weighted ensemble .",
    "this work was partially supported by ( i ) aro grant w911nf-12 - 1 - 0443 and ( ii ) nih grant 2p01ca087634 - 06a2 .",
    "we first establish moment properties for uniform kernel density estimates in appendix  [ sec : kernmoments ] .",
    "subsequently , we prove theorems [ knnbiash ] and [ knnvarh ] in appendix  [ sec : biasvarproof ] .",
    "throughout this section , we assume without loss of generality that the support @xmath275^d$ ] .",
    "observe that @xmath101 is a binomial random variable with parameters @xmath108 and @xmath276 .",
    "the probability mass function of the binomial random variable @xmath101 is given by @xmath277    define the error function of the truncated uniform kernel density , @xmath278 \\nonumber \\\\ & = & \\frac{l_k(x)}{mv_k(x ) } - \\frac{u_k(x)}{v_k(x ) } \\nonumber \\\\ & = & \\frac{\\sum_{i=1}^m ( 1_{{\\mathbf}{x}_i \\in s_k(x ) } - u_k(x))}{mv_k(x)}.\\end{aligned}\\ ] ]    also define the error function of the standard uniform kernel density , @xmath279 \\nonumber \\\\ & = & ( mv_k(x)/k)\\hat{{\\mathbf}{e}}_k(x ) , \\nonumber\\end{aligned}\\ ] ] and note that when @xmath280 , @xmath281 .",
    "for any @xmath95 , the coverage function @xmath282 can be represented by using a @xmath1 order taylor series expansion of @xmath6 about @xmath98 as follows . because the density @xmath6 has continuous partial derivatives of order @xmath1 in @xmath89 , for any @xmath95 , @xmath283 where @xmath284 are functions which depend on @xmath4 and the unknown density @xmath6 .",
    "this implies that the expectation of the density estimate is given by @xmath285 } & = & u_k(x)/v_k(x )   \\nonumber \\\\ & = & f(x ) + \\sum_{i=1}^{d } c_{i , k}(x){\\left(\\frac{k}{m}\\right)}^{i / d } + o\\left({\\left(\\frac{k}{m}\\right)}\\right ) .",
    "\\label{inbias}\\end{aligned}\\ ] ]      because @xmath101 is a binomial random variable , standard chernoff inequalities can be applied to obtain concentration bounds on @xmath101 . in particular , for @xmath286 , @xmath287 let @xmath288 denote the event @xmath289 , where @xmath290 for some fixed @xmath142 .",
    "then , for @xmath291 , @xmath292 where @xmath143 satisfies the condition @xmath293 for any @xmath294 . also observe that under the event @xmath288 , @xmath295      let @xmath296 be an euclidean ball of radius @xmath297 centered at @xmath98 .",
    "let @xmath98 be a lebesgue point of @xmath6 , i.e. , an @xmath98 for which @xmath298 because @xmath6 is an density , we know that almost all @xmath95 satisfy the above property .",
    "now , fix @xmath141 and find @xmath299 such that @xmath300 for small values of @xmath301 , @xmath302 and therefore @xmath303 this implies that under the event @xmath288 defined in the previous subsection , @xmath304 let @xmath305 denote the event that @xmath306 .",
    "let @xmath307 denote the event @xmath308 and @xmath309 denote @xmath310 . then conditioned on the event @xmath307 @xmath311 and conditioned on the event @xmath309 @xmath312 observe that @xmath305 , @xmath307 , @xmath309 and @xmath288 form a disjoint partition of the event space .",
    "[ biaslemma ] let @xmath313 be an arbitrary function with @xmath1 partial derivatives wrt @xmath136 and @xmath314 .",
    "let @xmath315 denote @xmath316 i.i.d realizations of the density @xmath6 . then , @xmath317 - { { { \\mathbb{e}}}}[\\gamma({{f}}({\\mathbf}{z}),{\\mathbf}{z } ) ] = \\sum_{i=1}^{d } c_{1,i}(\\gamma(x , y))(k / m)^{i /",
    "d } + o((k / m ) ) , \\label{bknnbias}\\ ] ] where @xmath318 are functionals of @xmath11 and @xmath6 .    to analyze the bias ,",
    "first extend the density function @xmath6 as follows .",
    "in particular , extend the definition of @xmath6 to the domain @xmath319^d$ ] while ensuring that the extended function @xmath320 is differentiable @xmath1 times on this extended domain .",
    "let @xmath321 be the natural un - truncated ball .",
    "let @xmath322 .",
    "define the function @xmath323 . for any @xmath95 , using this extended definition , @xmath324 where @xmath38 are only functions of the unknown density @xmath320 .",
    "also define @xmath325 $ ] .",
    "define the interior region @xmath326 .",
    "note that @xmath327 for all @xmath280 .",
    "now , @xmath328 - { { { \\mathbb{e}}}}[\\gamma({{f}}({\\mathbf}{z}),{\\mathbf}{z } ) ]   & = & { { { \\mathbb{e}}}}[\\gamma(\\bar{f}_k({\\mathbf}{z}),{\\mathbf}{z } ) - \\gamma({f}({\\mathbf}{z}),{\\mathbf}{z } ) ] + { { { \\mathbb{e}}}}[\\gamma(\\check{{f}}_k({\\mathbf}{z}),{\\mathbf}{z } ) - \\gamma(\\bar{f}_k({\\mathbf}{z}),{\\mathbf}{z } ) ] \\nonumber \\\\ & = & { { { \\mathbb{e}}}}[\\gamma(\\bar{f}_k({\\mathbf}{z}),{\\mathbf}{z } ) - \\gamma({f}({\\mathbf}{z}),{\\mathbf}{z } ) ] + { { { \\mathbb{e}}}}[1_{{\\mathbf}{z } \\in { \\cal s - s}_i(k ) }   ( \\gamma(\\check{{f}}_k({\\mathbf}{z}),{\\mathbf}{z } ) - \\gamma(\\bar{f}_k({\\mathbf}{z}),{\\mathbf}{z } ) ) ] \\nonumber \\\\ & = & i+ii.\\end{aligned}\\ ] ]      : @xmath329 \\nonumber \\\\ & = & \\sum_{i=1}^d { { { \\mathbb{e}}}}\\left[\\gamma^{(i)}({f}({\\mathbf}{z}),{\\mathbf}{z } ) { \\left(\\bar{f}_k({\\mathbf}{z } ) - { f}({\\mathbf}{z})\\right)^i } \\right ] \\nonumber \\\\ & = & \\sum_{i=1}^{d } c_{11,i}(\\gamma(x , y))(k / m)^{i / d } + o((k / m)),\\end{aligned}\\ ] ] where @xmath330 are functionals of @xmath313 and its derivatives .",
    "let @xmath331 , @xmath332 and @xmath333 .",
    "define mappings @xmath334 , @xmath335 and @xmath336 : @xmath337 @xmath338 @xmath88 as follows .",
    "let @xmath339 denote the unit vector from the origin to @xmath98 , and define @xmath340 let @xmath341 be a reference set .",
    "define @xmath342 let @xmath343 . finally define @xmath344 where @xmath345 satisfies @xmath346 . for each @xmath347 , let @xmath348 and @xmath349 .",
    "let @xmath350 denote the set of all unit vectors : @xmath351 observe that , by definition , the _ shape _ of the regions @xmath102 and @xmath352 is identical .",
    "this is illustrated in fig .",
    "[ i - compare ] .",
    "[ [ analysis - of - barf_mcal - f_sx - checkf_mcal - f_sx ] ] analysis of @xmath353 , @xmath354 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    @xmath355 can represented in terms of @xmath356 as @xmath357 .",
    "using taylor series around @xmath358 , @xmath359 can then be evaluated as @xmath360 where the functionals @xmath361 depend only on the shape of the regions @xmath102 or @xmath352 and therefore only on @xmath355 .",
    "similarly , @xmath362 where the functionals @xmath363 again depend only on @xmath355 .",
    "this implies that for any fixed @xmath364 and corresponding @xmath365 , for any function @xmath366 and positive integer @xmath367 , integration over the line @xmath368 @xmath369 and @xmath370 where the functions @xmath371 and @xmath372 depend only on @xmath373 , @xmath218 , @xmath202 and are independent of @xmath374 and @xmath4 .    [ [ analysis - of - barf_kx - checkf_kx ] ] analysis of @xmath375 , @xmath376 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    @xmath355 can be represented in terms of @xmath98 as @xmath377 .",
    "identically , this gives , @xmath378 and @xmath379    this implies that for any fixed @xmath364 and corresponding @xmath365 , integration over the line @xmath380 @xmath381 and @xmath382    [ [ analysis - of - ii ] ] analysis of ii + + + + + + + + + + + + + +    @xmath383 \\nonumber \\\\ & = \\int_{{z } \\in { \\cal s - s}_i(k ) } ( \\gamma(\\check{{f}}_{k}({z}),{z } ) - \\gamma(\\bar{f}_k({z}),{z } ) ) f(z ) dz \\nonumber \\\\ & = \\int_{\\{x_b \\in { \\cal b}\\}\\cup\\{c \\in ( 0,k_ml_{max}(x_b))\\}}1_{\\{z = x_b - cu(x_b)\\ } } \\sum_{i=1}^d \\left[\\gamma^{(i)}({f}(x_b),x_b ) { \\left(\\check{f}_k(z ) - { f}({z})\\right)^i } \\right ] f(z ) dz \\nonumber \\\\ & - \\int_{\\{x_b \\in { \\cal b}\\}\\cup\\{c \\in ( 0,k_ml_{max}(x_b))\\}}1_{\\{z = x_b - cu(x_b)\\ } } \\sum_{i=1}^d \\left[\\gamma^{(i)}({f}(x_b),x_b ) { \\left(\\bar{f}_k(z ) - { f}({z})\\right)^i } \\right ] f(z ) dz \\nonumber \\\\ & =   \\sum_{i=1}^{d } c_{12,i}(\\gamma(x , y))(k / m)^{i / d } + o((k / m)),\\end{aligned}\\ ] ]    where @xmath384 are functionals of @xmath313 and its derivatives .",
    "this implies that @xmath385 - { { { \\mathbb{e}}}}[\\gamma({{f}}({\\mathbf}{z } ) ) ]   & = & i+ii \\nonumber \\\\ & = &   \\sum_{i=1}^{d } c_{1,i}(\\gamma(x , y))(k / m)^{i / d } + o((k / m ) ) , \\label{bknnbias2}\\end{aligned}\\ ] ] where the functionals @xmath318 are independent of @xmath4 .",
    "since @xmath101 is a binomial random variable , we can easily obtain moments of the uniform kernel density estimate in terms of @xmath282 .",
    "these are listed below .",
    "[ momentlemma ] let @xmath386 be an arbitrary function satisfying @xmath387 .",
    "let @xmath315 denote @xmath316 i.i.d realizations of the density @xmath6 .",
    "then , @xmath388 } = { 1_{\\{q=2\\}}}c_2(\\gamma(x))\\left(\\frac{1}{k}\\right ) + o\\left(\\frac{1}{k}\\right ) , \\label{bknncent}\\ ] ] @xmath389 } = { 1_{\\{q=2\\}}}c_2(\\gamma(x))\\left(\\frac{1}{k}\\right ) + o\\left(\\frac{1}{k}\\right ) , \\label{bknncent2}\\ ] ] where @xmath390 is a functional of @xmath11 and @xmath6 .    when @xmath391 , @xmath392 & = & { { { \\mathbb{e}}}}{[\\hat{{\\mathbf}{e}}^2_{k}(x ) ] }   \\nonumber \\\\ & = & \\frac{u_k(x)(1-u_k(x))}{mv^2_k(x ) } \\nonumber \\\\ & = & \\frac{f(x)}{mv_k(x ) } + o\\left(\\frac{1}{k}\\right )",
    ". \\label{secmom}\\end{aligned}\\ ] ]    for any integer @xmath393 , @xmath394 } & = & { { { \\mathbb{e}}}}{[1_{\\natural(x)}\\hat{{\\mathbf}{e}}^r_{k}(x ) ] } + { { { \\mathbb{e}}}}{[1_{\\natural^c(x)}\\hat{{\\mathbf}{e}}^r_{k}(x)]}\\nonumber \\\\ & = & o\\left(\\frac{1}{k^{\\delta r/2}}\\right ) = o(1/k).\\end{aligned}\\ ] ]    observe that @xmath395 and therefore @xmath396 } = \\theta(1/k ) + o(1/k)$ ] .",
    "this implies , @xmath388 } = { 1_{\\{q=2\\}}}c_2(\\gamma(x))\\left(\\frac{1}{k}\\right ) + o\\left(\\frac{1}{k}\\right ) .",
    "\\nonumber\\ ] ]    when @xmath280 , @xmath281 .",
    "also @xmath397 .",
    "this result in conjunction with the fact that @xmath398 , and @xmath395 gives @xmath389 } = { 1_{\\{q=2\\}}}c_2(\\gamma(x))\\left(\\frac{1}{k}\\right ) + o\\left(\\frac{1}{k}\\right ) . \\nonumber\\ ] ]      let @xmath98 and @xmath399 be two distinct points .",
    "clearly the density estimates at @xmath98 and @xmath399 are not independent .",
    "observe that the uniform kernel regions @xmath102 , @xmath400 are disjoint for the set of points given by @xmath401 , and have finite intersection on the complement of @xmath402 .    [ [ intersecting - balls ] ] intersecting balls + + + + + + + + + + + + + + + + + +    [ intersectu ] for a fixed pair of points @xmath403 , and positive integers @xmath404 , @xmath405 = 1_{\\{q=1,r=1\\}}\\left(\\frac{-f(x)f(y)}{m}\\right ) + o\\left(\\frac{1}{m}\\right ) .",
    "\\nonumber\\ ] ]    for a fixed pair of points @xmath406 , the joint probability mass function of the functions @xmath101,@xmath407 is given by @xmath408 denote the high probability event @xmath409 by @xmath410 .",
    "define @xmath411 , @xmath412 to be binomial random variables with parameters @xmath413,@xmath414 and @xmath415,@xmath416 respectively .",
    "the covariance between powers of density estimates is then given by @xmath417 \\nonumber \\\\ & & = \\left(\\frac{1}{m^{q+r}v^q_k(x)v^r_k(y)}\\right ) \\sum_{\\natural(x , y ) } { l_x^ql_y^r } \\left[pr({\\mathbf}{l}_k(x)=l_x,{\\mathbf}{l}_k(y)=l_y ) - pr({\\mathbf}{l}_k(x)=l_x)pr({\\mathbf}{l}_k(y)=l_y)\\right ] + o\\left(\\frac{1}{m}\\right ) \\nonumber \\\\ & & = \\left(\\frac{1}{m^{q+r}v^q_k(x)v^r_k(y)}\\right ) \\sum_{\\natural(x , y ) } \\frac{l_x^ql_y^ru_k^q(x)u_k^r(y)}{(l_x \\times \\ldots \\times l_x-{q+1})(l_y \\times \\ldots \\times l_y-{r+1 } ) } \\times \\nonumber \\\\ & & \\bigl[(m \\times \\ldots \\times m-(q+r-1))pr({\\mathbf}{\\hat{l}}_k(x)=l_x,{\\mathbf}{\\hat{l}}_k(y)=l_y ) \\nonumber \\\\ & & - ( m \\times \\ldots \\times m - q+1)(m \\times \\ldots \\times m - r+1 ) pr({\\mathbf}{\\hat{l}}_k(x)=l_x)pr({\\mathbf}{\\hat{l}}_k(y)=l_y ) \\bigr ]   + o\\left(\\frac{1}{m}\\right ) \\nonumber \\\\ & & = \\left(\\frac{f^q(x)f^r(y)}{m^{q+r}}\\right )   \\times \\nonumber \\\\ & & \\sum_{\\natural(x , y ) } \\bigl[(m \\times \\ldots \\times m-(q+r-1 ) ) pr({\\mathbf}{\\hat{l}}_k(x)=l_x,{\\mathbf}{\\hat{l}}_k(y)=l_y ) \\nonumber \\\\ & & - ( m \\times \\ldots \\times m-(q-1))(m \\times \\ldots \\times m-(r-1 ) ) pr({\\mathbf}{\\hat{l}}_k(x)=l_x)pr({\\mathbf}{\\hat{l}}_k(y)=l_y ) \\bigr ] + o\\left(\\frac{1}{m}\\right ) \\nonumber \\\\ & & = \\left(\\frac{f^q(x)f^r(y)}{m^{q+r}}\\right )   \\times \\nonumber \\\\ & & [ ( m \\times \\ldots \\times m-(q+r-1 ) ) - ( m \\times \\ldots \\times m-(q-1))(m \\times \\ldots \\times m-(r-1 ) ) ] \\nonumber \\\\ & & = \\frac{-qrf^q(x)f^r(y)}{m}+o\\left(\\frac{1}{m}\\right ) . \\nonumber\\end{aligned}\\ ] ]    then , the covariance between the powers of the error function is given by @xmath418)^q,(\\hat{{\\mathbf}{f}}_k(y)-{{{\\mathbb{e}}}}[\\hat{{\\mathbf}{f}}_k(y)])^r ) \\nonumber \\\\ & = & \\sum_{a=1}^{q } \\sum_{b=1}^{r } \\binom{q}{a } \\binom{r}{b } ( -{{{\\mathbb{e}}}}[\\hat{{\\mathbf}{f}}_k(x)])^a(-{{{\\mathbb{e}}}}[\\hat{{\\mathbf}{f}}_k(y)])^b cov(\\hat{{\\mathbf}{f}}_k^a(x),\\hat{{\\mathbf}{f}}_k^b(y ) ) \\nonumber \\\\ & = & \\sum_{a=1}^{q } \\sum_{b=1}^{r } \\binom{q}{a } \\binom{r}{b } [ ( -f(x))^a(-f(y))^b+o(1 ) ] cov(\\hat{{\\mathbf}{f}}_k^a(x),\\hat{{\\mathbf}{f}}_k^b(y ) ) \\nonumber \\\\ & = & -f^{q}(x)f^{r}(y ) \\sum_{a=1}^{q } \\sum_{b=1}^{r } \\binom{q}{a } \\binom{r}{b }   \\frac{(-1)^{a+b}ab}{m}+o\\left(\\frac{1}{m}\\right ) \\nonumber \\\\ & = & 1_{\\{q=1,r=1\\}}\\left(\\frac{-f(x)f(y)}{m}\\right ) + o\\left(\\frac{1}{m}\\right ) . \\nonumber \\end{aligned}\\ ] ]    [ [ disjoint - balls ] ] disjoint balls + + + + + + + + + + + + + +    for @xmath419 , there is no closed form expression for the covariance .",
    "however we have the following lemma by applying the cauchy - schwartz inequality :    [ disjointu ] for a fixed pair of points @xmath420 ,    @xmath421 = { 1_{\\{q=1,r=1\\}}}o\\left(\\frac{1}{k}\\right ) + o\\left(\\frac{1}{k}\\right ) .",
    "\\nonumber\\ ] ]    @xmath422    [ [ joint - expression ] ] joint expression + + + + + + + + + + + + + + + +    [ covariancelemma ] let @xmath423 , @xmath424 be arbitrary functions with @xmath238 partial derivative wrt @xmath136 and @xmath425 , @xmath426 .",
    "let @xmath427 denote @xmath428 i.i.d realizations of the density @xmath6 .",
    "then , @xmath429 } = { 1_{\\{q=1,r=1\\}}}c_{5}(\\gamma_1(x),\\gamma_2(x))\\left(\\frac{1}{m}\\right ) + o\\left(\\frac{1}{m}\\right ) , \\label{bknncross}\\ ] ] @xmath430 } = { 1_{\\{q=1,r=1\\}}}c_5(\\gamma_1(x),\\gamma_2(x))\\left(\\frac{1}{m}\\right ) + o\\left(\\frac{1}{m}\\right ) , \\label{bknncross2}\\ ] ] where @xmath431 is a functional of @xmath423 , @xmath424 and @xmath6 .",
    "let the indicator function @xmath432 denote the event @xmath433 .",
    "then @xmath434 } = i + d , \\nonumber\\end{aligned}\\ ] ] where @xmath435 stands for the contribution form the intersecting balls and @xmath436 for the contribution from the dis - joint balls . @xmath435 and @xmath436",
    "are given by @xmath437\\right]},\\nonumber \\\\ d & = & { { { \\mathbb{e}}}}{\\left[{\\mathbf}{(1-{\\mathbf}{1_{\\delta_k}}({\\mathbf}{x},{\\mathbf}{y } ) ) } cov \\left[\\gamma_1({x}){\\hat{{\\mathbf}{e}}}^q_k({x } ) , \\gamma_2({y } ) { \\hat{{\\mathbf}{e}}}^r_k({y } ) \\right]\\right]}. \\nonumber \\end{aligned}\\ ] ]    when @xmath438 , we have @xmath419",
    ". then , @xmath439 }   \\nonumber \\\\ & = & { { { \\mathbb{e}}}}{\\left[{\\mathbf}{1_{\\delta_k}}({\\mathbf}{x},{\\mathbf}{y } ) \\gamma_1({\\mathbf}{x})\\gamma_2({\\mathbf}{y}){{{\\mathbb{e}}}}_{{\\mathbf}{x},{\\mathbf}{y}}[{\\hat{{\\mathbf}{e}}}^q_k({x}){\\hat{{\\mathbf}{e}}}^r_k({y})]\\right ] } \\nonumber \\\\ & \\leq & { { { \\mathbb{e}}}}{\\left[{\\mathbf}{1_{\\delta_k}}({\\mathbf}{x},{\\mathbf}{y } ) \\gamma_1({\\mathbf}{x})\\gamma_2({\\mathbf}{y})\\sqrt{{{{\\mathbb{e}}}}_{{\\mathbf}{x}}[{\\hat{{\\mathbf}{e}}}^{2q}_k({x})]{{{\\mathbb{e}}}}_{{\\mathbf}{y}}[{\\hat{{\\mathbf}{e}}}^{2r}_k({y})]}\\right ] } \\nonumber \\\\ & = & { { { \\mathbb{e}}}}{\\left[{\\mathbf}{1_{\\delta_k}}({\\mathbf}{x},{\\mathbf}{y } ) \\gamma_1({\\mathbf}{x})\\gamma_2({\\mathbf}{y})\\left({1_{\\{q=1,r=1\\}}}o\\left(\\frac{1}{k}\\right ) + o\\left(\\frac{1}{k}\\right)\\right)\\right ] } \\nonumber \\\\ & = & \\int{\\left[\\left({1_{\\{q=1,r=1\\}}}o\\left(\\frac{1}{k}\\right ) + o\\left(\\frac{1}{k}\\right)\\right)(\\gamma_1(x)\\gamma_2(x ) + o(1))\\right ] \\left(\\int { \\delta_k}({x},{y } ) dy \\right ) } dx   \\nonumber \\\\ & = & \\int{\\left[\\left({1_{\\{q=1,r=1\\}}}o\\left(\\frac{1}{k}\\right ) + o\\left(\\frac{1}{k}\\right)\\right)(\\gamma_1(x)\\gamma_2(x ) + o(1))\\right ] \\left(2^{d}\\frac{k}{m } \\right ) } dx   \\nonumber \\\\ & = & { 1_{\\{q=1,r=1\\}}}c_{5,1}(\\gamma_1,\\gamma_2)\\left(\\frac{1}{m}\\right ) + o\\left(\\frac{1}{m}\\right ) , \\nonumber\\end{aligned}\\ ] ] where the bound is obtained using the cauchy - schwarz inequality and using eq.[fourmom ] . also , @xmath440\\right ] } \\\\",
    "\\nonumber   & = &   { 1_{\\{q=1,r=1\\}}}c_{5,2}(\\gamma_1,\\gamma_2)\\left(\\frac{1}{m}\\right ) + o\\left(\\frac{1}{m}\\right ) .",
    "\\nonumber\\end{aligned}\\ ] ] this gives @xmath429 } = { 1_{\\{q=1,r=1\\}}}c_5(\\gamma_1(x),\\gamma_2(x))\\left(\\frac{1}{m}\\right ) + o\\left(\\frac{1}{m}\\right ) .",
    "\\nonumber\\ ] ]    again , since @xmath280 implies @xmath281 and @xmath397 , @xmath430 } = { 1_{\\{q=1,r=1\\}}}c_5(\\gamma_1(x),\\gamma_2(x))\\left(\\frac{1}{m}\\right ) + o\\left(\\frac{1}{m}\\right ) .",
    "\\nonumber\\ ] ]    this concludes the proof .",
    "[ boundonexpec ]    assume that @xmath441 is any arbitrary functional which satisfies @xmath442 @xmath443 @xmath444 @xmath445{\\cal c}(m )    = g_4 < \\infty.\\ ] ] let @xmath25 denote @xmath446 for some fixed @xmath447 .",
    "let @xmath448 be any random variable which almost surely lies in the range @xmath449 .",
    "then , @xmath450 < \\infty.\\ ] ]    we will show that the conditional expectation @xmath451 < \\infty.$ ] because @xmath452 by @xmath125 , it immediately follows that @xmath450 = { { { \\mathbb{e}}}}[{{{\\mathbb{e}}}}[|u(\\zeta_{z},{z})| \\mid { \\cal x}_n ] ] < \\infty.\\ ] ] also observe that @xmath453 and therefore @xmath454 .",
    "finally observe that the events @xmath455 and @xmath456 occur with probability @xmath457 . using ( [ densityineqnatural ] ) , ( [ densityineqnaturalc1 ] ) , ( [ densityineqnaturalc2 ] ) , conditioned on @xmath458 , @xmath459 & = & { { { \\mathbb{e}}}}[1_{\\natural_0(z)}|u(\\zeta_{z},{z})| ] + { { { \\mathbb{e}}}}[1_{\\natural_1(z)}|u(\\zeta_{z},{z})| ] +   { { { \\mathbb{e}}}}[1_{\\natural_2(z)}|u(\\zeta_{z},{z})| ] + { { { \\mathbb{e}}}}[1_{\\natural(z)}|u(\\zeta_{z},{z})| ] \\nonumber \\\\ & \\leq & ( g_1 + g_2 ) + ( g_3+g_2 ) + ( g_4 + g_2 ) + ( g_2 ) \\nonumber \\\\ & = & g_1 + 4g_2+g_3+g_4 < \\infty.\\end{aligned}\\ ] ]      using the continuity of @xmath460 , construct the following third order taylor series of @xmath461 around the conditional expected value @xmath462 $ ] .",
    "@xmath463 where @xmath464 is defined by the mean value theorem .",
    "this gives @xmath465 } \\nonumber \\\\ & = { { { \\mathbb{e}}}}{\\left[\\frac{1}{2}g''({\\check{{\\mathbf}{f}}_k({\\mathbf}{z})},{\\mathbf}{z}){\\hat{{\\mathbf}{e}}}_k^2({\\mathbf}{z})\\right ] } + { { { \\mathbb{e}}}}{\\left[\\frac{1}{6}g^{(3)}(\\zeta_{\\mathbf}{z},{\\mathbf}{z}){\\hat{{\\mathbf}{e}}}_k^3({\\mathbf}{z})\\right ] } \\nonumber \\end{aligned}\\ ] ] let @xmath466 .",
    "direct application of lemma  [ boundonexpec ] in conjunction with assumption @xmath140 implies that @xmath467 = o(1)$ ] . by cauchy - schwarz and applying lemma  [ momentlemma ] for the choice @xmath468 , @xmath469 } \\right| \\leq \\sqrt{{{{\\mathbb{e}}}}{\\left[\\delta^2({\\mathbf}{z})\\right ] { { { \\mathbb{e}}}}\\left[{\\hat{{\\mathbf}{e}}}_k^6({\\mathbf}{z})\\right ] } } = o\\left(\\frac{1}{k}\\right ) .",
    "\\nonumber\\end{aligned}\\ ] ]    by observing that the density estimates @xmath470 are identical , we therefore have @xmath471 - g(f ) = { { { \\mathbb{e}}}}{[{g}(\\hat{{\\mathbf}{f}}_k({\\mathbf}{z}),{\\mathbf}{z } ) - { g}({{f}({\\mathbf}{z})},{\\mathbf}{z } ) ] } \\nonumber \\\\ & & = { { { \\mathbb{e}}}}{[{g}({\\check{{\\mathbf}{f}}_k({\\mathbf}{z})},{\\mathbf}{z } ) - { g}({{f}({\\mathbf}{z})},{\\mathbf}{z } ) ] } + { { { \\mathbb{e}}}}{\\left[\\frac{1}{2}g''({\\check{{\\mathbf}{f}}_k({\\mathbf}{z})},{\\mathbf}{z}){\\mathbf}{e}_k^2({\\mathbf}{z})\\right ] } + o(1/k ) .",
    "\\nonumber \\end{aligned}\\ ] ] by lemma  [ biaslemma ] and lemma  [ momentlemma ] for the choice @xmath472 , in conjunction with assumptions @xmath131 and @xmath138 , this implies that @xmath473 - g(f ) & = & \\sum_{i=1}^d c_{1,i}(g(x , y))\\left({\\frac{k}{m}}\\right)^{i / d }   +   c_2(g''(\\check{f}_k(x),x))\\left(\\frac{1}{k}\\right ) + o\\left(\\frac{1}{k } + \\frac{k}{m}\\right ) \\nonumber \\\\ & = & \\sum_{i=1}^d c_{1,i}(g(x , y))\\left({\\frac{k}{m}}\\right)^{i / d }   + c_2(g''({f}(x),x))\\left(\\frac{1}{k}\\right ) +   o\\left(\\frac{1}{k } + \\frac{k}{m}\\right ) \\nonumber \\\\ & = & \\sum_{i=1}^d c_{1,i}\\left({\\frac{k}{m}}\\right)^{i / d } + c_2\\left(\\frac{1}{k}\\right ) + o\\left(\\frac{1}{k } + \\frac{k}{m}\\right ) , \\nonumber\\end{aligned}\\ ] ] where the last but one step follows because , by ( [ inbias ] ) , we know @xmath474 .",
    "this in turn implies @xmath475 . finally , by assumptions @xmath130 and @xmath138 , the leading constants @xmath165 and @xmath167 are bounded .",
    "note that the natural density estimate @xmath476 is identical to the truncated kernel density estimate @xmath477 on the set @xmath280 . from the definition of set @xmath478 , @xmath479 .",
    "@xmath480 - g(f ) = { { { \\mathbb{e}}}}{[{g}(\\tilde{{\\mathbf}{f}}_k({\\mathbf}{z}),{\\mathbf}{z } ) - { g}({{f}({\\mathbf}{z})},{\\mathbf}{z } ) ] } \\nonumber \\\\ & & = { { { \\mathbb{e}}}}{[1_{\\{{\\mathbf}{z } \\in { \\cal s}_i(k)\\}}{g}(\\hat{{\\mathbf}{f}}_k({\\mathbf}{z}),{\\mathbf}{z } ) - { g}({{f}({\\mathbf}{z})},{\\mathbf}{z } ) ] } + { { { \\mathbb{e}}}}{[1_{\\{{\\mathbf}{z } \\in { \\cal s - s}_i(k)\\}}{g}(\\hat{{\\mathbf}{f}}_k({\\mathbf}{z}),{\\mathbf}{z } ) - { g}({{f}({\\mathbf}{z})},{\\mathbf}{z } ) ] } \\nonumber \\\\ & & = i+ii\\end{aligned}\\ ] ]      because we assume that @xmath158 satisfies assumption @xmath140 , from the proof of lemma  [ boundonexpec ] , for @xmath483 , we have @xmath484 } = o(1)$ ] .",
    "this implies that , @xmath485 } \\nonumber \\\\ & = & { { { \\mathbb{e}}}}\\left[{{{\\mathbb{e}}}}{[{g}(\\hat{{\\mathbf}{f}}_k({z}),{z } ) - { g}({{f}({z})},{z})]}\\mid { \\{{\\mathbf}{z } \\in { \\cal s - s}_i(k)\\ } } \\right ] \\times pr({\\mathbf}{z } \\notin { \\cal s}_i(k ) ) \\nonumber \\\\ & = & o(1 ) \\times o((k / m)^{1/d } ) = o((k / m)^{1/d}).\\end{aligned}\\ ] ] this implies that @xmath486 - g(f ) & = & i+ii \\nonumber \\\\ & = & c_{1}\\left({\\frac{k}{m}}\\right)^{1/d } + c_2\\left(\\frac{1}{k}\\right ) + o\\left(\\frac{1}{k } + \\left(\\frac{k}{m}\\right)^{1/d}\\right ) .",
    "\\nonumber\\end{aligned}\\ ] ]      by the continuity of @xmath487 , we can construct the following taylor series of @xmath461 around the conditional expected value @xmath488 .",
    "@xmath489 where @xmath490,g(\\hat{{\\mathbf}{f}}_k({\\mathbf}{z})))$ ] .",
    "denote @xmath491 by @xmath492 .",
    "further define the operator @xmath493 $ ] and @xmath494    the variance of the estimator @xmath495 is given by @xmath496 = { { { \\mathbb{e}}}}{[({{\\mathbf}{\\hat{g}}}(f)-{{{\\mathbb{e}}}}{[{{\\mathbf}{\\hat{g}}}(f)]})^2 ] } \\nonumber \\\\ & & = \\frac{1}{n}{{{\\mathbb{e}}}}{\\left[(p{_1 } + q{_1 } + r{_1 } + s_1)^2\\right ] } \\nonumber \\\\ & & + \\frac{n-1}{n}{{{\\mathbb{e}}}}{\\left[(p{_1 } + q{_1 } + r{_1 } + s_1)(p{_2 } + q{_2 } + r{_2 } + s_2)\\right]}. \\nonumber\\end{aligned}\\ ] ] because @xmath497 , @xmath498 are independent , we have @xmath499 } = 0 $ ] . furthermore , @xmath500 } & = & { { { \\mathbb{e}}}}{[p{_1}^2 ] } + o(1 ) = { { { \\mathbb{v}}}}[g(\\check{{f}}_k({\\mathbf}{z}),{\\mathbf}{z } ) ] + o(1 ) .",
    "\\nonumber\\end{aligned}\\ ] ] applying lemma  [ momentlemma ] and lemma  [ covariancelemma ] , in conjunction with assumptions @xmath131 and @xmath138 , it follows that    * @xmath501 } =   { { { \\mathbb{v}}}}[g({\\check{{\\mathbf}{f}}_k({\\mathbf}{z})},{\\mathbf}{z } ) ] = c_4(g(\\check{f}_k(x),x))$ ] * @xmath502 } = c_5(g'(\\check{f}_k(x),x),g'(\\check{f}_k(x),x))\\left(\\frac{1}{m}\\right ) + o\\left(\\frac{1}{m}\\right ) $ ] * @xmath503 } =   o\\left(\\frac{1}{m}\\right)$ ] * @xmath504 } = o\\left(\\frac{1}{m}\\right ) $ ]    since @xmath505 and @xmath506 are @xmath22 mean random variables @xmath507 } = { { { \\mathbb{e}}}}\\left[q_1 \\psi({\\mathbf}{x}_2)(\\hat{{\\mathbf}{f}}_{\\mathbf}{}({\\mathbf}{x}_2)-{\\check{{f}}_k({\\mathbf}{x}_2)})^{\\lambda } \\right ] \\nonumber \\\\   & & = { { { \\mathbb{e}}}}\\left[q_1 \\psi({\\mathbf}{x}_2){\\hat{{\\mathbf}{e}}}_k^\\lambda({\\mathbf}{x}_2 ) \\right ] \\nonumber \\\\   & & \\leq   \\sqrt{{{{\\mathbb{e}}}}\\left [ \\psi^2({\\mathbf}{x_2})\\right]{{{\\mathbb{e}}}}\\left[q^2_1{\\hat{{\\mathbf}{e}}}_k^{2\\lambda}({\\mathbf}{x}_2 ) \\right ] } \\nonumber \\\\ & & = \\sqrt{{{{\\mathbb{e}}}}\\left [ \\psi^2({\\mathbf}{z})\\right]}\\left(o\\left(\\frac{1}{k^{\\lambda}}\\right ) \\right)\\nonumber\\end{aligned}\\ ] ] direct application of lemma  [ boundonexpec ] in conjunction with assumptions @xmath140 implies that @xmath508 = o(1)$ ] .",
    "note that from assumption @xmath131 , @xmath509 . in a similar manner",
    ", it can be shown that @xmath510 } = o\\left(\\frac{1}{m}\\right)$ ] and @xmath511 } = o\\left(\\frac{1}{m}\\right ) $ ] .",
    "this implies that @xmath512 & = & \\frac{1}{n}{{{\\mathbb{e}}}}{\\left[p{_1}^2\\right ] } + \\frac{(n-1)}{n}{{{\\mathbb{e}}}}{\\left[q{_1}q_{2}\\right ] } \\nonumber +   o\\left(\\frac{1}{m}+\\frac{1}{n}\\right ) \\nonumber \\\\ & = & c_4(g(\\check{f}_k(x),x))\\left(\\frac{1}{n}\\right)+ c_5(g'(\\check{f}_k(x),x),g'(\\check{f}_k(x),x))\\left(\\frac{1}{m}\\right )   + o\\left(\\frac{1}{m } + \\frac{1}{n}\\right ) \\nonumber \\\\ & = & c_4(g({f}(x),x))\\left(\\frac{1}{n}\\right)+ c_5(g'({f}(x),x),g'({f}(x),x))\\left(\\frac{1}{m}\\right ) + o\\left(\\frac{1}{m } + \\frac{1}{n}\\right ) \\nonumber \\\\ & = & c_4\\left(\\frac{1}{n}\\right)+ c_5\\left(\\frac{1}{m}\\right )   + o\\left(\\frac{1}{m } + \\frac{1}{n}\\right ) , \\nonumber\\end{aligned}\\ ] ] where the last but one step follows because , by ( [ inbias ] ) , we know @xmath474 .",
    "this in turn implies @xmath513 and @xmath514 . finally , by assumptions @xmath130 and @xmath138 , the leading constants @xmath169 and @xmath170 are bounded .",
    "because of the identical nature of the expressions of @xmath515 and @xmath516 in lemma  [ momentlemma ] and lemma  [ covariancelemma ] , it immediately follows that @xmath517 & = & c_4\\left(\\frac{1}{n}\\right)+ c_5\\left(\\frac{1}{m}\\right )   + o\\left(\\frac{1}{m } + \\frac{1}{n}\\right ) .",
    "\\nonumber\\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> the problem of estimation of density functionals like entropy and mutual information has received much attention in the statistics and information theory communities . </S>",
    "<S> a large class of estimators of functionals of the probability density suffer from the curse of dimensionality , wherein the mean squared error ( mse ) decays increasingly slowly as a function of the sample size @xmath0 as the dimension @xmath1 of the samples increases . </S>",
    "<S> in particular , the rate is often glacially slow of order @xmath2 , where @xmath3 is a rate parameter . </S>",
    "<S> examples of such estimators include kernel density estimators , @xmath4-nearest neighbor ( @xmath4-nn ) density estimators , @xmath4-nn entropy estimators , intrinsic dimension estimators and other examples . in this paper , we propose a weighted affine combination of an ensemble of such estimators , where optimal weights can be chosen such that the weighted estimator converges at a much faster dimension invariant rate of @xmath5 . </S>",
    "<S> furthermore , we show that these optimal weights can be determined by solving a convex optimization problem which can be performed offline and does not require training data . </S>",
    "<S> we illustrate the superior performance of our weighted estimator for two important applications : ( i ) estimating the panter - dite distortion - rate factor and ( ii ) estimating the shannon entropy for testing the probability distribution of a random sample . </S>"
  ]
}