{
  "article_text": [
    "in a typical supervised or unsupervised classification problem , each observation can be treated as a single point in the feature space @xmath0 .",
    "the data set is a finite point configuration @xmath1 with or without class labels @xmath2 . a cox process , or a doubly stochastic poisson process ( @xcite 1980 ; @xcite 1993 ; @xcite 2003 ) , provides a rich family of spatial point processes for aggregated point patterns .",
    "unfortunately , for most cox processes considered in the literature , no closed form for the distribution of @xmath3 is available .",
    "markov chain monte carlo methods are commonly used for computational purposes . @xcite ( 2006 ) introduced a special class of cox process , the permanental process , which is fairly flexible and has a closed form for the marginal density of @xmath3 .",
    "@xcite ( 2006 ) proposed a classification model based on the permanental process .",
    "regardless of the number of classes or the dimension of the feature variables , the model requires only 23 parameters for fitting the covariance function of the random intensity .",
    "the method is effective even when the region predominantly occupied by one class is a patchwork interlaced with regions occupied predominantly by other classes .",
    "one problem of the permanental model is that it requires the calculation of ratios of weighted permanents , which is an np - hard problem ( @xcite 1979 ) .    in the computer science literature ,",
    "the best approximation algorithm proposed by @xcite ( 2008 ) runs at an unappealing rate of @xmath4 .",
    "@xcite ( 2009 ) use an importance - sampling estimator to approximate weighted permanents up to a few hundred points .",
    "we propose a different way to solve the problem .",
    "it involves a series of approximations for the weighted permanental ratio based on its cyclic expansion . the classification based on cyclic approximations works reasonably well for the examples studied .",
    "following  @xcite ( 2006 ) , the permanental process on the feature space @xmath0 is a cox process with random intensity function @xmath5 where @xmath6 are independent and identically distributed gaussian random fields with mean zero and covariance function @xmath7 . for many applications , @xmath8 or @xmath9 .",
    "typically , a spatial pattern consisting of @xmath10 points @xmath11 is observed within a compact subset @xmath12 , or a bounded window , in @xmath0 .",
    "if @xmath13 is continuous on @xmath14 , it has the spectral representation @xmath15 where @xmath16 and @xmath17 are the eigenvalues and the normalized eigenfunctions of @xmath13 on @xmath12 , respectively .",
    "define a new covariance function on @xmath12 by @xmath18 we call @xmath19 the covariance function of the permanental process on @xmath14 .",
    "note that @xmath20 if all eigenvalues are close to @xmath21 .",
    "the marginal density ( @xcite 2006 , section  3.2 ) of the permanental process with respect to lebesgue measure at @xmath1 is @xmath22 where @xmath23 , and @xmath24 is the @xmath25-permanent of the @xmath26 matrix @xmath27 with components @xmath28 ( @xcite 1988 ) . here",
    "the sum runs over all permutations of @xmath29 and @xmath30 indicates the number of cycles .",
    "the usual permanent ( @xcite 1978 ) corresponds to @xmath31 , and @xmath32 .    for general positive definite @xmath19",
    ", the permanental process is defined only for positive integer values of @xmath33 ( @xcite , 2012 ) , but if @xmath28 is everywhere non - negative , the process can be extended to positive  @xmath25 .    unlike general cox processes , the permanental process has its density function in explicit form  ( [ marginal ] ) .",
    "the flexibility in choosing @xmath25 and @xmath19 makes the permanental process potentially useful for applied work .      for supervised classification problem with finitely many classes , the observations @xmath34",
    "@xmath35 @xmath36 come from @xmath37 possible classes .",
    "assume that the observations in class @xmath38 follow a permanental process with parameter @xmath39 and covariance function @xmath19 as in ( [ marginal ] ) .",
    "the superposition of @xmath37 independent permanental processes with same @xmath19 is a permanental process with parameter @xmath40 and the same covariance function @xmath19 .",
    "@xcite ( 2006 ) show that the conditional distribution of the label vector @xmath41 given the feature observations @xmath3 is @xmath42 where @xmath43 denotes the observations belonging to class @xmath38 and @xmath44 is defined in ( [ weighted ] ) .",
    "note that @xmath45 for the empty set @xmath46 .    for a supervised classification with known label vector  @xmath41 , the goal is to classify a new unit @xmath47 with observed feature vector @xmath48 into one of the @xmath37 classes .",
    "since the conditional distribution ( [ ygivenx ] ) applies to the extended sample , the conditional distribution is given by the theorem as follows .",
    "[ finiteclasstheorem ] given @xmath3 and @xmath41 , the conditional probability that a new unit @xmath47 with observed feature @xmath48 belongs to class @xmath38 is @xmath49 if @xmath50 , that is , no observation from class  @xmath38 has yet been observed , then the probability is proportional to @xmath51 .      for many classification applications , for example , to identify species of animal or type of cancer ,",
    "it is not appropriate to assume a finite number of classes in the population .",
    "we may consider the limit of ( [ ygivenx ] ) as @xmath52 , @xmath53 for all @xmath38 , and @xmath54 is fixed .",
    "fixing the number of observations @xmath10 , the limit distribution for the unlabelled partition @xmath55 of @xmath56 is @xmath57 where @xmath58 is the number of blocks of @xmath55 , @xmath59 is the set of observations belonging to block @xmath60 , and @xmath61 is the sum of cyclic products .",
    "the product in ( [ bgivenx ] ) runs over all blocks of @xmath55 . for example , @xmath62 is a partition of @xmath63 , then the blocks of @xmath55 are @xmath64 , @xmath65 , and @xmath66 , and the number of blocks @xmath67 . by ( [ bgivenx ] ) and the properties of conditional probability , we have    [ infiniteclasstheorem ] suppose there are infinitely many classes .",
    "given @xmath68 , the conditional probability of assigning a new unit @xmath47 with feature @xmath48 to block @xmath69 is @xmath70 the conditional probability of assigning @xmath47 to a new class @xmath71 is proportional to @xmath72 .",
    "if @xmath19 is constant on @xmath0 , equation  ( [ bgivenx ] ) reduces to the ewens sampling distribution ( @xcite 1972 ; @xcite 2006 ) , and expression  ( [ cycratio ] ) reduces to the seating plan of a chinese restaurant process ( @xcite 1985 ; @xcite 2006 ) .",
    "to apply the permanental classification model , we need to calculate the ratio @xmath73 or to calculate the cyclic ratio @xmath74 for each labelled class or unlabelled block .",
    "an efficient algorithm is critical .",
    "we propose analytic approximations to the permanental ratio for classification applications .",
    "the @xmath25-permanent of the matrix @xmath75 $ ] is a sum over @xmath76 terms . in a subset consisting of @xmath77 terms",
    ", the index  @xmath78 occurs in a cycle of length  1 , giving rise to the partial sum @xmath79 the index  @xmath78 may also occur in a cycle of length  2 such as @xmath80 or @xmath81 and so on .",
    "there are @xmath77 permutations in which @xmath78 occurs in a 2-cycle , giving rise to the additional sum @xmath82 where @xmath83 is the set of @xmath84 points with the @xmath85th element removed .",
    "similarly , the index  @xmath78 may occur in a 3-cycle such as @xmath86 or @xmath87 , giving rise to the sum @xmath88 in the cyclic expansion of the permanent of order  @xmath89 , there are @xmath77  terms in which @xmath78 occurs in a 1-cycle , @xmath77  terms in which @xmath78 occurs in a 2-cycle , @xmath77  terms in which @xmath78 occurs in a 3-cycle , and so on up to cycles of length  @xmath89 .",
    "therefore , we obtain the following finite expansion by cycles for ( [ perratior ] ) @xmath90\\biggr ) .",
    "\\nonumber\\end{aligned}\\ ] ] this cyclic expansion suggests a recursive approximation in which @xmath91 is the uni - cycle approximation for @xmath92 ; @xmath93 is the two - cycle approximation for @xmath94 ; @xmath95 is the three - cycle approximation for @xmath96 , and so on .",
    "the four - cycle approximation @xmath97 for @xmath98 is @xmath99 .",
    "\\nonumber\\end{aligned}\\ ] ] it is natural to let @xmath100 be the @xmath101-cycle approximation for @xmath102 .",
    "the two - cycle approximation @xmath103 or @xmath104 is a kernel function , which is an additive function of @xmath3 , while the three - cycle approximation is not .    for @xmath105 or @xmath106",
    ", @xmath107 is exact . for @xmath108 , @xmath109 in both cases , @xmath110 . by induction ,",
    "we obtain in general    [ rnnexacttheorem ] for @xmath111 , @xmath112 , and @xmath110 .    up to @xmath113 , that is , the four - cycle approximation , @xmath114 is easy to compute , even for fairly large values of  @xmath10 .",
    "the time complexity is @xmath115 for the two - cycle approximation , @xmath116 for the three - cycle approximation , and @xmath117 for the four - cycle approximation .",
    "for some special cases , the cyclic approximation provides an exact value for @xmath118 .",
    "[ example31 ] let @xmath119 , which corresponds to diagonal matrices . here",
    "@xmath120 is some positive non - random function on @xmath0 , and @xmath121 if @xmath122 and @xmath21 otherwise . if @xmath123 are pairwise different , then for each @xmath124 , @xmath125    [ example32 ] let @xmath126 for some constant @xmath127 , which corresponds to constant matrices .",
    "then @xmath128 . for each @xmath129 ,",
    "@xmath130 note that @xmath131 , @xmath132 .",
    "[ example33 ] let @xmath19 be a projection of rank  @xmath133 on @xmath0 .",
    "that is , @xmath134 then the two - cycle approximation determines a probability density in the sense that it is non - negative and has unit integral : @xmath135 a similar argument shows that the three - cycle and four - cycle approximations also integrate to unity , but it is not clear whether they are non - negative .",
    "[ exactratiotheorem ] suppose @xmath96 .",
    "( i ) if the @xmath26 matrix @xmath27 is diagonal , then @xmath136 ( ii ) if @xmath137 , @xmath138 , @xmath139 , then for @xmath140 , @xmath141 ( iii ) suppose @xmath27 is block - diagonal with constant blocks . that is , there exist a partition @xmath55 of @xmath142 and some constants @xmath143 , such that , @xmath144 if @xmath145 , and @xmath21 otherwise",
    ". then for @xmath140 , @xmath146    based on theorem  [ exactratiotheorem ] , the three - cycle or higher order cyclic approximation is exact if the @xmath26 matrix @xmath27 is diagonal , constant , or block - diagonal with constant blocks .",
    "the @xmath147 matrix @xmath148 may not be diagonal , constant , or block - diagonal .      for @xmath149 ,",
    "the accuracy of the approximation can be checked directly by comparison with the exact computation .",
    "our experience is that the three - cycle approximation is adequate in this range , and the four - cycle approximation usually has negligible error",
    ". for larger values , say @xmath150 , the accuracy can be checked by examining special cases in which the permanent can be calculated exactly in reasonable time . for example , to calculate the @xmath25-permanent of a penta - diagonal matrice @xmath151 , that is , @xmath152 for @xmath153 , three - cycle or higher order cyclic approximation is essentially exact . for more general matrices ,",
    "the accuracy can be gauged to some extent from an examination of the sequence of approximations .",
    "[ fig1 ]    -0.3 cm    the left panel of figure  1 shows the approximate values of the permanental ratio ( [ perratior ] ) for a sample of 100 @xmath3-values in @xmath154 , plotted as a function of @xmath78 in the same range .",
    "the 100 points are generated from the symmetric triangular distribution on @xmath154 .",
    "for this example , @xmath155 , and @xmath156 with @xmath157 . in the central peak ,",
    "the lowest curve is the two - cycle approximation , and the next two curves are successive approximations up to four - cycle .",
    "the highest curve is the estimated values from the importance sampler described by @xcite ( 2009 , section  4 ) .",
    "the shape of these relative intensity functions depends fairly strongly on the value of @xmath158 , but only slightly on @xmath25 . in all cases ,",
    "the difference between the three - cycle and four - cycle approximations is considerably smaller than the difference between the two - cycle and three - cycle ones . for @xmath159 ,",
    "the four - cycle approximation is approximately 6% larger than the three - cycle in the central peak , while the three - cycle approximation is approximately 18% larger than the two - cycle one . on average ,",
    "the relative differences between the cyclic approximations and @xcite s importance sampling estimate are 19% for two - cycle , 12% for three - cycle , and 10% for four - cycle approximations , respectively .",
    "to check the performance of our cyclic approximations for supervised classification applications , we generate another 100 points from the symmetric triangular distribution on @xmath160 denoted by class  2 and regard the first 100 points shown in figure  1 as class  1 s . according to expression  ( [ perratio ] ) , we can calculate the probability that a point with feature @xmath78 belongs to class  1 .",
    "the right panel of figure  1 plots the probabilities when the permanental ratios are calculated based on the cyclic approximations or @xcite s importance sampler .",
    "the differences among the four approximations are negligible .",
    "the maximum relative differences between the cyclic approximations and @xcite s estimate are 4.3% for two - cycle , 3.4% for three - cycle , and 3.3% for four - cycle , respectively .",
    "if we regenerate class  2 from a symmetric triangular distribution on @xmath161 which is overlapped with the region of class  1 , the maximum relative differences can be as large as @xmath162 and @xmath163 for the two - cycle and three - cycle approximations , while the four - cycle approximation still works reasonably well with a maximum relative difference @xmath164 .",
    "the worst cases usually occur at the boundary or the overlapped part @xmath165 . even for the overlapped distributions , the corresponding maximum absolute differences between the cyclic approximations and @xcite s estimate are @xmath166 for two - cycle , @xmath167 for three - cycle , and @xmath168 for four - cycle approximations in terms of class probability .    as for computation time",
    ", it took a personal computer with 2.8ghz cpu and 2 gb ram 1.3 seconds in total to finish all calculations based on two - cycle , three - cycle and four - cycle approximations , or about 700 seconds based on @xcite s importance sampler with sample size 20,000 .",
    "we use an artificial example to illustrate how the proposed model with cyclic approximation works for a supervised classification problem .",
    "this example has two classes in a @xmath169 by @xmath169 chequer - board layout with classes labelled as follows .    [",
    "cols=\"^,^,^\",options=\"header \" , ]     given that the correct classification is determined by the chequerboard rule , the error rates for training data and @xmath170 grid points serving as testing data are summarized in table  1 .",
    "for comparison purposes , some commonly used classifiers are listed in table  1 too .",
    "in addition to the neural network method and support vector machine , we also check the results based on an aggregated classification tree with bagging number 100 and a @xmath37-nearest neighbor classifier with @xmath171 chosen by cross - validation . diagonal linear discriminant analysis and logistic regression",
    "do not work for the original @xmath172 and @xmath173 in this case , because the class regions are non - convex and interlaced .",
    "the leukemia dataset described by @xcite uses microarray gene expression levels for cancer classification .",
    "it consists of 72 tissue samples from two types of acute leukemia , 47 samples of type all and 25 of type  aml .",
    "the version used here , from the r package golubesets downloaded from http://bioconductor.org , contains expression levels for 7129 genes in each of 72 tissue samples .",
    "[ fig4 ]    -0.3 cm    the left panel of figure  3 shows a two - dimensional projection in which the @xmath3  axis is the straight line joining the class centroids , and the @xmath41  axis is the first principal component . unlike the usual heatmap display such as fig .",
    "3b in @xcite , each sample is plotted here as a single point .",
    "the goal is to classify each new tissue sample as all or aml based on the gene expression levels .",
    "the leukemia dataset has been widely used for testing classifiers .",
    "@xcite did a comprehensive comparison of various discriminant methods using this dataset as well as two other popular microarray datasets . based on their study , the nearest neighbor classifier and the diagonal linear discriminant analysis",
    "work the best when @xmath174 selected genes are considered .    to compare the performance of the proposed method with other methods",
    ", we follow the training / testing partitioning procedure used by @xcite .",
    "the 72 samples are randomly divided into 48 training points and 24 testing points .",
    "each classifier is fitted or trained using the 48 training points and tested using the 24 testing points .",
    "the number of misclassified points out of 24 is recorded .",
    "the procedure is repeated 200 times for each classifier .",
    "the number of test errors on average is used to evaluate the performance of classifiers .",
    "the right panel of figure  3 shows the number of prediction errors on average over 200 random training / testing partitions .",
    "the genes used for discriminant analysis are selected according to the ratio of between - group variance to within - group variance ( * ? ? ?",
    "* section  3.4 ) .",
    "the proposed models with @xmath175 or @xmath176 are compared with the two winners , @xmath37-nearest neighbor and diagonal linear discriminant analysis methods , in @xcite , as well as the support vector machine method which became popular more recently . as the number of selected genes increases , the mean number of test errors of the four classifiers follows a similar pattern .",
    "it decreases initially as more information becomes available for parameter estimation , but subsequently increases as the signal becomes lost in the noise .",
    "the proposed models with @xmath175 and @xmath176 perform as well as the support vector machine , but better than the @xmath37-nearest neighbor and diagonal linear discriminant analysis methods , in the sense of minimum average error count . compared with the support vector machine",
    ", the proposed model performs reasonably well even with bad selection of covariates .",
    "it seems more capable of handling high - dimensional data .",
    "this is critical when the true classification relies on non - reducible high dimensional features . in terms of computational time ,",
    "the proposed method is comparable with the neural network and support vector machine methods with moderate data size , but slower than the diagonal linear discriminant and @xmath37-nearest neighbor methods .",
    "as the number of feature variables increases , the error rates increase for all classifiers , but more rapidly for the neural network and support vector machine than for either permanental classifier .",
    "the authors thank the editor , the associate editor , and the referee for valuable comments .",
    "this research was supported by grants from the u.s . national science foundation .",
    "* proof of theorem  [ exactratiotheorem ] * we only need to prove case  ( iii ) . because case  ( i ) corresponds to @xmath177 , while case  ( ii ) corresponds to @xmath178 .",
    "first if @xmath179 is also block - diagonal , then @xmath180 given @xmath181 , or @xmath182 given that @xmath78 does not belong any block of @xmath55 . here",
    "@xmath183 . therefore , @xmath184 given @xmath185 ; @xmath186 given @xmath187 ; and so on .",
    "the formula for @xmath118 in case  ( iii ) can be justified by applying mathematical induction on the cyclic expansion of @xmath188 . for its cyclic approximations ,",
    "it is straightforward to verify that @xmath190 .",
    "the formula for @xmath114 with @xmath191 can be justified using the equation below with index @xmath192 .",
    "@xmath193        bezkov , i. , stefankovic , d. , vazirani , v.  v. & vigoda , e. ( 2008 ) .",
    "accelerating simulated annealing algorithm for the permanent and combinatorial counting problems .",
    "_ siam journal on computing _ , 14291454 .",
    "dudoit , s. , fridlyand , j. & speed , t.  p. ( 2002 ) .",
    "comparison of discrimination methods for the classification of tumors using gene expression data . _ journal of the american statistical association _ * 97 * , 7787 .",
    "golub , t.  r. , slonim , d.  k. , tamayo , p. , huard , c. , gaasenbeek , m. , mesirov , j.  p. , coller , h. , loh , m.  l. , downing , j.  r. , caligiuri , m.  a. , bloomfield , c.  d. & lander , e.  s. ( 1999 ) . molecular classification of cancer : class discovery and class prediction by gene expression monitoring .",
    "_ science _ * 286 * , 531537 ."
  ],
  "abstract_text": [
    "<S> we introduce a doubly stochastic marked point process model for supervised classification problems . </S>",
    "<S> regardless of the number of classes or the dimension of the feature space , the model requires only 23 parameters for the covariance function . </S>",
    "<S> the classification criterion involves a permanental ratio for which an approximation using a polynomial - time cyclic expansion is proposed . </S>",
    "<S> the approximation is effective even if the feature region occupied by one class is a patchwork interlaced with regions occupied by other classes . </S>",
    "<S> an application to dna microarray analysis indicates that the cyclic approximation is effective even for high - dimensional data . </S>",
    "<S> it can employ feature variables in an efficient way to reduce the prediction error significantly . </S>",
    "<S> this is critical when the true classification relies on non - reducible high - dimensional features .    </S>",
    "<S> _ keywords : _ cyclic approximation ; dna microarray analysis ; high - dimensional data ; supervised classification ; weighted permanental ratio . </S>"
  ]
}