{
  "article_text": [
    "the @xmath0-body simulation technique , in which the equations of motion of @xmath0 particles are integrated numerically , has been one of the most powerful tools for the study of astronomical objects such as the solar system , star clusters , galaxies , clusters of galaxies and large - scale structures of the universe .",
    "roughly speaking , the target systems for @xmath0-body simulations can be classified into two categories : collisional systems and collisionless systems . in the case of collisional systems , the evolution of the system",
    "is driven by two - body relaxation process , in other words , by microscopic exchange of thermal energies between particles . in this case",
    ", the simulation timescale tends to be long , since the relaxation timescale measured by the dynamical timescale is proportional to @xmath1 , where @xmath0 is the number of particles in the system .",
    "the calculation cost of the simulation of collisional systems increases rapidly as we increase the number of particles @xmath0 , because of the following two reasons .",
    "first , as stated above , the relaxation timescale increases roughly linearly as we increase @xmath0 .",
    "this means the number of timesteps also increases at least linearly@xcite .",
    "the second reason is that it is not easy to use fast and approximate algorithms such as barnes - hut tree algorithm@xcite or the fast multipole method@xcite to calculate the interaction between particles .",
    "those imply that the cost per timestep is @xmath2 , and that the total cost of the simulation is @xmath3 .",
    "there are two reasons why the use of approximate algorithms for the force calculation is difficult .",
    "the first reason is the need for relatively high accuracy .",
    "since the total number of timesteps is very large , we need a rather high accuracy for the force calculation .",
    "the other reason is the wide difference in the orbital timescale of particles .",
    "a unique nature of the gravitational @xmath0-body problem is that particles interact only through gravity , which is an attractive force .",
    "this means that two particles can approach arbitrary close during a hyperbolic close encounter .",
    "in addition , spatial inhomogeneity tends to develop , resulting in a high - density core and a low - density halo . even on average , particles in the core",
    "require much smaller timesteps than particles in the halo do .",
    "it is clearly very wasteful to apply the same timestep to all particles in the system , and it is crucial to be able to apply individual and adaptive timestep to each particle .",
    "such an `` individual timestep '' algorithm , first developed by aarseth ( ) , has been the core for practically any program that handles the time integration of collisional @xmath0-body systems such as star clusters and systems of planetesimals .",
    "the basic idea of the individual timestep algorithm is to assign different times and timesteps to particles in the system . for particle @xmath4 ,",
    "its next time is @xmath5 , where @xmath6 is the current time and @xmath7 is the current timestep . to integrate the system , we first chose a particle with minimum @xmath8 and set the current system time @xmath9 to be @xmath8 .",
    "then , we predict the positions of all particles at time @xmath9 and calculate the force on particle @xmath4 .",
    "finally , we correct the position of particle @xmath4 using the calculated force , update @xmath6 and determine the new timestep @xmath7 . in practice",
    ", we force the size of timesteps to be powers of two , so that the system time is quantized and multiple particles have exactly the same time . in this way , we can use parallel or vector processors efficiently , since we can integrate multiple particles in parallel @xcite .",
    "it is necessary to use the linear multistep method ( predictor - corrector method ) with variable stepsize for the time integration .",
    "aarseth adopted an algorithm with third - order newton interpolation .",
    "recently , the method based on the third - order hermite interpolation@xcite has become widely used , because of its simplicity .    in principle , it is not impossible to combine individual timestep algorithm and fast algorithms such as barnes - hut tree algorithm or fmm .",
    "mcmillan and aarseth ( ) developed such a combination , where the tree structure is dynamically updated according to the move of particles and force is calculated using multipole expansion up to octupole .",
    "they assigned predictor polynomials to each node of the tree structure so that they could calculate the force from nodes to particles at arbitrary times .",
    "a serious problem with such a combination is that there is no known method to implement it on parallel computers with distributed memory .",
    "it is not simple to achieve a good parallel performance with individual timestep algorithm , even without the tree algorithm .",
    "the reason is that simple methods require fast and low - latency communication between processors .",
    "the recently proposed two - dimensional algorithm @xcite somewhat relaxes the requirement for the communication bandwidth , but it still requires low - latency communication . when combined with the tree algorithm , efficient parallelization becomes even more difficult .    distributed - memory parallel computers have been used to run large - scale cosmological simulations , with or without individual timestep algorithm@xcite . in this case , we use simple spatial decomposition to distribute particles over processors .",
    "this works fine with large - scale cosmological simulations , where the distribution of particles in large scale is almost uniform .",
    "many structures form from initial density fluctuations , and many small high - density regions develop .",
    "even so , we can still divide the entire system so that the calculation load is reasonably well balanced . in addition",
    ", the range of the timesteps is relatively small .",
    "to parallelize the simulation of a single star cluster is much more difficult , because the calculation cost is dominated by a small number of particles in a single , small core@xcite .",
    "therefore , communication latency becomes the bottleneck , and it is difficult to parallelize the simple direct summation algorithm . as a result , no good parallel implementation of the combination of the tree algorithm and individual timestep algorithm exists . to really accelerate calculation of a single cluster , we need an approach different from what has been tried .",
    "there are three different approaches to improve the speed of any simulation : a ) to use a faster computer , b ) to use algorithms with smaller calculation cost , and c ) to improve the efficiency of the algorithm used .",
    "usually , option ( a ) means to use commercially available fast computers , which , at present , means distributed - memory parallel computers . an alternative possibility is to develop a computer by ourselves .",
    "we have been pursuing this direction , starting with grape-1 @xcite .",
    "the basic idea of the grape ( gravity pipe ) architecture @xcite is to develop a fully pipelined processor specialized for the calculation of the gravitational interaction between particles . in this way , a single force - calculation pipeline integrates more than 30 arithmetic units , which all operate in parallel . in the cause of the hermite time integration",
    ", we also need to calculate the first time derivative of the force , resulting in nearly 60 arithmetic operations .",
    "this means that we can integrate a large number of arithmetic unit into a single hardware with minimal amount of additional logic .",
    "grape-1 was an experimental hardware with very short word format ( relative force accuracy of 5% or so ) , and not really suited for simulations of collisional systems .",
    "however , its exceptionally good cost - performance ratio made it useful for simulations of collisionless systems @xcite . also , we developed an algorithm to accelerate the barnes - hut tree algorithm using grape hardware @xcite , and developed grape-1a @xcite , which was designed to achieve good performance with treecode .",
    "thus , grape approach turned out to be quite effective , not only for collisional simulations but also for collisionless simulations , and also for sph simulations @xcite . grape-1a and",
    "its successors , grape-3 @xcite and grape-5 @xcite have been used by researchers worldwide for many different problems .    in this paper , we discuss grape-6 , our newest machine for the simulation of collisional systems .",
    "we briefly summarize the history of hardwares here .",
    "grape-2 @xcite adopted usual 64- and 32-bit floating point number format , and could be used with aarseth s nbody3 program .",
    "after grape-2 , we developed grape-3@xcite , which is essentially an lsi implementation of grape-1 . in grape-1 ,",
    "arithmetic operations were realized by fixed - point alu chips and rom chips , and in grape-2 by floating - point alu chips .",
    "thus , we needed several tens of lsis to realize a single pipeline .",
    "with grape-3 , we implemented a single pipeline to a single custom lsi chip , and developed a board with 24 chips .",
    "in this way , we achieved the speed of 9 gflops per board ( 24 chips each performing 38 operations on 10 mhz clock cycle ) .",
    "grape-4@xcite is similarly a single - lsi implementation of grape-2 , or actually that of harp-1@xcite , which was designed to calculate force and its time derivative .",
    "a single grape-4 chip calculates one interaction in every three clock cycles , performing 19 operations .",
    "its clock frequency was 32 mhz and peak speed of a chip was 608 mflops .",
    "a major difference between grape-4 and previous machines is its size .",
    "grape-4 integrated 1728 pipeline chips , for the peak speed of 1.08 tflops .",
    "the machine is composed of 4 clusters , each with 9 processor boards .",
    "a single processor board houses 48 processor chips , all of which share a single memory unit through another custom chip to handle predictor polynomials .",
    "grape-4 chip uses two - way virtual multiple pipeline , so that one chip looks like two chips with half the clock speed .",
    "thus , one grape-4 board calculates the forces on 96 processors in parallel .",
    "different boards calculate the forces from different particles , but to the same 96 particles .",
    "forces calculated in a single cluster are summed up by special hardware within the cluster .    in this paper",
    ", we describe the architecture and performance of grape-6 , which is the direct successor of grape-4 .",
    "the main difference between grape-4 and grape-6 is in the performance .",
    "the grape-6 chip integrates 6 pipelines operating at 90 mhz , offering the speed of 30.8 gflops , and the entire grape-6 system with 2048 chips offers the speed of 63.04 tflops .",
    "the plan of this paper is as follows . in section 2",
    ", we describe the overall architecture , and in sections 3 and 4 the details of implementation . in section 5 ,",
    "we discuss the difference between grape-4 and grape-6 . in section 6",
    "we discuss the performance .",
    "section 7 is for discussions .",
    "those who are interested in how to use grape-6 , but not much in the design details , could skip section 2.1 , most of section 3 and section 5 .",
    "# 1_#1 # 1^#1    in this section , we give the overview of the architecture of grape-6 .",
    "what grape-6 calculates are the following .",
    "first , it calculates the gravitational force , its time derivative , and potential , given by equations @xmath10,\\\\ \\label{eqn : phi } \\phi_i = & \\sum_j gm_j\\displaystyle{1 \\over ( r_{ij}^2+\\epsilon^2)^{1/2}},\\end{aligned}\\ ] ] where @xmath11 , @xmath12 , and @xmath13 are the gravitational acceleration , its first time derivative , and the potential of particle @xmath4 , @xmath14 , @xmath15 and @xmath16 are the mass , position and velocity of particle @xmath4 , @xmath17 is the gravitational constant and @xmath18 is the softening parameter .",
    "grape-6 hardware assumes @xmath19 .",
    "if necessary , the host computer can multiply the result calculated by grape-6 by some constant to use @xmath17 other than one . also note that potential is calculated without minus sign .",
    "relative position @xmath20 and relative velocity and @xmath21 are defined as @xmath22 while calculating the force , it also evaluates the distance to the nearest neighbor @xmath23 and the value of index @xmath24 which gives the minimum distance .",
    "in addition , it constructs the list of neighbor particles , whose distance squared ( with softening , @xmath25 ) is smaller than pre - specified value @xmath26 .",
    "the position @xmath27 and velocity @xmath28 of particles that exert the forces are `` predicted '' by the following predictor polynomial    @xmath29    where @xmath30 and @xmath31 are the predicted position and velocity , @xmath32 , @xmath33 , @xmath34 and @xmath35 are the position , velocity , acceleration and its time derivative of particle @xmath24 at time @xmath36 , and @xmath37 is the difference between the current time @xmath38 of particle @xmath24 and system time @xmath9 , @xmath39 @xmath40      here , we briefly summarize how grape-6 ( and grape-4 ) works with individual timestep algorithm . for more detailed discussion ,",
    "see @xcite or @xcite .",
    "the time integration proceeds in the following steps    \\a ) as the initialization procedure , the host sends all data ( position , velocity , acceleration , its first time derivative , mass and time ) of all particles to grape memory unit .",
    "\\b ) the host creates the list of particles to be integrated at the present timestep .",
    "\\c ) for each particles in the list , repeat the steps ( d)-(g ) .",
    "\\d ) the host predicts the position and velocity of the particle , and sends them to grape .",
    "grape stores them in the registers of the force calculation pipeline .",
    "it also sets the current time to a register in the predictor pipeline .",
    "\\e ) grape calculates the force from all other particles .",
    "positions and velocities of other particles at the current time are calculated in the predictor pipeline .",
    "\\f ) after the calculation is finished , the host retrieves the result .",
    "\\g ) the host integrates the orbits of the particles and determines new timesteps .",
    "\\h ) update the present system time and go back to step ( b ) .    here , the key to achieve good performance",
    "is to send only particles updated in the current timestep to grape hardware .",
    "thus , grape hardware need to have the memory unit large enough to keep all particles in the system .",
    "this is usually not a severe limitation , since even with fast grape hardwares , the number of particles we can handle with direct summation algorithm is not very large .",
    "the top - level architecture of grape-6 is shown in figure [ fig : toplevel ] .",
    "it consists of 4 `` clusters '' , each of which comprises 16 grape-6 processor boards ( pb ) , 4 host computers ( h ) , and interconnection networks .",
    "these 4 clusters are connected by gigabit ethernet . for host computers ,",
    "we currently use pcs with amd athlon xp 1800 + cpu and sis 745 chipset .",
    "ethernet cards are 1000bt cards with ns 83820 single - chip ethernet controllers .    in the following",
    ", we will describe how we run parallel program on grape-6 .",
    "first , let us concentrate on the parallelization within a cluster .",
    "( 6 cm,6 cm)grape6_full_network2.eps    figure [ fig : g6cluster ] shows one cluster . four processor boards are connected to a host computer through a network board .",
    "four network boards are connected to each other , so that we can use a cluster as single unit or as multiple units .",
    "first consider the simplest case , where we just use 4 hosts to run independent calculations . in this case",
    ", 4 processor boards connected to a host through one network board calculate the forces on the same set of particles , but from different set of particles [ what we called @xmath24-parallelism in @xcite ] .",
    "each processor board stores the different subset of particles in the particle memory , and calculates the forces on the particles stored in the registers in the processor chips .",
    "the partial forces calculated in different boards are sent in parallel to the network board , where they are added together by an adder tree .",
    "the host computer receives the summed - up forces .",
    "as will be discussed later , multiple processor chips on one board also have their local memories to store particles .",
    "they calculate the forces on the same set of particles , but from different sets of particles .",
    "the partial forces are summed up by the adder tree on the processor board . from the logical point of view",
    "there is no difference between a single - board system and multi - board system , as far as we use a single host .",
    "we can regard the entire system just as a huge adder tree with processor chips at all leaves .",
    "when all 16 boards and 4 hosts are used as a single unit , the particles are divided to 4 groups and each group is assigned to one host .",
    "conceptually , the @xmath24-th board connected to host @xmath4 calculates the force on particles in host @xmath4 , from particles in host @xmath24 .",
    "summation of the partial forces is performed in the same way as in the case of single - host calculation .",
    "the only difference is that the data to be stored in the memory come from other hosts .",
    "( 5 cm,5 cm)4x16cluster.eps    in order to allow both single - host and multi - host calculations , the network board must switch between broadcast mode ( for the single - host calculation ) and point - to - point mode ( for the multi - host calculation ) .",
    "it would also be useful if we can use two hosts together . in this case",
    ", it is necessary to accept two inputs , and to pass each of them to two boards .",
    "thus , we need three operation modes for the network board . one simple way to implement these three modes is shown in figure [ fig : scalablenet ] . here , nodes a and b simply output the inputs from the left - hand side ports to two output ports .",
    "nodes c , d , and e can select one from two inputs . in the case of node",
    "c , the selected input is sent to two output ports .",
    "( 6 cm,6 cm)scalablegrape2.eps    this network can be configured in three ways . in the first mode ,",
    "all nodes select input from the lower ports in figure [ fig : scalablenet ] .",
    "in other words , c takes input from input port 2 , d from input port 1 , and e from input port 3 . in this case",
    ", each grape receives data from the input port with the same index . in this mode",
    "we can use this 4-grape network as part of 4-host , 16-grape system . in the second mode ,",
    "node c selects the input from port 2 , while d and e selects data from upper input port in figure [ fig : scalablenet ] ( nodes b and c for nodes d and e , respectively ) . in this mode ,",
    "grapes 0 and 1 receive the same data from port 0 .",
    "similarly , grapes 2 and 3 receive the data from port 2 . in other words , grapes",
    "0 and 1 ( and 2 and 3 as well ) are effectively bundled together to behave as one system , and we can use this system as a part of 2-host , 8-grape system . in the third mode ,",
    "all nodes select upper inputs , thereby sending the data from port 0 to all grapes . in this way",
    ", we can use this 4-grape network as a single system connected to one host .",
    "an important character of this network is that its hardware cost is @xmath41 , where @xmath42 is the number of grape hardwares .",
    "thus , even for very large systems , the cost of the network remains small . by using this hardware network to send data from multiple host to processor boards under one host in parallel",
    ", we can improve the parallel efficiency quite significantly .",
    "there are many possible algorithms to parallelize the calculation over multiple clusters . here",
    "we show just one example , which is a generalization of the `` copy '' algorithm @xcite . in the copy algorithm",
    ", each node has the complete copy of the system . at each timestep , each node , however , integrates its own share of particles , which is either statically or dynamically assigned to it .",
    "after one step is finished , all nodes broadcast particles they updated , so that all nodes have the same updated system . in the case of multi - cluster calculation",
    ", each cluster has a complete copy of the system , which is distributed to 4 hosts .",
    "for example , host 0 of cluster 0 and host 0 of cluster 1 have the same data . in the time integration , calculation load",
    "is divided between all hosts in the different clusters with the same internal index .",
    "after one step is finished , updated data are exchanged again between hosts in different clusters with the same index .",
    "one could use `` ring '' algorithm or 2-d algorithm @xcite , but for 4 clusters the difference in the performance is rather small .    in principle , we could extend the network board to form 8-input , 8-output switch , so that we can use all 64 boards as a `` single cluster '' .",
    "we decided not to do this since for many scientific applications we will use the system as a correction of single - host systems to run multiple simulations independently . to run multiple calculations , it is more efficient to have larger number of host computers .",
    "figure [ fig : processorboard ] shows the structure of a processor board .",
    "it houses 8 processor modules .",
    "the processor board has one broadcast network that broadcasts data from the input port to all processor modules , and one reduction network that reduces the results obtained on 32 chips and returns it to the host through the output port .",
    "each processor module consists of 4 processor chips each with its memory , and one summation unit .",
    "the structure of a processor module is the same as that of the processor board , except that it has 4 processor chips instead of 8 processor modules .",
    "figure [ fig : processormodule ] shows the structure of a processor module .",
    "the memories attached to one processor chip can store up to 16,384 particles .",
    "thus , a single board with 32 chips can handle up to 524,288 particles , for direct summation code with individual timestep .",
    "a @xmath43-board cluster can handle up to 2 million particles .",
    "if one wants to use more than 2 million particles with direct summation , it is possible to use the ring algorithm ( see section 5.2 ) .",
    "the calculation with 8 million particles is theoretically possible on a single cluster with 16 processor boards .",
    "( , ) g6pb2c.eps    ( 6 cm,6 cm)g6module4d.eps    in the next two sections , we present the detailed description of the hardware , in a bottom - up fashion . in section [",
    "sect : chip ] , we describe the processor chip and in section [ sect : pbnh ] the processor board , network board and interconnection .",
    "the grape-6 processor chip was fabricated using toshiba tc-240 process ( nominal design rule of @xmath44 .",
    "the physical size of the chip is roughly 10 mm by 10 mm , and packaged into 480-contact bga package .",
    "it operates at 90 mhz clock cycle .",
    "power supply voltage is 2.5v .",
    "heat dissipation is around 12 w at the maximum .",
    "a processor chip consists of six force calculation pipelines , a predictor pipeline , a memory interface , a control unit and i / o ports .",
    "figure [ fig : processorchip ] shows the overview of the chip . in the following , we discuss each block in turn .",
    "( 7 cm,7 cm)g6_blocks_pasj.eps      the task of the force calculation pipeline is to evaluate equations ( [ eqn : force])([eqn : phi ] ) .",
    "it also determines the nearest neighbor particle and its distance .",
    "this function is rather convenient for detecting close encounters or physical collisions between particles that require special treatments . for this purpose ,",
    "the indices of particles that exert forces are supplied to the pipeline .",
    "the indices are also used to avoid self - interaction .",
    "the force calculation pipeline has the register for the index of the particle for which the force is calculated , and avoids the accumulation of the result if two indices are the same .",
    "this capability is introduced to avoid the need to send particles twice to the memory in the case of the individual timestep algorithms .    with the individual timestep algorithm and the hardwired predictor pipeline ,",
    "the data of particles which exert forces are evaluated by the predictor pipeline on chip , while the data for the particle for which the force is calculated is evaluated on the host computer and sent to the register of the force calculation pipeline .",
    "these two values are not exactly the same , since the data format and accuracy of the hardware predictor are different from that of the host computer .",
    "grape-4 pipeline did not have the logic to use the particle index , and the only way to avoid the self interaction was to make the data exactly the same . to achieve this , for the particles to be updated",
    ", we sent the predicted data at the current time to the memory as well as the registers .",
    "this means that we had to send @xmath24-particles twice per timestep . with the index - based approach",
    ", we need to send @xmath24-particles only once per timestep , resulting in a significant reduction in the total amount of communication .    for grape-6 pipeline , we adopted the 8-way vmp [ virtual multiple pipeline , @xcite ] , in which single physical pipeline serves as eight virtual pipelines , calculating the forces on 8 different particles . in this way",
    ", we can reduce the requirement for the memory bandwidth by a factor of 8 , since all vmps ( and also physical multiple pipelines on a chip ) calculate the forces from the same particle .    in the physical implementation of the pipeline",
    ", we adopted several different number representations , depending on the required accuracy . for input position data",
    ", we used 64-bit fixed point format .",
    "the reason we used the fixed point format here is to simplify the hardware .",
    "additional advantage of using the fixed - point format is that the implementation of the periodic boundary condition is simpler than that in the case of the floating - point data format@xcite .    after first subtraction between two position vectors ,",
    "the result is converted to floating - point format with 24-bit mantissa . here , floating - point format is preferred , since otherwize we need very large multipliers .    for the final accumulation",
    ", we return to the 64-bit fixed - point format , again to simplify the hardware . here , we specify the scaling factor for each particle , so that we can calculate the forces with very different magnitude , without causing overflow or underflow .",
    "the pipeline for the calculation of the time derivative is designed in a similar way , but with 20-bit mantissa for intermediate data and 32-bit fixed - point format for the final accumulation .",
    "since the time derivative is one order higher than the force , the required accuracy is lower .",
    "( , 6 cm)harppipe - pasj.eps    figure [ fig : forcepipe ] shows the block diagram of the pipeline .",
    "it consists of arithmetic units to perform the operations shown in table [ tab : forcepipe ]    .arithmetic operations in force calculation pipeline [ cols=\"<,<,^,^,^\",options=\"header \" , ]     ( 6 cm,6 cm)treespeed1.ps      since the performance of the single - host grape-6 is limited by the speed of the host computer , an obvious way to improve the performance is to use multi - host systems .",
    "figure [ fig : ptreespeed1 ] shows the performance of the parallel tree algorithm . the program used is a newly written one based on orthogonal recursive multi - section , a generalization of widely used orb tree that allows a division to arbitrary number of domains in one dimension , instead of allowing only bisection .",
    "the primary advantage of this algorithm is that it can be used on systems with number of host computers not exactly a power of two .",
    "we measured the performance on 1 , 2 , 3 , 4 , 6 , 8 and 12 hosts .",
    "the distribution of the particles is again the plummer model .",
    "one can see that the scaling is again pretty good .",
    "12-hosts calculation is 9.3 times faster than single - host calculation .",
    "parallel efficiency is better than 75% , even for relatively small number of particles shown here .",
    "( 6 cm,6 cm)ptreespeed2.ps",
    "though we regard grape-6 a reasonable success , this certainly does not mean we did everything right . we did make quite a few mistakes , some of them affected the performance , some affected the reliability , some extended the development time , and some limited the application range . in the following we briefly discuss them in turn .",
    "concerning the performance , the largest problem with grape-6 is that its clock frequency is somewhat below the expected value .",
    "the design goal ( for the `` worst case '' ) was 100 mhz , while our actual hardware is currently running at 90 mhz . with grape-4 ,",
    "the design goal was 33 mhz , and the machine operated without any problem at 32 mhz .",
    "the processor chip itself was confirmed to operate fine at 41 mhz .",
    "the primary reason for the low operating frequency is the problem with the stability of the power supply to the chip , or the impedance of the power line .",
    "compared to the grape-4 processor chip , grape-6 processor chip consumes about two times larger power , at half the supply voltage .",
    "thus , to keep the relative drop of the supply voltage to be the same , the impedance of the power line must be 1/8 of that of grape-4 .",
    "this is a quite difficult goal to achieve .",
    "initially even the manufacturer of the chip did not fully appreciated how hard it was . as a result",
    ", the first sample of the chip could not operate correctly at the clock speed higher than 60 mhz .",
    "the problem was that , when calculation starts , the power consumption of the chip increases by about a factor of two compared to that when the chip is idle .",
    "because of the large total resistance of the power line in the lsi package and the power plane of the silicon chip itself , the supply voltage to the transistors decreases , and as a result their switching speed slows down .    in the second design , the manufacturer came up with additional power plane and increased number of power and ground pins , which reduced the resistance significantly .",
    "however , the result was still rather unsatisfactory .",
    "the manufacturer was not alone in making this kind of mistakes . in the design of the processor board and the power supply",
    ", we also made similar mistakes .",
    "in the first design , we used traditional large switching power units with relatively low switching frequency .",
    "this unit turned out to be unable to react to the quick change of the load between idle and calculation states .",
    "the normal electrolytic capacitors also turned out to be completely useless in stabilizing the power supply voltage .",
    "thus , we need to redesign the power supply unit with high - frequency inverters and low - esr capacitors .    in hindsights , we could have borrowed the design of power supply units for standard pc motherboards ( for intel processors ) , which were designed to meet quite similar requirements , but for an extremely low cost .",
    "the power supply circuit for typical pc motherboard would be good enough to support single module with 4 chips .",
    "we then can supply 12 v to pcb .",
    "these apparently minor technical details are absolutely crucial for the manufacturing of high - performance computers .",
    "another problem with the current grape-6 chip is its limited i / o performance of only 90mb / s . as we stated earlier ,",
    "this bandwidth is sufficient to keep the standard pci interface busy , and it is not really the bottleneck , since , for many applications , the calculation on the host computer is more time - consuming .",
    "even so , in a few years the i / o performance will become a problem .",
    "additional problem is that host computers with faster pci interfaces ( pci64 and pci - x ) are now available .",
    "we can not take advantage of these faster interfaces with the current grape-6 design , because the i / o bandwidth of the processor chip is limited .",
    "we could have increased the i / o bandwidth of the chip without too much problem , by allowing the change in the ratio between the chip clock and board clock . with our current design ,",
    "this ratio is effectively fixed to 4 .",
    "even with the current chip design , we could have increased the communication bandwidth of the processor board , without increasing that of the chip , by letting multiple chips to transfer the data simultaneously .",
    "this possibility should have been considered , to increase the lifetime of the hardware .",
    "since the grape-6 system consists of exceptionally large number of arithmetic units , one might imagine that the primary source of the error is the calculation logic itself . in practice ,",
    "however , we have almost never seen any calculation error , once the power supply had become good enough . on the other hand",
    ", we saw quite a few errors in data transfer .",
    "we implemented ecc circuit for the memory interface of the processor chip , but only added parity detection circuit to i / o ports .",
    "we thought this is reasonable , since memory ports operate on 90 mhz clock and i / o ports on 22.5mhz .",
    "however , it turned out that memory parity error almost never occur , while parity error for i / o occurs rather frequently . since we do not exactly know the type of the error , it is",
    "not 100% clear whether the ecc capability would have helped or not .",
    "however , it is at least clear that more reliable data transmission would be better .",
    "a more serious problem with the reliability was very high defect rate for mass - produced processor boards and processor modules .",
    "practically all failures were due to unreliable soldering , and most of soldering problem turned out to be simply due to lack of skill of the manufacturer .",
    "this may be telling something about the present performance of japanese high - tech industry .",
    "even so , it is certainly true that to manufacture a rather small quantity of pcb board is difficult .",
    "we could have designed either more automated test procedures for boards ( with jtag standard ) or redundant connection . yet",
    "another possibility was to reduce the number of wires by using higher - frequency signals .",
    "as we discussed in section 5 , the use of the parallel host was inevitable .",
    "however , the use of the multicast network was not , at least in hindsight .",
    "we assumed that the price of high - end uniprocessor computers would not change greatly , and that the cost of high - bandwidth network adapters ( 1 gb / s or higher ) would remain high .",
    "in other words , we assumed that we could not afford to buy @xmath45 100 fast host computers and to connect all of them by a fast network .",
    "therefore , we designed our own network , which connects @xmath46 host computers to @xmath47 processor boards .",
    "this approach worked fine , as we have seen in the previous section .",
    "however , an alternative design , in which we connect each processor board to its own host computer , would have been much easier to develop .    in 1997 ,",
    "the fastest systems are risc - based unix workstations , with price higher than 20k usd . in 2003 , systems based on the intel x86 architecture offer the speed similar to that of the risc based systems with highest performance , for the cost of less than 2k usd . to illustrate this",
    ", we use specfp ( either 95 or 2000 ) numbers as representative of the performance . in 1997 the speed difference between risc systems and x86 systems were nearly a factor of three .",
    "this ratio had been almost constant during 1990s .",
    "the reason why the ratio started to shrink is simply that the rate of improvement of the performance of risc - based systems slowed down .",
    "thus , it would have been difficult to predict the present state in 1997 , or even in 1999 .",
    "in other words , even though now it is clear that network hardware of grape-6 is not necessary , until 2000 we had no other choice .",
    "another reason for the rather long development time ( aside from the problems with the power supply ) is the fact that we integrated effectively all functions of the system to the processor chip .",
    "it integrates the memory controller , the predictor pipeline , and all other control logics . except for the predictor pipeline , all these logics",
    "were implemented with fpgas on grape-4 .",
    "this integration simplified the design of the board , and fortunately , we have not made any serious mistake in the design of these parts .",
    "the integration of these complicated logics onto hardware required extremely careful design and test procedures which were time - consuming . with the present price of moderately large fpga chips ,",
    "all of these control logics could be implemented using fpgas , with very small additional cost .",
    "of course , such moderately large and inexpensive fpga was not there when we decided on the design of grape-6 chip .",
    "however , we could have predicted the direction of the evolution of fpga chips and estimated the price of them .",
    "since grape-6 is designed solely for the gravitational @xmath0-body problem , one might think there is not much of the range of applications .",
    "however , even within @xmath0-body simulations , there are many factors .",
    "the overall design of grape-6 is highly optimized to parallel execution of the direct force calculation with the individual timestep algorithm .",
    "this of course means it is not optimal for other applications , such as tree algorithm and sph calculations of self - gravitating fluid .    with the case of the tree algorithm",
    ", the performance is limited mainly by the speed of the host computer .",
    "so , in this case , adding more host computers would have greatly improved the performance .    in principle",
    "we could have improved the performance of the tree algorithm in several other ways .",
    "one obvious approach is to reduce the data to send . with tree algorithm",
    ", we would not use the predictor .",
    "moreover , we would not need the full 64-bit resolution for the position data .",
    "thus , we could have implemented some way to reduce the data to send for @xmath24-particles , if our memory controller was not implemented in hardware . actually , the memory controller of grape-6 has some programmability . however ,",
    "one `` feature '' of this memory controller prevented us from taking full advantage of this programmability to reduce the amount of the data transfer .    with an fpga implementation of the memory controller",
    ", we could implement other ways to further reduce the communication .",
    "for example , we could implement indirect addressing , so that we can send indices of @xmath24 particles instead of sending their physical data .    concerning the design of the pipeline",
    ", one thing which might have been useful for simulation of collisionless systems or composite @xmath0-body+sph systems is the ability to apply different softening length on different particles , in a symmetrized way .",
    "this can be achieved by calculating the softened distance as @xmath48 the pipeline will need one more addition , which is relatively inexpensive .    with sph ,",
    "the main problem is that the calculation of sph interactions itself can not be done on grape-6 .",
    "the progrape system@xcite , with the calculation pipeline fully implemented in fpga , could be used to perform the calculation of sph interaction .",
    "moderately large progrape system is currently under development .    with the logic design of the pipeline",
    ", we have noticed a few problems which we could not foresee .",
    "one is the length of the accumulator for the time derivative of the force . for the force and potential",
    ", we used 64-bit accumulators , but for the time derivative we used 32-bit accumulators .",
    "as far as the accuracy is concerned , this length is long enough .",
    "however , when we performed simulations with large number of particles , we realized that the overflow occurred rather frequently .",
    "the reason why the overflows occurs is that the magnitude of the time derivative of the force can change by a large factor in a single timestep .",
    "the large change occurs when the previous value happens to be almost zero .",
    "we could circumvent this problem with a combination of guess for the likely value of the time derivative of the force based on the value of force and timestep , but it is cumbersome to implement and expensive to evaluate . by increasing the accumulator length to , say , 40 bits , we could have almost completely eliminated the overflow .",
    "this overflow does not have any noticeable impact on performance .",
    "but the need to handle overflows made the interface program rather complicated .",
    "given that grape-6 is now completed and we already have the experience of running it for almost two years , it would be natural to put some thought on how its successor will look like . in this section ,",
    "we first discuss the change in the technologies , and then overview the design possibilities .",
    "compared to the technology used in grape-6 , what will used in the next grape system ( we call it ngs for short ) will be different in    \\(a ) semiconductor technology    \\(b ) development cost",
    "\\(c ) host i / o bus    first let us discuss the semiconductor technology .",
    "grape-6 used @xmath49 technology , while ngs would use , depending on the time to start , either @xmath50 or @xmath51 technology .",
    "since it seems we are not getting the budget too soon , we will probably use @xmath51 .",
    "this means we can pack about 8 times more transistors to the chip of the same size , and the switching speed will be about 3 times faster .",
    "thus , a single chip of the same size can offer 20 times more computing power . if the power supply voltage is reduced by the same factor , the power consumption would remain the same , but most likely the supply voltage would be somewhat higher , resulting in significant increase in the power consumption .",
    "to express in concrete numbers , a single chip would integrate around 50 pipeline processors , each with 60 arithmetic units operating on 300 mhz clock speed , with 1.2v supply voltage and power consumption of 20 w. the theoretical peak speed of the chip will be around 600 gflops .    compared to the projected speed of general - purpose microprocessors in , say , 2007 , this speed is quite attractive . in 2007",
    ", microprocessors will , at best , have the peak speed 10 times faster than they have now , or about 30 gflops",
    ". typical performance on real application would be around 10 gflops or less , for the power consumption of 100 w or more .",
    "a necessary consideration is how we connect the pipelines to memory .",
    "if we use the same memory system as we used for grape-6 , the total number of virtual pipelines per chip becomes 1,000 , which is too large for a simulation of any collisional system .",
    "as was the case with grape-6 , it is necessary to keep the number of @xmath4-particles calculated in parallel to be around 500 or less , for large systems with many chips .",
    "so the number of virtual pipelines per chip must be less than 200 , or ideally less than 100 . in other words , the memory bandwidth must be increased by at least a factor of five , to around 3.5 gb / s .",
    "this number , by itself , sounds relatively easy to achieve .",
    "it is the same as what was used with the first intel p4 processor ( 3.2gb / s ) , using two drdram channels each with 16-bit data width .",
    "intel p4 has been around for more than two years .",
    "now we can also use ddr 400 memory chips , which have 4 times more throughput than the ssram chips used in grape-6 .",
    "we could also use ddr srams .",
    "the choice of the memory interface has strong impact on the range of the applications .",
    "one major limitation of grape-6 was that , as was discussed in the previous section , its memory addressing scheme was limited only to the sequential access to a full set of predictor data .",
    "thus , it is not easy to use the tree or other sophisticated algorithms efficiently on grape-6 .",
    "one possibility to solve this problem is to implement the memory controller and other control logics in an fpga chip .",
    "the connection between the fpga chip and the pipeline chip must be quite fast , but this is relatively easy to achieve since the data transfer is unidirectional , from the fpga chip to the pipeline chip .",
    "the memory controller will be implemented in the fpga chip .",
    "thus , it will be possible to use different types of memory ( drdram , ddr dram / sram ) without any need to change the pipeline chip .",
    "as we discussed earlier , parallelism will be achieved by two - dimensional network of host computers .",
    "each of them will have a relatively small grape system .",
    "as an example , we consider a system with 256 host computers each with two grape cards .",
    "each card houses 4 processor chips with their own memory control units and memories .",
    "all of them can be packaged into single card of the pci form factor , though we need special care for the power supply .    for the interface to the host ,",
    "the easiest solution is to use pci - x , which is available now with the data transfer speed of up to 1 gb / s .",
    "pci - x gives us an order - of - magnitude increase in the communication speed , which roughly balances the increase in the performance of a factor of 20 .",
    "one problem is whether or not pci - x will be around 5 years from now .",
    "we need to predict the market trend , or develop a design that can use multiple interfaces .",
    "note that this factor - of-10 increase in the communication implies that the chip - to - chip communication must also be faster by the same factor .",
    "this is not easy , but since the physical size of the board will be much smaller , it would not be impossible to use fast clocks .    thus , the design of ngs seems to be simple , as far as we set the parallel execution of the individual timestep algorithm with direct summation as the primary design target .    the only , but quite serious",
    ", problem is that the predicted initial cost for the custom chip will be very high .",
    "the initial cost for a custom chip has been increasing quite steeply .",
    "roughly speaking , the initial cost has been proportional to the inverse of the design rule .",
    "thus , while the initial cost of the grape-4 chip was around 200k usd , that for grape-6 exceeded 1 m , and for ngs it will reach 3 m .",
    "even though this is `` small '' compared to the price of any massively - parallel supercomputer or even pc clusters , to get a grant of this size within the small community of theoretical astrophysics in japan is not easy .",
    "one rather fundamental question concerning the next grape system is whether the direct summation is really the best solution or not .",
    "mcmillan and aarseth ( ) have demonstrated that it is possible to implement a combination of the barnes - hut tree algorithm and the individual timestep algorithm that runs efficiently at least on a single - processor computer , and potentially also on shared - memory parallel computers .",
    "even when we require very high accuracy , the gain by tree algorithm is large for large @xmath0 .",
    "for example , the number of interactions per particle to achieve the relative force accuracy of @xmath52 is around 8,000 when quadrupole moment is used and around 2,000 when octupole moment is used , for number of particles around @xmath53 .",
    "thus , even if we assume the calculation of octupole costs a factor of 10 more than point - mass force , the calculation cost of the tree algorithm would be a factor of 50 less than that of the direct calculation .    even though the scaling is not as drastic as that of the tree algorithm , the ahmad - cohen scheme ( , also known as the neighbor scheme ) offers quite significant reduction of the calculation cost over the simple direct summation .",
    "the theoretical gain in the calculation cost is @xmath54 for the neighbor scheme@xcite .",
    "however , the actual speedup is nearly a factor of 10 , for only 1,000 particles .",
    "thus , for @xmath53 particles the gain can reach a factor of 50 .    for @xmath53 particles",
    "both the tree algorithm and neighbor scheme , at least theoretically , offer the reduction in the calculation cost of around a factor of 50 .",
    "this factor is certainly still smaller than the advantage of the grape hardware over general - purpose computers , since the difference in the price - performance ratio will exceed @xmath55 .",
    "however , if we can incorporate either of these sophisticated algorithms , even with significant loss in the hardware efficiency like a factor of 5 or even more , we can still achieve a very significant improvement in the overall speed .",
    "we are currently investigating several possible ways to achieve this goal .",
    "we would like to thank all of those who involved in the grape project .",
    "in particular , we thank daiichiro sugimoto for his continuous support to the project , atsushi kawai for helping the hardware design , yoko funato , eiichiro kokubo , simon portegies zwart , piet hut , steve mcmillan , makoto taiji , sverre aarseth , takayuki saito and many others for discussions on the experience with grape-4 and 5 .",
    "we are grateful to mary inaba and junichiro shitami for discussions on the network performance , and for providing us their software to measure the network performance .",
    "this work is supported by the research for the future program of japan society for the promotion of science ( jsps - rftf97p01102 ) ."
  ],
  "abstract_text": [
    "<S> in this paper , we describe the architecture and performance of the grape-6 system , a massively - parallel special - purpose computer for astrophysical @xmath0-body simulations . </S>",
    "<S> grape-6 is the successor of grape-4 , which was completed in 1995 and achieved the theoretical peak speed of 1.08 tflops . as was the case with grape-4 , </S>",
    "<S> the primary application of grape-6 is simulation of collisional systems , though it can be used for collisionless systems . </S>",
    "<S> the main differences between grape-4 and grape-6 are ( a ) the processor chip of grape-6 integrates 6 force - calculation pipelines , compared to one pipeline of grape-4 ( which needed 3 clock cycles to calculate one interaction ) , ( b ) the clock speed is increased from 32 to 90 mhz , and ( c ) the total number of processor chips is increased from 1728 to 2048 . </S>",
    "<S> these improvements resulted in the peak speed of 64 tflops . </S>",
    "<S> we also discuss the design of the successor of grape-6 . </S>"
  ]
}