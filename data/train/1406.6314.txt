{
  "article_text": [
    "clustering is the task that consists in grouping data into homogeneous clusters with the goal that intra - cluster data should be more similar than inter - cluster data .",
    "let @xmath4 be a set of @xmath5 points in @xmath6 .",
    "let @xmath7 be the @xmath0 _ non - empty _ clusters partitioning @xmath8 and denote by @xmath9 the set of @xmath0 cluster centers , the _ cluster prototypes_. @xmath0-means is one of the oldest and yet prevalent clustering technique that consists in minimizing :    @xmath10    where @xmath11 denotes the _ squared _ euclidean distance , and @xmath12 the index ( or label ) of the center of @xmath13 that is the closest nearest neighbor to @xmath14 ( say , in case of ties ,",
    "choose the minimum integer ) .",
    "finding an optimal clustering minimizing globally @xmath15 is np - hard when @xmath16 and @xmath17  @xcite , and polynomial when @xmath18 using dynamic programming  @xcite or when @xmath19 setting @xmath20 to the center of mass .",
    "note that there is an _ exponential number _ of optimal @xmath0-means clustering yielding the same optimal objective function : indeed , consider an equilateral triangle with @xmath21 and @xmath22 , we thus get @xmath23 equivalent optimal clustering related by rotational symmetries .",
    "then make @xmath24 far away separated copies so that @xmath25 and consider @xmath26 , we end up with @xmath27 optimal @xmath0-means clustering . minimizing the @xmath0-means function of eq .",
    "[ eq : kmeanscost ] is equivalent to minimizing the sum of intra - cluster squared distances or maximizing the sum of inter - cluster squared distances : @xmath28    many heuristics have been proposed to overcome the np - hardness of @xmath0-means .",
    "they can be classified into two main groups : the _ local search _ heuristics and the _ global _ heuristics that can be used to initialize the local heuristics .",
    "for example , the following four heuristics are classically implemented :    * forgy  @xcite ( random ) : draw uniformly at random @xmath0 points from @xmath8 to set the cluster prototypes @xmath13 inducing the partition . it can be proved that this best _ discrete _ @xmath0-means ( with @xmath29 ) yields a @xmath30-approximation factor compared to the ordinary @xmath0-means using a proof by contradiction based on the _ variance - bias decomposition _ : @xmath31 , where @xmath32 denotes the variance and @xmath33 the centroid .",
    "in fact , @xmath34 , the sum of intra - cluster variances ( and @xmath35 ) . *",
    "macqueen  @xcite ( online ) : from a given initialization of the @xmath0 centers defining singleton clusters ( say , @xmath36 for the @xmath0 clusters ) , we add a new point at a time to the cluster that contains the closest center , update that cluster centroid , and reiterate until convergence .",
    "this heuristic is also called the online or single - point @xmath0-means  @xcite . *",
    "lloyd  @xcite ( batched ) : from a given initialization of cluster prototypes , ( 1 ) assign points to their closest cluster , ( 2 ) relocate cluster centers to their cluster centroids , and reiterate those two steps until convergence .",
    "* hartigan  @xcite ( single - point relocation ) : from a given initialization , find how to move a point from a cluster to another cluster so that the @xmath0-means cost of eq .",
    "[ eq : kmeanscost ] strictly decreases and reiterate those single - point relocations until convergence is reached .",
    "note that a point maybe assigned to a cluster which center is _ not _ its closest center  @xcite .",
    "in general , a @xmath0-means clustering technique partitions the data into pairwise non - overlapping convex hulls @xmath37 : the _ voronoi partition_. a partition is said _",
    "stable _ when a local improvement of the heuristic can not improve its @xmath0-means score .",
    "let @xmath38 denotes the maximum number of stable @xmath0-means partitions obtained by forgy s , macqueen s , lloyd s and hartigan s schemes , respectively .",
    "we have @xmath39 and @xmath40 , where @xmath41 denotes the number of partitions of @xmath5 elements into @xmath0 non - empty subsets ( that is , the stirling numbers of the second kind ) and @xmath42 denotes the number of partitions with non - overlapping ( and non - empty ) convex hulls ( that is , the number of @xmath0-voronoi partitions ) .    hartigan s single - point relocation heuristic may improved lloyd s clustering but not the converse  @xcite . note that lloyd s heuristic may require an exponential number of iterations to converge  @xcite .",
    "it is an open question  @xcite to bound the maximum number of hartigan s iterations .",
    "on one hand , for those local heuristics performing pivots on voronoi partitions using primitives , initialization ( i.e. , the initial voronoi partition ) is crucial  @xcite to obtain a good clustering , and several restarts , denoted by @xmath43 , are performed in practice to choose the best clustering . in practice , forgy s initialization has been replaced by @xmath0-means++  @xcite which provides an _ expected _ @xmath44 competitive initialization .",
    "however , it was shown that there exits point sets ( even in 2d ) for which the probability to get such a good initialization is exponentially low  @xcite ( and thus requiring exponentially many initialization restarts to reach a good voronoi partition with high probability ) . on the other hand , the",
    "global @xmath0-means _",
    "@xcite builds incrementally the clustering by adding one seed at a time .",
    "given a current @xmath24-clustering it chooses the point in @xmath8 that minimizes the @xmath45-means objective function .",
    "thus initialization is limited to choosing the first point , and all points can be considered as this first starting point . however , global @xmath0-means requires more computation .    in this paper",
    ", we do not address the problem of choosing the most appropriate number , @xmath0 , of clusters : this model selection problem has been investigated in  @xcite .",
    "we also consider the squared euclidean distance although the results apply to any other bregman divergence  @xcite .    the paper is organized as follows :",
    "we investigate the blessing of empty - cluster exceptions in lloyd s heuristic in section  [ sec : ece ] , and of single - point - cluster exceptions in hartigan s scheme in section  [ sec : spe ] . in section",
    "[ sec : msc ] , we describe our novel heuristic merge - split - cluster @xmath0-means and report on its performances with respect to hartigan s heuristic . in section  [ sec : klmeans ] , we present a generalization of the @xmath0-means objective function where each point is associated to its @xmath3 closest clusters : the @xmath2-means clustering .",
    "we show how to directl convert or iteratively relax a sequence of @xmath2-means to a @xmath0-means and compare experimentally those solutions with a direct @xmath0-means .",
    "finally , section  [ sec : dis ] wrap ups the contributions and discusses further perspectives .",
    "lloyd s @xmath0-means  @xcite starts by initializing the seeds of the cluster centers @xmath9 , and then iterates by assigning the data to their closest cluster center with respect to the squared euclidean distance , and then relocates the cluster centers to their centroids .",
    "those batched assignment / relocation iterations are repeated until convergence is reached : the @xmath0-means cost monotonically decreases with guaranteed convergence after a finite number of iterations  @xcite .",
    "the complexity of lloyd s @xmath0-means is @xmath46 where @xmath24 denotes the number of iterations .",
    "it has been proved that lloyd s @xmath0-means performs a maximum number @xmath24 of iterations exponential  @xcite or polynomial in @xmath5 , @xmath1 and the spread is the ratio of the maximum point inter - distance over the minimum point inter - distance .",
    "] of the point set  @xcite .",
    "some 1d point set are reported to take @xmath47 iterations even for @xmath22 , see  @xcite .",
    "we first , report a lower bound on the number of lloyd s stable optima @xmath48 :    lloyd s @xmath0-means may have @xmath49 stable local minima .",
    "the proof follows from the gadget illustrated in figure  [ fig : kmeans ] .",
    "\\(a ) ( b ) ( c ) ( d ) + ( a ) ( b ) ( c ) ( d )    : get @xmath0 cluster centers @xmath50 by choosing cluster prototypes at random from @xmath8 ( e.g. , forgy or @xmath0-means++ ) @xmath51    the hartigan s heuristic  @xcite proceeds by relocating a single point between two clusters provided that the @xmath0-means cost function decreases .",
    "it can thus further decrease the @xmath0-means score when lloyd s batched algorithm is stuck into a local minimum ( but not the converse ) .",
    "recently , hartigan s heuristic  @xcite was suggested to replace lloyd s heuristic on the basis that hartigan s local minima is a subset of lloyd s optima ( theorem 2.2  of  @xcite ) .",
    "we argue that this is true only when no _ empty cluster exceptions _ ( eces ) are met by lloyd s iterations .",
    "figure  [ fig : emptyexception ] illustrates a toy data set where lloyd s @xmath0-means meets such an empty - cluster exception . in general at the end of the relocation stage , when points are assigned to their closest current centroids , we may have some empty clusters .    lloyd s batched @xmath0-means may produce @xmath52 empty cluster exceptions in a round .",
    "proof follows from figure  [ fig : emptyexception ] by creating @xmath53 far apart ( non - interacting ) copies of the gadget and setting @xmath54 .",
    "however , those empty - cluster exceptions are a _ blessing _ because we may add @xmath55 new seeds that will further decrease significantly the cost of @xmath0-means : this is a partial re - seeding .",
    "thus the extended lloyd s heuristic is : ( a ) assignment , ( b ) relocation , and ( c ) partial reseeding to keep exactly @xmath0 non - empty clusters for the next stage .",
    "we may use various heuristics for partially re - seeding like the incremental global @xmath56-means  @xcite starting from @xmath57 to @xmath58 , etc .    to evaluate the frequency",
    "at which those empty - cluster exceptions occur and their number @xmath55 , let us take the iris data set from the uci repository  @xcite : it consists of @xmath59 samples with @xmath60 features ( classified into @xmath61 labels ) that we first renormalize the data - set so that coordinates on each dimension have zero mean and unit standard deviation .",
    "let us run lloyd s @xmath0-means with ( forgy s ) random seed initialization ( with a maximum number of @xmath62 iterations ) for @xmath63 .",
    "we count the number of empty cluster exceptions and report their frequency in the graph of figure  [ fig : emptycluster ] .",
    "we observe that the larger the @xmath0 , and the more frequent the exceptions .",
    "this phenomenon was also noticed in  @xcite .",
    "furthermore , @xmath55 increases with the dimension @xmath1 too  @xcite .",
    "however , note that this is a tendency and the number of empty - cluster exceptions vary a lot from a data set to another one ( given an initialization heuristic ) .    [ cols=\"^,^ \" , ]     we can also perform a cascading conversion of @xmath2-means to @xmath0-means : once a local minimum is reached for @xmath2-means , we initialize a @xmath64 means by dropping for each point @xmath14 its farthest cluster , perform a lloyd s @xmath64-means , and we reiterate this scheme until we get a @xmath65-means : an ordinary @xmath0-means .",
    "let @xmath66-means denote this scheme .",
    "table  [ tab : kvskl ] ( right ) presents the performance comparisons of a regular lloyd s @xmath0-means with a lloyd s @xmath67-means for various values of @xmath3 with the initialization of both algorithms performed by the same seeding for fair comparisons .",
    "we have extended the classical lloyd s and hartigan s heuristics with partial re - seeding and proposed new local heuristics for @xmath0-means .",
    "we summarize our contributions as follows : first , we showed the _ blessing _ of empty - cluster events in lloyd s heuristic and of single - point - cluster events in hartigan s heuristic .",
    "these events happen increasingly when the number of cluster @xmath0 or the dimension @xmath1 increase , or when running those heuristics a given number of trials to choose the best solution .",
    "second , we proposed a novel _ merge - and - split - cluster @xmath0-means heuristic _ that improves over hartigan s heuristic that is currently the _ de facto _ method of choice  @xcite .",
    "we showed experimentally that this method brings better @xmath0-means result at the expense of computational cost .",
    "third , we generalized the @xmath0-means objective function to the @xmath2-means objective function and show how to directly convert or iteratively relax a @xmath2-means heuristic to a @xmath0-means avoiding potentially being trapped into too many local optima .",
    "@xmath2-means is yet another _ exploratory _ clustering technique for browsing the space of hard clustering partitions .",
    "for example , when @xmath0-means is trapped , we may consider a @xmath2-means to get out of the local minimum and then convert the @xmath2-means to a @xmath0-means to explore a new ( local ) minimum .",
    "a. bhattacharya , r. jaiswal , and n. ailon . a tight lower bound instance for @xmath0-means++ in constant dimension . in _ theory and applications of models of computation _ ,",
    "lncs 8402 , pages 722 , 2014 .",
    "s.  bubeck , m.  meila , and u.  von luxburg .",
    "how the initialization affects the stability of the @xmath0-means algorithm . , 16:436452 , 1 2012 .",
    "d. pelleg and a.  w. moore .",
    "@xmath68-means : extending @xmath0-means with efficient estimation of the number of clusters . in _ proceedings of the seventeenth international conference on machine learning _ , pages 727734 , 2000 ."
  ],
  "abstract_text": [
    "<S> the @xmath0-means clustering problem asks to partition the data into @xmath0 clusters so as to minimize the sum of the squared euclidean distances of the data points to their closest cluster center . </S>",
    "<S> finding the optimal @xmath0-means clustering of a @xmath1-dimensional data set is np - hard in general and many heuristics have been designed for minimizing monotonically the @xmath0-means objective function . </S>",
    "<S> those heuristics got trapped into local minima and thus heavily depend on the initial seeding of the cluster centers . </S>",
    "<S> the celebrated @xmath0-means++ algorithm is such a randomized seeding method which guarantees probabilistically a good initialization with respect to the global minimum . in this paper </S>",
    "<S> , we first show how to extend lloyd s batched relocation heuristic and hartigan s single - point relocation heuristic to take into account empty - cluster and single - point cluster events , respectively . </S>",
    "<S> those events tend to increasingly occur when @xmath0 or @xmath1 increases , or when performing several restarts of the @xmath0-means heuristic with a different seeding at each round in order to keep the best clustering in the lot . </S>",
    "<S> we show that those special events are a blessing because they allow to partially re - seed some cluster centers while further minimizing the @xmath0-means objective function . </S>",
    "<S> second , we describe a novel heuristic , called merge - and - split @xmath0-means , that consists in merging two clusters and splitting this merged cluster again with two new centers provided it improves the @xmath0-means objective . </S>",
    "<S> hartigan s heuristic can improve a lloyd s heuristic when it reaches a local minimum , and similarly this novel heuristic can improve hartigan s @xmath0-means when it has converged to a local minimum . </S>",
    "<S> we show empirically that this merge - and - split @xmath0-means improves over the hartigan s heuristic which is the _ de facto _ method of choice . </S>",
    "<S> finally , we propose the @xmath2-means objective that generalizes the @xmath0-means objective by associating the data points to their @xmath3 closest cluster centers , and show how to either directly convert or iteratively relax the @xmath2-means into a @xmath0-means in order to reach better local minima . </S>"
  ]
}