{
  "article_text": [
    "the problem of `` scaling up for high dimensional data and high speed data streams '' is among the `` ten challenging problems in data mining research''@xcite .",
    "this paper is devoted to estimating entropy of data streams .",
    "mining data streams@xcite in ( e.g. , ) 100 tb scale databases has become an important area of research , e.g. , @xcite , as network data can easily reach that scale@xcite .",
    "search engines are a typical source of data streams@xcite .",
    "consider the _ turnstile _",
    "stream model@xcite .",
    "the input stream @xmath14 , @xmath15 $ ] arriving sequentially describes the underlying signal @xmath16 , meaning @xmath17 = a_{t-1}[i_t ] + i_t,\\end{aligned}\\ ] ] where the increment @xmath18 can be either positive ( insertion ) or negative ( deletion ) . restricting @xmath19\\geq 0 $ ] results in the _ strict - turnstile",
    "_ model , which suffices for describing almost all natural phenomena .",
    "this study focuses on the _ strict - turnstile _ model and studies efficient algorithms for estimating the _ @xmath0th frequency moments _ of data streams @xmath20^\\alpha.\\end{aligned}\\ ] ] we are particularly interested in the case of @xmath21 , which is very important for estimating _",
    "shannon entropy_.      a very useful ( e.g. , in web and networks@xcite and neural comptutations@xcite ) summary statistic is the _",
    "shannon entropy _",
    "@xmath22}{f_{(1)}}\\log \\frac{a_t[i]}{f_{(1)}}.\\end{aligned}\\ ] ] various generalizations of the shannon entropy have been proposed .",
    "the rnyi entropy@xcite , denoted by @xmath23 , and the tsallis entropy@xcite , denoted by @xmath24 , are respectively defined as @xmath25^\\alpha}{\\left(\\sum_{i=1}^d a_t[i]\\right)^\\alpha } , \\hspace{0.5in}t_\\alpha = \\frac{1}{\\alpha -1 } \\left ( 1 - \\frac{f_{(\\alpha)}}{f_{(1)}^\\alpha}\\right).\\end{aligned}\\ ] ]    as @xmath21 , both rnyi entropy and tsallis entropy converge to shannon entropy : @xmath26 .",
    "thus , both rnyi entropy and tsallis entropy can be computed from the @xmath0th frequency moment ; and one can approximate shannon entropy from either @xmath23 or @xmath24 by letting @xmath27 .",
    "several studies@xcite ) used this idea to approximate shannon entropy , all of which relied on efficient algorithms for estimating the @xmath0th estimating frequency moments ( [ eqn_moment ] ) near @xmath28 .",
    "in fact , one can numerically verify that the @xmath0 values proposed in @xcite are extremely close to 1 , e.g. , @xmath29 .",
    "therefore , efficient algorithms for estimating @xmath30 near @xmath31 is critical for estimating shannon entropy .",
    "network traffic is a typical example of high - rate data streams .",
    "an effective and reliable measurement of network traffic in real - time is crucial for anomaly detection and network diagnosis ; and one such measurement metric is shannon entropy@xcite .",
    "the _ turnstile _ data stream model ( [ eqn_turnstile ] ) is naturally suitable for describing network traffic , especially when the goal is to characterize the statistical distribution of the traffic . in its empirical form ,",
    "a statistical distribution is described by histograms , @xmath19 $ ] , @xmath32 to @xmath33 .",
    "it is possible that @xmath34 ( ipv6 ) if one is interested in measuring the traffic streams of unique source or destination .    the distributed denial of service ( * ddos * )",
    "attack is a representative example of network anomalies .",
    "a ddos attack attempts to make computers unavailable to intended users , either by forcing users to reset the computers or by exhausting the resources of service - hosting sites .",
    "for example , hackers may maliciously saturate the victim machines by sending many external communication requests .",
    "ddos attacks typically target sites such as banks , credit card payment gateways , or military sites .    a ddos attack changes the statistical distribution of network traffic .",
    "therefore , a common practice to detect an attack is to monitor the network traffic using certain summary statics .",
    "since shannon entropy is a well - suited for characterizing a distribution , a popular detection method is to measure the time - history of entropy and alarm anomalies when the entropy becomes abnormal@xcite .",
    "entropy measurements do not have to be `` perfect '' for detecting attacks .",
    "it is however crucial that the algorithm should be computationally efficient at low memory cost , because the traffic data generated by large high - speed networks are enormous and transient ( e.g. , 1 gbits / second ) .",
    "algorithms should be real - time and one - pass , as the traffic data will not be stored@xcite .",
    "many algorithms have been proposed for `` sampling '' the traffic data and estimating entropy over data streams@xcite ,      the recent work@xcite was devoted to estimating the shannon entropy of msn search logs , to help answer some basic problems in web search , such as , _ how big is the web ? _    the search logs can be viewed as data streams , and @xcite analyzed several `` snapshots '' of a sample of msn search logs .",
    "the sample used in @xcite contained 10 million @xmath35query , url , ip@xmath36 triples ; each triple corresponded to a click from a particular ip address on a particular url for a particular query .",
    "@xcite drew their important conclusions on this ( hopefully ) representative sample .",
    "alternatively , one could apply data stream algorithms such as cc on the whole history of msn ( or other search engines ) .",
    "a workshop in nips03 was denoted to entropy estimation , owing to the wide - spread use of shannon entropy in neural computations@xcite .",
    "( http://www.menem.com/~ilya/pages/nips03 ) for example , one application of entropy is to study the underlying structure of spike trains .",
    "the problem of approximating @xmath30 has been very heavily studied in theoretical computer science and databases , since the pioneering work of @xcite , which studied @xmath37 , 2 , and @xmath38 .",
    "@xcite provided improved algorithms for @xmath39 .",
    "@xcite provided algorithms for @xmath40 to achieve the lower bounds proved by @xcite .",
    "@xcite suggested using even more space to trade for some speedup in the processing time .",
    "note that the first moment ( i.e. , the sum ) , @xmath41 , can be computed easily with a simple counter@xcite .",
    "this important property was recently somewhat captured by the method of _ compressed counting ( cc)_@xcite , which was based on the _ maximally - skewed stable random projections_. @xcite proved that , in the neighborhood of @xmath31 , the sample complexity is essentially @xmath2 , which was a large improvement over the well - known @xmath3 bound@xcite .",
    "this means the required sample size using cc should be @xmath2 in order to ensure that the estimated @xmath0th frequency moment will be within a @xmath42 factor of the truth , with high probability .",
    "+ the sample complexity bound of @xmath2 for cc is unsatisfactory , not just for theoretical reasons . from a practical point of view , @xmath43 can be too large to be practical , especially for entropy estimation .",
    "for example , one can numerically verify that the required @xmath44 values in @xcite for entropy estimation are very small .",
    "very recently , without providing any theoretical complexity bounds , @xcite proposed an empirically improved ( and quite sophisticated ) algorithm for cc . because the algorithm in @xcite is quite complex ,",
    "its theoretical analysis was difficult .",
    "this study proposes a very simple algorithm , which also allows us to analyze its sample complexity . the complexity is essentially @xmath45 , when @xmath46 .",
    "we consider the _ strict - turnstile _ model ( [ eqn_turnstile ] ) .",
    "conceptually , we multiply the data stream vector @xmath47 by a random projection matrix @xmath48 . the resultant vector @xmath49 is only of length @xmath6 .",
    "more specifically , the entries of the projected vector @xmath50 are @xmath51_j= \\sum_{i=1}^d r_{ij } a_t[i ] , \\ \\ j = 1 , 2 , ... , k\\end{aligned}\\ ] ]    @xmath52 s are random variables generated by @xmath53^{1/\\alpha } } \\left[\\frac{\\sin\\left ( v_{ij}\\delta\\right)}{w_{ij } } \\right]^{\\frac{\\delta}{\\alpha } } , \\ \\ \\ \\ \\delta = 1-\\alpha>0,\\end{aligned}\\ ] ] where @xmath54 ( i.i.d . ) and @xmath55 ( i.i.d . ) , an exponential distribution with mean 1 .    of course",
    ", in data stream computations , the matrix @xmath56 is never fully materialized .",
    "the standard procedure in data stream computations is to generate entries of @xmath56 on - demand@xcite . in other words ,",
    "whenever an stream element @xmath14 arrives , one updates entries of @xmath50 as @xmath57    the proposed algorithm is to take the _ sample minimum _ : @xmath58^\\alpha.\\end{aligned}\\ ] ] while this estimator is extremely simple , it has nice theoretical properties .",
    "[ thm_right_bound ]    as @xmath59 , for any fixed @xmath60 , @xmath61\\right)\\end{aligned}\\ ] ]    therefore , it suffices to let the sample size @xmath62 so that with probability at least @xmath8 , @xmath63 is within a @xmath9 factor of @xmath30 .",
    "the proof is deferred to section [ sec_proof_thm_right_bound ] , which will also demonstrate that the right tail bound ( [ eqn_right_bound ] ) can be slightly improved by essentially removing the @xmath64 term in ( [ eqn_right_bound ] ) .    to help verify the results in theorem [ thm_right_bound ] , figure [ fig_right_bound ] plots the right tail bounds ( [ eqn_right_bound ] ) for @xmath65 ( @xmath66 ) and @xmath67 ( @xmath68 only ) , together with the simulated tail probabilities .",
    "we can see that the tail probabilities decrease very rapidly .",
    "in fact , it is even difficult to simulate the tail probabilities if @xmath69 or @xmath70 .",
    "theorem [ thm_right_bound ] indicates that required sample size @xmath6 can be very small .",
    "for example , if we let @xmath10 , @xmath11 , and @xmath12 , then according to ( [ eqn_complexity ] ) , the required sample size is merely @xmath13    note that theorem [ thm_right_bound ] is just for the sample complexity . to obtain the space complexity , we must consider an multiplicative factor of @xmath71 .",
    "in addition , we must store @xmath52 with a sufficient accuracy . in section [ sec_preproofs ] , lemma [ lem_z_order ]",
    "shows that @xmath72 , which can be represented using @xmath73 bits .",
    "therefore , the required storage space would be the sample complexity ( [ eqn_complexity ] ) multiplied by a factor of @xmath74 + @xmath73 .",
    "+    theorem [ thm_left_bound ] presents the left tail bound .",
    "[ thm_left_bound ] for any @xmath75 , @xmath76 , and @xmath77 , @xmath78    the proof is deferred to section [ sec_proof_thm_left_bound ] .",
    "the left bound ( [ eqn_left_bound ] ) approaches zero extremely fast .",
    "for example , when @xmath67 and @xmath79 , @xmath80 ; and hence @xmath6 does not really matter for the left bound . in a sense ,",
    "the left bound will be used merely for the sanity check and one can determine the sample size mainly from the right bound in theorem [ thm_right_bound ] .",
    "we start with reviewing maximally - skewed stable distributions , because our formulation ( [ eqn_r_ij ] ) somewhat differs from the standard formulation .",
    "the standard procedure for sampling from skewed stable distributions is based on the chambers - mallows - stuck method@xcite . to generate a sample from @xmath81 ,",
    "i.e. , @xmath0-stable , maximally - skewed ( @xmath82 ) , with unit scale , one first generates an exponential random variable with mean 1 , @xmath83 , and a uniform random variable @xmath84 , then , @xmath85^{1/\\alpha } } \\left[\\frac{\\cos\\left ( u - \\alpha(u + \\rho)\\right)}{w } \\right]^{\\frac{1-\\alpha}{\\alpha } } \\sim s(\\alpha,\\beta=1,1),\\end{aligned}\\ ] ] where @xmath86 when @xmath76 and @xmath87 when @xmath88 .",
    "+ for convenience , we will use @xmath89    in this study , we will only consider   @xmath90 , i.e , @xmath91 .",
    "after simplification , we obtain @xmath92^{1/\\alpha } } \\left[\\frac{\\sin\\left ( v\\delta\\right)}{w } \\right]^{\\frac{\\delta}{\\alpha}},\\end{aligned}\\ ] ] where @xmath93 . this explains ( [ eqn_r_ij ] ) .",
    "+ lemma [ lem_z_order ] shows @xmath94 , which can be accurately represented using @xmath95 bits .",
    "the proof is omitted since it is straightforward .",
    "[ lem_z_order ] for any given @xmath96 , and @xmath97 , as @xmath98 , @xmath99      let @xmath100 , where entries are @xmath56 are i.i.d .",
    "samples of @xmath101",
    ". then by properties of stable distributions , entries of @xmath50 are @xmath102_j = \\sum_{i=1}^d r_{i , j}a_t[i ] \\sim s \\left(\\alpha,\\beta=1 , \\cos\\left(\\frac{\\pi}{2}\\alpha\\right)f_{(\\alpha)}\\right),\\end{aligned}\\ ] ] where @xmath103^\\alpha$ ] as defined in ( [ eqn_moment ] ) .",
    "the proposed estimator of @xmath30 is based on the _ sample minimum _ : @xmath104^\\alpha\\end{aligned}\\ ] ]      [ lem_cdf ] suppose a random variable @xmath105 , then the cumulative density function is @xmath106^{\\alpha/\\delta } } { t^{\\alpha/\\delta } \\left[\\sin \\theta \\right]^{1/\\delta } } \\sin\\left ( \\theta\\delta\\right)\\right ) d\\theta , \\hspace{0.5 in } ( \\delta = 1-\\alpha).\\end{aligned}\\ ] ]    * proof : * @xmath107^{1/\\alpha } } \\left[\\frac{\\sin\\left ( v\\delta\\right)}{w } \\right]^{\\frac{\\delta}{\\alpha } }   \\geq t \\right)\\\\\\notag = & \\mathbf{pr}\\left (   w \\leq \\frac {   \\left[\\sin\\left(\\alpha v \\right)\\right]^{\\alpha/\\delta } } { t^{\\alpha/\\delta } \\left[\\sin v \\right]^{1/\\delta } } \\sin\\left ( v\\delta\\right)\\right)\\\\\\notag = & \\text{e}\\left(\\mathbf{pr}\\left ( \\left . w \\leq \\frac {   \\left[\\sin\\left(\\alpha v \\right)\\right]^{\\alpha/\\delta } } { t^{\\alpha/\\delta } \\left[\\sin v \\right]^{1/\\delta } } \\sin\\left ( v\\delta\\right)\\right|v\\right)\\right)\\\\\\notag = & 1-\\text{e}\\left(\\exp\\left(- \\frac {   \\left[\\sin\\left(\\alpha v \\right)\\right]^{\\alpha/\\delta } } { t^{\\alpha/\\delta } \\left[\\sin v \\right]^{1/\\delta } } \\sin\\left ( v\\delta\\right)\\right)\\right)\\\\\\notag = & 1-\\frac{1}{\\pi}\\int_0^\\pi   \\exp\\left(- \\frac {   \\left[\\sin\\left(\\alpha \\theta \\right)\\right]^{\\alpha/\\delta } } { t^{\\alpha/\\delta } \\left[\\sin \\theta \\right]^{1/\\delta } } \\sin\\left ( \\theta\\delta\\right)\\right ) d\\theta\\box\\end{aligned}\\ ] ]    for @xmath108 , let @xmath109^{\\alpha/\\delta } } { \\left[\\sin \\theta \\right]^{1/\\delta } } \\sin\\left ( \\theta\\delta\\right),\\end{aligned}\\ ] ]    lemma [ lem_g ] includes some properties of @xmath110 , which will be useful for proving our main results in theorem [ thm_right_bound ] and theorem [ thm_left_bound ] .",
    "[ lem_g ] assume @xmath111 , then @xmath110 is monotonically increasing in @xmath112 , with @xmath113 moreover , @xmath110 is a convex function of @xmath114 .",
    "we first prove the left bound in theorem [ thm_left_bound ] .",
    "recall the _ sample minimum _ estimator is @xmath104^\\alpha , \\hspace{0.2 in } x_j \\sim s\\left(\\alpha<1 , \\beta=1 , \\cos\\left(\\frac{\\pi}{2}\\alpha\\right)f_{(\\alpha)}\\right).\\end{aligned}\\ ] ]    using the density function provided in lemma [ lem_cdf ] and properties of @xmath115^{\\alpha/\\delta } } { \\left[\\sin \\theta \\right]^{1/\\delta } } \\sin\\left ( \\theta\\delta\\right)$ ] proved in lemma [ lem_g ] , we obtain @xmath116^{\\alpha/\\delta } } { ( 1-\\epsilon)^{1/\\delta } \\left[\\sin \\theta \\right]^{1/\\delta } } \\sin\\left ( \\theta\\delta\\right)\\right ) d\\theta\\\\\\notag \\leq&k\\frac{1}{\\pi}\\int_0^\\pi \\exp\\left(- \\frac{\\lim_{\\theta\\rightarrow0+}g(\\theta,\\delta )   } { ( 1-\\epsilon)^{1/\\delta}}\\right )   d\\theta\\\\\\notag = & k\\frac{1}{\\pi}\\int_0^\\pi \\exp\\left(- \\frac{\\delta\\alpha^{1/\\delta-1 }   } { ( 1-\\epsilon)^{1/\\delta}}\\right )   d\\theta\\\\\\notag = & k\\exp\\left(-\\frac{\\delta\\alpha^{1/\\delta-1}}{(1-\\epsilon)^{1/\\delta}}\\right).\\end{aligned}\\ ] ]      using the density function provided in lemma [ lem_cdf ] , we can obtain @xmath117^{\\alpha/\\delta } } { ( 1+\\epsilon)^{1/\\delta } \\left[\\sin \\theta \\right]^{1/\\delta } } \\sin\\left ( \\theta\\delta\\right)\\right ) d\\theta \\right]^k\\\\\\notag = & \\exp\\left(k \\log",
    "\\left[1-\\frac{1}{\\pi}\\int_0^\\pi \\exp\\left(- \\frac { g(\\theta;\\delta ) } { ( 1+\\epsilon)^{1/\\delta } } \\right ) d\\theta \\right]\\right)\\end{aligned}\\ ] ]    we proceed the proof as follows :    1 .   using the fact that @xmath118 , we obtain @xmath119\\right)\\end{aligned}\\ ] ] where @xmath120 is the solution to @xmath121 2 .   we prove a more general result to solve for @xmath122 we show the asymptotic expression for @xmath123 is , as @xmath124 , @xmath125 3 .",
    "we approximate the integral @xmath126 by the trapezoid rule .",
    "because @xmath127 is a convex function of @xmath114 as proved in lemma [ lem_g ] , we know this approximation still leads to an upper bound we are after .",
    "4 .   to apply the trapezoid rule",
    ", it turns out that it suffices to use only one interior point , @xmath128 , in addition to the two end points , @xmath129 and @xmath130 .",
    "@xmath131 is the solution to @xmath132 .",
    "we can slightly improve the bound by using more points when applying the trapezoid rule , for example , @xmath133 , in addition to @xmath120 , @xmath131 , and @xmath134 .",
    "we defer the proof of ( [ eqn_theta_gamma ] ) to appendix [ app_proof_theta_gamma ] .",
    "assuming ( [ eqn_theta_gamma ] ) holds , we have @xmath135\\right)\\\\\\notag \\leq&\\exp\\left(k \\log \\left[1-\\frac{1}{\\pi}\\int_0^{\\theta_0 } 1- \\frac { g(\\theta;\\delta ) } { ( 1+\\epsilon)^{1/\\delta } }   d\\theta \\right]\\right)\\\\\\notag \\leq&\\exp\\left(k \\log \\left[1-\\frac{1}{\\pi}\\left[\\theta_1 - \\frac{1}{2}\\theta_1\\delta + \\frac{1}{2}(1-\\delta)(\\theta_0-\\theta_1)\\right]\\right]\\right)\\\\\\notag = & \\exp\\left(k \\log \\left[1-\\frac{1}{2\\pi}\\left[\\theta_0+\\theta_1-\\delta\\theta_0\\right]\\right]\\right)\\end{aligned}\\ ] ]    @xmath136\\\\\\notag = & \\frac{1}{1+\\log\\delta + \\frac{1}{\\delta}\\log(1+\\epsilon)+ \\log\\left(\\frac{1}{\\delta\\log\\delta+\\log(1+\\epsilon)}+1\\right)+o\\left(\\delta\\right)}\\\\\\notag + & \\frac{1}{1",
    "+ \\frac{1}{\\delta}\\log(1+\\epsilon)+ \\log\\left(\\frac{1}{\\log(1+\\epsilon)}+1\\right)+o\\left(\\delta\\right)}\\\\\\notag + & \\delta\\frac{\\frac{1}{\\delta}\\log(1+\\epsilon)+ \\log\\left(\\frac{1}{\\log(1+\\epsilon)}+1\\right)+o\\left(\\delta\\right ) } { 1+\\frac{1}{\\delta}\\log(1+\\epsilon)+ \\log\\left(\\frac{1}{\\log(1+\\epsilon)}+1\\right)+o\\left(\\delta\\right)}\\\\\\notag = & \\frac{\\delta}{\\delta+\\delta\\log\\delta + \\log(1+\\epsilon)+ \\delta\\log\\left(\\frac{1}{\\delta\\log\\delta+\\log(1+\\epsilon)}+1 \\right)+o\\left(\\delta^2\\right)}\\\\\\notag + & \\frac{\\delta}{\\delta+ \\log(1+\\epsilon)+ \\delta\\log\\left(\\frac{1}{\\log(1+\\epsilon)}+1\\right)+o\\left(\\delta^2\\right)}+\\delta+o\\left(\\delta^2\\right)\\\\\\notag = & \\delta+\\frac{\\delta}{\\log(1+\\epsilon)}+\\frac{\\delta}{\\delta\\log\\delta+\\log(1+\\epsilon)}+o\\left(\\delta^2\\right)\\end{aligned}\\ ] ]    therefore , if we require @xmath137\\right)\\\\\\notag   \\leq & \\delta,\\end{aligned}\\ ] ] we obtain our main result , the sample complexity bound , @xmath138    it turns out , the term @xmath64 can be almost removed , by using one additional interior point when applying the trapezoid rule .",
    "note that @xmath139 is almost as small as @xmath140 , but we do not want to simply ignore this term . + using two interior points , @xmath131 and @xmath141 , where @xmath142 , we obtain    @xmath143^{\\alpha/\\delta } } { ( 1+\\epsilon)^{1/\\delta } \\left[\\sin \\theta \\right]^{1/\\delta } } \\sin\\left ( \\theta\\delta\\right)\\right ) d\\theta \\right]\\right)\\\\\\notag \\leq&\\exp\\left(k \\log \\left[1-\\frac{1}{\\pi}\\int_0^{\\theta_0 } 1- \\frac {   \\left[\\sin\\left(\\alpha \\theta \\right)\\right]^{\\alpha/\\delta } } { ( 1+\\epsilon)^{1/\\delta } \\left[\\sin \\theta \\right]^{1/\\delta } } \\sin\\left ( \\theta\\delta\\right ) d\\theta \\right]\\right)\\\\\\notag \\leq&\\exp\\left(k",
    "\\log \\left[1-\\frac{1}{\\pi}\\left[\\theta_1 - \\frac{1}{2}\\theta_1\\delta + \\frac{1}{2}\\left(\\theta_t-\\theta_1\\right)(1-\\delta+1-\\delta^t)+\\frac{1}{2}(1-\\delta^t)(\\theta_0-\\theta_t)\\right]\\right]\\right)\\\\\\notag = & \\exp\\left(k \\log \\left[1-\\frac{1}{2\\pi}\\left[\\theta_0+\\theta_t-\\delta\\theta_t-\\delta^t\\theta_0+\\delta^t\\theta_1\\right]\\right]\\right)\\\\\\notag\\end{aligned}\\ ] ]    @xmath144\\\\\\notag = & \\frac{1}{1+t\\log\\delta + \\frac{1}{\\delta}\\log(1+\\epsilon)+ \\log\\left(\\frac{1}{t\\delta\\log\\delta+\\log(1+\\epsilon)}+1\\right)+o\\left(\\delta\\right)}\\\\\\notag + & \\frac{1}{1 + \\frac{1}{\\delta}\\log(1+\\epsilon)+ \\log\\left(\\frac{1}{\\log(1+\\epsilon)}+1\\right)+o\\left(\\delta\\right)}\\\\\\notag + & \\delta\\frac{t\\log\\delta+\\frac{1}{\\delta}\\log(1+\\epsilon)+ \\log\\left(\\frac{1}{t\\delta\\log\\delta+\\log(1+\\epsilon)}+1\\right)+o\\left(\\delta\\right ) } { 1+t\\log\\delta+\\frac{1}{\\delta}\\log(1+\\epsilon)+ \\log\\left(\\frac{1}{t\\log\\delta+\\log(1+\\epsilon)}+1\\right)+o\\left(\\delta\\right)}\\\\\\notag + & \\delta^t\\frac{\\frac{1}{\\delta}\\log(1+\\epsilon)+ \\log\\left(\\frac{1}{\\log(1+\\epsilon)}+1\\right)+o\\left(\\delta\\right ) } { 1+\\frac{1}{\\delta}\\log(1+\\epsilon)+ \\log\\left(\\frac{1}{\\log(1+\\epsilon)}+1\\right)+o\\left(\\delta\\right)}\\\\\\notag -&\\delta^t\\frac{\\log\\delta+\\frac{1}{\\delta}\\log(1+\\epsilon)+ \\log\\left(\\frac{1}{\\delta\\log\\delta+\\log(1+\\epsilon)}+1\\right)+o\\left(\\delta\\right ) } { 1+\\log\\delta+\\frac{1}{\\delta}\\log(1+\\epsilon)+ \\log\\left(\\frac{1}{\\delta\\log\\delta+\\log(1+\\epsilon)}+1\\right)+o\\left(\\delta\\right)}\\\\\\notag = & \\delta+\\frac{\\delta}{\\log(1+\\epsilon)}+\\frac{\\delta}{t\\delta\\log\\delta+\\log(1+\\epsilon)}+o\\left(\\delta^2\\right)\\end{aligned}\\ ] ]    note that , if we choose @xmath145 to be too small ( too close to 0 ) , then @xmath146 will be larger than @xmath147 and can not be ignored .",
    "therefore , although we can minimize the impact of the term @xmath64 to a very large extent , it can not be entirely removed , theoretically speaking .",
    "real - world data are often dynamic and can be modeled as data streams .",
    "measuring summary statistics of data streams such as the shannon entropy has become an important task in many applications , for example , detecting anomaly events in large - scale networks .",
    "one line of active research is to approximate the shannon entropy using the @xmath0th frequency moments of the stream with @xmath0 extremely close to 1 .    efficiently approximating the @xmath0th frequency moments of data streams",
    "has been very heavily studied in theoretical computer science and databases . when @xmath148 , it is well - known that efficient @xmath3-space algorithms exist , for example , _ symmetric stable random projections_@xcite , which however are impractical for estimating shannon entropy using @xmath0 extremely close to 1 .",
    "recently , @xcite provided an algorithm to achieve the @xmath2 bound in the neighborhood of @xmath31 , based on the idea of _ maximally - skewed stable random projections _",
    "( also called _ compressed counting ( cc ) _ ) .",
    "the @xmath2 bound , although a very large improvement over the previous @xmath3 bound , is still impractical .",
    "this study proposes a new algorithm for cc based on the _ sample minimum _ , which is simple , practical , and still has very nice theoretical properties . using this algorithm ,",
    "we have proved that the sample complexity is essentially @xmath149 as @xmath150 .",
    "this is a very large improvement over the previous @xmath151 bound and may impact the practice .",
    "for @xmath108 , let @xmath109^{\\alpha/\\delta } } { \\left[\\sin \\theta \\right]^{1/\\delta } } \\sin\\left ( \\theta\\delta\\right).\\end{aligned}\\ ] ] it is easy to show that , as @xmath152 , @xmath153^{\\alpha/\\delta } } { \\left[\\sin \\theta \\right]^{1/\\delta } } \\sin\\left ( \\theta\\delta\\right)\\\\\\notag = & \\lim_{\\theta\\rightarrow 0 + } \\left(\\frac{\\sin\\left(\\alpha \\theta \\right ) } { \\sin \\theta } \\right)^{1/\\delta } \\frac{\\sin\\left ( \\theta\\delta\\right)}{\\sin\\left(\\alpha \\theta \\right)}\\\\\\notag = & \\alpha^{1/\\delta}\\frac{\\delta}{\\alpha } = \\delta\\alpha^{1/\\delta-1}.\\end{aligned}\\ ] ]    the proof of the monotonicity of @xmath127 is omitted , because it is can be inferred from the proof of the convexity .",
    "to show @xmath110 is a convex function @xmath114 , it suffices to show it is log - convex . since @xmath154^{\\alpha/\\delta}}{[\\sin(\\theta)]^{1/\\delta } } = \\frac{\\sin(\\theta\\delta)}{\\sin(\\alpha\\theta)}\\left[\\frac{\\sin(\\alpha\\theta)}{\\sin(\\theta)}\\right]^{1/\\delta}\\end{aligned}\\ ] ] it suffices to show that both @xmath155 and @xmath156^{1/\\delta}$ ] are log - convex .",
    "@xmath157    @xmath158    @xmath159    therefore , @xmath160 and @xmath155 is convex .",
    "@xmath161    @xmath162    @xmath163    therefore , we have proved the convexity of @xmath164 .",
    "@xmath123 is the solution to @xmath165^{\\alpha/\\delta } } { ( 1+\\epsilon)^{1/\\delta } \\left[\\sin \\theta \\right]^{1/\\delta } } \\sin\\left ( \\theta\\delta\\right),\\end{aligned}\\ ] ] equivalently , @xmath166",
    "@xmath167 @xmath169 we apply taylor expansions , @xmath170 to obtain @xmath171 where we have replaced @xmath172 with @xmath147 ( as @xmath124 ) .",
    "this fact can be later verified .        at this point , we have reached an equilibrium .",
    "therefore , we know @xmath181 note that @xmath182 thus , assuming @xmath183 ( which can be verified ) , we obtain @xmath184 to complete the proof , we must verify @xmath183 and @xmath185 . indeed , @xmath186",
    "@xmath187                            laura feinstein , dan schnackenberg , ravindra balupari , and darrell kindred .",
    "statistical approaches to attack detection and response . in _",
    "darpa information survivability conference and exposition _ , pages 303314 , 2003 .",
    "kuai xu , zhi - li zhang , and supratik bhattacharyya .",
    "profiling internet backbone traffic : behavior models and applications . in _",
    "sigcomm 05 : proceedings of the 2005 conference on applications , technologies , architectures , and protocols for computer communications _ , pages 169180 , 2005 ."
  ],
  "abstract_text": [
    "<S> compressed counting ( cc)@xcite , based on _ maximally skewed stable random projections _ , was recently proposed for estimating the @xmath0th frequency moments of data streams . </S>",
    "<S> when @xmath1 , @xcite provided an algorithm based on the _ geometric mean _ estimator and proved that the sample complexity was essentially @xmath2 , which was a large improvement compared to the previously known @xmath3 bound . </S>",
    "<S> the case @xmath4 is extremely useful for estimating shannon entropy of data streams .    in this study , we provide a very simple algorithm based on the _ sample minimum _ estimator and prove that , when @xmath5 , it suffices to let the sample size @xmath6 be @xmath7 so that , with probability at least @xmath8 , the estimated @xmath0th frequency moments will be within a @xmath9 factor of the truth . for example , when @xmath10 , @xmath11 , and @xmath12 , the required sample size is merely @xmath13 . </S>"
  ]
}