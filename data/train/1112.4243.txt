{
  "article_text": [
    "audio feature extraction and classification methods have been studied by many researchers over the years  @xcite . in general ,",
    "audio classification can be performed in two steps , which involves reducing the audio sound to a small set of parameters using various feature extraction techniques and classifying or categorizing over these parameters .",
    "feature commonly exploited for audio classification can be roughly classified into time domain features , transformation domain features , time - transformation domain features or their combinations  @xcite .",
    "many of those features are common to audio signal processing and speech recognition and have many successful performances in various applications .",
    "however almost all these features are based on short time duration and in vector form ( it is easy to handle but sometimes not proper ) , although it is believed that long time duration ( seconds ) help a lot in decision making . in this work",
    "we will build robust features on a long time duration in matrix form which is the most natural way using long time audio information .    in order to map or smooth the audio segment into a robust matrix space",
    ", we introduce the trace norm regularization technique to audio signal processing .",
    "the trace norm regularization is a principled approach to learn low - rank matrices through convex optimization problems  @xcite .",
    "these similar problems arise in many machine learning tasks such as matrix completion  @xcite , multi - task learning  @xcite , robust principle component antilysis ( robust pca )  @xcite , and matrix classification  @xcite . in this paper ,",
    "robust pca is used to extract matrix representation features for audio segments . unlike traditional frame based vector features , these matrix features are extracted based on sequences of audio frames . it is believed that in a short duration the signals are contributed by a few factors . thus it is natural to approximate the frame sequence by low - rank features using robust pca which assumes that the observed matrices are combinations of some low - rank matrices and some corruption noise matrices .",
    "having extracted descriptive features , various machine learning methods are used to provide a final classification of the audio events such as rule - based approaches , gaussian mixture models , support vector machines , bayesian networks , and etc .",
    "@xcite . in most previous work ,",
    "these two steps for audio classification are always separate and independent . in this work",
    ", we can learn the classifiers in solving similar optimization problems using trace norm regularization .",
    "after extraction of the robust low - rank matrix feature , the regularization framework based matrix classification approach proposed by tomioka and aihara in  @xcite is used to predict the label .",
    "the problem of matrix classification ( mc ) with spectral regularization was first proposed by tomioka and aihara in  @xcite .",
    "the goal of the problem is to infer the weight matrix and bias under low trace norm constraints and low deviation of the empirical statistics from their predictions .",
    "the trace norm was use to measure the complexity of the weight matrix of the linear classifier for matrix classifications .",
    "this kind of inference task belongs to the more general problem of learning low - rank matrix through convex optimization . for the matrix rank minimization",
    "is np - hard in general due to the combinatorial nature of the rank function , a commonly - used convex relaxation of the rank function is the trace norm ( nuclear norm )  @xcite , defined as the sum of the singular values of the matrix .",
    "recent related researches are not focused on matrix classification directly , but rather on general trace norm minimization problem  @xcite .",
    "these general algorithm can be adapted to matrix classification suitably . in these methods ,",
    "most are iterative _ batch _ procedures  @xcite , accessing the whole training set at each iteration in order to minimize a weighted sum of a cost function and the trace norm .",
    "this kind of learning procedure can not deal with huge size training set for the data probably can not be loaded into memory simultaneously .",
    "furthermore it can not be started until the training data are prepared , hence can not effectively deal with training data appear in sequence , such as audio and video processing .    to address these problems , we propose an _ online _ approach that processes the training samples , one at a time , or in mini - batches to learn the weight matrix and the bias for matrix classification .",
    "we transform the general batch - mode accelerated proximal gradient ( apg )  @xcite method for trace norm minimization to the online learning framework . in this online learning framework , a slight improvement over the exact apg leads an inexact apg ( iapg ) method , which needs less computation in one iteration than using exact apg . in addition , as a special case of general convex optimization problem , we derived the closed - form of the lipschitz constant , hence the step size estimation  @xcite of the general apg method was omitted in our approach .",
    "our main contributions in this work can be summarized as follows :    1 .   to our best knowledge ,",
    "we are the first to introduce low - rank constraints in audio and speech signal processing , and the results show that these constrains make the systems more robust to noise , especially to large corruptions .",
    "we propose online learning algorithms to learn the trace norm minimization based matrix classifier , which make the approaches work in real applications .",
    "the paper is organized as follows : section  [ sec : matrixfeature ] presents the extraction of matrix representation feature .",
    "section  [ sec : matrixclassifyaed ] presents the matrix classification problem solving via the general apg method and the proposed audio event detection with matrix classification .",
    "the proposed online methods with exact and inexact apg for weight and bias learning are introduced in section  [ sec : onlinelearning ] .",
    "section  [ sec : experimental ] is devoted to experimental results to demonstrate the characteristics and merits of the proposed algorithm .",
    "finally we give some concluding remarks in section  [ sec : conclusions ] .",
    "over the past decades , a lot work has been done on audio and speech features for audio and speech processing  @xcite . due to convenience and the short - time stationary assumption , these features are mainly in vector form based on frames ,",
    "although it is believed that features based on longer duration help a lot in decision making . in order to build long term features ,",
    "the consecutive frame signals are made together as rows , then the audio segments become matrices .",
    "generally , it is assumed and believed that the consecutive frame signals are influenced by a few factors , thus these matrices are combinations of low - rank components and noise . hence it is natural to approximate these matrices by low - rank matrices . in this work ,",
    "transformations of these approximate low - rank matrices are used as features .",
    "given an observed data matrix @xmath1 , where @xmath2 is the number of frames and @xmath3 represents the number of samples in a frame , it is assumed that it can be decomposed as @xmath4 where @xmath5 is the low - rank component and @xmath6 is the error or noise matrix .",
    "the purpose here is to recover the low - rank component without knowing the rank of it . for this problem",
    ", pca is a suitable approach that it can find the low - dimensional approximating subspace by forming a low - rank approximation to the data matrix  @xcite .",
    "however , it breaks down under large corruption , even if that corruption affects only a very few of the observation which is often encountered in practice  @xcite . to solve this problem ,",
    "the following convex optimization formulation is proposed @xmath7 where @xmath8 denotes the trace norm of a matrix which is defined as the sum of the singular values , @xmath9 denotes the sum of the absolute values of matrix elements , and @xmath10 is a positive regularization parameter .",
    "this optimization is refereed to as _ robust pca _ in  @xcite for its ability to exactly recover underlying low - rank structure in data even in the presence of large errors or outliers . in order to solve equation  ( [ eq : robustpcaformulation ] ) ,",
    "several algorithms have been proposed , among which the augmented lagrange multiplier method is the most efficient and accurate at present  @xcite . in our work , this robust pca method is employed for the low - rank matrix extraction .    in order to apply the augmented lagrange multiplier ( alm ) to the robust pca problem , lin et .",
    "@xcite identify the problem as @xmath11 and the lagrangian function becomes @xmath12 two alm algorithms to solve the above formulation are proposed in  @xcite .",
    "considering a balance between processing speed and accuracy , the robust pca via the inexact alm method is chosen in our work . thus the matrix representation feature extraction process based on this approach is summarized in algorithm  [ algo : prcaviaialm ] . in algorithm",
    "[ algo : prcaviaialm ] , @xmath13 is defined as the larger one of @xmath14 and @xmath15 , where @xmath16 is the maximum absolute value of the matrix elements .",
    "the @xmath17 $ ] is the soft - thresholding operator introduced in  @xcite .",
    "[ fig : rpcatoaudioseg ] shows the recovered low - rank matrices via applying robust pca to the matrix form of a typical laugh sound effect audio segment with or without corruptions .",
    "in which , the regularization parameter is fixed as 1 .",
    "it can be seen that robust pca extracted matrices are robust to large errors and gaussian noise .",
    "ideally , these above recovered low - rank matrices can be used as features directly .",
    "but in order to balance the speed and performance , in this work the we transform the recovered low - rank matrices into mfccs ( mel - frequency cepstral coefficients ) matrices .",
    "all rows in the low - rank matrices are transformed into mfccs independently .",
    "[ fig : spectrograms_rpcatoaudioseg ] shows the spectrograms of the signal in fig .",
    "[ fig : rpcatoaudioseg ] respectively .",
    "it seems that the spectrograms of the low - rank components vary not much compare to the spectrograms of the corrupted signals .",
    "[ algo : prcaviaialm ] recovering of low - rank component from audio segments via rpca .",
    "* input * : @xmath18 ( matrix form of the audio segment ) .",
    "* initialize * : @xmath19    1 : * while * not converged * do *    2 : // lines 3 - 4 solve @xmath20    3 : @xmath21 .",
    "4 : @xmath22v^t$ ] .    5 : //",
    "line 6 solves @xmath23    6 : @xmath24.$ ]    7 : @xmath25 .    8 : update @xmath26 to @xmath27 .    9 : @xmath28 .    10 : *",
    "end while *    * output * : @xmath29 .",
    "having extracted robust matrix representation features , the linear matrix classification approach based on trace norm regularization framework proposed in  @xcite is used to classify them .",
    "the motivation for trace norm regularization framework is two fold : a ) trace norm considers the interactive information among the frames in the matrix while the simple approach that treat the matrix as a long vector would lose the information ; b ) trace norm is a suitable quantity that measures the complexity of the linear classifier .",
    "generally , the problem for trace norm regularization based matrix classification is formulated as    @xmath30    where @xmath31 is the unknown _ weight matrix _ , @xmath32 is the _ bias _ , @xmath33 denotes the trace norm defined as the sum of the singular values , and @xmath10 is the regularization parameter .",
    "@xmath34 is the empirical cost function induced by some convex smooth loss function @xmath35 , where @xmath36 denotes the trace , the subscript of @xmath37 indicates the number of training samples or time of training procedure which is apparent from context , and @xmath38 is the @xmath39th sample .",
    "in this work , the standard squared loss function is used .",
    "hence the empirical cost function becomes @xmath40 .",
    "recently toh and yun  @xcite , ji and ye  @xcite , and liu et al .",
    "@xcite independently proposed similar algorithms that converge as @xmath41 for problem  ( [ eq : prblmfrmltn ] ) by using apg , where @xmath42 is the iteration counter .",
    "the precondition of using apg algorithm is that the loss function should be smooth , convex , and the gradient should satisfy lipschitz condition . since @xmath37 in this work is a composition of smooth convex function with an affine mapping ,",
    "hence it is convex and smooth  @xcite . for lipschitz continuous ,",
    "it is shown in theorem  [ theorem : lipschitz ] that the gradient of @xmath37 , denoted as @xmath43 is lipschitz continuous .",
    "thus the apg method can be used to solve matrix classification problem . in order to solve the unconstrained convex optimization problem  ( [ eq : prblmfrmltn ] ) , apg approximate @xmath37 locally as a quadratic function with bias fixed and solve @xmath44 which is assumed to be easy , to update the solution @xmath45 . based on the the work of nesterov  @xcite , toh and yun  @xcite , ji and ye  @xcite , and liu et al .",
    "@xcite showed that setting @xmath46 for a sequence @xmath47 satisfying @xmath48 results in a convergence rate of @xmath49 . due to lemma  [ theorem : lipschitz ] , the estimation of step size @xmath50 in general apg  @xcite is omitted , for we have explicit lipschitz constant .",
    "the apg approach for batch - mode weight matrix learning is described in algorithm  [ algo : apg ] .",
    "the @xmath17 $ ] in algorithm  [ algo : apg ] is the soft - thresholding operator introduced in  @xcite : @xmath51\\doteq \\left\\ { \\begin{array}{l }   x-\\varepsilon , \\textrm{if } x>\\varepsilon ,    \\\\   x+\\varepsilon , \\textrm{if } x<-\\varepsilon ,    \\\\ 0 , \\textrm{otherwise }    \\\\   \\end{array } \\right.\\ ] ] where @xmath52 and @xmath53 . for vectors and matrices ,",
    "this operator is extended by applying element - wise .",
    "[ algo : apg ] batch - mode weight matrix learning via apg    * initialize * @xmath54    1 : * while * not converged * do *    2 : @xmath55 .",
    "3 : @xmath56v^t$ ] .    4 : @xmath57 .    5 : @xmath58 .    6 : @xmath59    7 : @xmath28 .    8 : * end while *    * output * : @xmath29 .",
    "the general apg  @xcite algorithms only provide the methods for learning weight matrices , do not give out the bias updating rules . in order to update",
    "the bias @xmath60 , fixes the weight matrix @xmath61 and solve the following problem @xmath62 which results in the bias updating rule @xmath63 this results in the line 6 of algorithm  [ algo : apg ] . for the stopping criteria of the iterations ,",
    "we take the following relative error conditions : @xmath64    after the weight matrix @xmath45 and bias @xmath60 are found , the observed mfccs matrix @xmath65 can be classified via @xmath66      as a special case of general convex optimization problem , we derived the closed - form of the lipschitz constant , hence the step size estimation  @xcite of the general apg method was omitted in all our approach .",
    "the determination of the lipschitz constant is shown in the following theorem .",
    "[ theorem : lipschitz ] @xmath67 is lipschitz continuous with constant @xmath68 , i.e. , @xmath69 , @xmath70 where @xmath71 denotes the frobenius norm .",
    "applying equation  ( [ eq : gradient ] ) with @xmath72 to the right of equation  ( [ eq : lipschitzcondition ] ) , we obtain @xmath73 where in the last inequality , the easily verified fact that @xmath74 for @xmath75 is used . here",
    "@xmath76 denotes the @xmath0 norm which is the sum of the absolute values of the matrix elements .",
    "thus the lemma is proofed , that is to say @xmath67 is lipschitz continuous with constant @xmath77 .",
    "the apg based batch - mode weight learning method is effective for small training set , but with large training sets , this classical optimization technique may become impractical in terms of memory requirements . furthermore , this method can not efficiently deal with dynamic training data of time sequences , such as audio and video processing . to tackle the insufficiency , we propose an online learning framework in the following section .",
    "we present in this section the basic components of our online learning algorithm for matrix classification , as well as a few minor variants which speed up our implementation in practice .",
    "[ algo : onlinelearning ] online mc learning based on apg .",
    "* initialize * @xmath78    1 : @xmath79(reset the `` past '' information ) .    2 : * for * @xmath80 * to * @xmath81",
    "* do *    3 : draw training sample @xmath82 from @xmath83 .    4 : // line 5 - 9 update `` past '' information .    5 : @xmath84 ;    6 : @xmath85 ;    7 : @xmath86 ;    8 : @xmath87 ;    9 : @xmath88 .    10 : // line 11 - 19 update @xmath89 and @xmath90 using algorithm  [ algo : apg ] , with @xmath91 and @xmath92 as warm restart .    11 : @xmath93    12 : * while * not converged * do *    13 : @xmath94 .    14 : @xmath95v^t$ ] .    15 : @xmath57 .    16 : @xmath96 .    17 : @xmath97    18 : @xmath28",
    ".    19 : * end while *    20 : @xmath98    21 : * end for *    * output * : @xmath99    our procedure is summarized in algorithm  [ algo : onlinelearning ] .",
    "the @xmath100 operator in step 6 of the algorithm denotes the kronecker product .",
    "given two matrices @xmath101 and @xmath102 , @xmath103 denotes the kronecker product between @xmath5 and @xmath104 , defined as the matrix in @xmath105 , defined by blocks of sizes @xmath106 equal to @xmath107b$ ] .",
    "@xmath108 in step 13 denotes an operator with input @xmath109 and @xmath110 , result in @xmath111 with the @xmath112th element defined as the trace of the product between @xmath113 and the @xmath112th @xmath111 block of @xmath114 .    assuming the training set composed of i.i.d .",
    "samples of a distribution @xmath83 , its inner loop draws one training sample @xmath82 at a time .",
    "this sample is first used to update the `` past '' information @xmath115 , @xmath116 , @xmath117 , and @xmath118 .",
    "then the algorithm  [ algo : apg ] is applied to update the weight matrix with the warm start @xmath91 obtained at the previous iteration . since @xmath119 is relative close to @xmath120 for large values of @xmath121 , so are @xmath89 and @xmath91 , under suitable assumptions , which makes it efficient to use @xmath91 as warm restart for computing @xmath89 .",
    "algorithm  [ algo : onlinelearning ] calls apg to update the weight matrix for each coming sample by solving the sub - problem with fixed bias @xmath60 @xmath122 exactly which cause computational load for large scale training set . fortunately ,",
    "due to the closeness of consecutive weight matrix , we do not have to solve the sub - problem exactly .",
    "rather , updating @xmath91 once when solving this sub - problem is sufficient in practice .",
    "this leads to an online mc learning method based on inexact apg , described in algorithm  [ algo : inexactonlinelearning ] .",
    "[ algo : inexactonlinelearning ] online mc learning with inexact apg .",
    "* initialize * @xmath78    1 : @xmath79 ( reset the `` past '' information ) .    2 : * for * @xmath80 * to * @xmath81 * do *    3 : draw training sample @xmath82 from @xmath83 .    4 : // line 5 - 9 update `` past '' information .    5 : @xmath84 ;    6 : @xmath85 .    7 : @xmath86 ;    8 : @xmath87 .    9 : @xmath88 .    10 : // line 11 - 16 compute @xmath89 using inexact apg , with @xmath91 as warm restart .    11 : @xmath123    12 : @xmath124 .    13 : @xmath125v^t$ ]",
    ".    14 : @xmath126 .    15 : @xmath127v^t$ ] .    16 : @xmath128    17 : // line 18 updates the bias @xmath90 .    18 : @xmath129    19 : * end for *    * output * : @xmath99      in some conditions , use the classical heuristic in gradient descent algorithm , we may also improve the convergence speed of our algorithm by drawing @xmath130 training samples at each iteration instead of a single one .",
    "let us denote by @xmath131 the samples drawn at iteration @xmath121 .",
    "we can now replace lines 5 and 9 of algorithm  [ algo : onlinelearning ] and [ algo : inexactonlinelearning ] by @xmath132 but in real applications , this batch method may not improve the convergence speed on the whole since the batch past information computation ( equation  ( [ eq : minibatchinfoupdata ] ) ) would occupy much of the time .",
    "the updating of @xmath114 needs to do kronecher product which spend much of the computing resource .",
    "if the computation cost of equation  ( [ eq : minibatchinfoupdata ] ) can be ignored or largely decreased , for example by parallel computing , the batch method would increase the convergence speed by a factor of @xmath133 .",
    "experiments are conducted on a collected database .",
    "we downloaded about 20hours videos from youku  @xcite , with different programs and different languages .",
    "the start and end position of all the applause and laugh of the audio - tracks are manually labeled .",
    "the database includes 800 segments of each sound effect .",
    "each segment is about 3 - 8s long and totally about 1hour data for each sound effect .",
    "all the audio recordings were converted to monaural wave format at a sampling frequency of 8khz and quantized 16bits .",
    "furthermore , the audio signals have been normalized , so that they have zero mean amplitude with unit variance in order to remove any factors related to the recording conditions .      in this section ,",
    "we conduct detailed experiments to demonstrate the characteristics and merits of the online learning for matrix classification problem .",
    "five algorithms are compared : the traditional batch algorithm with exact apg algorithm ( apg ) ; the online learning algorithm with exact apg ( ol_apg ) ; the online learning algorithm with inexact apg ( ol_iapg ) ; the online learning algorithm with exact apg and update equation  ( [ eq : minibatchinfoupdata ] ) ( ol_apg_batch ) ; the online learning algorithm with inexact apg and update equation  ( [ eq : minibatchinfoupdata ] ) ( ol_iapg_batch ) .",
    "all algorithms are run in matlab on a personal computer with an intel 3.40ghz dual - core central processing unit ( cpu ) and 2 gb memory .    for this experiment",
    ", audio streams were windowed into a sequence of short - term frames ( 20 ms long ) with non overlap .",
    "13 dimensional mfccs including energy are extracted , and adjacent 50 frames ( one second ) of mfccs form the mfccs matrix feature .",
    "the goal is to classify the matrices according to their labels .",
    "two learning tasks are used to evaluate the performance of the online learning method , which are laugh / non - laugh segment classifier learning and applause / non - applause segment classifier learning . for ol_apg and ol_apg_batch algorithms , the parameters in the stopping criteria  ( [ eq : stoppingcriteria ] ) are set @xmath134 and @xmath135 or smaller , which are determined by empirical evidence that larger values would make the algorithm diverge .",
    "the regularization constant @xmath10 is anchored by the large explicit fixed step size @xmath136 and the matrices involved , this can be seen from @xmath137 in the line 3 in algorithm  [ algo : apg ] , which means that in practice the parameter @xmath10 should be set adaptably with the step size @xmath136 in the online process .",
    "but due to this variation of @xmath10 , the comparisons between the algorithms would not bring into effect .",
    "hence in this work we use @xmath138 throughout .",
    "[ fig : performanccomparonline ] compares the five online algorithms .",
    "the proposed online algorithm draws samples from the entire training set .",
    "we use a logarithmic scale for the computation time .",
    "[ fig : performanccomparonline]a shows the values of the target functions as functions of time .",
    "it can be seen that the online learning methods without batch or with small batch past information updating converge faster than the methods with large batch past information updating and reason for this has been explained in the last paragraph of section  [ sec : onlinelearning ] .",
    "after online methods and batch methods converge , the two methods result in almost equal performance .",
    "[ fig : performanccomparonline](b)(d ) shows the classification rates for different algorithms respectively . in accordance with the values of the target functions , the classification accuracies of online methods without or with small batch updating become stable quickly than that of methods with batch updating .",
    "although the inexact algorithms process samples much fast with less resources than exact ones , they converge slowly .",
    "this section is to assess the effectiveness of robust pca extracted low - rank matrix features .",
    "original features ( mfccs_matrix ) , corrupted with 0db and -5db white gaussian noise ( wgn snr=5db , 0db , -5db ) and 10% , 30% , 50% random large errors ( le 10% , 30% , 50% ) , and parallelism robust pca extracted features ( rpca ) are compared . in the comparisons , the parameters in the stopping criteria  ( [ eq : stoppingcriteria ] )",
    "are set @xmath139 and @xmath140 , which are determined by the same method as in section  [ sec : onlinelearning ] .",
    "the regularization constant @xmath10 is set @xmath141 which is a classical normalization factor according to  @xcite .",
    "the classification accuracy of the one second audio segments is used to evaluate the performance of the methods .",
    "[ fig : performanccompar ] shows the performances of the methods with different matrix features under different noise conditions as the functions of the training time used in algorithm  [ algo : apg ] .",
    "it can be seen that the original mfccs matrix feature is not robust to noises , especially random large errors .",
    "if 10% of the elements of the mfccs matrix feature are corrupted with random large errors , then generally there would be a decrease of 25% in audio segments classification accuracy , while for robust pca extracted low - rank features , the decrease are 5% in average . for wgn",
    ", the robust pca features also perform better than original features , although not so sharp as in the situation of large errors .",
    "the experiments show that the low - rank components are more robust to noises and errors than the original features .    [",
    "cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]     we also compare our method with the state - of - the - art svm classifier with long vector feature ( 650 dimension ) obtained by vectorizing the matrix .",
    "the results are summarized in table  [ tab : appcomparewithsvm ] and table  [ tab : laucomparewithsvm ] for applause / non - applause and laugh / non - laugh classification respectively .",
    "the results show that the svm become useless under 5db wight noise and 10% large corruptions , while our methods still works .",
    "but for the low - rank component , the svm performs better on some situations for which is due to the robustness of the features .",
    "in this work , we present a novel framework based on trace norm minimization for audio segment classification . the novel method unified feature extraction and pattern classification into the same framework . in this framework , robust pca extracted low - rank component of original signal is more robust to corrupted noise and errors , especially to random large errors .",
    "we also introduced online learning algorithms for matrices classification tasks .",
    "we obtain the closed - form updating rules of the weight matrix and the bias .",
    "we derive the explicit form of the lipschitz constant , which saves the computation burden in searching step size .",
    "experiments show that even the percent of the original feature elements corrupted with random large errors is up to 50% , the performance of the robust pca extracted features almost have no decrease . in future work , we plan to test this robust feature in other audio or speech processing related applications and extend robust pca , even trace norm minimization related methods from matrices to the more general multi - way arrays ( tensors ) . some work related to learning methods are also worth considering , such that the alternating between minimization with respect to weight matrix and bias may results in fluctuation of target value ( even in batch mode ) , thus optimization algorithm that minimization jointly on weight matrix and bias are required ; for multi - classification problems with more classes , some hierarchy methods may be introduced to improve the classification accuracy .",
    "k. a. pradeep , c. m. namunu , and s. k. mohan , `` audio based event detection for multimedia surveillance '' , in _ proceedings of ieee international conference on acoustics , speech and signal processing _ , 2006 .",
    "k. umapathy , s. krishnan , and r.k .",
    "rao , `` audio signal feature extraction and classification using local discriminant bases '' , in _ ieee transactions on audio , speech , and language processing _ , vol .",
    "1 , pp . 1236 - 1246 , 2007 .",
    "x. zhuang , x. zhou , t.s .",
    "huang , and m. hasegawa - johnson , `` feature analysis and selection for acoustic event detection '' , in _ proceedings of ieee international conference on acoustics , speech and signal processing _",
    "17 - 20 , 2008 .",
    "g. guo and s.z .",
    "li , `` content - based audio classification and retrieval by support vector machines '' , ieee transactions on neural networks vol .",
    "1 , pp . 209 - 215 , 2003 .",
    "m. fazel , h. hindi , and s. p. boyd , `` a rank minimization heuristic with application to minimum order system approximation '' , in _ proceedings of the american control conference _ , pp .",
    "4734 - 4739 , 2001 .",
    "j. wright , a. ganesh , s. rao , y. peng , and y. ma , `` robust principal component analysis : exact recovery of corrupted low - rank matrices via convex optimization '' , in _ proceedings of advances in neural information processing systems _ , 2009 .",
    "i. t. jolliffe , _ principal component analysis _ , springer series in statistics , berlin : springer , 1986 .",
    "e. j. candes and b. recht , `` exact matrix completion via convex optimization '' , technical report , ucla computational and applied math , 2008 ."
  ],
  "abstract_text": [
    "<S> in this paper , a novel framework based on trace norm minimization for audio segment is proposed . in this framework , both the feature extraction and classification are obtained by solving corresponding convex optimization problem with trace norm regularization . for feature extraction , robust principle component analysis ( robust pca ) via minimization a combination of the nuclear norm and the @xmath0-norm is used to extract low - rank features which are robust to white noise and gross corruption for audio segments . </S>",
    "<S> these low - rank features are fed to a linear classifier where the weight and bias are learned by solving similar trace norm constrained problems . for this classifier </S>",
    "<S> , most methods find the weight and bias in batch - mode learning , which makes them inefficient for large - scale problems . in this paper </S>",
    "<S> , we propose an online framework using accelerated proximal gradient method . </S>",
    "<S> this framework has a main advantage in memory cost . </S>",
    "<S> in addition , as a result of the regularization formulation of matrix classification , the lipschitz constant was given explicitly , and hence the step size estimation of general proximal gradient method was omitted in our approach . </S>",
    "<S> experiments on real data sets for laugh / non - laugh and applause / non - applause classification indicate that this novel framework is effective and noise robust . </S>"
  ]
}