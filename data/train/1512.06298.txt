{
  "article_text": [
    "in many multimedia applications , a stream of data packets is required to be sequentially encoded and decoded under strict latency constraints .",
    "for such a streaming setup , both the fundamental limits and optimal schemes can differ from classical communication systems . in recent years",
    ", there has been a growing interest in the characterization of fundamental limits for streaming data transmission @xcite . in @xcite ,",
    "coding techniques based on tree codes were proposed for streaming setup with applications to control systems . in @xcite ,",
    "khisti and draper established the optimal diversity - multiplexing tradeoff ( dmt ) for streaming over a block - fading multiple - input multiple - output channel . in @xcite ,",
    "the same authors proposed a coding technique using finite memory for streaming over discrete memoryless channels ( dmcs ) that attains the same reliability as previously known semi - infinite coding techniques with growing memory . in @xcite ,",
    "the error exponent was studied in a streaming setup of distributed source coding .",
    "we note that these prior works assumed that the code operates in the large deviations regime in which the rate is bounded away from capacity ( or the rate pair is strictly inside the optimal rate region for compression problems ) and the error probability decays exponentially as the blocklength increases .",
    "other interesting asymptotic regimes include the central limit and moderate deviations regimes .",
    "let @xmath2 denote the blocklength of a single message henceforth .",
    "in the central limit regime , the rate approaches to the capacity at a speed proportional to @xmath3 and the error probability does not vanish as the blocklength increases . in the moderate deviations regime ,",
    "the rate approaches to the capacity strictly slower than @xmath3 and the error probability decays sub - exponentially fast as the blocklength increases . for block coding problems ,",
    "both regimes have received a fair amount of attention recently . these works",
    "aim to characterize the fundamental interplay between the coding rate and error probability .",
    "the most notable early work on channel coding in the central limit regime ( also known as second - order asymptotics or the normal approximation regime ) is that of strassen @xcite , who considered dmcs and showed that the backoff from capacity scales as @xmath4 when the error probability is fixed .",
    "strassen also deduced the constant of proportionality , which is related to the so - called _ dispersion _",
    "hayashi @xcite considered dmcs with cost constraints as well as discrete channels with markovian memory .",
    "@xcite refined the asymptotic expansions and also compared the normal approximation to the finite blocklength ( non - asymptotic ) fundamental limits . for a review and extensions to multi - terminal models ,",
    "the reader is referred to @xcite . for the moderate deviations regime , he _",
    "et al . _",
    "@xcite considered fixed - to - variable length source coding with decoder side information .",
    "altu and wagner  @xcite initiated the study of moderate deviations for channel coding , specifically dmcs .",
    "polyanskiy and verd  @xcite relaxed some assumptions in the conference version of altu and wagner s work  @xcite and they also considered moderate deviations for additive white gaussian noise ( awgn ) channels .",
    "however , this line of research has not been extensively studied for the streaming setup . to the best of our knowledge , there has been no prior work on the streaming setup in the moderate deviations and central limit regimes with the exception @xcite where the focus is on source coding .    in this paper",
    ", we study streaming data transmission over a dmc in the moderate deviations and central limit regimes .",
    "our streaming setup is illustrated in fig .",
    "[ fig : streaming ] . in each block of length @xmath2 ,",
    "a new message is given to the encoder at the beginning , and the encoder generates a codeword as a function of all the past and current messages and transmits it over the channel .",
    "the decoder , given all the past received channel output sequences , decodes each message after a delay of @xmath0 blocks .",
    "this streaming setup introduces a new dimension not present in the block coding problems studied previously .",
    "in the special case of @xmath5 , the setup reduces to the block channel coding problem .",
    "if @xmath6 , however , there exists an inherent tension in whether we utilize a block only for the fresh message or use it also for the previous messages with earlier deadlines .",
    "it is not difficult to see that due to the memoryless nature of the model , a time sharing scheme will not provide any gain compared to the case of @xmath5 .",
    "a natural question is whether a joint encoding of fresh and previous messages would improve the performance when @xmath6 .",
    "our results indicate that the fundamental interplay between the rate and error probability can be greatly improved when delay is allowed in the streaming setup . in the moderate deviations regime",
    ", the moderate deviations constant is shown to improve over the block coding or non - streaming setup by a factor of @xmath0 . in the central limit regime , the second - order coding rate is shown to improve by a factor of approximately @xmath1 for a wide range of channel parameters .",
    "for both asymptotic regimes , we propose coding techniques that incorporate a joint encoding of fresh and previous messages . for the moderate deviations regime ,",
    "we propose a coding technique in which , for every block , the encoder jointly encodes all the previous and fresh messages and the decoder re - decodes all the previous messages in addition to the current target message . for the error analysis of this coding technique , we develop a refined and non - asymptotic version of the moderate deviations upper bound in ( * ? ? ?",
    "* theorem 3.7.1 ) that allows us to uniformly bound the error probabilities associated with the previous messages . on the other hand , for the central limit regime",
    ", we can not apply such a coding technique whose memory is linear in the block index . in the error analysis in the central limit regime",
    ", we encounter a summation of constants as a result of applications of the central limit theorem .",
    "if the memory is linear in the block index , this summation causes the upper bound on the error probability to diverge as the block index tends to infinity .",
    "hence , for the central limit regime , we propose a coding technique with _ truncated _ memory where the memory at the encoder varies in a periodic fashion .",
    "our proposed construction judiciously balances the rate penalty imposed due to the truncation and the growth in the error probability due to the contribution from previous messages . by analyzing the second - order coding rate of our proposed setup ,",
    "we conclude that the channel dispersion parameter also decreases approximately by a factor of @xmath0 for a wide range of channel parameters .",
    "furthermore , we explore interesting variants of the basic streaming setup in the moderate deviations regime .",
    "first , we consider a scenario where there is an erasure option at the decoder and analyze the undetected error and the total error probabilities , extending a result by hayashi and tan  @xcite .",
    "next , by utilizing the erasure option , we analyze the rate of decay of the error probability when a variable decoding delay is allowed .",
    "we show that such a flexibility in the decoding delay can dramatically improve the error probability in the streaming setup .",
    "this result is the analog of the classical results on variable - length decoding ( see e.g. , @xcite ) to the streaming setup . finally , as a simple example for the case where the message rates are not constant , we consider a scenario where the rate of the messages in odd block indices and the rate of the messages in even block indices are different and analyze the moderate deviations constants separately for the two types of messages .",
    "this setting finds applications in video and audio coding where streams of data packets do not necessarily have a constant rate .",
    "the rest of this paper is organized as follows . in section [ sec :",
    "model ] , we formally state our streaming setup .",
    "the main theorems are presented in section [ sec : main ] and proved in section [ sec : proof ] . in section [ sec : extension ] , the moderate deviations result for the basic streaming setup is extended in various directions .",
    "we conclude this paper in section [ sec : conclusion ] .",
    "the following notation is used throughout the paper .",
    "we reserve bold - font for vectors whose lengths are the same as blocklength @xmath2 . for two integers @xmath7 and @xmath8 , @xmath9 $ ]",
    "denotes the set @xmath10 . for constants",
    "@xmath11 and @xmath12 $ ] , @xmath13 denotes the vector @xmath14 and @xmath15 denotes @xmath16}$ ] where the subscript is omitted when @xmath17 , i.e. , @xmath18}$ ] .",
    "this notation is naturally extended for vectors @xmath19 , random variables @xmath20 , and random vectors @xmath21 .",
    "@xmath22 for an event @xmath23 denotes the indicator function , i.e. , it is 1 if @xmath23 is true and 0 otherwise .",
    "@xmath24 and @xmath25 denote the ceiling and floor functions , respectively .",
    "for a dmc @xmath26 and an input distribution @xmath27 , we use the following standard notation and terminology in information theory :    * information density : @xmath28 where @xmath29 denotes the output distribution .",
    "we note that @xmath30 depends on @xmath27 and @xmath31 but this dependence is suppressed .",
    "the definition can be generalized for two vectors @xmath32 and @xmath33 of length @xmath34 as follows : @xmath35 * mutual information : @xmath36\\\\ & = \\sum_{x\\in \\mathcal{x}}\\sum_{y\\in \\mathcal{y } } p(x)w(y|x)\\log\\frac{w(y|x)}{pw(y)}.\\end{aligned}\\ ] ] * unconditional information variance : @xmath37.\\end{aligned}\\ ] ] * conditional information variance : @xmath38.\\end{aligned}\\ ] ] * capacity : @xmath39 where @xmath40 denotes the probability simplex on @xmath41 . *",
    "set of capacity - achieving input distributions : @xmath42 * channel dispersion @xmath43 where @xmath44 is from ( * ? ? ?",
    "* lemma 62 ) , where it is shown that @xmath45 for all @xmath46 .",
    "consider a dmc @xmath26 .",
    "a streaming code is defined as follows :    [ def : basic ] an @xmath47-streaming code consists of    * a sequence of messages @xmath48 each distributed uniformly over @xmath49 $ ] , * a sequence of encoding functions @xmath50 that maps the message sequence @xmath51 to the channel input codeword @xmath52 , and * a sequence of decoding functions @xmath53 that maps the channel output sequences @xmath54 to a message estimate @xmath55 ,    that satisfies @xmath56 i.e. , the probability of error averaged over all block messages does not exceed @xmath57 .",
    "we note that a streaming code with a _ fixed _",
    "blocklength @xmath2 consists of a _ sequence _ of encoding and decoding functions since a stream of messages is sequentially encoded and decoded .",
    "[ fig : streaming ] illustrates our streaming setup for the case with @xmath58 . in the beginning of block",
    "@xmath59 , new message @xmath60 is given to the encoder .",
    "the encoder generates a codeword @xmath61 as a function of all the past and current messages @xmath62 and transmits it over the channel in block @xmath63 . since @xmath58 ,",
    "the decoder decodes message @xmath60 at the end of block @xmath64 , as a function of all the past received channel output sequences @xmath65 .",
    "in this section , we state our main results .",
    "the following two theorems present achievability bounds for the moderate deviations and the central limit regimes , respectively , which are proved in section [ sec : proof ] .",
    "[ thm : md ] consider a dmc @xmath26 with @xmath66 and any sequence of integers @xmath67 such that @xmath68 , where @xmath69 and @xmath70 .. ] then , there exists a sequence of @xmath71-streaming codes such that for some @xmath72 , @xmath73 corresponds to an upper bound on the moderate deviations constant . in the special case of @xmath5 ,",
    "the moderate deviations constant is shown to be the channel dispersion @xmath74 in @xcite . ]",
    "@xmath75    [ thm : cl ] consider a dmc @xmath26 with @xmath66 .",
    "for any @xmath76 and @xmath77 , there exists a sequence of @xmath71-streaming codes such that is termed second - order coding rate in this paper .",
    "this is slightly different from what is common in the literature where instead @xmath78 is known as the second - order coding rate @xcite . ]",
    "@xmath79 and @xmath80    the following corollary , whose proof is in appendix [ appendix : asymptotic ] , elucidates a closed - form and interpretable expression for the upper bound on the error probability in .",
    "[ coro : cl_asymp ] consider a dmc @xmath26 with @xmath66 .",
    "for any @xmath76 , there exists a sequence of @xmath71-streaming codes such that @xmath81 and @xmath82 where @xmath83 defined in the following has the property that for every @xmath84 , @xmath83 tends to 1 as @xmath85 tends to infinity : @xmath86    fig .",
    "[ fig : const ] illustrates how fast the constant @xmath83 in corollary [ coro : cl_asymp ] converges to 1 as @xmath85 increases . for @xmath58",
    ", we can see that @xmath83 is less than 1.1 when @xmath87 and is less than 1.05 when @xmath88 .",
    "hence , the effect of the constant @xmath83 is not significant for a wide range of @xmath89 and @xmath0 .",
    "theorems [ thm : md ] and [ thm : cl ] illustrate that the fundamental interplay between the rate and probability of error can be greatly improved when delay is allowed in the streaming setup . in the moderate deviations regime ,",
    "the moderate deviations constant improves by a factor of @xmath0 . assuming that @xmath83 can be approximated sufficiently well by @xmath90 , for the central limit regime",
    ", the second - order coding rate @xmath91 is improved ( reduced ) by a factor of @xmath1 . another way to view this via the lens of the channel dispersion @xmath74 ; this parameter is approximately reduced by a factor of @xmath0 .",
    "consider a dmc @xmath26 with @xmath66 and any sequence of integers @xmath67 such that @xmath68 , where @xmath69 and @xmath70 .",
    "we denote by @xmath92 an input distribution that achieves the dispersion .",
    "for each @xmath59 and @xmath93 , generate @xmath94 in an i.i.d .",
    "manner according to @xmath92 .",
    "the generated codewords constitute the codebook @xmath95 . in block",
    "@xmath63 , after observing the true message sequence @xmath62 , the encoder sends @xmath96 .",
    "consider the decoding of @xmath60 at the end of block @xmath97 . in our scheme ,",
    "the decoder not only decodes @xmath60 , but also re - decodes @xmath98 at the end of block @xmath99 . for @xmath100",
    "$ ] has been already decoded at the end of block @xmath101 .",
    "nevertheless , the decoder re - decodes @xmath102 at the end of @xmath99 , because the decoder needs to decode @xmath102 to decode @xmath60 and the probability of error associated with @xmath102 becomes lower ( in general ) by utilizing recent channel output sequences .",
    "] let @xmath103 denote the estimate of @xmath104 at the end of block @xmath99 .",
    "the decoder decodes @xmath104 sequentially from @xmath105 to @xmath106 as follows :    * given @xmath107}$ ] , the decoder chooses @xmath103 according to the following rule .",
    ", @xmath108 is null . ]",
    "if there is a unique index @xmath109 that satisfies for @xmath110 denote the set of message indices mapped to the @xmath8-th codeword according to the encoding procedure . for @xmath111 and @xmath112 , we denote by @xmath113 the set of codewords @xmath114 . ] @xmath115}(\\hat{g}_{t_k , [ 1:j-1 ] } , g_{[j : t_k ] } ) , \\mathbf{y}_{[j : t_k ] } ) & > ( t_k - j+1 ) \\cdot",
    "\\log m_n \\label{eqn : dec_rule}\\end{aligned}\\ ] ] for some @xmath116}$ ] , let @xmath117 .",
    "in is defined in terms of @xmath92 and @xmath31 .",
    "this dependence is suppressed henceforth . ] if there is none or more than one such @xmath118 , let @xmath119 . *",
    "if @xmath120 , repeat the above procedure by increasing @xmath8 to @xmath121 . if @xmath106 , the decoding procedure terminates and the decoder declares that the @xmath63-th message is @xmath122 .",
    "now , fix an arbitrary @xmath131 . by applying the chain of inequalities ( * ? ? ?",
    "* eq . ( 53)-(56 ) ) , we have @xmath132^+\\right\\}\\cr & \\leq   \\mathbbm{1}\\left\\{\\sum_{l=1}^{n(t_k - j+1 ) } i(x_l;y_l)\\leq ( t_k - j+1)n(c-\\lambda\\rho_n)\\right\\ } + \\exp\\left\\{-(t_k - j+1)n(1-\\lambda)\\rho_n\\right\\}.   \\label{eqn : spr}\\end{aligned}\\ ] ] combining the bounds in and , we obtain @xmath133 for sufficiently large @xmath2 , where @xmath134 is some non - negative constant dependent only on the input distribution @xmath135 and channel statistics @xmath136 and @xmath44 is from the moderate deviations upper bound in lemma  [ lemma : nonasymptotic_md ] , which is relegated to the end of this subsection . also see remark [ rem : md ] .",
    "now , we have @xmath137\\cr & \\leq \\sum_{j=1}^k \\left ( \\exp\\left\\{-(t_k - j+1)n\\rho_n^2\\lambda^2\\left(\\frac{1}{2v}-\\lambda \\rho_n \\tau\\right)\\right\\}+\\exp\\left\\{-(t_k - j+1)n(1-\\lambda)\\rho_n\\right\\ } \\right)\\\\ & \\leq \\sum_{j = t}^{t_k } \\left ( \\exp\\left\\{-jn\\rho_n^2\\lambda^2\\left(\\frac{1 } { 2v}-\\lambda \\rho_n \\tau\\right)\\right\\}+\\exp\\left\\{-jn(1-\\lambda)\\rho_n\\right\\}\\right ) \\\\ & \\leq \\frac{\\exp\\left\\{-tn\\rho_n^2\\lambda^2\\left(\\frac{1 } { 2v}-\\lambda \\rho_n \\tau\\right)\\right\\}}{1-\\exp\\{-n\\rho_n^2\\lambda^2\\left(\\frac{1 } { 2v}-\\lambda \\rho_n \\tau\\right)\\}}+\\frac{\\exp\\left\\{-tn(1-\\lambda)\\rho_n\\right\\}}{1-\\exp\\left\\{-n(1-\\lambda)\\rho_n\\right\\ } } \\label{eqn : md_sumup}\\end{aligned}\\ ] ] for sufficiently large @xmath2 , which leads to @xmath138\\leq -\\frac{t\\lambda^2}{2v}.\\end{aligned}\\ ] ] finally , by taking @xmath139 , we have @xmath138\\leq -\\frac{t}{2v}.\\end{aligned}\\ ] ] hence , there must exist a sequence of codes @xmath95 that satisfies , which completes the proof .",
    "the following lemma used in the proof of theorem [ thm : md ] corresponds to a non - asymptotic upper bound of the moderate deviations theorem ( * ? ? ?",
    "* theorem 3.7.1 ) , whose proof is in appendix [ appendix : finite_md ] .    [",
    "lemma : nonasymptotic_md ] let @xmath140 be a sequence of i.i.d .",
    "random variables such that @xmath141 = 0 $ ] , @xmath142 = \\sigma^2>0 $ ] , and its cumulant generating function @xmath143 $ ] for @xmath144 is analytic around the origin and satisfies that @xmath145 }   | h'''(s ) |$ ] is finite . for a sequence @xmath146 satisfying the moderate deviations constraints , i.e. , @xmath147 and @xmath148 , the following bound holds : @xmath149 for sufficiently large @xmath2 .",
    "[ rem : md ] let us comment on the assumption in lemma [ lemma : nonasymptotic_md ] that @xmath150 is finite . in our application",
    ", @xmath151 then , we have @xmath152\\\\    & = -s i(x_1;y_1 ) + \\log \\e \\left [   \\big(\\frac{w(y_1 |x_1)}{p_xw(y_1 ) }    \\big)^s\\right ] .   \\end{aligned}\\ ] ] by differentiating thrice , we can show that @xmath153 is continuous in @xmath154 . restricting @xmath154 to @xmath155 $ ]",
    "means that @xmath153 is a continuous function over a compact set .",
    "hence its maximum is attained and is necessarily finite .      consider a dmc @xmath26 with @xmath66 .",
    "we remark that in the moderate deviations regime , for every block , the encoder maps _ all _ the previous messages to a codeword . for the central limit regime ,",
    "we propose a coding strategy where the encoder maps only _ some _ recent messages to the codeword in each block .",
    "similar idea of incorporating truncated memory was used in @xcite with the focus on reducing the complexity . here",
    ", we use a different memory structure from @xcite .",
    "let @xmath156 and @xmath157 denote the maximum and the minimum numbers of messages that can possibly be mapped to a codeword in each block , respectively .",
    "we choose the size @xmath67 of message alphabet as follows : @xmath158 for some @xmath76 . to make the above choice of @xmath67 valid",
    ", we assume @xmath159 .",
    "furthermore , we assume that the minimum encoding memory is at least @xmath0 , i.e. , @xmath160 .",
    "we denote by @xmath92 an input distribution that achieves the dispersion .",
    "our encoder has a periodically time - varying memory @xmath161 $ ] with a period of @xmath162 blocks , after an initialization step of the first @xmath163 blocks .",
    "let us first describe our message - codeword mapping rule for the case of @xmath164 and @xmath165 , which is illustrated in fig .",
    "[ fig : mapping ] . for the first nine blocks , the encoder maps all the previous messages to a codeword .",
    "since the maximum encoding memory is nine in this example , we _ truncate _ the messages that are mapped to a codeword on and after the tenth block , so that the encoding memory is periodically time - varying from four to nine with a period of six blocks .",
    "for instance , let us consider the first period from the tenth block to the fifteenth block . in the tenth block , the encoder maps the messages @xmath166 to a codeword , thus ensuring that the encoding memory is four . in block",
    "@xmath167 $ ] , the encoder maps the messages @xmath168 to a codeword and hence the encoding memory becomes the maximum memory of nine when @xmath169 .",
    "now , let us formally describe the encoding procedure for the general case . for each @xmath170 $ ] and @xmath93 , generate @xmath94 in an i.i.d .",
    "manner according to @xmath92 . in block",
    "@xmath170 $ ] , the encoder sends @xmath96 .",
    "let @xmath171 for @xmath172 denote the set of @xmath173 block indices in the @xmath174-th period on and after the @xmath175-st block , i.e. , @xmath176 .",
    "for each @xmath177 and @xmath178 , , a total of @xmath179 messages , i.e. , @xmath180 , are mapped to a codeword . ]",
    "generate @xmath181 in an i.i.d .",
    "manner according to @xmath92 . in block",
    "@xmath182 , the encoder sends @xmath183})$ ] .    on the other hand",
    ", we note that our message - codeword mapping rule is also periodic in the ( vertical ) axis of message index .",
    "we can group the messages according to the maximum block index to which a message is mapped .",
    "let @xmath184 for @xmath185 denote the @xmath174-th group @xmath186 of messages that are mapped to a codeword up to block @xmath187 , which is illustrated in fig .",
    "[ fig : mapping ] for the example of @xmath164 and @xmath165 .",
    "this grouping rule is useful for describing the decoding rule .",
    "the decoding rule of @xmath188 at the end of block @xmath99 is exactly the same as that for the moderate deviations regime .",
    "hence , from now on , let us focus on the decoding of @xmath189 for @xmath190 at the end of block @xmath99 . at the end of block @xmath99 ,",
    "the decoder decodes not only @xmath60 , but also all the messages in the previous group and the previous messages in the current group , for @xmath100 $ ] has been already decoded at the end of block @xmath101 . nevertheless , the decoder re - decodes some of the previous messages at the end of @xmath99 . ]",
    "i.e. , @xmath191 .",
    "let @xmath103 denote the estimate of @xmath104 at the end of block @xmath99 .",
    "let us first describe our decoding procedure for the example of @xmath58 , @xmath164 , and @xmath165 illustrated in fig .",
    "[ fig : mapping ] . consider the decoding of @xmath192 at the end of block @xmath99 .",
    "for the cases @xmath193 and @xmath194 can be stated in a similar manner . ]",
    "the decoder decodes not only @xmath60 , but also all the messages @xmath195 in @xmath196 and the previous messages @xmath197 in @xmath198 .",
    "the underlying rules of our decoding procedure can be summarized as follows :    * since messages @xmath199 in @xmath200 , which we do not want to decode , are involved in blocks @xmath201 , we do not utilize the channel output sequences in those blocks for decoding . *",
    "for the decoding of the @xmath8-th message for @xmath202 $ ] , among the channel output sequences from block @xmath203 to block @xmath99 , we utilize the channel output sequences in which the @xmath8-th message is involved .    according to the above rules , the blocks to be considered for the decoding of messages @xmath204 are as follows :    a.   for @xmath205 , blocks are involved is @xmath99 if @xmath206 , and it is @xmath207 otherwise .",
    "in other words , the last block index to which the messages in @xmath196 are involved is @xmath208 . ]",
    "indexed from 10 to @xmath209 , b.   for @xmath104 for @xmath210 $ ] , blocks indexed from @xmath8 to @xmath211 , and c.   for @xmath212 for @xmath213 $ ] , blocks indexed from @xmath8 to @xmath99 .    in particular , since the pairs of the first block index and the last block index to be considered for the decoding of messages @xmath205 are the same , we decode @xmath205 simultaneously . by keeping this in mind , our decoding procedure for @xmath214 for the example of @xmath58 , @xmath164 and @xmath165 is formally stated as follows :    a.   if there is a unique index vector @xmath215}$ ] that satisfies , the following notation is used for the set of codewords .",
    "let @xmath216 for @xmath110 denote the set of message indices mapped to the @xmath8-th codeword according to the encoding procedure . for @xmath111 and @xmath112",
    ", we denote by @xmath113 the set of codewords @xmath114 . ]",
    "@xmath217}(g_{[7:\\nu_k]}),\\mathbf{y}_{[10:\\nu_k]})&>(\\nu_k-6)\\cdot \\log m_n\\label{eqn : dec_rule_cl1_toy}\\end{aligned}\\ ] ] for some @xmath218}$ ] , let @xmath219}=g_{[7:10]}$ ] .",
    "if there is none or more than one such @xmath215}$ ] , let @xmath219}=(1,\\cdots , 1)$ ] .",
    "b.   the decoder sequentially decodes @xmath118 from @xmath220 to @xmath221 as follows : * given @xmath222}$ ] , the decoder chooses @xmath103 according to the following rule . if there is a unique index @xmath109 that satisfies @xmath223}(\\hat{g}_{t_k,[7:j-1 ] } , g_{[j:\\nu_k ] } ) , \\mathbf{y}_{[j:\\nu_k ) ] } ) & > ( \\nu_k - j+1 ) \\cdot \\log m_n \\label{eqn : dec_rule_cl2_toy}\\end{aligned}\\ ] ] for some @xmath224}$ ] , let @xmath117 . if there is none or more than one such @xmath118 , let @xmath119 .",
    "* if @xmath220 , repeat the above procedure by increasing @xmath8 to @xmath225 . if @xmath221 , proceed to the next decoding procedure .",
    "c.   the decoder sequentially decodes @xmath118 from @xmath226 to @xmath106 as follows : * given @xmath227}$ ] , the decoder chooses @xmath228 according to the following rule . if there is a unique index @xmath109 that satisfies @xmath115}(\\hat{g}_{t_k,[7:j-1 ] } , g_{[j : t_k ] } ) , \\mathbf{y}_{[j : t_k ] } ) & > ( t_k - j+1 ) \\cdot",
    "\\log m_n \\label{eqn : dec_rule_cl3_toy}\\end{aligned}\\ ] ] for some @xmath116}$ ] , let @xmath117 .",
    "if there is none or more than one such @xmath118 , let @xmath119 .",
    "* if @xmath120 , repeat the above procedure by increasing @xmath8 to @xmath121 . if @xmath106 , the whole decoding procedure terminates and the decoder declares that the @xmath63-th message is @xmath122 .    the above description of the decoding procedure for the example in fig .",
    "[ fig : mapping ] is naturally extended for the general case . in general , the procedure for decoding of @xmath189 for @xmath190 at the end of block @xmath99 consists of the following three steps : ( i ) simultaneous non - unique decoding of the first @xmath229 messages in the previous group , ( ii ) sequential decoding of the remaining @xmath230 messages in the previous group , and ( iii ) sequential decoding of the messages in the current group up to the current block .",
    "let us describe the decoding rule when @xmath193 in the following :    a.   if there is a unique index vector @xmath231 that satisfies @xmath232}(g^{\\min(a , t_k)}),\\mathbf{y}_{[b:\\min ( a , t_k)]})&>\\min(a , t_k)\\cdot \\log m_n\\label{eqn : dec_rule_cl1}\\end{aligned}\\ ] ] for some @xmath233}$ ] , let @xmath234}=g^b$ ] . if there is none or more than one such @xmath231 , let @xmath235}=(1,\\cdots,1)$ ] .",
    "b.   the decoder sequentially decodes @xmath118 from @xmath236 to @xmath237 as follows : * given @xmath107}$ ] , the decoder chooses @xmath103 according to the following rule . if there is a unique index @xmath109 that satisfies @xmath238}(\\hat{g}_{t_k,[1:j-1 ] } , g_{[j:\\min(a , t_k ) ] } ) , \\mathbf{y}_{[j:\\min(a , t_k ) ] } ) & > ( \\min(a , t_k)-j+1 ) \\cdot \\log m_n \\label{eqn : dec_rule_cl2}\\end{aligned}\\ ] ] for some @xmath239}$ ] , let @xmath117 . if there is none or more than one such @xmath118 , let @xmath119 .",
    "* if @xmath240 , repeat the above procedure by increasing @xmath8 to @xmath121 . if @xmath237 , proceed to the next decoding procedure .",
    "c.   the decoder sequentially decodes @xmath118 from @xmath241 to @xmath106 as follows : * given @xmath107}$ ] , the decoder chooses @xmath103 according to the following rule . if there is a unique index @xmath109 that satisfies @xmath115}(\\hat{g}_{t_k,[1:j-1 ] } , g_{[j : t_k ] } ) , \\mathbf{y}_{[j : t_k ] } ) & > ( t_k - j+1 ) \\cdot",
    "\\log m_n \\label{eqn : dec_rule_cl3}\\end{aligned}\\ ] ] for some @xmath116}$ ] , let @xmath117 .",
    "if there is none or more than one such @xmath118 , let @xmath119 .",
    "* if @xmath120 , repeat the above procedure by increasing @xmath8 to @xmath121 .",
    "if @xmath106 , the whole decoding procedure terminates and the decoder declares that the @xmath63-th message is @xmath122 .    by exploiting the symmetry of the message - codeword mapping rule , the decoding rule for @xmath242 proceeds similarly .",
    "we note that the superscript in each error event represents the decoding step in which the error event is involved .",
    "now , we have @xmath126 & \\leq \\pr(\\mathcal{e}_{k}^{\\mathrm{(i)}})+\\pr ( \\tilde{\\mathcal{e}}_{k}^{\\mathrm{(i)}})+ \\sum_{j = b+1}^{a - b+1 } \\pr(\\mathcal{e}_{k , j}^{\\mathrm{(ii ) } } )   + \\sum_{j = b+1}^{a - b+1 } \\pr ( \\tilde{\\mathcal{e}}_{k , j}^{\\mathrm{(ii ) } } ) \\cr & \\quad \\quad \\quad\\quad \\quad+\\sum_{j = a - b+2}^{k } \\pr ( \\mathcal{e}_{k , j}^{\\mathrm{(iii ) } } )   + \\sum_{j = a - b+2}^{k } \\pr ( \\tilde{\\mathcal{e}}_{k , j}^{\\mathrm{(iii ) } } ) .",
    "\\label{eqn : first_case_sum}\\end{aligned}\\ ] ] let us bound each term in the rhs of ( [ eqn : first_case_sum ] ) .",
    "first , @xmath247 is upper - bounded as follows : @xmath248}(g^{\\alpha } ) , \\mathbf{y}_{[b:\\alpha]})\\leq \\alpha\\cdot \\log m_n\\right)\\\\ & \\leq \\pr\\left(\\sum_{l=1}^{n(\\alpha - b+1 ) } i(x_l;y_l)\\leq \\alpha \\cdot \\log m_n\\right)\\\\ & \\overset{(a)}{\\leq } \\pr\\left(\\sum_{l=1}^{n(\\alpha - b+1 ) } i(x_l;y_l)\\leq ( \\alpha - b+1)(nc - l\\sqrt{n})\\right)\\\\ & \\overset{(b)}{\\leq } q\\left(\\frac{\\sqrt{\\alpha - b+1}}{\\sqrt{v}}l\\right)+\\frac{\\tau_1}{\\sqrt{(\\alpha - b+1)n } } \\label{eqn : berry_1}\\end{aligned}\\ ] ] for some non - negative constant @xmath249 that is dependent only on the input distribution @xmath92 and the channel statistics @xmath136 , where @xmath250 s are i.i.d .",
    "random variables each generated according to @xmath251 , @xmath44 is from the choice of @xmath67 in , and @xmath252 is from the berry - esseen theorem ( e.g. , @xcite ) . similarly , we can show @xmath253 and @xmath254    next , @xmath255 is upper - bounded as follows : @xmath256}(g^{\\alpha } ) , \\mathbf{y}_{[b:\\alpha ] } ) >   \\alpha \\cdot \\log m_n \\mbox { for some } g^{\\alpha } \\mbox { such that } g^{b } \\neq g^b)\\\\ & \\leq   m_n^{\\alpha } \\cdot \\pr\\left(\\sum_{l=1}^{n(\\alpha - b+1 ) } i(x_l;\\bar{y}_l ) >",
    "\\alpha\\cdot \\log m_n\\right)\\\\ & \\overset{(a)}{= } m_n^{\\alpha}\\cdot \\e\\left[\\exp\\left\\{-\\sum_{l=1}^{n(\\alpha - b+1 ) } i(x_l;y_l)\\right\\}\\right .",
    "\\cdot \\left.\\mathbbm{1}\\big\\{\\sum_{l=1}^{n(\\alpha - b+1 ) } i(x_l;y_l)>\\alpha \\log m_n\\big\\}\\right]\\\\ & \\overset{(b)}{\\leq } \\frac{\\tau_2}{\\sqrt{(\\alpha - b+1)n } } \\ ] ] for some non - negative constant @xmath257 that is dependent only on the input distribution @xmath92 and channel statistics @xmath136 , where @xmath129 s are i.i.d .",
    "random variables each generated according to @xmath251 @xmath258 , @xmath44 is due to an elementary chain of equalities given in appendix [ appendix : chain ] , and @xmath252 is from ( * ? ? ?",
    "* lemma 47 ) .",
    "similarly , we can show @xmath259 and @xmath260    by substituting the above bounds into the rhs of , we obtain @xmath137\\cr & \\leq \\sum_{j = b}^{a - b+1 } \\left(q\\left(\\frac{\\sqrt{\\alpha - j+1}}{\\sqrt{v}}l\\right)+\\frac{\\tau_1+\\tau_2}{\\sqrt{(\\alpha - j+1)n } } \\right)\\cr & \\qquad\\qquad+\\sum_{j = a - b+2}^{k } \\left(q\\left(\\frac{\\sqrt{t_k - j+1}}{\\sqrt{v}}l\\right)+\\frac{\\tau_1+\\tau_2}{\\sqrt{(t_k - j+1)n}}\\right ) \\\\ & \\leq \\sum_{j=\\alpha - a+b}^{\\alpha - b+1 } \\left(q\\left(\\frac{\\sqrt{j}}{\\sqrt{v}}l\\right)+\\frac{\\tau_1+\\tau_2}{\\sqrt{jn } } \\right)+\\sum_{j = t}^{t_k - a+b-1 } \\left(q\\left(\\frac{\\sqrt{j}}{\\sqrt{v}}l\\right)+\\frac{\\tau_1+\\tau_2}{\\sqrt{jn}}\\right ) \\label{eqn : sumall}\\\\ & \\overset{(a)}{\\leq } \\sum_{j = b}^{a - b+1 } \\left(q\\left(\\frac{\\sqrt{j}}{\\sqrt{v}}l\\right)+\\frac{\\tau_1+\\tau_2}{\\sqrt{jn } } \\right)+\\sum_{j = t}^{a - b+t } \\left(q\\left(\\frac{\\sqrt{j}}{\\sqrt{v}}l\\right)+\\frac{\\tau_1+\\tau_2}{\\sqrt{jn}}\\right),\\label{eqn : sumall_ub}\\end{aligned}\\ ] ] where @xmath44 is because if @xmath261 , which implies @xmath262 , the rhs of is upper - bounded as follows : @xmath263 and if @xmath264 , which implies @xmath265 , the rhs of is upper - bounded as follows : @xmath266 now , the rhs of is bounded as follows : @xmath267 where @xmath44 is from lemma [ lemma : sum_integral ] ( with the identification of @xmath268 ) , which is relegated to the end of this subsection , and @xmath252 is obtained by applying similar steps as in the proof of corollary [ coro : cl_asymp ] .",
    "can be obtained by replacing @xmath0 by @xmath229 in the rhs of . ]",
    "now let us choose @xmath269 and @xmath270 for @xmath271 . by substituting this choice of @xmath163 and @xmath229 into the rhs of and the rhs of",
    ", we obtain @xmath272 and @xmath126\\leq \\sum_{j = t}^{\\infty } q\\left(\\frac{\\sqrt{j}}{\\sqrt{v}}l\\right)+o(n^{-\\delta/2 } ) , \\label{eqn : ensemble_cl}\\end{aligned}\\ ] ] respectively . due to the symmetry of the decoding procedure , the bound holds for @xmath189 for @xmath242 . for @xmath188 , by defining the error events in the same way as for the moderate deviations regime and then applying similar bounding techniques used in the above , it can be verified that @xmath126&\\leq \\sum_{j = t}^{t_k } q\\left(\\frac{\\sqrt{j}}{\\sqrt{v}}l\\right)+\\frac{\\tau_1+\\tau_2}{\\sqrt{jn}}\\\\ & \\leq   \\sum_{j = t}^{a - b+t } q\\left(\\frac{\\sqrt{j}}{\\sqrt{v}}l\\right)+\\frac{\\tau_1+\\tau_2}{\\sqrt{jn } } \\\\ & \\leq \\sum_{j = t}^{\\infty } q\\left(\\frac{\\sqrt{j}}{\\sqrt{v}}l\\right)+o(n^{-\\delta/2}).\\end{aligned}\\ ] ] hence , there must exist a sequence of codes @xmath95 that satisfies and , which completes the proof .",
    "the following basic lemma is used in the proof of theorem [ thm : cl ] , whose proof is omitted .",
    "[ lemma : sum_integral ] assume two integers @xmath273 and @xmath274 such that @xmath275 .",
    "if @xmath276 is monotonically decreasing and integrable on @xmath277 $ ] , we have @xmath278 where @xmath279 denotes the antiderivative of @xmath276 .",
    "in this section , we explore interesting variations of the basic streaming setup in section [ sec : model ] . for the brevity of the results , we focus on the moderate deviations regime .",
    "consider the scenario where there is an erasure option at the decoder , i.e. , the decoder can output an erasure symbol instead of a message estimate . in the presence of an erasure option",
    ", there are two types of error events : ( i ) the decoder declares an erasure and ( ii ) the decoder outputs an incorrect message , not an erasure . in many applications , the undetected error ( the latter event ) is more undesirable than an erasure ( the former event ) . in the following ,",
    "we define a streaming code with an erasure option by taking into account the undetected error and the total error probabilities separately .",
    "an @xmath280-streaming code with an erasure option consists of    * a sequence of messages @xmath48 each distributed uniformly over @xmath49 $ ] , * a sequence of encoding functions @xmath50 that maps the message sequence @xmath51 to the channel input codeword @xmath52 , and * a sequence of decoding functions @xmath281 that maps the channel output sequences @xmath54 to a message estimate @xmath282 or an erasure symbol @xmath283 ,    that satisfies @xmath56 i.e. , the total error probability does not exceed @xmath57 , and @xmath284 i.e. , the undetected error probability does not exceed @xmath285 .",
    "the following theorem presents upper bounds on the undetected error and the total error probabilities .",
    "the proof of this theorem is provided in appendix [ appendix : erasure ] .",
    "[ thm : erasure ] consider a dmc @xmath26 with @xmath66 and any sequence of integers @xmath67 such that @xmath68 , where @xmath69 and @xmath70 .",
    "for any @xmath286 , there exists a sequence of @xmath287-streaming codes with an erasure option such that @xmath288    theorem [ thm : erasure ] indicates that for our proposed scheme , the undetected error probability decays much faster than the total error probability , i.e. , the exponent of the undetected error probability is the order of @xmath289 , whereas that of the total error probability is the order of @xmath290 .",
    "we note that when @xmath5 and @xmath291 for @xmath292 and @xmath293 , theorem [ thm : erasure ] reduces to ( * ? ? ?",
    "* theorem 1 ) . in the streaming setup",
    ", both the exponents of the total error and the undetected error probabilities improve over the block coding or non - streaming setup in ( * ? ? ?",
    "* theorem 1 ) by factors of @xmath0 .",
    "we note that the decoding delay is assumed to be fixed to @xmath0 up to this point . in this subsection",
    ", we relax this constraint by requiring the _ average _ decoding delay not to exceed @xmath0 .",
    "a streaming code with average delay constraint is defined as follows :    an @xmath294-streaming code with average delay constraint consists of    * a sequence of messages @xmath48 each distributed uniformly over @xmath49 $ ] , * a sequence of encoding functions @xmath50 that maps the message sequence @xmath51 to the channel input codeword @xmath52 , and * a sequence of decoding functions @xmath295 that maps the channel output sequences @xmath296 to a message estimate @xmath297 or an erasure symbol @xmath298 for every @xmath127 $ ]    that satisfies @xmath299 and @xmath300}{n}\\leq t,\\end{aligned}\\ ] ] where @xmath301 for @xmath59 denotes the ( random ) decoding delay of the @xmath63-th message . is required to be decoded at the end of every block on and after the @xmath63-th block in this definition .",
    "one may wonder why the decoder does not stop decoding @xmath60 after it outputs an estimate of @xmath60 , not an erasure .",
    "we note that our definition includes such a operation as a special case by letting the decoder simply fix the estimate of @xmath60 once it outputs a message estimate . ]    for block channel coding with feedback , it is known that the error exponent can be significantly improved by allowing variable decoding delay , e.g. , @xcite . for streaming setup , the following theorem , which is proved in appendix  [ appendix : variable ] , shows that such an improvement can be obtained in the absence of feedback .",
    "[ thm : variable ] consider a dmc @xmath26 with @xmath66 and any sequence of integers @xmath67 such that @xmath68 , where @xmath69 and @xmath70 .",
    "for any @xmath302 , there exists a sequence of @xmath303-streaming codes with average delay constraint such that @xmath304    we note that the exponent of the error probability @xmath305 is of the order @xmath289 ( instead of @xmath290 as in ) , and hence it is improved tremendously by allowing variable decoding delay .",
    "we note that the rates of the messages are assumed to be fixed across time thus far . in many practical streaming applications ,",
    "however , a stream of data packets does not have a constant rate .",
    "for example , in the mpec standard for video coding , i frames have higher rates than p frames in general .",
    "similarly , in audio coding , voice packets have higher rates than silent packets . in this subsection , to obtain useful insights when the message rates vary across time , we assume a simple example where the rate of the messages in odd block indices and the rate of the messages in even block indices are different . a streaming code with alternating message rates is defined as follows :    an @xmath306-streaming code with alternating message rates . ]",
    "consists of    * a sequence of messages @xmath48 where message @xmath307 for @xmath110 is distributed uniformly over @xmath308 $ ] and message @xmath309 for @xmath110 is distributed uniformly over @xmath310 $ ] , * a sequence of encoding functions @xmath311 that maps the message sequence @xmath312 \\ } \\cup \\{g_{2j } : j\\in [ 1:\\lfloor k/2 \\rfloor ] \\ }",
    "\\in   \\mathcal{g}_1^{\\lceil k/2 \\rceil}\\times \\mathcal{g}_2^{\\lfloor k/2 \\rfloor } $ ] to the channel input codeword @xmath52 , and * a sequence of decoding functions @xmath313 that maps the channel output sequences @xmath54 to the message estimate @xmath314 ,    that satisfies @xmath315 and @xmath316    we note that the error probabilities are considered separately for the messages in the odd block indices and for the messages in the even block indices .",
    "the following theorem , which is proved in appendix [ appendix : alternate ] , gives achievability bounds when the message rates are alternating .",
    "[ thm : alternate ] consider a dmc @xmath26 with @xmath66 and any sequence of integers @xmath67 such that @xmath68 , where @xmath69 and @xmath70 .",
    "for any @xmath317 , there exists a sequence of @xmath318-streaming codes with alternating message rates such that @xmath319 if @xmath0 is odd , and @xmath320 if @xmath0 is even .",
    "theorem [ thm : alternate ] indicates that for our coding strategy , the average of the moderate deviations constants does not change even if the rates are alternating .",
    "we note that the moderate deviations constants of the odd and even messages are asymmetric if @xmath0 is odd and they are symmetric otherwise . to illustrate the intuition behind this , let us consider the most recent @xmath0 blocks , which dominates the error probability .",
    "if @xmath0 is odd , the average message rate in those @xmath0 blocks depends on whether the current target message index is odd or even and this leads to the asymmetry . on the other hand ,",
    "if @xmath0 is even , it is fixed to @xmath321 regardless of the current target message index .",
    "more details are provided in appendix [ appendix : alternate ] .",
    "in this paper , we studied the fundamental interplay between the rate and error probability for a streaming setup with a decoding delay of @xmath0 blocks . in the moderate deviations regime",
    ", the moderate deviations constant was shown to improve by at least a factor of @xmath0 .",
    "we proposed a coding technique with infinite memory such that all the previous and fresh messages are jointly encoded in each block . on the other hand , in the central limit regime",
    ", the second - order coding rate was shown to improve by approximately a factor of @xmath1 for a wide range of channel parameters . to ensure that the summation of berry - esseen constants ( e.g. , the last terms in the rhs of - ) does not diverge in the error analysis , we proposed a coding technique with truncated memory such that the encoding and decoding memories do not grow with the block index . furthermore , we generalized the moderate deviations result in various directions .",
    "we first considered a scenario with an erasure option at the decoder and showed that both the exponents of the total error and the undetected error probabilities improve by factors of @xmath0 . by utilizing the erasure option",
    ", we showed that the exponent of the total error probability can be improved to that of the undetected error probability ( in the order sense ) at the expense of a variable decoding delay .",
    "finally , we considered a scenario where the message rates are alternating and showed that the same average moderate deviations constant as the case of constant rate can be obtained .",
    "we note that all of our encoding strategies do not depend on @xmath0 .",
    "hence , our coding techniques are directly applicable for multicast scenario where a sender transmits a common stream of data packets to multiple receivers with possibly different decoding constraints .",
    "let us conclude with a final remark on proving a converse in our streaming setup .",
    "our problem appears to be closely related to the bit - wise unequal protection ( uep ) problem in the sense that we need to capture the tension that arises when a common channel is used for more than one messages with individual error criteria .",
    "for the seemingly simpler bit - wise uep problem @xcite for the block channel coding with the _ same _ decoding deadline , however , tight characterizations of various asymptotic fundamental limits ( e.g. , error exponents ) remain challenging open problems in general .",
    "this indicates that a highly - nontrivial converse technique , perhaps along the lines of sahai s work @xcite , would be needed for our streaming setup where the messages have _ different _ decoding deadlines .",
    "let @xmath322 . note that corollary [ coro : cl_asymp ]",
    "is proved if we show @xmath323 to that end , we use the following bounds on the @xmath324-function : @xmath325 where @xmath326",
    ". then , we have @xmath327 which completes the proof .",
    "fix @xmath328 and @xmath329 . then , we have @xmath330   \\\\   & \\overset{(b)}{=}\\exp\\left\\{-n \\left(s{\\varepsilon}_n-\\log\\e [ \\exp\\{sz_1\\}]\\right)\\right\\ } \\\\     & = \\exp\\left\\{-n \\left ( s{\\varepsilon}_n - h(s)\\right)\\right\\ } \\label{eqn : chernoff } .\\end{aligned}\\ ] ] where @xmath44 follows from markov s inequality and @xmath252 follows from the independence of @xmath331 s .",
    "the third - order taylor series expansion of the cumulant generating function @xmath332 can be written as @xmath333 for some @xmath334 $ ] .",
    "it is easy to check that @xmath335 , @xmath336 = 0 $ ] and @xmath337=\\sigma^2 $ ] .",
    "now , we take @xmath338 plugging this into and yields @xmath339 where the final inequality holds for all @xmath2 sufficiently large since @xmath147 and @xmath340 as @xmath341 and thus @xmath342 .",
    "the following chain of equalities is used in the proof theorem [ thm : cl ] .",
    "\\label{eqn : chain4}\\end{aligned}\\ ] ]",
    "consider a dmc @xmath26 with @xmath66 and any sequence of integers @xmath67 such that @xmath68 , where @xmath69 and @xmath70 .",
    "we denote by @xmath92 an input distribution that achieves the dispersion .",
    "fix @xmath286 .",
    "the encoding procedure is the same as that for the basic streaming setup in section [ subsec : md_pf ] .",
    "let us consider the decoding of @xmath60 at the end of block @xmath99 .",
    "the decoding procedure is modified from that for the basic streaming setup in section [ subsec : md_pf ] as follows :    * the decoding test is modified as follows : @xmath115}(\\hat{g}_{t_k , [ 1:j-1 ] } , g_{[j : t_k ] } ) , \\mathbf{y}_{[j : t_k]})>(t_k - j+1 ) \\cdot ( \\log m_n+ \\gamma n\\rho_n ) ,   \\label{eqn : dec_rule_erasure}\\end{aligned}\\ ] ] i.e. , the threshold value is increased proportional to @xmath344 . *",
    "if there is none or more than one @xmath118 that satisfies the decoding test for some @xmath116}$ ] , the decoder declares an erasure , i.e. , @xmath345 , and terminates the decoding procedure .",
    "similarly as in section [ subsec : md_pf ] , we first consider the probability of error averaged over random codebook @xmath95 .",
    "the error event @xmath123 for @xmath59 happens only if at least one of the following @xmath124 events occurs : @xmath346}(g^{t_k } ) , \\mathbf{y}_{[j : t_k]})\\leq ( t_k - j+1 ) \\cdot ( \\log m_n+ \\gamma n\\rho_n ) \\},~ j\\in [ 1:k ] \\label{eqn : err1_ers}\\\\ \\tilde{\\mathcal{e}}_{k , j}'&:= \\{i(\\mathbf{x}_{[j : t_k]}(g^{j-1},g_{[j : t_k ] } ) , \\mathbf{y}_{[j : t_k ] } ) > ( t_k - j+1 ) \\cdot ( \\log m_n+ \\gamma n\\rho_n ) \\cr & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\mbox { for some } g_{[j : t_k ] } \\mbox { such that } g_{j } \\neq g_{j } \\},~ j\\in [ 1:k ] .",
    "\\label{eqn : err2_ers}\\end{aligned}\\ ] ] we note that and are obtained by replacing @xmath321 by @xmath347 in and , respectively .",
    "then , we have @xmath126&\\leq \\sum_{j=1}^k \\left(\\pr(\\mathcal{e}_{k , j}')+\\pr(\\tilde{\\mathcal{e}}_{k , j } ' ) \\right).\\end{aligned}\\ ] ] on the other hand , the undetected error event @xmath348 has the following relationship : @xmath349}\\neq g_{[1:k ] } , \\hat{g}_k\\neq 0\\ } \\\\ & = \\cup_{j\\in [ 1:k]}\\{\\hat{g}_{t_k , [ 1:j-1]}= g_{[1:j-1 ] } , \\hat{g}_{t_k , j}\\neq g_{j } , \\hat{g}_k\\neq 0\\}. \\end{aligned}\\ ] ]    hence , the undetected error probability is bounded as follows : @xmath350&\\leq \\sum_{j=1}^k \\pr(\\hat{g}_{t_k , [ 1:j-1]}= g_{[1:j-1 ] } , \\hat{g}_{t_k , j}\\neq g_{j } , \\hat{g}_k\\neq 0)\\\\ & \\leq \\sum_{j=1}^k \\pr(\\tilde{\\mathcal{e}}_{k , j}').\\end{aligned}\\ ] ] now , for @xmath127 $ ] , let us bound @xmath351 and @xmath352 .",
    "similarly as in section [ subsec : md_pf ] , @xmath129 s denote i.i.d .",
    "random variables each generated according to @xmath251@xmath258 in the following .",
    "first , we have @xmath353 for sufficiently large @xmath2 , where @xmath134 is some non - negative constant dependent only on the input distribution @xmath135 and channel statistics @xmath136 and @xmath44 is from lemma  [ lemma : nonasymptotic_md ] in section [ subsec : md_pf ] .",
    "next , we have @xmath354\\\\ & \\leq m_n^{t_k - j+1}\\cdot \\exp\\left\\{-(t_k - j+1 ) \\cdot ( \\log m_n+\\gamma n\\rho_n)\\right\\}\\\\ & = \\exp\\left\\{-(t_k - j+1)\\gamma n\\rho_n\\right\\},\\end{aligned}\\ ] ] where @xmath44 is obtained by applying a chain of equalities similar to that in appendix [ appendix : chain ] .    hence",
    ", we obtain @xmath126&\\leq \\sum_{j=1}^k \\big ( \\exp\\left\\{-(t_k - j+1)n\\rho_n^2(1-\\gamma)^2\\left(\\frac{1}{2v}-(1-\\gamma ) \\rho_n \\tau\\right)\\right\\}\\cr & \\qquad \\qquad \\qquad + \\exp\\left\\{-(t_k - j+1)n\\gamma\\rho_n\\right\\ } \\big)\\\\ & \\leq \\sum_{j = t}^{t_k } \\left ( \\exp\\left\\{-jn\\rho_n^2(1-\\gamma)^2\\left(\\frac{1 } { 2v}-(1-\\gamma ) \\rho_n \\tau\\right)\\right\\}+\\exp\\left\\{-jn\\gamma \\rho_n\\right\\}\\right ) \\\\ & \\leq \\frac{\\exp\\left\\{-tn\\rho_n^2(1-\\gamma)^2\\left(\\frac{1 } { 2v}-(1-\\gamma)\\rho_n \\tau\\right)\\right\\}}{1-\\exp\\{-n\\rho_n^2(1-\\gamma)^2\\left(\\frac{1 } { 2v}-(1-\\gamma ) \\rho_n \\tau\\right)\\}}+\\frac{\\exp\\left\\{-tn\\gamma\\rho_n\\right\\}}{1-\\exp\\left\\{-n\\gamma \\rho_n\\right\\ } } \\label{eqn : averg_total}\\end{aligned}\\ ] ] and @xmath350&\\leq\\frac{\\exp\\left\\{-tn\\gamma\\rho_n\\right\\}}{1-\\exp\\left\\{-n\\gamma \\rho_n\\right\\ } } \\label{eqn : averg_undetec}\\end{aligned}\\ ] ] for sufficiently large @xmath2 .    to show the existence of a deterministic code , we apply markov s inequality as follows : @xmath355    } { n}\\right)<\\frac{1}{2 } \\\\ & \\pr\\left(\\limsup_{n\\rightarrow \\infty } \\sum_{k=1}^n\\frac{\\pr(\\hat{g}_k\\neq g_k , \\hat{g}_k\\neq 0|\\mathcal{c}_n)}{n}>2\\limsup_{n\\rightarrow \\infty } \\sum_{k=1}^n\\frac{\\e_{\\mathcal{c}_n}[\\pr(\\hat{g}_k\\neq g_k , \\hat{g}_k\\neq 0|\\mathcal{c}_n)]}{n }    \\right)<\\frac{1}{2}. \\end{aligned}\\ ] ] then ,",
    "from the union bound , we have @xmath356    } { n } \\mbox { or } \\cr & \\limsup_{n\\rightarrow \\infty } \\sum_{k=1}^n\\frac{\\pr(\\hat{g}_k\\neq g_k , \\hat{g}_k\\neq 0|\\mathcal{c}_n)}{n}>2\\limsup_{n\\rightarrow \\infty } \\sum_{k=1}^n\\frac{\\e_{\\mathcal{c}_n}[\\pr(\\hat{g}_k\\neq g_k , \\hat{g}_k\\neq 0|\\mathcal{c}_n)]}{n }    \\big)<1.\\end{aligned}\\ ] ] therefore , there must exist a sequence of codes @xmath95 that satisfies @xmath357 and @xmath358 which completes the proof .      consider a dmc @xmath26 with @xmath66 and any sequence of integers @xmath67 such that @xmath68 , where @xmath69 and @xmath70 .",
    "we denote by @xmath92 an input distribution that achieves the dispersion .",
    "fix @xmath302 and @xmath286 .",
    "the encoding procedure is the same as that for the basic streaming setup in section [ subsec : md_pf ] .",
    "let us consider the decoding of message @xmath60 at the end of block @xmath359 for @xmath360 . at the end of _",
    "every _ block @xmath359 for @xmath361 . ]",
    "if @xmath362 $ ] , the decoder outputs @xmath363 . for @xmath364 ,",
    "the decoder outputs a message estimate @xmath365 or an erasure symbol @xmath366 according to the same decoding rule illustrated in appendix [ appendix : erasure ] with delay @xmath367 .",
    "then , the error probability of @xmath60 after the random decoding delay @xmath368 ( averaged over the random codebook generation ) is bounded as follows : @xmath369&=\\sum_{d = t}^{\\infty } \\e_{\\mathcal{c}_n}\\big[\\pr(d_k = d , \\hat{g}_{k+d-1,k}\\neq g_{k } , \\hat{g}_{k+d-1,k}\\neq 0|\\mathcal{c}_n)\\big]\\\\ & \\leq \\sum_{d = t}^{\\infty } \\e_{\\mathcal{c}_n}\\big[\\pr(\\hat{g}_{k+d-1,k}\\neq g_{k } , \\hat{g}_{k+d-1,k}\\neq 0|\\mathcal{c}_n)\\big]\\\\ & \\overset{(a)}{\\leq } \\sum_{d = t}^{\\infty}\\frac{\\exp\\left\\{-dn\\gamma \\rho_n\\right\\}}{1-\\exp\\left\\{-n\\gamma \\rho_n\\right\\}}\\\\ & \\leq \\frac{\\exp\\left\\{-tn\\gamma\\rho_n\\right\\}}{(1-\\exp\\left\\{-n\\gamma\\rho_n\\right\\})^2},\\end{aligned}\\ ] ] where @xmath44 is from the upper bound on the undetected error probability with delay @xmath367 .    on the other hand ,",
    "the excess of delay averaged over the random codebook generation is bounded as @xmath370 & =   \\pr(d_k = t+1|\\mathcal{c}_n)+2 \\pr(d_k = t+2|\\mathcal{c}_n)+\\cdots\\\\ & = \\pr(\\hat{g}_{k+t-1,k}=0 , \\hat{g}_{k+t , k}\\neq 0|\\mathcal{c}_n)\\cr & \\qquad \\qquad \\qquad+2 \\pr(\\hat{g}_{k+t-1,k}=0 , \\hat{g}_{k+t , k}=0 ,   \\hat{g}_{k+t+1,k}\\neq 0|\\mathcal{c}_n)+\\cdots\\\\ & \\leq \\sum_{d = t+1}^{\\infty } ( d - t)\\cdot \\pr\\left(\\hat{g}_{k+d-2,k } = 0|\\mathcal{c}_n\\right ) \\\\ & \\leq \\sum_{d = t+1}^{\\infty } ( d - t)\\cdot \\pr \\left(\\hat{g}_{k+d-2,k } \\neq g_k|\\mathcal{c}_n\\right)\\\\ & \\overset{(a)}{\\leq } \\sum_{d = t+1}^{\\infty } ( d - t)\\cdot \\big ( \\frac{\\exp\\left\\{-(d-1)n\\rho_n^2(1-\\gamma)^2\\left(\\frac{1 } { 2v}-(1-\\gamma ) \\rho_n \\tau\\right)\\right\\}}{1-\\exp\\{-n\\rho_n^2(1-\\gamma)^2\\left(\\frac{1 } { 2v}-(1-\\gamma ) \\rho_n \\tau\\right)\\}}\\cr & \\qquad\\qquad\\qquad\\qquad\\qquad",
    "+ \\frac{\\exp\\left\\{-(d-1)n\\gamma\\rho_n\\right\\}}{1-\\exp\\left\\{-n\\gamma \\rho_n\\right\\}}\\big),\\label{eqn : delay_ub}\\end{aligned}\\ ] ] where @xmath44 is from the upper bound on the total error probability with delay @xmath371 .    by following similar statements using markov s inequality in the proof of theorem [ thm : erasure",
    "] , we can obtain @xmath372    } { n}\\cr & \\qquad\\qquad\\qquad\\qquad \\mbox { or } \\limsup_{n\\rightarrow \\infty } \\sum_{k=1}^n\\frac{\\e[d_k|\\mathcal{c}_n]}{n}-t>2\\limsup_{n\\rightarrow \\infty } \\sum_{k=1}^n\\frac{\\e_{\\mathcal{c}_n}[d_k - t|\\mathcal{c}_n]}{n }    \\big)<1.\\end{aligned}\\ ] ] therefore , there must exist a sequence of codes @xmath95 that satisfies @xmath373 and tends to infinity . ]",
    "@xmath374}{n}\\leq   t+o(1).\\end{aligned}\\ ] ] we note that implies @xmath375 by taking @xmath376 , this completes the proof .      consider a dmc @xmath26 with @xmath66 and any sequence of integers @xmath67 such that @xmath68 , where @xmath69 and @xmath70 .",
    "we denote by @xmath92 an input distribution that achieves the dispersion .",
    "fix @xmath377 .",
    "for each @xmath59 and @xmath378 , generate @xmath94 in an i.i.d .",
    "manner according to @xmath92 .",
    "the generated codewords constitute the codebook @xmath95 . in block",
    "@xmath63 , after observing the true message sequence @xmath62 , the encoder sends @xmath96 .",
    "consider the decoding of @xmath60 at the end of block @xmath97 .",
    "similarly as in the decoding procedure in section [ subsec : md_pf ] for the basic streaming setup , the decoder not only decodes @xmath60 , but also re - decodes @xmath98 at the end of block @xmath99 .",
    "let @xmath103 denote the estimate of @xmath104 at the end of block @xmath99 . for alternating message rates ,",
    "we modify the decoding procedure for the basic streaming setup in section [ subsec : md_pf ] according to the following rules and the messages in even block indices have a higher rate of @xmath379 because @xmath377 . ] :    * in the basic streaming setup , we consider the window of blocks @xmath380 $ ] for the decoding of @xmath104 for @xmath127 $ ] as shown in .",
    "note that the last block index in each decoding window is @xmath99 . for alternating message rates",
    ", we choose the last block index to be an odd number , to avoid the average message rate in each decoding window of blocks exceeding the capacity .",
    "hence , the last block index in each decoding window is chosen to be @xmath99 , if both @xmath63 and @xmath0 are odd numbers or if both @xmath63 and @xmath0 are even numbers .",
    "otherwise , it is chosen to be @xmath381 . * in the basic streaming setup , we note that the error event related to the smallest decoding window dominates the error probability . for alternating message rates ,",
    "if the smallest decoding window ( after the last block index is chosen to be an odd number ) consists of an odd number of blocks , the average message rate in that window is away from the capacity .",
    "hence , in that case , we adjust the threshold in the decoding test to ensure that the contribution of the smallest decoding window to the error probability is negligible .        if @xmath63 is also an odd number , the decoding procedure is the same as that for the basic streaming setup in section [ subsec : md_pf ] , except that when @xmath106 , we perform the following decoding test instead of : @xmath382}(\\hat{g}_{t_k , [ 1:k-1 ] } , g_{[k : t_k ] } ) , \\mathbf{y}_{[k : t_k ] } ) & > ( t+r-1 ) \\cdot \\log m_n . \\end{aligned}\\",
    "] ] if @xmath63 is an even number , the decoding procedure is the same as that for the basic streaming setup in section [ subsec : md_pf ] , except that the last block index @xmath99 in each decoding window is replaced by @xmath383 .",
    "if @xmath63 is an odd number , the decoding procedure is the same as that for the basic streaming setup in section [ subsec : md_pf ] , except that the last block index @xmath99 in each decoding window is replaced by @xmath383 and when @xmath106 , we perform the following decoding test : @xmath384}(\\hat{g}_{t_k , [ 1:k-1 ] } , g_{[k : t_k-1 ] } ) , \\mathbf{y}_{[k : t_k-1 ] } ) & > ( t+r-2 ) \\cdot \\log m_n . \\end{aligned}\\ ] ] if @xmath63 is an even number , the decoding procedure is the same as that for the basic streaming setup in section [ subsec : md_pf ] .",
    "let us first consider the case where both @xmath0 and @xmath63 are odd numbers .",
    "the error event @xmath123 happens only if at least one of the following @xmath124 events occurs : @xmath385}(g^{t_k } ) , \\mathbf{y}_{[j : t_k]})\\leq ( t_k - j+1 ) \\cdot",
    "\\log m_n \\},~ j\\in [ 1:k-1 ] \\label{eqn : err1_al}\\\\ \\tilde{\\mathcal{e}}_{k ,",
    "j}''&:= \\{i(\\mathbf{x}_{[j : t_k]}(g^{j-1},g_{[j : t_k ] } ) , \\mathbf{y}_{[j : t_k ] } ) > ( t_k - j+1 ) \\cdot",
    "\\log m_n \\cr & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\mbox { for some } g_{[j : t_k ] } \\mbox { such that } g_{j } \\neq g_{j } \\},~ j\\in [ 1:k-1 ] \\label{eqn :",
    "err2_al}\\\\ \\mathcal{e}_{k , k}''&:= \\ { i(\\mathbf{x}_{[k : t_k]}(g^{t_k } ) , \\mathbf{y}_{[k : t_k]})\\leq ( t+r-1 ) \\cdot \\log m_n \\}\\\\ \\tilde{\\mathcal{e}}_{k , k}''&:= \\{i(\\mathbf{x}_{[k : t_k]}(g^{k-1},g_{[k : t_k ] } ) , \\mathbf{y}_{[k : t_k ] } ) > ( t+r-1 ) \\cdot \\log m_n \\cr & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\mbox { for some } g_{[k : t_k ] } \\mbox { such that } g_{k } \\neq g_{k } \\}.\\end{aligned}\\ ] ] now , we have @xmath126&\\leq \\sum_{j=1}^{k } \\left(\\pr(\\mathcal{e}_{k , j}'')+\\pr(\\tilde{\\mathcal{e}}_{k , j } '' ) \\right).\\end{aligned}\\ ] ] fix arbitrary @xmath131 . by applying similar steps as in the error analysis for the basic streaming setup in section [ subsec : md_pf ]",
    ", we can show by @xmath386 in the rhs of . ]",
    "@xmath387 for sufficiently large @xmath2 , where @xmath134 is some non - negative constant dependent only on the input distribution @xmath135 and channel statistics @xmath136 .",
    "now , @xmath388 is bounded as follows : @xmath389^+\\right\\}\\right]\\\\ & \\overset{(b)}{\\leq } \\pr\\left(\\sum_{l=1}^{nt } i(x_l;y_l)\\leq ( t+r-1)n(c-\\lambda\\rho_n)\\right)+\\exp\\left\\{-(t+r-1)n(1-\\lambda)\\rho_n\\right\\}\\\\ & \\leq \\pr\\left(\\sum_{l=1}^{nt } i(x_l;y_l)\\leq ( t+r-1)nc\\right)+\\exp\\left\\{-(t+r-1)n(1-\\lambda)\\rho_n\\right\\}\\\\ & \\overset{(c)}{\\leq}\\exp\\left\\{-tn\\tau'\\right\\}+\\exp\\left\\{-(t+r-1)n(1-\\lambda)\\rho_n\\right\\}\\end{aligned}\\ ] ] for some constant @xmath390 that depends only on the input distribution @xmath135 and channel statistics @xmath136 , where @xmath129 s are i.i.d .",
    "random variables each generated according to @xmath130 , @xmath44 is from the identity ( * ? ? ?",
    "* eq .  ( 69 ) ) , @xmath252 follows from similar steps as - , and @xmath391 is from lemma [ lemma : central ] that is relegated to the end of this appendix .",
    "thus , we obtain @xmath126&\\leq\\frac{\\exp\\left\\{-(t+1)n\\rho_n^2\\lambda^2\\left(\\frac{1 } { 2v}-\\lambda \\rho_n \\tau\\right)\\right\\}}{1-\\exp\\{-n\\rho_n^2\\lambda^2\\left(\\frac{1 } { 2v}-\\lambda \\rho_n \\tau\\right)\\}}+\\frac{\\exp\\left\\{-(t+1)n(1-\\lambda)\\rho_n\\right\\}}{1-\\exp\\left\\{-n(1-\\lambda)\\rho_n\\right\\}}\\cr & \\qquad\\qquad+\\exp\\left\\{-tn\\tau'\\right\\}+\\exp\\left\\{-(t+r-1)n(1-\\lambda)\\rho_n\\right\\}\\end{aligned}\\ ] ] when both @xmath0 and @xmath63 are odd numbers .",
    "now , let us consider the case where @xmath0 is an odd number and @xmath63 is an even number .",
    "we remind that the decoding procedure is the same as that for the basic streaming setup in section [ subsec : md_pf ] , except that @xmath99 is replaced by @xmath383 . by applying similar steps as in the error analysis for the basic streaming setup in section [ subsec : md_pf ]",
    ", we can show by @xmath392 in the rhs of . ]",
    "\\frac{\\exp\\left\\{-(t-1)n\\rho_n^2\\lambda^2\\left(\\frac{1 } { 2v}-\\lambda \\rho_n \\tau\\right)\\right\\}}{1-\\exp\\{-n\\rho_n^2\\lambda^2\\left(\\frac{1 } { 2v}-\\lambda \\rho_n \\tau\\right)\\}}+\\frac{\\exp\\left\\{-(t-1)n(1-\\lambda)\\rho_n\\right\\}}{1-\\exp\\left\\{-n(1-\\lambda)\\rho_n\\right\\ } } \\label{eqn : md_sumup_al23}\\end{aligned}\\ ] ] for sufficiently large @xmath2 .    by following similar statements using markov s inequality in the proof of theorem [ thm : erasure",
    "] , we can show that there must exist a sequence of codes @xmath95 that satisfies @xmath393 and @xmath394 which completes the proof for the case where @xmath0 is an odd number , by taking @xmath139 .",
    "the proof for the case where @xmath0 is an even number can be done in a similar manner , and hence it is omitted .",
    "[ lemma : central ] let @xmath140 be a sequence of i.i.d .",
    "random variables with zero mean such that its cumulant generating function @xmath143 $ ] for @xmath144 is continuously differentiable . for @xmath395 ,",
    "the following bound holds : @xmath396 where @xmath397 is the _ rate function _ defined as follows : @xmath398    for any @xmath399 , we obtain @xmath400 by applying the same steps used to obtain in the proof of lemma [ lemma : nonasymptotic_md ] . since @xmath399 is arbitrary ,",
    "we obtain the following bound @xmath401 furthermore , because @xmath402 and @xmath403 , we conclude that @xmath404 .",
    "s.  c. draper and a.  khisti , `` truncated tree codes for streaming data : infinite - memory reliability using finite memory , '' in _ proc . international symposium on wireless communication systems ( iswcs ) _ , nov .",
    "2011 , pp . 136140 .",
    "v.  strassen , `` asymptotische abschtzungen in shannons informationstheorie , '' in _ trans .",
    "third prague conf .",
    "theory _ , prague , 1962 ,",
    "689723 , http://www.math.cornell.edu/@xmath405pmlut/strassen.pdf .        v.  y.  f. tan , _",
    "asymptotic estimates in information theory with non - vanishing error probabilities_.1em plus 0.5em minus 0.4em foundations and trends in communications and information theory , 2015 , vol .  11 , no . 1 - 2",
    ".        y.  polyanskiy and s.  verd , `` channel dispersion and moderate deviations limits for memoryless channels , '' in _ proc . 48th annual allerton conference on communication , control , and computing _ , monticello , il , 2010 .",
    "z.  lin , v.  y.  f. tan , and m.  motani , `` on error exponents and moderate deviations for lossless streaming compression of correlated sources , '' _ ieee trans .",
    "inf . theory _ , submitted for publication .",
    "[ online ] .",
    "available : http://arxiv.org/abs/1507.03190 ."
  ],
  "abstract_text": [
    "<S> we consider streaming data transmission over a discrete memoryless channel . a new message is given to the encoder at the beginning of each block and the decoder </S>",
    "<S> decodes each message sequentially , after a delay of @xmath0 blocks . in this streaming setup , </S>",
    "<S> we study the fundamental interplay between the rate and error probability in the central limit and moderate deviations regimes and show that i ) in the moderate deviations regime , the moderate deviations constant improves over the block coding or non - streaming setup by a factor of @xmath0 and ii ) in the central limit regime , the second - order coding rate improves by a factor of approximately @xmath1 for a wide range of channel parameters . for both regimes , </S>",
    "<S> we propose coding techniques that incorporate a joint encoding of fresh and previous messages . in particular , </S>",
    "<S> for the central limit regime , we propose a coding technique with truncated memory to ensure that a summation of constants , which arises as a result of applications of the central limit theorem , does not diverge in the error analysis .    </S>",
    "<S> furthermore , we explore interesting variants of the basic streaming setup in the moderate deviations regime . </S>",
    "<S> we first consider a scenario with an erasure option at the decoder and show that both the exponents of the total error and the undetected error probabilities improve by factors of @xmath0 . </S>",
    "<S> next , by utilizing the erasure option , we show that the exponent of the total error probability can be improved to that of the undetected error probability ( in the order sense ) at the expense of a variable decoding delay . </S>",
    "<S> finally , we also extend our results to the case where the message rate is not fixed but alternates between two values . </S>"
  ]
}