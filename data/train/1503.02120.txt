{
  "article_text": [
    "starting with the work of shannon @xcite , information theory has grown enormously and has been shown by jaynes to have deep connections to statistical mechanics  @xcite .",
    "we focus on a particular aspect of shannon s work , namely joint probability distributions between word - types ( denoted @xmath1 ) , and their groupings by appearance - orderings , or , _ contexts _ ( denoted @xmath2 ) . for a word appearing in text ,",
    "shannon s model assigned context according to the word s immediate antecedent . in other words ,",
    "the sequence @xmath3 places this occurrence of the word - type of @xmath4 in the context of @xmath5 ( uniquely defined by the word - type of @xmath6 ) , where `` @xmath7 '' denotes `` any word '' .",
    "this experiment was novel , and when these transition probabilities were observed , he found a method for the automated production of language that far better resembled true english text than simple adherence to relative word frequencies .",
    "later , though still early on in the history of modern computational linguistics and natural language processing , theory caught up with shannon s work . in 1975 , becker wrote  @xcite :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ my guess is that phrase - adaption and generative gap - filling are very roughly equally important in language production , as measured in processing time spent on each , or in constituents arising from each .",
    "one way of making such an intuitive estimate is simply to listen to what people actually say when they speak . an independent way of gauging the importance of the phrasal lexicon is to determine its size .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    since then , with the rise of computation and increasing availability of electronic text , there have been numerous extensions of shannon s context model .",
    "these models have generally been information - theoretic applications as well , mainly used to predict word associations  @xcite and to extract multi - word expressions ( mwes )  @xcite .",
    "this latter topic has been one of extreme importance for the computational linguistics community  @xcite , and has seen many approaches aside from the information - theoretic , including with part - of - speech taggers  @xcite ( where categories , e.g. , noun , verb , etc .",
    "are used to identify word combinations ) and with syntactic parsers  @xcite ( where rules of grammar are used to identify word combinations ) .",
    "however , almost all of these methods have the common issue of scalability  @xcite , making them difficult to use for the extraction of phrases of more than two words .",
    "information - theoretic extensions of shannon s context model have also been used by piantadosi et al .",
    "@xcite to extend the work of zipf  @xcite , using an entropic derivation called the information content ( ic ) : @xmath8 and measuring its associations to word lengths .",
    "though there have been concerns over some of the conclusions reached in this work  @xcite , shannon s model was somewhat generalized , and applied to @xmath9-gram , @xmath10-gram and @xmath11-gram context models to predict word lengths .",
    "this model was also used by garcia et al .",
    "@xcite to assess the relationship between sentiment ( surveyed emotional response ) norms and ic measurements of words .",
    "however their application of the formula @xmath12 to @xmath13-grams data was wholly incorrect , as this special representation applies only to corpus - level data , i.e. , plot line - human readable text , and _ not _ the frequency - based @xmath13-grams .",
    "in addition to the above considerations , there is also the general concern of non - physicality with imperfect word frequency conservation , which is exacerbated by the piantadosi et al .",
    "extension of shannon s model . to be precise , for a joint distribution of words and contexts that is _ physically _",
    "related to the appearance of words on `` the page '' , there should be conservation in the marginal frequencies : @xmath14 much like that discussed in  @xcite .",
    "this property is not upheld using any true , sliding - window @xmath13-gram data ( e.g. ,  @xcite ) . to see this",
    ", we recall that in both of  @xcite and @xcite , a word s @xmath13-gram context was defined by its immediate @xmath15 antecedents .",
    "however , by this formulation we note that the first word of a page appears as _",
    "last _ in no @xmath16-gram , the second appears as _ last _ in no @xmath9-gram , and so on .",
    "these word frequency misrepresentations may seem to be of little importance at the text or page level , but since the methods for large - scale @xmath13-gram parsing have adopted the practice of stopping at sentence and clause boundaries  @xcite , word frequency misrepresentations ( like those discussed above ) have become very significant . in the new format , @xmath17 of the words in a sentence or clause of length five",
    "have no @xmath9-gram context ( the first two ) .",
    "as such , when these context models are applied to modern @xmath13-gram data , they are incapable of accurately representing the frequencies of words expressed .",
    "we also note that despite the advances in processing made in the construction of the current google @xmath13-grams corpus  @xcite , other issues have been found , namely regarding the source texts taken  @xcite .",
    "the @xmath13-gram expansion of shannon s model incorporated more information on relative word placement , but perhaps an ideal scenario would arise when the frequencies of author - intended phrases are exactly known . here",
    ", one can conserve word frequencies ( as we discuss in section ii ) when a context for an instance of a word is defined by its removal pattern , i.e. , the word `` cat '' appears in the context `` @xmath18 in the hat '' , when the phrase `` cat in the hat '' is observed . in this way , a word - type appears in as many contexts as there are phrase - types that contain the word .",
    "while we consider the different phrase - types as having rigid and different meanings , the words underneath can be looked at as having more flexibility , often in need of disambiguation .",
    "this flexibility is quite similar to an aspect of a physical model of lexicon learning  @xcite , where a `` context size '' control parameter was used to tune the number of plausible but unintended meanings that accompany a single word s true meaning .",
    "an enhanced model of lexicon learning that focuses on meanings of phrases could then explain the need for disambiguation when reading by words .",
    "we also note that there exist many other methods for grouping occurrences of lexical units to produce informative context models . as early as @xmath19  @xcite",
    ", resnik showed class categorizations of words ( e.g. , verbs and nouns ) could be used to produce informative joint probability distributions . in 2010 , montemurro et al .",
    "@xcite used joint distributions of words and arbitrary equal - length parts of texts to entropically quantify the semantic information encoded in written language .",
    "texts tagged with metadata like genera  @xcite , time  @xcite , location  @xcite , and language  @xcite , have rendered straightforward and clear examples of the power in a ( word - frequency conserving ) joint pmf , at shedding light on social phenomena by relating words to classes . additionally , while their work did not leverage word frequencies or the joint pmf s possible , benedetto et al .",
    "@xcite used metadata of texts to train language and authorship detection algorithms , and further , construct accurate phylogenetic - like trees through application of compression distances .",
    "though metadata approaches to context are informative , with their power there is simultaneously a loss of applicability ( metadata is frequently not present ) , as well as a loss of bio - communicative relevance ( humans are capable of inferring social information from text in isolation ) .",
    "in previous work  @xcite we developed a scalable and general framework for generating frequency data for @xmath13-grams , called random text partitioning . since a phrase - frequency distribution , @xmath20 , is balanced with regard to its underlying word - frequency distribution , @xmath21 , @xmath22 ( where @xmath23 denotes the phrase - length norm , which returns the length of a phrase in numbers of words ) it is easy to produce a symmetric generalization of shannon s model that integrates all phrase/@xmath13-gram lengths and all word placement / removal points .",
    "to do so , we define @xmath21 and @xmath20 to be the sets of words and ( text - partitioned ) phrases from a text respectively , and let @xmath24 be the collection of all single word - removal patterns from the phrases of @xmath20 . a joint frequency , @xmath25 ,",
    "is then defined by the partition frequency of the phrase that is formed when @xmath26 and @xmath27 are composed . in particular , if @xmath28 , we then set @xmath29 , which produces a context model on the words whose marginal frequencies preserve their original frequencies from `` the page . '' in particular we refer to this , or such a model for phrases , as an ` external context model , ' since the relations are produced by structure external to the semantic unit .",
    "it is good to see the external word - context generalization emerge , but our interest actually lies in the development of a context model for the phrases themselves .",
    "to do so , we define the ` internal contexts ' of a phrase by the patterns generated through the removal of sub - phrases . to be precise , for a phrase @xmath30 , and a sub - phrase @xmath31 ranging over words @xmath32 through @xmath33",
    ", we define the context @xmath34 to be the collection of same - length phrases whose analogous word removal ( @xmath32 through @xmath33 ) renders the same pattern ( when word - types are considered ) .",
    "we present the contexts of generalized phrases of lengths @xmath35@xmath10 in tab .  [",
    "tab : contab ] , as described above .",
    "looking at the table , it becomes clear that these contexts are actually a mathematical formalization of the generative gap filling proposed in @xcite , which was semi - formalized by the phrasal templates discussed at length by smadja et al . in  @xcite . between our formulation and that of smadja",
    ", the main difference of definition lies in our restriction to contiguous word sequence ( i.e. , sub - phrase ) removals , as is necessitated by the mechanics of the secondary partition process , which defines the context lists .",
    "the weighting of the contexts for a phrase is accomplished simultaneously with their definition through a secondary partition process describing the inner - contextual modes of interpretation for the phrase .",
    "the process is as follows . in an effort to relate an observed phrase to other known phrases",
    ", the observer selectively ignores a sub - phrase of the original phrase . to retain generality",
    ", we do this by considering the _ random _ partitions of the original phrase , and then assume that a sub - phrase is ignored from a partition with probability proportional to its length , to preserve word ( and hence phrase ) frequencies .",
    "the conditional probabilities of inner context are then : @xmath36 utilizing the partition probability and our assumption , we note from our work in @xcite that @xmath37 which ensures through defining @xmath38 the production of a valid , phrase - frequency preserving context model : @xmath39 which preserves the underlying frequency distribution of phrases . note here that beyond this point in the document we will used the normalized form , @xmath40 for convenience in the derivation of expectations in the next section .",
    "in this section we exhibit the power of the internal context model through a lexicographic application , deriving a measure of meaning and definition for phrases with empirical phrase - definition data taken from a collaborative open - access dictionary  @xcite ( see sec .",
    "[ sec : matmet ] for more information on our data and the wiktionary ) . with the rankings that this measure derives",
    ", we will go on to propose phrases for definition with the editorial community of the wiktionary in an ongoing live experiment , discussed in sec .  [",
    "sec : misent ] .    to begin",
    ", we define the dictionary indicator , @xmath41 , to be a binary norm on phrases , taking value @xmath35 when a phrase appears in the dictionary , ( i.e. , has definition ) and taking value @xmath42 when a phrase is unreferenced .",
    "the dictionary indicator tells us when a phrase has reference in the dictionary , and in principle can be replaced with other indicator norms , for other purposes . moving forward , we note of an intuitive description of the distribution average : @xmath43 and",
    "go on to derive an alternative expansion through application of the context model : @xmath44 in the last line we then interpret : @xmath45 to be the likelihood ( analogous to the ic equation presented here as equation  [ eq : ic ] ) that a phrase , which is randomly drawn from a context of @xmath30 , to have definition in the dictionary . to be precise ,",
    "we say @xmath46 is the likelihood of dictionary definition of the context model @xmath24 , given the phrase @xmath30 , or , when only one @xmath2 is considered , we say @xmath47 is the likelihood of dictionary definition of the context @xmath26 , given @xmath20 .",
    "numerically , we note that the distribution - level values , @xmath48 `` extend '' the dictionary over all @xmath20 , smoothing out the binary data to the full lexicon ( uniquely for phrases of more than one word , which have no interesting space - defined internal structure ) through the relations of the model . in other words , though @xmath49 may now only indicate the _ possibility _ of a phrase having definition , it is still a strong indicator , and most importantly , may be applied to never - before - seen expressions .",
    "we illustrate the extension of the dictionary through @xmath50 in fig .",
    "[ fig : contexttree ] , where it becomes clear that the topological structure of the associated network of contexts is crystalline , unlike the small - world phenomenon observed for the words of a thesaurus in  @xcite . however , this is not surprising , given that the latter is a conceptual network defined by common meanings , as opposed to a rigid , physical property , such as word order .    at ( 0,-0.25 ) ( eps ) ;    at ( 0,2 ) ( lc ) ; at ( 0,2.75 ) ( lc2 ) ;    ( wo ) @xmath7 @xmath7 contrary ; ( ma ) in @xmath7 @xmath7 ; ( fu ) on @xmath7 @xmath7 ;    at ( 0,4 ) ( mc ) ; at ( 2.8,3.6 ) ( pdefb1 ) @xmath51 ; at ( -2.75,3.6 ) ( pdefb1 ) @xmath52 ; at ( 0.95,2.25 ) ( pdefb1 ) @xmath53 ; at ( 0,4.5 ) ( mc2 ) ;    ( wiwo ) @xmath7 the contrary ; ( mawo ) in @xmath7 contrary ; ( mawi ) in the @xmath7 ; ( fuwi ) on the @xmath7 ; ( fuwo ) on @xmath7 contrary ;    at ( 0,6.5 ) ( c ) ;    ( mawiwo ) in the contrary ( @xmath54 ) ; ( fuwiwo ) on the contrary ( @xmath55 ) ;    ( wo )  ( mawo ) ; ( wo )  ( wiwo ) ; ( wo )  ( fuwo ) ;    ( fu )  ( fuwi ) ; ( fu )  ( fuwo ) ;    ( ma )  ( mawo ) ; ( ma )  ( mawi ) ;    ( wiwo )  ( mawiwo ) ; ( wiwo )  ( fuwiwo ) ;    ( mawi )  ( mawiwo ) ; ( mawo )  ( mawiwo ) ; ( fuwi )  ( fuwiwo ) ; ( fuwo )  ( fuwiwo ) ;    ( eps )  ( ma ) ; ( eps )  ( fu ) ; ( eps )  ( wo ) ;",
    "starting with the work of sinclair  @xcite ( though the idea was proposed more than @xmath56 years earlier by becker in @xcite ) , lexicographers have been building dictionaries based on language as it is spoken and written , including idiomatic , slang - filled , and grammatical expressions @xcite .",
    "these dictionaries have proven highly - effective for non - primary language learners , who may not be privy to cultural metaphors . in this spirit , we utilize the context model derived above to discover phrases that are undefined , but which may be in need of definition for their similarity to other , defined phrases .",
    "we do this in a corpus - based way , using the definition likelihood @xmath46 as a secondary filter to frequency .",
    "the process is in general quite straightforward , and first requires a ranking of phrases by frequency of occurrence , @xmath57 . upon taking the first @xmath58 frequency - ranked phrases ( @xmath59 , for our experiments ) , we reorder the list according to the values @xmath46 ( descending ) .",
    "the top of such a double - sorted list then includes phrases that are both frequent and similar to defined phrases .    with our double - sorted lists",
    "we then record those phrases having no definition or dictionary reference , but which are at the top .",
    "these phrases are quite often meaningful ( as we have found experimentally , see below ) despite their lack of definition , and as such we propose this method for the automated generation of short lists for editorial investigation of definition .",
    "for its breadth , open - source nature , and large editorial community , we utilize dictionary data from the wiktionary  @xcite ( a wiki - based open content dictionary ) to build the dictionary - indicator norm , setting @xmath60 if a phrase @xmath30 has reference or redirect .",
    "we apply our filter for missing entry detection to several large corpora from a wide scope of content .",
    "these corpora are : twenty years of new york times articles ( nyt , 19872007 )  @xcite , approximately @xmath61 of a year s tweets ( twitter , 2009 )  @xcite , music lyrics from thousands of songs and authors ( lyrics , 19602007 )  @xcite , complete wikipedia articles ( wikipedia , 2010 )  @xcite , and project gutenberg ebooks collection ( ebooks , 2012 )  @xcite of more than @xmath62 public - domain texts .",
    "we note that these are all unsorted texts , and that twitter , ebooks , lyrics , and to an extent , wikipedia are mixtures of many languages ( though majority english ) .",
    "we only attempt missing entry prediction for phrase lengths ( @xmath16@xmath11 ) , for their inclusion in other major collocation corpora  @xcite , as well as their having the most data in the dictionary .",
    "we also note that all text processed is taken lower - case .",
    "to understand our results , we perform a 10-fold cross - validation on the frequency and likelihood filters .",
    "this is executed by random splitting the wiktionary s list of defined phrases into @xmath56 equal - length pieces , and then performing @xmath56 parallel experiments in each of these experiments we determine the likelihood values , @xmath46 , by a distinct @xmath63 s of the data .",
    "we then order the union set of the @xmath64-withheld and the wiktionary - undefined phrases by their likelihood ( and frequency ) values descending , and accept some top segment of the list , or , ` short  list ' , coding them as positive by the experiment .",
    "for such a short list , we then record the true positive rates , i.e. , portion of all @xmath64-withheld truly - defined phrases we coded positive , the false positive rates , i.e. , portion of all truly - undefined phrases we coded positive , and the number of entries discovered . upon performing these experiments ,",
    "the average of the ten trials is taken for each of the three parameters , for a number of short list lengths ( scanning @xmath65 @xmath66-spaced lengths ) , and plotted as a receiver operating characteristic ( roc ) curve ( see figs .",
    "[ fig : twitter.crossval][fig : gutenberg.crossval ] ) .",
    "we also note that each is also presented with its area under curve ( auc ) , which measures the accuracy of the expanding - list classifier as a whole .    .",
    "summarizing our results from the cross - validation procedure * ( above ) * , we present the mean numbers of missing entries discovered when @xmath67 guesses were made for @xmath13-grams / phrases of lengths @xmath16 , @xmath9 , @xmath10 , and @xmath11 , each . for each of the @xmath11 large corpora ( see materials and methods )",
    "we make predictions according our likelihood filter , and according to frequency ( in parentheses ) as a baseline . when considering the @xmath16-grams ( for which the most definition information exists ) , short lists of @xmath67 rendered up to @xmath68 correct predictions on average by the definition likelihood , as opposed to the frequency ranking , by which no more than @xmath69 could be expected .",
    "we also summarize the results to - date from the live experiment * ( below ) * ( updated february 19 , 2015 ) , and present the numbers of missing entries correctly discovered on the wiktionary ( i.e. , reference added since july 1 , 2014 , when the dictionary s data was accessed ) by the @xmath67-phrase shortlists produced in our experiments for both the likelihood and frequency ( in parentheses ) filters . here",
    "we see that all of the corpora analyzed were generative of phrases , with twitter far and away being the most productive , and the reference corpus wikipedia the least so . [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     -fold ) cross - validation results for the filtration procedures . for each of the lengths @xmath16 , @xmath9 , @xmath10 , and @xmath11",
    ", we show the roc curves ( * main axes * ) , comparing true and false positive rates for both the likelihood filters ( black ) , and for the frequency filters ( gray ) .",
    "there , we see increased performance in the likelihood classifiers ( except possibly for length @xmath11 ) , which is reflected in the aucs ( where an auc of @xmath35 indicates a perfect classifier ) .",
    "we also monitor the average number of missing entries discovered as a function of the number of entries proposed ( * insets * ) , for each length .",
    "there , the horizontal dotted lines indicate the average numbers of missing entries discovered for both the likelihood filters ( black ) and for the frequency filters ( gray ) when short lists of @xmath67 phrases were taken ( red dotted vertical lines ) . from this",
    "we see an indication that even the @xmath11-gram likelihood filter is effective at detecting missing entries in short lists , while the frequency filter is not .",
    ", scaledwidth=49.5% ]",
    "before observing output from our model we take the time to perform a cross - validation ( @xmath56-fold ) , and compare our context filter to a sort by frequency alone . from this",
    "we have found that our likelihood filter renders missing entries much more efficiently than by frequency ( see tab .  [",
    "tab : numsdisc ] , and figs .",
    "[ fig : twitter.crossval][fig : gutenberg.crossval ] ) , already discovering missing entries from short lists of as little as twenty ( see the insets of figs .  [",
    "fig : twitter.crossval][fig : gutenberg.crossval ] as well as tabs .",
    "[ tab : numsdisc ] ,  [ tab : twitter ] ,  and  [ tab : times][tab : gutenberg ] ) . as such we adhere to this standard , and only publish short lists of @xmath67 predictions per corpus per phrase lengths @xmath16@xmath11 . in parallel , we also present phrase frequency - generated short - lists for comparison .",
    "in addition to listing them in the appendices , we have presented the results of our experiment from across the @xmath11 large , disparate corpora on the wiktionary in a pilot program , where we are tracking the success of the filters  .",
    "looking at the lexical tables , where defined phrases are highlighted in red , we can see that many of the predictions by the likelihood filter ( especially those obtained from the twitter corpus ) have already been defined in the wiktionary following our recommendation ( as of feb .",
    "19th 2015 ) since we accessed its data in july of 2014  @xcite .",
    "we also summarize these results from the live experiment in tab .",
    "[ tab : numsdisc ] .",
    "looking at the lexical tables more closely , we note that all corpora present highly idiomatic expressions under the likelihood filter , many of which are variants of existing idiomatic phrases that will likely be granted inclusion into the dictionary through redirects or alternative - forms listings . to name a few , the twitter ( tab .",
    "[ tab : twitter ] ) , times ( tab .",
    "[ tab : times ] ) , and lyrics ( tab .",
    "[ tab : lyrics ] ) corpora consistently predict large families derived from phrases like `` at the same time '' , and `` you know what i mean '' , while the ebooks and wikipedia corpora predict families derived from phrases like `` on the other hand '' , and `` at the same time '' . in general",
    "we see no such structure or predictive power emerge from the frequency filter .",
    "we also observe that from those corpora which are less pure of english context ( namely , the ebooks , lyrics , and twitter corpora ) , extra - english expressions have crept in .",
    "this highlights an important feature of the likelihood filter  it does not intrinsically rely on the syntax or grammar of the language to which it is applied , beyond the extent to which syntax and grammar effect the shapes of collocations .",
    "for example , the ebooks predict ( see tab .  [",
    "tab : gutenberg ] ) the undefined french phrase `` tu ne sais pas '' , or `` you do not know '' , which is a syntactic variant of the english - wiktionary defined french , `` je ne sais pas '' , meaning `` i do not know '' . seeing this , we note that it would be straightforward to construct a likelihood filter with a language indicator norm to create an alternative framework for language identification .",
    "there are also a fair number of phrases predicted by the likelihood filter which in fact are spelling errors , typos , and grammatical errors . in terms of the context model ,",
    "these erroneous forms are quite near to those defined in the dictionary , and so rise in the short lists generated from the less - well edited corpora , e.g. , `` actions  speak  louder  _ then _  words '' in the twitter corpus .",
    "this then seems to indicate the potential for the likelihood filter to be integrated into auto - correct algorithms , and further points to the possibility of constructing syntactic indicator norms of phrases , making estimations of tenses and parts of speech ( whose data is also available from the wiktionary  @xcite ) possible through application of the model in precisely the same manner presented in sec .",
    "[ sec : deflik ] .",
    "regardless of the future applications , we have developed and presented a novel , powerful , and scalable mwe extraction technique .",
    "40 natexlab#1#1bibnamefont # 1#1bibfnamefont # 1#1citenamefont # 1#1url # 1`#1`urlprefix[2]#2 [ 2][]#2    , * * , ( ) , issn , http://doi.acm.org/10.1145/584091.584093 .",
    ", * * , ( ) , http://link.aps.org/doi/10.1103/physrev.106.620 .    , in _ _",
    "( , , ) , tinlap 75 , pp . , http://dx.doi.org/10.3115/980190.980212 .    , * * , ( ) , issn , http://dl.acm.org/citation.cfm?id=89086.89095 .    , * * , ( ) , issn , http://dl.acm.org/citation.cfm?id=972450.972458 .    ,",
    "_ _ ( , ) , isbn .    , pp . ( ) .    , _ _ ( ) , http://books.google.com/books?id=nirjsaaacaaj .    , * * , ( ) ,",
    "issn , http://dx.doi.org/10.1007/s10579-009-9101-4 .    , , , * * , ( ) , http://colala.bcs.rochester.edu/papers/pnas-2011-piantadosi-1012551108.pdf .    ,",
    "_ _ ( , ) .    , * * , ( ) , http://www.pnas.org/content/108/20/e108.short .    , , , * * , ( ) , http://colala.bcs.rochester.edu/papers/pnas-2011-piantadosi-1103550108_reply.pdf .    , * * ( ) , http://arxiv.org/abs/1209.1751",
    ", , , ( ) , .    , , , * * , ( ) , http://dx.doi.org/10.1140/epjds3 .    , _",
    "_ ( ) , .    , , , , , , , , , , , * * , ( ) , http://www.sciencemag.org/content/331/6014/176.abstract .    , , , , , , in _ _",
    "( , , ) , acl 12 , pp . , http://dl.acm.org/citation.cfm?id=2390470.2390499 .    , , , * * ( ) , http://arxiv.org/abs/1501.00960 .    , , , * * , ( ) , http://link.aps.org/doi/10.1103/physrevlett.110.258701 .    ,",
    "( ) , .    , * * , ( ) , http://www.worldscientific.com/doi/abs/10.1142/s0219525910002530 .    ,",
    "* * , ( ) , issn , http://dx.doi.org/10.1007/s10902-009-9150-9 .",
    ", , , , , * * , ( ) , http://dx.doi.org/10.1371%2fjournal.pone.0026752 .    , , , , ,",
    "* * , ( ) , http://dx.doi.org/10.1371%2fjournal.pone.0064417 .    , , , , , , , , , , , ( ) , http://www.pnas.org/content/early/2015/02/04/1411678112.abstract .    , , , * * , ( ) , http://link.aps.org/doi/10.1103/physrevlett.88.048702 .",
    ", , , , , , , * * ( ) , http://arxiv.org/abs/1406.5181 .    .    , , , , * * , ( ) , http://link.aps.org/doi/10.1103/physreve.65.065102 .    , , , , , _ _ ( , , ) .    .    .",
    ".    .    , _",
    "_ , ( ) .    ."
  ],
  "abstract_text": [
    "<S> in an effort to better understand meaning from natural language texts , we explore methods aimed at organizing lexical objects into contexts . a number of these methods for organization fall into a family defined by word ordering . unlike demographic or spatial partitions of data , these collocation models are of special importance for their universal applicability . while we are interested here in text and have framed our treatment appropriately , our work is potentially applicable to other areas of research ( e.g. , speech , genomics , and mobility patterns ) where one has ordered categorical data , ( e.g. , sounds , genes , and locations ) . </S>",
    "<S> our approach focuses on the phrase ( whether word or larger ) as the primary meaning - bearing lexical unit and object of study . to do so </S>",
    "<S> , we employ our previously developed framework for generating word - conserving phrase - frequency data . upon training our model with the wiktionary </S>",
    "<S>  an extensive , online , collaborative , and open - source dictionary that contains over @xmath0 phrasal - definitions  we develop highly effective filters for the identification of meaningful , missing phrase - entries . with our predictions </S>",
    "<S> we then engage the editorial community of the wiktionary and propose short lists of potential missing entries for definition , developing a breakthrough , lexical extraction technique , and expanding our knowledge of the defined english lexicon of phrases . </S>"
  ]
}