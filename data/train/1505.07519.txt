{
  "article_text": [
    "max - convolution occurs frequently in signal processing and bayesian inference : it is used in image analysis  @xcite , in network calculus  @xcite , in economic equilibrium analysis  @xcite , and in a probabilistic variant of combinatoric generating functions , wherein information on a sum of values into their most probable constituent parts ( _ e.g. _ identifying proteins from mass spectrometry  @xcite ) .",
    "max - convolution operates on the semi - ring @xmath7 , meaning that it behaves identically to a standard convolution , except it employs a @xmath8 operation in lieu of the @xmath9 operation in standard convolution ( max - convolution is also equivalent to min - convolution , also called infimal convolution , which operates on the tropical semi - ring @xmath10 ) . due to the importance and ubiquity of max - convolution ,",
    "substantial effort has been invested into highly optimized implementations ( _ e.g. _ , implementations of the quadratic method on gpus ; @xcite ) .",
    "max - convolution can be defined using vectors ( or discrete random variables , whose probability mass functions are analogous to nonnegative vectors ) with the relationship @xmath11 .",
    "given the target sum @xmath12 , the max - convolution finds the largest values @xmath13 $ ] and @xmath14 $ ] for which @xmath15 .",
    "@xmath16 & = & \\max_{\\ell , r : \\,m = \\ell+r } l[\\ell ] r[r ] \\\\ & = & \\max_\\ell l[\\ell ] r[{m-\\ell}]\\\\ & = & \\left ( l ~*_{\\max}~ r \\right)[m ] \\\\\\end{aligned}\\ ] ] where @xmath17 denotes the max - convolution operator . in probabilistic terms",
    ", this is equivalent to finding the highest probability of the joint events @xmath18 that would produce each possible value of the sum @xmath11 ( note that in the probabilistic version , the vector @xmath19 would subsequently need to be normalized so that its sum is 1 ) .",
    "although applications of max - convolution are numerous , only a small number of methods exist for solving it  @xcite .",
    "these methods fall into two main categories , each with their own drawbacks : the first category consists of very accurate methods that are have worst - case runtimes either quadratic  @xcite or slightly more efficient than quadratic in the worst - case  @xcite .",
    "conversely , the second type of method computes a numerical approximation to the desired result , but in @xmath20 steps ; however , no bound for the numerical accuracy of this method has been derived  @xcite .",
    "while the two approaches from the first category of methods for solving max - convolution do so by either using complicated sorting routines or by creating a bijection to an optimization problem , the numerical approach solves max - convolution by showing an equivalence between @xmath17 and the process of first generating a vector @xmath21 for each index @xmath22 of the result ( where @xmath23 = l[\\ell ] r[{m-\\ell}]$ ] for all in - bounds indices ) and subsequently computing the maximum @xmath24 = \\max_\\ell u^{(m)}[\\ell]$ ] .",
    "when @xmath25 and @xmath26 are nonnegative , the maximization over the vector @xmath21 can be computed exactly via the chebyshev norm @xmath16 & = & \\max_\\ell u^{(m)}[\\ell ] \\\\ & = & \\lim_{p \\to \\infty } \\| u^{(m ) } \\|_p\\\\\\end{aligned}\\ ] ] but requires @xmath27 steps ( where @xmath28 is the length of vectors @xmath25 and @xmath26 ) .",
    "however , once a fixed @xmath29-norm is chosen , the approximation corresponding to that @xmath29 can be computed by expanding the @xmath29-norm to yield @xmath30 \\right)}^{p } \\right)}^{\\frac{1}{p } } \\\\    & \\approx & { \\left ( \\sum_\\ell { \\left ( u^{(m)}[\\ell ] \\right)}^{p^ * } \\right)}^{\\frac{1}{p^*}}\\\\    & = & { \\left ( \\sum_\\ell { l[\\ell]}^{p^ * } ~ { r[{m-\\ell}]}^{p^ * } \\right)}^{\\frac{1}{p^*}}\\\\    & = & { \\left ( \\sum_\\ell { \\left(l^{p^*}\\right)}[\\ell ] ~ { \\left(r^{p^*}\\right)}[{m-\\ell } ] \\right)}^{\\frac{1}{p^*}}\\\\    & = & { \\left ( l^{p^ * } ~*~ r^{p^ * } \\right)}^{\\frac{1}{p^*}}[m]\\end{aligned}\\ ] ] where @xmath31 \\right)}^{p^ * } , { \\left (    l[1 ] \\right)}^{p^ * } , ~\\ldots,~{\\left ( l[{k-1 } ] \\right)}^{p^ * } ~\\rangle$ ] and @xmath32 denotes standard convolution .",
    "the standard convolution can be done via fast fourier transform ( fft ) in @xmath33 steps , which is substantially more efficient than the @xmath27 required by the naive method ( * algorithm  [ algorithm : numericalmaxconvolvegivenpstar ] * ) .    to date",
    ", the numerical method has currently demonstrated the best speed - accuracy trade - off on bayesian inference tasks , and can be generalized to multiple dimensions ( _ i.e. _ , tensors ) .",
    "in particular , they have been used with probabilistic convolution trees  @xcite to efficiently compute the most probable values of discrete random variables @xmath34 for which the sum is known @xmath35  @xcite . the one - dimensional variant of this problem ( _ i.e. _ , where each @xmath36 is a one - dimensional vector ) solves the probabilistic generalization of the subset sum problem , while the two - dimensional variant ( _ i.e. _ , where each @xmath36 is a one - dimensional matrix ) solves the generalization of the knapsack problem ( note that these problems are not np - hard in this specific case , because we assume an evenly - spaced discretization of the possible values of the random variables ) .",
    "however , despite the practical performance that has been demonstrated by the numerical method , only cursory analysis has been performed to formalize the influence of the value of @xmath29 on the accuracy of the result and to bound the error of the @xmath29-norm approximation .",
    "optimizing the choice of @xmath29 is non - trivial : larger values of @xmath29 more closely resemble a true maximization under the @xmath29-norm , but result in underflow ( note that in * algorithm  [ algorithm : numericalmaxconvolvegivenpstar ] * , the maximum values of both @xmath25 and @xmath26 can be divided out and then multiplied back in after max - convolution so that overflow is not an issue ) .",
    "conversely , smaller values of @xmath29 suffer less underflow , but compute a norm with less resemblance to maximization . here",
    "we perform an in - depth analysis of the influence of @xmath29 on the accuracy of numerical max - convolution , and from that analysis we construct a modified piecewise algorithm , on which we demonstrate bounds on the worst - case absolute error .",
    "this modified algorithm , which runs in @xmath37 steps , is demonstrated using a hidden markov model describing the relationship between u.s .",
    "unemployment and the s&p 500 stock index .",
    "we then extend the modified algorithm and introduce a second modified algorithm , which not only uses a single @xmath1-norm as a means of approximating the chebyshev norm , but instead uses a sequence of @xmath1-norms and assembles them using a projection as a means to approximate the chebyshev norm . using numerical simulations as evidence , we make a conjecture regarding the relative error of the null space projection method . in practice , this null space projection algorithm is shown to have similar runtime and higher accuracy when compared with the piecewise algorithm .",
    "we begin by outlining and comparing three numerical methods for max - convolution . by analyzing the benefits and deficits of each of these methods ,",
    "we create improved variants .",
    "all of these methods will make use of the basic numerical max - convolution idea summarized in the introduction , and as such we first declare a method for computing the numerical max - convolution estimate for a given @xmath29 as numericalmaxconvolvegivenpstar ( * algorithm  [ algorithm : numericalmaxconvolvegivenpstar ] * ) .",
    "@xmath38 \\gets { l[\\ell ] } ^{p^*}$ ]    @xmath39 \\gets { r[r ] } ^{p^*}$ ]    @xmath40    @xmath41 \\gets { vm[m ] } ^{\\frac{1}{p^*}}$ ]    @xmath42      the effects of underflow will be minimal ( as it is not very far from standard fft convolution , an operation with high numerical stability ) , but it can still be imprecise due to numerical `` bleed - in '' ( _ i.e. _ error due to contributions from non - maximal terms for a given @xmath21 because the @xmath29-norm is not identical to the chebyshev norm ) .",
    "overall , this will perform well on indices where the exact value of the result is small , but perform poorly when the exact value of the result is large .      as noted above , will offer the converse pros and cons compared to using a low @xmath29 : numerical artifacts due to bleed - in will be smaller ( thus achieving greater performance on indices where the exact values of the result are larger ) , but underflow may be significant ( and therefore , indices where the exact results of the max - convolution are small will be inaccurate ) .      the higher - order piecewise method formalizes the empirical cutoff values found in serang 2015 ; previously , numerical stability boundaries were found for each @xmath29 by computing both the exact max - convolution ( via the naive @xmath27 method ) and via the numerical method using the ascribed value of @xmath29 , and finding the value below which the numerical values experienced a high increase in relative absolute error .",
    "those previously observed empirical numerical stability boundaries can be formalized by using the fact that the employed numpy implementation of fft convolution has high accuracy on indices where the result has a value @xmath45 relative to the maximum value ; therefore , if the arguments @xmath25 and @xmath26 are both normalized so that each has a maximum value of 1 , the fast max - convolution approximation is numerically stable for any index @xmath22 where the result of the fft convolution , _",
    "i.e. _ @xmath46 $ ] , is @xmath45 .",
    "the numpy documentation defines a conservative numeric tolerance for underflow @xmath47 , which is a conservative estimate of the numerical stability boundary demonstrated in * figure  [ figure : baderrorsforallpstar ] * ( those boundary points occur very close to the true machine precision @xmath48 ) .        because cooley - tukey implementations of fft - based convolution ( _ e.g. _ , the numpy implementation ) are widely applied to large problems with extremely small error , we will make a simplification and assume that , when constraining the fft result to reach a value higher than machine epsilon ( + tolerance threshold ) , the error from the fft is negligible in comparison to the error introduced by the @xmath29-norm approximation .",
    "this is firstly because the only source of numerical error during fft ( assuming an fft implementation with numerically precise twiddle factors ) on vectors in @xmath49}^k$ ] will be the result of underflow from repeated addition and subtraction ( neglecting the non - influencing multiplication with twiddle factors , which each have magnitude @xmath50 ) .",
    "the numerically imprecise routines are thus limited to @xmath51 ; when @xmath52 ( _ i.e. _ , @xmath53 , the machine precision ) , then @xmath51 will return @xmath54 instead of @xmath55 . to recover at least one bit of the significand , the intermediate results of the fft must surpass machine precision @xmath56 ( since the worst case addition initially happens with the maximum @xmath57 ) .",
    "the maximum sum of any values from a list of @xmath28 such elements can never exceed @xmath28 ; for this reason , a conservative estimate of the numerical tolerance of an fft ( with regard to underflow ) will be the smallest value of @xmath55 for which @xmath58 ; thus , @xmath59 .",
    "this yields a conservative estimate of the minimum value in one index at the result of an fft convolution : when the result at some index @xmath22 is @xmath60 , then the result should be numerically stable . for this reason",
    ", we use a numerical tolerance @xmath61 , thereby ensuring that the vast majority of numerical error for the numerical max - convolution algorithm is due to the @xmath29-norm approximation ( _ i.e. _ , employing @xmath62 instead of @xmath63 ) and not due to the long - used and numerically performant fft result .",
    "furthermore , in practice the mean squared error due to fft will be much smaller than the conservative worst - case outlined here , because it is difficult for the largest intermediate summed value ( in this case @xmath64 ) to be consistently large when many such very small values ( in this case @xmath55 ) are encountered in the same list .",
    "although @xmath65 could be chosen specifically for a problem of size @xmath28 , note that this simple derivation is very conservative and thus it would be better to use a tighter bound for choosing @xmath65 .",
    "regardless , for an fft implementation that is nt as performant ( _ e.g. _ , because it uses float types instead of double ) , increasing @xmath65 slightly would suffice .    therefore , from this point forward we consider that the dominant cause of error to come from the max - convolution approximation .",
    "using larger @xmath29 values will provide a closer approximation ; however , using a larger value of @xmath29 may also drive values to zero ( because the inputs @xmath25 and @xmath26 will be normalized within * algorithm  [ algorithm : numericalmaxconvolvegivenpstar ] * so that the maximum of each is 1 when convolved via fft ) , limiting the applicability of large @xmath29 to indices @xmath22 for which @xmath46 \\geq \\tau$ ] .    through this lens ,",
    "the choice of @xmath29 can be characterized by two opposing sources of error : higher @xmath29 values better approximate @xmath66 but will be numerically unstable for many indices ; lower @xmath29 values provide worse approximations of @xmath62 but will be numerically unstable for only few indices .",
    "these opposing sources of error pose a natural method for improving the accuracy of this max - convolution approximation . by considering a small collection of @xmath29 values",
    ", we can compute the full numerical estimate ( at all indices ) with each @xmath29 using * algorithm  [ algorithm : numericalmaxconvolvegivenpstar ] * ; computing the full result at a given @xmath29 is @xmath67 , so doing so on some small number @xmath68 of @xmath29 values considered , then the overall runtime will be @xmath69 . then",
    ", a final estimate is computed at each index by using the largest @xmath29 that is stable ( with respect to underflow ) at that index .",
    "choosing the largest @xmath29 ( of those that are stable with respect to underflow ) corresponds to minimizing the bleed - in error , because the larger @xmath29 becomes , the more muted the non - maximal terms in the norm become ( and thus the closer the @xmath29-norm becomes to the true maximum ) .    here",
    "we introduce this piecewise method and compare it to the simpler low - value @xmath43 and high - value @xmath44 methods and analyze the worst - case error of the piecewise method .",
    "@xmath70 $ ] @xmath71 $ ] @xmath72}$ ] @xmath73}$ ]    @xmath74{\\log_2(p^*_{\\max } ) } } ] $ ]    @xmath75 \\gets$ ] ` fftnonnegmaxconvolvegivenpstar`(@xmath76 , @xmath77 , @xmath78 $ ] )    @xmath79 \\gets \\max \\ { i:~ { \\left ( resforallpstar[i][m ] \\right)}^{\\text{allpstar[$i$ ] } } \\geq \\tau ) \\}$ ]    @xmath80 $ ] @xmath81 \\gets resforallpstar[i][m]$ ]    @xmath82 \\times r[r_{\\max } ] \\times result$ ]",
    "this section derives theoretical error bounds as well as a practical comparison on an example for the standard piecewise method .",
    "furthermore the development of an improvement with affine scaling is shown .",
    "eventually , an evaluation of the latter is performed on a larger problem .",
    "therefore we applied our technique to compute the viterbi path for a hidden markov model ( hmm ) to assess runtime and the level of error propagation .",
    "we first analyze the error for a particular underflow - stable @xmath29 and then use that to generalize to the piecewise method , which seeks to use the highest underflow - stable @xmath29 .",
    "we first scale @xmath25 and @xmath26 into @xmath76 and @xmath77 respectively , where the maximum elements of both @xmath76 and @xmath77 are @xmath50 ; the absolute error can be found by unscaling the absolute error of the scaled problem : @xmath83 - numeric(l',r')[m ] |\\\\    = \\max_\\ell l[\\ell ] ~ \\max_r r[r ] \\ ; | exact(l',r')[m ] - numeric(l',r')[m ] |.\\end{gathered}\\ ] ] we first derive an error bound for the scaled problem on @xmath84 ( any mention of a vector @xmath21 refers to the scaled problem ) , and then reverse the scaling to demonstrate the error bound on the original problem on @xmath85 .",
    "for any particular `` underflow - stable '' @xmath29 ( _ i.e. _ , any value of @xmath29 for which @xmath86 ) , the absolute error for the numerical method for fast max - convolution can be bound fairly easily by factoring out the maximum element of @xmath21 ( this maximum element is equivalent to the chebyshev norm ) from the @xmath29-norm :    @xmath87 - numeric(l',r')[m ] |\\ ] ] @xmath88    where @xmath89 is a nonnegative vector of the same length as @xmath21 ( this length is denoted @xmath90 ) where @xmath89 contains one element equal to @xmath50 ( because the maximum element of @xmath21 must , by definition , be contained within @xmath21 ) and where no element of @xmath89 is greater than 1 ( also provided by the definition of the maximum ) .",
    "@xmath91    thus , since @xmath92 , the error is bound : @xmath93 - numeric(l',r')[m]\\\\    & = & \\| u^{(m ) } \\|_\\infty \\left(\\| v^{(m ) } \\|_{p^ * } - 1 \\right)\\\\    & \\leq & \\| v^{(m ) } \\|_{p^ * } - 1\\\\    & \\leq & k_m^\\frac{1}{p^ * } - 1,\\\\\\end{aligned}\\ ] ] because @xmath94 for a scaled problem on @xmath84 .      however , the bounds derived above are only applicable for @xmath29 where @xmath95 .",
    "the piecewise method is slightly more complicated , and can be partitioned into two cases : in the first case , the top contour is used ( _ i.e. _ , when @xmath96 is underflow - stable ) .",
    "conversely , in the second case , a middle contour is used ( _ i.e. _ , when @xmath96 is not underflow - stable ) . in this context ,",
    "in general a contour comprises of a set of indices @xmath22 with the same maximum stable @xmath29 .    in the first case ,",
    "when we use the top contour @xmath97 , we know that @xmath96 must be underflow - stable , and thus we can reuse the bound given an underflow - stable @xmath29 .    in the second case , because the @xmath29 used is @xmath98 , it follows that the next higher contour ( using @xmath99 ) must not be underflow - stable ( because the highest underflow - stable @xmath29 is used and because the @xmath29 are searched in log - space ) .",
    "the bound derived above that demonstrated @xmath100 can be combined with the property that @xmath101 for any @xmath102 to show that @xmath103.\\ ] ] thus the absolute error can be bound again using the fact that we are in a middle contour : @xmath104    the absolute error from middle contours will be quite small when @xmath105 is the maximum underflow - stable value of @xmath29 at index @xmath22 , because @xmath106 , the first factor in the error bound , will become @xmath107 , and @xmath108 ( qualitatively , this indicates that a small @xmath29 is only used when the result is very close to zero , leaving little room for absolute error ) .",
    "likewise , when a very large @xmath29 is used , then @xmath109 becomes very small , while @xmath110 ( qualitatively , this indicates that when a large @xmath29 is used , the @xmath111 , and thus there is little absolute error ) .",
    "thus for the extreme values of @xmath29 , middle contours will produce fairly small absolute errors .",
    "the unique mode @xmath112 can be found by finding the value that solves @xmath113 which yields @xmath114    an appropriate choice of @xmath96 should be @xmath115 so that the error for any contour ( both middle contours and the top contour ) is smaller than the error achieved at @xmath112 , allowing us to use a single bound for both .",
    "choosing @xmath116 would guarantee that all contours are no worse than the middle - contour error at @xmath112 ; however , using @xmath116 is still quite liberal , because it would mean that for indices in the highest contour ( there must be a nonempty set of such indices , because the scaling on @xmath76 and @xmath77 guarantees that the maximum index will have an exact value of @xmath50 , meaning that the approximation endures no underflow and is underflow - stable for every @xmath29 ) , a better error _ could _ be achieved by increasing @xmath96 .",
    "for this reason , we choose @xmath96 so that the top - contour error produced at @xmath96 is not substantially larger than all errors produced for @xmath29 before the mode ( _ i.e. _ , for @xmath117 ) .    choosing any value of @xmath118 guarantees the worst - case absolute error bound derived here ; however , increasing @xmath96 further over @xmath112 may possibly improve the mean squared error in practice ( because it is possible that many indices in the result would be numerically stable with @xmath29 values substantially larger than @xmath112 ) .",
    "however , increasing @xmath119 will produce diminishing returns and generally benefit only a very small number of indices in the result , which have exact values very close to @xmath50 . in order to balance these two aims ( increasing @xmath96 enough over @xmath112 but not excessively so )",
    ", we make a qualitative assumption that a non - trivial number of indices require us to use a @xmath29 below @xmath112 ; therefore , increasing @xmath96 to produce an error significantly smaller than the lowest worst - case error for contours below the mode ( _ i.e. _ @xmath117 ) will increase the runtime without significantly decreasing the mean squared error ( which will become dominated by the errors from indices that use @xmath117 ) .",
    "the lowest worst - case error contour below the mode is @xmath120 ( because the absolute error function is unimodal , and thus must be increasing until @xmath112 and decreasing afterward ) ; therefore , we heuristically specify that @xmath96 should produce a worst - case error on a similar order of magnitude to the worst - case error produced with @xmath120 . in practice , specifying the errors at @xmath96 and @xmath120 should be equal is very conservative ( it produces very large estimates of @xmath96 , which sometimes benefit only one or two indices in the result ) ; for this reason , we heuristically choose that the worst - case error at @xmath96 should be no worse than square root of the worst case error at @xmath120 ( this makes the choice of @xmath96 less conservative because the errors at @xmath120 are very close to zero , and thus their square root is larger ) . the square root was chosen because it produced , for the applications described in this paper , the smallest value of @xmath96 for which the mean squared error was significantly lower than using @xmath116 ( the lowest value of @xmath96 guaranteed to produce the absolute error bound ) .",
    "this heuristic does satisfy the worst - case bound outlined here ( because , again , @xmath118 ) , but it could be substantially improved if an expected distribution of magnitudes in the result vector were known ahead of time : prior knowledge regarding the number of points stable at each @xmath29 considered would enable a well - motivated choice of @xmath96 that truly optimizes the expected mean squared error .    from this heuristic choice of @xmath96 , solving @xmath121 ( with the square root of the worst - case at @xmath120 on the left and the worst - case error at @xmath96 on the right ) yields @xmath122 for any non - trivial problem ( _ i.e. _ , when @xmath123 ) , and thus @xmath124 indicating that the absolute error at the top contour will be roughly equal to the fourth root of @xmath65 .      by setting @xmath96 in this manner ,",
    "we guarantee that the absolute error at any index of any unscaled problem on @xmath125 is less than @xmath126 ~ \\max_r r[r ] ~ \\tau^\\frac{1}{2 p^*_{mode } } \\left ( 1 - k_m^\\frac{-1}{p^*_{mode } } \\right)\\ ] ] where @xmath112 is defined above . the full formula for the middle - contour error at this value of @xmath112 does not simplify and is therefore quite large ; for this reason , it is not reported here , but this gives a numeric bound of the worst case middle - contour error that is bound in terms of the variable @xmath28 ( and with no other free variables ) .",
    "the piecewise method clearly performs @xmath127 ffts ( each requiring @xmath20 steps ) ; therefore , since @xmath96 is chosen to be @xmath128 ( to achieve the desired error bound ) , the total runtime is thus @xmath129 for any practically sized problem , the @xmath130 factor is essentially a constant ; even when @xmath28 is chosen to be the number of particles in the observable universe ( @xmath131 ; @xcite ) , the @xmath130 is @xmath132 , meaning that for any problem of practical size , the full piecewise method is no more expensive than computing between @xmath50 and @xmath133 ffts .",
    "we first use an example max - convolution problem to compare the results from the low - value @xmath43 , the high - value @xmath44 and piecewise methods . at every index ,",
    "these various approximation results are compared to the exact values , as computed by the naive quadratic method ( * figure  [ figure : doublefigmethodsbimodalexa ] * ) .",
    ".48     .48       * figure  [ figure : doublefigmethodsbimodalexb ] * depicts a scatter plot of the exact result vs. the piecewise approximation at every index ( using the same problem from * figure  [ figure : doublefigmethodsbimodalexa ] * ) .",
    "it shows a clear banding pattern : the exact and approximate results are clearly correlated , but each contour ( _ i.e. _ , each collection of indices that use a specific @xmath29 ) has a different average slope between the exact and approximate values , with higher @xmath29 contours showing a generally larger slope and smaller @xmath29 contours showing greater spread and lower slopes .",
    "this intuitively makes sense , because the bounds on @xmath134 $ ] derived above constrain the scatter plot points inside a quadrilateral envelope ( * figure  [ figure : approxexactlinearzoom ] * ) .",
    "( worst - case approximation ) .",
    "the points are well described by an affine function fit using the left - most and right - most points.,width=480 ]    the correlations within each contour can be exploited to correct biases that emerge for smaller @xmath29 values . in order to do this ,",
    "@xmath135 must be computed for at least two points @xmath136 and @xmath137 within the contour , so that a mapping @xmath138 can be constructed .",
    "fortunately , a single @xmath63 can be computed exactly in @xmath139 ( by actually computing a single @xmath21 and computing its max , which is equivalent to computing a single index result via the naive quadratic method ) . as long as the exact value @xmath135 is computed for only a small number of indices ,",
    "the order of the runtime will not change ( each contour already costs @xmath20 , so adding a small number of @xmath139 steps for each contour will not change the asymptotic runtime ) .    if the two indices chosen are @xmath140 @xmath141 then we are guaranteed that the affine function @xmath142 can be written as a convex combination of the exact values at those extreme points ( using barycentric coordinates ) :    @xmath143 @xmath144\\ ] ]    thus , by computing @xmath145 and @xmath146 ( each in @xmath139 steps ) , we can compute an affine function @xmath142 to correct contour - specific trends ( * algorithm  [ algorithm : numericalmaxconvolvepiecewiseimproved ] * ) .",
    ".48     .48     @xmath70 $ ] @xmath71 $ ] @xmath72}$ ] @xmath73}$ ]    @xmath74{\\log_2(p^*_{\\max } ) } } ] $ ]    @xmath75 \\gets$ ] ` fftnonnegmaxconvolvegivenpstar`(@xmath76 , @xmath77 , @xmath78 $ ] )    @xmath79 \\gets \\max \\ { i:~ { \\left ( resforallpstar[i][m ] \\right)}^{\\text{allpstar[$i$ ] } } \\geq \\tau ) \\}$ ]    @xmath147`affinecorrect`@xmath148    @xmath82 \\times r[r_{\\max } ] \\times result$ ]    @xmath149 \\gets 1 $ ] @xmath150 \\gets 0 $ ]    @xmath151 @xmath152 = i \\}$ ] @xmath153[m]$ ] @xmath154[m]$ ]    @xmath155[mmin]$ ] @xmath156[mmax]$ ] @xmath157 @xmath158 @xmath159 \\gets \\frac{ymax - ymin}{xmax - xmin}$ ] @xmath160 \\gets ymin - slope[i ] \\times xmin$ ] @xmath159 \\gets \\frac{ymax}{xmax}$ ]    @xmath80 $ ] @xmath81 \\gets resforallpstar[i][m ] \\times slope[i ] + bias[i]$ ]    @xmath161      by exploiting the convex combination used to define @xmath142 , the absolute error of the affine piecewise method can also be bound .",
    "qualitatively , this is because , by fitting on the extrema in the contour , we are now interpolating .",
    "if the two points used to determine the parameters of the affine function were not chosen in this manner to fit the affine function , then it would be possible to choose two points with very close x - values ( _ i.e. _ , similar approximate values ) and disparate y - values ( _ i.e. _ , different exact values ) , and extrapolating to other points could propagate a large slope over a large distance ; using the extreme points forces the affine function to be a convex combination of the extrema , thereby avoiding this problem .",
    "@xmath162\\\\ \\shoveleft{\\subseteq   \\left [ \\lambda_m \\frac{\\| u^{(m_{\\max } ) } \\|_{p^*}}{k^\\frac{1}{p^ * } } + \\left ( 1 - \\lambda_m \\right ) \\frac{\\| u^{(m_{\\min } ) } \\|_{p^*}}{k^\\frac{1}{p^ * } } , \\right.}\\\\ \\left .",
    "\\lambda_m \\| u^{(m_{\\max } ) } \\|_{p^ * } + \\left ( 1 - \\lambda_m \\right ) \\| u^{(m_{\\min } ) } \\|_{p^ * } \\right]\\\\ \\shoveleft { = \\left [ k^\\frac{-1}{p^ * } \\left ( \\lambda_m \\| u^{(m_{\\max } ) } \\|_{p^ * } + \\left ( 1 - \\lambda_m \\right ) \\| u^{(m_{\\min } ) } \\|_{p^ * } \\right ) , \\right.}\\\\ \\left .",
    "\\lambda_m \\| u^{(m_{\\max } ) } \\|_{p^ * } + \\left ( 1 - \\lambda_m \\right ) \\| u^{(m_{\\min } ) } \\|_{p^ * } \\right]\\\\ \\shoveleft{= \\left [ k^\\frac{-1}{p^ * } \\| u^{(m ) } \\|_{p^ * } , \\| u^{(m ) } \\|_{p^ * } \\right]}\\end{gathered}\\ ] ]    the worst - case absolute error of the scaled problem on @xmath84 can be defined @xmath163 because the function @xmath164 is affine , it s derivative can never be zero , and thus lagrangian theory states that the maximum must occur at a boundary point .",
    "therefore , the worst - case absolute error is @xmath165 which is identical to the worst - case error bound before applying the affine transformation @xmath142 . thus applying",
    "the affine transformation can dramatically improve error , but will not make it worse than the original worst - case .",
    "one example that profits from fast max - convolution of non - negative vectors is computing the viterbi path using a hidden markov model ( hmm ) ( _ i.e. _ , the _ maximum a posteriori _ states ) with an additive transition function satisfying @xmath166 for some arbitrary function @xmath167 ( @xmath167 can be represented as a table , because we are considering all possible discrete functions ) .",
    "this additivity constraint is equivalent to the transition matrix being a `` toeplitz matrix '' : the transition matrix @xmath168 is a toeplitz matrix when all cells diagonal from each other ( to the upper left and lower right ) have identical values ( _ i.e. _ , @xmath169 ) . because of the markov property of the chain , we only need to max - marginalize out the latent variable at time @xmath170 to compute the distribution for the next latent variable @xmath171 and all observed values of the data variables @xmath172 .",
    "this procedure , called the viterbi algorithm , is continued inductively :    @xmath173    and continuing by exploiting the self - similarity on a smaller problem to proceed inductively , revealing a max - convolution ( for this specialized hmm with additive transitions ) : @xmath174 \\pr(d_{i-1 } | x_{i-1}=x_{i-1 } ) \\delta[x_i - x_{i-1 } ] = } \\\\ \\shoveright{\\left(fromleft[i-1]~likelihood[d_{i-1 } ] \\right ) ~*_{\\max}~ \\delta[x_i - x_{i-1}].}\\\\\\end{gathered}\\ ] ]    after computing this left - to - right pass ( which consisted of @xmath175 max - convolutions and vector multiplications ) , we can find the _ maximum a posteriori _ configuration of the latent variables @xmath176 backtracking right - to - left , which can be done by finding the variable value @xmath177 that maximizes @xmath178[x_i ] \\times \\delta[x_{i+1}^ * - x_i]$ ] ( thus defining @xmath179 and enabling induction on the right - to - left pass ) .",
    "the right - to - left pass thus requires @xmath180 steps ( * algorithm  [ algorithm : viterbi ] * ) .",
    "note that the full max - marginal distributions on each latent variable @xmath36 can be computed via a small modification , which would perform a more complex right - to - left pass that is nearly identical to the left - to - right pass , but which performs subtraction instead of addition ( _ i.e. _ , by reversing the vector representation of the pmf of the subtracted argument before it is max - convolved ;  @xcite ) .",
    "@xmath181 \\gets prior$ ] @xmath178 \\gets fromleft[i ] \\times likelihood[data[i]]$ ] @xmath182 \\gets fromleft[i ] * _ { \\max } \\delta$ ] @xmath183 \\gets fromleft[n ] \\times likelihood[data[n]]$ ] @xmath184 \\gets { \\operatornamewithlimits{argmax}}_{j } fromleft[n-1][j]$ ] @xmath185 @xmath186 @xmath187 \\times \\delta[l - path[i+1]]$ ] @xmath188 @xmath189 @xmath190 \\gets argmaxprodposterior$ ] @xmath191    we apply this hmm with additive transition probabilities to a data analysis problem from economics .",
    "it is known for example , that the current figures of unemployment in a country have ( among others ) impact on prices of commodities like oil .",
    "if one could predict unemployment figures before the usual weekly or monthly release by the responsible government bureaus , this would lead to an information advantage and an opportunity for short - term arbitrage .",
    "the close relation of economic indicators like market prices and stock market indices ( especially of indices combining several stocks of different industries ) to unemployment statistics can be used to tackle this problem .    in the following demonstration of our method",
    ", we create a simple hmm with additive transitions and use it to infer the _ maximum a posteriori _ unemployment statistics given past history ( _ i.e. _ how often unemployment is low and high , as well as how often unemployment goes down or up in a short amount of time ) and current stock market prices ( the observed data ) .",
    "we discretized random variables for the observed data ( s&p 500 , adjusted closing prices ; retrieved from yahoo !",
    "historical stock prices : http://data.bls.gov/cgi-bin/surveymost?bls series cuur0000sa0[http://data.bls.gov / cgi - bin / surveymost?bls series cuur0000sa0 ] ) , and `` latent '' variables ( unemployment insurance claims , seasonally adjusted , were retrieved from the u.s .",
    "department of labor : https://www.oui.doleta.gov/unemploy/claims.asp ) .",
    "stock prices were additionally inflation adjusted by ( _ i.e. _ divided by ) the consumer price index ( cpi ) ( retrieved from the u.s .",
    "bureau of labor statistics : https://finance.yahoo.com/q?s=^gspc[https://finance.yahoo.com/q?s=^gspc ] ) . the intersection of both `` latent '' and observed data was available weekly from week 4 in 1967 to week 52 in 2014 , resulting in 2500 data points for each type of variable .    to investigate the influence of overfitting",
    ", we partition the data in two parts , before june 2005 and after june 2005 , so that we are effectively training on @xmath192 of the data points , and then demonstrate the viterbi path on the entirety of the data ( both the @xmath193 training data and the @xmath194 of the data withheld from empirical parameter estimation ) .",
    "unemployment insurance claims were discretized into @xmath195 and stock prices were discretized into @xmath196 bins .",
    "simple empirical models of the prior distribution for unemployment , the likelihood of unemployment given stock prices , and the transition probability of unemployment were built as follows : the initial or prior distribution for unemployment claims at @xmath197 was calculated by marginalizing the time series of training data for the claims ( _ i.e. _ counting the number of times any particular unemployment value was reached over all possible bins ) .",
    "our transition function ( the conditional probability @xmath198 ) similarly counts the number of times each possible change @xmath199 occurred over all available time points .",
    "interestingly , the resulting transition distribution roughly resembles a gaussian ( but is not an exact gaussian ) .",
    "this underscores a great quality of working with discrete distributions : while continuous distributions may have closed - forms for max - convolution ( which can be computed quickly ) , discrete distributions have the distinct advantage that they can accurately approximate any smooth distribution .",
    "lastly , the likelihoods of observing a stock price given the unemployment at the same time were trained using an empirical joint distribution ( essentially a heatmap ) , which is displayed in * figure  [ figure : likelihoodheatmap]*.        we compute the viterbi path two times : first we use naive , exact max - convolution , which requires a total of @xmath200 steps .",
    "second , we use fast numerical max - convolution , which requires @xmath201 steps . despite the simplicity of the model ,",
    "the exact viterbi path ( computed via exact max - convolution ) is highly informative for predicting the value of unemployment , even for the @xmath194 of the data that were not used to estimate the empirical prior , likelihood , and transition distributions .",
    "also , the numerical max - convolution method is nearly identical to the exact max - convolution method at every index ( * figure  [ figure : viterbi ] * ) . even with a fairly rough discretization ( _ i.e. _ , @xmath202 ) , the fast numerical method used @xmath203 seconds compared to the @xmath204 seconds required by the naive approach .",
    "this speedup will increase dramatically as @xmath28 is increased , because the @xmath205 term in the runtime of the numerical max - convolution method is essentially bounded above @xmath206 .",
    "although the @xmath29-norm provides a good approximation of the chebyshev norm , it discards significant information ; specifically the curve @xmath66 for various @xmath207 could be used to identify and correct the worst - case scenario where @xmath208 ; using only two points , the exact value of @xmath63 can be computed for those worst - case @xmath21 vectors by computing the norms at two different @xmath29 values and solving the following equations for @xmath209 : @xmath210 where the proportionality constant is @xmath211 and where the computed value @xmath209 yields the exact chebyshev norm @xmath135 .      more generally , when there are @xmath212 unique values ( @xmath213 ) in @xmath21 , we can model the norms perfectly with @xmath214 where @xmath215 is an integer that indicates the number of times @xmath213 occurs in @xmath21 ( and where @xmath216 ) . this multi - set view of the vector @xmath21 can be used to project it down to a dimension @xmath217 : @xmath218 \\cdot \\left [ \\begin{array}{c } n_1\\\\ n_2\\\\ n_3\\\\ \\vdots\\\\ n_r\\\\ \\end{array } \\right ] = \\left [ \\begin{array}{c } \\| u^{(m ) } \\|_{p^*}^{p^ * } \\\\ \\| u^{(m ) } \\|_{2 { p^*}}^{2 { p^*}}\\\\ \\| u^{(m ) } \\|_{2 { p^*}}^{2 { p^*}}\\\\ \\vdots\\\\ \\| u^{(m ) } \\|_{\\ell { p^*}}^{\\ell { p^*}}\\\\ \\end{array } \\right].\\ ] ] by solving the above system of equations for all @xmath219 , the maximum @xmath220 can be used to approximate the true maximum @xmath221",
    ". this projection can be thought of as querying distinct moments of the distribution @xmath222 that corresponds to some unknown vector @xmath21 , and then assembling the moments into a model in order to predict the unknown maximum value in @xmath21 .",
    "of course , when @xmath217 , the number of terms in our model , is sufficiently large , then computing @xmath217 norms of @xmath21 will result in an exact result , but it could result in @xmath223 execution time , meaning that our numerical max - convolution algorithm becomes quadratic ; therefore , we must consider that a small number of distinct moments are queried in order to estimate the maximum value in @xmath21 .",
    "regardless , the system of equations above is quite difficult to solve directly via elimination for even very small values of @xmath217 , because the symbolic expressions become quite large and because symbolic polynomial roots can not be reliably computed when the degree of the polynomial is @xmath224 . even in cases",
    "when it can be solved directly , it will be far too inefficient .",
    "for this reason , we solve for the @xmath219 values using an exact , alternative approach : if we define a polynomial @xmath225 , then @xmath226 .",
    "we can expand @xmath227 , and then write    @xmath228 \\cdot \\left [ \\begin{array}{cccc } \\alpha_1^{p^ * } & \\alpha_2^{p^ * } & & \\alpha_r^{p^ * } \\\\ \\alpha_1^{2 { p^ * } } & \\alpha_2^{2 { p^ * } } & \\cdots & \\alpha_r^{2 { p^ * } } \\\\",
    "\\alpha_1^{3 { p^ * } } & \\alpha_2^{3 { p^ * } } & & \\alpha_r^{3 { p^ * } } \\\\ \\vdots & \\vdots & & \\vdots \\\\ \\alpha_1^{\\ell { p^ * } } & \\alpha_2^{\\ell { p^ * } } & & \\alpha_r^{\\ell { p^ * } } \\\\ \\end{array } \\right ] \\cdot \\left [ \\begin{array}{ccccc } n_1\\\\ n_2\\\\ n_3\\\\ \\vdots\\\\ n_r\\\\ \\end{array } \\right ] = \\\\ \\left [ \\begin{array}{ccccc } \\alpha_1^{p^ * } \\gamma(\\alpha_1^{p^ * } ) & \\alpha_2^{p^ * } \\gamma(\\alpha_2^{p^ * } ) & \\alpha_3^{p^ * } \\gamma(\\alpha_3^{p^ * } ) & \\cdots & \\alpha_r^{p^ * } \\gamma(\\alpha_r^{p^ * } ) \\end{array } \\right ] \\cdot \\left [ \\begin{array}{ccccc } n_1\\\\ n_2\\\\ n_3\\\\ \\vdots\\\\ n_r\\\\ \\end{array } \\right ] = \\\\ \\left [ \\begin{array}{ccccc } 0 & 0 & 0 & \\cdots & 0 \\end{array } \\right ] \\cdot   \\left [ \\begin{array}{ccccc } n_1\\\\ n_2\\\\ n_3\\\\ \\vdots\\\\ n_r\\\\ \\end{array } \\right ] = 0,\\end{gathered}\\ ] ]    which indicates that @xmath229 \\cdot \\left [ \\begin{array}{c } \\| u^{(m ) } \\|_{p^*}^{p^ * } \\\\ \\| u^{(m ) } \\|_{2 { p^*}}^{2 { p^*}}\\\\ \\| u^{(m ) } \\|_{2 { p^*}}^{2 { p^*}}\\\\ \\vdots\\\\ \\| u^{(m ) } \\|_{\\ell { p^*}}^{\\ell { p^*}}\\\\ \\end{array } \\right ] = 0.\\ ] ]    furthermore , @xmath230 ; therefore we can write @xmath231 \\cdot \\left [ \\begin{array}{c } \\| u^{(m ) } \\|_{p^*}^{p^ * } \\\\ \\| u^{(m ) } \\|_{2 { p^*}}^{2 { p^*}}\\\\ \\| u^{(m ) } \\|_{2 { p^*}}^{2 { p^*}}\\\\ \\vdots\\\\ \\| u^{(m ) } \\|_{\\ell { p^*}}^{\\ell { p^*}}\\\\ \\end{array } \\right ] = \\\\ \\left [ \\begin{array}{ccccc } \\| u^{(m ) } \\|_{p^*}^{p^ * } & \\| u^{(m ) } \\|_{2 { p^*}}^{2 { p^ * } } & \\| u^{(m ) } \\|_{3 { p^*}}^{3 { p^ * } } & \\cdots & \\| u^{(m ) } \\|_{(r+1 ) { p^*}}^{(r+1 ) { p^ * } } \\\\ \\| u^{(m ) } \\|_{2 { p^*}}^{2 { p^ * } } & \\| u^{(m ) } \\|_{3 { p^*}}^{3 { p^ * } } & \\| u^{(m ) } \\|_{4 { p^*}}^{4 { p^ * } } & \\cdots & \\| u^{(m ) } \\|_{(r+2 ) { p^*}}^{(r+2 ) { p^ * } } \\\\ \\| u^{(m ) } \\|_{3 { p^*}}^{3 { p^ * } } & \\| u^{(m ) } \\|_{4 { p^*}}^{4 { p^ * } } & \\| u^{(m ) } \\|_{5 { p^*}}^{5 { p^ * } } & \\cdots & \\| u^{(m ) } \\|_{(r+3 ) { p^*}}^{(r+3 ) { p^ * } } \\\\ & & \\vdots & & \\\\",
    "\\| u^{(m ) } \\|_{(\\ell - r-1 ) { p^*}}^{(\\ell - r-1 ) { p^ * } } & \\cdots & \\| u^{(m ) } \\|_{(\\ell-2 ) { p^*}}^{(\\ell-2 ) { p^ * } } & \\| u^{(m ) } \\|_{(\\ell-1 ) { p^*}}^{(\\ell-1 ) { p^ * } } & \\| u^{(m ) } \\|_{\\ell { p^*}}^{\\ell { p^ * } } \\\\",
    "\\end{array } \\right ] \\cdot \\left [ \\begin{array}{c } \\gamma_0\\\\ \\gamma_1\\\\ \\gamma_2\\\\ \\vdots\\\\ \\gamma_r \\end{array } \\right ] = 0.\\end{gathered}\\ ] ]    therefore , @xmath232",
    "\\in null\\left ( \\left [ \\begin{array}{ccccc } \\| u^{(m ) } \\|_{p^*}^{p^ * } & \\| u^{(m ) } \\|_{2 { p^*}}^{2 { p^ * } } & \\| u^{(m ) } \\|_{3 { p^*}}^{3 { p^ * } } & \\cdots & \\| u^{(m ) } \\|_{(r+1 ) { p^*}}^{(r+1 ) { p^ * } } \\\\",
    "\\| u^{(m ) } \\|_{2 { p^*}}^{2 { p^ * } } & \\| u^{(m ) } \\|_{3 { p^*}}^{3 { p^ * } } & \\| u^{(m ) } \\|_{4 { p^*}}^{4 { p^ * } } & \\cdots & \\| u^{(m ) } \\|_{(r+2 ) { p^*}}^{(r+2 ) { p^ * } } \\\\ \\| u^{(m ) } \\|_{3 { p^*}}^{3 { p^ * } } & \\| u^{(m ) } \\|_{4 { p^*}}^{4 { p^ * } } & \\| u^{(m ) } \\|_{5 { p^*}}^{5 { p^ * } } & \\cdots & \\| u^{(m ) } \\|_{(r+3 ) { p^*}}^{(r+3 ) { p^ * } } \\\\ & & \\vdots & & \\\\",
    "\\| u^{(m ) } \\|_{(\\ell - r-1 ) { p^*}}^{(\\ell - r-1 ) { p^ * } } & \\cdots & \\| u^{(m ) } \\|_{(\\ell-2 ) { p^*}}^{(\\ell-2 ) { p^ * } } & \\| u^{(m ) } \\|_{(\\ell-1 ) { p^*}}^{(\\ell-1 ) { p^ * } } & \\| u^{(m ) } \\|_{\\ell { p^*}}^{\\ell { p^ * } } \\\\",
    "\\end{array } \\right ] \\right).\\ ] ]    because the columns of @xmath218\\ ] ] must be linearly independent when @xmath233 are distinct ( which is the case by the definition of our multiset formulation of the norm ) , then @xmath234 will determine a unique solution ; thus the null space above is computed from a matrix with @xmath235 columns and @xmath217 rows , yielding a single vector for @xmath236 .",
    "this vector can then be used to compute the roots of the polynomial @xmath237 , which will determine the values @xmath238 , which can each be taken to the @xmath239 power to compute @xmath240 ; the largest of those @xmath219 values is used as the estimate of the maximum element in @xmath21 .",
    "when @xmath21 contains at least @xmath217 distinct values ( _ i.e. _ , @xmath241 ) , then the problem will be well - defined ; thus , if the roots of the null space spanning vector are not well - defined , then a smaller @xmath217 can be used ( and should be able to compute an exact estimate of the maximum , since @xmath21 can be projected exactly when @xmath217 is the precise number of unique elements found in @xmath21 ) .    note that this projection method is valid for any sequence of norms with even spacing : @xmath242 .",
    "in general , the computation of both the null space spanning vector @xmath236 and of machine - precision approximations for the roots of the polynomial @xmath244 ( which can be approximated by constructing a matrix with that characteristic polynomial and performing eigendecomposition  @xcite ) are both in @xmath245 for each index @xmath22 in the result ; however , by using a small @xmath243 , we can compute a closed form solution of both the null space spanning vector and of the resulting quadratic roots .",
    "this enables faster exploitation of the curve of norms for estimating the maximum value of @xmath21 ( although it does nt achieve the high accuracy possible with a much larger @xmath246 ) .",
    "this is equivalent to approximating @xmath247 , where @xmath248 .    in this case , the single spanning vector of the null space of @xmath249\\ ] ] will be @xmath250 =   \\left [ \\begin{array}{c } \\| u^{(m ) } \\|_{2 p^*}^{2 p^ * } \\| u^{(m ) } \\|_{4 p^*}^{4 p^ * } - { \\left ( \\| u^{(m ) } \\|_{3 p^*}^{3 p^ * } \\right)}^2\\\\ \\| u^{(m ) } \\|_{p^*}^{p^ * } \\| u^{(m ) } \\|_{4 p^*}^{4 p^ * } - \\| u^{(m ) } \\|_{2 p^*}^{2 p^ * } \\| u^{(m ) } \\|_{3 p^*}^{3 p^*}\\\\ \\| u^{(m ) } \\|_{p^*}^{p^ * } \\| u^{(m ) } \\|_{3 p^*}^{3 p^ * } - { \\left ( \\| u^{(m ) } \\|_{2 p^*}^{2 p^ * } \\right)}^2\\\\ \\end{array } \\right]\\ ] ] and thus @xmath251 can be computed by using the quadratic formula to solve @xmath252 for @xmath64 , and computing @xmath253 using the maximum of those zeros : @xmath254 .",
    "when the quadratic is not well defined , then this indicates that the number of unique elements in @xmath21 is less than 2 , and thus can not be projected uniquely ( _ i.e. _ , @xmath255 ) ; in this case , the closed - form linear solution can be used rather than a closed - form quadratic solution : @xmath256 when the closed - form linear solution is not numerically stable ( due to division by a value close to zero ) , then the @xmath29-norm approximation can likewise be used .      because the norms must have evenly spaced @xmath29 values in order to use the projection method described above",
    ", the exponential sequence of @xmath29 values used in the original piecewise algorithm will not contain four evenly spaced points ( which are necessary to solve the quadratic formulation , _",
    "i.e. _ @xmath243 ) .",
    "one possible solution would be to take the maximal stable value of @xmath29 for any index ( which will be a power of two found using the original piecewise method ) , and then also computing norms ( via the fft , as before ) for @xmath257 ; however , this will result in a @xmath258 slowdown in the algorithm , because for every @xmath29-norm computed via fft before , now four must be computed .",
    "an alternative approach reuses existing values in the @xmath259 sequence of @xmath29 : for @xmath29 sufficiently large , then the exponential sequence is guaranteed to include these stable @xmath29 values : @xmath260 . by considering @xmath261 in @xmath29 candidates , then we can be guaranteed to have four evenly spaced and stable @xmath29 values .",
    "this can be achieved easily by noting that @xmath262 meaning that we can insert all possible necessary @xmath29 values for evenly spaced sequences of length four by first computing the exponential sequence of @xmath29 values and then inserting the averages between every pair of adjacent powers of two ( and inserting them in a way that maintains the sorted order ) : @xmath263 becomes @xmath264 .",
    "thus , if ( for some index @xmath22 ) 16 is the highest stable @xmath29 that is a power of two ( _ i.e. _ , the @xmath29 value that would be used by the original piecewise algorithm ) , then we are guaranteed to use the evenly spaced sequence @xmath265 . by interleaving the powers of two with the averages from the following powers of two , we reduce the number of ffts to @xmath266 that used by the original piecewise algorithm . for small values of @xmath217 ( such as the @xmath243 used here ) , the estimation of the maximum from each sequence of four norms is in @xmath267 , meaning the total time will still be @xmath268 , which is the same as before . because the spacing in this formulation is @xmath269 , and given the maximal root of the quadratic polynomial @xmath270 , then @xmath271 ( taking the maximal root @xmath272 to the power @xmath273 instead of @xmath274 , which had been the spacing used in the description of the projection method ) .",
    "the null space projection method is shown in * algorithm  [ algorithm : piecewisewithprojection]*.    @xmath70 $ ] @xmath71 $ ] @xmath72}$ ] @xmath73}$ ]    @xmath275{\\log_2(p^*_{\\max } ) } } ] $ ]    @xmath276 \\gets allpstar[i ] $ ] @xmath277 \\gets 0.5 \\times ( allpstar[i]+allpstar[i+1 ] ) $ ]    @xmath75 \\gets$ ] ` fftnonnegmaxconvolvegivenpstar`(@xmath76 , @xmath77 , @xmath278 $ ] )    @xmath79 \\gets \\max \\ { i:~ { \\left ( resforallpstar[i][m ] \\right)}^{\\text{allpstarinterleaved[$i$ ] } } \\geq \\tau ) \\}$ ]    @xmath279 -= maxstablepstarindex[o ] \\% 2 $ ]    @xmath280 $ ] @xmath281 @xmath282 $ ] @xmath283 - 1]$ ] @xmath284 \\gets$ ] ` maxlin`@xmath285 @xmath286 - 2]$ ] @xmath287 - 4]$ ] @xmath284 \\gets$ ] ` maxquad`@xmath288    @xmath147`affinecorrect`@xmath148    @xmath82 \\times r[r_{\\max } ] \\times result$ ]    @xmath289 @xmath290 @xmath291    @xmath292 @xmath293 @xmath294    @xmath295 @xmath296    @xmath297 @xmath298 ` maxlin`@xmath285 @xmath291      the full closed - form of the quadratic roots used above ( which solve the projection when @xmath243 ) will be    @xmath299    where @xmath300 in the pseudocode ( _ i.e. _ , the maximum numerically stable @xmath29 used by the piecewise algorithm at that index ) .",
    "note that @xmath301 can be factored out because the exponents in every term in the numerator will be @xmath302 ( _ i.e. _ , @xmath303 in the square root ) .",
    "similarly the terms in the denominator each contain @xmath304 .",
    "factoring out the maximum value is then the same as operating on scaled vectors @xmath305 ( instead of @xmath306 ) with the maximum entry being @xmath50 , and at least one element of value @xmath50 .",
    "furthermore , the denominator @xmath307 ; even though the terms summed to compute @xmath308 are not exclusively nonnegative , symmetry can be used to demonstrate that every negative term is outweighed by a unique corresponding term : @xmath309 thus , for well - defined problems ( _ i.e. _ , when @xmath310 ) , the denominator @xmath311 , and therefore , the maximum root of the quadratic polynomial will correspond to the term that adds ( rather than subtracts ) the square root term : @xmath312    the relative absolute error is defined as @xmath313 ; therefore , a bound on the relative error of the projection method can be established by bounding @xmath314 where the length of the @xmath21 ( respectively @xmath89 ) is @xmath90 . using this reformulation , @xmath315 indicates a zero - error approximation .",
    "this can be rewritten to bound its value before taking to the power @xmath274 : @xmath316 where @xmath317    the extreme values of @xmath318 can be found by minimizing and maximizing over the possible values of @xmath319}^{k_m } : \\exists i , v_i = 1 , \\exists j , v_j \\in ( 0,1 ) \\}$ ] .",
    "the final constraint on @xmath320 in ( 0,1 ) is because any @xmath305 containing only one unique value ( which must be @xmath50 in this case since dividing by the maximum element in @xmath21 to compute @xmath89 has divided the value at that index by itself ( @xmath321 ) will lead to instabilities . when values in @xmath305 are identical to one another , using @xmath322 yields an exact solution , and thus solving with @xmath243 is not well - defined because @xmath323 . because all elements @xmath324 $ ] and @xmath102 , we can perform a change of variables @xmath325 , thereby eliminating references to @xmath29 : @xmath326 @xmath327    r|ccccc @xmath90 & 3 & 4 & 5 & 6 & 7 +   + minimum & 0.935537 & 0.902161 & 0.895671 & 0.880487 & 0.85343 + maximum & 1 & 1 & 1 & 1 & 1 +    for small vector lengths , the exact bounds of @xmath328 are shown in * table  [ table : boundsmathematica]*. notice that the upper bound is fixed , but the lower bound grows monotonically smaller as @xmath90 , the length of the vector considered , increases . for larger vectors ,",
    "mathematica does not find optima in a matter of hours , and for arbitrary - length vectors , the karush - kuhn - tucker criteria do not easily yield minima or maxima ; however , we do observe that all maxima are achieved by vectors that are permutations ( order does not influence the result ) of @xmath329 ( again , when only two unique values are found in @xmath305 , the approximation is exact and thus @xmath330 ) . likewise ,",
    "the minima are achieved by permutations of @xmath331 .",
    "for this reason , we perform further empirical estimation of the bound by randomly sampling vectors of the form @xmath332 with @xmath333 degrees of freedom ( d.o.f . ) and sampling vectors of the form @xmath334 ( with @xmath335 d.o.f . ) , whose extrema are shown in * table  [ table : boundspysim]*.    r|ccccc @xmath90 & 4 & 64 & 1024 +   + minimum ( @xmath333 d.o.f . ) & 0.90221268 & 0.74942834 & 0.81858283 + maximum ( @xmath333 d.o.f . ) & 0.99999986 & 0.92482416 & 0.86795636 +   + minimum ( vectors of form @xmath336 , @xmath335 d.o.f . ) & 0.90216688 & 0.75455478 & 0.71695386 + maximum ( vectors of form @xmath336 , @xmath335 d.o.f . ) & 1.00000000 & 1.0000000 & 1.00000000 +    at length @xmath337 we see that due to an extreme value scenario , an unconstrained vector scores slightly lower than a vector holding the worst - case pattern @xmath336 , because both forms of sampling approach the true lower bound , but one of the unconstrained @xmath338 d.o.f .",
    "is slightly closer .    from these results",
    ", we conjecture that @xmath339 is bounded above @xmath340 ( this is achievable at any length @xmath90 by letting @xmath305 contain exactly two distinct values ) . in this manner",
    ", we achieve our predicted upper bound of @xmath50 regardless of the length @xmath90 .",
    "likewise , we conjecture that at any @xmath90 ( not simply the lengths investigated , where this principle is true ) , the lower bound is given by vectors of the form @xmath341 .",
    "qualitatively , this conjecture stems from the fact that since the estimate is perfect when @xmath305 contains exactly two distinct elements , then the worst - case lower bound when @xmath305 contains three distinct values will concentrate the points at some value far from the other two distinct values .",
    "when four distinct values are permitted , then we conjecture that the optimal choice ( for minimizing @xmath339 ) of the fourth value will equal the choice for the third distinct value , since that was already determined to be the best point for deceiving the quadratic approximation . from this conjecture",
    ", we can then use the fact that the bounds should only grow more extreme as @xmath90 increases , since @xmath342 ( _ i.e. _ lower - dimensional solutions can always be reached in a higher dimension by setting some of the values to @xmath54 ) .",
    "thus the minimum for any possible vector should be conservatively bounded below on vectors of the form @xmath341 and is achieved by letting @xmath90 approach @xmath343 : @xmath344 the minimum value of this expression over all @xmath345 , b \\in [ 0,1]$ ] is @xmath346 ( computed again with mathematica ) .",
    "overall , assuming our conjecture regarding the forms of the vectors achieving the minima and maxima , then it follows that @xmath347 $ ] , and the worst - case relative error at the @xmath96 contour will be bounded @xmath348 the steeper decrease in relative error as @xmath29 is increased means that the same procedure can be used to achieve an absolute error bound : @xmath349 which achieves a unique maximum at @xmath350 as before , the worst - case absolute error of the unscaled problem will be found by simply scaling the absolute error at @xmath112 : @xmath126~ \\max_r r[r]~ \\tau^{\\frac{1}{2 p^*_{mode } } } \\left ( 1 -      { 0.7}^{\\frac{4}{p^*_{mode } } } \\right).\\ ] ] because @xmath112 ( the value of @xmath29 producing the worst - case absolute error ) for the null space projection method it is invariant to the length of the list @xmath28 ( enabling us to compute a numeric value ) , and because its numeric value is so small , even a fairly small choice of @xmath96 will suffice ( now @xmath351 rather than in @xmath352 as it was with the original piecewise method ) . for example , the approximation of the viterbi path to infer the unemployment data is slightly superior with the null space projection method , even when @xmath353 is used ( in contrast to the @xmath354 used in the figure  [ figure : viterbi ] ) . the null space projection method required @xmath355 seconds ( slightly faster than the @xmath203 seconds required by the original piecewise method ) .",
    "the one caveat of this worst - case absolute error bound is that it presumes at least four evenly spaced , stable @xmath29 can be found ( which may not be the case by choosing @xmath29 from the sequence @xmath259 in cases when @xmath356 ) ; however , assuming standard fast convolution can be performed ( a reasonable assumption given it is one of the essential numeric algorithms ) , then four evenly spaced @xmath29 values could be chosen very close to @xmath50 ; therefore , these values of @xmath29 could be added to the sequence so that the algorithm is slightly slower , but essentially always yield this worst - case absolute error bound .    in practice , we can demonstrate that the null space projection method is very accurate .",
    "first we show the impact of using the quadratic ( _ i.e. _ , @xmath243 ) projection method on unscaled single @xmath21 vectors .",
    "the projection method was tested on vectors of different lengths drawn from different types of beta distributions and are compared with the results of the @xmath1-norms with the highest stable @xmath1 ( figure  [ figure : noextrapolvsextrapol ] ) .",
    "the relative errors between the original piecewise method and the null space projection method are compared using a max - convolution on two randomly created input pmfs of lengths 1024 ( figure  [ figure : affinevsaffineextrapol ] ) .",
    "note that the null space projection can also be paired with affine scaling on the back end , just as the original piecewise method can be . in practice",
    ", the null space projection increases the accuracy demonstrably on a variety of different problems , although the original piecewise method also performs well .",
    "although the worst - case runtime of the null space projection method is roughly @xmath266 that of the original piecewise method , the error bound no longer depends on the length of the result @xmath28 .",
    "thus , for a given relative error bound on the top contour ( _ i.e. _ , the equivalent of the derivation of @xmath96 in the original piecewise algorithm ) , the value of @xmath96 is fixed and no longer @xmath357 .",
    "for example , achieving a @xmath358 relative error in the top contour would require @xmath359 meaning that choosing @xmath360 would achieve a very high accuracy , but while only performing @xmath361 ffts .",
    "for very large vectors , this will not be substantially more expensive than the original piecewise algorithm , which uses a higher value of @xmath96 ( in this case , @xmath362 , which continues to grow as @xmath28 does ) to keep the error lower in practice . as a result",
    ", the runtime of the null space projection approximation is @xmath363 rather than @xmath364 , despite the similar runtime in practice to the original piecewise method ( the null space projection method uses @xmath266 as many ffts performed per @xmath29 value , but requires slightly fewer @xmath29 values ) .",
    "r|ccccccc @xmath28 & @xmath365 & @xmath366 & @xmath367 & @xmath368 & @xmath369 & @xmath370 & @xmath371 +   + naive & * 0.0142 * & 0.0530 & 0.192 & 0.767 & 3.03 & 12.1 & 48.2 + naive ( vectorized ) & 0.0175 & 0.0381 & 0.0908 & 0.251 & 0.790 & 2.75 & 10.1 + fill1 ( @xcite ) & 0.0866 & 1.09 & 7.21 & 19.4 & 457 &  &  + max .",
    "stable @xmath29 , affine corrected & 0.0277 & 0.0353 & 0.0533 & 0.0848 & 0.149 & 0.274 & 0.537 + projection , affine corrected & 0.0236 & * 0.0307 * & * 0.0467 * & * 0.0760 * & * 0.137 * & * 0.258 * & * 0.520 * +    to compare the actual runtimes of the final algorithm developed in this manuscript with a naive max - convolution and a previously proposed method from @xcite , all methods were run on vectors of different random ( uniform in @xmath372 $ ] ) composition and length ( @xmath28 ) .",
    "the first and second input vector were generated seperately but are always of same length .",
    "* table  [ table : runtimesinclbussieck ] * shows the result of this experiment .",
    "all methods were implemented in python , using numpy where applicable ( _ e.g. _ to vectorize ) . a non - vectorized version of naive max - convolution was included to estimate the effects of vectorization .",
    "the approach from bussieck et al . ran as a reimplementation based on the pseudocode in their manuscript . from their variants of proposed methods ,",
    "fill1 was chosen because of its use in their corresponding benchmark and its recommendation by the authors for having a lower runtime constant in practice compared to other methods they proposed .",
    "the method is based on sorting the input vectors and traversing the ( implicitly ) resulting partially ordered matrix of products in a way that not all entries need to be evaluated , while only keeping track of the so - called cover of maximal elements .",
    "fill1 already includes some more sophisticated checks to keep the cover small and thereby reducing the overhead per iteration .",
    "unfortunately , although we observed that the fill1 method requires between @xmath373 and @xmath374 iterations in practice , this per - iteration overhead results in a worst - case cost of @xmath375 per iteration , yielding an overall runtime in practice between @xmath376 and @xmath377 . as the authors state , this overhead is due to the expense of storing the cover , which can be implemented _",
    "e.g. _ using a binary heap ( recommended by the authors and used in this reimplementation ) . additionally , due to the fairly sophisticated datastructures needed for this algorithm it had a higher runtime constant than the other methods presented here , and furthermore we saw no means to vectorize it to improve the efficiency . for this reason ,",
    "it is not truly fair to compare the raw runtimes to the other vectorized algorithms ( and it is not likely that this python reimplementation is as efficient as the original version , which @xcite implemented in ansi - c ) ; however , comparing a non - vectorized implementation of the naive @xmath374 approach with its vectorized counterpart gives an estimated @xmath378 speedup from vectorization , suggesting that it is not substantially faster than the naive approach on these problems ( it should be noted that whereas the methods presented here have tight runtime bounds but produce approximate results , the fill1 algorithm is exact , but its runtime depends on the data processed ) . during investigation of these runtimes , we found that on the given problems , the proposed average case of @xmath373 iterations was rarely reached .",
    "a reason might be an unrecognized violation of the assumptions of the theory behind this theoretical average runtime in how the input vectors were generated .",
    "in contrast to the exact method from @xcite , the herein proposed approximate procedure are faster whenever the input vectors are at least @xmath196 elements long ( shorter vectors are most efficiently processed with the naive approach ) .",
    "the null space projection method is the fastest method presented here ( because it can use a lower @xmath96 ) , although the higher density of @xmath29 values it uses ( and thus , additional ffts ) make the runtimes nearly identical for both approximation methods .",
    "both piecewise numerical max - convolution methods are highly accurate in practice and achieve a substantial speedup over both the naive approach and the approach proposed by @xcite .",
    "this is particularly true for large problems : for the original piecewise method presented here , the @xmath379 multiplier may never be small , but it grows so slowly with @xmath28 that it will be @xmath380 even when @xmath28 is on the same order of magnitude as the number of particles in the observable universe .",
    "this means that , for all practical purposes , the method behaves asymptotically as a slightly slower @xmath20 method , which means the speedup relative to the naive method becomes more pronounced as @xmath28 becomes large . for the second method presented ( the null space projection ) , the runtime for a given relative error bound will be in @xmath33 . in practice ,",
    "both methods have similar runtime on large problems .    the basic motivation of the first approach described _ i.e. _ , the idea of approximating the chebyshev norm with the largest @xmath29-norm that can be computed accurately , and then convolving according to this norm using fft also suggests further possible avenues of research .",
    "for instance , it may be possible to compute a single fft ( rather than an fft at each of several contours ) on a more precise implementation of complex numbers .",
    "such an implementation of complex values could store not only the real and imaginary components , but also other much smaller real and imaginary components that have been accumulated through @xmath9 operations , even those which have small enough magnitudes that they are dwarfed by other summands .",
    "with such an approach it would be possible to numerically approximate the max - convolution result in the same overall runtime as long as only a bounded `` history '' of such summands was recorded ( _ i.e. _ , if the top few magnitude summands  whether that be the top 7 or the top @xmath381was stored and operated on ) . in a similar vein , it would be interesting to investigate the utility of complex values that use rational numbers ( rather than fixed - precision floating point values ) , which will be highly precise , but will increase in precision ( and therefore , computational complexity of each arithmetic operation ) as the dynamic range between the smallest and largest nonzero values in @xmath25 and @xmath26 increases ( because taking @xmath76 to a large power @xmath29 may produce a very small value ) . other simpler improvements could include optimizing the error vs. runtime trade - off between the log - base of the contour search : the method currently searches @xmath127 contours , but a smaller or larger log - base could be used in order to optimize the trade - off between error and runtime .",
    "it is likely that the best trade - off will occur by performing the fast @xmath29-norm convolution with a number type that sums values over vast dynamic ranges by appending them in a short ( _ i.e. _ , bounded or constant size ) list or tree and sums values within the same dynamic range by querying the list or tree and then summing in at the appropriate magnitude .",
    "this is reminiscent of the fast multipole algorithm  @xcite .",
    "this would permit the method to use a single large @xmath29 rather than a piecewise approach , by moving the complexity into operations on a single number rather than by performing multiple ffts with simple floating - point numbers .",
    "the basic motivation of the second approach described _ i.e. _ , using the _ sequence _ of @xmath29-norms ( each computed via fft ) to estimate the maximum value generalizes the @xmath29-norm fast convolution numerical approach into an interesting theoretical problem in its own right : given an oracle that delivers a small number of norms ( the number of norms retrieved must be @xmath382 to significantly outperform the naive quadratic approach ) about each vector @xmath21 , amalgamate these norms in an efficient manner to estimate the maximum value in each @xmath21 .",
    "this method may be applicable to other problems , such as databases where the maximum values of some combinatorial operation ( in this case the _ maximum a posteriori _ distribution of the sum of two random variables @xmath383 ) is desired but where caching all possible queries and their maxima would be time or space prohibitive . in a manner reminiscent of how we employ fft , it may be possible to retrieve moments of the result of some combinatoric combination between distributions on the fly , and then use these moments to approximate true maximum ( or , in general , other sought quantities describing the distribution of interest ) .    in practice",
    ", the worst - case relative error of our quadratic approximation is quite low .",
    "for example , when @xmath384 is stable , then the relative error is less than @xmath385 , regardless of the lengths of the vectors being max - convolved .",
    "in contrast , the worst - case relative error using the original piecewise method would be @xmath386 , where @xmath28 is the length of the max - convolution result ( when @xmath387 , the relative error of the original piecewise method would be @xmath388 ) .    of course",
    ", the use of the null space projection method is predicated on the existence of at least four sequential @xmath29 points , but it would be possible to use finer spacing between @xmath29 values ( _ e.g. _ , @xmath389 to guarantee that this will essentially be the case as long as fft ( _ i.e. _ @xmath120 ) is stable . but more generally , the problem of estimating extrema from @xmath29-norms ( or , equivalently , from the @xmath29-th roots of the @xmath29-th moments of a distribution with bounded support ) , will undoubtedly permit many more possible approaches that we have not yet considered .",
    "one that would be compelling is to relate the fourier transform of the sequential moments to the maximum value in the distribution ; such an approach could permit all stable @xmath29 at any index @xmath22 to be used to efficiently approximate the maximum value ( by computing the fft of the sequence of norms ) .",
    "such new adaptations of the method could permit low worst - case error without any noticable runtime increase .",
    "the fast numerical piecewise method for max - convolution ( and the affine piecewise modification ) are both applicable to matrices as well as vectors ( and , most generally , to tensors of any dimension ) .",
    "this is because the @xmath29-norm ( as well as the derived error bounds as an approximation of the chebyshev norm ) can likewise approximate the maximum element in the tensor @xmath390 generated to find the max - convolution result at index @xmath391 of a multidimensional problem , because the sum @xmath392 computed by convolution corresponds to the frobenius norm ( _ i.e. _ the `` entrywise norm '' ) of the tensor , and after taking the result of the sum to the power @xmath274 , will converge to the maximum value in the tensor ( if @xmath29 is large enough ) .",
    "this means that the fast numerical approximation , including the affine piecewise modification , can be used without modification by invoking standard multidimensional convolution ( _ i.e. _ , @xmath32 ) .",
    "matrix ( and , in general , tensor ) convolution is likewise possible for any dimension via the row - column algorithm , which transforms the fft of a matrix into sequential ffts on each row and column .",
    "the accompanying python code demonstrates the fast numerical max - convolution method on matrices , and the code can be run on tensors of any dimension ( without requiring any modification ) .",
    "the speedup of fft tensor convolution ( relative to naive convolution ) becomes considerably higher as the dimension of the tensors increases ; for this reason , the speedup of fast numerical max - convolution becomes even more pronounced as the dimension increases .",
    "for a tensor of dimension @xmath393 and width @xmath28 ( _ i.e. _ , where the index bounds of every dimension are @xmath394 ) , the cost of naive max - convolution will be in @xmath395 , whereas the cost of numerical max - convolution is @xmath396 ( ignoring the @xmath397 multiplier ) , meaning that there is an @xmath398 speedup from the numerical approach .",
    "examples of such tensor problems include graph theory , where adjacency matrix representation can be used to describe respective distances between nodes in a network .    as a concrete example",
    ", the demonstration python code computes the max - convolution between two @xmath399 matrices .",
    "the naive method required @xmath400 seconds , but the numerical result with the original piecewise method was computed in @xmath401 seconds ( yielding a maximum absolute error of @xmath402 and a maximum relative error of @xmath403 ) and the numerical result with the null space projection method was computed in @xmath404 seconds ( using @xmath360 , which corresponds to a relative error of @xmath405 in the top contour , yielding a maximum absolute error of @xmath406 and a maximum relative error of @xmath407 ) and in @xmath408 seconds ( using @xmath409 , which corresponds to a relative error of @xmath410 in the top contour , yielding a maximum absolute error of @xmath411 and a maximum relative error of @xmath412 ) . not only does the speedup of the proposed methods relative to naive max - convolution increase significantly as the dimension of the tensor is increased , no other faster - than - naive algorithms exist for max - convolution of matrices or tensors .",
    "multidimensional max - convolution can likewise be applied to hidden markov models with additive transitions over multidimensional variables ( _ e.g. _ , allowing the latent variable to be a two - dimensional joint distribution of american and german unemployment with a two - dimensional joint transition probability ) .",
    "the same @xmath29-norm approximation can also be applied to the problem of max - deconvolution ( _ i.e. _ , solving @xmath413 for @xmath26 when given @xmath19 and @xmath25 ) .",
    "this can be accomplished by computing the ratio of @xmath414 to @xmath415 ( assuming @xmath25 has already been properly zero - padded ) , and then computing the inverse fft of the result to approximate @xmath416 ; however , it should be noted that deconvolution methods are typically less stable than the corresponding convolution methods , computing a ratio is less stable than computing a product ( particularly when the denominator is close to zero ) .",
    "although the largest absolute error of the affine piecewise method is the same as the largest absolute error of the original piecewise method , the mean squared error ( mse ) of the affine piecewise method will be lower than the square of the worst - case absolute error .    to achieve the worst - case absolute error for a given contour the affine correction must be negligible ; therefore , there must be two nearly vertical points on the scatter plot of @xmath417 vs. @xmath418 , which are both extremes of the bounding envelope from * figure  [ figure : approxexactlinearzoom]*. thus , there must exist two different indices @xmath136 and @xmath137 with vectors where @xmath419 and where @xmath420 ( creating two vertical points on the scatter plot , and forcing that both can not simultaneously be corrected by a single affine mapping ) . in order to do this , it is required to have @xmath421 filled with a single nonzero value and for the remaining elements of @xmath421 to equal zero .",
    "conversely , @xmath422 must be filled entirely with large , nonzero values ( the largest values possible that would still use the same contour @xmath29 ) . together , these two arguments place strong constraints on the vectors @xmath76 and @xmath77 ( and transitively , also constrains the unscaled vectors @xmath25 and @xmath26 ) : on one hand , filling @xmath421 with @xmath423 zeros requires that @xmath423 elements from either @xmath25 or @xmath26 must be zero ( because at least one factor must be zero to achieve a product of zero ) . on the other hand , filling @xmath422 with all large - value nonzeros requires that @xmath424 elements of _ both _ @xmath25 and @xmath26 are nonzero .",
    "together , these requirements stipulate that both @xmath425 , because entries of @xmath25 and @xmath26 can not simultaneously be zero and nonzero . therefore , in order to have many such vertical points , constrains the lengths of the @xmath426 vectors corresponding to those points .",
    "while the worst - case absolute error bound presumes that an individual vector @xmath21 may have length @xmath28 , this will not be possible for many vectors corresponding to vertical points on the scatter plot .",
    "for this reason , the mse will be significantly lower than the square of the worst - case absolute error , because making a high affine - corrected absolute error on one index necessitates that the absolute errors at another index can not be the worst - case absolute error ( if the sizes of @xmath25 and @xmath26 are fixed ) .",
    "code for exact max - convolution and the fast numerical method ( which includes both @xmath427 and null space projection methods ) is implemented in python and available at https://bitbucket.org/orserang/fast-numerical-max-convolution .",
    "all included code works for numpy arrays of any dimension , _",
    "i.e. _ tensors ) .",
    "we would like to thank mattias frnberg , knut reinert , and oliver kohlbacher for the interesting discussions and suggestions .",
    "j.p . acknowledges funding from bmbf ( center for integrative bioinformatics , grant no . 031a367 ) .",
    "o.s . acknowledges generous start - up funds from freie universitt berlin and the leibniz - institute for freshwater ecology and inland fisheries .",
    "david bremner , timothy  m chan , erik  d demaine , jeff erickson , ferran hurtado , john iacono , stefan langerman , and perouz taslakian .",
    "necklaces , convolutions , and x+ y. in _ algorithms  esa 2006 _ , pages 160171 .",
    "springer , 2006 ."
  ],
  "abstract_text": [
    "<S> max - convolution is an important problem closely resembling standard convolution ; as such , max - convolution occurs frequently across many fields . here </S>",
    "<S> we extend the method with fastest known worst - case runtime , which can be applied to nonnegative vectors by numerically approximating the chebyshev norm @xmath0 , and use this approach to derive two numerically stable methods based on the idea of computing @xmath1-norms via fast convolution : the first method proposed , with runtime in @xmath2 ( which is less than @xmath3 for any vectors that can be practically realized ) , uses the @xmath1-norm as a direct approximation of the chebyshev norm . </S>",
    "<S> the second approach proposed , with runtime in @xmath4 ( although in practice both perform similarly ) , uses a novel null space projection method , which extracts information from a sequence of @xmath1-norms to estimate the maximum value in the vector ( this is equivalent to querying a small number of moments from a distribution of bounded support in order to estimate the maximum ) . </S>",
    "<S> the @xmath1-norm approaches are compared to one another and are shown to compute an approximation of the viterbi path in a hidden markov model where the transition matrix is a toeplitz matrix ; the runtime of approximating the viterbi path is thus reduced from @xmath5 steps to @xmath6 steps in practice , and is demonstrated by inferring the u.s . </S>",
    "<S> unemployment rate from the s&p 500 stock index . </S>"
  ]
}