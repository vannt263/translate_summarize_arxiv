{
  "article_text": [
    "the advent of the internet has led to generation of massive and inherently high dimensional data . in many industrial applications ,",
    "the size of the datasets has long exceeded the memory capacity of a single machine . in web domains , it is not difficult to find datasets with the number of instances and the number of dimensions going into billions  @xcite .",
    "the reality that web data are typically sparse and high dimensional is due to the wide adoption of the `` bag of words '' ( bow ) representations for documents and images . in bow representations",
    ", it is known that the word frequency within a document follows power law .",
    "most of the words occur rarely in a document and most of the higher order shingles in the document occur only once .",
    "it is often the case that just the presence or absence information suffices in practice  @xcite .",
    "leading search companies routinely use sparse binary representations in their large data systems  @xcite .",
    "* locality sensitive hashing ( lsh ) *  @xcite is a general framework of indexing technique , devised for efficiently solving the approximate near neighbor search problem  @xcite .",
    "the performance of lsh largely depends on the underlying particular hashing methods .",
    "two popular hashing algorithms are * minhash *  @xcite and * simhash * ( sign normal random projections )  @xcite .",
    "minhash is an lsh for * resemblance similarity * which is defined over binary vectors , while simhash is an lsh for * cosine similarity * which works for general real - valued data . with the abundance of binary data over the web",
    ", it has become a practically important question : _ which lsh should be preferred in binary data?_. this question has not been adequately answered in existing literature . there were prior attempts to address this problem from various aspects .",
    "for example , the paper on _ conditional random sampling ( crs ) _",
    "@xcite showed that random projections can be very inaccurate especially in binary data , for the task of inner product estimation ( which is not the same as near neighbor search ) . a more recent paper  @xcite",
    "empirically demonstrated that @xmath7-bit minwise hashing  @xcite outperformed simhash and spectral hashing  @xcite .    * our contribution * : our paper provides an essentially conclusive answer that minhash should be used for near neighbor search in binary data , both theoretically and empirically . to favor simhash ,",
    "our theoretical analysis and experiments evaluate the retrieval results of minhash in terms of cosine similarity ( instead of resemblance ) .",
    "this is possible because we are able to show that minhash can be proved to be an lsh for cosine similarity by establishing an inequality which bounds resemblance by purely functions of cosine .",
    "because we evaluate minhash ( which was designed for resemblance ) in terms of cosine , we will first illustrate the close connection between these two similarities .",
    "we focus on binary data , which can be viewed as sets ( locations of nonzeros ) .",
    "consider two sets @xmath8 .",
    "the cosine similarity ( @xmath1 ) is @xmath9 the resemblance similarity , denoted by @xmath0 , is @xmath10 clearly these two similarities are closely related . to better illustrate the connection , we re - write @xmath0 as @xmath11 there are two degrees of freedom : @xmath12 , @xmath13 , which make it inconvenient for analysis",
    "fortunately , in theorem  [ thm_ineq ] , we can bound @xmath0 by purely functions of @xmath1 .",
    "[ thm_ineq ] @xmath14 * tightness * without making assumptions on the data , neither the lower bound @xmath15 or the upper bound @xmath16 can be improved in the domain of continuous functions .    * data dependent bound * if the data satisfy @xmath17 , where @xmath4 is defined in ( [ eqn_z ] )",
    ", then @xmath18 * proof : *  see appendix a. @xmath19    figure  [ fig : bounds ] illustrates that in high similarity region , the upper and lower bounds essentially overlap . note that , in order to obtain @xmath20 , we need @xmath21 ( i.e. , @xmath22 ) .    while the high similarity region is often of interest , we must also handle data in the low similarity region , because in a realistic dataset , the majority of the pairs are usually not similar .",
    "interestingly , we observe that for the six datasets in table  [ tab_data ] , we often have @xmath23 with @xmath4 only being slightly larger than 2 ; see figure  [ fig_z ] .",
    "[ tab_data ]    for each dataset , we compute both cosine and resemblance for every query - train pair ( e.g. , @xmath24 pairs for mnist dataset ) . for each query point",
    ", we rank its similarities to all training points in descending order .",
    "we examine the top-1000 locations as in figure  [ fig_top1000 ] . in the left panels , for every top location",
    ", we plot the median ( among all query points ) of the similarities , separately for cosine ( dashed ) and resemblance ( solid ) , together with the lower and upper bounds of @xmath0 ( dot - dashed ) .",
    "we can see for news20 , nytimes , and rcv1 , the data are not too similar .",
    "interestingly , for all six datasets , @xmath0 matches fairly well with the upper bound @xmath16 . in other words , the lower bound @xmath15 can be very conservative even in low similarity region .",
    "the right panels of figure  [ fig_top1000 ] present the comparisons of the orderings of similarities in an interesting way . for every query point",
    ", we rank the training points in descending order of similarities , separately for cosine and resemblance . this way , for every query point we have two lists of numbers ( of the data points ) .",
    "we truncate the lists at top-@xmath25 and compute the resemblance between the two lists . by varying @xmath25 from 1 to 1000",
    ", we obtain a curve which roughly measures the `` similarity '' of cosine and resemblance .",
    "we present the averaged curve over all query points",
    ". clearly figure  [ fig_top1000 ] shows there is a strong correlation between the two measures in all datasets , as one would expect .",
    "a common formalism for approximate near neighbor problem is the @xmath26-approximate near neighbor or @xmath26-nn .    *",
    "definition * : ( @xmath26-approximate near neighbor or @xmath26-nn ) . given a set of points in a @xmath27-dimensional space @xmath28 , and parameters @xmath29 , @xmath30 ,",
    "construct a data structure which , given any query point @xmath31 , does the following with probability @xmath32 : if there exist an @xmath33-near neighbor of @xmath31 in @xmath34 , it reports some @xmath35-near neighbor of @xmath31 in @xmath34 .",
    "the usual notion of @xmath33-near neighbor is in terms of the distance function .",
    "since we are dealing with similarities , we can equivalently define @xmath33-near neighbor of point @xmath31 as a point @xmath36 with @xmath37 , where @xmath38 is the similarity function of interest .    a popular technique for @xmath26-nn , uses the underlying theory of _ locality sensitive hashing _ ( lsh )  @xcite .",
    "lsh is a family of functions , with the property that similar input objects in the domain of these functions have a higher probability of colliding in the range space than non - similar ones . in formal terms , consider @xmath39 a family of hash functions mapping @xmath40 to some set @xmath1 .    * definition : locality sensitive hashing *  a family @xmath39",
    "is called @xmath41-sensitive if for any two points @xmath42 and @xmath43 chosen uniformly from @xmath39 satisfies the following :    * if @xmath44 then @xmath45 * if @xmath46 then @xmath47    for approximate nearest neighbor search typically , @xmath48 and @xmath49 is needed . since we are defining neighbors in terms of similarity we have @xmath49 .",
    "to get distance analogy we can use the transformation @xmath50 with a requirement of @xmath51 .    the definition of lsh family @xmath39 is tightly linked with the similarity function of interest @xmath38 .",
    "an lsh allows us to construct data structures that give provably efficient query time algorithms for @xmath26-nn problem .",
    "* fact * : given a family of @xmath41 -sensitive hash functions , one can construct a data structure for @xmath26-nn with @xmath52 query time , where @xmath53 . the quantity @xmath54 measures the efficiency of a given lsh , the smaller the better . in theory , in the worst case ,",
    "the number of points scanned by a given lsh to find a @xmath26-approximate near neighbor is @xmath55  @xcite , which is dependent on @xmath56 . thus given two lshs , for the same @xmath26-nn problem , the lsh with smaller value of @xmath56 will achieve the same approximation guarantee and at the same time will have faster query time .",
    "lsh with lower value of @xmath56 will report fewer points from the database as the potential near neighbors .",
    "these reported points need additional re - ranking to find the true @xmath26-approximate near neighbor , which is a costly step .",
    "it should be noted that the efficiency of an lsh scheme , the @xmath56 value , is dependent on many things .",
    "it depends on the similarity threshold @xmath33 and the value of @xmath26 which is the approximation parameter .",
    "[ sec : minhash ]    minwise hashing  @xcite is the lsh for resemblance similarity .",
    "the minwise hashing family applies a random permutation @xmath57 , on the given set @xmath58 , and stores only the minimum value after the permutation mapping .",
    "formally minhash is defined as : @xmath59 given sets @xmath60 and @xmath61 , it can be shown by elementary probability argument that @xmath62    it follows from ( [ eq : minhash ] ) that minwise hashing is @xmath63 sensitive family of hash function when the similarity function of interest is resemblance i.e @xmath0 .",
    "it has efficiency @xmath64 for approximate resemblance based search .",
    "simhash is another popular lsh for the cosine similarity measure , which originates from the concept of _ sign random projections ( srp ) _",
    "@xcite . given a vector @xmath65",
    ", srp utilizes a random vector @xmath66 with each component generated from i.i.d .",
    "normal , i.e. , @xmath67 , and only stores the sign of the projected data .",
    "formally , simhash is given by @xmath68 it was shown in  @xcite that the collision under srp satisfies the following equation : @xmath69 where @xmath70 .",
    "the term @xmath71 , is the cosine similarity for data vectors @xmath65 and @xmath72 , which becomes @xmath73 when the data are binary .    since @xmath74 is monotonic with respect to cosine similarity @xmath1 .",
    "( [ eq : srp ] ) implies that simhash is a @xmath75 sensitive hash function with efficiency @xmath76 .",
    "we would like to highlight here that the @xmath56 values for minhash and simhash , shown in the previous section , are not directly comparable because they are in the context of different similarity measures .",
    "consequently , it was not clear , before our work , if there is any theoretical way of finding conditions under which minhash is preferable over simhash and vice versa .",
    "it turns out that the two sided bounds in theorem  [ thm_ineq ] allow us to prove minhash is also an lsh for cosine similarity .",
    "we fix our gold standard similarity measure to be the cosine similarity @xmath77 .",
    "theorem  [ thm_ineq ] leads to two simple corollaries :    [ cor : p1 ] if @xmath78 , then we have @xmath79    [ cor : p2 ] if @xmath80 , then we have @xmath81    immediate consequence of these two corollaries combined with the definition of lsh is the following :    [ theo : minrho ] for binary data , minhash is @xmath82 sensitive family of hash function for cosine similarity with @xmath83 .",
    "simhash generates a single bit output ( only the signs ) whereas minhash generates an integer value .",
    "recently proposed @xmath7-bit minwise hashing  @xcite provides simple strategy to generate an informative single bit output from minhash , by using the parity of minhash values : @xmath84    for 1-bit minhash and very sparse data ( i.e. , @xmath85 , @xmath86 ) , we have the following collision probability @xmath87 the analysis presented in previous sections allows us to theoretically analyze this new scheme . the inequality in theorem  [ thm_ineq ]",
    "can be modified for @xmath88 and using similar arguments as for minhash we obtain    [ theo:1bitmin ] for binary data , 1-bit mh ( minwise hashing ) is @xmath89 sensitive family of hash function for cosine similarity with @xmath90 .",
    "we will compare the gap ( @xmath56 ) values of the three hashing methods we have studied : @xmath91 this is a worst case analysis .",
    "we know the lower bound @xmath92 is usually very conservative in real data when the similarity level is low .",
    "nevertheless , for high similarity region , the comparisons of the @xmath56 values indicate that minhash significantly outperforms simhash as shown in figure  [ fig_worstrho ] , at least for @xmath93 .",
    "the worst case analysis does not make any assumption on the data .",
    "it is obviously too conservative when the data are not too similar .",
    "figure  [ fig_z ] has demonstrated that in real data , we can fairly safely replace the lower bound @xmath15 with @xmath94 for some @xmath4 which , defined in ( [ eqn_z ] ) , is very close to 2 ( for example , 2.1 ) . if we are willing to make this assumption , then we can go through the same analysis for minhash as an lsh for cosine and compute the corresponding @xmath56 values : @xmath95 note that this is still a worst case analysis ( and hence can still be very conservative ) .",
    "figure  [ fig_resworstrho ] presents the @xmath56 values for this restricted worst case gap analysis , for two values of @xmath4 ( 2.1 and 2.3 ) and @xmath96 as small as 0.2 .",
    "the results confirms that minhash still significantly outperforms simhash even in low similarity region .",
    "both figure  [ fig_worstrho ] and figure  [ fig_resworstrho ] show that 1-bit minhash can be less competitive when the similarity is not high .",
    "this is expected as analyzed in the original paper of @xmath7-bit minwise hashing  @xcite .",
    "the remedy is to use more bits .",
    "as shown in figure  [ fig_bbitmh ] , once we use @xmath97 ( or even @xmath98 ) bits , the performance of @xmath7-bit minwise hashing is not much different from minhash",
    ".      the restricted worst case analysis can still be very conservative and may not fully explain the stunning performance of minhash in our experiments on datasets of low similarities . here",
    ", we also provide an analysis based on fixed @xmath4 value .",
    "that is , we only analyze the gap @xmath56 by assuming @xmath23 for a fixed @xmath4 .",
    "we call this idealized gap analysis",
    ". not surprisingly , figure  [ fig_idealrho ] confirms that , with this assumption , minhash significantly outperform simhash even for extremely low similarity .",
    "we should keep in mind that this idealized gap analysis can be somewhat optimistic and should only be used as some side information .",
    "we evaluate both minhash and simhash in the actual task of retrieving top-@xmath99 near neighbors .",
    "we implemented the standard @xmath100 parameterized lsh  @xcite algorithms with both minhash and simhash .",
    "that is , we concatenate @xmath101 hash functions to form a new hash function for each table , and we generate @xmath102 such tables ( see  @xcite for more details about the implementation ) .",
    "we used all the six binarized datasets with the query and training partitions as shown in table  [ tab_data ] . for each dataset",
    ", elements from training partition were used for constructing hash tables , while the elements of the query partition were used as query for top-@xmath99 neighbor search . for every query ,",
    "we compute the gold standard top-@xmath99 near neighbors using the cosine similarity as the underlying similarity measure .    in standard",
    "@xmath100 parameterized bucketing scheme the choice of @xmath101 and @xmath102 is dependent on the similarity thresholds and the hash function under consideration . in the task of top-@xmath99 near neighbor retrieval , the similarity thresholds vary with the datasets .",
    "hence , the actual choice of ideal @xmath101 and @xmath102 is difficult to determine . to ensure that this choice does not affect our evaluations",
    ", we implemented all the combinations of @xmath103 and @xmath104 .",
    "these combinations include the reasonable choices for both the hash function and different threshold levels .    for each combination of @xmath100 and for both of the hash functions , we computed the mean recall of the top-@xmath99 gold standard neighbors along with the average number of points reported per query .",
    "we then compute the least number of points needed , by each of the two hash functions , to achieve a given percentage of recall of the gold standard top-@xmath99 , where the least was computed over the choices of @xmath101 and @xmath102 .",
    "we are therefore ensuring the best over all the choices of @xmath101 and @xmath102 for each hash function independently .",
    "this eliminates the effect of @xmath101 and @xmath102 , if any , in the evaluations .",
    "the plots of the fraction of points retrieved at different recall levels , for @xmath105 , are in figure  [ fig_topk ] .",
    "a good hash function , at a given recall should retrieve less number of points .",
    "minhash needs to evaluate significantly less fraction of the total data points to achieve a given recall compared to simhash .",
    "minhash is consistently better than simhash , in most cases very significantly , irrespective of the choices of dataset and @xmath99 .",
    "it should be noted that our gold standard measure for computing top-@xmath99 neighbors is cosine similarity .",
    "this should favor simhash because it was the only known lsh for cosine similarity .",
    "despite this `` disadvantage '' , minhash still outperforms simhash in top near neighbor search with cosine similarity .",
    "this nicely confirms our theoretical gap analysis .    to conclude this section",
    ", we also add a set of experiments using the original ( real - valued ) data , for mnist and rcv1 .",
    "we apply simhash on the original data and minhash on the binarized data .",
    "we also evaluate the retrieval results based on the cosine similarities of the original data .",
    "this set - up places minhash in a very disadvantageous place compared to simhash .",
    "nevertheless , we can see from figure  [ fig_topkreal ] that minhash still noticeably outperforms simhash , although the improvements are not as significant , compared to the experiments on binarized data ( figure  [ fig_topk ] ) .",
    "minwise hashing ( minhash ) , originally designed for detecting duplicate web pages  @xcite , has been widely adopted in the search industry , with numerous applications , for example , large - sale machine learning systems  @xcite , web spam  @xcite , content matching for online advertising  @xcite , compressing social networks  @xcite , advertising diversification  @xcite , graph sampling  @xcite , web graph compression @xcite , etc .",
    "furthermore , the recent development of _ one permutation hashing _  @xcite has substantially reduced the preprocessing costs of minhash , making the method more practical .    in machine learning research literature , however , it appears that simhash is more popular for approximate near neighbor search .",
    "we believe part of the reason is that researchers tend to use the cosine similarity , for which simhash can be directly applied .",
    "it is usually taken for granted that minhash and simhash are theoretically incomparable and the choice between them is decided based on whether the desired notion of similarity is cosine similarity or resemblance .",
    "this paper has shown that minhash is provably a better lsh than simhash even for cosine similarity .",
    "our analysis provides a first provable way of comparing two lshs devised for different similarity measures .",
    "theoretical and experimental evidence indicates significant computational advantage of using minhash in place of simhash .",
    "since lsh is a concept studied by a wide variety of researchers and practitioners , we believe that the results shown in this paper will be useful from both theoretical as well as practical point of view .",
    "* acknowledgements * : anshumali shrivastava is a ph.d . student supported by nsf ( dms0808864 , ses1131848 , iii1249316 ) and onr ( n00014 - 13 - 1 - 0764 ) .",
    "ping li is partially supported by afosr ( fa9550 - 13 - 1 - 0137 ) , onr ( n00014 - 13 - 1 - 0764 ) , and nsf ( iii1360971 , bigdata1419210 ) .",
    "the only less obvious step is the _ * proof of tightness : * _ let a continuous function @xmath106 be a sharper upper bound i.e. , @xmath107 . for any rational @xmath108 , with @xmath109 and @xmath110 , choose @xmath111 and @xmath112 .",
    "note that @xmath113 are positive integers .",
    "this choice leads to @xmath114 .",
    "thus , the upper bound is achievable for all rational @xmath1 .",
    "hence , it must be the case that @xmath115 for all rational values of @xmath1 .",
    "for any real number @xmath116 $ ] , there exists a cauchy sequence of rational numbers @xmath117 such that @xmath118 and @xmath119 .",
    "since all @xmath120 s are rational , @xmath121 . from the continuity of both @xmath122 and @xmath16 , we have @xmath123 which implies @xmath124 implying @xmath125 $ ] .    for tightness of @xmath15 ,",
    "let @xmath126 , choosing @xmath127 and @xmath128 gives an infinite set of points having @xmath129 .",
    "we now use similar arguments in the proof tightness of upper bound .",
    "all we need is the existence of a cauchy sequence of square root of rational numbers converging to any real @xmath26.@xmath130                tushar chandra , eugene ie , kenneth goldman , tomas  lloret llinares , jim mcfadden , fernando pereira , joshua redstone , tal shaked , and yoram singer .",
    "sibyl : a system for large scale machine learning",
    ". technical report , 2010 .",
    "sandeep pandey , andrei broder , flavio chierichetti , vanja josifovski , ravi kumar , and sergei vassilvitskii .",
    "nearest - neighbor caching for content - match applications . in _ www _ , pages 441450 , madrid , spain , 2009 ."
  ],
  "abstract_text": [
    "<S> minhash and simhash are the two widely adopted locality sensitive hashing ( lsh ) algorithms for large - scale data processing applications . deciding which lsh to use for a particular problem at hand is an important question , which has no clear answer in the existing literature . in this study </S>",
    "<S> , we provide a theoretical answer ( validated by experiments ) that minhash virtually always outperforms simhash when the data are binary , as common in practice such as search .    </S>",
    "<S> the collision probability of minhash is a function of _ resemblance _ similarity ( @xmath0 ) , while the collision probability of simhash is a function of _ cosine _ similarity ( @xmath1 ) . to provide a common basis for comparison , we evaluate retrieval results in terms of @xmath1 for both minhash and simhash </S>",
    "<S> this evaluation is valid as we can prove that minhash is a valid lsh with respect to @xmath1 , by using a general inequality @xmath2 . </S>",
    "<S> our * worst case * analysis can show that minhash significantly outperforms simhash in * high similarity * region .    </S>",
    "<S> interestingly , our intensive experiments reveal that minhash is also substantially better than simhash even in datasets where most of the data points are not too similar to each other . </S>",
    "<S> this is partly because , in practical data , often @xmath3 holds where @xmath4 is only slightly larger than 2 ( e.g. , @xmath5 ) . </S>",
    "<S> our * restricted worst case * analysis by assuming @xmath6 shows that minhash indeed significantly outperforms simhash even in * low similarity * region .    </S>",
    "<S> we believe the results in this paper will provide valuable guidelines for search in practice , especially when the data are sparse . </S>"
  ]
}