{
  "article_text": [
    "as is well - known , how to solve effectively linear systems is a very important problem in scientific and engineering fields .",
    "many of linear solvers have been researched such as gaussian elimination @xcite , relaxation methods @xcite , row - action iteration schemes @xcite and ( block ) krylov subspace @xcite .",
    "recently , a low communication condensation - based linear system solver utilizing cramer s rule is presented in @xcite . as the authors stated that unique combination between cramer s rule and matrix condensation techniques yields an elegant parallel computing architectures , by constructing a binary , tree - based data flow in which the algorithm mirrors the matrix at critical points during the condensation process .",
    "moreover , the accuracy and computational complexity of the proposed algorithm are similar to lu - decomposition @xcite .    in this paper , we will continue research this kind of parallel algorithms and give some theoretical analysis and a generalized chi s determinant condensation process , which perfect the corresponding conclusions .",
    "this paper is organized as follows . in second 2",
    ", we will review some more general determinant condensation algorithms ",
    "sylvester s determinant identity , and then give theoretical basis on the above parallel computing architectures @xcite , which shows the negation in mirroring process is not necessary to arrive at the correct answer .",
    "moreover , a more general scheme utilizing cramer s rule and matrix condensation techniques is also given .",
    "in addition , the scheme suitable for parallel computing on the sylvester s identity is proposed in section 3 .",
    "finally , a simple example is used to illustrate this new algorithm in section 4 .",
    "throughout this section , we mainly consider an @xmath0 matrix @xmath1 @xmath2 with elements @xmath3 and determinant @xmath4 , also written @xmath5 .",
    "recently , a chi condensation method @xcite is applied to solve large linear systems in @xcite .",
    "in fact , the prototype of this method may be traced back to the following sylvester s determinant identity for calculating a determinant of arbitrary order in 1851 .",
    "[ th2.1](sylvester s identity,@xcite ) .",
    "let @xmath1 be an @xmath0 matrix over a commutative ring .",
    "for a submatrix @xmath6 , @xmath7 of @xmath8 , set @xmath9.\\ ] ] let @xmath10 , @xmath11 .",
    "then @xmath12 specially when @xmath13 is an invertible matrix , we have that @xmath14    [ co2.2](chi s method , @xcite ) . for an @xmath0 matrix @xmath1 with @xmath15 ,",
    "let @xmath16 be the @xmath17 matrix defined by @xmath18 then @xmath19    obviously , the above theorem [ th2.1 ] reduces a matrix of order @xmath20 to order @xmath21 to evaluate its determinant . repeating the procedure numerous times can reduce a large matrix to a small one , which is convenient for the calculation .",
    "this process is called by condensation method @xcite . as an example of chi s condensation",
    ", the paper @xcite considers the following @xmath22 matrix : @xmath23    in fact , the above condensation processes are not only used to evaluate determinants but also can be used to solve linear systems . for example , one can derive the following equivalence relation on the solution formula of linear systems .",
    "[ th2.2](equivalence relation ) .",
    "the linear system in the form @xmath24 ( where @xmath1 is an @xmath0 invertible coefficient matrix ) has the same corresponding solution as the linear system @xmath25 , where @xmath26 is defined as in theorem [ th2.1 ] , @xmath27^t}$ ] and @xmath28^t}$ ] . here",
    "@xmath29,j = k + 1 , \\ldots , n.\\ ] ]    according to theorem [ th2.1 ] or eq . , we know that there exists a constant @xmath30 between the determinant of @xmath8 and the determinant of @xmath31 , which is only dependent on the given submatrix @xmath13",
    ". therefore , for any given submatrix @xmath13 , there also exists the same constant @xmath30 between the determinant of @xmath32 ( @xmath33 ) , the matrix @xmath8 with its @xmath34 column replaced by @xmath35 , and the determinant of @xmath36 .",
    "thus , by cramer s rule , we have that @xmath37 the conclusion holds .    obviously ,",
    "when the submatrix @xmath13 is singular , the solution of linear systems can not be evaluated by this method . since interchanging the @xmath38th and @xmath20th rows and the @xmath39th and @xmath20th columns of linear systems has only an effect on the order of the unknowns @xmath40 , which has no effect on the whole solution @xmath41 .",
    "therefore , we may obtain the following more general conclusion .    for convenience , we firstly define the ordered index list @xmath42 for any positive integer @xmath20 . for two ordered index ( i.e. , for any @xmath43 )",
    "lists @xmath44 and @xmath45 , we denote the corresponding complementary ordered index lists by @xmath46 and @xmath47 , respectively .",
    "that is , @xmath48 .",
    "[ th3.4 ] .",
    "let @xmath1 be an @xmath0 matrix and @xmath49 be a fixed integer @xmath50 . @xmath51 and",
    "@xmath52 are two ordered index lists .",
    "we denote the corresponding submatrix , extracted from @xmath8 , as @xmath53 = a\\left [ { \\begin{array}{*{20}{c } } { { i_1 } , \\ldots , { i_k}}\\\\ { { j_1 } , \\ldots , { j_k } } \\end{array } } \\right ] \\triangleq \\left [ { \\begin{array}{*{20}{c } } { { a_{{i_1}{j_1 } } } } & \\cdots & { { a_{{i_1}{j_k}}}}\\\\   \\vdots & \\ddots & \\vdots \\\\ { { a_{{i_k}{j_1 } } } } & \\cdots & { { a_{{i_k}{j_k } } } } \\end{array } } \\right].\\ ] ] suppose that the invertible submatrix @xmath54 $ ] in the theorem [ th2.1 ] , then the linear system @xmath24 has the same corresponding solutions as the linear system @xmath55 , where @xmath56 is defined as the subset of @xmath57 with the index coming from @xmath47 .",
    "according to the above theorem , one can easily see that though the condensation process removes information associated with discarded columns , we may obtain certain variables values by controlling the elements in the set @xmath47 , see example [ e4.1 ] .",
    "in addition , the matrix mirroring and the negation of matrix mirroring process in @xcite are also not necessary to arrive at the correct answer , since we may obtain the similar parallel computing process by condensing the index set @xmath47 from both sides ( left and right ) , see figure 1 .",
    "@xmath58    [ fig1 ]        similar to @xcite , copying occurs with the initial matrix and then each time a matrix is reduced in half .",
    "an @xmath59 matrix is copied when it reaches the size of @xmath60 .",
    "once the matrix is copied , there is double the work . in other words , two @xmath60 matrices",
    "each require a condensation .",
    "obviously , the amount of work for two matrices of half the size is much lower than that of one @xmath61 matrix , which avoids the @xmath62 growth pattern in computations .",
    "this is due to the @xmath63 nature of the condensation process ( see @xcite ) .",
    "similarly , one may consider a scenario in which the algorithm creates more than two matrices during each copying step , according to corollary [ th3.4 ] . on its computational complexity and more details ,",
    "see @xcite .",
    "the sylvester s identity [ th2.1 ] reduces a matrix of order @xmath20 to order @xmath21 when evaluating its determinant . since when @xmath64",
    ", it is just the chi s method .",
    "therefore , for convenience , we call the sylvester s identity a * k - chi s method * from now on .",
    "as have been shown above , repeating the procedure numerous times can reduce a large matrix to a size convenient for the computations .",
    "however , in order to condense a matrix from @xmath61 to @xmath65 , the core calculation is repeated @xmath66 times .",
    "obviously , this is very expensive .",
    "in fact , we may parallel computing each row of the matrix @xmath26 in , since @xmath13 or @xmath67 $ ] is common to each element of the row and may be calculated but once for each row via expanding by the last column .",
    "for example , for the @xmath68-th row @xmath69 of @xmath31 , we may write @xmath70\\\\   = \\left [ { \\underbrace { a_0^{p,1},a_0^{p,2 } , \\cdots , a_0^{p , k},|{a_0}|}_{k + 1 } } \\right ] \\cdot \\left [ { \\begin{array}{*{20}{c } } { { a_{1,k + 1}}}&{{a_{1,k + 2 } } } & \\cdots & { { a_{1,n}}}\\\\ { { a_{2,k + 1}}}&{{a_{2,k + 2 } } } & \\cdots & { { a_{2,n}}}\\\\   \\cdots & \\cdots & \\cdots & \\cdots \\\\ { { a_{p , k + 1}}}&{{a_{p , k + 2 } } } & \\cdots & { { a_{p , n } } } \\end{array } } \\right ] , \\end{array}\\ ] ] where @xmath71 therefore , only @xmath49 determents @xmath72 ( @xmath73 ) and a common @xmath74 determent @xmath75 are needed for each row of the matrix @xmath26 . therefore , the matrix @xmath31 is essentially suitable for parallel computations since the each row of matrix @xmath31 may be independently computed by .",
    "see example [ e3.3 ] below .",
    "[ e3.3 ] consider the following four order determent @xmath76    let @xmath77 $ ] , then @xmath78 , and @xmath79\\left [ { \\begin{array}{*{20}{c } } 3&1\\\\ { - 1}&0\\\\ 1&5 \\end{array } } \\right ] = \\left [ { \\begin{array}{*{20}{c } } { 36}&{58 } \\end{array } } \\right];\\\\ { \\alpha _ 4 } = \\left [ { -\\left| { \\begin{array}{*{20}{c } } \\mathbf{-3}&\\mathbf{3}\\\\ 4&2 \\end{array } } \\right|,-\\left| { \\begin{array}{*{20}{c } } 1 & { - 2}\\\\ \\mathbf{-3}&\\mathbf{3 } \\end{array } } \\right|,\\mathbf{10 } } \\right]\\left [ { \\begin{array}{*{20}{c } } 3&1\\\\ { - 1}&0\\\\ 1&2 \\end{array } } \\right ] = \\left [ { \\begin{array}{*{20}{c } } { 61}&{38 } \\end{array } } \\right ] .",
    "\\end{array}\\ ] ]    therefore , @xmath80    from here , we note that only six @xmath81 determinants is needed .",
    "however , chi s method will require fourteen @xmath81 determinants to be computed .",
    "in addition , comparing with the gaussian elimination , our method increases only two multiplications .",
    "but gaussian elimination method is not too suitable for parallel computing .",
    "thus , the whole computational amount on the matrix @xmath26 will be much less than that involved in the old process of computation @xcite . concretely speaking ,",
    "if we denote the total of multiplications / diversions on the @xmath49-order determinant @xmath82 by @xmath83 , then the total of multiplications / diversions by using the k - chi s method is about @xmath84 + km\\left [ { k + 2k +   \\ldots   + ( n - k ) } \\right ] + \\frac{{n - k}}{k}(m + 1 ) + m\\\\   \\approx o\\left ( { \\frac{1}{3}(1 + \\frac{1}{k}){n^3 } } \\right )",
    ". \\end{array}\\ ] ] similarly , the computational complexity of other algorithms is also described as follows , see table 1 .",
    "[ t1 ]    .comparisons of the computational complexity for different algorithms on determinant calculations in the nonparallel setting . [ cols=\"<,^,^\",options=\"header \" , ]     from table 1 , we note that additions / subtractions on these algorithms are almost the same .",
    "however , multiplications / diversions mainly depend on the parameter @xmath49 for k - chi s condensation method . but this does not show that the total computational complexity on k - chi s method is tending to decrease with the @xmath49 increasing , since the core loop of the k - chi s condensation method involves the calculation of @xmath74 determinants for each element of the matrix during condensation . normally , this would necessitate the standard computational workload to calculate the @xmath49-order determinant , i.e. , @xmath85 multiplications / divisions and @xmath85 additions / subtractions , using a method such as gaussian elimination @xcite . therefore , the parameter @xmath49 is not the better for the bigger number , see the following experimental results figure 1 and 2 on the @xmath86-order and @xmath87-order determinants , respectively .",
    "the small subgraphs in fig . 1 and 2 show the optimal parameter @xmath49 value ranges .",
    "for example , the optimal parameter @xmath49 is approximately ten for a @xmath87-order determinant . in addition , for matrices of different dimensions , we specifically compute the optimal parameters @xmath49 , we find the optimal parameter values increasing as the matrix dimension increases . but",
    "this increase is still relatively slow , see fig .",
    "[ fig1 ]     and the number of multiplication / diversion for a 5000-order determinant on k - chi s condensation method.,width=710,height=664 ]    [ fig2 ]     and the number of multiplication / diversion for a 20000-order determinant on k - chi s condensation method.,width=748,height=664 ]    [ fig3 ]     and the dimension of matrices , left : only consider multiplications ; right : consider all computational complexity.,width=768,height=768 ]    since the optimal parameter @xmath49 is usually small , by , we may normalize the each row of matrix @xmath26 by dividing the determinant of @xmath13 , which will further reduce the computational complexity of k - chi s condensation method , see example [ e4.1 ] .",
    "as is well - known , the classical cramer s rule states that the components of the solution to a linear system in the form @xmath24 ( where @xmath1 is an @xmath0 invertible coefficient matrix ) are given by @xmath88 where @xmath40 is the @xmath89th unknown .    in @xcite , an algorithm based on chi s condensation and cramer s rule for solving large - scale linear systems is achieved by constructing a binary , tree - based data flow in which the algorithm mirrors the matrix at critical points during the condensation process . however , according to the above corollary [ th3.4 ] , one may obtain certain unknowns values by * freely controlling the elements in the set * @xmath47 without matrix mirroring , see example [ e4.1 ] .",
    "this also makes it more easily for more cpus to be used in computing process and even without any communication . at the same time",
    ", the scheme also reduce the memory space .",
    "[ e4.1 ] solve the following six - order linear system @xmath90\\left [ { \\begin{array}{*{20}{c } } { { x_1}}\\\\ { { x_2}}\\\\ { { x_3}}\\\\ { { x_4}}\\\\ { { x_5}}\\\\ { { x_6 } } \\end{array } } \\right ] = \\left [ { \\begin{array}{*{20}{c } } \\textcolor[rgb]{0,0,1}{1}\\\\ \\textcolor[rgb]{0,0,1}{-1}\\\\ \\textcolor[rgb]{0,0,1}{1}\\\\ \\textcolor[rgb]{0,0,1 } { - 1}\\\\ \\textcolor[rgb]{0,0,1}1\\\\ \\textcolor[rgb]{0,0,1 } { - 1 } \\end{array } } \\right].\\ ] ]    let @xmath91 and @xmath92 , then @xmath93 . denote @xmath94 } \\right| = \\left| { \\begin{array}{*{20}{c } } 1&5&9\\\\ 2&0&0\\\\ 3&5&0 \\end{array } } \\right|.\\ ] ] then , @xmath95 and @xmath96\\left [ { \\begin{array}{*{20}{c } } 3&7&{11}&\\textcolor[rgb]{0,0,1}{1}\\\\ 0&0&9&\\textcolor[rgb]{0,0,1 } { - 1}\\\\ 0&7&7&\\textcolor[rgb]{0,0,1}{1}\\\\ \\mathbf{0}&\\mathbf{8}&\\mathbf{5 } & { \\textcolor[rgb]{0,0,1 } { \\textbf{- 1 } } } \\end{array } } \\right]\\\\   & = & \\left [ { \\begin{array}{*{20}{c } } { 0,}&{-36,}&{-468,}&\\textcolor[rgb]{0,0,1}{-180 } \\end{array } } \\right ] ; \\end{array}\\ ] ]    @xmath97\\left [ { \\begin{array}{*{20}{c } } 3&7&{11}&\\textcolor[rgb]{0,0,1}{1}\\\\ 0&0&9&\\textcolor[rgb]{0,0,1 } { - 1}\\\\ 0&7&7&\\textcolor[rgb]{0,0,1}{1}\\\\ \\mathbf{0}&\\mathbf{0}&\\mathbf{3}&\\textcolor[rgb]{0,0,1}{\\textbf{1 } } \\end{array } } \\right]\\\\   & = & \\left [ { \\begin{array}{*{20}{c } } { 0,}&{0,}&{-1755,}&\\textcolor[rgb]{0,0,1}{315 } \\end{array } } \\right ] ; \\end{array}\\ ] ]    @xmath98\\left [ { \\begin{array}{*{20}{c } } 3&7&{11}&\\textcolor[rgb]{0,0,1}{1}\\\\ 0&0&9&\\textcolor[rgb]{0,0,1 } { - 1}\\\\ 0&7&7&\\textcolor[rgb]{0,0,1}{1}\\\\ \\mathbf{5}&\\mathbf{3}&\\mathbf{1}&{\\textcolor[rgb]{0,0,1 } { \\textbf{- 1 } } } \\end{array } } \\right]\\\\   & = & \\left [ { \\begin{array}{*{20}{c } } { 390,}&{-234,}&{-2132,}&\\textcolor[rgb]{0,0,1}{20 } \\end{array } } \\right ] .",
    "\\end{array}\\ ] ] therefore , we need only solve the condensed linear system @xmath55 , i.e. ,    @xmath99\\left [ { \\begin{array}{*{20}{c } } { { x_2}}\\\\ { { x_4}}\\\\ { { x_6 } } \\end{array } } \\right ] = \\left [ { \\begin{array}{*{20}{c } } \\textcolor[rgb]{0,0,1}{-180}\\\\ \\textcolor[rgb]{0,0,1}{315}\\\\ \\textcolor[rgb]{0,0,1}{20 } \\end{array } } \\right].\\ ] ]    by cramer s rule or gaussian elimination , the solution of above sub - linear system is @xmath100 which is also the corresponding solution of original linear system . similarly , let @xmath101 and @xmath102 , then we may also obtain the solution of the unknown @xmath103 , @xmath104 and @xmath105 : @xmath106    in addition , we may continue condense the above @xmath107 , @xmath108 and @xmath109 . for example , we condense them from right side for @xmath93 . without loss of generality , we may let @xmath110 , @xmath111 , then @xmath112 and we have @xmath113\\left [ { \\begin{array}{*{20}{c } } { 390 } & { - 234}&\\textcolor[rgb]{0,0,1}{20}\\\\ 0 & { - 36}&\\textcolor[rgb]{0,0,1}{-180 } \\end{array } } \\right ] = \\left [ { 182520 , - 32760,\\textcolor[rgb]{0,0,1}{393120 } } \\right];\\\\ \\alpha _ 5 ' = \\left [ { 1755 , - 2132 } \\right]\\left [ { \\begin{array}{*{20}{c } } { 390 } & { - 234}&\\textcolor[rgb]{0,0,1}{20}\\\\ 0&0&\\textcolor[rgb]{0,0,1}{315 } \\end{array } } \\right ] = \\left [ { 684450 , - 410670,\\textcolor[rgb]{0,0,1}{- 636480 } } \\right ]",
    ". \\end{array}\\ ] ] by gaussian elimination , we obtain the solution of the above linear system : @xmath114    moreover , to further reduce the computational complexity of k - chi s condensation method , we may normalize the each row of matrix @xmath26 by dividing the determinant of @xmath13 .",
    "for example , the above @xmath115 and @xmath116 may be written as @xmath117\\left [ { \\begin{array}{*{20}{c } } { 390}&{-234}&\\textcolor[rgb]{0,0,1}{20}\\\\ 0 & { - 36}&\\textcolor[rgb]{0,0,1 } { - 180 } \\end{array } } \\right ] = \\left [ { -3510/41,630/41 , \\textcolor[rgb]{0,0,1}{-7560/41 } } \\right];\\\\ \\alpha _ 5 ' = \\left [ { 1755/ - 2132,\\textbf{1 } } \\right]\\left [ { \\begin{array}{*{20}{c } } { 390}&{-234}&\\textcolor[rgb]{0,0,1}{20}\\\\ 0&0&\\textcolor[rgb]{0,0,1}{315 } \\end{array } } \\right ] = \\left [ { -26325/82,15795/82,\\textcolor[rgb]{0,0,1}{12240/41 } } \\right ] . \\end{array}\\ ] ]    from the above example , we know that applying gaussian elimination method instead of cramer s rule to solve the small sub - linear system @xmath55 is also very convenient .",
    "from the above discussion , one can see that unique utilization of matrix condensation techniques yields an elegant process that has promise for parallel computing architectures .",
    "moreover , as was also mentioned in @xcite , these condensation methods become extremely interesting , since they still retain an @xmath118 complexity with pragmatic forward and backward stability properties when they are applied to solve large - scale linear systems by the cramer s rule or gaussian elimination .    in this paper ,",
    "some condensation methods are introduced and some existing problems on these techniques are also discussed . though the condensation process removes information associated with discarded columns , this makes the computation of linear systems become feasible by more freely parallel process .    * acknowledgements . *",
    "_ partial results of this paper were completed while the first author was visiting the college of william and mary in 2013 .",
    "the first author is very grateful to professor chi - kwong li for the invitation to the college of william and mary . _",
    "n. galoppo , n.k .",
    "govindaraju , m. henson , and d. manocha .",
    "lu - gpu : efficient algorithms for solving dense linear systems on graphics hardware .",
    "proceedings of the acm / ieee sc2005 conference on high performance networking and computing , university of north carolina at chapel hill , 2005 ."
  ],
  "abstract_text": [
    "<S> the object of this paper is to introduce a new and fascinating method of solving large linear equations , based on cramer s rule or gaussian elimination but employing sylvester s determinant identity in its computation process . </S>",
    "<S> in addition , a scheme suitable for parallel computing is presented for this kind of generalized chi s determinant condensation processes , which makes this new method have a property of natural parallelism . </S>",
    "<S> finally , some numerical experiments also confirm our theoretical analysis .    </S>",
    "<S> _ keywords _ : sylvester s determinant identity ; cramer s rule ; chi s method ; parallel process . </S>"
  ]
}