{
  "article_text": [
    "the pseudo - spectral method has been very popular in the research of highly accurate numerical simulations since the pioneer work of orszag and patterson  @xcite . for smooth solutions ,",
    "the convergence order of the spectral methods is higher than any algebraic power of mesh size . for a comparable error on the uniform mesh",
    ", a much finer mesh is required for finite difference or finite element methods .",
    "this is one of the reasons that the spectral method has been widely used in spite of the prosperous development of adaptive grid methods .",
    "some comprehensive overviews of the various applications of spectral methods in fluid dynamics can be found in  @xcite . with the fast development of supercomputers ,",
    "more and more efforts have been devoted to parallelizing spectral methods . for challenging simulation problems such as turbulence research , many new interesting phenomena are discovered using large scale parallel computations ( @xmath2 ) ( e.g. see  @xcite , and a review paper  @xcite ) .",
    "the parallel schemes based on spectral methods have also been intensively investigated over the last decade  mainly for three - dimensional ( 3d ) problems  @xcite .",
    "the main computation time for spectral methods is concentrated in the part of the fast fourier transform ( fft ) , around which the 3d parallel schemes are constructed . in the following",
    ", we will denote this parallel fft procedure as pfft .    for high dimensional fft",
    ", the transpose - split method can provide very high parallel efficiency , so most commonly used 3d parallel schemes employ this kind of parallel fft  @xcite .",
    "et al . _",
    "@xcite combined the transpose - split method and de - aliased procedure in pseudo - spectral codes together to yield high parallel efficiency .",
    "based on the three time - evolution equations of the 3d navier - stokes ( ns ) equations , basu adopted a parallel scheme which can only be used on a 3-processor computer  @xcite . ling _",
    "et al . _",
    "@xcite try to parallel the 3d code by combining the 3-cpu method and the pfft scheme , and a comparison showed that the combined scheme is always slower than pfft .",
    "in contrast to the world - wide efforts in 3d parallelization , rather limited efforts have been devoted to parallelizing the two - dimensional simulations  @xcite .",
    "it is quite common to treat the 2d parallel spectral code as a simplified version of the 3d codes , which are very efficient only in 3d cases ( pfft ) . as a result",
    ", the ratio of the communication time to the computation time in those 2d parallel codes is relatively large , and the parallel efficiency is much lower compared with the corresponding 3d codes ( see a short discussion about this topic in  @xcite ) .    to minimize the relative long communication time , yin _ et al . _",
    "@xcite propose a parallel task distribution scheme ( ptd ) in the 2d pseudo - spectral ns code .",
    "although this scheme is very easy to implement with a good parallel efficiency , it has the limitation that the code can only use 2 , 4 , and 6 processors to do the calculation .    in this paper",
    ", we will parallel the 2d spectral codes by combining the ptd and pfft schemes .",
    "the new strategy overcomes the shortcomings of the former schemes and shows a significant improvement in parallel efficiency . in the following",
    ", we will denote this combined strategy as ptf scheme ( parallelization through task distribution and fft ) .",
    "the paper is organized as follows . in section 2 ,",
    "the ptf scheme is applied to solve the 2d ns equation ; the benchmark of the parallel code on sgi 3800 is presented .",
    "we also show several long - time numerical simulations with high resolutions , which reveal some interesting physical phenomena of 2d decaying turbulence  @xcite .",
    "of course , ptf plays an important role in those long - time runs . in section 3 ,",
    "the new scheme is applied to solve the 2d inviscid boussinesq equations .",
    "a challenging numerical problem , which is studied previously in  @xcite , is investigated with three kinds of resolutions ( @xmath3 , @xmath4 , and @xmath5 ) .",
    "finally , a summary of this work is given , where the prospects of the new scheme are also discussed .",
    "the study of the 2d turbulence distinguishes itself from the 3d turbulence due to its unique phenomena such as inverse energy cascade and self - organization . in the past few decades , a particular kind of statistical mechanics  @xcite has been widely adopted to study the 2d freely decaying turbulence ( see  @xcite and references therein ) .    as a powerful tool , direct numerical simulations ( dns )",
    "provide some useful theory check and inspire the deeper thoughts of the statistical theory ( e.g. see  @xcite ) .",
    "those simulations normally need to last as long as 100 - 1000 eddy turnover times before final states of the 2d decaying turbulence are reached .",
    "this means that the total calculations of this kind of 2d dns , although has a smaller array , are more or less the same as those of some short - term 3d dns .",
    "for example , 20 time steps of a 2d dns with the resolution of @xmath3 will take roughly the same cpu time as one time step of a 3d dns on a grid of @xmath6 on the same computer . on the other hand",
    ", because the time evolution loop is impossible to be parallelized , the parallel procedure within one time step becomes essential to improve the performance of a 2d dns code .    on modern supercomputers ,",
    "the total cpu time involved in the 2d dns is not as enormous as that of the 3d dns if the grid points are the same in each dimension .",
    "the peak performance of the parallel code , which is indicated by the shortest wall clock time regardless of the number of cpus used , becomes more important in 2d dns . in our research",
    ", we would like to get our 2d dns results in 3 days using 32 cpus rather than wait for a week using 8 cpus , although we only achieve a speedup of 2.33 by the fourfold processors .",
    "as will be seen later in this paper , the significant improvement of the peak speed is a big advantage of the ptf scheme .",
    "the 2d incompressible ns equation are usually written as @xmath7 @xmath8 where @xmath9 is the velocity , @xmath10 is the pressure , and @xmath11 is the kinematic viscosity . using the vorticity @xmath12 and stream function @xmath1 , eq .",
    "( 1 ) and ( 2 ) can be rewritten as : @xmath13 @xmath14    the stream function is related to velocity by @xmath15 and @xmath16 . in the conservative form , eq .",
    "( 3 ) becomes @xmath17    we adopted abcn scheme to carry out the time integration , which discretizes the nonlinear term @xmath18 with a 2@xmath19 order adams - bashforth scheme and the dissipation term @xmath20 with the crank - nicolson scheme .",
    "the particular time - stepping used is independent of the parallelization .",
    "more accurate schemes , such as 4@xmath21 order runge - kutta , could also be used .    when eqs .",
    "( 4 ) and ( 5 ) are solved by galerkin pseudo - spectral methods , the fourier coefficients of @xmath22 are evaluated at each time step . the nonlinear term @xmath23 is obtained by being transferred back and from the physical space with ffts ; in the meantime , the de - aliasing procedure has to be adopted to remove the aliasing error .",
    "there are two kinds of de - aliasing techniques available : padding - truncation and phase - shifts  @xcite .",
    "the fft we used requires the dimension size to be the power of 2 , which is faster than those ffts that has prime factors other than 2 .",
    "the costs of two de - aliasing techniques are the same as that for our ffts .",
    "therefore , in this paper we use the phase - shifts , which reserves more high wave numbers information than that for padding - truncation .     1 & @xmath24 & @xmath25 & @xmath26 & @xmath27 + 2 & & + 3 & @xmath28 & @xmath29 & @xmath30 & @xmath31",
    "+    table 1 shows the ten ffts needed to solve eqs .",
    "( 4 ) and ( 5 ) in each time step .",
    "the expression with a hat ( e.g. @xmath32 ) is the spectral space value of the corresponding physical value without hat ( @xmath33 , etc . ) .",
    "the capital letters denote the phase - shift values of the corresponding lower case variables .",
    "for example , in a simulation with @xmath34 resolution : @xmath35 where , @xmath36 indicates the grid point , @xmath37 is the wave number , @xmath38 , and @xmath39 .",
    "each arrow in table 1 indicated one 2d fft , and the four ffts in the third row should be calculated after the six ffts in the first two rows ( see fig.3 of  @xcite for an even clear picture ) .",
    "it is worth to mention that the pseudo - spectral method needs to evaluate twelve ffts for the non - conservative form ( eqs .",
    "( 3 ) and ( 4 ) ) ) , which should be in conservative form ( @xmath40 ) to march the structure of the program . ] , since the two ffts in the 2@xmath19 row of table 1 are replaced by four ffts to transfer @xmath41 , @xmath42 , and their phase - shift counterparts to physical space .",
    "( of course , the ffts in the 3@xmath43 row need to be changed correspondingly although no more fft is needed . )",
    "hence , it is a natural choice to use the conservative formulation when we use pfft scheme to parallel the 2d ns code .    in the following subsection",
    ", we will give a brief discussion for the pfft scheme .",
    "we will also introduce some symbols and analyzing tools that will be used throughout the paper .",
    "the pfft scheme calculates the ten parallelized ffts one after another according to certain sequence mentioned in the previous subsection . in our case ,",
    "the parallelized fft is the widely - adopted transpose - split parallel scheme , which computes one dimensional ( 1d ) fft in each direction combined with the data transfer between cpus and matrix transpose  @xcite .",
    "it is observed that the research on parallel fft is a fast developed field ( e.g. see  @xcite , or a relatively complete review in  @xcite ) .",
    "it is difficult to rank the available ffts , since there are so many different versions of ffts , different parallel schemes for the ffts , and different parallel computers to implement them .",
    "there is an argument that the transpose - split scheme _ is no longer the clear winner _",
    "( chapter 23 of ref  @xcite ) because it can not avoid the data communication while using 1d fft in two directions . in this paper",
    ", we will not devote our efforts to parallel fft ( simply use the most popular transpose - split ) , instead , we will try to find other methods to improve the parallel efficiency .",
    "our parallel scheme will be even faster if a faster parallel fft is adopted .",
    "the total time ( @xmath44 ) for each processor is the sum of the computation time ( @xmath45 ) and the communication time ( @xmath46 ) :    @xmath47    in the case of pfft , the 2d ns equations need to calculate ten ffts , which makes the major part of the computations .",
    "if the resolution of the simulation is @xmath34 , then    @xmath48    where , @xmath10 is the total number of processors , @xmath49 is the time that requires by one processor to compute one fft , and @xmath50 is cpu time of one fft in a single processor times a factor ( the factor is 5 in the case of full complex fft , and 5/2 for real - complex fft )  @xcite . for each fft",
    ", one processor needs to send ( @xmath51 ) blocks of data to other ( @xmath51 ) processors , and receives the same amount of data from all other processors .",
    "the size of each block is @xmath52 , so    @xmath53    where , @xmath54 is the time to transmit a word between processors , and @xmath55 is the latency time for a message passing . for the convenience of the analysis",
    "later , it is useful to divide @xmath46 into two parts  transmission time ( @xmath56 ) and latency time ( @xmath57 ) : @xmath58 hence , according to our analytical model , the total time for one time step on one processor is : @xmath59 in fact , the values of @xmath60 is smaller for larger cpu numbers because of the cache effect ( see the discussion later in this subsection ) , while @xmath54 and @xmath55 are almost constant for a given parallel computer . here , we are only trying to give an estimated analysis and treat @xmath54 , @xmath61 , and @xmath60 as constant ; accurate timing work will be the speedup plot resulting from the wall clock time ( see , e.g. , fig .",
    "1 , the similar approach is also adopted in  @xcite ) .    for many parallel systems ,",
    "@xmath55 is much larger than @xmath54 ( sometimes a factor of 1000  @xcite ) , so @xmath57 is a nontrivial part of @xmath44 , especially when @xmath34 is small .",
    "according to eq . ( 11 ) , if we use more processors in the simulation , @xmath57 will become larger while the rest part ( @xmath62 ) will be smaller . eventually , @xmath57 will become the dominating part of @xmath44 , which affects the parallel efficiency of the pfft scheme . in the case of the 2d dns ,",
    "this phenomenon is seen in relatively small cpu numbers , because @xmath62 is not very large . in the 3d dns with high resolutions ( @xmath63 or higher )",
    ", @xmath57 will take a very small portion in @xmath44 except for very large @xmath10 .",
    "this is the main difference between the 2d and 3d problems .",
    "[ 1 ]    fig .",
    "1 shows the speedup of pfft on different resolutions .",
    "the speedup factor is the wall clock time of serial code divided by that of the parallel code in the same resolution . for the resolution of @xmath64 , the run with eight processors",
    "has the top speed .",
    "if the cpu number is larger than eight , the speed of the code drops down , and the parallel code with 32 or 64 processors is even slower than the serial code . for resolutions of @xmath65 and @xmath66 , 16 cpus give the best performance , while 32 and 64 cpus lead to worse performance . for @xmath3 ,",
    "the fastest run is the one with the maximum available cpus .",
    "( the sgi 3800 in our laboratory is a 64-nodes system ; we can not test the cases with more than 64 processors . )",
    "we can predict from the tendency of the curve ( or , eq . ( 11 ) ) that the speedup will also drop down for the @xmath3 run with 128 or more processors .",
    "one may notice that the super - linear speedup for runs with @xmath3 resolution : in the case of 16 cpus , a speedup of 21 is observed .",
    "the super - linear speedup is due to the so - called _",
    "cache effect_. in the architecture of modern computers , caches , which can be viewed as a smaller and faster memory , are widely used to increase the computer performance . for the pfft ns simulation with fixed @xmath34 ,",
    "the more cpus are involved , the higher hit rate of cache will be obtained because the code exhibits sufficient data locality in its memory accesses .",
    "hence , the unit calculation finished on one cpu in the pfft code is faster than one cpu with the serial codes , that is , @xmath60 is smaller .",
    "the behavior of the cache memory is very hard to predict , because modern computer architectures have very complex internal organizations : different levels of caches , branch predictors , etc .",
    "it is challenging to predict the cache effect in the research of parallel computation  @xcite .",
    "therefore , we do not take the cache effect into the consideration of the analyzing model in eqs .",
    "( 6 - 11 ) .",
    "to sum up , @xmath67 takes a large portion in the total communication time ( @xmath46 ) and seriously reduces the parallel efficiency for larger cpu numbers in the pfft scheme .",
    "the issue of minimizing @xmath57 is the key to enhance the parallel efficiency .",
    "if we start parallel programming from an available serial code , the ptd scheme is always the first choice because it is very easy to implement  @xcite . our new parallel scheme ( ptf )",
    "will combine ptd and pfft scheme together , with ptd being a special case .",
    "as mentioned earlier , there are ten ffts needed to be evaluated at each time step . in ptf scheme",
    ", they can be divided into two , four , and six groups corresponding to the 2 , 4 , and 6 cpus scheme in ptd  @xcite . in the following",
    ", we will call these three ptf schemes as `` 2-n , '' `` 4-n , '' and `` 6-n '' scheme , respectively .",
    "( we also denote pfft scheme as `` 1-n '' scheme to unify notations ) .",
    "there are six ffts in the 2-n scheme for each group , while three ffts in the 4-n scheme .",
    "note that there are twelve ffts in the 2-n and 4-n schemes at each time step ( there is only ten ffts in serial and pfft codes ) because the two ffts in the 2@xmath19 row of table 1 are calculated twice to eliminate the total communication time .",
    "the 6-n scheme will not be discussed in the rest of the paper , because we want to compare the parallel efficiency of the ptf scheme with the pfft ( 1-n ) scheme , the number of cpus involved should be powers of 2 .    for any type of ptf scheme , one group will be called `` master group , '' on which the time integration and data input and output ( i / o ) are carried out , and the other groups will be called `` slave groups . '' when calculating fft , data are exchanged only within each group .",
    "when it is necessary to transfer information between groups , the first node in master group will and only will communicate with the first nodes in the slave groups ; likewise , the second nodes in different groups will communicate with each other , etc .    for the 2-n scheme ,",
    "the computation time of one processor is    @xmath68    where , the factor 6 comes from the six ffts in each group , and @xmath49 is divided by @xmath69 because the processors available are split into two groups . in the beginning of time loop , each node in the master group needs to send one @xmath70 block data to the corresponding node in the slave group , and receive the same amount of data at the end of time loop .",
    "so together with the data transferred within each fft , the two parts of the communication time are : @xmath71 the total time per time step is @xmath72 similarly , for the 4-n scheme , there is one master group and three slave groups .",
    "the computation time is @xmath73 in the beginning of time loop , each node in the master group needs to broadcast one @xmath74 block data to the corresponding node in slave groups , and gather the same amount of data at the end of time loop .",
    "the standard broadcast and gather routine in mpi require @xmath75 steps ( @xmath76 is the size of the group ; here , @xmath77 ) in sending and receiving to finish the group communication .",
    "so together with the data transferred within each fft , the two parts of the communication time are : @xmath78 and the total time used per time step is @xmath79    as can be seen from eqs .",
    "( 11 ) , ( 15 ) , and ( 19 ) , @xmath45 in the 1-n , 2-n , and 4-n schemes are roughly the same ( 20% difference at most ) for fixed @xmath80 and @xmath10 ; @xmath57 in the 1-n scheme is about three times as large as that in the 2-n scheme , and about thirteen times as large as that in 4-n scheme . in the meanwhile , @xmath56 in the 2-n or 4-n scheme is increased by relatively small factor ( no more than 2 ) compared with 1-n scheme .",
    "hence , @xmath57 in the 1-n scheme will occupy the largest partition of @xmath44 among the 1-n , 2-n , and 4-n schemes .    as discussed in the previous subsection",
    ", @xmath57 is the main barrier for achieving high parallel efficiency when @xmath10 is large for relatively low resolution ( @xmath34 ) .",
    "this problem is partly solved by using 2-n and 4-n schemes .",
    "however , even in the 2-n and 4-n schemes , @xmath67 still increases and the remaining parts of @xmath44 decrease for larger @xmath10 ; the bottleneck effect in parallelization still exits .    [ 1 ]    [ 1 ]    figs .",
    "2(a ) shows the speedup plot for the 2-n scheme . for the resolution of @xmath64 , the peak speed appears at 16 cpus run ( compared with 8 cpus run for the 1-n scheme ) . for the resolution of @xmath65 , the peak speed appears at 32 cpus run ( compared with 16 cpus run for 1-n scheme ) . for @xmath66 and @xmath3 in the 2-n scheme , and all the resolutions in the 4-n scheme ( fig .",
    "2(b ) ) , 64 cpus run is the fastest .    because two extra ffts are introduced and @xmath81 is slightly larger in 2-n and 4-n schemes , some runs in figs . 2 are slower than the corresponding 1-n ones for certain resolution and @xmath10 .",
    "for example , there is no super - linear speedup observed for the @xmath3 curve in fig .",
    "2(b ) , although the 4-n scheme is twice as fast as the 1-n scheme when 64 processors are used .    in practice , the fastest scheme for fixed",
    "@xmath10 and @xmath34 is always what we want to use to do long time simulation .",
    "fig.3 is a combined figure which consists of the best performance point in figs . 1 and 2 . table 2 indicates the corresponding schemes adopted to draw this `` best speedup '' plot .",
    "it should be emphasized that most points in the @xmath3 curve show super - linear speedup .",
    "the points when @xmath82 come from the 1-n scheme , while the 2-n and 4-n schemes are faster for @xmath83 .",
    "[ 1 ]     @xmath65 * * * * & @xmath66 * * * * & @xmath3 * * * * + 1 cpu & 1-n & 1-n & 1-n & 1-n + 2 cpus & 2-n & 1-n & 1-n & 1-n + 4 cpus & 4-n & 4-n & 1-n & 1-n + 8 cpus & 4-n & 2-n & 1-n & 1-n + 16 cpus & 4-n & 4-n & 4-n & 1-n + 32 cpus & 4-n & 4-n & 4-n & 2-n + 64 cpus & 4-n & 4-n & 4-n & 4-n +    for lower resolutions , the 1-n scheme works the best for small @xmath10 , and the 2-n or 4-n scheme dominates the points on the corresponding curve in fig.3 gradually for larger @xmath10 . on the first column of table 2 , which corresponds to the resolution of @xmath64 , the 4-n scheme dominates for @xmath84 .",
    "the situations of @xmath85 in the 2-n scheme and @xmath86 in the 4-n scheme correspond to the ptd scheme .",
    "although the benchmark results are slightly different from  @xcite due to different computers used , the conclusion is the same : ptd is an easily implemented and efficient parallel scheme for the 2d dns for relatively small resolutions .",
    "the easy implementary property of ptd is inherited by ptf schemes : once the pfft code is ready , it is very easy to change it to ptf codes .",
    "hence , ptf is an attractive strategy , especially in 2d simulations .      in this subsection",
    ", we will use our ptf codes to investigate an interesting phenomenon in 2d decaying turbulence  the multi - valued @xmath0-@xmath1 structure .",
    "it is well known that if there is a functional relation @xmath87 , then the nonlinear term in eq .",
    "( 3 ) gives :    @xmath88    thus eq . ( 3 ) becomes    @xmath89    for very high reynolds number or @xmath90 , eq .",
    "( 21 ) turns to a stationary equation    @xmath91    it is common to treat @xmath92 as an indication of the final state of 2d turbulence , but the simulation shown in figs .",
    "18 - 19 of ref  @xcite gives a counter example : the double - valued @xmath0-@xmath1 structure ( see  @xcite for a detailed discussion about this kind of structure ) .",
    "we will seek some other simulations to validate the generality of multi - valued structures , and there are mainly two choices to do this :    * make changes in the initial conditions ; * test different values of @xmath11 for certain kind of initial condition .",
    "the parallel code we used before ( ptd scheme  @xcite ) has a very limited speedup because the maximum number of cpus used is limited to six .",
    "most simulations were carried out by changing the initial condition for relatively small @xmath11 with the resolution of @xmath66  @xcite , which normally last from several days to a few weeks . for @xmath3 runs ,",
    "the calculations for one time step are four times as large as those for @xmath66 , and the time step has to be smaller due to the cfl condition .",
    "a typical 2d decaying turbulence lasts from two months to half year if we only use ptd schemes . with the ptf codes",
    ", it is now possible to carry out @xmath3 simulations to find the new double - valued @xmath0-@xmath93 structure .",
    "we carried out two simulations starting from four equal sized vortices patches , which are asymmetrically placed in a double periodic box ( fig .",
    "the first run adopted relatively low reynolds number ( @xmath94 ) with the resolution of @xmath66 .",
    "the time step is 0.0005 .",
    "we used 32 cpus in total ( this is the maximum nodes available for long time simulations in lsec ) . according to table 2 ,",
    "the 4-n scheme is the fastest approach under this situation .",
    "the simulation lasted for 17 hours , and reached the quasi - steady state at @xmath95 ( fig .",
    "5(b ) shows a simple function relation of @xmath0-@xmath96 .",
    "the same run lasts for 29 hours if we use the pfft scheme ( 32 cpus ) , and 82 hours for the ptd scheme ( 4 cpus ) .",
    "[ 1 ]    [ 1 ] evolving from the initial condition shown in fig . 4 ( re=4000 ) .",
    "the resolution adopted is @xmath66 .",
    "( b ) is the @xmath0-@xmath1 scatter plot at the same time , which shows a simple functional relation.,title=\"fig : \" ]    [ 1.2 ] evolving from the initial condition shown in fig . 4 ( re=4000 ) .",
    "the resolution adopted is @xmath66 .",
    "( b ) is the @xmath0-@xmath1 scatter plot at the same time , which shows a simple functional relation.,title=\"fig : \" ]    [ 1 ] evolving from the initial condition shown in fig . 4 ( re=40000 ) .",
    "the resolution adopted is @xmath3 .",
    "( b ) is the @xmath0-@xmath1 scatter plot at the same time , which shows a double - valued structure.,title=\"fig : \" ]    [ 1.2 ] evolving from the initial condition shown in fig . 4 ( re=40000 ) .",
    "the resolution adopted is @xmath3 .",
    "( b ) is the @xmath0-@xmath1 scatter plot at the same time , which shows a double - valued structure.,title=\"fig : \" ]    the second run adopted a large reynolds number ( @xmath97 ) with the resolution of @xmath3 .",
    "we used the 2-n scheme code on 32 processors , and the quasi - steady state was reached at @xmath98 after 16 days calculation . in the late state of this simulation , the orientation of the flow pattern is totally different from that obtained with the @xmath66 run ( fig .",
    "moreover , the double - valued @xmath0-@xmath1 structure is recovered ( fig .",
    "6(b ) ) , which fits the definition of the final state of the 2d turbulence given in  @xcite .",
    "hence , it seems that further computations do not lead to any new phenomenon . the same run lasts for 20 days if we use the pfft scheme ( 32 nodes ) , and about one year for the ptd scheme ( 4 nodes ) .",
    "it is interesting to see the appearance of multi - valued @xmath0-@xmath99 structures in high reynolds number ( 40000 ) , while relatively low reynolds number ( 4000 ) leads to traditional functional relation .",
    "although the physical mechanism of the multi - valued structure is still unclear , our simulations reveal some limitations of the statistical mechanics  @xcite , and it is worthwhile to carry on further investigations in this direction .",
    "of course , the ptf scheme may play an important role in these large scale computations because of its high parallel efficiency .    with maximum 32 processors available for long time simulation ,",
    "the peak speed of pft is only about 70% faster than that of pfft for the resolution of @xmath66 , and 24% for @xmath3 .",
    "however , for runs like what are shown in figs . 6",
    ", 24% shortage still means 4 days calculation on 32 nodes , which is definitely nontrivial .",
    "it is very interesting to understand whether a finite time blow - up of the vorticity and temperature gradient can happen from a smooth initial condition in 2d inviscid boussinesq convection .",
    "this has been studied recently by several groups ending with different conclusions : pumir and siggia used the tvd scheme with the adaptive meshes ( the maximum grid size is @xmath65 ) , which observed the blow - up process  @xcite ; e and shu did not obtain the blow - up phenomena using the eno scheme on a @xmath66 grid and spectral methods on a @xmath100 grid  @xcite ; and ceniceros and hou also obtained blow - up free solutions using the adaptive mesh computation with a @xmath66 grid  @xcite .",
    "it is worth mentioning that the maximum mesh compression ratio in  @xcite is 8.83 , which gave an effective resolution of @xmath101 on uniform mesh . although those groups yielded different conclusions , they all tried to use the highest possible resolutions to investigate the problem .",
    "hence , parallel computing is a natural choice .",
    "the 2d inviscid boussinesq convection equations can be written in @xmath0-@xmath1 formulation : @xmath102 again , abcn scheme is adopted to carry out the time integration . below",
    "we will discuss how to implement our parallel strategy for eqs .",
    "( 23 ) - ( 25 ) .",
    "numerical results on the finite time blow - up will be reported .      as shown in table 3 , there are 20 ffts involved when the fourier - galerkin spectral methods are used to solve eqs .",
    "( 23)-(25 ) .",
    "these ffts can be divided into four independent groups , which are indicated by the different columns in table 3 .",
    "there is no communication within the different columns until all ffts are finished . in each column , the 1@xmath103 and the 2@xmath19 fft need to be evaluated before the 4@xmath21 one , while the 5@xmath21 fft should be calculated after the 1@xmath104 and 3@xmath43 one are finished .     1 & @xmath24 & @xmath105 & @xmath106 & @xmath107 + 2 & @xmath108 & @xmath109 & @xmath110 & @xmath111 + 3 & @xmath112 & @xmath113 & @xmath114 & @xmath115 + 4 & @xmath116 & @xmath117 & @xmath118 & @xmath119 + 5 & @xmath120 & @xmath121 & @xmath122 & @xmath123 +     1 & @xmath124 & @xmath125 & @xmath126 & @xmath27 + 2 & @xmath108 & @xmath109 & @xmath110 & @xmath111 + 3 & @xmath116 & @xmath117 & @xmath118 & @xmath119 + & e & f & g & h + 1 & @xmath24 & @xmath105 & @xmath106 & @xmath107 + 2 & @xmath112 & @xmath113 & @xmath114 & @xmath115 + 3 & @xmath120 & @xmath121 & @xmath122 & @xmath123 +    when",
    "the ptf scheme is used to parallelize the code , there are five options to divide the groups without too much extra data communication :    1 .   1 - n scheme ",
    "one group ; each fft is computed by all the cpus involved , i.e. pfft scheme .",
    "( here we use `` n '' instead of",
    "`` n '' to avoid conflicts with the parallel schemes for the ns equations ) .",
    "2 .   2 - n scheme  two groups ; column a & b in one group , and column c & d in the other group .",
    "4 - n scheme  four groups , which belongs to different columns in table 3 , respectively .",
    "8 - n scheme  eight groups ,",
    "see column a - h of table 4 .",
    "note that there are four extra ffts introduced to save the communication time : .",
    "the first row ffts in table 3 are calculated twice in the first row of table 4 .",
    "12 - n scheme ",
    "twelve groups ; the first three rows ffts in table 3 are calculated simultaneously in twelve groups , the results from the four groups calculating the first row ffts are transferred to the eight groups computing the 2@xmath19 and 3@xmath43 row respectively , and the rest eight ffts in the 4@xmath21 and 5@xmath21 rows are performed in those eight groups .",
    "( like the 6-n scheme in the ns solver , the 12-n scheme will not be discussed here because the number of processors involved is not of the power of 2 . )",
    "similar to section 2 , we will analyze the total computation time for different schemes in the following :    * 1-n scheme , 20 ffts are involved : @xmath127 * 2-n scheme , 10 ffts in each groups , and two blocks ( there are two equations in the system - eqs . ( 23 ) and ( 24 ) ) of @xmath128 size data will be transferred between the master group and the slave group in the beginning and end parts of the time loop : @xmath129\\times t_{sendrec } \\\\   \\quad \\quad \\quad + \\left ( { 10\\times ( \\frac{\\textstyle p}{\\textstyle 2 } - 1)\\times 2 + 2\\times 2 } \\right)\\times t_{delay } + \\frac{\\textstyle 10}{\\textstyle p / 2}\\times \\left ( { m^2\\log _ 2 \\left({m^2 } \\right)\\;t_c } \\right ) \\\\   \\;\\quad = ( 48p - 80)\\times \\frac{\\textstyle m^2}{\\textstyle p^2}\\times t_{sendrec }",
    "+ ( 10p - 16)\\times t_{delay } + \\frac{\\textstyle 20}{\\textstyle p } \\times \\left ( { m^2\\log _ 2 \\left ( { m^2 } \\right)\\;t_c } \\right ) . \\\\   \\end{array}\\ ] ] * 4-n scheme , 5 ffts in each groups , and two blocks of @xmath130 size data will be transferred between the master group and the slave groups in the beginning and end parts of the time loop : @xmath131\\times t_{sendrec } \\\\   \\quad \\quad \\quad + \\left ( { 5\\times ( \\frac{\\textstyle p}{\\textstyle 4 } - 1)\\times 2 + 2\\times 2\\times \\log _ 2 ^ 4 } \\right)\\times t_{delay } + \\frac{\\textstyle 5}{\\textstyle p / 4}\\times \\left ( { m^2\\log _ 2 \\left ( { m^2 } \\right)\\;t_c } \\right ) \\\\   \\quad \\quad = ( 72p - 160)\\times \\frac{\\textstyle m^2}{\\textstyle p^2}\\times t_{sendrec }",
    "+ ( \\frac{\\textstyle 5}{\\textstyle 2}p - 2)\\times t_{delay } + \\frac{\\textstyle 20}{\\textstyle p}\\times \\left ( { m^2\\log _ 2 \\left ( { m^2 } \\right)\\;t_c } \\right ) .",
    "\\\\   \\end{array}\\ ] ] * 8-n scheme , 3 ffts in each group , and two blocks of @xmath132 size data will be transferred between the master group and the slave groups in the beginning and end parts of the time loop : @xmath133\\times t_{sendrec } \\\\",
    "\\quad \\quad \\quad + \\left ( { 3\\times ( \\frac{\\textstyle p}{\\textstyle 8 } - 1)\\times 2 + 2\\times 2\\times \\log _ 2 ^ 8 } \\right)\\times t_{delay } + \\frac{\\textstyle 3}{\\textstyle p / 8}\\times \\left ( { m^2\\log _ 2 \\left ( { m^2 } \\right)\\;t_c } \\right ) \\\\",
    "\\quad \\quad = ( 144p - 384)\\times \\frac{\\textstyle m^2}{\\textstyle p^2}\\times t_{sendrec } + ( \\frac{\\textstyle 3}{\\textstyle 4}p + 6)\\times t_{delay } + \\frac{\\textstyle 24}{\\textstyle p}\\times \\left ( { m^2\\log _ 2 \\left ( { m^2 } \\right)\\;t_c } \\right ) .",
    "\\\\   \\end{array}\\ ] ]    for all the ptf discussed above ( eq . ( 26)-(29 ) ) , it is clear that @xmath57 will take larger portion of @xmath44 when @xmath10 becomes larger in those schemes . for fixed @xmath10",
    ", @xmath57 will take smaller portion of @xmath44 when more groups are adopted in the ptf schemes ( e.g. 4-n or 8-n scheme ) .",
    "hence , the 4-n and 8-n schemes have some advantage over the 1-n and 2-n schemes when @xmath10 is large . some further discussions for eqs .",
    "( 26 ) - ( 29 ) will be continued in the next subsection together with the speedup plots ( figs.7 ) .",
    "like the 2d ns equations ( eqs .",
    "( 4 ) and ( 5 ) ) , eq . ( 24 ) and ( 25 ) can also be written in the conservative form : @xmath134 when the fourier - galerkin spectral methods are used to solve the equations above , eq .",
    "( 31 ) presents some problem because the time evolution step is carried out in spectral space : to get the value of @xmath135 in the next time step , @xmath136 has to be transferred back to physical space so the new value can be obtained by using the relation @xmath137 .",
    "furthermore , there is an extra nonlinear term @xmath138 in eq .",
    "( 33 ) which requires one extra fft .",
    "the total number of ffts in conservative form is the same as non - conservative form .",
    "hence , unlike what we did for 2d ns equations in section 2 , the non - conservative equations are solved here .      in this subsection , we will show the speedup plots of the boussinesq equations on sgi3800 .",
    "unlike what we did for the ns equations , we added a new resolution ( @xmath139 ) to show the effectiveness of the new parallel scheme in the low resolution .",
    "the maximum processors used here is 32 , which makes the resulting speedup plots ( figs .",
    "7 ) more concise .",
    "it is also easier to use less than 32 processors in our 64-nodes machine ; because we have to shut all other computing jobs down if we want to do the 64 cpus run .",
    "[ 1 ]    [ 1 ]    fig . 7 ( a ) shows the speedup plot for pfft ( or .",
    "1-n ) scheme .",
    "the top speedup is obtained on 4 processors for the @xmath139 grid , while 8 and 16 processors reach the top speed for the resolutions of @xmath64 and @xmath65 , respectively .",
    "the speedup curves of @xmath66 and @xmath3 show reasonably good parallel efficiency of pfft scheme .",
    "the @xmath66 runs show super linear speedup for 2 , 4 , 8 , and 16 nodes , which is due to the cache effect .",
    "the @xmath3 runs only have super linear speedup in the cases of 8 and 16 nodes .",
    "the speedups of 32-nodes run for these two high resolutions are lower than 32 because @xmath57 begins to dominate the total computation time .",
    "we did not show the speedup plots for resolutions higher than @xmath3 because of the limit of the maximum local memory .",
    "however , since solving the equations with the highest possible resolution is the main task of our research , we will discuss the parallel efficiency on those high resolutions ( @xmath4 and @xmath5 ) in other ways later in the next subsection .",
    "7(b ) shows the `` best speedup '' plot of ptf schemes .",
    "all the resolutions have their top speed in the 32 cpus case .",
    "moreover , almost all the points on the curves show super linear speedup for higher resolutions ( @xmath66 and @xmath3 ) .",
    "the peak speeds shown in fig .",
    "7(b ) are increased by a factor of 27% ( @xmath3 resolution ) to 171% ( @xmath139 resolution ) compared to the pfft scheme .",
    "the lower resolutions get a higher factor because the limitation of the total cpus available .",
    "we can predict that the factor of @xmath3 resolution will be larger than 27% if the maximum number of cpus used is larger than 32 .     @xmath64 * * * * & @xmath65 * * * * & @xmath66 * * * * & @xmath3 * * * * + 1 cpu & 1-n & 1-n & 1-n & 1-n & 1-n + 2 cpus & 2-n & 2-n & 1-n & 1-n & 2-n + 4 cpus & 2-n & 2-n & 1-n & 1-n & 2-n + 8 cpus & 8-n & 2-n & 2-n & 1-n & 1-n + 16 cpus & 8-n & 8-n & 2-n & 2-n & 1-n + 32 cpus & 8-n & 8-n & 8-n & 4-n & 2-n +    table 5 shows the corresponding schemes to the points on the fig .",
    "pfft ( or , 1-n ) scheme never show the best performance for resolutions of @xmath139 and @xmath64 for @xmath140 ( for @xmath141 , 1-n is the only choice , which is not necessary for comparison ) . for resolutions higher than @xmath65 , the 1-n scheme reaches the peak speed more frequently , especially for runs with fewer processors .    as indicated in table 5 ,",
    "the ptf schemes with more groups ( e.g. 4-n or 8-n scheme ) have better speedup for lower resolutions and larger number of cpus , while the schemes divided into fewer groups ( e.g. 1-n or 2-n scheme ) work best for higher resolutions and relatively smaller number of cpus .    in the real programming efforts",
    ", we found that it is more convenient to fix the size of each group ( instead of fixing @xmath10 ) because the size of main calculating arrays can be determined beforehand .",
    "the code will get the number of groups ( 1 , 2 , 4 , or 8) at the initial stage of real runs , and distribute the 20 ffts into different groups .",
    "thus , the programming effort concerning the pft scheme is trivial if the pfft code is already available .",
    "attention should be drawn to the performance of the 2-n scheme , which occupy eight of the total thirty places in table 5 . in",
    "the @xmath3 runs , the 2-n scheme is faster than the 1-n scheme on 2 and 4 nodes simulations , which is counter to the conclusion we made in the above paragraph .",
    "although most of the weird speedup behaviors in parallel computing can be attributed to cache effect , this one presents some difficulties because normally the more cpus are used to calculate one fft , the smaller @xmath60 will be ( see the discussion in section 2.2 ) . to explain this , we need to calculate the exact values of @xmath46 for the 1-n and 2-n schemes in our analytical models ( eqs . (",
    "26 ) and ( 27 ) ) .    for the 1-n scheme ,",
    "@xmath142    for the 2-n scheme ,    @xmath143    it is clear that all parts of @xmath46 in the 2-n scheme are smaller than the 1-n scheme in the case of @xmath144 , although @xmath56 of the 2-n scheme is larger than that of the 1-n scheme for @xmath145 .",
    "again , we assume that @xmath60 remains roughly the same for different schemes .",
    "the complete explanation has to take `` cache effect '' into consideration .",
    "[ 1].,title=\"fig : \" ]    in this subsection , we will show some numerical simulations calculated by our parallel codes .",
    "for ease of comparison with former results , we adopted the same initial condition in ref  @xcite :    @xmath146    @xmath147,\\ ] ]    where @xmath148    @xmath149    fig .",
    "8 shows the initial contour of @xmath150 on a grid of @xmath3 .",
    "the cap - like contour will develop into a rising bubble during the evolution , with the edge of the cap rolling up .    [",
    "1.0 ] , ( c ) and ( d ) show the contour plots at t=3.7.,title=\"fig : \" ]    [ 1.0 ] , ( c ) and ( d ) show the contour plots at t=3.7.,title=\"fig : \" ]    [ 1.0 ] , ( c ) and ( d ) show the contour plots at t=3.7.,title=\"fig : \" ]    [ 1.0 ] , ( c ) and ( d ) show the contour plots at t=3.7.,title=\"fig : \" ]    [ 1.0 ] .",
    "( c ) and ( d ) are the contour plots with the resolution of @xmath5 at t=3.7.,title=\"fig : \" ]    [ 1.0 ] .",
    "( c ) and ( d ) are the contour plots with the resolution of @xmath5 at t=3.7.,title=\"fig : \" ]    [ 1.0 ] .",
    "( c ) and ( d ) are the contour plots with the resolution of @xmath5 at t=3.7.,title=\"fig : \" ]    [ 1.0 ] .",
    "( c ) and ( d ) are the contour plots with the resolution of @xmath5 at t=3.7.,title=\"fig : \" ]    at @xmath151 , the density and vorticity contour develop into the shape of `` two eyes . ''",
    "9 ( a ) and ( b ) show the results with the resolution of @xmath3 . for spectral methods with different resolutions ( @xmath4 run of our simulation , and @xmath100 run of  @xcite ) and eno finite difference methods in  @xcite , some good agreements have been observed .",
    "arguments arise when the computations are carried on . at @xmath152 ,",
    "the smooth edge of the rolling eyes becomes unstable , see figs .",
    "9(c ) and ( d ) .",
    "whether this phenomenon is physically real or not is still an open question .",
    "it was argued that this collapse is due to the numerical effect  @xcite .    as discussed above , the most straightforward way to investigate",
    "this problem is to increase the resolution of simulations .",
    "therefore , we also adopted grids of @xmath4 and @xmath5 in the simulations .    for the @xmath3 run ,",
    "the time step used is 0.0008 which is determined by the cfl condition .",
    "it took the parallel 2-n scheme twenty hours to reach @xmath153 with 32 processors .",
    "it takes 25 hours for pfft schemes , and 28 days for the serial code .    for the @xmath4 run ,",
    "the time step used is 0.0004 .",
    "the speed of the 1-n scheme and the 2-n scheme with 32 nodes are very close ( a difference of 2% ) .",
    "we use the 1-n scheme because it is slightly faster .",
    "the code lasts for 13 days before it reaches @xmath154 . for serial code",
    ", it may take one year provided the memory of the computer is large enough .    for the @xmath5 run , the time step used is 0.0002 .",
    "the 1-n scheme is the only choice because other schemes require larger computer memory : in principle , the memory size of the 2-n , 4-n , and 8-n schemes are 2 , 4 , and 8 times as large as that of the 1-n scheme .",
    "for this run , it is even a burdensome job for the parallel computation with 32 processors , which may last for four months in total .",
    "we actually use the interpolated data of the @xmath4 run at @xmath155 as the initial condition .",
    "it is believed that the solution at @xmath155 is smooth .",
    "the code lasts for three weeks before it reaches @xmath153 .    figs .",
    "10(a , b ) show contour plots of the @xmath4 run at @xmath154 , on which the collapse is observed in the smooth edge ; the same phenomenon is observed in the @xmath5 run ( figs .",
    "10(c , d ) ) .",
    "it is noticed that more rolling vortices are observed on the edge of the `` eyes '' in fig .",
    "10(a ) than that in fig .",
    "9(c ) , and even more vortices appear in fig .",
    "there are some trivial differences if we make a detailed comparison within these three runs , but figs .",
    "9(c , d ) and figs . 10 clearly show that the collapsing process is not a non - physical phenomenon from the numerical artifacts .",
    "if we started the @xmath5 run from @xmath156 using eq .",
    "( 32 ) and ( 33 ) to get the initial data , the contour plots might be slightly different from what we obtained in figs . 10(c , d ) .",
    "the main task here is to determine the possibility of the collapse phenomenon , the potential small difference in the flow pattern will not hurt the general conclusion .",
    "some further details of the physical phenomenon will be discussed in another work . here , we will try to focus our topic on the parallel solver .",
    "unlike the simulations in section 2 , most of computations ( two of the three ) in this section use the traditional pfft scheme mainly due to the limitation of the maximum cpu number .",
    "if more processors are available so that the computer memory presents no problem to the ptf scheme , then the speed of ptf will also be faster than pfft .",
    "to sum up , the ptf scheme takes the advantages of the ptd and pfft schemes , and can increase peak speed of codes significantly .",
    "the ptf scheme works best for low resolution run , or relatively large resolution with large number of cpus . for very high resolutions ,",
    "the ptf scheme is most likely slower than pfft if not many processors are involved ; ptf also needs more memory than that for pfft , which presents some limitations on the scale of the highest resolution that one particular parallel computer can handle .",
    "these two disadvantages of ptf explain why similar schemes are not very popular in the 3d simulations where the array sizes may be even larger than that for the 2d simulations with high resolutions .",
    "it is advisable to treat pfft as a special case of ptf , and the combined version of the parallel codes will always yield the best performance after a small amount of preparing work ( e.g. making a table similar to table 2 and table 5 on the particular parallel machine to be used ) . in the meanwhile ,",
    "the simple analytical mode of the parallel scheme , which divide @xmath44 into three parts : @xmath56 , @xmath57 , and @xmath45 , is useful in explaining the performance of the parallel codes .",
    "the parallel code of the ns equations helps us to find another example of double - valued @xmath0-@xmath1 structure ; and the parallel boussinesq code reveals the insight of an open question , which suggests that the collapse of the bubble cap may be a candidate for the finite time singularity formation .",
    "both sets of investigation show that the parallel computing is useful in large scale numerical simulations .",
    "we conclude our paper by some discussions on the future parallel computing .",
    "it is well - known that the speed of single processor increases much faster than the speed of memory access ( five times faster in the last decade ) .",
    "hence the latency of memory access in terms of processor clock cycle grows by a factor of five in the last ten years  @xcite . for a fixed type of parallel scheme , the speedup curves ( see figs",
    ". 1 - 3 and 7 ) for high resolution ( e.g. @xmath3 ) in future machines may be similar to that of low resolutions in current computers ( @xmath139 or @xmath64 ) . in the case of pfft scheme applied on a @xmath3 resolution , it is possible that the speed of 64-nodes run is slower than that of the serial code in the future . on the other hand ,",
    "the ptf scheme which uses much less @xmath57 than that of the pfft scheme has greater advantage over the pfft scheme in parallel computing .",
    "in other words , the ptf strategy is also a scheme for the future .",
    ".25 cm * acknowledgments .",
    "* we would like to thank linbo zhang , zhongze li , and ying bai for the support of using local parallel computers .",
    "zy also thanks prof .",
    "matthaeus who supplied the original serial fortran 77 navier - stokes pseudospectral codes .",
    "tt thanks the supports from international research team on complex system , chinese academy of sciences , and from hong kong research grant council .",
    "z. yin , h.j.h .",
    "clercx , and d.c .",
    "montgomery , an easily implemented task - based parallel scheme for the fourier pseudospectral solver applied to 2d navier - stokes turbulence , _ comput . fluids _ , * 33 * , 509 ( 2004 ) .",
    "y. kaneda , t. ishihara , m. yokokawa , k. itakura , and a. uno , energy dissipation rate and energy spectrum in high resolution direct numerical simulations of turbulence in a periodic box , _ phys .",
    "fluids , _ * 15 , * l21 ( 2003 )",
    ".                        z. yin , d.c .",
    "montgomery , and h.j.h .",
    "clercx , alternative statistical - mechanical descriptions of decaying two - dimensional turbulence in terms of `` patches '' and `` points , '' _ phys .",
    "fluids , _ * 15 , * 1937 ( 2003 ) .              c.y .",
    "chu , comparison of two - dimensional fft methods on the hypercube , in _ proc .",
    "third confer . on hypercube",
    "concurrent comput .",
    "_ , pasadena , california , united states , 1988 , ( acm press , new york , ny , usa , 1989 ) , vol . 2 , pp ."
  ],
  "abstract_text": [
    "<S> a novel parallel technique for fourier - galerkin pseudo - spectral methods with applications to two - dimensional navier - stokes equations and inviscid boussinesq approximation equations is presented . </S>",
    "<S> it takes the advantage of the programming structure of the phase - shift de - aliased scheme for pseudo - spectral codes , and combines the task - distribution strategy [ yin , clercx and montgomery , comput . </S>",
    "<S> fluids , 33 , 509 ( 2004 ) ] and parallelized fast fourier transform scheme . the performances of the resulting mpi fortran90 codes with the new procedure on sgi 3800 are reported . for fixed resolution of the same problem </S>",
    "<S> , the peak speed of the new scheme can be twice as fast as the old parallel methods . </S>",
    "<S> the parallelized codes are used to solve some challenging numerical problems governed by the navier - stokes equations and the boussinesq equations . </S>",
    "<S> two interesting physical problems , namely , the double - valued @xmath0-@xmath1 structure in two - dimensional decaying turbulence and the collapse of the bubble cap in the boussinesq simulation , are solved by using the proposed parallel algorithms .    ,    ,    parallel computing ; pseudo - spectral methods ; task distribution ; navier - stokes equations ; boussinesq equations </S>"
  ]
}