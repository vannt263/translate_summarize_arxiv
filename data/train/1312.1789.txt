{
  "article_text": [
    "-gpu 6@xmath1 order scheme on the k20 gpu cards .",
    "the lines with different symbols presents the different particle numbers.,scaledwidth=84.0% ]    -gpu 6@xmath1 order scheme on the k20 gpu cards .",
    "the lines with different symbols presents the different particle numbers.,scaledwidth=84.0% ]    many , if not all galaxies harbor supermassive black holes  @xcite .",
    "if galaxies merge , which is quite common in the process of hierarchical structure formation in the universe , their black holes sink to the center of the merger remnant and form a tight binary  @xcite . depending on initial conditions and time supermassive black hole binaries are prominent gravitational wave sources , if they ultimately come close together and coalesce  @xcite .",
    "we model such systems as gravitating n - body systems ( stars ) with two or more massive bodies ( black holes ) , including if necessary relativistic corrections to the classical newtonian gravitational forces , and model their gravitational radiation emission directly from the simulation  @xcite .",
    "competitive astronomical and astrophysical research requires access to competitive computing facilities .",
    "theoretical numerical modeling of astrophysical objects , their composition , radiation , and dynamical evolution has become a third basic method of astrophysical research , besides observation and pure theory  @xcite .",
    "numerical modeling allows one to compare theory with observational data in unprecedented detail , and it also provides theoretical insight into physical processes at work in complex systems  @xcite .",
    "similarly , data processing of astrophysical observations comprises the use of complex software pipeline to bring raw data into a form digestible for observational astronomers and ready for exchange and publication ; these are , e.g. , mathematical transformations like fourier analysis of time series or spatial structures , complex template analysis or huge matrix - vector operations . here",
    "fast access to and transmission of data , require supercomputing capacities .",
    "we are undergoing a new revolution on parallel processor technologies , especially with regard to the graphic processing units .",
    "gpu s have become widely used nowadays to accelerate a broad range of applications , including computational physics and astrophysics , image / video processing , engineering simulations , quantum chemistry .",
    "graphics processing units ( gpu s ) are rapidly emerging as a powerful and cost - effective platform for high performance parallel computing .",
    "recent gpu s , such as the nvidia fermi c2050 and kepler k20 computing processors offer correspondingly 448 and 2496 processor cores with the extremely fast on - chip - memory , as compared to only 6 or 8 cores on a standard intel or amd cpu . in this paper",
    "we present the set of large scale n - body benchmarks of our astrophysical applications on a large kepler and fermi based gpu clusters .",
    "the application codes which we use for benchmarking here are direct n - body simulation codes for astrophysics , both using a high order hermite integration scheme and hierarchical block time steps .",
    "one is called @xmath0-gpu @xcite , it has been developed from our earlier published versions @xmath0-grape ( which used grape hardware as a accelerator @xcite ) , the other is a legacy code named nbody6++gpu @xcite , which has only recently been accelerated by gpu .",
    "both use mpi ( message passing interface ) , and on each node use many cores of the special gpu hardware to compute gravitational forces between particles .",
    "parts of the codes were developed in cooperation with keigo nitadori ( riken japan ) and tsuyoshi hamada ( nagasaki univ .",
    "@xmath0-gpu @xcite is written in c++ with mpi and cuda - c .",
    "nbody6++gpu is written in fortran with cuda - c extensions @xcite .",
    "we present benchmarks on different gpu accelerated hardware , including fermi and kepler architectures .",
    "we report results obtained with our parallel direct n - body code on the cas gpu clusters at our institute naoc / cas and at the ipe / cas .",
    "we are also planning to run benchmarks on the oak ridge titan supercomputer ( using its kepler k20 gpu s ) , but at the time of writing we can not foresee whether they will be ready for the conference .    the naoc cluster ( 85 nodes ) laohu is now , after a recent upgrade , running with the new kepler k20 gpu s ( each with 2496 cores ) , and has in total roughly @xmath2170k computing gpu cores .",
    "the ipe / cas system mole-8.5 is a much larger system with 362 nodes ( each with 6 fermi c2050 accelerators ) and has in total almost @xmath21 m ( 973k ) gpu cores . on the newest kepler k20 gpu s",
    "we reach almost 1.5 tflop / s sustained speed per one gpu using the 6@xmath1 order @xmath0-gpu code ( see * figure  [ fig : gflops ] . * and * figure  [ fig : time ] . * ) . on the fermi c2050 cards on the mole-8.5 system we get the sustained speed of around 550 gflop / s per one gpu with the same @xmath0-gpu code",
    "( see * figure  [ fig : res6 ] . * and * figure  [ fig : res6-prog ] . * ) .",
    "these numbers are roughly close to the 50% of the peak performance of such a gpu cards , which shows a very good utilization of the gpu hardware with our high order direct n - body codes .",
    "note that for the largest simulation conducted for this system used 1536 fermi c2050 accelerators each with 448 computing cores totaling almost 700k gpu cores of the mole-8.5 system .",
    "after our extensive scaling measurement on the mole-8.5 system , which we have presented in our earlier paper @xcite , we make a detail performance analysis of our @xmath0-gpu code and derive the `` semi - analytic '' scaling formula for the performance ( see also the equations ( 17 ) , ( 18 ) and ( 19 ) in the paper @xcite ) :    @xmath3    the formula ( based on the experimental data fitting ) quite well predict the code speed for different combination of particle and processor numbers ( see the * figure 4 .",
    "* in the paper @xcite ) .",
    "based on this and using the extrapolation we predict our @xmath0-gpu code performance for the much larger system ( like the oak ridge titan supercomputer system with @xmath220k gpu accelerator cards ) .    in the * figure  [ fig : res6-prog ] .",
    "* we present this extrapolation exercise .",
    "we first simply extend the scale of the n=6 m particle simulation for the larger gpu accelerator numbers up to 20k . as we can see even for this very high particle number the maximum performance which we can reach with",
    "the current gpu and network hardware is around @xmath2350 - 400 tflop / s ( see the line with n=6 m label ) .",
    "we can have a significantly better speedup factor if we were to use the roughly 4 times faster network ( see the line with lanx4 label ) . in this case",
    "we can reach above the 1 pflop / s speed .",
    "if we could simultaneously speedup the network 10 times and also speedup the gpu hardware with 100 times we can reach sub exaflop / s speed for n=300 m particles .",
    "such a very large particle number is needed to describe the full galactic bulge system , consisting of a few 10@xmath4 m@xmath5 , in star by star way .",
    "the second set of the tests were done on the laohu cluster using the newly developed gpu enabled nbody6++gpu code .",
    "this code is more complex including many binaries , stellar evolution and an ahmad - cohen neighbor scheme , and will be used for high resolution star cluster modeling .",
    "the results are shown in the plots * figure  [ fig : nb6time ] . * and * figure  [ fig : nb6pie].*. the new nbody6++gpu code also shows a very good gpu utilization performance data and quite good network communication scaling .",
    "right now , we are still working on the nbody6++gpu code optimization , so , these results should be considered as `` preliminary '' . after the next round of network communication optimization ,",
    "we hope to get a speedup factor of @xmath22 especially for larger particle and processor numbers .    as examples of the science applications done with these codes we will present new simulations for binary black holes to reach relativistic coalescence after galaxy mergers @xcite .",
    "the simulations show unexpectedly short times to reach full coalescence of the black holes , compared to the typical galaxy merging time ( less than a gyr ) . in another paper",
    "we show how the dynamics of binary black holes before the final merger gives rise to significant gravitational wave emission in the pulsar timing band , due to their high eccentricities @xcite .",
    "our massively parallel codes ( @xmath0-gpu and nbody6++gpu ) , which use mpi parallelization as well as acceleration by many gpu s , scale well on large numbers of cores .",
    "they both run very well with no sign of saturation e.g. by communication on the new kepler k20 gpu accelerator , reaching almost 1.5 tflop / s per gpu with 2496 cores .",
    "these codes are currently being used for astrophysical research on galactic nuclei , requiring large particle resolution . with realistic technical improvements of gpu hardware and network speed",
    "we expect to reach approximately 0.2 exaflop / s speed for n=300 m particles .",
    "we acknowledge support by chinese academy of sciences through the silk road project at naoc , through the chinese academy of sciences visiting professorship for senior international scientists , grant number @xmath6 ( rs ) , and through the `` qianren '' special foreign experts program of china .    the special gpu accelerated supercomputer laohu at the center of information and computing at national astronomical observatories , chinese academy of sciences , funded by ministry of finance of people s republic of china under the grant @xmath7 .",
    "we also used smaller gpu clusters titan , hydra and kepler , funded under the grants i/80041 - 043 and i/84678/84680 of the volkswagen foundation and grants 823.219 - 439/30 and /36 of the ministry of science , research and the arts of baden - wrttemberg , germany .",
    "some code development was also done on the milky way supercomputer , funded by the deutsche forschungsgemeinschaft ( dfg ) through collaborative research center ( sfb 881 ) `` the milky way system '' ( subproject z2 ) , hosted and co - funded by the jlich supercomputing center ( jsc ) .",
    "-gpu 6@xmath1 order scheme on the c2050 gpu cards .",
    "the lines with different symbols presents the different particle numbers .",
    "the dotted horizontal lines shows the 1 petaflops & 1 exaflops levels.,scaledwidth=84.0% ]      spurzem r. , berczik p. , zhong s. , nitadori k. , hamada t. , berentzen i. , veles a. : supermassive black hole binaries in high performance massively parallel direct n - body simulations on large gpu clusters .",
    "_ advances in computational astrophysics : methods , tools , and outcome .",
    "astronomical society of the pacific conference series _ , edited by r. capuzzo - dolcetta , m. limongi , and a. tornambe , 453 : 223 - 233 , 2012 .",
    "berczik p. , nitadori k. , zhong s. , spurzem r. , hamada t. , wang x. , berentzen i. , veles a. , ge w. : high performance massively parallel direct n - body simulations on large gpu clusters .",
    "_ international conference on high performance computing _ , kyiv , ukraine , october 8 - 10 , 2011 , p. 8",
    "+ http://www.hpc-ua.org/sites/default/files/proceedings-2011/1.1(8).pdf harfst s. , gualandris a. , merritt d. , spurzem r. , portegies zwart s. , berczik p. : performance analysis of direct n - body algorithms on special - purpose supercomputers . _ new astronomy _",
    ", 12 : 357 - 377 , 2012 .",
    ", holley - bockelmann k. , berczik p. , just a. : supermassive black hole binary evolution in axisymmetric galaxies : the final parsec problem is not a problem .",
    "_ the astrophysical journal _ , 773 : 100 ( 6 pp ) , 2013 .",
    ", berentzen i. , berczik p. , just a. , mayer l. , nitadori k. , callegari s. : formation and hardening of supermassive black hole binaries in minor mergers of disk galaxies .",
    "_ the astrophysical journal _ , 756 : 30 ( 10 pp ) , 2012 .",
    ", preto m. , berczik p. , berentzen i. , just a. , spurzem r. : mergers of unequal - mass galaxies : supermassive black hole binary evolution and structure of merger remnants .",
    "_ the astrophysical journal _ , 749 : 147 ( 14 pp ) , 2012 .",
    "preto m. , berentzen i. , berczik p. , spurzem r. : fast coalescence of massive black hole binaries from mergers of galactic nuclei : implications for low - frequency gravitational - wave astrophysics .",
    "_ the astrophysical journal letters _ , 732 : l26 ( 6 pp ) , 2011 .",
    "preto m .. berentzen i. , berczik p. , merritt d. , spurzem r. : merger of massive black holes using n - body simulations with post - newtonian corrections .",
    "_ journal of physics : conference series _ , 154 : 012049 ( 6 pp ) , 2009 .",
    "just a. , yurin d. , makukov m. , berczik p. , omarov c. , spurzem r. , vilkoviskij e.y . : enhanced accretion rates of stars on supermassive black holes by star - disk interactions in galactic nuclei . _ the astrophysical journal _ , 758 : 51 ( 10 pp ) , 2012 ."
  ],
  "abstract_text": [
    "<S> we present direct astrophysical n - body simulations with up to a few million bodies using our parallel mpi / cuda code on large gpu clusters in china , ukraine and germany , with different kinds of gpu hardware . </S>",
    "<S> these clusters are directly linked under the chinese academy of sciences special gpu cluster program in the cooperation of iccs ( international center for computational science ) . </S>",
    "<S> we reach about the half the peak kepler k20 gpu performance for our @xmath0-gpu code  @xcite , in a real application scenario with individual hierarchically block time - steps with the high ( 4@xmath1 , 6@xmath1 and 8@xmath1 ) order hermite integration schemes and a real core - halo density structure of the modeled stellar systems . </S>",
    "<S> the code and hardware are mainly used to simulate star clusters  @xcite and galactic nuclei with supermassive black holes  @xcite , in which correlations between distant particles can not be neglected .    </S>",
    "<S> n - body simulation , parallel computing , many core , gpu acceleration , star clusters , galactic nuclei , black hole physics , astrophysics . </S>"
  ]
}