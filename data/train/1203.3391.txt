{
  "article_text": [
    "for successful experimental implementation of any quantum protocol , the quantum states and operations involved must be confirmed to be sufficiently closed to their theoretical targets .",
    "one way to obtain such a confirmation is to perform another experiment and from the obtained data make an estimate of the quantum operator involved .",
    "statistically , this is a constrained multi - parameter estimation problem  the quantum estimation problem  where we assume we are given a finite number of identical copies of a quantum state or operation , we perform measurements whose mathematical description is assumed to be known , and from the outcome statistics we make our estimate .",
    "due to the probabilistic behavior of the measurement outcomes and the finiteness of the number of measurement trials , there always exist statistical errors in any quantum estimate .",
    "the size of the error depends on the choice of measurements and the estimation procedure . in statistics , the former",
    "is called an _ experimental design _ , while the latter is called an _",
    "estimator_. it is , therefore , a key aim of both classical and quantum estimation theory to find a combination of experimental design and estimator which gives us more precise estimation results using fewer measurement trials .",
    "a standard combination in quantum information experiments is that of quantum tomography and maximum likelihood estimator .",
    "although the term `` quantum tomography '' can be used in several different contexts , we use it to mean an experimental design in which an independently and identically prepared set of measurements are used throughout the entire experiment @xcite .",
    "the performance of different choices for the set of tomographic measurements have been studied , in , for example , @xcite .",
    "this of course raises the question of the performance of _ adaptive _ experimental designs , in which the measurements performed from trial to trial are not independent , and are chosen according to previous measurement settings and the outcomes obtained . clearly , adaptive experimental designs are a superset of the nonadaptive ones , and as such can potentially achieve higher performance .",
    "adaptive designs are characterized by the way in which measurements are related from trial to trial , referred to as an _ update criterion_. previously proposed update criteria include those based on asymptotic statistical estimation theory ( fisher information ) @xcite , direct calculations of the estimates expected to be obtained in the next measurement @xcite , mutually unbiased basis @xcite , as well as bayesian estimators and shannon entropy @xcite .",
    "theoretical investigations report that some of the proposed update criteria give more precise estimates than nonadaptive quantum tomography , and an experimental implementation of the update criterion proposed in @xcite in an ion trap system has been performed @xcite .",
    "if @xmath0 denotes the number of measurement trials and @xmath0 is sufficiently large , it is known in 1-qubit state estimation that the expectation value of infidelity averaged over states , a measure of the estimation error , can decrease at best as @xmath1 in a nonadaptive experiment @xcite , compared to @xmath2 in adaptive experiments @xcite .",
    "most of the proposed update criteria , however , have high computational cost that makes real experiments infeasible . in this paper",
    ", we propose an adaptive experimental design whose average expected infidelity decreases as @xmath2 and whose update criterion , known as average - variance optimality ( a - optimality ) in classical statistics , has low computational cost for 1-qubit state estimation .",
    "the paper is structured as follows . in sec .",
    "[ sec : preliminaries ] we lay out the notation and terminology that will be used throughout in this paper , by explaining basic concepts in adaptive experimental design , statistical parameter estimation , and a - optimality criteria .",
    "we also give a brief review of some of the proposed update criteria in the literature . in sec .",
    "[ section : result ] we give the explicit form of the analytic solution of the a - optimal update criterion , ( the derivation is given in the appendix ) .",
    "this analytic solution makes it possible to reduce the computational cost for updating measurements , and using this we compare several estimation schemes numerically , showing that our proposal is more precise than standard quantum tomography . in sec .",
    "[ section : discussion ] , we discuss the feasibility of implementing the proposed scheme experimentally .",
    "a summary appears in sec . [",
    "section : summary ] .",
    "we will adopt terms from the statistical literature , since they afford us the precision we need to properly discuss details of estimation schemes that can sometimes be subtle . in this subsection",
    "we will introduce a formalism for quantum estimation using that terminology , and apply it in a survey of several existing update criteria in sec .",
    "[ subsec : survey ] .      in statistical estimation theory ,",
    "a statistical model is defined as a set of probability distributions , and we assume that the true probability distribution of interest is included in the set . in the quantum case ,",
    "a probability distribution is determined by the state of the system and the action of the measurement on the state system .",
    "let @xmath3 be a hilbert space with finite dimension @xmath4 and @xmath5 be the set of all density matrices acting on that hilbert space .",
    "suppose we know that the object we are trying to estimate lies in a subset @xmath6 , that is , the true density matrix @xmath7 is included in @xmath8 .",
    "for example , when we know that the true state is pure , @xmath8 is the set of all pure states . in this paper , we consider mixed state estimation , and we assume that in our finite @xmath0 measurement trials we prepare identical copies of an unknown state @xmath9 .",
    "a probability distribution of outcomes in quantum measurement requires not only a density matrix , but also a positive operator valued measure ( povm ) , @xmath10 , where @xmath11 is the set of outcomes . when the measurement is characterized by a povm @xmath12 and the measured quantum state is characterized by a density matrix @xmath7 , the probability distribution of the outcomes is given by born s rule @xmath13 $ ] , where @xmath14 denotes the trace operation with respect to @xmath3 , ( note that in the next subsection , a different trace operation represented as @xmath15 , is introduced ) .",
    "we consider sequential measurements , as opposed to collective measurements , on copies of @xmath7 .",
    "we will index measurement trials using subscripts @xmath16 , and sequences using superscripts .",
    "thus , for some symbol @xmath17 , @xmath18 is its value taken at the @xmath19-th trial , while @xmath20 is the sequence @xmath21 .",
    "we will also try to use calligraphic fonts for supersets .",
    "adaptivity in our sense means that the povm performed at @xmath22-th trial can depend on all the previous @xmath19 trials outcomes and povms .",
    "the measurement class @xmath23 is the set of povms which are available at the @xmath19-th trial .",
    "we choose the @xmath19-th povm , @xmath24 from @xmath23 , where @xmath25 denotes the set of measurement outcomes for the @xmath19-th trial . when it is independent of the trial , as is usually the case , we omit the index , using @xmath26 for the measurement class and @xmath11 for the outcome set .",
    "let @xmath27 denote the sequence of outcomes obtained up to the @xmath19-th trial , where @xmath28 .",
    "we will denote the pair of measurement performed and outcome obtained by @xmath29 , and refer to it as the data for trial @xmath19 .",
    "the sequence of data up to trial @xmath19 is thus @xmath30 . after the @xmath19-th measurement",
    ", we choose the next , @xmath22-th , povm @xmath31 according to the previously obtained data .",
    "let @xmath32 denote the map from the data to the next measurement , that is , @xmath33 , @xmath34 .",
    "we call @xmath32 the measurement update criterion for the @xmath19-th trial and @xmath35 the measurement update rule . note that @xmath36 is a map from @xmath37 to @xmath38 and corresponds to the choice of the first measurement .",
    "an estimator @xmath39 is a set of maps from the data to the model space , @xmath40 so that @xmath41 . the estimated density matrix @xmath42 is called the @xmath19-th estimate",
    ". we will often omit the data dependency . in this paper",
    "we use a maximum likelihood estimator @xmath43 defined as @xmath44 where @xmath45 .",
    "\\end{aligned}\\ ] ]    a quintuplet @xmath46 specifies an estimation scheme .",
    "a sketch of the procedure for a generic adaptive quantum estimation scheme is given in fig .",
    "[ fig : sketch ] .      in order to evaluate the precision of estimates of the true density matrix",
    ", we introduce a _ loss function _",
    "( sometimes called a cost function ) .",
    "a loss function @xmath47 is a map from @xmath48 to @xmath49 such that ( i ) @xmath50 and ( ii ) @xmath51 .",
    "for example , the trace - distance and the infidelity ( one minus the fidelity ) are loss functions for density matrices . the outcomes of quantum measurements are random variables , and the value of the loss function between an estimate and the true density matrix is also a random variable .",
    "thus , in order to evaluate the precision of the estimator ( not the estimate ) for the true density matrix , we use the statistical expectation value of the loss function , called an _ expected loss _",
    "( sometimes called a risk function ) @xcite .",
    "the explicit form is given by @xmath52 the value of the expected loss depends on the choice of the estimator as well as the true density matrix .",
    "the latter is of course unknown in an experiment , and there are at least two approaches to eliminate its dependence , namely the average and the maximal ( or worst case ) expected loss , given explicitly by @xmath53 where @xmath54 is a probability measure on @xmath8 .",
    "the task in this paper is to find a combination of a measurement update rule @xmath55 and estimator @xmath56 with average expected loss as small as possible .",
    "the a - optimality criterion is a measurement update criterion based on the asymptotic theory of statistical parameter estimation @xcite . in this subsection",
    "we introduce a few basic results of the asymptotic theory .",
    "first let us parametrize the state space @xmath5 .",
    "any density matrix on @xmath4-dimensional hilbert space can be parametrized by @xmath57 real numbers , @xmath58 , i.e. @xmath59 . in the @xmath60 case , we take @xmath61 , where @xmath62 , @xmath63 are the pauli matrices , and @xmath64 , is called the bloch vector .",
    "the estimation of @xmath7 is equivalent to the estimation of @xmath65 , and we let @xmath66 denote the estimator .",
    "estimates of a density matrix and of a bloch vector are related as @xmath67 .    for any estimator @xmath66 , any number of measurement trials @xmath0 , and any positive semidefinite matrix @xmath68 , the inequality @xmath69^t h(\\bm{s } ) [ \\bm{s}^{\\mathrm{est}}_{n}(d^n ) - \\bm{s } ] \\notag \\\\",
    "\\ge \\trace [ h(\\bm{s } ) g_{n}(u^{n } , \\bm{s}^{\\mathrm{est } } , \\bm{s})^{t } f_{n}(u^{n } , \\bm{s})^{-1 } g_{n}(u^{n } , \\bm{s}^{\\mathrm{est } } , \\bm{s } ) ] \\label{eq : gcrineq }    \\end{aligned}\\ ] ] holds , where @xmath70 and @xmath15 denotes the trace operation with respect to the parameter space .",
    "eq.([eq : gcrineq ] ) is a known generalization of the cramr - rao inequality @xcite , and we give a simple proof in appendix [ sec : proofcrineq ] .",
    "@xmath71 is a @xmath72 positive semidefinite matrix called the fisher matrix of the probability distribution @xmath73 .",
    "if the estimate converges to the true parameter , i.e. , @xmath74 as @xmath75 with probability 1 , the lhs of eq.([eq : gcrineq ] ) converges to 0 and therefore the rhs should converge to 0 . in this case , if we assume the exchangeability of the limit and derivative , the matrix @xmath76 converges to the identity matrix @xmath77 , and the quantity @xmath78 defined as @xmath79     \\end{aligned}\\ ] ] converges to @xmath80 .",
    "this @xmath78 can be interpreted as a lower bound of the weighted ( by @xmath68 ) mean squared error when @xmath0 is sufficiently large .",
    "it is known that under certain regularity conditions , a maximum likelihood estimator achieves the equality of eq.([eq : gcrineq ] ) asymptotically .",
    "for a given @xmath65 , it would be wise to choose a measurement update rule which makes the value of @xmath78 as small as possible .",
    "this is the guiding principle of the a - optimality criterion .",
    "we move on to the explanation of the procedure of a - optimality .",
    "the  a \" stands for  average - variance \" @xcite . according to the asymptotic theory of statistical parameter estimation described in the previous subsection , we wish to minimize the value of @xmath78 .",
    "suppose that we perform @xmath19 trials and obtained the data sequence @xmath81 .",
    "we would like to choose the povm minimizing @xmath82 in @xmath83 as the next , @xmath22-th , measurement .",
    "when we consider minimizing this function , there are two problems . in order to avoid them ,",
    "we introduce two approximations .",
    "the first problem is that the minimized function depends on the true parameter @xmath65 .",
    "of course the true parameter is unknown in parameter estimation problems , and we must use an estimate in the update criterion , @xmath84 , instead . the mesurement update estimator @xmath85 is not necessarily the same as @xmath66 .",
    "the second problem is that unlike the independent and identically distributed ( i.i.d . )",
    "measurement case , calculation of the fisher matrix in the adaptive case requires summing over an exponential amount of data , and is computationally intensive . to avoid this problem , we approximate the sum over all possible measurements by that over only those measurements that have been performed : @xmath86 where @xmath87 the matrix @xmath88 is the fisher matrix for the @xmath89-th measurement probability distribution @xmath90 , and @xmath91 is the sum of the fisher matrices from the first to the ( @xmath92)-th trial . instead of minimizing @xmath93",
    ", we consider the minimization of @xmath94 .",
    "\\end{aligned}\\ ] ] it is known that the convergence of @xmath95 to @xmath80 is part of a sufficient condition for the convergence of a maximum likelihood estimator @xcite , and this justifies the use of this second approximation .",
    "we explain the relationship between the conditional and unconditional fisher matrices with respect to the estimator s convergence in appendix [ sec : conditionalfishermatrices ] . after making these two approximations",
    ", we define the a - optimality criterion as @xmath96.\\label{eq : defaopt }     \\end{aligned}\\ ] ] finding @xmath97 is a nonlinear minimization problem with high computational cost in general . in this paper",
    ", we derive the analytic solution of eq .",
    "( [ eq : defaopt ] ) in the 1-qubit case , reducing the computational cost significantly .",
    "we consider a 1-qubit mixed state estimation problem , so that @xmath98 .",
    "we identify the bloch parameter space @xmath99 with @xmath8 , where we restrict the true state space to be strictly the interior in order to avoid the possible divergence of the fisher matrix .",
    "suppose that we can choose any rank-1 projective measurement in each trial .",
    "let @xmath100 denote the povm corresponding to the projective measurement onto the @xmath101-axis @xmath102 , whose elements can be represented as @xmath103 this is the bloch parametrization of projective measurements .",
    "we identify the set of parameters @xmath104 with the measurement class @xmath105 .    for our loss functions ,",
    "we use both the squared hilbert - schmidt distance @xmath106 and the infidelity",
    "@xmath107 @xcite : @xmath108 \\\\        & = \\frac{1}{4 } ( \\bm{s } - \\bm{s}^{\\prime } ) ^2 , \\label{eq : hsdistance2_para}\\\\       \\delta^{\\mathrm{if } } ( \\bm{s } , \\bm{s}^{\\prime } )        : & = 1 - \\trace \\bigl [ \\sqrt { \\sqrt{\\rho ( \\bm{s } ) } \\rho ( \\bm{s}^{\\prime } ) \\sqrt{\\rho ( \\bm{s } ) } } \\bigr]^2 \\\\        & = \\frac{1}{2}\\bigl ( 1 - \\bm{s}\\cdot \\bm{s}^{\\prime } - \\sqrt{1-\\|\\bm{s}\\|^2}\\sqrt{1-\\|\\bm{s}^{\\prime}\\|^2 }   \\bigr).\\label{eq : infidelity_para }    \\end{aligned}\\ ] ] we note that the hilbert - schmidt distance coincides with the trace distance in a 1-qubit system .",
    "the asymptotic behavior of the average expected fidelity @xmath109 is known in the 1-qubit state estimation case @xcite .",
    "the measure used for calculating this average is the bures distribution , @xmath110 .",
    "if we limit our available measurements to be sequential and independent ( i.e. , nonadaptive ) , @xmath109 behaves at best as @xmath1 @xcite . on the other hand ,",
    "if we are allowed to use adaptive , separable , or collective measurements , @xmath111 can behave as @xmath2 @xcite . in @xcite ,",
    "the coefficient of the dominant term in the asymptotic limit is also derived .    in sec .",
    "[ subsubsec : averagedmeanlosses ] , we show numerical results",
    ". a maximum likelihood estimator is used , and it is shown that the average expected infidelity of an a - optimal scheme behaves as @xmath2 , illustrating that the a - optimality criterion is indeed making use of adaptation to outperform nonadaptive schemes .",
    "we briefly review some of the other adaptive measurement update criteria proposed in the literature , using our terminology and notation introduced in the previous subsections .      before explaining update criteria that are performed at each and every trial , such as a - optimality",
    ", we briefly review a simpler update criterion .",
    "the two - step adaptation criterion requires the measurement update only once during a measurement sequence .",
    "we have @xmath112 thus , for all trials up to and including trial @xmath113 a fixed povm @xmath114 is performed , and an estimate is calculated from the obtained data . using that data we choose a new povm @xmath115 for the remaining @xmath116 copies . in @xcite ,",
    "two - step adaptation criteria are used to prove mathematically an asymptotic bound for weighted mean squared errors in 1-qubit state estimation . in @xcite ,",
    "some numerical results are shown for a few two - step adaptation schemes .      in @xcite , an update criterion based on the cramr - rao inequality",
    "is proposed .",
    "the update criterion is given by @xmath117.\\label{eq : nagaokaupdate }      \\end{aligned}\\ ] ] the difference from the a - optimality criterion is that in eq .",
    "( [ eq : nagaokaupdate ] ) the fisher information matrix used in the update does not take into account all @xmath92 measurements , but about only the @xmath22-th measurement .",
    "the advantage of course is that this reduces the computational cost of updates .",
    "the disadvantage is that when @xmath118 consists of informationally incomplete povms , as is the case in most experiments , the estimates can not converge to the true state . as explained in sec .",
    "[ subsec : estimationsetting ] , in this paper @xmath23 is restricted to rank-1 projective measurements , and in this setting eq .",
    "( [ eq : nagaokaupdate ] ) does not work well .      in @xcite ,",
    "two update criteria are proposed .",
    "a.   the first criterion is based on the shannon entropy of the estimated measurement probability distribution , and is given by @xmath119 b.   the second criterion uses a third state estimator @xmath120 such that @xmath121    numerical simulation is performed for the case where @xmath8 is the set of 1-qubit pure states and @xmath122 is the set of projective measurements , while @xmath123 is a biased maximum likelihood estimator , @xmath124 is a bayesian estimator up to @xmath125 .",
    "average ( not expected ) infidelity is used as the evaluation function .      in @xcite , an update criterion given by @xmath126",
    "is proposed .",
    "a numerical simulation is performed in @xcite , where the setting is that @xmath8 is the set of 1-qubit pure states , @xmath26 is a set of parity measurements using an ancilla system , and @xmath123 and @xmath124 are maximum likelihood estimators .",
    "the behavior of the average expected fidelity is numerically analyzed up to @xmath127 .",
    "an update criterion proposed in @xcite is given by @xmath128 \\ln \\trace [ \\pi_{i , x_{i } } \\pi_{x } ]             \\bigr ) ,       \\end{aligned}\\ ] ] and the estimator is defined as @xmath129,\\\\          \\bar{\\rho}(d^n ) = \\frac{1}{n}\\sum_{i=1}^{n}\\pi_{i , x_{i}}.       \\end{aligned}\\ ] ] in the numerical simulations , the estimation setting is such that @xmath8 is the set of pure states on @xmath4-dimensional hilbert space @xmath3 , and @xmath23 is the set of projective measurements on @xmath3 .",
    "numerical simulations of average expected fidelity are shown for @xmath130 and @xmath131 , all up to @xmath132 .      in @xcite , an update criterion based on bayesian",
    "estimation and shannon entropy is proposed .",
    "let @xmath133 denote a prior distribution on @xmath8 .",
    "the update criterion is @xmath134 where @xmath135 in @xcite , the case in which @xmath8 is the set of 1-qubit mixed states and @xmath26 is the set of projective measurements is numerically analyzed up to @xmath132 .",
    "the evaluation function used is the average ( not expected ) infidelity .      in @xcite , an update criterion given by @xmath136",
    "is proposed , where eqs .",
    "( [ eq : pave ] ) and ( [ eq : p ] ) have been used . from a simple calculation",
    ", we can see that the criteria defined in eq .",
    "( [ eq : ff00 ] ) and in eq .",
    "( [ eq : huszar ] ) are equivalent .",
    "this criterion involves an integration which requires high computational cost . in @xcite , a special technique for calculating the integral ,",
    "called a sequential importance sampling method , is used in order to reduce that computational cost .",
    "the authors performed numerical simulation for the case in which @xmath8 is the set of 1-qubit mixed states and @xmath122 are projective measurements up to @xmath137 .",
    "they also considered the case in which @xmath8 is the set of 2-qubit states and @xmath26 are a set of mutually unbiased bases , a set of pairwise pauli measurements , and a set of separable measurements up to @xmath138 .",
    "the evaluation function is the average expected infidelity , and it is shown that their scheme is more precise than standard quantum tomography . in sec .",
    "[ subsubsec : averagedmeanlosses ] , we point out that our numerical results for 1-qubit show that a - optimality gives even more precise estimates than those given by eq .",
    "( [ eq : huszar ] ) , at least from @xmath139 to @xmath140 .",
    "as explained in sec . [ subsec : estimationsetting ] , we consider the a - optimality criterion for 1-qubit state estimation using projective measurements . in sec .",
    "[ subsec : analyticresults ] we give the analytic solution , and in sec .",
    "[ subsec : numericalsimulations ] we show the results of numerical simulations .",
    "first , we give the explicit form of the fisher matrix for projective measurements . the probability distribution for the rank-1 projective measurement @xmath141",
    "is given by @xmath142 and the fisher matrix is @xmath143 in this case , eq .",
    "( [ eq : defaopt ] ) is rewritten in the bloch vector representation as @xmath144 .",
    "\\label{eq : aopt1qubit }   \\end{aligned}\\ ] ] we present the analytic solution of eq.([eq : aopt1qubit ] ) in the form of the following theorem .",
    "[ theorem:1 ] given a sequence of data @xmath145 , the @xmath19-th estimate @xmath146 , and a real positive matrix @xmath147 , the a - optimal povm bloch vector is given by @xmath148 where @xmath149 @xmath150 is the eigenvector of the matrix @xmath151 corresponding to the minimal eigenvalue , and @xmath77 is the identity in the parameter space .",
    "we give the proof of theorem [ theorem:1 ] in appendix [ sec : appendixa ] .    in eq .",
    "( [ eq : aopttheorem1 - 3 ] ) , the inverse of the matrix @xmath152 appears . in the proof of theorem [ theorem:1 ] ,",
    "the invertibility of @xmath152 is assumed .",
    "the invertibility of @xmath153 is equivalent to the condition that @xmath154 is a basis of @xmath155 .",
    "when we choose the second and third measurements , @xmath156 and @xmath157 are not invertible .",
    "thus the update scheme does not apply to these steps , and the choices are arbitrary .",
    "one simple choice is to perform @xmath158- , @xmath159- , and @xmath160-projective measurements at the first , second and third trials respectively , and this can be shown to satisfy theorem [ theorem:1 ] as follows .",
    "the choice of the first measurement is always arbitrary , and we choose @xmath161 , a @xmath158-projective measurement .",
    "then for any true bloch vector @xmath65 the rank of @xmath156 is @xmath162 , and if we interpret the inverse matrix in eq .",
    "( [ eq : aopttheorem1 - 3 ] ) as a generalized inverse matrix , @xmath163 is a rank @xmath162 matrix with minimal eigenvalues @xmath80 .",
    "the supports of @xmath156 , @xmath164 , and @xmath163 are the span of @xmath165 .",
    "therefore @xmath166 is an arbitrary vector in the @xmath167-dimensional space spanned by @xmath168 and @xmath169 , and we choose @xmath170 . then using the same logic , the third measurement is fixed to @xmath171 .    from the explicit formulae of the squared hilbert - schmidt distance and infidelity in eqs .",
    "( [ eq : hsdistance2_para ] ) and ( [ eq : infidelity_para ] ) , we have @xmath172 therefore when we use the hilbert - schmidt distance as our loss function , we substitute @xmath173 and @xmath174 into eqs.([eq : aopttheorem1 - 1 ] ) , ( [ eq : aopttheorem1 - 2 ] ) , and ( [ eq : aopttheorem1 - 3 ] ) to obtain @xmath175 and we do not need to explicitly calculate the inverse or square root matrices for a - optimality . on the other hand , when our loss function is the infidelity , we must use @xmath176 and @xmath177 .",
    "we performed monte carlo simulations of the following four experimental designs described in detail below ; a - optimal adaptive scheme for the squared hilbert - schmidt distance , the same for infidelity , xyz repetition , and uniformly random selection .",
    "a - optimality for the squared hilbert - schmidt distance is the adaptive scheme defined by eq.([eq : aopt1qubit ] ) with @xmath178 .",
    "similarly , a - optimality for the infidelity is that with @xmath179 . as explained in the previous subsection , the choice of measurement bloch vectors at the first and second trials is arbitrary",
    "; we choose @xmath161 and @xmath180 , i.e. , at the first trial we perform the projective measurement of @xmath158 , and that of @xmath159 at the second  the third trial is automatically the projective measurement of @xmath160 , corresponding to @xmath171 .",
    "the xyz repetition scheme is nonadaptive , in which we repeat the measurements of @xmath181 , @xmath182 , and @xmath183 , corresponding to standard quantum state tomography .",
    "uniformly random selection is also nonadaptive , where at each trial we choose the next measurement direction randomly on the bloch surface , according to the so(3 ) haar measure . for consistency with the other three schemes , we fix the first , second and third measurements to be the projective measurements of @xmath184 , respectively , and randomly select directions from the fourth trial on .",
    "we choose a maximum likelihood estimator in all four experimental designs .",
    "it is known that the estimators minimizing @xmath185 and @xmath186 are bayesian estimators @xcite , but the integrations necessary for bayesian estimation take too much computation time .",
    "for the two a - optimality criteria , we choose both the real and the dummy estimators to be maximum likelihood , @xmath187 .",
    "we used a newton - raphson method to solve the ( log-)likelihood equation and the completely mixed state @xmath188 as the initial point of the iterative method .",
    "when a search point came out of the bloch sphere during the procedure , we chose the previous point ( included in the sphere ) as the estimate .    in the following subsections",
    ", we show the plots for two loss functions ; the squared hilbert - schmidt distance @xmath106 and infidelity @xmath107 .",
    "the average expected losses @xmath189 are shown in sec .",
    "[ subsubsec : averagedmeanlosses ] , and pointwise expected losses @xmath190 are shown in sec .",
    "[ susubsec : pointwisemeanlosses ] . in the both subsections ,",
    "the line styles are fixed as follows : solid ( black ) line for a - optimality for the squared hilbert - schmidt distance ( ahs ) , dashed ( red ) line for a - optimality for the infidelity ( aif ) , chain ( blue ) line for xyz repetition ( xyz ) , dotted ( green ) line for uniformly random selection ( urs ) .",
    "we analyse the average behaviour of the estimation errors over the bloch sphere . the integration for averaging is approximated by a monte carlo routine , and the statistical expectation is approximated by an arithmetric mean using pseudo - random numbers .",
    "figure [ fig : averagedexpectedlosses ] shows the average expected loss functions @xmath189 against the number of trials @xmath0 ( the horizontal and vertical axes are both logarithmic scale ) : ( hs - bures ) @xmath191 integrated via the bures distribution @xmath192 , ( hs - euclid ) @xmath191 integrated via the euclidean distribution @xmath193 , ( if - bures ) @xmath109 integrated via @xmath192 , and ( if - euclid ) @xmath109 integrated via @xmath194 .",
    "[ fig : averagedexpectedlosses ] ( hs - bures ) and ( hs - euclid ) shows that the estimation errors of the four experimental designs are almost equivalent from the viewpoint of the squared hilbert - schmidt distance . as depicted in ( hs - bures ) , the estimation errors of the two a - optimality schemes are slightly larger than the other nonadaptive schemes ; as we show in the next subsection ( pointwise analysis ) , this gap decreases as @xmath0 becomes larger .",
    "on the other hand , fig .",
    "[ fig : averagedexpectedlosses ] ( if - bures ) and ( if - euclid ) show the explicit gap between the adaptive and nonadaptive schemes .",
    "the gradients of the curves begin to differentiate from around @xmath139 , and as depicted in ( if - bures ) , the gradients of xyz and urs are almost @xmath195 around @xmath196 .",
    "this means that the average expected infidelity behaves as @xmath1 and is consistent with the result of the asymptotic analysis presented in @xcite . on the other hand ,",
    "the gradients of ahs and aif are greater than the nonadaptive limit @xmath195 , indicating that ahs and aif make good use of adaptive resources . around @xmath196",
    "the gradient of aif is almost @xmath197 , which is the bound for adaptive experimental designs @xcite .",
    "let us compare the estimation errors of a - optimality and the hh11 criteria explained in sec .",
    "[ subsubsec : ff00criterion ] . from fig .",
    "[ fig : averagedexpectedlosses ] ( if - bures ) , the average expected infidelity of ahs and aif are @xmath198 and @xmath199 at @xmath196 . on the other hand ,",
    "the corresponding amount for the hh11 criterion can be estimated roughly from fig .",
    "2 ( a ) in @xcite to be @xmath200 .",
    "this implies that for 1-qubit state estimation , the average expected infidelity of the a - optimality criterion is about two - times smaller than that of eq .",
    "( [ eq : huszar ] ) , at least around @xmath196 .",
    "next , we analyse the behaviour of the estimation errors at several true bloch vectors @xmath65 .",
    "figure [ fig : pointwisemeanlosses ] shows the pointwise expected loss functions @xmath201 against the number of trials @xmath0 ( the horizontal and vertical axes are both logarithmic scale ) : ( hs - p1 ) , ( hs - p2 ) , and ( hs - p3 ) are plots of the expected squared hilbert - schmidt distances for @xmath65 given by @xmath202 , and ( if - p1 ) , ( if - p2 ) , and ( if - p3 ) are the expected infidelities for the same three true states , respectively .    as depicted in ( hs - p1 ) and ( if - p1 ) , the estimation errors of all four schemes are almost equivalent for the completely mixed state , @xmath188 .",
    "as the bloch radius @xmath203 becomes larger , the differences between the four schemes become clearer .",
    "figure [ fig : pointwisemeanlosses ] ( hs - p2 ) and ( hs - p3 ) are the plots of the expected squared hilbert - schmidt distances at a high purity point , @xmath204 . in the region of @xmath205 to around @xmath206 ,",
    "the squared hilbert - schmidt error of the two adaptive schemes is larger than that of the two nonadaptive schemes .",
    "in particular , the error of ahs is larger that that of aif ; this might seem strange , but in the region of @xmath207 , the error of ahs becomes smaller than that of aif , indeed it eventually becomes the smallest of the four schemes .",
    "we believe that there are two reasons for a - optimality s large error for small @xmath0 .",
    "first , the a - optimality criterion is based on an asymptotic theory of statistical estimation .",
    "when the number of measurement trials @xmath0 is small , the cramr - rao bound is not necessary suitable for characterizing estimation errors .",
    "second , it uses a dummy estimator in the measurement update .",
    "when @xmath0 is small , @xmath208 is not a good estimate , and thus the choice of the next measurements can be unreliable .",
    "of course , when @xmath0 becomes sufficiently large , both of these problems are alleviated .",
    "the gap between the estimation errors of adaptive and nonadaptive schemes becomes smaller as @xmath0 becomes larger in ( hs - p2 ) and ( hs - p3 ) , while it grows in ( if - p2 ) and ( if - p3 ) .",
    "only the xyz scheme changes dramatically between ( if - p2 ) and ( if - p3 ) ; the other three schemes do not because ahs , aif , and urs are invariant under rotation of the true bloch vector ( for very small @xmath0 , there are differences , and these are because the first three measurements are fixed to @xmath209-projective measurements and not rotationally invariant ) . figure [ fig : pointwisemeanlosses ] ( if - p2 ) is the case in which the directions of the measurement and the true bloch vector are matched ( to @xmath210 ) . in this case , xyz is the best scheme , exhibiting the smallest estimation error . around @xmath211 ,",
    "the estimation error of aif becomes as small as that of xyz .",
    "that of ahs is smaller than urs , but larger than the other two schemes .",
    "we believe that this is because the selected hessian matrix @xmath212 used in the update routine is unsuitable for the loss function @xmath107 in ( if - p2 ) ( and ( if - p3 ) ) .",
    "figure [ fig : pointwisemeanlosses ] ( if - p3 ) is the case in which the directions of the measurement and the true bloch vector are the most discrepant ( for a fixed purity ) . in this case , the estimation errors of xyz and urs are almost the same and behave as @xmath213 , and those of the adaptive schemes are smaller than those of the nonadaptive ones , ( this behavior of expected infidelity for i.i.d .",
    "measurements is discussed in @xcite , and a detailed analysis will appear in @xcite ) .",
    "when we consider the whole bloch sphere , of course the cases in which the direction of xyz measurements and the bloch vector are matched are few , and therefore the average expected infidelities of ahs and aif are smaller than those of xyz and urs . this also indicates that the adaptive schemes have better worst - case performance ( lower @xmath214 , eq . ( [ eq : maximalloss ] ) ) than the nonadaptive schemes .",
    "figure [ fig : results_purity ] shows the purity dependence of the average expected infidelity at @xmath196 .",
    "the average is taken over all directions @xmath215 and @xmath216 for each bloch radius @xmath203 .",
    "it indicates that the average expected infidelities of the two adaptive schemes are smaller than those of the two nonadaptive schemes .",
    "the appearance of peaks for xyz and urs is discussed in appendix  [ sec : purityxyzurs ] .",
    "figure [ fig : measurementanalysis ] is a plot of the measurement bloch vectors at @xmath139 ( left column ) , @xmath140 ( middle column ) , and @xmath217 ( right column ) for @xmath218 runs .",
    "the true state is @xmath219 , and the upper three subplots are ahs while the lower three are aif .",
    "figure [ fig : measurementanalysis ] shows that the measurement bloch vectors are clustered around the true state , with some interesting behaviour at @xmath211 . in ( ahs-10000 ) , the measurement directions are clustered very narrowly at the true state and also around the great circle that it defines . in ( aif-10000 ) , on the other hand , the directions are clustered widely around the true state .",
    "this is due to the difference between the loss functions employed in the update routine , namely squared hilbert - schmidt distance in the former and infidelity in the latter .",
    "we mention that for a completely mixed true state , the measurement bloch vectors are distributed randomly on the bloch sphere for large @xmath0 .",
    "there are two main issues when considering the practical implementation of an adaptive scheme , namely the ease with which measurement updates can be made in the apparatus , and the time required to compute those updates . in quantum optics , projective measurements and single qubit rotations",
    "are standard tools in quantum information processing experiments .",
    "figure [ fig : polarizationimplimentation ] illustrates a simple implementation example for a one photon polarization system . in this regard , the first issue is not a problem  in general , of course it will depend on the experimental state of the art .      in order to compare the performance of the a - optimality criterion to the other update schemes",
    ", we have considered 1-qubit states as the estimation objective .",
    "current and future quantum information processing is concerned with higher dimensional estimation objectives , not only states but also processes . in 1-qubit state estimation",
    ", we can reduce the computational cost for a - optimality by using the analytic solution of theorem [ theorem:1 ] , but as we see in appendix [ sec : appendixa ] , the techniques used to derive that solution depend on the properties of 1-qubit states and projective measurements .",
    "a - optimality in higher dimensional systems will need a new solution , or must deal with the increasing complexity of the nonlinear minimization problem .",
    "one possible approach is to place constraints on the measurement class @xmath23 . instead of considering a continuous set of measurement candidates",
    ", we could consider a discrete set .",
    "one expects that the resulting discrete minimization problem would be much simpler . if the number of discrete measurement candidates is too small however , the estimation error could be worse than standard quantum tomography .",
    "the relation between the reduction in computational cost and the ( probable ) increase in estimation error by introducing such discrete minimization is an open problem .",
    "in this paper , we considered adaptive experimental design and applied a measurement update method known in statistics as the a - optimality criterion to 1-qubit mixed state estimation using arbitrary rank-1 projective measurements .",
    "we derived an analytic solution of the a - optimality update procedure in this case , reducing the complexity of measurement updates considerably .",
    "our analytic solution is applicable to any case in which the loss function can be approximated by a quadratic function to least order .",
    "we performed monte carlo simulation of this and several nonadaptive schemes in order to compare the behaviour of estimation errors for a finite number of measurement trials .",
    "we compared the average and pointwise expected squared hilbert - schmidt distance and infidelity of the following four measurement update criteria : a - optimality for the squared hilbert - schmidt distance ( ahs ) , a - optimality for the infidelity ( aif ) , repetition of three orthogonal projective measurements ( xyz ) , and uniformly random selection of projective measurements ( urs ) .",
    "the numerical results showed that ahs and aif give more precise estimates than urs and xyz which corresponds to standard quantum tomography with respect to expected infidelity .",
    "t.s . would like to thank fuyuhiko tanaka for helpful discussion on mathematical statistics and terumasa tadano for useful advice on numerical simulation .",
    "this work was supported by jsps research fellowships for young scientists ( 22 - 7564 ) and project for developing innovation systems of the ministry of education , culture , sports , science and technology ( mext ) , japan .",
    "we give the proof of theorem [ theorem:1 ] .",
    "first , we introduce a lemma about matrix inverses .",
    "@xcite let @xmath220 denote a @xmath221 invertible matrix .",
    "let us consider a matrix @xmath222 , where @xmath223 is a @xmath224-dimensional vector .",
    "if @xmath225 is not singular , then @xmath226    by substituting @xmath227 into eq.([eq : inverse2 ] ) ( in our case @xmath228 and @xmath229 ) , we obtain @xmath230 and @xmath231 =   \\notag \\\\       & \\mbox{tr}[h(\\bm{s } ) v^{-1 } ] - \\frac{\\bm{a}^{t } v^{-1}h(\\bm{s})v^{-1 } \\bm{a}}{1-(\\bm{a}\\cdot \\bm{s})^2 + \\bm{a}^{t } v^{-1}\\bm{a}}.\\label{eq : a3 }    \\end{aligned}\\ ] ] the first term of the rhs in eq.([eq : a3 ] ) is independent of @xmath101 and therefore we obtain @xmath232\\notag \\\\       = \\argmax_{\\bm{a}\\in \\mathcal{a}}\\frac{\\bm{a}^{t } v^{-1}h(\\bm{s } ) v^{-1}\\bm{a}}{\\bm{a}^{t}(i-\\bm{s}\\bm{s}^{t } + v^{-1})\\bm{a } } \\\\       = \\argmin_{\\bm{a}\\in",
    "\\mathcal{a } } \\frac{\\bm{a}^{t}(i-\\bm{s}\\bm{s}^{t } + v^{-1})\\bm{a}}{\\bm{a}^{t } v^{-1}h(\\bm{s } ) v^{-1}\\bm{a } } ,    \\end{aligned}\\ ] ] where we used the relation @xmath233 .",
    "let us introduce a vector @xmath234 note that @xmath235 and @xmath101 take values in the same set , so that the vector @xmath101 can be represented in terms of @xmath235 as @xmath236 then the minimization function is represented by using @xmath235 as @xmath237 the vector @xmath235 minimizing eq.([eq : a8 ] ) is the eigenvector with the minimal eigenvalue of the matirx @xmath238 i.e. , @xmath239 . by substituting @xmath240 and @xmath241 into eqs .",
    "( [ eq : a7 ] ) and ( [ eq : a9 ] ) , we obtain theorem [ theorem:1 ] . @xmath242",
    "we give a proof of eq.([eq : gcrineq ] ) .",
    "we consider a general probability distribution @xmath243 , where @xmath244 is a set of outcomes and @xmath245 .",
    "it does not necessarily obey born s rule .",
    "we assume differentiability of a sufficient order with respect to @xmath215 . from the definition of probability distributions ,",
    "we obtain @xmath246 where we assumed that @xmath247 .",
    "this assumption is valid for all full rank density matrices in any finite dimensional system .",
    "the contrapositive is that there can exist non full rank density matrices which do not satisfy the assumption .",
    "this is the reason why we restrict our estimation objective to mixed states , the interior of the bloch sphere in sec .",
    "[ subsec : estimationsetting ] .",
    "let us define a @xmath221 matrix @xmath248 as @xmath249 where we used eq.([eq : b2 ] ) .",
    "for any vectors @xmath250 and @xmath251 in @xmath252 , we obtain @xmath253 [ ( \\theta^{\\est}(y)-\\theta)^{t}w ] \\bigr)^2 \\\\       & \\le&\\bigl(\\sum_{y\\in\\mathcal{y}}p(y|\\theta)[u^t \\nabla_{\\theta}\\ln",
    "p(y|\\theta)]^2 \\bigr ) \\notag \\\\        & & \\ \\",
    "\\times   \\bigl ( \\sum_{y^{\\prime}\\in\\mathcal{y } } p(y^{\\prime}|\\theta)[(\\theta^{\\est}(y)-\\theta)^{t}w]^2 \\bigr )   \\notag \\\\       & = & ( u^t f u ) ( w^t e w ) ,     \\end{aligned}\\ ] ] where @xmath254 therefore @xmath255 , we obtain @xmath256 we would like to obtain an inequality as tight as possible , so let us consider the maximization of the rhs of eq.([eq : b10 ] ) .",
    "it is maximized when @xmath257 , and the maximal value is @xmath258 .",
    "we obtain a matrix inequality @xmath259 multiplying by a positive semidefinite matrix @xmath147 and taking the trace of eq.([eq : b11 ] ) , we obtain @xmath260 \\ge \\trace [ h g^{t } f^{-1 } g ] .",
    "\\label{eq : b12 }     \\end{aligned}\\ ] ] by substituting @xmath261 , @xmath262 , and @xmath263 , we obtain eq .",
    "( [ eq : gcrineq ] ) .",
    "when the estimator @xmath264 is unbiased , i.e. , @xmath265 , the matrix @xmath248 is the identity matrix , and we obtain the ( standard ) cramer - rao inequality : @xmath266",
    "in this section we explain the relation between conditional and unconditional fisher matrices . from a simple calculation ,",
    "we can obtain @xmath267 where the sum is taken over @xmath268 .",
    "this is the reason why @xmath153 is called the conditional fisher matrix of @xmath269 . in statistical parameter estimation theory ,",
    "it is known that the divergence of the conditional fisher matrix ( @xmath270 as @xmath271 ) almost everywhere in @xmath272 is part of a sufficient condition for the convergence ( known as _ strong consistency _ in statistics ) of a maximum likelihood estimator @xcite .",
    "if we assume that the other elements of the set of sufficient conditions are satisfied , the divergence of the conditional fisher matrix is sufficient for the convergence of a mle . in this case , from eq.([eq : c1 ] ) , the unconditional fisher matrix also diverges ( @xmath273 ) , and this is equivalent to the condition that @xmath274\\to 0 $ ] .",
    "therefore , the divergence of the unconditional fisher matrix is a necessary condition for the convergence of a mle .",
    "the divergence of @xmath269 is , however , not sufficient for the convergence of a mle .",
    "we illustrate this with a simple example .",
    "suppose that our estimation objective is @xmath275 .",
    "at the first trial , we perform a povm @xmath276 , where @xmath277 .",
    "we obtain the outcome @xmath278 and @xmath279 both with @xmath280 probability .",
    "when we obtain an outcome @xmath278 at the first measurement , we perform standard quantum tomography for the rest of all the trials . in this case",
    ", a mle converges to the true state , and the conditional fisher matrix @xmath281 whose @xmath282 includes @xmath283 diverges .",
    "let @xmath284 denote the conditional fisher matrix . on the other hand , when we obtain @xmath279 in the first measurement , we repeat the same povm @xmath12 for the remaining trials .",
    "let @xmath285 denote the conditional fisher matrix whose @xmath282 includes @xmath286 . in this case",
    ", no estimator converges to the true state because the povm @xmath12 does not give us any information , ( the probability distribution is ( 1/2 , 1/2 ) , independent of the true state )",
    ". then we obtain @xmath287 .",
    "the unconditional fisher matrix is calculated as @xmath288 i.e. , the unconditional fisher matrix @xmath269 diverges even though no estimator converges to the true state with probability @xmath280 .",
    "therefore the divergence of @xmath269 is necessary , but not sufficient for the convergence of a mle .",
    "as we can see from the above example , in adaptive experimental designs , the essential characteristic of the scheme is not the unconditional fisher matrix but the conditional fisher matrices . in order to make a mle converge ,",
    "we need to design an experiment such that almost all ( not necessarily strictly all ) the conditional fisher matrices diverge . from this point of view",
    ", the approximation eq.([eq : aoptapprox2 ] ) lies at the heart of adaptive experimental designs .",
    "in fig . [ fig : results_purity ] it is shown that the average expected infidelities of xyz and urs at @xmath196 have a peak around @xmath289 . here",
    "we explain the origin of the peak .",
    "[ fig : xyzurs_purity ] is a plot of average expected infidelity for six bloch radii ( purities ) @xmath203 .",
    "we choose six purities from the fourteen purities in fig .",
    "[ fig : results_purity ] to make things easier to see .",
    "the average is taken over all directions @xmath215 and @xmath216 for each bloch radius @xmath203 ; ( xyz ) is for xyz and ( urs ) is for urs . roughly speaking",
    ", the plots can be interpreted as straight lines with different slopes and @xmath290-intercepts on a log - log scale . as the purity ( @xmath203 ) increases ,",
    "two things occur : ( i ) the slope of the curves becomes less steep , and ( ii ) the @xmath290-intercept decreases . at @xmath196 ,",
    "these two effects combine in such a way as to create a peak in the estimation error around @xmath289 .",
    "30 natexlab#1#1bibnamefont # 1#1bibfnamefont # 1#1citenamefont # 1#1url # 1`#1`urlprefix[2]#2 [ 2][]#2 , eds . , _ _ , lecture notes in physics ( , , ) .",
    ", , , , * * , ( ) . , , , , , * * , ( ) . , in _ _",
    ", in _ _ , edited by ( , , ) , chap .  . , * * , ( ) . , , , * * , ( ) .",
    ", * * , ( ) . , * * , ( ) . , * * , ( ) .",
    "( ) , . , , , , , , * * , ( ) .",
    ", , , , * * , ( ) . , , , , , * * , ( ) . . , , , , , , , _ _ ( , , ) , . , _",
    "_ , classics in applied mathematics ( , , ) . , _",
    "_ , wiley series in probability and statistics ( , , ) , ed . , . , _",
    "_ , probability and mathematical statistics ( , , ) .",
    ", , , , , * * , ( ) . , in _ _",
    "( , ) , vol . , p.  , .",
    ", in _ _ , edited by ( , , ) , chap .  . , * * , ( ) . , , , * * , ( ) . , ,",
    ", in _ _ , edited by , , , ( , , ) , vol .   of _",
    "* * , ( ) .",
    ", , , ( ) , . , _ _ ( , , ) .",
    ", , , * * , ( ) ."
  ],
  "abstract_text": [
    "<S> we consider 1-qubit mixed quantum state estimation by adaptively updating measurements according to previously obtained outcomes and measurement settings . </S>",
    "<S> updates are determined by the average - variance - optimality ( a - optimality ) criterion , known in the classical theory of experimental design and applied here to quantum state estimation . in general , a - optimization is a nonlinear minimization problem ; however , we find an analytic solution for 1-qubit state estimation using projective measurements , reducing computational effort . </S>",
    "<S> we compare numerically two adaptive and two nonadaptive schemes for finite data sets and show that the a - optimality criterion gives more precise estimates than standard quantum tomography . </S>"
  ]
}