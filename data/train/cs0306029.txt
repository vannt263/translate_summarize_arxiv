{
  "article_text": [
    "alice ( a large ion collider experiment ) @xcite is a heavy ion experiment that is being built for the future large hadron collider ( lhc ) @xcite at cern .",
    "it is designed primarily for operation in the collider s heavy ion ( hi ) mode , but will also acquire data in proton - proton ( pp ) mode .",
    "heavy ion mode is characterized by very large multiplicities of up to 15.000 particles per event , a resulting maximum event size of about 70  mb , and an allowed data rate into the last trigger stage , the high level trigger ( hlt ) , of up to 25  gb / s , with the largest data contributor being the time projection chamber ( tpc ) .",
    "event rates into the hlt are 200  hz and 1  khz for the tpc and other participating central detectors in hi and pp mode respectively . in the hlt",
    "the complete event data of the participating detectors is available , and its task is to perform a full event reconstruction with this data . as for other lhc experiments ,",
    "the hlt s architecture is a large farm consisting of the order of a thousand pc nodes running the linux operating system and connected via a fast network .",
    "readout data passes through the cluster in several steps of analysis and merging .    in the tpc , as an example , the reconstruction process starts with the raw adc values that are read out from the detector via the fiber optical detector data links ( ddls ) .",
    "each link corresponds to one of 6 sub - sectors , called patches , of the tpc s 36 sectors , called slices , and is terminated in a pci card through which data from the detector is read out into the hlt system . during the readout process an fpga on the pci card can perform the first stage of an analysis together with programs on the node .",
    "for instance three dimensional space - points of the charge clusters can be determined directly by the fpga using the adc values .",
    "these event space - points are then placed into the node s main memory where they are used for further analysis . on the same or other hlt nodes tracklets",
    "can be calculated from these space - points . for load - balancing purposes this task will be distributed among several cluster nodes .",
    "once the tracklets have been determined they are sent to nodes in the next stage .",
    "tracklets that belong to the same sector , and the same event , are sent to the same node in the next stage . on that node the tracklets",
    "are then merged across slice boundaries , to form tracklets for a whole sector . in the following steps ,",
    "the tracklets from multiple sectors are sent to the next level of nodes , to be merged into larger groups .",
    "one sample sequence is to first merge groups of adjacent sector - sextetts and then merge the tracks from the resulting six sextetts .",
    "these hierarchical stages of analysis map naturally onto the detector geometry and hierarchy , as the preceeding description shows .",
    "a second mapping is possible between this hierarchy and the topology used for the network connecting the hlt nodes .",
    "as a node in one specific stage has its main amount of communication with the nodes in the directly preceeding and following stages , it does not require full bandwidth to every node in the cluster .",
    "this in turn drastically reduces the requirement for the bisection bandwidth of the network in the cluster , and thus also its cost . to retain maximum flexibility in the network choice the software framework developed for the hlt encapsulates network functionality in a separate class library , described below in section [ sec : networking ]",
    "for the transport of the event data through the hlt cluster a software framework was needed with two main requirements set for it : flexibility and efficiency . with regard to flexibility ,",
    "the designed architecture is a number of independant components that communicate via a common interface . using this interface it is possible to plug the components together in different configurations , as required by the current conditions , e.g. a test run or pp or heavy ion mode .",
    "even a reconfiguration of the system at runtime , with the necessary reconnections of the components , should be supported by the interface .    concerning efficiency , the primary requirement is a minimal cpu usage during the transfer of data , both between components on the same and on different nodes .",
    "any cpu cycle used for data transport is unavailable for analysis , increasing the number of cpus needed for the hlt , and as a result also its cost . as a secondary efficiency requirement",
    ", the transport of the data should be performed as quickly as possible to reduce the latency for an event .",
    "this latency reduction is not of prime importance as an increased latency can always be compensated by large enough buffer memory and memory prices are steadily decreasing with time .",
    "c++ was chosen as the framework s implementation language , to take advantage of object - oriented programming capabilities , such as encapsulation and inheritance .",
    "java and other oo languages were not considered , as either their basically interpreted nature rendered them unsuitable for the set performance and efficiency requirements and/or because of their less widespread support and usage .      for efficiency reasons , the component communication interface as such works only locally on a node . to avoid unnecessary copying steps between the components ,",
    "shared memory is used for the exchange of data .",
    "descriptors of the data , holding the shared memory i d and the offset and size of the data block , are transmitted from a data producer to a consumer .",
    "the publisher - subscriber paradigm , also known as producer - consumer - principle , is used for the interface , so that multiple consumers can attach to one producer .",
    "each subscriber uses a separate set of named pipes for communication with the publisher but all access the same data blocks in shared memory .",
    "the descriptors sent from a publisher to its subscribers are thus identical for each subscriber .",
    "a descriptor can hold multiple data blocks of the same event and in addition to the blocks locations also contains their datatypes as well as an indicator of a block data s origin    one implication of this approach of using one data block in shared memory for processing by multiple subscribers , is that the buffer management has to be done in the publisher .",
    "this further implies , that each subscriber has to inform its publisher when it has finished processing an event so that the publisher can safely reuse the shared memory block for another event .      as explained above",
    ", the interface for communication between the framework components is purely local on one node .",
    "therefor network communication between components on different nodes has to be handled in a different way .",
    "two specialized components have been written , described below in section [ sec : dataflowcomponents ] , that work as bridges between nodes .",
    "for the network communication these two components make use of a class library that provides two abstract network communication apis , one for small message - like sends and one for large data block transfers . by splitting communication into these two apis",
    "each of them can be optimized for the corresponding task .",
    "both interfaces are defined , each in its own abstract base class , so that derived implementation classes for a specific network technology and protocol can make use of low overhead , low latency , or high speed features present in that technology .",
    "as the call interface is only defined in an abstract base class it is possible to provide several derived classes with implementations for each of the two communication types , with each implementation supporting a different network technology and/or protocol . in the current version of the library ,",
    "classes for both communication types are provided for tcp / ip , as the most widely available baseline network protocol , and sci network cards by dolphin @xcite using the sisci api @xcite , as an example of a system - area - network ( san ) .",
    "[ fig : comclasses ] shows the relation of the different communication classes in the library .      one major aspect of any system based on large scale pc clusters , must be the tolerance of the system as a whole with respect to the failure of hardware or software components on a node , or even of a whole node . by using the dynamic reconnection feature of the component communication interface , as well as a number of specialized components ,",
    "the framework is able to support configurations that can handle these faults .",
    "a number of components are contained in the framework that allow to setup configurations that handle faults by distributing workload among a number of nodes . if a component related to one of these nodes fails , the whole node is deactivated and the load distributed among the remaining nodes . when a stand - by node is available it can be activated to take over the processing tasks of the deactivated node .",
    "a more sophisticated system of only temporarily routing a data flow around a fault , restarting and reconnecting failed components and then again activating the original data flow , is not yet available for the framework , but can be implemented using external programs .",
    "actual functionality of the framework is provided by its components , separate programs that can be connected together using the interface described in section [ sec : interface ] .",
    "a number of fully functional components that allow to setup the data flow in a cluster are part of the framework .",
    "these components , described in section [ sec : dataflowcomponents ] , can be used without change in setups utilizing the framework .",
    "further components are provided as templates that can be used to add user specific functionality to a framework system .",
    "three templates are present , for data sources , data processing components , and data sinks , covered in section [ sec : componenttemplates ] . adding the appropriate functionality to these templates allows to create a framework configuration that is able to address specific requirements for a local system .",
    "the final group of components presented in section [ sec : faulttolerancecomponents ] addresses the fault tolerance ( ft ) abilities described in section [ sec : faulttolerance ] .",
    "a number of these components are extended versions of the basic data flow components , adding special fault tolerance handling capabilities , while others are custom components with specific ft functionality .",
    "the first data flow component , the ` eventmerger ` shown in fig .",
    "[ fig : eventmerger ] , merges multiple data streams , containing subevents belonging to the same event , into one data stream with larger subevent parts or even whole events . for this purpose",
    "it waits to receive one subevent from each of its configured input streams .",
    "once these subevents are received , it constructs a new event descriptor , containing all data blocks from the received subevents and publishes it for its attached subscribers .    working in conjunction , the ` eventscatterer ` and ` eventmerger ` components , shown in fig .",
    "[ fig : eventscatterer - gatherer ] , provide a mechanism to split a single stream of events into multiple smaller event streams , that are later recombined again into one single stream .",
    "this stream splitting can be used for load balancing purposes , with the resulting multiple streams being distributed among a number of nodes for processing , to be later re - combined .",
    "splitting up is done on an event - by - event basis , with each single event being dispatched to one output stream as a whole .",
    "no event is split up into multiple sub - events for distribution .",
    "another group of components , the ` subscriberbridgehead ` and ` publisherbridgehead ` shown in fig .  [",
    "fig : bridgecomponents ] , provide a mechanism of transparently connecting components on different nodes , as mentioned in section [ sec : networking ] .",
    "the ` subscriberbridgehead ` component contains a subscriber class to accept events using the common communication mechanism for network transmission to the ` publisherbridgehead ` on another node . after receiving the event data and",
    "the necessary descriptor field subset the ` publisherbridgehead ` again makes the data available to other components on its node using an instance of the standard publisher class . in this way",
    "a transparent component connection is enabled across nodes .",
    "components connected to the ` publisherbridgehead ` will behave as if they had been subscribed directly to the data s original publishing component .",
    "network communication between the two bridge head components is performed using the network class library presented in section [ sec : networking ] , making the bridges independant of a specific networking technology . to use them with a specific networking technology or protocol only an appropriate implementation of the library classes",
    "is required .",
    "no change to the bridge components themselves is necessary .",
    "user specific tasks related to framework components can basically be grouped into three different categories : data sources , processing components , and data sinks .",
    "data sources are components that obtain data from a source outside of the framework , examples of this are special readout devices like pci cards with links to detectors .",
    "the data source component template already contains the code for the shared memory buffer management and for the publishing of events . to use the component , only the specific code to access the data source concerned (",
    "so that event data is placed in the shared memory buffers ) has to be added .",
    "if the buffer management code does not meet the requirements of the data source it can also be easily replaced .",
    "processing components are located in the middle of an analysis chain .",
    "they accept new data from data producers , perform analysis steps with the data , mostly producing some new output data , and make the new output data available to other data consumers . in the analysis template functionalities for accepting new input data via the framework interface , dereferencing that data in shared memory for access , shared memory output buffer management , and the publishing of newly created output data are already provided . the processing function for the data",
    "is called from this gluecode after dereferencing the input data and has just to be implemented as needed in order to create a fully functional data processing component .",
    "data sinks are components at the end of a processing chain , accepting data that is then forwarded to a destination outside of the framework , e.g. simply written to disk or tape , passed to a daq system , or sent to a mass storage system . in the provided sink template the code to accept input data from publisher components and dereference and access that data is already provided .",
    "the data output part that has to be implemented is called after the data has been made accessible inside the component .      to handle the tolerance with regard to component faults as described in section [ sec : faulttolerance ] three new components",
    "have been introduced into the framework and four of the data flow components described in section [ sec : dataflowcomponents ] have been enhanced with ft capabilities . modified versions of both bridge components",
    "have been enhanced with the abilities to accept commands from external controlling instances .",
    "primary commands for the bridges include connect and disconnect commands , as well as the specification of the remote bridge components address , which together allow to dynamically establish bridge connections between nodes .",
    "the other two enhanced components are the event scatterer and gatherer components , that have also been modified to accept commands from supervising programs . here",
    "the primary commands for both components are the disabling and enabling of output or input event stream paths .",
    "when an ouput path is disabled in the scatterer , the events that have been sent to that path and not received back as finished , are distributed again among the remaining paths .",
    "events that arrive after a path has been disabled are also dispatched only to one of the remaining streams . in the gatherer component disabling",
    "a path causes events that have been received through that path and that are released by its subscribers , to not be released directly , but only to be stored as released .",
    "as the events for the failed path will be sent on another path by the scatterer , they will be received by one of the other input paths in the gatherer .",
    "as soon as this happens , the event release is immediately sent to the publisher at the path from which the event was re - received . for events that are still being processed downstream of the gatherer",
    ", just the new received input path is stored instead of the original disabled one .",
    "together these components ensure that each event that arrives at the scatterer s input passes the gatherer s output exactly once , as long as there is at least one active event stream connecting them .    of the custom built components",
    "the first is a simple fault detector component that acts as a monitoring subscriber .",
    "it is attached to a publisher and if no events have been received from that publisher for a specified time it reports this associated publisher as defective to the second specialized component , the fault tolerance supervisor .",
    "this supervisor component then sends commands to specified targets , informing them of a disabled event path .",
    "targets for this command are ft scatterer and gatherer components , as well as the bridge manager , the third custom ft component . in the bridge manager ,",
    "lists are kept of nodes and their bridge components , associated with the active event streams as well as spare stream nodes . when a stream is deactivated the bridge manager looks for an available spare node and if one is found disconnects the bridges connected to the deactivated stream .",
    "the addresses of the spare nodes are then sent to the bridges , followed by connection commands .",
    "as soon as the bridges report their respective connections to be established , commands are sent to the scatterer and gatherer involved , re - activating the path that had failed .",
    "after the first events arrive in the fault detection subscriber the status of the path is changed back to operational in the supervisor component .",
    "to test the functionality and performance of the framework an analysis test has been performed on a cluster in heidelberg . in this test one slice of simulated proton - proton events of the alice tpc , with 25 events piled - up , has been processed",
    "processing has started with the compressed adc values , similar to the read - out data format , and has been executed up to merged tracks for the whole slice .",
    "the hardware used for the test was a mix of 19 dual smp linux pcs , with cpu speeds of 733  mhz and 800  mhz and 512  mb of ram per node .",
    "for the network connection between the nodes fast ethernet was sufficient due the comparatively small size of the pp events used , between 5  kb and 20  kb packed adc data per patch .",
    "suse linux 7.2 with a 2.4.18 kernel was used as the nodes operating system .",
    "as shown in the test s software configuration in fig .",
    "[ fig : slicetestsetup ] the nodes are arranged in four hierarchy levels ( hl ) , hl0 to hl3 .",
    "on the six nodes making up the first level the simulated data is read from files and published as the start of the processing chain . on each",
    "node subevents of the same patch in the slice are used for publishing .",
    "the two processing steps performed on the hl0 nodes are the unpacking of the runlength encoded data for processing and the cluster - finding on the unpacked raw data .",
    "both of these , as well as all following analysis stages , are implemented in a separate analysis component so that each processing step could theoretically be executed on a separate node .",
    "data of found clusters on one node is then shipped , via an event scatterer and three bridge component pairs , to three nodes in the next hierarchy level .",
    "of the nine nodes in hl1 three are shared between each pair of adjacent patch nodes in hl0 .",
    "cluster data from each hl0 node is distributed among its three assigned hl1 nodes for track finding .",
    "this is done in a way that on each hl1 node one tracking component runs for each of its two associated patches , to use the nodes two cpus .",
    "the reason for the even distribution of events among the three nodes , in contrast to e.g. using one node fully for each patch and sharing only one node between a pair of patches , lies in the better load balancing upon node failures .",
    "if one of the hl1 nodes fails only one third of the processing capability of the concerned patches is lost compared to up to two thirds for one patch in the other case . after tracking ,",
    "the six patches tracklets for an event are sent to one of the three nodes in hl2 where track merging on the slice level is performed . as the last step",
    ", the merged slice tracks from the three hl2 nodes are sent to a single event stream merger node in hl3 , collecting all processed events without any additional analysis .",
    "the processing components used in the test are the ones that have already been used for the test in @xcite .    in this test a sustained event processing rate of more than 420",
    "hz has been achieved .",
    "the limiting factor was the cpu utilization on the hl0 nodes that was at 100  % for both cpus . by splitting up the adc unpacking and cluster finding on separate nodes this bottleneck",
    "could be avoided , with the problem in this case , that the unpacked adc data constitutes the largest data volume of the different types of data ( e.g. packed adc , unpacked adc , clusters , or tracklets ) in the processing .",
    "another approach could be to use a scatterer and a bridge after the file publishing to send a portion of each patch s packed adc data for processing to a second hl0 node . with the result shown",
    ", it has been demonstrated that the framework can be used already today , e.g. in test beams , and should be ready for both pp and heavy ion mode with their required event rates of 1  khz and 200  hz respectively , given enough processors to perform the required data analysis .      as a demonstration of the fault tolerance capabilities contained in the framework a small test",
    "was performed with a setup of seven nodes similar to the one shown in fig .",
    "[ fig : fttest ] .",
    "one scattering node sends data , evenly distributed , to three processing nodes that again send their data to a fourth node , gathering the data .",
    "a fifth node is available as a stand - by processing node .",
    "the remaining seventh node is used to run the two control programs , the fault tolerance supervisor and the bridge manager .    in the test",
    ", the configuration was activated to send 128  kb events at approximately 100  hz frequency . after a short time of running the network cable of one of the three processing nodes",
    "was unplugged with the results shown in fig .",
    "[ fig : fttestresults ] .",
    "the figure shows the network transfers measured during the test on five nodes , the scatterer and gatherer nodes , one working processing node , the processing node whose network cable was unplugged , and the spare working node .",
    "all graphs are scaled independantly and arbitrarily for visualization purposes , as the absolute measured values are not relevant for this test .    at point 1 in the graph",
    "the network cable has been unplugged .",
    "the throughput on the worker node involved goes to zero and on the sending node the throughput drops to about two thirds of its original value , as expected .",
    "several seconds later , at point 2 , the fault detector has triggered and has informed the tolerance supervisor of the node s `` fault '' .",
    "the commands to disable the concerned path between scatterer and gatherer have been sent to those two components . as a result of this action",
    ", the throughput on the sender increases to its previous value . on the remaining worker",
    "shown traffic increases by about one - half its previous value , again as expected when the previous workload of three nodes is distributed among two remaining ones .",
    "finally at point 3 , the bridge manager has replaced the `` faulty '' node with the spare one and reactivated the respective path .",
    "network traffic on the spare node is now at the previous value of the removed node and on the remaining worker the traffic also decreases to its previous value . at that point",
    "the operation of the system has been fully restored to its state prior to the simulated fault . the time elapsed between unplugging and this restoration is primarily determined by the timeout values chosen for the test , e.g. in the fault detection subscriber .    with this test",
    "it has been shown that the fault tolerance implementation in the framework works and that faults can be handled by the system already now , although only with a granularity of complete nodes .      as a benchmark for the performance and scaling behaviour of the interface used for communication between the framework s components , a test has been run on three separate dual smp pcs , with cpu speeds of 733  mhz , 800  mhz , and 933  mhz . in the test",
    "a publisher announces a continuous stream of @xmath0 empty events to a subscriber and the subscriber immediately releases each received event .",
    "since the events are empty , no event data is used or copied in the test , restricting the test to measure the interface s performance .",
    ".[tab : pubsubaveragerates]average event rates and resulting time overheads . [ cols=\"<,^,^,^\",options=\"header \" , ]     the results that have been obtained from this test are given in table  [ tab : pubsubaveragerates ] and fig .",
    "[ fig : scalingresults ] . in the table ,",
    "the achieved average event rates over the test s running time are shown together with the corresponding time overhead .",
    "[ fig : scalingresults ] shows two different scaling behaviours , with and without a cut , obtained during the benchmark together with the clock speed scaling as a point of reference .",
    "application of the cut was performed in order to use the timing obtained from the fastest 90  % of events only .",
    "the motivation for this is that programs get descheduled during the test by the operating system .",
    "since the timing is measured by reading the current system time at the beginning and end of multiple involved function blocks , a descheduling in such a block will increase the measured time incorrectly .",
    "as these descheduling events do not cause a fixed time increment the cut was made to exclude the 10  % of events that took the longest time to process in each of the function blocks and thereby remove the influences of descheduling events on the interface performance measurements . in the table",
    "the cut is not included so that these numbers indicate the absolute rate that can be achieved in an actual system . as can be seen from the table and the figure , the interface scales reasonably well with the clock frequency .",
    "it does not scale perfectly but the framework should be able to take advantage of a good fraction of future increases in cpu clock frequency making it appropriate for use on cpus expected for the time when the alice hlt starts to operate .",
    "recent tests with a number of performance enhancements in the interface code have indicated that , for future versions , a performance increase of about a factor of four can be expected .",
    "we have presented a working framework for constructing distributed online data analysis chains running on linux clusters .",
    "the framework allows a flexible configuration due to its approach of components that can be connected together via a defined common interface .",
    "using a separate class library to encapsulate network communication makes the framework and its components independant of the actual network technology and protocol used , allowing for more flexibility in the construction of a cluster . with the described tests the framework has been shown to work and to be usable in real applications already today , provided that enough cpu power is available to perform the desired analysis which is independant of the framework as such .",
    "future work on the framework has to include a simple method to specify the configuration to be used in a cluster as well as an efficient method of starting and supervising the involved processes on the cluster nodes .",
    "this supervision , in principle a detector control system , also involves a central or distributed fault tolerance control and decision unit that should allow for a finer grained control of the fault tolerance and recovery actions .",
    "finally , more tuning measures on the framework , at least completing the above mentioned preliminary performance enhancements , will be done as well .",
    "work on the alice high level trigger has been financed by the german federal ministry of education and research ( bmbf ) as part of its program `` frderschwerpunkt hadronen- und kernphysik - grogerte der physikalischen grundlagenforschung '' .",
    "t. m. steinbeck et al . , `` a framework for building distributed data flow chains in clusters '' , in lecture notes in computer science lncs 2367 , proceedings of the 6th international conference on applied parallel computing , para 2002 , springer verlag berlin heidelberg , 2002"
  ],
  "abstract_text": [
    "<S> in the future alice heavy ion experiment at cern s large hadron collider input data rates of up to 25  gb / s have to be handled by the high level trigger ( hlt ) system , which has to scale them down to at most 1.25  gb / s before being written to permanent storage . the hlt system that is being designed to cope with these data rates </S>",
    "<S> consists of a large pc cluster , up to the order of a 1000 nodes , connected by a fast network . for the software that will run on these nodes a </S>",
    "<S> flexible data transport and distribution software framework has been developed . </S>",
    "<S> this framework consists of a set of separate components , that can be connected via a common interface , allowing to construct different configurations for the hlt , that are even changeable at runtime . to ensure a fault - tolerant operation of the hlt , </S>",
    "<S> the framework includes a basic fail - over mechanism that will be further expanded in the future , utilizing the runtime reconnection feature of the framework s component interface . </S>",
    "<S> first performance tests show very promising results for the software , indicating that it can achieve an event rate for the data transport sufficiently high to satisfy alice s requirements . </S>"
  ]
}