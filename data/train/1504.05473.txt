{
  "article_text": [
    "the topic of multiple classifier systems ( mcss ) is well studied in machine learning community @xcite .",
    "such algorithms appear with different names  mixture of experts , committee machines , classifier ensembles , classifier fusion and others .",
    "the underlying idea of all these systems is to train several ( base ) classifiers on a training set and to combine their predictions in order to classify objects from a test set @xcite .",
    "this idea probably dates back to as early as the 18@xmath0 century . the condorcet s jury theorem , that was formulated in 1785 in @xcite , claims that if a population makes a group decision and each voter most likely votes correctly , then adding more voters increases the probability that the majority decision is correct .",
    "the probability that the majority votes correctly tends to 1 as the number of voters increases .",
    "similarly , if we have multiple weak classifiers ( meaning that classifier s error on its training data is less than 50% but greater than 0% ) , we can combine their predictions and boost the classification accuracy as compared to those of each single base classifier .    among the most popular mcss",
    "are bagging @xcite , boosting @xcite , random forests @xcite , and stacked generalization ( or stacking ) @xcite .    in this paper , we present one more algorithm of such type  recommender - based multiple classifier system ( rmcs ) . here",
    "the underlying proposition is that a classifier is likely to predict the label of the object from a test set correctly if it has correctly classified its neighbors from a training set .",
    "the paper is organized as follows . in chapter",
    "[ sec : mcs ] , we discuss bagging , boosting and stacking . in section [ sec : fca ] , we introduce basic definitions of formal concept analysis ( fca ) .",
    "section [ sec : example ] provides an example of execution of the proposed rmcs algorithm on a toy synthetic dataset .",
    "then , section [ sec : rmcs ] describes the rmcs algorithm itself .",
    "further , the results of the experiments with real data are presented .",
    "section [ sec : conclusion ] concludes the paper .",
    "in this chapter , we consider several well - known multiple classier systems .",
    "the _ bootstrap sampling _ technique has been used in statistics for many years . _ bootstrap aggregating _ , or _",
    "bagging _ , is one of the applications of bootstrap sampling in machine learning . as sufficiently large data sets are often expensive or impossible to obtain , with bootstrap sampling , multiple random samples are created from the source data by sampling with replacement .",
    "samples may overlap or contain duplicate items , yet the combined results are usually more accurate than a single sampling of the entire source data achieves .    in machine learning the bootstrap samples",
    "are often used to train classifiers .",
    "each of these classifiers can classify new instances making a prediction ; then predictions are combined to obtain a final classification .",
    "the aggregation step of bagging is only helpful if the classifiers are different .",
    "this only happens if small changes in the training data can result in large changes in the resulting classifier  that is , if the learning method is unstable @xcite .",
    "the idea of _ boosting _ is to iteratively train classifiers with a weak learner ( the one with error better than 50% but worse than 0% ) @xcite .",
    "after each classifier is trained , its accuracy is measured , and misclassified instances are emphasized",
    ". then the algorithm trains a new classifier on the modified dataset . at classification time",
    ", the boosting classifier combines the results from the individual classifiers it trained .    boosting was originally proposed by schapire and freund @xcite . in their _ adaptive boosting _ , or _ adaboost _ , algorithm , each of the training instances starts with a weight that tells the base classifier its relative importance @xcite . at the initial step",
    "the weights of @xmath1 instances are evenly distributed as @xmath2 the individual classifier training algorithm should take into account these weights , resulting in different classifiers after each round of reweighting and reclassification .",
    "each classifier also receives a weight based on its accuracy ; its output at classification time is multiplied by this weight .",
    "freund and schapire proved that , if the base classifier used by adaboost has an error rate of just slightly less than 50% , the training error of the meta - classifier will approach zero exponentially fast @xcite .",
    "for a two - class problem the base classifier only needs to be slightly better than chance to achieve this error rate . for problems with more than two classes less than 50%",
    "error is harder to achieve .",
    "boosting appears to be vulnerable to overfitting .",
    "however , in tests it rarely overfits excessively @xcite .      in _ stacked generalization _ , or _",
    ", each individual classifier is called a _ level-0 model_. each may vote , or may have its output sent to a _",
    "level-1 model _",
    " another classifier that tries to learn which level-0 models are most reliable .",
    "level-1 models are usually more accurate than simple voting , provided they are given the class probability distributions from the level-0 models and not just the single predicted class @xcite .",
    "a _ formal context _ in fca is a triple @xmath3 , where @xmath4 is a set of objects , @xmath5 is a set of attributes , and the binary relation @xmath6 shows which object possesses which attribute .",
    "@xmath7 denotes that object @xmath8 has attribute @xmath9 . for subsets of objects and attributes",
    "@xmath10 and @xmath11 _ galois operators _ are defined as follows : @xmath12    a pair @xmath13 such that @xmath14 and @xmath15 , is called a _ formal concept _ of a context @xmath16 .",
    "the sets @xmath17 and @xmath18 are closed and called the _",
    "extent _ and the _ intent _ of a formal concept @xmath13 respectively .",
    "for the set of objects @xmath17 the set of their common attributes @xmath19 describes the similarity of objects of the set @xmath17 and the closed set @xmath20 is a cluster of similar objects ( with the set of common attributes @xmath21 ) @xcite",
    ".    the number of formal concepts of a context @xmath3 can be quite large ( @xmath22 in the worst case ) , and the problem of computing this number is # p - complete @xcite .",
    "there exist some ways to reduce the number of formal concepts , for instance , choosing concepts by stability , index or extent size @xcite .    for a context @xmath23 ,",
    "a concept @xmath24 is _ less general than or equal to _ a concept @xmath25 ( or @xmath26 ) if @xmath27 or , equivalently , @xmath28 . for two concepts @xmath29 and @xmath30 such that @xmath26 and there is no concept @xmath31 with @xmath32 , the concept @xmath29 is called a _",
    "lower neighbor _ of @xmath30 , and @xmath30 is called an _ upper neighbor _ of @xmath29 .",
    "this relationship is denoted by @xmath33 .",
    "formal concepts , ordered by this relationship , form a _ complete concept lattice _ which might be represented by a",
    "_ hasse diagram _ @xcite . several algorithms for building formal concepts ( including @xmath34 ) and constructing concept lattices",
    "are studied also in @xcite .",
    "one can address to @xcite and @xcite to find some examples of formal contexts , concepts and lattices with their applications .",
    "chapter [ sec : example ] also shows the usage of fca apparatus in a concrete task .",
    "however , in some applications there is no need to find all formal concepts of a formal context or to build the whole concept lattice .",
    "concept lattices , restricted to include only concepts with frequent intents , are called @xmath35 .",
    "they were shown to serve as a condensed representation of association rules and frequent itemsets in data mining @xcite .",
    "here we modified the @xmath34 algorithm slightly in order to obtain only the upper - most concept of a formal context and its lower neighbors .",
    "the description of the algorithm and details of its modification is beyond the scope of this paper .",
    "let us demonstrate the way rmcs works with a toy synthetic dataset shown in table [ toy_dataset ] .",
    "we consider a binary classification problem with 8 objects comprising a training set and 2 objects in a test set .",
    "each object has 4 binary attributes and a target attribute ( class ) .",
    "suppose we train 4 classifiers on this data and try to predict labels for objects 9 and 10 .",
    "using fca terms , we denote by @xmath36  the whole set of objects , @xmath37  the test set , @xmath38  the training set , @xmath39  the attribute set , @xmath40  the set of classifiers",
    ".    .a classification context [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     here we run leave - one - out cross - validation on this training set for 4 classifiers .",
    "further , we fill in table [ class_context ] , where a cross for object @xmath41 and classifier @xmath42 means that @xmath42 correctly classifies object @xmath41 in the process of cross - validation . to clarify",
    ", a cross for object 3 and classifier @xmath43 means that after being trained on the whole training set but object 3 ( i.e. on objects @xmath44 ) , classifier @xmath43 correctly predicted the label of object 3 .",
    "let us consider table [ class_context ] as a formal context with objects @xmath4 and attributes @xmath45 ( so now classifiers play the role of attributes ) .",
    "we refer to it as classification context . the concept lattice for this context",
    "is presented in fig .",
    "[ fig : lattice ] .",
    "as it was mentioned , the number of formal concepts of a context @xmath46 @xmath23 can be exponential in the worst case .",
    "but for the toy example it is possible to draw the whole lattice diagram .",
    "thankfully , we do not need to build the whole lattice in rmcs algorithm  we only keep track of its top concepts .    here",
    "are these top concepts : @xmath47 , @xmath48 , @xmath49 , @xmath50 , @xmath51 .",
    "to classify objects from @xmath52 , we first find their @xmath53 nearest neighbors from @xmath54 according to some distance metric . in this case , we use @xmath55 and hamming distance . in these conditions , we find that three nearest neighbors of object 9 are 4 , 5 and 7 , while those of object 10 are 1 , 6 and 8 .    then , we take these sets of nearest neighbors @xmath56 and + @xmath57 , and find maximal intersections of these sets with the extents of formal concepts presented above ( ignoring the concept @xmath47 ) . the intents ( i.e. classifiers ) of the corresponding concepts",
    "are given as recommendations for the objects from @xmath52 .",
    "the procedure is summarized in table [ recommend_clf ] .",
    "@xmath58    nearest neighbor & @xmath59    nearest neighbor & @xmath60    nearest neighbor & neighbors & classification concept which extent gives the maximal intersection with the neighbors set & recommended    classifier + 9 & 4 & 5 & 7 & @xmath61 & @xmath49 & @xmath62 + 10 & 1 & 6 & 8 & @xmath63 & @xmath51 & @xmath43 +    finally , the rmcs algorithm predicts the same labels for objects 9 and 10 as classifiers @xmath62 and @xmath43 do correspondingly .",
    "lastly , let us make the following remarks :    1",
    ".   we would not have ignored the upper - most concept with extent @xmath4 if it did not have an empty intent .",
    "that is , if we had the top concept of the classification context in a form @xmath64 it would mean that @xmath42 correctly classified all objects from the training set and we would therefore recommend it to the objects from the test set .",
    "one more situation might occur that two or more classifiers turn out to be equally good at classifying objects from @xmath54 .",
    "that would mean that the corresponding columns in classification table are identical and , therefore , the intent of some classification concept is comprised of more than one classifier .",
    "in such case , we do not have any argument for preferring one classifier to another and , hence , the final label would be defined as a result of voting procedure among the predicted labels of these classifiers .",
    "here we considered an input dataset with binary attributes and a binary target class .",
    "however , the idea of the rmcs algorithm is still applicable for datasets with numeric attributes and multi - class classification problems .",
    "in this section , we discuss the recommender - based multiple classifier system ( rmcs ) .",
    "the pseudocode of the rmcs algorithm is presented in the listing algorithm [ alg : rmcs ] .",
    "the inputs for the algorithm are the following :    1 .   @xmath65  is a training set , @xmath66  is a test set ; 2 .",
    "@xmath67  is a set of @xmath16 base classifiers .",
    "the algorithm is intended to perform a classification accuracy exceeding those of base classifiers ; 3 .",
    "@xmath68  is a distance function for objects which is defined in the attribute space .",
    "this might be the minkowski ( including hamming and euclidean ) distance , the distance weighted by attribute importance and others .",
    "@xmath69  are parameters .",
    "their meaning is explained below ; 5 .",
    "@xmath70  is a function for building the upper - most concept of a formal context and its lower neighbors . actually , it is not an input for the algorithm but rmcs uses it .",
    "the algorithm includes the following steps :    1 .",
    "cross - validation on the training set .",
    "all @xmath16 classifiers are trained on @xmath71 folds of @xmath72 .",
    "then a classification table ( or context ) is formed where a cross is put for object @xmath41 and classifier @xmath42 if @xmath42 correctly classifies object @xmath41 after training on @xmath71 folds ( where object @xmath41 belongs to the rest fold ) ; 2 .   running base classifiers .",
    "all @xmath16 classifiers are trained on the whole @xmath72 .",
    "then , a table of predictions is formed where @xmath73 position keeps the predicted label for object @xmath41 from @xmath66 by classifier @xmath42 ; 3 .   building top formal concepts of the classification context .",
    "the @xmath74 algorithm is run in order to build upper formal concepts of a classification context .",
    "these concepts have the largest possible number of objects in extents and minimal possible number of classifiers in their intents ( not counting the upper - most concept ) ; 4 .   finding neighbors of the objects from @xmath66 .",
    "the objects from the test set are processed one by one . for every object from @xmath66",
    "we find its @xmath53 nearest neighbors from @xmath72 according to the selected metric @xmath75 .",
    "let us say these @xmath53 objects form a set @xmath76 .",
    "then , we search for a concept of a classification context which extent yields maximal intersection with the set @xmath76 . if the intent of the upper - most concept is an empty set ( i.e. , no classifier correctly predicted the labels of all objects from @xmath72 , which is mostly the case ) , then the upper - most concept @xmath47 is ignored .",
    "thus , we select a classification concept , and its intent is a set of classifiers @xmath77 ; 5 .   classification .",
    "if @xmath77 consists of just one classifier , we predict the same label for the current object from @xmath66 as this classifier does .",
    "if there are several selected classifiers , then the predicted label is defined by majority rule .",
    "* input : * @xmath78  are training and test sets , @xmath67  is a set of base classifiers , @xmath79  is a function for building the upper - most concept of a formal context and its lower neighbors , @xmath68  is a distance function defined in the attribute space , @xmath53  is a parameter ( the number of neighbors ) , @xmath80  is the number of folds for cross - validation on a training set + * output : * @xmath81  are predicted labels for objects from @xmath66    @xmath82[\\ ] $ ]  is a 2-d array @xmath83[\\ ] $ ]  is a 2-d array train classifier @xmath84 on @xmath85 folds not including object @xmath86 $ ] @xmath87 = predicted label for @xmath86 $ ] by classifier @xmath84 @xmath88[cl ] = ( pred = = y_{train}[i])$ ]    train classifier @xmath84 on the whole @xmath72 @xmath87 = predicted labels for @xmath66 by classifier @xmath84 @xmath89[cl ] = pred$ ]    @xmath90 @xmath91 nearest neighbors of @xmath92 $ ] from @xmath72 according to @xmath93 @xmath94 @xmath95 @xmath96 predictions for @xmath92 $ ] made by classifiers from @xmath77 @xmath97 = argmax(count\\_freq(labels))$ ]",
    "the algorithm , described above , was implemented in python 2.7.3 and tested on a 2-processor machine ( core i3 - 370 m , 2.4 hgz ) with 3.87 gb ram .",
    "we used four uci datasets in these experiments - ` mushrooms ` , ` ionosphere ` , ` digits ` , and ` nursery ` .",
    "each of the datasets was divided into training and test sets in proportion 70:30 .",
    "rbf kernel    ( c=1 , @xmath98=0.02 ) & logit    ( c=10 ) & knn    ( euclidean ,    k=3 ) & rmcs    ( k=3 ,    n_folds=4 ) & bagging svm    ( c=1 , @xmath98=0.02 )    50 estimators & adaboost    on decision    stumps ,    50 iterations + 1 & 0.998    t=0.24 sec . &",
    "0.996    t=0.17 sec .",
    "& 0.989    t=1.2*@xmath99 sec . &",
    "0.997    t=29.45 sec . &",
    "0.998    t=3.35 sec . &",
    "0.998    t=44.86 sec .",
    "+ 2 & 0.906    t=5.7*@xmath100 sec . &",
    "0.868    t=@xmath99 sec . &",
    "0.858    t=8*@xmath101 sec . &",
    "0.933    t=3.63 sec . &",
    "0.896    t=0.24 sec . &",
    "0.934    t=22.78 sec .",
    "+ 3 & 0.917    t=0.25 sec . &",
    "0.87    t=0.6 sec . &",
    "0.857    t=1.1*@xmath99 sec . &",
    "0.947    t=34.7 sec . &",
    "0.92    t=4.12 sec . &",
    "0.889    t=120.34 sec .",
    "+ 4 & 0.914    t=3.23 sec .",
    "& 0.766    t=0.3 sec .",
    "& 0.893    t=3.1*@xmath99 sec . &",
    "0.927    t=220.6 sec . &",
    "0.913    t=38.52 sec . &",
    "0.903    t=1140 sec .",
    "+        rbf kernel    ( c=@xmath102 , @xmath98=0.02 ) & logit    ( c=@xmath102 ) & knn    ( minkowski , p=1 , k=5 ) & rmcs    ( k=5 ,    n_folds=10 ) & bagging svm    ( c=@xmath102 , @xmath98=0.02 )    50 estimators & adaboost    on decision    stumps ,    100 iterations + 1 & 0.998    t=0.16 sec .",
    "& 0.999    t=0.17 sec . &",
    "0.999 t=1.2*@xmath99sec . &",
    "0.999    t=29.45 sec . &",
    "0.999    t=3.54 sec . &",
    "0.998    t=49.56 sec .",
    "+ 2 & 0.906    t=4.3*@xmath100 sec . &",
    "0.868    t=@xmath99 sec .",
    "& 0.887    t=8*@xmath101 sec . & 0.9    t=3.63 sec . &",
    "0.925    t=0.23 sec .",
    "& 0.934    t=31.97 sec .",
    "+ 3 & 0.937    t=0.22 sec . &",
    "0.87    t=0.6 sec . &",
    "0.847    t=1.1*@xmath99 sec . &",
    "0.951    t=34.7 sec . &",
    "0.927    t=4.67 sec . &",
    "0.921    t=131.6 sec .",
    "+ 4 & 0.969    t=2.4 sec . &",
    "0.794    t=0.3 sec . &",
    "0.945    t=3*@xmath99 sec .",
    "& 0.973    t=580.2 sec . & 0.92    t=85.17 sec . & 0.912    t=2484 sec .",
    "+    we ran 3 classifiers implemented in ` scikit - learn ` library(written in python ) which served as base classifiers for the rmcs algorithm as well .",
    "these were a support vector machine with gaussian kernel ( ` svm.svc ( ) ` in ` scikit ` ) , logistic regression ( ` sklearn.linear_model.logisticregression ( ) ` ) and k nearest neighbors classifier ( ` sklearn.neighbors.classification . ` + ` kneighborsclassifier ( ) ` ) .",
    "the classification accuracy of each classifier on each dataset is presented in table [ table_tests ] along with special settings of parameters . moreover , for comparison , the results for ` scikit ` s implementation of bagging with svm as a base classifier and adaboost on decision stumps are presented .",
    "as we can see , rmcs outperformed its base classifiers in all cases , while it turned out to be better than bagging only in case of multi - class classification problems ( datasets ` digits ` and ` nursery ` ) .",
    "in this paper , we described the underlying idea of multiple classifier systems , discussed bagging , boosting and stacking . then , we proposed a multiple classifier system which turned out to outperform its base classifiers and two particular implementations of bagging and adaboost in two multi - class classification problems",
    ".    our further work on the algorithm will continue in the following directions : exploring the impact of different distance metrics ( such as the one based on attribute importance or information gain ) on the algorithm s performance , experimenting with various types of base classifiers , investigating the conditions preferable for rmcs ( in particular , when it outperforms bagging and boosting ) , improving execution time of the algorithm and analyzing rmcs s overfitting .",
    "the authors would like to thank their colleague from higher school of economics , sergei kuznetsov , jaume baixeries and konstantin vorontsov for their inspirational discussions which directly or implicitly influenced this study .",
    "stumme ,  g. , taouil ,  r. , bastide ,  y. , pasquier ,  n. , lakhal ,  l. : intelligent structuring and reducing of association rules with formal concept analysis . in : baader , f. , brewka , g. , and eiter , t. ( eds . ) ki 2001 : advances in artificial intelligence .",
    ". 335350 .",
    "springer berlin heidelberg ( 2001 )"
  ],
  "abstract_text": [
    "<S> the paper briefly introduces multiple classifier systems and describes a new algorithm , which improves classification accuracy by means of recommendation of a proper algorithm to an object classification . </S>",
    "<S> this recommendation is done assuming that a classifier is likely to predict the label of the object correctly if it has correctly classified its neighbors . </S>",
    "<S> the process of assigning a classifier to each object is based on formal concept analysis . </S>",
    "<S> we explain the idea of the algorithm with a toy example and describe our first experiments with real - world datasets . </S>"
  ]
}