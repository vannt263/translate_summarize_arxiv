{
  "article_text": [
    "in this paper we propose a simple method of unsupervised feature construction based on pairwise statistics of features . in the first step , we construct neighborhoods of features by regrouping features that correlate",
    ". then we use these subsets of features as filters to produce new _",
    "neighborhood features_. next , we connect neighborhood features that correlate , and construct _",
    "edge features _ by subtracting the correlated neighborhood features of each other .",
    "the method was motivated directly by the notion of low - level edge detector filters .",
    "these filters work well in practice , and they are ubiquitous in the first layer of both biological and artificial systems that learn on natural images .",
    "indeed , the four simple feature construction steps are directly based on an abstract notion of haar or gabor filters : homogeneous , locally connected patches of contrasting intensities . in a more high - level sense ,",
    "the technique is also inspired by a , perhaps naive , notion of how natural neural networks work : in the first layer they pick up correlations in stimuli , and settle in a `` simple '' theory of the world .",
    "next they pick up events when the correlations are broken , and assign a new units to the new `` edge '' features .",
    "yet another direct motivation comes from @xcite where they show that pixel order can be recovered using only feature correlations .",
    "once pixel order is recovered , one can immediately apply algorithms that explicitly use neighborhood - based filters . from this point of view , in this paper we show that it is possible to go from pixel correlations to feature construction , without going through the explicit mapping of the pixel order .    to validate the usefulness of the constructed features",
    ", we ran @xcite on multi - class classification problems .",
    "boosting is one of the best `` shallow '' multi - class classifiers , especially when the goal is to combine simple classifiers that act on small subsets a large set of possibly useful features . within multi - class boosting algorithms ,",
    "is the state of the art . on this statement",
    "we are at odds with recent results on multiclass boosting . on the two uci data sets we use in this paper ( and on others reported in @xcite ) , with hamming trees and products @xcite",
    "clearly outperforms @xcite , @xcite , and most importantly , the implementation of in @xcite , suggesting that was compared to a suboptimal implementation of in @xcite .",
    "our most significant result comes on the mnist set where we achieve a test error of @xmath0 with an algorithm which is essentially free of any image - specific priors ( e.g. , pixel order , preprocessing ) . on cifar-10 ,",
    "our method is suboptimal compared to today s best deep learning techniques , reproducing basically the results of the earliest attempts on the data set @xcite .",
    "the main point in these experiments is that the proposed method outperforms not only boosting on the raw pixels , but also boosting on haar filters .",
    "we also tried the technique on two relatively large uci data sets .",
    "the results here are essentially negative : the lack of correlations between the features do not allow us to improve significantly on `` shallow '' .",
    "the paper is organized as follows . in section  [ secconstructing ]",
    "we formally describe the method . in section  [ secexperiments ]",
    "we show experimental results , and we conclude with a discussion on future research in section  [ secconclusions ] .",
    "for the formal description of the method , let @xmath1 the training data , where @xmath2 are the input vectors and @xmath3 the labels .",
    "we will denote the input matrix and its elements by @xmath4_{i=1,\\ldots , n}^{j=1,\\ldots , d}$ ] , and its raw feature ( column ) vectors by @xmath5 .",
    "the algorithm consists of the following steps .    1 .",
    "we construct _ neighborhoods _",
    "@xmath6 for each feature vector @xmath7 , that is , sets of features that are correlated with @xmath7 .",
    "formally , @xmath8 , where @xmath9 is the correlation of two feature vectors and @xmath10 is a hyperparameter of the algorithm .",
    "2 .   we construct _ neighborhood features _ by using the neighborhoods as ( normalized ) filters , that is , @xmath11 for all @xmath12 and @xmath13 .",
    "3 .   we construct _ edges _ between neighborhoods by connecting correlated neighborhood features .",
    "formally , @xmath14 , where @xmath15 is a hyperparameter of the algorithm .",
    "we will denote elements of the set of edges by @xmath16 , and the size of the set by @xmath17 .",
    "4 .   we construct _ edge features _ by _ subtracting _ the responses to correlated neighborhoods of each other , that is , @xmath18 for all @xmath19 and @xmath20 .",
    "we concatenate neighborhood and edge features into a new representation of @xmath21 , that is , @xmath22 for all @xmath12 .",
    "both hyperparameters @xmath10 and @xmath15 threshold correlations , nevertheless , they have quite different roles : @xmath10 controls the neighborhood size whereas @xmath15 controls the distance of neighborhoods under which an edge ( that is , a significantly different response or a `` surprise '' ) is an interesting feature . in practice , we found that the results were rather insensitive to the value of these parameters in the @xmath23 $ ] interval . for now",
    "we manually set @xmath10 and @xmath15 in order to control the number of features . in our experiments we found that it was rarely detrimental to increase the number of features in terms of the asymptotic test error ( w.r.t . the number of boosting iterations @xmath24 ) but the convergence of slowed down if the number of features were larger than some thousands ( either because each boosting iteration took too much time , or because we had to seriously subsample the features in each iteration , essentially generating random trees , and so the number of iterations exploded ) .    on images ,",
    "where the dimensionality of the input space ( number of pixels ) is large , we subsample the pixels @xmath25_{j=1,\\ldots , d}$ ] before constructing the neighborhoods @xmath6 to control the number of neighborhood features , again , for computational rather than statistical reasons .",
    "we simply run with decision stumps in an autoassociative setup .",
    "a decision stump uses a single pixel as input and outputs a prediction on all the pixels .",
    "we take the first @xmath26 stumps that picks , and use the corresponding pixels ( a subset of the full set of pixels ) to construct the neighborhoods .    on small - dimensional ( non - image ) sets we face the opposite problem :",
    "the small number of input features limit the number of neighborhoods .",
    "this actually highlights a limitation of the algorithm : when the number of input features is small and they are not very correlated , the number of generated neighborhood and edge features is small , and they essentially contain the same information as the original features .",
    "nevertheless , we were curious whether we can see any improvement by blowing up the number of features ( similarly , in spirit , to what support vector machines do ) .",
    "we obtain a larger number of neighborhoods by defining a _ set _ of thresholds @xmath27 , and constructing @xmath28 `` concentric '' neighborhoods for each feature @xmath7 . on data sets with heterogeneous feature types",
    "it is also important to normalize the features by the usual transformation @xmath29 , where @xmath30 and @xmath31 denote the mean and the standard deviation of the elements of @xmath32 , respectively , before proceeding with the feature construction ( step  2 ) .",
    "optimally , of course , automatic hyperparameter optimization @xcite is the way to go , especially since also has two or three hyperparameters , and manual grid search in a four - to - five dimensional hyperparameter space is not feasible . for",
    "now , we set aside this issue for future work .",
    "the constructed features can be input to any `` shallow '' classifier .",
    "since we use with hamming trees , we briefly describe them here .",
    "the full formal description with the pseudocode is in the documentation of @xcite .",
    "it is available at the http://multiboost.org[multiboost.org ] website along with the code itself .",
    "the advantage of @xmath33 over other multi - class boosting approaches is that it does not require from the base learner to predict a single label @xmath34 for an input instance @xmath32 , rather , it uses _ vector - valued _ base learners @xmath35 .",
    "the requirement for these base learners is weaker : it suffices if the edge @xmath36 is slightly larger than zero , where @xmath37_{i=1,\\ldots , n}^{\\ell=1,\\ldots , k}$ ] is the weight matrix ( over instances and labels ) in the current boosting iteration , and @xmath38 is a @xmath39-valued one - hot code of the label .",
    "this makes it easy to turn weak _ binary _ classifiers into multi - class base classifiers , without requiring that the multi - class _ zero - one _ base error be less than @xmath40 . in case @xmath41",
    "is a decision tree , there are two important consequences . first , the size ( the number of leaves @xmath42 ) of the tree can be arbitrary and can be tuned freely ( whereas requiring a zero - one error to be less than @xmath40 usually implies large trees ) .",
    "second , one can design trees with binary @xmath43-valued outputs , which could not be used as standalone multi - class classifiers .    in a hamming tree , at each node , the split is learned by a multi - class decision stump of the form @xmath44 , where @xmath45 is a ( standard ) scalar @xmath39-valued decision stump , and @xmath46 is a @xmath47-valued vector . at leaf nodes ,",
    "the full @xmath43-valued vector @xmath48 is output for a given @xmath32 , whereas at inner nodes , only the binary function @xmath45 is used to decide whether the instance @xmath32 goes left or right .",
    "the tree is constructed top - down , and each node stump is optimized in a greedy manner , as usual . for a given @xmath32 , unless @xmath46 happens to be a one - hot vector , no single class can be output ( because of ties ) . at the same time",
    ", the tree is perfectly boostable : the weighted _ sum _ of hamming trees produces a _ real_-valued vector of length @xmath49 , of which the predicted class can be easily derived using the @xmath50 operator .",
    "we carried out experiments on four data sets : mnist and cifar-10 are standard image classification data sets , and the pendigit and letter sets are relatively large benchmarks from the uci repository .",
    "we boosted hamming trees @xcite on each data sets .",
    "boosted hamming trees have three hyperparameters : the number of boosting iterations @xmath24 , the number of leaves @xmath42 , and the number of ( random ) features @xmath26 considered at each split (  @xcite settings that give the flavor of a random forest to the final classifier ) . out of these three , we validated only the number of leaves @xmath42 in the `` classical '' way using @xmath51 single validation on the training set . since does not exhibit any overfitting even after a very large number of iterations ( see figure  [ figlearningcurves ] ) , we run it for a large number of @xmath52 iterations , and report the average test error of the last @xmath53 iterations .",
    "the number of ( random ) features @xmath26 considered at each split is another hyperparameter which does not have to be tuned in the traditional way . in our experience ,",
    "the larger it is , the smaller the _ asymptotic _ test error is . on the other hand ,",
    "the larger it is the slower the algorithm converges to this error .",
    "this means that @xmath26 controls the trade - off between the accuracy of the final classifier and the training time .",
    "we tuned it to obtain the full learning curves in reasonable time .",
    "the neighborhood and edge features were constructed as described in section  [ secconstructing ] .",
    "estimating correlations can be done robustly on relatively small random samples , so we ran the algorithm on a random input matrix @xmath54 with @xmath55 instances .",
    "mnist consists of @xmath56 grey - scale training images of hand - written digits of size @xmath57 . in all experiments on mnist ,",
    "@xmath26 was set to @xmath58 .",
    "the first baseline run was with hamming trees of @xmath59 leaves on the raw pixels ( green curve in figure  [ figmnist ] ) , achieving a test error of @xmath60 .",
    "we also ran with hamming trees of @xmath59 leaves in the roughly @xmath61-dimensional feature space generated by five types of haar filters ( @xcite ; red curve in figure  [ figmnist ] ) .",
    "this setup produced a test error of @xmath62 which is the state of the art among boosting algorithms .",
    "for generating neighborhood and edge features , we first ran autoassociative with decision stumps for @xmath63 iterations that picked the @xmath64 pixels depicted by the white pixels in figure  [ figmnistpixels ] .",
    "then we constructed @xmath64 neighborhood features using @xmath65 and @xmath66 edge features using @xmath67 .",
    "the @xmath58 most important features ( picked by running with decision stumps ) is depicted in figure  [ figmnistfilters ] .",
    "finally we ran with hamming trees of @xmath59 leaves on the constructed features ( blue curve in figure  [ figmnist ] ) , achieving a test error of @xmath0 which is one of the best results among methods that do not use explicit image priors ( pixel order , specific distortions , etc . ) .",
    "note that picked slightly more neighborhood than edge features relatively to their prior proportions . on the other hand",
    ", it was crucial to include both neighborhood and edge features : was way suboptimal on either subset .",
    "cifar-10 consists of @xmath68 color training images of 10 object categories of size @xmath69 , giving a total of @xmath70 features . in all experiments on cifar , @xmath26 was set to @xmath71 .",
    "the first baseline run was with hamming trees of @xmath72 leaves on the raw pixels ( green curve in figure  [ figcifar ] ) , achieving a test error of @xmath73 .",
    "we also ran with hamming trees of @xmath72 leaves in the roughly @xmath74-dimensional feature space generated by five types of haar filters ( @xcite ; red curve in figure  [ figcifar ] ) .",
    "this setup produced a test error of @xmath75 .",
    "for generating neighborhood and edge features , we first ran autoassociative with decision stumps for @xmath55 iterations that picked the @xmath76 color channels depicted by the white and colored pixels in figure  [ figcifarpixels ] . then we constructed @xmath76 neighborhood features using @xmath77 and @xmath78 edge features using @xmath79 .",
    "finally we ran with hamming trees of @xmath72 leaves on the constructed features ( blue curve in figure  [ figmnist ] ) , achieving a test error of @xmath80 .",
    "none of these results are close to the sate of the art , but they are not completely off the map , either : they match the performance of one of the early techniques that reported error on cifar-10 @xcite .",
    "the main significance of this experiment is that with neighborhood and edge features can beat not only on raw pixels but also with haar features .      in principle",
    ", there is no reason why neighborhood and edge features could not work on non - image sets . to investigate , we ran some preliminary tests on the relatively large uci data sets , pendigit and letter",
    ". both of them contain @xmath81 features and several thousand instances .",
    "the baseline results are @xmath82 on pendigit using with hamming trees of @xmath83 leaves ( green curve in figure  [ figpendigit ] ) and @xmath84 on letter using with hamming trees of @xmath72 leaves ( green curve in figure  [ figletter ] ) .",
    "we constructed neighborhoods using a set of thresholds @xmath85 , giving us @xmath86 unique neighborhoods on letter and @xmath87 unique neighborhoods on pendigit ( out of the possible @xmath88 ) .",
    "we then proceeded by constructing edge features with @xmath89 , giving us @xmath90 more features on letter , and @xmath91 more features on letter .",
    "we then ran with hamming trees of the same number of leaves as in the baseline experiments , using @xmath92 .",
    "on pendigit , we obtained @xmath93 , better than in the baseline ( blue curve in figure  [ figpendigit ] ) , while on letter we obtained @xmath94 , significantly worse than in the baseline ( blue curve in figure  [ figletter ] ) .",
    "we see two reasons why a larger gain is difficult on these sets .",
    "first , there is not much correlation between the features to exploit . indeed , setting @xmath10 and @xmath15 to similar values to those we used on the image sets , neighborhoods would have been very small , and there would have been almost no edges .",
    "second , with hamming trees is already a very good algorithm on these sets , so there is not much margin for improvement .",
    "besides running more experiments and tuning hyperparameters automatically , the most interesting question is whether stacking neighborhood and edge features would work .",
    "there is no technical problem of re - running the feature construction on the features obtained in the first round , but it is not clear whether there is any more structure this simple method can exploit .",
    "we did some preliminary trials on mnist where it did not improve the results , but this may be because mnist is a relatively simple set with not very complex features and rather homogeneous classes .",
    "experimenting with stacking on cifar is definitely the next step .",
    "another interesting avenue is to launch a large scale exploration on more non - image benchmark sets to see whether there is a subclass of sets where the correlation - based feature construction may work , and then to try to characterize this subclass .",
    "ranzato , m. , krizhevsky , a. , and hinton , g.  e. ( 2010 ) . factored 3-way restricted boltzmann machines for modeling natural images . in",
    "_ international conference on artificial intelligence and statistics_."
  ],
  "abstract_text": [
    "<S> motivated by an abstract notion of low - level edge detector filters , we propose a simple method of unsupervised feature construction based on pairwise statistics of features . in the first step , </S>",
    "<S> we construct neighborhoods of features by regrouping features that correlate . </S>",
    "<S> then we use these subsets as filters to produce new neighborhood features . </S>",
    "<S> next , we connect neighborhood features that correlate , and construct edge features by subtracting the correlated neighborhood features of each other . to validate the usefulness of the constructed features , we ran adaboost.mh on four multi - class classification problems . </S>",
    "<S> our most significant result is a test error of @xmath0 on mnist with an algorithm which is essentially free of any image - specific priors . on cifar-10 </S>",
    "<S> our method is suboptimal compared to today s best deep learning techniques , nevertheless , we show that the proposed method outperforms not only boosting on the raw pixels , but also boosting on haar filters . </S>"
  ]
}