{
  "article_text": [
    "in the present supplementary material we introduce two simple time series models and we compare how straightforward spectral methods and hierarchical methods are able to unveil the hierarchical properties of the models . the two models are hierarchically nested factor models .    as a first example , we consider a model ( already introduced in @xcite ) in which the @xmath4 variables follow a common factor @xmath103 and two other factors @xmath104 and @xmath105 which are affecting two distinct groups of @xmath106 and @xmath107 elements respectively .",
    "the equations of the model are @xmath108 where @xmath109 , @xmath110 , @xmath111 and @xmath112 @xmath113 are parameters . in this equation",
    "the factors @xmath114 and the terms @xmath115 are independent noise terms with zero mean and unit variance .",
    "we consider again variables @xmath50 with zero mean and unit variance without loss of generality .",
    "this choice fixes the value of @xmath112 .",
    "we set @xmath116 , @xmath117 and @xmath118 .",
    "the eigenvalue spectrum of the correlation coefficient matrix of this model has two large eigenvalues given by @xmath119/2 $ ] , where @xmath120 , @xmath121 eigenvalues equal to @xmath122 and @xmath123 eigenvalues equal to @xmath124 .",
    "thus , despite the fact that the original factor model of eq .",
    "( [ 2layer ] ) has three uncorrelated factors @xmath114 , @xmath125 , the spectrum has only two large eigenvalues .",
    "one could be tempted to interpret these large eigenvalues and the corresponding eigenvectors as describing the collective dynamics or the dynamics of the two groups . by analyzing the eigenvectors",
    ", it can be seen that this is not the case .",
    "the eigenvectors of the two largest eigenvalues have infra - group degenerate components and neither the first nor the second eigenvector is in general proportional to the vector @xmath126 representing the common behavior driven by the factor @xmath103 .",
    "similarly , when one attempts to associate the first two eigenvectors with the two groups , one is faced with the fact that the first two eigenvectors have all non vanishing components .",
    "our model indicates that the association between eigenvectors and factors is correct only in the limit when the system can be divided in groups of variables and each group is driven only by one factor .",
    "the generalization of the model to the case of heterogeneous @xmath43 parameters and/or the finiteness of empirical time series makes even more involved the task of associating factors with eigenvectors when the correlation matrix of the model has hierarchical features . on the other hand , by applying a hierarchical clustering procedure , e.g. single linkage , average linkage and complete linkage , to the correlation matrix of the model of eq .",
    "( [ 2layer ] ) one obtains the hierarchical tree of fig .",
    "[ dendro]a .",
    "the corresponding hnfm coincides with the model of eq .",
    "( [ 2layer ] ) .",
    "we have verified that by applying the bootstrap method we have introduced in our paper , we obtain back the hnfm of eq .",
    "( [ 2layer ] ) also when we take into account the role of a finite number of records of multivariate time series ( in our simulations we set @xmath89 and @xmath88 ) .",
    "moreover , simulations have been performed by assuming the variables @xmath127 being either gaussian distributed or student - t distributed with @xmath128 degrees of freedom . in both cases",
    "the recovered hnfm is the same and coincides with the model of eq .",
    "( [ 2layer ] ) .",
    "the threshold of reliability used to reduce the number of factors in the hnfm is @xmath129 and the hierarchical clustering algorithm used is the average linkage cluster analysis ( alca ) .",
    "this result shows that our method based on a hierarchical clustering procedure is able to recover the structure of the hnfm whereas basic spectral methods such as , for example , the principal component analysis are unable to uncover it . more specialized spectral methods , such as the varimax and the promax ( or oblique rotation ) methods @xcite , are in most cases also unable to transform the eigenvectors associated with the two large eigenvalues of the model of eq .",
    "( [ 2layer ] ) in such a way that each eigenvector has non - vanishing components only for the variables belonging to one of the two groups",
    ".    the second example we wish to consider is again a 3-factor model but with a completely nested structure .",
    "the equations of the model are : @xmath130 and , as in the previous case , we consider random variables with zero mean and unit variance .",
    "the dendrogram associated with this model is shown in fig .",
    "the eigenvalue spectrum of the correlation matrix has @xmath131 large eigenvalues and @xmath131 small eigenvalues each one with degeneracy @xmath132 .",
    "the most general case is analytically solvable but the eigenvalues and eigenvectors can not be expressed in a compact way .",
    "thus here we set @xmath133 and @xmath134 . with these simplifying parameters , the model of eq .",
    "( [ 3layer ] ) is depending only on the parameters @xmath71 and @xmath135 .",
    "the space described by the eigenvectors of the 3 largest eigenvalues is the space of vectors @xmath136 @xmath137 , i.e. the space of vectors with infra - group degenerate components .",
    "when @xmath138 , the first 3 eigenvalues are @xmath139 , @xmath140 and @xmath141 . since the components of the corresponding eigenvectors are defined only in terms of @xmath142 , @xmath23 and @xmath22 we represent eigenvectors as characterized by 3 parameters by using the formalism @xmath143 .",
    "it results that the non normalized eigenvectors are @xmath144 , @xmath145 and @xmath146 .",
    "this result implies that also in this case the first 3 eigenvalues are associated with eigenvectors with degenerate non vanishing infra - group components .",
    "moreover , none of these eigenvectors is proportional to the vector @xmath126 representing the common behavior driven by the factor @xmath103 . on the other hand , by applying the alca to the correlation matrix of the model of eq .",
    "( [ 3layer ] ) one obtains the dendrogram of fig 1b .",
    "the hnfm corresponding to this dendrogram coincides with the model of eq .",
    "( [ 3layer ] ) .        in summary",
    "this two examples of hnfm show that it is not always possible to associate the largest eigenvalues of the correlation matrix neither with specific groups of elements nor with all elements .",
    "it is also to notice that in the first example we have found 2 large eigenvalues in a system driven by 3 factors whereas in the second case we have observed 3 large eigenvalues for a model with 3 factors .",
    "this means that there is no direct relation between the number of factors in the hnfm and the number of large eigenvalues of the corresponding matrix @xmath6 .",
    "these results indicate that standard spectral methods are not always suitable for the analysis of systems in which hierarchies are present .",
    "ravasz e. , somera a.l.,mongru d.a . , oltvai z. n. & barabsi a .-",
    ", science * 297 * , 1551 - 1555 ( 2002 ) csete m. e. & doyle j. c. , science * 295 * , 1664 ( 2002 ) malone t. w. & crowston k. , acm computing surveys * 26 * , 87 ( 1994 ) mantegna r. n. , eur .",
    "j. b * 11 * , 193 ( 1999 ) .",
    "tumminello m. , aste t. , di matteo t. & mantegna r. n. , proc .",
    "usa * 102 * , 10421 ( 2005 ) .",
    "mardia k. v. , kent j. t. & bibby j. m. , _ multivariate analysis _ , ( academic press limited , san diego , ca , 1979 ) ."
  ],
  "abstract_text": [
    "<S> we show how to achieve a statistical description of the hierarchical structure of a multivariate data set . </S>",
    "<S> specifically we show that the similarity matrix resulting from a hierarchical clustering procedure is the correlation matrix of a factor model , the hierarchically nested factor model . in this model , factors are mutually independent and hierarchically organized . </S>",
    "<S> finally , we use a bootstrap based procedure to reduce the number of factors in the model with the aim of retaining only those factors significantly robust with respect to the statistical uncertainty due to the finite length of data records .    </S>",
    "<S> many complex systems observed in the physical , biological and social sciences are organized in a nested hierarchical structure , i.e. the elements of the system can be partitioned in clusters which in turn can be partitioned in subclusters and so on up to a certain level @xcite . </S>",
    "<S> several examples of hierarchically organized physical @xcite , biological @xcite and social @xcite systems have been investigated in the literature . </S>",
    "<S> the hierarchical structure of interactions among elements strongly affects the dynamics of complex systems . </S>",
    "<S> therefore , a quantitative description of hierarchical properties of the system is a key step in the modeling of complex systems . in this letter </S>",
    "<S> , we address the problem of inferring a factor model from a multivariate data set . </S>",
    "<S> a factor model is a mathematical model which attempts to explain the correlation between a large set of variables in terms of a small number of underlying factors . </S>",
    "<S> a major assumption of factor analysis is that it is not possible to observe these factors directly ; the variables depend upon the factors but are also subject to random errors @xcite . </S>",
    "<S> we show that the factor model we introduce fully describes the hierarchical structure of interactions among elements of the complex system . </S>",
    "<S> such a structure is elicited by hierarchical clustering of multivariate data . </S>",
    "<S> the analysis of multivariate data provides crucial information in the investigation of a wide variety of systems . </S>",
    "<S> multivariate analysis methods are designed to extract information both on the number of main factors characterizing the dynamics of the investigated system and on the composition of the groups ( clusters ) in which the system is intrinsically organized . </S>",
    "<S> recently , physicists started to contribute to the development of new multivariate techniques ( e.g. @xcite ) . among multivariate techniques , natural candidates for detecting the hierarchical structure of a set of data </S>",
    "<S> are hierarchical clustering methods @xcite . </S>",
    "<S> these methods allow to associate a dendrogram with a correlation matrix ( or more generally with a similarity matrix ) , i.e. they give a schematic description of hierarchies . </S>",
    "<S> it is worth pointing out that the whole information contained in the dendrogram can be stored in a filtered similarity matrix @xmath0 @xcite . </S>",
    "<S> the matrix @xmath1 has well defined metric properties . </S>",
    "<S> when the matrix @xmath1 of elements @xmath2 is obtained by starting from a correlation matrix , then the matrix of distances @xmath3 has ultrametric properties @xcite .    in this letter , we answer the following scientific question : given a multivariate data set is it possible to construct a factor model retaining the whole information about hierarchies which is detected by a hierarchical clustering ? in the following , we show that it is possible to give a description of hierarchies detected by hierarchical clustering in terms of a factor model , termed hierarchically nested factor model ( hnfm ) . </S>",
    "<S> this model is constructed in such a way that its correlation matrix coincides with the similarity matrix @xmath0 filtered by the chosen hierarchical clustering procedure . </S>",
    "<S> furthermore , for a hierarchical clustering performed by estimating a correlation matrix from an empirical data set which is unavoidably of finite size , i.e. a set of @xmath4 elements each characterized by a number @xmath5 of records , we provide a bootstrap based methodology allowing to remove from the model those factors which are characterized by a statistical reliability smaller than a predefined standard threshold , e.g. 95% . in this letter , we consider time series , however the results are general and also valid for any investigation of multivariate data . </S>",
    "<S> there are many clustering algorithms @xcite , here we use the average linkage cluster analysis ( alca ) . </S>",
    "<S> however , we wish to point out that our technique can be used with most clustering algorithms giving a dendrogram @xcite , such as , for example , the single linkage clustering algorithm .    hereafter , we provide a methodology to associate a nested factor model with a multivariate data set . </S>",
    "<S> the association is done by retaining all the information about the hierarchies detected by a hierarchical clustering . </S>",
    "<S> this is achieved by considering a factor model in bijective relation with a dendrogram ( or with the filtered matrix @xmath6),which is the output of a hierarchical clustering . </S>",
    "<S> we are going to introduce our method by making use of the illustrative dendrogram given in fig . </S>",
    "<S> [ dendro ] .     </S>",
    "<S> elements ( leaves in the tree ) . </S>",
    "<S> the symbols @xmath7 labels the @xmath8 internal nodes.,scaledwidth=45.0% ]    a dendrogram is a rooted tree , i.e. a tree in which a special node ( the root ) is singled out . in our example this node is @xmath9 . in the rooted tree , we distinguish between leaves and internal nodes . </S>",
    "<S> specifically , vertices of degree @xmath10 are representing leaves ( vertices labeled @xmath11 in fig . [ dendro ] ) while vertices of degree greater than 1 are representing internal nodes ( vertices labeled @xmath9 , @xmath12 , ... , </S>",
    "<S> @xmath13 in fig . </S>",
    "<S> [ dendro ] ) . </S>",
    "<S> we associate a _ genealogy _ </S>",
    "<S> @xmath14 ( @xmath15 ) with each leaf @xmath16 ( internal node @xmath17).the genealogy is the ordered set of internal nodes connecting leaf @xmath16 ( internal node @xmath17 ) to the root @xmath9 . for instance , in fig . </S>",
    "<S> [ dendro ] , the genealogy associated with the leaf 3 is @xmath18 and the genealogy of the internal node @xmath19 is @xmath20 . </S>",
    "<S> note that the internal node @xmath19 is included in @xmath21 . </S>",
    "<S> finally , we say that an internal node @xmath22 is the _ parent _ of the node @xmath23 , and we use the notation @xmath24 , if @xmath22 immediately precedes @xmath23 on the path from the root to @xmath23 . for example </S>",
    "<S> it is @xmath25 in fig . </S>",
    "<S> [ dendro ] . beside the topological structure , </S>",
    "<S> dendrograms obtained through standard hierarchical clustering algorithms applied to a correlation matrix have also metric properties . </S>",
    "<S> in fact , clustering algorithms associate a correlation coefficient @xmath26 with each internal node @xmath27 @xcite . </S>",
    "<S> our internal node labeling implies that @xmath28 and here we consider @xmath29 @xcite . </S>",
    "<S> the whole information about the rooted tree is stored in the @xmath30 matrix @xmath6 of elements @xmath31 , where @xmath32 is the first internal node in which leaves @xmath16 and @xmath33 are merged together @xcite . for example , in fig . </S>",
    "<S> [ dendro ] , it is @xmath34 and @xmath35 . in @xmath0 </S>",
    "<S> there are at most @xmath36 distinct coefficients . </S>",
    "<S> exactly @xmath36 distinct coefficients are obtained in case of binary rooted trees . since </S>",
    "<S> any rooted tree can be obtained from a rooted binary tree by introducing a degeneracy of nodes , in the following we consider binary rooted trees .    </S>",
    "<S> here we show that the matrix @xmath6 is the correlation matrix of a hnfm defined as @xmath37 where @xmath38 , @xmath39^{1/2}$ ] . the @xmath40 factor </S>",
    "<S> @xmath41 and @xmath42 are independent identically distributed ( i.i.d . ) </S>",
    "<S> random variables with zero mean and unit variance . in order to ensure that the correlation matrix of the model of eq .  </S>",
    "<S> ( [ model ] ) is @xmath6 , the @xmath43 parameters need to be chosen as @xmath44 where , assuming @xmath29 , all the coefficients @xmath45 are non negative real numbers . </S>",
    "<S> hereafter we show that the matrix @xmath6 is the correlation matrix of the factor model of eq . </S>",
    "<S> ( [ model ] ) with coefficients @xmath43 s given in eq .  </S>",
    "<S> ( [ coeffic ] ) . </S>",
    "<S> let us consider a generic pair of elements @xmath16 and @xmath33 merging together at the node @xmath32 corresponding to the correlation level @xmath46 . </S>",
    "<S> we prove that the cross correlation @xmath47 equals the correlation @xmath48 . </S>",
    "<S> in fact , the cross correlation @xmath47 depends only on the factors @xmath49 which are common to @xmath50 and @xmath51 . </S>",
    "<S> since we associate a factor with each internal node , we need to identify the internal nodes belonging to both the genealogies @xmath14 and @xmath52 . </S>",
    "<S> one can verify that @xmath53 . </S>",
    "<S> for example , in fig . </S>",
    "<S> [ dendro ] we have that @xmath54 and @xmath18 so that @xmath55 . by making use of eqs .  </S>",
    "<S> ( [ model ] , [ coeffic ] ) the cross correlation between variables @xmath50 and @xmath51 is @xmath56 for example , with reference to fig .  </S>",
    "<S> [ dendro ] , we have @xmath57 . thus the matrix @xmath6 is the correlation matrix associated with the factor model of eq . </S>",
    "<S> ( [ model ] ) . </S>",
    "<S> it is worth noting that the matrix @xmath6 is positive definite , because , as we have shown , @xmath6 is the correlation matrix of a factor model . in conclusion , </S>",
    "<S> the hnfm is a factor model taking into account the hierarchical properties of the investigated system which are elicited from data by hierarchical clustering .    </S>",
    "<S> it is worth pointing out that the simple investigation of the eigenvectors of the correlation matrix is not always suitable to detect the hierarchical structure and the group composition of the system . when the correlation matrix is block diagonal , the eigenvalue spectrum has a number of large eigenvalues equal to the number of groups . </S>",
    "<S> moreover , each corresponding eigenvector has non vanishing components only for the elements of a specific group . in this case </S>",
    "<S> spectral analysis directly allows to identify a partition of the variables . </S>",
    "<S> however these properties are no more true when the system is intrinsically hierarchically organized . </S>",
    "<S> in fact , the number of large eigenvalues can be different from the number of groups and the eigenvectors of the correlation matrix associated with large eigenvalues have in general all non vanishing components , i.e. large eigenvalues can not be associated with specific groups of variables . </S>",
    "<S> in the supplementary material of this paper we describe in detail two simple hnfms for which the direct eigenvectors analysis fails in identifying the groups and in unveiling the hierarchical structure of the system . </S>",
    "<S> this result suggests that it is not possible to associate the largest eigenvalues neither with specific groups of elements controlled by the same factors nor with a common behavior mode governing all elements of the system when the nested nature of groups of elements is significant . to make a specific example , </S>",
    "<S> consider a financial market . </S>",
    "<S> it has been recently suggested that there is a one to one association between the largest eigenvalues of the correlation matrix of stock returns with the global market behavior @xcite or specific economic sectors @xcite . </S>",
    "<S> if financial market is hierarchically organized ( as proposed below ) this association might be less straightforward than originally thought ( see also ref .  </S>",
    "<S> @xcite ) . in conclusion , </S>",
    "<S> basic spectral methods , such as principal component analysis , could be unable to fully describe the nested nature of hierarchical complex systems . for these cases </S>",
    "<S> our hnfm guarantees a proper hierarchical description of the elements of the investigated complex system . to the best of our knowledge </S>",
    "<S> , hnfm is the first model based on empirical data in which both the dependency of variables from factors is nested and the factors are independent one of each other . </S>",
    "<S> this choice allows to consider the hierarchical clustering procedure from a perspective which is different from the one which is commonly adopted . </S>",
    "<S> hierarchical clustering is not a tool which is only used to extract a partition of the elements but rather it is a tool that can also be used to associate a set of factors directly controlled by the genealogy of the element in the considered dendrogram with each element of the system . </S>",
    "<S> we believe this approach is useful in all the cases where a partition of the complex system is not straightforwardly feasible due to the fact that the system is clearly characterized by nested levels of hierarchies .    </S>",
    "<S> eq . </S>",
    "<S> ( [ model ] ) defines a hnfm of @xmath36 factors obtained from a dendrogram of @xmath4 elements . in general </S>",
    "<S> the number of factors determining the dynamics of the system can be significantly smaller than @xmath36 . </S>",
    "<S> moreover , several studies based on random matrix theory @xcite have shown that a correlation coefficient matrix obtained from a finite multivariate time series has associated an unavoidable statistical uncertainty that does not allow to discriminate between real and spurious factors . </S>",
    "<S> to overcome this problem , we propose here a method devised to select the hnfm characterized by the largest number of factors ( although in any case less than @xmath4 ) compatible with a predefined threshold of statistical reliability of retained factors . </S>",
    "<S> our method exploits the technique of non parametric bootstrap @xcite which is widely used in phylogenetic analysis .    </S>",
    "<S> the method is illustrated below after we briefly sketch the procedure used to associate a bootstrap value with each internal node of a dendrogram . </S>",
    "<S> consider a system of @xmath4 time series of length @xmath5 and suppose to collect data in a matrix @xmath58 with @xmath4 columns and @xmath5 rows . </S>",
    "<S> a bootstrap data matrix @xmath59 is formed by randomly sampling @xmath5 rows from the original data matrix @xmath58 allowing multiple sampling of the same row . for each replica @xmath59 , </S>",
    "<S> the associated correlation matrix @xmath60 is evaluated and a dendrogram is constructed by hierarchical clustering . </S>",
    "<S> some large number ( typically 1000 ) of independent bootstrap replicas is generated and for each internal node of the original data dendrogram we compute the fraction of bootstrap replicas ( commonly referred to as bootstrap value ) preserving the internal node in the dendrogram . </S>",
    "<S> given an internal node @xmath32 of the original dendrogram we say that a bootstrap replica is preserving that node if and only if a node @xmath61 in the replica dendrogram exists and identifies a branch characterized by the same leaves identified by @xmath32 in the original dendrogram . </S>",
    "<S> for instance , we say that the node @xmath62 of the dendrogram in fig . </S>",
    "<S> [ dendro ] is preserved in some replica dendrogram @xmath63 if and only if a node of @xmath63 exists such that it belongs to the genealogy of all and only the leaves 5 , 6 , 7 , 8 , 9 and 10 . </S>",
    "<S> the bootstrap technique allows to associate a bootstrap value with each internal node of a dendrogram . because of the one to one relation between nodes in the dendrogram and factors in the hnfm </S>",
    "<S> , the bootstrap value associated with a certain node of the dendrogram is associated also with the corresponding factor in the hnfm .    </S>",
    "<S> since the bootstrap value is a measure of the node s ( factor s ) reliability , we propose to remove those nodes ( factors ) with bootstrap value smaller than a given threshold @xmath64 . </S>",
    "<S> this is done by merging each node with a bootstrap value smaller than @xmath64 with its first ancestor node in the path to the root having a bootstrap value greater than @xmath64 and then by constructing the hnfm associated with this reduced dendrogram . </S>",
    "<S> the question is how to select a suitable threshold @xmath64 . </S>",
    "<S> the bootstrap value of a certain node ( factor ) can not be straightforwardly intended as the probability that the node ( factor ) belongs to the true and unknown hierarchy ( model ) of the system . </S>",
    "<S> for example , in phylogenetic analysis it has been shown @xcite that a bootstrap value of more than @xmath65 corresponds to a probability of more than @xmath66 that the true phylogeny has been found . by adapting the technique of hillis and bull @xcite , we do not choose _ a priori _ the value of @xmath64 but we infer a suitable value of the threshold from the data in a self consistent way . </S>",
    "<S> specifically , we choose a certain number of bootstrap value thresholds @xmath67 , e.g. @xmath68 . </S>",
    "<S> for each value of @xmath16 , we remove internal nodes from the dendrogram according to @xmath67 obtaining a reduced dendrogram @xmath69 and a corresponding hnfm labeled hnfm@xmath70 . for each value of @xmath16 , we perform @xmath71 simulations of data according to hnfm@xmath70 and we label @xmath72 with @xmath73 the data matrix of each simulation @xcite . to each @xmath72 </S>",
    "<S> we apply the clustering algorithm and the bootstrap node removal with the same threshold @xmath67 obtaining a reduced dendrogam @xmath74 . in order to compare the reduced dendrogram @xmath69 of the original data with the reduced dendrogram @xmath74 of the data simulation we measure the sensitivity @xmath75 and specificity @xmath76 ( see , for instance , @xcite ) . in our case , </S>",
    "<S> the sensitivity @xmath77 is the number of nodes in @xmath69 that are preserved in the reduced dendrogram @xmath74 divided by the total number of nodes in the reduced dendrogram @xmath69 . </S>",
    "<S> the specificity @xmath78 is the number of nodes in @xmath69 that are preserved in the reduced dendrogram @xmath74 divided by the total number of nodes in the reduced dendrogram @xmath74 . by averaging @xmath77 and @xmath78 over the @xmath71 different simulations we obtain the sensitivity @xmath79 and specificity @xmath80 of the node reduction associated with each bootstrap value threshold @xmath67 . </S>",
    "<S> finally , we obtain a measure of reliability of the dendrogram @xmath69 and of the corresponding hnfm@xmath70 obtained for each bootstrap value threshold @xmath67 , by averaging specificity and sensitivity @xmath81 @xcite . </S>",
    "<S> note that we have defined sensitivity and specificity in terms of the nodes of the dendrogram @xmath69 which are preserved in @xmath74 . in an equivalent way @xmath77 and @xmath78 can be defined in terms of the preserved factors in the corresponding models , hnfm@xmath70 and hnfm@xmath82 , i.e. the factors which determine the dynamics of exactly the same variables in both models . </S>",
    "<S> @xmath83 can be interpreted as the probability averaged over all factors of the hnfm@xmath70 that a hnfm@xmath84 contains a factor which is also present in the hnfm@xmath70 . </S>",
    "<S> removing factors from the hnfm reduces the quantity of the empirical variance explained by the model . </S>",
    "<S> therefore a satisfying bootstrap value threshold corresponds to the minimal value of @xmath67 such that @xmath83 is larger than some standard threshold of reliability , e.g. @xmath66 or @xmath85 . in the example shown in fig . </S>",
    "<S> [ reliability ] ( discussed below ) @xmath86 for @xmath87 . </S>",
    "<S> finally , it should be noted that no assumption about the data distribution is needed to implement the method . </S>",
    "<S> + we have concluded above that the matrix @xmath6 obtained by applying some hierarchical clustering technique to a correlation matrix is positive definite , provided that its elements are non negative numbers . </S>",
    "<S> of course the same holds true for the matrix of the hnmf reduced according to the described bootstrap technique .     as a function of the bootstrap value threshold . </S>",
    "<S> the error bar is one standard deviation . </S>",
    "<S> the dashed line indicates the chosen threshold of statistical reliability.,scaledwidth=49.0% ]    as an application of the described technique to real data we examine a system monitored by recording the set of daily equity returns of @xmath88 highly capitalized stocks traded at the new york stock exchange ( nyse ) during the period 1995 - 1998 ( @xmath89 ) . </S>",
    "<S> we apply the alca to the correlation matrix of the system and we obtain the dendrogram shown in fig . </S>",
    "<S> [ dendrodata ] . </S>",
    "<S> the dendrogram has @xmath90 nodes . </S>",
    "<S> the statistical reliability of these nodes is different from node to node due to metric and topological characteristics . </S>",
    "<S> the metric properties depend on the correlation coefficient values whereas the topological characteristics are depending on the ranking of these values and therefore on the complexity and number of hierarchies of the system .     </S>",
    "<S> highly capitalized stocks traded at the nyse during the period 1995 - 1998 obtained by applying the alca to the correlation matrix . </S>",
    "<S> colors are chosen according to the stock economic sector . </S>",
    "<S> specifically these sectors are basic materials ( violet ) , consumer cyclical ( tan ) , consumer non cyclical ( yellow ) , energy ( blue ) , services ( cyan ) , financial ( green ) , healthcare ( gray ) , technology ( red ) , utilities ( magenta ) , transportation ( brown ) , conglomerates ( orange ) and capital goods ( light green).,scaledwidth=49.0% ]    we use the bootstrap technique described above , in order to evaluate the statistical reliability of each node and to simplify the description in terms of a hnfm . </S>",
    "<S> in particular , we select the minimal bootstrap value threshold that guarantees a value of @xmath86 . </S>",
    "<S> we accordingly reduce the number of factors of the corresponding hnfm . in our investigation , the number of bootstrap replicas is @xmath91 and the number of simulations performed for each bootstrap value threshold is @xmath92 . </S>",
    "<S> simulated time series have been constructed by using original data . in fig . </S>",
    "<S> [ reliability ] we plot @xmath83 as a function of the bootstrap value threshold . </S>",
    "<S> a direct inspection shows that the bootstrap value threshold @xmath93 guarantees that @xmath94 . </S>",
    "<S> the corresponding reduced dendrogram has 23 nodes and it is reported in fig . </S>",
    "<S> [ nodered ] .    </S>",
    "<S> let us first comment the properties of the reduced hnfm . in the figure we observe several clusters and sub - clusters . </S>",
    "<S> as already noticed in previous studies @xcite , the detected clusters and sub - clusters are overlapping in part with economic classification such as the one provided by the forbes magazine . </S>",
    "<S> this can be seen in fig . </S>",
    "<S> [ dendrodata ] and [ nodered ] where we use this classification to characterize with a specific color each stock . </S>",
    "<S> for example , financial firms are represented in fig . </S>",
    "<S> [ dendrodata ] and [ nodered ] as green lines in the hierarchical tree . </S>",
    "<S> one prominent example is the group of financial stocks . for illustrative purposes , </S>",
    "<S> let us consider the equations of the financial elements of the reduced hnfm . </S>",
    "<S> the first three stocks from left to right of the group labeled as f in fig . </S>",
    "<S> [ nodered ] are described by the equation @xmath95 . </S>",
    "<S> the factor @xmath96 is common to all stocks and @xmath97 is common to all stocks except one , with tick symbol hm , which is a gold company . </S>",
    "<S> the factor @xmath98 is specific to these financial stocks ( their tick symbols are bac , jpm and mer ) . </S>",
    "<S> the other six financial stocks also belonging to the same group ( indicated by the tick symbols agc , aig , axp , one , wfc and usb ) are described by the equations @xmath99 . in this last case only the @xmath100 factor is present in addition to the @xmath96 and @xmath97 factors common to all financial stocks . since the factor </S>",
    "<S> @xmath100 is determining the dynamics of only financial stocks ( 9 out of 10 in the investigated sample ) , it is natural to consider @xmath100 as a factor characterizing financial stocks whereas @xmath98 is an additional factor further characterizing only the three stocks bac , jpm and mer . </S>",
    "<S> a similar organization in nested clusters is observed in all the groups detected by the reduced hnfm . </S>",
    "<S> the number of factors characterizing the various stocks is ranging from one to five . </S>",
    "<S> it is worth noting that each group of stocks , which are sharing at least 3 factors , is homogeneous with respect to the economic sector .    </S>",
    "<S> it is also worth to compare fig . </S>",
    "<S> [ dendrodata ] and [ nodered ] . </S>",
    "<S> the comparison shows that the self - consistent reduction of the number of factors allows a robust statistical validation of the groups that are detected from the data analysis . </S>",
    "<S> only the information which is statistically robust at the @xmath66 level is retained in the reduced hnfm . </S>",
    "<S> for example , the energy cluster observed in fig . </S>",
    "<S> [ dendrodata ] ( blue lines in the figure ) is not robust at the selected confidence level , whereas the two clusters indicated as e1 and e2 in fig . </S>",
    "<S> [ nodered ] , corresponding to the sub - sectors _ oil well services and equipment _ and _ oil and gas integrated _ , are robust . in fig . </S>",
    "<S> [ nodered ] all the detected clusters of more than @xmath101 elements and consistent with the forbes classification are indicated by rectangles at the bottom of the figure . </S>",
    "<S> the economic characterization of clusters is discussed in the figure caption .    ) of @xmath102 stock daily returns traded at the nyse during the period 1995 - 1998 . </S>",
    "<S> rectangles at the bottom are indicating 9 clusters and symbols label the classification of stocks in terms of economic sectors or sub - sectors according to the classification of forbes magazine . </S>",
    "<S> specifically , e1 is the sub - sector of _ oil well services and equipment _ and e2 is the sub - sector of _ oil and gas integrated_. both e1 and e2 belong to the economic sector of _ energy _ ; t and f are indicating the economic sectors of _ technology _ and _ financial _ respectively ; h indicates the sub - sector _ major drugs _ of the economic sector _ healthcare _ ; bm indicates a cluster of stocks within the _ basic material _ economic sector . s1 and s2 indicate the two sub - sectors of _ communication services _ and _ retail _ of the sector of _ services _ respectively . finally , u is representing the sub - sector _ </S>",
    "<S> electric utilities _ of the sector _ </S>",
    "<S> utilities_. colors are chosen according to the stock economic sector as described in the caption of fig .  </S>",
    "<S> [ dendrodata ] and the ordering of the stocks is the same as in fig .  </S>",
    "<S> [ dendrodata ] . </S>",
    "<S> the labeled internal nodes are discussed in the text . in the figure </S>",
    "<S> we do not comment on clusters composed by only two leaves.,scaledwidth=49.0% ]    in summary , we have introduced a method for associating a hierarchical factor model with a multivariate data set . the factor model is retaining all the information about hierarchies extracted from data by a hierarchical clustering procedure . </S>",
    "<S> we have also provided a bootstrap based procedure to obtain the hnfm with the largest number of factors compatible with a predefined threshold of their statistical reliability . </S>",
    "<S> this procedure selects in a self - consistent way the optimal bootstrap threshold for the considered set of data . </S>",
    "<S> we have also shown that the similarity matrix @xmath6 , which is the output of hierarchical clustering procedures , is the proper correlation matrix of our model and therefore it is positive definite . finally , we have used the hnfm to model a financial system of 100 highly capitalized stocks traded at nyse . </S>",
    "<S> this empirical analysis has shown the ability of hnfm in the modeling of a complex system characterized by nested levels of hierarchies inferred from data . </S>"
  ]
}