{
  "article_text": [
    "kullback - leibler ( kl ) divergence ( relative entropy ) can be considered as a measure of the difference / dissimilarity between sources .",
    "estimating kl divergence from finite realizations of a stochastic process with unknown memory is a long - standing problem , with interesting mathematical aspects and useful applications to automatic categorization of symbolic sequences .",
    "namely , an empirical estimation of the divergence can be used to classify sequences ( for approaches to this problem using other methods , in particular true metric distances , see @xcite , @xcite ; see also @xcite ) .    in @xcite ziv and",
    "merhav showed how to estimate the kl divergence between two sources , using the parsing scheme of lz77 algorithm @xcite on two finite length realizations .",
    "they proved the consistence of the method by showing that the estimate of the divergence for two markovian sources converges to their relative entropy when the length of the sequences diverges .",
    "furthermore they proposed this estimator as a tool for an `` universal classification '' of sequences .",
    "a procedure based on the implementations of lz77 algorithm ( gzip , winzip ) is proposed in @xcite .",
    "the estimate obtained of the relative entropy is then used to construct phylogenetic trees for languages and is proposed as a tool to solve authorship attribution problems . moreover ,",
    "the relation between the relative entropy and the estimate given by this procedure is analyzed in @xcite .",
    "two different algorithms are proposed and analyzed in @xcite , see also @xcite . the first one is based on the burrows - wheeler block sorting transform @xcite , while the other uses the context tree weighting method .",
    "the authors proved the consistence of these approximation methods and show that these methods outperform the others in experiments .    in @xcite",
    "it is shown how to construct an entropy estimator for stationary ergodic stochastic sources using non - sequential recursive pairs substitutions method , introduced in @xcite ( see also @xcite and references therein for similar approaches ) .    in this paper",
    "we want to discuss the use of similar techniques to construct an estimator of relative ( and cross ) entropy between a pair of stochastic sources .",
    "in particular we investigate how the asymptotic properties of concurrent pair substitutions might be used to construct an optimal ( in the sense of convergence ) relative entropy estimator .",
    "a second relevant question arises about the computational efficiency of the derived indicator .",
    "while here we address the first , mostly mathematical , question , we leave the computational and applicative aspects for forthcoming research .",
    "the paper is structured as follows : in section [ sec : notations ] we state the notations , in section [ sec : nsrps ] we describe the details of the non - sequential recursive pair substitutions ( nsrps ) method , in section [ sec : scaling ] we prove that nsrps preserve the cross and the relative entropy , in section [ sec : convergence ] we prove the main result : we can obtain an estimate of the relative entropy by calculating the 1-block relative entropy of the sequences we obtain using the nsrps method .",
    "we introduce here the main definitions and notations , often following the formalism used in @xcite . given a finite alphabet @xmath0 , we denote with @xmath1 the set of finite words .",
    "given a word @xmath2 , we denote by @xmath3 its length and if @xmath4 and @xmath5 , we use @xmath6 to indicate the subword @xmath7 .",
    "we use similar notations for one - sided infinite ( elements of @xmath8 ) or double infinite words ( elements of @xmath9 ) .",
    "often sequences will be seen as finite or infinite realizations of discrete - time stochastic stationary , ergodic processes of a random variable @xmath10 with values in @xmath0 .",
    "the @xmath11-th order joint distributions @xmath12 identify the process and its elements follow the consistency conditions : @xmath13 when no confusion will arise , the subscript @xmath11 will be omitted , and we will just use @xmath14 to denote both the measure of the cylinder and the probability of the finite word .",
    "equivalently , a distribution of a process can also be defined by specifying the initial one - character distribution @xmath15 and the successive conditional distributions : @xmath16 given an ergodic , stationary stochastic source we define as usual :    @xmath17    @xmath18 where @xmath19 denotes the concatenated word @xmath20 and @xmath21 is just the process average .",
    "@xmath22    the following properties and results are very well known @xcite , but at the same time quite important for the proofs and the techniques developed here ( and also in @xcite ) :    * @xmath23 * a process @xmath24 is @xmath25-markov if and only if @xmath26 . * _ entropy theorem _ : for almost all realizations of the process , we have @xmath27    in this paper we focus on properties involving pairs of stochastic sources on the same alphabet with distributions @xmath24 and @xmath28 , namely _",
    "cross entropy _ and the related _ relative entropy _ ( or _ kullback leibler divergence _ ) :    _ n - conditional cross entropy _ @xmath29 _",
    "cross entropy _",
    "@xmath30 _ relative entropy ( kullback - leibler divergence ) _",
    "@xmath31 note that @xmath32 moreover we stress that , if @xmath28 is k - markov then , for any @xmath24 @xmath33 namely @xmath34 for any @xmath35 : @xmath36 & = - \\sum_{\\omega \\in a^{l - k},\\,b\\in a^k,\\,a\\in a } \\mu ( \\omega ba ) \\log \\nu(a\\vert b ) \\\\ & =    - \\sum_{b\\in a^k,\\,a\\in a } \\mu(ba ) \\log \\nu(a\\vert b)= h_k(\\mu\\|\\nu ) \\end{array}\\ ] ] note that @xmath37 depends only on the two - symbol distribution of @xmath24 .",
    "entropy and cross entropy can be related to the asymptotic behavior of properly defined _ returning times _ and _ waiting times _ , respectively .",
    "more precisely , given an ergodic , stationary process @xmath24 , a sample sequence @xmath38 and @xmath39 , we define the returning time of the first @xmath11 characters as :    @xmath40    similarly , given two realizations @xmath41 and @xmath42 of @xmath24 and @xmath28 respectively , we define the    @xmath43    obviously @xmath44 .",
    "we now have the following two important results :    [ returning ] if @xmath24 is a stationary , ergodic process , then @xmath45    [ waiting ] if @xmath24 is stationary and ergodic , @xmath28 is k - markov and the marginals @xmath12 of @xmath24 are dominated by the corresponding marginals @xmath46 of @xmath28 , i.e. @xmath47 , then @xmath48",
    "we now introduce a family of transformations on sequences and the corresponding operators on distributions : given @xmath49 ( including @xmath50 ) , @xmath51 and @xmath52 , a _ pair substitution _ is a map @xmath53 which substitutes sequentially , from left to right , the occurrences of @xmath54 with @xmath55 . for example @xmath56 or : @xmath57 @xmath58 is always an injective but not surjective map that can be immediately extended also to infinite sequences @xmath59 .",
    "the action of @xmath60 shorten the original sequence : we denote by @xmath61 the inverse of the contraction rate : @xmath62 for @xmath24-_typical _ sequences we can pass to the limit and define : @xmath63    an important remark is that if we start from a source where admissible words are described by constraints on consecutive symbols , this property will remain true even after an arbitrary pair substitution . in other words ( see theorem 2.1 in @xcite ) : a pair substitution maps pair constraints in pair constraints .",
    "a pair substitution @xmath64 naturally induces a map on the set of ergodic stationary measures on @xmath65 by mapping typical sequences w.r.t . the original measure @xmath24 in typical sequences w.r.t .",
    "the transformed measure @xmath66 : given @xmath67 then ( theorem 2.2 in @xcite ) @xmath68 exists and is constant @xmath24 almost everywhere in @xmath69 , moreover @xmath70 are the marginals of an ergodic measure on @xmath71 .    again in @xcite ,",
    "the following results are proved showing how entropies transform under the action of @xmath72 , with expanding factor @xmath73 :    _ invariance of entropy _",
    "@xmath74 _ decreasing of the 1-conditional entropy _",
    "@xmath75 moreover , @xmath76 maps 1-markov measures in 1-markov measures .",
    "in fact : @xmath77 _ decreasing of the k - conditional entropy _",
    "@xmath78 moreover @xmath76 maps @xmath25-markov measures in @xmath25-markov measures .",
    "while later on we will give another proof of the first fact , we remark that this property , together with the decrease of the 1-conditional entropy , reflect , roughly speaking , the fact that the amount of information of @xmath79 , which is equal to that of @xmath80 , is more concentrated on the pairs of consecutive symbols .",
    "as we are interested in sequences of recursive pair substitutions , we assume to start with an initial alphabet @xmath0 and define an increasing alphabet sequence @xmath81 , @xmath82 ,  @xmath83 ,  .",
    "given @xmath84 and chosen @xmath85 ( not necessarily different ) :    * we indicate with @xmath86 a new symbol and define the new alphabet as @xmath87 ; * we denote with @xmath88 the substitution map @xmath89 which substitutes whit @xmath90 the occurrences of the pair @xmath91 in the strings on the alphabet @xmath92 ; * we denote with @xmath93 the corresponding map from the measures on @xmath94 to the measures on @xmath95 ; * we define by @xmath96 the corresponding normalization factor @xmath97 .",
    "we use the over - line to denote iterated quantities : @xmath98 and also @xmath99    the asymptotic properties of @xmath100 clearly depend on the pairs chosen in the substitutions .",
    "in particular , if at any step @xmath84 the chosen pair @xmath91 is the pair of maximum of frequency of @xmath101 then ( theorem 4.1 in @xcite ) : @xmath102    regarding the asymptotic properties of the entropy we have the following theorem that rigorously show that @xmath103 becomes asymptotically 1-markov :    if @xmath102 then @xmath104    the main results of this paper is the generalization of this theorem to the cross and relative entropy .    before entering in the details of our construction let us sketch here the main steps .",
    "in particular let us consider the cross entropy ( the same argument will apply to the relative entropy ) of the measure @xmath24 with respect to the measure @xmath28 : i.e. @xmath105 .    as we will show , but for the normalization factor @xmath106 , this is equal to the cross entropy of the measure @xmath107 w.r.t the measure @xmath108 : @xmath109    moreover , as we have seen above , if we choose the substitution in a suitable way ( for instance if at any step we substitute the pair with maximum frequency ) then @xmath110 and the measure @xmath108 becomes asymptotically 1-markov as @xmath111 .",
    "interestingly , we do not know if @xmath112 also diverges ( we will discuss this point in the sequel ) .",
    "nevertheless , noticing that the cross entropy of a 1-markov source w.r.t a generic ergodic source is equal to the 1-markov cross entropy between the two sources , it is reasonable to expect that the cross entropy @xmath105 can be obtained as the following limit :    @xmath113    this is exactly what we will prove in the two next sections .",
    "we first show how the relative entropy between two stochastic process @xmath24 and @xmath28 scales after acting with the _ same _ pair substitution on both sources to have @xmath66 and @xmath114 .",
    "more precisely we make use of theorem [ waiting ] and have the following :    [ main1 ] if @xmath24 is ergodic , @xmath28 is a markov chain and @xmath47 , then if @xmath60 is a pair substitution @xmath115    _ proof . _ to fix the notations , let us denote by @xmath116 and @xmath117 the infinite realizations of the process of measure @xmath24 and @xmath28 respectively , and by @xmath118 and @xmath119 the corresponding finite substrings .",
    "let us denote by @xmath49 the characters involved in the pair substitution @xmath58 .",
    "moreover let us denote the waiting time with the shorter notation : @xmath120 we now explore how the waiting time rescale with respect to the transformation @xmath60 : we consider the first time we see the sequence @xmath121 inside the sequence @xmath122 . to start with , we assume that @xmath123 as we can always consider th .",
    "[ waiting ] for realizations with a fixed prefix of positive probability .",
    "moreover we choose a subsequence @xmath124 such that @xmath125 is the smallest @xmath126 such that @xmath127 . of course @xmath128 as @xmath129 . in this case , it is easy to observe that @xmath130 then , using theorem [ waiting ] @xmath131 = \\nonumber\\\\ & = & \\lim_{i\\to + \\infty } \\frac{n_i}{|g(w_1^{n_i})|}\\frac{\\log|g(w_1^{t_{n_i}})|}{n_i}= \\nonumber\\\\ & = & \\lim_{i\\to + \\infty } \\frac{n_i}{|g(w_1^{n_i})|}\\left[\\frac{1}{n_i}\\log ( t_{n_i } ) + \\frac{1}{n_i}\\log\\left(\\frac{|g(w_1^{t_{n_i}})|}{t_{n_i}}\\right)\\right]=\\nonumber\\\\ & = & z^{\\mu } h(\\mu\\|\\nu ) \\label{kl1}\\end{aligned}\\ ] ] where in the last step we used the fact that @xmath132 as @xmath129 , the definition of @xmath133 and theorem [ waiting ] for @xmath24 and @xmath28 . note that for @xmath134 , equation ( [ kl1 ] ) reproduces the content of theorem 3.1 of @xcite : @xmath135 that thus implies @xmath136 note that the limit in th . [ waiting ]",
    "is almost surely unique and then the initial restrictive assumption @xmath137 and the use of the subsequence @xmath125 have no consequences on the thesis ; this concludes the proof .",
    "@xmath138    before discussing the convergence of relative entropy under successive substitutions we go thorough a simple explicit example of the theorem [ main1 ] , in order to show the difficulties we deal with , when we try to use the explicit expressions of the transformed measures we find in @xcite",
    ".    _ example .",
    "_ we treat here the most simple case : @xmath24 and @xmath28 are bernoulli binary processes with parameters @xmath139 and @xmath140 respectively .",
    "we consider the substitution @xmath141 given by @xmath142 .",
    "it is long but easy to verify that @xmath66 is a stationary , ergodic , 1-markov with equilibrium state @xmath143 where @xmath144 .",
    "for example , given a @xmath66-generic sequence @xmath145 , corresponding to a @xmath24-generic sequence @xmath146 ( @xmath147 ) : @xmath148 clearly : @xmath149    using the same argument as before , it is now possible to write down the probability distribution of pair of characters for @xmath66 . again",
    "the following holds for a generic process : @xmath150 \\frac{{\\mathcal g}\\mu(10)}z= \\mu(10)-\\mu(010 ) -\\mu(101)+\\mu(0101 ) &   \\frac{{\\mathcal g}\\mu(11)}z=",
    "\\mu(11)-\\mu(011 ) &   \\frac{{\\mathcal g}\\mu(12)}z= \\mu(101)-\\mu(0101)\\\\[4pt ] \\frac{{\\mathcal g}\\mu(20)}z= \\mu(010)-\\mu(0101 ) &   \\frac{{\\mathcal g}\\mu(21)}z= \\mu(011 ) & \\frac{{\\mathcal g}\\mu(22)}z= \\mu(0101 ) \\end{array}\\ ] ]    it is easy to see that @xmath151 .",
    "now we can write the transition matrix @xmath152 for the process @xmath66 as @xmath153 : @xmath154 for bernoulli processes : @xmath155    we now denote with @xmath156 the transition matrix for @xmath157 .",
    "for the two 1-markov processes , we have @xmath158 via straightforward calculations , using the product structure of the measure @xmath24 : @xmath159\\\\ + z\\mu(11)\\left[\\mu(00)\\log\\frac{\\mu(00)}{\\nu(00)}+\\mu(1)\\log\\frac{\\mu(1)}{\\nu(1)}+\\mu(01)\\log\\frac{\\mu(01)}{\\nu(01)}\\right]\\\\ + z\\mu(01)\\left[\\mu(00)\\log\\frac{\\mu(00)}{\\nu(00)}+\\mu(1)\\log\\frac{\\mu(1)}{\\nu(1)}+\\mu(01)\\log\\frac{\\mu(01)}{\\nu(01)}\\right]\\\\ = z\\mu(00 ) d(\\mu\\vert\\vert\\nu)+ z\\mu(1)\\left[\\mu(00)\\log\\frac{\\mu(00)}{\\nu(00)}+\\mu(1)\\log\\frac{\\mu(1)}{\\nu(1)}+\\mu(01)\\log\\frac{\\mu(01)}{\\nu(01)}\\right]\\\\ =   z\\mu(00 ) d(\\mu\\vert\\vert\\nu)+z\\mu(1)\\left[\\mu(0)d(\\mu\\vert\\vert\\nu)+ d(\\mu\\vert\\vert\\nu)\\right]\\\\ = z d(\\mu\\vert\\vert\\nu ) ( \\mu(00)+\\mu(10)+\\mu(1))\\\\ = z d(\\mu\\vert\\vert\\nu)\\end{aligned}\\ ] ]",
    "we now prove that the renormalized 1-markov cross entropy between @xmath12 and @xmath46 converges to the cross - entropy between @xmath160 and @xmath161 as the number of pair substitution @xmath11 goes to @xmath162 .    more precisely :    [ main2 ] if @xmath163 as @xmath164 , @xmath113    _ proof .",
    "_ let us define , as in @xcite the following operators on the ergodic measures : @xmath165 is the projection operator that maps a measure to its 1-markov approximation , whereas @xmath166 is the operator such that for any arbitrary @xmath28 @xmath167 we notice ( see @xcite for the details ) that the normalization constant for @xmath168 is the same of that for @xmath28 : @xmath169    the measure @xmath168 is not @xmath170-markov , but we know that it becomes 1-markov after @xmath84 steps of substitutions , in fact it becomes @xmath171 .",
    "moreover , as discussed in @xcite , it is an approximation of @xmath28 if @xmath172 diverges : for any @xmath80 of length @xmath25 , @xmath173 now it is easy to establish the following chain of equalities : @xmath174 where we have used the conservation of the cross entropy @xmath175 and the fact that @xmath176 if @xmath177 are 1-markov , as shown in eq .",
    "[ h - k - markov ] . to conclude the proof we have to show that @xmath178 this is an easy consequence of eq .",
    "[ convergenza ] the definition [ hk ] and eq .",
    "[ hktoh ] .",
    "it is important to remark that we are assuming the divergence of @xmath179 too , as not being necessary for the convergence to the ( rescaled ) two - characters relative entropy .",
    "nevertheless , it would be interesting to understand both the topological and statistical constraints that prevent or permit the divergence of the expanding factor @xmath179 .",
    "experimentally , it seems that if we start with two measures with finite relative entropy ( i.e. with absolutely continuous marginals ) , then if we choose the standard strategy ( most frequent pair substitution ) for the sequence of pair substitutions that yields the divergence of @xmath180 , we also simultaneously obtain the divergence of @xmath181 ( see for instance fig . [",
    "fig : z ] ) .        on the other hand",
    ", it seems possible to consider particular sources and particular strategies of pairs substitutions withdiverging @xmath180 , that prevent the divergence of @xmath181 . at this moment",
    "we do not have conclusive rigorous mathematical results on this subject .    finally , let us note that th .",
    "[ main2 ] do not give directly an algorithm to estimate the relative entropy : in any implementation we would have to specify the `` optimal '' number of pairs substitutions , with respect to the length of initial sequences and also with respect to the dimension of the initial alphabet .",
    "namely , in the estimate we have to take into account at least two correction terms , which diverges with @xmath84 : the entropy cost of writing the substitutions and the entropy cost of writing the frequencies of the pairs of characters in the alphabet we obtain after the substitutions ( or equivalent quantities if we use , for instance , arithmetic codings modeling the two character frequencies ) .    for what concerns possible implementations of the method it is important to notice that the nsrps procedure can be implemented in linear time @xcite .",
    "therefore it seems reasonable that reasonably fast algorithms to compute relative entropy via nsrps can be designed .",
    "anyway , preliminary numerical experiments show that for sources of finite memory this method seems to have the same limitations of that based on parsing procedures , with respect to the methods based on the analysis of context introduced in @xcite .    in fig .",
    "[ fig : h ] we show the convergence of the estimates of the entropies of the two sources and of the cross entropy , given th .",
    "[ main2 ] , for two markov process of memory 5 . in this case , the numbers of substitutions @xmath182 is small with respect to the length of the sequences @xmath183 , then the correction terms are negligible .",
    "let us finally note that the cross entropy estimate might show large variations for particular values of @xmath84 .",
    "this could be interpreted by the fact that for these values of @xmath84 pairs with particular relevance for one source with respect to the other have been substituted .",
    "this example suggest that the nsrps method for the estimation of the cross entropy should be useful in sequences analysis , for example in order to detect strings with a peculiar statistical role .",
    "99      d. benedetto , e. caglioti , d. gabrielli : non - sequential recursive pair substitution : some rigorous results .",
    "_ issn : 1742 - 5468 ( on line ) * 09 * pp . 121 doi:10.1088/1742.-5468/2006/09/p09011 ( 2006 )"
  ],
  "abstract_text": [
    "<S> the entropy of an ergodic source is the limit of properly rescaled 1-block entropies of sources obtained applying successive non - sequential recursive pairs substitutions @xcite,@xcite . in this paper </S>",
    "<S> we prove that the cross entropy and the kullback - leibler divergence can be obtained in a similar way .    </S>",
    "<S> _ keywords _ : information theory , source and channel coding , relative entropy . </S>"
  ]
}